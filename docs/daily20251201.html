<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251127.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis", "author": "Yiwei Li and Jiannong Cao and Penghui Ruan and Divya Saxena and Songye Zhu and Yinfeng Cao", "abstract": "Gaussian Splatting has been considered as a novel way for view synthesis of dynamic scenes, which shows great potential in AIoT applications such as digital twins. However, recent dynamic Gaussian Splatting methods significantly degrade when only sparse input views are available, limiting their applicability in practice. The issue arises from the incoherent learning of 4D geometry as input views decrease. This paper presents GC-4DGS, a novel framework that infuses geometric consistency into 4D Gaussian Splatting (4DGS), offering real-time and high-quality dynamic scene rendering from sparse input views. While learning-based Multi-View Stereo (MVS) and monocular depth estimators (MDEs) provide geometry priors, directly integrating these with 4DGS yields suboptimal results due to the ill-posed nature of sparse-input 4D geometric optimization. To address these problems, we introduce a dynamic consistency checking strategy to reduce estimation uncertainties of MVS across spacetime. Furthermore, we propose a global-local depth regularization approach to distill spatiotemporal-consistent geometric information from monocular depths, thereby enhancing the coherent geometry and appearance learning within the 4D volume. Extensive experiments on the popular N3DV and Technicolor datasets validate the effectiveness of GC-4DGS in rendering quality without sacrificing efficiency. Notably, our method outperforms RF-DeRF, the latest dynamic radiance field tailored for sparse-input dynamic view synthesis, and the original 4DGS by 2.62dB and 1.58dB in PSNR, respectively, with seamless deployability on resource-constrained IoT edge devices.", "link": "http://arxiv.org/abs/2511.23044v1", "date": "2025-11-28", "relevancy": 3.4976, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7212}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7075}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Consistent%204D%20Gaussian%20Splatting%20for%20Sparse-Input%20Dynamic%20View%20Synthesis&body=Title%3A%20Geometry-Consistent%204D%20Gaussian%20Splatting%20for%20Sparse-Input%20Dynamic%20View%20Synthesis%0AAuthor%3A%20Yiwei%20Li%20and%20Jiannong%20Cao%20and%20Penghui%20Ruan%20and%20Divya%20Saxena%20and%20Songye%20Zhu%20and%20Yinfeng%20Cao%0AAbstract%3A%20Gaussian%20Splatting%20has%20been%20considered%20as%20a%20novel%20way%20for%20view%20synthesis%20of%20dynamic%20scenes%2C%20which%20shows%20great%20potential%20in%20AIoT%20applications%20such%20as%20digital%20twins.%20However%2C%20recent%20dynamic%20Gaussian%20Splatting%20methods%20significantly%20degrade%20when%20only%20sparse%20input%20views%20are%20available%2C%20limiting%20their%20applicability%20in%20practice.%20The%20issue%20arises%20from%20the%20incoherent%20learning%20of%204D%20geometry%20as%20input%20views%20decrease.%20This%20paper%20presents%20GC-4DGS%2C%20a%20novel%20framework%20that%20infuses%20geometric%20consistency%20into%204D%20Gaussian%20Splatting%20%284DGS%29%2C%20offering%20real-time%20and%20high-quality%20dynamic%20scene%20rendering%20from%20sparse%20input%20views.%20While%20learning-based%20Multi-View%20Stereo%20%28MVS%29%20and%20monocular%20depth%20estimators%20%28MDEs%29%20provide%20geometry%20priors%2C%20directly%20integrating%20these%20with%204DGS%20yields%20suboptimal%20results%20due%20to%20the%20ill-posed%20nature%20of%20sparse-input%204D%20geometric%20optimization.%20To%20address%20these%20problems%2C%20we%20introduce%20a%20dynamic%20consistency%20checking%20strategy%20to%20reduce%20estimation%20uncertainties%20of%20MVS%20across%20spacetime.%20Furthermore%2C%20we%20propose%20a%20global-local%20depth%20regularization%20approach%20to%20distill%20spatiotemporal-consistent%20geometric%20information%20from%20monocular%20depths%2C%20thereby%20enhancing%20the%20coherent%20geometry%20and%20appearance%20learning%20within%20the%204D%20volume.%20Extensive%20experiments%20on%20the%20popular%20N3DV%20and%20Technicolor%20datasets%20validate%20the%20effectiveness%20of%20GC-4DGS%20in%20rendering%20quality%20without%20sacrificing%20efficiency.%20Notably%2C%20our%20method%20outperforms%20RF-DeRF%2C%20the%20latest%20dynamic%20radiance%20field%20tailored%20for%20sparse-input%20dynamic%20view%20synthesis%2C%20and%20the%20original%204DGS%20by%202.62dB%20and%201.58dB%20in%20PSNR%2C%20respectively%2C%20with%20seamless%20deployability%20on%20resource-constrained%20IoT%20edge%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Consistent%25204D%2520Gaussian%2520Splatting%2520for%2520Sparse-Input%2520Dynamic%2520View%2520Synthesis%26entry.906535625%3DYiwei%2520Li%2520and%2520Jiannong%2520Cao%2520and%2520Penghui%2520Ruan%2520and%2520Divya%2520Saxena%2520and%2520Songye%2520Zhu%2520and%2520Yinfeng%2520Cao%26entry.1292438233%3DGaussian%2520Splatting%2520has%2520been%2520considered%2520as%2520a%2520novel%2520way%2520for%2520view%2520synthesis%2520of%2520dynamic%2520scenes%252C%2520which%2520shows%2520great%2520potential%2520in%2520AIoT%2520applications%2520such%2520as%2520digital%2520twins.%2520However%252C%2520recent%2520dynamic%2520Gaussian%2520Splatting%2520methods%2520significantly%2520degrade%2520when%2520only%2520sparse%2520input%2520views%2520are%2520available%252C%2520limiting%2520their%2520applicability%2520in%2520practice.%2520The%2520issue%2520arises%2520from%2520the%2520incoherent%2520learning%2520of%25204D%2520geometry%2520as%2520input%2520views%2520decrease.%2520This%2520paper%2520presents%2520GC-4DGS%252C%2520a%2520novel%2520framework%2520that%2520infuses%2520geometric%2520consistency%2520into%25204D%2520Gaussian%2520Splatting%2520%25284DGS%2529%252C%2520offering%2520real-time%2520and%2520high-quality%2520dynamic%2520scene%2520rendering%2520from%2520sparse%2520input%2520views.%2520While%2520learning-based%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520and%2520monocular%2520depth%2520estimators%2520%2528MDEs%2529%2520provide%2520geometry%2520priors%252C%2520directly%2520integrating%2520these%2520with%25204DGS%2520yields%2520suboptimal%2520results%2520due%2520to%2520the%2520ill-posed%2520nature%2520of%2520sparse-input%25204D%2520geometric%2520optimization.%2520To%2520address%2520these%2520problems%252C%2520we%2520introduce%2520a%2520dynamic%2520consistency%2520checking%2520strategy%2520to%2520reduce%2520estimation%2520uncertainties%2520of%2520MVS%2520across%2520spacetime.%2520Furthermore%252C%2520we%2520propose%2520a%2520global-local%2520depth%2520regularization%2520approach%2520to%2520distill%2520spatiotemporal-consistent%2520geometric%2520information%2520from%2520monocular%2520depths%252C%2520thereby%2520enhancing%2520the%2520coherent%2520geometry%2520and%2520appearance%2520learning%2520within%2520the%25204D%2520volume.%2520Extensive%2520experiments%2520on%2520the%2520popular%2520N3DV%2520and%2520Technicolor%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520GC-4DGS%2520in%2520rendering%2520quality%2520without%2520sacrificing%2520efficiency.%2520Notably%252C%2520our%2520method%2520outperforms%2520RF-DeRF%252C%2520the%2520latest%2520dynamic%2520radiance%2520field%2520tailored%2520for%2520sparse-input%2520dynamic%2520view%2520synthesis%252C%2520and%2520the%2520original%25204DGS%2520by%25202.62dB%2520and%25201.58dB%2520in%2520PSNR%252C%2520respectively%252C%2520with%2520seamless%2520deployability%2520on%2520resource-constrained%2520IoT%2520edge%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Consistent%204D%20Gaussian%20Splatting%20for%20Sparse-Input%20Dynamic%20View%20Synthesis&entry.906535625=Yiwei%20Li%20and%20Jiannong%20Cao%20and%20Penghui%20Ruan%20and%20Divya%20Saxena%20and%20Songye%20Zhu%20and%20Yinfeng%20Cao&entry.1292438233=Gaussian%20Splatting%20has%20been%20considered%20as%20a%20novel%20way%20for%20view%20synthesis%20of%20dynamic%20scenes%2C%20which%20shows%20great%20potential%20in%20AIoT%20applications%20such%20as%20digital%20twins.%20However%2C%20recent%20dynamic%20Gaussian%20Splatting%20methods%20significantly%20degrade%20when%20only%20sparse%20input%20views%20are%20available%2C%20limiting%20their%20applicability%20in%20practice.%20The%20issue%20arises%20from%20the%20incoherent%20learning%20of%204D%20geometry%20as%20input%20views%20decrease.%20This%20paper%20presents%20GC-4DGS%2C%20a%20novel%20framework%20that%20infuses%20geometric%20consistency%20into%204D%20Gaussian%20Splatting%20%284DGS%29%2C%20offering%20real-time%20and%20high-quality%20dynamic%20scene%20rendering%20from%20sparse%20input%20views.%20While%20learning-based%20Multi-View%20Stereo%20%28MVS%29%20and%20monocular%20depth%20estimators%20%28MDEs%29%20provide%20geometry%20priors%2C%20directly%20integrating%20these%20with%204DGS%20yields%20suboptimal%20results%20due%20to%20the%20ill-posed%20nature%20of%20sparse-input%204D%20geometric%20optimization.%20To%20address%20these%20problems%2C%20we%20introduce%20a%20dynamic%20consistency%20checking%20strategy%20to%20reduce%20estimation%20uncertainties%20of%20MVS%20across%20spacetime.%20Furthermore%2C%20we%20propose%20a%20global-local%20depth%20regularization%20approach%20to%20distill%20spatiotemporal-consistent%20geometric%20information%20from%20monocular%20depths%2C%20thereby%20enhancing%20the%20coherent%20geometry%20and%20appearance%20learning%20within%20the%204D%20volume.%20Extensive%20experiments%20on%20the%20popular%20N3DV%20and%20Technicolor%20datasets%20validate%20the%20effectiveness%20of%20GC-4DGS%20in%20rendering%20quality%20without%20sacrificing%20efficiency.%20Notably%2C%20our%20method%20outperforms%20RF-DeRF%2C%20the%20latest%20dynamic%20radiance%20field%20tailored%20for%20sparse-input%20dynamic%20view%20synthesis%2C%20and%20the%20original%204DGS%20by%202.62dB%20and%201.58dB%20in%20PSNR%2C%20respectively%2C%20with%20seamless%20deployability%20on%20resource-constrained%20IoT%20edge%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2511.23044v1&entry.124074799=Read"},
{"title": "DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management", "author": "Casimir Feldmann and Maximum Wilder-Smith and Vaishakh Patil and Michael Oechsle and Michael Niemeyer and Keisuke Tateno and Marco Hutter", "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.", "link": "http://arxiv.org/abs/2511.23030v1", "date": "2025-11-28", "relevancy": 3.3158, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7437}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6432}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiskChunGS%3A%20Large-Scale%203D%20Gaussian%20SLAM%20Through%20Chunk-Based%20Memory%20Management&body=Title%3A%20DiskChunGS%3A%20Large-Scale%203D%20Gaussian%20SLAM%20Through%20Chunk-Based%20Memory%20Management%0AAuthor%3A%20Casimir%20Feldmann%20and%20Maximum%20Wilder-Smith%20and%20Vaishakh%20Patil%20and%20Michael%20Oechsle%20and%20Michael%20Niemeyer%20and%20Keisuke%20Tateno%20and%20Marco%20Hutter%0AAbstract%3A%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20impressive%20results%20for%20novel%20view%20synthesis%20with%20real-time%20rendering%20capabilities.%20However%2C%20integrating%203DGS%20with%20SLAM%20systems%20faces%20a%20fundamental%20scalability%20limitation%3A%20methods%20are%20constrained%20by%20GPU%20memory%20capacity%2C%20restricting%20reconstruction%20to%20small-scale%20environments.%20We%20present%20DiskChunGS%2C%20a%20scalable%203DGS%20SLAM%20system%20that%20overcomes%20this%20bottleneck%20through%20an%20out-of-core%20approach%20that%20partitions%20scenes%20into%20spatial%20chunks%20and%20maintains%20only%20active%20regions%20in%20GPU%20memory%20while%20storing%20inactive%20areas%20on%20disk.%20Our%20architecture%20integrates%20seamlessly%20with%20existing%20SLAM%20frameworks%20for%20pose%20estimation%20and%20loop%20closure%2C%20enabling%20globally%20consistent%20reconstruction%20at%20scale.%20We%20validate%20DiskChunGS%20on%20indoor%20scenes%20%28Replica%2C%20TUM-RGBD%29%2C%20urban%20driving%20scenarios%20%28KITTI%29%2C%20and%20resource-constrained%20Nvidia%20Jetson%20platforms.%20Our%20method%20uniquely%20completes%20all%2011%20KITTI%20sequences%20without%20memory%20failures%20while%20achieving%20superior%20visual%20quality%2C%20demonstrating%20that%20algorithmic%20innovation%20can%20overcome%20the%20memory%20constraints%20that%20have%20limited%20previous%203DGS%20SLAM%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiskChunGS%253A%2520Large-Scale%25203D%2520Gaussian%2520SLAM%2520Through%2520Chunk-Based%2520Memory%2520Management%26entry.906535625%3DCasimir%2520Feldmann%2520and%2520Maximum%2520Wilder-Smith%2520and%2520Vaishakh%2520Patil%2520and%2520Michael%2520Oechsle%2520and%2520Michael%2520Niemeyer%2520and%2520Keisuke%2520Tateno%2520and%2520Marco%2520Hutter%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520demonstrated%2520impressive%2520results%2520for%2520novel%2520view%2520synthesis%2520with%2520real-time%2520rendering%2520capabilities.%2520However%252C%2520integrating%25203DGS%2520with%2520SLAM%2520systems%2520faces%2520a%2520fundamental%2520scalability%2520limitation%253A%2520methods%2520are%2520constrained%2520by%2520GPU%2520memory%2520capacity%252C%2520restricting%2520reconstruction%2520to%2520small-scale%2520environments.%2520We%2520present%2520DiskChunGS%252C%2520a%2520scalable%25203DGS%2520SLAM%2520system%2520that%2520overcomes%2520this%2520bottleneck%2520through%2520an%2520out-of-core%2520approach%2520that%2520partitions%2520scenes%2520into%2520spatial%2520chunks%2520and%2520maintains%2520only%2520active%2520regions%2520in%2520GPU%2520memory%2520while%2520storing%2520inactive%2520areas%2520on%2520disk.%2520Our%2520architecture%2520integrates%2520seamlessly%2520with%2520existing%2520SLAM%2520frameworks%2520for%2520pose%2520estimation%2520and%2520loop%2520closure%252C%2520enabling%2520globally%2520consistent%2520reconstruction%2520at%2520scale.%2520We%2520validate%2520DiskChunGS%2520on%2520indoor%2520scenes%2520%2528Replica%252C%2520TUM-RGBD%2529%252C%2520urban%2520driving%2520scenarios%2520%2528KITTI%2529%252C%2520and%2520resource-constrained%2520Nvidia%2520Jetson%2520platforms.%2520Our%2520method%2520uniquely%2520completes%2520all%252011%2520KITTI%2520sequences%2520without%2520memory%2520failures%2520while%2520achieving%2520superior%2520visual%2520quality%252C%2520demonstrating%2520that%2520algorithmic%2520innovation%2520can%2520overcome%2520the%2520memory%2520constraints%2520that%2520have%2520limited%2520previous%25203DGS%2520SLAM%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiskChunGS%3A%20Large-Scale%203D%20Gaussian%20SLAM%20Through%20Chunk-Based%20Memory%20Management&entry.906535625=Casimir%20Feldmann%20and%20Maximum%20Wilder-Smith%20and%20Vaishakh%20Patil%20and%20Michael%20Oechsle%20and%20Michael%20Niemeyer%20and%20Keisuke%20Tateno%20and%20Marco%20Hutter&entry.1292438233=Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20impressive%20results%20for%20novel%20view%20synthesis%20with%20real-time%20rendering%20capabilities.%20However%2C%20integrating%203DGS%20with%20SLAM%20systems%20faces%20a%20fundamental%20scalability%20limitation%3A%20methods%20are%20constrained%20by%20GPU%20memory%20capacity%2C%20restricting%20reconstruction%20to%20small-scale%20environments.%20We%20present%20DiskChunGS%2C%20a%20scalable%203DGS%20SLAM%20system%20that%20overcomes%20this%20bottleneck%20through%20an%20out-of-core%20approach%20that%20partitions%20scenes%20into%20spatial%20chunks%20and%20maintains%20only%20active%20regions%20in%20GPU%20memory%20while%20storing%20inactive%20areas%20on%20disk.%20Our%20architecture%20integrates%20seamlessly%20with%20existing%20SLAM%20frameworks%20for%20pose%20estimation%20and%20loop%20closure%2C%20enabling%20globally%20consistent%20reconstruction%20at%20scale.%20We%20validate%20DiskChunGS%20on%20indoor%20scenes%20%28Replica%2C%20TUM-RGBD%29%2C%20urban%20driving%20scenarios%20%28KITTI%29%2C%20and%20resource-constrained%20Nvidia%20Jetson%20platforms.%20Our%20method%20uniquely%20completes%20all%2011%20KITTI%20sequences%20without%20memory%20failures%20while%20achieving%20superior%20visual%20quality%2C%20demonstrating%20that%20algorithmic%20innovation%20can%20overcome%20the%20memory%20constraints%20that%20have%20limited%20previous%203DGS%20SLAM%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.23030v1&entry.124074799=Read"},
{"title": "Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin", "author": "Andy Huynh and Jo\u00e3o Malheiro Silva and Holger Caesar and Tong Duy Son", "abstract": "3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.", "link": "http://arxiv.org/abs/2511.20348v2", "date": "2025-11-28", "relevancy": 3.2814, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6948}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.642}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Material-informed%20Gaussian%20Splatting%20for%203D%20World%20Reconstruction%20in%20a%20Digital%20Twin&body=Title%3A%20Material-informed%20Gaussian%20Splatting%20for%203D%20World%20Reconstruction%20in%20a%20Digital%20Twin%0AAuthor%3A%20Andy%20Huynh%20and%20Jo%C3%A3o%20Malheiro%20Silva%20and%20Holger%20Caesar%20and%20Tong%20Duy%20Son%0AAbstract%3A%203D%20reconstruction%20for%20Digital%20Twins%20often%20relies%20on%20LiDAR-based%20methods%2C%20which%20provide%20accurate%20geometry%20but%20lack%20the%20semantics%20and%20textures%20naturally%20captured%20by%20cameras.%20Traditional%20LiDAR-camera%20fusion%20approaches%20require%20complex%20calibration%20and%20still%20struggle%20with%20certain%20materials%20like%20glass%2C%20which%20are%20visible%20in%20images%20but%20poorly%20represented%20in%20point%20clouds.%20We%20propose%20a%20camera-only%20pipeline%20that%20reconstructs%20scenes%20using%203D%20Gaussian%20Splatting%20from%20multi-view%20images%2C%20extracts%20semantic%20material%20masks%20via%20vision%20models%2C%20converts%20Gaussian%20representations%20to%20mesh%20surfaces%20with%20projected%20material%20labels%2C%20and%20assigns%20physics-based%20material%20properties%20for%20accurate%20sensor%20simulation%20in%20modern%20graphics%20engines%20and%20simulators.%20This%20approach%20combines%20photorealistic%20reconstruction%20with%20physics-based%20material%20assignment%2C%20providing%20sensor%20simulation%20fidelity%20comparable%20to%20LiDAR-camera%20fusion%20while%20eliminating%20hardware%20complexity%20and%20calibration%20requirements.%20We%20validate%20our%20camera-only%20method%20using%20an%20internal%20dataset%20from%20an%20instrumented%20test%20vehicle%2C%20leveraging%20LiDAR%20as%20ground%20truth%20for%20reflectivity%20validation%20alongside%20image%20similarity%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20348v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterial-informed%2520Gaussian%2520Splatting%2520for%25203D%2520World%2520Reconstruction%2520in%2520a%2520Digital%2520Twin%26entry.906535625%3DAndy%2520Huynh%2520and%2520Jo%25C3%25A3o%2520Malheiro%2520Silva%2520and%2520Holger%2520Caesar%2520and%2520Tong%2520Duy%2520Son%26entry.1292438233%3D3D%2520reconstruction%2520for%2520Digital%2520Twins%2520often%2520relies%2520on%2520LiDAR-based%2520methods%252C%2520which%2520provide%2520accurate%2520geometry%2520but%2520lack%2520the%2520semantics%2520and%2520textures%2520naturally%2520captured%2520by%2520cameras.%2520Traditional%2520LiDAR-camera%2520fusion%2520approaches%2520require%2520complex%2520calibration%2520and%2520still%2520struggle%2520with%2520certain%2520materials%2520like%2520glass%252C%2520which%2520are%2520visible%2520in%2520images%2520but%2520poorly%2520represented%2520in%2520point%2520clouds.%2520We%2520propose%2520a%2520camera-only%2520pipeline%2520that%2520reconstructs%2520scenes%2520using%25203D%2520Gaussian%2520Splatting%2520from%2520multi-view%2520images%252C%2520extracts%2520semantic%2520material%2520masks%2520via%2520vision%2520models%252C%2520converts%2520Gaussian%2520representations%2520to%2520mesh%2520surfaces%2520with%2520projected%2520material%2520labels%252C%2520and%2520assigns%2520physics-based%2520material%2520properties%2520for%2520accurate%2520sensor%2520simulation%2520in%2520modern%2520graphics%2520engines%2520and%2520simulators.%2520This%2520approach%2520combines%2520photorealistic%2520reconstruction%2520with%2520physics-based%2520material%2520assignment%252C%2520providing%2520sensor%2520simulation%2520fidelity%2520comparable%2520to%2520LiDAR-camera%2520fusion%2520while%2520eliminating%2520hardware%2520complexity%2520and%2520calibration%2520requirements.%2520We%2520validate%2520our%2520camera-only%2520method%2520using%2520an%2520internal%2520dataset%2520from%2520an%2520instrumented%2520test%2520vehicle%252C%2520leveraging%2520LiDAR%2520as%2520ground%2520truth%2520for%2520reflectivity%2520validation%2520alongside%2520image%2520similarity%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20348v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Material-informed%20Gaussian%20Splatting%20for%203D%20World%20Reconstruction%20in%20a%20Digital%20Twin&entry.906535625=Andy%20Huynh%20and%20Jo%C3%A3o%20Malheiro%20Silva%20and%20Holger%20Caesar%20and%20Tong%20Duy%20Son&entry.1292438233=3D%20reconstruction%20for%20Digital%20Twins%20often%20relies%20on%20LiDAR-based%20methods%2C%20which%20provide%20accurate%20geometry%20but%20lack%20the%20semantics%20and%20textures%20naturally%20captured%20by%20cameras.%20Traditional%20LiDAR-camera%20fusion%20approaches%20require%20complex%20calibration%20and%20still%20struggle%20with%20certain%20materials%20like%20glass%2C%20which%20are%20visible%20in%20images%20but%20poorly%20represented%20in%20point%20clouds.%20We%20propose%20a%20camera-only%20pipeline%20that%20reconstructs%20scenes%20using%203D%20Gaussian%20Splatting%20from%20multi-view%20images%2C%20extracts%20semantic%20material%20masks%20via%20vision%20models%2C%20converts%20Gaussian%20representations%20to%20mesh%20surfaces%20with%20projected%20material%20labels%2C%20and%20assigns%20physics-based%20material%20properties%20for%20accurate%20sensor%20simulation%20in%20modern%20graphics%20engines%20and%20simulators.%20This%20approach%20combines%20photorealistic%20reconstruction%20with%20physics-based%20material%20assignment%2C%20providing%20sensor%20simulation%20fidelity%20comparable%20to%20LiDAR-camera%20fusion%20while%20eliminating%20hardware%20complexity%20and%20calibration%20requirements.%20We%20validate%20our%20camera-only%20method%20using%20an%20internal%20dataset%20from%20an%20instrumented%20test%20vehicle%2C%20leveraging%20LiDAR%20as%20ground%20truth%20for%20reflectivity%20validation%20alongside%20image%20similarity%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.20348v2&entry.124074799=Read"},
{"title": "Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation", "author": "M\u00e1t\u00e9 T\u00f3th and P\u00e9ter Kov\u00e1cs and R\u00e9ka Bencses and Zolt\u00e1n Bendefy and Zolt\u00e1n Hortsin and Bal\u00e1zs Ter\u00e9ki and Tam\u00e1s Matuszka", "abstract": "Neural reconstruction models for autonomous driving simulation have made significant strides in recent years, with dynamic models becoming increasingly prevalent. However, these models are typically limited to handling in-domain objects closely following their original trajectories. We introduce a hybrid approach that combines the strengths of neural reconstruction with physics-based rendering. This method enables the virtual placement of traditional mesh-based dynamic agents at arbitrary locations, adjustments to environmental conditions, and rendering from novel camera viewpoints. Our approach significantly enhances novel view synthesis quality -- especially for road surfaces and lane markings -- while maintaining interactive frame rates through our novel training method, NeRF2GS. This technique leverages the superior generalization capabilities of NeRF-based methods and the real-time rendering speed of 3D Gaussian Splatting (3DGS). We achieve this by training a customized NeRF model on the original images with depth regularization derived from a noisy LiDAR point cloud, then using it as a teacher model for 3DGS training. This process ensures accurate depth, surface normals, and camera appearance modeling as supervision. With our block-based training parallelization, the method can handle large-scale reconstructions (greater than or equal to 100,000 square meters) and predict segmentation masks, surface normals, and depth maps. During simulation, it supports a rasterization-based rendering backend with depth-based composition and multiple camera models for real-time camera simulation, as well as a ray-traced backend for precise LiDAR simulation.", "link": "http://arxiv.org/abs/2503.09464v2", "date": "2025-11-28", "relevancy": 3.1846, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6489}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6371}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Rendering%20for%20Multimodal%20Autonomous%20Driving%3A%20Merging%20Neural%20and%20Physics-Based%20Simulation&body=Title%3A%20Hybrid%20Rendering%20for%20Multimodal%20Autonomous%20Driving%3A%20Merging%20Neural%20and%20Physics-Based%20Simulation%0AAuthor%3A%20M%C3%A1t%C3%A9%20T%C3%B3th%20and%20P%C3%A9ter%20Kov%C3%A1cs%20and%20R%C3%A9ka%20Bencses%20and%20Zolt%C3%A1n%20Bendefy%20and%20Zolt%C3%A1n%20Hortsin%20and%20Bal%C3%A1zs%20Ter%C3%A9ki%20and%20Tam%C3%A1s%20Matuszka%0AAbstract%3A%20Neural%20reconstruction%20models%20for%20autonomous%20driving%20simulation%20have%20made%20significant%20strides%20in%20recent%20years%2C%20with%20dynamic%20models%20becoming%20increasingly%20prevalent.%20However%2C%20these%20models%20are%20typically%20limited%20to%20handling%20in-domain%20objects%20closely%20following%20their%20original%20trajectories.%20We%20introduce%20a%20hybrid%20approach%20that%20combines%20the%20strengths%20of%20neural%20reconstruction%20with%20physics-based%20rendering.%20This%20method%20enables%20the%20virtual%20placement%20of%20traditional%20mesh-based%20dynamic%20agents%20at%20arbitrary%20locations%2C%20adjustments%20to%20environmental%20conditions%2C%20and%20rendering%20from%20novel%20camera%20viewpoints.%20Our%20approach%20significantly%20enhances%20novel%20view%20synthesis%20quality%20--%20especially%20for%20road%20surfaces%20and%20lane%20markings%20--%20while%20maintaining%20interactive%20frame%20rates%20through%20our%20novel%20training%20method%2C%20NeRF2GS.%20This%20technique%20leverages%20the%20superior%20generalization%20capabilities%20of%20NeRF-based%20methods%20and%20the%20real-time%20rendering%20speed%20of%203D%20Gaussian%20Splatting%20%283DGS%29.%20We%20achieve%20this%20by%20training%20a%20customized%20NeRF%20model%20on%20the%20original%20images%20with%20depth%20regularization%20derived%20from%20a%20noisy%20LiDAR%20point%20cloud%2C%20then%20using%20it%20as%20a%20teacher%20model%20for%203DGS%20training.%20This%20process%20ensures%20accurate%20depth%2C%20surface%20normals%2C%20and%20camera%20appearance%20modeling%20as%20supervision.%20With%20our%20block-based%20training%20parallelization%2C%20the%20method%20can%20handle%20large-scale%20reconstructions%20%28greater%20than%20or%20equal%20to%20100%2C000%20square%20meters%29%20and%20predict%20segmentation%20masks%2C%20surface%20normals%2C%20and%20depth%20maps.%20During%20simulation%2C%20it%20supports%20a%20rasterization-based%20rendering%20backend%20with%20depth-based%20composition%20and%20multiple%20camera%20models%20for%20real-time%20camera%20simulation%2C%20as%20well%20as%20a%20ray-traced%20backend%20for%20precise%20LiDAR%20simulation.%0ALink%3A%20http%3A//arxiv.org/abs/2503.09464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Rendering%2520for%2520Multimodal%2520Autonomous%2520Driving%253A%2520Merging%2520Neural%2520and%2520Physics-Based%2520Simulation%26entry.906535625%3DM%25C3%25A1t%25C3%25A9%2520T%25C3%25B3th%2520and%2520P%25C3%25A9ter%2520Kov%25C3%25A1cs%2520and%2520R%25C3%25A9ka%2520Bencses%2520and%2520Zolt%25C3%25A1n%2520Bendefy%2520and%2520Zolt%25C3%25A1n%2520Hortsin%2520and%2520Bal%25C3%25A1zs%2520Ter%25C3%25A9ki%2520and%2520Tam%25C3%25A1s%2520Matuszka%26entry.1292438233%3DNeural%2520reconstruction%2520models%2520for%2520autonomous%2520driving%2520simulation%2520have%2520made%2520significant%2520strides%2520in%2520recent%2520years%252C%2520with%2520dynamic%2520models%2520becoming%2520increasingly%2520prevalent.%2520However%252C%2520these%2520models%2520are%2520typically%2520limited%2520to%2520handling%2520in-domain%2520objects%2520closely%2520following%2520their%2520original%2520trajectories.%2520We%2520introduce%2520a%2520hybrid%2520approach%2520that%2520combines%2520the%2520strengths%2520of%2520neural%2520reconstruction%2520with%2520physics-based%2520rendering.%2520This%2520method%2520enables%2520the%2520virtual%2520placement%2520of%2520traditional%2520mesh-based%2520dynamic%2520agents%2520at%2520arbitrary%2520locations%252C%2520adjustments%2520to%2520environmental%2520conditions%252C%2520and%2520rendering%2520from%2520novel%2520camera%2520viewpoints.%2520Our%2520approach%2520significantly%2520enhances%2520novel%2520view%2520synthesis%2520quality%2520--%2520especially%2520for%2520road%2520surfaces%2520and%2520lane%2520markings%2520--%2520while%2520maintaining%2520interactive%2520frame%2520rates%2520through%2520our%2520novel%2520training%2520method%252C%2520NeRF2GS.%2520This%2520technique%2520leverages%2520the%2520superior%2520generalization%2520capabilities%2520of%2520NeRF-based%2520methods%2520and%2520the%2520real-time%2520rendering%2520speed%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520We%2520achieve%2520this%2520by%2520training%2520a%2520customized%2520NeRF%2520model%2520on%2520the%2520original%2520images%2520with%2520depth%2520regularization%2520derived%2520from%2520a%2520noisy%2520LiDAR%2520point%2520cloud%252C%2520then%2520using%2520it%2520as%2520a%2520teacher%2520model%2520for%25203DGS%2520training.%2520This%2520process%2520ensures%2520accurate%2520depth%252C%2520surface%2520normals%252C%2520and%2520camera%2520appearance%2520modeling%2520as%2520supervision.%2520With%2520our%2520block-based%2520training%2520parallelization%252C%2520the%2520method%2520can%2520handle%2520large-scale%2520reconstructions%2520%2528greater%2520than%2520or%2520equal%2520to%2520100%252C000%2520square%2520meters%2529%2520and%2520predict%2520segmentation%2520masks%252C%2520surface%2520normals%252C%2520and%2520depth%2520maps.%2520During%2520simulation%252C%2520it%2520supports%2520a%2520rasterization-based%2520rendering%2520backend%2520with%2520depth-based%2520composition%2520and%2520multiple%2520camera%2520models%2520for%2520real-time%2520camera%2520simulation%252C%2520as%2520well%2520as%2520a%2520ray-traced%2520backend%2520for%2520precise%2520LiDAR%2520simulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Rendering%20for%20Multimodal%20Autonomous%20Driving%3A%20Merging%20Neural%20and%20Physics-Based%20Simulation&entry.906535625=M%C3%A1t%C3%A9%20T%C3%B3th%20and%20P%C3%A9ter%20Kov%C3%A1cs%20and%20R%C3%A9ka%20Bencses%20and%20Zolt%C3%A1n%20Bendefy%20and%20Zolt%C3%A1n%20Hortsin%20and%20Bal%C3%A1zs%20Ter%C3%A9ki%20and%20Tam%C3%A1s%20Matuszka&entry.1292438233=Neural%20reconstruction%20models%20for%20autonomous%20driving%20simulation%20have%20made%20significant%20strides%20in%20recent%20years%2C%20with%20dynamic%20models%20becoming%20increasingly%20prevalent.%20However%2C%20these%20models%20are%20typically%20limited%20to%20handling%20in-domain%20objects%20closely%20following%20their%20original%20trajectories.%20We%20introduce%20a%20hybrid%20approach%20that%20combines%20the%20strengths%20of%20neural%20reconstruction%20with%20physics-based%20rendering.%20This%20method%20enables%20the%20virtual%20placement%20of%20traditional%20mesh-based%20dynamic%20agents%20at%20arbitrary%20locations%2C%20adjustments%20to%20environmental%20conditions%2C%20and%20rendering%20from%20novel%20camera%20viewpoints.%20Our%20approach%20significantly%20enhances%20novel%20view%20synthesis%20quality%20--%20especially%20for%20road%20surfaces%20and%20lane%20markings%20--%20while%20maintaining%20interactive%20frame%20rates%20through%20our%20novel%20training%20method%2C%20NeRF2GS.%20This%20technique%20leverages%20the%20superior%20generalization%20capabilities%20of%20NeRF-based%20methods%20and%20the%20real-time%20rendering%20speed%20of%203D%20Gaussian%20Splatting%20%283DGS%29.%20We%20achieve%20this%20by%20training%20a%20customized%20NeRF%20model%20on%20the%20original%20images%20with%20depth%20regularization%20derived%20from%20a%20noisy%20LiDAR%20point%20cloud%2C%20then%20using%20it%20as%20a%20teacher%20model%20for%203DGS%20training.%20This%20process%20ensures%20accurate%20depth%2C%20surface%20normals%2C%20and%20camera%20appearance%20modeling%20as%20supervision.%20With%20our%20block-based%20training%20parallelization%2C%20the%20method%20can%20handle%20large-scale%20reconstructions%20%28greater%20than%20or%20equal%20to%20100%2C000%20square%20meters%29%20and%20predict%20segmentation%20masks%2C%20surface%20normals%2C%20and%20depth%20maps.%20During%20simulation%2C%20it%20supports%20a%20rasterization-based%20rendering%20backend%20with%20depth-based%20composition%20and%20multiple%20camera%20models%20for%20real-time%20camera%20simulation%2C%20as%20well%20as%20a%20ray-traced%20backend%20for%20precise%20LiDAR%20simulation.&entry.1838667208=http%3A//arxiv.org/abs/2503.09464v2&entry.124074799=Read"},
{"title": "Robust 3DGS-based SLAM via Adaptive Kernel Smoothing", "author": "Shouhe Zhang and Dayong Ren and Sensen Song and Wenjie Li and Piaopiao Yu and Yurong Qian", "abstract": "In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy. We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking. To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM. Unlike conventional methods that focus solely on minimizing rendering error, our core insight is to make the rasterization process more resilient to imperfections in the 3DGS parameters. We hypothesize that by allowing each Gaussian to influence a smoother, wider distribution of pixels during rendering, we can mitigate the detrimental effects of parameter noise from outlier Gaussians. This approach intentionally introduces a controlled blur to the rendered image, which acts as a regularization term, stabilizing the subsequent pose optimization. While a complete redesign of the rasterization pipeline is an ideal solution, we propose a practical and effective alternative that is readily integrated into existing 3DGS frameworks. Our method, termed Corrective Blurry KNN (CB-KNN), adaptively modifies the RGB values and locations of the K-nearest neighboring Gaussians within a local region. This dynamic adjustment generates a smoother local rendering, reducing the impact of erroneous GS parameters on the overall image. Experimental results demonstrate that our approach, while maintaining the overall quality of the scene reconstruction (mapping), significantly improves the robustness and accuracy of camera pose tracking.", "link": "http://arxiv.org/abs/2511.23221v1", "date": "2025-11-28", "relevancy": 3.1812, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6921}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6309}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%203DGS-based%20SLAM%20via%20Adaptive%20Kernel%20Smoothing&body=Title%3A%20Robust%203DGS-based%20SLAM%20via%20Adaptive%20Kernel%20Smoothing%0AAuthor%3A%20Shouhe%20Zhang%20and%20Dayong%20Ren%20and%20Sensen%20Song%20and%20Wenjie%20Li%20and%20Piaopiao%20Yu%20and%20Yurong%20Qian%0AAbstract%3A%20In%20this%20paper%2C%20we%20challenge%20the%20conventional%20notion%20in%203DGS-SLAM%20that%20rendering%20quality%20is%20the%20primary%20determinant%20of%20tracking%20accuracy.%20We%20argue%20that%2C%20compared%20to%20solely%20pursuing%20a%20perfect%20scene%20representation%2C%20it%20is%20more%20critical%20to%20enhance%20the%20robustness%20of%20the%20rasterization%20process%20against%20parameter%20errors%20to%20ensure%20stable%20camera%20pose%20tracking.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20a%20smooth%20kernel%20strategy%20to%20enhance%20the%20robustness%20of%203DGS-based%20SLAM.%20Unlike%20conventional%20methods%20that%20focus%20solely%20on%20minimizing%20rendering%20error%2C%20our%20core%20insight%20is%20to%20make%20the%20rasterization%20process%20more%20resilient%20to%20imperfections%20in%20the%203DGS%20parameters.%20We%20hypothesize%20that%20by%20allowing%20each%20Gaussian%20to%20influence%20a%20smoother%2C%20wider%20distribution%20of%20pixels%20during%20rendering%2C%20we%20can%20mitigate%20the%20detrimental%20effects%20of%20parameter%20noise%20from%20outlier%20Gaussians.%20This%20approach%20intentionally%20introduces%20a%20controlled%20blur%20to%20the%20rendered%20image%2C%20which%20acts%20as%20a%20regularization%20term%2C%20stabilizing%20the%20subsequent%20pose%20optimization.%20While%20a%20complete%20redesign%20of%20the%20rasterization%20pipeline%20is%20an%20ideal%20solution%2C%20we%20propose%20a%20practical%20and%20effective%20alternative%20that%20is%20readily%20integrated%20into%20existing%203DGS%20frameworks.%20Our%20method%2C%20termed%20Corrective%20Blurry%20KNN%20%28CB-KNN%29%2C%20adaptively%20modifies%20the%20RGB%20values%20and%20locations%20of%20the%20K-nearest%20neighboring%20Gaussians%20within%20a%20local%20region.%20This%20dynamic%20adjustment%20generates%20a%20smoother%20local%20rendering%2C%20reducing%20the%20impact%20of%20erroneous%20GS%20parameters%20on%20the%20overall%20image.%20Experimental%20results%20demonstrate%20that%20our%20approach%2C%20while%20maintaining%20the%20overall%20quality%20of%20the%20scene%20reconstruction%20%28mapping%29%2C%20significantly%20improves%20the%20robustness%20and%20accuracy%20of%20camera%20pose%20tracking.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%25203DGS-based%2520SLAM%2520via%2520Adaptive%2520Kernel%2520Smoothing%26entry.906535625%3DShouhe%2520Zhang%2520and%2520Dayong%2520Ren%2520and%2520Sensen%2520Song%2520and%2520Wenjie%2520Li%2520and%2520Piaopiao%2520Yu%2520and%2520Yurong%2520Qian%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520challenge%2520the%2520conventional%2520notion%2520in%25203DGS-SLAM%2520that%2520rendering%2520quality%2520is%2520the%2520primary%2520determinant%2520of%2520tracking%2520accuracy.%2520We%2520argue%2520that%252C%2520compared%2520to%2520solely%2520pursuing%2520a%2520perfect%2520scene%2520representation%252C%2520it%2520is%2520more%2520critical%2520to%2520enhance%2520the%2520robustness%2520of%2520the%2520rasterization%2520process%2520against%2520parameter%2520errors%2520to%2520ensure%2520stable%2520camera%2520pose%2520tracking.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520leverages%2520a%2520smooth%2520kernel%2520strategy%2520to%2520enhance%2520the%2520robustness%2520of%25203DGS-based%2520SLAM.%2520Unlike%2520conventional%2520methods%2520that%2520focus%2520solely%2520on%2520minimizing%2520rendering%2520error%252C%2520our%2520core%2520insight%2520is%2520to%2520make%2520the%2520rasterization%2520process%2520more%2520resilient%2520to%2520imperfections%2520in%2520the%25203DGS%2520parameters.%2520We%2520hypothesize%2520that%2520by%2520allowing%2520each%2520Gaussian%2520to%2520influence%2520a%2520smoother%252C%2520wider%2520distribution%2520of%2520pixels%2520during%2520rendering%252C%2520we%2520can%2520mitigate%2520the%2520detrimental%2520effects%2520of%2520parameter%2520noise%2520from%2520outlier%2520Gaussians.%2520This%2520approach%2520intentionally%2520introduces%2520a%2520controlled%2520blur%2520to%2520the%2520rendered%2520image%252C%2520which%2520acts%2520as%2520a%2520regularization%2520term%252C%2520stabilizing%2520the%2520subsequent%2520pose%2520optimization.%2520While%2520a%2520complete%2520redesign%2520of%2520the%2520rasterization%2520pipeline%2520is%2520an%2520ideal%2520solution%252C%2520we%2520propose%2520a%2520practical%2520and%2520effective%2520alternative%2520that%2520is%2520readily%2520integrated%2520into%2520existing%25203DGS%2520frameworks.%2520Our%2520method%252C%2520termed%2520Corrective%2520Blurry%2520KNN%2520%2528CB-KNN%2529%252C%2520adaptively%2520modifies%2520the%2520RGB%2520values%2520and%2520locations%2520of%2520the%2520K-nearest%2520neighboring%2520Gaussians%2520within%2520a%2520local%2520region.%2520This%2520dynamic%2520adjustment%2520generates%2520a%2520smoother%2520local%2520rendering%252C%2520reducing%2520the%2520impact%2520of%2520erroneous%2520GS%2520parameters%2520on%2520the%2520overall%2520image.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%252C%2520while%2520maintaining%2520the%2520overall%2520quality%2520of%2520the%2520scene%2520reconstruction%2520%2528mapping%2529%252C%2520significantly%2520improves%2520the%2520robustness%2520and%2520accuracy%2520of%2520camera%2520pose%2520tracking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%203DGS-based%20SLAM%20via%20Adaptive%20Kernel%20Smoothing&entry.906535625=Shouhe%20Zhang%20and%20Dayong%20Ren%20and%20Sensen%20Song%20and%20Wenjie%20Li%20and%20Piaopiao%20Yu%20and%20Yurong%20Qian&entry.1292438233=In%20this%20paper%2C%20we%20challenge%20the%20conventional%20notion%20in%203DGS-SLAM%20that%20rendering%20quality%20is%20the%20primary%20determinant%20of%20tracking%20accuracy.%20We%20argue%20that%2C%20compared%20to%20solely%20pursuing%20a%20perfect%20scene%20representation%2C%20it%20is%20more%20critical%20to%20enhance%20the%20robustness%20of%20the%20rasterization%20process%20against%20parameter%20errors%20to%20ensure%20stable%20camera%20pose%20tracking.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20a%20smooth%20kernel%20strategy%20to%20enhance%20the%20robustness%20of%203DGS-based%20SLAM.%20Unlike%20conventional%20methods%20that%20focus%20solely%20on%20minimizing%20rendering%20error%2C%20our%20core%20insight%20is%20to%20make%20the%20rasterization%20process%20more%20resilient%20to%20imperfections%20in%20the%203DGS%20parameters.%20We%20hypothesize%20that%20by%20allowing%20each%20Gaussian%20to%20influence%20a%20smoother%2C%20wider%20distribution%20of%20pixels%20during%20rendering%2C%20we%20can%20mitigate%20the%20detrimental%20effects%20of%20parameter%20noise%20from%20outlier%20Gaussians.%20This%20approach%20intentionally%20introduces%20a%20controlled%20blur%20to%20the%20rendered%20image%2C%20which%20acts%20as%20a%20regularization%20term%2C%20stabilizing%20the%20subsequent%20pose%20optimization.%20While%20a%20complete%20redesign%20of%20the%20rasterization%20pipeline%20is%20an%20ideal%20solution%2C%20we%20propose%20a%20practical%20and%20effective%20alternative%20that%20is%20readily%20integrated%20into%20existing%203DGS%20frameworks.%20Our%20method%2C%20termed%20Corrective%20Blurry%20KNN%20%28CB-KNN%29%2C%20adaptively%20modifies%20the%20RGB%20values%20and%20locations%20of%20the%20K-nearest%20neighboring%20Gaussians%20within%20a%20local%20region.%20This%20dynamic%20adjustment%20generates%20a%20smoother%20local%20rendering%2C%20reducing%20the%20impact%20of%20erroneous%20GS%20parameters%20on%20the%20overall%20image.%20Experimental%20results%20demonstrate%20that%20our%20approach%2C%20while%20maintaining%20the%20overall%20quality%20of%20the%20scene%20reconstruction%20%28mapping%29%2C%20significantly%20improves%20the%20robustness%20and%20accuracy%20of%20camera%20pose%20tracking.&entry.1838667208=http%3A//arxiv.org/abs/2511.23221v1&entry.124074799=Read"},
{"title": "FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting", "author": "Tianhao Xie and Linlian Jiang and Xinxin Zuo and Yang Wang and Tiberiu Popa", "abstract": "Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.", "link": "http://arxiv.org/abs/2511.23292v1", "date": "2025-11-28", "relevancy": 3.0876, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6482}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6148}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FACT-GS%3A%20Frequency-Aligned%20Complexity-Aware%20Texture%20Reparameterization%20for%202D%20Gaussian%20Splatting&body=Title%3A%20FACT-GS%3A%20Frequency-Aligned%20Complexity-Aware%20Texture%20Reparameterization%20for%202D%20Gaussian%20Splatting%0AAuthor%3A%20Tianhao%20Xie%20and%20Linlian%20Jiang%20and%20Xinxin%20Zuo%20and%20Yang%20Wang%20and%20Tiberiu%20Popa%0AAbstract%3A%20Realistic%20scene%20appearance%20modeling%20has%20advanced%20rapidly%20with%20Gaussian%20Splatting%2C%20which%20enables%20real-time%2C%20high-quality%20rendering.%20Recent%20advances%20introduced%20per-primitive%20textures%20that%20incorporate%20spatial%20color%20variations%20within%20each%20Gaussian%2C%20improving%20their%20expressiveness.%20However%2C%20texture-based%20Gaussians%20parameterize%20appearance%20with%20a%20uniform%20per-Gaussian%20sampling%20grid%2C%20allocating%20equal%20sampling%20density%20regardless%20of%20local%20visual%20complexity.%20This%20leads%20to%20inefficient%20texture%20space%20utilization%2C%20where%20high-frequency%20regions%20are%20under-sampled%20and%20smooth%20regions%20waste%20capacity%2C%20causing%20blurred%20appearance%20and%20loss%20of%20fine%20structural%20detail.%20We%20introduce%20FACT-GS%2C%20a%20Frequency-Aligned%20Complexity-aware%20Texture%20Gaussian%20Splatting%20framework%20that%20allocates%20texture%20sampling%20density%20according%20to%20local%20visual%20frequency.%20Grounded%20in%20adaptive%20sampling%20theory%2C%20FACT-GS%20reformulates%20texture%20parameterization%20as%20a%20differentiable%20sampling-density%20allocation%20problem%2C%20replacing%20the%20uniform%20textures%20with%20a%20learnable%20frequency-aware%20allocation%20strategy%20implemented%20via%20a%20deformation%20field%20whose%20Jacobian%20modulates%20local%20sampling%20density.%20Built%20on%202D%20Gaussian%20Splatting%2C%20FACT-GS%20performs%20non-uniform%20sampling%20on%20fixed-resolution%20texture%20grids%2C%20preserving%20real-time%20performance%20while%20recovering%20sharper%20high-frequency%20details%20under%20the%20same%20parameter%20budget.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFACT-GS%253A%2520Frequency-Aligned%2520Complexity-Aware%2520Texture%2520Reparameterization%2520for%25202D%2520Gaussian%2520Splatting%26entry.906535625%3DTianhao%2520Xie%2520and%2520Linlian%2520Jiang%2520and%2520Xinxin%2520Zuo%2520and%2520Yang%2520Wang%2520and%2520Tiberiu%2520Popa%26entry.1292438233%3DRealistic%2520scene%2520appearance%2520modeling%2520has%2520advanced%2520rapidly%2520with%2520Gaussian%2520Splatting%252C%2520which%2520enables%2520real-time%252C%2520high-quality%2520rendering.%2520Recent%2520advances%2520introduced%2520per-primitive%2520textures%2520that%2520incorporate%2520spatial%2520color%2520variations%2520within%2520each%2520Gaussian%252C%2520improving%2520their%2520expressiveness.%2520However%252C%2520texture-based%2520Gaussians%2520parameterize%2520appearance%2520with%2520a%2520uniform%2520per-Gaussian%2520sampling%2520grid%252C%2520allocating%2520equal%2520sampling%2520density%2520regardless%2520of%2520local%2520visual%2520complexity.%2520This%2520leads%2520to%2520inefficient%2520texture%2520space%2520utilization%252C%2520where%2520high-frequency%2520regions%2520are%2520under-sampled%2520and%2520smooth%2520regions%2520waste%2520capacity%252C%2520causing%2520blurred%2520appearance%2520and%2520loss%2520of%2520fine%2520structural%2520detail.%2520We%2520introduce%2520FACT-GS%252C%2520a%2520Frequency-Aligned%2520Complexity-aware%2520Texture%2520Gaussian%2520Splatting%2520framework%2520that%2520allocates%2520texture%2520sampling%2520density%2520according%2520to%2520local%2520visual%2520frequency.%2520Grounded%2520in%2520adaptive%2520sampling%2520theory%252C%2520FACT-GS%2520reformulates%2520texture%2520parameterization%2520as%2520a%2520differentiable%2520sampling-density%2520allocation%2520problem%252C%2520replacing%2520the%2520uniform%2520textures%2520with%2520a%2520learnable%2520frequency-aware%2520allocation%2520strategy%2520implemented%2520via%2520a%2520deformation%2520field%2520whose%2520Jacobian%2520modulates%2520local%2520sampling%2520density.%2520Built%2520on%25202D%2520Gaussian%2520Splatting%252C%2520FACT-GS%2520performs%2520non-uniform%2520sampling%2520on%2520fixed-resolution%2520texture%2520grids%252C%2520preserving%2520real-time%2520performance%2520while%2520recovering%2520sharper%2520high-frequency%2520details%2520under%2520the%2520same%2520parameter%2520budget.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FACT-GS%3A%20Frequency-Aligned%20Complexity-Aware%20Texture%20Reparameterization%20for%202D%20Gaussian%20Splatting&entry.906535625=Tianhao%20Xie%20and%20Linlian%20Jiang%20and%20Xinxin%20Zuo%20and%20Yang%20Wang%20and%20Tiberiu%20Popa&entry.1292438233=Realistic%20scene%20appearance%20modeling%20has%20advanced%20rapidly%20with%20Gaussian%20Splatting%2C%20which%20enables%20real-time%2C%20high-quality%20rendering.%20Recent%20advances%20introduced%20per-primitive%20textures%20that%20incorporate%20spatial%20color%20variations%20within%20each%20Gaussian%2C%20improving%20their%20expressiveness.%20However%2C%20texture-based%20Gaussians%20parameterize%20appearance%20with%20a%20uniform%20per-Gaussian%20sampling%20grid%2C%20allocating%20equal%20sampling%20density%20regardless%20of%20local%20visual%20complexity.%20This%20leads%20to%20inefficient%20texture%20space%20utilization%2C%20where%20high-frequency%20regions%20are%20under-sampled%20and%20smooth%20regions%20waste%20capacity%2C%20causing%20blurred%20appearance%20and%20loss%20of%20fine%20structural%20detail.%20We%20introduce%20FACT-GS%2C%20a%20Frequency-Aligned%20Complexity-aware%20Texture%20Gaussian%20Splatting%20framework%20that%20allocates%20texture%20sampling%20density%20according%20to%20local%20visual%20frequency.%20Grounded%20in%20adaptive%20sampling%20theory%2C%20FACT-GS%20reformulates%20texture%20parameterization%20as%20a%20differentiable%20sampling-density%20allocation%20problem%2C%20replacing%20the%20uniform%20textures%20with%20a%20learnable%20frequency-aware%20allocation%20strategy%20implemented%20via%20a%20deformation%20field%20whose%20Jacobian%20modulates%20local%20sampling%20density.%20Built%20on%202D%20Gaussian%20Splatting%2C%20FACT-GS%20performs%20non-uniform%20sampling%20on%20fixed-resolution%20texture%20grids%2C%20preserving%20real-time%20performance%20while%20recovering%20sharper%20high-frequency%20details%20under%20the%20same%20parameter%20budget.&entry.1838667208=http%3A//arxiv.org/abs/2511.23292v1&entry.124074799=Read"},
{"title": "SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models", "author": "Ruosen Zhao and Zhikang Zhang and Jialei Xu and Jiahao Chang and Dong Chen and Lingyun Li and Weijian Sun and Zizhuang Wei", "abstract": "Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.", "link": "http://arxiv.org/abs/2511.23075v1", "date": "2025-11-28", "relevancy": 3.0778, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceMind%3A%20Camera-Guided%20Modality%20Fusion%20for%20Spatial%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20SpaceMind%3A%20Camera-Guided%20Modality%20Fusion%20for%20Spatial%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Ruosen%20Zhao%20and%20Zhikang%20Zhang%20and%20Jialei%20Xu%20and%20Jiahao%20Chang%20and%20Dong%20Chen%20and%20Lingyun%20Li%20and%20Weijian%20Sun%20and%20Zizhuang%20Wei%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20show%20strong%20multimodal%20understanding%20but%20still%20struggle%20with%203D%20spatial%20reasoning%2C%20such%20as%20distance%20estimation%2C%20size%20comparison%2C%20and%20cross-view%20consistency.%20Existing%203D-aware%20methods%20either%20depend%20on%20auxiliary%203D%20information%20or%20enhance%20RGB-only%20VLMs%20with%20geometry%20encoders%20through%20shallow%20feature%20fusion.%20We%20propose%20SpaceMind%2C%20a%20multimodal%20large%20language%20model%20explicitly%20designed%20for%20spatial%20reasoning%20solely%20from%20RGB%20inputs.%20The%20model%20adopts%20a%20dual-encoder%20architecture%2C%20integrating%20VGGT%20as%20a%20spatial%20understanding%20encoder%20and%20InternViT%20as%20a%202D%20visual%20encoder.%20The%20key%20idea%20is%20to%20treat%20the%20camera%20representation%20as%20an%20active%20guiding%20modality%20rather%20than%20passive%20metadata.%20Specifically%2C%20SpaceMind%20introduces%20a%20lightweight%20Camera-Guided%20Modality%20Fusion%20module%20before%20the%20language%20model%20to%20replace%20shallow%20fusion.%20It%20applies%20camera-conditioned%20biasing%20to%20spatial%20tokens%2C%20assigns%20query-independent%20weights%20reflecting%20their%20geometric%20importance%2C%20and%20uses%20the%20camera%20embedding%20to%20gate%20the%20fused%20representation.%20Empirically%2C%20SpaceMind%20establishes%20new%20state-of-the-art%20results%20on%20VSI-Bench%2C%20SQA3D%20and%20SPBench%2C%20surpassing%20both%20open%20and%20proprietary%20systems%20on%20VSI-Bench%20and%20SPBench%20by%20large%20margins%20and%20achieving%20state-of-the-art%20performance%20on%20SQA3D.%20These%20results%20demonstrate%20that%20camera-guided%20modality%20fusion%20is%20an%20effective%20and%20practical%20inductive%20bias%20for%20equipping%20VLMs%20with%20genuinely%20spatially%20grounded%20intelligence.%20We%20will%20release%20code%20and%20model%20checkpoints%20to%20support%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceMind%253A%2520Camera-Guided%2520Modality%2520Fusion%2520for%2520Spatial%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DRuosen%2520Zhao%2520and%2520Zhikang%2520Zhang%2520and%2520Jialei%2520Xu%2520and%2520Jiahao%2520Chang%2520and%2520Dong%2520Chen%2520and%2520Lingyun%2520Li%2520and%2520Weijian%2520Sun%2520and%2520Zizhuang%2520Wei%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520show%2520strong%2520multimodal%2520understanding%2520but%2520still%2520struggle%2520with%25203D%2520spatial%2520reasoning%252C%2520such%2520as%2520distance%2520estimation%252C%2520size%2520comparison%252C%2520and%2520cross-view%2520consistency.%2520Existing%25203D-aware%2520methods%2520either%2520depend%2520on%2520auxiliary%25203D%2520information%2520or%2520enhance%2520RGB-only%2520VLMs%2520with%2520geometry%2520encoders%2520through%2520shallow%2520feature%2520fusion.%2520We%2520propose%2520SpaceMind%252C%2520a%2520multimodal%2520large%2520language%2520model%2520explicitly%2520designed%2520for%2520spatial%2520reasoning%2520solely%2520from%2520RGB%2520inputs.%2520The%2520model%2520adopts%2520a%2520dual-encoder%2520architecture%252C%2520integrating%2520VGGT%2520as%2520a%2520spatial%2520understanding%2520encoder%2520and%2520InternViT%2520as%2520a%25202D%2520visual%2520encoder.%2520The%2520key%2520idea%2520is%2520to%2520treat%2520the%2520camera%2520representation%2520as%2520an%2520active%2520guiding%2520modality%2520rather%2520than%2520passive%2520metadata.%2520Specifically%252C%2520SpaceMind%2520introduces%2520a%2520lightweight%2520Camera-Guided%2520Modality%2520Fusion%2520module%2520before%2520the%2520language%2520model%2520to%2520replace%2520shallow%2520fusion.%2520It%2520applies%2520camera-conditioned%2520biasing%2520to%2520spatial%2520tokens%252C%2520assigns%2520query-independent%2520weights%2520reflecting%2520their%2520geometric%2520importance%252C%2520and%2520uses%2520the%2520camera%2520embedding%2520to%2520gate%2520the%2520fused%2520representation.%2520Empirically%252C%2520SpaceMind%2520establishes%2520new%2520state-of-the-art%2520results%2520on%2520VSI-Bench%252C%2520SQA3D%2520and%2520SPBench%252C%2520surpassing%2520both%2520open%2520and%2520proprietary%2520systems%2520on%2520VSI-Bench%2520and%2520SPBench%2520by%2520large%2520margins%2520and%2520achieving%2520state-of-the-art%2520performance%2520on%2520SQA3D.%2520These%2520results%2520demonstrate%2520that%2520camera-guided%2520modality%2520fusion%2520is%2520an%2520effective%2520and%2520practical%2520inductive%2520bias%2520for%2520equipping%2520VLMs%2520with%2520genuinely%2520spatially%2520grounded%2520intelligence.%2520We%2520will%2520release%2520code%2520and%2520model%2520checkpoints%2520to%2520support%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceMind%3A%20Camera-Guided%20Modality%20Fusion%20for%20Spatial%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Ruosen%20Zhao%20and%20Zhikang%20Zhang%20and%20Jialei%20Xu%20and%20Jiahao%20Chang%20and%20Dong%20Chen%20and%20Lingyun%20Li%20and%20Weijian%20Sun%20and%20Zizhuang%20Wei&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20show%20strong%20multimodal%20understanding%20but%20still%20struggle%20with%203D%20spatial%20reasoning%2C%20such%20as%20distance%20estimation%2C%20size%20comparison%2C%20and%20cross-view%20consistency.%20Existing%203D-aware%20methods%20either%20depend%20on%20auxiliary%203D%20information%20or%20enhance%20RGB-only%20VLMs%20with%20geometry%20encoders%20through%20shallow%20feature%20fusion.%20We%20propose%20SpaceMind%2C%20a%20multimodal%20large%20language%20model%20explicitly%20designed%20for%20spatial%20reasoning%20solely%20from%20RGB%20inputs.%20The%20model%20adopts%20a%20dual-encoder%20architecture%2C%20integrating%20VGGT%20as%20a%20spatial%20understanding%20encoder%20and%20InternViT%20as%20a%202D%20visual%20encoder.%20The%20key%20idea%20is%20to%20treat%20the%20camera%20representation%20as%20an%20active%20guiding%20modality%20rather%20than%20passive%20metadata.%20Specifically%2C%20SpaceMind%20introduces%20a%20lightweight%20Camera-Guided%20Modality%20Fusion%20module%20before%20the%20language%20model%20to%20replace%20shallow%20fusion.%20It%20applies%20camera-conditioned%20biasing%20to%20spatial%20tokens%2C%20assigns%20query-independent%20weights%20reflecting%20their%20geometric%20importance%2C%20and%20uses%20the%20camera%20embedding%20to%20gate%20the%20fused%20representation.%20Empirically%2C%20SpaceMind%20establishes%20new%20state-of-the-art%20results%20on%20VSI-Bench%2C%20SQA3D%20and%20SPBench%2C%20surpassing%20both%20open%20and%20proprietary%20systems%20on%20VSI-Bench%20and%20SPBench%20by%20large%20margins%20and%20achieving%20state-of-the-art%20performance%20on%20SQA3D.%20These%20results%20demonstrate%20that%20camera-guided%20modality%20fusion%20is%20an%20effective%20and%20practical%20inductive%20bias%20for%20equipping%20VLMs%20with%20genuinely%20spatially%20grounded%20intelligence.%20We%20will%20release%20code%20and%20model%20checkpoints%20to%20support%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.23075v1&entry.124074799=Read"},
{"title": "Language-guided 3D scene synthesis for fine-grained functionality understanding", "author": "Jaime Corsetti and Francesco Giuliari and Davide Boscaini and Pedro Hermosilla and Andrea Pilzer and Guofeng Mei and Alexandros Delitzas and Francis Engelmann and Fabio Poiesi", "abstract": "Functionality understanding in 3D, which aims to identify the functional element in a 3D scene to complete an action (e.g., the correct handle to \"Open the second drawer of the cabinet near the bed\"), is hindered by the scarcity of real-world data due to the substantial effort needed for its collection and annotation. To address this, we introduce SynthFun3D, the first method for task-based 3D scene synthesis. Given the action description, SynthFun3D generates a 3D indoor environment using a furniture asset database with part-level annotation, ensuring the action can be accomplished. It reasons about the action to automatically identify and retrieve the 3D mask of the correct functional element, enabling the inexpensive and large-scale generation of high-quality annotated data. We validate SynthFun3D through user studies, which demonstrate improved scene-prompt coherence compared to other approaches. Our quantitative results further show that the generated data can either replace real data with minor performance loss or supplement real data for improved performance, thereby providing an inexpensive and scalable solution for data-hungry 3D applications. Project page: github.com/tev-fbk/synthfun3d.", "link": "http://arxiv.org/abs/2511.23230v1", "date": "2025-11-28", "relevancy": 3.0574, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-guided%203D%20scene%20synthesis%20for%20fine-grained%20functionality%20understanding&body=Title%3A%20Language-guided%203D%20scene%20synthesis%20for%20fine-grained%20functionality%20understanding%0AAuthor%3A%20Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Davide%20Boscaini%20and%20Pedro%20Hermosilla%20and%20Andrea%20Pilzer%20and%20Guofeng%20Mei%20and%20Alexandros%20Delitzas%20and%20Francis%20Engelmann%20and%20Fabio%20Poiesi%0AAbstract%3A%20Functionality%20understanding%20in%203D%2C%20which%20aims%20to%20identify%20the%20functional%20element%20in%20a%203D%20scene%20to%20complete%20an%20action%20%28e.g.%2C%20the%20correct%20handle%20to%20%22Open%20the%20second%20drawer%20of%20the%20cabinet%20near%20the%20bed%22%29%2C%20is%20hindered%20by%20the%20scarcity%20of%20real-world%20data%20due%20to%20the%20substantial%20effort%20needed%20for%20its%20collection%20and%20annotation.%20To%20address%20this%2C%20we%20introduce%20SynthFun3D%2C%20the%20first%20method%20for%20task-based%203D%20scene%20synthesis.%20Given%20the%20action%20description%2C%20SynthFun3D%20generates%20a%203D%20indoor%20environment%20using%20a%20furniture%20asset%20database%20with%20part-level%20annotation%2C%20ensuring%20the%20action%20can%20be%20accomplished.%20It%20reasons%20about%20the%20action%20to%20automatically%20identify%20and%20retrieve%20the%203D%20mask%20of%20the%20correct%20functional%20element%2C%20enabling%20the%20inexpensive%20and%20large-scale%20generation%20of%20high-quality%20annotated%20data.%20We%20validate%20SynthFun3D%20through%20user%20studies%2C%20which%20demonstrate%20improved%20scene-prompt%20coherence%20compared%20to%20other%20approaches.%20Our%20quantitative%20results%20further%20show%20that%20the%20generated%20data%20can%20either%20replace%20real%20data%20with%20minor%20performance%20loss%20or%20supplement%20real%20data%20for%20improved%20performance%2C%20thereby%20providing%20an%20inexpensive%20and%20scalable%20solution%20for%20data-hungry%203D%20applications.%20Project%20page%3A%20github.com/tev-fbk/synthfun3d.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-guided%25203D%2520scene%2520synthesis%2520for%2520fine-grained%2520functionality%2520understanding%26entry.906535625%3DJaime%2520Corsetti%2520and%2520Francesco%2520Giuliari%2520and%2520Davide%2520Boscaini%2520and%2520Pedro%2520Hermosilla%2520and%2520Andrea%2520Pilzer%2520and%2520Guofeng%2520Mei%2520and%2520Alexandros%2520Delitzas%2520and%2520Francis%2520Engelmann%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3DFunctionality%2520understanding%2520in%25203D%252C%2520which%2520aims%2520to%2520identify%2520the%2520functional%2520element%2520in%2520a%25203D%2520scene%2520to%2520complete%2520an%2520action%2520%2528e.g.%252C%2520the%2520correct%2520handle%2520to%2520%2522Open%2520the%2520second%2520drawer%2520of%2520the%2520cabinet%2520near%2520the%2520bed%2522%2529%252C%2520is%2520hindered%2520by%2520the%2520scarcity%2520of%2520real-world%2520data%2520due%2520to%2520the%2520substantial%2520effort%2520needed%2520for%2520its%2520collection%2520and%2520annotation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SynthFun3D%252C%2520the%2520first%2520method%2520for%2520task-based%25203D%2520scene%2520synthesis.%2520Given%2520the%2520action%2520description%252C%2520SynthFun3D%2520generates%2520a%25203D%2520indoor%2520environment%2520using%2520a%2520furniture%2520asset%2520database%2520with%2520part-level%2520annotation%252C%2520ensuring%2520the%2520action%2520can%2520be%2520accomplished.%2520It%2520reasons%2520about%2520the%2520action%2520to%2520automatically%2520identify%2520and%2520retrieve%2520the%25203D%2520mask%2520of%2520the%2520correct%2520functional%2520element%252C%2520enabling%2520the%2520inexpensive%2520and%2520large-scale%2520generation%2520of%2520high-quality%2520annotated%2520data.%2520We%2520validate%2520SynthFun3D%2520through%2520user%2520studies%252C%2520which%2520demonstrate%2520improved%2520scene-prompt%2520coherence%2520compared%2520to%2520other%2520approaches.%2520Our%2520quantitative%2520results%2520further%2520show%2520that%2520the%2520generated%2520data%2520can%2520either%2520replace%2520real%2520data%2520with%2520minor%2520performance%2520loss%2520or%2520supplement%2520real%2520data%2520for%2520improved%2520performance%252C%2520thereby%2520providing%2520an%2520inexpensive%2520and%2520scalable%2520solution%2520for%2520data-hungry%25203D%2520applications.%2520Project%2520page%253A%2520github.com/tev-fbk/synthfun3d.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-guided%203D%20scene%20synthesis%20for%20fine-grained%20functionality%20understanding&entry.906535625=Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Davide%20Boscaini%20and%20Pedro%20Hermosilla%20and%20Andrea%20Pilzer%20and%20Guofeng%20Mei%20and%20Alexandros%20Delitzas%20and%20Francis%20Engelmann%20and%20Fabio%20Poiesi&entry.1292438233=Functionality%20understanding%20in%203D%2C%20which%20aims%20to%20identify%20the%20functional%20element%20in%20a%203D%20scene%20to%20complete%20an%20action%20%28e.g.%2C%20the%20correct%20handle%20to%20%22Open%20the%20second%20drawer%20of%20the%20cabinet%20near%20the%20bed%22%29%2C%20is%20hindered%20by%20the%20scarcity%20of%20real-world%20data%20due%20to%20the%20substantial%20effort%20needed%20for%20its%20collection%20and%20annotation.%20To%20address%20this%2C%20we%20introduce%20SynthFun3D%2C%20the%20first%20method%20for%20task-based%203D%20scene%20synthesis.%20Given%20the%20action%20description%2C%20SynthFun3D%20generates%20a%203D%20indoor%20environment%20using%20a%20furniture%20asset%20database%20with%20part-level%20annotation%2C%20ensuring%20the%20action%20can%20be%20accomplished.%20It%20reasons%20about%20the%20action%20to%20automatically%20identify%20and%20retrieve%20the%203D%20mask%20of%20the%20correct%20functional%20element%2C%20enabling%20the%20inexpensive%20and%20large-scale%20generation%20of%20high-quality%20annotated%20data.%20We%20validate%20SynthFun3D%20through%20user%20studies%2C%20which%20demonstrate%20improved%20scene-prompt%20coherence%20compared%20to%20other%20approaches.%20Our%20quantitative%20results%20further%20show%20that%20the%20generated%20data%20can%20either%20replace%20real%20data%20with%20minor%20performance%20loss%20or%20supplement%20real%20data%20for%20improved%20performance%2C%20thereby%20providing%20an%20inexpensive%20and%20scalable%20solution%20for%20data-hungry%203D%20applications.%20Project%20page%3A%20github.com/tev-fbk/synthfun3d.&entry.1838667208=http%3A//arxiv.org/abs/2511.23230v1&entry.124074799=Read"},
{"title": "Visual Generation Tuning", "author": "Jiahao Guo and Sinan Du and Jingfeng Yao and Wenyu Liu and Bo Li and Haoxiang Cao and Kun Gai and Chun Yuan and Kai Wu and Xinggang Wang", "abstract": "Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.", "link": "http://arxiv.org/abs/2511.23469v1", "date": "2025-11-28", "relevancy": 3.0066, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.614}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6004}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Generation%20Tuning&body=Title%3A%20Visual%20Generation%20Tuning%0AAuthor%3A%20Jiahao%20Guo%20and%20Sinan%20Du%20and%20Jingfeng%20Yao%20and%20Wenyu%20Liu%20and%20Bo%20Li%20and%20Haoxiang%20Cao%20and%20Kun%20Gai%20and%20Chun%20Yuan%20and%20Kai%20Wu%20and%20Xinggang%20Wang%0AAbstract%3A%20Large%20Vision%20Language%20Models%20%28VLMs%29%20effectively%20bridge%20the%20modality%20gap%20through%20extensive%20pretraining%2C%20acquiring%20sophisticated%20visual%20representations%20aligned%20with%20language.%20However%2C%20it%20remains%20underexplored%20whether%20these%20representations%2C%20optimized%20for%20multimodal%20understanding%20tasks%2C%20harbor%20an%20inherent%20potential%20for%20visual%20generation.%20In%20this%20paper%2C%20we%20propose%20VGT%2C%20Visual%20Generation%20Tuning%2C%20a%20novel%20paradigm%20designed%20to%20stimulate%20the%20underlying%20capabilities%20of%20visual%20generation%20within%20any%20vision%20language%20models.%20By%20performing%20efficient%20visual%20generation%20tuning%20on%20well-pretrained%20VLMs%2C%20we%20significantly%20mitigate%20the%20alignment%20costs%20and%20accelerate%20the%20convergence%20of%20autoregressive%20modeling%20in%20the%20continuous%20space%20%2820x%20speedup%29.%20Specifically%2C%20we%20dismiss%20the%20entangled%20pixel-level%20VAEs%20designed%20for%20diffusion%20transformers%20and%20formulate%20VGT-AE%20through%20aligning%20the%20semantic%20encoders%20from%20pretrained%20VLMs%20with%20the%20latent%20representations%20of%20pixel%20decoders.%20In%20image%20reconstruction%20tasks%2C%20we%20achieve%2026.67%20PSNR%20and%200.50%20rFID%20at%20a%2028x%20compression%20ratio%2C%20outperforming%20specialized%20VAEs%3B%20in%20visual%20generation%20tasks%2C%20we%20achieve%20state-of-the-art%20outcomes%20among%20autoregressive%20models%2C%200.77%20on%20GenEval%20and%2078.73%20on%20DPG-Bench.%20Furthermore%2C%20our%20proposed%20VGT%20showcases%20significant%20scaling%20promise%20and%20is%20versatile%20for%20endowing%20any%20VLMs%20trained%20for%20multimodal%20understanding%20with%20the%20capabilities%20of%20visual%20generation%2C%20which%20paves%20the%20new%20avenue%20to%20explore%20next-generation%20unified%20multimodal%20foundation%20models.%20Models%20and%20codes%20are%20available%20at%20https%3A//github.com/hustvl/VGT.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Generation%2520Tuning%26entry.906535625%3DJiahao%2520Guo%2520and%2520Sinan%2520Du%2520and%2520Jingfeng%2520Yao%2520and%2520Wenyu%2520Liu%2520and%2520Bo%2520Li%2520and%2520Haoxiang%2520Cao%2520and%2520Kun%2520Gai%2520and%2520Chun%2520Yuan%2520and%2520Kai%2520Wu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DLarge%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520effectively%2520bridge%2520the%2520modality%2520gap%2520through%2520extensive%2520pretraining%252C%2520acquiring%2520sophisticated%2520visual%2520representations%2520aligned%2520with%2520language.%2520However%252C%2520it%2520remains%2520underexplored%2520whether%2520these%2520representations%252C%2520optimized%2520for%2520multimodal%2520understanding%2520tasks%252C%2520harbor%2520an%2520inherent%2520potential%2520for%2520visual%2520generation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520VGT%252C%2520Visual%2520Generation%2520Tuning%252C%2520a%2520novel%2520paradigm%2520designed%2520to%2520stimulate%2520the%2520underlying%2520capabilities%2520of%2520visual%2520generation%2520within%2520any%2520vision%2520language%2520models.%2520By%2520performing%2520efficient%2520visual%2520generation%2520tuning%2520on%2520well-pretrained%2520VLMs%252C%2520we%2520significantly%2520mitigate%2520the%2520alignment%2520costs%2520and%2520accelerate%2520the%2520convergence%2520of%2520autoregressive%2520modeling%2520in%2520the%2520continuous%2520space%2520%252820x%2520speedup%2529.%2520Specifically%252C%2520we%2520dismiss%2520the%2520entangled%2520pixel-level%2520VAEs%2520designed%2520for%2520diffusion%2520transformers%2520and%2520formulate%2520VGT-AE%2520through%2520aligning%2520the%2520semantic%2520encoders%2520from%2520pretrained%2520VLMs%2520with%2520the%2520latent%2520representations%2520of%2520pixel%2520decoders.%2520In%2520image%2520reconstruction%2520tasks%252C%2520we%2520achieve%252026.67%2520PSNR%2520and%25200.50%2520rFID%2520at%2520a%252028x%2520compression%2520ratio%252C%2520outperforming%2520specialized%2520VAEs%253B%2520in%2520visual%2520generation%2520tasks%252C%2520we%2520achieve%2520state-of-the-art%2520outcomes%2520among%2520autoregressive%2520models%252C%25200.77%2520on%2520GenEval%2520and%252078.73%2520on%2520DPG-Bench.%2520Furthermore%252C%2520our%2520proposed%2520VGT%2520showcases%2520significant%2520scaling%2520promise%2520and%2520is%2520versatile%2520for%2520endowing%2520any%2520VLMs%2520trained%2520for%2520multimodal%2520understanding%2520with%2520the%2520capabilities%2520of%2520visual%2520generation%252C%2520which%2520paves%2520the%2520new%2520avenue%2520to%2520explore%2520next-generation%2520unified%2520multimodal%2520foundation%2520models.%2520Models%2520and%2520codes%2520are%2520available%2520at%2520https%253A//github.com/hustvl/VGT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Generation%20Tuning&entry.906535625=Jiahao%20Guo%20and%20Sinan%20Du%20and%20Jingfeng%20Yao%20and%20Wenyu%20Liu%20and%20Bo%20Li%20and%20Haoxiang%20Cao%20and%20Kun%20Gai%20and%20Chun%20Yuan%20and%20Kai%20Wu%20and%20Xinggang%20Wang&entry.1292438233=Large%20Vision%20Language%20Models%20%28VLMs%29%20effectively%20bridge%20the%20modality%20gap%20through%20extensive%20pretraining%2C%20acquiring%20sophisticated%20visual%20representations%20aligned%20with%20language.%20However%2C%20it%20remains%20underexplored%20whether%20these%20representations%2C%20optimized%20for%20multimodal%20understanding%20tasks%2C%20harbor%20an%20inherent%20potential%20for%20visual%20generation.%20In%20this%20paper%2C%20we%20propose%20VGT%2C%20Visual%20Generation%20Tuning%2C%20a%20novel%20paradigm%20designed%20to%20stimulate%20the%20underlying%20capabilities%20of%20visual%20generation%20within%20any%20vision%20language%20models.%20By%20performing%20efficient%20visual%20generation%20tuning%20on%20well-pretrained%20VLMs%2C%20we%20significantly%20mitigate%20the%20alignment%20costs%20and%20accelerate%20the%20convergence%20of%20autoregressive%20modeling%20in%20the%20continuous%20space%20%2820x%20speedup%29.%20Specifically%2C%20we%20dismiss%20the%20entangled%20pixel-level%20VAEs%20designed%20for%20diffusion%20transformers%20and%20formulate%20VGT-AE%20through%20aligning%20the%20semantic%20encoders%20from%20pretrained%20VLMs%20with%20the%20latent%20representations%20of%20pixel%20decoders.%20In%20image%20reconstruction%20tasks%2C%20we%20achieve%2026.67%20PSNR%20and%200.50%20rFID%20at%20a%2028x%20compression%20ratio%2C%20outperforming%20specialized%20VAEs%3B%20in%20visual%20generation%20tasks%2C%20we%20achieve%20state-of-the-art%20outcomes%20among%20autoregressive%20models%2C%200.77%20on%20GenEval%20and%2078.73%20on%20DPG-Bench.%20Furthermore%2C%20our%20proposed%20VGT%20showcases%20significant%20scaling%20promise%20and%20is%20versatile%20for%20endowing%20any%20VLMs%20trained%20for%20multimodal%20understanding%20with%20the%20capabilities%20of%20visual%20generation%2C%20which%20paves%20the%20new%20avenue%20to%20explore%20next-generation%20unified%20multimodal%20foundation%20models.%20Models%20and%20codes%20are%20available%20at%20https%3A//github.com/hustvl/VGT.&entry.1838667208=http%3A//arxiv.org/abs/2511.23469v1&entry.124074799=Read"},
{"title": "SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features", "author": "Jinyuan Qu and Hongyang Li and Xingyu Chen and Shilong Liu and Yukai Shi and Tianhe Ren and Ruitao Jing and Lei Zhang", "abstract": "In this paper, we present SegDINO3D, a novel Transformer encoder-decoder framework for 3D instance segmentation. As 3D training data is generally not as sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D representation from a pre-trained 2D detection model, including both image-level and object-level features, for improving 3D representation. SegDINO3D takes both a point cloud and its associated 2D images as input. In the encoder stage, it first enriches each 3D point by retrieving 2D image features from its corresponding image views and then leverages a 3D encoder for 3D context fusion. In the decoder stage, it formulates 3D object queries as 3D anchor boxes and performs cross-attention from 3D queries to 2D object queries obtained from 2D images using the 2D detection model. These 2D object queries serve as a compact object-level representation of 2D images, effectively avoiding the challenge of keeping thousands of image feature maps in the memory while faithfully preserving the knowledge of the pre-trained 2D model. The introducing of 3D box queries also enables the model to modulate cross-attention using the predicted boxes for more precise querying. SegDINO3D achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D instance segmentation benchmarks. Notably, on the challenging ScanNet200 dataset, SegDINO3D significantly outperforms prior methods by +8.6 and +6.8 mAP on the validation and hidden test sets, respectively, demonstrating its superiority.", "link": "http://arxiv.org/abs/2509.16098v2", "date": "2025-11-28", "relevancy": 2.9908, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegDINO3D%3A%203D%20Instance%20Segmentation%20Empowered%20by%20Both%20Image-Level%20and%20Object-Level%202D%20Features&body=Title%3A%20SegDINO3D%3A%203D%20Instance%20Segmentation%20Empowered%20by%20Both%20Image-Level%20and%20Object-Level%202D%20Features%0AAuthor%3A%20Jinyuan%20Qu%20and%20Hongyang%20Li%20and%20Xingyu%20Chen%20and%20Shilong%20Liu%20and%20Yukai%20Shi%20and%20Tianhe%20Ren%20and%20Ruitao%20Jing%20and%20Lei%20Zhang%0AAbstract%3A%20In%20this%20paper%2C%20we%20present%20SegDINO3D%2C%20a%20novel%20Transformer%20encoder-decoder%20framework%20for%203D%20instance%20segmentation.%20As%203D%20training%20data%20is%20generally%20not%20as%20sufficient%20as%202D%20training%20images%2C%20SegDINO3D%20is%20designed%20to%20fully%20leverage%202D%20representation%20from%20a%20pre-trained%202D%20detection%20model%2C%20including%20both%20image-level%20and%20object-level%20features%2C%20for%20improving%203D%20representation.%20SegDINO3D%20takes%20both%20a%20point%20cloud%20and%20its%20associated%202D%20images%20as%20input.%20In%20the%20encoder%20stage%2C%20it%20first%20enriches%20each%203D%20point%20by%20retrieving%202D%20image%20features%20from%20its%20corresponding%20image%20views%20and%20then%20leverages%20a%203D%20encoder%20for%203D%20context%20fusion.%20In%20the%20decoder%20stage%2C%20it%20formulates%203D%20object%20queries%20as%203D%20anchor%20boxes%20and%20performs%20cross-attention%20from%203D%20queries%20to%202D%20object%20queries%20obtained%20from%202D%20images%20using%20the%202D%20detection%20model.%20These%202D%20object%20queries%20serve%20as%20a%20compact%20object-level%20representation%20of%202D%20images%2C%20effectively%20avoiding%20the%20challenge%20of%20keeping%20thousands%20of%20image%20feature%20maps%20in%20the%20memory%20while%20faithfully%20preserving%20the%20knowledge%20of%20the%20pre-trained%202D%20model.%20The%20introducing%20of%203D%20box%20queries%20also%20enables%20the%20model%20to%20modulate%20cross-attention%20using%20the%20predicted%20boxes%20for%20more%20precise%20querying.%20SegDINO3D%20achieves%20the%20state-of-the-art%20performance%20on%20the%20ScanNetV2%20and%20ScanNet200%203D%20instance%20segmentation%20benchmarks.%20Notably%2C%20on%20the%20challenging%20ScanNet200%20dataset%2C%20SegDINO3D%20significantly%20outperforms%20prior%20methods%20by%20%2B8.6%20and%20%2B6.8%20mAP%20on%20the%20validation%20and%20hidden%20test%20sets%2C%20respectively%2C%20demonstrating%20its%20superiority.%0ALink%3A%20http%3A//arxiv.org/abs/2509.16098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegDINO3D%253A%25203D%2520Instance%2520Segmentation%2520Empowered%2520by%2520Both%2520Image-Level%2520and%2520Object-Level%25202D%2520Features%26entry.906535625%3DJinyuan%2520Qu%2520and%2520Hongyang%2520Li%2520and%2520Xingyu%2520Chen%2520and%2520Shilong%2520Liu%2520and%2520Yukai%2520Shi%2520and%2520Tianhe%2520Ren%2520and%2520Ruitao%2520Jing%2520and%2520Lei%2520Zhang%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520present%2520SegDINO3D%252C%2520a%2520novel%2520Transformer%2520encoder-decoder%2520framework%2520for%25203D%2520instance%2520segmentation.%2520As%25203D%2520training%2520data%2520is%2520generally%2520not%2520as%2520sufficient%2520as%25202D%2520training%2520images%252C%2520SegDINO3D%2520is%2520designed%2520to%2520fully%2520leverage%25202D%2520representation%2520from%2520a%2520pre-trained%25202D%2520detection%2520model%252C%2520including%2520both%2520image-level%2520and%2520object-level%2520features%252C%2520for%2520improving%25203D%2520representation.%2520SegDINO3D%2520takes%2520both%2520a%2520point%2520cloud%2520and%2520its%2520associated%25202D%2520images%2520as%2520input.%2520In%2520the%2520encoder%2520stage%252C%2520it%2520first%2520enriches%2520each%25203D%2520point%2520by%2520retrieving%25202D%2520image%2520features%2520from%2520its%2520corresponding%2520image%2520views%2520and%2520then%2520leverages%2520a%25203D%2520encoder%2520for%25203D%2520context%2520fusion.%2520In%2520the%2520decoder%2520stage%252C%2520it%2520formulates%25203D%2520object%2520queries%2520as%25203D%2520anchor%2520boxes%2520and%2520performs%2520cross-attention%2520from%25203D%2520queries%2520to%25202D%2520object%2520queries%2520obtained%2520from%25202D%2520images%2520using%2520the%25202D%2520detection%2520model.%2520These%25202D%2520object%2520queries%2520serve%2520as%2520a%2520compact%2520object-level%2520representation%2520of%25202D%2520images%252C%2520effectively%2520avoiding%2520the%2520challenge%2520of%2520keeping%2520thousands%2520of%2520image%2520feature%2520maps%2520in%2520the%2520memory%2520while%2520faithfully%2520preserving%2520the%2520knowledge%2520of%2520the%2520pre-trained%25202D%2520model.%2520The%2520introducing%2520of%25203D%2520box%2520queries%2520also%2520enables%2520the%2520model%2520to%2520modulate%2520cross-attention%2520using%2520the%2520predicted%2520boxes%2520for%2520more%2520precise%2520querying.%2520SegDINO3D%2520achieves%2520the%2520state-of-the-art%2520performance%2520on%2520the%2520ScanNetV2%2520and%2520ScanNet200%25203D%2520instance%2520segmentation%2520benchmarks.%2520Notably%252C%2520on%2520the%2520challenging%2520ScanNet200%2520dataset%252C%2520SegDINO3D%2520significantly%2520outperforms%2520prior%2520methods%2520by%2520%252B8.6%2520and%2520%252B6.8%2520mAP%2520on%2520the%2520validation%2520and%2520hidden%2520test%2520sets%252C%2520respectively%252C%2520demonstrating%2520its%2520superiority.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegDINO3D%3A%203D%20Instance%20Segmentation%20Empowered%20by%20Both%20Image-Level%20and%20Object-Level%202D%20Features&entry.906535625=Jinyuan%20Qu%20and%20Hongyang%20Li%20and%20Xingyu%20Chen%20and%20Shilong%20Liu%20and%20Yukai%20Shi%20and%20Tianhe%20Ren%20and%20Ruitao%20Jing%20and%20Lei%20Zhang&entry.1292438233=In%20this%20paper%2C%20we%20present%20SegDINO3D%2C%20a%20novel%20Transformer%20encoder-decoder%20framework%20for%203D%20instance%20segmentation.%20As%203D%20training%20data%20is%20generally%20not%20as%20sufficient%20as%202D%20training%20images%2C%20SegDINO3D%20is%20designed%20to%20fully%20leverage%202D%20representation%20from%20a%20pre-trained%202D%20detection%20model%2C%20including%20both%20image-level%20and%20object-level%20features%2C%20for%20improving%203D%20representation.%20SegDINO3D%20takes%20both%20a%20point%20cloud%20and%20its%20associated%202D%20images%20as%20input.%20In%20the%20encoder%20stage%2C%20it%20first%20enriches%20each%203D%20point%20by%20retrieving%202D%20image%20features%20from%20its%20corresponding%20image%20views%20and%20then%20leverages%20a%203D%20encoder%20for%203D%20context%20fusion.%20In%20the%20decoder%20stage%2C%20it%20formulates%203D%20object%20queries%20as%203D%20anchor%20boxes%20and%20performs%20cross-attention%20from%203D%20queries%20to%202D%20object%20queries%20obtained%20from%202D%20images%20using%20the%202D%20detection%20model.%20These%202D%20object%20queries%20serve%20as%20a%20compact%20object-level%20representation%20of%202D%20images%2C%20effectively%20avoiding%20the%20challenge%20of%20keeping%20thousands%20of%20image%20feature%20maps%20in%20the%20memory%20while%20faithfully%20preserving%20the%20knowledge%20of%20the%20pre-trained%202D%20model.%20The%20introducing%20of%203D%20box%20queries%20also%20enables%20the%20model%20to%20modulate%20cross-attention%20using%20the%20predicted%20boxes%20for%20more%20precise%20querying.%20SegDINO3D%20achieves%20the%20state-of-the-art%20performance%20on%20the%20ScanNetV2%20and%20ScanNet200%203D%20instance%20segmentation%20benchmarks.%20Notably%2C%20on%20the%20challenging%20ScanNet200%20dataset%2C%20SegDINO3D%20significantly%20outperforms%20prior%20methods%20by%20%2B8.6%20and%20%2B6.8%20mAP%20on%20the%20validation%20and%20hidden%20test%20sets%2C%20respectively%2C%20demonstrating%20its%20superiority.&entry.1838667208=http%3A//arxiv.org/abs/2509.16098v2&entry.124074799=Read"},
{"title": "MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?", "author": "Yuandong Wang and Yao Cui and Yuxin Zhao and Zhen Yang and Yangfu Zhu and Zhenzhou Shao", "abstract": "Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.", "link": "http://arxiv.org/abs/2511.23112v1", "date": "2025-11-28", "relevancy": 2.913, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathSight%3A%20A%20Benchmark%20Exploring%20Have%20Vision-Language%20Models%20Really%20Seen%20in%20University-Level%20Mathematical%20Reasoning%3F&body=Title%3A%20MathSight%3A%20A%20Benchmark%20Exploring%20Have%20Vision-Language%20Models%20Really%20Seen%20in%20University-Level%20Mathematical%20Reasoning%3F%0AAuthor%3A%20Yuandong%20Wang%20and%20Yao%20Cui%20and%20Yuxin%20Zhao%20and%20Zhen%20Yang%20and%20Yangfu%20Zhu%20and%20Zhenzhou%20Shao%0AAbstract%3A%20Recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20progress%20in%20multimodal%20mathematical%20reasoning.%20Yet%2C%20how%20much%20visual%20information%20truly%20contributes%20to%20reasoning%20remains%20unclear.%20Existing%20benchmarks%20report%20strong%20overall%20performance%20but%20seldom%20isolate%20the%20role%20of%20the%20image%20modality%2C%20leaving%20open%20whether%20VLMs%20genuinely%20leverage%20visual%20understanding%20or%20merely%20depend%20on%20linguistic%20priors.%20To%20address%20this%2C%20we%20present%20MathSight%2C%20a%20university-level%20multimodal%20mathematical%20reasoning%20benchmark%20designed%20to%20disentangle%20and%20quantify%20the%20effect%20of%20visual%20input.%20Each%20problem%20includes%20multiple%20visual%20variants%20--%20original%2C%20hand-drawn%2C%20photo-captured%20--%20and%20a%20text-only%20condition%20for%20controlled%20comparison.%20Experiments%20on%20state-of-the-art%20VLMs%20reveal%20a%20consistent%20trend%3A%20the%20contribution%20of%20visual%20information%20diminishes%20with%20increasing%20problem%20difficulty.%20Remarkably%2C%20Qwen3-VL%20without%20any%20image%20input%20surpasses%20both%20its%20multimodal%20variants%20and%20GPT-5%2C%20underscoring%20the%20need%20for%20benchmarks%20like%20MathSight%20to%20advance%20genuine%20vision-grounded%20reasoning%20in%20future%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathSight%253A%2520A%2520Benchmark%2520Exploring%2520Have%2520Vision-Language%2520Models%2520Really%2520Seen%2520in%2520University-Level%2520Mathematical%2520Reasoning%253F%26entry.906535625%3DYuandong%2520Wang%2520and%2520Yao%2520Cui%2520and%2520Yuxin%2520Zhao%2520and%2520Zhen%2520Yang%2520and%2520Yangfu%2520Zhu%2520and%2520Zhenzhou%2520Shao%26entry.1292438233%3DRecent%2520advances%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520progress%2520in%2520multimodal%2520mathematical%2520reasoning.%2520Yet%252C%2520how%2520much%2520visual%2520information%2520truly%2520contributes%2520to%2520reasoning%2520remains%2520unclear.%2520Existing%2520benchmarks%2520report%2520strong%2520overall%2520performance%2520but%2520seldom%2520isolate%2520the%2520role%2520of%2520the%2520image%2520modality%252C%2520leaving%2520open%2520whether%2520VLMs%2520genuinely%2520leverage%2520visual%2520understanding%2520or%2520merely%2520depend%2520on%2520linguistic%2520priors.%2520To%2520address%2520this%252C%2520we%2520present%2520MathSight%252C%2520a%2520university-level%2520multimodal%2520mathematical%2520reasoning%2520benchmark%2520designed%2520to%2520disentangle%2520and%2520quantify%2520the%2520effect%2520of%2520visual%2520input.%2520Each%2520problem%2520includes%2520multiple%2520visual%2520variants%2520--%2520original%252C%2520hand-drawn%252C%2520photo-captured%2520--%2520and%2520a%2520text-only%2520condition%2520for%2520controlled%2520comparison.%2520Experiments%2520on%2520state-of-the-art%2520VLMs%2520reveal%2520a%2520consistent%2520trend%253A%2520the%2520contribution%2520of%2520visual%2520information%2520diminishes%2520with%2520increasing%2520problem%2520difficulty.%2520Remarkably%252C%2520Qwen3-VL%2520without%2520any%2520image%2520input%2520surpasses%2520both%2520its%2520multimodal%2520variants%2520and%2520GPT-5%252C%2520underscoring%2520the%2520need%2520for%2520benchmarks%2520like%2520MathSight%2520to%2520advance%2520genuine%2520vision-grounded%2520reasoning%2520in%2520future%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathSight%3A%20A%20Benchmark%20Exploring%20Have%20Vision-Language%20Models%20Really%20Seen%20in%20University-Level%20Mathematical%20Reasoning%3F&entry.906535625=Yuandong%20Wang%20and%20Yao%20Cui%20and%20Yuxin%20Zhao%20and%20Zhen%20Yang%20and%20Yangfu%20Zhu%20and%20Zhenzhou%20Shao&entry.1292438233=Recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20progress%20in%20multimodal%20mathematical%20reasoning.%20Yet%2C%20how%20much%20visual%20information%20truly%20contributes%20to%20reasoning%20remains%20unclear.%20Existing%20benchmarks%20report%20strong%20overall%20performance%20but%20seldom%20isolate%20the%20role%20of%20the%20image%20modality%2C%20leaving%20open%20whether%20VLMs%20genuinely%20leverage%20visual%20understanding%20or%20merely%20depend%20on%20linguistic%20priors.%20To%20address%20this%2C%20we%20present%20MathSight%2C%20a%20university-level%20multimodal%20mathematical%20reasoning%20benchmark%20designed%20to%20disentangle%20and%20quantify%20the%20effect%20of%20visual%20input.%20Each%20problem%20includes%20multiple%20visual%20variants%20--%20original%2C%20hand-drawn%2C%20photo-captured%20--%20and%20a%20text-only%20condition%20for%20controlled%20comparison.%20Experiments%20on%20state-of-the-art%20VLMs%20reveal%20a%20consistent%20trend%3A%20the%20contribution%20of%20visual%20information%20diminishes%20with%20increasing%20problem%20difficulty.%20Remarkably%2C%20Qwen3-VL%20without%20any%20image%20input%20surpasses%20both%20its%20multimodal%20variants%20and%20GPT-5%2C%20underscoring%20the%20need%20for%20benchmarks%20like%20MathSight%20to%20advance%20genuine%20vision-grounded%20reasoning%20in%20future%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.23112v1&entry.124074799=Read"},
{"title": "From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning", "author": "Changpeng Wang and Haozhe Wang and Xi Chen and Junhan Liu and Taofeng Xue and Chong Peng and Donglian Qi and Fangzhen Lin and Yunfeng Yan", "abstract": "Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to \"get the right answer for the right visual reason\". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.", "link": "http://arxiv.org/abs/2511.23031v1", "date": "2025-11-28", "relevancy": 2.899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Illusion%20to%20Intention%3A%20Visual%20Rationale%20Learning%20for%20Vision-Language%20Reasoning&body=Title%3A%20From%20Illusion%20to%20Intention%3A%20Visual%20Rationale%20Learning%20for%20Vision-Language%20Reasoning%0AAuthor%3A%20Changpeng%20Wang%20and%20Haozhe%20Wang%20and%20Xi%20Chen%20and%20Junhan%20Liu%20and%20Taofeng%20Xue%20and%20Chong%20Peng%20and%20Donglian%20Qi%20and%20Fangzhen%20Lin%20and%20Yunfeng%20Yan%0AAbstract%3A%20Recent%20advances%20in%20vision-language%20reasoning%20underscore%20the%20importance%20of%20thinking%20with%20images%2C%20where%20models%20actively%20ground%20their%20reasoning%20in%20visual%20evidence.%20Yet%2C%20prevailing%20frameworks%20treat%20visual%20actions%20as%20optional%20tools%2C%20boosting%20metrics%20but%20leaving%20reasoning%20ungrounded%20and%20crops%20ineffective.%20This%20gap%20gives%20rise%20to%20the%20illusion%20of%20thinking%20with%20images%3A%20models%20seem%20visually%20grounded%20but%20rely%20on%20context-agnostic%20actions%20that%20neither%20refine%20perception%20nor%20guide%20reasoning%20toward%20correct%20answers.%20We%20address%20this%20problem%20by%20reframing%20visual%20actions%20as%20core%20reasoning%20primitives%20rather%20than%20optional%20tools%2C%20which%20we%20term%20visual%20rationalization%2C%20the%20visual%20analogue%20of%20textual%20Chain-of-Thought.%20Building%20on%20this%20insight%2C%20we%20propose%20Visual%20Rationale%20Learning%20%28ViRL%29%2C%20an%20end-to-end%20paradigm%20that%20grounds%20training%20in%20the%20visual%20rationale%20itself.%20ViRL%20integrates%20%281%29%20Process%20Supervision%20with%20ground-truth%20rationales%2C%20%282%29%20Objective%20Alignment%20via%20step-level%20reward%20shaping%2C%20and%20%283%29%20Fine-Grained%20Credit%20Assignment%20to%20distinguish%20correct%2C%20redundant%2C%20and%20erroneous%20actions.%20By%20ensuring%20each%20action%20contributes%20meaningfully%20to%20the%20reasoning%20chain%2C%20ViRL%20enables%20models%20to%20%22get%20the%20right%20answer%20for%20the%20right%20visual%20reason%22.%20Trained%20purely%20with%20end-to-end%20RL%2C%20ViRL%20achieves%20state-of-the-art%20results%20across%20benchmarks%20spanning%20perception%2C%20hallucination%2C%20and%20reasoning.%20This%20work%20establishes%20visual%20rationalization%20as%20a%20task-agnostic%2C%20process-grounded%20paradigm%20for%20building%20transparent%2C%20verifiable%2C%20and%20trustworthy%20vision-language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Illusion%2520to%2520Intention%253A%2520Visual%2520Rationale%2520Learning%2520for%2520Vision-Language%2520Reasoning%26entry.906535625%3DChangpeng%2520Wang%2520and%2520Haozhe%2520Wang%2520and%2520Xi%2520Chen%2520and%2520Junhan%2520Liu%2520and%2520Taofeng%2520Xue%2520and%2520Chong%2520Peng%2520and%2520Donglian%2520Qi%2520and%2520Fangzhen%2520Lin%2520and%2520Yunfeng%2520Yan%26entry.1292438233%3DRecent%2520advances%2520in%2520vision-language%2520reasoning%2520underscore%2520the%2520importance%2520of%2520thinking%2520with%2520images%252C%2520where%2520models%2520actively%2520ground%2520their%2520reasoning%2520in%2520visual%2520evidence.%2520Yet%252C%2520prevailing%2520frameworks%2520treat%2520visual%2520actions%2520as%2520optional%2520tools%252C%2520boosting%2520metrics%2520but%2520leaving%2520reasoning%2520ungrounded%2520and%2520crops%2520ineffective.%2520This%2520gap%2520gives%2520rise%2520to%2520the%2520illusion%2520of%2520thinking%2520with%2520images%253A%2520models%2520seem%2520visually%2520grounded%2520but%2520rely%2520on%2520context-agnostic%2520actions%2520that%2520neither%2520refine%2520perception%2520nor%2520guide%2520reasoning%2520toward%2520correct%2520answers.%2520We%2520address%2520this%2520problem%2520by%2520reframing%2520visual%2520actions%2520as%2520core%2520reasoning%2520primitives%2520rather%2520than%2520optional%2520tools%252C%2520which%2520we%2520term%2520visual%2520rationalization%252C%2520the%2520visual%2520analogue%2520of%2520textual%2520Chain-of-Thought.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Visual%2520Rationale%2520Learning%2520%2528ViRL%2529%252C%2520an%2520end-to-end%2520paradigm%2520that%2520grounds%2520training%2520in%2520the%2520visual%2520rationale%2520itself.%2520ViRL%2520integrates%2520%25281%2529%2520Process%2520Supervision%2520with%2520ground-truth%2520rationales%252C%2520%25282%2529%2520Objective%2520Alignment%2520via%2520step-level%2520reward%2520shaping%252C%2520and%2520%25283%2529%2520Fine-Grained%2520Credit%2520Assignment%2520to%2520distinguish%2520correct%252C%2520redundant%252C%2520and%2520erroneous%2520actions.%2520By%2520ensuring%2520each%2520action%2520contributes%2520meaningfully%2520to%2520the%2520reasoning%2520chain%252C%2520ViRL%2520enables%2520models%2520to%2520%2522get%2520the%2520right%2520answer%2520for%2520the%2520right%2520visual%2520reason%2522.%2520Trained%2520purely%2520with%2520end-to-end%2520RL%252C%2520ViRL%2520achieves%2520state-of-the-art%2520results%2520across%2520benchmarks%2520spanning%2520perception%252C%2520hallucination%252C%2520and%2520reasoning.%2520This%2520work%2520establishes%2520visual%2520rationalization%2520as%2520a%2520task-agnostic%252C%2520process-grounded%2520paradigm%2520for%2520building%2520transparent%252C%2520verifiable%252C%2520and%2520trustworthy%2520vision-language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Illusion%20to%20Intention%3A%20Visual%20Rationale%20Learning%20for%20Vision-Language%20Reasoning&entry.906535625=Changpeng%20Wang%20and%20Haozhe%20Wang%20and%20Xi%20Chen%20and%20Junhan%20Liu%20and%20Taofeng%20Xue%20and%20Chong%20Peng%20and%20Donglian%20Qi%20and%20Fangzhen%20Lin%20and%20Yunfeng%20Yan&entry.1292438233=Recent%20advances%20in%20vision-language%20reasoning%20underscore%20the%20importance%20of%20thinking%20with%20images%2C%20where%20models%20actively%20ground%20their%20reasoning%20in%20visual%20evidence.%20Yet%2C%20prevailing%20frameworks%20treat%20visual%20actions%20as%20optional%20tools%2C%20boosting%20metrics%20but%20leaving%20reasoning%20ungrounded%20and%20crops%20ineffective.%20This%20gap%20gives%20rise%20to%20the%20illusion%20of%20thinking%20with%20images%3A%20models%20seem%20visually%20grounded%20but%20rely%20on%20context-agnostic%20actions%20that%20neither%20refine%20perception%20nor%20guide%20reasoning%20toward%20correct%20answers.%20We%20address%20this%20problem%20by%20reframing%20visual%20actions%20as%20core%20reasoning%20primitives%20rather%20than%20optional%20tools%2C%20which%20we%20term%20visual%20rationalization%2C%20the%20visual%20analogue%20of%20textual%20Chain-of-Thought.%20Building%20on%20this%20insight%2C%20we%20propose%20Visual%20Rationale%20Learning%20%28ViRL%29%2C%20an%20end-to-end%20paradigm%20that%20grounds%20training%20in%20the%20visual%20rationale%20itself.%20ViRL%20integrates%20%281%29%20Process%20Supervision%20with%20ground-truth%20rationales%2C%20%282%29%20Objective%20Alignment%20via%20step-level%20reward%20shaping%2C%20and%20%283%29%20Fine-Grained%20Credit%20Assignment%20to%20distinguish%20correct%2C%20redundant%2C%20and%20erroneous%20actions.%20By%20ensuring%20each%20action%20contributes%20meaningfully%20to%20the%20reasoning%20chain%2C%20ViRL%20enables%20models%20to%20%22get%20the%20right%20answer%20for%20the%20right%20visual%20reason%22.%20Trained%20purely%20with%20end-to-end%20RL%2C%20ViRL%20achieves%20state-of-the-art%20results%20across%20benchmarks%20spanning%20perception%2C%20hallucination%2C%20and%20reasoning.%20This%20work%20establishes%20visual%20rationalization%20as%20a%20task-agnostic%2C%20process-grounded%20paradigm%20for%20building%20transparent%2C%20verifiable%2C%20and%20trustworthy%20vision-language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.23031v1&entry.124074799=Read"},
{"title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction", "author": "Sinan Du and Jiahao Guo and Bo Li and Shuhao Cui and Zhengzhuo Xu and Yifu Luo and Yongxian Wei and Kun Gai and Xinggang Wang and Kai Wu and Chun Yuan", "abstract": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.", "link": "http://arxiv.org/abs/2511.23386v1", "date": "2025-11-28", "relevancy": 2.8963, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQRAE%3A%20Representation%20Quantization%20Autoencoders%20for%20Multimodal%20Understanding%2C%20Generation%20and%20Reconstruction&body=Title%3A%20VQRAE%3A%20Representation%20Quantization%20Autoencoders%20for%20Multimodal%20Understanding%2C%20Generation%20and%20Reconstruction%0AAuthor%3A%20Sinan%20Du%20and%20Jiahao%20Guo%20and%20Bo%20Li%20and%20Shuhao%20Cui%20and%20Zhengzhuo%20Xu%20and%20Yifu%20Luo%20and%20Yongxian%20Wei%20and%20Kun%20Gai%20and%20Xinggang%20Wang%20and%20Kai%20Wu%20and%20Chun%20Yuan%0AAbstract%3A%20Unifying%20multimodal%20understanding%2C%20generation%20and%20reconstruction%20representation%20in%20a%20single%20tokenizer%20remains%20a%20key%20challenge%20in%20building%20unified%20models.%20Previous%20research%20predominantly%20attempts%20to%20address%20this%20in%20a%20dual%20encoder%20paradigm%2C%20e.g.%2C%20utilizing%20the%20separate%20encoders%20for%20understanding%20and%20generation%20respectively%20or%20balancing%20semantic%20representations%20and%20low-level%20features%20with%20contrastive%20loss.%20In%20this%20paper%2C%20we%20propose%20VQRAE%2C%20a%20Vector%20Quantization%20version%20of%20Representation%20AutoEncoders%2C%20which%20pioneers%20the%20first%20exploration%20in%20unified%20representation%20to%20produce%20Continuous%20semantic%20features%20for%20image%20understanding%20and%20Discrete%20tokens%20for%20visual%20generation%20within%20a%20unified%20tokenizer.%20Specifically%2C%20we%20build%20upon%20pretrained%20vision%20foundation%20models%20with%20a%20symmetric%20ViT%20decoder%20and%20adopt%20a%20two-stage%20training%20strategy%3A%20first%2C%20it%20freezes%20the%20encoder%20and%20learns%20a%20high-dimensional%20semantic%20VQ%20codebook%20with%20pixel%20reconstruction%20objective%3B%20then%20jointly%20optimizes%20the%20encoder%20with%20self-distillation%20constraints.%20This%20design%20enables%20negligible%20semantic%20information%20for%20maintaining%20the%20ability%20of%20multimodal%20understanding%2C%20discrete%20tokens%20that%20are%20compatible%20for%20generation%20and%20fine-grained%20reconstruction.%20Besides%2C%20we%20identify%20the%20intriguing%20property%20in%20quantizing%20semantic%20encoders%20that%20rely%20on%20high-dimensional%20codebook%20in%20contrast%20to%20the%20previous%20common%20practice%20of%20low-dimensional%20codebook%20in%20image%20reconstruction.%20The%20semantic%20VQ%20codebook%20can%20achieve%20a%20100%25%20utilization%20ratio%20at%20a%20dimension%20of%201536.%20VQRAE%20presents%20competitive%20performance%20on%20several%20benchmarks%20of%20visual%20understanding%2C%20generation%20and%20reconstruction%20with%20promising%20scaling%20property%20in%20the%20autoregressive%20paradigm%20for%20its%20discrete%20merits.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQRAE%253A%2520Representation%2520Quantization%2520Autoencoders%2520for%2520Multimodal%2520Understanding%252C%2520Generation%2520and%2520Reconstruction%26entry.906535625%3DSinan%2520Du%2520and%2520Jiahao%2520Guo%2520and%2520Bo%2520Li%2520and%2520Shuhao%2520Cui%2520and%2520Zhengzhuo%2520Xu%2520and%2520Yifu%2520Luo%2520and%2520Yongxian%2520Wei%2520and%2520Kun%2520Gai%2520and%2520Xinggang%2520Wang%2520and%2520Kai%2520Wu%2520and%2520Chun%2520Yuan%26entry.1292438233%3DUnifying%2520multimodal%2520understanding%252C%2520generation%2520and%2520reconstruction%2520representation%2520in%2520a%2520single%2520tokenizer%2520remains%2520a%2520key%2520challenge%2520in%2520building%2520unified%2520models.%2520Previous%2520research%2520predominantly%2520attempts%2520to%2520address%2520this%2520in%2520a%2520dual%2520encoder%2520paradigm%252C%2520e.g.%252C%2520utilizing%2520the%2520separate%2520encoders%2520for%2520understanding%2520and%2520generation%2520respectively%2520or%2520balancing%2520semantic%2520representations%2520and%2520low-level%2520features%2520with%2520contrastive%2520loss.%2520In%2520this%2520paper%252C%2520we%2520propose%2520VQRAE%252C%2520a%2520Vector%2520Quantization%2520version%2520of%2520Representation%2520AutoEncoders%252C%2520which%2520pioneers%2520the%2520first%2520exploration%2520in%2520unified%2520representation%2520to%2520produce%2520Continuous%2520semantic%2520features%2520for%2520image%2520understanding%2520and%2520Discrete%2520tokens%2520for%2520visual%2520generation%2520within%2520a%2520unified%2520tokenizer.%2520Specifically%252C%2520we%2520build%2520upon%2520pretrained%2520vision%2520foundation%2520models%2520with%2520a%2520symmetric%2520ViT%2520decoder%2520and%2520adopt%2520a%2520two-stage%2520training%2520strategy%253A%2520first%252C%2520it%2520freezes%2520the%2520encoder%2520and%2520learns%2520a%2520high-dimensional%2520semantic%2520VQ%2520codebook%2520with%2520pixel%2520reconstruction%2520objective%253B%2520then%2520jointly%2520optimizes%2520the%2520encoder%2520with%2520self-distillation%2520constraints.%2520This%2520design%2520enables%2520negligible%2520semantic%2520information%2520for%2520maintaining%2520the%2520ability%2520of%2520multimodal%2520understanding%252C%2520discrete%2520tokens%2520that%2520are%2520compatible%2520for%2520generation%2520and%2520fine-grained%2520reconstruction.%2520Besides%252C%2520we%2520identify%2520the%2520intriguing%2520property%2520in%2520quantizing%2520semantic%2520encoders%2520that%2520rely%2520on%2520high-dimensional%2520codebook%2520in%2520contrast%2520to%2520the%2520previous%2520common%2520practice%2520of%2520low-dimensional%2520codebook%2520in%2520image%2520reconstruction.%2520The%2520semantic%2520VQ%2520codebook%2520can%2520achieve%2520a%2520100%2525%2520utilization%2520ratio%2520at%2520a%2520dimension%2520of%25201536.%2520VQRAE%2520presents%2520competitive%2520performance%2520on%2520several%2520benchmarks%2520of%2520visual%2520understanding%252C%2520generation%2520and%2520reconstruction%2520with%2520promising%2520scaling%2520property%2520in%2520the%2520autoregressive%2520paradigm%2520for%2520its%2520discrete%2520merits.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQRAE%3A%20Representation%20Quantization%20Autoencoders%20for%20Multimodal%20Understanding%2C%20Generation%20and%20Reconstruction&entry.906535625=Sinan%20Du%20and%20Jiahao%20Guo%20and%20Bo%20Li%20and%20Shuhao%20Cui%20and%20Zhengzhuo%20Xu%20and%20Yifu%20Luo%20and%20Yongxian%20Wei%20and%20Kun%20Gai%20and%20Xinggang%20Wang%20and%20Kai%20Wu%20and%20Chun%20Yuan&entry.1292438233=Unifying%20multimodal%20understanding%2C%20generation%20and%20reconstruction%20representation%20in%20a%20single%20tokenizer%20remains%20a%20key%20challenge%20in%20building%20unified%20models.%20Previous%20research%20predominantly%20attempts%20to%20address%20this%20in%20a%20dual%20encoder%20paradigm%2C%20e.g.%2C%20utilizing%20the%20separate%20encoders%20for%20understanding%20and%20generation%20respectively%20or%20balancing%20semantic%20representations%20and%20low-level%20features%20with%20contrastive%20loss.%20In%20this%20paper%2C%20we%20propose%20VQRAE%2C%20a%20Vector%20Quantization%20version%20of%20Representation%20AutoEncoders%2C%20which%20pioneers%20the%20first%20exploration%20in%20unified%20representation%20to%20produce%20Continuous%20semantic%20features%20for%20image%20understanding%20and%20Discrete%20tokens%20for%20visual%20generation%20within%20a%20unified%20tokenizer.%20Specifically%2C%20we%20build%20upon%20pretrained%20vision%20foundation%20models%20with%20a%20symmetric%20ViT%20decoder%20and%20adopt%20a%20two-stage%20training%20strategy%3A%20first%2C%20it%20freezes%20the%20encoder%20and%20learns%20a%20high-dimensional%20semantic%20VQ%20codebook%20with%20pixel%20reconstruction%20objective%3B%20then%20jointly%20optimizes%20the%20encoder%20with%20self-distillation%20constraints.%20This%20design%20enables%20negligible%20semantic%20information%20for%20maintaining%20the%20ability%20of%20multimodal%20understanding%2C%20discrete%20tokens%20that%20are%20compatible%20for%20generation%20and%20fine-grained%20reconstruction.%20Besides%2C%20we%20identify%20the%20intriguing%20property%20in%20quantizing%20semantic%20encoders%20that%20rely%20on%20high-dimensional%20codebook%20in%20contrast%20to%20the%20previous%20common%20practice%20of%20low-dimensional%20codebook%20in%20image%20reconstruction.%20The%20semantic%20VQ%20codebook%20can%20achieve%20a%20100%25%20utilization%20ratio%20at%20a%20dimension%20of%201536.%20VQRAE%20presents%20competitive%20performance%20on%20several%20benchmarks%20of%20visual%20understanding%2C%20generation%20and%20reconstruction%20with%20promising%20scaling%20property%20in%20the%20autoregressive%20paradigm%20for%20its%20discrete%20merits.&entry.1838667208=http%3A//arxiv.org/abs/2511.23386v1&entry.124074799=Read"},
{"title": "GOATex: Geometry & Occlusion-Aware Texturing", "author": "Hyunjin Kim and Kunho Kim and Adam Lee and Wonkwang Lee", "abstract": "We present GOATex, a diffusion-based method for 3D mesh texturing that generates high-quality textures for both exterior and interior surfaces. While existing methods perform well on visible regions, they inherently lack mechanisms to handle occluded interiors, resulting in incomplete textures and visible seams. To address this, we introduce an occlusion-aware texturing framework based on the concept of hit levels, which quantify the relative depth of mesh faces via multi-view ray casting. This allows us to partition mesh faces into ordered visibility layers, from outermost to innermost. We then apply a two-stage visibility control strategy that progressively reveals interior regions with structural coherence, followed by texturing each layer using a pretrained diffusion model. To seamlessly merge textures obtained across layers, we propose a soft UV-space blending technique that weighs each texture's contribution based on view-dependent visibility confidence. Empirical results demonstrate that GOATex consistently outperforms existing methods, producing seamless, high-fidelity textures across both visible and occluded surfaces. Unlike prior works, GOATex operates entirely without costly fine-tuning of a pretrained diffusion model and allows separate prompting for exterior and interior mesh regions, enabling fine-grained control over layered appearances. For more qualitative results, please visit our project page: https://goatex3d.github.io/.", "link": "http://arxiv.org/abs/2511.23051v1", "date": "2025-11-28", "relevancy": 2.8301, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5691}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5645}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GOATex%3A%20Geometry%20%26%20Occlusion-Aware%20Texturing&body=Title%3A%20GOATex%3A%20Geometry%20%26%20Occlusion-Aware%20Texturing%0AAuthor%3A%20Hyunjin%20Kim%20and%20Kunho%20Kim%20and%20Adam%20Lee%20and%20Wonkwang%20Lee%0AAbstract%3A%20We%20present%20GOATex%2C%20a%20diffusion-based%20method%20for%203D%20mesh%20texturing%20that%20generates%20high-quality%20textures%20for%20both%20exterior%20and%20interior%20surfaces.%20While%20existing%20methods%20perform%20well%20on%20visible%20regions%2C%20they%20inherently%20lack%20mechanisms%20to%20handle%20occluded%20interiors%2C%20resulting%20in%20incomplete%20textures%20and%20visible%20seams.%20To%20address%20this%2C%20we%20introduce%20an%20occlusion-aware%20texturing%20framework%20based%20on%20the%20concept%20of%20hit%20levels%2C%20which%20quantify%20the%20relative%20depth%20of%20mesh%20faces%20via%20multi-view%20ray%20casting.%20This%20allows%20us%20to%20partition%20mesh%20faces%20into%20ordered%20visibility%20layers%2C%20from%20outermost%20to%20innermost.%20We%20then%20apply%20a%20two-stage%20visibility%20control%20strategy%20that%20progressively%20reveals%20interior%20regions%20with%20structural%20coherence%2C%20followed%20by%20texturing%20each%20layer%20using%20a%20pretrained%20diffusion%20model.%20To%20seamlessly%20merge%20textures%20obtained%20across%20layers%2C%20we%20propose%20a%20soft%20UV-space%20blending%20technique%20that%20weighs%20each%20texture%27s%20contribution%20based%20on%20view-dependent%20visibility%20confidence.%20Empirical%20results%20demonstrate%20that%20GOATex%20consistently%20outperforms%20existing%20methods%2C%20producing%20seamless%2C%20high-fidelity%20textures%20across%20both%20visible%20and%20occluded%20surfaces.%20Unlike%20prior%20works%2C%20GOATex%20operates%20entirely%20without%20costly%20fine-tuning%20of%20a%20pretrained%20diffusion%20model%20and%20allows%20separate%20prompting%20for%20exterior%20and%20interior%20mesh%20regions%2C%20enabling%20fine-grained%20control%20over%20layered%20appearances.%20For%20more%20qualitative%20results%2C%20please%20visit%20our%20project%20page%3A%20https%3A//goatex3d.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGOATex%253A%2520Geometry%2520%2526%2520Occlusion-Aware%2520Texturing%26entry.906535625%3DHyunjin%2520Kim%2520and%2520Kunho%2520Kim%2520and%2520Adam%2520Lee%2520and%2520Wonkwang%2520Lee%26entry.1292438233%3DWe%2520present%2520GOATex%252C%2520a%2520diffusion-based%2520method%2520for%25203D%2520mesh%2520texturing%2520that%2520generates%2520high-quality%2520textures%2520for%2520both%2520exterior%2520and%2520interior%2520surfaces.%2520While%2520existing%2520methods%2520perform%2520well%2520on%2520visible%2520regions%252C%2520they%2520inherently%2520lack%2520mechanisms%2520to%2520handle%2520occluded%2520interiors%252C%2520resulting%2520in%2520incomplete%2520textures%2520and%2520visible%2520seams.%2520To%2520address%2520this%252C%2520we%2520introduce%2520an%2520occlusion-aware%2520texturing%2520framework%2520based%2520on%2520the%2520concept%2520of%2520hit%2520levels%252C%2520which%2520quantify%2520the%2520relative%2520depth%2520of%2520mesh%2520faces%2520via%2520multi-view%2520ray%2520casting.%2520This%2520allows%2520us%2520to%2520partition%2520mesh%2520faces%2520into%2520ordered%2520visibility%2520layers%252C%2520from%2520outermost%2520to%2520innermost.%2520We%2520then%2520apply%2520a%2520two-stage%2520visibility%2520control%2520strategy%2520that%2520progressively%2520reveals%2520interior%2520regions%2520with%2520structural%2520coherence%252C%2520followed%2520by%2520texturing%2520each%2520layer%2520using%2520a%2520pretrained%2520diffusion%2520model.%2520To%2520seamlessly%2520merge%2520textures%2520obtained%2520across%2520layers%252C%2520we%2520propose%2520a%2520soft%2520UV-space%2520blending%2520technique%2520that%2520weighs%2520each%2520texture%2527s%2520contribution%2520based%2520on%2520view-dependent%2520visibility%2520confidence.%2520Empirical%2520results%2520demonstrate%2520that%2520GOATex%2520consistently%2520outperforms%2520existing%2520methods%252C%2520producing%2520seamless%252C%2520high-fidelity%2520textures%2520across%2520both%2520visible%2520and%2520occluded%2520surfaces.%2520Unlike%2520prior%2520works%252C%2520GOATex%2520operates%2520entirely%2520without%2520costly%2520fine-tuning%2520of%2520a%2520pretrained%2520diffusion%2520model%2520and%2520allows%2520separate%2520prompting%2520for%2520exterior%2520and%2520interior%2520mesh%2520regions%252C%2520enabling%2520fine-grained%2520control%2520over%2520layered%2520appearances.%2520For%2520more%2520qualitative%2520results%252C%2520please%2520visit%2520our%2520project%2520page%253A%2520https%253A//goatex3d.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOATex%3A%20Geometry%20%26%20Occlusion-Aware%20Texturing&entry.906535625=Hyunjin%20Kim%20and%20Kunho%20Kim%20and%20Adam%20Lee%20and%20Wonkwang%20Lee&entry.1292438233=We%20present%20GOATex%2C%20a%20diffusion-based%20method%20for%203D%20mesh%20texturing%20that%20generates%20high-quality%20textures%20for%20both%20exterior%20and%20interior%20surfaces.%20While%20existing%20methods%20perform%20well%20on%20visible%20regions%2C%20they%20inherently%20lack%20mechanisms%20to%20handle%20occluded%20interiors%2C%20resulting%20in%20incomplete%20textures%20and%20visible%20seams.%20To%20address%20this%2C%20we%20introduce%20an%20occlusion-aware%20texturing%20framework%20based%20on%20the%20concept%20of%20hit%20levels%2C%20which%20quantify%20the%20relative%20depth%20of%20mesh%20faces%20via%20multi-view%20ray%20casting.%20This%20allows%20us%20to%20partition%20mesh%20faces%20into%20ordered%20visibility%20layers%2C%20from%20outermost%20to%20innermost.%20We%20then%20apply%20a%20two-stage%20visibility%20control%20strategy%20that%20progressively%20reveals%20interior%20regions%20with%20structural%20coherence%2C%20followed%20by%20texturing%20each%20layer%20using%20a%20pretrained%20diffusion%20model.%20To%20seamlessly%20merge%20textures%20obtained%20across%20layers%2C%20we%20propose%20a%20soft%20UV-space%20blending%20technique%20that%20weighs%20each%20texture%27s%20contribution%20based%20on%20view-dependent%20visibility%20confidence.%20Empirical%20results%20demonstrate%20that%20GOATex%20consistently%20outperforms%20existing%20methods%2C%20producing%20seamless%2C%20high-fidelity%20textures%20across%20both%20visible%20and%20occluded%20surfaces.%20Unlike%20prior%20works%2C%20GOATex%20operates%20entirely%20without%20costly%20fine-tuning%20of%20a%20pretrained%20diffusion%20model%20and%20allows%20separate%20prompting%20for%20exterior%20and%20interior%20mesh%20regions%2C%20enabling%20fine-grained%20control%20over%20layered%20appearances.%20For%20more%20qualitative%20results%2C%20please%20visit%20our%20project%20page%3A%20https%3A//goatex3d.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2511.23051v1&entry.124074799=Read"},
{"title": "Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent", "author": "Jianzhe Lin and Zeyu Pan and Yun Zhu and Ruiqi Song and Jining Yang", "abstract": "We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.", "link": "http://arxiv.org/abs/2511.23436v1", "date": "2025-11-28", "relevancy": 2.8153, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6253}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.57}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Continuous%20Intelligence%20Growth%3A%20Self-Training%2C%20Continual%20Learning%2C%20and%20Dual-Scale%20Memory%20in%20SuperIntelliAgent&body=Title%3A%20Towards%20Continuous%20Intelligence%20Growth%3A%20Self-Training%2C%20Continual%20Learning%2C%20and%20Dual-Scale%20Memory%20in%20SuperIntelliAgent%0AAuthor%3A%20Jianzhe%20Lin%20and%20Zeyu%20Pan%20and%20Yun%20Zhu%20and%20Ruiqi%20Song%20and%20Jining%20Yang%0AAbstract%3A%20We%20introduce%20SuperIntelliAgent%2C%20an%20agentic%20learning%20framework%20that%20couples%20a%20trainable%20small%20diffusion%20model%20%28the%20learner%29%20with%20a%20frozen%20large%20language%20model%20%28the%20verifier%29%20to%20enable%20continual%20intelligence%20growth%20through%20self-supervised%20interaction.%20Unlike%20conventional%20supervised%20fine-tuning%2C%20SuperIntelliAgent%20learns%20autonomously%20without%20annotation%3A%20the%20learner%20generates%20candidate%20outputs%2C%20the%20verifier%20evaluates%20them%20through%20step-by-step%20reasoning%2C%20and%20their%20interaction%20produces%20chosen/rejected%20pairs%20for%20Direct%20Preference%20Optimization%20%28DPO%29.%20This%20converts%20each%20input%20into%20a%20pseudo-training%20signal%20for%20continual%20improvement.%20The%20framework%20integrates%20dual-scale%20memory%3A%20short-term%20in-context%20memory%20that%20preserves%20reasoning%20traces%20across%20refinement%20cycles%2C%20and%20long-term%20memory%20that%20consolidates%20acquired%20knowledge%20through%20lightweight%20on-the-fly%20fine-tuning.%20A%20replay%20buffer%20retains%20samples%20that%20show%20verifiable%20progress%20and%20replays%20them%20as%20auxiliary%20supervision%2C%20reinforcing%20recent%20learning%20while%20forming%20adaptive%20curricula.%20SuperIntelliAgent%20is%20infrastructure-agnostic%20and%20can%20be%20plugged%20into%20existing%20agentic%20frameworks%20while%20turning%20ordinary%20inference%20loops%20into%20a%20lifelong%20optimization%20process.%20We%20posit%20that%20pairing%20a%20trainable%20learner%20with%20a%20reasoning-capable%20verifier%20forms%20a%20minimal%20reliable%20unit%20of%20growing%20intelligence%2C%20as%20paired%20feedback%20and%20partial-history%20replay%20yield%20richer%20learning%20curricula%20and%20stronger%20preference%20alignment.%20With%20a%20small%20number%20of%20automatically%20generated%20DPO%20pairs%2C%20the%20learner%20improves%20across%20all%20benchmarks%2C%20indicating%20that%20this%20mechanism%20provides%20a%20promising%20direction%20for%20continual%20intelligence%20accumulation%20and%20real-world%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Continuous%2520Intelligence%2520Growth%253A%2520Self-Training%252C%2520Continual%2520Learning%252C%2520and%2520Dual-Scale%2520Memory%2520in%2520SuperIntelliAgent%26entry.906535625%3DJianzhe%2520Lin%2520and%2520Zeyu%2520Pan%2520and%2520Yun%2520Zhu%2520and%2520Ruiqi%2520Song%2520and%2520Jining%2520Yang%26entry.1292438233%3DWe%2520introduce%2520SuperIntelliAgent%252C%2520an%2520agentic%2520learning%2520framework%2520that%2520couples%2520a%2520trainable%2520small%2520diffusion%2520model%2520%2528the%2520learner%2529%2520with%2520a%2520frozen%2520large%2520language%2520model%2520%2528the%2520verifier%2529%2520to%2520enable%2520continual%2520intelligence%2520growth%2520through%2520self-supervised%2520interaction.%2520Unlike%2520conventional%2520supervised%2520fine-tuning%252C%2520SuperIntelliAgent%2520learns%2520autonomously%2520without%2520annotation%253A%2520the%2520learner%2520generates%2520candidate%2520outputs%252C%2520the%2520verifier%2520evaluates%2520them%2520through%2520step-by-step%2520reasoning%252C%2520and%2520their%2520interaction%2520produces%2520chosen/rejected%2520pairs%2520for%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529.%2520This%2520converts%2520each%2520input%2520into%2520a%2520pseudo-training%2520signal%2520for%2520continual%2520improvement.%2520The%2520framework%2520integrates%2520dual-scale%2520memory%253A%2520short-term%2520in-context%2520memory%2520that%2520preserves%2520reasoning%2520traces%2520across%2520refinement%2520cycles%252C%2520and%2520long-term%2520memory%2520that%2520consolidates%2520acquired%2520knowledge%2520through%2520lightweight%2520on-the-fly%2520fine-tuning.%2520A%2520replay%2520buffer%2520retains%2520samples%2520that%2520show%2520verifiable%2520progress%2520and%2520replays%2520them%2520as%2520auxiliary%2520supervision%252C%2520reinforcing%2520recent%2520learning%2520while%2520forming%2520adaptive%2520curricula.%2520SuperIntelliAgent%2520is%2520infrastructure-agnostic%2520and%2520can%2520be%2520plugged%2520into%2520existing%2520agentic%2520frameworks%2520while%2520turning%2520ordinary%2520inference%2520loops%2520into%2520a%2520lifelong%2520optimization%2520process.%2520We%2520posit%2520that%2520pairing%2520a%2520trainable%2520learner%2520with%2520a%2520reasoning-capable%2520verifier%2520forms%2520a%2520minimal%2520reliable%2520unit%2520of%2520growing%2520intelligence%252C%2520as%2520paired%2520feedback%2520and%2520partial-history%2520replay%2520yield%2520richer%2520learning%2520curricula%2520and%2520stronger%2520preference%2520alignment.%2520With%2520a%2520small%2520number%2520of%2520automatically%2520generated%2520DPO%2520pairs%252C%2520the%2520learner%2520improves%2520across%2520all%2520benchmarks%252C%2520indicating%2520that%2520this%2520mechanism%2520provides%2520a%2520promising%2520direction%2520for%2520continual%2520intelligence%2520accumulation%2520and%2520real-world%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Continuous%20Intelligence%20Growth%3A%20Self-Training%2C%20Continual%20Learning%2C%20and%20Dual-Scale%20Memory%20in%20SuperIntelliAgent&entry.906535625=Jianzhe%20Lin%20and%20Zeyu%20Pan%20and%20Yun%20Zhu%20and%20Ruiqi%20Song%20and%20Jining%20Yang&entry.1292438233=We%20introduce%20SuperIntelliAgent%2C%20an%20agentic%20learning%20framework%20that%20couples%20a%20trainable%20small%20diffusion%20model%20%28the%20learner%29%20with%20a%20frozen%20large%20language%20model%20%28the%20verifier%29%20to%20enable%20continual%20intelligence%20growth%20through%20self-supervised%20interaction.%20Unlike%20conventional%20supervised%20fine-tuning%2C%20SuperIntelliAgent%20learns%20autonomously%20without%20annotation%3A%20the%20learner%20generates%20candidate%20outputs%2C%20the%20verifier%20evaluates%20them%20through%20step-by-step%20reasoning%2C%20and%20their%20interaction%20produces%20chosen/rejected%20pairs%20for%20Direct%20Preference%20Optimization%20%28DPO%29.%20This%20converts%20each%20input%20into%20a%20pseudo-training%20signal%20for%20continual%20improvement.%20The%20framework%20integrates%20dual-scale%20memory%3A%20short-term%20in-context%20memory%20that%20preserves%20reasoning%20traces%20across%20refinement%20cycles%2C%20and%20long-term%20memory%20that%20consolidates%20acquired%20knowledge%20through%20lightweight%20on-the-fly%20fine-tuning.%20A%20replay%20buffer%20retains%20samples%20that%20show%20verifiable%20progress%20and%20replays%20them%20as%20auxiliary%20supervision%2C%20reinforcing%20recent%20learning%20while%20forming%20adaptive%20curricula.%20SuperIntelliAgent%20is%20infrastructure-agnostic%20and%20can%20be%20plugged%20into%20existing%20agentic%20frameworks%20while%20turning%20ordinary%20inference%20loops%20into%20a%20lifelong%20optimization%20process.%20We%20posit%20that%20pairing%20a%20trainable%20learner%20with%20a%20reasoning-capable%20verifier%20forms%20a%20minimal%20reliable%20unit%20of%20growing%20intelligence%2C%20as%20paired%20feedback%20and%20partial-history%20replay%20yield%20richer%20learning%20curricula%20and%20stronger%20preference%20alignment.%20With%20a%20small%20number%20of%20automatically%20generated%20DPO%20pairs%2C%20the%20learner%20improves%20across%20all%20benchmarks%2C%20indicating%20that%20this%20mechanism%20provides%20a%20promising%20direction%20for%20continual%20intelligence%20accumulation%20and%20real-world%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2511.23436v1&entry.124074799=Read"},
{"title": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search", "author": "Yunqi Zhou and Chengjie Jiang and Chun Yuan and Jing Li", "abstract": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.", "link": "http://arxiv.org/abs/2511.20460v2", "date": "2025-11-28", "relevancy": 2.8048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Where%20It%20Matters%3A%20Training-Free%20Ultra-HR%20Remote%20Sensing%20VQA%20via%20Adaptive%20Zoom%20Search&body=Title%3A%20Look%20Where%20It%20Matters%3A%20Training-Free%20Ultra-HR%20Remote%20Sensing%20VQA%20via%20Adaptive%20Zoom%20Search%0AAuthor%3A%20Yunqi%20Zhou%20and%20Chengjie%20Jiang%20and%20Chun%20Yuan%20and%20Jing%20Li%0AAbstract%3A%20With%20advances%20in%20satellite%20constellations%2C%20sensor%20technologies%2C%20and%20imaging%20pipelines%2C%20ultra-high-resolution%20%28Ultra-HR%29%20remote%20sensing%20imagery%20is%20becoming%20increasingly%20widespread.%20However%2C%20current%20remote%20sensing%20foundation%20models%20are%20ill-suited%20to%20such%20inputs%3A%20full-image%20encoding%20exhausts%20token%20and%20memory%20budgets%2C%20while%20resize-based%20preprocessing%20loses%20fine-grained%20and%20answer-critical%20details.%20In%20this%20context%2C%20guiding%20the%20model%20look%20where%20it%20matters%20before%20prediction%20becomes%20crucial.%20Therefore%2C%20we%20present%20ZoomSearch%2C%20a%20training-free%2C%20plug-and-play%20pipeline%20that%20decouples%20%27where%20to%20look%27%20from%20%27how%20to%20answer%27%20for%20Ultra-HR%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RS-VQA%29.%20ZoomSearch%20combines%20Adaptive%20Multi-Branch%20Zoom%20Search%2C%20which%20performs%20a%20hierarchical%20search%20over%20image%20patches%20to%20localize%20query-relevant%20regions%2C%20with%20Layout-Aware%20Patch%20Reassembly%2C%20which%20reorganizes%20the%20selected%20patches%20into%20a%20compact%2C%20layout-faithful%20canvas.%20We%20conduct%20comprehensive%20experiments%20on%20Ultra-HR%20RS-VQA%20benchmarks%20MME-RealWorld-RS%20and%20LRS-VQA%2C%20comparing%20against%20%28i%29%20strong%20general%20foundation%20models%2C%20%28ii%29%20remote%20sensing%20foundation%20models%2C%20%28iii%29%20Ultra-HR%20RS-VQA%20methods%2C%20and%20%28iv%29%20plug-and-play%20search-based%20VQA%20methods.%20When%20integrated%20with%20LLaVA-ov%2C%20ZoomSearch%20attains%20state-of-the-art%20accuracy%20across%20diverse%20tasks%2C%20improving%20the%20LLaVA-ov%20baseline%20by%2026.3%25%20on%20LRS-VQA%20and%20114.8%25%20on%20MME-RealWorld-RS.%20Meanwhile%2C%20it%20achieves%20much%20higher%20inference%20efficiency%2C%20outperforming%20prior%20search-based%20methods%20by%2020%25~44%25%20in%20speed.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20460v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Where%2520It%2520Matters%253A%2520Training-Free%2520Ultra-HR%2520Remote%2520Sensing%2520VQA%2520via%2520Adaptive%2520Zoom%2520Search%26entry.906535625%3DYunqi%2520Zhou%2520and%2520Chengjie%2520Jiang%2520and%2520Chun%2520Yuan%2520and%2520Jing%2520Li%26entry.1292438233%3DWith%2520advances%2520in%2520satellite%2520constellations%252C%2520sensor%2520technologies%252C%2520and%2520imaging%2520pipelines%252C%2520ultra-high-resolution%2520%2528Ultra-HR%2529%2520remote%2520sensing%2520imagery%2520is%2520becoming%2520increasingly%2520widespread.%2520However%252C%2520current%2520remote%2520sensing%2520foundation%2520models%2520are%2520ill-suited%2520to%2520such%2520inputs%253A%2520full-image%2520encoding%2520exhausts%2520token%2520and%2520memory%2520budgets%252C%2520while%2520resize-based%2520preprocessing%2520loses%2520fine-grained%2520and%2520answer-critical%2520details.%2520In%2520this%2520context%252C%2520guiding%2520the%2520model%2520look%2520where%2520it%2520matters%2520before%2520prediction%2520becomes%2520crucial.%2520Therefore%252C%2520we%2520present%2520ZoomSearch%252C%2520a%2520training-free%252C%2520plug-and-play%2520pipeline%2520that%2520decouples%2520%2527where%2520to%2520look%2527%2520from%2520%2527how%2520to%2520answer%2527%2520for%2520Ultra-HR%2520Remote%2520Sensing%2520Visual%2520Question%2520Answering%2520%2528RS-VQA%2529.%2520ZoomSearch%2520combines%2520Adaptive%2520Multi-Branch%2520Zoom%2520Search%252C%2520which%2520performs%2520a%2520hierarchical%2520search%2520over%2520image%2520patches%2520to%2520localize%2520query-relevant%2520regions%252C%2520with%2520Layout-Aware%2520Patch%2520Reassembly%252C%2520which%2520reorganizes%2520the%2520selected%2520patches%2520into%2520a%2520compact%252C%2520layout-faithful%2520canvas.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520Ultra-HR%2520RS-VQA%2520benchmarks%2520MME-RealWorld-RS%2520and%2520LRS-VQA%252C%2520comparing%2520against%2520%2528i%2529%2520strong%2520general%2520foundation%2520models%252C%2520%2528ii%2529%2520remote%2520sensing%2520foundation%2520models%252C%2520%2528iii%2529%2520Ultra-HR%2520RS-VQA%2520methods%252C%2520and%2520%2528iv%2529%2520plug-and-play%2520search-based%2520VQA%2520methods.%2520When%2520integrated%2520with%2520LLaVA-ov%252C%2520ZoomSearch%2520attains%2520state-of-the-art%2520accuracy%2520across%2520diverse%2520tasks%252C%2520improving%2520the%2520LLaVA-ov%2520baseline%2520by%252026.3%2525%2520on%2520LRS-VQA%2520and%2520114.8%2525%2520on%2520MME-RealWorld-RS.%2520Meanwhile%252C%2520it%2520achieves%2520much%2520higher%2520inference%2520efficiency%252C%2520outperforming%2520prior%2520search-based%2520methods%2520by%252020%2525~44%2525%2520in%2520speed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20460v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Where%20It%20Matters%3A%20Training-Free%20Ultra-HR%20Remote%20Sensing%20VQA%20via%20Adaptive%20Zoom%20Search&entry.906535625=Yunqi%20Zhou%20and%20Chengjie%20Jiang%20and%20Chun%20Yuan%20and%20Jing%20Li&entry.1292438233=With%20advances%20in%20satellite%20constellations%2C%20sensor%20technologies%2C%20and%20imaging%20pipelines%2C%20ultra-high-resolution%20%28Ultra-HR%29%20remote%20sensing%20imagery%20is%20becoming%20increasingly%20widespread.%20However%2C%20current%20remote%20sensing%20foundation%20models%20are%20ill-suited%20to%20such%20inputs%3A%20full-image%20encoding%20exhausts%20token%20and%20memory%20budgets%2C%20while%20resize-based%20preprocessing%20loses%20fine-grained%20and%20answer-critical%20details.%20In%20this%20context%2C%20guiding%20the%20model%20look%20where%20it%20matters%20before%20prediction%20becomes%20crucial.%20Therefore%2C%20we%20present%20ZoomSearch%2C%20a%20training-free%2C%20plug-and-play%20pipeline%20that%20decouples%20%27where%20to%20look%27%20from%20%27how%20to%20answer%27%20for%20Ultra-HR%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RS-VQA%29.%20ZoomSearch%20combines%20Adaptive%20Multi-Branch%20Zoom%20Search%2C%20which%20performs%20a%20hierarchical%20search%20over%20image%20patches%20to%20localize%20query-relevant%20regions%2C%20with%20Layout-Aware%20Patch%20Reassembly%2C%20which%20reorganizes%20the%20selected%20patches%20into%20a%20compact%2C%20layout-faithful%20canvas.%20We%20conduct%20comprehensive%20experiments%20on%20Ultra-HR%20RS-VQA%20benchmarks%20MME-RealWorld-RS%20and%20LRS-VQA%2C%20comparing%20against%20%28i%29%20strong%20general%20foundation%20models%2C%20%28ii%29%20remote%20sensing%20foundation%20models%2C%20%28iii%29%20Ultra-HR%20RS-VQA%20methods%2C%20and%20%28iv%29%20plug-and-play%20search-based%20VQA%20methods.%20When%20integrated%20with%20LLaVA-ov%2C%20ZoomSearch%20attains%20state-of-the-art%20accuracy%20across%20diverse%20tasks%2C%20improving%20the%20LLaVA-ov%20baseline%20by%2026.3%25%20on%20LRS-VQA%20and%20114.8%25%20on%20MME-RealWorld-RS.%20Meanwhile%2C%20it%20achieves%20much%20higher%20inference%20efficiency%2C%20outperforming%20prior%20search-based%20methods%20by%2020%25~44%25%20in%20speed.&entry.1838667208=http%3A//arxiv.org/abs/2511.20460v2&entry.124074799=Read"},
{"title": "Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach", "author": "Haruki Sakajo and Hiroshi Takato and Hiroshi Tsutsui and Komei Soda and Hidetaka Kamigaito and Taro Watanabe", "abstract": "Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.", "link": "http://arxiv.org/abs/2511.23311v1", "date": "2025-11-28", "relevancy": 2.7582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Automatic%20Safe%20Driving%20Instruction%3A%20A%20Large-Scale%20Vision%20Language%20Model%20Approach&body=Title%3A%20Toward%20Automatic%20Safe%20Driving%20Instruction%3A%20A%20Large-Scale%20Vision%20Language%20Model%20Approach%0AAuthor%3A%20Haruki%20Sakajo%20and%20Hiroshi%20Takato%20and%20Hiroshi%20Tsutsui%20and%20Komei%20Soda%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe%0AAbstract%3A%20Large-scale%20Vision%20Language%20Models%20%28LVLMs%29%20exhibit%20advanced%20capabilities%20in%20tasks%20that%20require%20visual%20information%2C%20including%20object%20detection.%20These%20capabilities%20have%20promising%20applications%20in%20various%20industrial%20domains%2C%20such%20as%20autonomous%20driving.%20For%20example%2C%20LVLMs%20can%20generate%20safety-oriented%20descriptions%20of%20videos%20captured%20by%20road-facing%20cameras.%20However%2C%20ensuring%20comprehensive%20safety%20requires%20monitoring%20driver-facing%20views%20as%20well%20to%20detect%20risky%20events%2C%20such%20as%20the%20use%20of%20mobiles%20while%20driving.%20Thus%2C%20the%20ability%20to%20process%20synchronized%20inputs%20is%20necessary%20from%20both%20driver-facing%20and%20road-facing%20cameras.%20In%20this%20study%2C%20we%20develop%20models%20and%20investigate%20the%20capabilities%20of%20LVLMs%20by%20constructing%20a%20dataset%20and%20evaluating%20their%20performance%20on%20this%20dataset.%20Our%20experimental%20results%20demonstrate%20that%20while%20pre-trained%20LVLMs%20have%20limited%20effectiveness%2C%20fine-tuned%20LVLMs%20can%20generate%20accurate%20and%20safety-aware%20driving%20instructions.%20Nonetheless%2C%20several%20challenges%20remain%2C%20particularly%20in%20detecting%20subtle%20or%20complex%20events%20in%20the%20video.%20Our%20findings%20and%20error%20analysis%20provide%20valuable%20insights%20that%20can%20contribute%20to%20the%20improvement%20of%20LVLM-based%20systems%20in%20this%20domain.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Automatic%2520Safe%2520Driving%2520Instruction%253A%2520A%2520Large-Scale%2520Vision%2520Language%2520Model%2520Approach%26entry.906535625%3DHaruki%2520Sakajo%2520and%2520Hiroshi%2520Takato%2520and%2520Hiroshi%2520Tsutsui%2520and%2520Komei%2520Soda%2520and%2520Hidetaka%2520Kamigaito%2520and%2520Taro%2520Watanabe%26entry.1292438233%3DLarge-scale%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520exhibit%2520advanced%2520capabilities%2520in%2520tasks%2520that%2520require%2520visual%2520information%252C%2520including%2520object%2520detection.%2520These%2520capabilities%2520have%2520promising%2520applications%2520in%2520various%2520industrial%2520domains%252C%2520such%2520as%2520autonomous%2520driving.%2520For%2520example%252C%2520LVLMs%2520can%2520generate%2520safety-oriented%2520descriptions%2520of%2520videos%2520captured%2520by%2520road-facing%2520cameras.%2520However%252C%2520ensuring%2520comprehensive%2520safety%2520requires%2520monitoring%2520driver-facing%2520views%2520as%2520well%2520to%2520detect%2520risky%2520events%252C%2520such%2520as%2520the%2520use%2520of%2520mobiles%2520while%2520driving.%2520Thus%252C%2520the%2520ability%2520to%2520process%2520synchronized%2520inputs%2520is%2520necessary%2520from%2520both%2520driver-facing%2520and%2520road-facing%2520cameras.%2520In%2520this%2520study%252C%2520we%2520develop%2520models%2520and%2520investigate%2520the%2520capabilities%2520of%2520LVLMs%2520by%2520constructing%2520a%2520dataset%2520and%2520evaluating%2520their%2520performance%2520on%2520this%2520dataset.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520while%2520pre-trained%2520LVLMs%2520have%2520limited%2520effectiveness%252C%2520fine-tuned%2520LVLMs%2520can%2520generate%2520accurate%2520and%2520safety-aware%2520driving%2520instructions.%2520Nonetheless%252C%2520several%2520challenges%2520remain%252C%2520particularly%2520in%2520detecting%2520subtle%2520or%2520complex%2520events%2520in%2520the%2520video.%2520Our%2520findings%2520and%2520error%2520analysis%2520provide%2520valuable%2520insights%2520that%2520can%2520contribute%2520to%2520the%2520improvement%2520of%2520LVLM-based%2520systems%2520in%2520this%2520domain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Automatic%20Safe%20Driving%20Instruction%3A%20A%20Large-Scale%20Vision%20Language%20Model%20Approach&entry.906535625=Haruki%20Sakajo%20and%20Hiroshi%20Takato%20and%20Hiroshi%20Tsutsui%20and%20Komei%20Soda%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe&entry.1292438233=Large-scale%20Vision%20Language%20Models%20%28LVLMs%29%20exhibit%20advanced%20capabilities%20in%20tasks%20that%20require%20visual%20information%2C%20including%20object%20detection.%20These%20capabilities%20have%20promising%20applications%20in%20various%20industrial%20domains%2C%20such%20as%20autonomous%20driving.%20For%20example%2C%20LVLMs%20can%20generate%20safety-oriented%20descriptions%20of%20videos%20captured%20by%20road-facing%20cameras.%20However%2C%20ensuring%20comprehensive%20safety%20requires%20monitoring%20driver-facing%20views%20as%20well%20to%20detect%20risky%20events%2C%20such%20as%20the%20use%20of%20mobiles%20while%20driving.%20Thus%2C%20the%20ability%20to%20process%20synchronized%20inputs%20is%20necessary%20from%20both%20driver-facing%20and%20road-facing%20cameras.%20In%20this%20study%2C%20we%20develop%20models%20and%20investigate%20the%20capabilities%20of%20LVLMs%20by%20constructing%20a%20dataset%20and%20evaluating%20their%20performance%20on%20this%20dataset.%20Our%20experimental%20results%20demonstrate%20that%20while%20pre-trained%20LVLMs%20have%20limited%20effectiveness%2C%20fine-tuned%20LVLMs%20can%20generate%20accurate%20and%20safety-aware%20driving%20instructions.%20Nonetheless%2C%20several%20challenges%20remain%2C%20particularly%20in%20detecting%20subtle%20or%20complex%20events%20in%20the%20video.%20Our%20findings%20and%20error%20analysis%20provide%20valuable%20insights%20that%20can%20contribute%20to%20the%20improvement%20of%20LVLM-based%20systems%20in%20this%20domain.&entry.1838667208=http%3A//arxiv.org/abs/2511.23311v1&entry.124074799=Read"},
{"title": "PointCNN++: Performant Convolution on Native Points", "author": "Lihan Li and Haofeng Zhong and Rui Bu and Mingchao Sun and Wenzheng Chen and Baoquan Chen and Yangyan Li", "abstract": "Existing convolutional learning methods for 3D point cloud data are divided into two paradigms: point-based methods that preserve geometric precision but often face performance challenges, and voxel-based methods that achieve high efficiency through quantization at the cost of geometric fidelity. This loss of precision is a critical bottleneck for tasks such as point cloud registration. We propose PointCNN++, a novel architectural design that fundamentally mitigates this precision-performance trade-off. It \\textbf{generalizes sparse convolution from voxels to points}, treating voxel-based convolution as a specialized, degraded case of our more general point-based convolution. First, we introduce a point-centric convolution where the receptive field is centered on the original, high-precision point coordinates. Second, to make this high-fidelity operation performant, we design a computational strategy that operates \\textbf{natively} on points. We formulate the convolution on native points as a Matrix-Vector Multiplication and Reduction (MVMR) problem, for which we develop a dedicated, highly-optimized GPU kernel. Experiments demonstrate that PointCNN++ \\textbf{uses an order of magnitude less memory and is several times faster} than representative point-based methods. Furthermore, when used as a simple replacement for the voxel-based backbones it generalizes, it \\textbf{significantly improves point cloud registration accuracies while proving both more memory-efficient and faster}. PointCNN++ shows that preserving geometric detail and achieving high performance are not mutually exclusive, paving the way for a new class of 3D learning with high fidelity and efficiency. Our code will be open sourced.", "link": "http://arxiv.org/abs/2511.23227v1", "date": "2025-11-28", "relevancy": 2.7404, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5769}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5354}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointCNN%2B%2B%3A%20Performant%20Convolution%20on%20Native%20Points&body=Title%3A%20PointCNN%2B%2B%3A%20Performant%20Convolution%20on%20Native%20Points%0AAuthor%3A%20Lihan%20Li%20and%20Haofeng%20Zhong%20and%20Rui%20Bu%20and%20Mingchao%20Sun%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen%20and%20Yangyan%20Li%0AAbstract%3A%20Existing%20convolutional%20learning%20methods%20for%203D%20point%20cloud%20data%20are%20divided%20into%20two%20paradigms%3A%20point-based%20methods%20that%20preserve%20geometric%20precision%20but%20often%20face%20performance%20challenges%2C%20and%20voxel-based%20methods%20that%20achieve%20high%20efficiency%20through%20quantization%20at%20the%20cost%20of%20geometric%20fidelity.%20This%20loss%20of%20precision%20is%20a%20critical%20bottleneck%20for%20tasks%20such%20as%20point%20cloud%20registration.%20We%20propose%20PointCNN%2B%2B%2C%20a%20novel%20architectural%20design%20that%20fundamentally%20mitigates%20this%20precision-performance%20trade-off.%20It%20%5Ctextbf%7Bgeneralizes%20sparse%20convolution%20from%20voxels%20to%20points%7D%2C%20treating%20voxel-based%20convolution%20as%20a%20specialized%2C%20degraded%20case%20of%20our%20more%20general%20point-based%20convolution.%20First%2C%20we%20introduce%20a%20point-centric%20convolution%20where%20the%20receptive%20field%20is%20centered%20on%20the%20original%2C%20high-precision%20point%20coordinates.%20Second%2C%20to%20make%20this%20high-fidelity%20operation%20performant%2C%20we%20design%20a%20computational%20strategy%20that%20operates%20%5Ctextbf%7Bnatively%7D%20on%20points.%20We%20formulate%20the%20convolution%20on%20native%20points%20as%20a%20Matrix-Vector%20Multiplication%20and%20Reduction%20%28MVMR%29%20problem%2C%20for%20which%20we%20develop%20a%20dedicated%2C%20highly-optimized%20GPU%20kernel.%20Experiments%20demonstrate%20that%20PointCNN%2B%2B%20%5Ctextbf%7Buses%20an%20order%20of%20magnitude%20less%20memory%20and%20is%20several%20times%20faster%7D%20than%20representative%20point-based%20methods.%20Furthermore%2C%20when%20used%20as%20a%20simple%20replacement%20for%20the%20voxel-based%20backbones%20it%20generalizes%2C%20it%20%5Ctextbf%7Bsignificantly%20improves%20point%20cloud%20registration%20accuracies%20while%20proving%20both%20more%20memory-efficient%20and%20faster%7D.%20PointCNN%2B%2B%20shows%20that%20preserving%20geometric%20detail%20and%20achieving%20high%20performance%20are%20not%20mutually%20exclusive%2C%20paving%20the%20way%20for%20a%20new%20class%20of%203D%20learning%20with%20high%20fidelity%20and%20efficiency.%20Our%20code%20will%20be%20open%20sourced.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointCNN%252B%252B%253A%2520Performant%2520Convolution%2520on%2520Native%2520Points%26entry.906535625%3DLihan%2520Li%2520and%2520Haofeng%2520Zhong%2520and%2520Rui%2520Bu%2520and%2520Mingchao%2520Sun%2520and%2520Wenzheng%2520Chen%2520and%2520Baoquan%2520Chen%2520and%2520Yangyan%2520Li%26entry.1292438233%3DExisting%2520convolutional%2520learning%2520methods%2520for%25203D%2520point%2520cloud%2520data%2520are%2520divided%2520into%2520two%2520paradigms%253A%2520point-based%2520methods%2520that%2520preserve%2520geometric%2520precision%2520but%2520often%2520face%2520performance%2520challenges%252C%2520and%2520voxel-based%2520methods%2520that%2520achieve%2520high%2520efficiency%2520through%2520quantization%2520at%2520the%2520cost%2520of%2520geometric%2520fidelity.%2520This%2520loss%2520of%2520precision%2520is%2520a%2520critical%2520bottleneck%2520for%2520tasks%2520such%2520as%2520point%2520cloud%2520registration.%2520We%2520propose%2520PointCNN%252B%252B%252C%2520a%2520novel%2520architectural%2520design%2520that%2520fundamentally%2520mitigates%2520this%2520precision-performance%2520trade-off.%2520It%2520%255Ctextbf%257Bgeneralizes%2520sparse%2520convolution%2520from%2520voxels%2520to%2520points%257D%252C%2520treating%2520voxel-based%2520convolution%2520as%2520a%2520specialized%252C%2520degraded%2520case%2520of%2520our%2520more%2520general%2520point-based%2520convolution.%2520First%252C%2520we%2520introduce%2520a%2520point-centric%2520convolution%2520where%2520the%2520receptive%2520field%2520is%2520centered%2520on%2520the%2520original%252C%2520high-precision%2520point%2520coordinates.%2520Second%252C%2520to%2520make%2520this%2520high-fidelity%2520operation%2520performant%252C%2520we%2520design%2520a%2520computational%2520strategy%2520that%2520operates%2520%255Ctextbf%257Bnatively%257D%2520on%2520points.%2520We%2520formulate%2520the%2520convolution%2520on%2520native%2520points%2520as%2520a%2520Matrix-Vector%2520Multiplication%2520and%2520Reduction%2520%2528MVMR%2529%2520problem%252C%2520for%2520which%2520we%2520develop%2520a%2520dedicated%252C%2520highly-optimized%2520GPU%2520kernel.%2520Experiments%2520demonstrate%2520that%2520PointCNN%252B%252B%2520%255Ctextbf%257Buses%2520an%2520order%2520of%2520magnitude%2520less%2520memory%2520and%2520is%2520several%2520times%2520faster%257D%2520than%2520representative%2520point-based%2520methods.%2520Furthermore%252C%2520when%2520used%2520as%2520a%2520simple%2520replacement%2520for%2520the%2520voxel-based%2520backbones%2520it%2520generalizes%252C%2520it%2520%255Ctextbf%257Bsignificantly%2520improves%2520point%2520cloud%2520registration%2520accuracies%2520while%2520proving%2520both%2520more%2520memory-efficient%2520and%2520faster%257D.%2520PointCNN%252B%252B%2520shows%2520that%2520preserving%2520geometric%2520detail%2520and%2520achieving%2520high%2520performance%2520are%2520not%2520mutually%2520exclusive%252C%2520paving%2520the%2520way%2520for%2520a%2520new%2520class%2520of%25203D%2520learning%2520with%2520high%2520fidelity%2520and%2520efficiency.%2520Our%2520code%2520will%2520be%2520open%2520sourced.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointCNN%2B%2B%3A%20Performant%20Convolution%20on%20Native%20Points&entry.906535625=Lihan%20Li%20and%20Haofeng%20Zhong%20and%20Rui%20Bu%20and%20Mingchao%20Sun%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen%20and%20Yangyan%20Li&entry.1292438233=Existing%20convolutional%20learning%20methods%20for%203D%20point%20cloud%20data%20are%20divided%20into%20two%20paradigms%3A%20point-based%20methods%20that%20preserve%20geometric%20precision%20but%20often%20face%20performance%20challenges%2C%20and%20voxel-based%20methods%20that%20achieve%20high%20efficiency%20through%20quantization%20at%20the%20cost%20of%20geometric%20fidelity.%20This%20loss%20of%20precision%20is%20a%20critical%20bottleneck%20for%20tasks%20such%20as%20point%20cloud%20registration.%20We%20propose%20PointCNN%2B%2B%2C%20a%20novel%20architectural%20design%20that%20fundamentally%20mitigates%20this%20precision-performance%20trade-off.%20It%20%5Ctextbf%7Bgeneralizes%20sparse%20convolution%20from%20voxels%20to%20points%7D%2C%20treating%20voxel-based%20convolution%20as%20a%20specialized%2C%20degraded%20case%20of%20our%20more%20general%20point-based%20convolution.%20First%2C%20we%20introduce%20a%20point-centric%20convolution%20where%20the%20receptive%20field%20is%20centered%20on%20the%20original%2C%20high-precision%20point%20coordinates.%20Second%2C%20to%20make%20this%20high-fidelity%20operation%20performant%2C%20we%20design%20a%20computational%20strategy%20that%20operates%20%5Ctextbf%7Bnatively%7D%20on%20points.%20We%20formulate%20the%20convolution%20on%20native%20points%20as%20a%20Matrix-Vector%20Multiplication%20and%20Reduction%20%28MVMR%29%20problem%2C%20for%20which%20we%20develop%20a%20dedicated%2C%20highly-optimized%20GPU%20kernel.%20Experiments%20demonstrate%20that%20PointCNN%2B%2B%20%5Ctextbf%7Buses%20an%20order%20of%20magnitude%20less%20memory%20and%20is%20several%20times%20faster%7D%20than%20representative%20point-based%20methods.%20Furthermore%2C%20when%20used%20as%20a%20simple%20replacement%20for%20the%20voxel-based%20backbones%20it%20generalizes%2C%20it%20%5Ctextbf%7Bsignificantly%20improves%20point%20cloud%20registration%20accuracies%20while%20proving%20both%20more%20memory-efficient%20and%20faster%7D.%20PointCNN%2B%2B%20shows%20that%20preserving%20geometric%20detail%20and%20achieving%20high%20performance%20are%20not%20mutually%20exclusive%2C%20paving%20the%20way%20for%20a%20new%20class%20of%203D%20learning%20with%20high%20fidelity%20and%20efficiency.%20Our%20code%20will%20be%20open%20sourced.&entry.1838667208=http%3A//arxiv.org/abs/2511.23227v1&entry.124074799=Read"},
{"title": "Learning Contrastive Feature Representations for Facial Action Unit Detection", "author": "Ziqiao Shang and Bin Liu and Fengmao Lv and Fei Teng and Tianrui Li and Lan-Zhe Guo", "abstract": "For the Facial Action Unit (AU) detection task, accurately capturing the subtle facial differences between distinct AUs is essential for reliable detection. Additionally, AU detection faces challenges from class imbalance and the presence of noisy or false labels, which undermine detection accuracy. In this paper, we introduce a novel contrastive learning framework aimed for AU detection that incorporates both self-supervised and supervised signals, thereby enhancing the learning of discriminative features for accurate AU detection. To tackle the class imbalance issue, we employ a negative sample re-weighting strategy that adjusts the step size of updating parameters for minority and majority class samples. Moreover, to address the challenges posed by noisy and false AU labels, we employ a sampling technique that encompasses three distinct types of positive sample pairs. This enables us to inject self-supervised signals into the supervised signal, effectively mitigating the adverse effects of noisy labels. Our experimental assessments, conducted on five widely-utilized benchmark datasets (BP4D, DISFA, BP4D+, GFT and Aff-Wild2), underscore the superior performance of our approach compared to state-of-the-art methods of AU detection. Our code is available at https://github.com/Ziqiao-Shang/AUNCE.", "link": "http://arxiv.org/abs/2402.06165v9", "date": "2025-11-28", "relevancy": 2.7251, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5624}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5372}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Contrastive%20Feature%20Representations%20for%20Facial%20Action%20Unit%20Detection&body=Title%3A%20Learning%20Contrastive%20Feature%20Representations%20for%20Facial%20Action%20Unit%20Detection%0AAuthor%3A%20Ziqiao%20Shang%20and%20Bin%20Liu%20and%20Fengmao%20Lv%20and%20Fei%20Teng%20and%20Tianrui%20Li%20and%20Lan-Zhe%20Guo%0AAbstract%3A%20For%20the%20Facial%20Action%20Unit%20%28AU%29%20detection%20task%2C%20accurately%20capturing%20the%20subtle%20facial%20differences%20between%20distinct%20AUs%20is%20essential%20for%20reliable%20detection.%20Additionally%2C%20AU%20detection%20faces%20challenges%20from%20class%20imbalance%20and%20the%20presence%20of%20noisy%20or%20false%20labels%2C%20which%20undermine%20detection%20accuracy.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20contrastive%20learning%20framework%20aimed%20for%20AU%20detection%20that%20incorporates%20both%20self-supervised%20and%20supervised%20signals%2C%20thereby%20enhancing%20the%20learning%20of%20discriminative%20features%20for%20accurate%20AU%20detection.%20To%20tackle%20the%20class%20imbalance%20issue%2C%20we%20employ%20a%20negative%20sample%20re-weighting%20strategy%20that%20adjusts%20the%20step%20size%20of%20updating%20parameters%20for%20minority%20and%20majority%20class%20samples.%20Moreover%2C%20to%20address%20the%20challenges%20posed%20by%20noisy%20and%20false%20AU%20labels%2C%20we%20employ%20a%20sampling%20technique%20that%20encompasses%20three%20distinct%20types%20of%20positive%20sample%20pairs.%20This%20enables%20us%20to%20inject%20self-supervised%20signals%20into%20the%20supervised%20signal%2C%20effectively%20mitigating%20the%20adverse%20effects%20of%20noisy%20labels.%20Our%20experimental%20assessments%2C%20conducted%20on%20five%20widely-utilized%20benchmark%20datasets%20%28BP4D%2C%20DISFA%2C%20BP4D%2B%2C%20GFT%20and%20Aff-Wild2%29%2C%20underscore%20the%20superior%20performance%20of%20our%20approach%20compared%20to%20state-of-the-art%20methods%20of%20AU%20detection.%20Our%20code%20is%20available%20at%20https%3A//github.com/Ziqiao-Shang/AUNCE.%0ALink%3A%20http%3A//arxiv.org/abs/2402.06165v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Contrastive%2520Feature%2520Representations%2520for%2520Facial%2520Action%2520Unit%2520Detection%26entry.906535625%3DZiqiao%2520Shang%2520and%2520Bin%2520Liu%2520and%2520Fengmao%2520Lv%2520and%2520Fei%2520Teng%2520and%2520Tianrui%2520Li%2520and%2520Lan-Zhe%2520Guo%26entry.1292438233%3DFor%2520the%2520Facial%2520Action%2520Unit%2520%2528AU%2529%2520detection%2520task%252C%2520accurately%2520capturing%2520the%2520subtle%2520facial%2520differences%2520between%2520distinct%2520AUs%2520is%2520essential%2520for%2520reliable%2520detection.%2520Additionally%252C%2520AU%2520detection%2520faces%2520challenges%2520from%2520class%2520imbalance%2520and%2520the%2520presence%2520of%2520noisy%2520or%2520false%2520labels%252C%2520which%2520undermine%2520detection%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520contrastive%2520learning%2520framework%2520aimed%2520for%2520AU%2520detection%2520that%2520incorporates%2520both%2520self-supervised%2520and%2520supervised%2520signals%252C%2520thereby%2520enhancing%2520the%2520learning%2520of%2520discriminative%2520features%2520for%2520accurate%2520AU%2520detection.%2520To%2520tackle%2520the%2520class%2520imbalance%2520issue%252C%2520we%2520employ%2520a%2520negative%2520sample%2520re-weighting%2520strategy%2520that%2520adjusts%2520the%2520step%2520size%2520of%2520updating%2520parameters%2520for%2520minority%2520and%2520majority%2520class%2520samples.%2520Moreover%252C%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520noisy%2520and%2520false%2520AU%2520labels%252C%2520we%2520employ%2520a%2520sampling%2520technique%2520that%2520encompasses%2520three%2520distinct%2520types%2520of%2520positive%2520sample%2520pairs.%2520This%2520enables%2520us%2520to%2520inject%2520self-supervised%2520signals%2520into%2520the%2520supervised%2520signal%252C%2520effectively%2520mitigating%2520the%2520adverse%2520effects%2520of%2520noisy%2520labels.%2520Our%2520experimental%2520assessments%252C%2520conducted%2520on%2520five%2520widely-utilized%2520benchmark%2520datasets%2520%2528BP4D%252C%2520DISFA%252C%2520BP4D%252B%252C%2520GFT%2520and%2520Aff-Wild2%2529%252C%2520underscore%2520the%2520superior%2520performance%2520of%2520our%2520approach%2520compared%2520to%2520state-of-the-art%2520methods%2520of%2520AU%2520detection.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Ziqiao-Shang/AUNCE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06165v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Contrastive%20Feature%20Representations%20for%20Facial%20Action%20Unit%20Detection&entry.906535625=Ziqiao%20Shang%20and%20Bin%20Liu%20and%20Fengmao%20Lv%20and%20Fei%20Teng%20and%20Tianrui%20Li%20and%20Lan-Zhe%20Guo&entry.1292438233=For%20the%20Facial%20Action%20Unit%20%28AU%29%20detection%20task%2C%20accurately%20capturing%20the%20subtle%20facial%20differences%20between%20distinct%20AUs%20is%20essential%20for%20reliable%20detection.%20Additionally%2C%20AU%20detection%20faces%20challenges%20from%20class%20imbalance%20and%20the%20presence%20of%20noisy%20or%20false%20labels%2C%20which%20undermine%20detection%20accuracy.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20contrastive%20learning%20framework%20aimed%20for%20AU%20detection%20that%20incorporates%20both%20self-supervised%20and%20supervised%20signals%2C%20thereby%20enhancing%20the%20learning%20of%20discriminative%20features%20for%20accurate%20AU%20detection.%20To%20tackle%20the%20class%20imbalance%20issue%2C%20we%20employ%20a%20negative%20sample%20re-weighting%20strategy%20that%20adjusts%20the%20step%20size%20of%20updating%20parameters%20for%20minority%20and%20majority%20class%20samples.%20Moreover%2C%20to%20address%20the%20challenges%20posed%20by%20noisy%20and%20false%20AU%20labels%2C%20we%20employ%20a%20sampling%20technique%20that%20encompasses%20three%20distinct%20types%20of%20positive%20sample%20pairs.%20This%20enables%20us%20to%20inject%20self-supervised%20signals%20into%20the%20supervised%20signal%2C%20effectively%20mitigating%20the%20adverse%20effects%20of%20noisy%20labels.%20Our%20experimental%20assessments%2C%20conducted%20on%20five%20widely-utilized%20benchmark%20datasets%20%28BP4D%2C%20DISFA%2C%20BP4D%2B%2C%20GFT%20and%20Aff-Wild2%29%2C%20underscore%20the%20superior%20performance%20of%20our%20approach%20compared%20to%20state-of-the-art%20methods%20of%20AU%20detection.%20Our%20code%20is%20available%20at%20https%3A//github.com/Ziqiao-Shang/AUNCE.&entry.1838667208=http%3A//arxiv.org/abs/2402.06165v9&entry.124074799=Read"},
{"title": "FIELDS: Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision", "author": "Chen Ling and Henglin Shi and Hedvig Kjellstr\u00f6m", "abstract": "Facial expressions convey the bulk of emotional information in human communication, yet existing 3D face reconstruction methods often miss subtle affective details due to reliance on 2D supervision and lack of 3D ground truth. We propose FIELDS (Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision) to address these limitations by extending self-supervised 2D image consistency cues with direct 3D expression parameter supervision and an auxiliary emotion recognition branch. Our encoder is guided by authentic expression parameters from spontaneous 4D facial scans, while an intensity-aware emotion loss encourages the 3D expression parameters to capture genuine emotion content without exaggeration. This dual-supervision strategy bridges the 2D/3D domain gap and mitigates expression-intensity bias, yielding high-fidelity 3D reconstructions that preserve subtle emotional cues. From a single image, FIELDS produces emotion-rich face models with highly realistic expressions, significantly improving in-the-wild facial expression recognition performance without sacrificing naturalness.", "link": "http://arxiv.org/abs/2511.21245v2", "date": "2025-11-28", "relevancy": 2.7146, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FIELDS%3A%20Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision&body=Title%3A%20FIELDS%3A%20Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision%0AAuthor%3A%20Chen%20Ling%20and%20Henglin%20Shi%20and%20Hedvig%20Kjellstr%C3%B6m%0AAbstract%3A%20Facial%20expressions%20convey%20the%20bulk%20of%20emotional%20information%20in%20human%20communication%2C%20yet%20existing%203D%20face%20reconstruction%20methods%20often%20miss%20subtle%20affective%20details%20due%20to%20reliance%20on%202D%20supervision%20and%20lack%20of%203D%20ground%20truth.%20We%20propose%20FIELDS%20%28Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision%29%20to%20address%20these%20limitations%20by%20extending%20self-supervised%202D%20image%20consistency%20cues%20with%20direct%203D%20expression%20parameter%20supervision%20and%20an%20auxiliary%20emotion%20recognition%20branch.%20Our%20encoder%20is%20guided%20by%20authentic%20expression%20parameters%20from%20spontaneous%204D%20facial%20scans%2C%20while%20an%20intensity-aware%20emotion%20loss%20encourages%20the%203D%20expression%20parameters%20to%20capture%20genuine%20emotion%20content%20without%20exaggeration.%20This%20dual-supervision%20strategy%20bridges%20the%202D/3D%20domain%20gap%20and%20mitigates%20expression-intensity%20bias%2C%20yielding%20high-fidelity%203D%20reconstructions%20that%20preserve%20subtle%20emotional%20cues.%20From%20a%20single%20image%2C%20FIELDS%20produces%20emotion-rich%20face%20models%20with%20highly%20realistic%20expressions%2C%20significantly%20improving%20in-the-wild%20facial%20expression%20recognition%20performance%20without%20sacrificing%20naturalness.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21245v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFIELDS%253A%2520Face%2520reconstruction%2520with%2520accurate%2520Inference%2520of%2520Expression%2520using%2520Learning%2520with%2520Direct%2520Supervision%26entry.906535625%3DChen%2520Ling%2520and%2520Henglin%2520Shi%2520and%2520Hedvig%2520Kjellstr%25C3%25B6m%26entry.1292438233%3DFacial%2520expressions%2520convey%2520the%2520bulk%2520of%2520emotional%2520information%2520in%2520human%2520communication%252C%2520yet%2520existing%25203D%2520face%2520reconstruction%2520methods%2520often%2520miss%2520subtle%2520affective%2520details%2520due%2520to%2520reliance%2520on%25202D%2520supervision%2520and%2520lack%2520of%25203D%2520ground%2520truth.%2520We%2520propose%2520FIELDS%2520%2528Face%2520reconstruction%2520with%2520accurate%2520Inference%2520of%2520Expression%2520using%2520Learning%2520with%2520Direct%2520Supervision%2529%2520to%2520address%2520these%2520limitations%2520by%2520extending%2520self-supervised%25202D%2520image%2520consistency%2520cues%2520with%2520direct%25203D%2520expression%2520parameter%2520supervision%2520and%2520an%2520auxiliary%2520emotion%2520recognition%2520branch.%2520Our%2520encoder%2520is%2520guided%2520by%2520authentic%2520expression%2520parameters%2520from%2520spontaneous%25204D%2520facial%2520scans%252C%2520while%2520an%2520intensity-aware%2520emotion%2520loss%2520encourages%2520the%25203D%2520expression%2520parameters%2520to%2520capture%2520genuine%2520emotion%2520content%2520without%2520exaggeration.%2520This%2520dual-supervision%2520strategy%2520bridges%2520the%25202D/3D%2520domain%2520gap%2520and%2520mitigates%2520expression-intensity%2520bias%252C%2520yielding%2520high-fidelity%25203D%2520reconstructions%2520that%2520preserve%2520subtle%2520emotional%2520cues.%2520From%2520a%2520single%2520image%252C%2520FIELDS%2520produces%2520emotion-rich%2520face%2520models%2520with%2520highly%2520realistic%2520expressions%252C%2520significantly%2520improving%2520in-the-wild%2520facial%2520expression%2520recognition%2520performance%2520without%2520sacrificing%2520naturalness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21245v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIELDS%3A%20Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision&entry.906535625=Chen%20Ling%20and%20Henglin%20Shi%20and%20Hedvig%20Kjellstr%C3%B6m&entry.1292438233=Facial%20expressions%20convey%20the%20bulk%20of%20emotional%20information%20in%20human%20communication%2C%20yet%20existing%203D%20face%20reconstruction%20methods%20often%20miss%20subtle%20affective%20details%20due%20to%20reliance%20on%202D%20supervision%20and%20lack%20of%203D%20ground%20truth.%20We%20propose%20FIELDS%20%28Face%20reconstruction%20with%20accurate%20Inference%20of%20Expression%20using%20Learning%20with%20Direct%20Supervision%29%20to%20address%20these%20limitations%20by%20extending%20self-supervised%202D%20image%20consistency%20cues%20with%20direct%203D%20expression%20parameter%20supervision%20and%20an%20auxiliary%20emotion%20recognition%20branch.%20Our%20encoder%20is%20guided%20by%20authentic%20expression%20parameters%20from%20spontaneous%204D%20facial%20scans%2C%20while%20an%20intensity-aware%20emotion%20loss%20encourages%20the%203D%20expression%20parameters%20to%20capture%20genuine%20emotion%20content%20without%20exaggeration.%20This%20dual-supervision%20strategy%20bridges%20the%202D/3D%20domain%20gap%20and%20mitigates%20expression-intensity%20bias%2C%20yielding%20high-fidelity%203D%20reconstructions%20that%20preserve%20subtle%20emotional%20cues.%20From%20a%20single%20image%2C%20FIELDS%20produces%20emotion-rich%20face%20models%20with%20highly%20realistic%20expressions%2C%20significantly%20improving%20in-the-wild%20facial%20expression%20recognition%20performance%20without%20sacrificing%20naturalness.&entry.1838667208=http%3A//arxiv.org/abs/2511.21245v2&entry.124074799=Read"},
{"title": "Image Valuation in NeRF-based 3D reconstruction", "author": "Grigorios Aris Cheimariotis and Antonis Karakottas and Vangelis Chatzis and Angelos Kanlis and Dimitrios Zarpalas", "abstract": "Data valuation and monetization are becoming increasingly important across domains such as eXtended Reality (XR) and digital media. In the context of 3D scene reconstruction from a set of images -- whether casually or professionally captured -- not all inputs contribute equally to the final output. Neural Radiance Fields (NeRFs) enable photorealistic 3D reconstruction of scenes by optimizing a volumetric radiance field given a set of images. However, in-the-wild scenes often include image captures of varying quality, occlusions, and transient objects, resulting in uneven utility across inputs. In this paper we propose a method to quantify the individual contribution of each image to NeRF-based reconstructions of in-the-wild image sets. Contribution is assessed through reconstruction quality metrics based on PSNR and MSE. We validate our approach by removing low-contributing images during training and measuring the resulting impact on reconstruction fidelity.", "link": "http://arxiv.org/abs/2511.23052v1", "date": "2025-11-28", "relevancy": 2.7023, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Valuation%20in%20NeRF-based%203D%20reconstruction&body=Title%3A%20Image%20Valuation%20in%20NeRF-based%203D%20reconstruction%0AAuthor%3A%20Grigorios%20Aris%20Cheimariotis%20and%20Antonis%20Karakottas%20and%20Vangelis%20Chatzis%20and%20Angelos%20Kanlis%20and%20Dimitrios%20Zarpalas%0AAbstract%3A%20Data%20valuation%20and%20monetization%20are%20becoming%20increasingly%20important%20across%20domains%20such%20as%20eXtended%20Reality%20%28XR%29%20and%20digital%20media.%20In%20the%20context%20of%203D%20scene%20reconstruction%20from%20a%20set%20of%20images%20--%20whether%20casually%20or%20professionally%20captured%20--%20not%20all%20inputs%20contribute%20equally%20to%20the%20final%20output.%20Neural%20Radiance%20Fields%20%28NeRFs%29%20enable%20photorealistic%203D%20reconstruction%20of%20scenes%20by%20optimizing%20a%20volumetric%20radiance%20field%20given%20a%20set%20of%20images.%20However%2C%20in-the-wild%20scenes%20often%20include%20image%20captures%20of%20varying%20quality%2C%20occlusions%2C%20and%20transient%20objects%2C%20resulting%20in%20uneven%20utility%20across%20inputs.%20In%20this%20paper%20we%20propose%20a%20method%20to%20quantify%20the%20individual%20contribution%20of%20each%20image%20to%20NeRF-based%20reconstructions%20of%20in-the-wild%20image%20sets.%20Contribution%20is%20assessed%20through%20reconstruction%20quality%20metrics%20based%20on%20PSNR%20and%20MSE.%20We%20validate%20our%20approach%20by%20removing%20low-contributing%20images%20during%20training%20and%20measuring%20the%20resulting%20impact%20on%20reconstruction%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Valuation%2520in%2520NeRF-based%25203D%2520reconstruction%26entry.906535625%3DGrigorios%2520Aris%2520Cheimariotis%2520and%2520Antonis%2520Karakottas%2520and%2520Vangelis%2520Chatzis%2520and%2520Angelos%2520Kanlis%2520and%2520Dimitrios%2520Zarpalas%26entry.1292438233%3DData%2520valuation%2520and%2520monetization%2520are%2520becoming%2520increasingly%2520important%2520across%2520domains%2520such%2520as%2520eXtended%2520Reality%2520%2528XR%2529%2520and%2520digital%2520media.%2520In%2520the%2520context%2520of%25203D%2520scene%2520reconstruction%2520from%2520a%2520set%2520of%2520images%2520--%2520whether%2520casually%2520or%2520professionally%2520captured%2520--%2520not%2520all%2520inputs%2520contribute%2520equally%2520to%2520the%2520final%2520output.%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520enable%2520photorealistic%25203D%2520reconstruction%2520of%2520scenes%2520by%2520optimizing%2520a%2520volumetric%2520radiance%2520field%2520given%2520a%2520set%2520of%2520images.%2520However%252C%2520in-the-wild%2520scenes%2520often%2520include%2520image%2520captures%2520of%2520varying%2520quality%252C%2520occlusions%252C%2520and%2520transient%2520objects%252C%2520resulting%2520in%2520uneven%2520utility%2520across%2520inputs.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520method%2520to%2520quantify%2520the%2520individual%2520contribution%2520of%2520each%2520image%2520to%2520NeRF-based%2520reconstructions%2520of%2520in-the-wild%2520image%2520sets.%2520Contribution%2520is%2520assessed%2520through%2520reconstruction%2520quality%2520metrics%2520based%2520on%2520PSNR%2520and%2520MSE.%2520We%2520validate%2520our%2520approach%2520by%2520removing%2520low-contributing%2520images%2520during%2520training%2520and%2520measuring%2520the%2520resulting%2520impact%2520on%2520reconstruction%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Valuation%20in%20NeRF-based%203D%20reconstruction&entry.906535625=Grigorios%20Aris%20Cheimariotis%20and%20Antonis%20Karakottas%20and%20Vangelis%20Chatzis%20and%20Angelos%20Kanlis%20and%20Dimitrios%20Zarpalas&entry.1292438233=Data%20valuation%20and%20monetization%20are%20becoming%20increasingly%20important%20across%20domains%20such%20as%20eXtended%20Reality%20%28XR%29%20and%20digital%20media.%20In%20the%20context%20of%203D%20scene%20reconstruction%20from%20a%20set%20of%20images%20--%20whether%20casually%20or%20professionally%20captured%20--%20not%20all%20inputs%20contribute%20equally%20to%20the%20final%20output.%20Neural%20Radiance%20Fields%20%28NeRFs%29%20enable%20photorealistic%203D%20reconstruction%20of%20scenes%20by%20optimizing%20a%20volumetric%20radiance%20field%20given%20a%20set%20of%20images.%20However%2C%20in-the-wild%20scenes%20often%20include%20image%20captures%20of%20varying%20quality%2C%20occlusions%2C%20and%20transient%20objects%2C%20resulting%20in%20uneven%20utility%20across%20inputs.%20In%20this%20paper%20we%20propose%20a%20method%20to%20quantify%20the%20individual%20contribution%20of%20each%20image%20to%20NeRF-based%20reconstructions%20of%20in-the-wild%20image%20sets.%20Contribution%20is%20assessed%20through%20reconstruction%20quality%20metrics%20based%20on%20PSNR%20and%20MSE.%20We%20validate%20our%20approach%20by%20removing%20low-contributing%20images%20during%20training%20and%20measuring%20the%20resulting%20impact%20on%20reconstruction%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2511.23052v1&entry.124074799=Read"},
{"title": "PoseAdapt: Sustainable Human Pose Estimation via Continual Learning Benchmarks and Toolkit", "author": "Muhammad Saif Ullah Khan and Didier Stricker", "abstract": "Human pose estimators are typically retrained from scratch or naively fine-tuned whenever keypoint sets, sensing modalities, or deployment domains change--an inefficient, compute-intensive practice that rarely matches field constraints. We present PoseAdapt, an open-source framework and benchmark suite for continual pose model adaptation. PoseAdapt defines domain-incremental and class-incremental tracks that simulate realistic changes in density, lighting, and sensing modality, as well as skeleton growth. The toolkit supports two workflows: (i) Strategy Benchmarking, which lets researchers implement continual learning (CL) methods as plugins and evaluate them under standardized protocols; and (ii) Model Adaptation, which allows practitioners to adapt strong pretrained models to new tasks with minimal supervision. We evaluate representative regularization-based methods in single-step and sequential settings. Benchmarks enforce a fixed lightweight backbone, no access to past data, and tight per-step budgets. This isolates adaptation strategy effects, highlighting the difficulty of maintaining accuracy under strict resource limits. PoseAdapt connects modern CL techniques with practical pose estimation needs, enabling adaptable models that improve over time without repeated full retraining.", "link": "http://arxiv.org/abs/2409.20469v2", "date": "2025-11-28", "relevancy": 2.6946, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5478}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseAdapt%3A%20Sustainable%20Human%20Pose%20Estimation%20via%20Continual%20Learning%20Benchmarks%20and%20Toolkit&body=Title%3A%20PoseAdapt%3A%20Sustainable%20Human%20Pose%20Estimation%20via%20Continual%20Learning%20Benchmarks%20and%20Toolkit%0AAuthor%3A%20Muhammad%20Saif%20Ullah%20Khan%20and%20Didier%20Stricker%0AAbstract%3A%20Human%20pose%20estimators%20are%20typically%20retrained%20from%20scratch%20or%20naively%20fine-tuned%20whenever%20keypoint%20sets%2C%20sensing%20modalities%2C%20or%20deployment%20domains%20change--an%20inefficient%2C%20compute-intensive%20practice%20that%20rarely%20matches%20field%20constraints.%20We%20present%20PoseAdapt%2C%20an%20open-source%20framework%20and%20benchmark%20suite%20for%20continual%20pose%20model%20adaptation.%20PoseAdapt%20defines%20domain-incremental%20and%20class-incremental%20tracks%20that%20simulate%20realistic%20changes%20in%20density%2C%20lighting%2C%20and%20sensing%20modality%2C%20as%20well%20as%20skeleton%20growth.%20The%20toolkit%20supports%20two%20workflows%3A%20%28i%29%20Strategy%20Benchmarking%2C%20which%20lets%20researchers%20implement%20continual%20learning%20%28CL%29%20methods%20as%20plugins%20and%20evaluate%20them%20under%20standardized%20protocols%3B%20and%20%28ii%29%20Model%20Adaptation%2C%20which%20allows%20practitioners%20to%20adapt%20strong%20pretrained%20models%20to%20new%20tasks%20with%20minimal%20supervision.%20We%20evaluate%20representative%20regularization-based%20methods%20in%20single-step%20and%20sequential%20settings.%20Benchmarks%20enforce%20a%20fixed%20lightweight%20backbone%2C%20no%20access%20to%20past%20data%2C%20and%20tight%20per-step%20budgets.%20This%20isolates%20adaptation%20strategy%20effects%2C%20highlighting%20the%20difficulty%20of%20maintaining%20accuracy%20under%20strict%20resource%20limits.%20PoseAdapt%20connects%20modern%20CL%20techniques%20with%20practical%20pose%20estimation%20needs%2C%20enabling%20adaptable%20models%20that%20improve%20over%20time%20without%20repeated%20full%20retraining.%0ALink%3A%20http%3A//arxiv.org/abs/2409.20469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseAdapt%253A%2520Sustainable%2520Human%2520Pose%2520Estimation%2520via%2520Continual%2520Learning%2520Benchmarks%2520and%2520Toolkit%26entry.906535625%3DMuhammad%2520Saif%2520Ullah%2520Khan%2520and%2520Didier%2520Stricker%26entry.1292438233%3DHuman%2520pose%2520estimators%2520are%2520typically%2520retrained%2520from%2520scratch%2520or%2520naively%2520fine-tuned%2520whenever%2520keypoint%2520sets%252C%2520sensing%2520modalities%252C%2520or%2520deployment%2520domains%2520change--an%2520inefficient%252C%2520compute-intensive%2520practice%2520that%2520rarely%2520matches%2520field%2520constraints.%2520We%2520present%2520PoseAdapt%252C%2520an%2520open-source%2520framework%2520and%2520benchmark%2520suite%2520for%2520continual%2520pose%2520model%2520adaptation.%2520PoseAdapt%2520defines%2520domain-incremental%2520and%2520class-incremental%2520tracks%2520that%2520simulate%2520realistic%2520changes%2520in%2520density%252C%2520lighting%252C%2520and%2520sensing%2520modality%252C%2520as%2520well%2520as%2520skeleton%2520growth.%2520The%2520toolkit%2520supports%2520two%2520workflows%253A%2520%2528i%2529%2520Strategy%2520Benchmarking%252C%2520which%2520lets%2520researchers%2520implement%2520continual%2520learning%2520%2528CL%2529%2520methods%2520as%2520plugins%2520and%2520evaluate%2520them%2520under%2520standardized%2520protocols%253B%2520and%2520%2528ii%2529%2520Model%2520Adaptation%252C%2520which%2520allows%2520practitioners%2520to%2520adapt%2520strong%2520pretrained%2520models%2520to%2520new%2520tasks%2520with%2520minimal%2520supervision.%2520We%2520evaluate%2520representative%2520regularization-based%2520methods%2520in%2520single-step%2520and%2520sequential%2520settings.%2520Benchmarks%2520enforce%2520a%2520fixed%2520lightweight%2520backbone%252C%2520no%2520access%2520to%2520past%2520data%252C%2520and%2520tight%2520per-step%2520budgets.%2520This%2520isolates%2520adaptation%2520strategy%2520effects%252C%2520highlighting%2520the%2520difficulty%2520of%2520maintaining%2520accuracy%2520under%2520strict%2520resource%2520limits.%2520PoseAdapt%2520connects%2520modern%2520CL%2520techniques%2520with%2520practical%2520pose%2520estimation%2520needs%252C%2520enabling%2520adaptable%2520models%2520that%2520improve%2520over%2520time%2520without%2520repeated%2520full%2520retraining.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseAdapt%3A%20Sustainable%20Human%20Pose%20Estimation%20via%20Continual%20Learning%20Benchmarks%20and%20Toolkit&entry.906535625=Muhammad%20Saif%20Ullah%20Khan%20and%20Didier%20Stricker&entry.1292438233=Human%20pose%20estimators%20are%20typically%20retrained%20from%20scratch%20or%20naively%20fine-tuned%20whenever%20keypoint%20sets%2C%20sensing%20modalities%2C%20or%20deployment%20domains%20change--an%20inefficient%2C%20compute-intensive%20practice%20that%20rarely%20matches%20field%20constraints.%20We%20present%20PoseAdapt%2C%20an%20open-source%20framework%20and%20benchmark%20suite%20for%20continual%20pose%20model%20adaptation.%20PoseAdapt%20defines%20domain-incremental%20and%20class-incremental%20tracks%20that%20simulate%20realistic%20changes%20in%20density%2C%20lighting%2C%20and%20sensing%20modality%2C%20as%20well%20as%20skeleton%20growth.%20The%20toolkit%20supports%20two%20workflows%3A%20%28i%29%20Strategy%20Benchmarking%2C%20which%20lets%20researchers%20implement%20continual%20learning%20%28CL%29%20methods%20as%20plugins%20and%20evaluate%20them%20under%20standardized%20protocols%3B%20and%20%28ii%29%20Model%20Adaptation%2C%20which%20allows%20practitioners%20to%20adapt%20strong%20pretrained%20models%20to%20new%20tasks%20with%20minimal%20supervision.%20We%20evaluate%20representative%20regularization-based%20methods%20in%20single-step%20and%20sequential%20settings.%20Benchmarks%20enforce%20a%20fixed%20lightweight%20backbone%2C%20no%20access%20to%20past%20data%2C%20and%20tight%20per-step%20budgets.%20This%20isolates%20adaptation%20strategy%20effects%2C%20highlighting%20the%20difficulty%20of%20maintaining%20accuracy%20under%20strict%20resource%20limits.%20PoseAdapt%20connects%20modern%20CL%20techniques%20with%20practical%20pose%20estimation%20needs%2C%20enabling%20adaptable%20models%20that%20improve%20over%20time%20without%20repeated%20full%20retraining.&entry.1838667208=http%3A//arxiv.org/abs/2409.20469v2&entry.124074799=Read"},
{"title": "UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes", "author": "Shuo Ni and Di Wang and He Chen and Haonan Guo and Ning Zhang and Jing Zhang", "abstract": "Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.", "link": "http://arxiv.org/abs/2511.23332v1", "date": "2025-11-28", "relevancy": 2.6736, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGeoSeg%3A%20Towards%20Unified%20Open-World%20Segmentation%20for%20Geospatial%20Scenes&body=Title%3A%20UniGeoSeg%3A%20Towards%20Unified%20Open-World%20Segmentation%20for%20Geospatial%20Scenes%0AAuthor%3A%20Shuo%20Ni%20and%20Di%20Wang%20and%20He%20Chen%20and%20Haonan%20Guo%20and%20Ning%20Zhang%20and%20Jing%20Zhang%0AAbstract%3A%20Instruction-driven%20segmentation%20in%20remote%20sensing%20generates%20masks%20from%20guidance%2C%20offering%20great%20potential%20for%20accessible%20and%20generalizable%20applications.%20However%2C%20existing%20methods%20suffer%20from%20fragmented%20task%20formulations%20and%20limited%20instruction%20data%2C%20hindering%20effective%20understanding%20and%20generalization.%20To%20address%20these%20issues%2C%20we%20introduce%20GeoSeg-1M%2C%20the%20first%20million-scale%20dataset%20for%20remote%20sensing%20instruction-driven%20segmentation%2C%20constructed%20via%20an%20automatic%20mask%20filtering%20and%20instruction%20generation%20pipeline%20that%20synthesizes%20referring%2C%20interactive%2C%20and%20reasoning%20segmentation%20instructions%20from%20multiple%20public%20datasets.%20GeoSeg-1M%20contains%20590K%20images%2C%20117%20categories%2C%20and%201.1M%20image-mask-instruction%20triplets.%20Building%20upon%20this%20foundation%2C%20we%20further%20curate%20GeoSeg-Bench%2C%20a%20challenging%20benchmark%20designed%20to%20evaluate%20contextual%20understanding%20and%20reasoning%20capabilities%20across%20diverse%20instruction-driven%20tasks%20and%20complex%20geospatial%20scenes.%20Furthermore%2C%20we%20present%20UniGeoSeg%2C%20a%20unified%20framework%20that%20serves%20as%20a%20strong%20baseline%2C%20incorporating%20task-aware%20text%20enhancement%2C%20latent%20knowledge%20memory%2C%20and%20a%20progressive%20training%20strategy%20to%20facilitate%20multi-task%20learning.%20Extensive%20experiments%20demonstrate%20the%20state-of-the-art%20performance%20of%20UniGeoSeg%20across%20GeoSeg-Bench%20and%20diverse%20public%20benchmarks%2C%20while%20exhibiting%20strong%20zero-shot%20generalization.%20Datasets%20and%20source%20code%20were%20released%20at%20https%3A//github.com/MiliLab/UniGeoSeg.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGeoSeg%253A%2520Towards%2520Unified%2520Open-World%2520Segmentation%2520for%2520Geospatial%2520Scenes%26entry.906535625%3DShuo%2520Ni%2520and%2520Di%2520Wang%2520and%2520He%2520Chen%2520and%2520Haonan%2520Guo%2520and%2520Ning%2520Zhang%2520and%2520Jing%2520Zhang%26entry.1292438233%3DInstruction-driven%2520segmentation%2520in%2520remote%2520sensing%2520generates%2520masks%2520from%2520guidance%252C%2520offering%2520great%2520potential%2520for%2520accessible%2520and%2520generalizable%2520applications.%2520However%252C%2520existing%2520methods%2520suffer%2520from%2520fragmented%2520task%2520formulations%2520and%2520limited%2520instruction%2520data%252C%2520hindering%2520effective%2520understanding%2520and%2520generalization.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520GeoSeg-1M%252C%2520the%2520first%2520million-scale%2520dataset%2520for%2520remote%2520sensing%2520instruction-driven%2520segmentation%252C%2520constructed%2520via%2520an%2520automatic%2520mask%2520filtering%2520and%2520instruction%2520generation%2520pipeline%2520that%2520synthesizes%2520referring%252C%2520interactive%252C%2520and%2520reasoning%2520segmentation%2520instructions%2520from%2520multiple%2520public%2520datasets.%2520GeoSeg-1M%2520contains%2520590K%2520images%252C%2520117%2520categories%252C%2520and%25201.1M%2520image-mask-instruction%2520triplets.%2520Building%2520upon%2520this%2520foundation%252C%2520we%2520further%2520curate%2520GeoSeg-Bench%252C%2520a%2520challenging%2520benchmark%2520designed%2520to%2520evaluate%2520contextual%2520understanding%2520and%2520reasoning%2520capabilities%2520across%2520diverse%2520instruction-driven%2520tasks%2520and%2520complex%2520geospatial%2520scenes.%2520Furthermore%252C%2520we%2520present%2520UniGeoSeg%252C%2520a%2520unified%2520framework%2520that%2520serves%2520as%2520a%2520strong%2520baseline%252C%2520incorporating%2520task-aware%2520text%2520enhancement%252C%2520latent%2520knowledge%2520memory%252C%2520and%2520a%2520progressive%2520training%2520strategy%2520to%2520facilitate%2520multi-task%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520UniGeoSeg%2520across%2520GeoSeg-Bench%2520and%2520diverse%2520public%2520benchmarks%252C%2520while%2520exhibiting%2520strong%2520zero-shot%2520generalization.%2520Datasets%2520and%2520source%2520code%2520were%2520released%2520at%2520https%253A//github.com/MiliLab/UniGeoSeg.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGeoSeg%3A%20Towards%20Unified%20Open-World%20Segmentation%20for%20Geospatial%20Scenes&entry.906535625=Shuo%20Ni%20and%20Di%20Wang%20and%20He%20Chen%20and%20Haonan%20Guo%20and%20Ning%20Zhang%20and%20Jing%20Zhang&entry.1292438233=Instruction-driven%20segmentation%20in%20remote%20sensing%20generates%20masks%20from%20guidance%2C%20offering%20great%20potential%20for%20accessible%20and%20generalizable%20applications.%20However%2C%20existing%20methods%20suffer%20from%20fragmented%20task%20formulations%20and%20limited%20instruction%20data%2C%20hindering%20effective%20understanding%20and%20generalization.%20To%20address%20these%20issues%2C%20we%20introduce%20GeoSeg-1M%2C%20the%20first%20million-scale%20dataset%20for%20remote%20sensing%20instruction-driven%20segmentation%2C%20constructed%20via%20an%20automatic%20mask%20filtering%20and%20instruction%20generation%20pipeline%20that%20synthesizes%20referring%2C%20interactive%2C%20and%20reasoning%20segmentation%20instructions%20from%20multiple%20public%20datasets.%20GeoSeg-1M%20contains%20590K%20images%2C%20117%20categories%2C%20and%201.1M%20image-mask-instruction%20triplets.%20Building%20upon%20this%20foundation%2C%20we%20further%20curate%20GeoSeg-Bench%2C%20a%20challenging%20benchmark%20designed%20to%20evaluate%20contextual%20understanding%20and%20reasoning%20capabilities%20across%20diverse%20instruction-driven%20tasks%20and%20complex%20geospatial%20scenes.%20Furthermore%2C%20we%20present%20UniGeoSeg%2C%20a%20unified%20framework%20that%20serves%20as%20a%20strong%20baseline%2C%20incorporating%20task-aware%20text%20enhancement%2C%20latent%20knowledge%20memory%2C%20and%20a%20progressive%20training%20strategy%20to%20facilitate%20multi-task%20learning.%20Extensive%20experiments%20demonstrate%20the%20state-of-the-art%20performance%20of%20UniGeoSeg%20across%20GeoSeg-Bench%20and%20diverse%20public%20benchmarks%2C%20while%20exhibiting%20strong%20zero-shot%20generalization.%20Datasets%20and%20source%20code%20were%20released%20at%20https%3A//github.com/MiliLab/UniGeoSeg.&entry.1838667208=http%3A//arxiv.org/abs/2511.23332v1&entry.124074799=Read"},
{"title": "PowerCLIP: Powerset Alignment for Contrastive Pre-Training", "author": "Masaki Kawamura and Nakamasa Inoue and Rintaro Yanagi and Hirokatsu Kataoka and Rio Yokota", "abstract": "Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.", "link": "http://arxiv.org/abs/2511.23170v1", "date": "2025-11-28", "relevancy": 2.6563, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5735}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PowerCLIP%3A%20Powerset%20Alignment%20for%20Contrastive%20Pre-Training&body=Title%3A%20PowerCLIP%3A%20Powerset%20Alignment%20for%20Contrastive%20Pre-Training%0AAuthor%3A%20Masaki%20Kawamura%20and%20Nakamasa%20Inoue%20and%20Rintaro%20Yanagi%20and%20Hirokatsu%20Kataoka%20and%20Rio%20Yokota%0AAbstract%3A%20Contrastive%20vision-language%20pre-training%20frameworks%20such%20as%20CLIP%20have%20demonstrated%20impressive%20zero-shot%20performance%20across%20a%20range%20of%20vision-language%20tasks.%20Recent%20studies%20have%20shown%20that%20aligning%20individual%20text%20tokens%20with%20specific%20image%20patches%20or%20regions%20enhances%20fine-grained%20compositional%20understanding.%20However%2C%20it%20remains%20challenging%20to%20capture%20compositional%20semantics%20that%20span%20multiple%20image%20regions.%20To%20address%20this%20limitation%2C%20we%20propose%20PowerCLIP%2C%20a%20novel%20contrastive%20pre-training%20framework%20enhanced%20by%20powerset%20alignment%2C%20which%20exhaustively%20optimizes%20region-to-phrase%20alignments%20by%20minimizing%20the%20loss%20defined%20between%20powersets%20of%20image%20regions%20and%20textual%20parse%20trees.%20Since%20the%20naive%20powerset%20construction%20incurs%20exponential%20computational%20cost%20due%20to%20the%20combinatorial%20explosion%20in%20the%20number%20of%20region%20subsets%2C%20we%20introduce%20efficient%20non-linear%20aggregators%20%28NLAs%29%20that%20reduce%20complexity%20from%20O%282%5EM%29%20to%20O%28M%29%20with%20respect%20to%20the%20number%20of%20regions%20M%2C%20while%20approximating%20the%20exact%20loss%20value%20with%20arbitrary%20precision.%20Our%20extensive%20experiments%20demonstrate%20that%20PowerCLIP%20outperforms%20state-of-the-art%20methods%20in%20zero-shot%20classification%20and%20retrieval%20tasks%2C%20underscoring%20the%20compositionality%20and%20robustness%20of%20our%20approach.%20Our%20code%20will%20be%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPowerCLIP%253A%2520Powerset%2520Alignment%2520for%2520Contrastive%2520Pre-Training%26entry.906535625%3DMasaki%2520Kawamura%2520and%2520Nakamasa%2520Inoue%2520and%2520Rintaro%2520Yanagi%2520and%2520Hirokatsu%2520Kataoka%2520and%2520Rio%2520Yokota%26entry.1292438233%3DContrastive%2520vision-language%2520pre-training%2520frameworks%2520such%2520as%2520CLIP%2520have%2520demonstrated%2520impressive%2520zero-shot%2520performance%2520across%2520a%2520range%2520of%2520vision-language%2520tasks.%2520Recent%2520studies%2520have%2520shown%2520that%2520aligning%2520individual%2520text%2520tokens%2520with%2520specific%2520image%2520patches%2520or%2520regions%2520enhances%2520fine-grained%2520compositional%2520understanding.%2520However%252C%2520it%2520remains%2520challenging%2520to%2520capture%2520compositional%2520semantics%2520that%2520span%2520multiple%2520image%2520regions.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520PowerCLIP%252C%2520a%2520novel%2520contrastive%2520pre-training%2520framework%2520enhanced%2520by%2520powerset%2520alignment%252C%2520which%2520exhaustively%2520optimizes%2520region-to-phrase%2520alignments%2520by%2520minimizing%2520the%2520loss%2520defined%2520between%2520powersets%2520of%2520image%2520regions%2520and%2520textual%2520parse%2520trees.%2520Since%2520the%2520naive%2520powerset%2520construction%2520incurs%2520exponential%2520computational%2520cost%2520due%2520to%2520the%2520combinatorial%2520explosion%2520in%2520the%2520number%2520of%2520region%2520subsets%252C%2520we%2520introduce%2520efficient%2520non-linear%2520aggregators%2520%2528NLAs%2529%2520that%2520reduce%2520complexity%2520from%2520O%25282%255EM%2529%2520to%2520O%2528M%2529%2520with%2520respect%2520to%2520the%2520number%2520of%2520regions%2520M%252C%2520while%2520approximating%2520the%2520exact%2520loss%2520value%2520with%2520arbitrary%2520precision.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520PowerCLIP%2520outperforms%2520state-of-the-art%2520methods%2520in%2520zero-shot%2520classification%2520and%2520retrieval%2520tasks%252C%2520underscoring%2520the%2520compositionality%2520and%2520robustness%2520of%2520our%2520approach.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PowerCLIP%3A%20Powerset%20Alignment%20for%20Contrastive%20Pre-Training&entry.906535625=Masaki%20Kawamura%20and%20Nakamasa%20Inoue%20and%20Rintaro%20Yanagi%20and%20Hirokatsu%20Kataoka%20and%20Rio%20Yokota&entry.1292438233=Contrastive%20vision-language%20pre-training%20frameworks%20such%20as%20CLIP%20have%20demonstrated%20impressive%20zero-shot%20performance%20across%20a%20range%20of%20vision-language%20tasks.%20Recent%20studies%20have%20shown%20that%20aligning%20individual%20text%20tokens%20with%20specific%20image%20patches%20or%20regions%20enhances%20fine-grained%20compositional%20understanding.%20However%2C%20it%20remains%20challenging%20to%20capture%20compositional%20semantics%20that%20span%20multiple%20image%20regions.%20To%20address%20this%20limitation%2C%20we%20propose%20PowerCLIP%2C%20a%20novel%20contrastive%20pre-training%20framework%20enhanced%20by%20powerset%20alignment%2C%20which%20exhaustively%20optimizes%20region-to-phrase%20alignments%20by%20minimizing%20the%20loss%20defined%20between%20powersets%20of%20image%20regions%20and%20textual%20parse%20trees.%20Since%20the%20naive%20powerset%20construction%20incurs%20exponential%20computational%20cost%20due%20to%20the%20combinatorial%20explosion%20in%20the%20number%20of%20region%20subsets%2C%20we%20introduce%20efficient%20non-linear%20aggregators%20%28NLAs%29%20that%20reduce%20complexity%20from%20O%282%5EM%29%20to%20O%28M%29%20with%20respect%20to%20the%20number%20of%20regions%20M%2C%20while%20approximating%20the%20exact%20loss%20value%20with%20arbitrary%20precision.%20Our%20extensive%20experiments%20demonstrate%20that%20PowerCLIP%20outperforms%20state-of-the-art%20methods%20in%20zero-shot%20classification%20and%20retrieval%20tasks%2C%20underscoring%20the%20compositionality%20and%20robustness%20of%20our%20approach.%20Our%20code%20will%20be%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.23170v1&entry.124074799=Read"},
{"title": "InstanceV: Instance-Level Video Generation", "author": "Yuheng Chen and Teng Hu and Jiangning Zhang and Zhucun Xue and Ran Yi and Lizhuang Ma", "abstract": "Recent advances in text-to-video diffusion models have enabled the generation of high-quality videos conditioned on textual descriptions. However, most existing text-to-video models rely solely on textual conditions, lacking general fine-grained controllability over video generation. To address this challenge, we propose InstanceV, a video generation framework that enables i) instance-level control and ii) global semantic consistency. Specifically, with the aid of proposed Instance-aware Masked Cross-Attention mechanism, InstanceV maximizes the utilization of additional instance-level grounding information to generate correctly attributed instances at designated spatial locations. To improve overall consistency, We introduce the Shared Timestep-Adaptive Prompt Enhancement module, which connects local instances with global semantics in a parameter-efficient manner. Furthermore, we incorporate Spatially-Aware Unconditional Guidance during both training and inference to alleviate the disappearance of small instances. Finally, we propose a new benchmark, named InstanceBench, which combines general video quality metrics with instance-aware metrics for more comprehensive evaluation on instance-level video generation. Extensive experiments demonstrate that InstanceV not only achieves remarkable instance-level controllability in video generation, but also outperforms existing state-of-the-art models in both general quality and instance-aware metrics across qualitative and quantitative evaluations.", "link": "http://arxiv.org/abs/2511.23146v1", "date": "2025-11-28", "relevancy": 2.6407, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6971}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6596}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstanceV%3A%20Instance-Level%20Video%20Generation&body=Title%3A%20InstanceV%3A%20Instance-Level%20Video%20Generation%0AAuthor%3A%20Yuheng%20Chen%20and%20Teng%20Hu%20and%20Jiangning%20Zhang%20and%20Zhucun%20Xue%20and%20Ran%20Yi%20and%20Lizhuang%20Ma%0AAbstract%3A%20Recent%20advances%20in%20text-to-video%20diffusion%20models%20have%20enabled%20the%20generation%20of%20high-quality%20videos%20conditioned%20on%20textual%20descriptions.%20However%2C%20most%20existing%20text-to-video%20models%20rely%20solely%20on%20textual%20conditions%2C%20lacking%20general%20fine-grained%20controllability%20over%20video%20generation.%20To%20address%20this%20challenge%2C%20we%20propose%20InstanceV%2C%20a%20video%20generation%20framework%20that%20enables%20i%29%20instance-level%20control%20and%20ii%29%20global%20semantic%20consistency.%20Specifically%2C%20with%20the%20aid%20of%20proposed%20Instance-aware%20Masked%20Cross-Attention%20mechanism%2C%20InstanceV%20maximizes%20the%20utilization%20of%20additional%20instance-level%20grounding%20information%20to%20generate%20correctly%20attributed%20instances%20at%20designated%20spatial%20locations.%20To%20improve%20overall%20consistency%2C%20We%20introduce%20the%20Shared%20Timestep-Adaptive%20Prompt%20Enhancement%20module%2C%20which%20connects%20local%20instances%20with%20global%20semantics%20in%20a%20parameter-efficient%20manner.%20Furthermore%2C%20we%20incorporate%20Spatially-Aware%20Unconditional%20Guidance%20during%20both%20training%20and%20inference%20to%20alleviate%20the%20disappearance%20of%20small%20instances.%20Finally%2C%20we%20propose%20a%20new%20benchmark%2C%20named%20InstanceBench%2C%20which%20combines%20general%20video%20quality%20metrics%20with%20instance-aware%20metrics%20for%20more%20comprehensive%20evaluation%20on%20instance-level%20video%20generation.%20Extensive%20experiments%20demonstrate%20that%20InstanceV%20not%20only%20achieves%20remarkable%20instance-level%20controllability%20in%20video%20generation%2C%20but%20also%20outperforms%20existing%20state-of-the-art%20models%20in%20both%20general%20quality%20and%20instance-aware%20metrics%20across%20qualitative%20and%20quantitative%20evaluations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstanceV%253A%2520Instance-Level%2520Video%2520Generation%26entry.906535625%3DYuheng%2520Chen%2520and%2520Teng%2520Hu%2520and%2520Jiangning%2520Zhang%2520and%2520Zhucun%2520Xue%2520and%2520Ran%2520Yi%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3DRecent%2520advances%2520in%2520text-to-video%2520diffusion%2520models%2520have%2520enabled%2520the%2520generation%2520of%2520high-quality%2520videos%2520conditioned%2520on%2520textual%2520descriptions.%2520However%252C%2520most%2520existing%2520text-to-video%2520models%2520rely%2520solely%2520on%2520textual%2520conditions%252C%2520lacking%2520general%2520fine-grained%2520controllability%2520over%2520video%2520generation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520InstanceV%252C%2520a%2520video%2520generation%2520framework%2520that%2520enables%2520i%2529%2520instance-level%2520control%2520and%2520ii%2529%2520global%2520semantic%2520consistency.%2520Specifically%252C%2520with%2520the%2520aid%2520of%2520proposed%2520Instance-aware%2520Masked%2520Cross-Attention%2520mechanism%252C%2520InstanceV%2520maximizes%2520the%2520utilization%2520of%2520additional%2520instance-level%2520grounding%2520information%2520to%2520generate%2520correctly%2520attributed%2520instances%2520at%2520designated%2520spatial%2520locations.%2520To%2520improve%2520overall%2520consistency%252C%2520We%2520introduce%2520the%2520Shared%2520Timestep-Adaptive%2520Prompt%2520Enhancement%2520module%252C%2520which%2520connects%2520local%2520instances%2520with%2520global%2520semantics%2520in%2520a%2520parameter-efficient%2520manner.%2520Furthermore%252C%2520we%2520incorporate%2520Spatially-Aware%2520Unconditional%2520Guidance%2520during%2520both%2520training%2520and%2520inference%2520to%2520alleviate%2520the%2520disappearance%2520of%2520small%2520instances.%2520Finally%252C%2520we%2520propose%2520a%2520new%2520benchmark%252C%2520named%2520InstanceBench%252C%2520which%2520combines%2520general%2520video%2520quality%2520metrics%2520with%2520instance-aware%2520metrics%2520for%2520more%2520comprehensive%2520evaluation%2520on%2520instance-level%2520video%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520InstanceV%2520not%2520only%2520achieves%2520remarkable%2520instance-level%2520controllability%2520in%2520video%2520generation%252C%2520but%2520also%2520outperforms%2520existing%2520state-of-the-art%2520models%2520in%2520both%2520general%2520quality%2520and%2520instance-aware%2520metrics%2520across%2520qualitative%2520and%2520quantitative%2520evaluations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstanceV%3A%20Instance-Level%20Video%20Generation&entry.906535625=Yuheng%20Chen%20and%20Teng%20Hu%20and%20Jiangning%20Zhang%20and%20Zhucun%20Xue%20and%20Ran%20Yi%20and%20Lizhuang%20Ma&entry.1292438233=Recent%20advances%20in%20text-to-video%20diffusion%20models%20have%20enabled%20the%20generation%20of%20high-quality%20videos%20conditioned%20on%20textual%20descriptions.%20However%2C%20most%20existing%20text-to-video%20models%20rely%20solely%20on%20textual%20conditions%2C%20lacking%20general%20fine-grained%20controllability%20over%20video%20generation.%20To%20address%20this%20challenge%2C%20we%20propose%20InstanceV%2C%20a%20video%20generation%20framework%20that%20enables%20i%29%20instance-level%20control%20and%20ii%29%20global%20semantic%20consistency.%20Specifically%2C%20with%20the%20aid%20of%20proposed%20Instance-aware%20Masked%20Cross-Attention%20mechanism%2C%20InstanceV%20maximizes%20the%20utilization%20of%20additional%20instance-level%20grounding%20information%20to%20generate%20correctly%20attributed%20instances%20at%20designated%20spatial%20locations.%20To%20improve%20overall%20consistency%2C%20We%20introduce%20the%20Shared%20Timestep-Adaptive%20Prompt%20Enhancement%20module%2C%20which%20connects%20local%20instances%20with%20global%20semantics%20in%20a%20parameter-efficient%20manner.%20Furthermore%2C%20we%20incorporate%20Spatially-Aware%20Unconditional%20Guidance%20during%20both%20training%20and%20inference%20to%20alleviate%20the%20disappearance%20of%20small%20instances.%20Finally%2C%20we%20propose%20a%20new%20benchmark%2C%20named%20InstanceBench%2C%20which%20combines%20general%20video%20quality%20metrics%20with%20instance-aware%20metrics%20for%20more%20comprehensive%20evaluation%20on%20instance-level%20video%20generation.%20Extensive%20experiments%20demonstrate%20that%20InstanceV%20not%20only%20achieves%20remarkable%20instance-level%20controllability%20in%20video%20generation%2C%20but%20also%20outperforms%20existing%20state-of-the-art%20models%20in%20both%20general%20quality%20and%20instance-aware%20metrics%20across%20qualitative%20and%20quantitative%20evaluations.&entry.1838667208=http%3A//arxiv.org/abs/2511.23146v1&entry.124074799=Read"},
{"title": "Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach", "author": "Irina Jurenka and Markus Kunesch and Kevin R. McKee and Daniel Gillick and Shaojian Zhu and Sara Wiltberger and Shubham Milind Phal and Katherine Hermann and Daniel Kasenberg and Avishkar Bhoopchand and Ankit Anand and Miruna P\u00eeslar and Stephanie Chan and Lisa Wang and Jennifer She and Parsa Mahmoudieh and Aliya Rysbek and Wei-Jen Ko and Andrea Huber and Brett Wiltshire and Gal Elidan and Roni Rabin and Jasmin Rubinovitz and Amit Pitaru and Mac McAllister and Julia Wilkowski and David Choi and Roee Engelberg and Lidan Hackmon and Adva Levin and Rachel Griffin and Michael Sears and Filip Bar and Mia Mesar and Mana Jabbour and Arslan Chaudhry and James Cohan and Sridhar Thiagarajan and Nir Levine and Ben Brown and Dilan Gorur and Svetlana Grant and Rachel Hashimshoni and Laura Weidinger and Jieru Hu and Dawn Chen and Kuba Dolecki and Canfer Akbulut and Maxwell Bileschi and Laura Culp and Wen-Xin Dong and Nahema Marchal and Kelsie Van Deman and Hema Bajaj Misra and Michael Duah and Moran Ambar and Avi Caciularu and Sandra Lefdal and Chris Summerfield and James An and Pierre-Alexandre Kamienny and Abhinit Mohdi and Theofilos Strinopoulous and Annie Hale and Wayne Anderson and Luis C. Cobo and Niv Efron and Muktha Ananda and Shakir Mohamed and Maureen Heymans and Zoubin Ghahramani and Yossi Matias and Ben Gomes and Lila Ibrahim", "abstract": "A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards maximising the positive impact of gen AI in education.", "link": "http://arxiv.org/abs/2407.12687v3", "date": "2025-11-28", "relevancy": 2.5985, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5667}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5092}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%20Evaluation-Driven%20Approach&body=Title%3A%20Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%20Evaluation-Driven%20Approach%0AAuthor%3A%20Irina%20Jurenka%20and%20Markus%20Kunesch%20and%20Kevin%20R.%20McKee%20and%20Daniel%20Gillick%20and%20Shaojian%20Zhu%20and%20Sara%20Wiltberger%20and%20Shubham%20Milind%20Phal%20and%20Katherine%20Hermann%20and%20Daniel%20Kasenberg%20and%20Avishkar%20Bhoopchand%20and%20Ankit%20Anand%20and%20Miruna%20P%C3%AEslar%20and%20Stephanie%20Chan%20and%20Lisa%20Wang%20and%20Jennifer%20She%20and%20Parsa%20Mahmoudieh%20and%20Aliya%20Rysbek%20and%20Wei-Jen%20Ko%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Gal%20Elidan%20and%20Roni%20Rabin%20and%20Jasmin%20Rubinovitz%20and%20Amit%20Pitaru%20and%20Mac%20McAllister%20and%20Julia%20Wilkowski%20and%20David%20Choi%20and%20Roee%20Engelberg%20and%20Lidan%20Hackmon%20and%20Adva%20Levin%20and%20Rachel%20Griffin%20and%20Michael%20Sears%20and%20Filip%20Bar%20and%20Mia%20Mesar%20and%20Mana%20Jabbour%20and%20Arslan%20Chaudhry%20and%20James%20Cohan%20and%20Sridhar%20Thiagarajan%20and%20Nir%20Levine%20and%20Ben%20Brown%20and%20Dilan%20Gorur%20and%20Svetlana%20Grant%20and%20Rachel%20Hashimshoni%20and%20Laura%20Weidinger%20and%20Jieru%20Hu%20and%20Dawn%20Chen%20and%20Kuba%20Dolecki%20and%20Canfer%20Akbulut%20and%20Maxwell%20Bileschi%20and%20Laura%20Culp%20and%20Wen-Xin%20Dong%20and%20Nahema%20Marchal%20and%20Kelsie%20Van%20Deman%20and%20Hema%20Bajaj%20Misra%20and%20Michael%20Duah%20and%20Moran%20Ambar%20and%20Avi%20Caciularu%20and%20Sandra%20Lefdal%20and%20Chris%20Summerfield%20and%20James%20An%20and%20Pierre-Alexandre%20Kamienny%20and%20Abhinit%20Mohdi%20and%20Theofilos%20Strinopoulous%20and%20Annie%20Hale%20and%20Wayne%20Anderson%20and%20Luis%20C.%20Cobo%20and%20Niv%20Efron%20and%20Muktha%20Ananda%20and%20Shakir%20Mohamed%20and%20Maureen%20Heymans%20and%20Zoubin%20Ghahramani%20and%20Yossi%20Matias%20and%20Ben%20Gomes%20and%20Lila%20Ibrahim%0AAbstract%3A%20A%20major%20challenge%20facing%20the%20world%20is%20the%20provision%20of%20equitable%20and%20universal%20access%20to%20quality%20education.%20Recent%20advances%20in%20generative%20AI%20%28gen%20AI%29%20have%20created%20excitement%20about%20the%20potential%20of%20new%20technologies%20to%20offer%20a%20personal%20tutor%20for%20every%20learner%20and%20a%20teaching%20assistant%20for%20every%20teacher.%20The%20full%20extent%20of%20this%20dream%2C%20however%2C%20has%20not%20yet%20materialised.%20We%20argue%20that%20this%20is%20primarily%20due%20to%20the%20difficulties%20with%20verbalising%20pedagogical%20intuitions%20into%20gen%20AI%20prompts%20and%20the%20lack%20of%20good%20evaluation%20practices%2C%20reinforced%20by%20the%20challenges%20in%20defining%20excellent%20pedagogy.%20Here%20we%20present%20our%20work%20collaborating%20with%20learners%20and%20educators%20to%20translate%20high%20level%20principles%20from%20learning%20science%20into%20a%20pragmatic%20set%20of%20seven%20diverse%20educational%20benchmarks%2C%20spanning%20quantitative%2C%20qualitative%2C%20automatic%20and%20human%20evaluations%3B%20and%20to%20develop%20a%20new%20set%20of%20fine-tuning%20datasets%20to%20improve%20the%20pedagogical%20capabilities%20of%20Gemini%2C%20introducing%20LearnLM-Tutor.%20Our%20evaluations%20show%20that%20LearnLM-Tutor%20is%20consistently%20preferred%20over%20a%20prompt%20tuned%20Gemini%20by%20educators%20and%20learners%20on%20a%20number%20of%20pedagogical%20dimensions.%20We%20hope%20that%20this%20work%20can%20serve%20as%20a%20first%20step%20towards%20developing%20a%20comprehensive%20educational%20evaluation%20framework%2C%20and%20that%20this%20can%20enable%20rapid%20progress%20within%20the%20AI%20and%20EdTech%20communities%20towards%20maximising%20the%20positive%20impact%20of%20gen%20AI%20in%20education.%0ALink%3A%20http%3A//arxiv.org/abs/2407.12687v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Responsible%2520Development%2520of%2520Generative%2520AI%2520for%2520Education%253A%2520An%2520Evaluation-Driven%2520Approach%26entry.906535625%3DIrina%2520Jurenka%2520and%2520Markus%2520Kunesch%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Daniel%2520Gillick%2520and%2520Shaojian%2520Zhu%2520and%2520Sara%2520Wiltberger%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Katherine%2520Hermann%2520and%2520Daniel%2520Kasenberg%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Ankit%2520Anand%2520and%2520Miruna%2520P%25C3%25AEslar%2520and%2520Stephanie%2520Chan%2520and%2520Lisa%2520Wang%2520and%2520Jennifer%2520She%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Aliya%2520Rysbek%2520and%2520Wei-Jen%2520Ko%2520and%2520Andrea%2520Huber%2520and%2520Brett%2520Wiltshire%2520and%2520Gal%2520Elidan%2520and%2520Roni%2520Rabin%2520and%2520Jasmin%2520Rubinovitz%2520and%2520Amit%2520Pitaru%2520and%2520Mac%2520McAllister%2520and%2520Julia%2520Wilkowski%2520and%2520David%2520Choi%2520and%2520Roee%2520Engelberg%2520and%2520Lidan%2520Hackmon%2520and%2520Adva%2520Levin%2520and%2520Rachel%2520Griffin%2520and%2520Michael%2520Sears%2520and%2520Filip%2520Bar%2520and%2520Mia%2520Mesar%2520and%2520Mana%2520Jabbour%2520and%2520Arslan%2520Chaudhry%2520and%2520James%2520Cohan%2520and%2520Sridhar%2520Thiagarajan%2520and%2520Nir%2520Levine%2520and%2520Ben%2520Brown%2520and%2520Dilan%2520Gorur%2520and%2520Svetlana%2520Grant%2520and%2520Rachel%2520Hashimshoni%2520and%2520Laura%2520Weidinger%2520and%2520Jieru%2520Hu%2520and%2520Dawn%2520Chen%2520and%2520Kuba%2520Dolecki%2520and%2520Canfer%2520Akbulut%2520and%2520Maxwell%2520Bileschi%2520and%2520Laura%2520Culp%2520and%2520Wen-Xin%2520Dong%2520and%2520Nahema%2520Marchal%2520and%2520Kelsie%2520Van%2520Deman%2520and%2520Hema%2520Bajaj%2520Misra%2520and%2520Michael%2520Duah%2520and%2520Moran%2520Ambar%2520and%2520Avi%2520Caciularu%2520and%2520Sandra%2520Lefdal%2520and%2520Chris%2520Summerfield%2520and%2520James%2520An%2520and%2520Pierre-Alexandre%2520Kamienny%2520and%2520Abhinit%2520Mohdi%2520and%2520Theofilos%2520Strinopoulous%2520and%2520Annie%2520Hale%2520and%2520Wayne%2520Anderson%2520and%2520Luis%2520C.%2520Cobo%2520and%2520Niv%2520Efron%2520and%2520Muktha%2520Ananda%2520and%2520Shakir%2520Mohamed%2520and%2520Maureen%2520Heymans%2520and%2520Zoubin%2520Ghahramani%2520and%2520Yossi%2520Matias%2520and%2520Ben%2520Gomes%2520and%2520Lila%2520Ibrahim%26entry.1292438233%3DA%2520major%2520challenge%2520facing%2520the%2520world%2520is%2520the%2520provision%2520of%2520equitable%2520and%2520universal%2520access%2520to%2520quality%2520education.%2520Recent%2520advances%2520in%2520generative%2520AI%2520%2528gen%2520AI%2529%2520have%2520created%2520excitement%2520about%2520the%2520potential%2520of%2520new%2520technologies%2520to%2520offer%2520a%2520personal%2520tutor%2520for%2520every%2520learner%2520and%2520a%2520teaching%2520assistant%2520for%2520every%2520teacher.%2520The%2520full%2520extent%2520of%2520this%2520dream%252C%2520however%252C%2520has%2520not%2520yet%2520materialised.%2520We%2520argue%2520that%2520this%2520is%2520primarily%2520due%2520to%2520the%2520difficulties%2520with%2520verbalising%2520pedagogical%2520intuitions%2520into%2520gen%2520AI%2520prompts%2520and%2520the%2520lack%2520of%2520good%2520evaluation%2520practices%252C%2520reinforced%2520by%2520the%2520challenges%2520in%2520defining%2520excellent%2520pedagogy.%2520Here%2520we%2520present%2520our%2520work%2520collaborating%2520with%2520learners%2520and%2520educators%2520to%2520translate%2520high%2520level%2520principles%2520from%2520learning%2520science%2520into%2520a%2520pragmatic%2520set%2520of%2520seven%2520diverse%2520educational%2520benchmarks%252C%2520spanning%2520quantitative%252C%2520qualitative%252C%2520automatic%2520and%2520human%2520evaluations%253B%2520and%2520to%2520develop%2520a%2520new%2520set%2520of%2520fine-tuning%2520datasets%2520to%2520improve%2520the%2520pedagogical%2520capabilities%2520of%2520Gemini%252C%2520introducing%2520LearnLM-Tutor.%2520Our%2520evaluations%2520show%2520that%2520LearnLM-Tutor%2520is%2520consistently%2520preferred%2520over%2520a%2520prompt%2520tuned%2520Gemini%2520by%2520educators%2520and%2520learners%2520on%2520a%2520number%2520of%2520pedagogical%2520dimensions.%2520We%2520hope%2520that%2520this%2520work%2520can%2520serve%2520as%2520a%2520first%2520step%2520towards%2520developing%2520a%2520comprehensive%2520educational%2520evaluation%2520framework%252C%2520and%2520that%2520this%2520can%2520enable%2520rapid%2520progress%2520within%2520the%2520AI%2520and%2520EdTech%2520communities%2520towards%2520maximising%2520the%2520positive%2520impact%2520of%2520gen%2520AI%2520in%2520education.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12687v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%20Evaluation-Driven%20Approach&entry.906535625=Irina%20Jurenka%20and%20Markus%20Kunesch%20and%20Kevin%20R.%20McKee%20and%20Daniel%20Gillick%20and%20Shaojian%20Zhu%20and%20Sara%20Wiltberger%20and%20Shubham%20Milind%20Phal%20and%20Katherine%20Hermann%20and%20Daniel%20Kasenberg%20and%20Avishkar%20Bhoopchand%20and%20Ankit%20Anand%20and%20Miruna%20P%C3%AEslar%20and%20Stephanie%20Chan%20and%20Lisa%20Wang%20and%20Jennifer%20She%20and%20Parsa%20Mahmoudieh%20and%20Aliya%20Rysbek%20and%20Wei-Jen%20Ko%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Gal%20Elidan%20and%20Roni%20Rabin%20and%20Jasmin%20Rubinovitz%20and%20Amit%20Pitaru%20and%20Mac%20McAllister%20and%20Julia%20Wilkowski%20and%20David%20Choi%20and%20Roee%20Engelberg%20and%20Lidan%20Hackmon%20and%20Adva%20Levin%20and%20Rachel%20Griffin%20and%20Michael%20Sears%20and%20Filip%20Bar%20and%20Mia%20Mesar%20and%20Mana%20Jabbour%20and%20Arslan%20Chaudhry%20and%20James%20Cohan%20and%20Sridhar%20Thiagarajan%20and%20Nir%20Levine%20and%20Ben%20Brown%20and%20Dilan%20Gorur%20and%20Svetlana%20Grant%20and%20Rachel%20Hashimshoni%20and%20Laura%20Weidinger%20and%20Jieru%20Hu%20and%20Dawn%20Chen%20and%20Kuba%20Dolecki%20and%20Canfer%20Akbulut%20and%20Maxwell%20Bileschi%20and%20Laura%20Culp%20and%20Wen-Xin%20Dong%20and%20Nahema%20Marchal%20and%20Kelsie%20Van%20Deman%20and%20Hema%20Bajaj%20Misra%20and%20Michael%20Duah%20and%20Moran%20Ambar%20and%20Avi%20Caciularu%20and%20Sandra%20Lefdal%20and%20Chris%20Summerfield%20and%20James%20An%20and%20Pierre-Alexandre%20Kamienny%20and%20Abhinit%20Mohdi%20and%20Theofilos%20Strinopoulous%20and%20Annie%20Hale%20and%20Wayne%20Anderson%20and%20Luis%20C.%20Cobo%20and%20Niv%20Efron%20and%20Muktha%20Ananda%20and%20Shakir%20Mohamed%20and%20Maureen%20Heymans%20and%20Zoubin%20Ghahramani%20and%20Yossi%20Matias%20and%20Ben%20Gomes%20and%20Lila%20Ibrahim&entry.1292438233=A%20major%20challenge%20facing%20the%20world%20is%20the%20provision%20of%20equitable%20and%20universal%20access%20to%20quality%20education.%20Recent%20advances%20in%20generative%20AI%20%28gen%20AI%29%20have%20created%20excitement%20about%20the%20potential%20of%20new%20technologies%20to%20offer%20a%20personal%20tutor%20for%20every%20learner%20and%20a%20teaching%20assistant%20for%20every%20teacher.%20The%20full%20extent%20of%20this%20dream%2C%20however%2C%20has%20not%20yet%20materialised.%20We%20argue%20that%20this%20is%20primarily%20due%20to%20the%20difficulties%20with%20verbalising%20pedagogical%20intuitions%20into%20gen%20AI%20prompts%20and%20the%20lack%20of%20good%20evaluation%20practices%2C%20reinforced%20by%20the%20challenges%20in%20defining%20excellent%20pedagogy.%20Here%20we%20present%20our%20work%20collaborating%20with%20learners%20and%20educators%20to%20translate%20high%20level%20principles%20from%20learning%20science%20into%20a%20pragmatic%20set%20of%20seven%20diverse%20educational%20benchmarks%2C%20spanning%20quantitative%2C%20qualitative%2C%20automatic%20and%20human%20evaluations%3B%20and%20to%20develop%20a%20new%20set%20of%20fine-tuning%20datasets%20to%20improve%20the%20pedagogical%20capabilities%20of%20Gemini%2C%20introducing%20LearnLM-Tutor.%20Our%20evaluations%20show%20that%20LearnLM-Tutor%20is%20consistently%20preferred%20over%20a%20prompt%20tuned%20Gemini%20by%20educators%20and%20learners%20on%20a%20number%20of%20pedagogical%20dimensions.%20We%20hope%20that%20this%20work%20can%20serve%20as%20a%20first%20step%20towards%20developing%20a%20comprehensive%20educational%20evaluation%20framework%2C%20and%20that%20this%20can%20enable%20rapid%20progress%20within%20the%20AI%20and%20EdTech%20communities%20towards%20maximising%20the%20positive%20impact%20of%20gen%20AI%20in%20education.&entry.1838667208=http%3A//arxiv.org/abs/2407.12687v3&entry.124074799=Read"},
{"title": "Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach", "author": "Shuqi Liu and Han Wu and Guanzhi Deng and Jianshu Chen and Xiaoyang Wang and Linqi Song", "abstract": "Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.", "link": "http://arxiv.org/abs/2511.23335v1", "date": "2025-11-28", "relevancy": 2.5877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5192}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Improving%20Interpretability%20of%20Language%20Model%20Generation%20through%20a%20Structured%20Knowledge%20Discovery%20Approach&body=Title%3A%20Towards%20Improving%20Interpretability%20of%20Language%20Model%20Generation%20through%20a%20Structured%20Knowledge%20Discovery%20Approach%0AAuthor%3A%20Shuqi%20Liu%20and%20Han%20Wu%20and%20Guanzhi%20Deng%20and%20Jianshu%20Chen%20and%20Xiaoyang%20Wang%20and%20Linqi%20Song%0AAbstract%3A%20Knowledge-enhanced%20text%20generation%20aims%20to%20enhance%20the%20quality%20of%20generated%20text%20by%20utilizing%20internal%20or%20external%20knowledge%20sources.%20While%20language%20models%20have%20demonstrated%20impressive%20capabilities%20in%20generating%20coherent%20and%20fluent%20text%2C%20the%20lack%20of%20interpretability%20presents%20a%20substantial%20obstacle.%20The%20limited%20interpretability%20of%20generated%20text%20significantly%20impacts%20its%20practical%20usability%2C%20particularly%20in%20knowledge-enhanced%20text%20generation%20tasks%20that%20necessitate%20reliability%20and%20explainability.%20Existing%20methods%20often%20employ%20domain-specific%20knowledge%20retrievers%20that%20are%20tailored%20to%20specific%20data%20characteristics%2C%20limiting%20their%20generalizability%20to%20diverse%20data%20types%20and%20tasks.%20To%20overcome%20this%20limitation%2C%20we%20directly%20leverage%20the%20two-tier%20architecture%20of%20structured%20knowledge%2C%20consisting%20of%20high-level%20entities%20and%20low-level%20knowledge%20triples%2C%20to%20design%20our%20task-agnostic%20structured%20knowledge%20hunter.%20Specifically%2C%20we%20employ%20a%20local-global%20interaction%20scheme%20for%20structured%20knowledge%20representation%20learning%20and%20a%20hierarchical%20transformer-based%20pointer%20network%20as%20the%20backbone%20for%20selecting%20relevant%20knowledge%20triples%20and%20entities.%20By%20combining%20the%20strong%20generative%20ability%20of%20language%20models%20with%20the%20high%20faithfulness%20of%20the%20knowledge%20hunter%2C%20our%20model%20achieves%20high%20interpretability%2C%20enabling%20users%20to%20comprehend%20the%20model%20output%20generation%20process.%20Furthermore%2C%20we%20empirically%20demonstrate%20the%20effectiveness%20of%20our%20model%20in%20both%20internal%20knowledge-enhanced%20table-to-text%20generation%20on%20the%20RotoWireFG%20dataset%20and%20external%20knowledge-enhanced%20dialogue%20response%20generation%20on%20the%20KdConv%20dataset.%20Our%20task-agnostic%20model%20outperforms%20state-of-the-art%20methods%20and%20corresponding%20language%20models%2C%20setting%20new%20standards%20on%20the%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Improving%2520Interpretability%2520of%2520Language%2520Model%2520Generation%2520through%2520a%2520Structured%2520Knowledge%2520Discovery%2520Approach%26entry.906535625%3DShuqi%2520Liu%2520and%2520Han%2520Wu%2520and%2520Guanzhi%2520Deng%2520and%2520Jianshu%2520Chen%2520and%2520Xiaoyang%2520Wang%2520and%2520Linqi%2520Song%26entry.1292438233%3DKnowledge-enhanced%2520text%2520generation%2520aims%2520to%2520enhance%2520the%2520quality%2520of%2520generated%2520text%2520by%2520utilizing%2520internal%2520or%2520external%2520knowledge%2520sources.%2520While%2520language%2520models%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520generating%2520coherent%2520and%2520fluent%2520text%252C%2520the%2520lack%2520of%2520interpretability%2520presents%2520a%2520substantial%2520obstacle.%2520The%2520limited%2520interpretability%2520of%2520generated%2520text%2520significantly%2520impacts%2520its%2520practical%2520usability%252C%2520particularly%2520in%2520knowledge-enhanced%2520text%2520generation%2520tasks%2520that%2520necessitate%2520reliability%2520and%2520explainability.%2520Existing%2520methods%2520often%2520employ%2520domain-specific%2520knowledge%2520retrievers%2520that%2520are%2520tailored%2520to%2520specific%2520data%2520characteristics%252C%2520limiting%2520their%2520generalizability%2520to%2520diverse%2520data%2520types%2520and%2520tasks.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520directly%2520leverage%2520the%2520two-tier%2520architecture%2520of%2520structured%2520knowledge%252C%2520consisting%2520of%2520high-level%2520entities%2520and%2520low-level%2520knowledge%2520triples%252C%2520to%2520design%2520our%2520task-agnostic%2520structured%2520knowledge%2520hunter.%2520Specifically%252C%2520we%2520employ%2520a%2520local-global%2520interaction%2520scheme%2520for%2520structured%2520knowledge%2520representation%2520learning%2520and%2520a%2520hierarchical%2520transformer-based%2520pointer%2520network%2520as%2520the%2520backbone%2520for%2520selecting%2520relevant%2520knowledge%2520triples%2520and%2520entities.%2520By%2520combining%2520the%2520strong%2520generative%2520ability%2520of%2520language%2520models%2520with%2520the%2520high%2520faithfulness%2520of%2520the%2520knowledge%2520hunter%252C%2520our%2520model%2520achieves%2520high%2520interpretability%252C%2520enabling%2520users%2520to%2520comprehend%2520the%2520model%2520output%2520generation%2520process.%2520Furthermore%252C%2520we%2520empirically%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520model%2520in%2520both%2520internal%2520knowledge-enhanced%2520table-to-text%2520generation%2520on%2520the%2520RotoWireFG%2520dataset%2520and%2520external%2520knowledge-enhanced%2520dialogue%2520response%2520generation%2520on%2520the%2520KdConv%2520dataset.%2520Our%2520task-agnostic%2520model%2520outperforms%2520state-of-the-art%2520methods%2520and%2520corresponding%2520language%2520models%252C%2520setting%2520new%2520standards%2520on%2520the%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Improving%20Interpretability%20of%20Language%20Model%20Generation%20through%20a%20Structured%20Knowledge%20Discovery%20Approach&entry.906535625=Shuqi%20Liu%20and%20Han%20Wu%20and%20Guanzhi%20Deng%20and%20Jianshu%20Chen%20and%20Xiaoyang%20Wang%20and%20Linqi%20Song&entry.1292438233=Knowledge-enhanced%20text%20generation%20aims%20to%20enhance%20the%20quality%20of%20generated%20text%20by%20utilizing%20internal%20or%20external%20knowledge%20sources.%20While%20language%20models%20have%20demonstrated%20impressive%20capabilities%20in%20generating%20coherent%20and%20fluent%20text%2C%20the%20lack%20of%20interpretability%20presents%20a%20substantial%20obstacle.%20The%20limited%20interpretability%20of%20generated%20text%20significantly%20impacts%20its%20practical%20usability%2C%20particularly%20in%20knowledge-enhanced%20text%20generation%20tasks%20that%20necessitate%20reliability%20and%20explainability.%20Existing%20methods%20often%20employ%20domain-specific%20knowledge%20retrievers%20that%20are%20tailored%20to%20specific%20data%20characteristics%2C%20limiting%20their%20generalizability%20to%20diverse%20data%20types%20and%20tasks.%20To%20overcome%20this%20limitation%2C%20we%20directly%20leverage%20the%20two-tier%20architecture%20of%20structured%20knowledge%2C%20consisting%20of%20high-level%20entities%20and%20low-level%20knowledge%20triples%2C%20to%20design%20our%20task-agnostic%20structured%20knowledge%20hunter.%20Specifically%2C%20we%20employ%20a%20local-global%20interaction%20scheme%20for%20structured%20knowledge%20representation%20learning%20and%20a%20hierarchical%20transformer-based%20pointer%20network%20as%20the%20backbone%20for%20selecting%20relevant%20knowledge%20triples%20and%20entities.%20By%20combining%20the%20strong%20generative%20ability%20of%20language%20models%20with%20the%20high%20faithfulness%20of%20the%20knowledge%20hunter%2C%20our%20model%20achieves%20high%20interpretability%2C%20enabling%20users%20to%20comprehend%20the%20model%20output%20generation%20process.%20Furthermore%2C%20we%20empirically%20demonstrate%20the%20effectiveness%20of%20our%20model%20in%20both%20internal%20knowledge-enhanced%20table-to-text%20generation%20on%20the%20RotoWireFG%20dataset%20and%20external%20knowledge-enhanced%20dialogue%20response%20generation%20on%20the%20KdConv%20dataset.%20Our%20task-agnostic%20model%20outperforms%20state-of-the-art%20methods%20and%20corresponding%20language%20models%2C%20setting%20new%20standards%20on%20the%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2511.23335v1&entry.124074799=Read"},
{"title": "Infrared and Visible Image Fusion with Language-Driven Loss in CLIP Embedding Space", "author": "Yuhao Wang and Lingjuan Miao and Zhiqiang Zhou and Lei Zhang and Yajun Qiao", "abstract": "Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities. Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically. As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited. In this paper, we propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance. For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the multi-modal embedding space using CLIP. A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors representing the fusion objective and input image modalities. Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via supervised training. Experiments show that our method can obtain much better fusion results than existing techniques. The code is available at https://github.com/wyhlaowang/LDFusion.", "link": "http://arxiv.org/abs/2402.16267v3", "date": "2025-11-28", "relevancy": 2.582, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5249}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5209}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infrared%20and%20Visible%20Image%20Fusion%20with%20Language-Driven%20Loss%20in%20CLIP%20Embedding%20Space&body=Title%3A%20Infrared%20and%20Visible%20Image%20Fusion%20with%20Language-Driven%20Loss%20in%20CLIP%20Embedding%20Space%0AAuthor%3A%20Yuhao%20Wang%20and%20Lingjuan%20Miao%20and%20Zhiqiang%20Zhou%20and%20Lei%20Zhang%20and%20Yajun%20Qiao%0AAbstract%3A%20Infrared-visible%20image%20fusion%20%28IVIF%29%20has%20attracted%20much%20attention%20owing%20to%20the%20highly-complementary%20properties%20of%20the%20two%20image%20modalities.%20Due%20to%20the%20lack%20of%20ground-truth%20fused%20images%2C%20the%20fusion%20output%20of%20current%20deep-learning%20based%20methods%20heavily%20depends%20on%20the%20loss%20functions%20defined%20mathematically.%20As%20it%20is%20hard%20to%20well%20mathematically%20define%20the%20fused%20image%20without%20ground%20truth%2C%20the%20performance%20of%20existing%20fusion%20methods%20is%20limited.%20In%20this%20paper%2C%20we%20propose%20to%20use%20natural%20language%20to%20express%20the%20objective%20of%20IVIF%2C%20which%20can%20avoid%20the%20explicit%20mathematical%20modeling%20of%20fusion%20output%20in%20current%20losses%2C%20and%20make%20full%20use%20of%20the%20advantage%20of%20language%20expression%20to%20improve%20the%20fusion%20performance.%20For%20this%20purpose%2C%20we%20present%20a%20comprehensive%20language-expressed%20fusion%20objective%2C%20and%20encode%20relevant%20texts%20into%20the%20multi-modal%20embedding%20space%20using%20CLIP.%20A%20language-driven%20fusion%20model%20is%20then%20constructed%20in%20the%20embedding%20space%2C%20by%20establishing%20the%20relationship%20among%20the%20embedded%20vectors%20representing%20the%20fusion%20objective%20and%20input%20image%20modalities.%20Finally%2C%20a%20language-driven%20loss%20is%20derived%20to%20make%20the%20actual%20IVIF%20aligned%20with%20the%20embedded%20language-driven%20fusion%20model%20via%20supervised%20training.%20Experiments%20show%20that%20our%20method%20can%20obtain%20much%20better%20fusion%20results%20than%20existing%20techniques.%20The%20code%20is%20available%20at%20https%3A//github.com/wyhlaowang/LDFusion.%0ALink%3A%20http%3A//arxiv.org/abs/2402.16267v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfrared%2520and%2520Visible%2520Image%2520Fusion%2520with%2520Language-Driven%2520Loss%2520in%2520CLIP%2520Embedding%2520Space%26entry.906535625%3DYuhao%2520Wang%2520and%2520Lingjuan%2520Miao%2520and%2520Zhiqiang%2520Zhou%2520and%2520Lei%2520Zhang%2520and%2520Yajun%2520Qiao%26entry.1292438233%3DInfrared-visible%2520image%2520fusion%2520%2528IVIF%2529%2520has%2520attracted%2520much%2520attention%2520owing%2520to%2520the%2520highly-complementary%2520properties%2520of%2520the%2520two%2520image%2520modalities.%2520Due%2520to%2520the%2520lack%2520of%2520ground-truth%2520fused%2520images%252C%2520the%2520fusion%2520output%2520of%2520current%2520deep-learning%2520based%2520methods%2520heavily%2520depends%2520on%2520the%2520loss%2520functions%2520defined%2520mathematically.%2520As%2520it%2520is%2520hard%2520to%2520well%2520mathematically%2520define%2520the%2520fused%2520image%2520without%2520ground%2520truth%252C%2520the%2520performance%2520of%2520existing%2520fusion%2520methods%2520is%2520limited.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520use%2520natural%2520language%2520to%2520express%2520the%2520objective%2520of%2520IVIF%252C%2520which%2520can%2520avoid%2520the%2520explicit%2520mathematical%2520modeling%2520of%2520fusion%2520output%2520in%2520current%2520losses%252C%2520and%2520make%2520full%2520use%2520of%2520the%2520advantage%2520of%2520language%2520expression%2520to%2520improve%2520the%2520fusion%2520performance.%2520For%2520this%2520purpose%252C%2520we%2520present%2520a%2520comprehensive%2520language-expressed%2520fusion%2520objective%252C%2520and%2520encode%2520relevant%2520texts%2520into%2520the%2520multi-modal%2520embedding%2520space%2520using%2520CLIP.%2520A%2520language-driven%2520fusion%2520model%2520is%2520then%2520constructed%2520in%2520the%2520embedding%2520space%252C%2520by%2520establishing%2520the%2520relationship%2520among%2520the%2520embedded%2520vectors%2520representing%2520the%2520fusion%2520objective%2520and%2520input%2520image%2520modalities.%2520Finally%252C%2520a%2520language-driven%2520loss%2520is%2520derived%2520to%2520make%2520the%2520actual%2520IVIF%2520aligned%2520with%2520the%2520embedded%2520language-driven%2520fusion%2520model%2520via%2520supervised%2520training.%2520Experiments%2520show%2520that%2520our%2520method%2520can%2520obtain%2520much%2520better%2520fusion%2520results%2520than%2520existing%2520techniques.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/wyhlaowang/LDFusion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16267v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrared%20and%20Visible%20Image%20Fusion%20with%20Language-Driven%20Loss%20in%20CLIP%20Embedding%20Space&entry.906535625=Yuhao%20Wang%20and%20Lingjuan%20Miao%20and%20Zhiqiang%20Zhou%20and%20Lei%20Zhang%20and%20Yajun%20Qiao&entry.1292438233=Infrared-visible%20image%20fusion%20%28IVIF%29%20has%20attracted%20much%20attention%20owing%20to%20the%20highly-complementary%20properties%20of%20the%20two%20image%20modalities.%20Due%20to%20the%20lack%20of%20ground-truth%20fused%20images%2C%20the%20fusion%20output%20of%20current%20deep-learning%20based%20methods%20heavily%20depends%20on%20the%20loss%20functions%20defined%20mathematically.%20As%20it%20is%20hard%20to%20well%20mathematically%20define%20the%20fused%20image%20without%20ground%20truth%2C%20the%20performance%20of%20existing%20fusion%20methods%20is%20limited.%20In%20this%20paper%2C%20we%20propose%20to%20use%20natural%20language%20to%20express%20the%20objective%20of%20IVIF%2C%20which%20can%20avoid%20the%20explicit%20mathematical%20modeling%20of%20fusion%20output%20in%20current%20losses%2C%20and%20make%20full%20use%20of%20the%20advantage%20of%20language%20expression%20to%20improve%20the%20fusion%20performance.%20For%20this%20purpose%2C%20we%20present%20a%20comprehensive%20language-expressed%20fusion%20objective%2C%20and%20encode%20relevant%20texts%20into%20the%20multi-modal%20embedding%20space%20using%20CLIP.%20A%20language-driven%20fusion%20model%20is%20then%20constructed%20in%20the%20embedding%20space%2C%20by%20establishing%20the%20relationship%20among%20the%20embedded%20vectors%20representing%20the%20fusion%20objective%20and%20input%20image%20modalities.%20Finally%2C%20a%20language-driven%20loss%20is%20derived%20to%20make%20the%20actual%20IVIF%20aligned%20with%20the%20embedded%20language-driven%20fusion%20model%20via%20supervised%20training.%20Experiments%20show%20that%20our%20method%20can%20obtain%20much%20better%20fusion%20results%20than%20existing%20techniques.%20The%20code%20is%20available%20at%20https%3A//github.com/wyhlaowang/LDFusion.&entry.1838667208=http%3A//arxiv.org/abs/2402.16267v3&entry.124074799=Read"},
{"title": "SDFs from Unoriented Point Clouds using Neural Variational Heat Distances", "author": "Samuel Weidemaier and Florine Hartwig and Josua Sassen and Sergio Conti and Mirela Ben-Chen and Martin Rumpf", "abstract": "We propose a novel variational approach for computing neural Signed Distance Fields (SDF) from unoriented point clouds. To this end, we replace the commonly used eikonal equation with the heat method, carrying over to the neural domain what has long been standard practice for computing distances on discrete surfaces. This yields two convex optimization problems for whose solution we employ neural networks: We first compute a neural approximation of the gradients of the unsigned distance field through a small time step of heat flow with weighted point cloud densities as initial data. Then we use it to compute a neural approximation of the SDF. We prove that the underlying variational problems are well-posed. Through numerical experiments, we demonstrate that our method provides state-of-the-art surface reconstruction and consistent SDF gradients. Furthermore, we show in a proof-of-concept that it is accurate enough for solving a PDE on the zero-level set.", "link": "http://arxiv.org/abs/2504.11212v3", "date": "2025-11-28", "relevancy": 2.5716, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.519}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5138}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDFs%20from%20Unoriented%20Point%20Clouds%20using%20Neural%20Variational%20Heat%20Distances&body=Title%3A%20SDFs%20from%20Unoriented%20Point%20Clouds%20using%20Neural%20Variational%20Heat%20Distances%0AAuthor%3A%20Samuel%20Weidemaier%20and%20Florine%20Hartwig%20and%20Josua%20Sassen%20and%20Sergio%20Conti%20and%20Mirela%20Ben-Chen%20and%20Martin%20Rumpf%0AAbstract%3A%20We%20propose%20a%20novel%20variational%20approach%20for%20computing%20neural%20Signed%20Distance%20Fields%20%28SDF%29%20from%20unoriented%20point%20clouds.%20To%20this%20end%2C%20we%20replace%20the%20commonly%20used%20eikonal%20equation%20with%20the%20heat%20method%2C%20carrying%20over%20to%20the%20neural%20domain%20what%20has%20long%20been%20standard%20practice%20for%20computing%20distances%20on%20discrete%20surfaces.%20This%20yields%20two%20convex%20optimization%20problems%20for%20whose%20solution%20we%20employ%20neural%20networks%3A%20We%20first%20compute%20a%20neural%20approximation%20of%20the%20gradients%20of%20the%20unsigned%20distance%20field%20through%20a%20small%20time%20step%20of%20heat%20flow%20with%20weighted%20point%20cloud%20densities%20as%20initial%20data.%20Then%20we%20use%20it%20to%20compute%20a%20neural%20approximation%20of%20the%20SDF.%20We%20prove%20that%20the%20underlying%20variational%20problems%20are%20well-posed.%20Through%20numerical%20experiments%2C%20we%20demonstrate%20that%20our%20method%20provides%20state-of-the-art%20surface%20reconstruction%20and%20consistent%20SDF%20gradients.%20Furthermore%2C%20we%20show%20in%20a%20proof-of-concept%20that%20it%20is%20accurate%20enough%20for%20solving%20a%20PDE%20on%20the%20zero-level%20set.%0ALink%3A%20http%3A//arxiv.org/abs/2504.11212v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDFs%2520from%2520Unoriented%2520Point%2520Clouds%2520using%2520Neural%2520Variational%2520Heat%2520Distances%26entry.906535625%3DSamuel%2520Weidemaier%2520and%2520Florine%2520Hartwig%2520and%2520Josua%2520Sassen%2520and%2520Sergio%2520Conti%2520and%2520Mirela%2520Ben-Chen%2520and%2520Martin%2520Rumpf%26entry.1292438233%3DWe%2520propose%2520a%2520novel%2520variational%2520approach%2520for%2520computing%2520neural%2520Signed%2520Distance%2520Fields%2520%2528SDF%2529%2520from%2520unoriented%2520point%2520clouds.%2520To%2520this%2520end%252C%2520we%2520replace%2520the%2520commonly%2520used%2520eikonal%2520equation%2520with%2520the%2520heat%2520method%252C%2520carrying%2520over%2520to%2520the%2520neural%2520domain%2520what%2520has%2520long%2520been%2520standard%2520practice%2520for%2520computing%2520distances%2520on%2520discrete%2520surfaces.%2520This%2520yields%2520two%2520convex%2520optimization%2520problems%2520for%2520whose%2520solution%2520we%2520employ%2520neural%2520networks%253A%2520We%2520first%2520compute%2520a%2520neural%2520approximation%2520of%2520the%2520gradients%2520of%2520the%2520unsigned%2520distance%2520field%2520through%2520a%2520small%2520time%2520step%2520of%2520heat%2520flow%2520with%2520weighted%2520point%2520cloud%2520densities%2520as%2520initial%2520data.%2520Then%2520we%2520use%2520it%2520to%2520compute%2520a%2520neural%2520approximation%2520of%2520the%2520SDF.%2520We%2520prove%2520that%2520the%2520underlying%2520variational%2520problems%2520are%2520well-posed.%2520Through%2520numerical%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520provides%2520state-of-the-art%2520surface%2520reconstruction%2520and%2520consistent%2520SDF%2520gradients.%2520Furthermore%252C%2520we%2520show%2520in%2520a%2520proof-of-concept%2520that%2520it%2520is%2520accurate%2520enough%2520for%2520solving%2520a%2520PDE%2520on%2520the%2520zero-level%2520set.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11212v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDFs%20from%20Unoriented%20Point%20Clouds%20using%20Neural%20Variational%20Heat%20Distances&entry.906535625=Samuel%20Weidemaier%20and%20Florine%20Hartwig%20and%20Josua%20Sassen%20and%20Sergio%20Conti%20and%20Mirela%20Ben-Chen%20and%20Martin%20Rumpf&entry.1292438233=We%20propose%20a%20novel%20variational%20approach%20for%20computing%20neural%20Signed%20Distance%20Fields%20%28SDF%29%20from%20unoriented%20point%20clouds.%20To%20this%20end%2C%20we%20replace%20the%20commonly%20used%20eikonal%20equation%20with%20the%20heat%20method%2C%20carrying%20over%20to%20the%20neural%20domain%20what%20has%20long%20been%20standard%20practice%20for%20computing%20distances%20on%20discrete%20surfaces.%20This%20yields%20two%20convex%20optimization%20problems%20for%20whose%20solution%20we%20employ%20neural%20networks%3A%20We%20first%20compute%20a%20neural%20approximation%20of%20the%20gradients%20of%20the%20unsigned%20distance%20field%20through%20a%20small%20time%20step%20of%20heat%20flow%20with%20weighted%20point%20cloud%20densities%20as%20initial%20data.%20Then%20we%20use%20it%20to%20compute%20a%20neural%20approximation%20of%20the%20SDF.%20We%20prove%20that%20the%20underlying%20variational%20problems%20are%20well-posed.%20Through%20numerical%20experiments%2C%20we%20demonstrate%20that%20our%20method%20provides%20state-of-the-art%20surface%20reconstruction%20and%20consistent%20SDF%20gradients.%20Furthermore%2C%20we%20show%20in%20a%20proof-of-concept%20that%20it%20is%20accurate%20enough%20for%20solving%20a%20PDE%20on%20the%20zero-level%20set.&entry.1838667208=http%3A//arxiv.org/abs/2504.11212v3&entry.124074799=Read"},
{"title": "AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture", "author": "Yibin Wen and Qingmei Li and Zi Ye and Jiarui Zhang and Jing Wu and Zurong Mai and Shuohong Lou and Yuhang Chen and Henglian Huang and Xiaoya Fan and Yang Zhang and Lingyuan Zhao and Haohuan Fu and Huang Jianxi and Juepeng Zheng", "abstract": "Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.", "link": "http://arxiv.org/abs/2511.23253v1", "date": "2025-11-28", "relevancy": 2.5685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgriCoT%3A%20A%20Chain-of-Thought%20Benchmark%20for%20Evaluating%20Reasoning%20in%20Vision-Language%20Models%20for%20Agriculture&body=Title%3A%20AgriCoT%3A%20A%20Chain-of-Thought%20Benchmark%20for%20Evaluating%20Reasoning%20in%20Vision-Language%20Models%20for%20Agriculture%0AAuthor%3A%20Yibin%20Wen%20and%20Qingmei%20Li%20and%20Zi%20Ye%20and%20Jiarui%20Zhang%20and%20Jing%20Wu%20and%20Zurong%20Mai%20and%20Shuohong%20Lou%20and%20Yuhang%20Chen%20and%20Henglian%20Huang%20and%20Xiaoya%20Fan%20and%20Yang%20Zhang%20and%20Lingyuan%20Zhao%20and%20Haohuan%20Fu%20and%20Huang%20Jianxi%20and%20Juepeng%20Zheng%0AAbstract%3A%20Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20significantly%20transformed%20various%20industries.%20In%20agriculture%2C%20these%20dual-modal%20capabilities%20offer%20promising%20applications%20such%20as%20precision%20farming%2C%20crop%20monitoring%2C%20pest%20detection%2C%20and%20environmental%20sustainability.%20While%20several%20Visual%20Question%20Answering%20%28VQA%29%20datasets%20and%20benchmarks%20have%20been%20developed%20to%20evaluate%20VLM%20performance%2C%20they%20often%20fail%20to%20adequately%20assess%20the%20critical%20reasoning%20and%20problem-solving%20skills%20required%20in%20complex%20agricultural%20contexts.%20To%20address%20this%20gap%2C%20we%20introduce%20AgriCoT%2C%20a%20VQA%20dataset%20that%20incorporates%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20specifically%20designed%20to%20evaluate%20the%20reasoning%20capabilities%20of%20VLMs.%20With%204%2C535%20carefully%20curated%20samples%2C%20AgriCoT%20offers%20a%20comprehensive%20and%20robust%20evaluation%20of%20reasoning%20abilities%20for%20VLMs%2C%20particularly%20in%20zero-shot%20scenarios%2C%20by%20focusing%20on%20their%20capacity%20to%20engage%20in%20logical%20reasoning%20and%20effective%20problem-solving.%20Our%20evaluations%2C%20conducted%20with%2026%20representative%20VLMs%2C%20including%20both%20proprietary%20and%20open-source%20models%2C%20reveal%20that%20while%20some%20proprietary%20models%20excel%20at%20answering%20questions%2C%20there%20is%20a%20notable%20and%20significant%20gap%20in%20their%20reasoning%20capabilities.%20This%20underscores%20the%20importance%20of%20incorporating%20CoT%20for%20more%20precise%20and%20effective%20assessments.%20Our%20dataset%20are%20available%20at%20https%3A//huggingface.co/datasets/wenyb/AgriCoT.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgriCoT%253A%2520A%2520Chain-of-Thought%2520Benchmark%2520for%2520Evaluating%2520Reasoning%2520in%2520Vision-Language%2520Models%2520for%2520Agriculture%26entry.906535625%3DYibin%2520Wen%2520and%2520Qingmei%2520Li%2520and%2520Zi%2520Ye%2520and%2520Jiarui%2520Zhang%2520and%2520Jing%2520Wu%2520and%2520Zurong%2520Mai%2520and%2520Shuohong%2520Lou%2520and%2520Yuhang%2520Chen%2520and%2520Henglian%2520Huang%2520and%2520Xiaoya%2520Fan%2520and%2520Yang%2520Zhang%2520and%2520Lingyuan%2520Zhao%2520and%2520Haohuan%2520Fu%2520and%2520Huang%2520Jianxi%2520and%2520Juepeng%2520Zheng%26entry.1292438233%3DRecent%2520advancements%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520significantly%2520transformed%2520various%2520industries.%2520In%2520agriculture%252C%2520these%2520dual-modal%2520capabilities%2520offer%2520promising%2520applications%2520such%2520as%2520precision%2520farming%252C%2520crop%2520monitoring%252C%2520pest%2520detection%252C%2520and%2520environmental%2520sustainability.%2520While%2520several%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520datasets%2520and%2520benchmarks%2520have%2520been%2520developed%2520to%2520evaluate%2520VLM%2520performance%252C%2520they%2520often%2520fail%2520to%2520adequately%2520assess%2520the%2520critical%2520reasoning%2520and%2520problem-solving%2520skills%2520required%2520in%2520complex%2520agricultural%2520contexts.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520AgriCoT%252C%2520a%2520VQA%2520dataset%2520that%2520incorporates%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%252C%2520specifically%2520designed%2520to%2520evaluate%2520the%2520reasoning%2520capabilities%2520of%2520VLMs.%2520With%25204%252C535%2520carefully%2520curated%2520samples%252C%2520AgriCoT%2520offers%2520a%2520comprehensive%2520and%2520robust%2520evaluation%2520of%2520reasoning%2520abilities%2520for%2520VLMs%252C%2520particularly%2520in%2520zero-shot%2520scenarios%252C%2520by%2520focusing%2520on%2520their%2520capacity%2520to%2520engage%2520in%2520logical%2520reasoning%2520and%2520effective%2520problem-solving.%2520Our%2520evaluations%252C%2520conducted%2520with%252026%2520representative%2520VLMs%252C%2520including%2520both%2520proprietary%2520and%2520open-source%2520models%252C%2520reveal%2520that%2520while%2520some%2520proprietary%2520models%2520excel%2520at%2520answering%2520questions%252C%2520there%2520is%2520a%2520notable%2520and%2520significant%2520gap%2520in%2520their%2520reasoning%2520capabilities.%2520This%2520underscores%2520the%2520importance%2520of%2520incorporating%2520CoT%2520for%2520more%2520precise%2520and%2520effective%2520assessments.%2520Our%2520dataset%2520are%2520available%2520at%2520https%253A//huggingface.co/datasets/wenyb/AgriCoT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgriCoT%3A%20A%20Chain-of-Thought%20Benchmark%20for%20Evaluating%20Reasoning%20in%20Vision-Language%20Models%20for%20Agriculture&entry.906535625=Yibin%20Wen%20and%20Qingmei%20Li%20and%20Zi%20Ye%20and%20Jiarui%20Zhang%20and%20Jing%20Wu%20and%20Zurong%20Mai%20and%20Shuohong%20Lou%20and%20Yuhang%20Chen%20and%20Henglian%20Huang%20and%20Xiaoya%20Fan%20and%20Yang%20Zhang%20and%20Lingyuan%20Zhao%20and%20Haohuan%20Fu%20and%20Huang%20Jianxi%20and%20Juepeng%20Zheng&entry.1292438233=Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20significantly%20transformed%20various%20industries.%20In%20agriculture%2C%20these%20dual-modal%20capabilities%20offer%20promising%20applications%20such%20as%20precision%20farming%2C%20crop%20monitoring%2C%20pest%20detection%2C%20and%20environmental%20sustainability.%20While%20several%20Visual%20Question%20Answering%20%28VQA%29%20datasets%20and%20benchmarks%20have%20been%20developed%20to%20evaluate%20VLM%20performance%2C%20they%20often%20fail%20to%20adequately%20assess%20the%20critical%20reasoning%20and%20problem-solving%20skills%20required%20in%20complex%20agricultural%20contexts.%20To%20address%20this%20gap%2C%20we%20introduce%20AgriCoT%2C%20a%20VQA%20dataset%20that%20incorporates%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20specifically%20designed%20to%20evaluate%20the%20reasoning%20capabilities%20of%20VLMs.%20With%204%2C535%20carefully%20curated%20samples%2C%20AgriCoT%20offers%20a%20comprehensive%20and%20robust%20evaluation%20of%20reasoning%20abilities%20for%20VLMs%2C%20particularly%20in%20zero-shot%20scenarios%2C%20by%20focusing%20on%20their%20capacity%20to%20engage%20in%20logical%20reasoning%20and%20effective%20problem-solving.%20Our%20evaluations%2C%20conducted%20with%2026%20representative%20VLMs%2C%20including%20both%20proprietary%20and%20open-source%20models%2C%20reveal%20that%20while%20some%20proprietary%20models%20excel%20at%20answering%20questions%2C%20there%20is%20a%20notable%20and%20significant%20gap%20in%20their%20reasoning%20capabilities.%20This%20underscores%20the%20importance%20of%20incorporating%20CoT%20for%20more%20precise%20and%20effective%20assessments.%20Our%20dataset%20are%20available%20at%20https%3A//huggingface.co/datasets/wenyb/AgriCoT.&entry.1838667208=http%3A//arxiv.org/abs/2511.23253v1&entry.124074799=Read"},
{"title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation", "author": "Hongfei Zhang and Kanghao Chen and Zixin Zhang and Harold Haodong Chen and Yuanhuiyi Lyu and Yuqi Zhang and Shuai Yang and Kun Zhou and Yingcong Chen", "abstract": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl\\-page/", "link": "http://arxiv.org/abs/2511.23127v1", "date": "2025-11-28", "relevancy": 2.5647, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7252}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6312}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualCamCtrl%3A%20Dual-Branch%20Diffusion%20Model%20for%20Geometry-Aware%20Camera-Controlled%20Video%20Generation&body=Title%3A%20DualCamCtrl%3A%20Dual-Branch%20Diffusion%20Model%20for%20Geometry-Aware%20Camera-Controlled%20Video%20Generation%0AAuthor%3A%20Hongfei%20Zhang%20and%20Kanghao%20Chen%20and%20Zixin%20Zhang%20and%20Harold%20Haodong%20Chen%20and%20Yuanhuiyi%20Lyu%20and%20Yuqi%20Zhang%20and%20Shuai%20Yang%20and%20Kun%20Zhou%20and%20Yingcong%20Chen%0AAbstract%3A%20This%20paper%20presents%20DualCamCtrl%2C%20a%20novel%20end-to-end%20diffusion%20model%20for%20camera-controlled%20video%20generation.%20Recent%20works%20have%20advanced%20this%20field%20by%20representing%20camera%20poses%20as%20ray-based%20conditions%2C%20yet%20they%20often%20lack%20sufficient%20scene%20understanding%20and%20geometric%20awareness.%20DualCamCtrl%20specifically%20targets%20this%20limitation%20by%20introducing%20a%20dual-branch%20framework%20that%20mutually%20generates%20camera-consistent%20RGB%20and%20depth%20sequences.%20To%20harmonize%20these%20two%20modalities%2C%20we%20further%20propose%20the%20Semantic%20Guided%20Mutual%20Alignment%20%28SIGMA%29%20mechanism%2C%20which%20performs%20RGB-depth%20fusion%20in%20a%20semantics-guided%20and%20mutually%20reinforced%20manner.%20These%20designs%20collectively%20enable%20DualCamCtrl%20to%20better%20disentangle%20appearance%20and%20geometry%20modeling%2C%20generating%20videos%20that%20more%20faithfully%20adhere%20to%20the%20specified%20camera%20trajectories.%20Additionally%2C%20we%20analyze%20and%20reveal%20the%20distinct%20influence%20of%20depth%20and%20camera%20poses%20across%20denoising%20stages%20and%20further%20demonstrate%20that%20early%20and%20late%20stages%20play%20complementary%20roles%20in%20forming%20global%20structure%20and%20refining%20local%20details.%20Extensive%20experiments%20demonstrate%20that%20DualCamCtrl%20achieves%20more%20consistent%20camera-controlled%20video%20generation%2C%20with%20over%2040%5C%25%20reduction%20in%20camera%20motion%20errors%20compared%20with%20prior%20methods.%20Our%20project%20page%3A%20https%3A//soyouthinkyoucantell.github.io/dualcamctrl%5C-page/%0ALink%3A%20http%3A//arxiv.org/abs/2511.23127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualCamCtrl%253A%2520Dual-Branch%2520Diffusion%2520Model%2520for%2520Geometry-Aware%2520Camera-Controlled%2520Video%2520Generation%26entry.906535625%3DHongfei%2520Zhang%2520and%2520Kanghao%2520Chen%2520and%2520Zixin%2520Zhang%2520and%2520Harold%2520Haodong%2520Chen%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Yuqi%2520Zhang%2520and%2520Shuai%2520Yang%2520and%2520Kun%2520Zhou%2520and%2520Yingcong%2520Chen%26entry.1292438233%3DThis%2520paper%2520presents%2520DualCamCtrl%252C%2520a%2520novel%2520end-to-end%2520diffusion%2520model%2520for%2520camera-controlled%2520video%2520generation.%2520Recent%2520works%2520have%2520advanced%2520this%2520field%2520by%2520representing%2520camera%2520poses%2520as%2520ray-based%2520conditions%252C%2520yet%2520they%2520often%2520lack%2520sufficient%2520scene%2520understanding%2520and%2520geometric%2520awareness.%2520DualCamCtrl%2520specifically%2520targets%2520this%2520limitation%2520by%2520introducing%2520a%2520dual-branch%2520framework%2520that%2520mutually%2520generates%2520camera-consistent%2520RGB%2520and%2520depth%2520sequences.%2520To%2520harmonize%2520these%2520two%2520modalities%252C%2520we%2520further%2520propose%2520the%2520Semantic%2520Guided%2520Mutual%2520Alignment%2520%2528SIGMA%2529%2520mechanism%252C%2520which%2520performs%2520RGB-depth%2520fusion%2520in%2520a%2520semantics-guided%2520and%2520mutually%2520reinforced%2520manner.%2520These%2520designs%2520collectively%2520enable%2520DualCamCtrl%2520to%2520better%2520disentangle%2520appearance%2520and%2520geometry%2520modeling%252C%2520generating%2520videos%2520that%2520more%2520faithfully%2520adhere%2520to%2520the%2520specified%2520camera%2520trajectories.%2520Additionally%252C%2520we%2520analyze%2520and%2520reveal%2520the%2520distinct%2520influence%2520of%2520depth%2520and%2520camera%2520poses%2520across%2520denoising%2520stages%2520and%2520further%2520demonstrate%2520that%2520early%2520and%2520late%2520stages%2520play%2520complementary%2520roles%2520in%2520forming%2520global%2520structure%2520and%2520refining%2520local%2520details.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DualCamCtrl%2520achieves%2520more%2520consistent%2520camera-controlled%2520video%2520generation%252C%2520with%2520over%252040%255C%2525%2520reduction%2520in%2520camera%2520motion%2520errors%2520compared%2520with%2520prior%2520methods.%2520Our%2520project%2520page%253A%2520https%253A//soyouthinkyoucantell.github.io/dualcamctrl%255C-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualCamCtrl%3A%20Dual-Branch%20Diffusion%20Model%20for%20Geometry-Aware%20Camera-Controlled%20Video%20Generation&entry.906535625=Hongfei%20Zhang%20and%20Kanghao%20Chen%20and%20Zixin%20Zhang%20and%20Harold%20Haodong%20Chen%20and%20Yuanhuiyi%20Lyu%20and%20Yuqi%20Zhang%20and%20Shuai%20Yang%20and%20Kun%20Zhou%20and%20Yingcong%20Chen&entry.1292438233=This%20paper%20presents%20DualCamCtrl%2C%20a%20novel%20end-to-end%20diffusion%20model%20for%20camera-controlled%20video%20generation.%20Recent%20works%20have%20advanced%20this%20field%20by%20representing%20camera%20poses%20as%20ray-based%20conditions%2C%20yet%20they%20often%20lack%20sufficient%20scene%20understanding%20and%20geometric%20awareness.%20DualCamCtrl%20specifically%20targets%20this%20limitation%20by%20introducing%20a%20dual-branch%20framework%20that%20mutually%20generates%20camera-consistent%20RGB%20and%20depth%20sequences.%20To%20harmonize%20these%20two%20modalities%2C%20we%20further%20propose%20the%20Semantic%20Guided%20Mutual%20Alignment%20%28SIGMA%29%20mechanism%2C%20which%20performs%20RGB-depth%20fusion%20in%20a%20semantics-guided%20and%20mutually%20reinforced%20manner.%20These%20designs%20collectively%20enable%20DualCamCtrl%20to%20better%20disentangle%20appearance%20and%20geometry%20modeling%2C%20generating%20videos%20that%20more%20faithfully%20adhere%20to%20the%20specified%20camera%20trajectories.%20Additionally%2C%20we%20analyze%20and%20reveal%20the%20distinct%20influence%20of%20depth%20and%20camera%20poses%20across%20denoising%20stages%20and%20further%20demonstrate%20that%20early%20and%20late%20stages%20play%20complementary%20roles%20in%20forming%20global%20structure%20and%20refining%20local%20details.%20Extensive%20experiments%20demonstrate%20that%20DualCamCtrl%20achieves%20more%20consistent%20camera-controlled%20video%20generation%2C%20with%20over%2040%5C%25%20reduction%20in%20camera%20motion%20errors%20compared%20with%20prior%20methods.%20Our%20project%20page%3A%20https%3A//soyouthinkyoucantell.github.io/dualcamctrl%5C-page/&entry.1838667208=http%3A//arxiv.org/abs/2511.23127v1&entry.124074799=Read"},
{"title": "OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning", "author": "Timothy Ossowski and Sheng Zhang and Qianchu Liu and Guanghui Qin and Reuben Tan and Tristan Naumann and Junjie Hu and Hoifung Poon", "abstract": "High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.", "link": "http://arxiv.org/abs/2511.23269v1", "date": "2025-11-28", "relevancy": 2.5644, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OctoMed%3A%20Data%20Recipes%20for%20State-of-the-Art%20Multimodal%20Medical%20Reasoning&body=Title%3A%20OctoMed%3A%20Data%20Recipes%20for%20State-of-the-Art%20Multimodal%20Medical%20Reasoning%0AAuthor%3A%20Timothy%20Ossowski%20and%20Sheng%20Zhang%20and%20Qianchu%20Liu%20and%20Guanghui%20Qin%20and%20Reuben%20Tan%20and%20Tristan%20Naumann%20and%20Junjie%20Hu%20and%20Hoifung%20Poon%0AAbstract%3A%20High-quality%20and%20carefully%20curated%20data%20is%20a%20cornerstone%20of%20training%20medical%20large%20language%20models%2C%20as%20it%20directly%20impacts%20both%20generalization%20and%20robustness%20to%20unseen%20clinical%20tasks.%20We%20investigate%20strategies%20for%20training%20and%20data%20curation%20to%20develop%20a%20robust%20multimodal%20reasoning%20model%20in%20the%20medical%20domain.%20Our%20work%20focuses%20on%20supervised%20fine-tuning%20%28SFT%29%20and%20explores%20data%20recipes%20that%20leverage%20structured%20reasoning%20traces.%20Using%20our%20proposed%20data%20recipe%2C%20we%20scale%20experiments%20to%20a%20dataset%20of%20over%208%20million%20examples%20and%206.8%20billion%20response%20tokens%2C%20achieving%20state-of-the-art%20performance%20among%20open-source%20models%20across%20diverse%20out-of-distribution%20medical%20benchmark%20tasks.%20Our%20results%20further%20indicate%20that%20curating%20a%20high-quality%2C%20diverse%20training%20dataset%20with%20varying%20structured%20reasoning%20trace%20lengths%20enables%20the%20fine-tuned%20model%20to%20self-calibrate%20its%20reasoning%20trajectory%20lengths%20based%20on%20the%20downstream%20task%2C%20without%20explicit%20supervision.%20We%20present%20key%20insights%2C%20describe%20the%20data%20curation%20strategy%2C%20and%20outline%20next%20steps%20toward%20developing%20robust%20medical%20vision-language%20reasoning%20system.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOctoMed%253A%2520Data%2520Recipes%2520for%2520State-of-the-Art%2520Multimodal%2520Medical%2520Reasoning%26entry.906535625%3DTimothy%2520Ossowski%2520and%2520Sheng%2520Zhang%2520and%2520Qianchu%2520Liu%2520and%2520Guanghui%2520Qin%2520and%2520Reuben%2520Tan%2520and%2520Tristan%2520Naumann%2520and%2520Junjie%2520Hu%2520and%2520Hoifung%2520Poon%26entry.1292438233%3DHigh-quality%2520and%2520carefully%2520curated%2520data%2520is%2520a%2520cornerstone%2520of%2520training%2520medical%2520large%2520language%2520models%252C%2520as%2520it%2520directly%2520impacts%2520both%2520generalization%2520and%2520robustness%2520to%2520unseen%2520clinical%2520tasks.%2520We%2520investigate%2520strategies%2520for%2520training%2520and%2520data%2520curation%2520to%2520develop%2520a%2520robust%2520multimodal%2520reasoning%2520model%2520in%2520the%2520medical%2520domain.%2520Our%2520work%2520focuses%2520on%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520explores%2520data%2520recipes%2520that%2520leverage%2520structured%2520reasoning%2520traces.%2520Using%2520our%2520proposed%2520data%2520recipe%252C%2520we%2520scale%2520experiments%2520to%2520a%2520dataset%2520of%2520over%25208%2520million%2520examples%2520and%25206.8%2520billion%2520response%2520tokens%252C%2520achieving%2520state-of-the-art%2520performance%2520among%2520open-source%2520models%2520across%2520diverse%2520out-of-distribution%2520medical%2520benchmark%2520tasks.%2520Our%2520results%2520further%2520indicate%2520that%2520curating%2520a%2520high-quality%252C%2520diverse%2520training%2520dataset%2520with%2520varying%2520structured%2520reasoning%2520trace%2520lengths%2520enables%2520the%2520fine-tuned%2520model%2520to%2520self-calibrate%2520its%2520reasoning%2520trajectory%2520lengths%2520based%2520on%2520the%2520downstream%2520task%252C%2520without%2520explicit%2520supervision.%2520We%2520present%2520key%2520insights%252C%2520describe%2520the%2520data%2520curation%2520strategy%252C%2520and%2520outline%2520next%2520steps%2520toward%2520developing%2520robust%2520medical%2520vision-language%2520reasoning%2520system.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OctoMed%3A%20Data%20Recipes%20for%20State-of-the-Art%20Multimodal%20Medical%20Reasoning&entry.906535625=Timothy%20Ossowski%20and%20Sheng%20Zhang%20and%20Qianchu%20Liu%20and%20Guanghui%20Qin%20and%20Reuben%20Tan%20and%20Tristan%20Naumann%20and%20Junjie%20Hu%20and%20Hoifung%20Poon&entry.1292438233=High-quality%20and%20carefully%20curated%20data%20is%20a%20cornerstone%20of%20training%20medical%20large%20language%20models%2C%20as%20it%20directly%20impacts%20both%20generalization%20and%20robustness%20to%20unseen%20clinical%20tasks.%20We%20investigate%20strategies%20for%20training%20and%20data%20curation%20to%20develop%20a%20robust%20multimodal%20reasoning%20model%20in%20the%20medical%20domain.%20Our%20work%20focuses%20on%20supervised%20fine-tuning%20%28SFT%29%20and%20explores%20data%20recipes%20that%20leverage%20structured%20reasoning%20traces.%20Using%20our%20proposed%20data%20recipe%2C%20we%20scale%20experiments%20to%20a%20dataset%20of%20over%208%20million%20examples%20and%206.8%20billion%20response%20tokens%2C%20achieving%20state-of-the-art%20performance%20among%20open-source%20models%20across%20diverse%20out-of-distribution%20medical%20benchmark%20tasks.%20Our%20results%20further%20indicate%20that%20curating%20a%20high-quality%2C%20diverse%20training%20dataset%20with%20varying%20structured%20reasoning%20trace%20lengths%20enables%20the%20fine-tuned%20model%20to%20self-calibrate%20its%20reasoning%20trajectory%20lengths%20based%20on%20the%20downstream%20task%2C%20without%20explicit%20supervision.%20We%20present%20key%20insights%2C%20describe%20the%20data%20curation%20strategy%2C%20and%20outline%20next%20steps%20toward%20developing%20robust%20medical%20vision-language%20reasoning%20system.&entry.1838667208=http%3A//arxiv.org/abs/2511.23269v1&entry.124074799=Read"},
{"title": "Progressive Localisation in Localist LLMs", "author": "Joachim Diederich", "abstract": "This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate seven locality configurations ranging from fully distributed to strictly localist, with five progressive schedules implementing polynomial increases (linear through quintic). We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.", "link": "http://arxiv.org/abs/2511.18375v2", "date": "2025-11-28", "relevancy": 2.5512, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.515}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Localisation%20in%20Localist%20LLMs&body=Title%3A%20Progressive%20Localisation%20in%20Localist%20LLMs%0AAuthor%3A%20Joachim%20Diederich%0AAbstract%3A%20This%20paper%20demonstrates%20that%20progressive%20localization%2C%20the%20gradual%20increase%20of%20attention%20locality%20from%20early%20distributed%20layers%20to%20late%20localized%20layers%2C%20represents%20the%20optimal%20architecture%20for%20creating%20interpretable%20large%20language%20models%20%28LLMs%29%20while%20preserving%20performance.%20Through%20systematic%20experimentation%20with%20GPT-2%20fine-tuned%20on%20The%20Psychology%20of%20Artificial%20Superintelligence%2C%20we%20evaluate%20seven%20locality%20configurations%20ranging%20from%20fully%20distributed%20to%20strictly%20localist%2C%20with%20five%20progressive%20schedules%20implementing%20polynomial%20increases%20%28linear%20through%20quintic%29.%20We%20investigate%20whether%20interpretability%20constraints%20can%20be%20aligned%20with%20natural%20semantic%20structure%20while%20being%20applied%20strategically%20across%20network%20depth.%20We%20demonstrate%20that%20progressive%20semantic%20localization%2C%20combining%20adaptive%20semantic%20block%20partitioning%20with%20steep%20polynomial%20locality%20schedules%2C%20achieves%20near-baseline%20language%20modeling%20performance%20while%20providing%20interpretable%20attention%20patterns.%20Multiple%20independent%20training%20runs%20with%20different%20random%20seeds%20establish%20that%20results%20are%20statistically%20robust%20and%20highly%20reproducible.%20The%20approach%20dramatically%20outperforms%20both%20fixed-window%20localization%20and%20naive%20uniform%20locality%20constraints.%20Analysis%20reveals%20that%20maintaining%20flexibility%20through%20low-fidelity%20constraints%20preserves%20model%20capacity%20while%20providing%20interpretability%20benefits%2C%20and%20that%20steep%20schedules%20concentrating%20locality%20in%20decision-critical%20final%20layers%20while%20preserving%20distributed%20learning%20in%20early%20layers%20achieve%20near-baseline%20attention%20distribution%20characteristics.%20These%20findings%20demonstrate%20that%20interpretability%20mechanisms%20should%20align%20with%20semantic%20structure%20to%20achieve%20practical%20performance-interpretability%20tradeoffs%20for%20trustworthy%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18375v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Localisation%2520in%2520Localist%2520LLMs%26entry.906535625%3DJoachim%2520Diederich%26entry.1292438233%3DThis%2520paper%2520demonstrates%2520that%2520progressive%2520localization%252C%2520the%2520gradual%2520increase%2520of%2520attention%2520locality%2520from%2520early%2520distributed%2520layers%2520to%2520late%2520localized%2520layers%252C%2520represents%2520the%2520optimal%2520architecture%2520for%2520creating%2520interpretable%2520large%2520language%2520models%2520%2528LLMs%2529%2520while%2520preserving%2520performance.%2520Through%2520systematic%2520experimentation%2520with%2520GPT-2%2520fine-tuned%2520on%2520The%2520Psychology%2520of%2520Artificial%2520Superintelligence%252C%2520we%2520evaluate%2520seven%2520locality%2520configurations%2520ranging%2520from%2520fully%2520distributed%2520to%2520strictly%2520localist%252C%2520with%2520five%2520progressive%2520schedules%2520implementing%2520polynomial%2520increases%2520%2528linear%2520through%2520quintic%2529.%2520We%2520investigate%2520whether%2520interpretability%2520constraints%2520can%2520be%2520aligned%2520with%2520natural%2520semantic%2520structure%2520while%2520being%2520applied%2520strategically%2520across%2520network%2520depth.%2520We%2520demonstrate%2520that%2520progressive%2520semantic%2520localization%252C%2520combining%2520adaptive%2520semantic%2520block%2520partitioning%2520with%2520steep%2520polynomial%2520locality%2520schedules%252C%2520achieves%2520near-baseline%2520language%2520modeling%2520performance%2520while%2520providing%2520interpretable%2520attention%2520patterns.%2520Multiple%2520independent%2520training%2520runs%2520with%2520different%2520random%2520seeds%2520establish%2520that%2520results%2520are%2520statistically%2520robust%2520and%2520highly%2520reproducible.%2520The%2520approach%2520dramatically%2520outperforms%2520both%2520fixed-window%2520localization%2520and%2520naive%2520uniform%2520locality%2520constraints.%2520Analysis%2520reveals%2520that%2520maintaining%2520flexibility%2520through%2520low-fidelity%2520constraints%2520preserves%2520model%2520capacity%2520while%2520providing%2520interpretability%2520benefits%252C%2520and%2520that%2520steep%2520schedules%2520concentrating%2520locality%2520in%2520decision-critical%2520final%2520layers%2520while%2520preserving%2520distributed%2520learning%2520in%2520early%2520layers%2520achieve%2520near-baseline%2520attention%2520distribution%2520characteristics.%2520These%2520findings%2520demonstrate%2520that%2520interpretability%2520mechanisms%2520should%2520align%2520with%2520semantic%2520structure%2520to%2520achieve%2520practical%2520performance-interpretability%2520tradeoffs%2520for%2520trustworthy%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18375v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Localisation%20in%20Localist%20LLMs&entry.906535625=Joachim%20Diederich&entry.1292438233=This%20paper%20demonstrates%20that%20progressive%20localization%2C%20the%20gradual%20increase%20of%20attention%20locality%20from%20early%20distributed%20layers%20to%20late%20localized%20layers%2C%20represents%20the%20optimal%20architecture%20for%20creating%20interpretable%20large%20language%20models%20%28LLMs%29%20while%20preserving%20performance.%20Through%20systematic%20experimentation%20with%20GPT-2%20fine-tuned%20on%20The%20Psychology%20of%20Artificial%20Superintelligence%2C%20we%20evaluate%20seven%20locality%20configurations%20ranging%20from%20fully%20distributed%20to%20strictly%20localist%2C%20with%20five%20progressive%20schedules%20implementing%20polynomial%20increases%20%28linear%20through%20quintic%29.%20We%20investigate%20whether%20interpretability%20constraints%20can%20be%20aligned%20with%20natural%20semantic%20structure%20while%20being%20applied%20strategically%20across%20network%20depth.%20We%20demonstrate%20that%20progressive%20semantic%20localization%2C%20combining%20adaptive%20semantic%20block%20partitioning%20with%20steep%20polynomial%20locality%20schedules%2C%20achieves%20near-baseline%20language%20modeling%20performance%20while%20providing%20interpretable%20attention%20patterns.%20Multiple%20independent%20training%20runs%20with%20different%20random%20seeds%20establish%20that%20results%20are%20statistically%20robust%20and%20highly%20reproducible.%20The%20approach%20dramatically%20outperforms%20both%20fixed-window%20localization%20and%20naive%20uniform%20locality%20constraints.%20Analysis%20reveals%20that%20maintaining%20flexibility%20through%20low-fidelity%20constraints%20preserves%20model%20capacity%20while%20providing%20interpretability%20benefits%2C%20and%20that%20steep%20schedules%20concentrating%20locality%20in%20decision-critical%20final%20layers%20while%20preserving%20distributed%20learning%20in%20early%20layers%20achieve%20near-baseline%20attention%20distribution%20characteristics.%20These%20findings%20demonstrate%20that%20interpretability%20mechanisms%20should%20align%20with%20semantic%20structure%20to%20achieve%20practical%20performance-interpretability%20tradeoffs%20for%20trustworthy%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.18375v2&entry.124074799=Read"},
{"title": "Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding", "author": "Jin-Seop Lee and SungJoon Lee and SeongJun Jung and Boyang Li and Jee-Hyong Lee", "abstract": "Video Temporal Grounding (VTG) aims to localize a temporal segment in a video corresponding to a natural language query. However, existing VTG models assume that a relevant segment always exists, causing them to always predict a target segment even when the query is irrelevant to the video. While recent approaches attempt to handle irrelevant queries, they can only reject those that are entirely unrelated to the video and still fail to handle hard-irrelevant queries that are semantically similar but not actually relevant. To address this, we propose Refusal-Aware Reinforcement Fine-Tuning (RA-RFT) to effectively refuse hard-irrelevant queries in VTG. Our method is based on the Group Relative Policy Optimization (GRPO) framework and integrates four reward objectives-format, refuse-IoU, explain, and query correction-to improve both relevance discrimination and fine-grained semantic reasoning. In addition, to effectively support RA-RFT, we construct a Hard-Irrelevant VTG (HI-VTG) dataset, which includes hard-irrelevant queries and their refusal answers. We demonstrate the effectiveness of our method across various relevance-aware VTG scenarios, including hard-irrelevant VTG, simply-shuffled RA-VTG, and human-annotated RA-VTG settings. We also show that the proposed method is scalable by applying it to various LVLM-based VTG models. Our code is available at https://github.com/JINSUBY/RA-RFT.", "link": "http://arxiv.org/abs/2511.23151v1", "date": "2025-11-28", "relevancy": 2.535, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5125}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5113}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Refuse%3A%20Refusal-Aware%20Reinforcement%20Fine-Tuning%20for%20Hard-Irrelevant%20Queries%20in%20Video%20Temporal%20Grounding&body=Title%3A%20Learning%20to%20Refuse%3A%20Refusal-Aware%20Reinforcement%20Fine-Tuning%20for%20Hard-Irrelevant%20Queries%20in%20Video%20Temporal%20Grounding%0AAuthor%3A%20Jin-Seop%20Lee%20and%20SungJoon%20Lee%20and%20SeongJun%20Jung%20and%20Boyang%20Li%20and%20Jee-Hyong%20Lee%0AAbstract%3A%20Video%20Temporal%20Grounding%20%28VTG%29%20aims%20to%20localize%20a%20temporal%20segment%20in%20a%20video%20corresponding%20to%20a%20natural%20language%20query.%20However%2C%20existing%20VTG%20models%20assume%20that%20a%20relevant%20segment%20always%20exists%2C%20causing%20them%20to%20always%20predict%20a%20target%20segment%20even%20when%20the%20query%20is%20irrelevant%20to%20the%20video.%20While%20recent%20approaches%20attempt%20to%20handle%20irrelevant%20queries%2C%20they%20can%20only%20reject%20those%20that%20are%20entirely%20unrelated%20to%20the%20video%20and%20still%20fail%20to%20handle%20hard-irrelevant%20queries%20that%20are%20semantically%20similar%20but%20not%20actually%20relevant.%20To%20address%20this%2C%20we%20propose%20Refusal-Aware%20Reinforcement%20Fine-Tuning%20%28RA-RFT%29%20to%20effectively%20refuse%20hard-irrelevant%20queries%20in%20VTG.%20Our%20method%20is%20based%20on%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20framework%20and%20integrates%20four%20reward%20objectives-format%2C%20refuse-IoU%2C%20explain%2C%20and%20query%20correction-to%20improve%20both%20relevance%20discrimination%20and%20fine-grained%20semantic%20reasoning.%20In%20addition%2C%20to%20effectively%20support%20RA-RFT%2C%20we%20construct%20a%20Hard-Irrelevant%20VTG%20%28HI-VTG%29%20dataset%2C%20which%20includes%20hard-irrelevant%20queries%20and%20their%20refusal%20answers.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20across%20various%20relevance-aware%20VTG%20scenarios%2C%20including%20hard-irrelevant%20VTG%2C%20simply-shuffled%20RA-VTG%2C%20and%20human-annotated%20RA-VTG%20settings.%20We%20also%20show%20that%20the%20proposed%20method%20is%20scalable%20by%20applying%20it%20to%20various%20LVLM-based%20VTG%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/JINSUBY/RA-RFT.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Refuse%253A%2520Refusal-Aware%2520Reinforcement%2520Fine-Tuning%2520for%2520Hard-Irrelevant%2520Queries%2520in%2520Video%2520Temporal%2520Grounding%26entry.906535625%3DJin-Seop%2520Lee%2520and%2520SungJoon%2520Lee%2520and%2520SeongJun%2520Jung%2520and%2520Boyang%2520Li%2520and%2520Jee-Hyong%2520Lee%26entry.1292438233%3DVideo%2520Temporal%2520Grounding%2520%2528VTG%2529%2520aims%2520to%2520localize%2520a%2520temporal%2520segment%2520in%2520a%2520video%2520corresponding%2520to%2520a%2520natural%2520language%2520query.%2520However%252C%2520existing%2520VTG%2520models%2520assume%2520that%2520a%2520relevant%2520segment%2520always%2520exists%252C%2520causing%2520them%2520to%2520always%2520predict%2520a%2520target%2520segment%2520even%2520when%2520the%2520query%2520is%2520irrelevant%2520to%2520the%2520video.%2520While%2520recent%2520approaches%2520attempt%2520to%2520handle%2520irrelevant%2520queries%252C%2520they%2520can%2520only%2520reject%2520those%2520that%2520are%2520entirely%2520unrelated%2520to%2520the%2520video%2520and%2520still%2520fail%2520to%2520handle%2520hard-irrelevant%2520queries%2520that%2520are%2520semantically%2520similar%2520but%2520not%2520actually%2520relevant.%2520To%2520address%2520this%252C%2520we%2520propose%2520Refusal-Aware%2520Reinforcement%2520Fine-Tuning%2520%2528RA-RFT%2529%2520to%2520effectively%2520refuse%2520hard-irrelevant%2520queries%2520in%2520VTG.%2520Our%2520method%2520is%2520based%2520on%2520the%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520framework%2520and%2520integrates%2520four%2520reward%2520objectives-format%252C%2520refuse-IoU%252C%2520explain%252C%2520and%2520query%2520correction-to%2520improve%2520both%2520relevance%2520discrimination%2520and%2520fine-grained%2520semantic%2520reasoning.%2520In%2520addition%252C%2520to%2520effectively%2520support%2520RA-RFT%252C%2520we%2520construct%2520a%2520Hard-Irrelevant%2520VTG%2520%2528HI-VTG%2529%2520dataset%252C%2520which%2520includes%2520hard-irrelevant%2520queries%2520and%2520their%2520refusal%2520answers.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520across%2520various%2520relevance-aware%2520VTG%2520scenarios%252C%2520including%2520hard-irrelevant%2520VTG%252C%2520simply-shuffled%2520RA-VTG%252C%2520and%2520human-annotated%2520RA-VTG%2520settings.%2520We%2520also%2520show%2520that%2520the%2520proposed%2520method%2520is%2520scalable%2520by%2520applying%2520it%2520to%2520various%2520LVLM-based%2520VTG%2520models.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/JINSUBY/RA-RFT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Refuse%3A%20Refusal-Aware%20Reinforcement%20Fine-Tuning%20for%20Hard-Irrelevant%20Queries%20in%20Video%20Temporal%20Grounding&entry.906535625=Jin-Seop%20Lee%20and%20SungJoon%20Lee%20and%20SeongJun%20Jung%20and%20Boyang%20Li%20and%20Jee-Hyong%20Lee&entry.1292438233=Video%20Temporal%20Grounding%20%28VTG%29%20aims%20to%20localize%20a%20temporal%20segment%20in%20a%20video%20corresponding%20to%20a%20natural%20language%20query.%20However%2C%20existing%20VTG%20models%20assume%20that%20a%20relevant%20segment%20always%20exists%2C%20causing%20them%20to%20always%20predict%20a%20target%20segment%20even%20when%20the%20query%20is%20irrelevant%20to%20the%20video.%20While%20recent%20approaches%20attempt%20to%20handle%20irrelevant%20queries%2C%20they%20can%20only%20reject%20those%20that%20are%20entirely%20unrelated%20to%20the%20video%20and%20still%20fail%20to%20handle%20hard-irrelevant%20queries%20that%20are%20semantically%20similar%20but%20not%20actually%20relevant.%20To%20address%20this%2C%20we%20propose%20Refusal-Aware%20Reinforcement%20Fine-Tuning%20%28RA-RFT%29%20to%20effectively%20refuse%20hard-irrelevant%20queries%20in%20VTG.%20Our%20method%20is%20based%20on%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20framework%20and%20integrates%20four%20reward%20objectives-format%2C%20refuse-IoU%2C%20explain%2C%20and%20query%20correction-to%20improve%20both%20relevance%20discrimination%20and%20fine-grained%20semantic%20reasoning.%20In%20addition%2C%20to%20effectively%20support%20RA-RFT%2C%20we%20construct%20a%20Hard-Irrelevant%20VTG%20%28HI-VTG%29%20dataset%2C%20which%20includes%20hard-irrelevant%20queries%20and%20their%20refusal%20answers.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20across%20various%20relevance-aware%20VTG%20scenarios%2C%20including%20hard-irrelevant%20VTG%2C%20simply-shuffled%20RA-VTG%2C%20and%20human-annotated%20RA-VTG%20settings.%20We%20also%20show%20that%20the%20proposed%20method%20is%20scalable%20by%20applying%20it%20to%20various%20LVLM-based%20VTG%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/JINSUBY/RA-RFT.&entry.1838667208=http%3A//arxiv.org/abs/2511.23151v1&entry.124074799=Read"},
{"title": "DisMo: Disentangled Motion Representations for Open-World Motion Transfer", "author": "Thomas Ressler-Antal and Frank Fundel and Malek Ben Alaya and Stefan Andreas Baumann and Felix Krause and Ming Gui and Bj\u00f6rn Ommer", "abstract": "Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo", "link": "http://arxiv.org/abs/2511.23428v1", "date": "2025-11-28", "relevancy": 2.5154, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.661}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6278}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisMo%3A%20Disentangled%20Motion%20Representations%20for%20Open-World%20Motion%20Transfer&body=Title%3A%20DisMo%3A%20Disentangled%20Motion%20Representations%20for%20Open-World%20Motion%20Transfer%0AAuthor%3A%20Thomas%20Ressler-Antal%20and%20Frank%20Fundel%20and%20Malek%20Ben%20Alaya%20and%20Stefan%20Andreas%20Baumann%20and%20Felix%20Krause%20and%20Ming%20Gui%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20and%20image-to-video%20%28I2V%29%20models%2C%20have%20enabled%20the%20creation%20of%20visually%20compelling%20and%20dynamic%20videos%20from%20simple%20textual%20descriptions%20or%20initial%20frames.%20However%2C%20these%20models%20often%20fail%20to%20provide%20an%20explicit%20representation%20of%20motion%20separate%20from%20content%2C%20limiting%20their%20applicability%20for%20content%20creators.%20To%20address%20this%20gap%2C%20we%20propose%20DisMo%2C%20a%20novel%20paradigm%20for%20learning%20abstract%20motion%20representations%20directly%20from%20raw%20video%20data%20via%20an%20image-space%20reconstruction%20objective.%20Our%20representation%20is%20generic%20and%20independent%20of%20static%20information%20such%20as%20appearance%2C%20object%20identity%2C%20or%20pose.%20This%20enables%20open-world%20motion%20transfer%2C%20allowing%20motion%20to%20be%20transferred%20across%20semantically%20unrelated%20entities%20without%20requiring%20object%20correspondences%2C%20even%20between%20vastly%20different%20categories.%20Unlike%20prior%20methods%2C%20which%20trade%20off%20motion%20fidelity%20and%20prompt%20adherence%2C%20are%20overfitting%20to%20source%20structure%20or%20drifting%20from%20the%20described%20action%2C%20our%20approach%20disentangles%20motion%20semantics%20from%20appearance%2C%20enabling%20accurate%20transfer%20and%20faithful%20conditioning.%20Furthermore%2C%20our%20motion%20representation%20can%20be%20combined%20with%20any%20existing%20video%20generator%20via%20lightweight%20adapters%2C%20allowing%20us%20to%20effortlessly%20benefit%20from%20future%20advancements%20in%20video%20models.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%20a%20diverse%20set%20of%20motion%20transfer%20tasks.%20Finally%2C%20we%20show%20that%20the%20learned%20representations%20are%20well-suited%20for%20downstream%20motion%20understanding%20tasks%2C%20consistently%20outperforming%20state-of-the-art%20video%20representation%20models%20such%20as%20V-JEPA%20in%20zero-shot%20action%20classification%20on%20benchmarks%20including%20Something-Something%20v2%20and%20Jester.%20Project%20page%3A%20https%3A//compvis.github.io/DisMo%0ALink%3A%20http%3A//arxiv.org/abs/2511.23428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisMo%253A%2520Disentangled%2520Motion%2520Representations%2520for%2520Open-World%2520Motion%2520Transfer%26entry.906535625%3DThomas%2520Ressler-Antal%2520and%2520Frank%2520Fundel%2520and%2520Malek%2520Ben%2520Alaya%2520and%2520Stefan%2520Andreas%2520Baumann%2520and%2520Felix%2520Krause%2520and%2520Ming%2520Gui%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3DRecent%2520advances%2520in%2520text-to-video%2520%2528T2V%2529%2520and%2520image-to-video%2520%2528I2V%2529%2520models%252C%2520have%2520enabled%2520the%2520creation%2520of%2520visually%2520compelling%2520and%2520dynamic%2520videos%2520from%2520simple%2520textual%2520descriptions%2520or%2520initial%2520frames.%2520However%252C%2520these%2520models%2520often%2520fail%2520to%2520provide%2520an%2520explicit%2520representation%2520of%2520motion%2520separate%2520from%2520content%252C%2520limiting%2520their%2520applicability%2520for%2520content%2520creators.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520DisMo%252C%2520a%2520novel%2520paradigm%2520for%2520learning%2520abstract%2520motion%2520representations%2520directly%2520from%2520raw%2520video%2520data%2520via%2520an%2520image-space%2520reconstruction%2520objective.%2520Our%2520representation%2520is%2520generic%2520and%2520independent%2520of%2520static%2520information%2520such%2520as%2520appearance%252C%2520object%2520identity%252C%2520or%2520pose.%2520This%2520enables%2520open-world%2520motion%2520transfer%252C%2520allowing%2520motion%2520to%2520be%2520transferred%2520across%2520semantically%2520unrelated%2520entities%2520without%2520requiring%2520object%2520correspondences%252C%2520even%2520between%2520vastly%2520different%2520categories.%2520Unlike%2520prior%2520methods%252C%2520which%2520trade%2520off%2520motion%2520fidelity%2520and%2520prompt%2520adherence%252C%2520are%2520overfitting%2520to%2520source%2520structure%2520or%2520drifting%2520from%2520the%2520described%2520action%252C%2520our%2520approach%2520disentangles%2520motion%2520semantics%2520from%2520appearance%252C%2520enabling%2520accurate%2520transfer%2520and%2520faithful%2520conditioning.%2520Furthermore%252C%2520our%2520motion%2520representation%2520can%2520be%2520combined%2520with%2520any%2520existing%2520video%2520generator%2520via%2520lightweight%2520adapters%252C%2520allowing%2520us%2520to%2520effortlessly%2520benefit%2520from%2520future%2520advancements%2520in%2520video%2520models.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520through%2520a%2520diverse%2520set%2520of%2520motion%2520transfer%2520tasks.%2520Finally%252C%2520we%2520show%2520that%2520the%2520learned%2520representations%2520are%2520well-suited%2520for%2520downstream%2520motion%2520understanding%2520tasks%252C%2520consistently%2520outperforming%2520state-of-the-art%2520video%2520representation%2520models%2520such%2520as%2520V-JEPA%2520in%2520zero-shot%2520action%2520classification%2520on%2520benchmarks%2520including%2520Something-Something%2520v2%2520and%2520Jester.%2520Project%2520page%253A%2520https%253A//compvis.github.io/DisMo%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisMo%3A%20Disentangled%20Motion%20Representations%20for%20Open-World%20Motion%20Transfer&entry.906535625=Thomas%20Ressler-Antal%20and%20Frank%20Fundel%20and%20Malek%20Ben%20Alaya%20and%20Stefan%20Andreas%20Baumann%20and%20Felix%20Krause%20and%20Ming%20Gui%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=Recent%20advances%20in%20text-to-video%20%28T2V%29%20and%20image-to-video%20%28I2V%29%20models%2C%20have%20enabled%20the%20creation%20of%20visually%20compelling%20and%20dynamic%20videos%20from%20simple%20textual%20descriptions%20or%20initial%20frames.%20However%2C%20these%20models%20often%20fail%20to%20provide%20an%20explicit%20representation%20of%20motion%20separate%20from%20content%2C%20limiting%20their%20applicability%20for%20content%20creators.%20To%20address%20this%20gap%2C%20we%20propose%20DisMo%2C%20a%20novel%20paradigm%20for%20learning%20abstract%20motion%20representations%20directly%20from%20raw%20video%20data%20via%20an%20image-space%20reconstruction%20objective.%20Our%20representation%20is%20generic%20and%20independent%20of%20static%20information%20such%20as%20appearance%2C%20object%20identity%2C%20or%20pose.%20This%20enables%20open-world%20motion%20transfer%2C%20allowing%20motion%20to%20be%20transferred%20across%20semantically%20unrelated%20entities%20without%20requiring%20object%20correspondences%2C%20even%20between%20vastly%20different%20categories.%20Unlike%20prior%20methods%2C%20which%20trade%20off%20motion%20fidelity%20and%20prompt%20adherence%2C%20are%20overfitting%20to%20source%20structure%20or%20drifting%20from%20the%20described%20action%2C%20our%20approach%20disentangles%20motion%20semantics%20from%20appearance%2C%20enabling%20accurate%20transfer%20and%20faithful%20conditioning.%20Furthermore%2C%20our%20motion%20representation%20can%20be%20combined%20with%20any%20existing%20video%20generator%20via%20lightweight%20adapters%2C%20allowing%20us%20to%20effortlessly%20benefit%20from%20future%20advancements%20in%20video%20models.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%20a%20diverse%20set%20of%20motion%20transfer%20tasks.%20Finally%2C%20we%20show%20that%20the%20learned%20representations%20are%20well-suited%20for%20downstream%20motion%20understanding%20tasks%2C%20consistently%20outperforming%20state-of-the-art%20video%20representation%20models%20such%20as%20V-JEPA%20in%20zero-shot%20action%20classification%20on%20benchmarks%20including%20Something-Something%20v2%20and%20Jester.%20Project%20page%3A%20https%3A//compvis.github.io/DisMo&entry.1838667208=http%3A//arxiv.org/abs/2511.23428v1&entry.124074799=Read"},
{"title": "Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL", "author": "Thomas Cook and Kelly Patel and Sivapriya Vellaichamy and Udari Madhushani Sehwag and Saba Rahimi and Zhen Zeng and Sumitra Ganesh", "abstract": "Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.", "link": "http://arxiv.org/abs/2511.10674v2", "date": "2025-11-28", "relevancy": 2.5154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20of%20Domain%20Knowledge%20from%20Human%20Feedback%20in%20Text-to-SQL&body=Title%3A%20Continual%20Learning%20of%20Domain%20Knowledge%20from%20Human%20Feedback%20in%20Text-to-SQL%0AAuthor%3A%20Thomas%20Cook%20and%20Kelly%20Patel%20and%20Sivapriya%20Vellaichamy%20and%20Udari%20Madhushani%20Sehwag%20and%20Saba%20Rahimi%20and%20Zhen%20Zeng%20and%20Sumitra%20Ganesh%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20can%20generate%20SQL%20queries%20from%20natural%20language%20questions%20but%20struggle%20with%20database-specific%20schemas%20and%20tacit%20domain%20knowledge.%20We%20introduce%20a%20framework%20for%20continual%20learning%20from%20human%20feedback%20in%20text-to-SQL%2C%20where%20a%20learning%20agent%20receives%20natural%20language%20feedback%20to%20refine%20queries%20and%20distills%20the%20revealed%20knowledge%20for%20reuse%20on%20future%20tasks.%20This%20distilled%20knowledge%20is%20stored%20in%20a%20structured%20memory%2C%20enabling%20the%20agent%20to%20improve%20execution%20accuracy%20over%20time.%20We%20design%20and%20evaluate%20multiple%20variations%20of%20a%20learning%20agent%20architecture%20that%20vary%20in%20how%20they%20capture%20and%20retrieve%20past%20experiences.%20Experiments%20on%20the%20BIRD%20benchmark%20Dev%20set%20show%20that%20memory-augmented%20agents%2C%20particularly%20the%20Procedural%20Agent%2C%20achieve%20significant%20accuracy%20gains%20and%20error%20reduction%20by%20leveraging%20human-in-the-loop%20feedback.%20Our%20results%20highlight%20the%20importance%20of%20transforming%20tacit%20human%20expertise%20into%20reusable%20knowledge%2C%20paving%20the%20way%20for%20more%20adaptive%2C%20domain-aware%20text-to-SQL%20systems%20that%20continually%20learn%20from%20a%20human-in-the-loop.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520of%2520Domain%2520Knowledge%2520from%2520Human%2520Feedback%2520in%2520Text-to-SQL%26entry.906535625%3DThomas%2520Cook%2520and%2520Kelly%2520Patel%2520and%2520Sivapriya%2520Vellaichamy%2520and%2520Udari%2520Madhushani%2520Sehwag%2520and%2520Saba%2520Rahimi%2520and%2520Zhen%2520Zeng%2520and%2520Sumitra%2520Ganesh%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520generate%2520SQL%2520queries%2520from%2520natural%2520language%2520questions%2520but%2520struggle%2520with%2520database-specific%2520schemas%2520and%2520tacit%2520domain%2520knowledge.%2520We%2520introduce%2520a%2520framework%2520for%2520continual%2520learning%2520from%2520human%2520feedback%2520in%2520text-to-SQL%252C%2520where%2520a%2520learning%2520agent%2520receives%2520natural%2520language%2520feedback%2520to%2520refine%2520queries%2520and%2520distills%2520the%2520revealed%2520knowledge%2520for%2520reuse%2520on%2520future%2520tasks.%2520This%2520distilled%2520knowledge%2520is%2520stored%2520in%2520a%2520structured%2520memory%252C%2520enabling%2520the%2520agent%2520to%2520improve%2520execution%2520accuracy%2520over%2520time.%2520We%2520design%2520and%2520evaluate%2520multiple%2520variations%2520of%2520a%2520learning%2520agent%2520architecture%2520that%2520vary%2520in%2520how%2520they%2520capture%2520and%2520retrieve%2520past%2520experiences.%2520Experiments%2520on%2520the%2520BIRD%2520benchmark%2520Dev%2520set%2520show%2520that%2520memory-augmented%2520agents%252C%2520particularly%2520the%2520Procedural%2520Agent%252C%2520achieve%2520significant%2520accuracy%2520gains%2520and%2520error%2520reduction%2520by%2520leveraging%2520human-in-the-loop%2520feedback.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520transforming%2520tacit%2520human%2520expertise%2520into%2520reusable%2520knowledge%252C%2520paving%2520the%2520way%2520for%2520more%2520adaptive%252C%2520domain-aware%2520text-to-SQL%2520systems%2520that%2520continually%2520learn%2520from%2520a%2520human-in-the-loop.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20of%20Domain%20Knowledge%20from%20Human%20Feedback%20in%20Text-to-SQL&entry.906535625=Thomas%20Cook%20and%20Kelly%20Patel%20and%20Sivapriya%20Vellaichamy%20and%20Udari%20Madhushani%20Sehwag%20and%20Saba%20Rahimi%20and%20Zhen%20Zeng%20and%20Sumitra%20Ganesh&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20can%20generate%20SQL%20queries%20from%20natural%20language%20questions%20but%20struggle%20with%20database-specific%20schemas%20and%20tacit%20domain%20knowledge.%20We%20introduce%20a%20framework%20for%20continual%20learning%20from%20human%20feedback%20in%20text-to-SQL%2C%20where%20a%20learning%20agent%20receives%20natural%20language%20feedback%20to%20refine%20queries%20and%20distills%20the%20revealed%20knowledge%20for%20reuse%20on%20future%20tasks.%20This%20distilled%20knowledge%20is%20stored%20in%20a%20structured%20memory%2C%20enabling%20the%20agent%20to%20improve%20execution%20accuracy%20over%20time.%20We%20design%20and%20evaluate%20multiple%20variations%20of%20a%20learning%20agent%20architecture%20that%20vary%20in%20how%20they%20capture%20and%20retrieve%20past%20experiences.%20Experiments%20on%20the%20BIRD%20benchmark%20Dev%20set%20show%20that%20memory-augmented%20agents%2C%20particularly%20the%20Procedural%20Agent%2C%20achieve%20significant%20accuracy%20gains%20and%20error%20reduction%20by%20leveraging%20human-in-the-loop%20feedback.%20Our%20results%20highlight%20the%20importance%20of%20transforming%20tacit%20human%20expertise%20into%20reusable%20knowledge%2C%20paving%20the%20way%20for%20more%20adaptive%2C%20domain-aware%20text-to-SQL%20systems%20that%20continually%20learn%20from%20a%20human-in-the-loop.&entry.1838667208=http%3A//arxiv.org/abs/2511.10674v2&entry.124074799=Read"},
{"title": "Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding", "author": "Anik De and Abhirama Subramanyam Penamakuri and Rajeev Yadav and Aditya Rathore and Harshiv Shah and Devesh Sharma and Sagar Agarwal and Pravin Kumar and Anand Mishra", "abstract": "Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.", "link": "http://arxiv.org/abs/2511.23071v1", "date": "2025-11-28", "relevancy": 2.5154, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bharat%20Scene%20Text%3A%20A%20Novel%20Comprehensive%20Dataset%20and%20Benchmark%20for%20Indian%20Language%20Scene%20Text%20Understanding&body=Title%3A%20Bharat%20Scene%20Text%3A%20A%20Novel%20Comprehensive%20Dataset%20and%20Benchmark%20for%20Indian%20Language%20Scene%20Text%20Understanding%0AAuthor%3A%20Anik%20De%20and%20Abhirama%20Subramanyam%20Penamakuri%20and%20Rajeev%20Yadav%20and%20Aditya%20Rathore%20and%20Harshiv%20Shah%20and%20Devesh%20Sharma%20and%20Sagar%20Agarwal%20and%20Pravin%20Kumar%20and%20Anand%20Mishra%0AAbstract%3A%20Reading%20scene%20text%2C%20that%20is%2C%20text%20appearing%20in%20images%2C%20has%20numerous%20application%20areas%2C%20including%20assistive%20technology%2C%20search%2C%20and%20e-commerce.%20Although%20scene%20text%20recognition%20in%20English%20has%20advanced%20significantly%20and%20is%20often%20considered%20nearly%20a%20solved%20problem%2C%20Indian%20language%20scene%20text%20recognition%20remains%20an%20open%20challenge.%20This%20is%20due%20to%20script%20diversity%2C%20non-standard%20fonts%2C%20and%20varying%20writing%20styles%2C%20and%2C%20more%20importantly%2C%20the%20lack%20of%20high-quality%20datasets%20and%20open-source%20models.%20To%20address%20these%20gaps%2C%20we%20introduce%20the%20Bharat%20Scene%20Text%20Dataset%20%28BSTD%29%20-%20a%20large-scale%20and%20comprehensive%20benchmark%20for%20studying%20Indian%20Language%20Scene%20Text%20Recognition.%20It%20comprises%20more%20than%20100K%20words%20that%20span%2011%20Indian%20languages%20and%20English%2C%20sourced%20from%20over%206%2C500%20scene%20images%20captured%20across%20various%20linguistic%20regions%20of%20India.%20The%20dataset%20is%20meticulously%20annotated%20and%20supports%20multiple%20scene%20text%20tasks%2C%20including%3A%20%28i%29%20Scene%20Text%20Detection%2C%20%28ii%29%20Script%20Identification%2C%20%28iii%29%20Cropped%20Word%20Recognition%2C%20and%20%28iv%29%20End-to-End%20Scene%20Text%20Recognition.%20We%20evaluated%20state-of-the-art%20models%20originally%20developed%20for%20English%20by%20adapting%20%28fine-tuning%29%20them%20for%20Indian%20languages.%20Our%20results%20highlight%20the%20challenges%20and%20opportunities%20in%20Indian%20language%20scene%20text%20recognition.%20We%20believe%20that%20this%20dataset%20represents%20a%20significant%20step%20toward%20advancing%20research%20in%20this%20domain.%20All%20our%20models%20and%20data%20are%20open%20source.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBharat%2520Scene%2520Text%253A%2520A%2520Novel%2520Comprehensive%2520Dataset%2520and%2520Benchmark%2520for%2520Indian%2520Language%2520Scene%2520Text%2520Understanding%26entry.906535625%3DAnik%2520De%2520and%2520Abhirama%2520Subramanyam%2520Penamakuri%2520and%2520Rajeev%2520Yadav%2520and%2520Aditya%2520Rathore%2520and%2520Harshiv%2520Shah%2520and%2520Devesh%2520Sharma%2520and%2520Sagar%2520Agarwal%2520and%2520Pravin%2520Kumar%2520and%2520Anand%2520Mishra%26entry.1292438233%3DReading%2520scene%2520text%252C%2520that%2520is%252C%2520text%2520appearing%2520in%2520images%252C%2520has%2520numerous%2520application%2520areas%252C%2520including%2520assistive%2520technology%252C%2520search%252C%2520and%2520e-commerce.%2520Although%2520scene%2520text%2520recognition%2520in%2520English%2520has%2520advanced%2520significantly%2520and%2520is%2520often%2520considered%2520nearly%2520a%2520solved%2520problem%252C%2520Indian%2520language%2520scene%2520text%2520recognition%2520remains%2520an%2520open%2520challenge.%2520This%2520is%2520due%2520to%2520script%2520diversity%252C%2520non-standard%2520fonts%252C%2520and%2520varying%2520writing%2520styles%252C%2520and%252C%2520more%2520importantly%252C%2520the%2520lack%2520of%2520high-quality%2520datasets%2520and%2520open-source%2520models.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520the%2520Bharat%2520Scene%2520Text%2520Dataset%2520%2528BSTD%2529%2520-%2520a%2520large-scale%2520and%2520comprehensive%2520benchmark%2520for%2520studying%2520Indian%2520Language%2520Scene%2520Text%2520Recognition.%2520It%2520comprises%2520more%2520than%2520100K%2520words%2520that%2520span%252011%2520Indian%2520languages%2520and%2520English%252C%2520sourced%2520from%2520over%25206%252C500%2520scene%2520images%2520captured%2520across%2520various%2520linguistic%2520regions%2520of%2520India.%2520The%2520dataset%2520is%2520meticulously%2520annotated%2520and%2520supports%2520multiple%2520scene%2520text%2520tasks%252C%2520including%253A%2520%2528i%2529%2520Scene%2520Text%2520Detection%252C%2520%2528ii%2529%2520Script%2520Identification%252C%2520%2528iii%2529%2520Cropped%2520Word%2520Recognition%252C%2520and%2520%2528iv%2529%2520End-to-End%2520Scene%2520Text%2520Recognition.%2520We%2520evaluated%2520state-of-the-art%2520models%2520originally%2520developed%2520for%2520English%2520by%2520adapting%2520%2528fine-tuning%2529%2520them%2520for%2520Indian%2520languages.%2520Our%2520results%2520highlight%2520the%2520challenges%2520and%2520opportunities%2520in%2520Indian%2520language%2520scene%2520text%2520recognition.%2520We%2520believe%2520that%2520this%2520dataset%2520represents%2520a%2520significant%2520step%2520toward%2520advancing%2520research%2520in%2520this%2520domain.%2520All%2520our%2520models%2520and%2520data%2520are%2520open%2520source.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bharat%20Scene%20Text%3A%20A%20Novel%20Comprehensive%20Dataset%20and%20Benchmark%20for%20Indian%20Language%20Scene%20Text%20Understanding&entry.906535625=Anik%20De%20and%20Abhirama%20Subramanyam%20Penamakuri%20and%20Rajeev%20Yadav%20and%20Aditya%20Rathore%20and%20Harshiv%20Shah%20and%20Devesh%20Sharma%20and%20Sagar%20Agarwal%20and%20Pravin%20Kumar%20and%20Anand%20Mishra&entry.1292438233=Reading%20scene%20text%2C%20that%20is%2C%20text%20appearing%20in%20images%2C%20has%20numerous%20application%20areas%2C%20including%20assistive%20technology%2C%20search%2C%20and%20e-commerce.%20Although%20scene%20text%20recognition%20in%20English%20has%20advanced%20significantly%20and%20is%20often%20considered%20nearly%20a%20solved%20problem%2C%20Indian%20language%20scene%20text%20recognition%20remains%20an%20open%20challenge.%20This%20is%20due%20to%20script%20diversity%2C%20non-standard%20fonts%2C%20and%20varying%20writing%20styles%2C%20and%2C%20more%20importantly%2C%20the%20lack%20of%20high-quality%20datasets%20and%20open-source%20models.%20To%20address%20these%20gaps%2C%20we%20introduce%20the%20Bharat%20Scene%20Text%20Dataset%20%28BSTD%29%20-%20a%20large-scale%20and%20comprehensive%20benchmark%20for%20studying%20Indian%20Language%20Scene%20Text%20Recognition.%20It%20comprises%20more%20than%20100K%20words%20that%20span%2011%20Indian%20languages%20and%20English%2C%20sourced%20from%20over%206%2C500%20scene%20images%20captured%20across%20various%20linguistic%20regions%20of%20India.%20The%20dataset%20is%20meticulously%20annotated%20and%20supports%20multiple%20scene%20text%20tasks%2C%20including%3A%20%28i%29%20Scene%20Text%20Detection%2C%20%28ii%29%20Script%20Identification%2C%20%28iii%29%20Cropped%20Word%20Recognition%2C%20and%20%28iv%29%20End-to-End%20Scene%20Text%20Recognition.%20We%20evaluated%20state-of-the-art%20models%20originally%20developed%20for%20English%20by%20adapting%20%28fine-tuning%29%20them%20for%20Indian%20languages.%20Our%20results%20highlight%20the%20challenges%20and%20opportunities%20in%20Indian%20language%20scene%20text%20recognition.%20We%20believe%20that%20this%20dataset%20represents%20a%20significant%20step%20toward%20advancing%20research%20in%20this%20domain.%20All%20our%20models%20and%20data%20are%20open%20source.&entry.1838667208=http%3A//arxiv.org/abs/2511.23071v1&entry.124074799=Read"},
{"title": "TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency", "author": "Minye Shao and Xingyu Miao and Haoran Duan and Zeyu Wang and Jingkun Chen and Yawen Huang and Xian Wu and Jingjing Deng and Yang Long and Yefeng Zheng", "abstract": "3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code is available at: https://github.com/VinyehShaw/TRACE.", "link": "http://arxiv.org/abs/2507.00802v2", "date": "2025-11-28", "relevancy": 2.5122, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6359}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6359}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRACE%3A%20Temporally%20Reliable%20Anatomically-Conditioned%203D%20CT%20Generation%20with%20Enhanced%20Efficiency&body=Title%3A%20TRACE%3A%20Temporally%20Reliable%20Anatomically-Conditioned%203D%20CT%20Generation%20with%20Enhanced%20Efficiency%0AAuthor%3A%20Minye%20Shao%20and%20Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Zeyu%20Wang%20and%20Jingkun%20Chen%20and%20Yawen%20Huang%20and%20Xian%20Wu%20and%20Jingjing%20Deng%20and%20Yang%20Long%20and%20Yefeng%20Zheng%0AAbstract%3A%203D%20medical%20image%20generation%20is%20essential%20for%20data%20augmentation%20and%20patient%20privacy%2C%20calling%20for%20reliable%20and%20efficient%20models%20suited%20for%20clinical%20practice.%20However%2C%20current%20methods%20suffer%20from%20limited%20anatomical%20fidelity%2C%20restricted%20axial%20length%2C%20and%20substantial%20computational%20cost%2C%20placing%20them%20beyond%20reach%20for%20regions%20with%20limited%20resources%20and%20infrastructure.%20We%20introduce%20TRACE%2C%20a%20framework%20that%20generates%203D%20medical%20images%20with%20spatiotemporal%20alignment%20using%20a%202D%20multimodal-conditioned%20diffusion%20approach.%20TRACE%20models%20sequential%202D%20slices%20as%20video%20frame%20pairs%2C%20combining%20segmentation%20priors%20and%20radiology%20reports%20for%20anatomical%20alignment%2C%20incorporating%20optical%20flow%20to%20sustain%20temporal%20coherence.%20During%20inference%2C%20an%20overlapping-frame%20strategy%20links%20frame%20pairs%20into%20a%20flexible%20length%20sequence%2C%20reconstructed%20into%20a%20spatiotemporally%20and%20anatomically%20aligned%203D%20volume.%20Experimental%20results%20demonstrate%20that%20TRACE%20effectively%20balances%20computational%20efficiency%20with%20preserving%20anatomical%20fidelity%20and%20spatiotemporal%20consistency.%20Code%20is%20available%20at%3A%20https%3A//github.com/VinyehShaw/TRACE.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRACE%253A%2520Temporally%2520Reliable%2520Anatomically-Conditioned%25203D%2520CT%2520Generation%2520with%2520Enhanced%2520Efficiency%26entry.906535625%3DMinye%2520Shao%2520and%2520Xingyu%2520Miao%2520and%2520Haoran%2520Duan%2520and%2520Zeyu%2520Wang%2520and%2520Jingkun%2520Chen%2520and%2520Yawen%2520Huang%2520and%2520Xian%2520Wu%2520and%2520Jingjing%2520Deng%2520and%2520Yang%2520Long%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D3D%2520medical%2520image%2520generation%2520is%2520essential%2520for%2520data%2520augmentation%2520and%2520patient%2520privacy%252C%2520calling%2520for%2520reliable%2520and%2520efficient%2520models%2520suited%2520for%2520clinical%2520practice.%2520However%252C%2520current%2520methods%2520suffer%2520from%2520limited%2520anatomical%2520fidelity%252C%2520restricted%2520axial%2520length%252C%2520and%2520substantial%2520computational%2520cost%252C%2520placing%2520them%2520beyond%2520reach%2520for%2520regions%2520with%2520limited%2520resources%2520and%2520infrastructure.%2520We%2520introduce%2520TRACE%252C%2520a%2520framework%2520that%2520generates%25203D%2520medical%2520images%2520with%2520spatiotemporal%2520alignment%2520using%2520a%25202D%2520multimodal-conditioned%2520diffusion%2520approach.%2520TRACE%2520models%2520sequential%25202D%2520slices%2520as%2520video%2520frame%2520pairs%252C%2520combining%2520segmentation%2520priors%2520and%2520radiology%2520reports%2520for%2520anatomical%2520alignment%252C%2520incorporating%2520optical%2520flow%2520to%2520sustain%2520temporal%2520coherence.%2520During%2520inference%252C%2520an%2520overlapping-frame%2520strategy%2520links%2520frame%2520pairs%2520into%2520a%2520flexible%2520length%2520sequence%252C%2520reconstructed%2520into%2520a%2520spatiotemporally%2520and%2520anatomically%2520aligned%25203D%2520volume.%2520Experimental%2520results%2520demonstrate%2520that%2520TRACE%2520effectively%2520balances%2520computational%2520efficiency%2520with%2520preserving%2520anatomical%2520fidelity%2520and%2520spatiotemporal%2520consistency.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/VinyehShaw/TRACE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRACE%3A%20Temporally%20Reliable%20Anatomically-Conditioned%203D%20CT%20Generation%20with%20Enhanced%20Efficiency&entry.906535625=Minye%20Shao%20and%20Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Zeyu%20Wang%20and%20Jingkun%20Chen%20and%20Yawen%20Huang%20and%20Xian%20Wu%20and%20Jingjing%20Deng%20and%20Yang%20Long%20and%20Yefeng%20Zheng&entry.1292438233=3D%20medical%20image%20generation%20is%20essential%20for%20data%20augmentation%20and%20patient%20privacy%2C%20calling%20for%20reliable%20and%20efficient%20models%20suited%20for%20clinical%20practice.%20However%2C%20current%20methods%20suffer%20from%20limited%20anatomical%20fidelity%2C%20restricted%20axial%20length%2C%20and%20substantial%20computational%20cost%2C%20placing%20them%20beyond%20reach%20for%20regions%20with%20limited%20resources%20and%20infrastructure.%20We%20introduce%20TRACE%2C%20a%20framework%20that%20generates%203D%20medical%20images%20with%20spatiotemporal%20alignment%20using%20a%202D%20multimodal-conditioned%20diffusion%20approach.%20TRACE%20models%20sequential%202D%20slices%20as%20video%20frame%20pairs%2C%20combining%20segmentation%20priors%20and%20radiology%20reports%20for%20anatomical%20alignment%2C%20incorporating%20optical%20flow%20to%20sustain%20temporal%20coherence.%20During%20inference%2C%20an%20overlapping-frame%20strategy%20links%20frame%20pairs%20into%20a%20flexible%20length%20sequence%2C%20reconstructed%20into%20a%20spatiotemporally%20and%20anatomically%20aligned%203D%20volume.%20Experimental%20results%20demonstrate%20that%20TRACE%20effectively%20balances%20computational%20efficiency%20with%20preserving%20anatomical%20fidelity%20and%20spatiotemporal%20consistency.%20Code%20is%20available%20at%3A%20https%3A//github.com/VinyehShaw/TRACE.&entry.1838667208=http%3A//arxiv.org/abs/2507.00802v2&entry.124074799=Read"},
{"title": "Buffer replay enhances the robustness of multimodal learning under missing-modality", "author": "Hongye Zhu and Xuan Liu and Yanwen Ba and Jingye Xue and Shigeng Zhang", "abstract": "Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.", "link": "http://arxiv.org/abs/2511.23070v1", "date": "2025-11-28", "relevancy": 2.5099, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Buffer%20replay%20enhances%20the%20robustness%20of%20multimodal%20learning%20under%20missing-modality&body=Title%3A%20Buffer%20replay%20enhances%20the%20robustness%20of%20multimodal%20learning%20under%20missing-modality%0AAuthor%3A%20Hongye%20Zhu%20and%20Xuan%20Liu%20and%20Yanwen%20Ba%20and%20Jingye%20Xue%20and%20Shigeng%20Zhang%0AAbstract%3A%20Missing%20modalities%20consistently%20lead%20to%20significant%20performance%20degradation%20in%20multimodal%20models.%20Existing%20approaches%20either%20synthesize%20missing%20modalities%20at%20high%20computational%20cost%20or%20apply%20prompt-based%20fine-tuning%20that%20relies%20only%20on%20adjacent-layer%20features%20and%20overlooks%20long-distance%20contextual%20information%2C%20which%20may%20offer%20additional%20tolerance%20to%20errors%20when%20one%20or%20more%20modalities%20are%20missing.%20To%20address%20this%2C%20we%20introduce%20REplay%20Prompting%20%28REP%29%3A%20%281%29%20construct%20modality-wise%20feature%20buffers%20via%20a%20residual%20bypass%20to%20cache%20early-layer%20representations%20and%20replay%20them%20in%20deeper%20layers%2C%20mitigating%20information%20loss%20as%20network%20depth%20increases%3B%20%282%29%20employ%20a%20private-shared%20feature%20decoupling%20strategy%2C%20where%20private%20buffers%20preserve%20modality-specific%20signals%20and%20shared%20buffers%20encode%20cross-modal%20semantics%3B%20and%20%283%29%20design%20a%20task-aware%20dynamic%20initialization%20mechanism%20to%20configure%20these%20buffers%20differently%2C%20improving%20stability%20and%20generalization%20under%20diverse%20missing-modality%20conditions.%20Experiments%20on%20vision-language%2C%20vision-language-audio%2C%20and%20temporal%20multimodal%20benchmarks%20demonstrate%20that%20REP%20consistently%20outperforms%20prior%20methods%20under%20both%20single-%20and%20multi-modality%20missing%20scenarios%2C%20while%20introducing%20only%20negligible%20parameter%20overhead.%20These%20results%20establish%20REP%20as%20a%20lightweight%20and%20effective%20paradigm%20for%20robust%20multimodal%20learning%20in%20challenging%20missing-modality%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuffer%2520replay%2520enhances%2520the%2520robustness%2520of%2520multimodal%2520learning%2520under%2520missing-modality%26entry.906535625%3DHongye%2520Zhu%2520and%2520Xuan%2520Liu%2520and%2520Yanwen%2520Ba%2520and%2520Jingye%2520Xue%2520and%2520Shigeng%2520Zhang%26entry.1292438233%3DMissing%2520modalities%2520consistently%2520lead%2520to%2520significant%2520performance%2520degradation%2520in%2520multimodal%2520models.%2520Existing%2520approaches%2520either%2520synthesize%2520missing%2520modalities%2520at%2520high%2520computational%2520cost%2520or%2520apply%2520prompt-based%2520fine-tuning%2520that%2520relies%2520only%2520on%2520adjacent-layer%2520features%2520and%2520overlooks%2520long-distance%2520contextual%2520information%252C%2520which%2520may%2520offer%2520additional%2520tolerance%2520to%2520errors%2520when%2520one%2520or%2520more%2520modalities%2520are%2520missing.%2520To%2520address%2520this%252C%2520we%2520introduce%2520REplay%2520Prompting%2520%2528REP%2529%253A%2520%25281%2529%2520construct%2520modality-wise%2520feature%2520buffers%2520via%2520a%2520residual%2520bypass%2520to%2520cache%2520early-layer%2520representations%2520and%2520replay%2520them%2520in%2520deeper%2520layers%252C%2520mitigating%2520information%2520loss%2520as%2520network%2520depth%2520increases%253B%2520%25282%2529%2520employ%2520a%2520private-shared%2520feature%2520decoupling%2520strategy%252C%2520where%2520private%2520buffers%2520preserve%2520modality-specific%2520signals%2520and%2520shared%2520buffers%2520encode%2520cross-modal%2520semantics%253B%2520and%2520%25283%2529%2520design%2520a%2520task-aware%2520dynamic%2520initialization%2520mechanism%2520to%2520configure%2520these%2520buffers%2520differently%252C%2520improving%2520stability%2520and%2520generalization%2520under%2520diverse%2520missing-modality%2520conditions.%2520Experiments%2520on%2520vision-language%252C%2520vision-language-audio%252C%2520and%2520temporal%2520multimodal%2520benchmarks%2520demonstrate%2520that%2520REP%2520consistently%2520outperforms%2520prior%2520methods%2520under%2520both%2520single-%2520and%2520multi-modality%2520missing%2520scenarios%252C%2520while%2520introducing%2520only%2520negligible%2520parameter%2520overhead.%2520These%2520results%2520establish%2520REP%2520as%2520a%2520lightweight%2520and%2520effective%2520paradigm%2520for%2520robust%2520multimodal%2520learning%2520in%2520challenging%2520missing-modality%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Buffer%20replay%20enhances%20the%20robustness%20of%20multimodal%20learning%20under%20missing-modality&entry.906535625=Hongye%20Zhu%20and%20Xuan%20Liu%20and%20Yanwen%20Ba%20and%20Jingye%20Xue%20and%20Shigeng%20Zhang&entry.1292438233=Missing%20modalities%20consistently%20lead%20to%20significant%20performance%20degradation%20in%20multimodal%20models.%20Existing%20approaches%20either%20synthesize%20missing%20modalities%20at%20high%20computational%20cost%20or%20apply%20prompt-based%20fine-tuning%20that%20relies%20only%20on%20adjacent-layer%20features%20and%20overlooks%20long-distance%20contextual%20information%2C%20which%20may%20offer%20additional%20tolerance%20to%20errors%20when%20one%20or%20more%20modalities%20are%20missing.%20To%20address%20this%2C%20we%20introduce%20REplay%20Prompting%20%28REP%29%3A%20%281%29%20construct%20modality-wise%20feature%20buffers%20via%20a%20residual%20bypass%20to%20cache%20early-layer%20representations%20and%20replay%20them%20in%20deeper%20layers%2C%20mitigating%20information%20loss%20as%20network%20depth%20increases%3B%20%282%29%20employ%20a%20private-shared%20feature%20decoupling%20strategy%2C%20where%20private%20buffers%20preserve%20modality-specific%20signals%20and%20shared%20buffers%20encode%20cross-modal%20semantics%3B%20and%20%283%29%20design%20a%20task-aware%20dynamic%20initialization%20mechanism%20to%20configure%20these%20buffers%20differently%2C%20improving%20stability%20and%20generalization%20under%20diverse%20missing-modality%20conditions.%20Experiments%20on%20vision-language%2C%20vision-language-audio%2C%20and%20temporal%20multimodal%20benchmarks%20demonstrate%20that%20REP%20consistently%20outperforms%20prior%20methods%20under%20both%20single-%20and%20multi-modality%20missing%20scenarios%2C%20while%20introducing%20only%20negligible%20parameter%20overhead.%20These%20results%20establish%20REP%20as%20a%20lightweight%20and%20effective%20paradigm%20for%20robust%20multimodal%20learning%20in%20challenging%20missing-modality%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.23070v1&entry.124074799=Read"},
{"title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models", "author": "Xiang Hu and Zhanchao Zhou and Ruiqi Liang and Zehuan Li and Wei Wu and Jianguo Li", "abstract": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \\textbf{sparsity}, \\textbf{random-access flexibility}, and \\textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.", "link": "http://arxiv.org/abs/2511.23319v1", "date": "2025-11-28", "relevancy": 2.4997, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Every%20Token%20Counts%3A%20Generalizing%2016M%20Ultra-Long%20Context%20in%20Large%20Language%20Models&body=Title%3A%20Every%20Token%20Counts%3A%20Generalizing%2016M%20Ultra-Long%20Context%20in%20Large%20Language%20Models%0AAuthor%3A%20Xiang%20Hu%20and%20Zhanchao%20Zhou%20and%20Ruiqi%20Liang%20and%20Zehuan%20Li%20and%20Wei%20Wu%20and%20Jianguo%20Li%0AAbstract%3A%20This%20work%20explores%20the%20challenge%20of%20building%20%60%60Machines%20that%20Can%20Remember%27%27%2C%20framing%20long-term%20memory%20as%20the%20problem%20of%20efficient%20ultra-long%20context%20modeling.%20We%20argue%20that%20this%20requires%20three%20key%20properties%3A%20%5Ctextbf%7Bsparsity%7D%2C%20%5Ctextbf%7Brandom-access%20flexibility%7D%2C%20and%20%5Ctextbf%7Blength%20generalization%7D.%20To%20address%20ultra-long-context%20modeling%2C%20we%20leverage%20Hierarchical%20Sparse%20Attention%20%28HSA%29%2C%20a%20novel%20attention%20mechanism%20that%20satisfies%20all%20three%20properties.%20We%20integrate%20HSA%20into%20Transformers%20to%20build%20HSA-UltraLong%2C%20which%20is%20an%208B-parameter%20MoE%20model%20trained%20on%20over%208%20trillion%20tokens%20and%20is%20rigorously%20evaluated%20on%20different%20tasks%20with%20in-domain%20and%20out-of-domain%20context%20lengths%20to%20demonstrate%20its%20capability%20in%20handling%20ultra-long%20contexts.%20Results%20show%20that%20our%20model%20performs%20comparably%20to%20full-attention%20baselines%20on%20in-domain%20lengths%20while%20achieving%20over%2090%5C%25%20accuracy%20on%20most%20in-context%20retrieval%20tasks%20with%20contexts%20up%20to%2016M.%20This%20report%20outlines%20our%20experimental%20insights%20and%20open%20problems%2C%20contributing%20a%20foundation%20for%20future%20research%20in%20ultra-long%20context%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvery%2520Token%2520Counts%253A%2520Generalizing%252016M%2520Ultra-Long%2520Context%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DXiang%2520Hu%2520and%2520Zhanchao%2520Zhou%2520and%2520Ruiqi%2520Liang%2520and%2520Zehuan%2520Li%2520and%2520Wei%2520Wu%2520and%2520Jianguo%2520Li%26entry.1292438233%3DThis%2520work%2520explores%2520the%2520challenge%2520of%2520building%2520%2560%2560Machines%2520that%2520Can%2520Remember%2527%2527%252C%2520framing%2520long-term%2520memory%2520as%2520the%2520problem%2520of%2520efficient%2520ultra-long%2520context%2520modeling.%2520We%2520argue%2520that%2520this%2520requires%2520three%2520key%2520properties%253A%2520%255Ctextbf%257Bsparsity%257D%252C%2520%255Ctextbf%257Brandom-access%2520flexibility%257D%252C%2520and%2520%255Ctextbf%257Blength%2520generalization%257D.%2520To%2520address%2520ultra-long-context%2520modeling%252C%2520we%2520leverage%2520Hierarchical%2520Sparse%2520Attention%2520%2528HSA%2529%252C%2520a%2520novel%2520attention%2520mechanism%2520that%2520satisfies%2520all%2520three%2520properties.%2520We%2520integrate%2520HSA%2520into%2520Transformers%2520to%2520build%2520HSA-UltraLong%252C%2520which%2520is%2520an%25208B-parameter%2520MoE%2520model%2520trained%2520on%2520over%25208%2520trillion%2520tokens%2520and%2520is%2520rigorously%2520evaluated%2520on%2520different%2520tasks%2520with%2520in-domain%2520and%2520out-of-domain%2520context%2520lengths%2520to%2520demonstrate%2520its%2520capability%2520in%2520handling%2520ultra-long%2520contexts.%2520Results%2520show%2520that%2520our%2520model%2520performs%2520comparably%2520to%2520full-attention%2520baselines%2520on%2520in-domain%2520lengths%2520while%2520achieving%2520over%252090%255C%2525%2520accuracy%2520on%2520most%2520in-context%2520retrieval%2520tasks%2520with%2520contexts%2520up%2520to%252016M.%2520This%2520report%2520outlines%2520our%2520experimental%2520insights%2520and%2520open%2520problems%252C%2520contributing%2520a%2520foundation%2520for%2520future%2520research%2520in%2520ultra-long%2520context%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Every%20Token%20Counts%3A%20Generalizing%2016M%20Ultra-Long%20Context%20in%20Large%20Language%20Models&entry.906535625=Xiang%20Hu%20and%20Zhanchao%20Zhou%20and%20Ruiqi%20Liang%20and%20Zehuan%20Li%20and%20Wei%20Wu%20and%20Jianguo%20Li&entry.1292438233=This%20work%20explores%20the%20challenge%20of%20building%20%60%60Machines%20that%20Can%20Remember%27%27%2C%20framing%20long-term%20memory%20as%20the%20problem%20of%20efficient%20ultra-long%20context%20modeling.%20We%20argue%20that%20this%20requires%20three%20key%20properties%3A%20%5Ctextbf%7Bsparsity%7D%2C%20%5Ctextbf%7Brandom-access%20flexibility%7D%2C%20and%20%5Ctextbf%7Blength%20generalization%7D.%20To%20address%20ultra-long-context%20modeling%2C%20we%20leverage%20Hierarchical%20Sparse%20Attention%20%28HSA%29%2C%20a%20novel%20attention%20mechanism%20that%20satisfies%20all%20three%20properties.%20We%20integrate%20HSA%20into%20Transformers%20to%20build%20HSA-UltraLong%2C%20which%20is%20an%208B-parameter%20MoE%20model%20trained%20on%20over%208%20trillion%20tokens%20and%20is%20rigorously%20evaluated%20on%20different%20tasks%20with%20in-domain%20and%20out-of-domain%20context%20lengths%20to%20demonstrate%20its%20capability%20in%20handling%20ultra-long%20contexts.%20Results%20show%20that%20our%20model%20performs%20comparably%20to%20full-attention%20baselines%20on%20in-domain%20lengths%20while%20achieving%20over%2090%5C%25%20accuracy%20on%20most%20in-context%20retrieval%20tasks%20with%20contexts%20up%20to%2016M.%20This%20report%20outlines%20our%20experimental%20insights%20and%20open%20problems%2C%20contributing%20a%20foundation%20for%20future%20research%20in%20ultra-long%20context%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2511.23319v1&entry.124074799=Read"},
{"title": "Fast Multi-view Consistent 3D Editing with Video Priors", "author": "Liyi Chen and Ruihuang Li and Guowen Zhang and Pengfei Wang and Lei Zhang", "abstract": "Text-driven 3D editing enables user-friendly 3D object or scene editing with text instructions. Due to the lack of multi-view consistency priors, existing methods typically resort to employing 2D generation or editing models to process each view individually, followed by iterative 2D-3D-2D updating. However, these methods are not only time-consuming but also prone to over-smoothed results because the different editing signals gathered from different views are averaged during the iterative process. In this paper, we propose generative Video Prior based 3D Editing (ViP3DE) to employ the temporal consistency priors from pre-trained video generation models for multi-view consistent 3D editing in a single forward pass. Our key insight is to condition the video generation model on a single edited view to generate other consistent edited views for 3D updating directly, thereby bypassing the iterative editing paradigm. Since 3D updating requires edited views to be paired with specific camera poses, we propose motion-preserved noise blending for the video model to generate edited views at predefined camera poses. In addition, we introduce geometry-aware denoising to further enhance multi-view consistency by integrating 3D geometric priors into video models. Extensive experiments demonstrate that our proposed ViP3DE can achieve high-quality 3D editing results even within a single forward pass, significantly outperforming existing methods in both editing quality and speed.", "link": "http://arxiv.org/abs/2511.23172v1", "date": "2025-11-28", "relevancy": 2.4935, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6285}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6224}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Multi-view%20Consistent%203D%20Editing%20with%20Video%20Priors&body=Title%3A%20Fast%20Multi-view%20Consistent%203D%20Editing%20with%20Video%20Priors%0AAuthor%3A%20Liyi%20Chen%20and%20Ruihuang%20Li%20and%20Guowen%20Zhang%20and%20Pengfei%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%20Text-driven%203D%20editing%20enables%20user-friendly%203D%20object%20or%20scene%20editing%20with%20text%20instructions.%20Due%20to%20the%20lack%20of%20multi-view%20consistency%20priors%2C%20existing%20methods%20typically%20resort%20to%20employing%202D%20generation%20or%20editing%20models%20to%20process%20each%20view%20individually%2C%20followed%20by%20iterative%202D-3D-2D%20updating.%20However%2C%20these%20methods%20are%20not%20only%20time-consuming%20but%20also%20prone%20to%20over-smoothed%20results%20because%20the%20different%20editing%20signals%20gathered%20from%20different%20views%20are%20averaged%20during%20the%20iterative%20process.%20In%20this%20paper%2C%20we%20propose%20generative%20Video%20Prior%20based%203D%20Editing%20%28ViP3DE%29%20to%20employ%20the%20temporal%20consistency%20priors%20from%20pre-trained%20video%20generation%20models%20for%20multi-view%20consistent%203D%20editing%20in%20a%20single%20forward%20pass.%20Our%20key%20insight%20is%20to%20condition%20the%20video%20generation%20model%20on%20a%20single%20edited%20view%20to%20generate%20other%20consistent%20edited%20views%20for%203D%20updating%20directly%2C%20thereby%20bypassing%20the%20iterative%20editing%20paradigm.%20Since%203D%20updating%20requires%20edited%20views%20to%20be%20paired%20with%20specific%20camera%20poses%2C%20we%20propose%20motion-preserved%20noise%20blending%20for%20the%20video%20model%20to%20generate%20edited%20views%20at%20predefined%20camera%20poses.%20In%20addition%2C%20we%20introduce%20geometry-aware%20denoising%20to%20further%20enhance%20multi-view%20consistency%20by%20integrating%203D%20geometric%20priors%20into%20video%20models.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20ViP3DE%20can%20achieve%20high-quality%203D%20editing%20results%20even%20within%20a%20single%20forward%20pass%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20editing%20quality%20and%20speed.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Multi-view%2520Consistent%25203D%2520Editing%2520with%2520Video%2520Priors%26entry.906535625%3DLiyi%2520Chen%2520and%2520Ruihuang%2520Li%2520and%2520Guowen%2520Zhang%2520and%2520Pengfei%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3DText-driven%25203D%2520editing%2520enables%2520user-friendly%25203D%2520object%2520or%2520scene%2520editing%2520with%2520text%2520instructions.%2520Due%2520to%2520the%2520lack%2520of%2520multi-view%2520consistency%2520priors%252C%2520existing%2520methods%2520typically%2520resort%2520to%2520employing%25202D%2520generation%2520or%2520editing%2520models%2520to%2520process%2520each%2520view%2520individually%252C%2520followed%2520by%2520iterative%25202D-3D-2D%2520updating.%2520However%252C%2520these%2520methods%2520are%2520not%2520only%2520time-consuming%2520but%2520also%2520prone%2520to%2520over-smoothed%2520results%2520because%2520the%2520different%2520editing%2520signals%2520gathered%2520from%2520different%2520views%2520are%2520averaged%2520during%2520the%2520iterative%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520generative%2520Video%2520Prior%2520based%25203D%2520Editing%2520%2528ViP3DE%2529%2520to%2520employ%2520the%2520temporal%2520consistency%2520priors%2520from%2520pre-trained%2520video%2520generation%2520models%2520for%2520multi-view%2520consistent%25203D%2520editing%2520in%2520a%2520single%2520forward%2520pass.%2520Our%2520key%2520insight%2520is%2520to%2520condition%2520the%2520video%2520generation%2520model%2520on%2520a%2520single%2520edited%2520view%2520to%2520generate%2520other%2520consistent%2520edited%2520views%2520for%25203D%2520updating%2520directly%252C%2520thereby%2520bypassing%2520the%2520iterative%2520editing%2520paradigm.%2520Since%25203D%2520updating%2520requires%2520edited%2520views%2520to%2520be%2520paired%2520with%2520specific%2520camera%2520poses%252C%2520we%2520propose%2520motion-preserved%2520noise%2520blending%2520for%2520the%2520video%2520model%2520to%2520generate%2520edited%2520views%2520at%2520predefined%2520camera%2520poses.%2520In%2520addition%252C%2520we%2520introduce%2520geometry-aware%2520denoising%2520to%2520further%2520enhance%2520multi-view%2520consistency%2520by%2520integrating%25203D%2520geometric%2520priors%2520into%2520video%2520models.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520ViP3DE%2520can%2520achieve%2520high-quality%25203D%2520editing%2520results%2520even%2520within%2520a%2520single%2520forward%2520pass%252C%2520significantly%2520outperforming%2520existing%2520methods%2520in%2520both%2520editing%2520quality%2520and%2520speed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Multi-view%20Consistent%203D%20Editing%20with%20Video%20Priors&entry.906535625=Liyi%20Chen%20and%20Ruihuang%20Li%20and%20Guowen%20Zhang%20and%20Pengfei%20Wang%20and%20Lei%20Zhang&entry.1292438233=Text-driven%203D%20editing%20enables%20user-friendly%203D%20object%20or%20scene%20editing%20with%20text%20instructions.%20Due%20to%20the%20lack%20of%20multi-view%20consistency%20priors%2C%20existing%20methods%20typically%20resort%20to%20employing%202D%20generation%20or%20editing%20models%20to%20process%20each%20view%20individually%2C%20followed%20by%20iterative%202D-3D-2D%20updating.%20However%2C%20these%20methods%20are%20not%20only%20time-consuming%20but%20also%20prone%20to%20over-smoothed%20results%20because%20the%20different%20editing%20signals%20gathered%20from%20different%20views%20are%20averaged%20during%20the%20iterative%20process.%20In%20this%20paper%2C%20we%20propose%20generative%20Video%20Prior%20based%203D%20Editing%20%28ViP3DE%29%20to%20employ%20the%20temporal%20consistency%20priors%20from%20pre-trained%20video%20generation%20models%20for%20multi-view%20consistent%203D%20editing%20in%20a%20single%20forward%20pass.%20Our%20key%20insight%20is%20to%20condition%20the%20video%20generation%20model%20on%20a%20single%20edited%20view%20to%20generate%20other%20consistent%20edited%20views%20for%203D%20updating%20directly%2C%20thereby%20bypassing%20the%20iterative%20editing%20paradigm.%20Since%203D%20updating%20requires%20edited%20views%20to%20be%20paired%20with%20specific%20camera%20poses%2C%20we%20propose%20motion-preserved%20noise%20blending%20for%20the%20video%20model%20to%20generate%20edited%20views%20at%20predefined%20camera%20poses.%20In%20addition%2C%20we%20introduce%20geometry-aware%20denoising%20to%20further%20enhance%20multi-view%20consistency%20by%20integrating%203D%20geometric%20priors%20into%20video%20models.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20ViP3DE%20can%20achieve%20high-quality%203D%20editing%20results%20even%20within%20a%20single%20forward%20pass%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20editing%20quality%20and%20speed.&entry.1838667208=http%3A//arxiv.org/abs/2511.23172v1&entry.124074799=Read"},
{"title": "OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning", "author": "Zixun Huang and Jiayi Sheng and Zeyu Zheng", "abstract": "Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.", "link": "http://arxiv.org/abs/2511.23310v1", "date": "2025-11-28", "relevancy": 2.4819, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OBLR-PO%3A%20A%20Theoretical%20Framework%20for%20Stable%20Reinforcement%20Learning&body=Title%3A%20OBLR-PO%3A%20A%20Theoretical%20Framework%20for%20Stable%20Reinforcement%20Learning%0AAuthor%3A%20Zixun%20Huang%20and%20Jiayi%20Sheng%20and%20Zeyu%20Zheng%0AAbstract%3A%20Existing%20reinforcement%20learning%20%28RL%29-based%20post-training%20methods%20for%20large%20language%20models%20have%20advanced%20rapidly%2C%20yet%20their%20design%20has%20largely%20been%20guided%20by%20heuristics%20rather%20than%20systematic%20theoretical%20principles.%20This%20gap%20limits%20our%20understanding%20of%20the%20properties%20of%20the%20gradient%20estimators%20and%20the%20associated%20optimization%20algorithms%2C%20thereby%20constraining%20opportunities%20to%20improve%20training%20stability%20and%20overall%20performance.%20In%20this%20work%2C%20we%20provide%20a%20unified%20theoretical%20framework%20that%20characterizes%20the%20statistical%20properties%20of%20commonly%20used%20policy-gradient%20estimators%20under%20mild%20assumptions.%20Our%20analysis%20establishes%20unbiasedness%2C%20derives%20exact%20variance%20expressions%2C%20and%20yields%20an%20optimization-loss%20upper%20bound%20that%20enables%20principled%20reasoning%20about%20learning%20dynamics.%20Building%20on%20these%20results%2C%20we%20prove%20convergence%20guarantees%20and%20derive%20an%20adaptive%20learning-rate%20schedule%20governed%20by%20the%20signal-to-noise%20ratio%20%28SNR%29%20of%20gradients.%20We%20further%20show%20that%20the%20variance-optimal%20baseline%20is%20a%20gradient-weighted%20estimator%2C%20offering%20a%20new%20principle%20for%20variance%20reduction%20and%20naturally%20enhancing%20stability%20beyond%20existing%20methods.%20These%20insights%20motivate%20Optimal%20Baseline%20and%20Learning-Rate%20Policy%20Optimization%20%28OBLR-PO%29%2C%20an%20algorithm%20that%20jointly%20adapts%20learning%20rates%20and%20baselines%20in%20a%20theoretically%20grounded%20manner.%20Experiments%20on%20Qwen3-4B-Base%20and%20Qwen3-8B-Base%20demonstrate%20consistent%20gains%20over%20existing%20policy%20optimization%20methods%2C%20validating%20that%20our%20theoretical%20contributions%20translate%20into%20practical%20improvements%20in%20large-scale%20post-training.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOBLR-PO%253A%2520A%2520Theoretical%2520Framework%2520for%2520Stable%2520Reinforcement%2520Learning%26entry.906535625%3DZixun%2520Huang%2520and%2520Jiayi%2520Sheng%2520and%2520Zeyu%2520Zheng%26entry.1292438233%3DExisting%2520reinforcement%2520learning%2520%2528RL%2529-based%2520post-training%2520methods%2520for%2520large%2520language%2520models%2520have%2520advanced%2520rapidly%252C%2520yet%2520their%2520design%2520has%2520largely%2520been%2520guided%2520by%2520heuristics%2520rather%2520than%2520systematic%2520theoretical%2520principles.%2520This%2520gap%2520limits%2520our%2520understanding%2520of%2520the%2520properties%2520of%2520the%2520gradient%2520estimators%2520and%2520the%2520associated%2520optimization%2520algorithms%252C%2520thereby%2520constraining%2520opportunities%2520to%2520improve%2520training%2520stability%2520and%2520overall%2520performance.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520unified%2520theoretical%2520framework%2520that%2520characterizes%2520the%2520statistical%2520properties%2520of%2520commonly%2520used%2520policy-gradient%2520estimators%2520under%2520mild%2520assumptions.%2520Our%2520analysis%2520establishes%2520unbiasedness%252C%2520derives%2520exact%2520variance%2520expressions%252C%2520and%2520yields%2520an%2520optimization-loss%2520upper%2520bound%2520that%2520enables%2520principled%2520reasoning%2520about%2520learning%2520dynamics.%2520Building%2520on%2520these%2520results%252C%2520we%2520prove%2520convergence%2520guarantees%2520and%2520derive%2520an%2520adaptive%2520learning-rate%2520schedule%2520governed%2520by%2520the%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520of%2520gradients.%2520We%2520further%2520show%2520that%2520the%2520variance-optimal%2520baseline%2520is%2520a%2520gradient-weighted%2520estimator%252C%2520offering%2520a%2520new%2520principle%2520for%2520variance%2520reduction%2520and%2520naturally%2520enhancing%2520stability%2520beyond%2520existing%2520methods.%2520These%2520insights%2520motivate%2520Optimal%2520Baseline%2520and%2520Learning-Rate%2520Policy%2520Optimization%2520%2528OBLR-PO%2529%252C%2520an%2520algorithm%2520that%2520jointly%2520adapts%2520learning%2520rates%2520and%2520baselines%2520in%2520a%2520theoretically%2520grounded%2520manner.%2520Experiments%2520on%2520Qwen3-4B-Base%2520and%2520Qwen3-8B-Base%2520demonstrate%2520consistent%2520gains%2520over%2520existing%2520policy%2520optimization%2520methods%252C%2520validating%2520that%2520our%2520theoretical%2520contributions%2520translate%2520into%2520practical%2520improvements%2520in%2520large-scale%2520post-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OBLR-PO%3A%20A%20Theoretical%20Framework%20for%20Stable%20Reinforcement%20Learning&entry.906535625=Zixun%20Huang%20and%20Jiayi%20Sheng%20and%20Zeyu%20Zheng&entry.1292438233=Existing%20reinforcement%20learning%20%28RL%29-based%20post-training%20methods%20for%20large%20language%20models%20have%20advanced%20rapidly%2C%20yet%20their%20design%20has%20largely%20been%20guided%20by%20heuristics%20rather%20than%20systematic%20theoretical%20principles.%20This%20gap%20limits%20our%20understanding%20of%20the%20properties%20of%20the%20gradient%20estimators%20and%20the%20associated%20optimization%20algorithms%2C%20thereby%20constraining%20opportunities%20to%20improve%20training%20stability%20and%20overall%20performance.%20In%20this%20work%2C%20we%20provide%20a%20unified%20theoretical%20framework%20that%20characterizes%20the%20statistical%20properties%20of%20commonly%20used%20policy-gradient%20estimators%20under%20mild%20assumptions.%20Our%20analysis%20establishes%20unbiasedness%2C%20derives%20exact%20variance%20expressions%2C%20and%20yields%20an%20optimization-loss%20upper%20bound%20that%20enables%20principled%20reasoning%20about%20learning%20dynamics.%20Building%20on%20these%20results%2C%20we%20prove%20convergence%20guarantees%20and%20derive%20an%20adaptive%20learning-rate%20schedule%20governed%20by%20the%20signal-to-noise%20ratio%20%28SNR%29%20of%20gradients.%20We%20further%20show%20that%20the%20variance-optimal%20baseline%20is%20a%20gradient-weighted%20estimator%2C%20offering%20a%20new%20principle%20for%20variance%20reduction%20and%20naturally%20enhancing%20stability%20beyond%20existing%20methods.%20These%20insights%20motivate%20Optimal%20Baseline%20and%20Learning-Rate%20Policy%20Optimization%20%28OBLR-PO%29%2C%20an%20algorithm%20that%20jointly%20adapts%20learning%20rates%20and%20baselines%20in%20a%20theoretically%20grounded%20manner.%20Experiments%20on%20Qwen3-4B-Base%20and%20Qwen3-8B-Base%20demonstrate%20consistent%20gains%20over%20existing%20policy%20optimization%20methods%2C%20validating%20that%20our%20theoretical%20contributions%20translate%20into%20practical%20improvements%20in%20large-scale%20post-training.&entry.1838667208=http%3A//arxiv.org/abs/2511.23310v1&entry.124074799=Read"},
{"title": "Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion", "author": "Keyang Lu and Sifan Zhou and Hongbin Xu and Gang Xu and Zhifei Yang and Yikai Wang and Zhen Xiao and Jieyi Long and Ming Li", "abstract": "Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.", "link": "http://arxiv.org/abs/2511.18734v2", "date": "2025-11-28", "relevancy": 2.4806, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6547}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6132}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yo%27City%3A%20Personalized%20and%20Boundless%203D%20Realistic%20City%20Scene%20Generation%20via%20Self-Critic%20Expansion&body=Title%3A%20Yo%27City%3A%20Personalized%20and%20Boundless%203D%20Realistic%20City%20Scene%20Generation%20via%20Self-Critic%20Expansion%0AAuthor%3A%20Keyang%20Lu%20and%20Sifan%20Zhou%20and%20Hongbin%20Xu%20and%20Gang%20Xu%20and%20Zhifei%20Yang%20and%20Yikai%20Wang%20and%20Zhen%20Xiao%20and%20Jieyi%20Long%20and%20Ming%20Li%0AAbstract%3A%20Realistic%203D%20city%20generation%20is%20fundamental%20to%20a%20wide%20range%20of%20applications%2C%20including%20virtual%20reality%20and%20digital%20twins.%20However%2C%20most%20existing%20methods%20rely%20on%20training%20a%20single%20diffusion%20model%2C%20which%20limits%20their%20ability%20to%20generate%20personalized%20and%20boundless%20city-scale%20scenes.%20In%20this%20paper%2C%20we%20present%20Yo%27City%2C%20a%20novel%20agentic%20framework%20that%20enables%20user-customized%20and%20infinitely%20expandable%203D%20city%20generation%20by%20leveraging%20the%20reasoning%20and%20compositional%20capabilities%20of%20off-the-shelf%20large%20models.%20Specifically%2C%20Yo%27City%20first%20conceptualize%20the%20city%20through%20a%20top-down%20planning%20strategy%20that%20defines%20a%20hierarchical%20%22City-District-Grid%22%20structure.%20The%20Global%20Planner%20determines%20the%20overall%20layout%20and%20potential%20functional%20districts%2C%20while%20the%20Local%20Designer%20further%20refines%20each%20district%20with%20detailed%20grid-level%20descriptions.%20Subsequently%2C%20the%20grid-level%203D%20generation%20is%20achieved%20through%20a%20%22produce-refine-evaluate%22%20isometric%20image%20synthesis%20loop%2C%20followed%20by%20image-to-3D%20generation.%20To%20simulate%20continuous%20city%20evolution%2C%20Yo%27City%20further%20introduces%20a%20user-interactive%2C%20relationship-guided%20expansion%20mechanism%2C%20which%20performs%20scene%20graph-based%20distance-%20and%20semantics-aware%20layout%20optimization%2C%20ensuring%20spatially%20coherent%20city%20growth.%20To%20comprehensively%20evaluate%20our%20method%2C%20we%20construct%20a%20diverse%20benchmark%20dataset%20and%20design%20six%20multi-dimensional%20metrics%20that%20assess%20generation%20quality%20from%20the%20perspectives%20of%20semantics%2C%20geometry%2C%20texture%2C%20and%20layout.%20Extensive%20experiments%20demonstrate%20that%20Yo%27City%20consistently%20outperforms%20existing%20state-of-the-art%20methods%20across%20all%20evaluation%20aspects.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYo%2527City%253A%2520Personalized%2520and%2520Boundless%25203D%2520Realistic%2520City%2520Scene%2520Generation%2520via%2520Self-Critic%2520Expansion%26entry.906535625%3DKeyang%2520Lu%2520and%2520Sifan%2520Zhou%2520and%2520Hongbin%2520Xu%2520and%2520Gang%2520Xu%2520and%2520Zhifei%2520Yang%2520and%2520Yikai%2520Wang%2520and%2520Zhen%2520Xiao%2520and%2520Jieyi%2520Long%2520and%2520Ming%2520Li%26entry.1292438233%3DRealistic%25203D%2520city%2520generation%2520is%2520fundamental%2520to%2520a%2520wide%2520range%2520of%2520applications%252C%2520including%2520virtual%2520reality%2520and%2520digital%2520twins.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520on%2520training%2520a%2520single%2520diffusion%2520model%252C%2520which%2520limits%2520their%2520ability%2520to%2520generate%2520personalized%2520and%2520boundless%2520city-scale%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520present%2520Yo%2527City%252C%2520a%2520novel%2520agentic%2520framework%2520that%2520enables%2520user-customized%2520and%2520infinitely%2520expandable%25203D%2520city%2520generation%2520by%2520leveraging%2520the%2520reasoning%2520and%2520compositional%2520capabilities%2520of%2520off-the-shelf%2520large%2520models.%2520Specifically%252C%2520Yo%2527City%2520first%2520conceptualize%2520the%2520city%2520through%2520a%2520top-down%2520planning%2520strategy%2520that%2520defines%2520a%2520hierarchical%2520%2522City-District-Grid%2522%2520structure.%2520The%2520Global%2520Planner%2520determines%2520the%2520overall%2520layout%2520and%2520potential%2520functional%2520districts%252C%2520while%2520the%2520Local%2520Designer%2520further%2520refines%2520each%2520district%2520with%2520detailed%2520grid-level%2520descriptions.%2520Subsequently%252C%2520the%2520grid-level%25203D%2520generation%2520is%2520achieved%2520through%2520a%2520%2522produce-refine-evaluate%2522%2520isometric%2520image%2520synthesis%2520loop%252C%2520followed%2520by%2520image-to-3D%2520generation.%2520To%2520simulate%2520continuous%2520city%2520evolution%252C%2520Yo%2527City%2520further%2520introduces%2520a%2520user-interactive%252C%2520relationship-guided%2520expansion%2520mechanism%252C%2520which%2520performs%2520scene%2520graph-based%2520distance-%2520and%2520semantics-aware%2520layout%2520optimization%252C%2520ensuring%2520spatially%2520coherent%2520city%2520growth.%2520To%2520comprehensively%2520evaluate%2520our%2520method%252C%2520we%2520construct%2520a%2520diverse%2520benchmark%2520dataset%2520and%2520design%2520six%2520multi-dimensional%2520metrics%2520that%2520assess%2520generation%2520quality%2520from%2520the%2520perspectives%2520of%2520semantics%252C%2520geometry%252C%2520texture%252C%2520and%2520layout.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Yo%2527City%2520consistently%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520across%2520all%2520evaluation%2520aspects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yo%27City%3A%20Personalized%20and%20Boundless%203D%20Realistic%20City%20Scene%20Generation%20via%20Self-Critic%20Expansion&entry.906535625=Keyang%20Lu%20and%20Sifan%20Zhou%20and%20Hongbin%20Xu%20and%20Gang%20Xu%20and%20Zhifei%20Yang%20and%20Yikai%20Wang%20and%20Zhen%20Xiao%20and%20Jieyi%20Long%20and%20Ming%20Li&entry.1292438233=Realistic%203D%20city%20generation%20is%20fundamental%20to%20a%20wide%20range%20of%20applications%2C%20including%20virtual%20reality%20and%20digital%20twins.%20However%2C%20most%20existing%20methods%20rely%20on%20training%20a%20single%20diffusion%20model%2C%20which%20limits%20their%20ability%20to%20generate%20personalized%20and%20boundless%20city-scale%20scenes.%20In%20this%20paper%2C%20we%20present%20Yo%27City%2C%20a%20novel%20agentic%20framework%20that%20enables%20user-customized%20and%20infinitely%20expandable%203D%20city%20generation%20by%20leveraging%20the%20reasoning%20and%20compositional%20capabilities%20of%20off-the-shelf%20large%20models.%20Specifically%2C%20Yo%27City%20first%20conceptualize%20the%20city%20through%20a%20top-down%20planning%20strategy%20that%20defines%20a%20hierarchical%20%22City-District-Grid%22%20structure.%20The%20Global%20Planner%20determines%20the%20overall%20layout%20and%20potential%20functional%20districts%2C%20while%20the%20Local%20Designer%20further%20refines%20each%20district%20with%20detailed%20grid-level%20descriptions.%20Subsequently%2C%20the%20grid-level%203D%20generation%20is%20achieved%20through%20a%20%22produce-refine-evaluate%22%20isometric%20image%20synthesis%20loop%2C%20followed%20by%20image-to-3D%20generation.%20To%20simulate%20continuous%20city%20evolution%2C%20Yo%27City%20further%20introduces%20a%20user-interactive%2C%20relationship-guided%20expansion%20mechanism%2C%20which%20performs%20scene%20graph-based%20distance-%20and%20semantics-aware%20layout%20optimization%2C%20ensuring%20spatially%20coherent%20city%20growth.%20To%20comprehensively%20evaluate%20our%20method%2C%20we%20construct%20a%20diverse%20benchmark%20dataset%20and%20design%20six%20multi-dimensional%20metrics%20that%20assess%20generation%20quality%20from%20the%20perspectives%20of%20semantics%2C%20geometry%2C%20texture%2C%20and%20layout.%20Extensive%20experiments%20demonstrate%20that%20Yo%27City%20consistently%20outperforms%20existing%20state-of-the-art%20methods%20across%20all%20evaluation%20aspects.&entry.1838667208=http%3A//arxiv.org/abs/2511.18734v2&entry.124074799=Read"},
{"title": "Privacy Reasoning in Ambiguous Contexts", "author": "Ren Yi and Octavian Suciu and Adria Gascon and Sarah Meiklejohn and Eugene Bagdasarian and Marco Gruteser", "abstract": "We study the ability of language models to reason about appropriate information disclosure - a central aspect of the evolving field of agentic privacy. Whereas previous works have focused on evaluating a model's ability to align with human decisions, we examine the role of ambiguity and missing context on model performance when making information-sharing decisions. We identify context ambiguity as a crucial barrier for high performance in privacy assessments. By designing Camber, a framework for context disambiguation, we show that model-generated decision rationales can reveal ambiguities and that systematically disambiguating context based on these rationales leads to significant accuracy improvements (up to 13.3% in precision and up to 22.3% in recall) as well as reductions in prompt sensitivity. Overall, our results indicate that approaches for context disambiguation are a promising way forward to enhance agentic privacy reasoning.", "link": "http://arxiv.org/abs/2506.12241v2", "date": "2025-11-28", "relevancy": 2.48, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20Reasoning%20in%20Ambiguous%20Contexts&body=Title%3A%20Privacy%20Reasoning%20in%20Ambiguous%20Contexts%0AAuthor%3A%20Ren%20Yi%20and%20Octavian%20Suciu%20and%20Adria%20Gascon%20and%20Sarah%20Meiklejohn%20and%20Eugene%20Bagdasarian%20and%20Marco%20Gruteser%0AAbstract%3A%20We%20study%20the%20ability%20of%20language%20models%20to%20reason%20about%20appropriate%20information%20disclosure%20-%20a%20central%20aspect%20of%20the%20evolving%20field%20of%20agentic%20privacy.%20Whereas%20previous%20works%20have%20focused%20on%20evaluating%20a%20model%27s%20ability%20to%20align%20with%20human%20decisions%2C%20we%20examine%20the%20role%20of%20ambiguity%20and%20missing%20context%20on%20model%20performance%20when%20making%20information-sharing%20decisions.%20We%20identify%20context%20ambiguity%20as%20a%20crucial%20barrier%20for%20high%20performance%20in%20privacy%20assessments.%20By%20designing%20Camber%2C%20a%20framework%20for%20context%20disambiguation%2C%20we%20show%20that%20model-generated%20decision%20rationales%20can%20reveal%20ambiguities%20and%20that%20systematically%20disambiguating%20context%20based%20on%20these%20rationales%20leads%20to%20significant%20accuracy%20improvements%20%28up%20to%2013.3%25%20in%20precision%20and%20up%20to%2022.3%25%20in%20recall%29%20as%20well%20as%20reductions%20in%20prompt%20sensitivity.%20Overall%2C%20our%20results%20indicate%20that%20approaches%20for%20context%20disambiguation%20are%20a%20promising%20way%20forward%20to%20enhance%20agentic%20privacy%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2506.12241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520Reasoning%2520in%2520Ambiguous%2520Contexts%26entry.906535625%3DRen%2520Yi%2520and%2520Octavian%2520Suciu%2520and%2520Adria%2520Gascon%2520and%2520Sarah%2520Meiklejohn%2520and%2520Eugene%2520Bagdasarian%2520and%2520Marco%2520Gruteser%26entry.1292438233%3DWe%2520study%2520the%2520ability%2520of%2520language%2520models%2520to%2520reason%2520about%2520appropriate%2520information%2520disclosure%2520-%2520a%2520central%2520aspect%2520of%2520the%2520evolving%2520field%2520of%2520agentic%2520privacy.%2520Whereas%2520previous%2520works%2520have%2520focused%2520on%2520evaluating%2520a%2520model%2527s%2520ability%2520to%2520align%2520with%2520human%2520decisions%252C%2520we%2520examine%2520the%2520role%2520of%2520ambiguity%2520and%2520missing%2520context%2520on%2520model%2520performance%2520when%2520making%2520information-sharing%2520decisions.%2520We%2520identify%2520context%2520ambiguity%2520as%2520a%2520crucial%2520barrier%2520for%2520high%2520performance%2520in%2520privacy%2520assessments.%2520By%2520designing%2520Camber%252C%2520a%2520framework%2520for%2520context%2520disambiguation%252C%2520we%2520show%2520that%2520model-generated%2520decision%2520rationales%2520can%2520reveal%2520ambiguities%2520and%2520that%2520systematically%2520disambiguating%2520context%2520based%2520on%2520these%2520rationales%2520leads%2520to%2520significant%2520accuracy%2520improvements%2520%2528up%2520to%252013.3%2525%2520in%2520precision%2520and%2520up%2520to%252022.3%2525%2520in%2520recall%2529%2520as%2520well%2520as%2520reductions%2520in%2520prompt%2520sensitivity.%2520Overall%252C%2520our%2520results%2520indicate%2520that%2520approaches%2520for%2520context%2520disambiguation%2520are%2520a%2520promising%2520way%2520forward%2520to%2520enhance%2520agentic%2520privacy%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20Reasoning%20in%20Ambiguous%20Contexts&entry.906535625=Ren%20Yi%20and%20Octavian%20Suciu%20and%20Adria%20Gascon%20and%20Sarah%20Meiklejohn%20and%20Eugene%20Bagdasarian%20and%20Marco%20Gruteser&entry.1292438233=We%20study%20the%20ability%20of%20language%20models%20to%20reason%20about%20appropriate%20information%20disclosure%20-%20a%20central%20aspect%20of%20the%20evolving%20field%20of%20agentic%20privacy.%20Whereas%20previous%20works%20have%20focused%20on%20evaluating%20a%20model%27s%20ability%20to%20align%20with%20human%20decisions%2C%20we%20examine%20the%20role%20of%20ambiguity%20and%20missing%20context%20on%20model%20performance%20when%20making%20information-sharing%20decisions.%20We%20identify%20context%20ambiguity%20as%20a%20crucial%20barrier%20for%20high%20performance%20in%20privacy%20assessments.%20By%20designing%20Camber%2C%20a%20framework%20for%20context%20disambiguation%2C%20we%20show%20that%20model-generated%20decision%20rationales%20can%20reveal%20ambiguities%20and%20that%20systematically%20disambiguating%20context%20based%20on%20these%20rationales%20leads%20to%20significant%20accuracy%20improvements%20%28up%20to%2013.3%25%20in%20precision%20and%20up%20to%2022.3%25%20in%20recall%29%20as%20well%20as%20reductions%20in%20prompt%20sensitivity.%20Overall%2C%20our%20results%20indicate%20that%20approaches%20for%20context%20disambiguation%20are%20a%20promising%20way%20forward%20to%20enhance%20agentic%20privacy%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2506.12241v2&entry.124074799=Read"},
{"title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge", "author": "Lei Zan and Keli Zhang and Ruichu Cai and Lujia Pan", "abstract": "Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician (\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.", "link": "http://arxiv.org/abs/2508.02583v4", "date": "2025-11-28", "relevancy": 2.4482, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMA%3A%20Enhancing%20Mathematical%20Reasoning%20in%20Large%20Language%20Models%20with%20Causal%20Knowledge&body=Title%3A%20CAMA%3A%20Enhancing%20Mathematical%20Reasoning%20in%20Large%20Language%20Models%20with%20Causal%20Knowledge%0AAuthor%3A%20Lei%20Zan%20and%20Keli%20Zhang%20and%20Ruichu%20Cai%20and%20Lujia%20Pan%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20still%20struggle%20with%20complex%20mathematical%20reasoning%2C%20a%20challenge%20fundamentally%20rooted%20in%20deep%20structural%20dependencies.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BCA%7Dusal%20%5Ctextbf%7BMA%7Dthematician%20%28%5Ctextbf%7BCAMA%7D%29%2C%20a%20two-stage%20causal%20framework%20that%20equips%20LLMs%20with%20explicit%2C%20reusable%20mathematical%20structure.%20In%20the%20learning%20stage%2C%20CAMA%20first%20constructs%20the%20%5Ctextbf%7BM%7Dathematical%20%5Ctextbf%7BC%7Dausal%20%5Ctextbf%7BG%7Draph%20%28%5Ctextbf%7BMCG%7D%29%2C%20a%20high-level%20representation%20of%20solution%20strategies%2C%20by%20combining%20LLM%20priors%20with%20causal%20discovery%20algorithms%20applied%20to%20a%20corpus%20of%20question-solution%20pairs.%20The%20resulting%20MCG%20encodes%20essential%20knowledge%20points%20and%20their%20causal%20dependencies.%20To%20better%20align%20the%20graph%20with%20downstream%20reasoning%20tasks%2C%20CAMA%20further%20refines%20the%20MCG%20through%20iterative%20feedback%20derived%20from%20a%20selected%20subset%20of%20the%20question-solution%20pairs.%20In%20the%20reasoning%20stage%2C%20given%20a%20new%20question%2C%20CAMA%20dynamically%20extracts%20a%20task-relevant%20subgraph%20from%20the%20MCG%2C%20conditioned%20on%20both%20the%20question%20content%20and%20the%20LLM%27s%20intermediate%20reasoning%20trace.%20This%20subgraph%2C%20which%20encodes%20the%20most%20pertinent%20knowledge%20points%20and%20their%20causal%20dependencies%2C%20is%20then%20injected%20back%20into%20the%20LLM%20to%20guide%20its%20reasoning%20process.%20Empirical%20results%20on%20real-world%20datasets%20show%20that%20CAMA%20significantly%20improves%20LLM%20performance%20on%20challenging%20mathematical%20problems.%20Furthermore%2C%20our%20experiments%20demonstrate%20that%20structured%20guidance%20consistently%20outperforms%20unstructured%20alternatives%2C%20and%20that%20incorporating%20asymmetric%20causal%20relationships%20yields%20greater%20improvements%20than%20using%20symmetric%20associations%20alone.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02583v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMA%253A%2520Enhancing%2520Mathematical%2520Reasoning%2520in%2520Large%2520Language%2520Models%2520with%2520Causal%2520Knowledge%26entry.906535625%3DLei%2520Zan%2520and%2520Keli%2520Zhang%2520and%2520Ruichu%2520Cai%2520and%2520Lujia%2520Pan%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520yet%2520they%2520still%2520struggle%2520with%2520complex%2520mathematical%2520reasoning%252C%2520a%2520challenge%2520fundamentally%2520rooted%2520in%2520deep%2520structural%2520dependencies.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520%255Ctextbf%257BCA%257Dusal%2520%255Ctextbf%257BMA%257Dthematician%2520%2528%255Ctextbf%257BCAMA%257D%2529%252C%2520a%2520two-stage%2520causal%2520framework%2520that%2520equips%2520LLMs%2520with%2520explicit%252C%2520reusable%2520mathematical%2520structure.%2520In%2520the%2520learning%2520stage%252C%2520CAMA%2520first%2520constructs%2520the%2520%255Ctextbf%257BM%257Dathematical%2520%255Ctextbf%257BC%257Dausal%2520%255Ctextbf%257BG%257Draph%2520%2528%255Ctextbf%257BMCG%257D%2529%252C%2520a%2520high-level%2520representation%2520of%2520solution%2520strategies%252C%2520by%2520combining%2520LLM%2520priors%2520with%2520causal%2520discovery%2520algorithms%2520applied%2520to%2520a%2520corpus%2520of%2520question-solution%2520pairs.%2520The%2520resulting%2520MCG%2520encodes%2520essential%2520knowledge%2520points%2520and%2520their%2520causal%2520dependencies.%2520To%2520better%2520align%2520the%2520graph%2520with%2520downstream%2520reasoning%2520tasks%252C%2520CAMA%2520further%2520refines%2520the%2520MCG%2520through%2520iterative%2520feedback%2520derived%2520from%2520a%2520selected%2520subset%2520of%2520the%2520question-solution%2520pairs.%2520In%2520the%2520reasoning%2520stage%252C%2520given%2520a%2520new%2520question%252C%2520CAMA%2520dynamically%2520extracts%2520a%2520task-relevant%2520subgraph%2520from%2520the%2520MCG%252C%2520conditioned%2520on%2520both%2520the%2520question%2520content%2520and%2520the%2520LLM%2527s%2520intermediate%2520reasoning%2520trace.%2520This%2520subgraph%252C%2520which%2520encodes%2520the%2520most%2520pertinent%2520knowledge%2520points%2520and%2520their%2520causal%2520dependencies%252C%2520is%2520then%2520injected%2520back%2520into%2520the%2520LLM%2520to%2520guide%2520its%2520reasoning%2520process.%2520Empirical%2520results%2520on%2520real-world%2520datasets%2520show%2520that%2520CAMA%2520significantly%2520improves%2520LLM%2520performance%2520on%2520challenging%2520mathematical%2520problems.%2520Furthermore%252C%2520our%2520experiments%2520demonstrate%2520that%2520structured%2520guidance%2520consistently%2520outperforms%2520unstructured%2520alternatives%252C%2520and%2520that%2520incorporating%2520asymmetric%2520causal%2520relationships%2520yields%2520greater%2520improvements%2520than%2520using%2520symmetric%2520associations%2520alone.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02583v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMA%3A%20Enhancing%20Mathematical%20Reasoning%20in%20Large%20Language%20Models%20with%20Causal%20Knowledge&entry.906535625=Lei%20Zan%20and%20Keli%20Zhang%20and%20Ruichu%20Cai%20and%20Lujia%20Pan&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20still%20struggle%20with%20complex%20mathematical%20reasoning%2C%20a%20challenge%20fundamentally%20rooted%20in%20deep%20structural%20dependencies.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BCA%7Dusal%20%5Ctextbf%7BMA%7Dthematician%20%28%5Ctextbf%7BCAMA%7D%29%2C%20a%20two-stage%20causal%20framework%20that%20equips%20LLMs%20with%20explicit%2C%20reusable%20mathematical%20structure.%20In%20the%20learning%20stage%2C%20CAMA%20first%20constructs%20the%20%5Ctextbf%7BM%7Dathematical%20%5Ctextbf%7BC%7Dausal%20%5Ctextbf%7BG%7Draph%20%28%5Ctextbf%7BMCG%7D%29%2C%20a%20high-level%20representation%20of%20solution%20strategies%2C%20by%20combining%20LLM%20priors%20with%20causal%20discovery%20algorithms%20applied%20to%20a%20corpus%20of%20question-solution%20pairs.%20The%20resulting%20MCG%20encodes%20essential%20knowledge%20points%20and%20their%20causal%20dependencies.%20To%20better%20align%20the%20graph%20with%20downstream%20reasoning%20tasks%2C%20CAMA%20further%20refines%20the%20MCG%20through%20iterative%20feedback%20derived%20from%20a%20selected%20subset%20of%20the%20question-solution%20pairs.%20In%20the%20reasoning%20stage%2C%20given%20a%20new%20question%2C%20CAMA%20dynamically%20extracts%20a%20task-relevant%20subgraph%20from%20the%20MCG%2C%20conditioned%20on%20both%20the%20question%20content%20and%20the%20LLM%27s%20intermediate%20reasoning%20trace.%20This%20subgraph%2C%20which%20encodes%20the%20most%20pertinent%20knowledge%20points%20and%20their%20causal%20dependencies%2C%20is%20then%20injected%20back%20into%20the%20LLM%20to%20guide%20its%20reasoning%20process.%20Empirical%20results%20on%20real-world%20datasets%20show%20that%20CAMA%20significantly%20improves%20LLM%20performance%20on%20challenging%20mathematical%20problems.%20Furthermore%2C%20our%20experiments%20demonstrate%20that%20structured%20guidance%20consistently%20outperforms%20unstructured%20alternatives%2C%20and%20that%20incorporating%20asymmetric%20causal%20relationships%20yields%20greater%20improvements%20than%20using%20symmetric%20associations%20alone.&entry.1838667208=http%3A//arxiv.org/abs/2508.02583v4&entry.124074799=Read"},
{"title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement", "author": "Zhizhou Zhong and Yicheng Ji and Zhe Kong and Yiying Liu and Jiarui Wang and Jiasun Feng and Lupeng Liu and Xiangyi Wang and Yanjia Li and Yuqing She and Ying Qin and Huan Li and Shuiyang Mao and Wei Liu and Wenhan Luo", "abstract": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.", "link": "http://arxiv.org/abs/2511.23475v1", "date": "2025-11-28", "relevancy": 2.4312, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6527}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyTalker%3A%20Scaling%20Multi-Person%20Talking%20Video%20Generation%20with%20Interactivity%20Refinement&body=Title%3A%20AnyTalker%3A%20Scaling%20Multi-Person%20Talking%20Video%20Generation%20with%20Interactivity%20Refinement%0AAuthor%3A%20Zhizhou%20Zhong%20and%20Yicheng%20Ji%20and%20Zhe%20Kong%20and%20Yiying%20Liu%20and%20Jiarui%20Wang%20and%20Jiasun%20Feng%20and%20Lupeng%20Liu%20and%20Xiangyi%20Wang%20and%20Yanjia%20Li%20and%20Yuqing%20She%20and%20Ying%20Qin%20and%20Huan%20Li%20and%20Shuiyang%20Mao%20and%20Wei%20Liu%20and%20Wenhan%20Luo%0AAbstract%3A%20Recently%2C%20multi-person%20video%20generation%20has%20started%20to%20gain%20prominence.%20While%20a%20few%20preliminary%20works%20have%20explored%20audio-driven%20multi-person%20talking%20video%20generation%2C%20they%20often%20face%20challenges%20due%20to%20the%20high%20costs%20of%20diverse%20multi-person%20data%20collection%20and%20the%20difficulty%20of%20driving%20multiple%20identities%20with%20coherent%20interactivity.%20To%20address%20these%20challenges%2C%20we%20propose%20AnyTalker%2C%20a%20multi-person%20generation%20framework%20that%20features%20an%20extensible%20multi-stream%20processing%20architecture.%20Specifically%2C%20we%20extend%20Diffusion%20Transformer%27s%20attention%20block%20with%20a%20novel%20identity-aware%20attention%20mechanism%20that%20iteratively%20processes%20identity-audio%20pairs%2C%20allowing%20arbitrary%20scaling%20of%20drivable%20identities.%20Besides%2C%20training%20multi-person%20generative%20models%20demands%20massive%20multi-person%20data.%20Our%20proposed%20training%20pipeline%20depends%20solely%20on%20single-person%20videos%20to%20learn%20multi-person%20speaking%20patterns%20and%20refines%20interactivity%20with%20only%20a%20few%20real%20multi-person%20clips.%20Furthermore%2C%20we%20contribute%20a%20targeted%20metric%20and%20dataset%20designed%20to%20evaluate%20the%20naturalness%20and%20interactivity%20of%20the%20generated%20multi-person%20videos.%20Extensive%20experiments%20demonstrate%20that%20AnyTalker%20achieves%20remarkable%20lip%20synchronization%2C%20visual%20quality%2C%20and%20natural%20interactivity%2C%20striking%20a%20favorable%20balance%20between%20data%20costs%20and%20identity%20scalability.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyTalker%253A%2520Scaling%2520Multi-Person%2520Talking%2520Video%2520Generation%2520with%2520Interactivity%2520Refinement%26entry.906535625%3DZhizhou%2520Zhong%2520and%2520Yicheng%2520Ji%2520and%2520Zhe%2520Kong%2520and%2520Yiying%2520Liu%2520and%2520Jiarui%2520Wang%2520and%2520Jiasun%2520Feng%2520and%2520Lupeng%2520Liu%2520and%2520Xiangyi%2520Wang%2520and%2520Yanjia%2520Li%2520and%2520Yuqing%2520She%2520and%2520Ying%2520Qin%2520and%2520Huan%2520Li%2520and%2520Shuiyang%2520Mao%2520and%2520Wei%2520Liu%2520and%2520Wenhan%2520Luo%26entry.1292438233%3DRecently%252C%2520multi-person%2520video%2520generation%2520has%2520started%2520to%2520gain%2520prominence.%2520While%2520a%2520few%2520preliminary%2520works%2520have%2520explored%2520audio-driven%2520multi-person%2520talking%2520video%2520generation%252C%2520they%2520often%2520face%2520challenges%2520due%2520to%2520the%2520high%2520costs%2520of%2520diverse%2520multi-person%2520data%2520collection%2520and%2520the%2520difficulty%2520of%2520driving%2520multiple%2520identities%2520with%2520coherent%2520interactivity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520AnyTalker%252C%2520a%2520multi-person%2520generation%2520framework%2520that%2520features%2520an%2520extensible%2520multi-stream%2520processing%2520architecture.%2520Specifically%252C%2520we%2520extend%2520Diffusion%2520Transformer%2527s%2520attention%2520block%2520with%2520a%2520novel%2520identity-aware%2520attention%2520mechanism%2520that%2520iteratively%2520processes%2520identity-audio%2520pairs%252C%2520allowing%2520arbitrary%2520scaling%2520of%2520drivable%2520identities.%2520Besides%252C%2520training%2520multi-person%2520generative%2520models%2520demands%2520massive%2520multi-person%2520data.%2520Our%2520proposed%2520training%2520pipeline%2520depends%2520solely%2520on%2520single-person%2520videos%2520to%2520learn%2520multi-person%2520speaking%2520patterns%2520and%2520refines%2520interactivity%2520with%2520only%2520a%2520few%2520real%2520multi-person%2520clips.%2520Furthermore%252C%2520we%2520contribute%2520a%2520targeted%2520metric%2520and%2520dataset%2520designed%2520to%2520evaluate%2520the%2520naturalness%2520and%2520interactivity%2520of%2520the%2520generated%2520multi-person%2520videos.%2520Extensive%2520experiments%2520demonstrate%2520that%2520AnyTalker%2520achieves%2520remarkable%2520lip%2520synchronization%252C%2520visual%2520quality%252C%2520and%2520natural%2520interactivity%252C%2520striking%2520a%2520favorable%2520balance%2520between%2520data%2520costs%2520and%2520identity%2520scalability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyTalker%3A%20Scaling%20Multi-Person%20Talking%20Video%20Generation%20with%20Interactivity%20Refinement&entry.906535625=Zhizhou%20Zhong%20and%20Yicheng%20Ji%20and%20Zhe%20Kong%20and%20Yiying%20Liu%20and%20Jiarui%20Wang%20and%20Jiasun%20Feng%20and%20Lupeng%20Liu%20and%20Xiangyi%20Wang%20and%20Yanjia%20Li%20and%20Yuqing%20She%20and%20Ying%20Qin%20and%20Huan%20Li%20and%20Shuiyang%20Mao%20and%20Wei%20Liu%20and%20Wenhan%20Luo&entry.1292438233=Recently%2C%20multi-person%20video%20generation%20has%20started%20to%20gain%20prominence.%20While%20a%20few%20preliminary%20works%20have%20explored%20audio-driven%20multi-person%20talking%20video%20generation%2C%20they%20often%20face%20challenges%20due%20to%20the%20high%20costs%20of%20diverse%20multi-person%20data%20collection%20and%20the%20difficulty%20of%20driving%20multiple%20identities%20with%20coherent%20interactivity.%20To%20address%20these%20challenges%2C%20we%20propose%20AnyTalker%2C%20a%20multi-person%20generation%20framework%20that%20features%20an%20extensible%20multi-stream%20processing%20architecture.%20Specifically%2C%20we%20extend%20Diffusion%20Transformer%27s%20attention%20block%20with%20a%20novel%20identity-aware%20attention%20mechanism%20that%20iteratively%20processes%20identity-audio%20pairs%2C%20allowing%20arbitrary%20scaling%20of%20drivable%20identities.%20Besides%2C%20training%20multi-person%20generative%20models%20demands%20massive%20multi-person%20data.%20Our%20proposed%20training%20pipeline%20depends%20solely%20on%20single-person%20videos%20to%20learn%20multi-person%20speaking%20patterns%20and%20refines%20interactivity%20with%20only%20a%20few%20real%20multi-person%20clips.%20Furthermore%2C%20we%20contribute%20a%20targeted%20metric%20and%20dataset%20designed%20to%20evaluate%20the%20naturalness%20and%20interactivity%20of%20the%20generated%20multi-person%20videos.%20Extensive%20experiments%20demonstrate%20that%20AnyTalker%20achieves%20remarkable%20lip%20synchronization%2C%20visual%20quality%2C%20and%20natural%20interactivity%2C%20striking%20a%20favorable%20balance%20between%20data%20costs%20and%20identity%20scalability.&entry.1838667208=http%3A//arxiv.org/abs/2511.23475v1&entry.124074799=Read"},
{"title": "Zero-Shot Multi-Criteria Visual Quality Inspection for Semi-Controlled Industrial Environments via Real-Time 3D Digital Twin Simulation", "author": "Jose Moises Araya-Martinez and Gautham Mohan and Kenichi Hayakawa Bola\u00f1os and Roberto Mendieta and Sarvenaz Sardari and Jens Lambrecht and J\u00f6rg Kr\u00fcger", "abstract": "Early-stage visual quality inspection is vital for achieving Zero-Defect Manufacturing and minimizing production waste in modern industrial environments. However, the complexity of robust visual inspection systems and their extensive data requirements hinder widespread adoption in semi-controlled industrial settings. In this context, we propose a pose-agnostic, zero-shot quality inspection framework that compares real scenes against real-time Digital Twins (DT) in the RGB-D space. Our approach enables efficient real-time DT rendering by semantically describing industrial scenes through object detection and pose estimation of known Computer-Aided Design models. We benchmark tools for real-time, multimodal RGB-D DT creation while tracking consumption of computational resources. Additionally, we provide an extensible and hierarchical annotation strategy for multi-criteria defect detection, unifying pose labelling with logical and structural defect annotations. Based on an automotive use case featuring the quality inspection of an axial flux motor, we demonstrate the effectiveness of our framework. Our results demonstrate detection performace, achieving intersection-over-union (IoU) scores of up to 63.3% compared to ground-truth masks, even if using simple distance measurements under semi-controlled industrial conditions. Our findings lay the groundwork for future research on generalizable, low-data defect detection methods in dynamic manufacturing settings.", "link": "http://arxiv.org/abs/2511.23214v1", "date": "2025-11-28", "relevancy": 2.4184, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6167}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6022}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Multi-Criteria%20Visual%20Quality%20Inspection%20for%20Semi-Controlled%20Industrial%20Environments%20via%20Real-Time%203D%20Digital%20Twin%20Simulation&body=Title%3A%20Zero-Shot%20Multi-Criteria%20Visual%20Quality%20Inspection%20for%20Semi-Controlled%20Industrial%20Environments%20via%20Real-Time%203D%20Digital%20Twin%20Simulation%0AAuthor%3A%20Jose%20Moises%20Araya-Martinez%20and%20Gautham%20Mohan%20and%20Kenichi%20Hayakawa%20Bola%C3%B1os%20and%20Roberto%20Mendieta%20and%20Sarvenaz%20Sardari%20and%20Jens%20Lambrecht%20and%20J%C3%B6rg%20Kr%C3%BCger%0AAbstract%3A%20Early-stage%20visual%20quality%20inspection%20is%20vital%20for%20achieving%20Zero-Defect%20Manufacturing%20and%20minimizing%20production%20waste%20in%20modern%20industrial%20environments.%20However%2C%20the%20complexity%20of%20robust%20visual%20inspection%20systems%20and%20their%20extensive%20data%20requirements%20hinder%20widespread%20adoption%20in%20semi-controlled%20industrial%20settings.%20In%20this%20context%2C%20we%20propose%20a%20pose-agnostic%2C%20zero-shot%20quality%20inspection%20framework%20that%20compares%20real%20scenes%20against%20real-time%20Digital%20Twins%20%28DT%29%20in%20the%20RGB-D%20space.%20Our%20approach%20enables%20efficient%20real-time%20DT%20rendering%20by%20semantically%20describing%20industrial%20scenes%20through%20object%20detection%20and%20pose%20estimation%20of%20known%20Computer-Aided%20Design%20models.%20We%20benchmark%20tools%20for%20real-time%2C%20multimodal%20RGB-D%20DT%20creation%20while%20tracking%20consumption%20of%20computational%20resources.%20Additionally%2C%20we%20provide%20an%20extensible%20and%20hierarchical%20annotation%20strategy%20for%20multi-criteria%20defect%20detection%2C%20unifying%20pose%20labelling%20with%20logical%20and%20structural%20defect%20annotations.%20Based%20on%20an%20automotive%20use%20case%20featuring%20the%20quality%20inspection%20of%20an%20axial%20flux%20motor%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework.%20Our%20results%20demonstrate%20detection%20performace%2C%20achieving%20intersection-over-union%20%28IoU%29%20scores%20of%20up%20to%2063.3%25%20compared%20to%20ground-truth%20masks%2C%20even%20if%20using%20simple%20distance%20measurements%20under%20semi-controlled%20industrial%20conditions.%20Our%20findings%20lay%20the%20groundwork%20for%20future%20research%20on%20generalizable%2C%20low-data%20defect%20detection%20methods%20in%20dynamic%20manufacturing%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Multi-Criteria%2520Visual%2520Quality%2520Inspection%2520for%2520Semi-Controlled%2520Industrial%2520Environments%2520via%2520Real-Time%25203D%2520Digital%2520Twin%2520Simulation%26entry.906535625%3DJose%2520Moises%2520Araya-Martinez%2520and%2520Gautham%2520Mohan%2520and%2520Kenichi%2520Hayakawa%2520Bola%25C3%25B1os%2520and%2520Roberto%2520Mendieta%2520and%2520Sarvenaz%2520Sardari%2520and%2520Jens%2520Lambrecht%2520and%2520J%25C3%25B6rg%2520Kr%25C3%25BCger%26entry.1292438233%3DEarly-stage%2520visual%2520quality%2520inspection%2520is%2520vital%2520for%2520achieving%2520Zero-Defect%2520Manufacturing%2520and%2520minimizing%2520production%2520waste%2520in%2520modern%2520industrial%2520environments.%2520However%252C%2520the%2520complexity%2520of%2520robust%2520visual%2520inspection%2520systems%2520and%2520their%2520extensive%2520data%2520requirements%2520hinder%2520widespread%2520adoption%2520in%2520semi-controlled%2520industrial%2520settings.%2520In%2520this%2520context%252C%2520we%2520propose%2520a%2520pose-agnostic%252C%2520zero-shot%2520quality%2520inspection%2520framework%2520that%2520compares%2520real%2520scenes%2520against%2520real-time%2520Digital%2520Twins%2520%2528DT%2529%2520in%2520the%2520RGB-D%2520space.%2520Our%2520approach%2520enables%2520efficient%2520real-time%2520DT%2520rendering%2520by%2520semantically%2520describing%2520industrial%2520scenes%2520through%2520object%2520detection%2520and%2520pose%2520estimation%2520of%2520known%2520Computer-Aided%2520Design%2520models.%2520We%2520benchmark%2520tools%2520for%2520real-time%252C%2520multimodal%2520RGB-D%2520DT%2520creation%2520while%2520tracking%2520consumption%2520of%2520computational%2520resources.%2520Additionally%252C%2520we%2520provide%2520an%2520extensible%2520and%2520hierarchical%2520annotation%2520strategy%2520for%2520multi-criteria%2520defect%2520detection%252C%2520unifying%2520pose%2520labelling%2520with%2520logical%2520and%2520structural%2520defect%2520annotations.%2520Based%2520on%2520an%2520automotive%2520use%2520case%2520featuring%2520the%2520quality%2520inspection%2520of%2520an%2520axial%2520flux%2520motor%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework.%2520Our%2520results%2520demonstrate%2520detection%2520performace%252C%2520achieving%2520intersection-over-union%2520%2528IoU%2529%2520scores%2520of%2520up%2520to%252063.3%2525%2520compared%2520to%2520ground-truth%2520masks%252C%2520even%2520if%2520using%2520simple%2520distance%2520measurements%2520under%2520semi-controlled%2520industrial%2520conditions.%2520Our%2520findings%2520lay%2520the%2520groundwork%2520for%2520future%2520research%2520on%2520generalizable%252C%2520low-data%2520defect%2520detection%2520methods%2520in%2520dynamic%2520manufacturing%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Multi-Criteria%20Visual%20Quality%20Inspection%20for%20Semi-Controlled%20Industrial%20Environments%20via%20Real-Time%203D%20Digital%20Twin%20Simulation&entry.906535625=Jose%20Moises%20Araya-Martinez%20and%20Gautham%20Mohan%20and%20Kenichi%20Hayakawa%20Bola%C3%B1os%20and%20Roberto%20Mendieta%20and%20Sarvenaz%20Sardari%20and%20Jens%20Lambrecht%20and%20J%C3%B6rg%20Kr%C3%BCger&entry.1292438233=Early-stage%20visual%20quality%20inspection%20is%20vital%20for%20achieving%20Zero-Defect%20Manufacturing%20and%20minimizing%20production%20waste%20in%20modern%20industrial%20environments.%20However%2C%20the%20complexity%20of%20robust%20visual%20inspection%20systems%20and%20their%20extensive%20data%20requirements%20hinder%20widespread%20adoption%20in%20semi-controlled%20industrial%20settings.%20In%20this%20context%2C%20we%20propose%20a%20pose-agnostic%2C%20zero-shot%20quality%20inspection%20framework%20that%20compares%20real%20scenes%20against%20real-time%20Digital%20Twins%20%28DT%29%20in%20the%20RGB-D%20space.%20Our%20approach%20enables%20efficient%20real-time%20DT%20rendering%20by%20semantically%20describing%20industrial%20scenes%20through%20object%20detection%20and%20pose%20estimation%20of%20known%20Computer-Aided%20Design%20models.%20We%20benchmark%20tools%20for%20real-time%2C%20multimodal%20RGB-D%20DT%20creation%20while%20tracking%20consumption%20of%20computational%20resources.%20Additionally%2C%20we%20provide%20an%20extensible%20and%20hierarchical%20annotation%20strategy%20for%20multi-criteria%20defect%20detection%2C%20unifying%20pose%20labelling%20with%20logical%20and%20structural%20defect%20annotations.%20Based%20on%20an%20automotive%20use%20case%20featuring%20the%20quality%20inspection%20of%20an%20axial%20flux%20motor%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework.%20Our%20results%20demonstrate%20detection%20performace%2C%20achieving%20intersection-over-union%20%28IoU%29%20scores%20of%20up%20to%2063.3%25%20compared%20to%20ground-truth%20masks%2C%20even%20if%20using%20simple%20distance%20measurements%20under%20semi-controlled%20industrial%20conditions.%20Our%20findings%20lay%20the%20groundwork%20for%20future%20research%20on%20generalizable%2C%20low-data%20defect%20detection%20methods%20in%20dynamic%20manufacturing%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.23214v1&entry.124074799=Read"},
{"title": "SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data", "author": "Yuting Fang and Qouc Le Gia and Flora Salim", "abstract": "Irregularly sampled time series with substantial missing observations are common in healthcare and sensor networks. We introduce SDE-Attention, a family of SDE-RNNs equipped with channel-level attention on the latent pre-RNN state, including channel recalibration, time-varying feature attention, and pyramidal multi-scale self-attention. We therefore conduct a comparison on a synthetic periodic dataset and real-world benchmarks, under varying missing rate. Latent-space attention consistently improves over a vanilla SDE-RNN. On the univariate UCR datasets, the LSTM-based time-varying feature model SDE-TVF-L achieves the highest average accuracy, raising mean performance by approximately 4, 6, and 10 percentage points over the baseline at 30%, 60% and 90% missingness, respectively (averaged across datasets). On multivariate UEA benchmarks, attention-augmented models again outperform the backbone, with SDE-TVF-L yielding up to a 7% gain in mean accuracy under high missingness. Among the proposed mechanisms, time-varying feature attention is the most robust on univariate datasets. On multivariate datasets, different attention types excel on different tasks, showing that SDE-Attention can be flexibly adapted to the structure of each problem.", "link": "http://arxiv.org/abs/2511.23238v1", "date": "2025-11-28", "relevancy": 2.4175, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4958}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4787}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDE-Attention%3A%20Latent%20Attention%20in%20SDE-RNNs%20for%20Irregularly%20Sampled%20Time%20Series%20with%20Missing%20Data&body=Title%3A%20SDE-Attention%3A%20Latent%20Attention%20in%20SDE-RNNs%20for%20Irregularly%20Sampled%20Time%20Series%20with%20Missing%20Data%0AAuthor%3A%20Yuting%20Fang%20and%20Qouc%20Le%20Gia%20and%20Flora%20Salim%0AAbstract%3A%20Irregularly%20sampled%20time%20series%20with%20substantial%20missing%20observations%20are%20common%20in%20healthcare%20and%20sensor%20networks.%20We%20introduce%20SDE-Attention%2C%20a%20family%20of%20SDE-RNNs%20equipped%20with%20channel-level%20attention%20on%20the%20latent%20pre-RNN%20state%2C%20including%20channel%20recalibration%2C%20time-varying%20feature%20attention%2C%20and%20pyramidal%20multi-scale%20self-attention.%20We%20therefore%20conduct%20a%20comparison%20on%20a%20synthetic%20periodic%20dataset%20and%20real-world%20benchmarks%2C%20under%20varying%20missing%20rate.%20Latent-space%20attention%20consistently%20improves%20over%20a%20vanilla%20SDE-RNN.%20On%20the%20univariate%20UCR%20datasets%2C%20the%20LSTM-based%20time-varying%20feature%20model%20SDE-TVF-L%20achieves%20the%20highest%20average%20accuracy%2C%20raising%20mean%20performance%20by%20approximately%204%2C%206%2C%20and%2010%20percentage%20points%20over%20the%20baseline%20at%2030%25%2C%2060%25%20and%2090%25%20missingness%2C%20respectively%20%28averaged%20across%20datasets%29.%20On%20multivariate%20UEA%20benchmarks%2C%20attention-augmented%20models%20again%20outperform%20the%20backbone%2C%20with%20SDE-TVF-L%20yielding%20up%20to%20a%207%25%20gain%20in%20mean%20accuracy%20under%20high%20missingness.%20Among%20the%20proposed%20mechanisms%2C%20time-varying%20feature%20attention%20is%20the%20most%20robust%20on%20univariate%20datasets.%20On%20multivariate%20datasets%2C%20different%20attention%20types%20excel%20on%20different%20tasks%2C%20showing%20that%20SDE-Attention%20can%20be%20flexibly%20adapted%20to%20the%20structure%20of%20each%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDE-Attention%253A%2520Latent%2520Attention%2520in%2520SDE-RNNs%2520for%2520Irregularly%2520Sampled%2520Time%2520Series%2520with%2520Missing%2520Data%26entry.906535625%3DYuting%2520Fang%2520and%2520Qouc%2520Le%2520Gia%2520and%2520Flora%2520Salim%26entry.1292438233%3DIrregularly%2520sampled%2520time%2520series%2520with%2520substantial%2520missing%2520observations%2520are%2520common%2520in%2520healthcare%2520and%2520sensor%2520networks.%2520We%2520introduce%2520SDE-Attention%252C%2520a%2520family%2520of%2520SDE-RNNs%2520equipped%2520with%2520channel-level%2520attention%2520on%2520the%2520latent%2520pre-RNN%2520state%252C%2520including%2520channel%2520recalibration%252C%2520time-varying%2520feature%2520attention%252C%2520and%2520pyramidal%2520multi-scale%2520self-attention.%2520We%2520therefore%2520conduct%2520a%2520comparison%2520on%2520a%2520synthetic%2520periodic%2520dataset%2520and%2520real-world%2520benchmarks%252C%2520under%2520varying%2520missing%2520rate.%2520Latent-space%2520attention%2520consistently%2520improves%2520over%2520a%2520vanilla%2520SDE-RNN.%2520On%2520the%2520univariate%2520UCR%2520datasets%252C%2520the%2520LSTM-based%2520time-varying%2520feature%2520model%2520SDE-TVF-L%2520achieves%2520the%2520highest%2520average%2520accuracy%252C%2520raising%2520mean%2520performance%2520by%2520approximately%25204%252C%25206%252C%2520and%252010%2520percentage%2520points%2520over%2520the%2520baseline%2520at%252030%2525%252C%252060%2525%2520and%252090%2525%2520missingness%252C%2520respectively%2520%2528averaged%2520across%2520datasets%2529.%2520On%2520multivariate%2520UEA%2520benchmarks%252C%2520attention-augmented%2520models%2520again%2520outperform%2520the%2520backbone%252C%2520with%2520SDE-TVF-L%2520yielding%2520up%2520to%2520a%25207%2525%2520gain%2520in%2520mean%2520accuracy%2520under%2520high%2520missingness.%2520Among%2520the%2520proposed%2520mechanisms%252C%2520time-varying%2520feature%2520attention%2520is%2520the%2520most%2520robust%2520on%2520univariate%2520datasets.%2520On%2520multivariate%2520datasets%252C%2520different%2520attention%2520types%2520excel%2520on%2520different%2520tasks%252C%2520showing%2520that%2520SDE-Attention%2520can%2520be%2520flexibly%2520adapted%2520to%2520the%2520structure%2520of%2520each%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDE-Attention%3A%20Latent%20Attention%20in%20SDE-RNNs%20for%20Irregularly%20Sampled%20Time%20Series%20with%20Missing%20Data&entry.906535625=Yuting%20Fang%20and%20Qouc%20Le%20Gia%20and%20Flora%20Salim&entry.1292438233=Irregularly%20sampled%20time%20series%20with%20substantial%20missing%20observations%20are%20common%20in%20healthcare%20and%20sensor%20networks.%20We%20introduce%20SDE-Attention%2C%20a%20family%20of%20SDE-RNNs%20equipped%20with%20channel-level%20attention%20on%20the%20latent%20pre-RNN%20state%2C%20including%20channel%20recalibration%2C%20time-varying%20feature%20attention%2C%20and%20pyramidal%20multi-scale%20self-attention.%20We%20therefore%20conduct%20a%20comparison%20on%20a%20synthetic%20periodic%20dataset%20and%20real-world%20benchmarks%2C%20under%20varying%20missing%20rate.%20Latent-space%20attention%20consistently%20improves%20over%20a%20vanilla%20SDE-RNN.%20On%20the%20univariate%20UCR%20datasets%2C%20the%20LSTM-based%20time-varying%20feature%20model%20SDE-TVF-L%20achieves%20the%20highest%20average%20accuracy%2C%20raising%20mean%20performance%20by%20approximately%204%2C%206%2C%20and%2010%20percentage%20points%20over%20the%20baseline%20at%2030%25%2C%2060%25%20and%2090%25%20missingness%2C%20respectively%20%28averaged%20across%20datasets%29.%20On%20multivariate%20UEA%20benchmarks%2C%20attention-augmented%20models%20again%20outperform%20the%20backbone%2C%20with%20SDE-TVF-L%20yielding%20up%20to%20a%207%25%20gain%20in%20mean%20accuracy%20under%20high%20missingness.%20Among%20the%20proposed%20mechanisms%2C%20time-varying%20feature%20attention%20is%20the%20most%20robust%20on%20univariate%20datasets.%20On%20multivariate%20datasets%2C%20different%20attention%20types%20excel%20on%20different%20tasks%2C%20showing%20that%20SDE-Attention%20can%20be%20flexibly%20adapted%20to%20the%20structure%20of%20each%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2511.23238v1&entry.124074799=Read"},
{"title": "HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models", "author": "Zhiguang Lu and Qianqian Xu and Peisong Wen and Siran Dai and Qingming Huang", "abstract": "Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.", "link": "http://arxiv.org/abs/2511.12547v3", "date": "2025-11-28", "relevancy": 2.3839, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6134}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5961}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiGFA%3A%20Hierarchical%20Guidance%20for%20Fine-grained%20Data%20Augmentation%20with%20Diffusion%20Models&body=Title%3A%20HiGFA%3A%20Hierarchical%20Guidance%20for%20Fine-grained%20Data%20Augmentation%20with%20Diffusion%20Models%0AAuthor%3A%20Zhiguang%20Lu%20and%20Qianqian%20Xu%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang%0AAbstract%3A%20Generative%20diffusion%20models%20show%20promise%20for%20data%20augmentation.%20However%2C%20applying%20them%20to%20fine-grained%20tasks%20presents%20a%20significant%20challenge%3A%20ensuring%20synthetic%20images%20accurately%20capture%20the%20subtle%2C%20category-defining%20features%20critical%20for%20high%20fidelity.%20Standard%20approaches%2C%20such%20as%20text-based%20Classifier-Free%20Guidance%20%28CFG%29%2C%20often%20lack%20the%20required%20specificity%2C%20potentially%20generating%20misleading%20examples%20that%20degrade%20fine-grained%20classifier%20performance.%20To%20address%20this%2C%20we%20propose%20Hierarchically%20Guided%20Fine-grained%20Augmentation%20%28HiGFA%29.%20HiGFA%20leverages%20the%20temporal%20dynamics%20of%20the%20diffusion%20sampling%20process.%20It%20employs%20strong%20text%20and%20transformed%20contour%20guidance%20with%20fixed%20strengths%20in%20the%20early-to-mid%20sampling%20stages%20to%20establish%20overall%20scene%2C%20style%2C%20and%20structure.%20In%20the%20final%20sampling%20stages%2C%20HiGFA%20activates%20a%20specialized%20fine-grained%20classifier%20guidance%20and%20dynamically%20modulates%20the%20strength%20of%20all%20guidance%20signals%20based%20on%20prediction%20confidence.%20This%20hierarchical%2C%20confidence-driven%20orchestration%20enables%20HiGFA%20to%20generate%20diverse%20yet%20faithful%20synthetic%20images%20by%20intelligently%20balancing%20global%20structure%20formation%20with%20precise%20detail%20refinement.%20Experiments%20on%20several%20FGVC%20datasets%20demonstrate%20the%20effectiveness%20of%20HiGFA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12547v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiGFA%253A%2520Hierarchical%2520Guidance%2520for%2520Fine-grained%2520Data%2520Augmentation%2520with%2520Diffusion%2520Models%26entry.906535625%3DZhiguang%2520Lu%2520and%2520Qianqian%2520Xu%2520and%2520Peisong%2520Wen%2520and%2520Siran%2520Dai%2520and%2520Qingming%2520Huang%26entry.1292438233%3DGenerative%2520diffusion%2520models%2520show%2520promise%2520for%2520data%2520augmentation.%2520However%252C%2520applying%2520them%2520to%2520fine-grained%2520tasks%2520presents%2520a%2520significant%2520challenge%253A%2520ensuring%2520synthetic%2520images%2520accurately%2520capture%2520the%2520subtle%252C%2520category-defining%2520features%2520critical%2520for%2520high%2520fidelity.%2520Standard%2520approaches%252C%2520such%2520as%2520text-based%2520Classifier-Free%2520Guidance%2520%2528CFG%2529%252C%2520often%2520lack%2520the%2520required%2520specificity%252C%2520potentially%2520generating%2520misleading%2520examples%2520that%2520degrade%2520fine-grained%2520classifier%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520Hierarchically%2520Guided%2520Fine-grained%2520Augmentation%2520%2528HiGFA%2529.%2520HiGFA%2520leverages%2520the%2520temporal%2520dynamics%2520of%2520the%2520diffusion%2520sampling%2520process.%2520It%2520employs%2520strong%2520text%2520and%2520transformed%2520contour%2520guidance%2520with%2520fixed%2520strengths%2520in%2520the%2520early-to-mid%2520sampling%2520stages%2520to%2520establish%2520overall%2520scene%252C%2520style%252C%2520and%2520structure.%2520In%2520the%2520final%2520sampling%2520stages%252C%2520HiGFA%2520activates%2520a%2520specialized%2520fine-grained%2520classifier%2520guidance%2520and%2520dynamically%2520modulates%2520the%2520strength%2520of%2520all%2520guidance%2520signals%2520based%2520on%2520prediction%2520confidence.%2520This%2520hierarchical%252C%2520confidence-driven%2520orchestration%2520enables%2520HiGFA%2520to%2520generate%2520diverse%2520yet%2520faithful%2520synthetic%2520images%2520by%2520intelligently%2520balancing%2520global%2520structure%2520formation%2520with%2520precise%2520detail%2520refinement.%2520Experiments%2520on%2520several%2520FGVC%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520HiGFA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12547v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiGFA%3A%20Hierarchical%20Guidance%20for%20Fine-grained%20Data%20Augmentation%20with%20Diffusion%20Models&entry.906535625=Zhiguang%20Lu%20and%20Qianqian%20Xu%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang&entry.1292438233=Generative%20diffusion%20models%20show%20promise%20for%20data%20augmentation.%20However%2C%20applying%20them%20to%20fine-grained%20tasks%20presents%20a%20significant%20challenge%3A%20ensuring%20synthetic%20images%20accurately%20capture%20the%20subtle%2C%20category-defining%20features%20critical%20for%20high%20fidelity.%20Standard%20approaches%2C%20such%20as%20text-based%20Classifier-Free%20Guidance%20%28CFG%29%2C%20often%20lack%20the%20required%20specificity%2C%20potentially%20generating%20misleading%20examples%20that%20degrade%20fine-grained%20classifier%20performance.%20To%20address%20this%2C%20we%20propose%20Hierarchically%20Guided%20Fine-grained%20Augmentation%20%28HiGFA%29.%20HiGFA%20leverages%20the%20temporal%20dynamics%20of%20the%20diffusion%20sampling%20process.%20It%20employs%20strong%20text%20and%20transformed%20contour%20guidance%20with%20fixed%20strengths%20in%20the%20early-to-mid%20sampling%20stages%20to%20establish%20overall%20scene%2C%20style%2C%20and%20structure.%20In%20the%20final%20sampling%20stages%2C%20HiGFA%20activates%20a%20specialized%20fine-grained%20classifier%20guidance%20and%20dynamically%20modulates%20the%20strength%20of%20all%20guidance%20signals%20based%20on%20prediction%20confidence.%20This%20hierarchical%2C%20confidence-driven%20orchestration%20enables%20HiGFA%20to%20generate%20diverse%20yet%20faithful%20synthetic%20images%20by%20intelligently%20balancing%20global%20structure%20formation%20with%20precise%20detail%20refinement.%20Experiments%20on%20several%20FGVC%20datasets%20demonstrate%20the%20effectiveness%20of%20HiGFA.&entry.1838667208=http%3A//arxiv.org/abs/2511.12547v3&entry.124074799=Read"},
{"title": "Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework", "author": "Jiatong Han", "abstract": "Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.", "link": "http://arxiv.org/abs/2511.23059v1", "date": "2025-11-28", "relevancy": 2.3837, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conveying%20Imagistic%20Thinking%20in%20TCM%20Translation%3A%20A%20Prompt%20Engineering%20and%20LLM-Based%20Evaluation%20Framework&body=Title%3A%20Conveying%20Imagistic%20Thinking%20in%20TCM%20Translation%3A%20A%20Prompt%20Engineering%20and%20LLM-Based%20Evaluation%20Framework%0AAuthor%3A%20Jiatong%20Han%0AAbstract%3A%20Traditional%20Chinese%20Medicine%20theory%20is%20built%20on%20imagistic%20thinking%2C%20in%20which%20medical%20principles%20and%20diagnostic%20and%20therapeutic%20logic%20are%20structured%20through%20metaphor%20and%20metonymy.%20However%2C%20existing%20English%20translations%20largely%20rely%20on%20literal%20rendering%2C%20making%20it%20difficult%20for%20target-language%20readers%20to%20reconstruct%20the%20underlying%20conceptual%20networks%20and%20apply%20them%20in%20clinical%20practice.%20This%20study%20adopted%20a%20human-in-the-loop%20framework%20and%20selected%20four%20passages%20from%20the%20medical%20canon%20Huangdi%20Neijing%20that%20are%20fundamental%20in%20theory.%20Through%20prompt-based%20cognitive%20scaffolding%2C%20DeepSeek%20V3.1%20was%20guided%20to%20identify%20metaphor%20and%20metonymy%20in%20the%20source%20text%20and%20convey%20the%20theory%20in%20translation.%20In%20the%20evaluation%20stage%2C%20ChatGPT%205%20Pro%20and%20Gemini%202.5%20Pro%20were%20instructed%20by%20prompts%20to%20simulate%20three%20types%20of%20real-world%20readers.%20Human%20translations%2C%20baseline%20model%20translations%2C%20and%20prompt-adjusted%20translations%20were%20scored%20by%20the%20simulated%20readers%20across%20five%20cognitive%20dimensions%2C%20followed%20by%20structured%20interviews%20and%20Interpretative%20Phenomenological%20Analysis.%20Results%20show%20that%20the%20prompt-adjusted%20LLM%20translations%20perform%20best%20across%20all%20five%20dimensions%2C%20with%20high%20cross-model%20and%20cross-role%20consistency.%20The%20interview%20themes%20reveal%20differences%20between%20human%20and%20machine%20translation%2C%20effective%20strategies%20for%20metaphor%20and%20metonymy%20transfer%2C%20and%20readers%27%20cognitive%20preferences.%20This%20study%20provides%20a%20cognitive%2C%20efficient%20and%20replicable%20HITL%20methodological%20pathway%20for%20translation%20of%20ancient%2C%20concept-dense%20texts%20like%20TCM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConveying%2520Imagistic%2520Thinking%2520in%2520TCM%2520Translation%253A%2520A%2520Prompt%2520Engineering%2520and%2520LLM-Based%2520Evaluation%2520Framework%26entry.906535625%3DJiatong%2520Han%26entry.1292438233%3DTraditional%2520Chinese%2520Medicine%2520theory%2520is%2520built%2520on%2520imagistic%2520thinking%252C%2520in%2520which%2520medical%2520principles%2520and%2520diagnostic%2520and%2520therapeutic%2520logic%2520are%2520structured%2520through%2520metaphor%2520and%2520metonymy.%2520However%252C%2520existing%2520English%2520translations%2520largely%2520rely%2520on%2520literal%2520rendering%252C%2520making%2520it%2520difficult%2520for%2520target-language%2520readers%2520to%2520reconstruct%2520the%2520underlying%2520conceptual%2520networks%2520and%2520apply%2520them%2520in%2520clinical%2520practice.%2520This%2520study%2520adopted%2520a%2520human-in-the-loop%2520framework%2520and%2520selected%2520four%2520passages%2520from%2520the%2520medical%2520canon%2520Huangdi%2520Neijing%2520that%2520are%2520fundamental%2520in%2520theory.%2520Through%2520prompt-based%2520cognitive%2520scaffolding%252C%2520DeepSeek%2520V3.1%2520was%2520guided%2520to%2520identify%2520metaphor%2520and%2520metonymy%2520in%2520the%2520source%2520text%2520and%2520convey%2520the%2520theory%2520in%2520translation.%2520In%2520the%2520evaluation%2520stage%252C%2520ChatGPT%25205%2520Pro%2520and%2520Gemini%25202.5%2520Pro%2520were%2520instructed%2520by%2520prompts%2520to%2520simulate%2520three%2520types%2520of%2520real-world%2520readers.%2520Human%2520translations%252C%2520baseline%2520model%2520translations%252C%2520and%2520prompt-adjusted%2520translations%2520were%2520scored%2520by%2520the%2520simulated%2520readers%2520across%2520five%2520cognitive%2520dimensions%252C%2520followed%2520by%2520structured%2520interviews%2520and%2520Interpretative%2520Phenomenological%2520Analysis.%2520Results%2520show%2520that%2520the%2520prompt-adjusted%2520LLM%2520translations%2520perform%2520best%2520across%2520all%2520five%2520dimensions%252C%2520with%2520high%2520cross-model%2520and%2520cross-role%2520consistency.%2520The%2520interview%2520themes%2520reveal%2520differences%2520between%2520human%2520and%2520machine%2520translation%252C%2520effective%2520strategies%2520for%2520metaphor%2520and%2520metonymy%2520transfer%252C%2520and%2520readers%2527%2520cognitive%2520preferences.%2520This%2520study%2520provides%2520a%2520cognitive%252C%2520efficient%2520and%2520replicable%2520HITL%2520methodological%2520pathway%2520for%2520translation%2520of%2520ancient%252C%2520concept-dense%2520texts%2520like%2520TCM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conveying%20Imagistic%20Thinking%20in%20TCM%20Translation%3A%20A%20Prompt%20Engineering%20and%20LLM-Based%20Evaluation%20Framework&entry.906535625=Jiatong%20Han&entry.1292438233=Traditional%20Chinese%20Medicine%20theory%20is%20built%20on%20imagistic%20thinking%2C%20in%20which%20medical%20principles%20and%20diagnostic%20and%20therapeutic%20logic%20are%20structured%20through%20metaphor%20and%20metonymy.%20However%2C%20existing%20English%20translations%20largely%20rely%20on%20literal%20rendering%2C%20making%20it%20difficult%20for%20target-language%20readers%20to%20reconstruct%20the%20underlying%20conceptual%20networks%20and%20apply%20them%20in%20clinical%20practice.%20This%20study%20adopted%20a%20human-in-the-loop%20framework%20and%20selected%20four%20passages%20from%20the%20medical%20canon%20Huangdi%20Neijing%20that%20are%20fundamental%20in%20theory.%20Through%20prompt-based%20cognitive%20scaffolding%2C%20DeepSeek%20V3.1%20was%20guided%20to%20identify%20metaphor%20and%20metonymy%20in%20the%20source%20text%20and%20convey%20the%20theory%20in%20translation.%20In%20the%20evaluation%20stage%2C%20ChatGPT%205%20Pro%20and%20Gemini%202.5%20Pro%20were%20instructed%20by%20prompts%20to%20simulate%20three%20types%20of%20real-world%20readers.%20Human%20translations%2C%20baseline%20model%20translations%2C%20and%20prompt-adjusted%20translations%20were%20scored%20by%20the%20simulated%20readers%20across%20five%20cognitive%20dimensions%2C%20followed%20by%20structured%20interviews%20and%20Interpretative%20Phenomenological%20Analysis.%20Results%20show%20that%20the%20prompt-adjusted%20LLM%20translations%20perform%20best%20across%20all%20five%20dimensions%2C%20with%20high%20cross-model%20and%20cross-role%20consistency.%20The%20interview%20themes%20reveal%20differences%20between%20human%20and%20machine%20translation%2C%20effective%20strategies%20for%20metaphor%20and%20metonymy%20transfer%2C%20and%20readers%27%20cognitive%20preferences.%20This%20study%20provides%20a%20cognitive%2C%20efficient%20and%20replicable%20HITL%20methodological%20pathway%20for%20translation%20of%20ancient%2C%20concept-dense%20texts%20like%20TCM.&entry.1838667208=http%3A//arxiv.org/abs/2511.23059v1&entry.124074799=Read"},
{"title": "Closing the Generalization Gap in Parameter-efficient Federated Edge Learning", "author": "Xinnong Du and Zhonghao Lyu and Xiaowen Cao and Chunyang Wen and Shuguang Cui and Jie Xu", "abstract": "Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.", "link": "http://arxiv.org/abs/2511.23282v1", "date": "2025-11-28", "relevancy": 2.3833, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5027}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4686}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Generalization%20Gap%20in%20Parameter-efficient%20Federated%20Edge%20Learning&body=Title%3A%20Closing%20the%20Generalization%20Gap%20in%20Parameter-efficient%20Federated%20Edge%20Learning%0AAuthor%3A%20Xinnong%20Du%20and%20Zhonghao%20Lyu%20and%20Xiaowen%20Cao%20and%20Chunyang%20Wen%20and%20Shuguang%20Cui%20and%20Jie%20Xu%0AAbstract%3A%20Federated%20edge%20learning%20%28FEEL%29%20provides%20a%20promising%20foundation%20for%20edge%20artificial%20intelligence%20%28AI%29%20by%20enabling%20collaborative%20model%20training%20while%20preserving%20data%20privacy.%20However%2C%20limited%20and%20heterogeneous%20local%20datasets%2C%20as%20well%20as%20resource-constrained%20deployment%2C%20severely%20degrade%20both%20model%20generalization%20and%20resource%20utilization%2C%20leading%20to%20a%20compromised%20learning%20performance.%20Therefore%2C%20we%20propose%20a%20parameter-efficient%20FEEL%20framework%20that%20jointly%20leverages%20model%20pruning%20and%20client%20selection%20to%20tackle%20such%20challenges.%20First%2C%20we%20derive%20an%20information-theoretic%20generalization%20statement%20that%20characterizes%20the%20discrepancy%20between%20training%20and%20testing%20function%20losses%20and%20embed%20it%20into%20the%20convergence%20analysis.%20It%20reveals%20that%20a%20larger%20local%20generalization%20statement%20can%20undermine%20the%20global%20convergence.%20Then%2C%20we%20formulate%20a%20generalization-aware%20average%20squared%20gradient%20norm%20bound%20minimization%20problem%2C%20by%20jointly%20optimizing%20the%20pruning%20ratios%2C%20client%20selection%2C%20and%20communication-computation%20resources%20under%20energy%20and%20delay%20constraints.%20Despite%20its%20non-convexity%2C%20the%20resulting%20mixed-integer%20problem%20is%20efficiently%20solved%20via%20an%20alternating%20optimization%20algorithm.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20design%20achieves%20superior%20learning%20performance%20than%20state-of-the-art%20baselines%2C%20validating%20the%20effectiveness%20of%20coupling%20generalization-aware%20analysis%20with%20system-level%20optimization%20for%20efficient%20FEEL.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Generalization%2520Gap%2520in%2520Parameter-efficient%2520Federated%2520Edge%2520Learning%26entry.906535625%3DXinnong%2520Du%2520and%2520Zhonghao%2520Lyu%2520and%2520Xiaowen%2520Cao%2520and%2520Chunyang%2520Wen%2520and%2520Shuguang%2520Cui%2520and%2520Jie%2520Xu%26entry.1292438233%3DFederated%2520edge%2520learning%2520%2528FEEL%2529%2520provides%2520a%2520promising%2520foundation%2520for%2520edge%2520artificial%2520intelligence%2520%2528AI%2529%2520by%2520enabling%2520collaborative%2520model%2520training%2520while%2520preserving%2520data%2520privacy.%2520However%252C%2520limited%2520and%2520heterogeneous%2520local%2520datasets%252C%2520as%2520well%2520as%2520resource-constrained%2520deployment%252C%2520severely%2520degrade%2520both%2520model%2520generalization%2520and%2520resource%2520utilization%252C%2520leading%2520to%2520a%2520compromised%2520learning%2520performance.%2520Therefore%252C%2520we%2520propose%2520a%2520parameter-efficient%2520FEEL%2520framework%2520that%2520jointly%2520leverages%2520model%2520pruning%2520and%2520client%2520selection%2520to%2520tackle%2520such%2520challenges.%2520First%252C%2520we%2520derive%2520an%2520information-theoretic%2520generalization%2520statement%2520that%2520characterizes%2520the%2520discrepancy%2520between%2520training%2520and%2520testing%2520function%2520losses%2520and%2520embed%2520it%2520into%2520the%2520convergence%2520analysis.%2520It%2520reveals%2520that%2520a%2520larger%2520local%2520generalization%2520statement%2520can%2520undermine%2520the%2520global%2520convergence.%2520Then%252C%2520we%2520formulate%2520a%2520generalization-aware%2520average%2520squared%2520gradient%2520norm%2520bound%2520minimization%2520problem%252C%2520by%2520jointly%2520optimizing%2520the%2520pruning%2520ratios%252C%2520client%2520selection%252C%2520and%2520communication-computation%2520resources%2520under%2520energy%2520and%2520delay%2520constraints.%2520Despite%2520its%2520non-convexity%252C%2520the%2520resulting%2520mixed-integer%2520problem%2520is%2520efficiently%2520solved%2520via%2520an%2520alternating%2520optimization%2520algorithm.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520design%2520achieves%2520superior%2520learning%2520performance%2520than%2520state-of-the-art%2520baselines%252C%2520validating%2520the%2520effectiveness%2520of%2520coupling%2520generalization-aware%2520analysis%2520with%2520system-level%2520optimization%2520for%2520efficient%2520FEEL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Generalization%20Gap%20in%20Parameter-efficient%20Federated%20Edge%20Learning&entry.906535625=Xinnong%20Du%20and%20Zhonghao%20Lyu%20and%20Xiaowen%20Cao%20and%20Chunyang%20Wen%20and%20Shuguang%20Cui%20and%20Jie%20Xu&entry.1292438233=Federated%20edge%20learning%20%28FEEL%29%20provides%20a%20promising%20foundation%20for%20edge%20artificial%20intelligence%20%28AI%29%20by%20enabling%20collaborative%20model%20training%20while%20preserving%20data%20privacy.%20However%2C%20limited%20and%20heterogeneous%20local%20datasets%2C%20as%20well%20as%20resource-constrained%20deployment%2C%20severely%20degrade%20both%20model%20generalization%20and%20resource%20utilization%2C%20leading%20to%20a%20compromised%20learning%20performance.%20Therefore%2C%20we%20propose%20a%20parameter-efficient%20FEEL%20framework%20that%20jointly%20leverages%20model%20pruning%20and%20client%20selection%20to%20tackle%20such%20challenges.%20First%2C%20we%20derive%20an%20information-theoretic%20generalization%20statement%20that%20characterizes%20the%20discrepancy%20between%20training%20and%20testing%20function%20losses%20and%20embed%20it%20into%20the%20convergence%20analysis.%20It%20reveals%20that%20a%20larger%20local%20generalization%20statement%20can%20undermine%20the%20global%20convergence.%20Then%2C%20we%20formulate%20a%20generalization-aware%20average%20squared%20gradient%20norm%20bound%20minimization%20problem%2C%20by%20jointly%20optimizing%20the%20pruning%20ratios%2C%20client%20selection%2C%20and%20communication-computation%20resources%20under%20energy%20and%20delay%20constraints.%20Despite%20its%20non-convexity%2C%20the%20resulting%20mixed-integer%20problem%20is%20efficiently%20solved%20via%20an%20alternating%20optimization%20algorithm.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20design%20achieves%20superior%20learning%20performance%20than%20state-of-the-art%20baselines%2C%20validating%20the%20effectiveness%20of%20coupling%20generalization-aware%20analysis%20with%20system-level%20optimization%20for%20efficient%20FEEL.&entry.1838667208=http%3A//arxiv.org/abs/2511.23282v1&entry.124074799=Read"},
{"title": "CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx", "author": "Lukas Picek and Elisa Belotti and Michal Bojda and Ludek Bufka and Vojtech Cermak and Martin Dula and Rostislav Dvorak and Luboslav Hrdy and Miroslav Jirik and Vaclav Kocourek and Josefa Krausova and Jir\u0131 Labuda and Jakub Straka and Ludek Toman and Vlado Trul\u0131k and Martin Vana and Miroslav Kutal", "abstract": "We introduce CzechLynx, the first large-scale, open-access dataset for individual identification, pose estimation, and instance segmentation of the Eurasian lynx (Lynx lynx). CzechLynx contains 39,760 camera trap images annotated with segmentation masks, identity labels, and 20-point skeletons and covers 319 unique individuals across 15 years of systematic monitoring in two geographically distinct regions: southwest Bohemia and the Western Carpathians. In addition to the real camera trap data, we provide a large complementary set of photorealistic synthetic images and a Unity-based generation pipeline with diffusion-based text-to-texture modeling, capable of producing arbitrarily large amounts of synthetic data spanning diverse environments, poses, and coat-pattern variations. To enable systematic testing across realistic ecological scenarios, we define three complementary evaluation protocols: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware closed-set, covering cross-regional and long-term monitoring settings. With the provided resources, CzechLynx offers a unique, flexible benchmark for robust evaluation of computer vision and machine learning models across realistic ecological scenarios.", "link": "http://arxiv.org/abs/2506.04931v2", "date": "2025-11-28", "relevancy": 2.3759, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4762}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CzechLynx%3A%20A%20Dataset%20for%20Individual%20Identification%20and%20Pose%20Estimation%20of%20the%20Eurasian%20Lynx&body=Title%3A%20CzechLynx%3A%20A%20Dataset%20for%20Individual%20Identification%20and%20Pose%20Estimation%20of%20the%20Eurasian%20Lynx%0AAuthor%3A%20Lukas%20Picek%20and%20Elisa%20Belotti%20and%20Michal%20Bojda%20and%20Ludek%20Bufka%20and%20Vojtech%20Cermak%20and%20Martin%20Dula%20and%20Rostislav%20Dvorak%20and%20Luboslav%20Hrdy%20and%20Miroslav%20Jirik%20and%20Vaclav%20Kocourek%20and%20Josefa%20Krausova%20and%20Jir%C4%B1%20Labuda%20and%20Jakub%20Straka%20and%20Ludek%20Toman%20and%20Vlado%20Trul%C4%B1k%20and%20Martin%20Vana%20and%20Miroslav%20Kutal%0AAbstract%3A%20We%20introduce%20CzechLynx%2C%20the%20first%20large-scale%2C%20open-access%20dataset%20for%20individual%20identification%2C%20pose%20estimation%2C%20and%20instance%20segmentation%20of%20the%20Eurasian%20lynx%20%28Lynx%20lynx%29.%20CzechLynx%20contains%2039%2C760%20camera%20trap%20images%20annotated%20with%20segmentation%20masks%2C%20identity%20labels%2C%20and%2020-point%20skeletons%20and%20covers%20319%20unique%20individuals%20across%2015%20years%20of%20systematic%20monitoring%20in%20two%20geographically%20distinct%20regions%3A%20southwest%20Bohemia%20and%20the%20Western%20Carpathians.%20In%20addition%20to%20the%20real%20camera%20trap%20data%2C%20we%20provide%20a%20large%20complementary%20set%20of%20photorealistic%20synthetic%20images%20and%20a%20Unity-based%20generation%20pipeline%20with%20diffusion-based%20text-to-texture%20modeling%2C%20capable%20of%20producing%20arbitrarily%20large%20amounts%20of%20synthetic%20data%20spanning%20diverse%20environments%2C%20poses%2C%20and%20coat-pattern%20variations.%20To%20enable%20systematic%20testing%20across%20realistic%20ecological%20scenarios%2C%20we%20define%20three%20complementary%20evaluation%20protocols%3A%20%28i%29%20geo-aware%2C%20%28ii%29%20time-aware%20open-set%2C%20and%20%28iii%29%20time-aware%20closed-set%2C%20covering%20cross-regional%20and%20long-term%20monitoring%20settings.%20With%20the%20provided%20resources%2C%20CzechLynx%20offers%20a%20unique%2C%20flexible%20benchmark%20for%20robust%20evaluation%20of%20computer%20vision%20and%20machine%20learning%20models%20across%20realistic%20ecological%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2506.04931v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCzechLynx%253A%2520A%2520Dataset%2520for%2520Individual%2520Identification%2520and%2520Pose%2520Estimation%2520of%2520the%2520Eurasian%2520Lynx%26entry.906535625%3DLukas%2520Picek%2520and%2520Elisa%2520Belotti%2520and%2520Michal%2520Bojda%2520and%2520Ludek%2520Bufka%2520and%2520Vojtech%2520Cermak%2520and%2520Martin%2520Dula%2520and%2520Rostislav%2520Dvorak%2520and%2520Luboslav%2520Hrdy%2520and%2520Miroslav%2520Jirik%2520and%2520Vaclav%2520Kocourek%2520and%2520Josefa%2520Krausova%2520and%2520Jir%25C4%25B1%2520Labuda%2520and%2520Jakub%2520Straka%2520and%2520Ludek%2520Toman%2520and%2520Vlado%2520Trul%25C4%25B1k%2520and%2520Martin%2520Vana%2520and%2520Miroslav%2520Kutal%26entry.1292438233%3DWe%2520introduce%2520CzechLynx%252C%2520the%2520first%2520large-scale%252C%2520open-access%2520dataset%2520for%2520individual%2520identification%252C%2520pose%2520estimation%252C%2520and%2520instance%2520segmentation%2520of%2520the%2520Eurasian%2520lynx%2520%2528Lynx%2520lynx%2529.%2520CzechLynx%2520contains%252039%252C760%2520camera%2520trap%2520images%2520annotated%2520with%2520segmentation%2520masks%252C%2520identity%2520labels%252C%2520and%252020-point%2520skeletons%2520and%2520covers%2520319%2520unique%2520individuals%2520across%252015%2520years%2520of%2520systematic%2520monitoring%2520in%2520two%2520geographically%2520distinct%2520regions%253A%2520southwest%2520Bohemia%2520and%2520the%2520Western%2520Carpathians.%2520In%2520addition%2520to%2520the%2520real%2520camera%2520trap%2520data%252C%2520we%2520provide%2520a%2520large%2520complementary%2520set%2520of%2520photorealistic%2520synthetic%2520images%2520and%2520a%2520Unity-based%2520generation%2520pipeline%2520with%2520diffusion-based%2520text-to-texture%2520modeling%252C%2520capable%2520of%2520producing%2520arbitrarily%2520large%2520amounts%2520of%2520synthetic%2520data%2520spanning%2520diverse%2520environments%252C%2520poses%252C%2520and%2520coat-pattern%2520variations.%2520To%2520enable%2520systematic%2520testing%2520across%2520realistic%2520ecological%2520scenarios%252C%2520we%2520define%2520three%2520complementary%2520evaluation%2520protocols%253A%2520%2528i%2529%2520geo-aware%252C%2520%2528ii%2529%2520time-aware%2520open-set%252C%2520and%2520%2528iii%2529%2520time-aware%2520closed-set%252C%2520covering%2520cross-regional%2520and%2520long-term%2520monitoring%2520settings.%2520With%2520the%2520provided%2520resources%252C%2520CzechLynx%2520offers%2520a%2520unique%252C%2520flexible%2520benchmark%2520for%2520robust%2520evaluation%2520of%2520computer%2520vision%2520and%2520machine%2520learning%2520models%2520across%2520realistic%2520ecological%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04931v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CzechLynx%3A%20A%20Dataset%20for%20Individual%20Identification%20and%20Pose%20Estimation%20of%20the%20Eurasian%20Lynx&entry.906535625=Lukas%20Picek%20and%20Elisa%20Belotti%20and%20Michal%20Bojda%20and%20Ludek%20Bufka%20and%20Vojtech%20Cermak%20and%20Martin%20Dula%20and%20Rostislav%20Dvorak%20and%20Luboslav%20Hrdy%20and%20Miroslav%20Jirik%20and%20Vaclav%20Kocourek%20and%20Josefa%20Krausova%20and%20Jir%C4%B1%20Labuda%20and%20Jakub%20Straka%20and%20Ludek%20Toman%20and%20Vlado%20Trul%C4%B1k%20and%20Martin%20Vana%20and%20Miroslav%20Kutal&entry.1292438233=We%20introduce%20CzechLynx%2C%20the%20first%20large-scale%2C%20open-access%20dataset%20for%20individual%20identification%2C%20pose%20estimation%2C%20and%20instance%20segmentation%20of%20the%20Eurasian%20lynx%20%28Lynx%20lynx%29.%20CzechLynx%20contains%2039%2C760%20camera%20trap%20images%20annotated%20with%20segmentation%20masks%2C%20identity%20labels%2C%20and%2020-point%20skeletons%20and%20covers%20319%20unique%20individuals%20across%2015%20years%20of%20systematic%20monitoring%20in%20two%20geographically%20distinct%20regions%3A%20southwest%20Bohemia%20and%20the%20Western%20Carpathians.%20In%20addition%20to%20the%20real%20camera%20trap%20data%2C%20we%20provide%20a%20large%20complementary%20set%20of%20photorealistic%20synthetic%20images%20and%20a%20Unity-based%20generation%20pipeline%20with%20diffusion-based%20text-to-texture%20modeling%2C%20capable%20of%20producing%20arbitrarily%20large%20amounts%20of%20synthetic%20data%20spanning%20diverse%20environments%2C%20poses%2C%20and%20coat-pattern%20variations.%20To%20enable%20systematic%20testing%20across%20realistic%20ecological%20scenarios%2C%20we%20define%20three%20complementary%20evaluation%20protocols%3A%20%28i%29%20geo-aware%2C%20%28ii%29%20time-aware%20open-set%2C%20and%20%28iii%29%20time-aware%20closed-set%2C%20covering%20cross-regional%20and%20long-term%20monitoring%20settings.%20With%20the%20provided%20resources%2C%20CzechLynx%20offers%20a%20unique%2C%20flexible%20benchmark%20for%20robust%20evaluation%20of%20computer%20vision%20and%20machine%20learning%20models%20across%20realistic%20ecological%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2506.04931v2&entry.124074799=Read"},
{"title": "ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction", "author": "Marco Aruta and Ciro Listone and Giuseppe Murano and Aniello Murano", "abstract": "Leukemia diagnosis and monitoring rely increasingly on high-throughput image data, yet conventional clustering methods lack the flexibility to accommodate evolving cellular patterns and quantify uncertainty in real time. We introduce Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable framework that combines Convolutional Neural Network-based feature extraction with an online fuzzy clustering engine. ADNF initializes soft partitions via Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy evolution. A topology refinement stage performs density-weighted merging and entropy-guided splitting to guard against over- and under-segmentation. On the C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of 0.51, demonstrating superior cohesion and separation over static baselines. The method's adaptive uncertainty modeling and label-free operation hold immediate potential for integration within the INFANT pediatric oncology network, enabling scalable, up-to-date support for personalized leukemia management.", "link": "http://arxiv.org/abs/2506.18396v2", "date": "2025-11-28", "relevancy": 2.3625, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5115}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4616}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADNF-Clustering%3A%20An%20Adaptive%20and%20Dynamic%20Neuro-Fuzzy%20Clustering%20for%20Leukemia%20Prediction&body=Title%3A%20ADNF-Clustering%3A%20An%20Adaptive%20and%20Dynamic%20Neuro-Fuzzy%20Clustering%20for%20Leukemia%20Prediction%0AAuthor%3A%20Marco%20Aruta%20and%20Ciro%20Listone%20and%20Giuseppe%20Murano%20and%20Aniello%20Murano%0AAbstract%3A%20Leukemia%20diagnosis%20and%20monitoring%20rely%20increasingly%20on%20high-throughput%20image%20data%2C%20yet%20conventional%20clustering%20methods%20lack%20the%20flexibility%20to%20accommodate%20evolving%20cellular%20patterns%20and%20quantify%20uncertainty%20in%20real%20time.%20We%20introduce%20Adaptive%20and%20Dynamic%20Neuro-Fuzzy%20Clustering%2C%20a%20novel%20streaming-capable%20framework%20that%20combines%20Convolutional%20Neural%20Network-based%20feature%20extraction%20with%20an%20online%20fuzzy%20clustering%20engine.%20ADNF%20initializes%20soft%20partitions%20via%20Fuzzy%20C-Means%2C%20then%20continuously%20updates%20micro-cluster%20centers%2C%20densities%2C%20and%20fuzziness%20parameters%20using%20a%20Fuzzy%20Temporal%20Index%20%28FTI%29%20that%20measures%20entropy%20evolution.%20A%20topology%20refinement%20stage%20performs%20density-weighted%20merging%20and%20entropy-guided%20splitting%20to%20guard%20against%20over-%20and%20under-segmentation.%20On%20the%20C-NMC%20leukemia%20microscopy%20dataset%2C%20our%20tool%20achieves%20a%20silhouette%20score%20of%200.51%2C%20demonstrating%20superior%20cohesion%20and%20separation%20over%20static%20baselines.%20The%20method%27s%20adaptive%20uncertainty%20modeling%20and%20label-free%20operation%20hold%20immediate%20potential%20for%20integration%20within%20the%20INFANT%20pediatric%20oncology%20network%2C%20enabling%20scalable%2C%20up-to-date%20support%20for%20personalized%20leukemia%20management.%0ALink%3A%20http%3A//arxiv.org/abs/2506.18396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADNF-Clustering%253A%2520An%2520Adaptive%2520and%2520Dynamic%2520Neuro-Fuzzy%2520Clustering%2520for%2520Leukemia%2520Prediction%26entry.906535625%3DMarco%2520Aruta%2520and%2520Ciro%2520Listone%2520and%2520Giuseppe%2520Murano%2520and%2520Aniello%2520Murano%26entry.1292438233%3DLeukemia%2520diagnosis%2520and%2520monitoring%2520rely%2520increasingly%2520on%2520high-throughput%2520image%2520data%252C%2520yet%2520conventional%2520clustering%2520methods%2520lack%2520the%2520flexibility%2520to%2520accommodate%2520evolving%2520cellular%2520patterns%2520and%2520quantify%2520uncertainty%2520in%2520real%2520time.%2520We%2520introduce%2520Adaptive%2520and%2520Dynamic%2520Neuro-Fuzzy%2520Clustering%252C%2520a%2520novel%2520streaming-capable%2520framework%2520that%2520combines%2520Convolutional%2520Neural%2520Network-based%2520feature%2520extraction%2520with%2520an%2520online%2520fuzzy%2520clustering%2520engine.%2520ADNF%2520initializes%2520soft%2520partitions%2520via%2520Fuzzy%2520C-Means%252C%2520then%2520continuously%2520updates%2520micro-cluster%2520centers%252C%2520densities%252C%2520and%2520fuzziness%2520parameters%2520using%2520a%2520Fuzzy%2520Temporal%2520Index%2520%2528FTI%2529%2520that%2520measures%2520entropy%2520evolution.%2520A%2520topology%2520refinement%2520stage%2520performs%2520density-weighted%2520merging%2520and%2520entropy-guided%2520splitting%2520to%2520guard%2520against%2520over-%2520and%2520under-segmentation.%2520On%2520the%2520C-NMC%2520leukemia%2520microscopy%2520dataset%252C%2520our%2520tool%2520achieves%2520a%2520silhouette%2520score%2520of%25200.51%252C%2520demonstrating%2520superior%2520cohesion%2520and%2520separation%2520over%2520static%2520baselines.%2520The%2520method%2527s%2520adaptive%2520uncertainty%2520modeling%2520and%2520label-free%2520operation%2520hold%2520immediate%2520potential%2520for%2520integration%2520within%2520the%2520INFANT%2520pediatric%2520oncology%2520network%252C%2520enabling%2520scalable%252C%2520up-to-date%2520support%2520for%2520personalized%2520leukemia%2520management.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADNF-Clustering%3A%20An%20Adaptive%20and%20Dynamic%20Neuro-Fuzzy%20Clustering%20for%20Leukemia%20Prediction&entry.906535625=Marco%20Aruta%20and%20Ciro%20Listone%20and%20Giuseppe%20Murano%20and%20Aniello%20Murano&entry.1292438233=Leukemia%20diagnosis%20and%20monitoring%20rely%20increasingly%20on%20high-throughput%20image%20data%2C%20yet%20conventional%20clustering%20methods%20lack%20the%20flexibility%20to%20accommodate%20evolving%20cellular%20patterns%20and%20quantify%20uncertainty%20in%20real%20time.%20We%20introduce%20Adaptive%20and%20Dynamic%20Neuro-Fuzzy%20Clustering%2C%20a%20novel%20streaming-capable%20framework%20that%20combines%20Convolutional%20Neural%20Network-based%20feature%20extraction%20with%20an%20online%20fuzzy%20clustering%20engine.%20ADNF%20initializes%20soft%20partitions%20via%20Fuzzy%20C-Means%2C%20then%20continuously%20updates%20micro-cluster%20centers%2C%20densities%2C%20and%20fuzziness%20parameters%20using%20a%20Fuzzy%20Temporal%20Index%20%28FTI%29%20that%20measures%20entropy%20evolution.%20A%20topology%20refinement%20stage%20performs%20density-weighted%20merging%20and%20entropy-guided%20splitting%20to%20guard%20against%20over-%20and%20under-segmentation.%20On%20the%20C-NMC%20leukemia%20microscopy%20dataset%2C%20our%20tool%20achieves%20a%20silhouette%20score%20of%200.51%2C%20demonstrating%20superior%20cohesion%20and%20separation%20over%20static%20baselines.%20The%20method%27s%20adaptive%20uncertainty%20modeling%20and%20label-free%20operation%20hold%20immediate%20potential%20for%20integration%20within%20the%20INFANT%20pediatric%20oncology%20network%2C%20enabling%20scalable%2C%20up-to-date%20support%20for%20personalized%20leukemia%20management.&entry.1838667208=http%3A//arxiv.org/abs/2506.18396v2&entry.124074799=Read"},
{"title": "DINO-Foresight: Looking into the Future with DINO", "author": "Efstathios Karypidis and Ioannis Kakogeorgiou and Spyros Gidaris and Nikos Komodakis", "abstract": "Predicting future dynamics is crucial for applications like autonomous driving and robotics, where understanding the environment is key. Existing pixel-level methods are computationally expensive and often focus on irrelevant details. To address these challenges, we introduce DINO-Foresight, a novel framework that operates in the semantic feature space of pretrained Vision Foundation Models (VFMs). Our approach trains a masked feature transformer in a self-supervised manner to predict the evolution of VFM features over time. By forecasting these features, we can apply off-the-shelf, task-specific heads for various scene understanding tasks. In this framework, VFM features are treated as a latent space, to which different heads attach to perform specific tasks for future-frame analysis. Extensive experiments show the very strong performance, robustness and scalability of our framework. Project page and code at https://dino-foresight.github.io/ .", "link": "http://arxiv.org/abs/2412.11673v2", "date": "2025-11-28", "relevancy": 2.3483, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5926}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-Foresight%3A%20Looking%20into%20the%20Future%20with%20DINO&body=Title%3A%20DINO-Foresight%3A%20Looking%20into%20the%20Future%20with%20DINO%0AAuthor%3A%20Efstathios%20Karypidis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis%0AAbstract%3A%20Predicting%20future%20dynamics%20is%20crucial%20for%20applications%20like%20autonomous%20driving%20and%20robotics%2C%20where%20understanding%20the%20environment%20is%20key.%20Existing%20pixel-level%20methods%20are%20computationally%20expensive%20and%20often%20focus%20on%20irrelevant%20details.%20To%20address%20these%20challenges%2C%20we%20introduce%20DINO-Foresight%2C%20a%20novel%20framework%20that%20operates%20in%20the%20semantic%20feature%20space%20of%20pretrained%20Vision%20Foundation%20Models%20%28VFMs%29.%20Our%20approach%20trains%20a%20masked%20feature%20transformer%20in%20a%20self-supervised%20manner%20to%20predict%20the%20evolution%20of%20VFM%20features%20over%20time.%20By%20forecasting%20these%20features%2C%20we%20can%20apply%20off-the-shelf%2C%20task-specific%20heads%20for%20various%20scene%20understanding%20tasks.%20In%20this%20framework%2C%20VFM%20features%20are%20treated%20as%20a%20latent%20space%2C%20to%20which%20different%20heads%20attach%20to%20perform%20specific%20tasks%20for%20future-frame%20analysis.%20Extensive%20experiments%20show%20the%20very%20strong%20performance%2C%20robustness%20and%20scalability%20of%20our%20framework.%20Project%20page%20and%20code%20at%20https%3A//dino-foresight.github.io/%20.%0ALink%3A%20http%3A//arxiv.org/abs/2412.11673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-Foresight%253A%2520Looking%2520into%2520the%2520Future%2520with%2520DINO%26entry.906535625%3DEfstathios%2520Karypidis%2520and%2520Ioannis%2520Kakogeorgiou%2520and%2520Spyros%2520Gidaris%2520and%2520Nikos%2520Komodakis%26entry.1292438233%3DPredicting%2520future%2520dynamics%2520is%2520crucial%2520for%2520applications%2520like%2520autonomous%2520driving%2520and%2520robotics%252C%2520where%2520understanding%2520the%2520environment%2520is%2520key.%2520Existing%2520pixel-level%2520methods%2520are%2520computationally%2520expensive%2520and%2520often%2520focus%2520on%2520irrelevant%2520details.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520DINO-Foresight%252C%2520a%2520novel%2520framework%2520that%2520operates%2520in%2520the%2520semantic%2520feature%2520space%2520of%2520pretrained%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529.%2520Our%2520approach%2520trains%2520a%2520masked%2520feature%2520transformer%2520in%2520a%2520self-supervised%2520manner%2520to%2520predict%2520the%2520evolution%2520of%2520VFM%2520features%2520over%2520time.%2520By%2520forecasting%2520these%2520features%252C%2520we%2520can%2520apply%2520off-the-shelf%252C%2520task-specific%2520heads%2520for%2520various%2520scene%2520understanding%2520tasks.%2520In%2520this%2520framework%252C%2520VFM%2520features%2520are%2520treated%2520as%2520a%2520latent%2520space%252C%2520to%2520which%2520different%2520heads%2520attach%2520to%2520perform%2520specific%2520tasks%2520for%2520future-frame%2520analysis.%2520Extensive%2520experiments%2520show%2520the%2520very%2520strong%2520performance%252C%2520robustness%2520and%2520scalability%2520of%2520our%2520framework.%2520Project%2520page%2520and%2520code%2520at%2520https%253A//dino-foresight.github.io/%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-Foresight%3A%20Looking%20into%20the%20Future%20with%20DINO&entry.906535625=Efstathios%20Karypidis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis&entry.1292438233=Predicting%20future%20dynamics%20is%20crucial%20for%20applications%20like%20autonomous%20driving%20and%20robotics%2C%20where%20understanding%20the%20environment%20is%20key.%20Existing%20pixel-level%20methods%20are%20computationally%20expensive%20and%20often%20focus%20on%20irrelevant%20details.%20To%20address%20these%20challenges%2C%20we%20introduce%20DINO-Foresight%2C%20a%20novel%20framework%20that%20operates%20in%20the%20semantic%20feature%20space%20of%20pretrained%20Vision%20Foundation%20Models%20%28VFMs%29.%20Our%20approach%20trains%20a%20masked%20feature%20transformer%20in%20a%20self-supervised%20manner%20to%20predict%20the%20evolution%20of%20VFM%20features%20over%20time.%20By%20forecasting%20these%20features%2C%20we%20can%20apply%20off-the-shelf%2C%20task-specific%20heads%20for%20various%20scene%20understanding%20tasks.%20In%20this%20framework%2C%20VFM%20features%20are%20treated%20as%20a%20latent%20space%2C%20to%20which%20different%20heads%20attach%20to%20perform%20specific%20tasks%20for%20future-frame%20analysis.%20Extensive%20experiments%20show%20the%20very%20strong%20performance%2C%20robustness%20and%20scalability%20of%20our%20framework.%20Project%20page%20and%20code%20at%20https%3A//dino-foresight.github.io/%20.&entry.1838667208=http%3A//arxiv.org/abs/2412.11673v2&entry.124074799=Read"},
{"title": "Robust HRRP Recognition under Interrupted Sampling Repeater Jamming using a Prior Jamming Information-Guided Network", "author": "Guozheng Sun and Lei Wang and Yanhao Wang and Jie Wang and Yimin Liu", "abstract": "Radar automatic target recognition (RATR) based on high-resolution range profile (HRRP) has attracted increasing attention due to its ability to capture fine-grained structural features. However, recognizing targets under electronic countermeasures (ECM), especially the mainstream interrupted-sampling repeater jamming (ISRJ), remains a significant challenge, as HRRPs often suffer from serious feature distortion. To address this, we propose a robust HRRP recognition method guided by prior jamming information. Specifically, we introduce a point spread function (PSF) as prior information to model the HRRP distortion induced by ISRJ. Based on this, we design a recognition network that leverages this prior through a prior-guided feature interaction module and a hybrid loss function to enhance the model's discriminative capability. With the aid of prior information, the model can learn invariant features within distorted HRRP under different jamming parameters. Both the simulated and measured-data experiments demonstrate that our method consistently outperforms state-of-the-art approaches and exhibits stronger generalization capabilities when facing unseen jamming parameters.", "link": "http://arxiv.org/abs/2511.23256v1", "date": "2025-11-28", "relevancy": 2.3197, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4818}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4573}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20HRRP%20Recognition%20under%20Interrupted%20Sampling%20Repeater%20Jamming%20using%20a%20Prior%20Jamming%20Information-Guided%20Network&body=Title%3A%20Robust%20HRRP%20Recognition%20under%20Interrupted%20Sampling%20Repeater%20Jamming%20using%20a%20Prior%20Jamming%20Information-Guided%20Network%0AAuthor%3A%20Guozheng%20Sun%20and%20Lei%20Wang%20and%20Yanhao%20Wang%20and%20Jie%20Wang%20and%20Yimin%20Liu%0AAbstract%3A%20Radar%20automatic%20target%20recognition%20%28RATR%29%20based%20on%20high-resolution%20range%20profile%20%28HRRP%29%20has%20attracted%20increasing%20attention%20due%20to%20its%20ability%20to%20capture%20fine-grained%20structural%20features.%20However%2C%20recognizing%20targets%20under%20electronic%20countermeasures%20%28ECM%29%2C%20especially%20the%20mainstream%20interrupted-sampling%20repeater%20jamming%20%28ISRJ%29%2C%20remains%20a%20significant%20challenge%2C%20as%20HRRPs%20often%20suffer%20from%20serious%20feature%20distortion.%20To%20address%20this%2C%20we%20propose%20a%20robust%20HRRP%20recognition%20method%20guided%20by%20prior%20jamming%20information.%20Specifically%2C%20we%20introduce%20a%20point%20spread%20function%20%28PSF%29%20as%20prior%20information%20to%20model%20the%20HRRP%20distortion%20induced%20by%20ISRJ.%20Based%20on%20this%2C%20we%20design%20a%20recognition%20network%20that%20leverages%20this%20prior%20through%20a%20prior-guided%20feature%20interaction%20module%20and%20a%20hybrid%20loss%20function%20to%20enhance%20the%20model%27s%20discriminative%20capability.%20With%20the%20aid%20of%20prior%20information%2C%20the%20model%20can%20learn%20invariant%20features%20within%20distorted%20HRRP%20under%20different%20jamming%20parameters.%20Both%20the%20simulated%20and%20measured-data%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20approaches%20and%20exhibits%20stronger%20generalization%20capabilities%20when%20facing%20unseen%20jamming%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520HRRP%2520Recognition%2520under%2520Interrupted%2520Sampling%2520Repeater%2520Jamming%2520using%2520a%2520Prior%2520Jamming%2520Information-Guided%2520Network%26entry.906535625%3DGuozheng%2520Sun%2520and%2520Lei%2520Wang%2520and%2520Yanhao%2520Wang%2520and%2520Jie%2520Wang%2520and%2520Yimin%2520Liu%26entry.1292438233%3DRadar%2520automatic%2520target%2520recognition%2520%2528RATR%2529%2520based%2520on%2520high-resolution%2520range%2520profile%2520%2528HRRP%2529%2520has%2520attracted%2520increasing%2520attention%2520due%2520to%2520its%2520ability%2520to%2520capture%2520fine-grained%2520structural%2520features.%2520However%252C%2520recognizing%2520targets%2520under%2520electronic%2520countermeasures%2520%2528ECM%2529%252C%2520especially%2520the%2520mainstream%2520interrupted-sampling%2520repeater%2520jamming%2520%2528ISRJ%2529%252C%2520remains%2520a%2520significant%2520challenge%252C%2520as%2520HRRPs%2520often%2520suffer%2520from%2520serious%2520feature%2520distortion.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520robust%2520HRRP%2520recognition%2520method%2520guided%2520by%2520prior%2520jamming%2520information.%2520Specifically%252C%2520we%2520introduce%2520a%2520point%2520spread%2520function%2520%2528PSF%2529%2520as%2520prior%2520information%2520to%2520model%2520the%2520HRRP%2520distortion%2520induced%2520by%2520ISRJ.%2520Based%2520on%2520this%252C%2520we%2520design%2520a%2520recognition%2520network%2520that%2520leverages%2520this%2520prior%2520through%2520a%2520prior-guided%2520feature%2520interaction%2520module%2520and%2520a%2520hybrid%2520loss%2520function%2520to%2520enhance%2520the%2520model%2527s%2520discriminative%2520capability.%2520With%2520the%2520aid%2520of%2520prior%2520information%252C%2520the%2520model%2520can%2520learn%2520invariant%2520features%2520within%2520distorted%2520HRRP%2520under%2520different%2520jamming%2520parameters.%2520Both%2520the%2520simulated%2520and%2520measured-data%2520experiments%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520state-of-the-art%2520approaches%2520and%2520exhibits%2520stronger%2520generalization%2520capabilities%2520when%2520facing%2520unseen%2520jamming%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20HRRP%20Recognition%20under%20Interrupted%20Sampling%20Repeater%20Jamming%20using%20a%20Prior%20Jamming%20Information-Guided%20Network&entry.906535625=Guozheng%20Sun%20and%20Lei%20Wang%20and%20Yanhao%20Wang%20and%20Jie%20Wang%20and%20Yimin%20Liu&entry.1292438233=Radar%20automatic%20target%20recognition%20%28RATR%29%20based%20on%20high-resolution%20range%20profile%20%28HRRP%29%20has%20attracted%20increasing%20attention%20due%20to%20its%20ability%20to%20capture%20fine-grained%20structural%20features.%20However%2C%20recognizing%20targets%20under%20electronic%20countermeasures%20%28ECM%29%2C%20especially%20the%20mainstream%20interrupted-sampling%20repeater%20jamming%20%28ISRJ%29%2C%20remains%20a%20significant%20challenge%2C%20as%20HRRPs%20often%20suffer%20from%20serious%20feature%20distortion.%20To%20address%20this%2C%20we%20propose%20a%20robust%20HRRP%20recognition%20method%20guided%20by%20prior%20jamming%20information.%20Specifically%2C%20we%20introduce%20a%20point%20spread%20function%20%28PSF%29%20as%20prior%20information%20to%20model%20the%20HRRP%20distortion%20induced%20by%20ISRJ.%20Based%20on%20this%2C%20we%20design%20a%20recognition%20network%20that%20leverages%20this%20prior%20through%20a%20prior-guided%20feature%20interaction%20module%20and%20a%20hybrid%20loss%20function%20to%20enhance%20the%20model%27s%20discriminative%20capability.%20With%20the%20aid%20of%20prior%20information%2C%20the%20model%20can%20learn%20invariant%20features%20within%20distorted%20HRRP%20under%20different%20jamming%20parameters.%20Both%20the%20simulated%20and%20measured-data%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20approaches%20and%20exhibits%20stronger%20generalization%20capabilities%20when%20facing%20unseen%20jamming%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2511.23256v1&entry.124074799=Read"},
{"title": "Video-CoM: Interactive Video Reasoning via Chain of Manipulations", "author": "Hanoona Rasheed and Mohammed Zumri and Muhammad Maaz and Ming-Hsuan Yang and Fahad Shahbaz Khan and Salman Khan", "abstract": "Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still \"think about videos\" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to \"think with videos\". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM", "link": "http://arxiv.org/abs/2511.23477v1", "date": "2025-11-28", "relevancy": 2.3135, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-CoM%3A%20Interactive%20Video%20Reasoning%20via%20Chain%20of%20Manipulations&body=Title%3A%20Video-CoM%3A%20Interactive%20Video%20Reasoning%20via%20Chain%20of%20Manipulations%0AAuthor%3A%20Hanoona%20Rasheed%20and%20Mohammed%20Zumri%20and%20Muhammad%20Maaz%20and%20Ming-Hsuan%20Yang%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20Recent%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20video%20understanding%2C%20yet%20most%20still%20%22think%20about%20videos%22%20ie%20once%20a%20video%20is%20encoded%2C%20reasoning%20unfolds%20entirely%20in%20text%2C%20treating%20visual%20input%20as%20a%20static%20context.%20This%20passive%20paradigm%20creates%20a%20semantic%20bottleneck%3A%20models%20cannot%20rewatch%2C%20refocus%2C%20or%20verify%20evidence%2C%20leading%20to%20shallow%20visual%20reasoning%20on%20tasks%20requiring%20fine%20grained%20spatio%20temporal%20understanding.%20In%20this%20work%2C%20we%20introduce%20Interactive%20Video%20Reasoning%2C%20a%20new%20paradigm%20that%20transforms%20video%20into%20an%20active%20cognitive%20workspace%2C%20enabling%20models%20to%20%22think%20with%20videos%22.%20Our%20model%2C%20Video%20CoM%2C%20reasons%20through%20a%20Chain%20of%20Manipulations%20%28CoM%29%2C%20performing%20iterative%20visual%20actions%20to%20gather%20and%20refine%20evidence.%20To%20support%20this%20behavior%2C%20we%20construct%20Video%20CoM%20Instruct%2C%20an%2018K%20instruction%20tuning%20dataset%20curated%20for%20multi%20step%20manipulation%20reasoning.%20Beyond%20supervised%20learning%2C%20we%20further%20optimize%20the%20manipulation%20policy%20via%20reinforcement%20learning%20with%20reasoning%20aware%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Unlike%20prior%20work%20that%20relies%20solely%20on%20sparse%20answer%20rewards%2C%20our%20method%20introduces%20step%20level%20reasoning%20rewards%2C%20guiding%20the%20model%20toward%20grounded%20and%20consistent%20reasoning.%20Video%20CoM%20achieves%20strong%20results%20across%20nine%20video%20reasoning%20benchmarks%2C%20improving%20average%20performance%20by%203.6%20percent%20over%20recent%20state%20of%20the%20art%20models%2C%20while%20training%20on%20only%2025K%20SFT%20and%203K%20GRPO%20video%20samples%2C%20significantly%20fewer%20than%20comparable%20large%20scale%20models.%20Ablation%20studies%20demonstrate%20that%20reasoning%20aware%20rewards%20improve%20both%20accuracy%20and%20interpretability.%20Code%3A%20https%3A//github.com/mbzuai-oryx/Video-CoM%0ALink%3A%20http%3A//arxiv.org/abs/2511.23477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-CoM%253A%2520Interactive%2520Video%2520Reasoning%2520via%2520Chain%2520of%2520Manipulations%26entry.906535625%3DHanoona%2520Rasheed%2520and%2520Mohammed%2520Zumri%2520and%2520Muhammad%2520Maaz%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3DRecent%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520advanced%2520video%2520understanding%252C%2520yet%2520most%2520still%2520%2522think%2520about%2520videos%2522%2520ie%2520once%2520a%2520video%2520is%2520encoded%252C%2520reasoning%2520unfolds%2520entirely%2520in%2520text%252C%2520treating%2520visual%2520input%2520as%2520a%2520static%2520context.%2520This%2520passive%2520paradigm%2520creates%2520a%2520semantic%2520bottleneck%253A%2520models%2520cannot%2520rewatch%252C%2520refocus%252C%2520or%2520verify%2520evidence%252C%2520leading%2520to%2520shallow%2520visual%2520reasoning%2520on%2520tasks%2520requiring%2520fine%2520grained%2520spatio%2520temporal%2520understanding.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Interactive%2520Video%2520Reasoning%252C%2520a%2520new%2520paradigm%2520that%2520transforms%2520video%2520into%2520an%2520active%2520cognitive%2520workspace%252C%2520enabling%2520models%2520to%2520%2522think%2520with%2520videos%2522.%2520Our%2520model%252C%2520Video%2520CoM%252C%2520reasons%2520through%2520a%2520Chain%2520of%2520Manipulations%2520%2528CoM%2529%252C%2520performing%2520iterative%2520visual%2520actions%2520to%2520gather%2520and%2520refine%2520evidence.%2520To%2520support%2520this%2520behavior%252C%2520we%2520construct%2520Video%2520CoM%2520Instruct%252C%2520an%252018K%2520instruction%2520tuning%2520dataset%2520curated%2520for%2520multi%2520step%2520manipulation%2520reasoning.%2520Beyond%2520supervised%2520learning%252C%2520we%2520further%2520optimize%2520the%2520manipulation%2520policy%2520via%2520reinforcement%2520learning%2520with%2520reasoning%2520aware%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529.%2520Unlike%2520prior%2520work%2520that%2520relies%2520solely%2520on%2520sparse%2520answer%2520rewards%252C%2520our%2520method%2520introduces%2520step%2520level%2520reasoning%2520rewards%252C%2520guiding%2520the%2520model%2520toward%2520grounded%2520and%2520consistent%2520reasoning.%2520Video%2520CoM%2520achieves%2520strong%2520results%2520across%2520nine%2520video%2520reasoning%2520benchmarks%252C%2520improving%2520average%2520performance%2520by%25203.6%2520percent%2520over%2520recent%2520state%2520of%2520the%2520art%2520models%252C%2520while%2520training%2520on%2520only%252025K%2520SFT%2520and%25203K%2520GRPO%2520video%2520samples%252C%2520significantly%2520fewer%2520than%2520comparable%2520large%2520scale%2520models.%2520Ablation%2520studies%2520demonstrate%2520that%2520reasoning%2520aware%2520rewards%2520improve%2520both%2520accuracy%2520and%2520interpretability.%2520Code%253A%2520https%253A//github.com/mbzuai-oryx/Video-CoM%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-CoM%3A%20Interactive%20Video%20Reasoning%20via%20Chain%20of%20Manipulations&entry.906535625=Hanoona%20Rasheed%20and%20Mohammed%20Zumri%20and%20Muhammad%20Maaz%20and%20Ming-Hsuan%20Yang%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=Recent%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20video%20understanding%2C%20yet%20most%20still%20%22think%20about%20videos%22%20ie%20once%20a%20video%20is%20encoded%2C%20reasoning%20unfolds%20entirely%20in%20text%2C%20treating%20visual%20input%20as%20a%20static%20context.%20This%20passive%20paradigm%20creates%20a%20semantic%20bottleneck%3A%20models%20cannot%20rewatch%2C%20refocus%2C%20or%20verify%20evidence%2C%20leading%20to%20shallow%20visual%20reasoning%20on%20tasks%20requiring%20fine%20grained%20spatio%20temporal%20understanding.%20In%20this%20work%2C%20we%20introduce%20Interactive%20Video%20Reasoning%2C%20a%20new%20paradigm%20that%20transforms%20video%20into%20an%20active%20cognitive%20workspace%2C%20enabling%20models%20to%20%22think%20with%20videos%22.%20Our%20model%2C%20Video%20CoM%2C%20reasons%20through%20a%20Chain%20of%20Manipulations%20%28CoM%29%2C%20performing%20iterative%20visual%20actions%20to%20gather%20and%20refine%20evidence.%20To%20support%20this%20behavior%2C%20we%20construct%20Video%20CoM%20Instruct%2C%20an%2018K%20instruction%20tuning%20dataset%20curated%20for%20multi%20step%20manipulation%20reasoning.%20Beyond%20supervised%20learning%2C%20we%20further%20optimize%20the%20manipulation%20policy%20via%20reinforcement%20learning%20with%20reasoning%20aware%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Unlike%20prior%20work%20that%20relies%20solely%20on%20sparse%20answer%20rewards%2C%20our%20method%20introduces%20step%20level%20reasoning%20rewards%2C%20guiding%20the%20model%20toward%20grounded%20and%20consistent%20reasoning.%20Video%20CoM%20achieves%20strong%20results%20across%20nine%20video%20reasoning%20benchmarks%2C%20improving%20average%20performance%20by%203.6%20percent%20over%20recent%20state%20of%20the%20art%20models%2C%20while%20training%20on%20only%2025K%20SFT%20and%203K%20GRPO%20video%20samples%2C%20significantly%20fewer%20than%20comparable%20large%20scale%20models.%20Ablation%20studies%20demonstrate%20that%20reasoning%20aware%20rewards%20improve%20both%20accuracy%20and%20interpretability.%20Code%3A%20https%3A//github.com/mbzuai-oryx/Video-CoM&entry.1838667208=http%3A//arxiv.org/abs/2511.23477v1&entry.124074799=Read"},
{"title": "Obstruction reasoning for robotic grasping", "author": "Runyu Jiao and Matteo Bortolon and Francesco Giuliari and Alice Fasoli and Sergio Povoli and Guofeng Mei and Yiming Wang and Fabio Poiesi", "abstract": "Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.", "link": "http://arxiv.org/abs/2511.23186v1", "date": "2025-11-28", "relevancy": 2.3021, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5953}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5795}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Obstruction%20reasoning%20for%20robotic%20grasping&body=Title%3A%20Obstruction%20reasoning%20for%20robotic%20grasping%0AAuthor%3A%20Runyu%20Jiao%20and%20Matteo%20Bortolon%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Sergio%20Povoli%20and%20Guofeng%20Mei%20and%20Yiming%20Wang%20and%20Fabio%20Poiesi%0AAbstract%3A%20Successful%20robotic%20grasping%20in%20cluttered%20environments%20not%20only%20requires%20a%20model%20to%20visually%20ground%20a%20target%20object%20but%20also%20to%20reason%20about%20obstructions%20that%20must%20be%20cleared%20beforehand.%20While%20current%20vision-language%20embodied%20reasoning%20models%20show%20emergent%20spatial%20understanding%2C%20they%20remain%20limited%20in%20terms%20of%20obstruction%20reasoning%20and%20accessibility%20planning.%20To%20bridge%20this%20gap%2C%20we%20present%20UNOGrasp%2C%20a%20learning-based%20vision-language%20model%20capable%20of%20performing%20visually-grounded%20obstruction%20reasoning%20to%20infer%20the%20sequence%20of%20actions%20needed%20to%20unobstruct%20the%20path%20and%20grasp%20the%20target%20object.%20We%20devise%20a%20novel%20multi-step%20reasoning%20process%20based%20on%20obstruction%20paths%20originated%20by%20the%20target%20object.%20We%20anchor%20each%20reasoning%20step%20with%20obstruction-aware%20visual%20cues%20to%20incentivize%20reasoning%20capability.%20UNOGrasp%20combines%20supervised%20and%20reinforcement%20finetuning%20through%20verifiable%20reasoning%20rewards.%20Moreover%2C%20we%20construct%20UNOBench%2C%20a%20large-scale%20dataset%20for%20both%20training%20and%20benchmarking%2C%20based%20on%20MetaGraspNetV2%2C%20with%20over%20100k%20obstruction%20paths%20annotated%20by%20humans%20with%20obstruction%20ratios%2C%20contact%20points%2C%20and%20natural-language%20instructions.%20Extensive%20experiments%20and%20real-robot%20evaluations%20show%20that%20UNOGrasp%20significantly%20improves%20obstruction%20reasoning%20and%20grasp%20success%20across%20both%20synthetic%20and%20real-world%20environments%2C%20outperforming%20generalist%20and%20proprietary%20alternatives.%20Project%20website%3A%20https%3A//tev-fbk.github.io/UnoGrasp/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObstruction%2520reasoning%2520for%2520robotic%2520grasping%26entry.906535625%3DRunyu%2520Jiao%2520and%2520Matteo%2520Bortolon%2520and%2520Francesco%2520Giuliari%2520and%2520Alice%2520Fasoli%2520and%2520Sergio%2520Povoli%2520and%2520Guofeng%2520Mei%2520and%2520Yiming%2520Wang%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3DSuccessful%2520robotic%2520grasping%2520in%2520cluttered%2520environments%2520not%2520only%2520requires%2520a%2520model%2520to%2520visually%2520ground%2520a%2520target%2520object%2520but%2520also%2520to%2520reason%2520about%2520obstructions%2520that%2520must%2520be%2520cleared%2520beforehand.%2520While%2520current%2520vision-language%2520embodied%2520reasoning%2520models%2520show%2520emergent%2520spatial%2520understanding%252C%2520they%2520remain%2520limited%2520in%2520terms%2520of%2520obstruction%2520reasoning%2520and%2520accessibility%2520planning.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520UNOGrasp%252C%2520a%2520learning-based%2520vision-language%2520model%2520capable%2520of%2520performing%2520visually-grounded%2520obstruction%2520reasoning%2520to%2520infer%2520the%2520sequence%2520of%2520actions%2520needed%2520to%2520unobstruct%2520the%2520path%2520and%2520grasp%2520the%2520target%2520object.%2520We%2520devise%2520a%2520novel%2520multi-step%2520reasoning%2520process%2520based%2520on%2520obstruction%2520paths%2520originated%2520by%2520the%2520target%2520object.%2520We%2520anchor%2520each%2520reasoning%2520step%2520with%2520obstruction-aware%2520visual%2520cues%2520to%2520incentivize%2520reasoning%2520capability.%2520UNOGrasp%2520combines%2520supervised%2520and%2520reinforcement%2520finetuning%2520through%2520verifiable%2520reasoning%2520rewards.%2520Moreover%252C%2520we%2520construct%2520UNOBench%252C%2520a%2520large-scale%2520dataset%2520for%2520both%2520training%2520and%2520benchmarking%252C%2520based%2520on%2520MetaGraspNetV2%252C%2520with%2520over%2520100k%2520obstruction%2520paths%2520annotated%2520by%2520humans%2520with%2520obstruction%2520ratios%252C%2520contact%2520points%252C%2520and%2520natural-language%2520instructions.%2520Extensive%2520experiments%2520and%2520real-robot%2520evaluations%2520show%2520that%2520UNOGrasp%2520significantly%2520improves%2520obstruction%2520reasoning%2520and%2520grasp%2520success%2520across%2520both%2520synthetic%2520and%2520real-world%2520environments%252C%2520outperforming%2520generalist%2520and%2520proprietary%2520alternatives.%2520Project%2520website%253A%2520https%253A//tev-fbk.github.io/UnoGrasp/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Obstruction%20reasoning%20for%20robotic%20grasping&entry.906535625=Runyu%20Jiao%20and%20Matteo%20Bortolon%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Sergio%20Povoli%20and%20Guofeng%20Mei%20and%20Yiming%20Wang%20and%20Fabio%20Poiesi&entry.1292438233=Successful%20robotic%20grasping%20in%20cluttered%20environments%20not%20only%20requires%20a%20model%20to%20visually%20ground%20a%20target%20object%20but%20also%20to%20reason%20about%20obstructions%20that%20must%20be%20cleared%20beforehand.%20While%20current%20vision-language%20embodied%20reasoning%20models%20show%20emergent%20spatial%20understanding%2C%20they%20remain%20limited%20in%20terms%20of%20obstruction%20reasoning%20and%20accessibility%20planning.%20To%20bridge%20this%20gap%2C%20we%20present%20UNOGrasp%2C%20a%20learning-based%20vision-language%20model%20capable%20of%20performing%20visually-grounded%20obstruction%20reasoning%20to%20infer%20the%20sequence%20of%20actions%20needed%20to%20unobstruct%20the%20path%20and%20grasp%20the%20target%20object.%20We%20devise%20a%20novel%20multi-step%20reasoning%20process%20based%20on%20obstruction%20paths%20originated%20by%20the%20target%20object.%20We%20anchor%20each%20reasoning%20step%20with%20obstruction-aware%20visual%20cues%20to%20incentivize%20reasoning%20capability.%20UNOGrasp%20combines%20supervised%20and%20reinforcement%20finetuning%20through%20verifiable%20reasoning%20rewards.%20Moreover%2C%20we%20construct%20UNOBench%2C%20a%20large-scale%20dataset%20for%20both%20training%20and%20benchmarking%2C%20based%20on%20MetaGraspNetV2%2C%20with%20over%20100k%20obstruction%20paths%20annotated%20by%20humans%20with%20obstruction%20ratios%2C%20contact%20points%2C%20and%20natural-language%20instructions.%20Extensive%20experiments%20and%20real-robot%20evaluations%20show%20that%20UNOGrasp%20significantly%20improves%20obstruction%20reasoning%20and%20grasp%20success%20across%20both%20synthetic%20and%20real-world%20environments%2C%20outperforming%20generalist%20and%20proprietary%20alternatives.%20Project%20website%3A%20https%3A//tev-fbk.github.io/UnoGrasp/.&entry.1838667208=http%3A//arxiv.org/abs/2511.23186v1&entry.124074799=Read"},
{"title": "Adapting Neural Audio Codecs to EEG", "author": "Ard Kastrati and Luca Lanzend\u00f6rfer and Riccardo Rigoni and John Staib Matilla and Roger Wattenhofer", "abstract": "EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec's input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec's stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.", "link": "http://arxiv.org/abs/2511.23142v1", "date": "2025-11-28", "relevancy": 2.3007, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4813}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4503}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Neural%20Audio%20Codecs%20to%20EEG&body=Title%3A%20Adapting%20Neural%20Audio%20Codecs%20to%20EEG%0AAuthor%3A%20Ard%20Kastrati%20and%20Luca%20Lanzend%C3%B6rfer%20and%20Riccardo%20Rigoni%20and%20John%20Staib%20Matilla%20and%20Roger%20Wattenhofer%0AAbstract%3A%20EEG%20and%20audio%20are%20inherently%20distinct%20modalities%2C%20differing%20in%20sampling%20rate%2C%20channel%20structure%2C%20and%20scale.%20Yet%2C%20we%20show%20that%20pretrained%20neural%20audio%20codecs%20can%20serve%20as%20effective%20starting%20points%20for%20EEG%20compression%2C%20provided%20that%20the%20data%20are%20preprocessed%20to%20be%20suitable%20to%20the%20codec%27s%20input%20constraints.%20Using%20DAC%2C%20a%20state-of-the-art%20neural%20audio%20codec%20as%20our%20base%2C%20we%20demonstrate%20that%20raw%20EEG%20can%20be%20mapped%20into%20the%20codec%27s%20stride-based%20framing%2C%20enabling%20direct%20reuse%20of%20the%20audio-pretrained%20encoder-decoder.%20Even%20without%20modification%2C%20this%20setup%20yields%20stable%20EEG%20reconstructions%2C%20and%20fine-tuning%20on%20EEG%20data%20further%20improves%20fidelity%20and%20generalization%20compared%20to%20training%20from%20scratch.%20We%20systematically%20explore%20compression-quality%20trade-offs%20by%20varying%20residual%20codebook%20depth%2C%20codebook%20%28vocabulary%29%20size%2C%20and%20input%20sampling%20rate.%20To%20capture%20spatial%20dependencies%20across%20electrodes%2C%20we%20propose%20DAC-MC%2C%20a%20multi-channel%20extension%20with%20attention-based%20cross-channel%20aggregation%20and%20channel-specific%20decoding%2C%20while%20retaining%20the%20audio-pretrained%20initialization.%20Evaluations%20on%20the%20TUH%20Abnormal%20and%20Epilepsy%20datasets%20show%20that%20the%20adapted%20codecs%20preserve%20clinically%20relevant%20information%2C%20as%20reflected%20in%20spectrogram-based%20reconstruction%20loss%20and%20downstream%20classification%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Neural%2520Audio%2520Codecs%2520to%2520EEG%26entry.906535625%3DArd%2520Kastrati%2520and%2520Luca%2520Lanzend%25C3%25B6rfer%2520and%2520Riccardo%2520Rigoni%2520and%2520John%2520Staib%2520Matilla%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3DEEG%2520and%2520audio%2520are%2520inherently%2520distinct%2520modalities%252C%2520differing%2520in%2520sampling%2520rate%252C%2520channel%2520structure%252C%2520and%2520scale.%2520Yet%252C%2520we%2520show%2520that%2520pretrained%2520neural%2520audio%2520codecs%2520can%2520serve%2520as%2520effective%2520starting%2520points%2520for%2520EEG%2520compression%252C%2520provided%2520that%2520the%2520data%2520are%2520preprocessed%2520to%2520be%2520suitable%2520to%2520the%2520codec%2527s%2520input%2520constraints.%2520Using%2520DAC%252C%2520a%2520state-of-the-art%2520neural%2520audio%2520codec%2520as%2520our%2520base%252C%2520we%2520demonstrate%2520that%2520raw%2520EEG%2520can%2520be%2520mapped%2520into%2520the%2520codec%2527s%2520stride-based%2520framing%252C%2520enabling%2520direct%2520reuse%2520of%2520the%2520audio-pretrained%2520encoder-decoder.%2520Even%2520without%2520modification%252C%2520this%2520setup%2520yields%2520stable%2520EEG%2520reconstructions%252C%2520and%2520fine-tuning%2520on%2520EEG%2520data%2520further%2520improves%2520fidelity%2520and%2520generalization%2520compared%2520to%2520training%2520from%2520scratch.%2520We%2520systematically%2520explore%2520compression-quality%2520trade-offs%2520by%2520varying%2520residual%2520codebook%2520depth%252C%2520codebook%2520%2528vocabulary%2529%2520size%252C%2520and%2520input%2520sampling%2520rate.%2520To%2520capture%2520spatial%2520dependencies%2520across%2520electrodes%252C%2520we%2520propose%2520DAC-MC%252C%2520a%2520multi-channel%2520extension%2520with%2520attention-based%2520cross-channel%2520aggregation%2520and%2520channel-specific%2520decoding%252C%2520while%2520retaining%2520the%2520audio-pretrained%2520initialization.%2520Evaluations%2520on%2520the%2520TUH%2520Abnormal%2520and%2520Epilepsy%2520datasets%2520show%2520that%2520the%2520adapted%2520codecs%2520preserve%2520clinically%2520relevant%2520information%252C%2520as%2520reflected%2520in%2520spectrogram-based%2520reconstruction%2520loss%2520and%2520downstream%2520classification%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Neural%20Audio%20Codecs%20to%20EEG&entry.906535625=Ard%20Kastrati%20and%20Luca%20Lanzend%C3%B6rfer%20and%20Riccardo%20Rigoni%20and%20John%20Staib%20Matilla%20and%20Roger%20Wattenhofer&entry.1292438233=EEG%20and%20audio%20are%20inherently%20distinct%20modalities%2C%20differing%20in%20sampling%20rate%2C%20channel%20structure%2C%20and%20scale.%20Yet%2C%20we%20show%20that%20pretrained%20neural%20audio%20codecs%20can%20serve%20as%20effective%20starting%20points%20for%20EEG%20compression%2C%20provided%20that%20the%20data%20are%20preprocessed%20to%20be%20suitable%20to%20the%20codec%27s%20input%20constraints.%20Using%20DAC%2C%20a%20state-of-the-art%20neural%20audio%20codec%20as%20our%20base%2C%20we%20demonstrate%20that%20raw%20EEG%20can%20be%20mapped%20into%20the%20codec%27s%20stride-based%20framing%2C%20enabling%20direct%20reuse%20of%20the%20audio-pretrained%20encoder-decoder.%20Even%20without%20modification%2C%20this%20setup%20yields%20stable%20EEG%20reconstructions%2C%20and%20fine-tuning%20on%20EEG%20data%20further%20improves%20fidelity%20and%20generalization%20compared%20to%20training%20from%20scratch.%20We%20systematically%20explore%20compression-quality%20trade-offs%20by%20varying%20residual%20codebook%20depth%2C%20codebook%20%28vocabulary%29%20size%2C%20and%20input%20sampling%20rate.%20To%20capture%20spatial%20dependencies%20across%20electrodes%2C%20we%20propose%20DAC-MC%2C%20a%20multi-channel%20extension%20with%20attention-based%20cross-channel%20aggregation%20and%20channel-specific%20decoding%2C%20while%20retaining%20the%20audio-pretrained%20initialization.%20Evaluations%20on%20the%20TUH%20Abnormal%20and%20Epilepsy%20datasets%20show%20that%20the%20adapted%20codecs%20preserve%20clinically%20relevant%20information%2C%20as%20reflected%20in%20spectrogram-based%20reconstruction%20loss%20and%20downstream%20classification%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2511.23142v1&entry.124074799=Read"},
{"title": "Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor Fusion", "author": "Lamiaa H. Zain", "abstract": "Obstacle avoidance is a critical component of the navigation stack required for mobile robots to operate effectively in complex and unknown environments. In this research, three end-to-end Convolutional Neural Networks (CNNs) were trained and evaluated offline and deployed on a differential-drive mobile robot for real-time obstacle avoidance to generate low-level steering commands from synchronized color and depth images acquired by an Intel RealSense D415 RGB-D camera in diverse environments. Offline evaluation showed that the NetConEmb model achieved the best performance with a notably low MedAE of $0.58 \\times 10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture, which reduces the number of trainable parameters by approximately 25\\% and converges faster, produced comparable results with an RMSE of $21.68 \\times 10^{-3}$ rad/s, close to the $21.42 \\times 10^{-3}$ rad/s obtained by NetConEmb. Real-time navigation further confirmed NetConEmb's robustness, achieving a 100\\% success rate in both known and unknown environments, while NetEmb and NetGated succeeded only in navigating the known environment.", "link": "http://arxiv.org/abs/2509.08095v2", "date": "2025-11-28", "relevancy": 2.294, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5761}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5722}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Obstacle%20Avoidance%20for%20a%20Mobile%20Robot%20Using%20CNN-Based%20Sensor%20Fusion&body=Title%3A%20Real-Time%20Obstacle%20Avoidance%20for%20a%20Mobile%20Robot%20Using%20CNN-Based%20Sensor%20Fusion%0AAuthor%3A%20Lamiaa%20H.%20Zain%0AAbstract%3A%20Obstacle%20avoidance%20is%20a%20critical%20component%20of%20the%20navigation%20stack%20required%20for%20mobile%20robots%20to%20operate%20effectively%20in%20complex%20and%20unknown%20environments.%20In%20this%20research%2C%20three%20end-to-end%20Convolutional%20Neural%20Networks%20%28CNNs%29%20were%20trained%20and%20evaluated%20offline%20and%20deployed%20on%20a%20differential-drive%20mobile%20robot%20for%20real-time%20obstacle%20avoidance%20to%20generate%20low-level%20steering%20commands%20from%20synchronized%20color%20and%20depth%20images%20acquired%20by%20an%20Intel%20RealSense%20D415%20RGB-D%20camera%20in%20diverse%20environments.%20Offline%20evaluation%20showed%20that%20the%20NetConEmb%20model%20achieved%20the%20best%20performance%20with%20a%20notably%20low%20MedAE%20of%20%240.58%20%5Ctimes%2010%5E%7B-3%7D%24%20rad/s.%20In%20comparison%2C%20the%20lighter%20NetEmb%20architecture%2C%20which%20reduces%20the%20number%20of%20trainable%20parameters%20by%20approximately%2025%5C%25%20and%20converges%20faster%2C%20produced%20comparable%20results%20with%20an%20RMSE%20of%20%2421.68%20%5Ctimes%2010%5E%7B-3%7D%24%20rad/s%2C%20close%20to%20the%20%2421.42%20%5Ctimes%2010%5E%7B-3%7D%24%20rad/s%20obtained%20by%20NetConEmb.%20Real-time%20navigation%20further%20confirmed%20NetConEmb%27s%20robustness%2C%20achieving%20a%20100%5C%25%20success%20rate%20in%20both%20known%20and%20unknown%20environments%2C%20while%20NetEmb%20and%20NetGated%20succeeded%20only%20in%20navigating%20the%20known%20environment.%0ALink%3A%20http%3A//arxiv.org/abs/2509.08095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Obstacle%2520Avoidance%2520for%2520a%2520Mobile%2520Robot%2520Using%2520CNN-Based%2520Sensor%2520Fusion%26entry.906535625%3DLamiaa%2520H.%2520Zain%26entry.1292438233%3DObstacle%2520avoidance%2520is%2520a%2520critical%2520component%2520of%2520the%2520navigation%2520stack%2520required%2520for%2520mobile%2520robots%2520to%2520operate%2520effectively%2520in%2520complex%2520and%2520unknown%2520environments.%2520In%2520this%2520research%252C%2520three%2520end-to-end%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520were%2520trained%2520and%2520evaluated%2520offline%2520and%2520deployed%2520on%2520a%2520differential-drive%2520mobile%2520robot%2520for%2520real-time%2520obstacle%2520avoidance%2520to%2520generate%2520low-level%2520steering%2520commands%2520from%2520synchronized%2520color%2520and%2520depth%2520images%2520acquired%2520by%2520an%2520Intel%2520RealSense%2520D415%2520RGB-D%2520camera%2520in%2520diverse%2520environments.%2520Offline%2520evaluation%2520showed%2520that%2520the%2520NetConEmb%2520model%2520achieved%2520the%2520best%2520performance%2520with%2520a%2520notably%2520low%2520MedAE%2520of%2520%25240.58%2520%255Ctimes%252010%255E%257B-3%257D%2524%2520rad/s.%2520In%2520comparison%252C%2520the%2520lighter%2520NetEmb%2520architecture%252C%2520which%2520reduces%2520the%2520number%2520of%2520trainable%2520parameters%2520by%2520approximately%252025%255C%2525%2520and%2520converges%2520faster%252C%2520produced%2520comparable%2520results%2520with%2520an%2520RMSE%2520of%2520%252421.68%2520%255Ctimes%252010%255E%257B-3%257D%2524%2520rad/s%252C%2520close%2520to%2520the%2520%252421.42%2520%255Ctimes%252010%255E%257B-3%257D%2524%2520rad/s%2520obtained%2520by%2520NetConEmb.%2520Real-time%2520navigation%2520further%2520confirmed%2520NetConEmb%2527s%2520robustness%252C%2520achieving%2520a%2520100%255C%2525%2520success%2520rate%2520in%2520both%2520known%2520and%2520unknown%2520environments%252C%2520while%2520NetEmb%2520and%2520NetGated%2520succeeded%2520only%2520in%2520navigating%2520the%2520known%2520environment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Obstacle%20Avoidance%20for%20a%20Mobile%20Robot%20Using%20CNN-Based%20Sensor%20Fusion&entry.906535625=Lamiaa%20H.%20Zain&entry.1292438233=Obstacle%20avoidance%20is%20a%20critical%20component%20of%20the%20navigation%20stack%20required%20for%20mobile%20robots%20to%20operate%20effectively%20in%20complex%20and%20unknown%20environments.%20In%20this%20research%2C%20three%20end-to-end%20Convolutional%20Neural%20Networks%20%28CNNs%29%20were%20trained%20and%20evaluated%20offline%20and%20deployed%20on%20a%20differential-drive%20mobile%20robot%20for%20real-time%20obstacle%20avoidance%20to%20generate%20low-level%20steering%20commands%20from%20synchronized%20color%20and%20depth%20images%20acquired%20by%20an%20Intel%20RealSense%20D415%20RGB-D%20camera%20in%20diverse%20environments.%20Offline%20evaluation%20showed%20that%20the%20NetConEmb%20model%20achieved%20the%20best%20performance%20with%20a%20notably%20low%20MedAE%20of%20%240.58%20%5Ctimes%2010%5E%7B-3%7D%24%20rad/s.%20In%20comparison%2C%20the%20lighter%20NetEmb%20architecture%2C%20which%20reduces%20the%20number%20of%20trainable%20parameters%20by%20approximately%2025%5C%25%20and%20converges%20faster%2C%20produced%20comparable%20results%20with%20an%20RMSE%20of%20%2421.68%20%5Ctimes%2010%5E%7B-3%7D%24%20rad/s%2C%20close%20to%20the%20%2421.42%20%5Ctimes%2010%5E%7B-3%7D%24%20rad/s%20obtained%20by%20NetConEmb.%20Real-time%20navigation%20further%20confirmed%20NetConEmb%27s%20robustness%2C%20achieving%20a%20100%5C%25%20success%20rate%20in%20both%20known%20and%20unknown%20environments%2C%20while%20NetEmb%20and%20NetGated%20succeeded%20only%20in%20navigating%20the%20known%20environment.&entry.1838667208=http%3A//arxiv.org/abs/2509.08095v2&entry.124074799=Read"},
{"title": "Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models", "author": "Muhammad Maaz and Hanoona Rasheed and Fahad Shahbaz Khan and Salman Khan", "abstract": "Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.", "link": "http://arxiv.org/abs/2511.23478v1", "date": "2025-11-28", "relevancy": 2.2778, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-R2%3A%20Reinforcing%20Consistent%20and%20Grounded%20Reasoning%20in%20Multimodal%20Language%20Models&body=Title%3A%20Video-R2%3A%20Reinforcing%20Consistent%20and%20Grounded%20Reasoning%20in%20Multimodal%20Language%20Models%0AAuthor%3A%20Muhammad%20Maaz%20and%20Hanoona%20Rasheed%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20Reasoning%20over%20dynamic%20visual%20content%20remains%20a%20central%20challenge%20for%20multimodal%20large%20language%20models.%20Recent%20thinking%20models%20generate%20explicit%20reasoning%20traces%20for%20interpretability%3B%20however%2C%20their%20reasoning%20often%20appears%20convincing%20while%20being%20logically%20inconsistent%20or%20weakly%20grounded%20in%20visual%20evidence.%20We%20identify%20and%20formalize%20these%20issues%20through%20two%20diagnostic%20metrics%3A%20Think%20Answer%20Consistency%20%28TAC%29%2C%20which%20measures%20the%20alignment%20between%20reasoning%20and%20answers%2C%20and%20Video%20Attention%20Score%20%28VAS%29%2C%20which%20captures%20the%20extent%20to%20which%20reasoning%20depends%20on%20visual%20versus%20textual%20cues.%20Analysis%20across%2011%20video%20reasoning%20benchmarks%20shows%20that%20current%20models%20rely%20heavily%20on%20linguistic%20priors%20rather%20than%20visual%20content.%20To%20address%20this%2C%20we%20propose%20a%20reinforcement%20learning%20approach%20that%20enhances%20both%20temporal%20precision%20and%20reasoning%20consistency.%20Our%20approach%20combines%20timestamp%20aware%20supervised%20fine%20tuning%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20guided%20by%20a%20novel%20Temporal%20Alignment%20Reward%20%28TAR%29.%20This%20dual%20step%20post%20training%20stage%20encourages%20temporally%20aligned%20and%20causally%20coherent%20video%20reasoning.%20The%20resulting%20model%2C%20Video%20R2%2C%20achieves%20consistently%20higher%20TAC%2C%20VAS%2C%20and%20accuracy%20across%20multiple%20benchmarks%2C%20demonstrating%20that%20improvements%20in%20temporal%20alignment%20and%20reasoning%20coherence%20lead%20to%20more%20accurate%20and%20trustworthy%20video%20understanding.%20Our%20code%2C%20dataset%2C%20and%20model%20will%20be%20open%20sourced.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-R2%253A%2520Reinforcing%2520Consistent%2520and%2520Grounded%2520Reasoning%2520in%2520Multimodal%2520Language%2520Models%26entry.906535625%3DMuhammad%2520Maaz%2520and%2520Hanoona%2520Rasheed%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3DReasoning%2520over%2520dynamic%2520visual%2520content%2520remains%2520a%2520central%2520challenge%2520for%2520multimodal%2520large%2520language%2520models.%2520Recent%2520thinking%2520models%2520generate%2520explicit%2520reasoning%2520traces%2520for%2520interpretability%253B%2520however%252C%2520their%2520reasoning%2520often%2520appears%2520convincing%2520while%2520being%2520logically%2520inconsistent%2520or%2520weakly%2520grounded%2520in%2520visual%2520evidence.%2520We%2520identify%2520and%2520formalize%2520these%2520issues%2520through%2520two%2520diagnostic%2520metrics%253A%2520Think%2520Answer%2520Consistency%2520%2528TAC%2529%252C%2520which%2520measures%2520the%2520alignment%2520between%2520reasoning%2520and%2520answers%252C%2520and%2520Video%2520Attention%2520Score%2520%2528VAS%2529%252C%2520which%2520captures%2520the%2520extent%2520to%2520which%2520reasoning%2520depends%2520on%2520visual%2520versus%2520textual%2520cues.%2520Analysis%2520across%252011%2520video%2520reasoning%2520benchmarks%2520shows%2520that%2520current%2520models%2520rely%2520heavily%2520on%2520linguistic%2520priors%2520rather%2520than%2520visual%2520content.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520reinforcement%2520learning%2520approach%2520that%2520enhances%2520both%2520temporal%2520precision%2520and%2520reasoning%2520consistency.%2520Our%2520approach%2520combines%2520timestamp%2520aware%2520supervised%2520fine%2520tuning%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520guided%2520by%2520a%2520novel%2520Temporal%2520Alignment%2520Reward%2520%2528TAR%2529.%2520This%2520dual%2520step%2520post%2520training%2520stage%2520encourages%2520temporally%2520aligned%2520and%2520causally%2520coherent%2520video%2520reasoning.%2520The%2520resulting%2520model%252C%2520Video%2520R2%252C%2520achieves%2520consistently%2520higher%2520TAC%252C%2520VAS%252C%2520and%2520accuracy%2520across%2520multiple%2520benchmarks%252C%2520demonstrating%2520that%2520improvements%2520in%2520temporal%2520alignment%2520and%2520reasoning%2520coherence%2520lead%2520to%2520more%2520accurate%2520and%2520trustworthy%2520video%2520understanding.%2520Our%2520code%252C%2520dataset%252C%2520and%2520model%2520will%2520be%2520open%2520sourced.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-R2%3A%20Reinforcing%20Consistent%20and%20Grounded%20Reasoning%20in%20Multimodal%20Language%20Models&entry.906535625=Muhammad%20Maaz%20and%20Hanoona%20Rasheed%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=Reasoning%20over%20dynamic%20visual%20content%20remains%20a%20central%20challenge%20for%20multimodal%20large%20language%20models.%20Recent%20thinking%20models%20generate%20explicit%20reasoning%20traces%20for%20interpretability%3B%20however%2C%20their%20reasoning%20often%20appears%20convincing%20while%20being%20logically%20inconsistent%20or%20weakly%20grounded%20in%20visual%20evidence.%20We%20identify%20and%20formalize%20these%20issues%20through%20two%20diagnostic%20metrics%3A%20Think%20Answer%20Consistency%20%28TAC%29%2C%20which%20measures%20the%20alignment%20between%20reasoning%20and%20answers%2C%20and%20Video%20Attention%20Score%20%28VAS%29%2C%20which%20captures%20the%20extent%20to%20which%20reasoning%20depends%20on%20visual%20versus%20textual%20cues.%20Analysis%20across%2011%20video%20reasoning%20benchmarks%20shows%20that%20current%20models%20rely%20heavily%20on%20linguistic%20priors%20rather%20than%20visual%20content.%20To%20address%20this%2C%20we%20propose%20a%20reinforcement%20learning%20approach%20that%20enhances%20both%20temporal%20precision%20and%20reasoning%20consistency.%20Our%20approach%20combines%20timestamp%20aware%20supervised%20fine%20tuning%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20guided%20by%20a%20novel%20Temporal%20Alignment%20Reward%20%28TAR%29.%20This%20dual%20step%20post%20training%20stage%20encourages%20temporally%20aligned%20and%20causally%20coherent%20video%20reasoning.%20The%20resulting%20model%2C%20Video%20R2%2C%20achieves%20consistently%20higher%20TAC%2C%20VAS%2C%20and%20accuracy%20across%20multiple%20benchmarks%2C%20demonstrating%20that%20improvements%20in%20temporal%20alignment%20and%20reasoning%20coherence%20lead%20to%20more%20accurate%20and%20trustworthy%20video%20understanding.%20Our%20code%2C%20dataset%2C%20and%20model%20will%20be%20open%20sourced.&entry.1838667208=http%3A//arxiv.org/abs/2511.23478v1&entry.124074799=Read"},
{"title": "Underactuated Robotic Hand with Grasp State Estimation Using Tendon-Based Proprioception", "author": "Jae-Hyun Lee and Jonghoo Park and Kyu-Jin Cho", "abstract": "Anthropomorphic underactuated hands are valued for their structural simplicity and inherent adaptability. However, the uncertainty arising from interdependent joint motions makes it challenging to capture various grasp states during hand-object interaction without increasing structural complexity through multiple embedded sensors. This motivates the need for an approach that can extract rich grasp-state information from a single sensing source while preserving the simplicity of underactuation. This study proposes an anthropomorphic underactuated hand that achieves comprehensive grasp state estimation, using only tendon-based proprioception provided by series elastic actuators (SEAs). Our approach is enabled by the design of a compact SEA with high accuracy and reliability that can be seamlessly integrated into sensorless fingers. By coupling accurate proprioceptive measurements with potential energy-based modeling, the system estimates multiple key grasp state variables, including contact timing, joint angles, relative object stiffness, and external disturbances. Finger-level experimental validations and extensive hand-level grasp functionality demonstrations confirmed the effectiveness of the proposed approach. These results highlight tendon-based proprioception as a compact and robust sensing modality for practical manipulation without reliance on vision or tactile feedback.", "link": "http://arxiv.org/abs/2509.12969v2", "date": "2025-11-28", "relevancy": 2.2775, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5935}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5774}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Underactuated%20Robotic%20Hand%20with%20Grasp%20State%20Estimation%20Using%20Tendon-Based%20Proprioception&body=Title%3A%20Underactuated%20Robotic%20Hand%20with%20Grasp%20State%20Estimation%20Using%20Tendon-Based%20Proprioception%0AAuthor%3A%20Jae-Hyun%20Lee%20and%20Jonghoo%20Park%20and%20Kyu-Jin%20Cho%0AAbstract%3A%20Anthropomorphic%20underactuated%20hands%20are%20valued%20for%20their%20structural%20simplicity%20and%20inherent%20adaptability.%20However%2C%20the%20uncertainty%20arising%20from%20interdependent%20joint%20motions%20makes%20it%20challenging%20to%20capture%20various%20grasp%20states%20during%20hand-object%20interaction%20without%20increasing%20structural%20complexity%20through%20multiple%20embedded%20sensors.%20This%20motivates%20the%20need%20for%20an%20approach%20that%20can%20extract%20rich%20grasp-state%20information%20from%20a%20single%20sensing%20source%20while%20preserving%20the%20simplicity%20of%20underactuation.%20This%20study%20proposes%20an%20anthropomorphic%20underactuated%20hand%20that%20achieves%20comprehensive%20grasp%20state%20estimation%2C%20using%20only%20tendon-based%20proprioception%20provided%20by%20series%20elastic%20actuators%20%28SEAs%29.%20Our%20approach%20is%20enabled%20by%20the%20design%20of%20a%20compact%20SEA%20with%20high%20accuracy%20and%20reliability%20that%20can%20be%20seamlessly%20integrated%20into%20sensorless%20fingers.%20By%20coupling%20accurate%20proprioceptive%20measurements%20with%20potential%20energy-based%20modeling%2C%20the%20system%20estimates%20multiple%20key%20grasp%20state%20variables%2C%20including%20contact%20timing%2C%20joint%20angles%2C%20relative%20object%20stiffness%2C%20and%20external%20disturbances.%20Finger-level%20experimental%20validations%20and%20extensive%20hand-level%20grasp%20functionality%20demonstrations%20confirmed%20the%20effectiveness%20of%20the%20proposed%20approach.%20These%20results%20highlight%20tendon-based%20proprioception%20as%20a%20compact%20and%20robust%20sensing%20modality%20for%20practical%20manipulation%20without%20reliance%20on%20vision%20or%20tactile%20feedback.%0ALink%3A%20http%3A//arxiv.org/abs/2509.12969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderactuated%2520Robotic%2520Hand%2520with%2520Grasp%2520State%2520Estimation%2520Using%2520Tendon-Based%2520Proprioception%26entry.906535625%3DJae-Hyun%2520Lee%2520and%2520Jonghoo%2520Park%2520and%2520Kyu-Jin%2520Cho%26entry.1292438233%3DAnthropomorphic%2520underactuated%2520hands%2520are%2520valued%2520for%2520their%2520structural%2520simplicity%2520and%2520inherent%2520adaptability.%2520However%252C%2520the%2520uncertainty%2520arising%2520from%2520interdependent%2520joint%2520motions%2520makes%2520it%2520challenging%2520to%2520capture%2520various%2520grasp%2520states%2520during%2520hand-object%2520interaction%2520without%2520increasing%2520structural%2520complexity%2520through%2520multiple%2520embedded%2520sensors.%2520This%2520motivates%2520the%2520need%2520for%2520an%2520approach%2520that%2520can%2520extract%2520rich%2520grasp-state%2520information%2520from%2520a%2520single%2520sensing%2520source%2520while%2520preserving%2520the%2520simplicity%2520of%2520underactuation.%2520This%2520study%2520proposes%2520an%2520anthropomorphic%2520underactuated%2520hand%2520that%2520achieves%2520comprehensive%2520grasp%2520state%2520estimation%252C%2520using%2520only%2520tendon-based%2520proprioception%2520provided%2520by%2520series%2520elastic%2520actuators%2520%2528SEAs%2529.%2520Our%2520approach%2520is%2520enabled%2520by%2520the%2520design%2520of%2520a%2520compact%2520SEA%2520with%2520high%2520accuracy%2520and%2520reliability%2520that%2520can%2520be%2520seamlessly%2520integrated%2520into%2520sensorless%2520fingers.%2520By%2520coupling%2520accurate%2520proprioceptive%2520measurements%2520with%2520potential%2520energy-based%2520modeling%252C%2520the%2520system%2520estimates%2520multiple%2520key%2520grasp%2520state%2520variables%252C%2520including%2520contact%2520timing%252C%2520joint%2520angles%252C%2520relative%2520object%2520stiffness%252C%2520and%2520external%2520disturbances.%2520Finger-level%2520experimental%2520validations%2520and%2520extensive%2520hand-level%2520grasp%2520functionality%2520demonstrations%2520confirmed%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%2520These%2520results%2520highlight%2520tendon-based%2520proprioception%2520as%2520a%2520compact%2520and%2520robust%2520sensing%2520modality%2520for%2520practical%2520manipulation%2520without%2520reliance%2520on%2520vision%2520or%2520tactile%2520feedback.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Underactuated%20Robotic%20Hand%20with%20Grasp%20State%20Estimation%20Using%20Tendon-Based%20Proprioception&entry.906535625=Jae-Hyun%20Lee%20and%20Jonghoo%20Park%20and%20Kyu-Jin%20Cho&entry.1292438233=Anthropomorphic%20underactuated%20hands%20are%20valued%20for%20their%20structural%20simplicity%20and%20inherent%20adaptability.%20However%2C%20the%20uncertainty%20arising%20from%20interdependent%20joint%20motions%20makes%20it%20challenging%20to%20capture%20various%20grasp%20states%20during%20hand-object%20interaction%20without%20increasing%20structural%20complexity%20through%20multiple%20embedded%20sensors.%20This%20motivates%20the%20need%20for%20an%20approach%20that%20can%20extract%20rich%20grasp-state%20information%20from%20a%20single%20sensing%20source%20while%20preserving%20the%20simplicity%20of%20underactuation.%20This%20study%20proposes%20an%20anthropomorphic%20underactuated%20hand%20that%20achieves%20comprehensive%20grasp%20state%20estimation%2C%20using%20only%20tendon-based%20proprioception%20provided%20by%20series%20elastic%20actuators%20%28SEAs%29.%20Our%20approach%20is%20enabled%20by%20the%20design%20of%20a%20compact%20SEA%20with%20high%20accuracy%20and%20reliability%20that%20can%20be%20seamlessly%20integrated%20into%20sensorless%20fingers.%20By%20coupling%20accurate%20proprioceptive%20measurements%20with%20potential%20energy-based%20modeling%2C%20the%20system%20estimates%20multiple%20key%20grasp%20state%20variables%2C%20including%20contact%20timing%2C%20joint%20angles%2C%20relative%20object%20stiffness%2C%20and%20external%20disturbances.%20Finger-level%20experimental%20validations%20and%20extensive%20hand-level%20grasp%20functionality%20demonstrations%20confirmed%20the%20effectiveness%20of%20the%20proposed%20approach.%20These%20results%20highlight%20tendon-based%20proprioception%20as%20a%20compact%20and%20robust%20sensing%20modality%20for%20practical%20manipulation%20without%20reliance%20on%20vision%20or%20tactile%20feedback.&entry.1838667208=http%3A//arxiv.org/abs/2509.12969v2&entry.124074799=Read"},
{"title": "Object-Centric Data Synthesis for Category-level Object Detection", "author": "Vikhyat Agarwal and Jiayi Cora Guo and Declan Hoban and Sissi Zhang and Nicholas Moran and Peter Cho and Srilakshmi Pattabiraman and Shantanu Joshi", "abstract": "Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model's detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.", "link": "http://arxiv.org/abs/2511.23450v1", "date": "2025-11-28", "relevancy": 2.2719, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5756}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5664}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Centric%20Data%20Synthesis%20for%20Category-level%20Object%20Detection&body=Title%3A%20Object-Centric%20Data%20Synthesis%20for%20Category-level%20Object%20Detection%0AAuthor%3A%20Vikhyat%20Agarwal%20and%20Jiayi%20Cora%20Guo%20and%20Declan%20Hoban%20and%20Sissi%20Zhang%20and%20Nicholas%20Moran%20and%20Peter%20Cho%20and%20Srilakshmi%20Pattabiraman%20and%20Shantanu%20Joshi%0AAbstract%3A%20Deep%20learning%20approaches%20to%20object%20detection%20have%20achieved%20reliable%20detection%20of%20specific%20object%20classes%20in%20images.%20However%2C%20extending%20a%20model%27s%20detection%20capability%20to%20new%20object%20classes%20requires%20large%20amounts%20of%20annotated%20training%20data%2C%20which%20is%20costly%20and%20time-consuming%20to%20acquire%2C%20especially%20for%20long-tailed%20classes%20with%20insufficient%20representation%20in%20existing%20datasets.%20Here%2C%20we%20introduce%20the%20object-centric%20data%20setting%2C%20when%20limited%20data%20is%20available%20in%20the%20form%20of%20object-centric%20data%20%28multi-view%20images%20or%203D%20models%29%2C%20and%20systematically%20evaluate%20the%20performance%20of%20four%20different%20data%20synthesis%20methods%20to%20finetune%20object%20detection%20models%20on%20novel%20object%20categories%20in%20this%20setting.%20The%20approaches%20are%20based%20on%20simple%20image%20processing%20techniques%2C%203D%20rendering%2C%20and%20image%20diffusion%20models%2C%20and%20use%20object-centric%20data%20to%20synthesize%20realistic%2C%20cluttered%20images%20with%20varying%20contextual%20coherence%20and%20complexity.%20We%20assess%20how%20these%20methods%20enable%20models%20to%20achieve%20category-level%20generalization%20in%20real-world%20data%2C%20and%20demonstrate%20significant%20performance%20boosts%20within%20this%20data-constrained%20experimental%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Centric%2520Data%2520Synthesis%2520for%2520Category-level%2520Object%2520Detection%26entry.906535625%3DVikhyat%2520Agarwal%2520and%2520Jiayi%2520Cora%2520Guo%2520and%2520Declan%2520Hoban%2520and%2520Sissi%2520Zhang%2520and%2520Nicholas%2520Moran%2520and%2520Peter%2520Cho%2520and%2520Srilakshmi%2520Pattabiraman%2520and%2520Shantanu%2520Joshi%26entry.1292438233%3DDeep%2520learning%2520approaches%2520to%2520object%2520detection%2520have%2520achieved%2520reliable%2520detection%2520of%2520specific%2520object%2520classes%2520in%2520images.%2520However%252C%2520extending%2520a%2520model%2527s%2520detection%2520capability%2520to%2520new%2520object%2520classes%2520requires%2520large%2520amounts%2520of%2520annotated%2520training%2520data%252C%2520which%2520is%2520costly%2520and%2520time-consuming%2520to%2520acquire%252C%2520especially%2520for%2520long-tailed%2520classes%2520with%2520insufficient%2520representation%2520in%2520existing%2520datasets.%2520Here%252C%2520we%2520introduce%2520the%2520object-centric%2520data%2520setting%252C%2520when%2520limited%2520data%2520is%2520available%2520in%2520the%2520form%2520of%2520object-centric%2520data%2520%2528multi-view%2520images%2520or%25203D%2520models%2529%252C%2520and%2520systematically%2520evaluate%2520the%2520performance%2520of%2520four%2520different%2520data%2520synthesis%2520methods%2520to%2520finetune%2520object%2520detection%2520models%2520on%2520novel%2520object%2520categories%2520in%2520this%2520setting.%2520The%2520approaches%2520are%2520based%2520on%2520simple%2520image%2520processing%2520techniques%252C%25203D%2520rendering%252C%2520and%2520image%2520diffusion%2520models%252C%2520and%2520use%2520object-centric%2520data%2520to%2520synthesize%2520realistic%252C%2520cluttered%2520images%2520with%2520varying%2520contextual%2520coherence%2520and%2520complexity.%2520We%2520assess%2520how%2520these%2520methods%2520enable%2520models%2520to%2520achieve%2520category-level%2520generalization%2520in%2520real-world%2520data%252C%2520and%2520demonstrate%2520significant%2520performance%2520boosts%2520within%2520this%2520data-constrained%2520experimental%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%20Data%20Synthesis%20for%20Category-level%20Object%20Detection&entry.906535625=Vikhyat%20Agarwal%20and%20Jiayi%20Cora%20Guo%20and%20Declan%20Hoban%20and%20Sissi%20Zhang%20and%20Nicholas%20Moran%20and%20Peter%20Cho%20and%20Srilakshmi%20Pattabiraman%20and%20Shantanu%20Joshi&entry.1292438233=Deep%20learning%20approaches%20to%20object%20detection%20have%20achieved%20reliable%20detection%20of%20specific%20object%20classes%20in%20images.%20However%2C%20extending%20a%20model%27s%20detection%20capability%20to%20new%20object%20classes%20requires%20large%20amounts%20of%20annotated%20training%20data%2C%20which%20is%20costly%20and%20time-consuming%20to%20acquire%2C%20especially%20for%20long-tailed%20classes%20with%20insufficient%20representation%20in%20existing%20datasets.%20Here%2C%20we%20introduce%20the%20object-centric%20data%20setting%2C%20when%20limited%20data%20is%20available%20in%20the%20form%20of%20object-centric%20data%20%28multi-view%20images%20or%203D%20models%29%2C%20and%20systematically%20evaluate%20the%20performance%20of%20four%20different%20data%20synthesis%20methods%20to%20finetune%20object%20detection%20models%20on%20novel%20object%20categories%20in%20this%20setting.%20The%20approaches%20are%20based%20on%20simple%20image%20processing%20techniques%2C%203D%20rendering%2C%20and%20image%20diffusion%20models%2C%20and%20use%20object-centric%20data%20to%20synthesize%20realistic%2C%20cluttered%20images%20with%20varying%20contextual%20coherence%20and%20complexity.%20We%20assess%20how%20these%20methods%20enable%20models%20to%20achieve%20category-level%20generalization%20in%20real-world%20data%2C%20and%20demonstrate%20significant%20performance%20boosts%20within%20this%20data-constrained%20experimental%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2511.23450v1&entry.124074799=Read"},
{"title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards", "author": "Charles Arnal and Ga\u00ebtan Narozniak and Vivien Cabannes and Yunhao Tang and Julia Kempe and Remi Munos", "abstract": "Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.", "link": "http://arxiv.org/abs/2506.20520v2", "date": "2025-11-28", "relevancy": 2.2609, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4642}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4551}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymmetric%20REINFORCE%20for%20off-Policy%20Reinforcement%20Learning%3A%20Balancing%20positive%20and%20negative%20rewards&body=Title%3A%20Asymmetric%20REINFORCE%20for%20off-Policy%20Reinforcement%20Learning%3A%20Balancing%20positive%20and%20negative%20rewards%0AAuthor%3A%20Charles%20Arnal%20and%20Ga%C3%ABtan%20Narozniak%20and%20Vivien%20Cabannes%20and%20Yunhao%20Tang%20and%20Julia%20Kempe%20and%20Remi%20Munos%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20is%20increasingly%20used%20to%20align%20large%20language%20models%20%28LLMs%29.%20Off-policy%20methods%20offer%20greater%20implementation%20simplicity%20and%20data%20efficiency%20than%20on-policy%20techniques%2C%20but%20often%20result%20in%20suboptimal%20performance.%20In%20this%20work%2C%20we%20study%20the%20intermediate%20range%20of%20algorithms%20between%20off-policy%20RL%20and%20supervised%20fine-tuning%20by%20analyzing%20a%20simple%20off-policy%20REINFORCE%20algorithm%2C%20where%20the%20advantage%20is%20defined%20as%20%24A%3Dr-V%24%2C%20with%20%24r%24%20a%20reward%20and%20%24V%24%20some%20tunable%20baseline.%20Intuitively%2C%20lowering%20%24V%24%20emphasizes%20high-reward%20samples%2C%20while%20raising%20it%20penalizes%20low-reward%20ones%20more%20heavily.%20We%20first%20provide%20a%20theoretical%20analysis%20of%20this%20off-policy%20REINFORCE%20algorithm%2C%20showing%20that%20when%20the%20baseline%20%24V%24%20lower-bounds%20the%20expected%20reward%2C%20the%20algorithm%20enjoys%20a%20policy%20improvement%20guarantee.%20Our%20analysis%20reveals%20that%20while%20on-policy%20updates%20can%20safely%20leverage%20both%20positive%20and%20negative%20signals%2C%20off-policy%20updates%20benefit%20from%20focusing%20more%20on%20positive%20rewards%20than%20on%20negative%20ones.%20We%20validate%20our%20findings%20experimentally%20in%20a%20controlled%20stochastic%20bandit%20setting%20and%20through%20fine-tuning%20state-of-the-art%20LLMs%20on%20reasoning%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2506.20520v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymmetric%2520REINFORCE%2520for%2520off-Policy%2520Reinforcement%2520Learning%253A%2520Balancing%2520positive%2520and%2520negative%2520rewards%26entry.906535625%3DCharles%2520Arnal%2520and%2520Ga%25C3%25ABtan%2520Narozniak%2520and%2520Vivien%2520Cabannes%2520and%2520Yunhao%2520Tang%2520and%2520Julia%2520Kempe%2520and%2520Remi%2520Munos%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520is%2520increasingly%2520used%2520to%2520align%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Off-policy%2520methods%2520offer%2520greater%2520implementation%2520simplicity%2520and%2520data%2520efficiency%2520than%2520on-policy%2520techniques%252C%2520but%2520often%2520result%2520in%2520suboptimal%2520performance.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520intermediate%2520range%2520of%2520algorithms%2520between%2520off-policy%2520RL%2520and%2520supervised%2520fine-tuning%2520by%2520analyzing%2520a%2520simple%2520off-policy%2520REINFORCE%2520algorithm%252C%2520where%2520the%2520advantage%2520is%2520defined%2520as%2520%2524A%253Dr-V%2524%252C%2520with%2520%2524r%2524%2520a%2520reward%2520and%2520%2524V%2524%2520some%2520tunable%2520baseline.%2520Intuitively%252C%2520lowering%2520%2524V%2524%2520emphasizes%2520high-reward%2520samples%252C%2520while%2520raising%2520it%2520penalizes%2520low-reward%2520ones%2520more%2520heavily.%2520We%2520first%2520provide%2520a%2520theoretical%2520analysis%2520of%2520this%2520off-policy%2520REINFORCE%2520algorithm%252C%2520showing%2520that%2520when%2520the%2520baseline%2520%2524V%2524%2520lower-bounds%2520the%2520expected%2520reward%252C%2520the%2520algorithm%2520enjoys%2520a%2520policy%2520improvement%2520guarantee.%2520Our%2520analysis%2520reveals%2520that%2520while%2520on-policy%2520updates%2520can%2520safely%2520leverage%2520both%2520positive%2520and%2520negative%2520signals%252C%2520off-policy%2520updates%2520benefit%2520from%2520focusing%2520more%2520on%2520positive%2520rewards%2520than%2520on%2520negative%2520ones.%2520We%2520validate%2520our%2520findings%2520experimentally%2520in%2520a%2520controlled%2520stochastic%2520bandit%2520setting%2520and%2520through%2520fine-tuning%2520state-of-the-art%2520LLMs%2520on%2520reasoning%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20520v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymmetric%20REINFORCE%20for%20off-Policy%20Reinforcement%20Learning%3A%20Balancing%20positive%20and%20negative%20rewards&entry.906535625=Charles%20Arnal%20and%20Ga%C3%ABtan%20Narozniak%20and%20Vivien%20Cabannes%20and%20Yunhao%20Tang%20and%20Julia%20Kempe%20and%20Remi%20Munos&entry.1292438233=Reinforcement%20learning%20%28RL%29%20is%20increasingly%20used%20to%20align%20large%20language%20models%20%28LLMs%29.%20Off-policy%20methods%20offer%20greater%20implementation%20simplicity%20and%20data%20efficiency%20than%20on-policy%20techniques%2C%20but%20often%20result%20in%20suboptimal%20performance.%20In%20this%20work%2C%20we%20study%20the%20intermediate%20range%20of%20algorithms%20between%20off-policy%20RL%20and%20supervised%20fine-tuning%20by%20analyzing%20a%20simple%20off-policy%20REINFORCE%20algorithm%2C%20where%20the%20advantage%20is%20defined%20as%20%24A%3Dr-V%24%2C%20with%20%24r%24%20a%20reward%20and%20%24V%24%20some%20tunable%20baseline.%20Intuitively%2C%20lowering%20%24V%24%20emphasizes%20high-reward%20samples%2C%20while%20raising%20it%20penalizes%20low-reward%20ones%20more%20heavily.%20We%20first%20provide%20a%20theoretical%20analysis%20of%20this%20off-policy%20REINFORCE%20algorithm%2C%20showing%20that%20when%20the%20baseline%20%24V%24%20lower-bounds%20the%20expected%20reward%2C%20the%20algorithm%20enjoys%20a%20policy%20improvement%20guarantee.%20Our%20analysis%20reveals%20that%20while%20on-policy%20updates%20can%20safely%20leverage%20both%20positive%20and%20negative%20signals%2C%20off-policy%20updates%20benefit%20from%20focusing%20more%20on%20positive%20rewards%20than%20on%20negative%20ones.%20We%20validate%20our%20findings%20experimentally%20in%20a%20controlled%20stochastic%20bandit%20setting%20and%20through%20fine-tuning%20state-of-the-art%20LLMs%20on%20reasoning%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2506.20520v2&entry.124074799=Read"},
{"title": "Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering", "author": "Zijian Fu and Changsheng Lv and Mengshi Qi and Huadong Ma", "abstract": "In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released.", "link": "http://arxiv.org/abs/2511.23304v1", "date": "2025-11-28", "relevancy": 2.2562, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Scene%20Graph%20with%20Kolmogorov-Arnold%20Experts%20for%20Audio-Visual%20Question%20Answering&body=Title%3A%20Multi-Modal%20Scene%20Graph%20with%20Kolmogorov-Arnold%20Experts%20for%20Audio-Visual%20Question%20Answering%0AAuthor%3A%20Zijian%20Fu%20and%20Changsheng%20Lv%20and%20Mengshi%20Qi%20and%20Huadong%20Ma%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Multi-Modal%20Scene%20Graph%20with%20Kolmogorov-Arnold%20Expert%20Network%20for%20Audio-Visual%20Question%20Answering%20%28SHRIKE%29.%20The%20task%20aims%20to%20mimic%20human%20reasoning%20by%20extracting%20and%20fusing%20information%20from%20audio-visual%20scenes%2C%20with%20the%20main%20challenge%20being%20the%20identification%20of%20question-relevant%20cues%20from%20the%20complex%20audio-visual%20content.%20Existing%20methods%20fail%20to%20capture%20the%20structural%20information%20within%20video%2C%20and%20suffer%20from%20insufficient%20fine-grained%20modeling%20of%20multi-modal%20features.%20To%20address%20these%20issues%2C%20we%20are%20the%20first%20to%20introduce%20a%20new%20multi-modal%20scene%20graph%20that%20explicitly%20models%20the%20objects%20and%20their%20relationship%20as%20a%20visually%20grounded%2C%20structured%20representation%20of%20the%20audio-visual%20scene.%20Furthermore%2C%20we%20design%20a%20Kolmogorov-Arnold%20Network~%28KAN%29-based%20Mixture%20of%20Experts%20%28MoE%29%20to%20enhance%20the%20expressive%20power%20of%20the%20temporal%20integration%20stage.%20This%20enables%20more%20fine-grained%20modeling%20of%20cross-modal%20interactions%20within%20the%20question-aware%20fused%20audio-visual%20representation%2C%20leading%20to%20capture%20richer%20and%20more%20nuanced%20patterns%20and%20then%20improve%20temporal%20reasoning%20performance.%20We%20evaluate%20the%20model%20on%20the%20established%20MUSIC-AVQA%20and%20MUSIC-AVQA%20v2%20benchmarks%2C%20where%20it%20achieves%20state-of-the-art%20performance.%20Code%20and%20model%20checkpoints%20will%20be%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Scene%2520Graph%2520with%2520Kolmogorov-Arnold%2520Experts%2520for%2520Audio-Visual%2520Question%2520Answering%26entry.906535625%3DZijian%2520Fu%2520and%2520Changsheng%2520Lv%2520and%2520Mengshi%2520Qi%2520and%2520Huadong%2520Ma%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Multi-Modal%2520Scene%2520Graph%2520with%2520Kolmogorov-Arnold%2520Expert%2520Network%2520for%2520Audio-Visual%2520Question%2520Answering%2520%2528SHRIKE%2529.%2520The%2520task%2520aims%2520to%2520mimic%2520human%2520reasoning%2520by%2520extracting%2520and%2520fusing%2520information%2520from%2520audio-visual%2520scenes%252C%2520with%2520the%2520main%2520challenge%2520being%2520the%2520identification%2520of%2520question-relevant%2520cues%2520from%2520the%2520complex%2520audio-visual%2520content.%2520Existing%2520methods%2520fail%2520to%2520capture%2520the%2520structural%2520information%2520within%2520video%252C%2520and%2520suffer%2520from%2520insufficient%2520fine-grained%2520modeling%2520of%2520multi-modal%2520features.%2520To%2520address%2520these%2520issues%252C%2520we%2520are%2520the%2520first%2520to%2520introduce%2520a%2520new%2520multi-modal%2520scene%2520graph%2520that%2520explicitly%2520models%2520the%2520objects%2520and%2520their%2520relationship%2520as%2520a%2520visually%2520grounded%252C%2520structured%2520representation%2520of%2520the%2520audio-visual%2520scene.%2520Furthermore%252C%2520we%2520design%2520a%2520Kolmogorov-Arnold%2520Network~%2528KAN%2529-based%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520to%2520enhance%2520the%2520expressive%2520power%2520of%2520the%2520temporal%2520integration%2520stage.%2520This%2520enables%2520more%2520fine-grained%2520modeling%2520of%2520cross-modal%2520interactions%2520within%2520the%2520question-aware%2520fused%2520audio-visual%2520representation%252C%2520leading%2520to%2520capture%2520richer%2520and%2520more%2520nuanced%2520patterns%2520and%2520then%2520improve%2520temporal%2520reasoning%2520performance.%2520We%2520evaluate%2520the%2520model%2520on%2520the%2520established%2520MUSIC-AVQA%2520and%2520MUSIC-AVQA%2520v2%2520benchmarks%252C%2520where%2520it%2520achieves%2520state-of-the-art%2520performance.%2520Code%2520and%2520model%2520checkpoints%2520will%2520be%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Scene%20Graph%20with%20Kolmogorov-Arnold%20Experts%20for%20Audio-Visual%20Question%20Answering&entry.906535625=Zijian%20Fu%20and%20Changsheng%20Lv%20and%20Mengshi%20Qi%20and%20Huadong%20Ma&entry.1292438233=In%20this%20paper%2C%20we%20propose%20a%20novel%20Multi-Modal%20Scene%20Graph%20with%20Kolmogorov-Arnold%20Expert%20Network%20for%20Audio-Visual%20Question%20Answering%20%28SHRIKE%29.%20The%20task%20aims%20to%20mimic%20human%20reasoning%20by%20extracting%20and%20fusing%20information%20from%20audio-visual%20scenes%2C%20with%20the%20main%20challenge%20being%20the%20identification%20of%20question-relevant%20cues%20from%20the%20complex%20audio-visual%20content.%20Existing%20methods%20fail%20to%20capture%20the%20structural%20information%20within%20video%2C%20and%20suffer%20from%20insufficient%20fine-grained%20modeling%20of%20multi-modal%20features.%20To%20address%20these%20issues%2C%20we%20are%20the%20first%20to%20introduce%20a%20new%20multi-modal%20scene%20graph%20that%20explicitly%20models%20the%20objects%20and%20their%20relationship%20as%20a%20visually%20grounded%2C%20structured%20representation%20of%20the%20audio-visual%20scene.%20Furthermore%2C%20we%20design%20a%20Kolmogorov-Arnold%20Network~%28KAN%29-based%20Mixture%20of%20Experts%20%28MoE%29%20to%20enhance%20the%20expressive%20power%20of%20the%20temporal%20integration%20stage.%20This%20enables%20more%20fine-grained%20modeling%20of%20cross-modal%20interactions%20within%20the%20question-aware%20fused%20audio-visual%20representation%2C%20leading%20to%20capture%20richer%20and%20more%20nuanced%20patterns%20and%20then%20improve%20temporal%20reasoning%20performance.%20We%20evaluate%20the%20model%20on%20the%20established%20MUSIC-AVQA%20and%20MUSIC-AVQA%20v2%20benchmarks%2C%20where%20it%20achieves%20state-of-the-art%20performance.%20Code%20and%20model%20checkpoints%20will%20be%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.23304v1&entry.124074799=Read"},
{"title": "Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods", "author": "Jose Moises Araya-Martinez and Adri\u00e1n Sanchis Reig and Gautham Mohan and Sarvenaz Sardari and Jens Lambrecht and J\u00f6rg Kr\u00fcger", "abstract": "Reducing the burden of data generation and annotation remains a major challenge for the cost-effective deployment of machine learning in industrial and robotics settings. While synthetic rendering is a promising solution, bridging the sim-to-real gap often requires expert intervention. In this work, we benchmark a range of domain randomization (DR) and domain adaptation (DA) techniques, including feature-based methods, generative AI (GenAI), and classical rendering approaches, for creating contextualized synthetic data without manual annotation. Our evaluation focuses on the effectiveness and efficiency of low-level and high-level feature alignment, as well as a controlled diffusion-based DA method guided by prompts generated from real-world contexts. We validate our methods on two datasets: a proprietary industrial dataset (automotive and logistics) and a public robotics dataset. Results show that if render-based data with enough variability is available as seed, simpler feature-based methods, such as brightness-based and perceptual hashing filtering, outperform more complex GenAI-based approaches in both accuracy and resource efficiency. Perceptual hashing consistently achieves the highest performance, with mAP50 scores of 98% and 67% on the industrial and robotics datasets, respectively. Additionally, GenAI methods present significant time overhead for data generation at no apparent improvement of sim-to-real mAP values compared to simpler methods. Our findings offer actionable insights for efficiently bridging the sim-to-real gap, enabling high real-world performance from models trained exclusively on synthetic data.", "link": "http://arxiv.org/abs/2511.23241v1", "date": "2025-11-28", "relevancy": 2.2485, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5869}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5657}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Industrial%20Object%20Detection%3A%20GenAI%20vs.%20Feature-Based%20Methods&body=Title%3A%20Synthetic%20Industrial%20Object%20Detection%3A%20GenAI%20vs.%20Feature-Based%20Methods%0AAuthor%3A%20Jose%20Moises%20Araya-Martinez%20and%20Adri%C3%A1n%20Sanchis%20Reig%20and%20Gautham%20Mohan%20and%20Sarvenaz%20Sardari%20and%20Jens%20Lambrecht%20and%20J%C3%B6rg%20Kr%C3%BCger%0AAbstract%3A%20Reducing%20the%20burden%20of%20data%20generation%20and%20annotation%20remains%20a%20major%20challenge%20for%20the%20cost-effective%20deployment%20of%20machine%20learning%20in%20industrial%20and%20robotics%20settings.%20While%20synthetic%20rendering%20is%20a%20promising%20solution%2C%20bridging%20the%20sim-to-real%20gap%20often%20requires%20expert%20intervention.%20In%20this%20work%2C%20we%20benchmark%20a%20range%20of%20domain%20randomization%20%28DR%29%20and%20domain%20adaptation%20%28DA%29%20techniques%2C%20including%20feature-based%20methods%2C%20generative%20AI%20%28GenAI%29%2C%20and%20classical%20rendering%20approaches%2C%20for%20creating%20contextualized%20synthetic%20data%20without%20manual%20annotation.%20Our%20evaluation%20focuses%20on%20the%20effectiveness%20and%20efficiency%20of%20low-level%20and%20high-level%20feature%20alignment%2C%20as%20well%20as%20a%20controlled%20diffusion-based%20DA%20method%20guided%20by%20prompts%20generated%20from%20real-world%20contexts.%20We%20validate%20our%20methods%20on%20two%20datasets%3A%20a%20proprietary%20industrial%20dataset%20%28automotive%20and%20logistics%29%20and%20a%20public%20robotics%20dataset.%20Results%20show%20that%20if%20render-based%20data%20with%20enough%20variability%20is%20available%20as%20seed%2C%20simpler%20feature-based%20methods%2C%20such%20as%20brightness-based%20and%20perceptual%20hashing%20filtering%2C%20outperform%20more%20complex%20GenAI-based%20approaches%20in%20both%20accuracy%20and%20resource%20efficiency.%20Perceptual%20hashing%20consistently%20achieves%20the%20highest%20performance%2C%20with%20mAP50%20scores%20of%2098%25%20and%2067%25%20on%20the%20industrial%20and%20robotics%20datasets%2C%20respectively.%20Additionally%2C%20GenAI%20methods%20present%20significant%20time%20overhead%20for%20data%20generation%20at%20no%20apparent%20improvement%20of%20sim-to-real%20mAP%20values%20compared%20to%20simpler%20methods.%20Our%20findings%20offer%20actionable%20insights%20for%20efficiently%20bridging%20the%20sim-to-real%20gap%2C%20enabling%20high%20real-world%20performance%20from%20models%20trained%20exclusively%20on%20synthetic%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Industrial%2520Object%2520Detection%253A%2520GenAI%2520vs.%2520Feature-Based%2520Methods%26entry.906535625%3DJose%2520Moises%2520Araya-Martinez%2520and%2520Adri%25C3%25A1n%2520Sanchis%2520Reig%2520and%2520Gautham%2520Mohan%2520and%2520Sarvenaz%2520Sardari%2520and%2520Jens%2520Lambrecht%2520and%2520J%25C3%25B6rg%2520Kr%25C3%25BCger%26entry.1292438233%3DReducing%2520the%2520burden%2520of%2520data%2520generation%2520and%2520annotation%2520remains%2520a%2520major%2520challenge%2520for%2520the%2520cost-effective%2520deployment%2520of%2520machine%2520learning%2520in%2520industrial%2520and%2520robotics%2520settings.%2520While%2520synthetic%2520rendering%2520is%2520a%2520promising%2520solution%252C%2520bridging%2520the%2520sim-to-real%2520gap%2520often%2520requires%2520expert%2520intervention.%2520In%2520this%2520work%252C%2520we%2520benchmark%2520a%2520range%2520of%2520domain%2520randomization%2520%2528DR%2529%2520and%2520domain%2520adaptation%2520%2528DA%2529%2520techniques%252C%2520including%2520feature-based%2520methods%252C%2520generative%2520AI%2520%2528GenAI%2529%252C%2520and%2520classical%2520rendering%2520approaches%252C%2520for%2520creating%2520contextualized%2520synthetic%2520data%2520without%2520manual%2520annotation.%2520Our%2520evaluation%2520focuses%2520on%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520low-level%2520and%2520high-level%2520feature%2520alignment%252C%2520as%2520well%2520as%2520a%2520controlled%2520diffusion-based%2520DA%2520method%2520guided%2520by%2520prompts%2520generated%2520from%2520real-world%2520contexts.%2520We%2520validate%2520our%2520methods%2520on%2520two%2520datasets%253A%2520a%2520proprietary%2520industrial%2520dataset%2520%2528automotive%2520and%2520logistics%2529%2520and%2520a%2520public%2520robotics%2520dataset.%2520Results%2520show%2520that%2520if%2520render-based%2520data%2520with%2520enough%2520variability%2520is%2520available%2520as%2520seed%252C%2520simpler%2520feature-based%2520methods%252C%2520such%2520as%2520brightness-based%2520and%2520perceptual%2520hashing%2520filtering%252C%2520outperform%2520more%2520complex%2520GenAI-based%2520approaches%2520in%2520both%2520accuracy%2520and%2520resource%2520efficiency.%2520Perceptual%2520hashing%2520consistently%2520achieves%2520the%2520highest%2520performance%252C%2520with%2520mAP50%2520scores%2520of%252098%2525%2520and%252067%2525%2520on%2520the%2520industrial%2520and%2520robotics%2520datasets%252C%2520respectively.%2520Additionally%252C%2520GenAI%2520methods%2520present%2520significant%2520time%2520overhead%2520for%2520data%2520generation%2520at%2520no%2520apparent%2520improvement%2520of%2520sim-to-real%2520mAP%2520values%2520compared%2520to%2520simpler%2520methods.%2520Our%2520findings%2520offer%2520actionable%2520insights%2520for%2520efficiently%2520bridging%2520the%2520sim-to-real%2520gap%252C%2520enabling%2520high%2520real-world%2520performance%2520from%2520models%2520trained%2520exclusively%2520on%2520synthetic%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Industrial%20Object%20Detection%3A%20GenAI%20vs.%20Feature-Based%20Methods&entry.906535625=Jose%20Moises%20Araya-Martinez%20and%20Adri%C3%A1n%20Sanchis%20Reig%20and%20Gautham%20Mohan%20and%20Sarvenaz%20Sardari%20and%20Jens%20Lambrecht%20and%20J%C3%B6rg%20Kr%C3%BCger&entry.1292438233=Reducing%20the%20burden%20of%20data%20generation%20and%20annotation%20remains%20a%20major%20challenge%20for%20the%20cost-effective%20deployment%20of%20machine%20learning%20in%20industrial%20and%20robotics%20settings.%20While%20synthetic%20rendering%20is%20a%20promising%20solution%2C%20bridging%20the%20sim-to-real%20gap%20often%20requires%20expert%20intervention.%20In%20this%20work%2C%20we%20benchmark%20a%20range%20of%20domain%20randomization%20%28DR%29%20and%20domain%20adaptation%20%28DA%29%20techniques%2C%20including%20feature-based%20methods%2C%20generative%20AI%20%28GenAI%29%2C%20and%20classical%20rendering%20approaches%2C%20for%20creating%20contextualized%20synthetic%20data%20without%20manual%20annotation.%20Our%20evaluation%20focuses%20on%20the%20effectiveness%20and%20efficiency%20of%20low-level%20and%20high-level%20feature%20alignment%2C%20as%20well%20as%20a%20controlled%20diffusion-based%20DA%20method%20guided%20by%20prompts%20generated%20from%20real-world%20contexts.%20We%20validate%20our%20methods%20on%20two%20datasets%3A%20a%20proprietary%20industrial%20dataset%20%28automotive%20and%20logistics%29%20and%20a%20public%20robotics%20dataset.%20Results%20show%20that%20if%20render-based%20data%20with%20enough%20variability%20is%20available%20as%20seed%2C%20simpler%20feature-based%20methods%2C%20such%20as%20brightness-based%20and%20perceptual%20hashing%20filtering%2C%20outperform%20more%20complex%20GenAI-based%20approaches%20in%20both%20accuracy%20and%20resource%20efficiency.%20Perceptual%20hashing%20consistently%20achieves%20the%20highest%20performance%2C%20with%20mAP50%20scores%20of%2098%25%20and%2067%25%20on%20the%20industrial%20and%20robotics%20datasets%2C%20respectively.%20Additionally%2C%20GenAI%20methods%20present%20significant%20time%20overhead%20for%20data%20generation%20at%20no%20apparent%20improvement%20of%20sim-to-real%20mAP%20values%20compared%20to%20simpler%20methods.%20Our%20findings%20offer%20actionable%20insights%20for%20efficiently%20bridging%20the%20sim-to-real%20gap%2C%20enabling%20high%20real-world%20performance%20from%20models%20trained%20exclusively%20on%20synthetic%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.23241v1&entry.124074799=Read"},
{"title": "Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation", "author": "Peiran Sun", "abstract": "Adversarial attack reveals the vulnerability of deep learning models. It is assumed that high curvature may give rise to rough decision boundary and thus result in less robust models. However, the most commonly used \\textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation (DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack (CDBA) with improved performance using the estimated curvature.", "link": "http://arxiv.org/abs/2505.19194v3", "date": "2025-11-28", "relevancy": 2.246, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4605}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.448}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curvature%20Dynamic%20Black-box%20Attack%3A%20revisiting%20adversarial%20robustness%20via%20dynamic%20curvature%20estimation&body=Title%3A%20Curvature%20Dynamic%20Black-box%20Attack%3A%20revisiting%20adversarial%20robustness%20via%20dynamic%20curvature%20estimation%0AAuthor%3A%20Peiran%20Sun%0AAbstract%3A%20Adversarial%20attack%20reveals%20the%20vulnerability%20of%20deep%20learning%20models.%20It%20is%20assumed%20that%20high%20curvature%20may%20give%20rise%20to%20rough%20decision%20boundary%20and%20thus%20result%20in%20less%20robust%20models.%20However%2C%20the%20most%20commonly%20used%20%5Ctextit%7Bcurvature%7D%20is%20the%20curvature%20of%20loss%20function%2C%20scores%20or%20other%20parameters%20from%20within%20the%20model%20as%20opposed%20to%20decision%20boundary%20curvature%2C%20since%20the%20former%20can%20be%20relatively%20easily%20formed%20using%20second%20order%20derivative.%20In%20this%20paper%2C%20we%20propose%20a%20new%20query-efficient%20method%2C%20dynamic%20curvature%20estimation%20%28DCE%29%2C%20to%20estimate%20the%20decision%20boundary%20curvature%20in%20a%20black-box%20setting.%20Our%20approach%20is%20based%20on%20CGBA%2C%20a%20black-box%20adversarial%20attack.%20By%20performing%20DCE%20on%20a%20wide%20range%20of%20classifiers%2C%20we%20discovered%2C%20statistically%2C%20a%20connection%20between%20decision%20boundary%20curvature%20and%20adversarial%20robustness.%20We%20also%20propose%20a%20new%20attack%20method%2C%20curvature%20dynamic%20black-box%20attack%20%28CDBA%29%20with%20improved%20performance%20using%20the%20estimated%20curvature.%0ALink%3A%20http%3A//arxiv.org/abs/2505.19194v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurvature%2520Dynamic%2520Black-box%2520Attack%253A%2520revisiting%2520adversarial%2520robustness%2520via%2520dynamic%2520curvature%2520estimation%26entry.906535625%3DPeiran%2520Sun%26entry.1292438233%3DAdversarial%2520attack%2520reveals%2520the%2520vulnerability%2520of%2520deep%2520learning%2520models.%2520It%2520is%2520assumed%2520that%2520high%2520curvature%2520may%2520give%2520rise%2520to%2520rough%2520decision%2520boundary%2520and%2520thus%2520result%2520in%2520less%2520robust%2520models.%2520However%252C%2520the%2520most%2520commonly%2520used%2520%255Ctextit%257Bcurvature%257D%2520is%2520the%2520curvature%2520of%2520loss%2520function%252C%2520scores%2520or%2520other%2520parameters%2520from%2520within%2520the%2520model%2520as%2520opposed%2520to%2520decision%2520boundary%2520curvature%252C%2520since%2520the%2520former%2520can%2520be%2520relatively%2520easily%2520formed%2520using%2520second%2520order%2520derivative.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520query-efficient%2520method%252C%2520dynamic%2520curvature%2520estimation%2520%2528DCE%2529%252C%2520to%2520estimate%2520the%2520decision%2520boundary%2520curvature%2520in%2520a%2520black-box%2520setting.%2520Our%2520approach%2520is%2520based%2520on%2520CGBA%252C%2520a%2520black-box%2520adversarial%2520attack.%2520By%2520performing%2520DCE%2520on%2520a%2520wide%2520range%2520of%2520classifiers%252C%2520we%2520discovered%252C%2520statistically%252C%2520a%2520connection%2520between%2520decision%2520boundary%2520curvature%2520and%2520adversarial%2520robustness.%2520We%2520also%2520propose%2520a%2520new%2520attack%2520method%252C%2520curvature%2520dynamic%2520black-box%2520attack%2520%2528CDBA%2529%2520with%2520improved%2520performance%2520using%2520the%2520estimated%2520curvature.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19194v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curvature%20Dynamic%20Black-box%20Attack%3A%20revisiting%20adversarial%20robustness%20via%20dynamic%20curvature%20estimation&entry.906535625=Peiran%20Sun&entry.1292438233=Adversarial%20attack%20reveals%20the%20vulnerability%20of%20deep%20learning%20models.%20It%20is%20assumed%20that%20high%20curvature%20may%20give%20rise%20to%20rough%20decision%20boundary%20and%20thus%20result%20in%20less%20robust%20models.%20However%2C%20the%20most%20commonly%20used%20%5Ctextit%7Bcurvature%7D%20is%20the%20curvature%20of%20loss%20function%2C%20scores%20or%20other%20parameters%20from%20within%20the%20model%20as%20opposed%20to%20decision%20boundary%20curvature%2C%20since%20the%20former%20can%20be%20relatively%20easily%20formed%20using%20second%20order%20derivative.%20In%20this%20paper%2C%20we%20propose%20a%20new%20query-efficient%20method%2C%20dynamic%20curvature%20estimation%20%28DCE%29%2C%20to%20estimate%20the%20decision%20boundary%20curvature%20in%20a%20black-box%20setting.%20Our%20approach%20is%20based%20on%20CGBA%2C%20a%20black-box%20adversarial%20attack.%20By%20performing%20DCE%20on%20a%20wide%20range%20of%20classifiers%2C%20we%20discovered%2C%20statistically%2C%20a%20connection%20between%20decision%20boundary%20curvature%20and%20adversarial%20robustness.%20We%20also%20propose%20a%20new%20attack%20method%2C%20curvature%20dynamic%20black-box%20attack%20%28CDBA%29%20with%20improved%20performance%20using%20the%20estimated%20curvature.&entry.1838667208=http%3A//arxiv.org/abs/2505.19194v3&entry.124074799=Read"},
{"title": "Group Relative Attention Guidance for Image Editing", "author": "Xuanpu Zhang and Xuesong Niu and Ruidong Chen and Dan Song and Jianhao Zeng and Penghui Du and Haoxiang Cao and Kai Wu and An-an Liu", "abstract": "Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.", "link": "http://arxiv.org/abs/2510.24657v2", "date": "2025-11-28", "relevancy": 2.2302, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5727}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5569}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group%20Relative%20Attention%20Guidance%20for%20Image%20Editing&body=Title%3A%20Group%20Relative%20Attention%20Guidance%20for%20Image%20Editing%0AAuthor%3A%20Xuanpu%20Zhang%20and%20Xuesong%20Niu%20and%20Ruidong%20Chen%20and%20Dan%20Song%20and%20Jianhao%20Zeng%20and%20Penghui%20Du%20and%20Haoxiang%20Cao%20and%20Kai%20Wu%20and%20An-an%20Liu%0AAbstract%3A%20Recently%2C%20image%20editing%20based%20on%20Diffusion-in-Transformer%20models%20has%20undergone%20rapid%20development.%20However%2C%20existing%20editing%20methods%20often%20lack%20effective%20control%20over%20the%20degree%20of%20editing%2C%20limiting%20their%20ability%20to%20achieve%20more%20customized%20results.%20To%20address%20this%20limitation%2C%20we%20investigate%20the%20MM-Attention%20mechanism%20within%20the%20DiT%20model%20and%20observe%20that%20the%20Query%20and%20Key%20tokens%20share%20a%20bias%20vector%20that%20is%20only%20layer-dependent.%20We%20interpret%20this%20bias%20as%20representing%20the%20model%27s%20inherent%20editing%20behavior%2C%20while%20the%20delta%20between%20each%20token%20and%20its%20corresponding%20bias%20encodes%20the%20content-specific%20editing%20signals.%20Based%20on%20this%20insight%2C%20we%20propose%20Group%20Relative%20Attention%20Guidance%2C%20a%20simple%20yet%20effective%20method%20that%20reweights%20the%20delta%20values%20of%20different%20tokens%20to%20modulate%20the%20focus%20of%20the%20model%20on%20the%20input%20image%20relative%20to%20the%20editing%20instruction%2C%20enabling%20continuous%20and%20fine-grained%20control%20over%20editing%20intensity%20without%20any%20tuning.%20Extensive%20experiments%20conducted%20on%20existing%20image%20editing%20frameworks%20demonstrate%20that%20GRAG%20can%20be%20integrated%20with%20as%20few%20as%20four%20lines%20of%20code%2C%20consistently%20enhancing%20editing%20quality.%20Moreover%2C%20compared%20to%20the%20commonly%20used%20Classifier-Free%20Guidance%2C%20GRAG%20achieves%20smoother%20and%20more%20precise%20control%20over%20the%20degree%20of%20editing.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/little-misfit/GRAG-Image-Editing.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup%2520Relative%2520Attention%2520Guidance%2520for%2520Image%2520Editing%26entry.906535625%3DXuanpu%2520Zhang%2520and%2520Xuesong%2520Niu%2520and%2520Ruidong%2520Chen%2520and%2520Dan%2520Song%2520and%2520Jianhao%2520Zeng%2520and%2520Penghui%2520Du%2520and%2520Haoxiang%2520Cao%2520and%2520Kai%2520Wu%2520and%2520An-an%2520Liu%26entry.1292438233%3DRecently%252C%2520image%2520editing%2520based%2520on%2520Diffusion-in-Transformer%2520models%2520has%2520undergone%2520rapid%2520development.%2520However%252C%2520existing%2520editing%2520methods%2520often%2520lack%2520effective%2520control%2520over%2520the%2520degree%2520of%2520editing%252C%2520limiting%2520their%2520ability%2520to%2520achieve%2520more%2520customized%2520results.%2520To%2520address%2520this%2520limitation%252C%2520we%2520investigate%2520the%2520MM-Attention%2520mechanism%2520within%2520the%2520DiT%2520model%2520and%2520observe%2520that%2520the%2520Query%2520and%2520Key%2520tokens%2520share%2520a%2520bias%2520vector%2520that%2520is%2520only%2520layer-dependent.%2520We%2520interpret%2520this%2520bias%2520as%2520representing%2520the%2520model%2527s%2520inherent%2520editing%2520behavior%252C%2520while%2520the%2520delta%2520between%2520each%2520token%2520and%2520its%2520corresponding%2520bias%2520encodes%2520the%2520content-specific%2520editing%2520signals.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520Group%2520Relative%2520Attention%2520Guidance%252C%2520a%2520simple%2520yet%2520effective%2520method%2520that%2520reweights%2520the%2520delta%2520values%2520of%2520different%2520tokens%2520to%2520modulate%2520the%2520focus%2520of%2520the%2520model%2520on%2520the%2520input%2520image%2520relative%2520to%2520the%2520editing%2520instruction%252C%2520enabling%2520continuous%2520and%2520fine-grained%2520control%2520over%2520editing%2520intensity%2520without%2520any%2520tuning.%2520Extensive%2520experiments%2520conducted%2520on%2520existing%2520image%2520editing%2520frameworks%2520demonstrate%2520that%2520GRAG%2520can%2520be%2520integrated%2520with%2520as%2520few%2520as%2520four%2520lines%2520of%2520code%252C%2520consistently%2520enhancing%2520editing%2520quality.%2520Moreover%252C%2520compared%2520to%2520the%2520commonly%2520used%2520Classifier-Free%2520Guidance%252C%2520GRAG%2520achieves%2520smoother%2520and%2520more%2520precise%2520control%2520over%2520the%2520degree%2520of%2520editing.%2520Our%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/little-misfit/GRAG-Image-Editing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group%20Relative%20Attention%20Guidance%20for%20Image%20Editing&entry.906535625=Xuanpu%20Zhang%20and%20Xuesong%20Niu%20and%20Ruidong%20Chen%20and%20Dan%20Song%20and%20Jianhao%20Zeng%20and%20Penghui%20Du%20and%20Haoxiang%20Cao%20and%20Kai%20Wu%20and%20An-an%20Liu&entry.1292438233=Recently%2C%20image%20editing%20based%20on%20Diffusion-in-Transformer%20models%20has%20undergone%20rapid%20development.%20However%2C%20existing%20editing%20methods%20often%20lack%20effective%20control%20over%20the%20degree%20of%20editing%2C%20limiting%20their%20ability%20to%20achieve%20more%20customized%20results.%20To%20address%20this%20limitation%2C%20we%20investigate%20the%20MM-Attention%20mechanism%20within%20the%20DiT%20model%20and%20observe%20that%20the%20Query%20and%20Key%20tokens%20share%20a%20bias%20vector%20that%20is%20only%20layer-dependent.%20We%20interpret%20this%20bias%20as%20representing%20the%20model%27s%20inherent%20editing%20behavior%2C%20while%20the%20delta%20between%20each%20token%20and%20its%20corresponding%20bias%20encodes%20the%20content-specific%20editing%20signals.%20Based%20on%20this%20insight%2C%20we%20propose%20Group%20Relative%20Attention%20Guidance%2C%20a%20simple%20yet%20effective%20method%20that%20reweights%20the%20delta%20values%20of%20different%20tokens%20to%20modulate%20the%20focus%20of%20the%20model%20on%20the%20input%20image%20relative%20to%20the%20editing%20instruction%2C%20enabling%20continuous%20and%20fine-grained%20control%20over%20editing%20intensity%20without%20any%20tuning.%20Extensive%20experiments%20conducted%20on%20existing%20image%20editing%20frameworks%20demonstrate%20that%20GRAG%20can%20be%20integrated%20with%20as%20few%20as%20four%20lines%20of%20code%2C%20consistently%20enhancing%20editing%20quality.%20Moreover%2C%20compared%20to%20the%20commonly%20used%20Classifier-Free%20Guidance%2C%20GRAG%20achieves%20smoother%20and%20more%20precise%20control%20over%20the%20degree%20of%20editing.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/little-misfit/GRAG-Image-Editing.&entry.1838667208=http%3A//arxiv.org/abs/2510.24657v2&entry.124074799=Read"},
{"title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space", "author": "Huijie Liu and Shuhao Cui and Haoxiang Cao and Shuai Ma and Kai Wu and Guoliang Kang", "abstract": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.", "link": "http://arxiv.org/abs/2511.10555v5", "date": "2025-11-28", "relevancy": 2.2257, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5817}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5468}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Style%20is%20Worth%20One%20Code%3A%20Unlocking%20Code-to-Style%20Image%20Generation%20with%20Discrete%20Style%20Space&body=Title%3A%20A%20Style%20is%20Worth%20One%20Code%3A%20Unlocking%20Code-to-Style%20Image%20Generation%20with%20Discrete%20Style%20Space%0AAuthor%3A%20Huijie%20Liu%20and%20Shuhao%20Cui%20and%20Haoxiang%20Cao%20and%20Shuai%20Ma%20and%20Kai%20Wu%20and%20Guoliang%20Kang%0AAbstract%3A%20Innovative%20visual%20stylization%20is%20a%20cornerstone%20of%20artistic%20creation%2C%20yet%20generating%20novel%20and%20consistent%20visual%20styles%20remains%20a%20significant%20challenge.%20Existing%20generative%20approaches%20typically%20rely%20on%20lengthy%20textual%20prompts%2C%20reference%20images%2C%20or%20parameter-efficient%20fine-tuning%20to%20guide%20style-aware%20image%20generation%2C%20but%20often%20struggle%20with%20style%20consistency%2C%20limited%20creativity%2C%20and%20complex%20style%20representations.%20In%20this%20paper%2C%20we%20affirm%20that%20a%20style%20is%20worth%20one%20numerical%20code%20by%20introducing%20the%20novel%20task%2C%20code-to-style%20image%20generation%2C%20which%20produces%20images%20with%20novel%2C%20consistent%20visual%20styles%20conditioned%20solely%20on%20a%20numerical%20style%20code.%20To%20date%2C%20this%20field%20has%20only%20been%20primarily%20explored%20by%20the%20industry%20%28e.g.%2C%20Midjourney%29%2C%20with%20no%20open-source%20research%20from%20the%20academic%20community.%20To%20fill%20this%20gap%2C%20we%20propose%20CoTyle%2C%20the%20first%20open-source%20method%20for%20this%20task.%20Specifically%2C%20we%20first%20train%20a%20discrete%20style%20codebook%20from%20a%20collection%20of%20images%20to%20extract%20style%20embeddings.%20These%20embeddings%20serve%20as%20conditions%20for%20a%20text-to-image%20diffusion%20model%20%28T2I-DM%29%20to%20generate%20stylistic%20images.%20Subsequently%2C%20we%20train%20an%20autoregressive%20style%20generator%20on%20the%20discrete%20style%20embeddings%20to%20model%20their%20distribution%2C%20allowing%20the%20synthesis%20of%20novel%20style%20embeddings.%20During%20inference%2C%20a%20numerical%20style%20code%20is%20mapped%20to%20a%20unique%20style%20embedding%20by%20the%20style%20generator%2C%20and%20this%20embedding%20guides%20the%20T2I-DM%20to%20generate%20images%20in%20the%20corresponding%20style.%20Unlike%20existing%20methods%2C%20our%20method%20offers%20unparalleled%20simplicity%20and%20diversity%2C%20unlocking%20a%20vast%20space%20of%20reproducible%20styles%20from%20minimal%20input.%20Extensive%20experiments%20validate%20that%20CoTyle%20effectively%20turns%20a%20numerical%20code%20into%20a%20style%20controller%2C%20demonstrating%20a%20style%20is%20worth%20one%20code.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10555v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Style%2520is%2520Worth%2520One%2520Code%253A%2520Unlocking%2520Code-to-Style%2520Image%2520Generation%2520with%2520Discrete%2520Style%2520Space%26entry.906535625%3DHuijie%2520Liu%2520and%2520Shuhao%2520Cui%2520and%2520Haoxiang%2520Cao%2520and%2520Shuai%2520Ma%2520and%2520Kai%2520Wu%2520and%2520Guoliang%2520Kang%26entry.1292438233%3DInnovative%2520visual%2520stylization%2520is%2520a%2520cornerstone%2520of%2520artistic%2520creation%252C%2520yet%2520generating%2520novel%2520and%2520consistent%2520visual%2520styles%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520generative%2520approaches%2520typically%2520rely%2520on%2520lengthy%2520textual%2520prompts%252C%2520reference%2520images%252C%2520or%2520parameter-efficient%2520fine-tuning%2520to%2520guide%2520style-aware%2520image%2520generation%252C%2520but%2520often%2520struggle%2520with%2520style%2520consistency%252C%2520limited%2520creativity%252C%2520and%2520complex%2520style%2520representations.%2520In%2520this%2520paper%252C%2520we%2520affirm%2520that%2520a%2520style%2520is%2520worth%2520one%2520numerical%2520code%2520by%2520introducing%2520the%2520novel%2520task%252C%2520code-to-style%2520image%2520generation%252C%2520which%2520produces%2520images%2520with%2520novel%252C%2520consistent%2520visual%2520styles%2520conditioned%2520solely%2520on%2520a%2520numerical%2520style%2520code.%2520To%2520date%252C%2520this%2520field%2520has%2520only%2520been%2520primarily%2520explored%2520by%2520the%2520industry%2520%2528e.g.%252C%2520Midjourney%2529%252C%2520with%2520no%2520open-source%2520research%2520from%2520the%2520academic%2520community.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520CoTyle%252C%2520the%2520first%2520open-source%2520method%2520for%2520this%2520task.%2520Specifically%252C%2520we%2520first%2520train%2520a%2520discrete%2520style%2520codebook%2520from%2520a%2520collection%2520of%2520images%2520to%2520extract%2520style%2520embeddings.%2520These%2520embeddings%2520serve%2520as%2520conditions%2520for%2520a%2520text-to-image%2520diffusion%2520model%2520%2528T2I-DM%2529%2520to%2520generate%2520stylistic%2520images.%2520Subsequently%252C%2520we%2520train%2520an%2520autoregressive%2520style%2520generator%2520on%2520the%2520discrete%2520style%2520embeddings%2520to%2520model%2520their%2520distribution%252C%2520allowing%2520the%2520synthesis%2520of%2520novel%2520style%2520embeddings.%2520During%2520inference%252C%2520a%2520numerical%2520style%2520code%2520is%2520mapped%2520to%2520a%2520unique%2520style%2520embedding%2520by%2520the%2520style%2520generator%252C%2520and%2520this%2520embedding%2520guides%2520the%2520T2I-DM%2520to%2520generate%2520images%2520in%2520the%2520corresponding%2520style.%2520Unlike%2520existing%2520methods%252C%2520our%2520method%2520offers%2520unparalleled%2520simplicity%2520and%2520diversity%252C%2520unlocking%2520a%2520vast%2520space%2520of%2520reproducible%2520styles%2520from%2520minimal%2520input.%2520Extensive%2520experiments%2520validate%2520that%2520CoTyle%2520effectively%2520turns%2520a%2520numerical%2520code%2520into%2520a%2520style%2520controller%252C%2520demonstrating%2520a%2520style%2520is%2520worth%2520one%2520code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10555v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Style%20is%20Worth%20One%20Code%3A%20Unlocking%20Code-to-Style%20Image%20Generation%20with%20Discrete%20Style%20Space&entry.906535625=Huijie%20Liu%20and%20Shuhao%20Cui%20and%20Haoxiang%20Cao%20and%20Shuai%20Ma%20and%20Kai%20Wu%20and%20Guoliang%20Kang&entry.1292438233=Innovative%20visual%20stylization%20is%20a%20cornerstone%20of%20artistic%20creation%2C%20yet%20generating%20novel%20and%20consistent%20visual%20styles%20remains%20a%20significant%20challenge.%20Existing%20generative%20approaches%20typically%20rely%20on%20lengthy%20textual%20prompts%2C%20reference%20images%2C%20or%20parameter-efficient%20fine-tuning%20to%20guide%20style-aware%20image%20generation%2C%20but%20often%20struggle%20with%20style%20consistency%2C%20limited%20creativity%2C%20and%20complex%20style%20representations.%20In%20this%20paper%2C%20we%20affirm%20that%20a%20style%20is%20worth%20one%20numerical%20code%20by%20introducing%20the%20novel%20task%2C%20code-to-style%20image%20generation%2C%20which%20produces%20images%20with%20novel%2C%20consistent%20visual%20styles%20conditioned%20solely%20on%20a%20numerical%20style%20code.%20To%20date%2C%20this%20field%20has%20only%20been%20primarily%20explored%20by%20the%20industry%20%28e.g.%2C%20Midjourney%29%2C%20with%20no%20open-source%20research%20from%20the%20academic%20community.%20To%20fill%20this%20gap%2C%20we%20propose%20CoTyle%2C%20the%20first%20open-source%20method%20for%20this%20task.%20Specifically%2C%20we%20first%20train%20a%20discrete%20style%20codebook%20from%20a%20collection%20of%20images%20to%20extract%20style%20embeddings.%20These%20embeddings%20serve%20as%20conditions%20for%20a%20text-to-image%20diffusion%20model%20%28T2I-DM%29%20to%20generate%20stylistic%20images.%20Subsequently%2C%20we%20train%20an%20autoregressive%20style%20generator%20on%20the%20discrete%20style%20embeddings%20to%20model%20their%20distribution%2C%20allowing%20the%20synthesis%20of%20novel%20style%20embeddings.%20During%20inference%2C%20a%20numerical%20style%20code%20is%20mapped%20to%20a%20unique%20style%20embedding%20by%20the%20style%20generator%2C%20and%20this%20embedding%20guides%20the%20T2I-DM%20to%20generate%20images%20in%20the%20corresponding%20style.%20Unlike%20existing%20methods%2C%20our%20method%20offers%20unparalleled%20simplicity%20and%20diversity%2C%20unlocking%20a%20vast%20space%20of%20reproducible%20styles%20from%20minimal%20input.%20Extensive%20experiments%20validate%20that%20CoTyle%20effectively%20turns%20a%20numerical%20code%20into%20a%20style%20controller%2C%20demonstrating%20a%20style%20is%20worth%20one%20code.&entry.1838667208=http%3A//arxiv.org/abs/2511.10555v5&entry.124074799=Read"},
{"title": "SimScale: Learning to Drive via Real-World Simulation at Scale", "author": "Haochen Tian and Tianyu Li and Haochen Liu and Jiazhi Yang and Yihang Qiu and Guang Li and Junli Wang and Yinfeng Gao and Zhang Zhang and Liang Wang and Hangjun Ye and Tieniu Tan and Long Chen and Hongyang Li", "abstract": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.", "link": "http://arxiv.org/abs/2511.23369v1", "date": "2025-11-28", "relevancy": 2.2176, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5728}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimScale%3A%20Learning%20to%20Drive%20via%20Real-World%20Simulation%20at%20Scale&body=Title%3A%20SimScale%3A%20Learning%20to%20Drive%20via%20Real-World%20Simulation%20at%20Scale%0AAuthor%3A%20Haochen%20Tian%20and%20Tianyu%20Li%20and%20Haochen%20Liu%20and%20Jiazhi%20Yang%20and%20Yihang%20Qiu%20and%20Guang%20Li%20and%20Junli%20Wang%20and%20Yinfeng%20Gao%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Hangjun%20Ye%20and%20Tieniu%20Tan%20and%20Long%20Chen%20and%20Hongyang%20Li%0AAbstract%3A%20Achieving%20fully%20autonomous%20driving%20systems%20requires%20learning%20rational%20decisions%20in%20a%20wide%20span%20of%20scenarios%2C%20including%20safety-critical%20and%20out-of-distribution%20ones.%20However%2C%20such%20cases%20are%20underrepresented%20in%20real-world%20corpus%20collected%20by%20human%20experts.%20To%20complement%20for%20the%20lack%20of%20data%20diversity%2C%20we%20introduce%20a%20novel%20and%20scalable%20simulation%20framework%20capable%20of%20synthesizing%20massive%20unseen%20states%20upon%20existing%20driving%20logs.%20Our%20pipeline%20utilizes%20advanced%20neural%20rendering%20with%20a%20reactive%20environment%20to%20generate%20high-fidelity%20multi-view%20observations%20controlled%20by%20the%20perturbed%20ego%20trajectory.%20Furthermore%2C%20we%20develop%20a%20pseudo-expert%20trajectory%20generation%20mechanism%20for%20these%20newly%20simulated%20states%20to%20provide%20action%20supervision.%20Upon%20the%20synthesized%20data%2C%20we%20find%20that%20a%20simple%20co-training%20strategy%20on%20both%20real-world%20and%20simulated%20samples%20can%20lead%20to%20significant%20improvements%20in%20both%20robustness%20and%20generalization%20for%20various%20planning%20methods%20on%20challenging%20real-world%20benchmarks%2C%20up%20to%20%2B6.8%20EPDMS%20on%20navhard%20and%20%2B2.9%20on%20navtest.%20More%20importantly%2C%20such%20policy%20improvement%20scales%20smoothly%20by%20increasing%20simulation%20data%20only%2C%20even%20without%20extra%20real-world%20data%20streaming%20in.%20We%20further%20reveal%20several%20crucial%20findings%20of%20such%20a%20sim-real%20learning%20system%2C%20which%20we%20term%20SimScale%2C%20including%20the%20design%20of%20pseudo-experts%20and%20the%20scaling%20properties%20for%20different%20policy%20architectures.%20Our%20simulation%20data%20and%20code%20would%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimScale%253A%2520Learning%2520to%2520Drive%2520via%2520Real-World%2520Simulation%2520at%2520Scale%26entry.906535625%3DHaochen%2520Tian%2520and%2520Tianyu%2520Li%2520and%2520Haochen%2520Liu%2520and%2520Jiazhi%2520Yang%2520and%2520Yihang%2520Qiu%2520and%2520Guang%2520Li%2520and%2520Junli%2520Wang%2520and%2520Yinfeng%2520Gao%2520and%2520Zhang%2520Zhang%2520and%2520Liang%2520Wang%2520and%2520Hangjun%2520Ye%2520and%2520Tieniu%2520Tan%2520and%2520Long%2520Chen%2520and%2520Hongyang%2520Li%26entry.1292438233%3DAchieving%2520fully%2520autonomous%2520driving%2520systems%2520requires%2520learning%2520rational%2520decisions%2520in%2520a%2520wide%2520span%2520of%2520scenarios%252C%2520including%2520safety-critical%2520and%2520out-of-distribution%2520ones.%2520However%252C%2520such%2520cases%2520are%2520underrepresented%2520in%2520real-world%2520corpus%2520collected%2520by%2520human%2520experts.%2520To%2520complement%2520for%2520the%2520lack%2520of%2520data%2520diversity%252C%2520we%2520introduce%2520a%2520novel%2520and%2520scalable%2520simulation%2520framework%2520capable%2520of%2520synthesizing%2520massive%2520unseen%2520states%2520upon%2520existing%2520driving%2520logs.%2520Our%2520pipeline%2520utilizes%2520advanced%2520neural%2520rendering%2520with%2520a%2520reactive%2520environment%2520to%2520generate%2520high-fidelity%2520multi-view%2520observations%2520controlled%2520by%2520the%2520perturbed%2520ego%2520trajectory.%2520Furthermore%252C%2520we%2520develop%2520a%2520pseudo-expert%2520trajectory%2520generation%2520mechanism%2520for%2520these%2520newly%2520simulated%2520states%2520to%2520provide%2520action%2520supervision.%2520Upon%2520the%2520synthesized%2520data%252C%2520we%2520find%2520that%2520a%2520simple%2520co-training%2520strategy%2520on%2520both%2520real-world%2520and%2520simulated%2520samples%2520can%2520lead%2520to%2520significant%2520improvements%2520in%2520both%2520robustness%2520and%2520generalization%2520for%2520various%2520planning%2520methods%2520on%2520challenging%2520real-world%2520benchmarks%252C%2520up%2520to%2520%252B6.8%2520EPDMS%2520on%2520navhard%2520and%2520%252B2.9%2520on%2520navtest.%2520More%2520importantly%252C%2520such%2520policy%2520improvement%2520scales%2520smoothly%2520by%2520increasing%2520simulation%2520data%2520only%252C%2520even%2520without%2520extra%2520real-world%2520data%2520streaming%2520in.%2520We%2520further%2520reveal%2520several%2520crucial%2520findings%2520of%2520such%2520a%2520sim-real%2520learning%2520system%252C%2520which%2520we%2520term%2520SimScale%252C%2520including%2520the%2520design%2520of%2520pseudo-experts%2520and%2520the%2520scaling%2520properties%2520for%2520different%2520policy%2520architectures.%2520Our%2520simulation%2520data%2520and%2520code%2520would%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimScale%3A%20Learning%20to%20Drive%20via%20Real-World%20Simulation%20at%20Scale&entry.906535625=Haochen%20Tian%20and%20Tianyu%20Li%20and%20Haochen%20Liu%20and%20Jiazhi%20Yang%20and%20Yihang%20Qiu%20and%20Guang%20Li%20and%20Junli%20Wang%20and%20Yinfeng%20Gao%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Hangjun%20Ye%20and%20Tieniu%20Tan%20and%20Long%20Chen%20and%20Hongyang%20Li&entry.1292438233=Achieving%20fully%20autonomous%20driving%20systems%20requires%20learning%20rational%20decisions%20in%20a%20wide%20span%20of%20scenarios%2C%20including%20safety-critical%20and%20out-of-distribution%20ones.%20However%2C%20such%20cases%20are%20underrepresented%20in%20real-world%20corpus%20collected%20by%20human%20experts.%20To%20complement%20for%20the%20lack%20of%20data%20diversity%2C%20we%20introduce%20a%20novel%20and%20scalable%20simulation%20framework%20capable%20of%20synthesizing%20massive%20unseen%20states%20upon%20existing%20driving%20logs.%20Our%20pipeline%20utilizes%20advanced%20neural%20rendering%20with%20a%20reactive%20environment%20to%20generate%20high-fidelity%20multi-view%20observations%20controlled%20by%20the%20perturbed%20ego%20trajectory.%20Furthermore%2C%20we%20develop%20a%20pseudo-expert%20trajectory%20generation%20mechanism%20for%20these%20newly%20simulated%20states%20to%20provide%20action%20supervision.%20Upon%20the%20synthesized%20data%2C%20we%20find%20that%20a%20simple%20co-training%20strategy%20on%20both%20real-world%20and%20simulated%20samples%20can%20lead%20to%20significant%20improvements%20in%20both%20robustness%20and%20generalization%20for%20various%20planning%20methods%20on%20challenging%20real-world%20benchmarks%2C%20up%20to%20%2B6.8%20EPDMS%20on%20navhard%20and%20%2B2.9%20on%20navtest.%20More%20importantly%2C%20such%20policy%20improvement%20scales%20smoothly%20by%20increasing%20simulation%20data%20only%2C%20even%20without%20extra%20real-world%20data%20streaming%20in.%20We%20further%20reveal%20several%20crucial%20findings%20of%20such%20a%20sim-real%20learning%20system%2C%20which%20we%20term%20SimScale%2C%20including%20the%20design%20of%20pseudo-experts%20and%20the%20scaling%20properties%20for%20different%20policy%20architectures.%20Our%20simulation%20data%20and%20code%20would%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.23369v1&entry.124074799=Read"},
{"title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy", "author": "Teng Hu and Zhentao Yu and Guozhen Zhang and Zihan Su and Zhengguang Zhou and Youliang Zhang and Yuan Zhou and Qinglin Lu and Ran Yi", "abstract": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.", "link": "http://arxiv.org/abs/2511.21579v2", "date": "2025-11-28", "relevancy": 2.2053, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5696}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmony%3A%20Harmonizing%20Audio%20and%20Video%20Generation%20through%20Cross-Task%20Synergy&body=Title%3A%20Harmony%3A%20Harmonizing%20Audio%20and%20Video%20Generation%20through%20Cross-Task%20Synergy%0AAuthor%3A%20Teng%20Hu%20and%20Zhentao%20Yu%20and%20Guozhen%20Zhang%20and%20Zihan%20Su%20and%20Zhengguang%20Zhou%20and%20Youliang%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Ran%20Yi%0AAbstract%3A%20The%20synthesis%20of%20synchronized%20audio-visual%20content%20is%20a%20key%20challenge%20in%20generative%20AI%2C%20with%20open-source%20models%20facing%20challenges%20in%20robust%20audio-video%20alignment.%20Our%20analysis%20reveals%20that%20this%20issue%20is%20rooted%20in%20three%20fundamental%20challenges%20of%20the%20joint%20diffusion%20process%3A%20%281%29%20Correspondence%20Drift%2C%20where%20concurrently%20evolving%20noisy%20latents%20impede%20stable%20learning%20of%20alignment%3B%20%282%29%20inefficient%20global%20attention%20mechanisms%20that%20fail%20to%20capture%20fine-grained%20temporal%20cues%3B%20and%20%283%29%20the%20intra-modal%20bias%20of%20conventional%20Classifier-Free%20Guidance%20%28CFG%29%2C%20which%20enhances%20conditionality%20but%20not%20cross-modal%20synchronization.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20Harmony%2C%20a%20novel%20framework%20that%20mechanistically%20enforces%20audio-visual%20synchronization.%20We%20first%20propose%20a%20Cross-Task%20Synergy%20training%20paradigm%20to%20mitigate%20drift%20by%20leveraging%20strong%20supervisory%20signals%20from%20audio-driven%20video%20and%20video-driven%20audio%20generation%20tasks.%20Then%2C%20we%20design%20a%20Global-Local%20Decoupled%20Interaction%20Module%20for%20efficient%20and%20precise%20temporal-style%20alignment.%20Finally%2C%20we%20present%20a%20novel%20Synchronization-Enhanced%20CFG%20%28SyncCFG%29%20that%20explicitly%20isolates%20and%20amplifies%20the%20alignment%20signal%20during%20inference.%20Extensive%20experiments%20demonstrate%20that%20Harmony%20establishes%20a%20new%20state-of-the-art%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20generation%20fidelity%20and%2C%20critically%2C%20in%20achieving%20fine-grained%20audio-visual%20synchronization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmony%253A%2520Harmonizing%2520Audio%2520and%2520Video%2520Generation%2520through%2520Cross-Task%2520Synergy%26entry.906535625%3DTeng%2520Hu%2520and%2520Zhentao%2520Yu%2520and%2520Guozhen%2520Zhang%2520and%2520Zihan%2520Su%2520and%2520Zhengguang%2520Zhou%2520and%2520Youliang%2520Zhang%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%2520and%2520Ran%2520Yi%26entry.1292438233%3DThe%2520synthesis%2520of%2520synchronized%2520audio-visual%2520content%2520is%2520a%2520key%2520challenge%2520in%2520generative%2520AI%252C%2520with%2520open-source%2520models%2520facing%2520challenges%2520in%2520robust%2520audio-video%2520alignment.%2520Our%2520analysis%2520reveals%2520that%2520this%2520issue%2520is%2520rooted%2520in%2520three%2520fundamental%2520challenges%2520of%2520the%2520joint%2520diffusion%2520process%253A%2520%25281%2529%2520Correspondence%2520Drift%252C%2520where%2520concurrently%2520evolving%2520noisy%2520latents%2520impede%2520stable%2520learning%2520of%2520alignment%253B%2520%25282%2529%2520inefficient%2520global%2520attention%2520mechanisms%2520that%2520fail%2520to%2520capture%2520fine-grained%2520temporal%2520cues%253B%2520and%2520%25283%2529%2520the%2520intra-modal%2520bias%2520of%2520conventional%2520Classifier-Free%2520Guidance%2520%2528CFG%2529%252C%2520which%2520enhances%2520conditionality%2520but%2520not%2520cross-modal%2520synchronization.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520Harmony%252C%2520a%2520novel%2520framework%2520that%2520mechanistically%2520enforces%2520audio-visual%2520synchronization.%2520We%2520first%2520propose%2520a%2520Cross-Task%2520Synergy%2520training%2520paradigm%2520to%2520mitigate%2520drift%2520by%2520leveraging%2520strong%2520supervisory%2520signals%2520from%2520audio-driven%2520video%2520and%2520video-driven%2520audio%2520generation%2520tasks.%2520Then%252C%2520we%2520design%2520a%2520Global-Local%2520Decoupled%2520Interaction%2520Module%2520for%2520efficient%2520and%2520precise%2520temporal-style%2520alignment.%2520Finally%252C%2520we%2520present%2520a%2520novel%2520Synchronization-Enhanced%2520CFG%2520%2528SyncCFG%2529%2520that%2520explicitly%2520isolates%2520and%2520amplifies%2520the%2520alignment%2520signal%2520during%2520inference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Harmony%2520establishes%2520a%2520new%2520state-of-the-art%252C%2520significantly%2520outperforming%2520existing%2520methods%2520in%2520both%2520generation%2520fidelity%2520and%252C%2520critically%252C%2520in%2520achieving%2520fine-grained%2520audio-visual%2520synchronization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmony%3A%20Harmonizing%20Audio%20and%20Video%20Generation%20through%20Cross-Task%20Synergy&entry.906535625=Teng%20Hu%20and%20Zhentao%20Yu%20and%20Guozhen%20Zhang%20and%20Zihan%20Su%20and%20Zhengguang%20Zhou%20and%20Youliang%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Ran%20Yi&entry.1292438233=The%20synthesis%20of%20synchronized%20audio-visual%20content%20is%20a%20key%20challenge%20in%20generative%20AI%2C%20with%20open-source%20models%20facing%20challenges%20in%20robust%20audio-video%20alignment.%20Our%20analysis%20reveals%20that%20this%20issue%20is%20rooted%20in%20three%20fundamental%20challenges%20of%20the%20joint%20diffusion%20process%3A%20%281%29%20Correspondence%20Drift%2C%20where%20concurrently%20evolving%20noisy%20latents%20impede%20stable%20learning%20of%20alignment%3B%20%282%29%20inefficient%20global%20attention%20mechanisms%20that%20fail%20to%20capture%20fine-grained%20temporal%20cues%3B%20and%20%283%29%20the%20intra-modal%20bias%20of%20conventional%20Classifier-Free%20Guidance%20%28CFG%29%2C%20which%20enhances%20conditionality%20but%20not%20cross-modal%20synchronization.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20Harmony%2C%20a%20novel%20framework%20that%20mechanistically%20enforces%20audio-visual%20synchronization.%20We%20first%20propose%20a%20Cross-Task%20Synergy%20training%20paradigm%20to%20mitigate%20drift%20by%20leveraging%20strong%20supervisory%20signals%20from%20audio-driven%20video%20and%20video-driven%20audio%20generation%20tasks.%20Then%2C%20we%20design%20a%20Global-Local%20Decoupled%20Interaction%20Module%20for%20efficient%20and%20precise%20temporal-style%20alignment.%20Finally%2C%20we%20present%20a%20novel%20Synchronization-Enhanced%20CFG%20%28SyncCFG%29%20that%20explicitly%20isolates%20and%20amplifies%20the%20alignment%20signal%20during%20inference.%20Extensive%20experiments%20demonstrate%20that%20Harmony%20establishes%20a%20new%20state-of-the-art%2C%20significantly%20outperforming%20existing%20methods%20in%20both%20generation%20fidelity%20and%2C%20critically%2C%20in%20achieving%20fine-grained%20audio-visual%20synchronization.&entry.1838667208=http%3A//arxiv.org/abs/2511.21579v2&entry.124074799=Read"},
{"title": "Predicting Video Slot Attention Queries from Random Slot-Feature Pairs", "author": "Rongzhen Zhao and Jian Li and Juho Kannala and Joni Pajarinen", "abstract": "Unsupervised video Object-Centric Learning (OCL) is promising as it enables object-level scene representation and understanding as we humans do. Mainstream video OCL methods adopt a recurrent architecture: An aggregator aggregates current video frame into object features, termed slots, under some queries; A transitioner transits current slots to queries for the next frame. This is an effective architecture but all existing implementations both (\\textit{i1}) neglect to incorporate next frame features, the most informative source for query prediction, and (\\textit{i2}) fail to learn transition dynamics, the knowledge essential for query prediction. To address these issues, we propose Random Slot-Feature pair for learning Query prediction (RandSF.Q): (\\textit{t1}) We design a new transitioner to incorporate both slots and features, which provides more information for query prediction; (\\textit{t2}) We train the transitioner to predict queries from slot-feature pairs randomly sampled from available recurrences, which drives it to learn transition dynamics. Experiments on scene representation demonstrate that our method surpass existing video OCL methods significantly, e.g., up to 10 points on object discovery, setting new state-of-the-art. Such superiority also benefits downstream tasks like scene understanding. Source Code, Model Checkpoints, Training Logs: https://github.com/Genera1Z/RandSF.Q", "link": "http://arxiv.org/abs/2508.01345v5", "date": "2025-11-28", "relevancy": 2.1939, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Video%20Slot%20Attention%20Queries%20from%20Random%20Slot-Feature%20Pairs&body=Title%3A%20Predicting%20Video%20Slot%20Attention%20Queries%20from%20Random%20Slot-Feature%20Pairs%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Jian%20Li%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20Unsupervised%20video%20Object-Centric%20Learning%20%28OCL%29%20is%20promising%20as%20it%20enables%20object-level%20scene%20representation%20and%20understanding%20as%20we%20humans%20do.%20Mainstream%20video%20OCL%20methods%20adopt%20a%20recurrent%20architecture%3A%20An%20aggregator%20aggregates%20current%20video%20frame%20into%20object%20features%2C%20termed%20slots%2C%20under%20some%20queries%3B%20A%20transitioner%20transits%20current%20slots%20to%20queries%20for%20the%20next%20frame.%20This%20is%20an%20effective%20architecture%20but%20all%20existing%20implementations%20both%20%28%5Ctextit%7Bi1%7D%29%20neglect%20to%20incorporate%20next%20frame%20features%2C%20the%20most%20informative%20source%20for%20query%20prediction%2C%20and%20%28%5Ctextit%7Bi2%7D%29%20fail%20to%20learn%20transition%20dynamics%2C%20the%20knowledge%20essential%20for%20query%20prediction.%20To%20address%20these%20issues%2C%20we%20propose%20Random%20Slot-Feature%20pair%20for%20learning%20Query%20prediction%20%28RandSF.Q%29%3A%20%28%5Ctextit%7Bt1%7D%29%20We%20design%20a%20new%20transitioner%20to%20incorporate%20both%20slots%20and%20features%2C%20which%20provides%20more%20information%20for%20query%20prediction%3B%20%28%5Ctextit%7Bt2%7D%29%20We%20train%20the%20transitioner%20to%20predict%20queries%20from%20slot-feature%20pairs%20randomly%20sampled%20from%20available%20recurrences%2C%20which%20drives%20it%20to%20learn%20transition%20dynamics.%20Experiments%20on%20scene%20representation%20demonstrate%20that%20our%20method%20surpass%20existing%20video%20OCL%20methods%20significantly%2C%20e.g.%2C%20up%20to%2010%20points%20on%20object%20discovery%2C%20setting%20new%20state-of-the-art.%20Such%20superiority%20also%20benefits%20downstream%20tasks%20like%20scene%20understanding.%20Source%20Code%2C%20Model%20Checkpoints%2C%20Training%20Logs%3A%20https%3A//github.com/Genera1Z/RandSF.Q%0ALink%3A%20http%3A//arxiv.org/abs/2508.01345v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Video%2520Slot%2520Attention%2520Queries%2520from%2520Random%2520Slot-Feature%2520Pairs%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Jian%2520Li%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3DUnsupervised%2520video%2520Object-Centric%2520Learning%2520%2528OCL%2529%2520is%2520promising%2520as%2520it%2520enables%2520object-level%2520scene%2520representation%2520and%2520understanding%2520as%2520we%2520humans%2520do.%2520Mainstream%2520video%2520OCL%2520methods%2520adopt%2520a%2520recurrent%2520architecture%253A%2520An%2520aggregator%2520aggregates%2520current%2520video%2520frame%2520into%2520object%2520features%252C%2520termed%2520slots%252C%2520under%2520some%2520queries%253B%2520A%2520transitioner%2520transits%2520current%2520slots%2520to%2520queries%2520for%2520the%2520next%2520frame.%2520This%2520is%2520an%2520effective%2520architecture%2520but%2520all%2520existing%2520implementations%2520both%2520%2528%255Ctextit%257Bi1%257D%2529%2520neglect%2520to%2520incorporate%2520next%2520frame%2520features%252C%2520the%2520most%2520informative%2520source%2520for%2520query%2520prediction%252C%2520and%2520%2528%255Ctextit%257Bi2%257D%2529%2520fail%2520to%2520learn%2520transition%2520dynamics%252C%2520the%2520knowledge%2520essential%2520for%2520query%2520prediction.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Random%2520Slot-Feature%2520pair%2520for%2520learning%2520Query%2520prediction%2520%2528RandSF.Q%2529%253A%2520%2528%255Ctextit%257Bt1%257D%2529%2520We%2520design%2520a%2520new%2520transitioner%2520to%2520incorporate%2520both%2520slots%2520and%2520features%252C%2520which%2520provides%2520more%2520information%2520for%2520query%2520prediction%253B%2520%2528%255Ctextit%257Bt2%257D%2529%2520We%2520train%2520the%2520transitioner%2520to%2520predict%2520queries%2520from%2520slot-feature%2520pairs%2520randomly%2520sampled%2520from%2520available%2520recurrences%252C%2520which%2520drives%2520it%2520to%2520learn%2520transition%2520dynamics.%2520Experiments%2520on%2520scene%2520representation%2520demonstrate%2520that%2520our%2520method%2520surpass%2520existing%2520video%2520OCL%2520methods%2520significantly%252C%2520e.g.%252C%2520up%2520to%252010%2520points%2520on%2520object%2520discovery%252C%2520setting%2520new%2520state-of-the-art.%2520Such%2520superiority%2520also%2520benefits%2520downstream%2520tasks%2520like%2520scene%2520understanding.%2520Source%2520Code%252C%2520Model%2520Checkpoints%252C%2520Training%2520Logs%253A%2520https%253A//github.com/Genera1Z/RandSF.Q%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01345v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Video%20Slot%20Attention%20Queries%20from%20Random%20Slot-Feature%20Pairs&entry.906535625=Rongzhen%20Zhao%20and%20Jian%20Li%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=Unsupervised%20video%20Object-Centric%20Learning%20%28OCL%29%20is%20promising%20as%20it%20enables%20object-level%20scene%20representation%20and%20understanding%20as%20we%20humans%20do.%20Mainstream%20video%20OCL%20methods%20adopt%20a%20recurrent%20architecture%3A%20An%20aggregator%20aggregates%20current%20video%20frame%20into%20object%20features%2C%20termed%20slots%2C%20under%20some%20queries%3B%20A%20transitioner%20transits%20current%20slots%20to%20queries%20for%20the%20next%20frame.%20This%20is%20an%20effective%20architecture%20but%20all%20existing%20implementations%20both%20%28%5Ctextit%7Bi1%7D%29%20neglect%20to%20incorporate%20next%20frame%20features%2C%20the%20most%20informative%20source%20for%20query%20prediction%2C%20and%20%28%5Ctextit%7Bi2%7D%29%20fail%20to%20learn%20transition%20dynamics%2C%20the%20knowledge%20essential%20for%20query%20prediction.%20To%20address%20these%20issues%2C%20we%20propose%20Random%20Slot-Feature%20pair%20for%20learning%20Query%20prediction%20%28RandSF.Q%29%3A%20%28%5Ctextit%7Bt1%7D%29%20We%20design%20a%20new%20transitioner%20to%20incorporate%20both%20slots%20and%20features%2C%20which%20provides%20more%20information%20for%20query%20prediction%3B%20%28%5Ctextit%7Bt2%7D%29%20We%20train%20the%20transitioner%20to%20predict%20queries%20from%20slot-feature%20pairs%20randomly%20sampled%20from%20available%20recurrences%2C%20which%20drives%20it%20to%20learn%20transition%20dynamics.%20Experiments%20on%20scene%20representation%20demonstrate%20that%20our%20method%20surpass%20existing%20video%20OCL%20methods%20significantly%2C%20e.g.%2C%20up%20to%2010%20points%20on%20object%20discovery%2C%20setting%20new%20state-of-the-art.%20Such%20superiority%20also%20benefits%20downstream%20tasks%20like%20scene%20understanding.%20Source%20Code%2C%20Model%20Checkpoints%2C%20Training%20Logs%3A%20https%3A//github.com/Genera1Z/RandSF.Q&entry.1838667208=http%3A//arxiv.org/abs/2508.01345v5&entry.124074799=Read"},
{"title": "Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day", "author": "Milad Abdollahzadeh and Abdul Raheem and Zilong Zhao and Uzair Javaid and Kevin Yee and Nalam Venkata Abhishek and Tram Truong-Huu and Biplab Sikdar", "abstract": "Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.", "link": "http://arxiv.org/abs/2511.23220v1", "date": "2025-11-28", "relevancy": 2.1931, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4422}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4392}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction%20Tuning%20of%20Large%20Language%20Models%20for%20Tabular%20Data%20Generation-in%20One%20Day&body=Title%3A%20Instruction%20Tuning%20of%20Large%20Language%20Models%20for%20Tabular%20Data%20Generation-in%20One%20Day%0AAuthor%3A%20Milad%20Abdollahzadeh%20and%20Abdul%20Raheem%20and%20Zilong%20Zhao%20and%20Uzair%20Javaid%20and%20Kevin%20Yee%20and%20Nalam%20Venkata%20Abhishek%20and%20Tram%20Truong-Huu%20and%20Biplab%20Sikdar%0AAbstract%3A%20Tabular%20instruction%20tuning%20has%20emerged%20as%20a%20promising%20research%20direction%20for%20improving%20LLMs%20understanding%20of%20tabular%20data.%20However%2C%20the%20majority%20of%20existing%20works%20only%20consider%20question-answering%20and%20reasoning%20tasks%20over%20tabular%20data%2C%20leaving%20tabular%20data%20generation%20largely%20unnoticed.%20In%20this%20work%2C%20for%20the%20first%20time%2C%20we%20explore%20the%20efficacy%20of%20instruction%20tuning%20in%20improving%20LLMs%20tabular%20data%20generation%20capabilities.%20More%20specifically%2C%20given%20the%20high%20data%20and%20computation%20requirements%20of%20tabular%20instruction%20tuning%2C%20we%20aim%20to%20address%20the%20possibility%20of%20instruction%20tuning%20for%20tabular%20data%20generation%20with%20limited%20data%20and%20computational%20resources.%20To%20achieve%20this%2C%20we%20first%20create%20a%20high-quality%20instruction%20dataset%20for%20tabular%20data%2C%20enabling%20efficient%20LLM%20comprehension.%20We%20then%20instruction-tune%20an%20open-source%20LLM%20%28Llama3.1-8B-Instruct%29%20on%20the%20training%20set%20of%20this%20dataset%20to%20improve%20its%20tabular%20data%20generation%20performance.%20Our%20experimental%20results%20show%20that%20by%20using%20our%20high-quality%20dataset%20and%20instruction-tuning%20on%20only%207K%20instructions%20with%20an%20A100%20GPU%2C%20for%20less%20than%206%20hours%2C%20we%20achieve%20tabular%20data%20generation%20performance%20on%20par%20with%20the%20most%20capable%20commercial%20LLM%2C%20GPT-4o.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction%2520Tuning%2520of%2520Large%2520Language%2520Models%2520for%2520Tabular%2520Data%2520Generation-in%2520One%2520Day%26entry.906535625%3DMilad%2520Abdollahzadeh%2520and%2520Abdul%2520Raheem%2520and%2520Zilong%2520Zhao%2520and%2520Uzair%2520Javaid%2520and%2520Kevin%2520Yee%2520and%2520Nalam%2520Venkata%2520Abhishek%2520and%2520Tram%2520Truong-Huu%2520and%2520Biplab%2520Sikdar%26entry.1292438233%3DTabular%2520instruction%2520tuning%2520has%2520emerged%2520as%2520a%2520promising%2520research%2520direction%2520for%2520improving%2520LLMs%2520understanding%2520of%2520tabular%2520data.%2520However%252C%2520the%2520majority%2520of%2520existing%2520works%2520only%2520consider%2520question-answering%2520and%2520reasoning%2520tasks%2520over%2520tabular%2520data%252C%2520leaving%2520tabular%2520data%2520generation%2520largely%2520unnoticed.%2520In%2520this%2520work%252C%2520for%2520the%2520first%2520time%252C%2520we%2520explore%2520the%2520efficacy%2520of%2520instruction%2520tuning%2520in%2520improving%2520LLMs%2520tabular%2520data%2520generation%2520capabilities.%2520More%2520specifically%252C%2520given%2520the%2520high%2520data%2520and%2520computation%2520requirements%2520of%2520tabular%2520instruction%2520tuning%252C%2520we%2520aim%2520to%2520address%2520the%2520possibility%2520of%2520instruction%2520tuning%2520for%2520tabular%2520data%2520generation%2520with%2520limited%2520data%2520and%2520computational%2520resources.%2520To%2520achieve%2520this%252C%2520we%2520first%2520create%2520a%2520high-quality%2520instruction%2520dataset%2520for%2520tabular%2520data%252C%2520enabling%2520efficient%2520LLM%2520comprehension.%2520We%2520then%2520instruction-tune%2520an%2520open-source%2520LLM%2520%2528Llama3.1-8B-Instruct%2529%2520on%2520the%2520training%2520set%2520of%2520this%2520dataset%2520to%2520improve%2520its%2520tabular%2520data%2520generation%2520performance.%2520Our%2520experimental%2520results%2520show%2520that%2520by%2520using%2520our%2520high-quality%2520dataset%2520and%2520instruction-tuning%2520on%2520only%25207K%2520instructions%2520with%2520an%2520A100%2520GPU%252C%2520for%2520less%2520than%25206%2520hours%252C%2520we%2520achieve%2520tabular%2520data%2520generation%2520performance%2520on%2520par%2520with%2520the%2520most%2520capable%2520commercial%2520LLM%252C%2520GPT-4o.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction%20Tuning%20of%20Large%20Language%20Models%20for%20Tabular%20Data%20Generation-in%20One%20Day&entry.906535625=Milad%20Abdollahzadeh%20and%20Abdul%20Raheem%20and%20Zilong%20Zhao%20and%20Uzair%20Javaid%20and%20Kevin%20Yee%20and%20Nalam%20Venkata%20Abhishek%20and%20Tram%20Truong-Huu%20and%20Biplab%20Sikdar&entry.1292438233=Tabular%20instruction%20tuning%20has%20emerged%20as%20a%20promising%20research%20direction%20for%20improving%20LLMs%20understanding%20of%20tabular%20data.%20However%2C%20the%20majority%20of%20existing%20works%20only%20consider%20question-answering%20and%20reasoning%20tasks%20over%20tabular%20data%2C%20leaving%20tabular%20data%20generation%20largely%20unnoticed.%20In%20this%20work%2C%20for%20the%20first%20time%2C%20we%20explore%20the%20efficacy%20of%20instruction%20tuning%20in%20improving%20LLMs%20tabular%20data%20generation%20capabilities.%20More%20specifically%2C%20given%20the%20high%20data%20and%20computation%20requirements%20of%20tabular%20instruction%20tuning%2C%20we%20aim%20to%20address%20the%20possibility%20of%20instruction%20tuning%20for%20tabular%20data%20generation%20with%20limited%20data%20and%20computational%20resources.%20To%20achieve%20this%2C%20we%20first%20create%20a%20high-quality%20instruction%20dataset%20for%20tabular%20data%2C%20enabling%20efficient%20LLM%20comprehension.%20We%20then%20instruction-tune%20an%20open-source%20LLM%20%28Llama3.1-8B-Instruct%29%20on%20the%20training%20set%20of%20this%20dataset%20to%20improve%20its%20tabular%20data%20generation%20performance.%20Our%20experimental%20results%20show%20that%20by%20using%20our%20high-quality%20dataset%20and%20instruction-tuning%20on%20only%207K%20instructions%20with%20an%20A100%20GPU%2C%20for%20less%20than%206%20hours%2C%20we%20achieve%20tabular%20data%20generation%20performance%20on%20par%20with%20the%20most%20capable%20commercial%20LLM%2C%20GPT-4o.&entry.1838667208=http%3A//arxiv.org/abs/2511.23220v1&entry.124074799=Read"},
{"title": "Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory", "author": "Akira Tamamori", "abstract": "High-capacity kernel Hopfield networks exhibit a \"Ridge of Optimization\" characterized by extreme stability. While previously linked to \"Spectral Concentration,\" its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the \"Edge of Stability,\" a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \\textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.", "link": "http://arxiv.org/abs/2511.23083v1", "date": "2025-11-28", "relevancy": 2.1921, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4448}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4385}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Concentration%20at%20the%20Edge%20of%20Stability%3A%20Information%20Geometry%20of%20Kernel%20Associative%20Memory&body=Title%3A%20Spectral%20Concentration%20at%20the%20Edge%20of%20Stability%3A%20Information%20Geometry%20of%20Kernel%20Associative%20Memory%0AAuthor%3A%20Akira%20Tamamori%0AAbstract%3A%20High-capacity%20kernel%20Hopfield%20networks%20exhibit%20a%20%22Ridge%20of%20Optimization%22%20characterized%20by%20extreme%20stability.%20While%20previously%20linked%20to%20%22Spectral%20Concentration%2C%22%20its%20origin%20remains%20elusive.%20Here%2C%20we%20analyze%20the%20network%20dynamics%20on%20a%20statistical%20manifold%2C%20revealing%20that%20the%20Ridge%20corresponds%20to%20the%20%22Edge%20of%20Stability%2C%22%20a%20critical%20boundary%20where%20the%20Fisher%20Information%20Matrix%20becomes%20singular.%20We%20demonstrate%20that%20the%20apparent%20Euclidean%20force%20antagonism%20is%20a%20manifestation%20of%20%5Ctextit%7BDual%20Equilibrium%7D%20in%20the%20Riemannian%20space.%20This%20unifies%20learning%20dynamics%20and%20capacity%20via%20the%20Minimum%20Description%20Length%20principle%2C%20offering%20a%20geometric%20theory%20of%20self-organized%20criticality.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Concentration%2520at%2520the%2520Edge%2520of%2520Stability%253A%2520Information%2520Geometry%2520of%2520Kernel%2520Associative%2520Memory%26entry.906535625%3DAkira%2520Tamamori%26entry.1292438233%3DHigh-capacity%2520kernel%2520Hopfield%2520networks%2520exhibit%2520a%2520%2522Ridge%2520of%2520Optimization%2522%2520characterized%2520by%2520extreme%2520stability.%2520While%2520previously%2520linked%2520to%2520%2522Spectral%2520Concentration%252C%2522%2520its%2520origin%2520remains%2520elusive.%2520Here%252C%2520we%2520analyze%2520the%2520network%2520dynamics%2520on%2520a%2520statistical%2520manifold%252C%2520revealing%2520that%2520the%2520Ridge%2520corresponds%2520to%2520the%2520%2522Edge%2520of%2520Stability%252C%2522%2520a%2520critical%2520boundary%2520where%2520the%2520Fisher%2520Information%2520Matrix%2520becomes%2520singular.%2520We%2520demonstrate%2520that%2520the%2520apparent%2520Euclidean%2520force%2520antagonism%2520is%2520a%2520manifestation%2520of%2520%255Ctextit%257BDual%2520Equilibrium%257D%2520in%2520the%2520Riemannian%2520space.%2520This%2520unifies%2520learning%2520dynamics%2520and%2520capacity%2520via%2520the%2520Minimum%2520Description%2520Length%2520principle%252C%2520offering%2520a%2520geometric%2520theory%2520of%2520self-organized%2520criticality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Concentration%20at%20the%20Edge%20of%20Stability%3A%20Information%20Geometry%20of%20Kernel%20Associative%20Memory&entry.906535625=Akira%20Tamamori&entry.1292438233=High-capacity%20kernel%20Hopfield%20networks%20exhibit%20a%20%22Ridge%20of%20Optimization%22%20characterized%20by%20extreme%20stability.%20While%20previously%20linked%20to%20%22Spectral%20Concentration%2C%22%20its%20origin%20remains%20elusive.%20Here%2C%20we%20analyze%20the%20network%20dynamics%20on%20a%20statistical%20manifold%2C%20revealing%20that%20the%20Ridge%20corresponds%20to%20the%20%22Edge%20of%20Stability%2C%22%20a%20critical%20boundary%20where%20the%20Fisher%20Information%20Matrix%20becomes%20singular.%20We%20demonstrate%20that%20the%20apparent%20Euclidean%20force%20antagonism%20is%20a%20manifestation%20of%20%5Ctextit%7BDual%20Equilibrium%7D%20in%20the%20Riemannian%20space.%20This%20unifies%20learning%20dynamics%20and%20capacity%20via%20the%20Minimum%20Description%20Length%20principle%2C%20offering%20a%20geometric%20theory%20of%20self-organized%20criticality.&entry.1838667208=http%3A//arxiv.org/abs/2511.23083v1&entry.124074799=Read"},
{"title": "Optimizing Multimodal Language Models through Attention-based Interpretability", "author": "Alexander Sergeev and Evgeny Kotelnikov", "abstract": "Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.", "link": "http://arxiv.org/abs/2511.23375v1", "date": "2025-11-28", "relevancy": 2.1889, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5639}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5536}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Multimodal%20Language%20Models%20through%20Attention-based%20Interpretability&body=Title%3A%20Optimizing%20Multimodal%20Language%20Models%20through%20Attention-based%20Interpretability%0AAuthor%3A%20Alexander%20Sergeev%20and%20Evgeny%20Kotelnikov%0AAbstract%3A%20Modern%20large%20language%20models%20become%20multimodal%2C%20analyzing%20various%20data%20formats%20like%20text%20and%20images.%20While%20fine-tuning%20is%20effective%20for%20adapting%20these%20multimodal%20language%20models%20%28MLMs%29%20to%20downstream%20tasks%2C%20full%20fine-tuning%20is%20computationally%20expensive.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20this%20by%20training%20only%20a%20small%20portion%20of%20model%20weights.%20However%2C%20MLMs%20are%20difficult%20to%20interpret%2C%20making%20it%20challenging%20to%20identify%20which%20components%20are%20most%20effective%20for%20training%20to%20balance%20efficiency%20and%20performance.%20We%20propose%20an%20attention-based%20interpretability%20method%20for%20MLMs%20by%20analyzing%20attention%20scores%20relative%20to%20image%20tokens.%20The%20core%20idea%20is%20to%20identify%20attention%20heads%20that%20focus%20on%20image%20key%20objects.%20We%20utilize%20this%20information%20to%20select%20optimal%20model%20components%20for%20PEFT%20in%20multimodal%20models.%20Our%20contributions%20include%20a%20method%20for%20identifying%20attention%20heads%20associated%20with%20image%20key%20objects%2C%20its%20application%20to%20PEFT%20for%20image%20captioning%2C%20and%20the%20creation%20of%20a%20new%20dataset%20containing%20images%2C%20key%20object%20masks%2C%20and%20their%20textual%20descriptions.%20We%20conducted%20experiments%20on%20MLMs%20with%202-3%20billion%20parameters%20to%20validate%20the%20method%27s%20effectiveness.%20By%20calculating%20Head%20Impact%20%28HI%29%20scores%20we%20quantify%20an%20attention%20head%27s%20focus%20on%20key%20objects%2C%20indicating%20its%20significance%20in%20image%20understanding.%20Our%20fine-tuning%20experiments%20demonstrate%20that%20adapting%20layers%20with%20the%20highest%20HI%20scores%20leads%20to%20the%20most%20significant%20shifts%20in%20metrics%20compared%20to%20pre-trained%2C%20randomly%20selected%2C%20or%20lowest-HI-score%20layers.%20This%20indicates%20that%20fine-tuning%20a%20small%20percentage%20%28around%200.01%25%29%20of%20parameters%20in%20these%20crucial%20layers%20can%20substantially%20influence%20image%20understanding%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Multimodal%2520Language%2520Models%2520through%2520Attention-based%2520Interpretability%26entry.906535625%3DAlexander%2520Sergeev%2520and%2520Evgeny%2520Kotelnikov%26entry.1292438233%3DModern%2520large%2520language%2520models%2520become%2520multimodal%252C%2520analyzing%2520various%2520data%2520formats%2520like%2520text%2520and%2520images.%2520While%2520fine-tuning%2520is%2520effective%2520for%2520adapting%2520these%2520multimodal%2520language%2520models%2520%2528MLMs%2529%2520to%2520downstream%2520tasks%252C%2520full%2520fine-tuning%2520is%2520computationally%2520expensive.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520address%2520this%2520by%2520training%2520only%2520a%2520small%2520portion%2520of%2520model%2520weights.%2520However%252C%2520MLMs%2520are%2520difficult%2520to%2520interpret%252C%2520making%2520it%2520challenging%2520to%2520identify%2520which%2520components%2520are%2520most%2520effective%2520for%2520training%2520to%2520balance%2520efficiency%2520and%2520performance.%2520We%2520propose%2520an%2520attention-based%2520interpretability%2520method%2520for%2520MLMs%2520by%2520analyzing%2520attention%2520scores%2520relative%2520to%2520image%2520tokens.%2520The%2520core%2520idea%2520is%2520to%2520identify%2520attention%2520heads%2520that%2520focus%2520on%2520image%2520key%2520objects.%2520We%2520utilize%2520this%2520information%2520to%2520select%2520optimal%2520model%2520components%2520for%2520PEFT%2520in%2520multimodal%2520models.%2520Our%2520contributions%2520include%2520a%2520method%2520for%2520identifying%2520attention%2520heads%2520associated%2520with%2520image%2520key%2520objects%252C%2520its%2520application%2520to%2520PEFT%2520for%2520image%2520captioning%252C%2520and%2520the%2520creation%2520of%2520a%2520new%2520dataset%2520containing%2520images%252C%2520key%2520object%2520masks%252C%2520and%2520their%2520textual%2520descriptions.%2520We%2520conducted%2520experiments%2520on%2520MLMs%2520with%25202-3%2520billion%2520parameters%2520to%2520validate%2520the%2520method%2527s%2520effectiveness.%2520By%2520calculating%2520Head%2520Impact%2520%2528HI%2529%2520scores%2520we%2520quantify%2520an%2520attention%2520head%2527s%2520focus%2520on%2520key%2520objects%252C%2520indicating%2520its%2520significance%2520in%2520image%2520understanding.%2520Our%2520fine-tuning%2520experiments%2520demonstrate%2520that%2520adapting%2520layers%2520with%2520the%2520highest%2520HI%2520scores%2520leads%2520to%2520the%2520most%2520significant%2520shifts%2520in%2520metrics%2520compared%2520to%2520pre-trained%252C%2520randomly%2520selected%252C%2520or%2520lowest-HI-score%2520layers.%2520This%2520indicates%2520that%2520fine-tuning%2520a%2520small%2520percentage%2520%2528around%25200.01%2525%2529%2520of%2520parameters%2520in%2520these%2520crucial%2520layers%2520can%2520substantially%2520influence%2520image%2520understanding%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Multimodal%20Language%20Models%20through%20Attention-based%20Interpretability&entry.906535625=Alexander%20Sergeev%20and%20Evgeny%20Kotelnikov&entry.1292438233=Modern%20large%20language%20models%20become%20multimodal%2C%20analyzing%20various%20data%20formats%20like%20text%20and%20images.%20While%20fine-tuning%20is%20effective%20for%20adapting%20these%20multimodal%20language%20models%20%28MLMs%29%20to%20downstream%20tasks%2C%20full%20fine-tuning%20is%20computationally%20expensive.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20this%20by%20training%20only%20a%20small%20portion%20of%20model%20weights.%20However%2C%20MLMs%20are%20difficult%20to%20interpret%2C%20making%20it%20challenging%20to%20identify%20which%20components%20are%20most%20effective%20for%20training%20to%20balance%20efficiency%20and%20performance.%20We%20propose%20an%20attention-based%20interpretability%20method%20for%20MLMs%20by%20analyzing%20attention%20scores%20relative%20to%20image%20tokens.%20The%20core%20idea%20is%20to%20identify%20attention%20heads%20that%20focus%20on%20image%20key%20objects.%20We%20utilize%20this%20information%20to%20select%20optimal%20model%20components%20for%20PEFT%20in%20multimodal%20models.%20Our%20contributions%20include%20a%20method%20for%20identifying%20attention%20heads%20associated%20with%20image%20key%20objects%2C%20its%20application%20to%20PEFT%20for%20image%20captioning%2C%20and%20the%20creation%20of%20a%20new%20dataset%20containing%20images%2C%20key%20object%20masks%2C%20and%20their%20textual%20descriptions.%20We%20conducted%20experiments%20on%20MLMs%20with%202-3%20billion%20parameters%20to%20validate%20the%20method%27s%20effectiveness.%20By%20calculating%20Head%20Impact%20%28HI%29%20scores%20we%20quantify%20an%20attention%20head%27s%20focus%20on%20key%20objects%2C%20indicating%20its%20significance%20in%20image%20understanding.%20Our%20fine-tuning%20experiments%20demonstrate%20that%20adapting%20layers%20with%20the%20highest%20HI%20scores%20leads%20to%20the%20most%20significant%20shifts%20in%20metrics%20compared%20to%20pre-trained%2C%20randomly%20selected%2C%20or%20lowest-HI-score%20layers.%20This%20indicates%20that%20fine-tuning%20a%20small%20percentage%20%28around%200.01%25%29%20of%20parameters%20in%20these%20crucial%20layers%20can%20substantially%20influence%20image%20understanding%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2511.23375v1&entry.124074799=Read"},
{"title": "Learning to Predict Aboveground Biomass from RGB Images with 3D Synthetic Scenes", "author": "Silvia Zuffi", "abstract": "Forests play a critical role in global ecosystems by supporting biodiversity and mitigating climate change via carbon sequestration. Accurate aboveground biomass (AGB) estimation is essential for assessing carbon storage and wildfire fuel loads, yet traditional methods rely on labor-intensive field measurements or remote sensing approaches with significant limitations in dense vegetation. In this work, we propose a novel learning-based method for estimating AGB from a single ground-based RGB image. We frame this as a dense prediction task, introducing AGB density maps, where each pixel represents tree biomass normalized by the plot area and each tree's image area. We leverage the recently introduced synthetic 3D SPREAD dataset, which provides realistic forest scenes with per-image tree attributes (height, trunk and canopy diameter) and instance segmentation masks. Using these assets, we compute AGB via allometric equations and train a model to predict AGB density maps, integrating them to recover the AGB estimate for the captured scene. Our approach achieves a median AGB estimation error of 1.22 kg/m^2 on held-out SPREAD data and 1.94 kg/m^2 on a real-image dataset. To our knowledge, this is the first method to estimate aboveground biomass directly from a single RGB image, opening up the possibility for a scalable, interpretable, and cost-effective solution for forest monitoring, while also enabling broader participation through citizen science initiatives.", "link": "http://arxiv.org/abs/2511.23249v1", "date": "2025-11-28", "relevancy": 2.1862, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5536}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5459}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Predict%20Aboveground%20Biomass%20from%20RGB%20Images%20with%203D%20Synthetic%20Scenes&body=Title%3A%20Learning%20to%20Predict%20Aboveground%20Biomass%20from%20RGB%20Images%20with%203D%20Synthetic%20Scenes%0AAuthor%3A%20Silvia%20Zuffi%0AAbstract%3A%20Forests%20play%20a%20critical%20role%20in%20global%20ecosystems%20by%20supporting%20biodiversity%20and%20mitigating%20climate%20change%20via%20carbon%20sequestration.%20Accurate%20aboveground%20biomass%20%28AGB%29%20estimation%20is%20essential%20for%20assessing%20carbon%20storage%20and%20wildfire%20fuel%20loads%2C%20yet%20traditional%20methods%20rely%20on%20labor-intensive%20field%20measurements%20or%20remote%20sensing%20approaches%20with%20significant%20limitations%20in%20dense%20vegetation.%20In%20this%20work%2C%20we%20propose%20a%20novel%20learning-based%20method%20for%20estimating%20AGB%20from%20a%20single%20ground-based%20RGB%20image.%20We%20frame%20this%20as%20a%20dense%20prediction%20task%2C%20introducing%20AGB%20density%20maps%2C%20where%20each%20pixel%20represents%20tree%20biomass%20normalized%20by%20the%20plot%20area%20and%20each%20tree%27s%20image%20area.%20We%20leverage%20the%20recently%20introduced%20synthetic%203D%20SPREAD%20dataset%2C%20which%20provides%20realistic%20forest%20scenes%20with%20per-image%20tree%20attributes%20%28height%2C%20trunk%20and%20canopy%20diameter%29%20and%20instance%20segmentation%20masks.%20Using%20these%20assets%2C%20we%20compute%20AGB%20via%20allometric%20equations%20and%20train%20a%20model%20to%20predict%20AGB%20density%20maps%2C%20integrating%20them%20to%20recover%20the%20AGB%20estimate%20for%20the%20captured%20scene.%20Our%20approach%20achieves%20a%20median%20AGB%20estimation%20error%20of%201.22%20kg/m%5E2%20on%20held-out%20SPREAD%20data%20and%201.94%20kg/m%5E2%20on%20a%20real-image%20dataset.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20to%20estimate%20aboveground%20biomass%20directly%20from%20a%20single%20RGB%20image%2C%20opening%20up%20the%20possibility%20for%20a%20scalable%2C%20interpretable%2C%20and%20cost-effective%20solution%20for%20forest%20monitoring%2C%20while%20also%20enabling%20broader%20participation%20through%20citizen%20science%20initiatives.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Predict%2520Aboveground%2520Biomass%2520from%2520RGB%2520Images%2520with%25203D%2520Synthetic%2520Scenes%26entry.906535625%3DSilvia%2520Zuffi%26entry.1292438233%3DForests%2520play%2520a%2520critical%2520role%2520in%2520global%2520ecosystems%2520by%2520supporting%2520biodiversity%2520and%2520mitigating%2520climate%2520change%2520via%2520carbon%2520sequestration.%2520Accurate%2520aboveground%2520biomass%2520%2528AGB%2529%2520estimation%2520is%2520essential%2520for%2520assessing%2520carbon%2520storage%2520and%2520wildfire%2520fuel%2520loads%252C%2520yet%2520traditional%2520methods%2520rely%2520on%2520labor-intensive%2520field%2520measurements%2520or%2520remote%2520sensing%2520approaches%2520with%2520significant%2520limitations%2520in%2520dense%2520vegetation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520learning-based%2520method%2520for%2520estimating%2520AGB%2520from%2520a%2520single%2520ground-based%2520RGB%2520image.%2520We%2520frame%2520this%2520as%2520a%2520dense%2520prediction%2520task%252C%2520introducing%2520AGB%2520density%2520maps%252C%2520where%2520each%2520pixel%2520represents%2520tree%2520biomass%2520normalized%2520by%2520the%2520plot%2520area%2520and%2520each%2520tree%2527s%2520image%2520area.%2520We%2520leverage%2520the%2520recently%2520introduced%2520synthetic%25203D%2520SPREAD%2520dataset%252C%2520which%2520provides%2520realistic%2520forest%2520scenes%2520with%2520per-image%2520tree%2520attributes%2520%2528height%252C%2520trunk%2520and%2520canopy%2520diameter%2529%2520and%2520instance%2520segmentation%2520masks.%2520Using%2520these%2520assets%252C%2520we%2520compute%2520AGB%2520via%2520allometric%2520equations%2520and%2520train%2520a%2520model%2520to%2520predict%2520AGB%2520density%2520maps%252C%2520integrating%2520them%2520to%2520recover%2520the%2520AGB%2520estimate%2520for%2520the%2520captured%2520scene.%2520Our%2520approach%2520achieves%2520a%2520median%2520AGB%2520estimation%2520error%2520of%25201.22%2520kg/m%255E2%2520on%2520held-out%2520SPREAD%2520data%2520and%25201.94%2520kg/m%255E2%2520on%2520a%2520real-image%2520dataset.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520method%2520to%2520estimate%2520aboveground%2520biomass%2520directly%2520from%2520a%2520single%2520RGB%2520image%252C%2520opening%2520up%2520the%2520possibility%2520for%2520a%2520scalable%252C%2520interpretable%252C%2520and%2520cost-effective%2520solution%2520for%2520forest%2520monitoring%252C%2520while%2520also%2520enabling%2520broader%2520participation%2520through%2520citizen%2520science%2520initiatives.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Predict%20Aboveground%20Biomass%20from%20RGB%20Images%20with%203D%20Synthetic%20Scenes&entry.906535625=Silvia%20Zuffi&entry.1292438233=Forests%20play%20a%20critical%20role%20in%20global%20ecosystems%20by%20supporting%20biodiversity%20and%20mitigating%20climate%20change%20via%20carbon%20sequestration.%20Accurate%20aboveground%20biomass%20%28AGB%29%20estimation%20is%20essential%20for%20assessing%20carbon%20storage%20and%20wildfire%20fuel%20loads%2C%20yet%20traditional%20methods%20rely%20on%20labor-intensive%20field%20measurements%20or%20remote%20sensing%20approaches%20with%20significant%20limitations%20in%20dense%20vegetation.%20In%20this%20work%2C%20we%20propose%20a%20novel%20learning-based%20method%20for%20estimating%20AGB%20from%20a%20single%20ground-based%20RGB%20image.%20We%20frame%20this%20as%20a%20dense%20prediction%20task%2C%20introducing%20AGB%20density%20maps%2C%20where%20each%20pixel%20represents%20tree%20biomass%20normalized%20by%20the%20plot%20area%20and%20each%20tree%27s%20image%20area.%20We%20leverage%20the%20recently%20introduced%20synthetic%203D%20SPREAD%20dataset%2C%20which%20provides%20realistic%20forest%20scenes%20with%20per-image%20tree%20attributes%20%28height%2C%20trunk%20and%20canopy%20diameter%29%20and%20instance%20segmentation%20masks.%20Using%20these%20assets%2C%20we%20compute%20AGB%20via%20allometric%20equations%20and%20train%20a%20model%20to%20predict%20AGB%20density%20maps%2C%20integrating%20them%20to%20recover%20the%20AGB%20estimate%20for%20the%20captured%20scene.%20Our%20approach%20achieves%20a%20median%20AGB%20estimation%20error%20of%201.22%20kg/m%5E2%20on%20held-out%20SPREAD%20data%20and%201.94%20kg/m%5E2%20on%20a%20real-image%20dataset.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20to%20estimate%20aboveground%20biomass%20directly%20from%20a%20single%20RGB%20image%2C%20opening%20up%20the%20possibility%20for%20a%20scalable%2C%20interpretable%2C%20and%20cost-effective%20solution%20for%20forest%20monitoring%2C%20while%20also%20enabling%20broader%20participation%20through%20citizen%20science%20initiatives.&entry.1838667208=http%3A//arxiv.org/abs/2511.23249v1&entry.124074799=Read"},
{"title": "Uncovering Zero-Shot Generalization Gaps in Time-Series Foundation Models Using Real-World Videos", "author": "Lujun Li and Lama Sleem and Yiqun Wang and Yangjie Xu and Niccol\u00f2 Gentile and Radu State", "abstract": "Recent research on time-series foundation models (TSFMs) has underscored the scarcity of real-world data, often supplemented with synthetic sources in existing datasets, whose generalizability remains however debated. As such, in this work, we propose a novel benchmarking approach: in particular, we aim at building a curated dataset reflecting real world physical temporal dynamics, extracting temporal signals from real-world videos using optical flow. As such, we introduce REAL-V-TSFM, a novel dataset designed to capture rich and diverse time series derived from real-world videos. Experimental results on state-of-the-art TSFMs under zero-shot forecasting show that, despite strong performance on conventional benchmarks, these models exhibit performance degradation on the proposed dataset, suggesting limited generalizability to novel datasets. These findings underscore the need for novel approaches to acquiring time series data and highlight the lack of universality in recent TSFMs, while further validating the effectiveness of our video-based time series data extraction pipeline.", "link": "http://arxiv.org/abs/2509.26347v2", "date": "2025-11-28", "relevancy": 2.1771, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5571}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.553}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Zero-Shot%20Generalization%20Gaps%20in%20Time-Series%20Foundation%20Models%20Using%20Real-World%20Videos&body=Title%3A%20Uncovering%20Zero-Shot%20Generalization%20Gaps%20in%20Time-Series%20Foundation%20Models%20Using%20Real-World%20Videos%0AAuthor%3A%20Lujun%20Li%20and%20Lama%20Sleem%20and%20Yiqun%20Wang%20and%20Yangjie%20Xu%20and%20Niccol%C3%B2%20Gentile%20and%20Radu%20State%0AAbstract%3A%20Recent%20research%20on%20time-series%20foundation%20models%20%28TSFMs%29%20has%20underscored%20the%20scarcity%20of%20real-world%20data%2C%20often%20supplemented%20with%20synthetic%20sources%20in%20existing%20datasets%2C%20whose%20generalizability%20remains%20however%20debated.%20As%20such%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%20benchmarking%20approach%3A%20in%20particular%2C%20we%20aim%20at%20building%20a%20curated%20dataset%20reflecting%20real%20world%20physical%20temporal%20dynamics%2C%20extracting%20temporal%20signals%20from%20real-world%20videos%20using%20optical%20flow.%20As%20such%2C%20we%20introduce%20REAL-V-TSFM%2C%20a%20novel%20dataset%20designed%20to%20capture%20rich%20and%20diverse%20time%20series%20derived%20from%20real-world%20videos.%20Experimental%20results%20on%20state-of-the-art%20TSFMs%20under%20zero-shot%20forecasting%20show%20that%2C%20despite%20strong%20performance%20on%20conventional%20benchmarks%2C%20these%20models%20exhibit%20performance%20degradation%20on%20the%20proposed%20dataset%2C%20suggesting%20limited%20generalizability%20to%20novel%20datasets.%20These%20findings%20underscore%20the%20need%20for%20novel%20approaches%20to%20acquiring%20time%20series%20data%20and%20highlight%20the%20lack%20of%20universality%20in%20recent%20TSFMs%2C%20while%20further%20validating%20the%20effectiveness%20of%20our%20video-based%20time%20series%20data%20extraction%20pipeline.%0ALink%3A%20http%3A//arxiv.org/abs/2509.26347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Zero-Shot%2520Generalization%2520Gaps%2520in%2520Time-Series%2520Foundation%2520Models%2520Using%2520Real-World%2520Videos%26entry.906535625%3DLujun%2520Li%2520and%2520Lama%2520Sleem%2520and%2520Yiqun%2520Wang%2520and%2520Yangjie%2520Xu%2520and%2520Niccol%25C3%25B2%2520Gentile%2520and%2520Radu%2520State%26entry.1292438233%3DRecent%2520research%2520on%2520time-series%2520foundation%2520models%2520%2528TSFMs%2529%2520has%2520underscored%2520the%2520scarcity%2520of%2520real-world%2520data%252C%2520often%2520supplemented%2520with%2520synthetic%2520sources%2520in%2520existing%2520datasets%252C%2520whose%2520generalizability%2520remains%2520however%2520debated.%2520As%2520such%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520benchmarking%2520approach%253A%2520in%2520particular%252C%2520we%2520aim%2520at%2520building%2520a%2520curated%2520dataset%2520reflecting%2520real%2520world%2520physical%2520temporal%2520dynamics%252C%2520extracting%2520temporal%2520signals%2520from%2520real-world%2520videos%2520using%2520optical%2520flow.%2520As%2520such%252C%2520we%2520introduce%2520REAL-V-TSFM%252C%2520a%2520novel%2520dataset%2520designed%2520to%2520capture%2520rich%2520and%2520diverse%2520time%2520series%2520derived%2520from%2520real-world%2520videos.%2520Experimental%2520results%2520on%2520state-of-the-art%2520TSFMs%2520under%2520zero-shot%2520forecasting%2520show%2520that%252C%2520despite%2520strong%2520performance%2520on%2520conventional%2520benchmarks%252C%2520these%2520models%2520exhibit%2520performance%2520degradation%2520on%2520the%2520proposed%2520dataset%252C%2520suggesting%2520limited%2520generalizability%2520to%2520novel%2520datasets.%2520These%2520findings%2520underscore%2520the%2520need%2520for%2520novel%2520approaches%2520to%2520acquiring%2520time%2520series%2520data%2520and%2520highlight%2520the%2520lack%2520of%2520universality%2520in%2520recent%2520TSFMs%252C%2520while%2520further%2520validating%2520the%2520effectiveness%2520of%2520our%2520video-based%2520time%2520series%2520data%2520extraction%2520pipeline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Zero-Shot%20Generalization%20Gaps%20in%20Time-Series%20Foundation%20Models%20Using%20Real-World%20Videos&entry.906535625=Lujun%20Li%20and%20Lama%20Sleem%20and%20Yiqun%20Wang%20and%20Yangjie%20Xu%20and%20Niccol%C3%B2%20Gentile%20and%20Radu%20State&entry.1292438233=Recent%20research%20on%20time-series%20foundation%20models%20%28TSFMs%29%20has%20underscored%20the%20scarcity%20of%20real-world%20data%2C%20often%20supplemented%20with%20synthetic%20sources%20in%20existing%20datasets%2C%20whose%20generalizability%20remains%20however%20debated.%20As%20such%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%20benchmarking%20approach%3A%20in%20particular%2C%20we%20aim%20at%20building%20a%20curated%20dataset%20reflecting%20real%20world%20physical%20temporal%20dynamics%2C%20extracting%20temporal%20signals%20from%20real-world%20videos%20using%20optical%20flow.%20As%20such%2C%20we%20introduce%20REAL-V-TSFM%2C%20a%20novel%20dataset%20designed%20to%20capture%20rich%20and%20diverse%20time%20series%20derived%20from%20real-world%20videos.%20Experimental%20results%20on%20state-of-the-art%20TSFMs%20under%20zero-shot%20forecasting%20show%20that%2C%20despite%20strong%20performance%20on%20conventional%20benchmarks%2C%20these%20models%20exhibit%20performance%20degradation%20on%20the%20proposed%20dataset%2C%20suggesting%20limited%20generalizability%20to%20novel%20datasets.%20These%20findings%20underscore%20the%20need%20for%20novel%20approaches%20to%20acquiring%20time%20series%20data%20and%20highlight%20the%20lack%20of%20universality%20in%20recent%20TSFMs%2C%20while%20further%20validating%20the%20effectiveness%20of%20our%20video-based%20time%20series%20data%20extraction%20pipeline.&entry.1838667208=http%3A//arxiv.org/abs/2509.26347v2&entry.124074799=Read"},
{"title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey", "author": "Qizhi Pei and Zhimeng Zhou and Kaiyuan Gao and Jinhua Zhu and Yue Wang and Zun Wang and Tao Qin and Lijun Wu and Rui Yan", "abstract": "The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D graphs, and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective multi-modal integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and summarize the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling.", "link": "http://arxiv.org/abs/2403.01528v3", "date": "2025-11-28", "relevancy": 2.1711, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Biomolecule%20and%20Natural%20Language%20through%20Multi-Modal%20Learning%3A%20A%20Survey&body=Title%3A%20Leveraging%20Biomolecule%20and%20Natural%20Language%20through%20Multi-Modal%20Learning%3A%20A%20Survey%0AAuthor%3A%20Qizhi%20Pei%20and%20Zhimeng%20Zhou%20and%20Kaiyuan%20Gao%20and%20Jinhua%20Zhu%20and%20Yue%20Wang%20and%20Zun%20Wang%20and%20Tao%20Qin%20and%20Lijun%20Wu%20and%20Rui%20Yan%0AAbstract%3A%20The%20integration%20of%20biomolecular%20modeling%20with%20natural%20language%20%28BL%29%20has%20emerged%20as%20a%20promising%20interdisciplinary%20area%20at%20the%20intersection%20of%20artificial%20intelligence%2C%20chemistry%20and%20biology.%20This%20approach%20leverages%20the%20rich%2C%20multifaceted%20descriptions%20of%20biomolecules%20contained%20within%20textual%20data%20sources%20to%20enhance%20our%20fundamental%20understanding%20and%20enable%20downstream%20computational%20tasks%20such%20as%20biomolecule%20property%20prediction.%20The%20fusion%20of%20the%20nuanced%20narratives%20expressed%20through%20natural%20language%20with%20the%20structural%20and%20functional%20specifics%20of%20biomolecules%20described%20via%20various%20molecular%20modeling%20techniques%20opens%20new%20avenues%20for%20comprehensively%20representing%20and%20analyzing%20biomolecules.%20By%20incorporating%20the%20contextual%20language%20data%20that%20surrounds%20biomolecules%20into%20their%20modeling%2C%20BL%20aims%20to%20capture%20a%20holistic%20view%20encompassing%20both%20the%20symbolic%20qualities%20conveyed%20through%20language%20as%20well%20as%20quantitative%20structural%20characteristics.%20In%20this%20review%2C%20we%20provide%20an%20extensive%20analysis%20of%20recent%20advancements%20achieved%20through%20cross%20modeling%20of%20biomolecules%20and%20natural%20language.%20%281%29%20We%20begin%20by%20outlining%20the%20technical%20representations%20of%20biomolecules%20employed%2C%20including%20sequences%2C%202D%20graphs%2C%20and%203D%20structures.%20%282%29%20We%20then%20examine%20in%20depth%20the%20rationale%20and%20key%20objectives%20underlying%20effective%20multi-modal%20integration%20of%20language%20and%20molecular%20data%20sources.%20%283%29%20We%20subsequently%20survey%20the%20practical%20applications%20enabled%20to%20date%20in%20this%20developing%20research%20area.%20%284%29%20We%20also%20compile%20and%20summarize%20the%20available%20resources%20and%20datasets%20to%20facilitate%20future%20work.%20%285%29%20Looking%20ahead%2C%20we%20identify%20several%20promising%20research%20directions%20worthy%20of%20further%20exploration%20and%20investment%20to%20continue%20advancing%20the%20field.%20The%20related%20resources%20and%20contents%20are%20updating%20in%20https%3A//github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2403.01528v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Biomolecule%2520and%2520Natural%2520Language%2520through%2520Multi-Modal%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DQizhi%2520Pei%2520and%2520Zhimeng%2520Zhou%2520and%2520Kaiyuan%2520Gao%2520and%2520Jinhua%2520Zhu%2520and%2520Yue%2520Wang%2520and%2520Zun%2520Wang%2520and%2520Tao%2520Qin%2520and%2520Lijun%2520Wu%2520and%2520Rui%2520Yan%26entry.1292438233%3DThe%2520integration%2520of%2520biomolecular%2520modeling%2520with%2520natural%2520language%2520%2528BL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520interdisciplinary%2520area%2520at%2520the%2520intersection%2520of%2520artificial%2520intelligence%252C%2520chemistry%2520and%2520biology.%2520This%2520approach%2520leverages%2520the%2520rich%252C%2520multifaceted%2520descriptions%2520of%2520biomolecules%2520contained%2520within%2520textual%2520data%2520sources%2520to%2520enhance%2520our%2520fundamental%2520understanding%2520and%2520enable%2520downstream%2520computational%2520tasks%2520such%2520as%2520biomolecule%2520property%2520prediction.%2520The%2520fusion%2520of%2520the%2520nuanced%2520narratives%2520expressed%2520through%2520natural%2520language%2520with%2520the%2520structural%2520and%2520functional%2520specifics%2520of%2520biomolecules%2520described%2520via%2520various%2520molecular%2520modeling%2520techniques%2520opens%2520new%2520avenues%2520for%2520comprehensively%2520representing%2520and%2520analyzing%2520biomolecules.%2520By%2520incorporating%2520the%2520contextual%2520language%2520data%2520that%2520surrounds%2520biomolecules%2520into%2520their%2520modeling%252C%2520BL%2520aims%2520to%2520capture%2520a%2520holistic%2520view%2520encompassing%2520both%2520the%2520symbolic%2520qualities%2520conveyed%2520through%2520language%2520as%2520well%2520as%2520quantitative%2520structural%2520characteristics.%2520In%2520this%2520review%252C%2520we%2520provide%2520an%2520extensive%2520analysis%2520of%2520recent%2520advancements%2520achieved%2520through%2520cross%2520modeling%2520of%2520biomolecules%2520and%2520natural%2520language.%2520%25281%2529%2520We%2520begin%2520by%2520outlining%2520the%2520technical%2520representations%2520of%2520biomolecules%2520employed%252C%2520including%2520sequences%252C%25202D%2520graphs%252C%2520and%25203D%2520structures.%2520%25282%2529%2520We%2520then%2520examine%2520in%2520depth%2520the%2520rationale%2520and%2520key%2520objectives%2520underlying%2520effective%2520multi-modal%2520integration%2520of%2520language%2520and%2520molecular%2520data%2520sources.%2520%25283%2529%2520We%2520subsequently%2520survey%2520the%2520practical%2520applications%2520enabled%2520to%2520date%2520in%2520this%2520developing%2520research%2520area.%2520%25284%2529%2520We%2520also%2520compile%2520and%2520summarize%2520the%2520available%2520resources%2520and%2520datasets%2520to%2520facilitate%2520future%2520work.%2520%25285%2529%2520Looking%2520ahead%252C%2520we%2520identify%2520several%2520promising%2520research%2520directions%2520worthy%2520of%2520further%2520exploration%2520and%2520investment%2520to%2520continue%2520advancing%2520the%2520field.%2520The%2520related%2520resources%2520and%2520contents%2520are%2520updating%2520in%2520https%253A//github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01528v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Biomolecule%20and%20Natural%20Language%20through%20Multi-Modal%20Learning%3A%20A%20Survey&entry.906535625=Qizhi%20Pei%20and%20Zhimeng%20Zhou%20and%20Kaiyuan%20Gao%20and%20Jinhua%20Zhu%20and%20Yue%20Wang%20and%20Zun%20Wang%20and%20Tao%20Qin%20and%20Lijun%20Wu%20and%20Rui%20Yan&entry.1292438233=The%20integration%20of%20biomolecular%20modeling%20with%20natural%20language%20%28BL%29%20has%20emerged%20as%20a%20promising%20interdisciplinary%20area%20at%20the%20intersection%20of%20artificial%20intelligence%2C%20chemistry%20and%20biology.%20This%20approach%20leverages%20the%20rich%2C%20multifaceted%20descriptions%20of%20biomolecules%20contained%20within%20textual%20data%20sources%20to%20enhance%20our%20fundamental%20understanding%20and%20enable%20downstream%20computational%20tasks%20such%20as%20biomolecule%20property%20prediction.%20The%20fusion%20of%20the%20nuanced%20narratives%20expressed%20through%20natural%20language%20with%20the%20structural%20and%20functional%20specifics%20of%20biomolecules%20described%20via%20various%20molecular%20modeling%20techniques%20opens%20new%20avenues%20for%20comprehensively%20representing%20and%20analyzing%20biomolecules.%20By%20incorporating%20the%20contextual%20language%20data%20that%20surrounds%20biomolecules%20into%20their%20modeling%2C%20BL%20aims%20to%20capture%20a%20holistic%20view%20encompassing%20both%20the%20symbolic%20qualities%20conveyed%20through%20language%20as%20well%20as%20quantitative%20structural%20characteristics.%20In%20this%20review%2C%20we%20provide%20an%20extensive%20analysis%20of%20recent%20advancements%20achieved%20through%20cross%20modeling%20of%20biomolecules%20and%20natural%20language.%20%281%29%20We%20begin%20by%20outlining%20the%20technical%20representations%20of%20biomolecules%20employed%2C%20including%20sequences%2C%202D%20graphs%2C%20and%203D%20structures.%20%282%29%20We%20then%20examine%20in%20depth%20the%20rationale%20and%20key%20objectives%20underlying%20effective%20multi-modal%20integration%20of%20language%20and%20molecular%20data%20sources.%20%283%29%20We%20subsequently%20survey%20the%20practical%20applications%20enabled%20to%20date%20in%20this%20developing%20research%20area.%20%284%29%20We%20also%20compile%20and%20summarize%20the%20available%20resources%20and%20datasets%20to%20facilitate%20future%20work.%20%285%29%20Looking%20ahead%2C%20we%20identify%20several%20promising%20research%20directions%20worthy%20of%20further%20exploration%20and%20investment%20to%20continue%20advancing%20the%20field.%20The%20related%20resources%20and%20contents%20are%20updating%20in%20https%3A//github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling.&entry.1838667208=http%3A//arxiv.org/abs/2403.01528v3&entry.124074799=Read"},
{"title": "REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection", "author": "Huangsen Cao and Qin Mei and Zhiheng Li and Yuxi Li and Ying Zhang and Chen Li and Zhimeng Zhang and Xin Ding and Yongwei Wang and Jing Lyu and Fei Wu", "abstract": "With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \\textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \\textbf{REVEAL} (\\underline{R}easoning-\\underline{e}nhanced Forensic E\\underline{v}id\\underline{e}nce \\underline{A}na\\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.", "link": "http://arxiv.org/abs/2511.23158v1", "date": "2025-11-28", "relevancy": 2.1681, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REVEAL%3A%20Reasoning-enhanced%20Forensic%20Evidence%20Analysis%20for%20Explainable%20AI-generated%20Image%20Detection&body=Title%3A%20REVEAL%3A%20Reasoning-enhanced%20Forensic%20Evidence%20Analysis%20for%20Explainable%20AI-generated%20Image%20Detection%0AAuthor%3A%20Huangsen%20Cao%20and%20Qin%20Mei%20and%20Zhiheng%20Li%20and%20Yuxi%20Li%20and%20Ying%20Zhang%20and%20Chen%20Li%20and%20Zhimeng%20Zhang%20and%20Xin%20Ding%20and%20Yongwei%20Wang%20and%20Jing%20Lyu%20and%20Fei%20Wu%0AAbstract%3A%20With%20the%20rapid%20advancement%20of%20generative%20models%2C%20visually%20realistic%20AI-generated%20images%20have%20become%20increasingly%20difficult%20to%20distinguish%20from%20authentic%20ones%2C%20posing%20severe%20threats%20to%20social%20trust%20and%20information%20integrity.%20Consequently%2C%20there%20is%20an%20urgent%20need%20for%20efficient%20and%20truly%20explainable%20image%20forensic%20methods.%20Recent%20detection%20paradigms%20have%20shifted%20towards%20explainable%20forensics.%20However%2C%20state-of-the-art%20approaches%20primarily%20rely%20on%20post-hoc%20rationalizations%20or%20visual%20discrimination%2C%20lacking%20a%20verifiable%20chain%20of%20evidence.%20This%20reliance%20on%20surface-level%20pattern%20matching%20limits%20the%20generation%20of%20causally%20grounded%20explanations%20and%20often%20results%20in%20poor%20generalization.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20%5Ctextbf%7BREVEAL-Bench%7D%2C%20the%20first%20reasoning-enhanced%20multimodal%20benchmark%20for%20AI-generated%20image%20detection%20that%20is%20explicitly%20structured%20around%20a%20chain-of-evidence%20derived%20from%20multiple%20lightweight%20expert%20models%2C%20then%20records%20step-by-step%20reasoning%20traces%20and%20evidential%20justifications.%20Building%20upon%20this%20dataset%2C%20we%20propose%20%5Ctextbf%7BREVEAL%7D%20%28%5Cunderline%7BR%7Deasoning-%5Cunderline%7Be%7Dnhanced%20Forensic%20E%5Cunderline%7Bv%7Did%5Cunderline%7Be%7Dnce%20%5Cunderline%7BA%7Dna%5Cunderline%7Bl%7Dysis%29%2C%20an%20effective%20and%20explainable%20forensic%20framework%20that%20integrates%20detection%20with%20a%20novel%20expert-grounded%20reinforcement%20learning.%20Our%20reward%20mechanism%20is%20specially%20tailored%20to%20jointly%20optimize%20detection%20accuracy%2C%20explanation%20fidelity%2C%20and%20logical%20coherence%20grounded%20in%20explicit%20forensic%20evidence%2C%20enabling%20REVEAL%20to%20produce%20fine-grained%2C%20interpretable%2C%20and%20verifiable%20reasoning%20chains%20alongside%20its%20detection%20outcomes.%20Extensive%20experimental%20results%20demonstrate%20that%20REVEAL%20significantly%20enhances%20detection%20accuracy%2C%20explanation%20fidelity%2C%20and%20robust%20cross-model%20generalization%2C%20benchmarking%20a%20new%20state%20of%20the%20art%20for%20explainable%20image%20forensics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREVEAL%253A%2520Reasoning-enhanced%2520Forensic%2520Evidence%2520Analysis%2520for%2520Explainable%2520AI-generated%2520Image%2520Detection%26entry.906535625%3DHuangsen%2520Cao%2520and%2520Qin%2520Mei%2520and%2520Zhiheng%2520Li%2520and%2520Yuxi%2520Li%2520and%2520Ying%2520Zhang%2520and%2520Chen%2520Li%2520and%2520Zhimeng%2520Zhang%2520and%2520Xin%2520Ding%2520and%2520Yongwei%2520Wang%2520and%2520Jing%2520Lyu%2520and%2520Fei%2520Wu%26entry.1292438233%3DWith%2520the%2520rapid%2520advancement%2520of%2520generative%2520models%252C%2520visually%2520realistic%2520AI-generated%2520images%2520have%2520become%2520increasingly%2520difficult%2520to%2520distinguish%2520from%2520authentic%2520ones%252C%2520posing%2520severe%2520threats%2520to%2520social%2520trust%2520and%2520information%2520integrity.%2520Consequently%252C%2520there%2520is%2520an%2520urgent%2520need%2520for%2520efficient%2520and%2520truly%2520explainable%2520image%2520forensic%2520methods.%2520Recent%2520detection%2520paradigms%2520have%2520shifted%2520towards%2520explainable%2520forensics.%2520However%252C%2520state-of-the-art%2520approaches%2520primarily%2520rely%2520on%2520post-hoc%2520rationalizations%2520or%2520visual%2520discrimination%252C%2520lacking%2520a%2520verifiable%2520chain%2520of%2520evidence.%2520This%2520reliance%2520on%2520surface-level%2520pattern%2520matching%2520limits%2520the%2520generation%2520of%2520causally%2520grounded%2520explanations%2520and%2520often%2520results%2520in%2520poor%2520generalization.%2520To%2520bridge%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520%255Ctextbf%257BREVEAL-Bench%257D%252C%2520the%2520first%2520reasoning-enhanced%2520multimodal%2520benchmark%2520for%2520AI-generated%2520image%2520detection%2520that%2520is%2520explicitly%2520structured%2520around%2520a%2520chain-of-evidence%2520derived%2520from%2520multiple%2520lightweight%2520expert%2520models%252C%2520then%2520records%2520step-by-step%2520reasoning%2520traces%2520and%2520evidential%2520justifications.%2520Building%2520upon%2520this%2520dataset%252C%2520we%2520propose%2520%255Ctextbf%257BREVEAL%257D%2520%2528%255Cunderline%257BR%257Deasoning-%255Cunderline%257Be%257Dnhanced%2520Forensic%2520E%255Cunderline%257Bv%257Did%255Cunderline%257Be%257Dnce%2520%255Cunderline%257BA%257Dna%255Cunderline%257Bl%257Dysis%2529%252C%2520an%2520effective%2520and%2520explainable%2520forensic%2520framework%2520that%2520integrates%2520detection%2520with%2520a%2520novel%2520expert-grounded%2520reinforcement%2520learning.%2520Our%2520reward%2520mechanism%2520is%2520specially%2520tailored%2520to%2520jointly%2520optimize%2520detection%2520accuracy%252C%2520explanation%2520fidelity%252C%2520and%2520logical%2520coherence%2520grounded%2520in%2520explicit%2520forensic%2520evidence%252C%2520enabling%2520REVEAL%2520to%2520produce%2520fine-grained%252C%2520interpretable%252C%2520and%2520verifiable%2520reasoning%2520chains%2520alongside%2520its%2520detection%2520outcomes.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520REVEAL%2520significantly%2520enhances%2520detection%2520accuracy%252C%2520explanation%2520fidelity%252C%2520and%2520robust%2520cross-model%2520generalization%252C%2520benchmarking%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520explainable%2520image%2520forensics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REVEAL%3A%20Reasoning-enhanced%20Forensic%20Evidence%20Analysis%20for%20Explainable%20AI-generated%20Image%20Detection&entry.906535625=Huangsen%20Cao%20and%20Qin%20Mei%20and%20Zhiheng%20Li%20and%20Yuxi%20Li%20and%20Ying%20Zhang%20and%20Chen%20Li%20and%20Zhimeng%20Zhang%20and%20Xin%20Ding%20and%20Yongwei%20Wang%20and%20Jing%20Lyu%20and%20Fei%20Wu&entry.1292438233=With%20the%20rapid%20advancement%20of%20generative%20models%2C%20visually%20realistic%20AI-generated%20images%20have%20become%20increasingly%20difficult%20to%20distinguish%20from%20authentic%20ones%2C%20posing%20severe%20threats%20to%20social%20trust%20and%20information%20integrity.%20Consequently%2C%20there%20is%20an%20urgent%20need%20for%20efficient%20and%20truly%20explainable%20image%20forensic%20methods.%20Recent%20detection%20paradigms%20have%20shifted%20towards%20explainable%20forensics.%20However%2C%20state-of-the-art%20approaches%20primarily%20rely%20on%20post-hoc%20rationalizations%20or%20visual%20discrimination%2C%20lacking%20a%20verifiable%20chain%20of%20evidence.%20This%20reliance%20on%20surface-level%20pattern%20matching%20limits%20the%20generation%20of%20causally%20grounded%20explanations%20and%20often%20results%20in%20poor%20generalization.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20%5Ctextbf%7BREVEAL-Bench%7D%2C%20the%20first%20reasoning-enhanced%20multimodal%20benchmark%20for%20AI-generated%20image%20detection%20that%20is%20explicitly%20structured%20around%20a%20chain-of-evidence%20derived%20from%20multiple%20lightweight%20expert%20models%2C%20then%20records%20step-by-step%20reasoning%20traces%20and%20evidential%20justifications.%20Building%20upon%20this%20dataset%2C%20we%20propose%20%5Ctextbf%7BREVEAL%7D%20%28%5Cunderline%7BR%7Deasoning-%5Cunderline%7Be%7Dnhanced%20Forensic%20E%5Cunderline%7Bv%7Did%5Cunderline%7Be%7Dnce%20%5Cunderline%7BA%7Dna%5Cunderline%7Bl%7Dysis%29%2C%20an%20effective%20and%20explainable%20forensic%20framework%20that%20integrates%20detection%20with%20a%20novel%20expert-grounded%20reinforcement%20learning.%20Our%20reward%20mechanism%20is%20specially%20tailored%20to%20jointly%20optimize%20detection%20accuracy%2C%20explanation%20fidelity%2C%20and%20logical%20coherence%20grounded%20in%20explicit%20forensic%20evidence%2C%20enabling%20REVEAL%20to%20produce%20fine-grained%2C%20interpretable%2C%20and%20verifiable%20reasoning%20chains%20alongside%20its%20detection%20outcomes.%20Extensive%20experimental%20results%20demonstrate%20that%20REVEAL%20significantly%20enhances%20detection%20accuracy%2C%20explanation%20fidelity%2C%20and%20robust%20cross-model%20generalization%2C%20benchmarking%20a%20new%20state%20of%20the%20art%20for%20explainable%20image%20forensics.&entry.1838667208=http%3A//arxiv.org/abs/2511.23158v1&entry.124074799=Read"},
{"title": "Source-free Video Domain Adaptation by Learning from Noisy Labels", "author": "Avijit Dasgupta and C. V. Jawahar and Karteek Alahari", "abstract": "Despite the progress seen in classification methods, current approaches for handling videos with distribution shifts in source and target domains remain source-dependent as they require access to the source data during the adaptation stage. In this paper, we present a self-training based source-free video domain adaptation approach to address this challenge by bridging the gap between the source and the target domains. We use the source pre-trained model to generate pseudo-labels for the target domain samples, which are inevitably noisy. Thus, we treat the problem of source-free video domain adaptation as learning from noisy labels and argue that the samples with correct pseudo-labels can help us in adaptation. To this end, we leverage the cross-entropy loss as an indicator of the correctness of the pseudo-labels and use the resulting small-loss samples from the target domain for fine-tuning the model. We further enhance the adaptation performance by implementing a teacher-student (TS) framework, in which the teacher, which is updated gradually, produces reliable pseudo-labels. Meanwhile, the student undergoes fine-tuning on the target domain videos using these generated pseudo-labels to improve its performance. Extensive experimental evaluations show that our methods, termed as CleanAdapt, CleanAdapt + TS, achieve state-of-the-art results, outperforming the existing approaches on various open datasets. Our source code is publicly available at https://avijit9.github.io/CleanAdapt.", "link": "http://arxiv.org/abs/2311.18572v2", "date": "2025-11-28", "relevancy": 2.1649, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5492}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5491}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source-free%20Video%20Domain%20Adaptation%20by%20Learning%20from%20Noisy%20Labels&body=Title%3A%20Source-free%20Video%20Domain%20Adaptation%20by%20Learning%20from%20Noisy%20Labels%0AAuthor%3A%20Avijit%20Dasgupta%20and%20C.%20V.%20Jawahar%20and%20Karteek%20Alahari%0AAbstract%3A%20Despite%20the%20progress%20seen%20in%20classification%20methods%2C%20current%20approaches%20for%20handling%20videos%20with%20distribution%20shifts%20in%20source%20and%20target%20domains%20remain%20source-dependent%20as%20they%20require%20access%20to%20the%20source%20data%20during%20the%20adaptation%20stage.%20In%20this%20paper%2C%20we%20present%20a%20self-training%20based%20source-free%20video%20domain%20adaptation%20approach%20to%20address%20this%20challenge%20by%20bridging%20the%20gap%20between%20the%20source%20and%20the%20target%20domains.%20We%20use%20the%20source%20pre-trained%20model%20to%20generate%20pseudo-labels%20for%20the%20target%20domain%20samples%2C%20which%20are%20inevitably%20noisy.%20Thus%2C%20we%20treat%20the%20problem%20of%20source-free%20video%20domain%20adaptation%20as%20learning%20from%20noisy%20labels%20and%20argue%20that%20the%20samples%20with%20correct%20pseudo-labels%20can%20help%20us%20in%20adaptation.%20To%20this%20end%2C%20we%20leverage%20the%20cross-entropy%20loss%20as%20an%20indicator%20of%20the%20correctness%20of%20the%20pseudo-labels%20and%20use%20the%20resulting%20small-loss%20samples%20from%20the%20target%20domain%20for%20fine-tuning%20the%20model.%20We%20further%20enhance%20the%20adaptation%20performance%20by%20implementing%20a%20teacher-student%20%28TS%29%20framework%2C%20in%20which%20the%20teacher%2C%20which%20is%20updated%20gradually%2C%20produces%20reliable%20pseudo-labels.%20Meanwhile%2C%20the%20student%20undergoes%20fine-tuning%20on%20the%20target%20domain%20videos%20using%20these%20generated%20pseudo-labels%20to%20improve%20its%20performance.%20Extensive%20experimental%20evaluations%20show%20that%20our%20methods%2C%20termed%20as%20CleanAdapt%2C%20CleanAdapt%20%2B%20TS%2C%20achieve%20state-of-the-art%20results%2C%20outperforming%20the%20existing%20approaches%20on%20various%20open%20datasets.%20Our%20source%20code%20is%20publicly%20available%20at%20https%3A//avijit9.github.io/CleanAdapt.%0ALink%3A%20http%3A//arxiv.org/abs/2311.18572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource-free%2520Video%2520Domain%2520Adaptation%2520by%2520Learning%2520from%2520Noisy%2520Labels%26entry.906535625%3DAvijit%2520Dasgupta%2520and%2520C.%2520V.%2520Jawahar%2520and%2520Karteek%2520Alahari%26entry.1292438233%3DDespite%2520the%2520progress%2520seen%2520in%2520classification%2520methods%252C%2520current%2520approaches%2520for%2520handling%2520videos%2520with%2520distribution%2520shifts%2520in%2520source%2520and%2520target%2520domains%2520remain%2520source-dependent%2520as%2520they%2520require%2520access%2520to%2520the%2520source%2520data%2520during%2520the%2520adaptation%2520stage.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520self-training%2520based%2520source-free%2520video%2520domain%2520adaptation%2520approach%2520to%2520address%2520this%2520challenge%2520by%2520bridging%2520the%2520gap%2520between%2520the%2520source%2520and%2520the%2520target%2520domains.%2520We%2520use%2520the%2520source%2520pre-trained%2520model%2520to%2520generate%2520pseudo-labels%2520for%2520the%2520target%2520domain%2520samples%252C%2520which%2520are%2520inevitably%2520noisy.%2520Thus%252C%2520we%2520treat%2520the%2520problem%2520of%2520source-free%2520video%2520domain%2520adaptation%2520as%2520learning%2520from%2520noisy%2520labels%2520and%2520argue%2520that%2520the%2520samples%2520with%2520correct%2520pseudo-labels%2520can%2520help%2520us%2520in%2520adaptation.%2520To%2520this%2520end%252C%2520we%2520leverage%2520the%2520cross-entropy%2520loss%2520as%2520an%2520indicator%2520of%2520the%2520correctness%2520of%2520the%2520pseudo-labels%2520and%2520use%2520the%2520resulting%2520small-loss%2520samples%2520from%2520the%2520target%2520domain%2520for%2520fine-tuning%2520the%2520model.%2520We%2520further%2520enhance%2520the%2520adaptation%2520performance%2520by%2520implementing%2520a%2520teacher-student%2520%2528TS%2529%2520framework%252C%2520in%2520which%2520the%2520teacher%252C%2520which%2520is%2520updated%2520gradually%252C%2520produces%2520reliable%2520pseudo-labels.%2520Meanwhile%252C%2520the%2520student%2520undergoes%2520fine-tuning%2520on%2520the%2520target%2520domain%2520videos%2520using%2520these%2520generated%2520pseudo-labels%2520to%2520improve%2520its%2520performance.%2520Extensive%2520experimental%2520evaluations%2520show%2520that%2520our%2520methods%252C%2520termed%2520as%2520CleanAdapt%252C%2520CleanAdapt%2520%252B%2520TS%252C%2520achieve%2520state-of-the-art%2520results%252C%2520outperforming%2520the%2520existing%2520approaches%2520on%2520various%2520open%2520datasets.%2520Our%2520source%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//avijit9.github.io/CleanAdapt.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-free%20Video%20Domain%20Adaptation%20by%20Learning%20from%20Noisy%20Labels&entry.906535625=Avijit%20Dasgupta%20and%20C.%20V.%20Jawahar%20and%20Karteek%20Alahari&entry.1292438233=Despite%20the%20progress%20seen%20in%20classification%20methods%2C%20current%20approaches%20for%20handling%20videos%20with%20distribution%20shifts%20in%20source%20and%20target%20domains%20remain%20source-dependent%20as%20they%20require%20access%20to%20the%20source%20data%20during%20the%20adaptation%20stage.%20In%20this%20paper%2C%20we%20present%20a%20self-training%20based%20source-free%20video%20domain%20adaptation%20approach%20to%20address%20this%20challenge%20by%20bridging%20the%20gap%20between%20the%20source%20and%20the%20target%20domains.%20We%20use%20the%20source%20pre-trained%20model%20to%20generate%20pseudo-labels%20for%20the%20target%20domain%20samples%2C%20which%20are%20inevitably%20noisy.%20Thus%2C%20we%20treat%20the%20problem%20of%20source-free%20video%20domain%20adaptation%20as%20learning%20from%20noisy%20labels%20and%20argue%20that%20the%20samples%20with%20correct%20pseudo-labels%20can%20help%20us%20in%20adaptation.%20To%20this%20end%2C%20we%20leverage%20the%20cross-entropy%20loss%20as%20an%20indicator%20of%20the%20correctness%20of%20the%20pseudo-labels%20and%20use%20the%20resulting%20small-loss%20samples%20from%20the%20target%20domain%20for%20fine-tuning%20the%20model.%20We%20further%20enhance%20the%20adaptation%20performance%20by%20implementing%20a%20teacher-student%20%28TS%29%20framework%2C%20in%20which%20the%20teacher%2C%20which%20is%20updated%20gradually%2C%20produces%20reliable%20pseudo-labels.%20Meanwhile%2C%20the%20student%20undergoes%20fine-tuning%20on%20the%20target%20domain%20videos%20using%20these%20generated%20pseudo-labels%20to%20improve%20its%20performance.%20Extensive%20experimental%20evaluations%20show%20that%20our%20methods%2C%20termed%20as%20CleanAdapt%2C%20CleanAdapt%20%2B%20TS%2C%20achieve%20state-of-the-art%20results%2C%20outperforming%20the%20existing%20approaches%20on%20various%20open%20datasets.%20Our%20source%20code%20is%20publicly%20available%20at%20https%3A//avijit9.github.io/CleanAdapt.&entry.1838667208=http%3A//arxiv.org/abs/2311.18572v2&entry.124074799=Read"},
{"title": "Configurable Fairness: Direct Optimization of Parity Metrics via Vision-Language Models", "author": "Miao Zhang and Rumi Chunara", "abstract": "Performance disparities of image recognition across demographic groups are known to exist in deep learning-based models, due to imbalanced group representations or spurious correlation between group and target labels. Previous work has addressed such challenges without relying on expensive group labels, typically by upweighting high-loss samples or balancing discovered clusters. However, these heuristic strategies lack direct connection to specific fairness metrics and cannot guarantee optimization of parity-based criteria like equal opportunity, which ensures equal chance to receive positive outcomes across groups. In this work, we propose a novel paradigm that directly optimizes parity-based fairness metrics through specifically designed training objectives, without requiring group labels. We leverage vision-language models to analyze sensitive attribute relevancy for individual samples, then formulate loss functions that mathematically connect to each target fairness metric. This enables flexible optimization of different fairness criteria based on application needs. Experiments on multiple image classification datasets show that our metric-specific approach significantly improves parity-based fairness criteria and outperforms existing methods.", "link": "http://arxiv.org/abs/2403.10624v3", "date": "2025-11-28", "relevancy": 2.1627, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5466}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Configurable%20Fairness%3A%20Direct%20Optimization%20of%20Parity%20Metrics%20via%20Vision-Language%20Models&body=Title%3A%20Configurable%20Fairness%3A%20Direct%20Optimization%20of%20Parity%20Metrics%20via%20Vision-Language%20Models%0AAuthor%3A%20Miao%20Zhang%20and%20Rumi%20Chunara%0AAbstract%3A%20Performance%20disparities%20of%20image%20recognition%20across%20demographic%20groups%20are%20known%20to%20exist%20in%20deep%20learning-based%20models%2C%20due%20to%20imbalanced%20group%20representations%20or%20spurious%20correlation%20between%20group%20and%20target%20labels.%20Previous%20work%20has%20addressed%20such%20challenges%20without%20relying%20on%20expensive%20group%20labels%2C%20typically%20by%20upweighting%20high-loss%20samples%20or%20balancing%20discovered%20clusters.%20However%2C%20these%20heuristic%20strategies%20lack%20direct%20connection%20to%20specific%20fairness%20metrics%20and%20cannot%20guarantee%20optimization%20of%20parity-based%20criteria%20like%20equal%20opportunity%2C%20which%20ensures%20equal%20chance%20to%20receive%20positive%20outcomes%20across%20groups.%20In%20this%20work%2C%20we%20propose%20a%20novel%20paradigm%20that%20directly%20optimizes%20parity-based%20fairness%20metrics%20through%20specifically%20designed%20training%20objectives%2C%20without%20requiring%20group%20labels.%20We%20leverage%20vision-language%20models%20to%20analyze%20sensitive%20attribute%20relevancy%20for%20individual%20samples%2C%20then%20formulate%20loss%20functions%20that%20mathematically%20connect%20to%20each%20target%20fairness%20metric.%20This%20enables%20flexible%20optimization%20of%20different%20fairness%20criteria%20based%20on%20application%20needs.%20Experiments%20on%20multiple%20image%20classification%20datasets%20show%20that%20our%20metric-specific%20approach%20significantly%20improves%20parity-based%20fairness%20criteria%20and%20outperforms%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2403.10624v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfigurable%2520Fairness%253A%2520Direct%2520Optimization%2520of%2520Parity%2520Metrics%2520via%2520Vision-Language%2520Models%26entry.906535625%3DMiao%2520Zhang%2520and%2520Rumi%2520Chunara%26entry.1292438233%3DPerformance%2520disparities%2520of%2520image%2520recognition%2520across%2520demographic%2520groups%2520are%2520known%2520to%2520exist%2520in%2520deep%2520learning-based%2520models%252C%2520due%2520to%2520imbalanced%2520group%2520representations%2520or%2520spurious%2520correlation%2520between%2520group%2520and%2520target%2520labels.%2520Previous%2520work%2520has%2520addressed%2520such%2520challenges%2520without%2520relying%2520on%2520expensive%2520group%2520labels%252C%2520typically%2520by%2520upweighting%2520high-loss%2520samples%2520or%2520balancing%2520discovered%2520clusters.%2520However%252C%2520these%2520heuristic%2520strategies%2520lack%2520direct%2520connection%2520to%2520specific%2520fairness%2520metrics%2520and%2520cannot%2520guarantee%2520optimization%2520of%2520parity-based%2520criteria%2520like%2520equal%2520opportunity%252C%2520which%2520ensures%2520equal%2520chance%2520to%2520receive%2520positive%2520outcomes%2520across%2520groups.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520paradigm%2520that%2520directly%2520optimizes%2520parity-based%2520fairness%2520metrics%2520through%2520specifically%2520designed%2520training%2520objectives%252C%2520without%2520requiring%2520group%2520labels.%2520We%2520leverage%2520vision-language%2520models%2520to%2520analyze%2520sensitive%2520attribute%2520relevancy%2520for%2520individual%2520samples%252C%2520then%2520formulate%2520loss%2520functions%2520that%2520mathematically%2520connect%2520to%2520each%2520target%2520fairness%2520metric.%2520This%2520enables%2520flexible%2520optimization%2520of%2520different%2520fairness%2520criteria%2520based%2520on%2520application%2520needs.%2520Experiments%2520on%2520multiple%2520image%2520classification%2520datasets%2520show%2520that%2520our%2520metric-specific%2520approach%2520significantly%2520improves%2520parity-based%2520fairness%2520criteria%2520and%2520outperforms%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10624v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Configurable%20Fairness%3A%20Direct%20Optimization%20of%20Parity%20Metrics%20via%20Vision-Language%20Models&entry.906535625=Miao%20Zhang%20and%20Rumi%20Chunara&entry.1292438233=Performance%20disparities%20of%20image%20recognition%20across%20demographic%20groups%20are%20known%20to%20exist%20in%20deep%20learning-based%20models%2C%20due%20to%20imbalanced%20group%20representations%20or%20spurious%20correlation%20between%20group%20and%20target%20labels.%20Previous%20work%20has%20addressed%20such%20challenges%20without%20relying%20on%20expensive%20group%20labels%2C%20typically%20by%20upweighting%20high-loss%20samples%20or%20balancing%20discovered%20clusters.%20However%2C%20these%20heuristic%20strategies%20lack%20direct%20connection%20to%20specific%20fairness%20metrics%20and%20cannot%20guarantee%20optimization%20of%20parity-based%20criteria%20like%20equal%20opportunity%2C%20which%20ensures%20equal%20chance%20to%20receive%20positive%20outcomes%20across%20groups.%20In%20this%20work%2C%20we%20propose%20a%20novel%20paradigm%20that%20directly%20optimizes%20parity-based%20fairness%20metrics%20through%20specifically%20designed%20training%20objectives%2C%20without%20requiring%20group%20labels.%20We%20leverage%20vision-language%20models%20to%20analyze%20sensitive%20attribute%20relevancy%20for%20individual%20samples%2C%20then%20formulate%20loss%20functions%20that%20mathematically%20connect%20to%20each%20target%20fairness%20metric.%20This%20enables%20flexible%20optimization%20of%20different%20fairness%20criteria%20based%20on%20application%20needs.%20Experiments%20on%20multiple%20image%20classification%20datasets%20show%20that%20our%20metric-specific%20approach%20significantly%20improves%20parity-based%20fairness%20criteria%20and%20outperforms%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2403.10624v3&entry.124074799=Read"},
{"title": "MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation", "author": "Mahdi Rahmani and AmirHossein Saffari and Reyhane Rahmani", "abstract": "Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet", "link": "http://arxiv.org/abs/2511.23397v1", "date": "2025-11-28", "relevancy": 2.1596, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4374}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4333}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaChat%3A%20A%20Synthetic%20Persian%20Q%26A%20Dataset%20for%20High-Quality%20Sales%20Chatbot%20Evaluation&body=Title%3A%20MegaChat%3A%20A%20Synthetic%20Persian%20Q%26A%20Dataset%20for%20High-Quality%20Sales%20Chatbot%20Evaluation%0AAuthor%3A%20Mahdi%20Rahmani%20and%20AmirHossein%20Saffari%20and%20Reyhane%20Rahmani%0AAbstract%3A%20Small%20and%20medium-sized%20enterprises%20%28SMEs%29%20in%20Iran%20increasingly%20leverage%20Telegram%20for%20sales%2C%20where%20real-time%20engagement%20is%20essential%20for%20conversion.%20However%2C%20developing%20AI-driven%20chatbots%20for%20this%20purpose%20requires%20large%2C%20high-quality%20question-and-answer%20%28Q%26A%29%20datasets%2C%20which%20are%20typically%20expensive%20and%20resource-intensive%20to%20produce%2C%20especially%20for%20low-resource%20languages%20like%20Persian.%20In%20this%20paper%2C%20we%20introduce%20MegaChat%2C%20the%20first%20fully%20synthetic%20Persian%20Q%26A%20dataset%20designed%20to%20evaluate%20intelligent%20sales%20chatbots%20in%20Telegram-based%20e-commerce.%20We%20propose%20a%20novel%2C%20automated%20multi-agent%20architecture%20that%20generates%20persona-aware%20Q%26A%20pairs%20by%20collecting%20data%20from%20active%20Telegram%20shopping%20channels.%20The%20system%20employs%20specialized%20agents%20for%20question%20generation%2C%20validation%2C%20and%20refinement%2C%20ensuring%20the%20production%20of%20realistic%20and%20diverse%20conversational%20data.%20To%20evaluate%20answer%20generation%2C%20we%20compare%20three%20classic%20retrieval-augmented%20generation%20%28RAG%29%20models%20with%20our%20advanced%20agentic%20system%2C%20which%20features%20multi-query%20retrieval%2C%20reranking%2C%20and%20persona-aligned%20response%20synthesis.%20Using%20GPT-5.1%20for%20evaluation%20across%20six%20quality%20dimensions%2C%20our%20results%20show%20that%20the%20agentic%20architecture%20outperformed%20traditional%20RAG%20models%20in%204%20out%20of%205%20diverse%20channels%2C%20demonstrating%20its%20ability%20to%20generate%20scalable%2C%20high-quality%20datasets%20without%20relying%20on%20expensive%20human%20annotation%20or%20complex%20fine-tuning.%20MegaChat%20provides%20SMEs%20with%20an%20efficient%2C%20cost-effective%20solution%20for%20building%20intelligent%20customer%20engagement%20systems%20in%20specialized%20commercial%20domains%2C%20enabling%20advancements%20in%20multilingual%20conversational%20AI%20for%20low-resource%20languages.%20Download%3A%20https%3A//github.com/MegaChat-Tech/MegaChat-DataSet%0ALink%3A%20http%3A//arxiv.org/abs/2511.23397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaChat%253A%2520A%2520Synthetic%2520Persian%2520Q%2526A%2520Dataset%2520for%2520High-Quality%2520Sales%2520Chatbot%2520Evaluation%26entry.906535625%3DMahdi%2520Rahmani%2520and%2520AmirHossein%2520Saffari%2520and%2520Reyhane%2520Rahmani%26entry.1292438233%3DSmall%2520and%2520medium-sized%2520enterprises%2520%2528SMEs%2529%2520in%2520Iran%2520increasingly%2520leverage%2520Telegram%2520for%2520sales%252C%2520where%2520real-time%2520engagement%2520is%2520essential%2520for%2520conversion.%2520However%252C%2520developing%2520AI-driven%2520chatbots%2520for%2520this%2520purpose%2520requires%2520large%252C%2520high-quality%2520question-and-answer%2520%2528Q%2526A%2529%2520datasets%252C%2520which%2520are%2520typically%2520expensive%2520and%2520resource-intensive%2520to%2520produce%252C%2520especially%2520for%2520low-resource%2520languages%2520like%2520Persian.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MegaChat%252C%2520the%2520first%2520fully%2520synthetic%2520Persian%2520Q%2526A%2520dataset%2520designed%2520to%2520evaluate%2520intelligent%2520sales%2520chatbots%2520in%2520Telegram-based%2520e-commerce.%2520We%2520propose%2520a%2520novel%252C%2520automated%2520multi-agent%2520architecture%2520that%2520generates%2520persona-aware%2520Q%2526A%2520pairs%2520by%2520collecting%2520data%2520from%2520active%2520Telegram%2520shopping%2520channels.%2520The%2520system%2520employs%2520specialized%2520agents%2520for%2520question%2520generation%252C%2520validation%252C%2520and%2520refinement%252C%2520ensuring%2520the%2520production%2520of%2520realistic%2520and%2520diverse%2520conversational%2520data.%2520To%2520evaluate%2520answer%2520generation%252C%2520we%2520compare%2520three%2520classic%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520models%2520with%2520our%2520advanced%2520agentic%2520system%252C%2520which%2520features%2520multi-query%2520retrieval%252C%2520reranking%252C%2520and%2520persona-aligned%2520response%2520synthesis.%2520Using%2520GPT-5.1%2520for%2520evaluation%2520across%2520six%2520quality%2520dimensions%252C%2520our%2520results%2520show%2520that%2520the%2520agentic%2520architecture%2520outperformed%2520traditional%2520RAG%2520models%2520in%25204%2520out%2520of%25205%2520diverse%2520channels%252C%2520demonstrating%2520its%2520ability%2520to%2520generate%2520scalable%252C%2520high-quality%2520datasets%2520without%2520relying%2520on%2520expensive%2520human%2520annotation%2520or%2520complex%2520fine-tuning.%2520MegaChat%2520provides%2520SMEs%2520with%2520an%2520efficient%252C%2520cost-effective%2520solution%2520for%2520building%2520intelligent%2520customer%2520engagement%2520systems%2520in%2520specialized%2520commercial%2520domains%252C%2520enabling%2520advancements%2520in%2520multilingual%2520conversational%2520AI%2520for%2520low-resource%2520languages.%2520Download%253A%2520https%253A//github.com/MegaChat-Tech/MegaChat-DataSet%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaChat%3A%20A%20Synthetic%20Persian%20Q%26A%20Dataset%20for%20High-Quality%20Sales%20Chatbot%20Evaluation&entry.906535625=Mahdi%20Rahmani%20and%20AmirHossein%20Saffari%20and%20Reyhane%20Rahmani&entry.1292438233=Small%20and%20medium-sized%20enterprises%20%28SMEs%29%20in%20Iran%20increasingly%20leverage%20Telegram%20for%20sales%2C%20where%20real-time%20engagement%20is%20essential%20for%20conversion.%20However%2C%20developing%20AI-driven%20chatbots%20for%20this%20purpose%20requires%20large%2C%20high-quality%20question-and-answer%20%28Q%26A%29%20datasets%2C%20which%20are%20typically%20expensive%20and%20resource-intensive%20to%20produce%2C%20especially%20for%20low-resource%20languages%20like%20Persian.%20In%20this%20paper%2C%20we%20introduce%20MegaChat%2C%20the%20first%20fully%20synthetic%20Persian%20Q%26A%20dataset%20designed%20to%20evaluate%20intelligent%20sales%20chatbots%20in%20Telegram-based%20e-commerce.%20We%20propose%20a%20novel%2C%20automated%20multi-agent%20architecture%20that%20generates%20persona-aware%20Q%26A%20pairs%20by%20collecting%20data%20from%20active%20Telegram%20shopping%20channels.%20The%20system%20employs%20specialized%20agents%20for%20question%20generation%2C%20validation%2C%20and%20refinement%2C%20ensuring%20the%20production%20of%20realistic%20and%20diverse%20conversational%20data.%20To%20evaluate%20answer%20generation%2C%20we%20compare%20three%20classic%20retrieval-augmented%20generation%20%28RAG%29%20models%20with%20our%20advanced%20agentic%20system%2C%20which%20features%20multi-query%20retrieval%2C%20reranking%2C%20and%20persona-aligned%20response%20synthesis.%20Using%20GPT-5.1%20for%20evaluation%20across%20six%20quality%20dimensions%2C%20our%20results%20show%20that%20the%20agentic%20architecture%20outperformed%20traditional%20RAG%20models%20in%204%20out%20of%205%20diverse%20channels%2C%20demonstrating%20its%20ability%20to%20generate%20scalable%2C%20high-quality%20datasets%20without%20relying%20on%20expensive%20human%20annotation%20or%20complex%20fine-tuning.%20MegaChat%20provides%20SMEs%20with%20an%20efficient%2C%20cost-effective%20solution%20for%20building%20intelligent%20customer%20engagement%20systems%20in%20specialized%20commercial%20domains%2C%20enabling%20advancements%20in%20multilingual%20conversational%20AI%20for%20low-resource%20languages.%20Download%3A%20https%3A//github.com/MegaChat-Tech/MegaChat-DataSet&entry.1838667208=http%3A//arxiv.org/abs/2511.23397v1&entry.124074799=Read"},
{"title": "DNA-Prior: Unsupervised Denoise Anything via Dual-Domain Prior", "author": "Yanqi Cheng and Chun-Wun Cheng and Jim Denholm and Thiago Lima and Javier A. Montoya-Zegarra and Richard Goodwin and Carola-Bibiane Sch\u00f6nlieb and Angelica I Aviles-Rivero", "abstract": "Medical imaging pipelines critically rely on robust denoising to stabilise downstream tasks such as segmentation and reconstruction. However, many existing denoisers depend on large annotated datasets or supervised learning, which restricts their usability in clinical environments with heterogeneous modalities and limited ground-truth data. To address this limitation, we introduce DNA-Prior, a universal unsupervised denoising framework that reconstructs clean images directly from corrupted observations through a mathematically principled hybrid prior. DNA-Prior integrates (i) an implicit architectural prior, enforced through a deep network parameterisation, with (ii) an explicit spectral-spatial prior composed of a frequency-domain fidelity term and a spatial regularisation functional. This dual-domain formulation yields a well-structured optimisation problem that jointly preserves global frequency characteristics and local anatomical structure, without requiring any external training data or modality-specific tuning. Experiments across multiple modalities show that DNA achieves consistent noise suppression and structural preservation under diverse noise conditions.", "link": "http://arxiv.org/abs/2511.23124v1", "date": "2025-11-28", "relevancy": 2.1489, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5645}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5226}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNA-Prior%3A%20Unsupervised%20Denoise%20Anything%20via%20Dual-Domain%20Prior&body=Title%3A%20DNA-Prior%3A%20Unsupervised%20Denoise%20Anything%20via%20Dual-Domain%20Prior%0AAuthor%3A%20Yanqi%20Cheng%20and%20Chun-Wun%20Cheng%20and%20Jim%20Denholm%20and%20Thiago%20Lima%20and%20Javier%20A.%20Montoya-Zegarra%20and%20Richard%20Goodwin%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20I%20Aviles-Rivero%0AAbstract%3A%20Medical%20imaging%20pipelines%20critically%20rely%20on%20robust%20denoising%20to%20stabilise%20downstream%20tasks%20such%20as%20segmentation%20and%20reconstruction.%20However%2C%20many%20existing%20denoisers%20depend%20on%20large%20annotated%20datasets%20or%20supervised%20learning%2C%20which%20restricts%20their%20usability%20in%20clinical%20environments%20with%20heterogeneous%20modalities%20and%20limited%20ground-truth%20data.%20To%20address%20this%20limitation%2C%20we%20introduce%20DNA-Prior%2C%20a%20universal%20unsupervised%20denoising%20framework%20that%20reconstructs%20clean%20images%20directly%20from%20corrupted%20observations%20through%20a%20mathematically%20principled%20hybrid%20prior.%20DNA-Prior%20integrates%20%28i%29%20an%20implicit%20architectural%20prior%2C%20enforced%20through%20a%20deep%20network%20parameterisation%2C%20with%20%28ii%29%20an%20explicit%20spectral-spatial%20prior%20composed%20of%20a%20frequency-domain%20fidelity%20term%20and%20a%20spatial%20regularisation%20functional.%20This%20dual-domain%20formulation%20yields%20a%20well-structured%20optimisation%20problem%20that%20jointly%20preserves%20global%20frequency%20characteristics%20and%20local%20anatomical%20structure%2C%20without%20requiring%20any%20external%20training%20data%20or%20modality-specific%20tuning.%20Experiments%20across%20multiple%20modalities%20show%20that%20DNA%20achieves%20consistent%20noise%20suppression%20and%20structural%20preservation%20under%20diverse%20noise%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNA-Prior%253A%2520Unsupervised%2520Denoise%2520Anything%2520via%2520Dual-Domain%2520Prior%26entry.906535625%3DYanqi%2520Cheng%2520and%2520Chun-Wun%2520Cheng%2520and%2520Jim%2520Denholm%2520and%2520Thiago%2520Lima%2520and%2520Javier%2520A.%2520Montoya-Zegarra%2520and%2520Richard%2520Goodwin%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Angelica%2520I%2520Aviles-Rivero%26entry.1292438233%3DMedical%2520imaging%2520pipelines%2520critically%2520rely%2520on%2520robust%2520denoising%2520to%2520stabilise%2520downstream%2520tasks%2520such%2520as%2520segmentation%2520and%2520reconstruction.%2520However%252C%2520many%2520existing%2520denoisers%2520depend%2520on%2520large%2520annotated%2520datasets%2520or%2520supervised%2520learning%252C%2520which%2520restricts%2520their%2520usability%2520in%2520clinical%2520environments%2520with%2520heterogeneous%2520modalities%2520and%2520limited%2520ground-truth%2520data.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520DNA-Prior%252C%2520a%2520universal%2520unsupervised%2520denoising%2520framework%2520that%2520reconstructs%2520clean%2520images%2520directly%2520from%2520corrupted%2520observations%2520through%2520a%2520mathematically%2520principled%2520hybrid%2520prior.%2520DNA-Prior%2520integrates%2520%2528i%2529%2520an%2520implicit%2520architectural%2520prior%252C%2520enforced%2520through%2520a%2520deep%2520network%2520parameterisation%252C%2520with%2520%2528ii%2529%2520an%2520explicit%2520spectral-spatial%2520prior%2520composed%2520of%2520a%2520frequency-domain%2520fidelity%2520term%2520and%2520a%2520spatial%2520regularisation%2520functional.%2520This%2520dual-domain%2520formulation%2520yields%2520a%2520well-structured%2520optimisation%2520problem%2520that%2520jointly%2520preserves%2520global%2520frequency%2520characteristics%2520and%2520local%2520anatomical%2520structure%252C%2520without%2520requiring%2520any%2520external%2520training%2520data%2520or%2520modality-specific%2520tuning.%2520Experiments%2520across%2520multiple%2520modalities%2520show%2520that%2520DNA%2520achieves%2520consistent%2520noise%2520suppression%2520and%2520structural%2520preservation%2520under%2520diverse%2520noise%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNA-Prior%3A%20Unsupervised%20Denoise%20Anything%20via%20Dual-Domain%20Prior&entry.906535625=Yanqi%20Cheng%20and%20Chun-Wun%20Cheng%20and%20Jim%20Denholm%20and%20Thiago%20Lima%20and%20Javier%20A.%20Montoya-Zegarra%20and%20Richard%20Goodwin%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20I%20Aviles-Rivero&entry.1292438233=Medical%20imaging%20pipelines%20critically%20rely%20on%20robust%20denoising%20to%20stabilise%20downstream%20tasks%20such%20as%20segmentation%20and%20reconstruction.%20However%2C%20many%20existing%20denoisers%20depend%20on%20large%20annotated%20datasets%20or%20supervised%20learning%2C%20which%20restricts%20their%20usability%20in%20clinical%20environments%20with%20heterogeneous%20modalities%20and%20limited%20ground-truth%20data.%20To%20address%20this%20limitation%2C%20we%20introduce%20DNA-Prior%2C%20a%20universal%20unsupervised%20denoising%20framework%20that%20reconstructs%20clean%20images%20directly%20from%20corrupted%20observations%20through%20a%20mathematically%20principled%20hybrid%20prior.%20DNA-Prior%20integrates%20%28i%29%20an%20implicit%20architectural%20prior%2C%20enforced%20through%20a%20deep%20network%20parameterisation%2C%20with%20%28ii%29%20an%20explicit%20spectral-spatial%20prior%20composed%20of%20a%20frequency-domain%20fidelity%20term%20and%20a%20spatial%20regularisation%20functional.%20This%20dual-domain%20formulation%20yields%20a%20well-structured%20optimisation%20problem%20that%20jointly%20preserves%20global%20frequency%20characteristics%20and%20local%20anatomical%20structure%2C%20without%20requiring%20any%20external%20training%20data%20or%20modality-specific%20tuning.%20Experiments%20across%20multiple%20modalities%20show%20that%20DNA%20achieves%20consistent%20noise%20suppression%20and%20structural%20preservation%20under%20diverse%20noise%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2511.23124v1&entry.124074799=Read"},
{"title": "Cascaded Robust Rectification for Arbitrary Document Images", "author": "Chaoyun Wang and Quanxin Huang and I-Chao Shen and Takeo Igarashi and Nanning Zheng and Caigui Jiang", "abstract": "Document rectification in real-world scenarios poses significant challenges due to extreme variations in camera perspectives and physical distortions. Driven by the insight that complex transformations can be decomposed and resolved progressively, we introduce a novel multi-stage framework that progressively reverses distinct distortion types in a coarse-to-fine manner. Specifically, our framework first performs a global affine transformation to correct perspective distortions arising from the camera's viewpoint, then rectifies geometric deformations resulting from physical paper curling and folding, and finally employs a content-aware iterative process to eliminate fine-grained content distortions. To address limitations in existing evaluation protocols, we also propose two enhanced metrics: layout-aligned OCR metrics (AED/ACER) for a stable assessment that decouples geometric rectification quality from the layout analysis errors of OCR engines, and masked AD/AAD (AD-M/AAD-M) tailored for accurately evaluating geometric distortions in documents with incomplete boundaries. Extensive experiments show that our method establishes new state-of-the-art performance on multiple challenging benchmarks, yielding a substantial reduction of 14.1\\%--34.7\\% in the AAD metric and demonstrating superior efficacy in real-world applications. The code will be publicly available at https://github.com/chaoyunwang/ArbDR.", "link": "http://arxiv.org/abs/2511.23150v1", "date": "2025-11-28", "relevancy": 2.1335, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5595}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5389}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascaded%20Robust%20Rectification%20for%20Arbitrary%20Document%20Images&body=Title%3A%20Cascaded%20Robust%20Rectification%20for%20Arbitrary%20Document%20Images%0AAuthor%3A%20Chaoyun%20Wang%20and%20Quanxin%20Huang%20and%20I-Chao%20Shen%20and%20Takeo%20Igarashi%20and%20Nanning%20Zheng%20and%20Caigui%20Jiang%0AAbstract%3A%20Document%20rectification%20in%20real-world%20scenarios%20poses%20significant%20challenges%20due%20to%20extreme%20variations%20in%20camera%20perspectives%20and%20physical%20distortions.%20Driven%20by%20the%20insight%20that%20complex%20transformations%20can%20be%20decomposed%20and%20resolved%20progressively%2C%20we%20introduce%20a%20novel%20multi-stage%20framework%20that%20progressively%20reverses%20distinct%20distortion%20types%20in%20a%20coarse-to-fine%20manner.%20Specifically%2C%20our%20framework%20first%20performs%20a%20global%20affine%20transformation%20to%20correct%20perspective%20distortions%20arising%20from%20the%20camera%27s%20viewpoint%2C%20then%20rectifies%20geometric%20deformations%20resulting%20from%20physical%20paper%20curling%20and%20folding%2C%20and%20finally%20employs%20a%20content-aware%20iterative%20process%20to%20eliminate%20fine-grained%20content%20distortions.%20To%20address%20limitations%20in%20existing%20evaluation%20protocols%2C%20we%20also%20propose%20two%20enhanced%20metrics%3A%20layout-aligned%20OCR%20metrics%20%28AED/ACER%29%20for%20a%20stable%20assessment%20that%20decouples%20geometric%20rectification%20quality%20from%20the%20layout%20analysis%20errors%20of%20OCR%20engines%2C%20and%20masked%20AD/AAD%20%28AD-M/AAD-M%29%20tailored%20for%20accurately%20evaluating%20geometric%20distortions%20in%20documents%20with%20incomplete%20boundaries.%20Extensive%20experiments%20show%20that%20our%20method%20establishes%20new%20state-of-the-art%20performance%20on%20multiple%20challenging%20benchmarks%2C%20yielding%20a%20substantial%20reduction%20of%2014.1%5C%25--34.7%5C%25%20in%20the%20AAD%20metric%20and%20demonstrating%20superior%20efficacy%20in%20real-world%20applications.%20The%20code%20will%20be%20publicly%20available%20at%20https%3A//github.com/chaoyunwang/ArbDR.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascaded%2520Robust%2520Rectification%2520for%2520Arbitrary%2520Document%2520Images%26entry.906535625%3DChaoyun%2520Wang%2520and%2520Quanxin%2520Huang%2520and%2520I-Chao%2520Shen%2520and%2520Takeo%2520Igarashi%2520and%2520Nanning%2520Zheng%2520and%2520Caigui%2520Jiang%26entry.1292438233%3DDocument%2520rectification%2520in%2520real-world%2520scenarios%2520poses%2520significant%2520challenges%2520due%2520to%2520extreme%2520variations%2520in%2520camera%2520perspectives%2520and%2520physical%2520distortions.%2520Driven%2520by%2520the%2520insight%2520that%2520complex%2520transformations%2520can%2520be%2520decomposed%2520and%2520resolved%2520progressively%252C%2520we%2520introduce%2520a%2520novel%2520multi-stage%2520framework%2520that%2520progressively%2520reverses%2520distinct%2520distortion%2520types%2520in%2520a%2520coarse-to-fine%2520manner.%2520Specifically%252C%2520our%2520framework%2520first%2520performs%2520a%2520global%2520affine%2520transformation%2520to%2520correct%2520perspective%2520distortions%2520arising%2520from%2520the%2520camera%2527s%2520viewpoint%252C%2520then%2520rectifies%2520geometric%2520deformations%2520resulting%2520from%2520physical%2520paper%2520curling%2520and%2520folding%252C%2520and%2520finally%2520employs%2520a%2520content-aware%2520iterative%2520process%2520to%2520eliminate%2520fine-grained%2520content%2520distortions.%2520To%2520address%2520limitations%2520in%2520existing%2520evaluation%2520protocols%252C%2520we%2520also%2520propose%2520two%2520enhanced%2520metrics%253A%2520layout-aligned%2520OCR%2520metrics%2520%2528AED/ACER%2529%2520for%2520a%2520stable%2520assessment%2520that%2520decouples%2520geometric%2520rectification%2520quality%2520from%2520the%2520layout%2520analysis%2520errors%2520of%2520OCR%2520engines%252C%2520and%2520masked%2520AD/AAD%2520%2528AD-M/AAD-M%2529%2520tailored%2520for%2520accurately%2520evaluating%2520geometric%2520distortions%2520in%2520documents%2520with%2520incomplete%2520boundaries.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520establishes%2520new%2520state-of-the-art%2520performance%2520on%2520multiple%2520challenging%2520benchmarks%252C%2520yielding%2520a%2520substantial%2520reduction%2520of%252014.1%255C%2525--34.7%255C%2525%2520in%2520the%2520AAD%2520metric%2520and%2520demonstrating%2520superior%2520efficacy%2520in%2520real-world%2520applications.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/chaoyunwang/ArbDR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascaded%20Robust%20Rectification%20for%20Arbitrary%20Document%20Images&entry.906535625=Chaoyun%20Wang%20and%20Quanxin%20Huang%20and%20I-Chao%20Shen%20and%20Takeo%20Igarashi%20and%20Nanning%20Zheng%20and%20Caigui%20Jiang&entry.1292438233=Document%20rectification%20in%20real-world%20scenarios%20poses%20significant%20challenges%20due%20to%20extreme%20variations%20in%20camera%20perspectives%20and%20physical%20distortions.%20Driven%20by%20the%20insight%20that%20complex%20transformations%20can%20be%20decomposed%20and%20resolved%20progressively%2C%20we%20introduce%20a%20novel%20multi-stage%20framework%20that%20progressively%20reverses%20distinct%20distortion%20types%20in%20a%20coarse-to-fine%20manner.%20Specifically%2C%20our%20framework%20first%20performs%20a%20global%20affine%20transformation%20to%20correct%20perspective%20distortions%20arising%20from%20the%20camera%27s%20viewpoint%2C%20then%20rectifies%20geometric%20deformations%20resulting%20from%20physical%20paper%20curling%20and%20folding%2C%20and%20finally%20employs%20a%20content-aware%20iterative%20process%20to%20eliminate%20fine-grained%20content%20distortions.%20To%20address%20limitations%20in%20existing%20evaluation%20protocols%2C%20we%20also%20propose%20two%20enhanced%20metrics%3A%20layout-aligned%20OCR%20metrics%20%28AED/ACER%29%20for%20a%20stable%20assessment%20that%20decouples%20geometric%20rectification%20quality%20from%20the%20layout%20analysis%20errors%20of%20OCR%20engines%2C%20and%20masked%20AD/AAD%20%28AD-M/AAD-M%29%20tailored%20for%20accurately%20evaluating%20geometric%20distortions%20in%20documents%20with%20incomplete%20boundaries.%20Extensive%20experiments%20show%20that%20our%20method%20establishes%20new%20state-of-the-art%20performance%20on%20multiple%20challenging%20benchmarks%2C%20yielding%20a%20substantial%20reduction%20of%2014.1%5C%25--34.7%5C%25%20in%20the%20AAD%20metric%20and%20demonstrating%20superior%20efficacy%20in%20real-world%20applications.%20The%20code%20will%20be%20publicly%20available%20at%20https%3A//github.com/chaoyunwang/ArbDR.&entry.1838667208=http%3A//arxiv.org/abs/2511.23150v1&entry.124074799=Read"},
{"title": "Network Inversion for Uncertainty-Aware Out-of-Distribution Detection", "author": "Pirzada Suhail and Rehna Afroz and Gouranga Bala and Amit Sethi", "abstract": "Out-of-distribution (OOD) detection and uncertainty estimation (UE) are critical components for building safe machine learning systems, especially in real-world scenarios where unexpected inputs are inevitable. However the two problems have, until recently, separately been addressed. In this work, we propose a novel framework that combines network inversion with classifier training to simultaneously address both OOD detection and uncertainty estimation. For a standard n-class classification task, we extend the classifier to an (n+1)-class model by introducing a \"garbage\" class, initially populated with random gaussian noise to represent outlier inputs. After each training epoch, we use network inversion to reconstruct input images corresponding to all output classes that initially appear as noisy and incoherent and are therefore excluded to the garbage class for retraining the classifier. This cycle of training, inversion, and exclusion continues iteratively till the inverted samples begin to resemble the in-distribution data more closely, with a significant drop in the uncertainty, suggesting that the classifier has learned to carve out meaningful decision boundaries while sanitising the class manifolds by pushing OOD content into the garbage class. During inference, this training scheme enables the model to effectively detect and reject OOD samples by classifying them into the garbage class. Furthermore, the confidence scores associated with each prediction can be used to estimate uncertainty for both in-distribution and OOD inputs. Our approach is scalable, interpretable, and does not require access to external OOD datasets or post-hoc calibration techniques while providing a unified solution to the dual challenges of OOD detection and uncertainty estimation.", "link": "http://arxiv.org/abs/2505.23448v2", "date": "2025-11-28", "relevancy": 2.129, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.549}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5369}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Network%20Inversion%20for%20Uncertainty-Aware%20Out-of-Distribution%20Detection&body=Title%3A%20Network%20Inversion%20for%20Uncertainty-Aware%20Out-of-Distribution%20Detection%0AAuthor%3A%20Pirzada%20Suhail%20and%20Rehna%20Afroz%20and%20Gouranga%20Bala%20and%20Amit%20Sethi%0AAbstract%3A%20Out-of-distribution%20%28OOD%29%20detection%20and%20uncertainty%20estimation%20%28UE%29%20are%20critical%20components%20for%20building%20safe%20machine%20learning%20systems%2C%20especially%20in%20real-world%20scenarios%20where%20unexpected%20inputs%20are%20inevitable.%20However%20the%20two%20problems%20have%2C%20until%20recently%2C%20separately%20been%20addressed.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20combines%20network%20inversion%20with%20classifier%20training%20to%20simultaneously%20address%20both%20OOD%20detection%20and%20uncertainty%20estimation.%20For%20a%20standard%20n-class%20classification%20task%2C%20we%20extend%20the%20classifier%20to%20an%20%28n%2B1%29-class%20model%20by%20introducing%20a%20%22garbage%22%20class%2C%20initially%20populated%20with%20random%20gaussian%20noise%20to%20represent%20outlier%20inputs.%20After%20each%20training%20epoch%2C%20we%20use%20network%20inversion%20to%20reconstruct%20input%20images%20corresponding%20to%20all%20output%20classes%20that%20initially%20appear%20as%20noisy%20and%20incoherent%20and%20are%20therefore%20excluded%20to%20the%20garbage%20class%20for%20retraining%20the%20classifier.%20This%20cycle%20of%20training%2C%20inversion%2C%20and%20exclusion%20continues%20iteratively%20till%20the%20inverted%20samples%20begin%20to%20resemble%20the%20in-distribution%20data%20more%20closely%2C%20with%20a%20significant%20drop%20in%20the%20uncertainty%2C%20suggesting%20that%20the%20classifier%20has%20learned%20to%20carve%20out%20meaningful%20decision%20boundaries%20while%20sanitising%20the%20class%20manifolds%20by%20pushing%20OOD%20content%20into%20the%20garbage%20class.%20During%20inference%2C%20this%20training%20scheme%20enables%20the%20model%20to%20effectively%20detect%20and%20reject%20OOD%20samples%20by%20classifying%20them%20into%20the%20garbage%20class.%20Furthermore%2C%20the%20confidence%20scores%20associated%20with%20each%20prediction%20can%20be%20used%20to%20estimate%20uncertainty%20for%20both%20in-distribution%20and%20OOD%20inputs.%20Our%20approach%20is%20scalable%2C%20interpretable%2C%20and%20does%20not%20require%20access%20to%20external%20OOD%20datasets%20or%20post-hoc%20calibration%20techniques%20while%20providing%20a%20unified%20solution%20to%20the%20dual%20challenges%20of%20OOD%20detection%20and%20uncertainty%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23448v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetwork%2520Inversion%2520for%2520Uncertainty-Aware%2520Out-of-Distribution%2520Detection%26entry.906535625%3DPirzada%2520Suhail%2520and%2520Rehna%2520Afroz%2520and%2520Gouranga%2520Bala%2520and%2520Amit%2520Sethi%26entry.1292438233%3DOut-of-distribution%2520%2528OOD%2529%2520detection%2520and%2520uncertainty%2520estimation%2520%2528UE%2529%2520are%2520critical%2520components%2520for%2520building%2520safe%2520machine%2520learning%2520systems%252C%2520especially%2520in%2520real-world%2520scenarios%2520where%2520unexpected%2520inputs%2520are%2520inevitable.%2520However%2520the%2520two%2520problems%2520have%252C%2520until%2520recently%252C%2520separately%2520been%2520addressed.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520combines%2520network%2520inversion%2520with%2520classifier%2520training%2520to%2520simultaneously%2520address%2520both%2520OOD%2520detection%2520and%2520uncertainty%2520estimation.%2520For%2520a%2520standard%2520n-class%2520classification%2520task%252C%2520we%2520extend%2520the%2520classifier%2520to%2520an%2520%2528n%252B1%2529-class%2520model%2520by%2520introducing%2520a%2520%2522garbage%2522%2520class%252C%2520initially%2520populated%2520with%2520random%2520gaussian%2520noise%2520to%2520represent%2520outlier%2520inputs.%2520After%2520each%2520training%2520epoch%252C%2520we%2520use%2520network%2520inversion%2520to%2520reconstruct%2520input%2520images%2520corresponding%2520to%2520all%2520output%2520classes%2520that%2520initially%2520appear%2520as%2520noisy%2520and%2520incoherent%2520and%2520are%2520therefore%2520excluded%2520to%2520the%2520garbage%2520class%2520for%2520retraining%2520the%2520classifier.%2520This%2520cycle%2520of%2520training%252C%2520inversion%252C%2520and%2520exclusion%2520continues%2520iteratively%2520till%2520the%2520inverted%2520samples%2520begin%2520to%2520resemble%2520the%2520in-distribution%2520data%2520more%2520closely%252C%2520with%2520a%2520significant%2520drop%2520in%2520the%2520uncertainty%252C%2520suggesting%2520that%2520the%2520classifier%2520has%2520learned%2520to%2520carve%2520out%2520meaningful%2520decision%2520boundaries%2520while%2520sanitising%2520the%2520class%2520manifolds%2520by%2520pushing%2520OOD%2520content%2520into%2520the%2520garbage%2520class.%2520During%2520inference%252C%2520this%2520training%2520scheme%2520enables%2520the%2520model%2520to%2520effectively%2520detect%2520and%2520reject%2520OOD%2520samples%2520by%2520classifying%2520them%2520into%2520the%2520garbage%2520class.%2520Furthermore%252C%2520the%2520confidence%2520scores%2520associated%2520with%2520each%2520prediction%2520can%2520be%2520used%2520to%2520estimate%2520uncertainty%2520for%2520both%2520in-distribution%2520and%2520OOD%2520inputs.%2520Our%2520approach%2520is%2520scalable%252C%2520interpretable%252C%2520and%2520does%2520not%2520require%2520access%2520to%2520external%2520OOD%2520datasets%2520or%2520post-hoc%2520calibration%2520techniques%2520while%2520providing%2520a%2520unified%2520solution%2520to%2520the%2520dual%2520challenges%2520of%2520OOD%2520detection%2520and%2520uncertainty%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23448v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network%20Inversion%20for%20Uncertainty-Aware%20Out-of-Distribution%20Detection&entry.906535625=Pirzada%20Suhail%20and%20Rehna%20Afroz%20and%20Gouranga%20Bala%20and%20Amit%20Sethi&entry.1292438233=Out-of-distribution%20%28OOD%29%20detection%20and%20uncertainty%20estimation%20%28UE%29%20are%20critical%20components%20for%20building%20safe%20machine%20learning%20systems%2C%20especially%20in%20real-world%20scenarios%20where%20unexpected%20inputs%20are%20inevitable.%20However%20the%20two%20problems%20have%2C%20until%20recently%2C%20separately%20been%20addressed.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%20combines%20network%20inversion%20with%20classifier%20training%20to%20simultaneously%20address%20both%20OOD%20detection%20and%20uncertainty%20estimation.%20For%20a%20standard%20n-class%20classification%20task%2C%20we%20extend%20the%20classifier%20to%20an%20%28n%2B1%29-class%20model%20by%20introducing%20a%20%22garbage%22%20class%2C%20initially%20populated%20with%20random%20gaussian%20noise%20to%20represent%20outlier%20inputs.%20After%20each%20training%20epoch%2C%20we%20use%20network%20inversion%20to%20reconstruct%20input%20images%20corresponding%20to%20all%20output%20classes%20that%20initially%20appear%20as%20noisy%20and%20incoherent%20and%20are%20therefore%20excluded%20to%20the%20garbage%20class%20for%20retraining%20the%20classifier.%20This%20cycle%20of%20training%2C%20inversion%2C%20and%20exclusion%20continues%20iteratively%20till%20the%20inverted%20samples%20begin%20to%20resemble%20the%20in-distribution%20data%20more%20closely%2C%20with%20a%20significant%20drop%20in%20the%20uncertainty%2C%20suggesting%20that%20the%20classifier%20has%20learned%20to%20carve%20out%20meaningful%20decision%20boundaries%20while%20sanitising%20the%20class%20manifolds%20by%20pushing%20OOD%20content%20into%20the%20garbage%20class.%20During%20inference%2C%20this%20training%20scheme%20enables%20the%20model%20to%20effectively%20detect%20and%20reject%20OOD%20samples%20by%20classifying%20them%20into%20the%20garbage%20class.%20Furthermore%2C%20the%20confidence%20scores%20associated%20with%20each%20prediction%20can%20be%20used%20to%20estimate%20uncertainty%20for%20both%20in-distribution%20and%20OOD%20inputs.%20Our%20approach%20is%20scalable%2C%20interpretable%2C%20and%20does%20not%20require%20access%20to%20external%20OOD%20datasets%20or%20post-hoc%20calibration%20techniques%20while%20providing%20a%20unified%20solution%20to%20the%20dual%20challenges%20of%20OOD%20detection%20and%20uncertainty%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2505.23448v2&entry.124074799=Read"},
{"title": "Structured Cognitive Loop for Behavioral Intelligence in Large Language Model Agents", "author": "Myung Ho Kim", "abstract": "Large language models have advanced natural language understanding and generation, but their use as autonomous agents introduces architectural challenges for multi-step tasks. Existing frameworks often mix cognition, memory, and control in a single prompt, reducing coherence and predictability. The Structured Cognitive Loop (SCL) is proposed as an alternative architecture that separates these functions. In SCL, the language model handles cognition, memory is stored externally, and execution is guided by a lightweight controller within a goal-directed loop. This design allows intermediate results to be recorded and verified before actions are taken, improving traceability and evaluation. SCL is evaluated against prompt-based baselines such as ReAct and LangChain agents across three tasks: travel planning, conditional email drafting, and constraint-guided image generation. Under matched settings, SCL achieves an average task success rate of 86.3 percent, compared with 70.5 to 76.8 percent for baselines. It also shows higher goal fidelity, fewer redundant calls, and reduced unsupported assertions. These results indicate that separating cognition, memory, and control can enhance reliability and interpretability without relying on larger models or heavier prompts. The findings should be regarded as preliminary evidence, with broader tests across model families and task domains planned for future work.", "link": "http://arxiv.org/abs/2510.05107v4", "date": "2025-11-28", "relevancy": 2.1263, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Cognitive%20Loop%20for%20Behavioral%20Intelligence%20in%20Large%20Language%20Model%20Agents&body=Title%3A%20Structured%20Cognitive%20Loop%20for%20Behavioral%20Intelligence%20in%20Large%20Language%20Model%20Agents%0AAuthor%3A%20Myung%20Ho%20Kim%0AAbstract%3A%20Large%20language%20models%20have%20advanced%20natural%20language%20understanding%20and%20generation%2C%20but%20their%20use%20as%20autonomous%20agents%20introduces%20architectural%20challenges%20for%20multi-step%20tasks.%20Existing%20frameworks%20often%20mix%20cognition%2C%20memory%2C%20and%20control%20in%20a%20single%20prompt%2C%20reducing%20coherence%20and%20predictability.%20The%20Structured%20Cognitive%20Loop%20%28SCL%29%20is%20proposed%20as%20an%20alternative%20architecture%20that%20separates%20these%20functions.%20In%20SCL%2C%20the%20language%20model%20handles%20cognition%2C%20memory%20is%20stored%20externally%2C%20and%20execution%20is%20guided%20by%20a%20lightweight%20controller%20within%20a%20goal-directed%20loop.%20This%20design%20allows%20intermediate%20results%20to%20be%20recorded%20and%20verified%20before%20actions%20are%20taken%2C%20improving%20traceability%20and%20evaluation.%20SCL%20is%20evaluated%20against%20prompt-based%20baselines%20such%20as%20ReAct%20and%20LangChain%20agents%20across%20three%20tasks%3A%20travel%20planning%2C%20conditional%20email%20drafting%2C%20and%20constraint-guided%20image%20generation.%20Under%20matched%20settings%2C%20SCL%20achieves%20an%20average%20task%20success%20rate%20of%2086.3%20percent%2C%20compared%20with%2070.5%20to%2076.8%20percent%20for%20baselines.%20It%20also%20shows%20higher%20goal%20fidelity%2C%20fewer%20redundant%20calls%2C%20and%20reduced%20unsupported%20assertions.%20These%20results%20indicate%20that%20separating%20cognition%2C%20memory%2C%20and%20control%20can%20enhance%20reliability%20and%20interpretability%20without%20relying%20on%20larger%20models%20or%20heavier%20prompts.%20The%20findings%20should%20be%20regarded%20as%20preliminary%20evidence%2C%20with%20broader%20tests%20across%20model%20families%20and%20task%20domains%20planned%20for%20future%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2510.05107v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Cognitive%2520Loop%2520for%2520Behavioral%2520Intelligence%2520in%2520Large%2520Language%2520Model%2520Agents%26entry.906535625%3DMyung%2520Ho%2520Kim%26entry.1292438233%3DLarge%2520language%2520models%2520have%2520advanced%2520natural%2520language%2520understanding%2520and%2520generation%252C%2520but%2520their%2520use%2520as%2520autonomous%2520agents%2520introduces%2520architectural%2520challenges%2520for%2520multi-step%2520tasks.%2520Existing%2520frameworks%2520often%2520mix%2520cognition%252C%2520memory%252C%2520and%2520control%2520in%2520a%2520single%2520prompt%252C%2520reducing%2520coherence%2520and%2520predictability.%2520The%2520Structured%2520Cognitive%2520Loop%2520%2528SCL%2529%2520is%2520proposed%2520as%2520an%2520alternative%2520architecture%2520that%2520separates%2520these%2520functions.%2520In%2520SCL%252C%2520the%2520language%2520model%2520handles%2520cognition%252C%2520memory%2520is%2520stored%2520externally%252C%2520and%2520execution%2520is%2520guided%2520by%2520a%2520lightweight%2520controller%2520within%2520a%2520goal-directed%2520loop.%2520This%2520design%2520allows%2520intermediate%2520results%2520to%2520be%2520recorded%2520and%2520verified%2520before%2520actions%2520are%2520taken%252C%2520improving%2520traceability%2520and%2520evaluation.%2520SCL%2520is%2520evaluated%2520against%2520prompt-based%2520baselines%2520such%2520as%2520ReAct%2520and%2520LangChain%2520agents%2520across%2520three%2520tasks%253A%2520travel%2520planning%252C%2520conditional%2520email%2520drafting%252C%2520and%2520constraint-guided%2520image%2520generation.%2520Under%2520matched%2520settings%252C%2520SCL%2520achieves%2520an%2520average%2520task%2520success%2520rate%2520of%252086.3%2520percent%252C%2520compared%2520with%252070.5%2520to%252076.8%2520percent%2520for%2520baselines.%2520It%2520also%2520shows%2520higher%2520goal%2520fidelity%252C%2520fewer%2520redundant%2520calls%252C%2520and%2520reduced%2520unsupported%2520assertions.%2520These%2520results%2520indicate%2520that%2520separating%2520cognition%252C%2520memory%252C%2520and%2520control%2520can%2520enhance%2520reliability%2520and%2520interpretability%2520without%2520relying%2520on%2520larger%2520models%2520or%2520heavier%2520prompts.%2520The%2520findings%2520should%2520be%2520regarded%2520as%2520preliminary%2520evidence%252C%2520with%2520broader%2520tests%2520across%2520model%2520families%2520and%2520task%2520domains%2520planned%2520for%2520future%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05107v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Cognitive%20Loop%20for%20Behavioral%20Intelligence%20in%20Large%20Language%20Model%20Agents&entry.906535625=Myung%20Ho%20Kim&entry.1292438233=Large%20language%20models%20have%20advanced%20natural%20language%20understanding%20and%20generation%2C%20but%20their%20use%20as%20autonomous%20agents%20introduces%20architectural%20challenges%20for%20multi-step%20tasks.%20Existing%20frameworks%20often%20mix%20cognition%2C%20memory%2C%20and%20control%20in%20a%20single%20prompt%2C%20reducing%20coherence%20and%20predictability.%20The%20Structured%20Cognitive%20Loop%20%28SCL%29%20is%20proposed%20as%20an%20alternative%20architecture%20that%20separates%20these%20functions.%20In%20SCL%2C%20the%20language%20model%20handles%20cognition%2C%20memory%20is%20stored%20externally%2C%20and%20execution%20is%20guided%20by%20a%20lightweight%20controller%20within%20a%20goal-directed%20loop.%20This%20design%20allows%20intermediate%20results%20to%20be%20recorded%20and%20verified%20before%20actions%20are%20taken%2C%20improving%20traceability%20and%20evaluation.%20SCL%20is%20evaluated%20against%20prompt-based%20baselines%20such%20as%20ReAct%20and%20LangChain%20agents%20across%20three%20tasks%3A%20travel%20planning%2C%20conditional%20email%20drafting%2C%20and%20constraint-guided%20image%20generation.%20Under%20matched%20settings%2C%20SCL%20achieves%20an%20average%20task%20success%20rate%20of%2086.3%20percent%2C%20compared%20with%2070.5%20to%2076.8%20percent%20for%20baselines.%20It%20also%20shows%20higher%20goal%20fidelity%2C%20fewer%20redundant%20calls%2C%20and%20reduced%20unsupported%20assertions.%20These%20results%20indicate%20that%20separating%20cognition%2C%20memory%2C%20and%20control%20can%20enhance%20reliability%20and%20interpretability%20without%20relying%20on%20larger%20models%20or%20heavier%20prompts.%20The%20findings%20should%20be%20regarded%20as%20preliminary%20evidence%2C%20with%20broader%20tests%20across%20model%20families%20and%20task%20domains%20planned%20for%20future%20work.&entry.1838667208=http%3A//arxiv.org/abs/2510.05107v4&entry.124074799=Read"},
{"title": "Machine Learning for Scientific Visualization: Ensemble Data Analysis", "author": "Hamid Gadirov", "abstract": "Scientific simulations and experimental measurements produce vast amounts of spatio-temporal data, yet extracting meaningful insights remains challenging due to high dimensionality, complex structures, and missing information. Traditional analysis methods often struggle with these issues, motivating the need for more robust, data-driven approaches. This dissertation explores deep learning methodologies to improve the analysis and visualization of spatio-temporal scientific ensembles, focusing on dimensionality reduction, flow estimation, and temporal interpolation. First, we address high-dimensional data representation through autoencoder-based dimensionality reduction for scientific ensembles. We evaluate the stability of projection metrics under partial labeling and introduce a Pareto-efficient selection strategy to identify optimal autoencoder variants, ensuring expressive and reliable low-dimensional embeddings. Next, we present FLINT, a deep learning model for high-quality flow estimation and temporal interpolation in both flow-supervised and flow-unsupervised settings. FLINT reconstructs missing velocity fields and generates high-fidelity temporal interpolants for scalar fields across 2D+time and 3D+time ensembles without domain-specific assumptions or extensive finetuning. To further improve adaptability and generalization, we introduce HyperFLINT, a hypernetwork-based approach that conditions on simulation parameters to estimate flow fields and interpolate scalar data. This parameter-aware adaptation yields more accurate reconstructions across diverse scientific domains, even with sparse or incomplete data. Overall, this dissertation advances deep learning techniques for scientific visualization, providing scalable, adaptable, and high-quality solutions for interpreting complex spatio-temporal ensembles.", "link": "http://arxiv.org/abs/2511.23290v1", "date": "2025-11-28", "relevancy": 2.1233, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5565}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5296}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20for%20Scientific%20Visualization%3A%20Ensemble%20Data%20Analysis&body=Title%3A%20Machine%20Learning%20for%20Scientific%20Visualization%3A%20Ensemble%20Data%20Analysis%0AAuthor%3A%20Hamid%20Gadirov%0AAbstract%3A%20Scientific%20simulations%20and%20experimental%20measurements%20produce%20vast%20amounts%20of%20spatio-temporal%20data%2C%20yet%20extracting%20meaningful%20insights%20remains%20challenging%20due%20to%20high%20dimensionality%2C%20complex%20structures%2C%20and%20missing%20information.%20Traditional%20analysis%20methods%20often%20struggle%20with%20these%20issues%2C%20motivating%20the%20need%20for%20more%20robust%2C%20data-driven%20approaches.%20This%20dissertation%20explores%20deep%20learning%20methodologies%20to%20improve%20the%20analysis%20and%20visualization%20of%20spatio-temporal%20scientific%20ensembles%2C%20focusing%20on%20dimensionality%20reduction%2C%20flow%20estimation%2C%20and%20temporal%20interpolation.%20First%2C%20we%20address%20high-dimensional%20data%20representation%20through%20autoencoder-based%20dimensionality%20reduction%20for%20scientific%20ensembles.%20We%20evaluate%20the%20stability%20of%20projection%20metrics%20under%20partial%20labeling%20and%20introduce%20a%20Pareto-efficient%20selection%20strategy%20to%20identify%20optimal%20autoencoder%20variants%2C%20ensuring%20expressive%20and%20reliable%20low-dimensional%20embeddings.%20Next%2C%20we%20present%20FLINT%2C%20a%20deep%20learning%20model%20for%20high-quality%20flow%20estimation%20and%20temporal%20interpolation%20in%20both%20flow-supervised%20and%20flow-unsupervised%20settings.%20FLINT%20reconstructs%20missing%20velocity%20fields%20and%20generates%20high-fidelity%20temporal%20interpolants%20for%20scalar%20fields%20across%202D%2Btime%20and%203D%2Btime%20ensembles%20without%20domain-specific%20assumptions%20or%20extensive%20finetuning.%20To%20further%20improve%20adaptability%20and%20generalization%2C%20we%20introduce%20HyperFLINT%2C%20a%20hypernetwork-based%20approach%20that%20conditions%20on%20simulation%20parameters%20to%20estimate%20flow%20fields%20and%20interpolate%20scalar%20data.%20This%20parameter-aware%20adaptation%20yields%20more%20accurate%20reconstructions%20across%20diverse%20scientific%20domains%2C%20even%20with%20sparse%20or%20incomplete%20data.%20Overall%2C%20this%20dissertation%20advances%20deep%20learning%20techniques%20for%20scientific%20visualization%2C%20providing%20scalable%2C%20adaptable%2C%20and%20high-quality%20solutions%20for%20interpreting%20complex%20spatio-temporal%20ensembles.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520for%2520Scientific%2520Visualization%253A%2520Ensemble%2520Data%2520Analysis%26entry.906535625%3DHamid%2520Gadirov%26entry.1292438233%3DScientific%2520simulations%2520and%2520experimental%2520measurements%2520produce%2520vast%2520amounts%2520of%2520spatio-temporal%2520data%252C%2520yet%2520extracting%2520meaningful%2520insights%2520remains%2520challenging%2520due%2520to%2520high%2520dimensionality%252C%2520complex%2520structures%252C%2520and%2520missing%2520information.%2520Traditional%2520analysis%2520methods%2520often%2520struggle%2520with%2520these%2520issues%252C%2520motivating%2520the%2520need%2520for%2520more%2520robust%252C%2520data-driven%2520approaches.%2520This%2520dissertation%2520explores%2520deep%2520learning%2520methodologies%2520to%2520improve%2520the%2520analysis%2520and%2520visualization%2520of%2520spatio-temporal%2520scientific%2520ensembles%252C%2520focusing%2520on%2520dimensionality%2520reduction%252C%2520flow%2520estimation%252C%2520and%2520temporal%2520interpolation.%2520First%252C%2520we%2520address%2520high-dimensional%2520data%2520representation%2520through%2520autoencoder-based%2520dimensionality%2520reduction%2520for%2520scientific%2520ensembles.%2520We%2520evaluate%2520the%2520stability%2520of%2520projection%2520metrics%2520under%2520partial%2520labeling%2520and%2520introduce%2520a%2520Pareto-efficient%2520selection%2520strategy%2520to%2520identify%2520optimal%2520autoencoder%2520variants%252C%2520ensuring%2520expressive%2520and%2520reliable%2520low-dimensional%2520embeddings.%2520Next%252C%2520we%2520present%2520FLINT%252C%2520a%2520deep%2520learning%2520model%2520for%2520high-quality%2520flow%2520estimation%2520and%2520temporal%2520interpolation%2520in%2520both%2520flow-supervised%2520and%2520flow-unsupervised%2520settings.%2520FLINT%2520reconstructs%2520missing%2520velocity%2520fields%2520and%2520generates%2520high-fidelity%2520temporal%2520interpolants%2520for%2520scalar%2520fields%2520across%25202D%252Btime%2520and%25203D%252Btime%2520ensembles%2520without%2520domain-specific%2520assumptions%2520or%2520extensive%2520finetuning.%2520To%2520further%2520improve%2520adaptability%2520and%2520generalization%252C%2520we%2520introduce%2520HyperFLINT%252C%2520a%2520hypernetwork-based%2520approach%2520that%2520conditions%2520on%2520simulation%2520parameters%2520to%2520estimate%2520flow%2520fields%2520and%2520interpolate%2520scalar%2520data.%2520This%2520parameter-aware%2520adaptation%2520yields%2520more%2520accurate%2520reconstructions%2520across%2520diverse%2520scientific%2520domains%252C%2520even%2520with%2520sparse%2520or%2520incomplete%2520data.%2520Overall%252C%2520this%2520dissertation%2520advances%2520deep%2520learning%2520techniques%2520for%2520scientific%2520visualization%252C%2520providing%2520scalable%252C%2520adaptable%252C%2520and%2520high-quality%2520solutions%2520for%2520interpreting%2520complex%2520spatio-temporal%2520ensembles.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20for%20Scientific%20Visualization%3A%20Ensemble%20Data%20Analysis&entry.906535625=Hamid%20Gadirov&entry.1292438233=Scientific%20simulations%20and%20experimental%20measurements%20produce%20vast%20amounts%20of%20spatio-temporal%20data%2C%20yet%20extracting%20meaningful%20insights%20remains%20challenging%20due%20to%20high%20dimensionality%2C%20complex%20structures%2C%20and%20missing%20information.%20Traditional%20analysis%20methods%20often%20struggle%20with%20these%20issues%2C%20motivating%20the%20need%20for%20more%20robust%2C%20data-driven%20approaches.%20This%20dissertation%20explores%20deep%20learning%20methodologies%20to%20improve%20the%20analysis%20and%20visualization%20of%20spatio-temporal%20scientific%20ensembles%2C%20focusing%20on%20dimensionality%20reduction%2C%20flow%20estimation%2C%20and%20temporal%20interpolation.%20First%2C%20we%20address%20high-dimensional%20data%20representation%20through%20autoencoder-based%20dimensionality%20reduction%20for%20scientific%20ensembles.%20We%20evaluate%20the%20stability%20of%20projection%20metrics%20under%20partial%20labeling%20and%20introduce%20a%20Pareto-efficient%20selection%20strategy%20to%20identify%20optimal%20autoencoder%20variants%2C%20ensuring%20expressive%20and%20reliable%20low-dimensional%20embeddings.%20Next%2C%20we%20present%20FLINT%2C%20a%20deep%20learning%20model%20for%20high-quality%20flow%20estimation%20and%20temporal%20interpolation%20in%20both%20flow-supervised%20and%20flow-unsupervised%20settings.%20FLINT%20reconstructs%20missing%20velocity%20fields%20and%20generates%20high-fidelity%20temporal%20interpolants%20for%20scalar%20fields%20across%202D%2Btime%20and%203D%2Btime%20ensembles%20without%20domain-specific%20assumptions%20or%20extensive%20finetuning.%20To%20further%20improve%20adaptability%20and%20generalization%2C%20we%20introduce%20HyperFLINT%2C%20a%20hypernetwork-based%20approach%20that%20conditions%20on%20simulation%20parameters%20to%20estimate%20flow%20fields%20and%20interpolate%20scalar%20data.%20This%20parameter-aware%20adaptation%20yields%20more%20accurate%20reconstructions%20across%20diverse%20scientific%20domains%2C%20even%20with%20sparse%20or%20incomplete%20data.%20Overall%2C%20this%20dissertation%20advances%20deep%20learning%20techniques%20for%20scientific%20visualization%2C%20providing%20scalable%2C%20adaptable%2C%20and%20high-quality%20solutions%20for%20interpreting%20complex%20spatio-temporal%20ensembles.&entry.1838667208=http%3A//arxiv.org/abs/2511.23290v1&entry.124074799=Read"},
{"title": "Distributed Dynamic Associative Memory via Online Convex Optimization", "author": "Bowen Wang and Matteo Zecchin and Osvaldo Simeone", "abstract": "An associative memory (AM) enables cue-response recall, and it has recently been recognized as a key mechanism underlying modern neural architectures such as Transformers. In this work, we introduce the concept of distributed dynamic associative memory (DDAM), which extends classical AM to settings with multiple agents and time-varying data streams. In DDAM, each agent maintains a local AM that must not only store its own associations but also selectively memorize information from other agents based on a specified interest matrix. To address this problem, we propose a novel tree-based distributed online gradient descent algorithm, termed DDAM-TOGD, which enables each agent to update its memory on the fly via inter-agent communication over designated routing trees. We derive rigorous performance guarantees for DDAM-TOGD, proving sublinear static regret in stationary environments and a path-length dependent dynamic regret bound in non-stationary environments. These theoretical results provide insights into how communication delays and network structure impact performance. Building on the regret analysis, we further introduce a combinatorial tree design strategy that optimizes the routing trees to minimize communication delays, thereby improving regret bounds. Numerical experiments demonstrate that the proposed DDAM-TOGD framework achieves superior accuracy and robustness compared to representative online learning baselines such as consensus-based distributed optimization, confirming the benefits of the proposed approach in dynamic, distributed environments.", "link": "http://arxiv.org/abs/2511.23347v1", "date": "2025-11-28", "relevancy": 2.122, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5629}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5078}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Dynamic%20Associative%20Memory%20via%20Online%20Convex%20Optimization&body=Title%3A%20Distributed%20Dynamic%20Associative%20Memory%20via%20Online%20Convex%20Optimization%0AAuthor%3A%20Bowen%20Wang%20and%20Matteo%20Zecchin%20and%20Osvaldo%20Simeone%0AAbstract%3A%20An%20associative%20memory%20%28AM%29%20enables%20cue-response%20recall%2C%20and%20it%20has%20recently%20been%20recognized%20as%20a%20key%20mechanism%20underlying%20modern%20neural%20architectures%20such%20as%20Transformers.%20In%20this%20work%2C%20we%20introduce%20the%20concept%20of%20distributed%20dynamic%20associative%20memory%20%28DDAM%29%2C%20which%20extends%20classical%20AM%20to%20settings%20with%20multiple%20agents%20and%20time-varying%20data%20streams.%20In%20DDAM%2C%20each%20agent%20maintains%20a%20local%20AM%20that%20must%20not%20only%20store%20its%20own%20associations%20but%20also%20selectively%20memorize%20information%20from%20other%20agents%20based%20on%20a%20specified%20interest%20matrix.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20tree-based%20distributed%20online%20gradient%20descent%20algorithm%2C%20termed%20DDAM-TOGD%2C%20which%20enables%20each%20agent%20to%20update%20its%20memory%20on%20the%20fly%20via%20inter-agent%20communication%20over%20designated%20routing%20trees.%20We%20derive%20rigorous%20performance%20guarantees%20for%20DDAM-TOGD%2C%20proving%20sublinear%20static%20regret%20in%20stationary%20environments%20and%20a%20path-length%20dependent%20dynamic%20regret%20bound%20in%20non-stationary%20environments.%20These%20theoretical%20results%20provide%20insights%20into%20how%20communication%20delays%20and%20network%20structure%20impact%20performance.%20Building%20on%20the%20regret%20analysis%2C%20we%20further%20introduce%20a%20combinatorial%20tree%20design%20strategy%20that%20optimizes%20the%20routing%20trees%20to%20minimize%20communication%20delays%2C%20thereby%20improving%20regret%20bounds.%20Numerical%20experiments%20demonstrate%20that%20the%20proposed%20DDAM-TOGD%20framework%20achieves%20superior%20accuracy%20and%20robustness%20compared%20to%20representative%20online%20learning%20baselines%20such%20as%20consensus-based%20distributed%20optimization%2C%20confirming%20the%20benefits%20of%20the%20proposed%20approach%20in%20dynamic%2C%20distributed%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Dynamic%2520Associative%2520Memory%2520via%2520Online%2520Convex%2520Optimization%26entry.906535625%3DBowen%2520Wang%2520and%2520Matteo%2520Zecchin%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3DAn%2520associative%2520memory%2520%2528AM%2529%2520enables%2520cue-response%2520recall%252C%2520and%2520it%2520has%2520recently%2520been%2520recognized%2520as%2520a%2520key%2520mechanism%2520underlying%2520modern%2520neural%2520architectures%2520such%2520as%2520Transformers.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520concept%2520of%2520distributed%2520dynamic%2520associative%2520memory%2520%2528DDAM%2529%252C%2520which%2520extends%2520classical%2520AM%2520to%2520settings%2520with%2520multiple%2520agents%2520and%2520time-varying%2520data%2520streams.%2520In%2520DDAM%252C%2520each%2520agent%2520maintains%2520a%2520local%2520AM%2520that%2520must%2520not%2520only%2520store%2520its%2520own%2520associations%2520but%2520also%2520selectively%2520memorize%2520information%2520from%2520other%2520agents%2520based%2520on%2520a%2520specified%2520interest%2520matrix.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520tree-based%2520distributed%2520online%2520gradient%2520descent%2520algorithm%252C%2520termed%2520DDAM-TOGD%252C%2520which%2520enables%2520each%2520agent%2520to%2520update%2520its%2520memory%2520on%2520the%2520fly%2520via%2520inter-agent%2520communication%2520over%2520designated%2520routing%2520trees.%2520We%2520derive%2520rigorous%2520performance%2520guarantees%2520for%2520DDAM-TOGD%252C%2520proving%2520sublinear%2520static%2520regret%2520in%2520stationary%2520environments%2520and%2520a%2520path-length%2520dependent%2520dynamic%2520regret%2520bound%2520in%2520non-stationary%2520environments.%2520These%2520theoretical%2520results%2520provide%2520insights%2520into%2520how%2520communication%2520delays%2520and%2520network%2520structure%2520impact%2520performance.%2520Building%2520on%2520the%2520regret%2520analysis%252C%2520we%2520further%2520introduce%2520a%2520combinatorial%2520tree%2520design%2520strategy%2520that%2520optimizes%2520the%2520routing%2520trees%2520to%2520minimize%2520communication%2520delays%252C%2520thereby%2520improving%2520regret%2520bounds.%2520Numerical%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520DDAM-TOGD%2520framework%2520achieves%2520superior%2520accuracy%2520and%2520robustness%2520compared%2520to%2520representative%2520online%2520learning%2520baselines%2520such%2520as%2520consensus-based%2520distributed%2520optimization%252C%2520confirming%2520the%2520benefits%2520of%2520the%2520proposed%2520approach%2520in%2520dynamic%252C%2520distributed%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Dynamic%20Associative%20Memory%20via%20Online%20Convex%20Optimization&entry.906535625=Bowen%20Wang%20and%20Matteo%20Zecchin%20and%20Osvaldo%20Simeone&entry.1292438233=An%20associative%20memory%20%28AM%29%20enables%20cue-response%20recall%2C%20and%20it%20has%20recently%20been%20recognized%20as%20a%20key%20mechanism%20underlying%20modern%20neural%20architectures%20such%20as%20Transformers.%20In%20this%20work%2C%20we%20introduce%20the%20concept%20of%20distributed%20dynamic%20associative%20memory%20%28DDAM%29%2C%20which%20extends%20classical%20AM%20to%20settings%20with%20multiple%20agents%20and%20time-varying%20data%20streams.%20In%20DDAM%2C%20each%20agent%20maintains%20a%20local%20AM%20that%20must%20not%20only%20store%20its%20own%20associations%20but%20also%20selectively%20memorize%20information%20from%20other%20agents%20based%20on%20a%20specified%20interest%20matrix.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20tree-based%20distributed%20online%20gradient%20descent%20algorithm%2C%20termed%20DDAM-TOGD%2C%20which%20enables%20each%20agent%20to%20update%20its%20memory%20on%20the%20fly%20via%20inter-agent%20communication%20over%20designated%20routing%20trees.%20We%20derive%20rigorous%20performance%20guarantees%20for%20DDAM-TOGD%2C%20proving%20sublinear%20static%20regret%20in%20stationary%20environments%20and%20a%20path-length%20dependent%20dynamic%20regret%20bound%20in%20non-stationary%20environments.%20These%20theoretical%20results%20provide%20insights%20into%20how%20communication%20delays%20and%20network%20structure%20impact%20performance.%20Building%20on%20the%20regret%20analysis%2C%20we%20further%20introduce%20a%20combinatorial%20tree%20design%20strategy%20that%20optimizes%20the%20routing%20trees%20to%20minimize%20communication%20delays%2C%20thereby%20improving%20regret%20bounds.%20Numerical%20experiments%20demonstrate%20that%20the%20proposed%20DDAM-TOGD%20framework%20achieves%20superior%20accuracy%20and%20robustness%20compared%20to%20representative%20online%20learning%20baselines%20such%20as%20consensus-based%20distributed%20optimization%2C%20confirming%20the%20benefits%20of%20the%20proposed%20approach%20in%20dynamic%2C%20distributed%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.23347v1&entry.124074799=Read"},
{"title": "MANTA: Physics-Informed Generalized Underwater Object Tracking", "author": "Suhas Srinath and Hemang Jamadagni and Aditya Chadrasekar and Prathosh AP", "abstract": "Underwater object tracking is challenging due to wavelength dependent attenuation and scattering, which severely distort appearance across depths and water conditions. Existing trackers trained on terrestrial data fail to generalize to these physics-driven degradations. We present MANTA, a physics-informed framework integrating representation learning with tracking design for underwater scenarios. We propose a dual-positive contrastive learning strategy coupling temporal consistency with Beer-Lambert augmentations to yield features robust to both temporal and underwater distortions. We further introduce a multi-stage pipeline augmenting motion-based tracking with a physics-informed secondary association algorithm that integrates geometric consistency and appearance similarity for re-identification under occlusion and drift. To complement standard IoU metrics, we propose Center-Scale Consistency (CSC) and Geometric Alignment Score (GAS) to assess geometric fidelity. Experiments on four underwater benchmarks (WebUOT-1M, UOT32, UTB180, UWCOT220) show that MANTA achieves state-of-the-art performance, improving Success AUC by up to 6 percent, while ensuring stable long-term generalized underwater tracking and efficient runtime.", "link": "http://arxiv.org/abs/2511.23405v1", "date": "2025-11-28", "relevancy": 2.1012, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5228}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MANTA%3A%20Physics-Informed%20Generalized%20Underwater%20Object%20Tracking&body=Title%3A%20MANTA%3A%20Physics-Informed%20Generalized%20Underwater%20Object%20Tracking%0AAuthor%3A%20Suhas%20Srinath%20and%20Hemang%20Jamadagni%20and%20Aditya%20Chadrasekar%20and%20Prathosh%20AP%0AAbstract%3A%20Underwater%20object%20tracking%20is%20challenging%20due%20to%20wavelength%20dependent%20attenuation%20and%20scattering%2C%20which%20severely%20distort%20appearance%20across%20depths%20and%20water%20conditions.%20Existing%20trackers%20trained%20on%20terrestrial%20data%20fail%20to%20generalize%20to%20these%20physics-driven%20degradations.%20We%20present%20MANTA%2C%20a%20physics-informed%20framework%20integrating%20representation%20learning%20with%20tracking%20design%20for%20underwater%20scenarios.%20We%20propose%20a%20dual-positive%20contrastive%20learning%20strategy%20coupling%20temporal%20consistency%20with%20Beer-Lambert%20augmentations%20to%20yield%20features%20robust%20to%20both%20temporal%20and%20underwater%20distortions.%20We%20further%20introduce%20a%20multi-stage%20pipeline%20augmenting%20motion-based%20tracking%20with%20a%20physics-informed%20secondary%20association%20algorithm%20that%20integrates%20geometric%20consistency%20and%20appearance%20similarity%20for%20re-identification%20under%20occlusion%20and%20drift.%20To%20complement%20standard%20IoU%20metrics%2C%20we%20propose%20Center-Scale%20Consistency%20%28CSC%29%20and%20Geometric%20Alignment%20Score%20%28GAS%29%20to%20assess%20geometric%20fidelity.%20Experiments%20on%20four%20underwater%20benchmarks%20%28WebUOT-1M%2C%20UOT32%2C%20UTB180%2C%20UWCOT220%29%20show%20that%20MANTA%20achieves%20state-of-the-art%20performance%2C%20improving%20Success%20AUC%20by%20up%20to%206%20percent%2C%20while%20ensuring%20stable%20long-term%20generalized%20underwater%20tracking%20and%20efficient%20runtime.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMANTA%253A%2520Physics-Informed%2520Generalized%2520Underwater%2520Object%2520Tracking%26entry.906535625%3DSuhas%2520Srinath%2520and%2520Hemang%2520Jamadagni%2520and%2520Aditya%2520Chadrasekar%2520and%2520Prathosh%2520AP%26entry.1292438233%3DUnderwater%2520object%2520tracking%2520is%2520challenging%2520due%2520to%2520wavelength%2520dependent%2520attenuation%2520and%2520scattering%252C%2520which%2520severely%2520distort%2520appearance%2520across%2520depths%2520and%2520water%2520conditions.%2520Existing%2520trackers%2520trained%2520on%2520terrestrial%2520data%2520fail%2520to%2520generalize%2520to%2520these%2520physics-driven%2520degradations.%2520We%2520present%2520MANTA%252C%2520a%2520physics-informed%2520framework%2520integrating%2520representation%2520learning%2520with%2520tracking%2520design%2520for%2520underwater%2520scenarios.%2520We%2520propose%2520a%2520dual-positive%2520contrastive%2520learning%2520strategy%2520coupling%2520temporal%2520consistency%2520with%2520Beer-Lambert%2520augmentations%2520to%2520yield%2520features%2520robust%2520to%2520both%2520temporal%2520and%2520underwater%2520distortions.%2520We%2520further%2520introduce%2520a%2520multi-stage%2520pipeline%2520augmenting%2520motion-based%2520tracking%2520with%2520a%2520physics-informed%2520secondary%2520association%2520algorithm%2520that%2520integrates%2520geometric%2520consistency%2520and%2520appearance%2520similarity%2520for%2520re-identification%2520under%2520occlusion%2520and%2520drift.%2520To%2520complement%2520standard%2520IoU%2520metrics%252C%2520we%2520propose%2520Center-Scale%2520Consistency%2520%2528CSC%2529%2520and%2520Geometric%2520Alignment%2520Score%2520%2528GAS%2529%2520to%2520assess%2520geometric%2520fidelity.%2520Experiments%2520on%2520four%2520underwater%2520benchmarks%2520%2528WebUOT-1M%252C%2520UOT32%252C%2520UTB180%252C%2520UWCOT220%2529%2520show%2520that%2520MANTA%2520achieves%2520state-of-the-art%2520performance%252C%2520improving%2520Success%2520AUC%2520by%2520up%2520to%25206%2520percent%252C%2520while%2520ensuring%2520stable%2520long-term%2520generalized%2520underwater%2520tracking%2520and%2520efficient%2520runtime.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MANTA%3A%20Physics-Informed%20Generalized%20Underwater%20Object%20Tracking&entry.906535625=Suhas%20Srinath%20and%20Hemang%20Jamadagni%20and%20Aditya%20Chadrasekar%20and%20Prathosh%20AP&entry.1292438233=Underwater%20object%20tracking%20is%20challenging%20due%20to%20wavelength%20dependent%20attenuation%20and%20scattering%2C%20which%20severely%20distort%20appearance%20across%20depths%20and%20water%20conditions.%20Existing%20trackers%20trained%20on%20terrestrial%20data%20fail%20to%20generalize%20to%20these%20physics-driven%20degradations.%20We%20present%20MANTA%2C%20a%20physics-informed%20framework%20integrating%20representation%20learning%20with%20tracking%20design%20for%20underwater%20scenarios.%20We%20propose%20a%20dual-positive%20contrastive%20learning%20strategy%20coupling%20temporal%20consistency%20with%20Beer-Lambert%20augmentations%20to%20yield%20features%20robust%20to%20both%20temporal%20and%20underwater%20distortions.%20We%20further%20introduce%20a%20multi-stage%20pipeline%20augmenting%20motion-based%20tracking%20with%20a%20physics-informed%20secondary%20association%20algorithm%20that%20integrates%20geometric%20consistency%20and%20appearance%20similarity%20for%20re-identification%20under%20occlusion%20and%20drift.%20To%20complement%20standard%20IoU%20metrics%2C%20we%20propose%20Center-Scale%20Consistency%20%28CSC%29%20and%20Geometric%20Alignment%20Score%20%28GAS%29%20to%20assess%20geometric%20fidelity.%20Experiments%20on%20four%20underwater%20benchmarks%20%28WebUOT-1M%2C%20UOT32%2C%20UTB180%2C%20UWCOT220%29%20show%20that%20MANTA%20achieves%20state-of-the-art%20performance%2C%20improving%20Success%20AUC%20by%20up%20to%206%20percent%2C%20while%20ensuring%20stable%20long-term%20generalized%20underwater%20tracking%20and%20efficient%20runtime.&entry.1838667208=http%3A//arxiv.org/abs/2511.23405v1&entry.124074799=Read"},
{"title": "FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis", "author": "Xichen Xu and Yanshu Wang and Jinbao Wang and Xiaoning Lei and Guoyang Xie and Guannan Jiang and Zhichao Lu", "abstract": "Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: https://github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.", "link": "http://arxiv.org/abs/2509.20295v3", "date": "2025-11-28", "relevancy": 2.0999, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5378}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5225}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAST%3A%20Foreground-aware%20Diffusion%20with%20Accelerated%20Sampling%20Trajectory%20for%20Segmentation-oriented%20Anomaly%20Synthesis&body=Title%3A%20FAST%3A%20Foreground-aware%20Diffusion%20with%20Accelerated%20Sampling%20Trajectory%20for%20Segmentation-oriented%20Anomaly%20Synthesis%0AAuthor%3A%20Xichen%20Xu%20and%20Yanshu%20Wang%20and%20Jinbao%20Wang%20and%20Xiaoning%20Lei%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Zhichao%20Lu%0AAbstract%3A%20Industrial%20anomaly%20segmentation%20relies%20heavily%20on%20pixel-level%20annotations%2C%20yet%20real-world%20anomalies%20are%20often%20scarce%2C%20diverse%2C%20and%20costly%20to%20label.%20Segmentation-oriented%20industrial%20anomaly%20synthesis%20%28SIAS%29%20has%20emerged%20as%20a%20promising%20alternative%3B%20however%2C%20existing%20methods%20struggle%20to%20balance%20sampling%20efficiency%20and%20generation%20quality.%20Moreover%2C%20most%20approaches%20treat%20all%20spatial%20regions%20uniformly%2C%20overlooking%20the%20distinct%20statistical%20differences%20between%20anomaly%20and%20background%20areas.%20This%20uniform%20treatment%20hinders%20the%20synthesis%20of%20controllable%2C%20structure-specific%20anomalies%20tailored%20for%20segmentation%20tasks.%20In%20this%20paper%2C%20we%20propose%20FAST%2C%20a%20foreground-aware%20diffusion%20framework%20featuring%20two%20novel%20modules%3A%20the%20Anomaly-Informed%20Accelerated%20Sampling%20%28AIAS%29%20and%20the%20Foreground-Aware%20Reconstruction%20Module%20%28FARM%29.%20AIAS%20is%20a%20training-free%20sampling%20algorithm%20specifically%20designed%20for%20segmentation-oriented%20industrial%20anomaly%20synthesis%2C%20which%20accelerates%20the%20reverse%20process%20through%20coarse-to-fine%20aggregation%20and%20enables%20the%20synthesis%20of%20state-of-the-art%20segmentation-oriented%20anomalies%20in%20as%20few%20as%2010%20steps.%20Meanwhile%2C%20FARM%20adaptively%20adjusts%20the%20anomaly-aware%20noise%20within%20the%20masked%20foreground%20regions%20at%20each%20sampling%20step%2C%20preserving%20localized%20anomaly%20signals%20throughout%20the%20denoising%20trajectory.%20Extensive%20experiments%20on%20multiple%20industrial%20benchmarks%20demonstrate%20that%20FAST%20consistently%20outperforms%20existing%20anomaly%20synthesis%20methods%20in%20downstream%20segmentation%20tasks.%20We%20release%20the%20code%20at%3A%20https%3A//github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20295v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAST%253A%2520Foreground-aware%2520Diffusion%2520with%2520Accelerated%2520Sampling%2520Trajectory%2520for%2520Segmentation-oriented%2520Anomaly%2520Synthesis%26entry.906535625%3DXichen%2520Xu%2520and%2520Yanshu%2520Wang%2520and%2520Jinbao%2520Wang%2520and%2520Xiaoning%2520Lei%2520and%2520Guoyang%2520Xie%2520and%2520Guannan%2520Jiang%2520and%2520Zhichao%2520Lu%26entry.1292438233%3DIndustrial%2520anomaly%2520segmentation%2520relies%2520heavily%2520on%2520pixel-level%2520annotations%252C%2520yet%2520real-world%2520anomalies%2520are%2520often%2520scarce%252C%2520diverse%252C%2520and%2520costly%2520to%2520label.%2520Segmentation-oriented%2520industrial%2520anomaly%2520synthesis%2520%2528SIAS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%253B%2520however%252C%2520existing%2520methods%2520struggle%2520to%2520balance%2520sampling%2520efficiency%2520and%2520generation%2520quality.%2520Moreover%252C%2520most%2520approaches%2520treat%2520all%2520spatial%2520regions%2520uniformly%252C%2520overlooking%2520the%2520distinct%2520statistical%2520differences%2520between%2520anomaly%2520and%2520background%2520areas.%2520This%2520uniform%2520treatment%2520hinders%2520the%2520synthesis%2520of%2520controllable%252C%2520structure-specific%2520anomalies%2520tailored%2520for%2520segmentation%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FAST%252C%2520a%2520foreground-aware%2520diffusion%2520framework%2520featuring%2520two%2520novel%2520modules%253A%2520the%2520Anomaly-Informed%2520Accelerated%2520Sampling%2520%2528AIAS%2529%2520and%2520the%2520Foreground-Aware%2520Reconstruction%2520Module%2520%2528FARM%2529.%2520AIAS%2520is%2520a%2520training-free%2520sampling%2520algorithm%2520specifically%2520designed%2520for%2520segmentation-oriented%2520industrial%2520anomaly%2520synthesis%252C%2520which%2520accelerates%2520the%2520reverse%2520process%2520through%2520coarse-to-fine%2520aggregation%2520and%2520enables%2520the%2520synthesis%2520of%2520state-of-the-art%2520segmentation-oriented%2520anomalies%2520in%2520as%2520few%2520as%252010%2520steps.%2520Meanwhile%252C%2520FARM%2520adaptively%2520adjusts%2520the%2520anomaly-aware%2520noise%2520within%2520the%2520masked%2520foreground%2520regions%2520at%2520each%2520sampling%2520step%252C%2520preserving%2520localized%2520anomaly%2520signals%2520throughout%2520the%2520denoising%2520trajectory.%2520Extensive%2520experiments%2520on%2520multiple%2520industrial%2520benchmarks%2520demonstrate%2520that%2520FAST%2520consistently%2520outperforms%2520existing%2520anomaly%2520synthesis%2520methods%2520in%2520downstream%2520segmentation%2520tasks.%2520We%2520release%2520the%2520code%2520at%253A%2520https%253A//github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20295v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAST%3A%20Foreground-aware%20Diffusion%20with%20Accelerated%20Sampling%20Trajectory%20for%20Segmentation-oriented%20Anomaly%20Synthesis&entry.906535625=Xichen%20Xu%20and%20Yanshu%20Wang%20and%20Jinbao%20Wang%20and%20Xiaoning%20Lei%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Zhichao%20Lu&entry.1292438233=Industrial%20anomaly%20segmentation%20relies%20heavily%20on%20pixel-level%20annotations%2C%20yet%20real-world%20anomalies%20are%20often%20scarce%2C%20diverse%2C%20and%20costly%20to%20label.%20Segmentation-oriented%20industrial%20anomaly%20synthesis%20%28SIAS%29%20has%20emerged%20as%20a%20promising%20alternative%3B%20however%2C%20existing%20methods%20struggle%20to%20balance%20sampling%20efficiency%20and%20generation%20quality.%20Moreover%2C%20most%20approaches%20treat%20all%20spatial%20regions%20uniformly%2C%20overlooking%20the%20distinct%20statistical%20differences%20between%20anomaly%20and%20background%20areas.%20This%20uniform%20treatment%20hinders%20the%20synthesis%20of%20controllable%2C%20structure-specific%20anomalies%20tailored%20for%20segmentation%20tasks.%20In%20this%20paper%2C%20we%20propose%20FAST%2C%20a%20foreground-aware%20diffusion%20framework%20featuring%20two%20novel%20modules%3A%20the%20Anomaly-Informed%20Accelerated%20Sampling%20%28AIAS%29%20and%20the%20Foreground-Aware%20Reconstruction%20Module%20%28FARM%29.%20AIAS%20is%20a%20training-free%20sampling%20algorithm%20specifically%20designed%20for%20segmentation-oriented%20industrial%20anomaly%20synthesis%2C%20which%20accelerates%20the%20reverse%20process%20through%20coarse-to-fine%20aggregation%20and%20enables%20the%20synthesis%20of%20state-of-the-art%20segmentation-oriented%20anomalies%20in%20as%20few%20as%2010%20steps.%20Meanwhile%2C%20FARM%20adaptively%20adjusts%20the%20anomaly-aware%20noise%20within%20the%20masked%20foreground%20regions%20at%20each%20sampling%20step%2C%20preserving%20localized%20anomaly%20signals%20throughout%20the%20denoising%20trajectory.%20Extensive%20experiments%20on%20multiple%20industrial%20benchmarks%20demonstrate%20that%20FAST%20consistently%20outperforms%20existing%20anomaly%20synthesis%20methods%20in%20downstream%20segmentation%20tasks.%20We%20release%20the%20code%20at%3A%20https%3A//github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2509.20295v3&entry.124074799=Read"},
{"title": "Activation Quantization of Vision Encoders Needs Prefixing Registers", "author": "Seunghyeon Kim and Jinho Kim and Taesun Yeom and Wonpyo Park and Kyuyeun Kim and Jaeho Lee", "abstract": "Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\\textit{RegCache}$, a training-free algorithm that mitigates outliers in large-scale pretrained vision encoders and serves as a plug-in module that can be applied on top of other quantization methods. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.", "link": "http://arxiv.org/abs/2510.04547v3", "date": "2025-11-28", "relevancy": 2.0853, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation%20Quantization%20of%20Vision%20Encoders%20Needs%20Prefixing%20Registers&body=Title%3A%20Activation%20Quantization%20of%20Vision%20Encoders%20Needs%20Prefixing%20Registers%0AAuthor%3A%20Seunghyeon%20Kim%20and%20Jinho%20Kim%20and%20Taesun%20Yeom%20and%20Wonpyo%20Park%20and%20Kyuyeun%20Kim%20and%20Jaeho%20Lee%0AAbstract%3A%20Transformer-based%20vision%20encoders%20--%20such%20as%20CLIP%20--%20are%20central%20to%20multimodal%20intelligence%2C%20powering%20applications%20from%20autonomous%20web%20agents%20to%20robotic%20control.%20Since%20these%20applications%20often%20demand%20real-time%20processing%20of%20massive%20visual%20data%2C%20reducing%20the%20inference%20cost%20of%20vision%20encoders%20is%20critical.%20Quantization%20offers%20a%20practical%20path%2C%20but%20remains%20challenging%20even%20at%208-bit%20precision%20due%20to%20massive-scale%20activations%20%28i.e.%2C%20outliers%29.%20In%20this%20work%2C%20we%20propose%20%24%5Ctextit%7BRegCache%7D%24%2C%20a%20training-free%20algorithm%20that%20mitigates%20outliers%20in%20large-scale%20pretrained%20vision%20encoders%20and%20serves%20as%20a%20plug-in%20module%20that%20can%20be%20applied%20on%20top%20of%20other%20quantization%20methods.%20The%20proposed%20RegCache%20introduces%20outlier-prone%20yet%20semantically%20meaningless%20prefix%20tokens%20to%20the%20target%20vision%20encoder%2C%20which%20prevents%20other%20tokens%20from%20having%20outliers.%20Notably%2C%20we%20observe%20that%20outliers%20in%20vision%20encoders%20behave%20differently%20from%20those%20in%20language%20models%2C%20motivating%20two%20technical%20innovations%3A%20middle-layer%20prefixing%20and%20token%20deletion.%20Experiments%20show%20that%20our%20method%20consistently%20improves%20the%20accuracy%20of%20quantized%20models%20across%20both%20text-supervised%20and%20self-supervised%20vision%20encoders.%0ALink%3A%20http%3A//arxiv.org/abs/2510.04547v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation%2520Quantization%2520of%2520Vision%2520Encoders%2520Needs%2520Prefixing%2520Registers%26entry.906535625%3DSeunghyeon%2520Kim%2520and%2520Jinho%2520Kim%2520and%2520Taesun%2520Yeom%2520and%2520Wonpyo%2520Park%2520and%2520Kyuyeun%2520Kim%2520and%2520Jaeho%2520Lee%26entry.1292438233%3DTransformer-based%2520vision%2520encoders%2520--%2520such%2520as%2520CLIP%2520--%2520are%2520central%2520to%2520multimodal%2520intelligence%252C%2520powering%2520applications%2520from%2520autonomous%2520web%2520agents%2520to%2520robotic%2520control.%2520Since%2520these%2520applications%2520often%2520demand%2520real-time%2520processing%2520of%2520massive%2520visual%2520data%252C%2520reducing%2520the%2520inference%2520cost%2520of%2520vision%2520encoders%2520is%2520critical.%2520Quantization%2520offers%2520a%2520practical%2520path%252C%2520but%2520remains%2520challenging%2520even%2520at%25208-bit%2520precision%2520due%2520to%2520massive-scale%2520activations%2520%2528i.e.%252C%2520outliers%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520%2524%255Ctextit%257BRegCache%257D%2524%252C%2520a%2520training-free%2520algorithm%2520that%2520mitigates%2520outliers%2520in%2520large-scale%2520pretrained%2520vision%2520encoders%2520and%2520serves%2520as%2520a%2520plug-in%2520module%2520that%2520can%2520be%2520applied%2520on%2520top%2520of%2520other%2520quantization%2520methods.%2520The%2520proposed%2520RegCache%2520introduces%2520outlier-prone%2520yet%2520semantically%2520meaningless%2520prefix%2520tokens%2520to%2520the%2520target%2520vision%2520encoder%252C%2520which%2520prevents%2520other%2520tokens%2520from%2520having%2520outliers.%2520Notably%252C%2520we%2520observe%2520that%2520outliers%2520in%2520vision%2520encoders%2520behave%2520differently%2520from%2520those%2520in%2520language%2520models%252C%2520motivating%2520two%2520technical%2520innovations%253A%2520middle-layer%2520prefixing%2520and%2520token%2520deletion.%2520Experiments%2520show%2520that%2520our%2520method%2520consistently%2520improves%2520the%2520accuracy%2520of%2520quantized%2520models%2520across%2520both%2520text-supervised%2520and%2520self-supervised%2520vision%2520encoders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04547v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation%20Quantization%20of%20Vision%20Encoders%20Needs%20Prefixing%20Registers&entry.906535625=Seunghyeon%20Kim%20and%20Jinho%20Kim%20and%20Taesun%20Yeom%20and%20Wonpyo%20Park%20and%20Kyuyeun%20Kim%20and%20Jaeho%20Lee&entry.1292438233=Transformer-based%20vision%20encoders%20--%20such%20as%20CLIP%20--%20are%20central%20to%20multimodal%20intelligence%2C%20powering%20applications%20from%20autonomous%20web%20agents%20to%20robotic%20control.%20Since%20these%20applications%20often%20demand%20real-time%20processing%20of%20massive%20visual%20data%2C%20reducing%20the%20inference%20cost%20of%20vision%20encoders%20is%20critical.%20Quantization%20offers%20a%20practical%20path%2C%20but%20remains%20challenging%20even%20at%208-bit%20precision%20due%20to%20massive-scale%20activations%20%28i.e.%2C%20outliers%29.%20In%20this%20work%2C%20we%20propose%20%24%5Ctextit%7BRegCache%7D%24%2C%20a%20training-free%20algorithm%20that%20mitigates%20outliers%20in%20large-scale%20pretrained%20vision%20encoders%20and%20serves%20as%20a%20plug-in%20module%20that%20can%20be%20applied%20on%20top%20of%20other%20quantization%20methods.%20The%20proposed%20RegCache%20introduces%20outlier-prone%20yet%20semantically%20meaningless%20prefix%20tokens%20to%20the%20target%20vision%20encoder%2C%20which%20prevents%20other%20tokens%20from%20having%20outliers.%20Notably%2C%20we%20observe%20that%20outliers%20in%20vision%20encoders%20behave%20differently%20from%20those%20in%20language%20models%2C%20motivating%20two%20technical%20innovations%3A%20middle-layer%20prefixing%20and%20token%20deletion.%20Experiments%20show%20that%20our%20method%20consistently%20improves%20the%20accuracy%20of%20quantized%20models%20across%20both%20text-supervised%20and%20self-supervised%20vision%20encoders.&entry.1838667208=http%3A//arxiv.org/abs/2510.04547v3&entry.124074799=Read"},
{"title": "A Theoretical Framework for Discovering Groups and Unitary Representations via Tensor Factorization", "author": "Dongsung Huh and Halyun Jeong", "abstract": "We analyze the HyperCube model, an \\textit{operator-valued} tensor factorization architecture that discovers group structures and their unitary representations. We provide a rigorous theoretical explanation for this inductive bias by decomposing its objective into a term regulating factor scales ($\\mathcal{B}$) and a term enforcing directional alignment ($\\mathcal{R} \\geq 0$). This decomposition isolates the \\textit{collinear manifold} ($\\mathcal{R}=0$), to which numerical optimization consistently converges for group isotopes. We prove that this manifold admits feasible solutions exclusively for group isotopes, and that within it, $\\mathcal{B}$ exerts a variational pressure toward unitarity. To bridge the gap to the global landscape, we formulate a \\textit{Collinearity Dominance Conjecture}, supported by empirical observations. Conditional on this dominance, we prove two key results: (1) the global minimum is achieved by the unitary regular representation for groups, and (2) non-group operations incur a strictly higher objective value, formally quantifying the model's inductive bias toward the associative structure of groups (up to isotopy).", "link": "http://arxiv.org/abs/2511.23152v1", "date": "2025-11-28", "relevancy": 1.6511, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theoretical%20Framework%20for%20Discovering%20Groups%20and%20Unitary%20Representations%20via%20Tensor%20Factorization&body=Title%3A%20A%20Theoretical%20Framework%20for%20Discovering%20Groups%20and%20Unitary%20Representations%20via%20Tensor%20Factorization%0AAuthor%3A%20Dongsung%20Huh%20and%20Halyun%20Jeong%0AAbstract%3A%20We%20analyze%20the%20HyperCube%20model%2C%20an%20%5Ctextit%7Boperator-valued%7D%20tensor%20factorization%20architecture%20that%20discovers%20group%20structures%20and%20their%20unitary%20representations.%20We%20provide%20a%20rigorous%20theoretical%20explanation%20for%20this%20inductive%20bias%20by%20decomposing%20its%20objective%20into%20a%20term%20regulating%20factor%20scales%20%28%24%5Cmathcal%7BB%7D%24%29%20and%20a%20term%20enforcing%20directional%20alignment%20%28%24%5Cmathcal%7BR%7D%20%5Cgeq%200%24%29.%20This%20decomposition%20isolates%20the%20%5Ctextit%7Bcollinear%20manifold%7D%20%28%24%5Cmathcal%7BR%7D%3D0%24%29%2C%20to%20which%20numerical%20optimization%20consistently%20converges%20for%20group%20isotopes.%20We%20prove%20that%20this%20manifold%20admits%20feasible%20solutions%20exclusively%20for%20group%20isotopes%2C%20and%20that%20within%20it%2C%20%24%5Cmathcal%7BB%7D%24%20exerts%20a%20variational%20pressure%20toward%20unitarity.%20To%20bridge%20the%20gap%20to%20the%20global%20landscape%2C%20we%20formulate%20a%20%5Ctextit%7BCollinearity%20Dominance%20Conjecture%7D%2C%20supported%20by%20empirical%20observations.%20Conditional%20on%20this%20dominance%2C%20we%20prove%20two%20key%20results%3A%20%281%29%20the%20global%20minimum%20is%20achieved%20by%20the%20unitary%20regular%20representation%20for%20groups%2C%20and%20%282%29%20non-group%20operations%20incur%20a%20strictly%20higher%20objective%20value%2C%20formally%20quantifying%20the%20model%27s%20inductive%20bias%20toward%20the%20associative%20structure%20of%20groups%20%28up%20to%20isotopy%29.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theoretical%2520Framework%2520for%2520Discovering%2520Groups%2520and%2520Unitary%2520Representations%2520via%2520Tensor%2520Factorization%26entry.906535625%3DDongsung%2520Huh%2520and%2520Halyun%2520Jeong%26entry.1292438233%3DWe%2520analyze%2520the%2520HyperCube%2520model%252C%2520an%2520%255Ctextit%257Boperator-valued%257D%2520tensor%2520factorization%2520architecture%2520that%2520discovers%2520group%2520structures%2520and%2520their%2520unitary%2520representations.%2520We%2520provide%2520a%2520rigorous%2520theoretical%2520explanation%2520for%2520this%2520inductive%2520bias%2520by%2520decomposing%2520its%2520objective%2520into%2520a%2520term%2520regulating%2520factor%2520scales%2520%2528%2524%255Cmathcal%257BB%257D%2524%2529%2520and%2520a%2520term%2520enforcing%2520directional%2520alignment%2520%2528%2524%255Cmathcal%257BR%257D%2520%255Cgeq%25200%2524%2529.%2520This%2520decomposition%2520isolates%2520the%2520%255Ctextit%257Bcollinear%2520manifold%257D%2520%2528%2524%255Cmathcal%257BR%257D%253D0%2524%2529%252C%2520to%2520which%2520numerical%2520optimization%2520consistently%2520converges%2520for%2520group%2520isotopes.%2520We%2520prove%2520that%2520this%2520manifold%2520admits%2520feasible%2520solutions%2520exclusively%2520for%2520group%2520isotopes%252C%2520and%2520that%2520within%2520it%252C%2520%2524%255Cmathcal%257BB%257D%2524%2520exerts%2520a%2520variational%2520pressure%2520toward%2520unitarity.%2520To%2520bridge%2520the%2520gap%2520to%2520the%2520global%2520landscape%252C%2520we%2520formulate%2520a%2520%255Ctextit%257BCollinearity%2520Dominance%2520Conjecture%257D%252C%2520supported%2520by%2520empirical%2520observations.%2520Conditional%2520on%2520this%2520dominance%252C%2520we%2520prove%2520two%2520key%2520results%253A%2520%25281%2529%2520the%2520global%2520minimum%2520is%2520achieved%2520by%2520the%2520unitary%2520regular%2520representation%2520for%2520groups%252C%2520and%2520%25282%2529%2520non-group%2520operations%2520incur%2520a%2520strictly%2520higher%2520objective%2520value%252C%2520formally%2520quantifying%2520the%2520model%2527s%2520inductive%2520bias%2520toward%2520the%2520associative%2520structure%2520of%2520groups%2520%2528up%2520to%2520isotopy%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theoretical%20Framework%20for%20Discovering%20Groups%20and%20Unitary%20Representations%20via%20Tensor%20Factorization&entry.906535625=Dongsung%20Huh%20and%20Halyun%20Jeong&entry.1292438233=We%20analyze%20the%20HyperCube%20model%2C%20an%20%5Ctextit%7Boperator-valued%7D%20tensor%20factorization%20architecture%20that%20discovers%20group%20structures%20and%20their%20unitary%20representations.%20We%20provide%20a%20rigorous%20theoretical%20explanation%20for%20this%20inductive%20bias%20by%20decomposing%20its%20objective%20into%20a%20term%20regulating%20factor%20scales%20%28%24%5Cmathcal%7BB%7D%24%29%20and%20a%20term%20enforcing%20directional%20alignment%20%28%24%5Cmathcal%7BR%7D%20%5Cgeq%200%24%29.%20This%20decomposition%20isolates%20the%20%5Ctextit%7Bcollinear%20manifold%7D%20%28%24%5Cmathcal%7BR%7D%3D0%24%29%2C%20to%20which%20numerical%20optimization%20consistently%20converges%20for%20group%20isotopes.%20We%20prove%20that%20this%20manifold%20admits%20feasible%20solutions%20exclusively%20for%20group%20isotopes%2C%20and%20that%20within%20it%2C%20%24%5Cmathcal%7BB%7D%24%20exerts%20a%20variational%20pressure%20toward%20unitarity.%20To%20bridge%20the%20gap%20to%20the%20global%20landscape%2C%20we%20formulate%20a%20%5Ctextit%7BCollinearity%20Dominance%20Conjecture%7D%2C%20supported%20by%20empirical%20observations.%20Conditional%20on%20this%20dominance%2C%20we%20prove%20two%20key%20results%3A%20%281%29%20the%20global%20minimum%20is%20achieved%20by%20the%20unitary%20regular%20representation%20for%20groups%2C%20and%20%282%29%20non-group%20operations%20incur%20a%20strictly%20higher%20objective%20value%2C%20formally%20quantifying%20the%20model%27s%20inductive%20bias%20toward%20the%20associative%20structure%20of%20groups%20%28up%20to%20isotopy%29.&entry.1838667208=http%3A//arxiv.org/abs/2511.23152v1&entry.124074799=Read"},
{"title": "Crowdsourcing the Frontier: Advancing Hybrid Physics-ML Climate Simulation via a $50,000 Kaggle Competition", "author": "Jerry Lin and Zeyuan Hu and Tom Beucler and Katherine Frields and Hannah Christensen and Walter Hannah and Helge Heuer and Peter Ukkonnen and Laura A. Mansfield and Tian Zheng and Liran Peng and Ritwik Gupta and Pierre Gentine and Yusef Al-Naher and Mingjiang Duan and Kyo Hattori and Weiliang Ji and Chunhan Li and Kippei Matsuda and Naoki Murakami and Shlomo Ron and Marec Serlin and Hongjian Song and Yuma Tanabe and Daisuke Yamamoto and Jianyao Zhou and Mike Pritchard", "abstract": "Subgrid machine-learning (ML) parameterizations have the potential to introduce a new generation of climate models that incorporate the effects of higher-resolution physics without incurring the prohibitive computational cost associated with more explicit physics-based simulations. However, important issues, ranging from online instability to inconsistent online performance, have limited their operational use for long-term climate projections. To more rapidly drive progress in solving these issues, domain scientists and machine learning researchers opened up the offline aspect of this problem to the broader machine learning and data science community with the release of ClimSim, a NeurIPS Datasets and Benchmarks publication, and an associated Kaggle competition. This paper reports on the downstream results of the Kaggle competition by coupling emulators inspired by the winning teams' architectures to an interactive climate model (including full cloud microphysics, a regime historically prone to online instability) and systematically evaluating their online performance. Our results demonstrate that online stability in the low-resolution, real-geography setting is reproducible across multiple diverse architectures, which we consider a key milestone. All tested architectures exhibit strikingly similar offline and online biases, though their responses to architecture-agnostic design choices (e.g., expanding the list of input variables) can differ significantly. Multiple Kaggle-inspired architectures achieve state-of-the-art (SOTA) results on certain metrics such as zonal mean bias patterns and global RMSE, indicating that crowdsourcing the essence of the offline problem is one path to improving online performance in hybrid physics-AI climate simulation.", "link": "http://arxiv.org/abs/2511.20963v2", "date": "2025-11-28", "relevancy": 0.9517, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4943}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4691}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crowdsourcing%20the%20Frontier%3A%20Advancing%20Hybrid%20Physics-ML%20Climate%20Simulation%20via%20a%20%2450%2C000%20Kaggle%20Competition&body=Title%3A%20Crowdsourcing%20the%20Frontier%3A%20Advancing%20Hybrid%20Physics-ML%20Climate%20Simulation%20via%20a%20%2450%2C000%20Kaggle%20Competition%0AAuthor%3A%20Jerry%20Lin%20and%20Zeyuan%20Hu%20and%20Tom%20Beucler%20and%20Katherine%20Frields%20and%20Hannah%20Christensen%20and%20Walter%20Hannah%20and%20Helge%20Heuer%20and%20Peter%20Ukkonnen%20and%20Laura%20A.%20Mansfield%20and%20Tian%20Zheng%20and%20Liran%20Peng%20and%20Ritwik%20Gupta%20and%20Pierre%20Gentine%20and%20Yusef%20Al-Naher%20and%20Mingjiang%20Duan%20and%20Kyo%20Hattori%20and%20Weiliang%20Ji%20and%20Chunhan%20Li%20and%20Kippei%20Matsuda%20and%20Naoki%20Murakami%20and%20Shlomo%20Ron%20and%20Marec%20Serlin%20and%20Hongjian%20Song%20and%20Yuma%20Tanabe%20and%20Daisuke%20Yamamoto%20and%20Jianyao%20Zhou%20and%20Mike%20Pritchard%0AAbstract%3A%20Subgrid%20machine-learning%20%28ML%29%20parameterizations%20have%20the%20potential%20to%20introduce%20a%20new%20generation%20of%20climate%20models%20that%20incorporate%20the%20effects%20of%20higher-resolution%20physics%20without%20incurring%20the%20prohibitive%20computational%20cost%20associated%20with%20more%20explicit%20physics-based%20simulations.%20However%2C%20important%20issues%2C%20ranging%20from%20online%20instability%20to%20inconsistent%20online%20performance%2C%20have%20limited%20their%20operational%20use%20for%20long-term%20climate%20projections.%20To%20more%20rapidly%20drive%20progress%20in%20solving%20these%20issues%2C%20domain%20scientists%20and%20machine%20learning%20researchers%20opened%20up%20the%20offline%20aspect%20of%20this%20problem%20to%20the%20broader%20machine%20learning%20and%20data%20science%20community%20with%20the%20release%20of%20ClimSim%2C%20a%20NeurIPS%20Datasets%20and%20Benchmarks%20publication%2C%20and%20an%20associated%20Kaggle%20competition.%20This%20paper%20reports%20on%20the%20downstream%20results%20of%20the%20Kaggle%20competition%20by%20coupling%20emulators%20inspired%20by%20the%20winning%20teams%27%20architectures%20to%20an%20interactive%20climate%20model%20%28including%20full%20cloud%20microphysics%2C%20a%20regime%20historically%20prone%20to%20online%20instability%29%20and%20systematically%20evaluating%20their%20online%20performance.%20Our%20results%20demonstrate%20that%20online%20stability%20in%20the%20low-resolution%2C%20real-geography%20setting%20is%20reproducible%20across%20multiple%20diverse%20architectures%2C%20which%20we%20consider%20a%20key%20milestone.%20All%20tested%20architectures%20exhibit%20strikingly%20similar%20offline%20and%20online%20biases%2C%20though%20their%20responses%20to%20architecture-agnostic%20design%20choices%20%28e.g.%2C%20expanding%20the%20list%20of%20input%20variables%29%20can%20differ%20significantly.%20Multiple%20Kaggle-inspired%20architectures%20achieve%20state-of-the-art%20%28SOTA%29%20results%20on%20certain%20metrics%20such%20as%20zonal%20mean%20bias%20patterns%20and%20global%20RMSE%2C%20indicating%20that%20crowdsourcing%20the%20essence%20of%20the%20offline%20problem%20is%20one%20path%20to%20improving%20online%20performance%20in%20hybrid%20physics-AI%20climate%20simulation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20963v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrowdsourcing%2520the%2520Frontier%253A%2520Advancing%2520Hybrid%2520Physics-ML%2520Climate%2520Simulation%2520via%2520a%2520%252450%252C000%2520Kaggle%2520Competition%26entry.906535625%3DJerry%2520Lin%2520and%2520Zeyuan%2520Hu%2520and%2520Tom%2520Beucler%2520and%2520Katherine%2520Frields%2520and%2520Hannah%2520Christensen%2520and%2520Walter%2520Hannah%2520and%2520Helge%2520Heuer%2520and%2520Peter%2520Ukkonnen%2520and%2520Laura%2520A.%2520Mansfield%2520and%2520Tian%2520Zheng%2520and%2520Liran%2520Peng%2520and%2520Ritwik%2520Gupta%2520and%2520Pierre%2520Gentine%2520and%2520Yusef%2520Al-Naher%2520and%2520Mingjiang%2520Duan%2520and%2520Kyo%2520Hattori%2520and%2520Weiliang%2520Ji%2520and%2520Chunhan%2520Li%2520and%2520Kippei%2520Matsuda%2520and%2520Naoki%2520Murakami%2520and%2520Shlomo%2520Ron%2520and%2520Marec%2520Serlin%2520and%2520Hongjian%2520Song%2520and%2520Yuma%2520Tanabe%2520and%2520Daisuke%2520Yamamoto%2520and%2520Jianyao%2520Zhou%2520and%2520Mike%2520Pritchard%26entry.1292438233%3DSubgrid%2520machine-learning%2520%2528ML%2529%2520parameterizations%2520have%2520the%2520potential%2520to%2520introduce%2520a%2520new%2520generation%2520of%2520climate%2520models%2520that%2520incorporate%2520the%2520effects%2520of%2520higher-resolution%2520physics%2520without%2520incurring%2520the%2520prohibitive%2520computational%2520cost%2520associated%2520with%2520more%2520explicit%2520physics-based%2520simulations.%2520However%252C%2520important%2520issues%252C%2520ranging%2520from%2520online%2520instability%2520to%2520inconsistent%2520online%2520performance%252C%2520have%2520limited%2520their%2520operational%2520use%2520for%2520long-term%2520climate%2520projections.%2520To%2520more%2520rapidly%2520drive%2520progress%2520in%2520solving%2520these%2520issues%252C%2520domain%2520scientists%2520and%2520machine%2520learning%2520researchers%2520opened%2520up%2520the%2520offline%2520aspect%2520of%2520this%2520problem%2520to%2520the%2520broader%2520machine%2520learning%2520and%2520data%2520science%2520community%2520with%2520the%2520release%2520of%2520ClimSim%252C%2520a%2520NeurIPS%2520Datasets%2520and%2520Benchmarks%2520publication%252C%2520and%2520an%2520associated%2520Kaggle%2520competition.%2520This%2520paper%2520reports%2520on%2520the%2520downstream%2520results%2520of%2520the%2520Kaggle%2520competition%2520by%2520coupling%2520emulators%2520inspired%2520by%2520the%2520winning%2520teams%2527%2520architectures%2520to%2520an%2520interactive%2520climate%2520model%2520%2528including%2520full%2520cloud%2520microphysics%252C%2520a%2520regime%2520historically%2520prone%2520to%2520online%2520instability%2529%2520and%2520systematically%2520evaluating%2520their%2520online%2520performance.%2520Our%2520results%2520demonstrate%2520that%2520online%2520stability%2520in%2520the%2520low-resolution%252C%2520real-geography%2520setting%2520is%2520reproducible%2520across%2520multiple%2520diverse%2520architectures%252C%2520which%2520we%2520consider%2520a%2520key%2520milestone.%2520All%2520tested%2520architectures%2520exhibit%2520strikingly%2520similar%2520offline%2520and%2520online%2520biases%252C%2520though%2520their%2520responses%2520to%2520architecture-agnostic%2520design%2520choices%2520%2528e.g.%252C%2520expanding%2520the%2520list%2520of%2520input%2520variables%2529%2520can%2520differ%2520significantly.%2520Multiple%2520Kaggle-inspired%2520architectures%2520achieve%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520on%2520certain%2520metrics%2520such%2520as%2520zonal%2520mean%2520bias%2520patterns%2520and%2520global%2520RMSE%252C%2520indicating%2520that%2520crowdsourcing%2520the%2520essence%2520of%2520the%2520offline%2520problem%2520is%2520one%2520path%2520to%2520improving%2520online%2520performance%2520in%2520hybrid%2520physics-AI%2520climate%2520simulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20963v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crowdsourcing%20the%20Frontier%3A%20Advancing%20Hybrid%20Physics-ML%20Climate%20Simulation%20via%20a%20%2450%2C000%20Kaggle%20Competition&entry.906535625=Jerry%20Lin%20and%20Zeyuan%20Hu%20and%20Tom%20Beucler%20and%20Katherine%20Frields%20and%20Hannah%20Christensen%20and%20Walter%20Hannah%20and%20Helge%20Heuer%20and%20Peter%20Ukkonnen%20and%20Laura%20A.%20Mansfield%20and%20Tian%20Zheng%20and%20Liran%20Peng%20and%20Ritwik%20Gupta%20and%20Pierre%20Gentine%20and%20Yusef%20Al-Naher%20and%20Mingjiang%20Duan%20and%20Kyo%20Hattori%20and%20Weiliang%20Ji%20and%20Chunhan%20Li%20and%20Kippei%20Matsuda%20and%20Naoki%20Murakami%20and%20Shlomo%20Ron%20and%20Marec%20Serlin%20and%20Hongjian%20Song%20and%20Yuma%20Tanabe%20and%20Daisuke%20Yamamoto%20and%20Jianyao%20Zhou%20and%20Mike%20Pritchard&entry.1292438233=Subgrid%20machine-learning%20%28ML%29%20parameterizations%20have%20the%20potential%20to%20introduce%20a%20new%20generation%20of%20climate%20models%20that%20incorporate%20the%20effects%20of%20higher-resolution%20physics%20without%20incurring%20the%20prohibitive%20computational%20cost%20associated%20with%20more%20explicit%20physics-based%20simulations.%20However%2C%20important%20issues%2C%20ranging%20from%20online%20instability%20to%20inconsistent%20online%20performance%2C%20have%20limited%20their%20operational%20use%20for%20long-term%20climate%20projections.%20To%20more%20rapidly%20drive%20progress%20in%20solving%20these%20issues%2C%20domain%20scientists%20and%20machine%20learning%20researchers%20opened%20up%20the%20offline%20aspect%20of%20this%20problem%20to%20the%20broader%20machine%20learning%20and%20data%20science%20community%20with%20the%20release%20of%20ClimSim%2C%20a%20NeurIPS%20Datasets%20and%20Benchmarks%20publication%2C%20and%20an%20associated%20Kaggle%20competition.%20This%20paper%20reports%20on%20the%20downstream%20results%20of%20the%20Kaggle%20competition%20by%20coupling%20emulators%20inspired%20by%20the%20winning%20teams%27%20architectures%20to%20an%20interactive%20climate%20model%20%28including%20full%20cloud%20microphysics%2C%20a%20regime%20historically%20prone%20to%20online%20instability%29%20and%20systematically%20evaluating%20their%20online%20performance.%20Our%20results%20demonstrate%20that%20online%20stability%20in%20the%20low-resolution%2C%20real-geography%20setting%20is%20reproducible%20across%20multiple%20diverse%20architectures%2C%20which%20we%20consider%20a%20key%20milestone.%20All%20tested%20architectures%20exhibit%20strikingly%20similar%20offline%20and%20online%20biases%2C%20though%20their%20responses%20to%20architecture-agnostic%20design%20choices%20%28e.g.%2C%20expanding%20the%20list%20of%20input%20variables%29%20can%20differ%20significantly.%20Multiple%20Kaggle-inspired%20architectures%20achieve%20state-of-the-art%20%28SOTA%29%20results%20on%20certain%20metrics%20such%20as%20zonal%20mean%20bias%20patterns%20and%20global%20RMSE%2C%20indicating%20that%20crowdsourcing%20the%20essence%20of%20the%20offline%20problem%20is%20one%20path%20to%20improving%20online%20performance%20in%20hybrid%20physics-AI%20climate%20simulation.&entry.1838667208=http%3A//arxiv.org/abs/2511.20963v2&entry.124074799=Read"},
{"title": "SAEmnesia: Erasing Concepts in Diffusion Models with Supervised Sparse Autoencoders", "author": "Enrico Cassano and Riccardo Renzulli and Marco Nurisso and Mirko Zaffaroni and Alan Perotti and Marco Grangetto", "abstract": "Concept unlearning in diffusion models is hampered by feature splitting, where concepts are distributed across many latent features, making their removal challenging and computationally expensive. We introduce SAEmnesia, a supervised sparse autoencoder framework that overcomes this by enforcing one-to-one concept-neuron mappings. By systematically labeling concepts during training, our method achieves feature centralization, binding each concept to a single, interpretable neuron. This enables highly targeted and efficient concept erasure. SAEmnesia reduces hyperparameter search by 96.7% and achieves a 9.2% improvement over the state-of-the-art on the UnlearnCanvas benchmark. Our method also demonstrates superior scalability in sequential unlearning, improving accuracy by 28.4% when removing nine objects, establishing a new standard for precise and controllable concept erasure. Moreover, SAEmnesia mitigates the possibility of generating unwanted content under adversarial attack and effectively removes nudity when evaluated with I2P.", "link": "http://arxiv.org/abs/2509.21379v2", "date": "2025-11-28", "relevancy": 2.0681, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5421}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5158}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAEmnesia%3A%20Erasing%20Concepts%20in%20Diffusion%20Models%20with%20Supervised%20Sparse%20Autoencoders&body=Title%3A%20SAEmnesia%3A%20Erasing%20Concepts%20in%20Diffusion%20Models%20with%20Supervised%20Sparse%20Autoencoders%0AAuthor%3A%20Enrico%20Cassano%20and%20Riccardo%20Renzulli%20and%20Marco%20Nurisso%20and%20Mirko%20Zaffaroni%20and%20Alan%20Perotti%20and%20Marco%20Grangetto%0AAbstract%3A%20Concept%20unlearning%20in%20diffusion%20models%20is%20hampered%20by%20feature%20splitting%2C%20where%20concepts%20are%20distributed%20across%20many%20latent%20features%2C%20making%20their%20removal%20challenging%20and%20computationally%20expensive.%20We%20introduce%20SAEmnesia%2C%20a%20supervised%20sparse%20autoencoder%20framework%20that%20overcomes%20this%20by%20enforcing%20one-to-one%20concept-neuron%20mappings.%20By%20systematically%20labeling%20concepts%20during%20training%2C%20our%20method%20achieves%20feature%20centralization%2C%20binding%20each%20concept%20to%20a%20single%2C%20interpretable%20neuron.%20This%20enables%20highly%20targeted%20and%20efficient%20concept%20erasure.%20SAEmnesia%20reduces%20hyperparameter%20search%20by%2096.7%25%20and%20achieves%20a%209.2%25%20improvement%20over%20the%20state-of-the-art%20on%20the%20UnlearnCanvas%20benchmark.%20Our%20method%20also%20demonstrates%20superior%20scalability%20in%20sequential%20unlearning%2C%20improving%20accuracy%20by%2028.4%25%20when%20removing%20nine%20objects%2C%20establishing%20a%20new%20standard%20for%20precise%20and%20controllable%20concept%20erasure.%20Moreover%2C%20SAEmnesia%20mitigates%20the%20possibility%20of%20generating%20unwanted%20content%20under%20adversarial%20attack%20and%20effectively%20removes%20nudity%20when%20evaluated%20with%20I2P.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAEmnesia%253A%2520Erasing%2520Concepts%2520in%2520Diffusion%2520Models%2520with%2520Supervised%2520Sparse%2520Autoencoders%26entry.906535625%3DEnrico%2520Cassano%2520and%2520Riccardo%2520Renzulli%2520and%2520Marco%2520Nurisso%2520and%2520Mirko%2520Zaffaroni%2520and%2520Alan%2520Perotti%2520and%2520Marco%2520Grangetto%26entry.1292438233%3DConcept%2520unlearning%2520in%2520diffusion%2520models%2520is%2520hampered%2520by%2520feature%2520splitting%252C%2520where%2520concepts%2520are%2520distributed%2520across%2520many%2520latent%2520features%252C%2520making%2520their%2520removal%2520challenging%2520and%2520computationally%2520expensive.%2520We%2520introduce%2520SAEmnesia%252C%2520a%2520supervised%2520sparse%2520autoencoder%2520framework%2520that%2520overcomes%2520this%2520by%2520enforcing%2520one-to-one%2520concept-neuron%2520mappings.%2520By%2520systematically%2520labeling%2520concepts%2520during%2520training%252C%2520our%2520method%2520achieves%2520feature%2520centralization%252C%2520binding%2520each%2520concept%2520to%2520a%2520single%252C%2520interpretable%2520neuron.%2520This%2520enables%2520highly%2520targeted%2520and%2520efficient%2520concept%2520erasure.%2520SAEmnesia%2520reduces%2520hyperparameter%2520search%2520by%252096.7%2525%2520and%2520achieves%2520a%25209.2%2525%2520improvement%2520over%2520the%2520state-of-the-art%2520on%2520the%2520UnlearnCanvas%2520benchmark.%2520Our%2520method%2520also%2520demonstrates%2520superior%2520scalability%2520in%2520sequential%2520unlearning%252C%2520improving%2520accuracy%2520by%252028.4%2525%2520when%2520removing%2520nine%2520objects%252C%2520establishing%2520a%2520new%2520standard%2520for%2520precise%2520and%2520controllable%2520concept%2520erasure.%2520Moreover%252C%2520SAEmnesia%2520mitigates%2520the%2520possibility%2520of%2520generating%2520unwanted%2520content%2520under%2520adversarial%2520attack%2520and%2520effectively%2520removes%2520nudity%2520when%2520evaluated%2520with%2520I2P.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAEmnesia%3A%20Erasing%20Concepts%20in%20Diffusion%20Models%20with%20Supervised%20Sparse%20Autoencoders&entry.906535625=Enrico%20Cassano%20and%20Riccardo%20Renzulli%20and%20Marco%20Nurisso%20and%20Mirko%20Zaffaroni%20and%20Alan%20Perotti%20and%20Marco%20Grangetto&entry.1292438233=Concept%20unlearning%20in%20diffusion%20models%20is%20hampered%20by%20feature%20splitting%2C%20where%20concepts%20are%20distributed%20across%20many%20latent%20features%2C%20making%20their%20removal%20challenging%20and%20computationally%20expensive.%20We%20introduce%20SAEmnesia%2C%20a%20supervised%20sparse%20autoencoder%20framework%20that%20overcomes%20this%20by%20enforcing%20one-to-one%20concept-neuron%20mappings.%20By%20systematically%20labeling%20concepts%20during%20training%2C%20our%20method%20achieves%20feature%20centralization%2C%20binding%20each%20concept%20to%20a%20single%2C%20interpretable%20neuron.%20This%20enables%20highly%20targeted%20and%20efficient%20concept%20erasure.%20SAEmnesia%20reduces%20hyperparameter%20search%20by%2096.7%25%20and%20achieves%20a%209.2%25%20improvement%20over%20the%20state-of-the-art%20on%20the%20UnlearnCanvas%20benchmark.%20Our%20method%20also%20demonstrates%20superior%20scalability%20in%20sequential%20unlearning%2C%20improving%20accuracy%20by%2028.4%25%20when%20removing%20nine%20objects%2C%20establishing%20a%20new%20standard%20for%20precise%20and%20controllable%20concept%20erasure.%20Moreover%2C%20SAEmnesia%20mitigates%20the%20possibility%20of%20generating%20unwanted%20content%20under%20adversarial%20attack%20and%20effectively%20removes%20nudity%20when%20evaluated%20with%20I2P.&entry.1838667208=http%3A//arxiv.org/abs/2509.21379v2&entry.124074799=Read"},
{"title": "The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference", "author": "Hans Gundlach and Jayson Lynch and Matthias Mertens and Neil Thompson", "abstract": "Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\\times$ to $10\\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.", "link": "http://arxiv.org/abs/2511.23455v1", "date": "2025-11-28", "relevancy": 1.2606, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4237}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4203}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Price%20of%20Progress%3A%20Algorithmic%20Efficiency%20and%20the%20Falling%20Cost%20of%20AI%20Inference&body=Title%3A%20The%20Price%20of%20Progress%3A%20Algorithmic%20Efficiency%20and%20the%20Falling%20Cost%20of%20AI%20Inference%0AAuthor%3A%20Hans%20Gundlach%20and%20Jayson%20Lynch%20and%20Matthias%20Mertens%20and%20Neil%20Thompson%0AAbstract%3A%20Language%20models%20have%20seen%20enormous%20progress%20on%20advanced%20benchmarks%20in%20recent%20years%2C%20but%20much%20of%20this%20progress%20has%20only%20been%20possible%20by%20using%20more%20costly%20models.%20Benchmarks%20may%20therefore%20present%20a%20warped%20picture%20of%20progress%20in%20practical%20capabilities%20per%20dollar.%20To%20remedy%20this%2C%20we%20use%20data%20from%20Artificial%20Analysis%20and%20Epoch%20AI%20to%20form%20the%20largest%20dataset%20of%20current%20and%20historical%20prices%20to%20run%20benchmarks%20to%20date.%20We%20find%20that%20the%20price%20for%20a%20given%20level%20of%20benchmark%20performance%20has%20decreased%20remarkably%20fast%2C%20around%20%245%5Ctimes%24%20to%20%2410%5Ctimes%24%20per%20year%2C%20for%20frontier%20models%20on%20knowledge%2C%20reasoning%2C%20math%2C%20and%20software%20engineering%20benchmarks.%20These%20reductions%20in%20the%20cost%20of%20AI%20inference%20are%20due%20to%20economic%20forces%2C%20hardware%20efficiency%20improvements%2C%20and%20algorithmic%20efficiency%20improvements.%20Isolating%20out%20open%20models%20to%20control%20for%20competition%20effects%20and%20dividing%20by%20hardware%20price%20declines%2C%20we%20estimate%20that%20algorithmic%20efficiency%20progress%20is%20around%20%243%5Ctimes%24%20per%20year.%20Finally%2C%20we%20recommend%20that%20evaluators%20both%20publicize%20and%20take%20into%20account%20the%20price%20of%20benchmarking%20as%20an%20essential%20part%20of%20measuring%20the%20real-world%20impact%20of%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Price%2520of%2520Progress%253A%2520Algorithmic%2520Efficiency%2520and%2520the%2520Falling%2520Cost%2520of%2520AI%2520Inference%26entry.906535625%3DHans%2520Gundlach%2520and%2520Jayson%2520Lynch%2520and%2520Matthias%2520Mertens%2520and%2520Neil%2520Thompson%26entry.1292438233%3DLanguage%2520models%2520have%2520seen%2520enormous%2520progress%2520on%2520advanced%2520benchmarks%2520in%2520recent%2520years%252C%2520but%2520much%2520of%2520this%2520progress%2520has%2520only%2520been%2520possible%2520by%2520using%2520more%2520costly%2520models.%2520Benchmarks%2520may%2520therefore%2520present%2520a%2520warped%2520picture%2520of%2520progress%2520in%2520practical%2520capabilities%2520per%2520dollar.%2520To%2520remedy%2520this%252C%2520we%2520use%2520data%2520from%2520Artificial%2520Analysis%2520and%2520Epoch%2520AI%2520to%2520form%2520the%2520largest%2520dataset%2520of%2520current%2520and%2520historical%2520prices%2520to%2520run%2520benchmarks%2520to%2520date.%2520We%2520find%2520that%2520the%2520price%2520for%2520a%2520given%2520level%2520of%2520benchmark%2520performance%2520has%2520decreased%2520remarkably%2520fast%252C%2520around%2520%25245%255Ctimes%2524%2520to%2520%252410%255Ctimes%2524%2520per%2520year%252C%2520for%2520frontier%2520models%2520on%2520knowledge%252C%2520reasoning%252C%2520math%252C%2520and%2520software%2520engineering%2520benchmarks.%2520These%2520reductions%2520in%2520the%2520cost%2520of%2520AI%2520inference%2520are%2520due%2520to%2520economic%2520forces%252C%2520hardware%2520efficiency%2520improvements%252C%2520and%2520algorithmic%2520efficiency%2520improvements.%2520Isolating%2520out%2520open%2520models%2520to%2520control%2520for%2520competition%2520effects%2520and%2520dividing%2520by%2520hardware%2520price%2520declines%252C%2520we%2520estimate%2520that%2520algorithmic%2520efficiency%2520progress%2520is%2520around%2520%25243%255Ctimes%2524%2520per%2520year.%2520Finally%252C%2520we%2520recommend%2520that%2520evaluators%2520both%2520publicize%2520and%2520take%2520into%2520account%2520the%2520price%2520of%2520benchmarking%2520as%2520an%2520essential%2520part%2520of%2520measuring%2520the%2520real-world%2520impact%2520of%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Price%20of%20Progress%3A%20Algorithmic%20Efficiency%20and%20the%20Falling%20Cost%20of%20AI%20Inference&entry.906535625=Hans%20Gundlach%20and%20Jayson%20Lynch%20and%20Matthias%20Mertens%20and%20Neil%20Thompson&entry.1292438233=Language%20models%20have%20seen%20enormous%20progress%20on%20advanced%20benchmarks%20in%20recent%20years%2C%20but%20much%20of%20this%20progress%20has%20only%20been%20possible%20by%20using%20more%20costly%20models.%20Benchmarks%20may%20therefore%20present%20a%20warped%20picture%20of%20progress%20in%20practical%20capabilities%20per%20dollar.%20To%20remedy%20this%2C%20we%20use%20data%20from%20Artificial%20Analysis%20and%20Epoch%20AI%20to%20form%20the%20largest%20dataset%20of%20current%20and%20historical%20prices%20to%20run%20benchmarks%20to%20date.%20We%20find%20that%20the%20price%20for%20a%20given%20level%20of%20benchmark%20performance%20has%20decreased%20remarkably%20fast%2C%20around%20%245%5Ctimes%24%20to%20%2410%5Ctimes%24%20per%20year%2C%20for%20frontier%20models%20on%20knowledge%2C%20reasoning%2C%20math%2C%20and%20software%20engineering%20benchmarks.%20These%20reductions%20in%20the%20cost%20of%20AI%20inference%20are%20due%20to%20economic%20forces%2C%20hardware%20efficiency%20improvements%2C%20and%20algorithmic%20efficiency%20improvements.%20Isolating%20out%20open%20models%20to%20control%20for%20competition%20effects%20and%20dividing%20by%20hardware%20price%20declines%2C%20we%20estimate%20that%20algorithmic%20efficiency%20progress%20is%20around%20%243%5Ctimes%24%20per%20year.%20Finally%2C%20we%20recommend%20that%20evaluators%20both%20publicize%20and%20take%20into%20account%20the%20price%20of%20benchmarking%20as%20an%20essential%20part%20of%20measuring%20the%20real-world%20impact%20of%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2511.23455v1&entry.124074799=Read"},
{"title": "NumeriKontrol: Adding Numeric Control to Diffusion Transformers for Instruction-based Image Editing", "author": "Zhenyu Xu and Xiaoqi Shen and Haotian Nan and Xinyu Zhang", "abstract": "Instruction-based image editing enables intuitive manipulation through natural language commands. However, text instructions alone often lack the precision required for fine-grained control over edit intensity. We introduce NumeriKontrol, a framework that allows users to precisely adjust image attributes using continuous scalar values with common units. NumeriKontrol encodes numeric editing scales via an effective Numeric Adapter and injects them into diffusion models in a plug-and-play manner. Thanks to a task-separated design, our approach supports zero-shot multi-condition editing, allowing users to specify multiple instructions in any order. To provide high-quality supervision, we synthesize precise training data from reliable sources, including high-fidelity rendering engines and DSLR cameras. Our Common Attribute Transform (CAT) dataset covers diverse attribute manipulations with accurate ground-truth scales, enabling NumeriKontrol to function as a simple yet powerful interactive editing studio. Extensive experiments show that NumeriKontrol delivers accurate, continuous, and stable scale control across a wide range of attribute editing scenarios. These contributions advance instruction-based image editing by enabling precise, scalable, and user-controllable image manipulation.", "link": "http://arxiv.org/abs/2511.23105v1", "date": "2025-11-28", "relevancy": 1.1453, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5777}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5703}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NumeriKontrol%3A%20Adding%20Numeric%20Control%20to%20Diffusion%20Transformers%20for%20Instruction-based%20Image%20Editing&body=Title%3A%20NumeriKontrol%3A%20Adding%20Numeric%20Control%20to%20Diffusion%20Transformers%20for%20Instruction-based%20Image%20Editing%0AAuthor%3A%20Zhenyu%20Xu%20and%20Xiaoqi%20Shen%20and%20Haotian%20Nan%20and%20Xinyu%20Zhang%0AAbstract%3A%20Instruction-based%20image%20editing%20enables%20intuitive%20manipulation%20through%20natural%20language%20commands.%20However%2C%20text%20instructions%20alone%20often%20lack%20the%20precision%20required%20for%20fine-grained%20control%20over%20edit%20intensity.%20We%20introduce%20NumeriKontrol%2C%20a%20framework%20that%20allows%20users%20to%20precisely%20adjust%20image%20attributes%20using%20continuous%20scalar%20values%20with%20common%20units.%20NumeriKontrol%20encodes%20numeric%20editing%20scales%20via%20an%20effective%20Numeric%20Adapter%20and%20injects%20them%20into%20diffusion%20models%20in%20a%20plug-and-play%20manner.%20Thanks%20to%20a%20task-separated%20design%2C%20our%20approach%20supports%20zero-shot%20multi-condition%20editing%2C%20allowing%20users%20to%20specify%20multiple%20instructions%20in%20any%20order.%20To%20provide%20high-quality%20supervision%2C%20we%20synthesize%20precise%20training%20data%20from%20reliable%20sources%2C%20including%20high-fidelity%20rendering%20engines%20and%20DSLR%20cameras.%20Our%20Common%20Attribute%20Transform%20%28CAT%29%20dataset%20covers%20diverse%20attribute%20manipulations%20with%20accurate%20ground-truth%20scales%2C%20enabling%20NumeriKontrol%20to%20function%20as%20a%20simple%20yet%20powerful%20interactive%20editing%20studio.%20Extensive%20experiments%20show%20that%20NumeriKontrol%20delivers%20accurate%2C%20continuous%2C%20and%20stable%20scale%20control%20across%20a%20wide%20range%20of%20attribute%20editing%20scenarios.%20These%20contributions%20advance%20instruction-based%20image%20editing%20by%20enabling%20precise%2C%20scalable%2C%20and%20user-controllable%20image%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNumeriKontrol%253A%2520Adding%2520Numeric%2520Control%2520to%2520Diffusion%2520Transformers%2520for%2520Instruction-based%2520Image%2520Editing%26entry.906535625%3DZhenyu%2520Xu%2520and%2520Xiaoqi%2520Shen%2520and%2520Haotian%2520Nan%2520and%2520Xinyu%2520Zhang%26entry.1292438233%3DInstruction-based%2520image%2520editing%2520enables%2520intuitive%2520manipulation%2520through%2520natural%2520language%2520commands.%2520However%252C%2520text%2520instructions%2520alone%2520often%2520lack%2520the%2520precision%2520required%2520for%2520fine-grained%2520control%2520over%2520edit%2520intensity.%2520We%2520introduce%2520NumeriKontrol%252C%2520a%2520framework%2520that%2520allows%2520users%2520to%2520precisely%2520adjust%2520image%2520attributes%2520using%2520continuous%2520scalar%2520values%2520with%2520common%2520units.%2520NumeriKontrol%2520encodes%2520numeric%2520editing%2520scales%2520via%2520an%2520effective%2520Numeric%2520Adapter%2520and%2520injects%2520them%2520into%2520diffusion%2520models%2520in%2520a%2520plug-and-play%2520manner.%2520Thanks%2520to%2520a%2520task-separated%2520design%252C%2520our%2520approach%2520supports%2520zero-shot%2520multi-condition%2520editing%252C%2520allowing%2520users%2520to%2520specify%2520multiple%2520instructions%2520in%2520any%2520order.%2520To%2520provide%2520high-quality%2520supervision%252C%2520we%2520synthesize%2520precise%2520training%2520data%2520from%2520reliable%2520sources%252C%2520including%2520high-fidelity%2520rendering%2520engines%2520and%2520DSLR%2520cameras.%2520Our%2520Common%2520Attribute%2520Transform%2520%2528CAT%2529%2520dataset%2520covers%2520diverse%2520attribute%2520manipulations%2520with%2520accurate%2520ground-truth%2520scales%252C%2520enabling%2520NumeriKontrol%2520to%2520function%2520as%2520a%2520simple%2520yet%2520powerful%2520interactive%2520editing%2520studio.%2520Extensive%2520experiments%2520show%2520that%2520NumeriKontrol%2520delivers%2520accurate%252C%2520continuous%252C%2520and%2520stable%2520scale%2520control%2520across%2520a%2520wide%2520range%2520of%2520attribute%2520editing%2520scenarios.%2520These%2520contributions%2520advance%2520instruction-based%2520image%2520editing%2520by%2520enabling%2520precise%252C%2520scalable%252C%2520and%2520user-controllable%2520image%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NumeriKontrol%3A%20Adding%20Numeric%20Control%20to%20Diffusion%20Transformers%20for%20Instruction-based%20Image%20Editing&entry.906535625=Zhenyu%20Xu%20and%20Xiaoqi%20Shen%20and%20Haotian%20Nan%20and%20Xinyu%20Zhang&entry.1292438233=Instruction-based%20image%20editing%20enables%20intuitive%20manipulation%20through%20natural%20language%20commands.%20However%2C%20text%20instructions%20alone%20often%20lack%20the%20precision%20required%20for%20fine-grained%20control%20over%20edit%20intensity.%20We%20introduce%20NumeriKontrol%2C%20a%20framework%20that%20allows%20users%20to%20precisely%20adjust%20image%20attributes%20using%20continuous%20scalar%20values%20with%20common%20units.%20NumeriKontrol%20encodes%20numeric%20editing%20scales%20via%20an%20effective%20Numeric%20Adapter%20and%20injects%20them%20into%20diffusion%20models%20in%20a%20plug-and-play%20manner.%20Thanks%20to%20a%20task-separated%20design%2C%20our%20approach%20supports%20zero-shot%20multi-condition%20editing%2C%20allowing%20users%20to%20specify%20multiple%20instructions%20in%20any%20order.%20To%20provide%20high-quality%20supervision%2C%20we%20synthesize%20precise%20training%20data%20from%20reliable%20sources%2C%20including%20high-fidelity%20rendering%20engines%20and%20DSLR%20cameras.%20Our%20Common%20Attribute%20Transform%20%28CAT%29%20dataset%20covers%20diverse%20attribute%20manipulations%20with%20accurate%20ground-truth%20scales%2C%20enabling%20NumeriKontrol%20to%20function%20as%20a%20simple%20yet%20powerful%20interactive%20editing%20studio.%20Extensive%20experiments%20show%20that%20NumeriKontrol%20delivers%20accurate%2C%20continuous%2C%20and%20stable%20scale%20control%20across%20a%20wide%20range%20of%20attribute%20editing%20scenarios.%20These%20contributions%20advance%20instruction-based%20image%20editing%20by%20enabling%20precise%2C%20scalable%2C%20and%20user-controllable%20image%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2511.23105v1&entry.124074799=Read"},
{"title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities", "author": "Aayush Garg and Zanis Ali Khan and Renzo Degiovanni and Qiang Tang", "abstract": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.", "link": "http://arxiv.org/abs/2511.23408v1", "date": "2025-11-28", "relevancy": 1.7228, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4533}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20LLMs%20for%20One-Shot%20Patching%20of%20Real%20and%20Artificial%20Vulnerabilities&body=Title%3A%20Evaluating%20LLMs%20for%20One-Shot%20Patching%20of%20Real%20and%20Artificial%20Vulnerabilities%0AAuthor%3A%20Aayush%20Garg%20and%20Zanis%20Ali%20Khan%20and%20Renzo%20Degiovanni%20and%20Qiang%20Tang%0AAbstract%3A%20Automated%20vulnerability%20patching%20is%20crucial%20for%20software%20security%2C%20and%20recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20present%20promising%20capabilities%20for%20automating%20this%20task.%20However%2C%20existing%20research%20has%20primarily%20assessed%20LLMs%20using%20publicly%20disclosed%20vulnerabilities%2C%20leaving%20their%20effectiveness%20on%20related%20artificial%20vulnerabilities%20largely%20unexplored.%20In%20this%20study%2C%20we%20empirically%20evaluate%20the%20patching%20effectiveness%20and%20complementarity%20of%20several%20prominent%20LLMs%2C%20such%20as%20OpenAI%27s%20GPT%20variants%2C%20LLaMA%2C%20DeepSeek%2C%20and%20Mistral%20models%2C%20using%20both%20real%20and%20artificial%20vulnerabilities.%20Our%20evaluation%20employs%20Proof-of-Vulnerability%20%28PoV%29%20test%20execution%20to%20concretely%20assess%20whether%20LLM-generated%20source%20code%20successfully%20patches%20vulnerabilities.%20Our%20results%20reveal%20that%20LLMs%20patch%20real%20vulnerabilities%20more%20effectively%20compared%20to%20artificial%20ones.%20Additionally%2C%20our%20analysis%20reveals%20significant%20variability%20across%20LLMs%20in%20terms%20of%20overlapping%20%28multiple%20LLMs%20patching%20the%20same%20vulnerabilities%29%20and%20complementarity%20%28vulnerabilities%20patched%20exclusively%20by%20a%20single%20LLM%29%2C%20emphasizing%20the%20importance%20of%20selecting%20appropriate%20LLMs%20for%20effective%20vulnerability%20patching.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520LLMs%2520for%2520One-Shot%2520Patching%2520of%2520Real%2520and%2520Artificial%2520Vulnerabilities%26entry.906535625%3DAayush%2520Garg%2520and%2520Zanis%2520Ali%2520Khan%2520and%2520Renzo%2520Degiovanni%2520and%2520Qiang%2520Tang%26entry.1292438233%3DAutomated%2520vulnerability%2520patching%2520is%2520crucial%2520for%2520software%2520security%252C%2520and%2520recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520present%2520promising%2520capabilities%2520for%2520automating%2520this%2520task.%2520However%252C%2520existing%2520research%2520has%2520primarily%2520assessed%2520LLMs%2520using%2520publicly%2520disclosed%2520vulnerabilities%252C%2520leaving%2520their%2520effectiveness%2520on%2520related%2520artificial%2520vulnerabilities%2520largely%2520unexplored.%2520In%2520this%2520study%252C%2520we%2520empirically%2520evaluate%2520the%2520patching%2520effectiveness%2520and%2520complementarity%2520of%2520several%2520prominent%2520LLMs%252C%2520such%2520as%2520OpenAI%2527s%2520GPT%2520variants%252C%2520LLaMA%252C%2520DeepSeek%252C%2520and%2520Mistral%2520models%252C%2520using%2520both%2520real%2520and%2520artificial%2520vulnerabilities.%2520Our%2520evaluation%2520employs%2520Proof-of-Vulnerability%2520%2528PoV%2529%2520test%2520execution%2520to%2520concretely%2520assess%2520whether%2520LLM-generated%2520source%2520code%2520successfully%2520patches%2520vulnerabilities.%2520Our%2520results%2520reveal%2520that%2520LLMs%2520patch%2520real%2520vulnerabilities%2520more%2520effectively%2520compared%2520to%2520artificial%2520ones.%2520Additionally%252C%2520our%2520analysis%2520reveals%2520significant%2520variability%2520across%2520LLMs%2520in%2520terms%2520of%2520overlapping%2520%2528multiple%2520LLMs%2520patching%2520the%2520same%2520vulnerabilities%2529%2520and%2520complementarity%2520%2528vulnerabilities%2520patched%2520exclusively%2520by%2520a%2520single%2520LLM%2529%252C%2520emphasizing%2520the%2520importance%2520of%2520selecting%2520appropriate%2520LLMs%2520for%2520effective%2520vulnerability%2520patching.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20LLMs%20for%20One-Shot%20Patching%20of%20Real%20and%20Artificial%20Vulnerabilities&entry.906535625=Aayush%20Garg%20and%20Zanis%20Ali%20Khan%20and%20Renzo%20Degiovanni%20and%20Qiang%20Tang&entry.1292438233=Automated%20vulnerability%20patching%20is%20crucial%20for%20software%20security%2C%20and%20recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20present%20promising%20capabilities%20for%20automating%20this%20task.%20However%2C%20existing%20research%20has%20primarily%20assessed%20LLMs%20using%20publicly%20disclosed%20vulnerabilities%2C%20leaving%20their%20effectiveness%20on%20related%20artificial%20vulnerabilities%20largely%20unexplored.%20In%20this%20study%2C%20we%20empirically%20evaluate%20the%20patching%20effectiveness%20and%20complementarity%20of%20several%20prominent%20LLMs%2C%20such%20as%20OpenAI%27s%20GPT%20variants%2C%20LLaMA%2C%20DeepSeek%2C%20and%20Mistral%20models%2C%20using%20both%20real%20and%20artificial%20vulnerabilities.%20Our%20evaluation%20employs%20Proof-of-Vulnerability%20%28PoV%29%20test%20execution%20to%20concretely%20assess%20whether%20LLM-generated%20source%20code%20successfully%20patches%20vulnerabilities.%20Our%20results%20reveal%20that%20LLMs%20patch%20real%20vulnerabilities%20more%20effectively%20compared%20to%20artificial%20ones.%20Additionally%2C%20our%20analysis%20reveals%20significant%20variability%20across%20LLMs%20in%20terms%20of%20overlapping%20%28multiple%20LLMs%20patching%20the%20same%20vulnerabilities%29%20and%20complementarity%20%28vulnerabilities%20patched%20exclusively%20by%20a%20single%20LLM%29%2C%20emphasizing%20the%20importance%20of%20selecting%20appropriate%20LLMs%20for%20effective%20vulnerability%20patching.&entry.1838667208=http%3A//arxiv.org/abs/2511.23408v1&entry.124074799=Read"},
{"title": "Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation", "author": "Bernhard Klein and Falk Selker and Hendrik Borras and Sophie Steger and Franz Pernkopf and Holger Fr\u00f6ning", "abstract": "Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.", "link": "http://arxiv.org/abs/2511.23440v1", "date": "2025-11-28", "relevancy": 1.5993, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5434}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5389}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Execution%20of%20Bayesian%20Neural%20Networks%20using%20a%20Single%20Probabilistic%20Forward%20Pass%20and%20Code%20Generation&body=Title%3A%20Accelerated%20Execution%20of%20Bayesian%20Neural%20Networks%20using%20a%20Single%20Probabilistic%20Forward%20Pass%20and%20Code%20Generation%0AAuthor%3A%20Bernhard%20Klein%20and%20Falk%20Selker%20and%20Hendrik%20Borras%20and%20Sophie%20Steger%20and%20Franz%20Pernkopf%20and%20Holger%20Fr%C3%B6ning%0AAbstract%3A%20Machine%20learning%20models%20perform%20well%20across%20domains%20such%20as%20diagnostics%2C%20weather%20forecasting%2C%20NLP%2C%20and%20autonomous%20driving%2C%20but%20their%20limited%20uncertainty%20handling%20restricts%20use%20in%20safety-critical%20settings.%20Traditional%20neural%20networks%20often%20fail%20to%20detect%20out-of-domain%20%28OOD%29%20data%20and%20may%20output%20confident%20yet%20incorrect%20predictions.%20Bayesian%20neural%20networks%20%28BNNs%29%20address%20this%20by%20providing%20probabilistic%20estimates%2C%20but%20incur%20high%20computational%20cost%20because%20predictions%20require%20sampling%20weight%20distributions%20and%20multiple%20forward%20passes.%20The%20Probabilistic%20Forward%20Pass%20%28PFP%29%20offers%20a%20highly%20efficient%20approximation%20to%20Stochastic%20Variational%20Inference%20%28SVI%29%20by%20assuming%20Gaussian-distributed%20weights%20and%20activations%2C%20enabling%20fully%20analytic%20uncertainty%20propagation%20and%20replacing%20sampling%20with%20a%20single%20deterministic%20forward%20pass.%20We%20present%20an%20end-to-end%20pipeline%20for%20training%2C%20compiling%2C%20optimizing%2C%20and%20deploying%20PFP-based%20BNNs%20on%20embedded%20ARM%20CPUs.%20Using%20the%20TVM%20deep%20learning%20compiler%2C%20we%20implement%20a%20dedicated%20library%20of%20Gaussian-propagating%20operators%20for%20multilayer%20perceptrons%20and%20convolutional%20neural%20networks%2C%20combined%20with%20manual%20and%20automated%20tuning%20strategies.%20Ablation%20studies%20show%20that%20PFP%20consistently%20outperforms%20SVI%20in%20computational%20efficiency%2C%20achieving%20speedups%20of%20up%20to%204200x%20for%20small%20mini-batches.%20PFP-BNNs%20match%20SVI-BNNs%20on%20Dirty-MNIST%20in%20accuracy%2C%20uncertainty%20estimation%2C%20and%20OOD%20detection%20while%20greatly%20reducing%20compute%20cost.%20These%20results%20highlight%20the%20potential%20of%20combining%20Bayesian%20approximations%20with%20code%20generation%20to%20enable%20efficient%20BNN%20deployment%20on%20resource-constrained%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Execution%2520of%2520Bayesian%2520Neural%2520Networks%2520using%2520a%2520Single%2520Probabilistic%2520Forward%2520Pass%2520and%2520Code%2520Generation%26entry.906535625%3DBernhard%2520Klein%2520and%2520Falk%2520Selker%2520and%2520Hendrik%2520Borras%2520and%2520Sophie%2520Steger%2520and%2520Franz%2520Pernkopf%2520and%2520Holger%2520Fr%25C3%25B6ning%26entry.1292438233%3DMachine%2520learning%2520models%2520perform%2520well%2520across%2520domains%2520such%2520as%2520diagnostics%252C%2520weather%2520forecasting%252C%2520NLP%252C%2520and%2520autonomous%2520driving%252C%2520but%2520their%2520limited%2520uncertainty%2520handling%2520restricts%2520use%2520in%2520safety-critical%2520settings.%2520Traditional%2520neural%2520networks%2520often%2520fail%2520to%2520detect%2520out-of-domain%2520%2528OOD%2529%2520data%2520and%2520may%2520output%2520confident%2520yet%2520incorrect%2520predictions.%2520Bayesian%2520neural%2520networks%2520%2528BNNs%2529%2520address%2520this%2520by%2520providing%2520probabilistic%2520estimates%252C%2520but%2520incur%2520high%2520computational%2520cost%2520because%2520predictions%2520require%2520sampling%2520weight%2520distributions%2520and%2520multiple%2520forward%2520passes.%2520The%2520Probabilistic%2520Forward%2520Pass%2520%2528PFP%2529%2520offers%2520a%2520highly%2520efficient%2520approximation%2520to%2520Stochastic%2520Variational%2520Inference%2520%2528SVI%2529%2520by%2520assuming%2520Gaussian-distributed%2520weights%2520and%2520activations%252C%2520enabling%2520fully%2520analytic%2520uncertainty%2520propagation%2520and%2520replacing%2520sampling%2520with%2520a%2520single%2520deterministic%2520forward%2520pass.%2520We%2520present%2520an%2520end-to-end%2520pipeline%2520for%2520training%252C%2520compiling%252C%2520optimizing%252C%2520and%2520deploying%2520PFP-based%2520BNNs%2520on%2520embedded%2520ARM%2520CPUs.%2520Using%2520the%2520TVM%2520deep%2520learning%2520compiler%252C%2520we%2520implement%2520a%2520dedicated%2520library%2520of%2520Gaussian-propagating%2520operators%2520for%2520multilayer%2520perceptrons%2520and%2520convolutional%2520neural%2520networks%252C%2520combined%2520with%2520manual%2520and%2520automated%2520tuning%2520strategies.%2520Ablation%2520studies%2520show%2520that%2520PFP%2520consistently%2520outperforms%2520SVI%2520in%2520computational%2520efficiency%252C%2520achieving%2520speedups%2520of%2520up%2520to%25204200x%2520for%2520small%2520mini-batches.%2520PFP-BNNs%2520match%2520SVI-BNNs%2520on%2520Dirty-MNIST%2520in%2520accuracy%252C%2520uncertainty%2520estimation%252C%2520and%2520OOD%2520detection%2520while%2520greatly%2520reducing%2520compute%2520cost.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520combining%2520Bayesian%2520approximations%2520with%2520code%2520generation%2520to%2520enable%2520efficient%2520BNN%2520deployment%2520on%2520resource-constrained%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Execution%20of%20Bayesian%20Neural%20Networks%20using%20a%20Single%20Probabilistic%20Forward%20Pass%20and%20Code%20Generation&entry.906535625=Bernhard%20Klein%20and%20Falk%20Selker%20and%20Hendrik%20Borras%20and%20Sophie%20Steger%20and%20Franz%20Pernkopf%20and%20Holger%20Fr%C3%B6ning&entry.1292438233=Machine%20learning%20models%20perform%20well%20across%20domains%20such%20as%20diagnostics%2C%20weather%20forecasting%2C%20NLP%2C%20and%20autonomous%20driving%2C%20but%20their%20limited%20uncertainty%20handling%20restricts%20use%20in%20safety-critical%20settings.%20Traditional%20neural%20networks%20often%20fail%20to%20detect%20out-of-domain%20%28OOD%29%20data%20and%20may%20output%20confident%20yet%20incorrect%20predictions.%20Bayesian%20neural%20networks%20%28BNNs%29%20address%20this%20by%20providing%20probabilistic%20estimates%2C%20but%20incur%20high%20computational%20cost%20because%20predictions%20require%20sampling%20weight%20distributions%20and%20multiple%20forward%20passes.%20The%20Probabilistic%20Forward%20Pass%20%28PFP%29%20offers%20a%20highly%20efficient%20approximation%20to%20Stochastic%20Variational%20Inference%20%28SVI%29%20by%20assuming%20Gaussian-distributed%20weights%20and%20activations%2C%20enabling%20fully%20analytic%20uncertainty%20propagation%20and%20replacing%20sampling%20with%20a%20single%20deterministic%20forward%20pass.%20We%20present%20an%20end-to-end%20pipeline%20for%20training%2C%20compiling%2C%20optimizing%2C%20and%20deploying%20PFP-based%20BNNs%20on%20embedded%20ARM%20CPUs.%20Using%20the%20TVM%20deep%20learning%20compiler%2C%20we%20implement%20a%20dedicated%20library%20of%20Gaussian-propagating%20operators%20for%20multilayer%20perceptrons%20and%20convolutional%20neural%20networks%2C%20combined%20with%20manual%20and%20automated%20tuning%20strategies.%20Ablation%20studies%20show%20that%20PFP%20consistently%20outperforms%20SVI%20in%20computational%20efficiency%2C%20achieving%20speedups%20of%20up%20to%204200x%20for%20small%20mini-batches.%20PFP-BNNs%20match%20SVI-BNNs%20on%20Dirty-MNIST%20in%20accuracy%2C%20uncertainty%20estimation%2C%20and%20OOD%20detection%20while%20greatly%20reducing%20compute%20cost.%20These%20results%20highlight%20the%20potential%20of%20combining%20Bayesian%20approximations%20with%20code%20generation%20to%20enable%20efficient%20BNN%20deployment%20on%20resource-constrained%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.23440v1&entry.124074799=Read"},
{"title": "Pathryoshka: Compressing Pathology Foundation Models via Multi-Teacher Knowledge Distillation with Nested Embeddings", "author": "Christian Grashei and Christian Brechenmacher and Rao Muhammad Umer and Jingsong Liu and Carsten Marr and Ewa Szczurek and Peter J. Sch\u00fcffler", "abstract": "Pathology foundation models (FMs) have driven significant progress in computational pathology. However, these high-performing models can easily exceed a billion parameters and produce high-dimensional embeddings, thus limiting their applicability for research or clinical use when computing resources are tight. Here, we introduce Pathryoshka, a multi-teacher distillation framework inspired by RADIO distillation and Matryoshka Representation Learning to reduce pathology FM sizes while allowing for adaptable embedding dimensions. We evaluate our framework with a distilled model on ten public pathology benchmarks with varying downstream tasks. Compared to its much larger teachers, Pathryoshka reduces the model size by 86-92% at on-par performance. It outperforms state-of-the-art single-teacher distillation models of comparable size by a median margin of 7.0 in accuracy. By enabling efficient local deployment without sacrificing accuracy or representational richness, Pathryoshka democratizes access to state-of-the-art pathology FMs for the broader research and clinical community.", "link": "http://arxiv.org/abs/2511.23204v1", "date": "2025-11-28", "relevancy": 1.4603, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4791}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pathryoshka%3A%20Compressing%20Pathology%20Foundation%20Models%20via%20Multi-Teacher%20Knowledge%20Distillation%20with%20Nested%20Embeddings&body=Title%3A%20Pathryoshka%3A%20Compressing%20Pathology%20Foundation%20Models%20via%20Multi-Teacher%20Knowledge%20Distillation%20with%20Nested%20Embeddings%0AAuthor%3A%20Christian%20Grashei%20and%20Christian%20Brechenmacher%20and%20Rao%20Muhammad%20Umer%20and%20Jingsong%20Liu%20and%20Carsten%20Marr%20and%20Ewa%20Szczurek%20and%20Peter%20J.%20Sch%C3%BCffler%0AAbstract%3A%20Pathology%20foundation%20models%20%28FMs%29%20have%20driven%20significant%20progress%20in%20computational%20pathology.%20However%2C%20these%20high-performing%20models%20can%20easily%20exceed%20a%20billion%20parameters%20and%20produce%20high-dimensional%20embeddings%2C%20thus%20limiting%20their%20applicability%20for%20research%20or%20clinical%20use%20when%20computing%20resources%20are%20tight.%20Here%2C%20we%20introduce%20Pathryoshka%2C%20a%20multi-teacher%20distillation%20framework%20inspired%20by%20RADIO%20distillation%20and%20Matryoshka%20Representation%20Learning%20to%20reduce%20pathology%20FM%20sizes%20while%20allowing%20for%20adaptable%20embedding%20dimensions.%20We%20evaluate%20our%20framework%20with%20a%20distilled%20model%20on%20ten%20public%20pathology%20benchmarks%20with%20varying%20downstream%20tasks.%20Compared%20to%20its%20much%20larger%20teachers%2C%20Pathryoshka%20reduces%20the%20model%20size%20by%2086-92%25%20at%20on-par%20performance.%20It%20outperforms%20state-of-the-art%20single-teacher%20distillation%20models%20of%20comparable%20size%20by%20a%20median%20margin%20of%207.0%20in%20accuracy.%20By%20enabling%20efficient%20local%20deployment%20without%20sacrificing%20accuracy%20or%20representational%20richness%2C%20Pathryoshka%20democratizes%20access%20to%20state-of-the-art%20pathology%20FMs%20for%20the%20broader%20research%20and%20clinical%20community.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathryoshka%253A%2520Compressing%2520Pathology%2520Foundation%2520Models%2520via%2520Multi-Teacher%2520Knowledge%2520Distillation%2520with%2520Nested%2520Embeddings%26entry.906535625%3DChristian%2520Grashei%2520and%2520Christian%2520Brechenmacher%2520and%2520Rao%2520Muhammad%2520Umer%2520and%2520Jingsong%2520Liu%2520and%2520Carsten%2520Marr%2520and%2520Ewa%2520Szczurek%2520and%2520Peter%2520J.%2520Sch%25C3%25BCffler%26entry.1292438233%3DPathology%2520foundation%2520models%2520%2528FMs%2529%2520have%2520driven%2520significant%2520progress%2520in%2520computational%2520pathology.%2520However%252C%2520these%2520high-performing%2520models%2520can%2520easily%2520exceed%2520a%2520billion%2520parameters%2520and%2520produce%2520high-dimensional%2520embeddings%252C%2520thus%2520limiting%2520their%2520applicability%2520for%2520research%2520or%2520clinical%2520use%2520when%2520computing%2520resources%2520are%2520tight.%2520Here%252C%2520we%2520introduce%2520Pathryoshka%252C%2520a%2520multi-teacher%2520distillation%2520framework%2520inspired%2520by%2520RADIO%2520distillation%2520and%2520Matryoshka%2520Representation%2520Learning%2520to%2520reduce%2520pathology%2520FM%2520sizes%2520while%2520allowing%2520for%2520adaptable%2520embedding%2520dimensions.%2520We%2520evaluate%2520our%2520framework%2520with%2520a%2520distilled%2520model%2520on%2520ten%2520public%2520pathology%2520benchmarks%2520with%2520varying%2520downstream%2520tasks.%2520Compared%2520to%2520its%2520much%2520larger%2520teachers%252C%2520Pathryoshka%2520reduces%2520the%2520model%2520size%2520by%252086-92%2525%2520at%2520on-par%2520performance.%2520It%2520outperforms%2520state-of-the-art%2520single-teacher%2520distillation%2520models%2520of%2520comparable%2520size%2520by%2520a%2520median%2520margin%2520of%25207.0%2520in%2520accuracy.%2520By%2520enabling%2520efficient%2520local%2520deployment%2520without%2520sacrificing%2520accuracy%2520or%2520representational%2520richness%252C%2520Pathryoshka%2520democratizes%2520access%2520to%2520state-of-the-art%2520pathology%2520FMs%2520for%2520the%2520broader%2520research%2520and%2520clinical%2520community.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pathryoshka%3A%20Compressing%20Pathology%20Foundation%20Models%20via%20Multi-Teacher%20Knowledge%20Distillation%20with%20Nested%20Embeddings&entry.906535625=Christian%20Grashei%20and%20Christian%20Brechenmacher%20and%20Rao%20Muhammad%20Umer%20and%20Jingsong%20Liu%20and%20Carsten%20Marr%20and%20Ewa%20Szczurek%20and%20Peter%20J.%20Sch%C3%BCffler&entry.1292438233=Pathology%20foundation%20models%20%28FMs%29%20have%20driven%20significant%20progress%20in%20computational%20pathology.%20However%2C%20these%20high-performing%20models%20can%20easily%20exceed%20a%20billion%20parameters%20and%20produce%20high-dimensional%20embeddings%2C%20thus%20limiting%20their%20applicability%20for%20research%20or%20clinical%20use%20when%20computing%20resources%20are%20tight.%20Here%2C%20we%20introduce%20Pathryoshka%2C%20a%20multi-teacher%20distillation%20framework%20inspired%20by%20RADIO%20distillation%20and%20Matryoshka%20Representation%20Learning%20to%20reduce%20pathology%20FM%20sizes%20while%20allowing%20for%20adaptable%20embedding%20dimensions.%20We%20evaluate%20our%20framework%20with%20a%20distilled%20model%20on%20ten%20public%20pathology%20benchmarks%20with%20varying%20downstream%20tasks.%20Compared%20to%20its%20much%20larger%20teachers%2C%20Pathryoshka%20reduces%20the%20model%20size%20by%2086-92%25%20at%20on-par%20performance.%20It%20outperforms%20state-of-the-art%20single-teacher%20distillation%20models%20of%20comparable%20size%20by%20a%20median%20margin%20of%207.0%20in%20accuracy.%20By%20enabling%20efficient%20local%20deployment%20without%20sacrificing%20accuracy%20or%20representational%20richness%2C%20Pathryoshka%20democratizes%20access%20to%20state-of-the-art%20pathology%20FMs%20for%20the%20broader%20research%20and%20clinical%20community.&entry.1838667208=http%3A//arxiv.org/abs/2511.23204v1&entry.124074799=Read"},
{"title": "Nonstabilizerness Estimation using Graph Neural Networks", "author": "Vincenzo Lipardi and Domenica Dibenedetto and Georgios Stamoulis and Evert van Nieuwenburg and Mark H. M. Winands", "abstract": "This article proposes a Graph Neural Network (GNN) approach to estimate nonstabilizerness in quantum circuits, measured by the stabilizer R\u00e9nyi entropy (SRE). Nonstabilizerness is a fundamental resource for quantum advantage, and efficient SRE estimations are highly beneficial in practical applications. We address the nonstabilizerness estimation problem through three supervised learning formulations starting from easier classification tasks to the more challenging regression task. Experimental results show that the proposed GNN manages to capture meaningful features from the graph-based circuit representation, resulting in robust generalization performances achieved across diverse scenarios. In classification tasks, the GNN is trained on product states and generalizes on circuits evolved under Clifford operations, entangled states, and circuits with higher number of qubits. In the regression task, the GNN significantly improves the SRE estimation on out-of-distribution circuits with higher number of qubits and gate counts compared to previous work, for both random quantum circuits and structured circuits derived from the transverse-field Ising model. Moreover, the graph representation of quantum circuits naturally integrates hardware-specific information. Simulations on noisy quantum hardware highlight the potential of the proposed GNN to predict the SRE measured on quantum devices.", "link": "http://arxiv.org/abs/2511.23224v1", "date": "2025-11-28", "relevancy": 1.698, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4348}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4186}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonstabilizerness%20Estimation%20using%20Graph%20Neural%20Networks&body=Title%3A%20Nonstabilizerness%20Estimation%20using%20Graph%20Neural%20Networks%0AAuthor%3A%20Vincenzo%20Lipardi%20and%20Domenica%20Dibenedetto%20and%20Georgios%20Stamoulis%20and%20Evert%20van%20Nieuwenburg%20and%20Mark%20H.%20M.%20Winands%0AAbstract%3A%20This%20article%20proposes%20a%20Graph%20Neural%20Network%20%28GNN%29%20approach%20to%20estimate%20nonstabilizerness%20in%20quantum%20circuits%2C%20measured%20by%20the%20stabilizer%20R%C3%A9nyi%20entropy%20%28SRE%29.%20Nonstabilizerness%20is%20a%20fundamental%20resource%20for%20quantum%20advantage%2C%20and%20efficient%20SRE%20estimations%20are%20highly%20beneficial%20in%20practical%20applications.%20We%20address%20the%20nonstabilizerness%20estimation%20problem%20through%20three%20supervised%20learning%20formulations%20starting%20from%20easier%20classification%20tasks%20to%20the%20more%20challenging%20regression%20task.%20Experimental%20results%20show%20that%20the%20proposed%20GNN%20manages%20to%20capture%20meaningful%20features%20from%20the%20graph-based%20circuit%20representation%2C%20resulting%20in%20robust%20generalization%20performances%20achieved%20across%20diverse%20scenarios.%20In%20classification%20tasks%2C%20the%20GNN%20is%20trained%20on%20product%20states%20and%20generalizes%20on%20circuits%20evolved%20under%20Clifford%20operations%2C%20entangled%20states%2C%20and%20circuits%20with%20higher%20number%20of%20qubits.%20In%20the%20regression%20task%2C%20the%20GNN%20significantly%20improves%20the%20SRE%20estimation%20on%20out-of-distribution%20circuits%20with%20higher%20number%20of%20qubits%20and%20gate%20counts%20compared%20to%20previous%20work%2C%20for%20both%20random%20quantum%20circuits%20and%20structured%20circuits%20derived%20from%20the%20transverse-field%20Ising%20model.%20Moreover%2C%20the%20graph%20representation%20of%20quantum%20circuits%20naturally%20integrates%20hardware-specific%20information.%20Simulations%20on%20noisy%20quantum%20hardware%20highlight%20the%20potential%20of%20the%20proposed%20GNN%20to%20predict%20the%20SRE%20measured%20on%20quantum%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonstabilizerness%2520Estimation%2520using%2520Graph%2520Neural%2520Networks%26entry.906535625%3DVincenzo%2520Lipardi%2520and%2520Domenica%2520Dibenedetto%2520and%2520Georgios%2520Stamoulis%2520and%2520Evert%2520van%2520Nieuwenburg%2520and%2520Mark%2520H.%2520M.%2520Winands%26entry.1292438233%3DThis%2520article%2520proposes%2520a%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520approach%2520to%2520estimate%2520nonstabilizerness%2520in%2520quantum%2520circuits%252C%2520measured%2520by%2520the%2520stabilizer%2520R%25C3%25A9nyi%2520entropy%2520%2528SRE%2529.%2520Nonstabilizerness%2520is%2520a%2520fundamental%2520resource%2520for%2520quantum%2520advantage%252C%2520and%2520efficient%2520SRE%2520estimations%2520are%2520highly%2520beneficial%2520in%2520practical%2520applications.%2520We%2520address%2520the%2520nonstabilizerness%2520estimation%2520problem%2520through%2520three%2520supervised%2520learning%2520formulations%2520starting%2520from%2520easier%2520classification%2520tasks%2520to%2520the%2520more%2520challenging%2520regression%2520task.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520GNN%2520manages%2520to%2520capture%2520meaningful%2520features%2520from%2520the%2520graph-based%2520circuit%2520representation%252C%2520resulting%2520in%2520robust%2520generalization%2520performances%2520achieved%2520across%2520diverse%2520scenarios.%2520In%2520classification%2520tasks%252C%2520the%2520GNN%2520is%2520trained%2520on%2520product%2520states%2520and%2520generalizes%2520on%2520circuits%2520evolved%2520under%2520Clifford%2520operations%252C%2520entangled%2520states%252C%2520and%2520circuits%2520with%2520higher%2520number%2520of%2520qubits.%2520In%2520the%2520regression%2520task%252C%2520the%2520GNN%2520significantly%2520improves%2520the%2520SRE%2520estimation%2520on%2520out-of-distribution%2520circuits%2520with%2520higher%2520number%2520of%2520qubits%2520and%2520gate%2520counts%2520compared%2520to%2520previous%2520work%252C%2520for%2520both%2520random%2520quantum%2520circuits%2520and%2520structured%2520circuits%2520derived%2520from%2520the%2520transverse-field%2520Ising%2520model.%2520Moreover%252C%2520the%2520graph%2520representation%2520of%2520quantum%2520circuits%2520naturally%2520integrates%2520hardware-specific%2520information.%2520Simulations%2520on%2520noisy%2520quantum%2520hardware%2520highlight%2520the%2520potential%2520of%2520the%2520proposed%2520GNN%2520to%2520predict%2520the%2520SRE%2520measured%2520on%2520quantum%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonstabilizerness%20Estimation%20using%20Graph%20Neural%20Networks&entry.906535625=Vincenzo%20Lipardi%20and%20Domenica%20Dibenedetto%20and%20Georgios%20Stamoulis%20and%20Evert%20van%20Nieuwenburg%20and%20Mark%20H.%20M.%20Winands&entry.1292438233=This%20article%20proposes%20a%20Graph%20Neural%20Network%20%28GNN%29%20approach%20to%20estimate%20nonstabilizerness%20in%20quantum%20circuits%2C%20measured%20by%20the%20stabilizer%20R%C3%A9nyi%20entropy%20%28SRE%29.%20Nonstabilizerness%20is%20a%20fundamental%20resource%20for%20quantum%20advantage%2C%20and%20efficient%20SRE%20estimations%20are%20highly%20beneficial%20in%20practical%20applications.%20We%20address%20the%20nonstabilizerness%20estimation%20problem%20through%20three%20supervised%20learning%20formulations%20starting%20from%20easier%20classification%20tasks%20to%20the%20more%20challenging%20regression%20task.%20Experimental%20results%20show%20that%20the%20proposed%20GNN%20manages%20to%20capture%20meaningful%20features%20from%20the%20graph-based%20circuit%20representation%2C%20resulting%20in%20robust%20generalization%20performances%20achieved%20across%20diverse%20scenarios.%20In%20classification%20tasks%2C%20the%20GNN%20is%20trained%20on%20product%20states%20and%20generalizes%20on%20circuits%20evolved%20under%20Clifford%20operations%2C%20entangled%20states%2C%20and%20circuits%20with%20higher%20number%20of%20qubits.%20In%20the%20regression%20task%2C%20the%20GNN%20significantly%20improves%20the%20SRE%20estimation%20on%20out-of-distribution%20circuits%20with%20higher%20number%20of%20qubits%20and%20gate%20counts%20compared%20to%20previous%20work%2C%20for%20both%20random%20quantum%20circuits%20and%20structured%20circuits%20derived%20from%20the%20transverse-field%20Ising%20model.%20Moreover%2C%20the%20graph%20representation%20of%20quantum%20circuits%20naturally%20integrates%20hardware-specific%20information.%20Simulations%20on%20noisy%20quantum%20hardware%20highlight%20the%20potential%20of%20the%20proposed%20GNN%20to%20predict%20the%20SRE%20measured%20on%20quantum%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2511.23224v1&entry.124074799=Read"},
{"title": "CheMixHub: Datasets and Benchmarks for Chemical Mixture Property Prediction", "author": "Ella Miray Rajaonson and Mahyar Rajabi Kochi and Luis Martin Mejia Mendoza and Seyed Mohamad Moosavi and Benjamin Sanchez-Lengeling", "abstract": "Developing improved predictive models for multi-molecular systems is crucial, as nearly every chemical product used results from a mixture of chemicals. While being a vital part of the industry pipeline, the chemical mixture space remains relatively unexplored by the Machine Learning community. In this paper, we introduce CheMixHub, a holistic benchmark for molecular mixtures, covering a corpus of 11 chemical mixtures property prediction tasks, from drug delivery formulations to battery electrolytes, totalling approximately 500k data points gathered and curated from 7 publicly available datasets. CheMixHub introduces various data splitting techniques to assess context-specific generalization and model robustness, providing a foundation for the development of predictive models for chemical mixture properties. Furthermore, we map out the modelling space of deep learning models for chemical mixtures, establishing initial benchmarks for the community. This dataset has the potential to accelerate chemical mixture development, encompassing reformulation, optimization, and discovery. The dataset and code for the benchmarks can be found at: https://github.com/chemcognition-lab/chemixhub", "link": "http://arxiv.org/abs/2506.12231v2", "date": "2025-11-28", "relevancy": 1.4288, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4676}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CheMixHub%3A%20Datasets%20and%20Benchmarks%20for%20Chemical%20Mixture%20Property%20Prediction&body=Title%3A%20CheMixHub%3A%20Datasets%20and%20Benchmarks%20for%20Chemical%20Mixture%20Property%20Prediction%0AAuthor%3A%20Ella%20Miray%20Rajaonson%20and%20Mahyar%20Rajabi%20Kochi%20and%20Luis%20Martin%20Mejia%20Mendoza%20and%20Seyed%20Mohamad%20Moosavi%20and%20Benjamin%20Sanchez-Lengeling%0AAbstract%3A%20Developing%20improved%20predictive%20models%20for%20multi-molecular%20systems%20is%20crucial%2C%20as%20nearly%20every%20chemical%20product%20used%20results%20from%20a%20mixture%20of%20chemicals.%20While%20being%20a%20vital%20part%20of%20the%20industry%20pipeline%2C%20the%20chemical%20mixture%20space%20remains%20relatively%20unexplored%20by%20the%20Machine%20Learning%20community.%20In%20this%20paper%2C%20we%20introduce%20CheMixHub%2C%20a%20holistic%20benchmark%20for%20molecular%20mixtures%2C%20covering%20a%20corpus%20of%2011%20chemical%20mixtures%20property%20prediction%20tasks%2C%20from%20drug%20delivery%20formulations%20to%20battery%20electrolytes%2C%20totalling%20approximately%20500k%20data%20points%20gathered%20and%20curated%20from%207%20publicly%20available%20datasets.%20CheMixHub%20introduces%20various%20data%20splitting%20techniques%20to%20assess%20context-specific%20generalization%20and%20model%20robustness%2C%20providing%20a%20foundation%20for%20the%20development%20of%20predictive%20models%20for%20chemical%20mixture%20properties.%20Furthermore%2C%20we%20map%20out%20the%20modelling%20space%20of%20deep%20learning%20models%20for%20chemical%20mixtures%2C%20establishing%20initial%20benchmarks%20for%20the%20community.%20This%20dataset%20has%20the%20potential%20to%20accelerate%20chemical%20mixture%20development%2C%20encompassing%20reformulation%2C%20optimization%2C%20and%20discovery.%20The%20dataset%20and%20code%20for%20the%20benchmarks%20can%20be%20found%20at%3A%20https%3A//github.com/chemcognition-lab/chemixhub%0ALink%3A%20http%3A//arxiv.org/abs/2506.12231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCheMixHub%253A%2520Datasets%2520and%2520Benchmarks%2520for%2520Chemical%2520Mixture%2520Property%2520Prediction%26entry.906535625%3DElla%2520Miray%2520Rajaonson%2520and%2520Mahyar%2520Rajabi%2520Kochi%2520and%2520Luis%2520Martin%2520Mejia%2520Mendoza%2520and%2520Seyed%2520Mohamad%2520Moosavi%2520and%2520Benjamin%2520Sanchez-Lengeling%26entry.1292438233%3DDeveloping%2520improved%2520predictive%2520models%2520for%2520multi-molecular%2520systems%2520is%2520crucial%252C%2520as%2520nearly%2520every%2520chemical%2520product%2520used%2520results%2520from%2520a%2520mixture%2520of%2520chemicals.%2520While%2520being%2520a%2520vital%2520part%2520of%2520the%2520industry%2520pipeline%252C%2520the%2520chemical%2520mixture%2520space%2520remains%2520relatively%2520unexplored%2520by%2520the%2520Machine%2520Learning%2520community.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CheMixHub%252C%2520a%2520holistic%2520benchmark%2520for%2520molecular%2520mixtures%252C%2520covering%2520a%2520corpus%2520of%252011%2520chemical%2520mixtures%2520property%2520prediction%2520tasks%252C%2520from%2520drug%2520delivery%2520formulations%2520to%2520battery%2520electrolytes%252C%2520totalling%2520approximately%2520500k%2520data%2520points%2520gathered%2520and%2520curated%2520from%25207%2520publicly%2520available%2520datasets.%2520CheMixHub%2520introduces%2520various%2520data%2520splitting%2520techniques%2520to%2520assess%2520context-specific%2520generalization%2520and%2520model%2520robustness%252C%2520providing%2520a%2520foundation%2520for%2520the%2520development%2520of%2520predictive%2520models%2520for%2520chemical%2520mixture%2520properties.%2520Furthermore%252C%2520we%2520map%2520out%2520the%2520modelling%2520space%2520of%2520deep%2520learning%2520models%2520for%2520chemical%2520mixtures%252C%2520establishing%2520initial%2520benchmarks%2520for%2520the%2520community.%2520This%2520dataset%2520has%2520the%2520potential%2520to%2520accelerate%2520chemical%2520mixture%2520development%252C%2520encompassing%2520reformulation%252C%2520optimization%252C%2520and%2520discovery.%2520The%2520dataset%2520and%2520code%2520for%2520the%2520benchmarks%2520can%2520be%2520found%2520at%253A%2520https%253A//github.com/chemcognition-lab/chemixhub%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CheMixHub%3A%20Datasets%20and%20Benchmarks%20for%20Chemical%20Mixture%20Property%20Prediction&entry.906535625=Ella%20Miray%20Rajaonson%20and%20Mahyar%20Rajabi%20Kochi%20and%20Luis%20Martin%20Mejia%20Mendoza%20and%20Seyed%20Mohamad%20Moosavi%20and%20Benjamin%20Sanchez-Lengeling&entry.1292438233=Developing%20improved%20predictive%20models%20for%20multi-molecular%20systems%20is%20crucial%2C%20as%20nearly%20every%20chemical%20product%20used%20results%20from%20a%20mixture%20of%20chemicals.%20While%20being%20a%20vital%20part%20of%20the%20industry%20pipeline%2C%20the%20chemical%20mixture%20space%20remains%20relatively%20unexplored%20by%20the%20Machine%20Learning%20community.%20In%20this%20paper%2C%20we%20introduce%20CheMixHub%2C%20a%20holistic%20benchmark%20for%20molecular%20mixtures%2C%20covering%20a%20corpus%20of%2011%20chemical%20mixtures%20property%20prediction%20tasks%2C%20from%20drug%20delivery%20formulations%20to%20battery%20electrolytes%2C%20totalling%20approximately%20500k%20data%20points%20gathered%20and%20curated%20from%207%20publicly%20available%20datasets.%20CheMixHub%20introduces%20various%20data%20splitting%20techniques%20to%20assess%20context-specific%20generalization%20and%20model%20robustness%2C%20providing%20a%20foundation%20for%20the%20development%20of%20predictive%20models%20for%20chemical%20mixture%20properties.%20Furthermore%2C%20we%20map%20out%20the%20modelling%20space%20of%20deep%20learning%20models%20for%20chemical%20mixtures%2C%20establishing%20initial%20benchmarks%20for%20the%20community.%20This%20dataset%20has%20the%20potential%20to%20accelerate%20chemical%20mixture%20development%2C%20encompassing%20reformulation%2C%20optimization%2C%20and%20discovery.%20The%20dataset%20and%20code%20for%20the%20benchmarks%20can%20be%20found%20at%3A%20https%3A//github.com/chemcognition-lab/chemixhub&entry.1838667208=http%3A//arxiv.org/abs/2506.12231v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


