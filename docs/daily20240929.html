<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240926.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with\n  Enhanced Generalization and Personalization Abilities", "author": "Peizhi Yan and Rabab Ward and Qiang Tang and Shan Du", "abstract": "  Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant\npotential for modeling 3D head avatars, providing greater flexibility than\nmesh-based methods and more efficient rendering compared to NeRF-based\napproaches. Despite these advancements, the creation of controllable 3DGS-based\nhead avatars remains time-intensive, often requiring tens of minutes to hours.\nTo expedite this process, we here introduce the ``Gaussian D\\'ej\\`a-vu\"\nframework, which first obtains a generalized model of the head avatar and then\npersonalizes the result. The generalized model is trained on large 2D\n(synthetic and real) image datasets. This model provides a well-initialized 3D\nGaussian head that is further refined using a monocular video to achieve the\npersonalized head avatar. For personalizing, we propose learnable\nexpression-aware rectification blendmaps to correct the initial 3D Gaussians,\nensuring rapid convergence without the reliance on neural networks. Experiments\ndemonstrate that the proposed method meets its objectives. It outperforms\nstate-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as\nwell as reduces training time consumption to at least a quarter of the existing\nmethods, producing the avatar in minutes.\n", "link": "http://arxiv.org/abs/2409.16147v2", "date": "2024-09-26", "relevancy": 3.9265, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8294}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8294}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Deja-vu%3A%20Creating%20Controllable%203D%20Gaussian%20Head-Avatars%20with%0A%20%20Enhanced%20Generalization%20and%20Personalization%20Abilities&body=Title%3A%20Gaussian%20Deja-vu%3A%20Creating%20Controllable%203D%20Gaussian%20Head-Avatars%20with%0A%20%20Enhanced%20Generalization%20and%20Personalization%20Abilities%0AAuthor%3A%20Peizhi%20Yan%20and%20Rabab%20Ward%20and%20Qiang%20Tang%20and%20Shan%20Du%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20unlocked%20significant%0Apotential%20for%20modeling%203D%20head%20avatars%2C%20providing%20greater%20flexibility%20than%0Amesh-based%20methods%20and%20more%20efficient%20rendering%20compared%20to%20NeRF-based%0Aapproaches.%20Despite%20these%20advancements%2C%20the%20creation%20of%20controllable%203DGS-based%0Ahead%20avatars%20remains%20time-intensive%2C%20often%20requiring%20tens%20of%20minutes%20to%20hours.%0ATo%20expedite%20this%20process%2C%20we%20here%20introduce%20the%20%60%60Gaussian%20D%5C%27ej%5C%60a-vu%22%0Aframework%2C%20which%20first%20obtains%20a%20generalized%20model%20of%20the%20head%20avatar%20and%20then%0Apersonalizes%20the%20result.%20The%20generalized%20model%20is%20trained%20on%20large%202D%0A%28synthetic%20and%20real%29%20image%20datasets.%20This%20model%20provides%20a%20well-initialized%203D%0AGaussian%20head%20that%20is%20further%20refined%20using%20a%20monocular%20video%20to%20achieve%20the%0Apersonalized%20head%20avatar.%20For%20personalizing%2C%20we%20propose%20learnable%0Aexpression-aware%20rectification%20blendmaps%20to%20correct%20the%20initial%203D%20Gaussians%2C%0Aensuring%20rapid%20convergence%20without%20the%20reliance%20on%20neural%20networks.%20Experiments%0Ademonstrate%20that%20the%20proposed%20method%20meets%20its%20objectives.%20It%20outperforms%0Astate-of-the-art%203D%20Gaussian%20head%20avatars%20in%20terms%20of%20photorealistic%20quality%20as%0Awell%20as%20reduces%20training%20time%20consumption%20to%20at%20least%20a%20quarter%20of%20the%20existing%0Amethods%2C%20producing%20the%20avatar%20in%20minutes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Deja-vu%253A%2520Creating%2520Controllable%25203D%2520Gaussian%2520Head-Avatars%2520with%250A%2520%2520Enhanced%2520Generalization%2520and%2520Personalization%2520Abilities%26entry.906535625%3DPeizhi%2520Yan%2520and%2520Rabab%2520Ward%2520and%2520Qiang%2520Tang%2520and%2520Shan%2520Du%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520unlocked%2520significant%250Apotential%2520for%2520modeling%25203D%2520head%2520avatars%252C%2520providing%2520greater%2520flexibility%2520than%250Amesh-based%2520methods%2520and%2520more%2520efficient%2520rendering%2520compared%2520to%2520NeRF-based%250Aapproaches.%2520Despite%2520these%2520advancements%252C%2520the%2520creation%2520of%2520controllable%25203DGS-based%250Ahead%2520avatars%2520remains%2520time-intensive%252C%2520often%2520requiring%2520tens%2520of%2520minutes%2520to%2520hours.%250ATo%2520expedite%2520this%2520process%252C%2520we%2520here%2520introduce%2520the%2520%2560%2560Gaussian%2520D%255C%2527ej%255C%2560a-vu%2522%250Aframework%252C%2520which%2520first%2520obtains%2520a%2520generalized%2520model%2520of%2520the%2520head%2520avatar%2520and%2520then%250Apersonalizes%2520the%2520result.%2520The%2520generalized%2520model%2520is%2520trained%2520on%2520large%25202D%250A%2528synthetic%2520and%2520real%2529%2520image%2520datasets.%2520This%2520model%2520provides%2520a%2520well-initialized%25203D%250AGaussian%2520head%2520that%2520is%2520further%2520refined%2520using%2520a%2520monocular%2520video%2520to%2520achieve%2520the%250Apersonalized%2520head%2520avatar.%2520For%2520personalizing%252C%2520we%2520propose%2520learnable%250Aexpression-aware%2520rectification%2520blendmaps%2520to%2520correct%2520the%2520initial%25203D%2520Gaussians%252C%250Aensuring%2520rapid%2520convergence%2520without%2520the%2520reliance%2520on%2520neural%2520networks.%2520Experiments%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520meets%2520its%2520objectives.%2520It%2520outperforms%250Astate-of-the-art%25203D%2520Gaussian%2520head%2520avatars%2520in%2520terms%2520of%2520photorealistic%2520quality%2520as%250Awell%2520as%2520reduces%2520training%2520time%2520consumption%2520to%2520at%2520least%2520a%2520quarter%2520of%2520the%2520existing%250Amethods%252C%2520producing%2520the%2520avatar%2520in%2520minutes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Deja-vu%3A%20Creating%20Controllable%203D%20Gaussian%20Head-Avatars%20with%0A%20%20Enhanced%20Generalization%20and%20Personalization%20Abilities&entry.906535625=Peizhi%20Yan%20and%20Rabab%20Ward%20and%20Qiang%20Tang%20and%20Shan%20Du&entry.1292438233=%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20unlocked%20significant%0Apotential%20for%20modeling%203D%20head%20avatars%2C%20providing%20greater%20flexibility%20than%0Amesh-based%20methods%20and%20more%20efficient%20rendering%20compared%20to%20NeRF-based%0Aapproaches.%20Despite%20these%20advancements%2C%20the%20creation%20of%20controllable%203DGS-based%0Ahead%20avatars%20remains%20time-intensive%2C%20often%20requiring%20tens%20of%20minutes%20to%20hours.%0ATo%20expedite%20this%20process%2C%20we%20here%20introduce%20the%20%60%60Gaussian%20D%5C%27ej%5C%60a-vu%22%0Aframework%2C%20which%20first%20obtains%20a%20generalized%20model%20of%20the%20head%20avatar%20and%20then%0Apersonalizes%20the%20result.%20The%20generalized%20model%20is%20trained%20on%20large%202D%0A%28synthetic%20and%20real%29%20image%20datasets.%20This%20model%20provides%20a%20well-initialized%203D%0AGaussian%20head%20that%20is%20further%20refined%20using%20a%20monocular%20video%20to%20achieve%20the%0Apersonalized%20head%20avatar.%20For%20personalizing%2C%20we%20propose%20learnable%0Aexpression-aware%20rectification%20blendmaps%20to%20correct%20the%20initial%203D%20Gaussians%2C%0Aensuring%20rapid%20convergence%20without%20the%20reliance%20on%20neural%20networks.%20Experiments%0Ademonstrate%20that%20the%20proposed%20method%20meets%20its%20objectives.%20It%20outperforms%0Astate-of-the-art%203D%20Gaussian%20head%20avatars%20in%20terms%20of%20photorealistic%20quality%20as%0Awell%20as%20reduces%20training%20time%20consumption%20to%20at%20least%20a%20quarter%20of%20the%20existing%0Amethods%2C%20producing%20the%20avatar%20in%20minutes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16147v2&entry.124074799=Read"},
{"title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS", "author": "Sharath Girish and Kamal Gupta and Abhinav Shrivastava", "abstract": "  Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view\nscene synthesis. It addresses the challenges of lengthy training times and slow\nrendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,\ndifferentiable rasterization of 3D Gaussians, 3D-GS achieves real-time\nrendering and accelerated training. They, however, demand substantial memory\nresources for both training and storage, as they require millions of Gaussians\nin their point cloud representation for each scene. We present a technique\nutilizing quantized embeddings to significantly reduce per-point memory storage\nrequirements and a coarse-to-fine training strategy for a faster and more\nstable optimization of the Gaussian point clouds. Our approach develops a\npruning stage which results in scene representations with fewer Gaussians,\nleading to faster training times and rendering speeds for real-time rendering\nof high resolution scenes. We reduce storage memory by more than an order of\nmagnitude all while preserving the reconstruction quality. We validate the\neffectiveness of our approach on a variety of datasets and scenes preserving\nthe visual quality while consuming 10-20x lesser memory and faster\ntraining/inference speed. Project page and code is available\nhttps://efficientgaussian.github.io\n", "link": "http://arxiv.org/abs/2312.04564v3", "date": "2024-09-26", "relevancy": 3.4462, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7238}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6904}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EAGLES%3A%20Efficient%20Accelerated%203D%20Gaussians%20with%20Lightweight%20EncodingS&body=Title%3A%20EAGLES%3A%20Efficient%20Accelerated%203D%20Gaussians%20with%20Lightweight%20EncodingS%0AAuthor%3A%20Sharath%20Girish%20and%20Kamal%20Gupta%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20gained%20popularity%20in%20novel-view%0Ascene%20synthesis.%20It%20addresses%20the%20challenges%20of%20lengthy%20training%20times%20and%20slow%0Arendering%20speeds%20associated%20with%20Neural%20Radiance%20Fields%20%28NeRFs%29.%20Through%20rapid%2C%0Adifferentiable%20rasterization%20of%203D%20Gaussians%2C%203D-GS%20achieves%20real-time%0Arendering%20and%20accelerated%20training.%20They%2C%20however%2C%20demand%20substantial%20memory%0Aresources%20for%20both%20training%20and%20storage%2C%20as%20they%20require%20millions%20of%20Gaussians%0Ain%20their%20point%20cloud%20representation%20for%20each%20scene.%20We%20present%20a%20technique%0Autilizing%20quantized%20embeddings%20to%20significantly%20reduce%20per-point%20memory%20storage%0Arequirements%20and%20a%20coarse-to-fine%20training%20strategy%20for%20a%20faster%20and%20more%0Astable%20optimization%20of%20the%20Gaussian%20point%20clouds.%20Our%20approach%20develops%20a%0Apruning%20stage%20which%20results%20in%20scene%20representations%20with%20fewer%20Gaussians%2C%0Aleading%20to%20faster%20training%20times%20and%20rendering%20speeds%20for%20real-time%20rendering%0Aof%20high%20resolution%20scenes.%20We%20reduce%20storage%20memory%20by%20more%20than%20an%20order%20of%0Amagnitude%20all%20while%20preserving%20the%20reconstruction%20quality.%20We%20validate%20the%0Aeffectiveness%20of%20our%20approach%20on%20a%20variety%20of%20datasets%20and%20scenes%20preserving%0Athe%20visual%20quality%20while%20consuming%2010-20x%20lesser%20memory%20and%20faster%0Atraining/inference%20speed.%20Project%20page%20and%20code%20is%20available%0Ahttps%3A//efficientgaussian.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04564v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEAGLES%253A%2520Efficient%2520Accelerated%25203D%2520Gaussians%2520with%2520Lightweight%2520EncodingS%26entry.906535625%3DSharath%2520Girish%2520and%2520Kamal%2520Gupta%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520splatting%2520%25283D-GS%2529%2520has%2520gained%2520popularity%2520in%2520novel-view%250Ascene%2520synthesis.%2520It%2520addresses%2520the%2520challenges%2520of%2520lengthy%2520training%2520times%2520and%2520slow%250Arendering%2520speeds%2520associated%2520with%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529.%2520Through%2520rapid%252C%250Adifferentiable%2520rasterization%2520of%25203D%2520Gaussians%252C%25203D-GS%2520achieves%2520real-time%250Arendering%2520and%2520accelerated%2520training.%2520They%252C%2520however%252C%2520demand%2520substantial%2520memory%250Aresources%2520for%2520both%2520training%2520and%2520storage%252C%2520as%2520they%2520require%2520millions%2520of%2520Gaussians%250Ain%2520their%2520point%2520cloud%2520representation%2520for%2520each%2520scene.%2520We%2520present%2520a%2520technique%250Autilizing%2520quantized%2520embeddings%2520to%2520significantly%2520reduce%2520per-point%2520memory%2520storage%250Arequirements%2520and%2520a%2520coarse-to-fine%2520training%2520strategy%2520for%2520a%2520faster%2520and%2520more%250Astable%2520optimization%2520of%2520the%2520Gaussian%2520point%2520clouds.%2520Our%2520approach%2520develops%2520a%250Apruning%2520stage%2520which%2520results%2520in%2520scene%2520representations%2520with%2520fewer%2520Gaussians%252C%250Aleading%2520to%2520faster%2520training%2520times%2520and%2520rendering%2520speeds%2520for%2520real-time%2520rendering%250Aof%2520high%2520resolution%2520scenes.%2520We%2520reduce%2520storage%2520memory%2520by%2520more%2520than%2520an%2520order%2520of%250Amagnitude%2520all%2520while%2520preserving%2520the%2520reconstruction%2520quality.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520on%2520a%2520variety%2520of%2520datasets%2520and%2520scenes%2520preserving%250Athe%2520visual%2520quality%2520while%2520consuming%252010-20x%2520lesser%2520memory%2520and%2520faster%250Atraining/inference%2520speed.%2520Project%2520page%2520and%2520code%2520is%2520available%250Ahttps%253A//efficientgaussian.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04564v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EAGLES%3A%20Efficient%20Accelerated%203D%20Gaussians%20with%20Lightweight%20EncodingS&entry.906535625=Sharath%20Girish%20and%20Kamal%20Gupta%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20splatting%20%283D-GS%29%20has%20gained%20popularity%20in%20novel-view%0Ascene%20synthesis.%20It%20addresses%20the%20challenges%20of%20lengthy%20training%20times%20and%20slow%0Arendering%20speeds%20associated%20with%20Neural%20Radiance%20Fields%20%28NeRFs%29.%20Through%20rapid%2C%0Adifferentiable%20rasterization%20of%203D%20Gaussians%2C%203D-GS%20achieves%20real-time%0Arendering%20and%20accelerated%20training.%20They%2C%20however%2C%20demand%20substantial%20memory%0Aresources%20for%20both%20training%20and%20storage%2C%20as%20they%20require%20millions%20of%20Gaussians%0Ain%20their%20point%20cloud%20representation%20for%20each%20scene.%20We%20present%20a%20technique%0Autilizing%20quantized%20embeddings%20to%20significantly%20reduce%20per-point%20memory%20storage%0Arequirements%20and%20a%20coarse-to-fine%20training%20strategy%20for%20a%20faster%20and%20more%0Astable%20optimization%20of%20the%20Gaussian%20point%20clouds.%20Our%20approach%20develops%20a%0Apruning%20stage%20which%20results%20in%20scene%20representations%20with%20fewer%20Gaussians%2C%0Aleading%20to%20faster%20training%20times%20and%20rendering%20speeds%20for%20real-time%20rendering%0Aof%20high%20resolution%20scenes.%20We%20reduce%20storage%20memory%20by%20more%20than%20an%20order%20of%0Amagnitude%20all%20while%20preserving%20the%20reconstruction%20quality.%20We%20validate%20the%0Aeffectiveness%20of%20our%20approach%20on%20a%20variety%20of%20datasets%20and%20scenes%20preserving%0Athe%20visual%20quality%20while%20consuming%2010-20x%20lesser%20memory%20and%20faster%0Atraining/inference%20speed.%20Project%20page%20and%20code%20is%20available%0Ahttps%3A//efficientgaussian.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04564v3&entry.124074799=Read"},
{"title": "Gaussian-LIC: Real-Time Photo-Realistic SLAM with Gaussian Splatting and\n  LiDAR-Inertial-Camera Fusion", "author": "Xiaolei Lang and Laijian Li and Chenming Wu and Chen Zhao and Lina Liu and Yong Liu and Jiajun Lv and Xingxing Zuo", "abstract": "  In this paper, we present a real-time photo-realistic SLAM method based on\nmarrying Gaussian Splatting with LiDAR-Inertial-Camera SLAM. Most existing\nradiance-field-based SLAM systems mainly focus on bounded indoor environments,\nequipped with RGB-D or RGB sensors. However, they are prone to decline when\nexpanding to unbounded scenes or encountering adverse conditions, such as\nviolent motions and changing illumination. In contrast, oriented to general\nscenarios, our approach additionally tightly fuses LiDAR, IMU, and camera for\nrobust pose estimation and photo-realistic online mapping. To compensate for\nregions unobserved by the LiDAR, we propose to integrate both the triangulated\nvisual points from images and LiDAR points for initializing 3D Gaussians. In\naddition, the modeling of the sky and varying camera exposure have been\nrealized for high-quality rendering. Notably, we implement our system purely\nwith C++ and CUDA, and meticulously design a series of strategies to accelerate\nthe online optimization of the Gaussian-based scene representation. Extensive\nexperiments demonstrate that our method outperforms its counterparts while\nmaintaining real-time capability. Impressively, regarding photo-realistic\nmapping, our method with our estimated poses even surpasses all the compared\napproaches that utilize privileged ground-truth poses for mapping. Our code\nwill be released on project page https://xingxingzuo.github.io/gaussian_lic.\n", "link": "http://arxiv.org/abs/2404.06926v2", "date": "2024-09-26", "relevancy": 3.4126, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7884}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6317}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian-LIC%3A%20Real-Time%20Photo-Realistic%20SLAM%20with%20Gaussian%20Splatting%20and%0A%20%20LiDAR-Inertial-Camera%20Fusion&body=Title%3A%20Gaussian-LIC%3A%20Real-Time%20Photo-Realistic%20SLAM%20with%20Gaussian%20Splatting%20and%0A%20%20LiDAR-Inertial-Camera%20Fusion%0AAuthor%3A%20Xiaolei%20Lang%20and%20Laijian%20Li%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Lina%20Liu%20and%20Yong%20Liu%20and%20Jiajun%20Lv%20and%20Xingxing%20Zuo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20real-time%20photo-realistic%20SLAM%20method%20based%20on%0Amarrying%20Gaussian%20Splatting%20with%20LiDAR-Inertial-Camera%20SLAM.%20Most%20existing%0Aradiance-field-based%20SLAM%20systems%20mainly%20focus%20on%20bounded%20indoor%20environments%2C%0Aequipped%20with%20RGB-D%20or%20RGB%20sensors.%20However%2C%20they%20are%20prone%20to%20decline%20when%0Aexpanding%20to%20unbounded%20scenes%20or%20encountering%20adverse%20conditions%2C%20such%20as%0Aviolent%20motions%20and%20changing%20illumination.%20In%20contrast%2C%20oriented%20to%20general%0Ascenarios%2C%20our%20approach%20additionally%20tightly%20fuses%20LiDAR%2C%20IMU%2C%20and%20camera%20for%0Arobust%20pose%20estimation%20and%20photo-realistic%20online%20mapping.%20To%20compensate%20for%0Aregions%20unobserved%20by%20the%20LiDAR%2C%20we%20propose%20to%20integrate%20both%20the%20triangulated%0Avisual%20points%20from%20images%20and%20LiDAR%20points%20for%20initializing%203D%20Gaussians.%20In%0Aaddition%2C%20the%20modeling%20of%20the%20sky%20and%20varying%20camera%20exposure%20have%20been%0Arealized%20for%20high-quality%20rendering.%20Notably%2C%20we%20implement%20our%20system%20purely%0Awith%20C%2B%2B%20and%20CUDA%2C%20and%20meticulously%20design%20a%20series%20of%20strategies%20to%20accelerate%0Athe%20online%20optimization%20of%20the%20Gaussian-based%20scene%20representation.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20its%20counterparts%20while%0Amaintaining%20real-time%20capability.%20Impressively%2C%20regarding%20photo-realistic%0Amapping%2C%20our%20method%20with%20our%20estimated%20poses%20even%20surpasses%20all%20the%20compared%0Aapproaches%20that%20utilize%20privileged%20ground-truth%20poses%20for%20mapping.%20Our%20code%0Awill%20be%20released%20on%20project%20page%20https%3A//xingxingzuo.github.io/gaussian_lic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian-LIC%253A%2520Real-Time%2520Photo-Realistic%2520SLAM%2520with%2520Gaussian%2520Splatting%2520and%250A%2520%2520LiDAR-Inertial-Camera%2520Fusion%26entry.906535625%3DXiaolei%2520Lang%2520and%2520Laijian%2520Li%2520and%2520Chenming%2520Wu%2520and%2520Chen%2520Zhao%2520and%2520Lina%2520Liu%2520and%2520Yong%2520Liu%2520and%2520Jiajun%2520Lv%2520and%2520Xingxing%2520Zuo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520real-time%2520photo-realistic%2520SLAM%2520method%2520based%2520on%250Amarrying%2520Gaussian%2520Splatting%2520with%2520LiDAR-Inertial-Camera%2520SLAM.%2520Most%2520existing%250Aradiance-field-based%2520SLAM%2520systems%2520mainly%2520focus%2520on%2520bounded%2520indoor%2520environments%252C%250Aequipped%2520with%2520RGB-D%2520or%2520RGB%2520sensors.%2520However%252C%2520they%2520are%2520prone%2520to%2520decline%2520when%250Aexpanding%2520to%2520unbounded%2520scenes%2520or%2520encountering%2520adverse%2520conditions%252C%2520such%2520as%250Aviolent%2520motions%2520and%2520changing%2520illumination.%2520In%2520contrast%252C%2520oriented%2520to%2520general%250Ascenarios%252C%2520our%2520approach%2520additionally%2520tightly%2520fuses%2520LiDAR%252C%2520IMU%252C%2520and%2520camera%2520for%250Arobust%2520pose%2520estimation%2520and%2520photo-realistic%2520online%2520mapping.%2520To%2520compensate%2520for%250Aregions%2520unobserved%2520by%2520the%2520LiDAR%252C%2520we%2520propose%2520to%2520integrate%2520both%2520the%2520triangulated%250Avisual%2520points%2520from%2520images%2520and%2520LiDAR%2520points%2520for%2520initializing%25203D%2520Gaussians.%2520In%250Aaddition%252C%2520the%2520modeling%2520of%2520the%2520sky%2520and%2520varying%2520camera%2520exposure%2520have%2520been%250Arealized%2520for%2520high-quality%2520rendering.%2520Notably%252C%2520we%2520implement%2520our%2520system%2520purely%250Awith%2520C%252B%252B%2520and%2520CUDA%252C%2520and%2520meticulously%2520design%2520a%2520series%2520of%2520strategies%2520to%2520accelerate%250Athe%2520online%2520optimization%2520of%2520the%2520Gaussian-based%2520scene%2520representation.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520its%2520counterparts%2520while%250Amaintaining%2520real-time%2520capability.%2520Impressively%252C%2520regarding%2520photo-realistic%250Amapping%252C%2520our%2520method%2520with%2520our%2520estimated%2520poses%2520even%2520surpasses%2520all%2520the%2520compared%250Aapproaches%2520that%2520utilize%2520privileged%2520ground-truth%2520poses%2520for%2520mapping.%2520Our%2520code%250Awill%2520be%2520released%2520on%2520project%2520page%2520https%253A//xingxingzuo.github.io/gaussian_lic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian-LIC%3A%20Real-Time%20Photo-Realistic%20SLAM%20with%20Gaussian%20Splatting%20and%0A%20%20LiDAR-Inertial-Camera%20Fusion&entry.906535625=Xiaolei%20Lang%20and%20Laijian%20Li%20and%20Chenming%20Wu%20and%20Chen%20Zhao%20and%20Lina%20Liu%20and%20Yong%20Liu%20and%20Jiajun%20Lv%20and%20Xingxing%20Zuo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20real-time%20photo-realistic%20SLAM%20method%20based%20on%0Amarrying%20Gaussian%20Splatting%20with%20LiDAR-Inertial-Camera%20SLAM.%20Most%20existing%0Aradiance-field-based%20SLAM%20systems%20mainly%20focus%20on%20bounded%20indoor%20environments%2C%0Aequipped%20with%20RGB-D%20or%20RGB%20sensors.%20However%2C%20they%20are%20prone%20to%20decline%20when%0Aexpanding%20to%20unbounded%20scenes%20or%20encountering%20adverse%20conditions%2C%20such%20as%0Aviolent%20motions%20and%20changing%20illumination.%20In%20contrast%2C%20oriented%20to%20general%0Ascenarios%2C%20our%20approach%20additionally%20tightly%20fuses%20LiDAR%2C%20IMU%2C%20and%20camera%20for%0Arobust%20pose%20estimation%20and%20photo-realistic%20online%20mapping.%20To%20compensate%20for%0Aregions%20unobserved%20by%20the%20LiDAR%2C%20we%20propose%20to%20integrate%20both%20the%20triangulated%0Avisual%20points%20from%20images%20and%20LiDAR%20points%20for%20initializing%203D%20Gaussians.%20In%0Aaddition%2C%20the%20modeling%20of%20the%20sky%20and%20varying%20camera%20exposure%20have%20been%0Arealized%20for%20high-quality%20rendering.%20Notably%2C%20we%20implement%20our%20system%20purely%0Awith%20C%2B%2B%20and%20CUDA%2C%20and%20meticulously%20design%20a%20series%20of%20strategies%20to%20accelerate%0Athe%20online%20optimization%20of%20the%20Gaussian-based%20scene%20representation.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20outperforms%20its%20counterparts%20while%0Amaintaining%20real-time%20capability.%20Impressively%2C%20regarding%20photo-realistic%0Amapping%2C%20our%20method%20with%20our%20estimated%20poses%20even%20surpasses%20all%20the%20compared%0Aapproaches%20that%20utilize%20privileged%20ground-truth%20poses%20for%20mapping.%20Our%20code%0Awill%20be%20released%20on%20project%20page%20https%3A//xingxingzuo.github.io/gaussian_lic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06926v2&entry.124074799=Read"},
{"title": "Direct Learning of Mesh and Appearance via 3D Gaussian Splatting", "author": "Ancheng Lin and Jun Li", "abstract": "  Accurately reconstructing a 3D scene including explicit geometry information\nis both attractive and challenging. Geometry reconstruction can benefit from\nincorporating differentiable appearance models, such as Neural Radiance Fields\nand 3D Gaussian Splatting (3DGS). However, existing methods encounter\nefficiency issues due to indirect geometry learning and the paradigm of\nseparately modeling geometry and surface appearance. In this work, we propose a\nlearnable scene model that incorporates 3DGS with an explicit geometry\nrepresentation, namely a mesh. Our model learns the mesh and appearance in an\nend-to-end manner, where we bind 3D Gaussians to the mesh faces and perform\ndifferentiable rendering of 3DGS to obtain photometric supervision. The model\ncreates an effective information pathway to supervise the learning of both 3DGS\nand mesh. Experimental results demonstrate that the learned scene model not\nonly achieves state-of-the-art efficiency and rendering quality but also\nsupports manipulation using the explicit mesh. In addition, our model has a\nunique advantage in adapting to scene updates, thanks to the end-to-end\nlearning of both mesh and appearance.\n", "link": "http://arxiv.org/abs/2405.06945v2", "date": "2024-09-26", "relevancy": 3.3384, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7028}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6529}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Learning%20of%20Mesh%20and%20Appearance%20via%203D%20Gaussian%20Splatting&body=Title%3A%20Direct%20Learning%20of%20Mesh%20and%20Appearance%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Ancheng%20Lin%20and%20Jun%20Li%0AAbstract%3A%20%20%20Accurately%20reconstructing%20a%203D%20scene%20including%20explicit%20geometry%20information%0Ais%20both%20attractive%20and%20challenging.%20Geometry%20reconstruction%20can%20benefit%20from%0Aincorporating%20differentiable%20appearance%20models%2C%20such%20as%20Neural%20Radiance%20Fields%0Aand%203D%20Gaussian%20Splatting%20%283DGS%29.%20However%2C%20existing%20methods%20encounter%0Aefficiency%20issues%20due%20to%20indirect%20geometry%20learning%20and%20the%20paradigm%20of%0Aseparately%20modeling%20geometry%20and%20surface%20appearance.%20In%20this%20work%2C%20we%20propose%20a%0Alearnable%20scene%20model%20that%20incorporates%203DGS%20with%20an%20explicit%20geometry%0Arepresentation%2C%20namely%20a%20mesh.%20Our%20model%20learns%20the%20mesh%20and%20appearance%20in%20an%0Aend-to-end%20manner%2C%20where%20we%20bind%203D%20Gaussians%20to%20the%20mesh%20faces%20and%20perform%0Adifferentiable%20rendering%20of%203DGS%20to%20obtain%20photometric%20supervision.%20The%20model%0Acreates%20an%20effective%20information%20pathway%20to%20supervise%20the%20learning%20of%20both%203DGS%0Aand%20mesh.%20Experimental%20results%20demonstrate%20that%20the%20learned%20scene%20model%20not%0Aonly%20achieves%20state-of-the-art%20efficiency%20and%20rendering%20quality%20but%20also%0Asupports%20manipulation%20using%20the%20explicit%20mesh.%20In%20addition%2C%20our%20model%20has%20a%0Aunique%20advantage%20in%20adapting%20to%20scene%20updates%2C%20thanks%20to%20the%20end-to-end%0Alearning%20of%20both%20mesh%20and%20appearance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Learning%2520of%2520Mesh%2520and%2520Appearance%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DAncheng%2520Lin%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520Accurately%2520reconstructing%2520a%25203D%2520scene%2520including%2520explicit%2520geometry%2520information%250Ais%2520both%2520attractive%2520and%2520challenging.%2520Geometry%2520reconstruction%2520can%2520benefit%2520from%250Aincorporating%2520differentiable%2520appearance%2520models%252C%2520such%2520as%2520Neural%2520Radiance%2520Fields%250Aand%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520However%252C%2520existing%2520methods%2520encounter%250Aefficiency%2520issues%2520due%2520to%2520indirect%2520geometry%2520learning%2520and%2520the%2520paradigm%2520of%250Aseparately%2520modeling%2520geometry%2520and%2520surface%2520appearance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Alearnable%2520scene%2520model%2520that%2520incorporates%25203DGS%2520with%2520an%2520explicit%2520geometry%250Arepresentation%252C%2520namely%2520a%2520mesh.%2520Our%2520model%2520learns%2520the%2520mesh%2520and%2520appearance%2520in%2520an%250Aend-to-end%2520manner%252C%2520where%2520we%2520bind%25203D%2520Gaussians%2520to%2520the%2520mesh%2520faces%2520and%2520perform%250Adifferentiable%2520rendering%2520of%25203DGS%2520to%2520obtain%2520photometric%2520supervision.%2520The%2520model%250Acreates%2520an%2520effective%2520information%2520pathway%2520to%2520supervise%2520the%2520learning%2520of%2520both%25203DGS%250Aand%2520mesh.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520learned%2520scene%2520model%2520not%250Aonly%2520achieves%2520state-of-the-art%2520efficiency%2520and%2520rendering%2520quality%2520but%2520also%250Asupports%2520manipulation%2520using%2520the%2520explicit%2520mesh.%2520In%2520addition%252C%2520our%2520model%2520has%2520a%250Aunique%2520advantage%2520in%2520adapting%2520to%2520scene%2520updates%252C%2520thanks%2520to%2520the%2520end-to-end%250Alearning%2520of%2520both%2520mesh%2520and%2520appearance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Learning%20of%20Mesh%20and%20Appearance%20via%203D%20Gaussian%20Splatting&entry.906535625=Ancheng%20Lin%20and%20Jun%20Li&entry.1292438233=%20%20Accurately%20reconstructing%20a%203D%20scene%20including%20explicit%20geometry%20information%0Ais%20both%20attractive%20and%20challenging.%20Geometry%20reconstruction%20can%20benefit%20from%0Aincorporating%20differentiable%20appearance%20models%2C%20such%20as%20Neural%20Radiance%20Fields%0Aand%203D%20Gaussian%20Splatting%20%283DGS%29.%20However%2C%20existing%20methods%20encounter%0Aefficiency%20issues%20due%20to%20indirect%20geometry%20learning%20and%20the%20paradigm%20of%0Aseparately%20modeling%20geometry%20and%20surface%20appearance.%20In%20this%20work%2C%20we%20propose%20a%0Alearnable%20scene%20model%20that%20incorporates%203DGS%20with%20an%20explicit%20geometry%0Arepresentation%2C%20namely%20a%20mesh.%20Our%20model%20learns%20the%20mesh%20and%20appearance%20in%20an%0Aend-to-end%20manner%2C%20where%20we%20bind%203D%20Gaussians%20to%20the%20mesh%20faces%20and%20perform%0Adifferentiable%20rendering%20of%203DGS%20to%20obtain%20photometric%20supervision.%20The%20model%0Acreates%20an%20effective%20information%20pathway%20to%20supervise%20the%20learning%20of%20both%203DGS%0Aand%20mesh.%20Experimental%20results%20demonstrate%20that%20the%20learned%20scene%20model%20not%0Aonly%20achieves%20state-of-the-art%20efficiency%20and%20rendering%20quality%20but%20also%0Asupports%20manipulation%20using%20the%20explicit%20mesh.%20In%20addition%2C%20our%20model%20has%20a%0Aunique%20advantage%20in%20adapting%20to%20scene%20updates%2C%20thanks%20to%20the%20end-to-end%0Alearning%20of%20both%20mesh%20and%20appearance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06945v2&entry.124074799=Read"},
{"title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with\n  3D-awareness", "author": "Chenming Zhu and Tai Wang and Wenwei Zhang and Jiangmiao Pang and Xihui Liu", "abstract": "  Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced\ntheir proficiency in 2D visual understanding tasks, enabling them to\neffectively process and understand images and videos. However, the development\nof LMMs with 3D-awareness for 3D scene understanding has been hindered by the\nlack of large-scale 3D vision-language datasets and powerful 3D encoders. In\nthis paper, we introduce a simple yet effective framework called LLaVA-3D.\nLeveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D\nefficiently adapts LLaVA for 3D scene understanding without compromising 2D\nunderstanding capabilities. To achieve this, we employ a simple yet effective\nrepresentation, 3D Patch, which connects 2D CLIP patch features with their\ncorresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs\nand employing joint 2D and 3D vision-language instruction tuning, we establish\na unified architecture for both 2D image understanding and 3D scene\nunderstanding. Experimental results show that LLaVA-3D converges 3.5x faster\nthan existing 3D LMMs when trained on 3D vision-language datasets. Moreover,\nLLaVA-3D not only achieves state-of-the-art performance across various 3D tasks\nbut also maintains comparable 2D image understanding and vision-language\nconversation capabilities with LLaVA.\n", "link": "http://arxiv.org/abs/2409.18125v1", "date": "2024-09-26", "relevancy": 3.3131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6633}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-3D%3A%20A%20Simple%20yet%20Effective%20Pathway%20to%20Empowering%20LMMs%20with%0A%20%203D-awareness&body=Title%3A%20LLaVA-3D%3A%20A%20Simple%20yet%20Effective%20Pathway%20to%20Empowering%20LMMs%20with%0A%20%203D-awareness%0AAuthor%3A%20Chenming%20Zhu%20and%20Tai%20Wang%20and%20Wenwei%20Zhang%20and%20Jiangmiao%20Pang%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20greatly%20enhanced%0Atheir%20proficiency%20in%202D%20visual%20understanding%20tasks%2C%20enabling%20them%20to%0Aeffectively%20process%20and%20understand%20images%20and%20videos.%20However%2C%20the%20development%0Aof%20LMMs%20with%203D-awareness%20for%203D%20scene%20understanding%20has%20been%20hindered%20by%20the%0Alack%20of%20large-scale%203D%20vision-language%20datasets%20and%20powerful%203D%20encoders.%20In%0Athis%20paper%2C%20we%20introduce%20a%20simple%20yet%20effective%20framework%20called%20LLaVA-3D.%0ALeveraging%20the%20strong%202D%20understanding%20priors%20from%20LLaVA%2C%20our%20LLaVA-3D%0Aefficiently%20adapts%20LLaVA%20for%203D%20scene%20understanding%20without%20compromising%202D%0Aunderstanding%20capabilities.%20To%20achieve%20this%2C%20we%20employ%20a%20simple%20yet%20effective%0Arepresentation%2C%203D%20Patch%2C%20which%20connects%202D%20CLIP%20patch%20features%20with%20their%0Acorresponding%20positions%20in%203D%20space.%20By%20integrating%20the%203D%20Patches%20into%202D%20LMMs%0Aand%20employing%20joint%202D%20and%203D%20vision-language%20instruction%20tuning%2C%20we%20establish%0Aa%20unified%20architecture%20for%20both%202D%20image%20understanding%20and%203D%20scene%0Aunderstanding.%20Experimental%20results%20show%20that%20LLaVA-3D%20converges%203.5x%20faster%0Athan%20existing%203D%20LMMs%20when%20trained%20on%203D%20vision-language%20datasets.%20Moreover%2C%0ALLaVA-3D%20not%20only%20achieves%20state-of-the-art%20performance%20across%20various%203D%20tasks%0Abut%20also%20maintains%20comparable%202D%20image%20understanding%20and%20vision-language%0Aconversation%20capabilities%20with%20LLaVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-3D%253A%2520A%2520Simple%2520yet%2520Effective%2520Pathway%2520to%2520Empowering%2520LMMs%2520with%250A%2520%25203D-awareness%26entry.906535625%3DChenming%2520Zhu%2520and%2520Tai%2520Wang%2520and%2520Wenwei%2520Zhang%2520and%2520Jiangmiao%2520Pang%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520greatly%2520enhanced%250Atheir%2520proficiency%2520in%25202D%2520visual%2520understanding%2520tasks%252C%2520enabling%2520them%2520to%250Aeffectively%2520process%2520and%2520understand%2520images%2520and%2520videos.%2520However%252C%2520the%2520development%250Aof%2520LMMs%2520with%25203D-awareness%2520for%25203D%2520scene%2520understanding%2520has%2520been%2520hindered%2520by%2520the%250Alack%2520of%2520large-scale%25203D%2520vision-language%2520datasets%2520and%2520powerful%25203D%2520encoders.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%2520framework%2520called%2520LLaVA-3D.%250ALeveraging%2520the%2520strong%25202D%2520understanding%2520priors%2520from%2520LLaVA%252C%2520our%2520LLaVA-3D%250Aefficiently%2520adapts%2520LLaVA%2520for%25203D%2520scene%2520understanding%2520without%2520compromising%25202D%250Aunderstanding%2520capabilities.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520simple%2520yet%2520effective%250Arepresentation%252C%25203D%2520Patch%252C%2520which%2520connects%25202D%2520CLIP%2520patch%2520features%2520with%2520their%250Acorresponding%2520positions%2520in%25203D%2520space.%2520By%2520integrating%2520the%25203D%2520Patches%2520into%25202D%2520LMMs%250Aand%2520employing%2520joint%25202D%2520and%25203D%2520vision-language%2520instruction%2520tuning%252C%2520we%2520establish%250Aa%2520unified%2520architecture%2520for%2520both%25202D%2520image%2520understanding%2520and%25203D%2520scene%250Aunderstanding.%2520Experimental%2520results%2520show%2520that%2520LLaVA-3D%2520converges%25203.5x%2520faster%250Athan%2520existing%25203D%2520LMMs%2520when%2520trained%2520on%25203D%2520vision-language%2520datasets.%2520Moreover%252C%250ALLaVA-3D%2520not%2520only%2520achieves%2520state-of-the-art%2520performance%2520across%2520various%25203D%2520tasks%250Abut%2520also%2520maintains%2520comparable%25202D%2520image%2520understanding%2520and%2520vision-language%250Aconversation%2520capabilities%2520with%2520LLaVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-3D%3A%20A%20Simple%20yet%20Effective%20Pathway%20to%20Empowering%20LMMs%20with%0A%20%203D-awareness&entry.906535625=Chenming%20Zhu%20and%20Tai%20Wang%20and%20Wenwei%20Zhang%20and%20Jiangmiao%20Pang%20and%20Xihui%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20greatly%20enhanced%0Atheir%20proficiency%20in%202D%20visual%20understanding%20tasks%2C%20enabling%20them%20to%0Aeffectively%20process%20and%20understand%20images%20and%20videos.%20However%2C%20the%20development%0Aof%20LMMs%20with%203D-awareness%20for%203D%20scene%20understanding%20has%20been%20hindered%20by%20the%0Alack%20of%20large-scale%203D%20vision-language%20datasets%20and%20powerful%203D%20encoders.%20In%0Athis%20paper%2C%20we%20introduce%20a%20simple%20yet%20effective%20framework%20called%20LLaVA-3D.%0ALeveraging%20the%20strong%202D%20understanding%20priors%20from%20LLaVA%2C%20our%20LLaVA-3D%0Aefficiently%20adapts%20LLaVA%20for%203D%20scene%20understanding%20without%20compromising%202D%0Aunderstanding%20capabilities.%20To%20achieve%20this%2C%20we%20employ%20a%20simple%20yet%20effective%0Arepresentation%2C%203D%20Patch%2C%20which%20connects%202D%20CLIP%20patch%20features%20with%20their%0Acorresponding%20positions%20in%203D%20space.%20By%20integrating%20the%203D%20Patches%20into%202D%20LMMs%0Aand%20employing%20joint%202D%20and%203D%20vision-language%20instruction%20tuning%2C%20we%20establish%0Aa%20unified%20architecture%20for%20both%202D%20image%20understanding%20and%203D%20scene%0Aunderstanding.%20Experimental%20results%20show%20that%20LLaVA-3D%20converges%203.5x%20faster%0Athan%20existing%203D%20LMMs%20when%20trained%20on%203D%20vision-language%20datasets.%20Moreover%2C%0ALLaVA-3D%20not%20only%20achieves%20state-of-the-art%20performance%20across%20various%203D%20tasks%0Abut%20also%20maintains%20comparable%202D%20image%20understanding%20and%20vision-language%0Aconversation%20capabilities%20with%20LLaVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18125v1&entry.124074799=Read"},
{"title": "Stable Video Portraits", "author": "Mirela Ostrek and Justus Thies", "abstract": "  Rapid advances in the field of generative AI and text-to-image methods in\nparticular have transformed the way we interact with and perceive\ncomputer-generated imagery today. In parallel, much progress has been made in\n3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we\npresent SVP, a novel hybrid 2D/3D generation method that outputs photorealistic\nvideos of talking faces leveraging a large pre-trained text-to-image prior\n(2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific\nfine-tuning of a general 2D stable diffusion model which we lift to a video\nmodel by providing temporal 3DMM sequences as conditioning and by introducing a\ntemporal denoising procedure. As an output, this model generates temporally\nsmooth imagery of a person with 3DMM-based controls, i.e., a person-specific\navatar. The facial appearance of this person-specific avatar can be edited and\nmorphed to text-defined celebrities, without any fine-tuning at test time. The\nmethod is analyzed quantitatively and qualitatively, and we show that our\nmethod outperforms state-of-the-art monocular head avatar methods.\n", "link": "http://arxiv.org/abs/2409.18083v1", "date": "2024-09-26", "relevancy": 3.2907, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6787}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6479}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Video%20Portraits&body=Title%3A%20Stable%20Video%20Portraits%0AAuthor%3A%20Mirela%20Ostrek%20and%20Justus%20Thies%0AAbstract%3A%20%20%20Rapid%20advances%20in%20the%20field%20of%20generative%20AI%20and%20text-to-image%20methods%20in%0Aparticular%20have%20transformed%20the%20way%20we%20interact%20with%20and%20perceive%0Acomputer-generated%20imagery%20today.%20In%20parallel%2C%20much%20progress%20has%20been%20made%20in%0A3D%20face%20reconstruction%2C%20using%203D%20Morphable%20Models%20%283DMM%29.%20In%20this%20paper%2C%20we%0Apresent%20SVP%2C%20a%20novel%20hybrid%202D/3D%20generation%20method%20that%20outputs%20photorealistic%0Avideos%20of%20talking%20faces%20leveraging%20a%20large%20pre-trained%20text-to-image%20prior%0A%282D%29%2C%20controlled%20via%20a%203DMM%20%283D%29.%20Specifically%2C%20we%20introduce%20a%20person-specific%0Afine-tuning%20of%20a%20general%202D%20stable%20diffusion%20model%20which%20we%20lift%20to%20a%20video%0Amodel%20by%20providing%20temporal%203DMM%20sequences%20as%20conditioning%20and%20by%20introducing%20a%0Atemporal%20denoising%20procedure.%20As%20an%20output%2C%20this%20model%20generates%20temporally%0Asmooth%20imagery%20of%20a%20person%20with%203DMM-based%20controls%2C%20i.e.%2C%20a%20person-specific%0Aavatar.%20The%20facial%20appearance%20of%20this%20person-specific%20avatar%20can%20be%20edited%20and%0Amorphed%20to%20text-defined%20celebrities%2C%20without%20any%20fine-tuning%20at%20test%20time.%20The%0Amethod%20is%20analyzed%20quantitatively%20and%20qualitatively%2C%20and%20we%20show%20that%20our%0Amethod%20outperforms%20state-of-the-art%20monocular%20head%20avatar%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Video%2520Portraits%26entry.906535625%3DMirela%2520Ostrek%2520and%2520Justus%2520Thies%26entry.1292438233%3D%2520%2520Rapid%2520advances%2520in%2520the%2520field%2520of%2520generative%2520AI%2520and%2520text-to-image%2520methods%2520in%250Aparticular%2520have%2520transformed%2520the%2520way%2520we%2520interact%2520with%2520and%2520perceive%250Acomputer-generated%2520imagery%2520today.%2520In%2520parallel%252C%2520much%2520progress%2520has%2520been%2520made%2520in%250A3D%2520face%2520reconstruction%252C%2520using%25203D%2520Morphable%2520Models%2520%25283DMM%2529.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520SVP%252C%2520a%2520novel%2520hybrid%25202D/3D%2520generation%2520method%2520that%2520outputs%2520photorealistic%250Avideos%2520of%2520talking%2520faces%2520leveraging%2520a%2520large%2520pre-trained%2520text-to-image%2520prior%250A%25282D%2529%252C%2520controlled%2520via%2520a%25203DMM%2520%25283D%2529.%2520Specifically%252C%2520we%2520introduce%2520a%2520person-specific%250Afine-tuning%2520of%2520a%2520general%25202D%2520stable%2520diffusion%2520model%2520which%2520we%2520lift%2520to%2520a%2520video%250Amodel%2520by%2520providing%2520temporal%25203DMM%2520sequences%2520as%2520conditioning%2520and%2520by%2520introducing%2520a%250Atemporal%2520denoising%2520procedure.%2520As%2520an%2520output%252C%2520this%2520model%2520generates%2520temporally%250Asmooth%2520imagery%2520of%2520a%2520person%2520with%25203DMM-based%2520controls%252C%2520i.e.%252C%2520a%2520person-specific%250Aavatar.%2520The%2520facial%2520appearance%2520of%2520this%2520person-specific%2520avatar%2520can%2520be%2520edited%2520and%250Amorphed%2520to%2520text-defined%2520celebrities%252C%2520without%2520any%2520fine-tuning%2520at%2520test%2520time.%2520The%250Amethod%2520is%2520analyzed%2520quantitatively%2520and%2520qualitatively%252C%2520and%2520we%2520show%2520that%2520our%250Amethod%2520outperforms%2520state-of-the-art%2520monocular%2520head%2520avatar%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Video%20Portraits&entry.906535625=Mirela%20Ostrek%20and%20Justus%20Thies&entry.1292438233=%20%20Rapid%20advances%20in%20the%20field%20of%20generative%20AI%20and%20text-to-image%20methods%20in%0Aparticular%20have%20transformed%20the%20way%20we%20interact%20with%20and%20perceive%0Acomputer-generated%20imagery%20today.%20In%20parallel%2C%20much%20progress%20has%20been%20made%20in%0A3D%20face%20reconstruction%2C%20using%203D%20Morphable%20Models%20%283DMM%29.%20In%20this%20paper%2C%20we%0Apresent%20SVP%2C%20a%20novel%20hybrid%202D/3D%20generation%20method%20that%20outputs%20photorealistic%0Avideos%20of%20talking%20faces%20leveraging%20a%20large%20pre-trained%20text-to-image%20prior%0A%282D%29%2C%20controlled%20via%20a%203DMM%20%283D%29.%20Specifically%2C%20we%20introduce%20a%20person-specific%0Afine-tuning%20of%20a%20general%202D%20stable%20diffusion%20model%20which%20we%20lift%20to%20a%20video%0Amodel%20by%20providing%20temporal%203DMM%20sequences%20as%20conditioning%20and%20by%20introducing%20a%0Atemporal%20denoising%20procedure.%20As%20an%20output%2C%20this%20model%20generates%20temporally%0Asmooth%20imagery%20of%20a%20person%20with%203DMM-based%20controls%2C%20i.e.%2C%20a%20person-specific%0Aavatar.%20The%20facial%20appearance%20of%20this%20person-specific%20avatar%20can%20be%20edited%20and%0Amorphed%20to%20text-defined%20celebrities%2C%20without%20any%20fine-tuning%20at%20test%20time.%20The%0Amethod%20is%20analyzed%20quantitatively%20and%20qualitatively%2C%20and%20we%20show%20that%20our%0Amethod%20outperforms%20state-of-the-art%20monocular%20head%20avatar%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18083v1&entry.124074799=Read"},
{"title": "RT-GuIDE: Real-Time Gaussian splatting for Information-Driven\n  Exploration", "author": "Yuezhan Tao and Dexter Ong and Varun Murali and Igor Spasojevic and Pratik Chaudhari and Vijay Kumar", "abstract": "  We propose a framework for active mapping and exploration that leverages\nGaussian splatting for constructing information-rich maps. Further, we develop\na parallelized motion planning algorithm that can exploit the Gaussian map for\nreal-time navigation. The Gaussian map constructed onboard the robot is\noptimized for both photometric and geometric quality while enabling real-time\nsituational awareness for autonomy. We show through simulation experiments that\nour method is competitive with approaches that use alternate information gain\nmetrics, while being orders of magnitude faster to compute. In real-world\nexperiments, our algorithm achieves better map quality (10% higher Peak\nSignal-to-Noise Ratio (PSNR) and 30% higher geometric reconstruction accuracy)\nthan Gaussian maps constructed by traditional exploration baselines. Experiment\nvideos and more details can be found on our project page:\nhttps://tyuezhan.github.io/RT_GuIDE/\n", "link": "http://arxiv.org/abs/2409.18122v1", "date": "2024-09-26", "relevancy": 3.2105, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7342}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6011}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-GuIDE%3A%20Real-Time%20Gaussian%20splatting%20for%20Information-Driven%0A%20%20Exploration&body=Title%3A%20RT-GuIDE%3A%20Real-Time%20Gaussian%20splatting%20for%20Information-Driven%0A%20%20Exploration%0AAuthor%3A%20Yuezhan%20Tao%20and%20Dexter%20Ong%20and%20Varun%20Murali%20and%20Igor%20Spasojevic%20and%20Pratik%20Chaudhari%20and%20Vijay%20Kumar%0AAbstract%3A%20%20%20We%20propose%20a%20framework%20for%20active%20mapping%20and%20exploration%20that%20leverages%0AGaussian%20splatting%20for%20constructing%20information-rich%20maps.%20Further%2C%20we%20develop%0Aa%20parallelized%20motion%20planning%20algorithm%20that%20can%20exploit%20the%20Gaussian%20map%20for%0Areal-time%20navigation.%20The%20Gaussian%20map%20constructed%20onboard%20the%20robot%20is%0Aoptimized%20for%20both%20photometric%20and%20geometric%20quality%20while%20enabling%20real-time%0Asituational%20awareness%20for%20autonomy.%20We%20show%20through%20simulation%20experiments%20that%0Aour%20method%20is%20competitive%20with%20approaches%20that%20use%20alternate%20information%20gain%0Ametrics%2C%20while%20being%20orders%20of%20magnitude%20faster%20to%20compute.%20In%20real-world%0Aexperiments%2C%20our%20algorithm%20achieves%20better%20map%20quality%20%2810%25%20higher%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%20and%2030%25%20higher%20geometric%20reconstruction%20accuracy%29%0Athan%20Gaussian%20maps%20constructed%20by%20traditional%20exploration%20baselines.%20Experiment%0Avideos%20and%20more%20details%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//tyuezhan.github.io/RT_GuIDE/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-GuIDE%253A%2520Real-Time%2520Gaussian%2520splatting%2520for%2520Information-Driven%250A%2520%2520Exploration%26entry.906535625%3DYuezhan%2520Tao%2520and%2520Dexter%2520Ong%2520and%2520Varun%2520Murali%2520and%2520Igor%2520Spasojevic%2520and%2520Pratik%2520Chaudhari%2520and%2520Vijay%2520Kumar%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520framework%2520for%2520active%2520mapping%2520and%2520exploration%2520that%2520leverages%250AGaussian%2520splatting%2520for%2520constructing%2520information-rich%2520maps.%2520Further%252C%2520we%2520develop%250Aa%2520parallelized%2520motion%2520planning%2520algorithm%2520that%2520can%2520exploit%2520the%2520Gaussian%2520map%2520for%250Areal-time%2520navigation.%2520The%2520Gaussian%2520map%2520constructed%2520onboard%2520the%2520robot%2520is%250Aoptimized%2520for%2520both%2520photometric%2520and%2520geometric%2520quality%2520while%2520enabling%2520real-time%250Asituational%2520awareness%2520for%2520autonomy.%2520We%2520show%2520through%2520simulation%2520experiments%2520that%250Aour%2520method%2520is%2520competitive%2520with%2520approaches%2520that%2520use%2520alternate%2520information%2520gain%250Ametrics%252C%2520while%2520being%2520orders%2520of%2520magnitude%2520faster%2520to%2520compute.%2520In%2520real-world%250Aexperiments%252C%2520our%2520algorithm%2520achieves%2520better%2520map%2520quality%2520%252810%2525%2520higher%2520Peak%250ASignal-to-Noise%2520Ratio%2520%2528PSNR%2529%2520and%252030%2525%2520higher%2520geometric%2520reconstruction%2520accuracy%2529%250Athan%2520Gaussian%2520maps%2520constructed%2520by%2520traditional%2520exploration%2520baselines.%2520Experiment%250Avideos%2520and%2520more%2520details%2520can%2520be%2520found%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//tyuezhan.github.io/RT_GuIDE/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-GuIDE%3A%20Real-Time%20Gaussian%20splatting%20for%20Information-Driven%0A%20%20Exploration&entry.906535625=Yuezhan%20Tao%20and%20Dexter%20Ong%20and%20Varun%20Murali%20and%20Igor%20Spasojevic%20and%20Pratik%20Chaudhari%20and%20Vijay%20Kumar&entry.1292438233=%20%20We%20propose%20a%20framework%20for%20active%20mapping%20and%20exploration%20that%20leverages%0AGaussian%20splatting%20for%20constructing%20information-rich%20maps.%20Further%2C%20we%20develop%0Aa%20parallelized%20motion%20planning%20algorithm%20that%20can%20exploit%20the%20Gaussian%20map%20for%0Areal-time%20navigation.%20The%20Gaussian%20map%20constructed%20onboard%20the%20robot%20is%0Aoptimized%20for%20both%20photometric%20and%20geometric%20quality%20while%20enabling%20real-time%0Asituational%20awareness%20for%20autonomy.%20We%20show%20through%20simulation%20experiments%20that%0Aour%20method%20is%20competitive%20with%20approaches%20that%20use%20alternate%20information%20gain%0Ametrics%2C%20while%20being%20orders%20of%20magnitude%20faster%20to%20compute.%20In%20real-world%0Aexperiments%2C%20our%20algorithm%20achieves%20better%20map%20quality%20%2810%25%20higher%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%20and%2030%25%20higher%20geometric%20reconstruction%20accuracy%29%0Athan%20Gaussian%20maps%20constructed%20by%20traditional%20exploration%20baselines.%20Experiment%0Avideos%20and%20more%20details%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//tyuezhan.github.io/RT_GuIDE/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18122v1&entry.124074799=Read"},
{"title": "Chat-Scene: Bridging 3D Scene and Large Language Models with Object\n  Identifiers", "author": "Haifeng Huang and Yilun Chen and Zehan Wang and Rongjie Huang and Runsen Xu and Tai Wang and Luping Liu and Xize Cheng and Yang Zhao and Jiangmiao Pang and Zhou Zhao", "abstract": "  Recent advancements in 3D Large Language Models (LLMs) have demonstrated\npromising capabilities for 3D scene understanding. However, previous methods\nexhibit deficiencies in general referencing and grounding capabilities for\nintricate scene comprehension. In this paper, we introduce the use of object\nidentifiers and object-centric representations to interact with scenes at the\nobject level. Specifically, we decompose the input 3D scene into a set of\nobject proposals, each assigned a unique identifier token, which enables\nefficient object referencing and grounding during user-assistant interactions.\nGiven the scarcity of scene-language data, we model the scene embeddings as a\nsequence of explicit object-level embeddings, derived from semantic-rich 2D or\n3D representations. By employing object identifiers, we transform diverse 3D\nscene-language tasks into a unified question-answering format, facilitating\njoint training without the need for additional task-specific heads. With\nminimal fine-tuning on all downstream tasks, our model significantly\noutperforms existing methods on benchmarks including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.\n", "link": "http://arxiv.org/abs/2312.08168v3", "date": "2024-09-26", "relevancy": 3.2051, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6749}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chat-Scene%3A%20Bridging%203D%20Scene%20and%20Large%20Language%20Models%20with%20Object%0A%20%20Identifiers&body=Title%3A%20Chat-Scene%3A%20Bridging%203D%20Scene%20and%20Large%20Language%20Models%20with%20Object%0A%20%20Identifiers%0AAuthor%3A%20Haifeng%20Huang%20and%20Yilun%20Chen%20and%20Zehan%20Wang%20and%20Rongjie%20Huang%20and%20Runsen%20Xu%20and%20Tai%20Wang%20and%20Luping%20Liu%20and%20Xize%20Cheng%20and%20Yang%20Zhao%20and%20Jiangmiao%20Pang%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%0Apromising%20capabilities%20for%203D%20scene%20understanding.%20However%2C%20previous%20methods%0Aexhibit%20deficiencies%20in%20general%20referencing%20and%20grounding%20capabilities%20for%0Aintricate%20scene%20comprehension.%20In%20this%20paper%2C%20we%20introduce%20the%20use%20of%20object%0Aidentifiers%20and%20object-centric%20representations%20to%20interact%20with%20scenes%20at%20the%0Aobject%20level.%20Specifically%2C%20we%20decompose%20the%20input%203D%20scene%20into%20a%20set%20of%0Aobject%20proposals%2C%20each%20assigned%20a%20unique%20identifier%20token%2C%20which%20enables%0Aefficient%20object%20referencing%20and%20grounding%20during%20user-assistant%20interactions.%0AGiven%20the%20scarcity%20of%20scene-language%20data%2C%20we%20model%20the%20scene%20embeddings%20as%20a%0Asequence%20of%20explicit%20object-level%20embeddings%2C%20derived%20from%20semantic-rich%202D%20or%0A3D%20representations.%20By%20employing%20object%20identifiers%2C%20we%20transform%20diverse%203D%0Ascene-language%20tasks%20into%20a%20unified%20question-answering%20format%2C%20facilitating%0Ajoint%20training%20without%20the%20need%20for%20additional%20task-specific%20heads.%20With%0Aminimal%20fine-tuning%20on%20all%20downstream%20tasks%2C%20our%20model%20significantly%0Aoutperforms%20existing%20methods%20on%20benchmarks%20including%20ScanRefer%2C%20Multi3DRefer%2C%0AScan2Cap%2C%20ScanQA%2C%20and%20SQA3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08168v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChat-Scene%253A%2520Bridging%25203D%2520Scene%2520and%2520Large%2520Language%2520Models%2520with%2520Object%250A%2520%2520Identifiers%26entry.906535625%3DHaifeng%2520Huang%2520and%2520Yilun%2520Chen%2520and%2520Zehan%2520Wang%2520and%2520Rongjie%2520Huang%2520and%2520Runsen%2520Xu%2520and%2520Tai%2520Wang%2520and%2520Luping%2520Liu%2520and%2520Xize%2520Cheng%2520and%2520Yang%2520Zhao%2520and%2520Jiangmiao%2520Pang%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%250Apromising%2520capabilities%2520for%25203D%2520scene%2520understanding.%2520However%252C%2520previous%2520methods%250Aexhibit%2520deficiencies%2520in%2520general%2520referencing%2520and%2520grounding%2520capabilities%2520for%250Aintricate%2520scene%2520comprehension.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520use%2520of%2520object%250Aidentifiers%2520and%2520object-centric%2520representations%2520to%2520interact%2520with%2520scenes%2520at%2520the%250Aobject%2520level.%2520Specifically%252C%2520we%2520decompose%2520the%2520input%25203D%2520scene%2520into%2520a%2520set%2520of%250Aobject%2520proposals%252C%2520each%2520assigned%2520a%2520unique%2520identifier%2520token%252C%2520which%2520enables%250Aefficient%2520object%2520referencing%2520and%2520grounding%2520during%2520user-assistant%2520interactions.%250AGiven%2520the%2520scarcity%2520of%2520scene-language%2520data%252C%2520we%2520model%2520the%2520scene%2520embeddings%2520as%2520a%250Asequence%2520of%2520explicit%2520object-level%2520embeddings%252C%2520derived%2520from%2520semantic-rich%25202D%2520or%250A3D%2520representations.%2520By%2520employing%2520object%2520identifiers%252C%2520we%2520transform%2520diverse%25203D%250Ascene-language%2520tasks%2520into%2520a%2520unified%2520question-answering%2520format%252C%2520facilitating%250Ajoint%2520training%2520without%2520the%2520need%2520for%2520additional%2520task-specific%2520heads.%2520With%250Aminimal%2520fine-tuning%2520on%2520all%2520downstream%2520tasks%252C%2520our%2520model%2520significantly%250Aoutperforms%2520existing%2520methods%2520on%2520benchmarks%2520including%2520ScanRefer%252C%2520Multi3DRefer%252C%250AScan2Cap%252C%2520ScanQA%252C%2520and%2520SQA3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08168v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chat-Scene%3A%20Bridging%203D%20Scene%20and%20Large%20Language%20Models%20with%20Object%0A%20%20Identifiers&entry.906535625=Haifeng%20Huang%20and%20Yilun%20Chen%20and%20Zehan%20Wang%20and%20Rongjie%20Huang%20and%20Runsen%20Xu%20and%20Tai%20Wang%20and%20Luping%20Liu%20and%20Xize%20Cheng%20and%20Yang%20Zhao%20and%20Jiangmiao%20Pang%20and%20Zhou%20Zhao&entry.1292438233=%20%20Recent%20advancements%20in%203D%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%0Apromising%20capabilities%20for%203D%20scene%20understanding.%20However%2C%20previous%20methods%0Aexhibit%20deficiencies%20in%20general%20referencing%20and%20grounding%20capabilities%20for%0Aintricate%20scene%20comprehension.%20In%20this%20paper%2C%20we%20introduce%20the%20use%20of%20object%0Aidentifiers%20and%20object-centric%20representations%20to%20interact%20with%20scenes%20at%20the%0Aobject%20level.%20Specifically%2C%20we%20decompose%20the%20input%203D%20scene%20into%20a%20set%20of%0Aobject%20proposals%2C%20each%20assigned%20a%20unique%20identifier%20token%2C%20which%20enables%0Aefficient%20object%20referencing%20and%20grounding%20during%20user-assistant%20interactions.%0AGiven%20the%20scarcity%20of%20scene-language%20data%2C%20we%20model%20the%20scene%20embeddings%20as%20a%0Asequence%20of%20explicit%20object-level%20embeddings%2C%20derived%20from%20semantic-rich%202D%20or%0A3D%20representations.%20By%20employing%20object%20identifiers%2C%20we%20transform%20diverse%203D%0Ascene-language%20tasks%20into%20a%20unified%20question-answering%20format%2C%20facilitating%0Ajoint%20training%20without%20the%20need%20for%20additional%20task-specific%20heads.%20With%0Aminimal%20fine-tuning%20on%20all%20downstream%20tasks%2C%20our%20model%20significantly%0Aoutperforms%20existing%20methods%20on%20benchmarks%20including%20ScanRefer%2C%20Multi3DRefer%2C%0AScan2Cap%2C%20ScanQA%2C%20and%20SQA3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08168v3&entry.124074799=Read"},
{"title": "LLM4Brain: Training a Large Language Model for Brain Video Understanding", "author": "Ruizhe Zheng and Lichao Sun", "abstract": "  Decoding visual-semantic information from brain signals, such as functional\nMRI (fMRI), across different subjects poses significant challenges, including\nlow signal-to-noise ratio, limited data availability, and cross-subject\nvariability. Recent advancements in large language models (LLMs) show\nremarkable effectiveness in processing multimodal information. In this study,\nwe introduce an LLM-based approach for reconstructing visual-semantic\ninformation from fMRI signals elicited by video stimuli. Specifically, we\nemploy fine-tuning techniques on an fMRI encoder equipped with adaptors to\ntransform brain responses into latent representations aligned with the video\nstimuli. Subsequently, these representations are mapped to textual modality by\nLLM. In particular, we integrate self-supervised domain adaptation methods to\nenhance the alignment between visual-semantic information and brain responses.\nOur proposed method achieves good results using various quantitative semantic\nmetrics, while yielding similarity with ground-truth information.\n", "link": "http://arxiv.org/abs/2409.17987v1", "date": "2024-09-26", "relevancy": 3.0407, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6286}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM4Brain%3A%20Training%20a%20Large%20Language%20Model%20for%20Brain%20Video%20Understanding&body=Title%3A%20LLM4Brain%3A%20Training%20a%20Large%20Language%20Model%20for%20Brain%20Video%20Understanding%0AAuthor%3A%20Ruizhe%20Zheng%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Decoding%20visual-semantic%20information%20from%20brain%20signals%2C%20such%20as%20functional%0AMRI%20%28fMRI%29%2C%20across%20different%20subjects%20poses%20significant%20challenges%2C%20including%0Alow%20signal-to-noise%20ratio%2C%20limited%20data%20availability%2C%20and%20cross-subject%0Avariability.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20show%0Aremarkable%20effectiveness%20in%20processing%20multimodal%20information.%20In%20this%20study%2C%0Awe%20introduce%20an%20LLM-based%20approach%20for%20reconstructing%20visual-semantic%0Ainformation%20from%20fMRI%20signals%20elicited%20by%20video%20stimuli.%20Specifically%2C%20we%0Aemploy%20fine-tuning%20techniques%20on%20an%20fMRI%20encoder%20equipped%20with%20adaptors%20to%0Atransform%20brain%20responses%20into%20latent%20representations%20aligned%20with%20the%20video%0Astimuli.%20Subsequently%2C%20these%20representations%20are%20mapped%20to%20textual%20modality%20by%0ALLM.%20In%20particular%2C%20we%20integrate%20self-supervised%20domain%20adaptation%20methods%20to%0Aenhance%20the%20alignment%20between%20visual-semantic%20information%20and%20brain%20responses.%0AOur%20proposed%20method%20achieves%20good%20results%20using%20various%20quantitative%20semantic%0Ametrics%2C%20while%20yielding%20similarity%20with%20ground-truth%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM4Brain%253A%2520Training%2520a%2520Large%2520Language%2520Model%2520for%2520Brain%2520Video%2520Understanding%26entry.906535625%3DRuizhe%2520Zheng%2520and%2520Lichao%2520Sun%26entry.1292438233%3D%2520%2520Decoding%2520visual-semantic%2520information%2520from%2520brain%2520signals%252C%2520such%2520as%2520functional%250AMRI%2520%2528fMRI%2529%252C%2520across%2520different%2520subjects%2520poses%2520significant%2520challenges%252C%2520including%250Alow%2520signal-to-noise%2520ratio%252C%2520limited%2520data%2520availability%252C%2520and%2520cross-subject%250Avariability.%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%250Aremarkable%2520effectiveness%2520in%2520processing%2520multimodal%2520information.%2520In%2520this%2520study%252C%250Awe%2520introduce%2520an%2520LLM-based%2520approach%2520for%2520reconstructing%2520visual-semantic%250Ainformation%2520from%2520fMRI%2520signals%2520elicited%2520by%2520video%2520stimuli.%2520Specifically%252C%2520we%250Aemploy%2520fine-tuning%2520techniques%2520on%2520an%2520fMRI%2520encoder%2520equipped%2520with%2520adaptors%2520to%250Atransform%2520brain%2520responses%2520into%2520latent%2520representations%2520aligned%2520with%2520the%2520video%250Astimuli.%2520Subsequently%252C%2520these%2520representations%2520are%2520mapped%2520to%2520textual%2520modality%2520by%250ALLM.%2520In%2520particular%252C%2520we%2520integrate%2520self-supervised%2520domain%2520adaptation%2520methods%2520to%250Aenhance%2520the%2520alignment%2520between%2520visual-semantic%2520information%2520and%2520brain%2520responses.%250AOur%2520proposed%2520method%2520achieves%2520good%2520results%2520using%2520various%2520quantitative%2520semantic%250Ametrics%252C%2520while%2520yielding%2520similarity%2520with%2520ground-truth%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM4Brain%3A%20Training%20a%20Large%20Language%20Model%20for%20Brain%20Video%20Understanding&entry.906535625=Ruizhe%20Zheng%20and%20Lichao%20Sun&entry.1292438233=%20%20Decoding%20visual-semantic%20information%20from%20brain%20signals%2C%20such%20as%20functional%0AMRI%20%28fMRI%29%2C%20across%20different%20subjects%20poses%20significant%20challenges%2C%20including%0Alow%20signal-to-noise%20ratio%2C%20limited%20data%20availability%2C%20and%20cross-subject%0Avariability.%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20show%0Aremarkable%20effectiveness%20in%20processing%20multimodal%20information.%20In%20this%20study%2C%0Awe%20introduce%20an%20LLM-based%20approach%20for%20reconstructing%20visual-semantic%0Ainformation%20from%20fMRI%20signals%20elicited%20by%20video%20stimuli.%20Specifically%2C%20we%0Aemploy%20fine-tuning%20techniques%20on%20an%20fMRI%20encoder%20equipped%20with%20adaptors%20to%0Atransform%20brain%20responses%20into%20latent%20representations%20aligned%20with%20the%20video%0Astimuli.%20Subsequently%2C%20these%20representations%20are%20mapped%20to%20textual%20modality%20by%0ALLM.%20In%20particular%2C%20we%20integrate%20self-supervised%20domain%20adaptation%20methods%20to%0Aenhance%20the%20alignment%20between%20visual-semantic%20information%20and%20brain%20responses.%0AOur%20proposed%20method%20achieves%20good%20results%20using%20various%20quantitative%20semantic%0Ametrics%2C%20while%20yielding%20similarity%20with%20ground-truth%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17987v1&entry.124074799=Read"},
{"title": "Disentangled Clothed Avatar Generation from Text Descriptions", "author": "Jionghao Wang and Yuan Liu and Zhiyang Dou and Zhengming Yu and Yongqing Liang and Cheng Lin and Xin Li and Wenping Wang and Rong Xie and Li Song", "abstract": "  In this paper, we introduce a novel text-to-avatar generation method that\nseparately generates the human body and the clothes and allows high-quality\nanimation on the generated avatar. While recent advancements in text-to-avatar\ngeneration have yielded diverse human avatars from text prompts, these methods\ntypically combine all elements-clothes, hair, and body-into a single 3D\nrepresentation. Such an entangled approach poses challenges for downstream\ntasks like editing or animation. To overcome these limitations, we propose a\nnovel disentangled 3D avatar representation named Sequentially Offset-SMPL\n(SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body and\nclothes with two separate meshes but associates them with offsets to ensure the\nphysical alignment between the body and the clothes. Then, we design a Score\nDistillation Sampling (SDS)-based distillation framework to generate the\nproposed SO-SMPL representation from text prompts. Our approach not only\nachieves higher texture and geometry quality and better semantic alignment with\ntext prompts, but also significantly improves the visual quality of character\nanimation, virtual try-on, and avatar editing. Project page:\nhttps://shanemankiw.github.io/SO-SMPL/.\n", "link": "http://arxiv.org/abs/2312.05295v2", "date": "2024-09-26", "relevancy": 3.0146, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6706}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5705}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Clothed%20Avatar%20Generation%20from%20Text%20Descriptions&body=Title%3A%20Disentangled%20Clothed%20Avatar%20Generation%20from%20Text%20Descriptions%0AAuthor%3A%20Jionghao%20Wang%20and%20Yuan%20Liu%20and%20Zhiyang%20Dou%20and%20Zhengming%20Yu%20and%20Yongqing%20Liang%20and%20Cheng%20Lin%20and%20Xin%20Li%20and%20Wenping%20Wang%20and%20Rong%20Xie%20and%20Li%20Song%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20text-to-avatar%20generation%20method%20that%0Aseparately%20generates%20the%20human%20body%20and%20the%20clothes%20and%20allows%20high-quality%0Aanimation%20on%20the%20generated%20avatar.%20While%20recent%20advancements%20in%20text-to-avatar%0Ageneration%20have%20yielded%20diverse%20human%20avatars%20from%20text%20prompts%2C%20these%20methods%0Atypically%20combine%20all%20elements-clothes%2C%20hair%2C%20and%20body-into%20a%20single%203D%0Arepresentation.%20Such%20an%20entangled%20approach%20poses%20challenges%20for%20downstream%0Atasks%20like%20editing%20or%20animation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20disentangled%203D%20avatar%20representation%20named%20Sequentially%20Offset-SMPL%0A%28SO-SMPL%29%2C%20building%20upon%20the%20SMPL%20model.%20SO-SMPL%20represents%20the%20human%20body%20and%0Aclothes%20with%20two%20separate%20meshes%20but%20associates%20them%20with%20offsets%20to%20ensure%20the%0Aphysical%20alignment%20between%20the%20body%20and%20the%20clothes.%20Then%2C%20we%20design%20a%20Score%0ADistillation%20Sampling%20%28SDS%29-based%20distillation%20framework%20to%20generate%20the%0Aproposed%20SO-SMPL%20representation%20from%20text%20prompts.%20Our%20approach%20not%20only%0Aachieves%20higher%20texture%20and%20geometry%20quality%20and%20better%20semantic%20alignment%20with%0Atext%20prompts%2C%20but%20also%20significantly%20improves%20the%20visual%20quality%20of%20character%0Aanimation%2C%20virtual%20try-on%2C%20and%20avatar%20editing.%20Project%20page%3A%0Ahttps%3A//shanemankiw.github.io/SO-SMPL/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Clothed%2520Avatar%2520Generation%2520from%2520Text%2520Descriptions%26entry.906535625%3DJionghao%2520Wang%2520and%2520Yuan%2520Liu%2520and%2520Zhiyang%2520Dou%2520and%2520Zhengming%2520Yu%2520and%2520Yongqing%2520Liang%2520and%2520Cheng%2520Lin%2520and%2520Xin%2520Li%2520and%2520Wenping%2520Wang%2520and%2520Rong%2520Xie%2520and%2520Li%2520Song%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520text-to-avatar%2520generation%2520method%2520that%250Aseparately%2520generates%2520the%2520human%2520body%2520and%2520the%2520clothes%2520and%2520allows%2520high-quality%250Aanimation%2520on%2520the%2520generated%2520avatar.%2520While%2520recent%2520advancements%2520in%2520text-to-avatar%250Ageneration%2520have%2520yielded%2520diverse%2520human%2520avatars%2520from%2520text%2520prompts%252C%2520these%2520methods%250Atypically%2520combine%2520all%2520elements-clothes%252C%2520hair%252C%2520and%2520body-into%2520a%2520single%25203D%250Arepresentation.%2520Such%2520an%2520entangled%2520approach%2520poses%2520challenges%2520for%2520downstream%250Atasks%2520like%2520editing%2520or%2520animation.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520disentangled%25203D%2520avatar%2520representation%2520named%2520Sequentially%2520Offset-SMPL%250A%2528SO-SMPL%2529%252C%2520building%2520upon%2520the%2520SMPL%2520model.%2520SO-SMPL%2520represents%2520the%2520human%2520body%2520and%250Aclothes%2520with%2520two%2520separate%2520meshes%2520but%2520associates%2520them%2520with%2520offsets%2520to%2520ensure%2520the%250Aphysical%2520alignment%2520between%2520the%2520body%2520and%2520the%2520clothes.%2520Then%252C%2520we%2520design%2520a%2520Score%250ADistillation%2520Sampling%2520%2528SDS%2529-based%2520distillation%2520framework%2520to%2520generate%2520the%250Aproposed%2520SO-SMPL%2520representation%2520from%2520text%2520prompts.%2520Our%2520approach%2520not%2520only%250Aachieves%2520higher%2520texture%2520and%2520geometry%2520quality%2520and%2520better%2520semantic%2520alignment%2520with%250Atext%2520prompts%252C%2520but%2520also%2520significantly%2520improves%2520the%2520visual%2520quality%2520of%2520character%250Aanimation%252C%2520virtual%2520try-on%252C%2520and%2520avatar%2520editing.%2520Project%2520page%253A%250Ahttps%253A//shanemankiw.github.io/SO-SMPL/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Clothed%20Avatar%20Generation%20from%20Text%20Descriptions&entry.906535625=Jionghao%20Wang%20and%20Yuan%20Liu%20and%20Zhiyang%20Dou%20and%20Zhengming%20Yu%20and%20Yongqing%20Liang%20and%20Cheng%20Lin%20and%20Xin%20Li%20and%20Wenping%20Wang%20and%20Rong%20Xie%20and%20Li%20Song&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20text-to-avatar%20generation%20method%20that%0Aseparately%20generates%20the%20human%20body%20and%20the%20clothes%20and%20allows%20high-quality%0Aanimation%20on%20the%20generated%20avatar.%20While%20recent%20advancements%20in%20text-to-avatar%0Ageneration%20have%20yielded%20diverse%20human%20avatars%20from%20text%20prompts%2C%20these%20methods%0Atypically%20combine%20all%20elements-clothes%2C%20hair%2C%20and%20body-into%20a%20single%203D%0Arepresentation.%20Such%20an%20entangled%20approach%20poses%20challenges%20for%20downstream%0Atasks%20like%20editing%20or%20animation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20disentangled%203D%20avatar%20representation%20named%20Sequentially%20Offset-SMPL%0A%28SO-SMPL%29%2C%20building%20upon%20the%20SMPL%20model.%20SO-SMPL%20represents%20the%20human%20body%20and%0Aclothes%20with%20two%20separate%20meshes%20but%20associates%20them%20with%20offsets%20to%20ensure%20the%0Aphysical%20alignment%20between%20the%20body%20and%20the%20clothes.%20Then%2C%20we%20design%20a%20Score%0ADistillation%20Sampling%20%28SDS%29-based%20distillation%20framework%20to%20generate%20the%0Aproposed%20SO-SMPL%20representation%20from%20text%20prompts.%20Our%20approach%20not%20only%0Aachieves%20higher%20texture%20and%20geometry%20quality%20and%20better%20semantic%20alignment%20with%0Atext%20prompts%2C%20but%20also%20significantly%20improves%20the%20visual%20quality%20of%20character%0Aanimation%2C%20virtual%20try-on%2C%20and%20avatar%20editing.%20Project%20page%3A%0Ahttps%3A//shanemankiw.github.io/SO-SMPL/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05295v2&entry.124074799=Read"},
{"title": "PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless\n  Imaging", "author": "Xin Cai and Zhiyuan You and Hailong Zhang and Wentao Liu and Jinwei Gu and Tianfan Xue", "abstract": "  Lensless cameras offer significant advantages in size, weight, and cost\ncompared to traditional lens-based systems. Without a focusing lens, lensless\ncameras rely on computational algorithms to recover the scenes from multiplexed\nmeasurements. However, current algorithms struggle with inaccurate forward\nimaging models and insufficient priors to reconstruct high-quality images. To\novercome these limitations, we introduce a novel two-stage approach for\nconsistent and photorealistic lensless image reconstruction. The first stage of\nour approach ensures data consistency by focusing on accurately reconstructing\nthe low-frequency content with a spatially varying deconvolution method that\nadjusts to changes in the Point Spread Function (PSF) across the camera's field\nof view. The second stage enhances photorealism by incorporating a generative\nprior from pre-trained diffusion models. By conditioning on the low-frequency\ncontent retrieved in the first stage, the diffusion model effectively\nreconstructs the high-frequency details that are typically lost in the lensless\nimaging process, while also maintaining image fidelity. Our method achieves a\nsuperior balance between data fidelity and visual quality compared to existing\nmethods, as demonstrated with two popular lensless systems, PhlatCam and\nDiffuserCam. Project website: https://phocolens.github.io/.\n", "link": "http://arxiv.org/abs/2409.17996v1", "date": "2024-09-26", "relevancy": 3.0027, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6121}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6121}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhoCoLens%3A%20Photorealistic%20and%20Consistent%20Reconstruction%20in%20Lensless%0A%20%20Imaging&body=Title%3A%20PhoCoLens%3A%20Photorealistic%20and%20Consistent%20Reconstruction%20in%20Lensless%0A%20%20Imaging%0AAuthor%3A%20Xin%20Cai%20and%20Zhiyuan%20You%20and%20Hailong%20Zhang%20and%20Wentao%20Liu%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20Lensless%20cameras%20offer%20significant%20advantages%20in%20size%2C%20weight%2C%20and%20cost%0Acompared%20to%20traditional%20lens-based%20systems.%20Without%20a%20focusing%20lens%2C%20lensless%0Acameras%20rely%20on%20computational%20algorithms%20to%20recover%20the%20scenes%20from%20multiplexed%0Ameasurements.%20However%2C%20current%20algorithms%20struggle%20with%20inaccurate%20forward%0Aimaging%20models%20and%20insufficient%20priors%20to%20reconstruct%20high-quality%20images.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20a%20novel%20two-stage%20approach%20for%0Aconsistent%20and%20photorealistic%20lensless%20image%20reconstruction.%20The%20first%20stage%20of%0Aour%20approach%20ensures%20data%20consistency%20by%20focusing%20on%20accurately%20reconstructing%0Athe%20low-frequency%20content%20with%20a%20spatially%20varying%20deconvolution%20method%20that%0Aadjusts%20to%20changes%20in%20the%20Point%20Spread%20Function%20%28PSF%29%20across%20the%20camera%27s%20field%0Aof%20view.%20The%20second%20stage%20enhances%20photorealism%20by%20incorporating%20a%20generative%0Aprior%20from%20pre-trained%20diffusion%20models.%20By%20conditioning%20on%20the%20low-frequency%0Acontent%20retrieved%20in%20the%20first%20stage%2C%20the%20diffusion%20model%20effectively%0Areconstructs%20the%20high-frequency%20details%20that%20are%20typically%20lost%20in%20the%20lensless%0Aimaging%20process%2C%20while%20also%20maintaining%20image%20fidelity.%20Our%20method%20achieves%20a%0Asuperior%20balance%20between%20data%20fidelity%20and%20visual%20quality%20compared%20to%20existing%0Amethods%2C%20as%20demonstrated%20with%20two%20popular%20lensless%20systems%2C%20PhlatCam%20and%0ADiffuserCam.%20Project%20website%3A%20https%3A//phocolens.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhoCoLens%253A%2520Photorealistic%2520and%2520Consistent%2520Reconstruction%2520in%2520Lensless%250A%2520%2520Imaging%26entry.906535625%3DXin%2520Cai%2520and%2520Zhiyuan%2520You%2520and%2520Hailong%2520Zhang%2520and%2520Wentao%2520Liu%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520Lensless%2520cameras%2520offer%2520significant%2520advantages%2520in%2520size%252C%2520weight%252C%2520and%2520cost%250Acompared%2520to%2520traditional%2520lens-based%2520systems.%2520Without%2520a%2520focusing%2520lens%252C%2520lensless%250Acameras%2520rely%2520on%2520computational%2520algorithms%2520to%2520recover%2520the%2520scenes%2520from%2520multiplexed%250Ameasurements.%2520However%252C%2520current%2520algorithms%2520struggle%2520with%2520inaccurate%2520forward%250Aimaging%2520models%2520and%2520insufficient%2520priors%2520to%2520reconstruct%2520high-quality%2520images.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520novel%2520two-stage%2520approach%2520for%250Aconsistent%2520and%2520photorealistic%2520lensless%2520image%2520reconstruction.%2520The%2520first%2520stage%2520of%250Aour%2520approach%2520ensures%2520data%2520consistency%2520by%2520focusing%2520on%2520accurately%2520reconstructing%250Athe%2520low-frequency%2520content%2520with%2520a%2520spatially%2520varying%2520deconvolution%2520method%2520that%250Aadjusts%2520to%2520changes%2520in%2520the%2520Point%2520Spread%2520Function%2520%2528PSF%2529%2520across%2520the%2520camera%2527s%2520field%250Aof%2520view.%2520The%2520second%2520stage%2520enhances%2520photorealism%2520by%2520incorporating%2520a%2520generative%250Aprior%2520from%2520pre-trained%2520diffusion%2520models.%2520By%2520conditioning%2520on%2520the%2520low-frequency%250Acontent%2520retrieved%2520in%2520the%2520first%2520stage%252C%2520the%2520diffusion%2520model%2520effectively%250Areconstructs%2520the%2520high-frequency%2520details%2520that%2520are%2520typically%2520lost%2520in%2520the%2520lensless%250Aimaging%2520process%252C%2520while%2520also%2520maintaining%2520image%2520fidelity.%2520Our%2520method%2520achieves%2520a%250Asuperior%2520balance%2520between%2520data%2520fidelity%2520and%2520visual%2520quality%2520compared%2520to%2520existing%250Amethods%252C%2520as%2520demonstrated%2520with%2520two%2520popular%2520lensless%2520systems%252C%2520PhlatCam%2520and%250ADiffuserCam.%2520Project%2520website%253A%2520https%253A//phocolens.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhoCoLens%3A%20Photorealistic%20and%20Consistent%20Reconstruction%20in%20Lensless%0A%20%20Imaging&entry.906535625=Xin%20Cai%20and%20Zhiyuan%20You%20and%20Hailong%20Zhang%20and%20Wentao%20Liu%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue&entry.1292438233=%20%20Lensless%20cameras%20offer%20significant%20advantages%20in%20size%2C%20weight%2C%20and%20cost%0Acompared%20to%20traditional%20lens-based%20systems.%20Without%20a%20focusing%20lens%2C%20lensless%0Acameras%20rely%20on%20computational%20algorithms%20to%20recover%20the%20scenes%20from%20multiplexed%0Ameasurements.%20However%2C%20current%20algorithms%20struggle%20with%20inaccurate%20forward%0Aimaging%20models%20and%20insufficient%20priors%20to%20reconstruct%20high-quality%20images.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20a%20novel%20two-stage%20approach%20for%0Aconsistent%20and%20photorealistic%20lensless%20image%20reconstruction.%20The%20first%20stage%20of%0Aour%20approach%20ensures%20data%20consistency%20by%20focusing%20on%20accurately%20reconstructing%0Athe%20low-frequency%20content%20with%20a%20spatially%20varying%20deconvolution%20method%20that%0Aadjusts%20to%20changes%20in%20the%20Point%20Spread%20Function%20%28PSF%29%20across%20the%20camera%27s%20field%0Aof%20view.%20The%20second%20stage%20enhances%20photorealism%20by%20incorporating%20a%20generative%0Aprior%20from%20pre-trained%20diffusion%20models.%20By%20conditioning%20on%20the%20low-frequency%0Acontent%20retrieved%20in%20the%20first%20stage%2C%20the%20diffusion%20model%20effectively%0Areconstructs%20the%20high-frequency%20details%20that%20are%20typically%20lost%20in%20the%20lensless%0Aimaging%20process%2C%20while%20also%20maintaining%20image%20fidelity.%20Our%20method%20achieves%20a%0Asuperior%20balance%20between%20data%20fidelity%20and%20visual%20quality%20compared%20to%20existing%0Amethods%2C%20as%20demonstrated%20with%20two%20popular%20lensless%20systems%2C%20PhlatCam%20and%0ADiffuserCam.%20Project%20website%3A%20https%3A//phocolens.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17996v1&entry.124074799=Read"},
{"title": "WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D\n  Gaussians", "author": "Dmytro Kotovenko and Olga Grebenkova and Nikolaos Sarafianos and Avinash Paliwal and Pingchuan Ma and Omid Poursaeed and Sreyas Mohan and Yuchen Fan and Yilei Li and Rakesh Ranjan and Bj\u00f6rn Ommer", "abstract": "  While style transfer techniques have been well-developed for 2D image\nstylization, the extension of these methods to 3D scenes remains relatively\nunexplored. Existing approaches demonstrate proficiency in transferring colors\nand textures but often struggle with replicating the geometry of the scenes. In\nour work, we leverage an explicit Gaussian Splatting (GS) representation and\ndirectly match the distributions of Gaussians between style and content scenes\nusing the Earth Mover's Distance (EMD). By employing the entropy-regularized\nWasserstein-2 distance, we ensure that the transformation maintains spatial\nsmoothness. Additionally, we decompose the scene stylization problem into\nsmaller chunks to enhance efficiency. This paradigm shift reframes stylization\nfrom a pure generative process driven by latent space losses to an explicit\nmatching of distributions between two Gaussian representations. Our method\nachieves high-resolution 3D stylization by faithfully transferring details from\n3D style scenes onto the content scene. Furthermore, WaSt-3D consistently\ndelivers results across diverse content and style scenes without necessitating\nany training, as it relies solely on optimization-based techniques. See our\nproject page for additional results and source code:\n$\\href{https://compvis.github.io/wast3d/}{https://compvis.github.io/wast3d/}$.\n", "link": "http://arxiv.org/abs/2409.17917v1", "date": "2024-09-26", "relevancy": 2.9375, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6306}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5707}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaSt-3D%3A%20Wasserstein-2%20Distance%20for%20Scene-to-Scene%20Stylization%20on%203D%0A%20%20Gaussians&body=Title%3A%20WaSt-3D%3A%20Wasserstein-2%20Distance%20for%20Scene-to-Scene%20Stylization%20on%203D%0A%20%20Gaussians%0AAuthor%3A%20Dmytro%20Kotovenko%20and%20Olga%20Grebenkova%20and%20Nikolaos%20Sarafianos%20and%20Avinash%20Paliwal%20and%20Pingchuan%20Ma%20and%20Omid%20Poursaeed%20and%20Sreyas%20Mohan%20and%20Yuchen%20Fan%20and%20Yilei%20Li%20and%20Rakesh%20Ranjan%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20While%20style%20transfer%20techniques%20have%20been%20well-developed%20for%202D%20image%0Astylization%2C%20the%20extension%20of%20these%20methods%20to%203D%20scenes%20remains%20relatively%0Aunexplored.%20Existing%20approaches%20demonstrate%20proficiency%20in%20transferring%20colors%0Aand%20textures%20but%20often%20struggle%20with%20replicating%20the%20geometry%20of%20the%20scenes.%20In%0Aour%20work%2C%20we%20leverage%20an%20explicit%20Gaussian%20Splatting%20%28GS%29%20representation%20and%0Adirectly%20match%20the%20distributions%20of%20Gaussians%20between%20style%20and%20content%20scenes%0Ausing%20the%20Earth%20Mover%27s%20Distance%20%28EMD%29.%20By%20employing%20the%20entropy-regularized%0AWasserstein-2%20distance%2C%20we%20ensure%20that%20the%20transformation%20maintains%20spatial%0Asmoothness.%20Additionally%2C%20we%20decompose%20the%20scene%20stylization%20problem%20into%0Asmaller%20chunks%20to%20enhance%20efficiency.%20This%20paradigm%20shift%20reframes%20stylization%0Afrom%20a%20pure%20generative%20process%20driven%20by%20latent%20space%20losses%20to%20an%20explicit%0Amatching%20of%20distributions%20between%20two%20Gaussian%20representations.%20Our%20method%0Aachieves%20high-resolution%203D%20stylization%20by%20faithfully%20transferring%20details%20from%0A3D%20style%20scenes%20onto%20the%20content%20scene.%20Furthermore%2C%20WaSt-3D%20consistently%0Adelivers%20results%20across%20diverse%20content%20and%20style%20scenes%20without%20necessitating%0Aany%20training%2C%20as%20it%20relies%20solely%20on%20optimization-based%20techniques.%20See%20our%0Aproject%20page%20for%20additional%20results%20and%20source%20code%3A%0A%24%5Chref%7Bhttps%3A//compvis.github.io/wast3d/%7D%7Bhttps%3A//compvis.github.io/wast3d/%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaSt-3D%253A%2520Wasserstein-2%2520Distance%2520for%2520Scene-to-Scene%2520Stylization%2520on%25203D%250A%2520%2520Gaussians%26entry.906535625%3DDmytro%2520Kotovenko%2520and%2520Olga%2520Grebenkova%2520and%2520Nikolaos%2520Sarafianos%2520and%2520Avinash%2520Paliwal%2520and%2520Pingchuan%2520Ma%2520and%2520Omid%2520Poursaeed%2520and%2520Sreyas%2520Mohan%2520and%2520Yuchen%2520Fan%2520and%2520Yilei%2520Li%2520and%2520Rakesh%2520Ranjan%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3D%2520%2520While%2520style%2520transfer%2520techniques%2520have%2520been%2520well-developed%2520for%25202D%2520image%250Astylization%252C%2520the%2520extension%2520of%2520these%2520methods%2520to%25203D%2520scenes%2520remains%2520relatively%250Aunexplored.%2520Existing%2520approaches%2520demonstrate%2520proficiency%2520in%2520transferring%2520colors%250Aand%2520textures%2520but%2520often%2520struggle%2520with%2520replicating%2520the%2520geometry%2520of%2520the%2520scenes.%2520In%250Aour%2520work%252C%2520we%2520leverage%2520an%2520explicit%2520Gaussian%2520Splatting%2520%2528GS%2529%2520representation%2520and%250Adirectly%2520match%2520the%2520distributions%2520of%2520Gaussians%2520between%2520style%2520and%2520content%2520scenes%250Ausing%2520the%2520Earth%2520Mover%2527s%2520Distance%2520%2528EMD%2529.%2520By%2520employing%2520the%2520entropy-regularized%250AWasserstein-2%2520distance%252C%2520we%2520ensure%2520that%2520the%2520transformation%2520maintains%2520spatial%250Asmoothness.%2520Additionally%252C%2520we%2520decompose%2520the%2520scene%2520stylization%2520problem%2520into%250Asmaller%2520chunks%2520to%2520enhance%2520efficiency.%2520This%2520paradigm%2520shift%2520reframes%2520stylization%250Afrom%2520a%2520pure%2520generative%2520process%2520driven%2520by%2520latent%2520space%2520losses%2520to%2520an%2520explicit%250Amatching%2520of%2520distributions%2520between%2520two%2520Gaussian%2520representations.%2520Our%2520method%250Aachieves%2520high-resolution%25203D%2520stylization%2520by%2520faithfully%2520transferring%2520details%2520from%250A3D%2520style%2520scenes%2520onto%2520the%2520content%2520scene.%2520Furthermore%252C%2520WaSt-3D%2520consistently%250Adelivers%2520results%2520across%2520diverse%2520content%2520and%2520style%2520scenes%2520without%2520necessitating%250Aany%2520training%252C%2520as%2520it%2520relies%2520solely%2520on%2520optimization-based%2520techniques.%2520See%2520our%250Aproject%2520page%2520for%2520additional%2520results%2520and%2520source%2520code%253A%250A%2524%255Chref%257Bhttps%253A//compvis.github.io/wast3d/%257D%257Bhttps%253A//compvis.github.io/wast3d/%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaSt-3D%3A%20Wasserstein-2%20Distance%20for%20Scene-to-Scene%20Stylization%20on%203D%0A%20%20Gaussians&entry.906535625=Dmytro%20Kotovenko%20and%20Olga%20Grebenkova%20and%20Nikolaos%20Sarafianos%20and%20Avinash%20Paliwal%20and%20Pingchuan%20Ma%20and%20Omid%20Poursaeed%20and%20Sreyas%20Mohan%20and%20Yuchen%20Fan%20and%20Yilei%20Li%20and%20Rakesh%20Ranjan%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20While%20style%20transfer%20techniques%20have%20been%20well-developed%20for%202D%20image%0Astylization%2C%20the%20extension%20of%20these%20methods%20to%203D%20scenes%20remains%20relatively%0Aunexplored.%20Existing%20approaches%20demonstrate%20proficiency%20in%20transferring%20colors%0Aand%20textures%20but%20often%20struggle%20with%20replicating%20the%20geometry%20of%20the%20scenes.%20In%0Aour%20work%2C%20we%20leverage%20an%20explicit%20Gaussian%20Splatting%20%28GS%29%20representation%20and%0Adirectly%20match%20the%20distributions%20of%20Gaussians%20between%20style%20and%20content%20scenes%0Ausing%20the%20Earth%20Mover%27s%20Distance%20%28EMD%29.%20By%20employing%20the%20entropy-regularized%0AWasserstein-2%20distance%2C%20we%20ensure%20that%20the%20transformation%20maintains%20spatial%0Asmoothness.%20Additionally%2C%20we%20decompose%20the%20scene%20stylization%20problem%20into%0Asmaller%20chunks%20to%20enhance%20efficiency.%20This%20paradigm%20shift%20reframes%20stylization%0Afrom%20a%20pure%20generative%20process%20driven%20by%20latent%20space%20losses%20to%20an%20explicit%0Amatching%20of%20distributions%20between%20two%20Gaussian%20representations.%20Our%20method%0Aachieves%20high-resolution%203D%20stylization%20by%20faithfully%20transferring%20details%20from%0A3D%20style%20scenes%20onto%20the%20content%20scene.%20Furthermore%2C%20WaSt-3D%20consistently%0Adelivers%20results%20across%20diverse%20content%20and%20style%20scenes%20without%20necessitating%0Aany%20training%2C%20as%20it%20relies%20solely%20on%20optimization-based%20techniques.%20See%20our%0Aproject%20page%20for%20additional%20results%20and%20source%20code%3A%0A%24%5Chref%7Bhttps%3A//compvis.github.io/wast3d/%7D%7Bhttps%3A//compvis.github.io/wast3d/%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17917v1&entry.124074799=Read"},
{"title": "OmniColor: A Global Camera Pose Optimization Approach of LiDAR-360Camera\n  Fusion for Colorizing Point Clouds", "author": "Bonan Liu and Guoyang Zhao and Jianhao Jiao and Guang Cai and Chengyang Li and Handi Yin and Yuyang Wang and Ming Liu and Pan Hui", "abstract": "  A Colored point cloud, as a simple and efficient 3D representation, has many\nadvantages in various fields, including robotic navigation and scene\nreconstruction. This representation is now commonly used in 3D reconstruction\ntasks relying on cameras and LiDARs. However, fusing data from these two types\nof sensors is poorly performed in many existing frameworks, leading to\nunsatisfactory mapping results, mainly due to inaccurate camera poses. This\npaper presents OmniColor, a novel and efficient algorithm to colorize point\nclouds using an independent 360-degree camera. Given a LiDAR-based point cloud\nand a sequence of panorama images with initial coarse camera poses, our\nobjective is to jointly optimize the poses of all frames for mapping images\nonto geometric reconstructions. Our pipeline works in an off-the-shelf manner\nthat does not require any feature extraction or matching process. Instead, we\nfind optimal poses by directly maximizing the photometric consistency of LiDAR\nmaps. In experiments, we show that our method can overcome the severe visual\ndistortion of omnidirectional images and greatly benefit from the wide field of\nview (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy\nand stability. The code will be released at\nhttps://github.com/liubonan123/OmniColor/.\n", "link": "http://arxiv.org/abs/2404.04693v2", "date": "2024-09-26", "relevancy": 2.8954, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5993}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5784}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniColor%3A%20A%20Global%20Camera%20Pose%20Optimization%20Approach%20of%20LiDAR-360Camera%0A%20%20Fusion%20for%20Colorizing%20Point%20Clouds&body=Title%3A%20OmniColor%3A%20A%20Global%20Camera%20Pose%20Optimization%20Approach%20of%20LiDAR-360Camera%0A%20%20Fusion%20for%20Colorizing%20Point%20Clouds%0AAuthor%3A%20Bonan%20Liu%20and%20Guoyang%20Zhao%20and%20Jianhao%20Jiao%20and%20Guang%20Cai%20and%20Chengyang%20Li%20and%20Handi%20Yin%20and%20Yuyang%20Wang%20and%20Ming%20Liu%20and%20Pan%20Hui%0AAbstract%3A%20%20%20A%20Colored%20point%20cloud%2C%20as%20a%20simple%20and%20efficient%203D%20representation%2C%20has%20many%0Aadvantages%20in%20various%20fields%2C%20including%20robotic%20navigation%20and%20scene%0Areconstruction.%20This%20representation%20is%20now%20commonly%20used%20in%203D%20reconstruction%0Atasks%20relying%20on%20cameras%20and%20LiDARs.%20However%2C%20fusing%20data%20from%20these%20two%20types%0Aof%20sensors%20is%20poorly%20performed%20in%20many%20existing%20frameworks%2C%20leading%20to%0Aunsatisfactory%20mapping%20results%2C%20mainly%20due%20to%20inaccurate%20camera%20poses.%20This%0Apaper%20presents%20OmniColor%2C%20a%20novel%20and%20efficient%20algorithm%20to%20colorize%20point%0Aclouds%20using%20an%20independent%20360-degree%20camera.%20Given%20a%20LiDAR-based%20point%20cloud%0Aand%20a%20sequence%20of%20panorama%20images%20with%20initial%20coarse%20camera%20poses%2C%20our%0Aobjective%20is%20to%20jointly%20optimize%20the%20poses%20of%20all%20frames%20for%20mapping%20images%0Aonto%20geometric%20reconstructions.%20Our%20pipeline%20works%20in%20an%20off-the-shelf%20manner%0Athat%20does%20not%20require%20any%20feature%20extraction%20or%20matching%20process.%20Instead%2C%20we%0Afind%20optimal%20poses%20by%20directly%20maximizing%20the%20photometric%20consistency%20of%20LiDAR%0Amaps.%20In%20experiments%2C%20we%20show%20that%20our%20method%20can%20overcome%20the%20severe%20visual%0Adistortion%20of%20omnidirectional%20images%20and%20greatly%20benefit%20from%20the%20wide%20field%20of%0Aview%20%28FOV%29%20of%20360-degree%20cameras%20to%20reconstruct%20various%20scenarios%20with%20accuracy%0Aand%20stability.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/liubonan123/OmniColor/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniColor%253A%2520A%2520Global%2520Camera%2520Pose%2520Optimization%2520Approach%2520of%2520LiDAR-360Camera%250A%2520%2520Fusion%2520for%2520Colorizing%2520Point%2520Clouds%26entry.906535625%3DBonan%2520Liu%2520and%2520Guoyang%2520Zhao%2520and%2520Jianhao%2520Jiao%2520and%2520Guang%2520Cai%2520and%2520Chengyang%2520Li%2520and%2520Handi%2520Yin%2520and%2520Yuyang%2520Wang%2520and%2520Ming%2520Liu%2520and%2520Pan%2520Hui%26entry.1292438233%3D%2520%2520A%2520Colored%2520point%2520cloud%252C%2520as%2520a%2520simple%2520and%2520efficient%25203D%2520representation%252C%2520has%2520many%250Aadvantages%2520in%2520various%2520fields%252C%2520including%2520robotic%2520navigation%2520and%2520scene%250Areconstruction.%2520This%2520representation%2520is%2520now%2520commonly%2520used%2520in%25203D%2520reconstruction%250Atasks%2520relying%2520on%2520cameras%2520and%2520LiDARs.%2520However%252C%2520fusing%2520data%2520from%2520these%2520two%2520types%250Aof%2520sensors%2520is%2520poorly%2520performed%2520in%2520many%2520existing%2520frameworks%252C%2520leading%2520to%250Aunsatisfactory%2520mapping%2520results%252C%2520mainly%2520due%2520to%2520inaccurate%2520camera%2520poses.%2520This%250Apaper%2520presents%2520OmniColor%252C%2520a%2520novel%2520and%2520efficient%2520algorithm%2520to%2520colorize%2520point%250Aclouds%2520using%2520an%2520independent%2520360-degree%2520camera.%2520Given%2520a%2520LiDAR-based%2520point%2520cloud%250Aand%2520a%2520sequence%2520of%2520panorama%2520images%2520with%2520initial%2520coarse%2520camera%2520poses%252C%2520our%250Aobjective%2520is%2520to%2520jointly%2520optimize%2520the%2520poses%2520of%2520all%2520frames%2520for%2520mapping%2520images%250Aonto%2520geometric%2520reconstructions.%2520Our%2520pipeline%2520works%2520in%2520an%2520off-the-shelf%2520manner%250Athat%2520does%2520not%2520require%2520any%2520feature%2520extraction%2520or%2520matching%2520process.%2520Instead%252C%2520we%250Afind%2520optimal%2520poses%2520by%2520directly%2520maximizing%2520the%2520photometric%2520consistency%2520of%2520LiDAR%250Amaps.%2520In%2520experiments%252C%2520we%2520show%2520that%2520our%2520method%2520can%2520overcome%2520the%2520severe%2520visual%250Adistortion%2520of%2520omnidirectional%2520images%2520and%2520greatly%2520benefit%2520from%2520the%2520wide%2520field%2520of%250Aview%2520%2528FOV%2529%2520of%2520360-degree%2520cameras%2520to%2520reconstruct%2520various%2520scenarios%2520with%2520accuracy%250Aand%2520stability.%2520The%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/liubonan123/OmniColor/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniColor%3A%20A%20Global%20Camera%20Pose%20Optimization%20Approach%20of%20LiDAR-360Camera%0A%20%20Fusion%20for%20Colorizing%20Point%20Clouds&entry.906535625=Bonan%20Liu%20and%20Guoyang%20Zhao%20and%20Jianhao%20Jiao%20and%20Guang%20Cai%20and%20Chengyang%20Li%20and%20Handi%20Yin%20and%20Yuyang%20Wang%20and%20Ming%20Liu%20and%20Pan%20Hui&entry.1292438233=%20%20A%20Colored%20point%20cloud%2C%20as%20a%20simple%20and%20efficient%203D%20representation%2C%20has%20many%0Aadvantages%20in%20various%20fields%2C%20including%20robotic%20navigation%20and%20scene%0Areconstruction.%20This%20representation%20is%20now%20commonly%20used%20in%203D%20reconstruction%0Atasks%20relying%20on%20cameras%20and%20LiDARs.%20However%2C%20fusing%20data%20from%20these%20two%20types%0Aof%20sensors%20is%20poorly%20performed%20in%20many%20existing%20frameworks%2C%20leading%20to%0Aunsatisfactory%20mapping%20results%2C%20mainly%20due%20to%20inaccurate%20camera%20poses.%20This%0Apaper%20presents%20OmniColor%2C%20a%20novel%20and%20efficient%20algorithm%20to%20colorize%20point%0Aclouds%20using%20an%20independent%20360-degree%20camera.%20Given%20a%20LiDAR-based%20point%20cloud%0Aand%20a%20sequence%20of%20panorama%20images%20with%20initial%20coarse%20camera%20poses%2C%20our%0Aobjective%20is%20to%20jointly%20optimize%20the%20poses%20of%20all%20frames%20for%20mapping%20images%0Aonto%20geometric%20reconstructions.%20Our%20pipeline%20works%20in%20an%20off-the-shelf%20manner%0Athat%20does%20not%20require%20any%20feature%20extraction%20or%20matching%20process.%20Instead%2C%20we%0Afind%20optimal%20poses%20by%20directly%20maximizing%20the%20photometric%20consistency%20of%20LiDAR%0Amaps.%20In%20experiments%2C%20we%20show%20that%20our%20method%20can%20overcome%20the%20severe%20visual%0Adistortion%20of%20omnidirectional%20images%20and%20greatly%20benefit%20from%20the%20wide%20field%20of%0Aview%20%28FOV%29%20of%20360-degree%20cameras%20to%20reconstruct%20various%20scenarios%20with%20accuracy%0Aand%20stability.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/liubonan123/OmniColor/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04693v2&entry.124074799=Read"},
{"title": "Spatial Hierarchy and Temporal Attention Guided Cross Masking for\n  Self-supervised Skeleton-based Action Recognition", "author": "Xinpeng Yin and Wenming Cao", "abstract": "  In self-supervised skeleton-based action recognition, the mask reconstruction\nparadigm is gaining interest in enhancing model refinement and robustness\nthrough effective masking. However, previous works primarily relied on a single\nmasking criterion, resulting in the model overfitting specific features and\noverlooking other effective information. In this paper, we introduce a\nhierarchy and attention guided cross-masking framework (HA-CM) that applies\nmasking to skeleton sequences from both spatial and temporal perspectives.\nSpecifically, in spatial graphs, we utilize hyperbolic space to maintain joint\ndistinctions and effectively preserve the hierarchical structure of\nhigh-dimensional skeletons, employing joint hierarchy as the masking criterion.\nIn temporal flows, we substitute traditional distance metrics with the global\nattention of joints for masking, addressing the convergence of distances in\nhigh-dimensional space and the lack of a global perspective. Additionally, we\nincorporate cross-contrast loss based on the cross-masking framework into the\nloss function to enhance the model's learning of instance-level features. HA-CM\nshows efficiency and universality on three public large-scale datasets, NTU-60,\nNTU-120, and PKU-MMD. The source code of our HA-CM is available at\nhttps://github.com/YinxPeng/HA-CM-main.\n", "link": "http://arxiv.org/abs/2409.17951v1", "date": "2024-09-26", "relevancy": 2.8739, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.617}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Hierarchy%20and%20Temporal%20Attention%20Guided%20Cross%20Masking%20for%0A%20%20Self-supervised%20Skeleton-based%20Action%20Recognition&body=Title%3A%20Spatial%20Hierarchy%20and%20Temporal%20Attention%20Guided%20Cross%20Masking%20for%0A%20%20Self-supervised%20Skeleton-based%20Action%20Recognition%0AAuthor%3A%20Xinpeng%20Yin%20and%20Wenming%20Cao%0AAbstract%3A%20%20%20In%20self-supervised%20skeleton-based%20action%20recognition%2C%20the%20mask%20reconstruction%0Aparadigm%20is%20gaining%20interest%20in%20enhancing%20model%20refinement%20and%20robustness%0Athrough%20effective%20masking.%20However%2C%20previous%20works%20primarily%20relied%20on%20a%20single%0Amasking%20criterion%2C%20resulting%20in%20the%20model%20overfitting%20specific%20features%20and%0Aoverlooking%20other%20effective%20information.%20In%20this%20paper%2C%20we%20introduce%20a%0Ahierarchy%20and%20attention%20guided%20cross-masking%20framework%20%28HA-CM%29%20that%20applies%0Amasking%20to%20skeleton%20sequences%20from%20both%20spatial%20and%20temporal%20perspectives.%0ASpecifically%2C%20in%20spatial%20graphs%2C%20we%20utilize%20hyperbolic%20space%20to%20maintain%20joint%0Adistinctions%20and%20effectively%20preserve%20the%20hierarchical%20structure%20of%0Ahigh-dimensional%20skeletons%2C%20employing%20joint%20hierarchy%20as%20the%20masking%20criterion.%0AIn%20temporal%20flows%2C%20we%20substitute%20traditional%20distance%20metrics%20with%20the%20global%0Aattention%20of%20joints%20for%20masking%2C%20addressing%20the%20convergence%20of%20distances%20in%0Ahigh-dimensional%20space%20and%20the%20lack%20of%20a%20global%20perspective.%20Additionally%2C%20we%0Aincorporate%20cross-contrast%20loss%20based%20on%20the%20cross-masking%20framework%20into%20the%0Aloss%20function%20to%20enhance%20the%20model%27s%20learning%20of%20instance-level%20features.%20HA-CM%0Ashows%20efficiency%20and%20universality%20on%20three%20public%20large-scale%20datasets%2C%20NTU-60%2C%0ANTU-120%2C%20and%20PKU-MMD.%20The%20source%20code%20of%20our%20HA-CM%20is%20available%20at%0Ahttps%3A//github.com/YinxPeng/HA-CM-main.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Hierarchy%2520and%2520Temporal%2520Attention%2520Guided%2520Cross%2520Masking%2520for%250A%2520%2520Self-supervised%2520Skeleton-based%2520Action%2520Recognition%26entry.906535625%3DXinpeng%2520Yin%2520and%2520Wenming%2520Cao%26entry.1292438233%3D%2520%2520In%2520self-supervised%2520skeleton-based%2520action%2520recognition%252C%2520the%2520mask%2520reconstruction%250Aparadigm%2520is%2520gaining%2520interest%2520in%2520enhancing%2520model%2520refinement%2520and%2520robustness%250Athrough%2520effective%2520masking.%2520However%252C%2520previous%2520works%2520primarily%2520relied%2520on%2520a%2520single%250Amasking%2520criterion%252C%2520resulting%2520in%2520the%2520model%2520overfitting%2520specific%2520features%2520and%250Aoverlooking%2520other%2520effective%2520information.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Ahierarchy%2520and%2520attention%2520guided%2520cross-masking%2520framework%2520%2528HA-CM%2529%2520that%2520applies%250Amasking%2520to%2520skeleton%2520sequences%2520from%2520both%2520spatial%2520and%2520temporal%2520perspectives.%250ASpecifically%252C%2520in%2520spatial%2520graphs%252C%2520we%2520utilize%2520hyperbolic%2520space%2520to%2520maintain%2520joint%250Adistinctions%2520and%2520effectively%2520preserve%2520the%2520hierarchical%2520structure%2520of%250Ahigh-dimensional%2520skeletons%252C%2520employing%2520joint%2520hierarchy%2520as%2520the%2520masking%2520criterion.%250AIn%2520temporal%2520flows%252C%2520we%2520substitute%2520traditional%2520distance%2520metrics%2520with%2520the%2520global%250Aattention%2520of%2520joints%2520for%2520masking%252C%2520addressing%2520the%2520convergence%2520of%2520distances%2520in%250Ahigh-dimensional%2520space%2520and%2520the%2520lack%2520of%2520a%2520global%2520perspective.%2520Additionally%252C%2520we%250Aincorporate%2520cross-contrast%2520loss%2520based%2520on%2520the%2520cross-masking%2520framework%2520into%2520the%250Aloss%2520function%2520to%2520enhance%2520the%2520model%2527s%2520learning%2520of%2520instance-level%2520features.%2520HA-CM%250Ashows%2520efficiency%2520and%2520universality%2520on%2520three%2520public%2520large-scale%2520datasets%252C%2520NTU-60%252C%250ANTU-120%252C%2520and%2520PKU-MMD.%2520The%2520source%2520code%2520of%2520our%2520HA-CM%2520is%2520available%2520at%250Ahttps%253A//github.com/YinxPeng/HA-CM-main.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Hierarchy%20and%20Temporal%20Attention%20Guided%20Cross%20Masking%20for%0A%20%20Self-supervised%20Skeleton-based%20Action%20Recognition&entry.906535625=Xinpeng%20Yin%20and%20Wenming%20Cao&entry.1292438233=%20%20In%20self-supervised%20skeleton-based%20action%20recognition%2C%20the%20mask%20reconstruction%0Aparadigm%20is%20gaining%20interest%20in%20enhancing%20model%20refinement%20and%20robustness%0Athrough%20effective%20masking.%20However%2C%20previous%20works%20primarily%20relied%20on%20a%20single%0Amasking%20criterion%2C%20resulting%20in%20the%20model%20overfitting%20specific%20features%20and%0Aoverlooking%20other%20effective%20information.%20In%20this%20paper%2C%20we%20introduce%20a%0Ahierarchy%20and%20attention%20guided%20cross-masking%20framework%20%28HA-CM%29%20that%20applies%0Amasking%20to%20skeleton%20sequences%20from%20both%20spatial%20and%20temporal%20perspectives.%0ASpecifically%2C%20in%20spatial%20graphs%2C%20we%20utilize%20hyperbolic%20space%20to%20maintain%20joint%0Adistinctions%20and%20effectively%20preserve%20the%20hierarchical%20structure%20of%0Ahigh-dimensional%20skeletons%2C%20employing%20joint%20hierarchy%20as%20the%20masking%20criterion.%0AIn%20temporal%20flows%2C%20we%20substitute%20traditional%20distance%20metrics%20with%20the%20global%0Aattention%20of%20joints%20for%20masking%2C%20addressing%20the%20convergence%20of%20distances%20in%0Ahigh-dimensional%20space%20and%20the%20lack%20of%20a%20global%20perspective.%20Additionally%2C%20we%0Aincorporate%20cross-contrast%20loss%20based%20on%20the%20cross-masking%20framework%20into%20the%0Aloss%20function%20to%20enhance%20the%20model%27s%20learning%20of%20instance-level%20features.%20HA-CM%0Ashows%20efficiency%20and%20universality%20on%20three%20public%20large-scale%20datasets%2C%20NTU-60%2C%0ANTU-120%2C%20and%20PKU-MMD.%20The%20source%20code%20of%20our%20HA-CM%20is%20available%20at%0Ahttps%3A//github.com/YinxPeng/HA-CM-main.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17951v1&entry.124074799=Read"},
{"title": "Revisit Anything: Visual Place Recognition via Image Segment Retrieval", "author": "Kartik Garg and Sai Shubodh Puligilla and Shishir Kolathaya and Madhava Krishna and Sourav Garg", "abstract": "  Accurately recognizing a revisited place is crucial for embodied agents to\nlocalize and navigate. This requires visual representations to be distinct,\ndespite strong variations in camera viewpoint and scene appearance. Existing\nvisual place recognition pipelines encode the \"whole\" image and search for\nmatches. This poses a fundamental challenge in matching two images of the same\nplace captured from different camera viewpoints: \"the similarity of what\noverlaps can be dominated by the dissimilarity of what does not overlap\". We\naddress this by encoding and searching for \"image segments\" instead of the\nwhole images. We propose to use open-set image segmentation to decompose an\nimage into `meaningful' entities (i.e., things and stuff). This enables us to\ncreate a novel image representation as a collection of multiple overlapping\nsubgraphs connecting a segment with its neighboring segments, dubbed\nSuperSegment. Furthermore, to efficiently encode these SuperSegments into\ncompact vector representations, we propose a novel factorized representation of\nfeature aggregation. We show that retrieving these partial representations\nleads to significantly higher recognition recall than the typical whole image\nbased retrieval. Our segments-based approach, dubbed SegVLAD, sets a new\nstate-of-the-art in place recognition on a diverse selection of benchmark\ndatasets, while being applicable to both generic and task-specialized image\nencoders. Finally, we demonstrate the potential of our method to ``revisit\nanything'' by evaluating our method on an object instance retrieval task, which\nbridges the two disparate areas of research: visual place recognition and\nobject-goal navigation, through their common aim of recognizing goal objects\nspecific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything.\n", "link": "http://arxiv.org/abs/2409.18049v1", "date": "2024-09-26", "relevancy": 2.8692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisit%20Anything%3A%20Visual%20Place%20Recognition%20via%20Image%20Segment%20Retrieval&body=Title%3A%20Revisit%20Anything%3A%20Visual%20Place%20Recognition%20via%20Image%20Segment%20Retrieval%0AAuthor%3A%20Kartik%20Garg%20and%20Sai%20Shubodh%20Puligilla%20and%20Shishir%20Kolathaya%20and%20Madhava%20Krishna%20and%20Sourav%20Garg%0AAbstract%3A%20%20%20Accurately%20recognizing%20a%20revisited%20place%20is%20crucial%20for%20embodied%20agents%20to%0Alocalize%20and%20navigate.%20This%20requires%20visual%20representations%20to%20be%20distinct%2C%0Adespite%20strong%20variations%20in%20camera%20viewpoint%20and%20scene%20appearance.%20Existing%0Avisual%20place%20recognition%20pipelines%20encode%20the%20%22whole%22%20image%20and%20search%20for%0Amatches.%20This%20poses%20a%20fundamental%20challenge%20in%20matching%20two%20images%20of%20the%20same%0Aplace%20captured%20from%20different%20camera%20viewpoints%3A%20%22the%20similarity%20of%20what%0Aoverlaps%20can%20be%20dominated%20by%20the%20dissimilarity%20of%20what%20does%20not%20overlap%22.%20We%0Aaddress%20this%20by%20encoding%20and%20searching%20for%20%22image%20segments%22%20instead%20of%20the%0Awhole%20images.%20We%20propose%20to%20use%20open-set%20image%20segmentation%20to%20decompose%20an%0Aimage%20into%20%60meaningful%27%20entities%20%28i.e.%2C%20things%20and%20stuff%29.%20This%20enables%20us%20to%0Acreate%20a%20novel%20image%20representation%20as%20a%20collection%20of%20multiple%20overlapping%0Asubgraphs%20connecting%20a%20segment%20with%20its%20neighboring%20segments%2C%20dubbed%0ASuperSegment.%20Furthermore%2C%20to%20efficiently%20encode%20these%20SuperSegments%20into%0Acompact%20vector%20representations%2C%20we%20propose%20a%20novel%20factorized%20representation%20of%0Afeature%20aggregation.%20We%20show%20that%20retrieving%20these%20partial%20representations%0Aleads%20to%20significantly%20higher%20recognition%20recall%20than%20the%20typical%20whole%20image%0Abased%20retrieval.%20Our%20segments-based%20approach%2C%20dubbed%20SegVLAD%2C%20sets%20a%20new%0Astate-of-the-art%20in%20place%20recognition%20on%20a%20diverse%20selection%20of%20benchmark%0Adatasets%2C%20while%20being%20applicable%20to%20both%20generic%20and%20task-specialized%20image%0Aencoders.%20Finally%2C%20we%20demonstrate%20the%20potential%20of%20our%20method%20to%20%60%60revisit%0Aanything%27%27%20by%20evaluating%20our%20method%20on%20an%20object%20instance%20retrieval%20task%2C%20which%0Abridges%20the%20two%20disparate%20areas%20of%20research%3A%20visual%20place%20recognition%20and%0Aobject-goal%20navigation%2C%20through%20their%20common%20aim%20of%20recognizing%20goal%20objects%0Aspecific%20to%20a%20place.%20Source%20code%3A%20https%3A//github.com/AnyLoc/Revisit-Anything.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisit%2520Anything%253A%2520Visual%2520Place%2520Recognition%2520via%2520Image%2520Segment%2520Retrieval%26entry.906535625%3DKartik%2520Garg%2520and%2520Sai%2520Shubodh%2520Puligilla%2520and%2520Shishir%2520Kolathaya%2520and%2520Madhava%2520Krishna%2520and%2520Sourav%2520Garg%26entry.1292438233%3D%2520%2520Accurately%2520recognizing%2520a%2520revisited%2520place%2520is%2520crucial%2520for%2520embodied%2520agents%2520to%250Alocalize%2520and%2520navigate.%2520This%2520requires%2520visual%2520representations%2520to%2520be%2520distinct%252C%250Adespite%2520strong%2520variations%2520in%2520camera%2520viewpoint%2520and%2520scene%2520appearance.%2520Existing%250Avisual%2520place%2520recognition%2520pipelines%2520encode%2520the%2520%2522whole%2522%2520image%2520and%2520search%2520for%250Amatches.%2520This%2520poses%2520a%2520fundamental%2520challenge%2520in%2520matching%2520two%2520images%2520of%2520the%2520same%250Aplace%2520captured%2520from%2520different%2520camera%2520viewpoints%253A%2520%2522the%2520similarity%2520of%2520what%250Aoverlaps%2520can%2520be%2520dominated%2520by%2520the%2520dissimilarity%2520of%2520what%2520does%2520not%2520overlap%2522.%2520We%250Aaddress%2520this%2520by%2520encoding%2520and%2520searching%2520for%2520%2522image%2520segments%2522%2520instead%2520of%2520the%250Awhole%2520images.%2520We%2520propose%2520to%2520use%2520open-set%2520image%2520segmentation%2520to%2520decompose%2520an%250Aimage%2520into%2520%2560meaningful%2527%2520entities%2520%2528i.e.%252C%2520things%2520and%2520stuff%2529.%2520This%2520enables%2520us%2520to%250Acreate%2520a%2520novel%2520image%2520representation%2520as%2520a%2520collection%2520of%2520multiple%2520overlapping%250Asubgraphs%2520connecting%2520a%2520segment%2520with%2520its%2520neighboring%2520segments%252C%2520dubbed%250ASuperSegment.%2520Furthermore%252C%2520to%2520efficiently%2520encode%2520these%2520SuperSegments%2520into%250Acompact%2520vector%2520representations%252C%2520we%2520propose%2520a%2520novel%2520factorized%2520representation%2520of%250Afeature%2520aggregation.%2520We%2520show%2520that%2520retrieving%2520these%2520partial%2520representations%250Aleads%2520to%2520significantly%2520higher%2520recognition%2520recall%2520than%2520the%2520typical%2520whole%2520image%250Abased%2520retrieval.%2520Our%2520segments-based%2520approach%252C%2520dubbed%2520SegVLAD%252C%2520sets%2520a%2520new%250Astate-of-the-art%2520in%2520place%2520recognition%2520on%2520a%2520diverse%2520selection%2520of%2520benchmark%250Adatasets%252C%2520while%2520being%2520applicable%2520to%2520both%2520generic%2520and%2520task-specialized%2520image%250Aencoders.%2520Finally%252C%2520we%2520demonstrate%2520the%2520potential%2520of%2520our%2520method%2520to%2520%2560%2560revisit%250Aanything%2527%2527%2520by%2520evaluating%2520our%2520method%2520on%2520an%2520object%2520instance%2520retrieval%2520task%252C%2520which%250Abridges%2520the%2520two%2520disparate%2520areas%2520of%2520research%253A%2520visual%2520place%2520recognition%2520and%250Aobject-goal%2520navigation%252C%2520through%2520their%2520common%2520aim%2520of%2520recognizing%2520goal%2520objects%250Aspecific%2520to%2520a%2520place.%2520Source%2520code%253A%2520https%253A//github.com/AnyLoc/Revisit-Anything.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisit%20Anything%3A%20Visual%20Place%20Recognition%20via%20Image%20Segment%20Retrieval&entry.906535625=Kartik%20Garg%20and%20Sai%20Shubodh%20Puligilla%20and%20Shishir%20Kolathaya%20and%20Madhava%20Krishna%20and%20Sourav%20Garg&entry.1292438233=%20%20Accurately%20recognizing%20a%20revisited%20place%20is%20crucial%20for%20embodied%20agents%20to%0Alocalize%20and%20navigate.%20This%20requires%20visual%20representations%20to%20be%20distinct%2C%0Adespite%20strong%20variations%20in%20camera%20viewpoint%20and%20scene%20appearance.%20Existing%0Avisual%20place%20recognition%20pipelines%20encode%20the%20%22whole%22%20image%20and%20search%20for%0Amatches.%20This%20poses%20a%20fundamental%20challenge%20in%20matching%20two%20images%20of%20the%20same%0Aplace%20captured%20from%20different%20camera%20viewpoints%3A%20%22the%20similarity%20of%20what%0Aoverlaps%20can%20be%20dominated%20by%20the%20dissimilarity%20of%20what%20does%20not%20overlap%22.%20We%0Aaddress%20this%20by%20encoding%20and%20searching%20for%20%22image%20segments%22%20instead%20of%20the%0Awhole%20images.%20We%20propose%20to%20use%20open-set%20image%20segmentation%20to%20decompose%20an%0Aimage%20into%20%60meaningful%27%20entities%20%28i.e.%2C%20things%20and%20stuff%29.%20This%20enables%20us%20to%0Acreate%20a%20novel%20image%20representation%20as%20a%20collection%20of%20multiple%20overlapping%0Asubgraphs%20connecting%20a%20segment%20with%20its%20neighboring%20segments%2C%20dubbed%0ASuperSegment.%20Furthermore%2C%20to%20efficiently%20encode%20these%20SuperSegments%20into%0Acompact%20vector%20representations%2C%20we%20propose%20a%20novel%20factorized%20representation%20of%0Afeature%20aggregation.%20We%20show%20that%20retrieving%20these%20partial%20representations%0Aleads%20to%20significantly%20higher%20recognition%20recall%20than%20the%20typical%20whole%20image%0Abased%20retrieval.%20Our%20segments-based%20approach%2C%20dubbed%20SegVLAD%2C%20sets%20a%20new%0Astate-of-the-art%20in%20place%20recognition%20on%20a%20diverse%20selection%20of%20benchmark%0Adatasets%2C%20while%20being%20applicable%20to%20both%20generic%20and%20task-specialized%20image%0Aencoders.%20Finally%2C%20we%20demonstrate%20the%20potential%20of%20our%20method%20to%20%60%60revisit%0Aanything%27%27%20by%20evaluating%20our%20method%20on%20an%20object%20instance%20retrieval%20task%2C%20which%0Abridges%20the%20two%20disparate%20areas%20of%20research%3A%20visual%20place%20recognition%20and%0Aobject-goal%20navigation%2C%20through%20their%20common%20aim%20of%20recognizing%20goal%20objects%0Aspecific%20to%20a%20place.%20Source%20code%3A%20https%3A//github.com/AnyLoc/Revisit-Anything.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18049v1&entry.124074799=Read"},
{"title": "EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation", "author": "Jiaxiang Tang and Zhaoshuo Li and Zekun Hao and Xian Liu and Gang Zeng and Ming-Yu Liu and Qinsheng Zhang", "abstract": "  Current auto-regressive mesh generation methods suffer from issues such as\nincompleteness, insufficient detail, and poor generalization. In this paper, we\npropose an Auto-regressive Auto-encoder (ArAE) model capable of generating\nhigh-quality 3D meshes with up to 4,000 faces at a spatial resolution of\n$512^3$. We introduce a novel mesh tokenization algorithm that efficiently\ncompresses triangular meshes into 1D token sequences, significantly enhancing\ntraining efficiency. Furthermore, our model compresses variable-length\ntriangular meshes into a fixed-length latent space, enabling training latent\ndiffusion models for better generalization. Extensive experiments demonstrate\nthe superior quality, diversity, and generalization capabilities of our model\nin both point cloud and image-conditioned mesh generation tasks.\n", "link": "http://arxiv.org/abs/2409.18114v1", "date": "2024-09-26", "relevancy": 2.8488, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.597}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5777}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeRunner%3A%20Auto-regressive%20Auto-encoder%20for%20Artistic%20Mesh%20Generation&body=Title%3A%20EdgeRunner%3A%20Auto-regressive%20Auto-encoder%20for%20Artistic%20Mesh%20Generation%0AAuthor%3A%20Jiaxiang%20Tang%20and%20Zhaoshuo%20Li%20and%20Zekun%20Hao%20and%20Xian%20Liu%20and%20Gang%20Zeng%20and%20Ming-Yu%20Liu%20and%20Qinsheng%20Zhang%0AAbstract%3A%20%20%20Current%20auto-regressive%20mesh%20generation%20methods%20suffer%20from%20issues%20such%20as%0Aincompleteness%2C%20insufficient%20detail%2C%20and%20poor%20generalization.%20In%20this%20paper%2C%20we%0Apropose%20an%20Auto-regressive%20Auto-encoder%20%28ArAE%29%20model%20capable%20of%20generating%0Ahigh-quality%203D%20meshes%20with%20up%20to%204%2C000%20faces%20at%20a%20spatial%20resolution%20of%0A%24512%5E3%24.%20We%20introduce%20a%20novel%20mesh%20tokenization%20algorithm%20that%20efficiently%0Acompresses%20triangular%20meshes%20into%201D%20token%20sequences%2C%20significantly%20enhancing%0Atraining%20efficiency.%20Furthermore%2C%20our%20model%20compresses%20variable-length%0Atriangular%20meshes%20into%20a%20fixed-length%20latent%20space%2C%20enabling%20training%20latent%0Adiffusion%20models%20for%20better%20generalization.%20Extensive%20experiments%20demonstrate%0Athe%20superior%20quality%2C%20diversity%2C%20and%20generalization%20capabilities%20of%20our%20model%0Ain%20both%20point%20cloud%20and%20image-conditioned%20mesh%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeRunner%253A%2520Auto-regressive%2520Auto-encoder%2520for%2520Artistic%2520Mesh%2520Generation%26entry.906535625%3DJiaxiang%2520Tang%2520and%2520Zhaoshuo%2520Li%2520and%2520Zekun%2520Hao%2520and%2520Xian%2520Liu%2520and%2520Gang%2520Zeng%2520and%2520Ming-Yu%2520Liu%2520and%2520Qinsheng%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520auto-regressive%2520mesh%2520generation%2520methods%2520suffer%2520from%2520issues%2520such%2520as%250Aincompleteness%252C%2520insufficient%2520detail%252C%2520and%2520poor%2520generalization.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520Auto-regressive%2520Auto-encoder%2520%2528ArAE%2529%2520model%2520capable%2520of%2520generating%250Ahigh-quality%25203D%2520meshes%2520with%2520up%2520to%25204%252C000%2520faces%2520at%2520a%2520spatial%2520resolution%2520of%250A%2524512%255E3%2524.%2520We%2520introduce%2520a%2520novel%2520mesh%2520tokenization%2520algorithm%2520that%2520efficiently%250Acompresses%2520triangular%2520meshes%2520into%25201D%2520token%2520sequences%252C%2520significantly%2520enhancing%250Atraining%2520efficiency.%2520Furthermore%252C%2520our%2520model%2520compresses%2520variable-length%250Atriangular%2520meshes%2520into%2520a%2520fixed-length%2520latent%2520space%252C%2520enabling%2520training%2520latent%250Adiffusion%2520models%2520for%2520better%2520generalization.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520superior%2520quality%252C%2520diversity%252C%2520and%2520generalization%2520capabilities%2520of%2520our%2520model%250Ain%2520both%2520point%2520cloud%2520and%2520image-conditioned%2520mesh%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeRunner%3A%20Auto-regressive%20Auto-encoder%20for%20Artistic%20Mesh%20Generation&entry.906535625=Jiaxiang%20Tang%20and%20Zhaoshuo%20Li%20and%20Zekun%20Hao%20and%20Xian%20Liu%20and%20Gang%20Zeng%20and%20Ming-Yu%20Liu%20and%20Qinsheng%20Zhang&entry.1292438233=%20%20Current%20auto-regressive%20mesh%20generation%20methods%20suffer%20from%20issues%20such%20as%0Aincompleteness%2C%20insufficient%20detail%2C%20and%20poor%20generalization.%20In%20this%20paper%2C%20we%0Apropose%20an%20Auto-regressive%20Auto-encoder%20%28ArAE%29%20model%20capable%20of%20generating%0Ahigh-quality%203D%20meshes%20with%20up%20to%204%2C000%20faces%20at%20a%20spatial%20resolution%20of%0A%24512%5E3%24.%20We%20introduce%20a%20novel%20mesh%20tokenization%20algorithm%20that%20efficiently%0Acompresses%20triangular%20meshes%20into%201D%20token%20sequences%2C%20significantly%20enhancing%0Atraining%20efficiency.%20Furthermore%2C%20our%20model%20compresses%20variable-length%0Atriangular%20meshes%20into%20a%20fixed-length%20latent%20space%2C%20enabling%20training%20latent%0Adiffusion%20models%20for%20better%20generalization.%20Extensive%20experiments%20demonstrate%0Athe%20superior%20quality%2C%20diversity%2C%20and%20generalization%20capabilities%20of%20our%20model%0Ain%20both%20point%20cloud%20and%20image-conditioned%20mesh%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18114v1&entry.124074799=Read"},
{"title": "Neural Light Spheres for Implicit Image Stitching and View Synthesis", "author": "Ilya Chugunov and Amogh Joshi and Kiran Murthy and Francois Bleibel and Felix Heide", "abstract": "  Challenging to capture, and challenging to display on a cellphone screen, the\npanorama paradoxically remains both a staple and underused feature of modern\nmobile camera applications. In this work we address both of these challenges\nwith a spherical neural light field model for implicit panoramic image\nstitching and re-rendering; able to accommodate for depth parallax,\nview-dependent lighting, and local scene motion and color changes during\ncapture. Fit during test-time to an arbitrary path panoramic video capture --\nvertical, horizontal, random-walk -- these neural light spheres jointly\nestimate the camera path and a high-resolution scene reconstruction to produce\nnovel wide field-of-view projections of the environment. Our single-layer model\navoids expensive volumetric sampling, and decomposes the scene into compact\nview-dependent ray offset and color components, with a total model size of 80\nMB per scene, and real-time (50 FPS) rendering at 1080p resolution. We\ndemonstrate improved reconstruction quality over traditional image stitching\nand radiance field methods, with significantly higher tolerance to scene motion\nand non-ideal capture settings.\n", "link": "http://arxiv.org/abs/2409.17924v1", "date": "2024-09-26", "relevancy": 2.8388, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5743}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5645}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Light%20Spheres%20for%20Implicit%20Image%20Stitching%20and%20View%20Synthesis&body=Title%3A%20Neural%20Light%20Spheres%20for%20Implicit%20Image%20Stitching%20and%20View%20Synthesis%0AAuthor%3A%20Ilya%20Chugunov%20and%20Amogh%20Joshi%20and%20Kiran%20Murthy%20and%20Francois%20Bleibel%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Challenging%20to%20capture%2C%20and%20challenging%20to%20display%20on%20a%20cellphone%20screen%2C%20the%0Apanorama%20paradoxically%20remains%20both%20a%20staple%20and%20underused%20feature%20of%20modern%0Amobile%20camera%20applications.%20In%20this%20work%20we%20address%20both%20of%20these%20challenges%0Awith%20a%20spherical%20neural%20light%20field%20model%20for%20implicit%20panoramic%20image%0Astitching%20and%20re-rendering%3B%20able%20to%20accommodate%20for%20depth%20parallax%2C%0Aview-dependent%20lighting%2C%20and%20local%20scene%20motion%20and%20color%20changes%20during%0Acapture.%20Fit%20during%20test-time%20to%20an%20arbitrary%20path%20panoramic%20video%20capture%20--%0Avertical%2C%20horizontal%2C%20random-walk%20--%20these%20neural%20light%20spheres%20jointly%0Aestimate%20the%20camera%20path%20and%20a%20high-resolution%20scene%20reconstruction%20to%20produce%0Anovel%20wide%20field-of-view%20projections%20of%20the%20environment.%20Our%20single-layer%20model%0Aavoids%20expensive%20volumetric%20sampling%2C%20and%20decomposes%20the%20scene%20into%20compact%0Aview-dependent%20ray%20offset%20and%20color%20components%2C%20with%20a%20total%20model%20size%20of%2080%0AMB%20per%20scene%2C%20and%20real-time%20%2850%20FPS%29%20rendering%20at%201080p%20resolution.%20We%0Ademonstrate%20improved%20reconstruction%20quality%20over%20traditional%20image%20stitching%0Aand%20radiance%20field%20methods%2C%20with%20significantly%20higher%20tolerance%20to%20scene%20motion%0Aand%20non-ideal%20capture%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Light%2520Spheres%2520for%2520Implicit%2520Image%2520Stitching%2520and%2520View%2520Synthesis%26entry.906535625%3DIlya%2520Chugunov%2520and%2520Amogh%2520Joshi%2520and%2520Kiran%2520Murthy%2520and%2520Francois%2520Bleibel%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Challenging%2520to%2520capture%252C%2520and%2520challenging%2520to%2520display%2520on%2520a%2520cellphone%2520screen%252C%2520the%250Apanorama%2520paradoxically%2520remains%2520both%2520a%2520staple%2520and%2520underused%2520feature%2520of%2520modern%250Amobile%2520camera%2520applications.%2520In%2520this%2520work%2520we%2520address%2520both%2520of%2520these%2520challenges%250Awith%2520a%2520spherical%2520neural%2520light%2520field%2520model%2520for%2520implicit%2520panoramic%2520image%250Astitching%2520and%2520re-rendering%253B%2520able%2520to%2520accommodate%2520for%2520depth%2520parallax%252C%250Aview-dependent%2520lighting%252C%2520and%2520local%2520scene%2520motion%2520and%2520color%2520changes%2520during%250Acapture.%2520Fit%2520during%2520test-time%2520to%2520an%2520arbitrary%2520path%2520panoramic%2520video%2520capture%2520--%250Avertical%252C%2520horizontal%252C%2520random-walk%2520--%2520these%2520neural%2520light%2520spheres%2520jointly%250Aestimate%2520the%2520camera%2520path%2520and%2520a%2520high-resolution%2520scene%2520reconstruction%2520to%2520produce%250Anovel%2520wide%2520field-of-view%2520projections%2520of%2520the%2520environment.%2520Our%2520single-layer%2520model%250Aavoids%2520expensive%2520volumetric%2520sampling%252C%2520and%2520decomposes%2520the%2520scene%2520into%2520compact%250Aview-dependent%2520ray%2520offset%2520and%2520color%2520components%252C%2520with%2520a%2520total%2520model%2520size%2520of%252080%250AMB%2520per%2520scene%252C%2520and%2520real-time%2520%252850%2520FPS%2529%2520rendering%2520at%25201080p%2520resolution.%2520We%250Ademonstrate%2520improved%2520reconstruction%2520quality%2520over%2520traditional%2520image%2520stitching%250Aand%2520radiance%2520field%2520methods%252C%2520with%2520significantly%2520higher%2520tolerance%2520to%2520scene%2520motion%250Aand%2520non-ideal%2520capture%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Light%20Spheres%20for%20Implicit%20Image%20Stitching%20and%20View%20Synthesis&entry.906535625=Ilya%20Chugunov%20and%20Amogh%20Joshi%20and%20Kiran%20Murthy%20and%20Francois%20Bleibel%20and%20Felix%20Heide&entry.1292438233=%20%20Challenging%20to%20capture%2C%20and%20challenging%20to%20display%20on%20a%20cellphone%20screen%2C%20the%0Apanorama%20paradoxically%20remains%20both%20a%20staple%20and%20underused%20feature%20of%20modern%0Amobile%20camera%20applications.%20In%20this%20work%20we%20address%20both%20of%20these%20challenges%0Awith%20a%20spherical%20neural%20light%20field%20model%20for%20implicit%20panoramic%20image%0Astitching%20and%20re-rendering%3B%20able%20to%20accommodate%20for%20depth%20parallax%2C%0Aview-dependent%20lighting%2C%20and%20local%20scene%20motion%20and%20color%20changes%20during%0Acapture.%20Fit%20during%20test-time%20to%20an%20arbitrary%20path%20panoramic%20video%20capture%20--%0Avertical%2C%20horizontal%2C%20random-walk%20--%20these%20neural%20light%20spheres%20jointly%0Aestimate%20the%20camera%20path%20and%20a%20high-resolution%20scene%20reconstruction%20to%20produce%0Anovel%20wide%20field-of-view%20projections%20of%20the%20environment.%20Our%20single-layer%20model%0Aavoids%20expensive%20volumetric%20sampling%2C%20and%20decomposes%20the%20scene%20into%20compact%0Aview-dependent%20ray%20offset%20and%20color%20components%2C%20with%20a%20total%20model%20size%20of%2080%0AMB%20per%20scene%2C%20and%20real-time%20%2850%20FPS%29%20rendering%20at%201080p%20resolution.%20We%0Ademonstrate%20improved%20reconstruction%20quality%20over%20traditional%20image%20stitching%0Aand%20radiance%20field%20methods%2C%20with%20significantly%20higher%20tolerance%20to%20scene%20motion%0Aand%20non-ideal%20capture%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17924v1&entry.124074799=Read"},
{"title": "Exploring Event-based Human Pose Estimation with 3D Event\n  Representations", "author": "Xiaoting Yin and Hao Shi and Jiaan Chen and Ze Wang and Yaozu Ye and Kailun Yang and Kaiwei Wang", "abstract": "  Human pose estimation is a fundamental and appealing task in computer vision.\nAlthough traditional cameras are commonly applied, their reliability decreases\nin scenarios under high dynamic range or heavy motion blur, where event cameras\noffer a robust solution. Predominant event-based methods accumulate events into\nframes, ignoring the asynchronous and high temporal resolution that is crucial\nfor distinguishing distinct actions. To address this issue and to unlock the 3D\npotential of event information, we introduce two 3D event representations: the\nRasterized Event Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). The\nRasEPC aggregates events within concise temporal slices at identical positions,\npreserving their 3D attributes along with statistical information, thereby\nsignificantly reducing memory and computational demands. Meanwhile, the DEV\nrepresentation discretizes events into voxels and projects them across three\northogonal planes, utilizing decoupled event attention to retrieve 3D cues from\nthe 2D planes. Furthermore, we develop and release EV-3DPW, a synthetic\nevent-based dataset crafted to facilitate training and quantitative analysis in\noutdoor scenes. Our methods are tested on the DHP19 public dataset, MMHPSD\ndataset, and our EV-3DPW dataset, with further qualitative validation via a\nderived driving scene dataset EV-JAAD and an outdoor collection vehicle. Our\ncode and dataset have been made publicly available at\nhttps://github.com/MasterHow/EventPointPose.\n", "link": "http://arxiv.org/abs/2311.04591v4", "date": "2024-09-26", "relevancy": 2.834, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5735}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Event-based%20Human%20Pose%20Estimation%20with%203D%20Event%0A%20%20Representations&body=Title%3A%20Exploring%20Event-based%20Human%20Pose%20Estimation%20with%203D%20Event%0A%20%20Representations%0AAuthor%3A%20Xiaoting%20Yin%20and%20Hao%20Shi%20and%20Jiaan%20Chen%20and%20Ze%20Wang%20and%20Yaozu%20Ye%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang%0AAbstract%3A%20%20%20Human%20pose%20estimation%20is%20a%20fundamental%20and%20appealing%20task%20in%20computer%20vision.%0AAlthough%20traditional%20cameras%20are%20commonly%20applied%2C%20their%20reliability%20decreases%0Ain%20scenarios%20under%20high%20dynamic%20range%20or%20heavy%20motion%20blur%2C%20where%20event%20cameras%0Aoffer%20a%20robust%20solution.%20Predominant%20event-based%20methods%20accumulate%20events%20into%0Aframes%2C%20ignoring%20the%20asynchronous%20and%20high%20temporal%20resolution%20that%20is%20crucial%0Afor%20distinguishing%20distinct%20actions.%20To%20address%20this%20issue%20and%20to%20unlock%20the%203D%0Apotential%20of%20event%20information%2C%20we%20introduce%20two%203D%20event%20representations%3A%20the%0ARasterized%20Event%20Point%20Cloud%20%28RasEPC%29%20and%20the%20Decoupled%20Event%20Voxel%20%28DEV%29.%20The%0ARasEPC%20aggregates%20events%20within%20concise%20temporal%20slices%20at%20identical%20positions%2C%0Apreserving%20their%203D%20attributes%20along%20with%20statistical%20information%2C%20thereby%0Asignificantly%20reducing%20memory%20and%20computational%20demands.%20Meanwhile%2C%20the%20DEV%0Arepresentation%20discretizes%20events%20into%20voxels%20and%20projects%20them%20across%20three%0Aorthogonal%20planes%2C%20utilizing%20decoupled%20event%20attention%20to%20retrieve%203D%20cues%20from%0Athe%202D%20planes.%20Furthermore%2C%20we%20develop%20and%20release%20EV-3DPW%2C%20a%20synthetic%0Aevent-based%20dataset%20crafted%20to%20facilitate%20training%20and%20quantitative%20analysis%20in%0Aoutdoor%20scenes.%20Our%20methods%20are%20tested%20on%20the%20DHP19%20public%20dataset%2C%20MMHPSD%0Adataset%2C%20and%20our%20EV-3DPW%20dataset%2C%20with%20further%20qualitative%20validation%20via%20a%0Aderived%20driving%20scene%20dataset%20EV-JAAD%20and%20an%20outdoor%20collection%20vehicle.%20Our%0Acode%20and%20dataset%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//github.com/MasterHow/EventPointPose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04591v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Event-based%2520Human%2520Pose%2520Estimation%2520with%25203D%2520Event%250A%2520%2520Representations%26entry.906535625%3DXiaoting%2520Yin%2520and%2520Hao%2520Shi%2520and%2520Jiaan%2520Chen%2520and%2520Ze%2520Wang%2520and%2520Yaozu%2520Ye%2520and%2520Kailun%2520Yang%2520and%2520Kaiwei%2520Wang%26entry.1292438233%3D%2520%2520Human%2520pose%2520estimation%2520is%2520a%2520fundamental%2520and%2520appealing%2520task%2520in%2520computer%2520vision.%250AAlthough%2520traditional%2520cameras%2520are%2520commonly%2520applied%252C%2520their%2520reliability%2520decreases%250Ain%2520scenarios%2520under%2520high%2520dynamic%2520range%2520or%2520heavy%2520motion%2520blur%252C%2520where%2520event%2520cameras%250Aoffer%2520a%2520robust%2520solution.%2520Predominant%2520event-based%2520methods%2520accumulate%2520events%2520into%250Aframes%252C%2520ignoring%2520the%2520asynchronous%2520and%2520high%2520temporal%2520resolution%2520that%2520is%2520crucial%250Afor%2520distinguishing%2520distinct%2520actions.%2520To%2520address%2520this%2520issue%2520and%2520to%2520unlock%2520the%25203D%250Apotential%2520of%2520event%2520information%252C%2520we%2520introduce%2520two%25203D%2520event%2520representations%253A%2520the%250ARasterized%2520Event%2520Point%2520Cloud%2520%2528RasEPC%2529%2520and%2520the%2520Decoupled%2520Event%2520Voxel%2520%2528DEV%2529.%2520The%250ARasEPC%2520aggregates%2520events%2520within%2520concise%2520temporal%2520slices%2520at%2520identical%2520positions%252C%250Apreserving%2520their%25203D%2520attributes%2520along%2520with%2520statistical%2520information%252C%2520thereby%250Asignificantly%2520reducing%2520memory%2520and%2520computational%2520demands.%2520Meanwhile%252C%2520the%2520DEV%250Arepresentation%2520discretizes%2520events%2520into%2520voxels%2520and%2520projects%2520them%2520across%2520three%250Aorthogonal%2520planes%252C%2520utilizing%2520decoupled%2520event%2520attention%2520to%2520retrieve%25203D%2520cues%2520from%250Athe%25202D%2520planes.%2520Furthermore%252C%2520we%2520develop%2520and%2520release%2520EV-3DPW%252C%2520a%2520synthetic%250Aevent-based%2520dataset%2520crafted%2520to%2520facilitate%2520training%2520and%2520quantitative%2520analysis%2520in%250Aoutdoor%2520scenes.%2520Our%2520methods%2520are%2520tested%2520on%2520the%2520DHP19%2520public%2520dataset%252C%2520MMHPSD%250Adataset%252C%2520and%2520our%2520EV-3DPW%2520dataset%252C%2520with%2520further%2520qualitative%2520validation%2520via%2520a%250Aderived%2520driving%2520scene%2520dataset%2520EV-JAAD%2520and%2520an%2520outdoor%2520collection%2520vehicle.%2520Our%250Acode%2520and%2520dataset%2520have%2520been%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/MasterHow/EventPointPose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04591v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Event-based%20Human%20Pose%20Estimation%20with%203D%20Event%0A%20%20Representations&entry.906535625=Xiaoting%20Yin%20and%20Hao%20Shi%20and%20Jiaan%20Chen%20and%20Ze%20Wang%20and%20Yaozu%20Ye%20and%20Kailun%20Yang%20and%20Kaiwei%20Wang&entry.1292438233=%20%20Human%20pose%20estimation%20is%20a%20fundamental%20and%20appealing%20task%20in%20computer%20vision.%0AAlthough%20traditional%20cameras%20are%20commonly%20applied%2C%20their%20reliability%20decreases%0Ain%20scenarios%20under%20high%20dynamic%20range%20or%20heavy%20motion%20blur%2C%20where%20event%20cameras%0Aoffer%20a%20robust%20solution.%20Predominant%20event-based%20methods%20accumulate%20events%20into%0Aframes%2C%20ignoring%20the%20asynchronous%20and%20high%20temporal%20resolution%20that%20is%20crucial%0Afor%20distinguishing%20distinct%20actions.%20To%20address%20this%20issue%20and%20to%20unlock%20the%203D%0Apotential%20of%20event%20information%2C%20we%20introduce%20two%203D%20event%20representations%3A%20the%0ARasterized%20Event%20Point%20Cloud%20%28RasEPC%29%20and%20the%20Decoupled%20Event%20Voxel%20%28DEV%29.%20The%0ARasEPC%20aggregates%20events%20within%20concise%20temporal%20slices%20at%20identical%20positions%2C%0Apreserving%20their%203D%20attributes%20along%20with%20statistical%20information%2C%20thereby%0Asignificantly%20reducing%20memory%20and%20computational%20demands.%20Meanwhile%2C%20the%20DEV%0Arepresentation%20discretizes%20events%20into%20voxels%20and%20projects%20them%20across%20three%0Aorthogonal%20planes%2C%20utilizing%20decoupled%20event%20attention%20to%20retrieve%203D%20cues%20from%0Athe%202D%20planes.%20Furthermore%2C%20we%20develop%20and%20release%20EV-3DPW%2C%20a%20synthetic%0Aevent-based%20dataset%20crafted%20to%20facilitate%20training%20and%20quantitative%20analysis%20in%0Aoutdoor%20scenes.%20Our%20methods%20are%20tested%20on%20the%20DHP19%20public%20dataset%2C%20MMHPSD%0Adataset%2C%20and%20our%20EV-3DPW%20dataset%2C%20with%20further%20qualitative%20validation%20via%20a%0Aderived%20driving%20scene%20dataset%20EV-JAAD%20and%20an%20outdoor%20collection%20vehicle.%20Our%0Acode%20and%20dataset%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//github.com/MasterHow/EventPointPose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04591v4&entry.124074799=Read"},
{"title": "The Hard Positive Truth about Vision-Language Compositionality", "author": "Amita Kamath and Cheng-Yu Hsieh and Kai-Wei Chang and Ranjay Krishna", "abstract": "  Several benchmarks have concluded that our best vision-language models (e.g.,\nCLIP) are lacking in compositionality. Given an image, these benchmarks probe a\nmodel's ability to identify its associated caption amongst a set of\ncompositional distractors. In response, a surge of recent proposals show\nimprovements by finetuning CLIP with distractors as hard negatives. Our\ninvestigations reveal that these improvements have, in fact, been significantly\noverstated -- because existing benchmarks do not probe whether finetuned\nvision-language models remain invariant to hard positives. By curating an\nevaluation dataset with 112,382 hard negatives and hard positives, we uncover\nthat including hard positives decreases CLIP's performance by 12.9%, while\nhumans perform effortlessly at 99%. CLIP finetuned with hard negatives results\nin an even larger decrease, up to 38.7%. With this finding, we then produce a\n1,775,259 image-text training set with both hard negative and hard positive\ncaptions. By training with both, we see improvements on existing benchmarks\nwhile simultaneously improving performance on hard positives, indicating a more\nrobust improvement in compositionality. Our work suggests the need for future\nresearch to rigorously test and improve CLIP's understanding of semantic\nrelationships between related \"positive\" concepts.\n", "link": "http://arxiv.org/abs/2409.17958v1", "date": "2024-09-26", "relevancy": 2.8256, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Hard%20Positive%20Truth%20about%20Vision-Language%20Compositionality&body=Title%3A%20The%20Hard%20Positive%20Truth%20about%20Vision-Language%20Compositionality%0AAuthor%3A%20Amita%20Kamath%20and%20Cheng-Yu%20Hsieh%20and%20Kai-Wei%20Chang%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Several%20benchmarks%20have%20concluded%20that%20our%20best%20vision-language%20models%20%28e.g.%2C%0ACLIP%29%20are%20lacking%20in%20compositionality.%20Given%20an%20image%2C%20these%20benchmarks%20probe%20a%0Amodel%27s%20ability%20to%20identify%20its%20associated%20caption%20amongst%20a%20set%20of%0Acompositional%20distractors.%20In%20response%2C%20a%20surge%20of%20recent%20proposals%20show%0Aimprovements%20by%20finetuning%20CLIP%20with%20distractors%20as%20hard%20negatives.%20Our%0Ainvestigations%20reveal%20that%20these%20improvements%20have%2C%20in%20fact%2C%20been%20significantly%0Aoverstated%20--%20because%20existing%20benchmarks%20do%20not%20probe%20whether%20finetuned%0Avision-language%20models%20remain%20invariant%20to%20hard%20positives.%20By%20curating%20an%0Aevaluation%20dataset%20with%20112%2C382%20hard%20negatives%20and%20hard%20positives%2C%20we%20uncover%0Athat%20including%20hard%20positives%20decreases%20CLIP%27s%20performance%20by%2012.9%25%2C%20while%0Ahumans%20perform%20effortlessly%20at%2099%25.%20CLIP%20finetuned%20with%20hard%20negatives%20results%0Ain%20an%20even%20larger%20decrease%2C%20up%20to%2038.7%25.%20With%20this%20finding%2C%20we%20then%20produce%20a%0A1%2C775%2C259%20image-text%20training%20set%20with%20both%20hard%20negative%20and%20hard%20positive%0Acaptions.%20By%20training%20with%20both%2C%20we%20see%20improvements%20on%20existing%20benchmarks%0Awhile%20simultaneously%20improving%20performance%20on%20hard%20positives%2C%20indicating%20a%20more%0Arobust%20improvement%20in%20compositionality.%20Our%20work%20suggests%20the%20need%20for%20future%0Aresearch%20to%20rigorously%20test%20and%20improve%20CLIP%27s%20understanding%20of%20semantic%0Arelationships%20between%20related%20%22positive%22%20concepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Hard%2520Positive%2520Truth%2520about%2520Vision-Language%2520Compositionality%26entry.906535625%3DAmita%2520Kamath%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Kai-Wei%2520Chang%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Several%2520benchmarks%2520have%2520concluded%2520that%2520our%2520best%2520vision-language%2520models%2520%2528e.g.%252C%250ACLIP%2529%2520are%2520lacking%2520in%2520compositionality.%2520Given%2520an%2520image%252C%2520these%2520benchmarks%2520probe%2520a%250Amodel%2527s%2520ability%2520to%2520identify%2520its%2520associated%2520caption%2520amongst%2520a%2520set%2520of%250Acompositional%2520distractors.%2520In%2520response%252C%2520a%2520surge%2520of%2520recent%2520proposals%2520show%250Aimprovements%2520by%2520finetuning%2520CLIP%2520with%2520distractors%2520as%2520hard%2520negatives.%2520Our%250Ainvestigations%2520reveal%2520that%2520these%2520improvements%2520have%252C%2520in%2520fact%252C%2520been%2520significantly%250Aoverstated%2520--%2520because%2520existing%2520benchmarks%2520do%2520not%2520probe%2520whether%2520finetuned%250Avision-language%2520models%2520remain%2520invariant%2520to%2520hard%2520positives.%2520By%2520curating%2520an%250Aevaluation%2520dataset%2520with%2520112%252C382%2520hard%2520negatives%2520and%2520hard%2520positives%252C%2520we%2520uncover%250Athat%2520including%2520hard%2520positives%2520decreases%2520CLIP%2527s%2520performance%2520by%252012.9%2525%252C%2520while%250Ahumans%2520perform%2520effortlessly%2520at%252099%2525.%2520CLIP%2520finetuned%2520with%2520hard%2520negatives%2520results%250Ain%2520an%2520even%2520larger%2520decrease%252C%2520up%2520to%252038.7%2525.%2520With%2520this%2520finding%252C%2520we%2520then%2520produce%2520a%250A1%252C775%252C259%2520image-text%2520training%2520set%2520with%2520both%2520hard%2520negative%2520and%2520hard%2520positive%250Acaptions.%2520By%2520training%2520with%2520both%252C%2520we%2520see%2520improvements%2520on%2520existing%2520benchmarks%250Awhile%2520simultaneously%2520improving%2520performance%2520on%2520hard%2520positives%252C%2520indicating%2520a%2520more%250Arobust%2520improvement%2520in%2520compositionality.%2520Our%2520work%2520suggests%2520the%2520need%2520for%2520future%250Aresearch%2520to%2520rigorously%2520test%2520and%2520improve%2520CLIP%2527s%2520understanding%2520of%2520semantic%250Arelationships%2520between%2520related%2520%2522positive%2522%2520concepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Hard%20Positive%20Truth%20about%20Vision-Language%20Compositionality&entry.906535625=Amita%20Kamath%20and%20Cheng-Yu%20Hsieh%20and%20Kai-Wei%20Chang%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Several%20benchmarks%20have%20concluded%20that%20our%20best%20vision-language%20models%20%28e.g.%2C%0ACLIP%29%20are%20lacking%20in%20compositionality.%20Given%20an%20image%2C%20these%20benchmarks%20probe%20a%0Amodel%27s%20ability%20to%20identify%20its%20associated%20caption%20amongst%20a%20set%20of%0Acompositional%20distractors.%20In%20response%2C%20a%20surge%20of%20recent%20proposals%20show%0Aimprovements%20by%20finetuning%20CLIP%20with%20distractors%20as%20hard%20negatives.%20Our%0Ainvestigations%20reveal%20that%20these%20improvements%20have%2C%20in%20fact%2C%20been%20significantly%0Aoverstated%20--%20because%20existing%20benchmarks%20do%20not%20probe%20whether%20finetuned%0Avision-language%20models%20remain%20invariant%20to%20hard%20positives.%20By%20curating%20an%0Aevaluation%20dataset%20with%20112%2C382%20hard%20negatives%20and%20hard%20positives%2C%20we%20uncover%0Athat%20including%20hard%20positives%20decreases%20CLIP%27s%20performance%20by%2012.9%25%2C%20while%0Ahumans%20perform%20effortlessly%20at%2099%25.%20CLIP%20finetuned%20with%20hard%20negatives%20results%0Ain%20an%20even%20larger%20decrease%2C%20up%20to%2038.7%25.%20With%20this%20finding%2C%20we%20then%20produce%20a%0A1%2C775%2C259%20image-text%20training%20set%20with%20both%20hard%20negative%20and%20hard%20positive%0Acaptions.%20By%20training%20with%20both%2C%20we%20see%20improvements%20on%20existing%20benchmarks%0Awhile%20simultaneously%20improving%20performance%20on%20hard%20positives%2C%20indicating%20a%20more%0Arobust%20improvement%20in%20compositionality.%20Our%20work%20suggests%20the%20need%20for%20future%0Aresearch%20to%20rigorously%20test%20and%20improve%20CLIP%27s%20understanding%20of%20semantic%0Arelationships%20between%20related%20%22positive%22%20concepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17958v1&entry.124074799=Read"},
{"title": "InterNet: Unsupervised Cross-modal Homography Estimation Based on\n  Interleaved Modality Transfer and Self-supervised Homography Prediction", "author": "Junchen Yu and Si-Yuan Cao and Runmin Zhang and Chenghao Zhang and Jianxin Hu and Zhu Yu and Hui-liang Shen", "abstract": "  We propose a novel unsupervised cross-modal homography estimation framework,\nbased on interleaved modality transfer and self-supervised homography\nprediction, named InterNet. InterNet integrates modality transfer and\nself-supervised homography estimation, introducing an innovative interleaved\noptimization framework to alternately promote both components. The modality\ntransfer gradually narrows the modality gaps, facilitating the self-supervised\nhomography estimation to fully leverage the synthetic intra-modal data. The\nself-supervised homography estimation progressively achieves reliable\npredictions, thereby providing robust cross-modal supervision for the modality\ntransfer. To further boost the estimation accuracy, we also formulate a\nfine-grained homography feature loss to improve the connection between two\ncomponents. Furthermore, we employ a simple yet effective distillation training\ntechnique to reduce model parameters and improve cross-domain generalization\nability while maintaining comparable performance. Experiments reveal that\nInterNet achieves the state-of-the-art (SOTA) performance among unsupervised\nmethods, and even outperforms many supervised methods such as MHN and\nLocalTrans.\n", "link": "http://arxiv.org/abs/2409.17993v1", "date": "2024-09-26", "relevancy": 2.8033, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5958}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5466}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterNet%3A%20Unsupervised%20Cross-modal%20Homography%20Estimation%20Based%20on%0A%20%20Interleaved%20Modality%20Transfer%20and%20Self-supervised%20Homography%20Prediction&body=Title%3A%20InterNet%3A%20Unsupervised%20Cross-modal%20Homography%20Estimation%20Based%20on%0A%20%20Interleaved%20Modality%20Transfer%20and%20Self-supervised%20Homography%20Prediction%0AAuthor%3A%20Junchen%20Yu%20and%20Si-Yuan%20Cao%20and%20Runmin%20Zhang%20and%20Chenghao%20Zhang%20and%20Jianxin%20Hu%20and%20Zhu%20Yu%20and%20Hui-liang%20Shen%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20unsupervised%20cross-modal%20homography%20estimation%20framework%2C%0Abased%20on%20interleaved%20modality%20transfer%20and%20self-supervised%20homography%0Aprediction%2C%20named%20InterNet.%20InterNet%20integrates%20modality%20transfer%20and%0Aself-supervised%20homography%20estimation%2C%20introducing%20an%20innovative%20interleaved%0Aoptimization%20framework%20to%20alternately%20promote%20both%20components.%20The%20modality%0Atransfer%20gradually%20narrows%20the%20modality%20gaps%2C%20facilitating%20the%20self-supervised%0Ahomography%20estimation%20to%20fully%20leverage%20the%20synthetic%20intra-modal%20data.%20The%0Aself-supervised%20homography%20estimation%20progressively%20achieves%20reliable%0Apredictions%2C%20thereby%20providing%20robust%20cross-modal%20supervision%20for%20the%20modality%0Atransfer.%20To%20further%20boost%20the%20estimation%20accuracy%2C%20we%20also%20formulate%20a%0Afine-grained%20homography%20feature%20loss%20to%20improve%20the%20connection%20between%20two%0Acomponents.%20Furthermore%2C%20we%20employ%20a%20simple%20yet%20effective%20distillation%20training%0Atechnique%20to%20reduce%20model%20parameters%20and%20improve%20cross-domain%20generalization%0Aability%20while%20maintaining%20comparable%20performance.%20Experiments%20reveal%20that%0AInterNet%20achieves%20the%20state-of-the-art%20%28SOTA%29%20performance%20among%20unsupervised%0Amethods%2C%20and%20even%20outperforms%20many%20supervised%20methods%20such%20as%20MHN%20and%0ALocalTrans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterNet%253A%2520Unsupervised%2520Cross-modal%2520Homography%2520Estimation%2520Based%2520on%250A%2520%2520Interleaved%2520Modality%2520Transfer%2520and%2520Self-supervised%2520Homography%2520Prediction%26entry.906535625%3DJunchen%2520Yu%2520and%2520Si-Yuan%2520Cao%2520and%2520Runmin%2520Zhang%2520and%2520Chenghao%2520Zhang%2520and%2520Jianxin%2520Hu%2520and%2520Zhu%2520Yu%2520and%2520Hui-liang%2520Shen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520unsupervised%2520cross-modal%2520homography%2520estimation%2520framework%252C%250Abased%2520on%2520interleaved%2520modality%2520transfer%2520and%2520self-supervised%2520homography%250Aprediction%252C%2520named%2520InterNet.%2520InterNet%2520integrates%2520modality%2520transfer%2520and%250Aself-supervised%2520homography%2520estimation%252C%2520introducing%2520an%2520innovative%2520interleaved%250Aoptimization%2520framework%2520to%2520alternately%2520promote%2520both%2520components.%2520The%2520modality%250Atransfer%2520gradually%2520narrows%2520the%2520modality%2520gaps%252C%2520facilitating%2520the%2520self-supervised%250Ahomography%2520estimation%2520to%2520fully%2520leverage%2520the%2520synthetic%2520intra-modal%2520data.%2520The%250Aself-supervised%2520homography%2520estimation%2520progressively%2520achieves%2520reliable%250Apredictions%252C%2520thereby%2520providing%2520robust%2520cross-modal%2520supervision%2520for%2520the%2520modality%250Atransfer.%2520To%2520further%2520boost%2520the%2520estimation%2520accuracy%252C%2520we%2520also%2520formulate%2520a%250Afine-grained%2520homography%2520feature%2520loss%2520to%2520improve%2520the%2520connection%2520between%2520two%250Acomponents.%2520Furthermore%252C%2520we%2520employ%2520a%2520simple%2520yet%2520effective%2520distillation%2520training%250Atechnique%2520to%2520reduce%2520model%2520parameters%2520and%2520improve%2520cross-domain%2520generalization%250Aability%2520while%2520maintaining%2520comparable%2520performance.%2520Experiments%2520reveal%2520that%250AInterNet%2520achieves%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520among%2520unsupervised%250Amethods%252C%2520and%2520even%2520outperforms%2520many%2520supervised%2520methods%2520such%2520as%2520MHN%2520and%250ALocalTrans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterNet%3A%20Unsupervised%20Cross-modal%20Homography%20Estimation%20Based%20on%0A%20%20Interleaved%20Modality%20Transfer%20and%20Self-supervised%20Homography%20Prediction&entry.906535625=Junchen%20Yu%20and%20Si-Yuan%20Cao%20and%20Runmin%20Zhang%20and%20Chenghao%20Zhang%20and%20Jianxin%20Hu%20and%20Zhu%20Yu%20and%20Hui-liang%20Shen&entry.1292438233=%20%20We%20propose%20a%20novel%20unsupervised%20cross-modal%20homography%20estimation%20framework%2C%0Abased%20on%20interleaved%20modality%20transfer%20and%20self-supervised%20homography%0Aprediction%2C%20named%20InterNet.%20InterNet%20integrates%20modality%20transfer%20and%0Aself-supervised%20homography%20estimation%2C%20introducing%20an%20innovative%20interleaved%0Aoptimization%20framework%20to%20alternately%20promote%20both%20components.%20The%20modality%0Atransfer%20gradually%20narrows%20the%20modality%20gaps%2C%20facilitating%20the%20self-supervised%0Ahomography%20estimation%20to%20fully%20leverage%20the%20synthetic%20intra-modal%20data.%20The%0Aself-supervised%20homography%20estimation%20progressively%20achieves%20reliable%0Apredictions%2C%20thereby%20providing%20robust%20cross-modal%20supervision%20for%20the%20modality%0Atransfer.%20To%20further%20boost%20the%20estimation%20accuracy%2C%20we%20also%20formulate%20a%0Afine-grained%20homography%20feature%20loss%20to%20improve%20the%20connection%20between%20two%0Acomponents.%20Furthermore%2C%20we%20employ%20a%20simple%20yet%20effective%20distillation%20training%0Atechnique%20to%20reduce%20model%20parameters%20and%20improve%20cross-domain%20generalization%0Aability%20while%20maintaining%20comparable%20performance.%20Experiments%20reveal%20that%0AInterNet%20achieves%20the%20state-of-the-art%20%28SOTA%29%20performance%20among%20unsupervised%0Amethods%2C%20and%20even%20outperforms%20many%20supervised%20methods%20such%20as%20MHN%20and%0ALocalTrans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17993v1&entry.124074799=Read"},
{"title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for\n  Zero-shot Captioning", "author": "Soeun Lee and Si-Woo Kim and Taewhan Kim and Dong-Jin Kim", "abstract": "  Recent advancements in image captioning have explored text-only training\nmethods to overcome the limitations of paired image-text data. However,\nexisting text-only training methods often overlook the modality gap between\nusing text data during training and employing images during inference. To\naddress this issue, we propose a novel approach called Image-like Retrieval,\nwhich aligns text features with visually relevant features to mitigate the\nmodality gap. Our method further enhances the accuracy of generated captions by\ndesigning a Fusion Module that integrates retrieved captions with input\nfeatures. Additionally, we introduce a Frequency-based Entity Filtering\ntechnique that significantly improves caption quality. We integrate these\nmethods into a unified framework, which we refer to as IFCap\n($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity\nFiltering for Zero-shot $\\textbf{Cap}$tioning). Through extensive\nexperimentation, our straightforward yet powerful approach has demonstrated its\nefficacy, outperforming the state-of-the-art methods by a significant margin in\nboth image captioning and video captioning compared to zero-shot captioning\nbased on text-only training.\n", "link": "http://arxiv.org/abs/2409.18046v1", "date": "2024-09-26", "relevancy": 2.7844, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5719}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5553}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IFCap%3A%20Image-like%20Retrieval%20and%20Frequency-based%20Entity%20Filtering%20for%0A%20%20Zero-shot%20Captioning&body=Title%3A%20IFCap%3A%20Image-like%20Retrieval%20and%20Frequency-based%20Entity%20Filtering%20for%0A%20%20Zero-shot%20Captioning%0AAuthor%3A%20Soeun%20Lee%20and%20Si-Woo%20Kim%20and%20Taewhan%20Kim%20and%20Dong-Jin%20Kim%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image%20captioning%20have%20explored%20text-only%20training%0Amethods%20to%20overcome%20the%20limitations%20of%20paired%20image-text%20data.%20However%2C%0Aexisting%20text-only%20training%20methods%20often%20overlook%20the%20modality%20gap%20between%0Ausing%20text%20data%20during%20training%20and%20employing%20images%20during%20inference.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20approach%20called%20Image-like%20Retrieval%2C%0Awhich%20aligns%20text%20features%20with%20visually%20relevant%20features%20to%20mitigate%20the%0Amodality%20gap.%20Our%20method%20further%20enhances%20the%20accuracy%20of%20generated%20captions%20by%0Adesigning%20a%20Fusion%20Module%20that%20integrates%20retrieved%20captions%20with%20input%0Afeatures.%20Additionally%2C%20we%20introduce%20a%20Frequency-based%20Entity%20Filtering%0Atechnique%20that%20significantly%20improves%20caption%20quality.%20We%20integrate%20these%0Amethods%20into%20a%20unified%20framework%2C%20which%20we%20refer%20to%20as%20IFCap%0A%28%24%5Ctextbf%7BI%7D%24mage-like%20Retrieval%20and%20%24%5Ctextbf%7BF%7D%24requency-based%20Entity%0AFiltering%20for%20Zero-shot%20%24%5Ctextbf%7BCap%7D%24tioning%29.%20Through%20extensive%0Aexperimentation%2C%20our%20straightforward%20yet%20powerful%20approach%20has%20demonstrated%20its%0Aefficacy%2C%20outperforming%20the%20state-of-the-art%20methods%20by%20a%20significant%20margin%20in%0Aboth%20image%20captioning%20and%20video%20captioning%20compared%20to%20zero-shot%20captioning%0Abased%20on%20text-only%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIFCap%253A%2520Image-like%2520Retrieval%2520and%2520Frequency-based%2520Entity%2520Filtering%2520for%250A%2520%2520Zero-shot%2520Captioning%26entry.906535625%3DSoeun%2520Lee%2520and%2520Si-Woo%2520Kim%2520and%2520Taewhan%2520Kim%2520and%2520Dong-Jin%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image%2520captioning%2520have%2520explored%2520text-only%2520training%250Amethods%2520to%2520overcome%2520the%2520limitations%2520of%2520paired%2520image-text%2520data.%2520However%252C%250Aexisting%2520text-only%2520training%2520methods%2520often%2520overlook%2520the%2520modality%2520gap%2520between%250Ausing%2520text%2520data%2520during%2520training%2520and%2520employing%2520images%2520during%2520inference.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520Image-like%2520Retrieval%252C%250Awhich%2520aligns%2520text%2520features%2520with%2520visually%2520relevant%2520features%2520to%2520mitigate%2520the%250Amodality%2520gap.%2520Our%2520method%2520further%2520enhances%2520the%2520accuracy%2520of%2520generated%2520captions%2520by%250Adesigning%2520a%2520Fusion%2520Module%2520that%2520integrates%2520retrieved%2520captions%2520with%2520input%250Afeatures.%2520Additionally%252C%2520we%2520introduce%2520a%2520Frequency-based%2520Entity%2520Filtering%250Atechnique%2520that%2520significantly%2520improves%2520caption%2520quality.%2520We%2520integrate%2520these%250Amethods%2520into%2520a%2520unified%2520framework%252C%2520which%2520we%2520refer%2520to%2520as%2520IFCap%250A%2528%2524%255Ctextbf%257BI%257D%2524mage-like%2520Retrieval%2520and%2520%2524%255Ctextbf%257BF%257D%2524requency-based%2520Entity%250AFiltering%2520for%2520Zero-shot%2520%2524%255Ctextbf%257BCap%257D%2524tioning%2529.%2520Through%2520extensive%250Aexperimentation%252C%2520our%2520straightforward%2520yet%2520powerful%2520approach%2520has%2520demonstrated%2520its%250Aefficacy%252C%2520outperforming%2520the%2520state-of-the-art%2520methods%2520by%2520a%2520significant%2520margin%2520in%250Aboth%2520image%2520captioning%2520and%2520video%2520captioning%2520compared%2520to%2520zero-shot%2520captioning%250Abased%2520on%2520text-only%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IFCap%3A%20Image-like%20Retrieval%20and%20Frequency-based%20Entity%20Filtering%20for%0A%20%20Zero-shot%20Captioning&entry.906535625=Soeun%20Lee%20and%20Si-Woo%20Kim%20and%20Taewhan%20Kim%20and%20Dong-Jin%20Kim&entry.1292438233=%20%20Recent%20advancements%20in%20image%20captioning%20have%20explored%20text-only%20training%0Amethods%20to%20overcome%20the%20limitations%20of%20paired%20image-text%20data.%20However%2C%0Aexisting%20text-only%20training%20methods%20often%20overlook%20the%20modality%20gap%20between%0Ausing%20text%20data%20during%20training%20and%20employing%20images%20during%20inference.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20approach%20called%20Image-like%20Retrieval%2C%0Awhich%20aligns%20text%20features%20with%20visually%20relevant%20features%20to%20mitigate%20the%0Amodality%20gap.%20Our%20method%20further%20enhances%20the%20accuracy%20of%20generated%20captions%20by%0Adesigning%20a%20Fusion%20Module%20that%20integrates%20retrieved%20captions%20with%20input%0Afeatures.%20Additionally%2C%20we%20introduce%20a%20Frequency-based%20Entity%20Filtering%0Atechnique%20that%20significantly%20improves%20caption%20quality.%20We%20integrate%20these%0Amethods%20into%20a%20unified%20framework%2C%20which%20we%20refer%20to%20as%20IFCap%0A%28%24%5Ctextbf%7BI%7D%24mage-like%20Retrieval%20and%20%24%5Ctextbf%7BF%7D%24requency-based%20Entity%0AFiltering%20for%20Zero-shot%20%24%5Ctextbf%7BCap%7D%24tioning%29.%20Through%20extensive%0Aexperimentation%2C%20our%20straightforward%20yet%20powerful%20approach%20has%20demonstrated%20its%0Aefficacy%2C%20outperforming%20the%20state-of-the-art%20methods%20by%20a%20significant%20margin%20in%0Aboth%20image%20captioning%20and%20video%20captioning%20compared%20to%20zero-shot%20captioning%0Abased%20on%20text-only%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18046v1&entry.124074799=Read"},
{"title": "MIO: A Foundation Model on Multimodal Tokens", "author": "Zekun Wang and King Zhu and Chunpu Xu and Wangchunshu Zhou and Jiaheng Liu and Yibo Zhang and Jiashuo Wang and Ning Shi and Siyu Li and Yizhi Li and Haoran Que and Zhaoxiang Zhang and Yuanxing Zhang and Ge Zhang and Ke Xu and Jie Fu and Wenhao Huang", "abstract": "  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n", "link": "http://arxiv.org/abs/2409.17692v1", "date": "2024-09-26", "relevancy": 2.7183, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIO%3A%20A%20Foundation%20Model%20on%20Multimodal%20Tokens&body=Title%3A%20MIO%3A%20A%20Foundation%20Model%20on%20Multimodal%20Tokens%0AAuthor%3A%20Zekun%20Wang%20and%20King%20Zhu%20and%20Chunpu%20Xu%20and%20Wangchunshu%20Zhou%20and%20Jiaheng%20Liu%20and%20Yibo%20Zhang%20and%20Jiashuo%20Wang%20and%20Ning%20Shi%20and%20Siyu%20Li%20and%20Yizhi%20Li%20and%20Haoran%20Que%20and%20Zhaoxiang%20Zhang%20and%20Yuanxing%20Zhang%20and%20Ge%20Zhang%20and%20Ke%20Xu%20and%20Jie%20Fu%20and%20Wenhao%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20MIO%2C%20a%20novel%20foundation%20model%20built%20on%20multimodal%0Atokens%2C%20capable%20of%20understanding%20and%20generating%20speech%2C%20text%2C%20images%2C%20and%0Avideos%20in%20an%20end-to-end%2C%20autoregressive%20manner.%20While%20the%20emergence%20of%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MM-LLMs%29%20propels%0Aadvancements%20in%20artificial%20general%20intelligence%20through%20their%20versatile%0Acapabilities%2C%20they%20still%20lack%20true%20any-to-any%20understanding%20and%20generation.%0ARecently%2C%20the%20release%20of%20GPT-4o%20has%20showcased%20the%20remarkable%20potential%20of%0Aany-to-any%20LLMs%20for%20complex%20real-world%20tasks%2C%20enabling%20omnidirectional%20input%0Aand%20output%20across%20images%2C%20speech%2C%20and%20text.%20However%2C%20it%20is%20closed-source%20and%0Adoes%20not%20support%20the%20generation%20of%20multimodal%20interleaved%20sequences.%20To%20address%0Athis%20gap%2C%20we%20present%20MIO%2C%20which%20is%20trained%20on%20a%20mixture%20of%20discrete%20tokens%0Aacross%20four%20modalities%20using%20causal%20multimodal%20modeling.%20MIO%20undergoes%20a%0Afour-stage%20training%20process%3A%20%281%29%20alignment%20pre-training%2C%20%282%29%20interleaved%0Apre-training%2C%20%283%29%20speech-enhanced%20pre-training%2C%20and%20%284%29%20comprehensive%0Asupervised%20fine-tuning%20on%20diverse%20textual%2C%20visual%2C%20and%20speech%20tasks.%20Our%0Aexperimental%20results%20indicate%20that%20MIO%20exhibits%20competitive%2C%20and%20in%20some%20cases%0Asuperior%2C%20performance%20compared%20to%20previous%20dual-modal%20baselines%2C%20any-to-any%0Amodel%20baselines%2C%20and%20even%20modality-specific%20baselines.%20Moreover%2C%20MIO%0Ademonstrates%20advanced%20capabilities%20inherent%20to%20its%20any-to-any%20feature%2C%20such%20as%0Ainterleaved%20video-text%20generation%2C%20chain-of-visual-thought%20reasoning%2C%20visual%0Aguideline%20generation%2C%20instructional%20image%20editing%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIO%253A%2520A%2520Foundation%2520Model%2520on%2520Multimodal%2520Tokens%26entry.906535625%3DZekun%2520Wang%2520and%2520King%2520Zhu%2520and%2520Chunpu%2520Xu%2520and%2520Wangchunshu%2520Zhou%2520and%2520Jiaheng%2520Liu%2520and%2520Yibo%2520Zhang%2520and%2520Jiashuo%2520Wang%2520and%2520Ning%2520Shi%2520and%2520Siyu%2520Li%2520and%2520Yizhi%2520Li%2520and%2520Haoran%2520Que%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Yuanxing%2520Zhang%2520and%2520Ge%2520Zhang%2520and%2520Ke%2520Xu%2520and%2520Jie%2520Fu%2520and%2520Wenhao%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MIO%252C%2520a%2520novel%2520foundation%2520model%2520built%2520on%2520multimodal%250Atokens%252C%2520capable%2520of%2520understanding%2520and%2520generating%2520speech%252C%2520text%252C%2520images%252C%2520and%250Avideos%2520in%2520an%2520end-to-end%252C%2520autoregressive%2520manner.%2520While%2520the%2520emergence%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520and%2520multimodal%2520large%2520language%2520models%2520%2528MM-LLMs%2529%2520propels%250Aadvancements%2520in%2520artificial%2520general%2520intelligence%2520through%2520their%2520versatile%250Acapabilities%252C%2520they%2520still%2520lack%2520true%2520any-to-any%2520understanding%2520and%2520generation.%250ARecently%252C%2520the%2520release%2520of%2520GPT-4o%2520has%2520showcased%2520the%2520remarkable%2520potential%2520of%250Aany-to-any%2520LLMs%2520for%2520complex%2520real-world%2520tasks%252C%2520enabling%2520omnidirectional%2520input%250Aand%2520output%2520across%2520images%252C%2520speech%252C%2520and%2520text.%2520However%252C%2520it%2520is%2520closed-source%2520and%250Adoes%2520not%2520support%2520the%2520generation%2520of%2520multimodal%2520interleaved%2520sequences.%2520To%2520address%250Athis%2520gap%252C%2520we%2520present%2520MIO%252C%2520which%2520is%2520trained%2520on%2520a%2520mixture%2520of%2520discrete%2520tokens%250Aacross%2520four%2520modalities%2520using%2520causal%2520multimodal%2520modeling.%2520MIO%2520undergoes%2520a%250Afour-stage%2520training%2520process%253A%2520%25281%2529%2520alignment%2520pre-training%252C%2520%25282%2529%2520interleaved%250Apre-training%252C%2520%25283%2529%2520speech-enhanced%2520pre-training%252C%2520and%2520%25284%2529%2520comprehensive%250Asupervised%2520fine-tuning%2520on%2520diverse%2520textual%252C%2520visual%252C%2520and%2520speech%2520tasks.%2520Our%250Aexperimental%2520results%2520indicate%2520that%2520MIO%2520exhibits%2520competitive%252C%2520and%2520in%2520some%2520cases%250Asuperior%252C%2520performance%2520compared%2520to%2520previous%2520dual-modal%2520baselines%252C%2520any-to-any%250Amodel%2520baselines%252C%2520and%2520even%2520modality-specific%2520baselines.%2520Moreover%252C%2520MIO%250Ademonstrates%2520advanced%2520capabilities%2520inherent%2520to%2520its%2520any-to-any%2520feature%252C%2520such%2520as%250Ainterleaved%2520video-text%2520generation%252C%2520chain-of-visual-thought%2520reasoning%252C%2520visual%250Aguideline%2520generation%252C%2520instructional%2520image%2520editing%252C%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIO%3A%20A%20Foundation%20Model%20on%20Multimodal%20Tokens&entry.906535625=Zekun%20Wang%20and%20King%20Zhu%20and%20Chunpu%20Xu%20and%20Wangchunshu%20Zhou%20and%20Jiaheng%20Liu%20and%20Yibo%20Zhang%20and%20Jiashuo%20Wang%20and%20Ning%20Shi%20and%20Siyu%20Li%20and%20Yizhi%20Li%20and%20Haoran%20Que%20and%20Zhaoxiang%20Zhang%20and%20Yuanxing%20Zhang%20and%20Ge%20Zhang%20and%20Ke%20Xu%20and%20Jie%20Fu%20and%20Wenhao%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20MIO%2C%20a%20novel%20foundation%20model%20built%20on%20multimodal%0Atokens%2C%20capable%20of%20understanding%20and%20generating%20speech%2C%20text%2C%20images%2C%20and%0Avideos%20in%20an%20end-to-end%2C%20autoregressive%20manner.%20While%20the%20emergence%20of%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MM-LLMs%29%20propels%0Aadvancements%20in%20artificial%20general%20intelligence%20through%20their%20versatile%0Acapabilities%2C%20they%20still%20lack%20true%20any-to-any%20understanding%20and%20generation.%0ARecently%2C%20the%20release%20of%20GPT-4o%20has%20showcased%20the%20remarkable%20potential%20of%0Aany-to-any%20LLMs%20for%20complex%20real-world%20tasks%2C%20enabling%20omnidirectional%20input%0Aand%20output%20across%20images%2C%20speech%2C%20and%20text.%20However%2C%20it%20is%20closed-source%20and%0Adoes%20not%20support%20the%20generation%20of%20multimodal%20interleaved%20sequences.%20To%20address%0Athis%20gap%2C%20we%20present%20MIO%2C%20which%20is%20trained%20on%20a%20mixture%20of%20discrete%20tokens%0Aacross%20four%20modalities%20using%20causal%20multimodal%20modeling.%20MIO%20undergoes%20a%0Afour-stage%20training%20process%3A%20%281%29%20alignment%20pre-training%2C%20%282%29%20interleaved%0Apre-training%2C%20%283%29%20speech-enhanced%20pre-training%2C%20and%20%284%29%20comprehensive%0Asupervised%20fine-tuning%20on%20diverse%20textual%2C%20visual%2C%20and%20speech%20tasks.%20Our%0Aexperimental%20results%20indicate%20that%20MIO%20exhibits%20competitive%2C%20and%20in%20some%20cases%0Asuperior%2C%20performance%20compared%20to%20previous%20dual-modal%20baselines%2C%20any-to-any%0Amodel%20baselines%2C%20and%20even%20modality-specific%20baselines.%20Moreover%2C%20MIO%0Ademonstrates%20advanced%20capabilities%20inherent%20to%20its%20any-to-any%20feature%2C%20such%20as%0Ainterleaved%20video-text%20generation%2C%20chain-of-visual-thought%20reasoning%2C%20visual%0Aguideline%20generation%2C%20instructional%20image%20editing%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17692v1&entry.124074799=Read"},
{"title": "EvMAPPER: High Altitude Orthomapping with Event Cameras", "author": "Fernando Cladera and Kenneth Chaney and M. Ani Hsieh and Camillo J. Taylor and Vijay Kumar", "abstract": "  Traditionally, unmanned aerial vehicles (UAVs) rely on CMOS-based cameras to\ncollect images about the world below. One of the most successful applications\nof UAVs is to generate orthomosaics or orthomaps, in which a series of images\nare integrated together to develop a larger map. However, the use of CMOS-based\ncameras with global or rolling shutters mean that orthomaps are vulnerable to\nchallenging light conditions, motion blur, and high-speed motion of\nindependently moving objects under the camera. Event cameras are less sensitive\nto these issues, as their pixels are able to trigger asynchronously on\nbrightness changes. This work introduces the first orthomosaic approach using\nevent cameras. In contrast to existing methods relying only on CMOS cameras,\nour approach enables map generation even in challenging light conditions,\nincluding direct sunlight and after sunset.\n", "link": "http://arxiv.org/abs/2409.18120v1", "date": "2024-09-26", "relevancy": 2.7089, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5757}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5248}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvMAPPER%3A%20High%20Altitude%20Orthomapping%20with%20Event%20Cameras&body=Title%3A%20EvMAPPER%3A%20High%20Altitude%20Orthomapping%20with%20Event%20Cameras%0AAuthor%3A%20Fernando%20Cladera%20and%20Kenneth%20Chaney%20and%20M.%20Ani%20Hsieh%20and%20Camillo%20J.%20Taylor%20and%20Vijay%20Kumar%0AAbstract%3A%20%20%20Traditionally%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%20rely%20on%20CMOS-based%20cameras%20to%0Acollect%20images%20about%20the%20world%20below.%20One%20of%20the%20most%20successful%20applications%0Aof%20UAVs%20is%20to%20generate%20orthomosaics%20or%20orthomaps%2C%20in%20which%20a%20series%20of%20images%0Aare%20integrated%20together%20to%20develop%20a%20larger%20map.%20However%2C%20the%20use%20of%20CMOS-based%0Acameras%20with%20global%20or%20rolling%20shutters%20mean%20that%20orthomaps%20are%20vulnerable%20to%0Achallenging%20light%20conditions%2C%20motion%20blur%2C%20and%20high-speed%20motion%20of%0Aindependently%20moving%20objects%20under%20the%20camera.%20Event%20cameras%20are%20less%20sensitive%0Ato%20these%20issues%2C%20as%20their%20pixels%20are%20able%20to%20trigger%20asynchronously%20on%0Abrightness%20changes.%20This%20work%20introduces%20the%20first%20orthomosaic%20approach%20using%0Aevent%20cameras.%20In%20contrast%20to%20existing%20methods%20relying%20only%20on%20CMOS%20cameras%2C%0Aour%20approach%20enables%20map%20generation%20even%20in%20challenging%20light%20conditions%2C%0Aincluding%20direct%20sunlight%20and%20after%20sunset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvMAPPER%253A%2520High%2520Altitude%2520Orthomapping%2520with%2520Event%2520Cameras%26entry.906535625%3DFernando%2520Cladera%2520and%2520Kenneth%2520Chaney%2520and%2520M.%2520Ani%2520Hsieh%2520and%2520Camillo%2520J.%2520Taylor%2520and%2520Vijay%2520Kumar%26entry.1292438233%3D%2520%2520Traditionally%252C%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520rely%2520on%2520CMOS-based%2520cameras%2520to%250Acollect%2520images%2520about%2520the%2520world%2520below.%2520One%2520of%2520the%2520most%2520successful%2520applications%250Aof%2520UAVs%2520is%2520to%2520generate%2520orthomosaics%2520or%2520orthomaps%252C%2520in%2520which%2520a%2520series%2520of%2520images%250Aare%2520integrated%2520together%2520to%2520develop%2520a%2520larger%2520map.%2520However%252C%2520the%2520use%2520of%2520CMOS-based%250Acameras%2520with%2520global%2520or%2520rolling%2520shutters%2520mean%2520that%2520orthomaps%2520are%2520vulnerable%2520to%250Achallenging%2520light%2520conditions%252C%2520motion%2520blur%252C%2520and%2520high-speed%2520motion%2520of%250Aindependently%2520moving%2520objects%2520under%2520the%2520camera.%2520Event%2520cameras%2520are%2520less%2520sensitive%250Ato%2520these%2520issues%252C%2520as%2520their%2520pixels%2520are%2520able%2520to%2520trigger%2520asynchronously%2520on%250Abrightness%2520changes.%2520This%2520work%2520introduces%2520the%2520first%2520orthomosaic%2520approach%2520using%250Aevent%2520cameras.%2520In%2520contrast%2520to%2520existing%2520methods%2520relying%2520only%2520on%2520CMOS%2520cameras%252C%250Aour%2520approach%2520enables%2520map%2520generation%2520even%2520in%2520challenging%2520light%2520conditions%252C%250Aincluding%2520direct%2520sunlight%2520and%2520after%2520sunset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvMAPPER%3A%20High%20Altitude%20Orthomapping%20with%20Event%20Cameras&entry.906535625=Fernando%20Cladera%20and%20Kenneth%20Chaney%20and%20M.%20Ani%20Hsieh%20and%20Camillo%20J.%20Taylor%20and%20Vijay%20Kumar&entry.1292438233=%20%20Traditionally%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%20rely%20on%20CMOS-based%20cameras%20to%0Acollect%20images%20about%20the%20world%20below.%20One%20of%20the%20most%20successful%20applications%0Aof%20UAVs%20is%20to%20generate%20orthomosaics%20or%20orthomaps%2C%20in%20which%20a%20series%20of%20images%0Aare%20integrated%20together%20to%20develop%20a%20larger%20map.%20However%2C%20the%20use%20of%20CMOS-based%0Acameras%20with%20global%20or%20rolling%20shutters%20mean%20that%20orthomaps%20are%20vulnerable%20to%0Achallenging%20light%20conditions%2C%20motion%20blur%2C%20and%20high-speed%20motion%20of%0Aindependently%20moving%20objects%20under%20the%20camera.%20Event%20cameras%20are%20less%20sensitive%0Ato%20these%20issues%2C%20as%20their%20pixels%20are%20able%20to%20trigger%20asynchronously%20on%0Abrightness%20changes.%20This%20work%20introduces%20the%20first%20orthomosaic%20approach%20using%0Aevent%20cameras.%20In%20contrast%20to%20existing%20methods%20relying%20only%20on%20CMOS%20cameras%2C%0Aour%20approach%20enables%20map%20generation%20even%20in%20challenging%20light%20conditions%2C%0Aincluding%20direct%20sunlight%20and%20after%20sunset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18120v1&entry.124074799=Read"},
{"title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions", "author": "Kai Chen and Yunhao Gou and Runhui Huang and Zhili Liu and Daxin Tan and Jing Xu and Chunwei Wang and Yi Zhu and Yihan Zeng and Kuo Yang and Dingdong Wang and Kun Xiang and Haoyuan Li and Haoli Bai and Jianhua Han and Xiaohui Li and Weike Jin and Nian Xie and Yu Zhang and James T. Kwok and Hengshuang Zhao and Xiaodan Liang and Dit-Yan Yeung and Xiao Chen and Zhenguo Li and Wei Zhang and Qun Liu and Lanqing Hong and Lu Hou and Hang Xu", "abstract": "  GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nin the open-source community. Existing vision-language models rely on external\ntools for the speech processing, while speech-language models still suffer from\nlimited or even without vision-understanding abilities. To address this gap, we\npropose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large\nLanguage Models with end-to-end speech capabilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we notice surprisingly that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the corresponding\nbi-modal aligned counterparts. Moreover, a lightweight style module is proposed\nfor flexible speech style controls (e.g., emotions and pitches). For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.\n", "link": "http://arxiv.org/abs/2409.18042v1", "date": "2024-09-26", "relevancy": 2.7005, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMOVA%3A%20Empowering%20Language%20Models%20to%20See%2C%20Hear%20and%20Speak%20with%20Vivid%0A%20%20Emotions&body=Title%3A%20EMOVA%3A%20Empowering%20Language%20Models%20to%20See%2C%20Hear%20and%20Speak%20with%20Vivid%0A%20%20Emotions%0AAuthor%3A%20Kai%20Chen%20and%20Yunhao%20Gou%20and%20Runhui%20Huang%20and%20Zhili%20Liu%20and%20Daxin%20Tan%20and%20Jing%20Xu%20and%20Chunwei%20Wang%20and%20Yi%20Zhu%20and%20Yihan%20Zeng%20and%20Kuo%20Yang%20and%20Dingdong%20Wang%20and%20Kun%20Xiang%20and%20Haoyuan%20Li%20and%20Haoli%20Bai%20and%20Jianhua%20Han%20and%20Xiaohui%20Li%20and%20Weike%20Jin%20and%20Nian%20Xie%20and%20Yu%20Zhang%20and%20James%20T.%20Kwok%20and%20Hengshuang%20Zhao%20and%20Xiaodan%20Liang%20and%20Dit-Yan%20Yeung%20and%20Xiao%20Chen%20and%20Zhenguo%20Li%20and%20Wei%20Zhang%20and%20Qun%20Liu%20and%20Lanqing%20Hong%20and%20Lu%20Hou%20and%20Hang%20Xu%0AAbstract%3A%20%20%20GPT-4o%2C%20an%20omni-modal%20model%20that%20enables%20vocal%20conversations%20with%20diverse%0Aemotions%20and%20tones%2C%20marks%20a%20milestone%20for%20omni-modal%20foundation%20models.%0AHowever%2C%20empowering%20Large%20Language%20Models%20to%20perceive%20and%20generate%20images%2C%0Atexts%2C%20and%20speeches%20end-to-end%20with%20publicly%20available%20data%20remains%20challenging%0Ain%20the%20open-source%20community.%20Existing%20vision-language%20models%20rely%20on%20external%0Atools%20for%20the%20speech%20processing%2C%20while%20speech-language%20models%20still%20suffer%20from%0Alimited%20or%20even%20without%20vision-understanding%20abilities.%20To%20address%20this%20gap%2C%20we%0Apropose%20EMOVA%20%28EMotionally%20Omni-present%20Voice%20Assistant%29%2C%20to%20enable%20Large%0ALanguage%20Models%20with%20end-to-end%20speech%20capabilities%20while%20maintaining%20the%0Aleading%20vision-language%20performance.%20With%20a%20semantic-acoustic%20disentangled%0Aspeech%20tokenizer%2C%20we%20notice%20surprisingly%20that%20omni-modal%20alignment%20can%20further%0Aenhance%20vision-language%20and%20speech%20abilities%20compared%20with%20the%20corresponding%0Abi-modal%20aligned%20counterparts.%20Moreover%2C%20a%20lightweight%20style%20module%20is%20proposed%0Afor%20flexible%20speech%20style%20controls%20%28e.g.%2C%20emotions%20and%20pitches%29.%20For%20the%20first%0Atime%2C%20EMOVA%20achieves%20state-of-the-art%20performance%20on%20both%20the%20vision-language%0Aand%20speech%20benchmarks%2C%20and%20meanwhile%2C%20supporting%20omni-modal%20spoken%20dialogue%0Awith%20vivid%20emotions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMOVA%253A%2520Empowering%2520Language%2520Models%2520to%2520See%252C%2520Hear%2520and%2520Speak%2520with%2520Vivid%250A%2520%2520Emotions%26entry.906535625%3DKai%2520Chen%2520and%2520Yunhao%2520Gou%2520and%2520Runhui%2520Huang%2520and%2520Zhili%2520Liu%2520and%2520Daxin%2520Tan%2520and%2520Jing%2520Xu%2520and%2520Chunwei%2520Wang%2520and%2520Yi%2520Zhu%2520and%2520Yihan%2520Zeng%2520and%2520Kuo%2520Yang%2520and%2520Dingdong%2520Wang%2520and%2520Kun%2520Xiang%2520and%2520Haoyuan%2520Li%2520and%2520Haoli%2520Bai%2520and%2520Jianhua%2520Han%2520and%2520Xiaohui%2520Li%2520and%2520Weike%2520Jin%2520and%2520Nian%2520Xie%2520and%2520Yu%2520Zhang%2520and%2520James%2520T.%2520Kwok%2520and%2520Hengshuang%2520Zhao%2520and%2520Xiaodan%2520Liang%2520and%2520Dit-Yan%2520Yeung%2520and%2520Xiao%2520Chen%2520and%2520Zhenguo%2520Li%2520and%2520Wei%2520Zhang%2520and%2520Qun%2520Liu%2520and%2520Lanqing%2520Hong%2520and%2520Lu%2520Hou%2520and%2520Hang%2520Xu%26entry.1292438233%3D%2520%2520GPT-4o%252C%2520an%2520omni-modal%2520model%2520that%2520enables%2520vocal%2520conversations%2520with%2520diverse%250Aemotions%2520and%2520tones%252C%2520marks%2520a%2520milestone%2520for%2520omni-modal%2520foundation%2520models.%250AHowever%252C%2520empowering%2520Large%2520Language%2520Models%2520to%2520perceive%2520and%2520generate%2520images%252C%250Atexts%252C%2520and%2520speeches%2520end-to-end%2520with%2520publicly%2520available%2520data%2520remains%2520challenging%250Ain%2520the%2520open-source%2520community.%2520Existing%2520vision-language%2520models%2520rely%2520on%2520external%250Atools%2520for%2520the%2520speech%2520processing%252C%2520while%2520speech-language%2520models%2520still%2520suffer%2520from%250Alimited%2520or%2520even%2520without%2520vision-understanding%2520abilities.%2520To%2520address%2520this%2520gap%252C%2520we%250Apropose%2520EMOVA%2520%2528EMotionally%2520Omni-present%2520Voice%2520Assistant%2529%252C%2520to%2520enable%2520Large%250ALanguage%2520Models%2520with%2520end-to-end%2520speech%2520capabilities%2520while%2520maintaining%2520the%250Aleading%2520vision-language%2520performance.%2520With%2520a%2520semantic-acoustic%2520disentangled%250Aspeech%2520tokenizer%252C%2520we%2520notice%2520surprisingly%2520that%2520omni-modal%2520alignment%2520can%2520further%250Aenhance%2520vision-language%2520and%2520speech%2520abilities%2520compared%2520with%2520the%2520corresponding%250Abi-modal%2520aligned%2520counterparts.%2520Moreover%252C%2520a%2520lightweight%2520style%2520module%2520is%2520proposed%250Afor%2520flexible%2520speech%2520style%2520controls%2520%2528e.g.%252C%2520emotions%2520and%2520pitches%2529.%2520For%2520the%2520first%250Atime%252C%2520EMOVA%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520the%2520vision-language%250Aand%2520speech%2520benchmarks%252C%2520and%2520meanwhile%252C%2520supporting%2520omni-modal%2520spoken%2520dialogue%250Awith%2520vivid%2520emotions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMOVA%3A%20Empowering%20Language%20Models%20to%20See%2C%20Hear%20and%20Speak%20with%20Vivid%0A%20%20Emotions&entry.906535625=Kai%20Chen%20and%20Yunhao%20Gou%20and%20Runhui%20Huang%20and%20Zhili%20Liu%20and%20Daxin%20Tan%20and%20Jing%20Xu%20and%20Chunwei%20Wang%20and%20Yi%20Zhu%20and%20Yihan%20Zeng%20and%20Kuo%20Yang%20and%20Dingdong%20Wang%20and%20Kun%20Xiang%20and%20Haoyuan%20Li%20and%20Haoli%20Bai%20and%20Jianhua%20Han%20and%20Xiaohui%20Li%20and%20Weike%20Jin%20and%20Nian%20Xie%20and%20Yu%20Zhang%20and%20James%20T.%20Kwok%20and%20Hengshuang%20Zhao%20and%20Xiaodan%20Liang%20and%20Dit-Yan%20Yeung%20and%20Xiao%20Chen%20and%20Zhenguo%20Li%20and%20Wei%20Zhang%20and%20Qun%20Liu%20and%20Lanqing%20Hong%20and%20Lu%20Hou%20and%20Hang%20Xu&entry.1292438233=%20%20GPT-4o%2C%20an%20omni-modal%20model%20that%20enables%20vocal%20conversations%20with%20diverse%0Aemotions%20and%20tones%2C%20marks%20a%20milestone%20for%20omni-modal%20foundation%20models.%0AHowever%2C%20empowering%20Large%20Language%20Models%20to%20perceive%20and%20generate%20images%2C%0Atexts%2C%20and%20speeches%20end-to-end%20with%20publicly%20available%20data%20remains%20challenging%0Ain%20the%20open-source%20community.%20Existing%20vision-language%20models%20rely%20on%20external%0Atools%20for%20the%20speech%20processing%2C%20while%20speech-language%20models%20still%20suffer%20from%0Alimited%20or%20even%20without%20vision-understanding%20abilities.%20To%20address%20this%20gap%2C%20we%0Apropose%20EMOVA%20%28EMotionally%20Omni-present%20Voice%20Assistant%29%2C%20to%20enable%20Large%0ALanguage%20Models%20with%20end-to-end%20speech%20capabilities%20while%20maintaining%20the%0Aleading%20vision-language%20performance.%20With%20a%20semantic-acoustic%20disentangled%0Aspeech%20tokenizer%2C%20we%20notice%20surprisingly%20that%20omni-modal%20alignment%20can%20further%0Aenhance%20vision-language%20and%20speech%20abilities%20compared%20with%20the%20corresponding%0Abi-modal%20aligned%20counterparts.%20Moreover%2C%20a%20lightweight%20style%20module%20is%20proposed%0Afor%20flexible%20speech%20style%20controls%20%28e.g.%2C%20emotions%20and%20pitches%29.%20For%20the%20first%0Atime%2C%20EMOVA%20achieves%20state-of-the-art%20performance%20on%20both%20the%20vision-language%0Aand%20speech%20benchmarks%2C%20and%20meanwhile%2C%20supporting%20omni-modal%20spoken%20dialogue%0Awith%20vivid%20emotions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18042v1&entry.124074799=Read"},
{"title": "AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with\n  Alternative Modality Masking", "author": "Shiqi Sun and Yantao Lu and Ning Liu and Bo Jiang and JinChao Chen and Ying Zhang", "abstract": "  Camera-LiDAR fusion models significantly enhance perception performance in\nautonomous driving. The fusion mechanism leverages the strengths of each\nmodality while minimizing their weaknesses. Moreover, in practice, camera-LiDAR\nfusion models utilize pre-trained backbones for efficient training. However, we\nargue that directly loading single-modal pre-trained camera and LiDAR backbones\ninto camera-LiDAR fusion models introduces similar feature redundancy across\nmodalities due to the nature of the fusion mechanism. Unfortunately, existing\npruning methods are developed explicitly for single-modal models, and thus,\nthey struggle to effectively identify these specific redundant parameters in\ncamera-LiDAR fusion models. In this paper, to address the issue above on\ncamera-LiDAR fusion models, we propose a novelty pruning framework Alternative\nModality Masking Pruning (AlterMOMA), which employs alternative masking on each\nmodality and identifies the redundant parameters. Specifically, when one\nmodality parameters are masked (deactivated), the absence of features from the\nmasked backbone compels the model to reactivate previous redundant features of\nthe other modality backbone. Therefore, these redundant features and relevant\nredundant parameters can be identified via the reactivation process. The\nredundant parameters can be pruned by our proposed importance score evaluation\nfunction, Alternative Evaluation (AlterEva), which is based on the observation\nof the loss changes when certain modality parameters are activated and\ndeactivated. Extensive experiments on the nuScene and KITTI datasets\nencompassing diverse tasks, baseline models, and pruning algorithms showcase\nthat AlterMOMA outperforms existing pruning methods, attaining state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2409.17728v1", "date": "2024-09-26", "relevancy": 2.6877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlterMOMA%3A%20Fusion%20Redundancy%20Pruning%20for%20Camera-LiDAR%20Fusion%20Models%20with%0A%20%20Alternative%20Modality%20Masking&body=Title%3A%20AlterMOMA%3A%20Fusion%20Redundancy%20Pruning%20for%20Camera-LiDAR%20Fusion%20Models%20with%0A%20%20Alternative%20Modality%20Masking%0AAuthor%3A%20Shiqi%20Sun%20and%20Yantao%20Lu%20and%20Ning%20Liu%20and%20Bo%20Jiang%20and%20JinChao%20Chen%20and%20Ying%20Zhang%0AAbstract%3A%20%20%20Camera-LiDAR%20fusion%20models%20significantly%20enhance%20perception%20performance%20in%0Aautonomous%20driving.%20The%20fusion%20mechanism%20leverages%20the%20strengths%20of%20each%0Amodality%20while%20minimizing%20their%20weaknesses.%20Moreover%2C%20in%20practice%2C%20camera-LiDAR%0Afusion%20models%20utilize%20pre-trained%20backbones%20for%20efficient%20training.%20However%2C%20we%0Aargue%20that%20directly%20loading%20single-modal%20pre-trained%20camera%20and%20LiDAR%20backbones%0Ainto%20camera-LiDAR%20fusion%20models%20introduces%20similar%20feature%20redundancy%20across%0Amodalities%20due%20to%20the%20nature%20of%20the%20fusion%20mechanism.%20Unfortunately%2C%20existing%0Apruning%20methods%20are%20developed%20explicitly%20for%20single-modal%20models%2C%20and%20thus%2C%0Athey%20struggle%20to%20effectively%20identify%20these%20specific%20redundant%20parameters%20in%0Acamera-LiDAR%20fusion%20models.%20In%20this%20paper%2C%20to%20address%20the%20issue%20above%20on%0Acamera-LiDAR%20fusion%20models%2C%20we%20propose%20a%20novelty%20pruning%20framework%20Alternative%0AModality%20Masking%20Pruning%20%28AlterMOMA%29%2C%20which%20employs%20alternative%20masking%20on%20each%0Amodality%20and%20identifies%20the%20redundant%20parameters.%20Specifically%2C%20when%20one%0Amodality%20parameters%20are%20masked%20%28deactivated%29%2C%20the%20absence%20of%20features%20from%20the%0Amasked%20backbone%20compels%20the%20model%20to%20reactivate%20previous%20redundant%20features%20of%0Athe%20other%20modality%20backbone.%20Therefore%2C%20these%20redundant%20features%20and%20relevant%0Aredundant%20parameters%20can%20be%20identified%20via%20the%20reactivation%20process.%20The%0Aredundant%20parameters%20can%20be%20pruned%20by%20our%20proposed%20importance%20score%20evaluation%0Afunction%2C%20Alternative%20Evaluation%20%28AlterEva%29%2C%20which%20is%20based%20on%20the%20observation%0Aof%20the%20loss%20changes%20when%20certain%20modality%20parameters%20are%20activated%20and%0Adeactivated.%20Extensive%20experiments%20on%20the%20nuScene%20and%20KITTI%20datasets%0Aencompassing%20diverse%20tasks%2C%20baseline%20models%2C%20and%20pruning%20algorithms%20showcase%0Athat%20AlterMOMA%20outperforms%20existing%20pruning%20methods%2C%20attaining%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlterMOMA%253A%2520Fusion%2520Redundancy%2520Pruning%2520for%2520Camera-LiDAR%2520Fusion%2520Models%2520with%250A%2520%2520Alternative%2520Modality%2520Masking%26entry.906535625%3DShiqi%2520Sun%2520and%2520Yantao%2520Lu%2520and%2520Ning%2520Liu%2520and%2520Bo%2520Jiang%2520and%2520JinChao%2520Chen%2520and%2520Ying%2520Zhang%26entry.1292438233%3D%2520%2520Camera-LiDAR%2520fusion%2520models%2520significantly%2520enhance%2520perception%2520performance%2520in%250Aautonomous%2520driving.%2520The%2520fusion%2520mechanism%2520leverages%2520the%2520strengths%2520of%2520each%250Amodality%2520while%2520minimizing%2520their%2520weaknesses.%2520Moreover%252C%2520in%2520practice%252C%2520camera-LiDAR%250Afusion%2520models%2520utilize%2520pre-trained%2520backbones%2520for%2520efficient%2520training.%2520However%252C%2520we%250Aargue%2520that%2520directly%2520loading%2520single-modal%2520pre-trained%2520camera%2520and%2520LiDAR%2520backbones%250Ainto%2520camera-LiDAR%2520fusion%2520models%2520introduces%2520similar%2520feature%2520redundancy%2520across%250Amodalities%2520due%2520to%2520the%2520nature%2520of%2520the%2520fusion%2520mechanism.%2520Unfortunately%252C%2520existing%250Apruning%2520methods%2520are%2520developed%2520explicitly%2520for%2520single-modal%2520models%252C%2520and%2520thus%252C%250Athey%2520struggle%2520to%2520effectively%2520identify%2520these%2520specific%2520redundant%2520parameters%2520in%250Acamera-LiDAR%2520fusion%2520models.%2520In%2520this%2520paper%252C%2520to%2520address%2520the%2520issue%2520above%2520on%250Acamera-LiDAR%2520fusion%2520models%252C%2520we%2520propose%2520a%2520novelty%2520pruning%2520framework%2520Alternative%250AModality%2520Masking%2520Pruning%2520%2528AlterMOMA%2529%252C%2520which%2520employs%2520alternative%2520masking%2520on%2520each%250Amodality%2520and%2520identifies%2520the%2520redundant%2520parameters.%2520Specifically%252C%2520when%2520one%250Amodality%2520parameters%2520are%2520masked%2520%2528deactivated%2529%252C%2520the%2520absence%2520of%2520features%2520from%2520the%250Amasked%2520backbone%2520compels%2520the%2520model%2520to%2520reactivate%2520previous%2520redundant%2520features%2520of%250Athe%2520other%2520modality%2520backbone.%2520Therefore%252C%2520these%2520redundant%2520features%2520and%2520relevant%250Aredundant%2520parameters%2520can%2520be%2520identified%2520via%2520the%2520reactivation%2520process.%2520The%250Aredundant%2520parameters%2520can%2520be%2520pruned%2520by%2520our%2520proposed%2520importance%2520score%2520evaluation%250Afunction%252C%2520Alternative%2520Evaluation%2520%2528AlterEva%2529%252C%2520which%2520is%2520based%2520on%2520the%2520observation%250Aof%2520the%2520loss%2520changes%2520when%2520certain%2520modality%2520parameters%2520are%2520activated%2520and%250Adeactivated.%2520Extensive%2520experiments%2520on%2520the%2520nuScene%2520and%2520KITTI%2520datasets%250Aencompassing%2520diverse%2520tasks%252C%2520baseline%2520models%252C%2520and%2520pruning%2520algorithms%2520showcase%250Athat%2520AlterMOMA%2520outperforms%2520existing%2520pruning%2520methods%252C%2520attaining%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlterMOMA%3A%20Fusion%20Redundancy%20Pruning%20for%20Camera-LiDAR%20Fusion%20Models%20with%0A%20%20Alternative%20Modality%20Masking&entry.906535625=Shiqi%20Sun%20and%20Yantao%20Lu%20and%20Ning%20Liu%20and%20Bo%20Jiang%20and%20JinChao%20Chen%20and%20Ying%20Zhang&entry.1292438233=%20%20Camera-LiDAR%20fusion%20models%20significantly%20enhance%20perception%20performance%20in%0Aautonomous%20driving.%20The%20fusion%20mechanism%20leverages%20the%20strengths%20of%20each%0Amodality%20while%20minimizing%20their%20weaknesses.%20Moreover%2C%20in%20practice%2C%20camera-LiDAR%0Afusion%20models%20utilize%20pre-trained%20backbones%20for%20efficient%20training.%20However%2C%20we%0Aargue%20that%20directly%20loading%20single-modal%20pre-trained%20camera%20and%20LiDAR%20backbones%0Ainto%20camera-LiDAR%20fusion%20models%20introduces%20similar%20feature%20redundancy%20across%0Amodalities%20due%20to%20the%20nature%20of%20the%20fusion%20mechanism.%20Unfortunately%2C%20existing%0Apruning%20methods%20are%20developed%20explicitly%20for%20single-modal%20models%2C%20and%20thus%2C%0Athey%20struggle%20to%20effectively%20identify%20these%20specific%20redundant%20parameters%20in%0Acamera-LiDAR%20fusion%20models.%20In%20this%20paper%2C%20to%20address%20the%20issue%20above%20on%0Acamera-LiDAR%20fusion%20models%2C%20we%20propose%20a%20novelty%20pruning%20framework%20Alternative%0AModality%20Masking%20Pruning%20%28AlterMOMA%29%2C%20which%20employs%20alternative%20masking%20on%20each%0Amodality%20and%20identifies%20the%20redundant%20parameters.%20Specifically%2C%20when%20one%0Amodality%20parameters%20are%20masked%20%28deactivated%29%2C%20the%20absence%20of%20features%20from%20the%0Amasked%20backbone%20compels%20the%20model%20to%20reactivate%20previous%20redundant%20features%20of%0Athe%20other%20modality%20backbone.%20Therefore%2C%20these%20redundant%20features%20and%20relevant%0Aredundant%20parameters%20can%20be%20identified%20via%20the%20reactivation%20process.%20The%0Aredundant%20parameters%20can%20be%20pruned%20by%20our%20proposed%20importance%20score%20evaluation%0Afunction%2C%20Alternative%20Evaluation%20%28AlterEva%29%2C%20which%20is%20based%20on%20the%20observation%0Aof%20the%20loss%20changes%20when%20certain%20modality%20parameters%20are%20activated%20and%0Adeactivated.%20Extensive%20experiments%20on%20the%20nuScene%20and%20KITTI%20datasets%0Aencompassing%20diverse%20tasks%2C%20baseline%20models%2C%20and%20pruning%20algorithms%20showcase%0Athat%20AlterMOMA%20outperforms%20existing%20pruning%20methods%2C%20attaining%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17728v1&entry.124074799=Read"},
{"title": "SR-CurvANN: Advancing 3D Surface Reconstruction through Curvature-Aware\n  Neural Networks", "author": "Marina Hern\u00e1ndez-Bautista and Francisco J. Melero", "abstract": "  Incomplete or missing data in three-dimensional (3D) models can lead to\nerroneous or flawed renderings, limiting their usefulness in applications such\nas visualization, geometric computation, and 3D printing. Conventional\nsurface-repair techniques often fail to infer complex geometric details in\nmissing areas. Neural networks successfully address hole-filling tasks in 2D\nimages using inpainting techniques. The combination of surface reconstruction\nalgorithms, guided by the model's curvature properties and the creativity of\nneural networks in the inpainting processes should provide realistic results in\nthe hole completion task. In this paper, we propose a novel method entitled\nSR-CurvANN (Surface Reconstruction Based on Curvature-Aware Neural Networks)\nthat incorporates neural network-based 2D inpainting to effectively reconstruct\n3D surfaces. We train the neural networks with images that represent planar\nrepresentations of the curvature at vertices of hundreds of 3D models. Once the\nmissing areas have been inferred, a coarse-to-fine surface deformation process\nensures that the surface fits the reconstructed curvature image. Our proposal\nmakes it possible to learn and generalize patterns from a wide variety of\ntraining 3D models, generating comprehensive inpainted curvature images and\nsurfaces. Experiments conducted on 959 models with several holes have\ndemonstrated that SR-CurvANN excels in the shape completion process, filling\nholes with a remarkable level of realism and precision.\n", "link": "http://arxiv.org/abs/2407.17896v2", "date": "2024-09-26", "relevancy": 2.6795, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5485}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5485}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SR-CurvANN%3A%20Advancing%203D%20Surface%20Reconstruction%20through%20Curvature-Aware%0A%20%20Neural%20Networks&body=Title%3A%20SR-CurvANN%3A%20Advancing%203D%20Surface%20Reconstruction%20through%20Curvature-Aware%0A%20%20Neural%20Networks%0AAuthor%3A%20Marina%20Hern%C3%A1ndez-Bautista%20and%20Francisco%20J.%20Melero%0AAbstract%3A%20%20%20Incomplete%20or%20missing%20data%20in%20three-dimensional%20%283D%29%20models%20can%20lead%20to%0Aerroneous%20or%20flawed%20renderings%2C%20limiting%20their%20usefulness%20in%20applications%20such%0Aas%20visualization%2C%20geometric%20computation%2C%20and%203D%20printing.%20Conventional%0Asurface-repair%20techniques%20often%20fail%20to%20infer%20complex%20geometric%20details%20in%0Amissing%20areas.%20Neural%20networks%20successfully%20address%20hole-filling%20tasks%20in%202D%0Aimages%20using%20inpainting%20techniques.%20The%20combination%20of%20surface%20reconstruction%0Aalgorithms%2C%20guided%20by%20the%20model%27s%20curvature%20properties%20and%20the%20creativity%20of%0Aneural%20networks%20in%20the%20inpainting%20processes%20should%20provide%20realistic%20results%20in%0Athe%20hole%20completion%20task.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20entitled%0ASR-CurvANN%20%28Surface%20Reconstruction%20Based%20on%20Curvature-Aware%20Neural%20Networks%29%0Athat%20incorporates%20neural%20network-based%202D%20inpainting%20to%20effectively%20reconstruct%0A3D%20surfaces.%20We%20train%20the%20neural%20networks%20with%20images%20that%20represent%20planar%0Arepresentations%20of%20the%20curvature%20at%20vertices%20of%20hundreds%20of%203D%20models.%20Once%20the%0Amissing%20areas%20have%20been%20inferred%2C%20a%20coarse-to-fine%20surface%20deformation%20process%0Aensures%20that%20the%20surface%20fits%20the%20reconstructed%20curvature%20image.%20Our%20proposal%0Amakes%20it%20possible%20to%20learn%20and%20generalize%20patterns%20from%20a%20wide%20variety%20of%0Atraining%203D%20models%2C%20generating%20comprehensive%20inpainted%20curvature%20images%20and%0Asurfaces.%20Experiments%20conducted%20on%20959%20models%20with%20several%20holes%20have%0Ademonstrated%20that%20SR-CurvANN%20excels%20in%20the%20shape%20completion%20process%2C%20filling%0Aholes%20with%20a%20remarkable%20level%20of%20realism%20and%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSR-CurvANN%253A%2520Advancing%25203D%2520Surface%2520Reconstruction%2520through%2520Curvature-Aware%250A%2520%2520Neural%2520Networks%26entry.906535625%3DMarina%2520Hern%25C3%25A1ndez-Bautista%2520and%2520Francisco%2520J.%2520Melero%26entry.1292438233%3D%2520%2520Incomplete%2520or%2520missing%2520data%2520in%2520three-dimensional%2520%25283D%2529%2520models%2520can%2520lead%2520to%250Aerroneous%2520or%2520flawed%2520renderings%252C%2520limiting%2520their%2520usefulness%2520in%2520applications%2520such%250Aas%2520visualization%252C%2520geometric%2520computation%252C%2520and%25203D%2520printing.%2520Conventional%250Asurface-repair%2520techniques%2520often%2520fail%2520to%2520infer%2520complex%2520geometric%2520details%2520in%250Amissing%2520areas.%2520Neural%2520networks%2520successfully%2520address%2520hole-filling%2520tasks%2520in%25202D%250Aimages%2520using%2520inpainting%2520techniques.%2520The%2520combination%2520of%2520surface%2520reconstruction%250Aalgorithms%252C%2520guided%2520by%2520the%2520model%2527s%2520curvature%2520properties%2520and%2520the%2520creativity%2520of%250Aneural%2520networks%2520in%2520the%2520inpainting%2520processes%2520should%2520provide%2520realistic%2520results%2520in%250Athe%2520hole%2520completion%2520task.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520entitled%250ASR-CurvANN%2520%2528Surface%2520Reconstruction%2520Based%2520on%2520Curvature-Aware%2520Neural%2520Networks%2529%250Athat%2520incorporates%2520neural%2520network-based%25202D%2520inpainting%2520to%2520effectively%2520reconstruct%250A3D%2520surfaces.%2520We%2520train%2520the%2520neural%2520networks%2520with%2520images%2520that%2520represent%2520planar%250Arepresentations%2520of%2520the%2520curvature%2520at%2520vertices%2520of%2520hundreds%2520of%25203D%2520models.%2520Once%2520the%250Amissing%2520areas%2520have%2520been%2520inferred%252C%2520a%2520coarse-to-fine%2520surface%2520deformation%2520process%250Aensures%2520that%2520the%2520surface%2520fits%2520the%2520reconstructed%2520curvature%2520image.%2520Our%2520proposal%250Amakes%2520it%2520possible%2520to%2520learn%2520and%2520generalize%2520patterns%2520from%2520a%2520wide%2520variety%2520of%250Atraining%25203D%2520models%252C%2520generating%2520comprehensive%2520inpainted%2520curvature%2520images%2520and%250Asurfaces.%2520Experiments%2520conducted%2520on%2520959%2520models%2520with%2520several%2520holes%2520have%250Ademonstrated%2520that%2520SR-CurvANN%2520excels%2520in%2520the%2520shape%2520completion%2520process%252C%2520filling%250Aholes%2520with%2520a%2520remarkable%2520level%2520of%2520realism%2520and%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SR-CurvANN%3A%20Advancing%203D%20Surface%20Reconstruction%20through%20Curvature-Aware%0A%20%20Neural%20Networks&entry.906535625=Marina%20Hern%C3%A1ndez-Bautista%20and%20Francisco%20J.%20Melero&entry.1292438233=%20%20Incomplete%20or%20missing%20data%20in%20three-dimensional%20%283D%29%20models%20can%20lead%20to%0Aerroneous%20or%20flawed%20renderings%2C%20limiting%20their%20usefulness%20in%20applications%20such%0Aas%20visualization%2C%20geometric%20computation%2C%20and%203D%20printing.%20Conventional%0Asurface-repair%20techniques%20often%20fail%20to%20infer%20complex%20geometric%20details%20in%0Amissing%20areas.%20Neural%20networks%20successfully%20address%20hole-filling%20tasks%20in%202D%0Aimages%20using%20inpainting%20techniques.%20The%20combination%20of%20surface%20reconstruction%0Aalgorithms%2C%20guided%20by%20the%20model%27s%20curvature%20properties%20and%20the%20creativity%20of%0Aneural%20networks%20in%20the%20inpainting%20processes%20should%20provide%20realistic%20results%20in%0Athe%20hole%20completion%20task.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20entitled%0ASR-CurvANN%20%28Surface%20Reconstruction%20Based%20on%20Curvature-Aware%20Neural%20Networks%29%0Athat%20incorporates%20neural%20network-based%202D%20inpainting%20to%20effectively%20reconstruct%0A3D%20surfaces.%20We%20train%20the%20neural%20networks%20with%20images%20that%20represent%20planar%0Arepresentations%20of%20the%20curvature%20at%20vertices%20of%20hundreds%20of%203D%20models.%20Once%20the%0Amissing%20areas%20have%20been%20inferred%2C%20a%20coarse-to-fine%20surface%20deformation%20process%0Aensures%20that%20the%20surface%20fits%20the%20reconstructed%20curvature%20image.%20Our%20proposal%0Amakes%20it%20possible%20to%20learn%20and%20generalize%20patterns%20from%20a%20wide%20variety%20of%0Atraining%203D%20models%2C%20generating%20comprehensive%20inpainted%20curvature%20images%20and%0Asurfaces.%20Experiments%20conducted%20on%20959%20models%20with%20several%20holes%20have%0Ademonstrated%20that%20SR-CurvANN%20excels%20in%20the%20shape%20completion%20process%2C%20filling%0Aholes%20with%20a%20remarkable%20level%20of%20realism%20and%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17896v2&entry.124074799=Read"},
{"title": "Enhancing elusive clues in knowledge learning by contrasting attention\n  of language models", "author": "Jian Gao and Xiao Zhang and Ji Wu and Miao Li", "abstract": "  Causal language models acquire vast amount of knowledge from general text\ncorpus during pretraining, but the efficiency of knowledge learning is known to\nbe unsatisfactory, especially when learning from knowledge-dense and\nsmall-sized corpora. The deficiency can come from long-distance dependencies\nwhich are hard to capture by language models, and overfitting to co-occurrence\npatterns and distracting clues in the training text. To address these issues,\nthe paper proposes a method to enhance knowledge learning during language model\npretraining, by enhancing elusive but important clues in text discovered by the\nlanguage model themselves. We found that larger language models pay more\nattention to non-obvious but important clues, which are often overlooked by\nsmaller language models. Therefore, we can identify these clues by contrasting\nthe attention weights of large and small language models. We use the identified\nclues as a guide to perform token-dropout data augmentation on the training\ntext, and observed a significant boost in both small and large models'\nperformance in fact memorization. This shows that the behavior contrast between\nmore and less-performant language models contains important clues for knowledge\nlearning, and it can be ``amplified\" for a straight-forward improvement in\nknowledge learning efficiency.\n", "link": "http://arxiv.org/abs/2409.17954v1", "date": "2024-09-26", "relevancy": 2.6287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20elusive%20clues%20in%20knowledge%20learning%20by%20contrasting%20attention%0A%20%20of%20language%20models&body=Title%3A%20Enhancing%20elusive%20clues%20in%20knowledge%20learning%20by%20contrasting%20attention%0A%20%20of%20language%20models%0AAuthor%3A%20Jian%20Gao%20and%20Xiao%20Zhang%20and%20Ji%20Wu%20and%20Miao%20Li%0AAbstract%3A%20%20%20Causal%20language%20models%20acquire%20vast%20amount%20of%20knowledge%20from%20general%20text%0Acorpus%20during%20pretraining%2C%20but%20the%20efficiency%20of%20knowledge%20learning%20is%20known%20to%0Abe%20unsatisfactory%2C%20especially%20when%20learning%20from%20knowledge-dense%20and%0Asmall-sized%20corpora.%20The%20deficiency%20can%20come%20from%20long-distance%20dependencies%0Awhich%20are%20hard%20to%20capture%20by%20language%20models%2C%20and%20overfitting%20to%20co-occurrence%0Apatterns%20and%20distracting%20clues%20in%20the%20training%20text.%20To%20address%20these%20issues%2C%0Athe%20paper%20proposes%20a%20method%20to%20enhance%20knowledge%20learning%20during%20language%20model%0Apretraining%2C%20by%20enhancing%20elusive%20but%20important%20clues%20in%20text%20discovered%20by%20the%0Alanguage%20model%20themselves.%20We%20found%20that%20larger%20language%20models%20pay%20more%0Aattention%20to%20non-obvious%20but%20important%20clues%2C%20which%20are%20often%20overlooked%20by%0Asmaller%20language%20models.%20Therefore%2C%20we%20can%20identify%20these%20clues%20by%20contrasting%0Athe%20attention%20weights%20of%20large%20and%20small%20language%20models.%20We%20use%20the%20identified%0Aclues%20as%20a%20guide%20to%20perform%20token-dropout%20data%20augmentation%20on%20the%20training%0Atext%2C%20and%20observed%20a%20significant%20boost%20in%20both%20small%20and%20large%20models%27%0Aperformance%20in%20fact%20memorization.%20This%20shows%20that%20the%20behavior%20contrast%20between%0Amore%20and%20less-performant%20language%20models%20contains%20important%20clues%20for%20knowledge%0Alearning%2C%20and%20it%20can%20be%20%60%60amplified%22%20for%20a%20straight-forward%20improvement%20in%0Aknowledge%20learning%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520elusive%2520clues%2520in%2520knowledge%2520learning%2520by%2520contrasting%2520attention%250A%2520%2520of%2520language%2520models%26entry.906535625%3DJian%2520Gao%2520and%2520Xiao%2520Zhang%2520and%2520Ji%2520Wu%2520and%2520Miao%2520Li%26entry.1292438233%3D%2520%2520Causal%2520language%2520models%2520acquire%2520vast%2520amount%2520of%2520knowledge%2520from%2520general%2520text%250Acorpus%2520during%2520pretraining%252C%2520but%2520the%2520efficiency%2520of%2520knowledge%2520learning%2520is%2520known%2520to%250Abe%2520unsatisfactory%252C%2520especially%2520when%2520learning%2520from%2520knowledge-dense%2520and%250Asmall-sized%2520corpora.%2520The%2520deficiency%2520can%2520come%2520from%2520long-distance%2520dependencies%250Awhich%2520are%2520hard%2520to%2520capture%2520by%2520language%2520models%252C%2520and%2520overfitting%2520to%2520co-occurrence%250Apatterns%2520and%2520distracting%2520clues%2520in%2520the%2520training%2520text.%2520To%2520address%2520these%2520issues%252C%250Athe%2520paper%2520proposes%2520a%2520method%2520to%2520enhance%2520knowledge%2520learning%2520during%2520language%2520model%250Apretraining%252C%2520by%2520enhancing%2520elusive%2520but%2520important%2520clues%2520in%2520text%2520discovered%2520by%2520the%250Alanguage%2520model%2520themselves.%2520We%2520found%2520that%2520larger%2520language%2520models%2520pay%2520more%250Aattention%2520to%2520non-obvious%2520but%2520important%2520clues%252C%2520which%2520are%2520often%2520overlooked%2520by%250Asmaller%2520language%2520models.%2520Therefore%252C%2520we%2520can%2520identify%2520these%2520clues%2520by%2520contrasting%250Athe%2520attention%2520weights%2520of%2520large%2520and%2520small%2520language%2520models.%2520We%2520use%2520the%2520identified%250Aclues%2520as%2520a%2520guide%2520to%2520perform%2520token-dropout%2520data%2520augmentation%2520on%2520the%2520training%250Atext%252C%2520and%2520observed%2520a%2520significant%2520boost%2520in%2520both%2520small%2520and%2520large%2520models%2527%250Aperformance%2520in%2520fact%2520memorization.%2520This%2520shows%2520that%2520the%2520behavior%2520contrast%2520between%250Amore%2520and%2520less-performant%2520language%2520models%2520contains%2520important%2520clues%2520for%2520knowledge%250Alearning%252C%2520and%2520it%2520can%2520be%2520%2560%2560amplified%2522%2520for%2520a%2520straight-forward%2520improvement%2520in%250Aknowledge%2520learning%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20elusive%20clues%20in%20knowledge%20learning%20by%20contrasting%20attention%0A%20%20of%20language%20models&entry.906535625=Jian%20Gao%20and%20Xiao%20Zhang%20and%20Ji%20Wu%20and%20Miao%20Li&entry.1292438233=%20%20Causal%20language%20models%20acquire%20vast%20amount%20of%20knowledge%20from%20general%20text%0Acorpus%20during%20pretraining%2C%20but%20the%20efficiency%20of%20knowledge%20learning%20is%20known%20to%0Abe%20unsatisfactory%2C%20especially%20when%20learning%20from%20knowledge-dense%20and%0Asmall-sized%20corpora.%20The%20deficiency%20can%20come%20from%20long-distance%20dependencies%0Awhich%20are%20hard%20to%20capture%20by%20language%20models%2C%20and%20overfitting%20to%20co-occurrence%0Apatterns%20and%20distracting%20clues%20in%20the%20training%20text.%20To%20address%20these%20issues%2C%0Athe%20paper%20proposes%20a%20method%20to%20enhance%20knowledge%20learning%20during%20language%20model%0Apretraining%2C%20by%20enhancing%20elusive%20but%20important%20clues%20in%20text%20discovered%20by%20the%0Alanguage%20model%20themselves.%20We%20found%20that%20larger%20language%20models%20pay%20more%0Aattention%20to%20non-obvious%20but%20important%20clues%2C%20which%20are%20often%20overlooked%20by%0Asmaller%20language%20models.%20Therefore%2C%20we%20can%20identify%20these%20clues%20by%20contrasting%0Athe%20attention%20weights%20of%20large%20and%20small%20language%20models.%20We%20use%20the%20identified%0Aclues%20as%20a%20guide%20to%20perform%20token-dropout%20data%20augmentation%20on%20the%20training%0Atext%2C%20and%20observed%20a%20significant%20boost%20in%20both%20small%20and%20large%20models%27%0Aperformance%20in%20fact%20memorization.%20This%20shows%20that%20the%20behavior%20contrast%20between%0Amore%20and%20less-performant%20language%20models%20contains%20important%20clues%20for%20knowledge%0Alearning%2C%20and%20it%20can%20be%20%60%60amplified%22%20for%20a%20straight-forward%20improvement%20in%0Aknowledge%20learning%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17954v1&entry.124074799=Read"},
{"title": "AI-driven View Guidance System in Intra-cardiac Echocardiography Imaging", "author": "Jaeyoung Huh and Paul Klein and Gareth Funka-Lea and Puneet Sharma and Ankur Kapoor and Young-Ho Kim", "abstract": "  Intra-cardiac Echocardiography (ICE) is a crucial imaging modality used in\nelectrophysiology (EP) and structural heart disease (SHD) interventions,\nproviding real-time, high-resolution views from within the heart. Despite its\nadvantages, effective manipulation of the ICE catheter requires significant\nexpertise, which can lead to inconsistent outcomes, particularly among less\nexperienced operators. To address this challenge, we propose an AI-driven\nclosed-loop view guidance system with human-in-the-loop feedback, designed to\nassist users in navigating ICE imaging without requiring specialized knowledge.\nOur method models the relative position and orientation vectors between\narbitrary views and clinically defined ICE views in a spatial coordinate\nsystem, guiding users on how to manipulate the ICE catheter to transition from\nthe current view to the desired view over time. Operating in a closed-loop\nconfiguration, the system continuously predicts and updates the necessary\ncatheter manipulations, ensuring seamless integration into existing clinical\nworkflows. The effectiveness of the proposed system is demonstrated through a\nsimulation-based evaluation, achieving an 89% success rate with the 6532 test\ndataset, highlighting its potential to improve the accuracy and efficiency of\nICE imaging procedures.\n", "link": "http://arxiv.org/abs/2409.16898v2", "date": "2024-09-26", "relevancy": 2.6069, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5527}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5159}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging&body=Title%3A%20AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging%0AAuthor%3A%20Jaeyoung%20Huh%20and%20Paul%20Klein%20and%20Gareth%20Funka-Lea%20and%20Puneet%20Sharma%20and%20Ankur%20Kapoor%20and%20Young-Ho%20Kim%0AAbstract%3A%20%20%20Intra-cardiac%20Echocardiography%20%28ICE%29%20is%20a%20crucial%20imaging%20modality%20used%20in%0Aelectrophysiology%20%28EP%29%20and%20structural%20heart%20disease%20%28SHD%29%20interventions%2C%0Aproviding%20real-time%2C%20high-resolution%20views%20from%20within%20the%20heart.%20Despite%20its%0Aadvantages%2C%20effective%20manipulation%20of%20the%20ICE%20catheter%20requires%20significant%0Aexpertise%2C%20which%20can%20lead%20to%20inconsistent%20outcomes%2C%20particularly%20among%20less%0Aexperienced%20operators.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20AI-driven%0Aclosed-loop%20view%20guidance%20system%20with%20human-in-the-loop%20feedback%2C%20designed%20to%0Aassist%20users%20in%20navigating%20ICE%20imaging%20without%20requiring%20specialized%20knowledge.%0AOur%20method%20models%20the%20relative%20position%20and%20orientation%20vectors%20between%0Aarbitrary%20views%20and%20clinically%20defined%20ICE%20views%20in%20a%20spatial%20coordinate%0Asystem%2C%20guiding%20users%20on%20how%20to%20manipulate%20the%20ICE%20catheter%20to%20transition%20from%0Athe%20current%20view%20to%20the%20desired%20view%20over%20time.%20Operating%20in%20a%20closed-loop%0Aconfiguration%2C%20the%20system%20continuously%20predicts%20and%20updates%20the%20necessary%0Acatheter%20manipulations%2C%20ensuring%20seamless%20integration%20into%20existing%20clinical%0Aworkflows.%20The%20effectiveness%20of%20the%20proposed%20system%20is%20demonstrated%20through%20a%0Asimulation-based%20evaluation%2C%20achieving%20an%2089%25%20success%20rate%20with%20the%206532%20test%0Adataset%2C%20highlighting%20its%20potential%20to%20improve%20the%20accuracy%20and%20efficiency%20of%0AICE%20imaging%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16898v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-driven%2520View%2520Guidance%2520System%2520in%2520Intra-cardiac%2520Echocardiography%2520Imaging%26entry.906535625%3DJaeyoung%2520Huh%2520and%2520Paul%2520Klein%2520and%2520Gareth%2520Funka-Lea%2520and%2520Puneet%2520Sharma%2520and%2520Ankur%2520Kapoor%2520and%2520Young-Ho%2520Kim%26entry.1292438233%3D%2520%2520Intra-cardiac%2520Echocardiography%2520%2528ICE%2529%2520is%2520a%2520crucial%2520imaging%2520modality%2520used%2520in%250Aelectrophysiology%2520%2528EP%2529%2520and%2520structural%2520heart%2520disease%2520%2528SHD%2529%2520interventions%252C%250Aproviding%2520real-time%252C%2520high-resolution%2520views%2520from%2520within%2520the%2520heart.%2520Despite%2520its%250Aadvantages%252C%2520effective%2520manipulation%2520of%2520the%2520ICE%2520catheter%2520requires%2520significant%250Aexpertise%252C%2520which%2520can%2520lead%2520to%2520inconsistent%2520outcomes%252C%2520particularly%2520among%2520less%250Aexperienced%2520operators.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520AI-driven%250Aclosed-loop%2520view%2520guidance%2520system%2520with%2520human-in-the-loop%2520feedback%252C%2520designed%2520to%250Aassist%2520users%2520in%2520navigating%2520ICE%2520imaging%2520without%2520requiring%2520specialized%2520knowledge.%250AOur%2520method%2520models%2520the%2520relative%2520position%2520and%2520orientation%2520vectors%2520between%250Aarbitrary%2520views%2520and%2520clinically%2520defined%2520ICE%2520views%2520in%2520a%2520spatial%2520coordinate%250Asystem%252C%2520guiding%2520users%2520on%2520how%2520to%2520manipulate%2520the%2520ICE%2520catheter%2520to%2520transition%2520from%250Athe%2520current%2520view%2520to%2520the%2520desired%2520view%2520over%2520time.%2520Operating%2520in%2520a%2520closed-loop%250Aconfiguration%252C%2520the%2520system%2520continuously%2520predicts%2520and%2520updates%2520the%2520necessary%250Acatheter%2520manipulations%252C%2520ensuring%2520seamless%2520integration%2520into%2520existing%2520clinical%250Aworkflows.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520system%2520is%2520demonstrated%2520through%2520a%250Asimulation-based%2520evaluation%252C%2520achieving%2520an%252089%2525%2520success%2520rate%2520with%2520the%25206532%2520test%250Adataset%252C%2520highlighting%2520its%2520potential%2520to%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%250AICE%2520imaging%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16898v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging&entry.906535625=Jaeyoung%20Huh%20and%20Paul%20Klein%20and%20Gareth%20Funka-Lea%20and%20Puneet%20Sharma%20and%20Ankur%20Kapoor%20and%20Young-Ho%20Kim&entry.1292438233=%20%20Intra-cardiac%20Echocardiography%20%28ICE%29%20is%20a%20crucial%20imaging%20modality%20used%20in%0Aelectrophysiology%20%28EP%29%20and%20structural%20heart%20disease%20%28SHD%29%20interventions%2C%0Aproviding%20real-time%2C%20high-resolution%20views%20from%20within%20the%20heart.%20Despite%20its%0Aadvantages%2C%20effective%20manipulation%20of%20the%20ICE%20catheter%20requires%20significant%0Aexpertise%2C%20which%20can%20lead%20to%20inconsistent%20outcomes%2C%20particularly%20among%20less%0Aexperienced%20operators.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20AI-driven%0Aclosed-loop%20view%20guidance%20system%20with%20human-in-the-loop%20feedback%2C%20designed%20to%0Aassist%20users%20in%20navigating%20ICE%20imaging%20without%20requiring%20specialized%20knowledge.%0AOur%20method%20models%20the%20relative%20position%20and%20orientation%20vectors%20between%0Aarbitrary%20views%20and%20clinically%20defined%20ICE%20views%20in%20a%20spatial%20coordinate%0Asystem%2C%20guiding%20users%20on%20how%20to%20manipulate%20the%20ICE%20catheter%20to%20transition%20from%0Athe%20current%20view%20to%20the%20desired%20view%20over%20time.%20Operating%20in%20a%20closed-loop%0Aconfiguration%2C%20the%20system%20continuously%20predicts%20and%20updates%20the%20necessary%0Acatheter%20manipulations%2C%20ensuring%20seamless%20integration%20into%20existing%20clinical%0Aworkflows.%20The%20effectiveness%20of%20the%20proposed%20system%20is%20demonstrated%20through%20a%0Asimulation-based%20evaluation%2C%20achieving%20an%2089%25%20success%20rate%20with%20the%206532%20test%0Adataset%2C%20highlighting%20its%20potential%20to%20improve%20the%20accuracy%20and%20efficiency%20of%0AICE%20imaging%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16898v2&entry.124074799=Read"},
{"title": "Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine\n  Segmentation", "author": "Rob A. J. de Mooij and Josien P. W. Pluim and Cian M. Scannell", "abstract": "  Self-supervised pretraining (SSP) has shown promising results in learning\nfrom large unlabeled datasets and, thus, could be useful for automated\ncardiovascular magnetic resonance (CMR) short-axis cine segmentation. However,\ninconsistent reports of the benefits of SSP for segmentation have made it\ndifficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP\nmethods for CMR cine segmentation.\n  To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were\nused for unlabeled pretraining with four SSP methods; SimCLR, positional\ncontrastive learning, DINO, and masked image modeling (MIM). Subsets of varying\nnumbers of subjects were used for supervised fine-tuning of 2D models for each\nSSP method, as well as to train a 2D baseline model from scratch. The\nfine-tuned models were compared to the baseline using the 3D Dice similarity\ncoefficient (DSC) in a test dataset of 140 subjects.\n  The SSP methods showed no performance gains with the largest supervised\nfine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects\n(231 2D slices) are available for supervised training, SSP using MIM (DSC =\n0.86) improves over training from scratch (DSC = 0.82).\n  This study found that SSP is valuable for CMR cine segmentation when labeled\ntraining data is scarce, but does not aid state-of-the-art deep learning\nmethods when ample labeled data is available. Moreover, the choice of SSP\nmethod is important. The code is publicly available at:\nhttps://github.com/q-cardIA/ssp-cmr-cine-segmentation\n", "link": "http://arxiv.org/abs/2409.18100v1", "date": "2024-09-26", "relevancy": 2.6012, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5293}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Pretraining%20for%20Cardiovascular%20Magnetic%20Resonance%20Cine%0A%20%20Segmentation&body=Title%3A%20Self-supervised%20Pretraining%20for%20Cardiovascular%20Magnetic%20Resonance%20Cine%0A%20%20Segmentation%0AAuthor%3A%20Rob%20A.%20J.%20de%20Mooij%20and%20Josien%20P.%20W.%20Pluim%20and%20Cian%20M.%20Scannell%0AAbstract%3A%20%20%20Self-supervised%20pretraining%20%28SSP%29%20has%20shown%20promising%20results%20in%20learning%0Afrom%20large%20unlabeled%20datasets%20and%2C%20thus%2C%20could%20be%20useful%20for%20automated%0Acardiovascular%20magnetic%20resonance%20%28CMR%29%20short-axis%20cine%20segmentation.%20However%2C%0Ainconsistent%20reports%20of%20the%20benefits%20of%20SSP%20for%20segmentation%20have%20made%20it%0Adifficult%20to%20apply%20SSP%20to%20CMR.%20Therefore%2C%20this%20study%20aimed%20to%20evaluate%20SSP%0Amethods%20for%20CMR%20cine%20segmentation.%0A%20%20To%20this%20end%2C%20short-axis%20cine%20stacks%20of%20296%20subjects%20%2890618%202D%20slices%29%20were%0Aused%20for%20unlabeled%20pretraining%20with%20four%20SSP%20methods%3B%20SimCLR%2C%20positional%0Acontrastive%20learning%2C%20DINO%2C%20and%20masked%20image%20modeling%20%28MIM%29.%20Subsets%20of%20varying%0Anumbers%20of%20subjects%20were%20used%20for%20supervised%20fine-tuning%20of%202D%20models%20for%20each%0ASSP%20method%2C%20as%20well%20as%20to%20train%20a%202D%20baseline%20model%20from%20scratch.%20The%0Afine-tuned%20models%20were%20compared%20to%20the%20baseline%20using%20the%203D%20Dice%20similarity%0Acoefficient%20%28DSC%29%20in%20a%20test%20dataset%20of%20140%20subjects.%0A%20%20The%20SSP%20methods%20showed%20no%20performance%20gains%20with%20the%20largest%20supervised%0Afine-tuning%20subset%20compared%20to%20the%20baseline%20%28DSC%20%3D%200.89%29.%20When%20only%2010%20subjects%0A%28231%202D%20slices%29%20are%20available%20for%20supervised%20training%2C%20SSP%20using%20MIM%20%28DSC%20%3D%0A0.86%29%20improves%20over%20training%20from%20scratch%20%28DSC%20%3D%200.82%29.%0A%20%20This%20study%20found%20that%20SSP%20is%20valuable%20for%20CMR%20cine%20segmentation%20when%20labeled%0Atraining%20data%20is%20scarce%2C%20but%20does%20not%20aid%20state-of-the-art%20deep%20learning%0Amethods%20when%20ample%20labeled%20data%20is%20available.%20Moreover%2C%20the%20choice%20of%20SSP%0Amethod%20is%20important.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/q-cardIA/ssp-cmr-cine-segmentation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Pretraining%2520for%2520Cardiovascular%2520Magnetic%2520Resonance%2520Cine%250A%2520%2520Segmentation%26entry.906535625%3DRob%2520A.%2520J.%2520de%2520Mooij%2520and%2520Josien%2520P.%2520W.%2520Pluim%2520and%2520Cian%2520M.%2520Scannell%26entry.1292438233%3D%2520%2520Self-supervised%2520pretraining%2520%2528SSP%2529%2520has%2520shown%2520promising%2520results%2520in%2520learning%250Afrom%2520large%2520unlabeled%2520datasets%2520and%252C%2520thus%252C%2520could%2520be%2520useful%2520for%2520automated%250Acardiovascular%2520magnetic%2520resonance%2520%2528CMR%2529%2520short-axis%2520cine%2520segmentation.%2520However%252C%250Ainconsistent%2520reports%2520of%2520the%2520benefits%2520of%2520SSP%2520for%2520segmentation%2520have%2520made%2520it%250Adifficult%2520to%2520apply%2520SSP%2520to%2520CMR.%2520Therefore%252C%2520this%2520study%2520aimed%2520to%2520evaluate%2520SSP%250Amethods%2520for%2520CMR%2520cine%2520segmentation.%250A%2520%2520To%2520this%2520end%252C%2520short-axis%2520cine%2520stacks%2520of%2520296%2520subjects%2520%252890618%25202D%2520slices%2529%2520were%250Aused%2520for%2520unlabeled%2520pretraining%2520with%2520four%2520SSP%2520methods%253B%2520SimCLR%252C%2520positional%250Acontrastive%2520learning%252C%2520DINO%252C%2520and%2520masked%2520image%2520modeling%2520%2528MIM%2529.%2520Subsets%2520of%2520varying%250Anumbers%2520of%2520subjects%2520were%2520used%2520for%2520supervised%2520fine-tuning%2520of%25202D%2520models%2520for%2520each%250ASSP%2520method%252C%2520as%2520well%2520as%2520to%2520train%2520a%25202D%2520baseline%2520model%2520from%2520scratch.%2520The%250Afine-tuned%2520models%2520were%2520compared%2520to%2520the%2520baseline%2520using%2520the%25203D%2520Dice%2520similarity%250Acoefficient%2520%2528DSC%2529%2520in%2520a%2520test%2520dataset%2520of%2520140%2520subjects.%250A%2520%2520The%2520SSP%2520methods%2520showed%2520no%2520performance%2520gains%2520with%2520the%2520largest%2520supervised%250Afine-tuning%2520subset%2520compared%2520to%2520the%2520baseline%2520%2528DSC%2520%253D%25200.89%2529.%2520When%2520only%252010%2520subjects%250A%2528231%25202D%2520slices%2529%2520are%2520available%2520for%2520supervised%2520training%252C%2520SSP%2520using%2520MIM%2520%2528DSC%2520%253D%250A0.86%2529%2520improves%2520over%2520training%2520from%2520scratch%2520%2528DSC%2520%253D%25200.82%2529.%250A%2520%2520This%2520study%2520found%2520that%2520SSP%2520is%2520valuable%2520for%2520CMR%2520cine%2520segmentation%2520when%2520labeled%250Atraining%2520data%2520is%2520scarce%252C%2520but%2520does%2520not%2520aid%2520state-of-the-art%2520deep%2520learning%250Amethods%2520when%2520ample%2520labeled%2520data%2520is%2520available.%2520Moreover%252C%2520the%2520choice%2520of%2520SSP%250Amethod%2520is%2520important.%2520The%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/q-cardIA/ssp-cmr-cine-segmentation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Pretraining%20for%20Cardiovascular%20Magnetic%20Resonance%20Cine%0A%20%20Segmentation&entry.906535625=Rob%20A.%20J.%20de%20Mooij%20and%20Josien%20P.%20W.%20Pluim%20and%20Cian%20M.%20Scannell&entry.1292438233=%20%20Self-supervised%20pretraining%20%28SSP%29%20has%20shown%20promising%20results%20in%20learning%0Afrom%20large%20unlabeled%20datasets%20and%2C%20thus%2C%20could%20be%20useful%20for%20automated%0Acardiovascular%20magnetic%20resonance%20%28CMR%29%20short-axis%20cine%20segmentation.%20However%2C%0Ainconsistent%20reports%20of%20the%20benefits%20of%20SSP%20for%20segmentation%20have%20made%20it%0Adifficult%20to%20apply%20SSP%20to%20CMR.%20Therefore%2C%20this%20study%20aimed%20to%20evaluate%20SSP%0Amethods%20for%20CMR%20cine%20segmentation.%0A%20%20To%20this%20end%2C%20short-axis%20cine%20stacks%20of%20296%20subjects%20%2890618%202D%20slices%29%20were%0Aused%20for%20unlabeled%20pretraining%20with%20four%20SSP%20methods%3B%20SimCLR%2C%20positional%0Acontrastive%20learning%2C%20DINO%2C%20and%20masked%20image%20modeling%20%28MIM%29.%20Subsets%20of%20varying%0Anumbers%20of%20subjects%20were%20used%20for%20supervised%20fine-tuning%20of%202D%20models%20for%20each%0ASSP%20method%2C%20as%20well%20as%20to%20train%20a%202D%20baseline%20model%20from%20scratch.%20The%0Afine-tuned%20models%20were%20compared%20to%20the%20baseline%20using%20the%203D%20Dice%20similarity%0Acoefficient%20%28DSC%29%20in%20a%20test%20dataset%20of%20140%20subjects.%0A%20%20The%20SSP%20methods%20showed%20no%20performance%20gains%20with%20the%20largest%20supervised%0Afine-tuning%20subset%20compared%20to%20the%20baseline%20%28DSC%20%3D%200.89%29.%20When%20only%2010%20subjects%0A%28231%202D%20slices%29%20are%20available%20for%20supervised%20training%2C%20SSP%20using%20MIM%20%28DSC%20%3D%0A0.86%29%20improves%20over%20training%20from%20scratch%20%28DSC%20%3D%200.82%29.%0A%20%20This%20study%20found%20that%20SSP%20is%20valuable%20for%20CMR%20cine%20segmentation%20when%20labeled%0Atraining%20data%20is%20scarce%2C%20but%20does%20not%20aid%20state-of-the-art%20deep%20learning%0Amethods%20when%20ample%20labeled%20data%20is%20available.%20Moreover%2C%20the%20choice%20of%20SSP%0Amethod%20is%20important.%20The%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/q-cardIA/ssp-cmr-cine-segmentation%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18100v1&entry.124074799=Read"},
{"title": "Language-Embedded Gaussian Splats (LEGS): Incrementally Building\n  Room-Scale Representations with a Mobile Robot", "author": "Justin Yu and Kush Hari and Kishore Srinivas and Karim El-Refai and Adam Rashid and Chung Min Kim and Justin Kerr and Richard Cheng and Muhammad Zubair Irshad and Ashwin Balakrishna and Thomas Kollar and Ken Goldberg", "abstract": "  Building semantic 3D maps is valuable for searching for objects of interest\nin offices, warehouses, stores, and homes. We present a mapping system that\nincrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D\nscene representation that encodes both appearance and semantics in a unified\nrepresentation. LEGS is trained online as a robot traverses its environment to\nenable localization of open-vocabulary object queries. We evaluate LEGS on 4\nroom-scale scenes where we query for objects in the scene to assess how LEGS\ncan capture semantic meaning. We compare LEGS to LERF and find that while both\nsystems have comparable object query success rates, LEGS trains over 3.5x\nfaster than LERF. Results suggest that a multi-camera setup and incremental\nbundle adjustment can boost visual reconstruction quality in constrained robot\ntrajectories, and suggest LEGS can localize open-vocabulary and long-tail\nobject queries with up to 66% accuracy.\n", "link": "http://arxiv.org/abs/2409.18108v1", "date": "2024-09-26", "relevancy": 2.5907, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6161}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Embedded%20Gaussian%20Splats%20%28LEGS%29%3A%20Incrementally%20Building%0A%20%20Room-Scale%20Representations%20with%20a%20Mobile%20Robot&body=Title%3A%20Language-Embedded%20Gaussian%20Splats%20%28LEGS%29%3A%20Incrementally%20Building%0A%20%20Room-Scale%20Representations%20with%20a%20Mobile%20Robot%0AAuthor%3A%20Justin%20Yu%20and%20Kush%20Hari%20and%20Kishore%20Srinivas%20and%20Karim%20El-Refai%20and%20Adam%20Rashid%20and%20Chung%20Min%20Kim%20and%20Justin%20Kerr%20and%20Richard%20Cheng%20and%20Muhammad%20Zubair%20Irshad%20and%20Ashwin%20Balakrishna%20and%20Thomas%20Kollar%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20Building%20semantic%203D%20maps%20is%20valuable%20for%20searching%20for%20objects%20of%20interest%0Ain%20offices%2C%20warehouses%2C%20stores%2C%20and%20homes.%20We%20present%20a%20mapping%20system%20that%0Aincrementally%20builds%20a%20Language-Embedded%20Gaussian%20Splat%20%28LEGS%29%3A%20a%20detailed%203D%0Ascene%20representation%20that%20encodes%20both%20appearance%20and%20semantics%20in%20a%20unified%0Arepresentation.%20LEGS%20is%20trained%20online%20as%20a%20robot%20traverses%20its%20environment%20to%0Aenable%20localization%20of%20open-vocabulary%20object%20queries.%20We%20evaluate%20LEGS%20on%204%0Aroom-scale%20scenes%20where%20we%20query%20for%20objects%20in%20the%20scene%20to%20assess%20how%20LEGS%0Acan%20capture%20semantic%20meaning.%20We%20compare%20LEGS%20to%20LERF%20and%20find%20that%20while%20both%0Asystems%20have%20comparable%20object%20query%20success%20rates%2C%20LEGS%20trains%20over%203.5x%0Afaster%20than%20LERF.%20Results%20suggest%20that%20a%20multi-camera%20setup%20and%20incremental%0Abundle%20adjustment%20can%20boost%20visual%20reconstruction%20quality%20in%20constrained%20robot%0Atrajectories%2C%20and%20suggest%20LEGS%20can%20localize%20open-vocabulary%20and%20long-tail%0Aobject%20queries%20with%20up%20to%2066%25%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Embedded%2520Gaussian%2520Splats%2520%2528LEGS%2529%253A%2520Incrementally%2520Building%250A%2520%2520Room-Scale%2520Representations%2520with%2520a%2520Mobile%2520Robot%26entry.906535625%3DJustin%2520Yu%2520and%2520Kush%2520Hari%2520and%2520Kishore%2520Srinivas%2520and%2520Karim%2520El-Refai%2520and%2520Adam%2520Rashid%2520and%2520Chung%2520Min%2520Kim%2520and%2520Justin%2520Kerr%2520and%2520Richard%2520Cheng%2520and%2520Muhammad%2520Zubair%2520Irshad%2520and%2520Ashwin%2520Balakrishna%2520and%2520Thomas%2520Kollar%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520Building%2520semantic%25203D%2520maps%2520is%2520valuable%2520for%2520searching%2520for%2520objects%2520of%2520interest%250Ain%2520offices%252C%2520warehouses%252C%2520stores%252C%2520and%2520homes.%2520We%2520present%2520a%2520mapping%2520system%2520that%250Aincrementally%2520builds%2520a%2520Language-Embedded%2520Gaussian%2520Splat%2520%2528LEGS%2529%253A%2520a%2520detailed%25203D%250Ascene%2520representation%2520that%2520encodes%2520both%2520appearance%2520and%2520semantics%2520in%2520a%2520unified%250Arepresentation.%2520LEGS%2520is%2520trained%2520online%2520as%2520a%2520robot%2520traverses%2520its%2520environment%2520to%250Aenable%2520localization%2520of%2520open-vocabulary%2520object%2520queries.%2520We%2520evaluate%2520LEGS%2520on%25204%250Aroom-scale%2520scenes%2520where%2520we%2520query%2520for%2520objects%2520in%2520the%2520scene%2520to%2520assess%2520how%2520LEGS%250Acan%2520capture%2520semantic%2520meaning.%2520We%2520compare%2520LEGS%2520to%2520LERF%2520and%2520find%2520that%2520while%2520both%250Asystems%2520have%2520comparable%2520object%2520query%2520success%2520rates%252C%2520LEGS%2520trains%2520over%25203.5x%250Afaster%2520than%2520LERF.%2520Results%2520suggest%2520that%2520a%2520multi-camera%2520setup%2520and%2520incremental%250Abundle%2520adjustment%2520can%2520boost%2520visual%2520reconstruction%2520quality%2520in%2520constrained%2520robot%250Atrajectories%252C%2520and%2520suggest%2520LEGS%2520can%2520localize%2520open-vocabulary%2520and%2520long-tail%250Aobject%2520queries%2520with%2520up%2520to%252066%2525%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Embedded%20Gaussian%20Splats%20%28LEGS%29%3A%20Incrementally%20Building%0A%20%20Room-Scale%20Representations%20with%20a%20Mobile%20Robot&entry.906535625=Justin%20Yu%20and%20Kush%20Hari%20and%20Kishore%20Srinivas%20and%20Karim%20El-Refai%20and%20Adam%20Rashid%20and%20Chung%20Min%20Kim%20and%20Justin%20Kerr%20and%20Richard%20Cheng%20and%20Muhammad%20Zubair%20Irshad%20and%20Ashwin%20Balakrishna%20and%20Thomas%20Kollar%20and%20Ken%20Goldberg&entry.1292438233=%20%20Building%20semantic%203D%20maps%20is%20valuable%20for%20searching%20for%20objects%20of%20interest%0Ain%20offices%2C%20warehouses%2C%20stores%2C%20and%20homes.%20We%20present%20a%20mapping%20system%20that%0Aincrementally%20builds%20a%20Language-Embedded%20Gaussian%20Splat%20%28LEGS%29%3A%20a%20detailed%203D%0Ascene%20representation%20that%20encodes%20both%20appearance%20and%20semantics%20in%20a%20unified%0Arepresentation.%20LEGS%20is%20trained%20online%20as%20a%20robot%20traverses%20its%20environment%20to%0Aenable%20localization%20of%20open-vocabulary%20object%20queries.%20We%20evaluate%20LEGS%20on%204%0Aroom-scale%20scenes%20where%20we%20query%20for%20objects%20in%20the%20scene%20to%20assess%20how%20LEGS%0Acan%20capture%20semantic%20meaning.%20We%20compare%20LEGS%20to%20LERF%20and%20find%20that%20while%20both%0Asystems%20have%20comparable%20object%20query%20success%20rates%2C%20LEGS%20trains%20over%203.5x%0Afaster%20than%20LERF.%20Results%20suggest%20that%20a%20multi-camera%20setup%20and%20incremental%0Abundle%20adjustment%20can%20boost%20visual%20reconstruction%20quality%20in%20constrained%20robot%0Atrajectories%2C%20and%20suggest%20LEGS%20can%20localize%20open-vocabulary%20and%20long-tail%0Aobject%20queries%20with%20up%20to%2066%25%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18108v1&entry.124074799=Read"},
{"title": "Spatiotemporal Learning on Cell-embedded Graphs", "author": "Yuan Mi and Hao Sun", "abstract": "  Data-driven simulation of physical systems has recently kindled significant\nattention, where many neural models have been developed. In particular,\nmesh-based graph neural networks (GNNs) have demonstrated significant potential\nin predicting spatiotemporal dynamics across arbitrary geometric domains.\nHowever, the existing node-edge message passing mechanism in GNNs limits the\nmodel's representation learning ability. In this paper, we proposed a\ncell-embedded GNN model (aka CeGNN) to learn spatiotemporal dynamics with\nlifted performance. Specifically, we introduce a learnable cell attribution to\nthe node-edge message passing process, which better captures the spatial\ndependency of regional features. Such a strategy essentially upgrades the local\naggregation scheme from the first order (e.g., from edge to node) to a higher\norder (e.g., from volume to edge and then to node), which takes advantage of\nvolumetric information in message passing. Meanwhile, a novel feature-enhanced\nblock is designed to further improve the performance of CeGNN and relieve the\nover-smoothness problem, via treating the latent features as basis functions.\nThe extensive experiments on various PDE systems and one real-world dataset\ndemonstrate that CeGNN achieves superior performance compared with other\nbaseline models, particularly reducing the prediction error with up to 1 orders\nof magnitude on several PDE systems.\n", "link": "http://arxiv.org/abs/2409.18013v1", "date": "2024-09-26", "relevancy": 2.5907, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5337}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5255}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatiotemporal%20Learning%20on%20Cell-embedded%20Graphs&body=Title%3A%20Spatiotemporal%20Learning%20on%20Cell-embedded%20Graphs%0AAuthor%3A%20Yuan%20Mi%20and%20Hao%20Sun%0AAbstract%3A%20%20%20Data-driven%20simulation%20of%20physical%20systems%20has%20recently%20kindled%20significant%0Aattention%2C%20where%20many%20neural%20models%20have%20been%20developed.%20In%20particular%2C%0Amesh-based%20graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20significant%20potential%0Ain%20predicting%20spatiotemporal%20dynamics%20across%20arbitrary%20geometric%20domains.%0AHowever%2C%20the%20existing%20node-edge%20message%20passing%20mechanism%20in%20GNNs%20limits%20the%0Amodel%27s%20representation%20learning%20ability.%20In%20this%20paper%2C%20we%20proposed%20a%0Acell-embedded%20GNN%20model%20%28aka%20CeGNN%29%20to%20learn%20spatiotemporal%20dynamics%20with%0Alifted%20performance.%20Specifically%2C%20we%20introduce%20a%20learnable%20cell%20attribution%20to%0Athe%20node-edge%20message%20passing%20process%2C%20which%20better%20captures%20the%20spatial%0Adependency%20of%20regional%20features.%20Such%20a%20strategy%20essentially%20upgrades%20the%20local%0Aaggregation%20scheme%20from%20the%20first%20order%20%28e.g.%2C%20from%20edge%20to%20node%29%20to%20a%20higher%0Aorder%20%28e.g.%2C%20from%20volume%20to%20edge%20and%20then%20to%20node%29%2C%20which%20takes%20advantage%20of%0Avolumetric%20information%20in%20message%20passing.%20Meanwhile%2C%20a%20novel%20feature-enhanced%0Ablock%20is%20designed%20to%20further%20improve%20the%20performance%20of%20CeGNN%20and%20relieve%20the%0Aover-smoothness%20problem%2C%20via%20treating%20the%20latent%20features%20as%20basis%20functions.%0AThe%20extensive%20experiments%20on%20various%20PDE%20systems%20and%20one%20real-world%20dataset%0Ademonstrate%20that%20CeGNN%20achieves%20superior%20performance%20compared%20with%20other%0Abaseline%20models%2C%20particularly%20reducing%20the%20prediction%20error%20with%20up%20to%201%20orders%0Aof%20magnitude%20on%20several%20PDE%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatiotemporal%2520Learning%2520on%2520Cell-embedded%2520Graphs%26entry.906535625%3DYuan%2520Mi%2520and%2520Hao%2520Sun%26entry.1292438233%3D%2520%2520Data-driven%2520simulation%2520of%2520physical%2520systems%2520has%2520recently%2520kindled%2520significant%250Aattention%252C%2520where%2520many%2520neural%2520models%2520have%2520been%2520developed.%2520In%2520particular%252C%250Amesh-based%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520significant%2520potential%250Ain%2520predicting%2520spatiotemporal%2520dynamics%2520across%2520arbitrary%2520geometric%2520domains.%250AHowever%252C%2520the%2520existing%2520node-edge%2520message%2520passing%2520mechanism%2520in%2520GNNs%2520limits%2520the%250Amodel%2527s%2520representation%2520learning%2520ability.%2520In%2520this%2520paper%252C%2520we%2520proposed%2520a%250Acell-embedded%2520GNN%2520model%2520%2528aka%2520CeGNN%2529%2520to%2520learn%2520spatiotemporal%2520dynamics%2520with%250Alifted%2520performance.%2520Specifically%252C%2520we%2520introduce%2520a%2520learnable%2520cell%2520attribution%2520to%250Athe%2520node-edge%2520message%2520passing%2520process%252C%2520which%2520better%2520captures%2520the%2520spatial%250Adependency%2520of%2520regional%2520features.%2520Such%2520a%2520strategy%2520essentially%2520upgrades%2520the%2520local%250Aaggregation%2520scheme%2520from%2520the%2520first%2520order%2520%2528e.g.%252C%2520from%2520edge%2520to%2520node%2529%2520to%2520a%2520higher%250Aorder%2520%2528e.g.%252C%2520from%2520volume%2520to%2520edge%2520and%2520then%2520to%2520node%2529%252C%2520which%2520takes%2520advantage%2520of%250Avolumetric%2520information%2520in%2520message%2520passing.%2520Meanwhile%252C%2520a%2520novel%2520feature-enhanced%250Ablock%2520is%2520designed%2520to%2520further%2520improve%2520the%2520performance%2520of%2520CeGNN%2520and%2520relieve%2520the%250Aover-smoothness%2520problem%252C%2520via%2520treating%2520the%2520latent%2520features%2520as%2520basis%2520functions.%250AThe%2520extensive%2520experiments%2520on%2520various%2520PDE%2520systems%2520and%2520one%2520real-world%2520dataset%250Ademonstrate%2520that%2520CeGNN%2520achieves%2520superior%2520performance%2520compared%2520with%2520other%250Abaseline%2520models%252C%2520particularly%2520reducing%2520the%2520prediction%2520error%2520with%2520up%2520to%25201%2520orders%250Aof%2520magnitude%2520on%2520several%2520PDE%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20Learning%20on%20Cell-embedded%20Graphs&entry.906535625=Yuan%20Mi%20and%20Hao%20Sun&entry.1292438233=%20%20Data-driven%20simulation%20of%20physical%20systems%20has%20recently%20kindled%20significant%0Aattention%2C%20where%20many%20neural%20models%20have%20been%20developed.%20In%20particular%2C%0Amesh-based%20graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20significant%20potential%0Ain%20predicting%20spatiotemporal%20dynamics%20across%20arbitrary%20geometric%20domains.%0AHowever%2C%20the%20existing%20node-edge%20message%20passing%20mechanism%20in%20GNNs%20limits%20the%0Amodel%27s%20representation%20learning%20ability.%20In%20this%20paper%2C%20we%20proposed%20a%0Acell-embedded%20GNN%20model%20%28aka%20CeGNN%29%20to%20learn%20spatiotemporal%20dynamics%20with%0Alifted%20performance.%20Specifically%2C%20we%20introduce%20a%20learnable%20cell%20attribution%20to%0Athe%20node-edge%20message%20passing%20process%2C%20which%20better%20captures%20the%20spatial%0Adependency%20of%20regional%20features.%20Such%20a%20strategy%20essentially%20upgrades%20the%20local%0Aaggregation%20scheme%20from%20the%20first%20order%20%28e.g.%2C%20from%20edge%20to%20node%29%20to%20a%20higher%0Aorder%20%28e.g.%2C%20from%20volume%20to%20edge%20and%20then%20to%20node%29%2C%20which%20takes%20advantage%20of%0Avolumetric%20information%20in%20message%20passing.%20Meanwhile%2C%20a%20novel%20feature-enhanced%0Ablock%20is%20designed%20to%20further%20improve%20the%20performance%20of%20CeGNN%20and%20relieve%20the%0Aover-smoothness%20problem%2C%20via%20treating%20the%20latent%20features%20as%20basis%20functions.%0AThe%20extensive%20experiments%20on%20various%20PDE%20systems%20and%20one%20real-world%20dataset%0Ademonstrate%20that%20CeGNN%20achieves%20superior%20performance%20compared%20with%20other%0Abaseline%20models%2C%20particularly%20reducing%20the%20prediction%20error%20with%20up%20to%201%20orders%0Aof%20magnitude%20on%20several%20PDE%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18013v1&entry.124074799=Read"},
{"title": "Recent advances in interpretable machine learning using structure-based\n  protein representations", "author": "Luiz Felipe Vecchietti and Minji Lee and Begench Hangeldiyev and Hyunkyu Jung and Hahnbeom Park and Tae-Kyun Kim and Meeyoung Cha and Ho Min Kim", "abstract": "  Recent advancements in machine learning (ML) are transforming the field of\nstructural biology. For example, AlphaFold, a groundbreaking neural network for\nprotein structure prediction, has been widely adopted by researchers. The\navailability of easy-to-use interfaces and interpretable outcomes from the\nneural network architecture, such as the confidence scores used to color the\npredicted structures, have made AlphaFold accessible even to non-ML experts. In\nthis paper, we present various methods for representing protein 3D structures\nfrom low- to high-resolution, and show how interpretable ML methods can support\ntasks such as predicting protein structures, protein function, and\nprotein-protein interactions. This survey also emphasizes the significance of\ninterpreting and visualizing ML-based inference for structure-based protein\nrepresentations that enhance interpretability and knowledge discovery.\nDeveloping such interpretable approaches promises to further accelerate fields\nincluding drug development and protein design.\n", "link": "http://arxiv.org/abs/2409.17726v1", "date": "2024-09-26", "relevancy": 2.5629, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20advances%20in%20interpretable%20machine%20learning%20using%20structure-based%0A%20%20protein%20representations&body=Title%3A%20Recent%20advances%20in%20interpretable%20machine%20learning%20using%20structure-based%0A%20%20protein%20representations%0AAuthor%3A%20Luiz%20Felipe%20Vecchietti%20and%20Minji%20Lee%20and%20Begench%20Hangeldiyev%20and%20Hyunkyu%20Jung%20and%20Hahnbeom%20Park%20and%20Tae-Kyun%20Kim%20and%20Meeyoung%20Cha%20and%20Ho%20Min%20Kim%0AAbstract%3A%20%20%20Recent%20advancements%20in%20machine%20learning%20%28ML%29%20are%20transforming%20the%20field%20of%0Astructural%20biology.%20For%20example%2C%20AlphaFold%2C%20a%20groundbreaking%20neural%20network%20for%0Aprotein%20structure%20prediction%2C%20has%20been%20widely%20adopted%20by%20researchers.%20The%0Aavailability%20of%20easy-to-use%20interfaces%20and%20interpretable%20outcomes%20from%20the%0Aneural%20network%20architecture%2C%20such%20as%20the%20confidence%20scores%20used%20to%20color%20the%0Apredicted%20structures%2C%20have%20made%20AlphaFold%20accessible%20even%20to%20non-ML%20experts.%20In%0Athis%20paper%2C%20we%20present%20various%20methods%20for%20representing%20protein%203D%20structures%0Afrom%20low-%20to%20high-resolution%2C%20and%20show%20how%20interpretable%20ML%20methods%20can%20support%0Atasks%20such%20as%20predicting%20protein%20structures%2C%20protein%20function%2C%20and%0Aprotein-protein%20interactions.%20This%20survey%20also%20emphasizes%20the%20significance%20of%0Ainterpreting%20and%20visualizing%20ML-based%20inference%20for%20structure-based%20protein%0Arepresentations%20that%20enhance%20interpretability%20and%20knowledge%20discovery.%0ADeveloping%20such%20interpretable%20approaches%20promises%20to%20further%20accelerate%20fields%0Aincluding%20drug%20development%20and%20protein%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520advances%2520in%2520interpretable%2520machine%2520learning%2520using%2520structure-based%250A%2520%2520protein%2520representations%26entry.906535625%3DLuiz%2520Felipe%2520Vecchietti%2520and%2520Minji%2520Lee%2520and%2520Begench%2520Hangeldiyev%2520and%2520Hyunkyu%2520Jung%2520and%2520Hahnbeom%2520Park%2520and%2520Tae-Kyun%2520Kim%2520and%2520Meeyoung%2520Cha%2520and%2520Ho%2520Min%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520machine%2520learning%2520%2528ML%2529%2520are%2520transforming%2520the%2520field%2520of%250Astructural%2520biology.%2520For%2520example%252C%2520AlphaFold%252C%2520a%2520groundbreaking%2520neural%2520network%2520for%250Aprotein%2520structure%2520prediction%252C%2520has%2520been%2520widely%2520adopted%2520by%2520researchers.%2520The%250Aavailability%2520of%2520easy-to-use%2520interfaces%2520and%2520interpretable%2520outcomes%2520from%2520the%250Aneural%2520network%2520architecture%252C%2520such%2520as%2520the%2520confidence%2520scores%2520used%2520to%2520color%2520the%250Apredicted%2520structures%252C%2520have%2520made%2520AlphaFold%2520accessible%2520even%2520to%2520non-ML%2520experts.%2520In%250Athis%2520paper%252C%2520we%2520present%2520various%2520methods%2520for%2520representing%2520protein%25203D%2520structures%250Afrom%2520low-%2520to%2520high-resolution%252C%2520and%2520show%2520how%2520interpretable%2520ML%2520methods%2520can%2520support%250Atasks%2520such%2520as%2520predicting%2520protein%2520structures%252C%2520protein%2520function%252C%2520and%250Aprotein-protein%2520interactions.%2520This%2520survey%2520also%2520emphasizes%2520the%2520significance%2520of%250Ainterpreting%2520and%2520visualizing%2520ML-based%2520inference%2520for%2520structure-based%2520protein%250Arepresentations%2520that%2520enhance%2520interpretability%2520and%2520knowledge%2520discovery.%250ADeveloping%2520such%2520interpretable%2520approaches%2520promises%2520to%2520further%2520accelerate%2520fields%250Aincluding%2520drug%2520development%2520and%2520protein%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20advances%20in%20interpretable%20machine%20learning%20using%20structure-based%0A%20%20protein%20representations&entry.906535625=Luiz%20Felipe%20Vecchietti%20and%20Minji%20Lee%20and%20Begench%20Hangeldiyev%20and%20Hyunkyu%20Jung%20and%20Hahnbeom%20Park%20and%20Tae-Kyun%20Kim%20and%20Meeyoung%20Cha%20and%20Ho%20Min%20Kim&entry.1292438233=%20%20Recent%20advancements%20in%20machine%20learning%20%28ML%29%20are%20transforming%20the%20field%20of%0Astructural%20biology.%20For%20example%2C%20AlphaFold%2C%20a%20groundbreaking%20neural%20network%20for%0Aprotein%20structure%20prediction%2C%20has%20been%20widely%20adopted%20by%20researchers.%20The%0Aavailability%20of%20easy-to-use%20interfaces%20and%20interpretable%20outcomes%20from%20the%0Aneural%20network%20architecture%2C%20such%20as%20the%20confidence%20scores%20used%20to%20color%20the%0Apredicted%20structures%2C%20have%20made%20AlphaFold%20accessible%20even%20to%20non-ML%20experts.%20In%0Athis%20paper%2C%20we%20present%20various%20methods%20for%20representing%20protein%203D%20structures%0Afrom%20low-%20to%20high-resolution%2C%20and%20show%20how%20interpretable%20ML%20methods%20can%20support%0Atasks%20such%20as%20predicting%20protein%20structures%2C%20protein%20function%2C%20and%0Aprotein-protein%20interactions.%20This%20survey%20also%20emphasizes%20the%20significance%20of%0Ainterpreting%20and%20visualizing%20ML-based%20inference%20for%20structure-based%20protein%0Arepresentations%20that%20enhance%20interpretability%20and%20knowledge%20discovery.%0ADeveloping%20such%20interpretable%20approaches%20promises%20to%20further%20accelerate%20fields%0Aincluding%20drug%20development%20and%20protein%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17726v1&entry.124074799=Read"},
{"title": "Cascade Prompt Learning for Vision-Language Model Adaptation", "author": "Ge Wu and Xin Zhang and Zheng Li and Zhaowei Chen and Jiajun Liang and Jian Yang and Xiang Li", "abstract": "  Prompt learning has surfaced as an effective approach to enhance the\nperformance of Vision-Language Models (VLMs) like CLIP when applied to\ndownstream tasks. However, current learnable prompt tokens are primarily used\nfor the single phase of adapting to tasks (i.e., adapting prompt), easily\nleading to overfitting risks. In this work, we propose a novel Cascade Prompt\nLearning CasPL framework to enable prompt learning to serve both generic and\nspecific expertise (i.e., boosting and adapting prompt) simultaneously.\nSpecifically, CasPL is a new learning paradigm comprising two distinct phases\nof learnable prompts: the first boosting prompt is crafted to extract\ndomain-general knowledge from a senior larger CLIP teacher model by aligning\ntheir predicted logits using extensive unlabeled domain images. The second\nadapting prompt is then cascaded with the frozen first set to fine-tune the\ndownstream tasks, following the approaches employed in prior research. In this\nmanner, CasPL can effectively capture both domain-general and task-specific\nrepresentations into explicitly different gradual groups of prompts, thus\npotentially alleviating overfitting issues in the target domain. It's worth\nnoting that CasPL serves as a plug-and-play module that can seamlessly\nintegrate into any existing prompt learning approach. CasPL achieves a\nsignificantly better balance between performance and inference speed, which is\nespecially beneficial for deploying smaller VLM models in resource-constrained\nenvironments. Compared to the previous state-of-the-art method PromptSRC, CasPL\nshows an average improvement of 1.85% for base classes, 3.44% for novel\nclasses, and 2.72% for the harmonic mean over 11 image classification datasets.\nCode is publicly available at: https://github.com/megvii-research/CasPL.\n", "link": "http://arxiv.org/abs/2409.17805v1", "date": "2024-09-26", "relevancy": 2.5564, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascade%20Prompt%20Learning%20for%20Vision-Language%20Model%20Adaptation&body=Title%3A%20Cascade%20Prompt%20Learning%20for%20Vision-Language%20Model%20Adaptation%0AAuthor%3A%20Ge%20Wu%20and%20Xin%20Zhang%20and%20Zheng%20Li%20and%20Zhaowei%20Chen%20and%20Jiajun%20Liang%20and%20Jian%20Yang%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Prompt%20learning%20has%20surfaced%20as%20an%20effective%20approach%20to%20enhance%20the%0Aperformance%20of%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20when%20applied%20to%0Adownstream%20tasks.%20However%2C%20current%20learnable%20prompt%20tokens%20are%20primarily%20used%0Afor%20the%20single%20phase%20of%20adapting%20to%20tasks%20%28i.e.%2C%20adapting%20prompt%29%2C%20easily%0Aleading%20to%20overfitting%20risks.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Cascade%20Prompt%0ALearning%20CasPL%20framework%20to%20enable%20prompt%20learning%20to%20serve%20both%20generic%20and%0Aspecific%20expertise%20%28i.e.%2C%20boosting%20and%20adapting%20prompt%29%20simultaneously.%0ASpecifically%2C%20CasPL%20is%20a%20new%20learning%20paradigm%20comprising%20two%20distinct%20phases%0Aof%20learnable%20prompts%3A%20the%20first%20boosting%20prompt%20is%20crafted%20to%20extract%0Adomain-general%20knowledge%20from%20a%20senior%20larger%20CLIP%20teacher%20model%20by%20aligning%0Atheir%20predicted%20logits%20using%20extensive%20unlabeled%20domain%20images.%20The%20second%0Aadapting%20prompt%20is%20then%20cascaded%20with%20the%20frozen%20first%20set%20to%20fine-tune%20the%0Adownstream%20tasks%2C%20following%20the%20approaches%20employed%20in%20prior%20research.%20In%20this%0Amanner%2C%20CasPL%20can%20effectively%20capture%20both%20domain-general%20and%20task-specific%0Arepresentations%20into%20explicitly%20different%20gradual%20groups%20of%20prompts%2C%20thus%0Apotentially%20alleviating%20overfitting%20issues%20in%20the%20target%20domain.%20It%27s%20worth%0Anoting%20that%20CasPL%20serves%20as%20a%20plug-and-play%20module%20that%20can%20seamlessly%0Aintegrate%20into%20any%20existing%20prompt%20learning%20approach.%20CasPL%20achieves%20a%0Asignificantly%20better%20balance%20between%20performance%20and%20inference%20speed%2C%20which%20is%0Aespecially%20beneficial%20for%20deploying%20smaller%20VLM%20models%20in%20resource-constrained%0Aenvironments.%20Compared%20to%20the%20previous%20state-of-the-art%20method%20PromptSRC%2C%20CasPL%0Ashows%20an%20average%20improvement%20of%201.85%25%20for%20base%20classes%2C%203.44%25%20for%20novel%0Aclasses%2C%20and%202.72%25%20for%20the%20harmonic%20mean%20over%2011%20image%20classification%20datasets.%0ACode%20is%20publicly%20available%20at%3A%20https%3A//github.com/megvii-research/CasPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascade%2520Prompt%2520Learning%2520for%2520Vision-Language%2520Model%2520Adaptation%26entry.906535625%3DGe%2520Wu%2520and%2520Xin%2520Zhang%2520and%2520Zheng%2520Li%2520and%2520Zhaowei%2520Chen%2520and%2520Jiajun%2520Liang%2520and%2520Jian%2520Yang%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520has%2520surfaced%2520as%2520an%2520effective%2520approach%2520to%2520enhance%2520the%250Aperformance%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520like%2520CLIP%2520when%2520applied%2520to%250Adownstream%2520tasks.%2520However%252C%2520current%2520learnable%2520prompt%2520tokens%2520are%2520primarily%2520used%250Afor%2520the%2520single%2520phase%2520of%2520adapting%2520to%2520tasks%2520%2528i.e.%252C%2520adapting%2520prompt%2529%252C%2520easily%250Aleading%2520to%2520overfitting%2520risks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520Cascade%2520Prompt%250ALearning%2520CasPL%2520framework%2520to%2520enable%2520prompt%2520learning%2520to%2520serve%2520both%2520generic%2520and%250Aspecific%2520expertise%2520%2528i.e.%252C%2520boosting%2520and%2520adapting%2520prompt%2529%2520simultaneously.%250ASpecifically%252C%2520CasPL%2520is%2520a%2520new%2520learning%2520paradigm%2520comprising%2520two%2520distinct%2520phases%250Aof%2520learnable%2520prompts%253A%2520the%2520first%2520boosting%2520prompt%2520is%2520crafted%2520to%2520extract%250Adomain-general%2520knowledge%2520from%2520a%2520senior%2520larger%2520CLIP%2520teacher%2520model%2520by%2520aligning%250Atheir%2520predicted%2520logits%2520using%2520extensive%2520unlabeled%2520domain%2520images.%2520The%2520second%250Aadapting%2520prompt%2520is%2520then%2520cascaded%2520with%2520the%2520frozen%2520first%2520set%2520to%2520fine-tune%2520the%250Adownstream%2520tasks%252C%2520following%2520the%2520approaches%2520employed%2520in%2520prior%2520research.%2520In%2520this%250Amanner%252C%2520CasPL%2520can%2520effectively%2520capture%2520both%2520domain-general%2520and%2520task-specific%250Arepresentations%2520into%2520explicitly%2520different%2520gradual%2520groups%2520of%2520prompts%252C%2520thus%250Apotentially%2520alleviating%2520overfitting%2520issues%2520in%2520the%2520target%2520domain.%2520It%2527s%2520worth%250Anoting%2520that%2520CasPL%2520serves%2520as%2520a%2520plug-and-play%2520module%2520that%2520can%2520seamlessly%250Aintegrate%2520into%2520any%2520existing%2520prompt%2520learning%2520approach.%2520CasPL%2520achieves%2520a%250Asignificantly%2520better%2520balance%2520between%2520performance%2520and%2520inference%2520speed%252C%2520which%2520is%250Aespecially%2520beneficial%2520for%2520deploying%2520smaller%2520VLM%2520models%2520in%2520resource-constrained%250Aenvironments.%2520Compared%2520to%2520the%2520previous%2520state-of-the-art%2520method%2520PromptSRC%252C%2520CasPL%250Ashows%2520an%2520average%2520improvement%2520of%25201.85%2525%2520for%2520base%2520classes%252C%25203.44%2525%2520for%2520novel%250Aclasses%252C%2520and%25202.72%2525%2520for%2520the%2520harmonic%2520mean%2520over%252011%2520image%2520classification%2520datasets.%250ACode%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/megvii-research/CasPL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascade%20Prompt%20Learning%20for%20Vision-Language%20Model%20Adaptation&entry.906535625=Ge%20Wu%20and%20Xin%20Zhang%20and%20Zheng%20Li%20and%20Zhaowei%20Chen%20and%20Jiajun%20Liang%20and%20Jian%20Yang%20and%20Xiang%20Li&entry.1292438233=%20%20Prompt%20learning%20has%20surfaced%20as%20an%20effective%20approach%20to%20enhance%20the%0Aperformance%20of%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20when%20applied%20to%0Adownstream%20tasks.%20However%2C%20current%20learnable%20prompt%20tokens%20are%20primarily%20used%0Afor%20the%20single%20phase%20of%20adapting%20to%20tasks%20%28i.e.%2C%20adapting%20prompt%29%2C%20easily%0Aleading%20to%20overfitting%20risks.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Cascade%20Prompt%0ALearning%20CasPL%20framework%20to%20enable%20prompt%20learning%20to%20serve%20both%20generic%20and%0Aspecific%20expertise%20%28i.e.%2C%20boosting%20and%20adapting%20prompt%29%20simultaneously.%0ASpecifically%2C%20CasPL%20is%20a%20new%20learning%20paradigm%20comprising%20two%20distinct%20phases%0Aof%20learnable%20prompts%3A%20the%20first%20boosting%20prompt%20is%20crafted%20to%20extract%0Adomain-general%20knowledge%20from%20a%20senior%20larger%20CLIP%20teacher%20model%20by%20aligning%0Atheir%20predicted%20logits%20using%20extensive%20unlabeled%20domain%20images.%20The%20second%0Aadapting%20prompt%20is%20then%20cascaded%20with%20the%20frozen%20first%20set%20to%20fine-tune%20the%0Adownstream%20tasks%2C%20following%20the%20approaches%20employed%20in%20prior%20research.%20In%20this%0Amanner%2C%20CasPL%20can%20effectively%20capture%20both%20domain-general%20and%20task-specific%0Arepresentations%20into%20explicitly%20different%20gradual%20groups%20of%20prompts%2C%20thus%0Apotentially%20alleviating%20overfitting%20issues%20in%20the%20target%20domain.%20It%27s%20worth%0Anoting%20that%20CasPL%20serves%20as%20a%20plug-and-play%20module%20that%20can%20seamlessly%0Aintegrate%20into%20any%20existing%20prompt%20learning%20approach.%20CasPL%20achieves%20a%0Asignificantly%20better%20balance%20between%20performance%20and%20inference%20speed%2C%20which%20is%0Aespecially%20beneficial%20for%20deploying%20smaller%20VLM%20models%20in%20resource-constrained%0Aenvironments.%20Compared%20to%20the%20previous%20state-of-the-art%20method%20PromptSRC%2C%20CasPL%0Ashows%20an%20average%20improvement%20of%201.85%25%20for%20base%20classes%2C%203.44%25%20for%20novel%0Aclasses%2C%20and%202.72%25%20for%20the%20harmonic%20mean%20over%2011%20image%20classification%20datasets.%0ACode%20is%20publicly%20available%20at%3A%20https%3A//github.com/megvii-research/CasPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17805v1&entry.124074799=Read"},
{"title": "Self-supervised Preference Optimization: Enhance Your Language Model\n  with Preference Degree Awareness", "author": "Jian Li and Haojing Huang and Yujia Zhang and Pengfei Xu and Xi Chen and Rui Song and Lida Shi and Jingwen Wang and Hao Xu", "abstract": "  Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO.\n", "link": "http://arxiv.org/abs/2409.17791v1", "date": "2024-09-26", "relevancy": 2.545, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Preference%20Optimization%3A%20Enhance%20Your%20Language%20Model%0A%20%20with%20Preference%20Degree%20Awareness&body=Title%3A%20Self-supervised%20Preference%20Optimization%3A%20Enhance%20Your%20Language%20Model%0A%20%20with%20Preference%20Degree%20Awareness%0AAuthor%3A%20Jian%20Li%20and%20Haojing%20Huang%20and%20Yujia%20Zhang%20and%20Pengfei%20Xu%20and%20Xi%20Chen%20and%20Rui%20Song%20and%20Lida%20Shi%20and%20Jingwen%20Wang%20and%20Hao%20Xu%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20significant%20interest%20in%20replacing%20the%20reward%20model%0Ain%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%20methods%20for%20Large%20Language%0AModels%20%28LLMs%29%2C%20such%20as%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20its%20variants.%0AThese%20approaches%20commonly%20use%20a%20binary%20cross-entropy%20mechanism%20on%20pairwise%0Asamples%2C%20i.e.%2C%20minimizing%20and%20maximizing%20the%20loss%20based%20on%20preferred%20or%0Adis-preferred%20responses%2C%20respectively.%20However%2C%20while%20this%20training%20strategy%0Aomits%20the%20reward%20model%2C%20it%20also%20overlooks%20the%20varying%20preference%20degrees%20within%0Adifferent%20responses.%20We%20hypothesize%20that%20this%20is%20a%20key%20factor%20hindering%20LLMs%0Afrom%20sufficiently%20understanding%20human%20preferences.%20To%20address%20this%20problem%2C%20we%0Apropose%20a%20novel%20Self-supervised%20Preference%20Optimization%20%28SPO%29%20framework%2C%20which%0Aconstructs%20a%20self-supervised%20preference%20degree%20loss%20combined%20with%20the%20alignment%0Aloss%2C%20thereby%20helping%20LLMs%20improve%20their%20ability%20to%20understand%20the%20degree%20of%0Apreference.%20Extensive%20experiments%20are%20conducted%20on%20two%20widely%20used%20datasets%20of%0Adifferent%20tasks.%20The%20results%20demonstrate%20that%20SPO%20can%20be%20seamlessly%20integrated%0Awith%20existing%20preference%20optimization%20methods%20and%20significantly%20boost%20their%0Aperformance%20to%20achieve%20state-of-the-art%20performance.%20We%20also%20conduct%20detailed%0Aanalyses%20to%20offer%20comprehensive%20insights%20into%20SPO%2C%20which%20verifies%20its%0Aeffectiveness.%20The%20code%20is%20available%20at%20https%3A//github.com/lijian16/SPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Preference%2520Optimization%253A%2520Enhance%2520Your%2520Language%2520Model%250A%2520%2520with%2520Preference%2520Degree%2520Awareness%26entry.906535625%3DJian%2520Li%2520and%2520Haojing%2520Huang%2520and%2520Yujia%2520Zhang%2520and%2520Pengfei%2520Xu%2520and%2520Xi%2520Chen%2520and%2520Rui%2520Song%2520and%2520Lida%2520Shi%2520and%2520Jingwen%2520Wang%2520and%2520Hao%2520Xu%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520significant%2520interest%2520in%2520replacing%2520the%2520reward%2520model%250Ain%2520Reinforcement%2520Learning%2520with%2520Human%2520Feedback%2520%2528RLHF%2529%2520methods%2520for%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520such%2520as%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520and%2520its%2520variants.%250AThese%2520approaches%2520commonly%2520use%2520a%2520binary%2520cross-entropy%2520mechanism%2520on%2520pairwise%250Asamples%252C%2520i.e.%252C%2520minimizing%2520and%2520maximizing%2520the%2520loss%2520based%2520on%2520preferred%2520or%250Adis-preferred%2520responses%252C%2520respectively.%2520However%252C%2520while%2520this%2520training%2520strategy%250Aomits%2520the%2520reward%2520model%252C%2520it%2520also%2520overlooks%2520the%2520varying%2520preference%2520degrees%2520within%250Adifferent%2520responses.%2520We%2520hypothesize%2520that%2520this%2520is%2520a%2520key%2520factor%2520hindering%2520LLMs%250Afrom%2520sufficiently%2520understanding%2520human%2520preferences.%2520To%2520address%2520this%2520problem%252C%2520we%250Apropose%2520a%2520novel%2520Self-supervised%2520Preference%2520Optimization%2520%2528SPO%2529%2520framework%252C%2520which%250Aconstructs%2520a%2520self-supervised%2520preference%2520degree%2520loss%2520combined%2520with%2520the%2520alignment%250Aloss%252C%2520thereby%2520helping%2520LLMs%2520improve%2520their%2520ability%2520to%2520understand%2520the%2520degree%2520of%250Apreference.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520two%2520widely%2520used%2520datasets%2520of%250Adifferent%2520tasks.%2520The%2520results%2520demonstrate%2520that%2520SPO%2520can%2520be%2520seamlessly%2520integrated%250Awith%2520existing%2520preference%2520optimization%2520methods%2520and%2520significantly%2520boost%2520their%250Aperformance%2520to%2520achieve%2520state-of-the-art%2520performance.%2520We%2520also%2520conduct%2520detailed%250Aanalyses%2520to%2520offer%2520comprehensive%2520insights%2520into%2520SPO%252C%2520which%2520verifies%2520its%250Aeffectiveness.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/lijian16/SPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Preference%20Optimization%3A%20Enhance%20Your%20Language%20Model%0A%20%20with%20Preference%20Degree%20Awareness&entry.906535625=Jian%20Li%20and%20Haojing%20Huang%20and%20Yujia%20Zhang%20and%20Pengfei%20Xu%20and%20Xi%20Chen%20and%20Rui%20Song%20and%20Lida%20Shi%20and%20Jingwen%20Wang%20and%20Hao%20Xu&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20significant%20interest%20in%20replacing%20the%20reward%20model%0Ain%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%20methods%20for%20Large%20Language%0AModels%20%28LLMs%29%2C%20such%20as%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20its%20variants.%0AThese%20approaches%20commonly%20use%20a%20binary%20cross-entropy%20mechanism%20on%20pairwise%0Asamples%2C%20i.e.%2C%20minimizing%20and%20maximizing%20the%20loss%20based%20on%20preferred%20or%0Adis-preferred%20responses%2C%20respectively.%20However%2C%20while%20this%20training%20strategy%0Aomits%20the%20reward%20model%2C%20it%20also%20overlooks%20the%20varying%20preference%20degrees%20within%0Adifferent%20responses.%20We%20hypothesize%20that%20this%20is%20a%20key%20factor%20hindering%20LLMs%0Afrom%20sufficiently%20understanding%20human%20preferences.%20To%20address%20this%20problem%2C%20we%0Apropose%20a%20novel%20Self-supervised%20Preference%20Optimization%20%28SPO%29%20framework%2C%20which%0Aconstructs%20a%20self-supervised%20preference%20degree%20loss%20combined%20with%20the%20alignment%0Aloss%2C%20thereby%20helping%20LLMs%20improve%20their%20ability%20to%20understand%20the%20degree%20of%0Apreference.%20Extensive%20experiments%20are%20conducted%20on%20two%20widely%20used%20datasets%20of%0Adifferent%20tasks.%20The%20results%20demonstrate%20that%20SPO%20can%20be%20seamlessly%20integrated%0Awith%20existing%20preference%20optimization%20methods%20and%20significantly%20boost%20their%0Aperformance%20to%20achieve%20state-of-the-art%20performance.%20We%20also%20conduct%20detailed%0Aanalyses%20to%20offer%20comprehensive%20insights%20into%20SPO%2C%20which%20verifies%20its%0Aeffectiveness.%20The%20code%20is%20available%20at%20https%3A//github.com/lijian16/SPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17791v1&entry.124074799=Read"},
{"title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient\n  Language Model Inference", "author": "Atsuki Yamaguchi and Aline Villavicencio and Nikolaos Aletras", "abstract": "  The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\n(CVA) methods have been proposed for adapting models to a target language\naiming to improve downstream performance. However, the effectiveness of these\nmethods on increasing inference efficiency of generative LLMs has yet to be\nexplored. In this paper, we perform an empirical study of five CVA methods on\nfour generative LLMs (including monolingual and multilingual models) across\nfour typologically-diverse languages and four natural language understanding\ntasks. We find that CVA substantially contributes to LLM inference speedups of\nup to 271.5\\%. We also show that adapting LLMs that have been pre-trained on\nmore balanced multilingual data results in downstream performance comparable to\nthe original models.\n", "link": "http://arxiv.org/abs/2402.10712v3", "date": "2024-09-26", "relevancy": 2.5434, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20on%20Cross-lingual%20Vocabulary%20Adaptation%20for%20Efficient%0A%20%20Language%20Model%20Inference&body=Title%3A%20An%20Empirical%20Study%20on%20Cross-lingual%20Vocabulary%20Adaptation%20for%20Efficient%0A%20%20Language%20Model%20Inference%0AAuthor%3A%20Atsuki%20Yamaguchi%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras%0AAbstract%3A%20%20%20The%20development%20of%20state-of-the-art%20generative%20large%20language%20models%20%28LLMs%29%0Adisproportionately%20relies%20on%20English-centric%20tokenizers%2C%20vocabulary%20and%0Apre-training%20data.%20Despite%20the%20fact%20that%20some%20LLMs%20have%20multilingual%0Acapabilities%2C%20recent%20studies%20have%20shown%20that%20their%20inference%20efficiency%0Adeteriorates%20when%20generating%20text%20in%20languages%20other%20than%20English.%20This%20results%0Ain%20increased%20inference%20time%20and%20costs.%20Cross-lingual%20vocabulary%20adaptation%0A%28CVA%29%20methods%20have%20been%20proposed%20for%20adapting%20models%20to%20a%20target%20language%0Aaiming%20to%20improve%20downstream%20performance.%20However%2C%20the%20effectiveness%20of%20these%0Amethods%20on%20increasing%20inference%20efficiency%20of%20generative%20LLMs%20has%20yet%20to%20be%0Aexplored.%20In%20this%20paper%2C%20we%20perform%20an%20empirical%20study%20of%20five%20CVA%20methods%20on%0Afour%20generative%20LLMs%20%28including%20monolingual%20and%20multilingual%20models%29%20across%0Afour%20typologically-diverse%20languages%20and%20four%20natural%20language%20understanding%0Atasks.%20We%20find%20that%20CVA%20substantially%20contributes%20to%20LLM%20inference%20speedups%20of%0Aup%20to%20271.5%5C%25.%20We%20also%20show%20that%20adapting%20LLMs%20that%20have%20been%20pre-trained%20on%0Amore%20balanced%20multilingual%20data%20results%20in%20downstream%20performance%20comparable%20to%0Athe%20original%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10712v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520on%2520Cross-lingual%2520Vocabulary%2520Adaptation%2520for%2520Efficient%250A%2520%2520Language%2520Model%2520Inference%26entry.906535625%3DAtsuki%2520Yamaguchi%2520and%2520Aline%2520Villavicencio%2520and%2520Nikolaos%2520Aletras%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520state-of-the-art%2520generative%2520large%2520language%2520models%2520%2528LLMs%2529%250Adisproportionately%2520relies%2520on%2520English-centric%2520tokenizers%252C%2520vocabulary%2520and%250Apre-training%2520data.%2520Despite%2520the%2520fact%2520that%2520some%2520LLMs%2520have%2520multilingual%250Acapabilities%252C%2520recent%2520studies%2520have%2520shown%2520that%2520their%2520inference%2520efficiency%250Adeteriorates%2520when%2520generating%2520text%2520in%2520languages%2520other%2520than%2520English.%2520This%2520results%250Ain%2520increased%2520inference%2520time%2520and%2520costs.%2520Cross-lingual%2520vocabulary%2520adaptation%250A%2528CVA%2529%2520methods%2520have%2520been%2520proposed%2520for%2520adapting%2520models%2520to%2520a%2520target%2520language%250Aaiming%2520to%2520improve%2520downstream%2520performance.%2520However%252C%2520the%2520effectiveness%2520of%2520these%250Amethods%2520on%2520increasing%2520inference%2520efficiency%2520of%2520generative%2520LLMs%2520has%2520yet%2520to%2520be%250Aexplored.%2520In%2520this%2520paper%252C%2520we%2520perform%2520an%2520empirical%2520study%2520of%2520five%2520CVA%2520methods%2520on%250Afour%2520generative%2520LLMs%2520%2528including%2520monolingual%2520and%2520multilingual%2520models%2529%2520across%250Afour%2520typologically-diverse%2520languages%2520and%2520four%2520natural%2520language%2520understanding%250Atasks.%2520We%2520find%2520that%2520CVA%2520substantially%2520contributes%2520to%2520LLM%2520inference%2520speedups%2520of%250Aup%2520to%2520271.5%255C%2525.%2520We%2520also%2520show%2520that%2520adapting%2520LLMs%2520that%2520have%2520been%2520pre-trained%2520on%250Amore%2520balanced%2520multilingual%2520data%2520results%2520in%2520downstream%2520performance%2520comparable%2520to%250Athe%2520original%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10712v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20on%20Cross-lingual%20Vocabulary%20Adaptation%20for%20Efficient%0A%20%20Language%20Model%20Inference&entry.906535625=Atsuki%20Yamaguchi%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras&entry.1292438233=%20%20The%20development%20of%20state-of-the-art%20generative%20large%20language%20models%20%28LLMs%29%0Adisproportionately%20relies%20on%20English-centric%20tokenizers%2C%20vocabulary%20and%0Apre-training%20data.%20Despite%20the%20fact%20that%20some%20LLMs%20have%20multilingual%0Acapabilities%2C%20recent%20studies%20have%20shown%20that%20their%20inference%20efficiency%0Adeteriorates%20when%20generating%20text%20in%20languages%20other%20than%20English.%20This%20results%0Ain%20increased%20inference%20time%20and%20costs.%20Cross-lingual%20vocabulary%20adaptation%0A%28CVA%29%20methods%20have%20been%20proposed%20for%20adapting%20models%20to%20a%20target%20language%0Aaiming%20to%20improve%20downstream%20performance.%20However%2C%20the%20effectiveness%20of%20these%0Amethods%20on%20increasing%20inference%20efficiency%20of%20generative%20LLMs%20has%20yet%20to%20be%0Aexplored.%20In%20this%20paper%2C%20we%20perform%20an%20empirical%20study%20of%20five%20CVA%20methods%20on%0Afour%20generative%20LLMs%20%28including%20monolingual%20and%20multilingual%20models%29%20across%0Afour%20typologically-diverse%20languages%20and%20four%20natural%20language%20understanding%0Atasks.%20We%20find%20that%20CVA%20substantially%20contributes%20to%20LLM%20inference%20speedups%20of%0Aup%20to%20271.5%5C%25.%20We%20also%20show%20that%20adapting%20LLMs%20that%20have%20been%20pre-trained%20on%0Amore%20balanced%20multilingual%20data%20results%20in%20downstream%20performance%20comparable%20to%0Athe%20original%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10712v3&entry.124074799=Read"},
{"title": "Ophthalmic Biomarker Detection with Parallel Prediction of Transformer\n  and Convolutional Architecture", "author": "Md. Touhidul Islam and Md. Abtahi Majeed Chowdhury and Mahmudul Hasan and Asif Quadir and Lutfa Aktar", "abstract": "  Ophthalmic diseases represent a significant global health issue,\nnecessitating the use of advanced precise diagnostic tools. Optical Coherence\nTomography (OCT) imagery which offers high-resolution cross-sectional images of\nthe retina has become a pivotal imaging modality in ophthalmology.\nTraditionally physicians have manually detected various diseases and biomarkers\nfrom such diagnostic imagery. In recent times, deep learning techniques have\nbeen extensively used for medical diagnostic tasks enabling fast and precise\ndiagnosis. This paper presents a novel approach for ophthalmic biomarker\ndetection using an ensemble of Convolutional Neural Network (CNN) and Vision\nTransformer. While CNNs are good for feature extraction within the local\ncontext of the image, transformers are known for their ability to extract\nfeatures from the global context of the image. Using an ensemble of both\ntechniques allows us to harness the best of both worlds. Our method has been\nimplemented on the OLIVES dataset to detect 6 major biomarkers from the OCT\nimages and shows significant improvement of the macro averaged F1 score on the\ndataset.\n", "link": "http://arxiv.org/abs/2409.17788v1", "date": "2024-09-26", "relevancy": 2.5419, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5003}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ophthalmic%20Biomarker%20Detection%20with%20Parallel%20Prediction%20of%20Transformer%0A%20%20and%20Convolutional%20Architecture&body=Title%3A%20Ophthalmic%20Biomarker%20Detection%20with%20Parallel%20Prediction%20of%20Transformer%0A%20%20and%20Convolutional%20Architecture%0AAuthor%3A%20Md.%20Touhidul%20Islam%20and%20Md.%20Abtahi%20Majeed%20Chowdhury%20and%20Mahmudul%20Hasan%20and%20Asif%20Quadir%20and%20Lutfa%20Aktar%0AAbstract%3A%20%20%20Ophthalmic%20diseases%20represent%20a%20significant%20global%20health%20issue%2C%0Anecessitating%20the%20use%20of%20advanced%20precise%20diagnostic%20tools.%20Optical%20Coherence%0ATomography%20%28OCT%29%20imagery%20which%20offers%20high-resolution%20cross-sectional%20images%20of%0Athe%20retina%20has%20become%20a%20pivotal%20imaging%20modality%20in%20ophthalmology.%0ATraditionally%20physicians%20have%20manually%20detected%20various%20diseases%20and%20biomarkers%0Afrom%20such%20diagnostic%20imagery.%20In%20recent%20times%2C%20deep%20learning%20techniques%20have%0Abeen%20extensively%20used%20for%20medical%20diagnostic%20tasks%20enabling%20fast%20and%20precise%0Adiagnosis.%20This%20paper%20presents%20a%20novel%20approach%20for%20ophthalmic%20biomarker%0Adetection%20using%20an%20ensemble%20of%20Convolutional%20Neural%20Network%20%28CNN%29%20and%20Vision%0ATransformer.%20While%20CNNs%20are%20good%20for%20feature%20extraction%20within%20the%20local%0Acontext%20of%20the%20image%2C%20transformers%20are%20known%20for%20their%20ability%20to%20extract%0Afeatures%20from%20the%20global%20context%20of%20the%20image.%20Using%20an%20ensemble%20of%20both%0Atechniques%20allows%20us%20to%20harness%20the%20best%20of%20both%20worlds.%20Our%20method%20has%20been%0Aimplemented%20on%20the%20OLIVES%20dataset%20to%20detect%206%20major%20biomarkers%20from%20the%20OCT%0Aimages%20and%20shows%20significant%20improvement%20of%20the%20macro%20averaged%20F1%20score%20on%20the%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOphthalmic%2520Biomarker%2520Detection%2520with%2520Parallel%2520Prediction%2520of%2520Transformer%250A%2520%2520and%2520Convolutional%2520Architecture%26entry.906535625%3DMd.%2520Touhidul%2520Islam%2520and%2520Md.%2520Abtahi%2520Majeed%2520Chowdhury%2520and%2520Mahmudul%2520Hasan%2520and%2520Asif%2520Quadir%2520and%2520Lutfa%2520Aktar%26entry.1292438233%3D%2520%2520Ophthalmic%2520diseases%2520represent%2520a%2520significant%2520global%2520health%2520issue%252C%250Anecessitating%2520the%2520use%2520of%2520advanced%2520precise%2520diagnostic%2520tools.%2520Optical%2520Coherence%250ATomography%2520%2528OCT%2529%2520imagery%2520which%2520offers%2520high-resolution%2520cross-sectional%2520images%2520of%250Athe%2520retina%2520has%2520become%2520a%2520pivotal%2520imaging%2520modality%2520in%2520ophthalmology.%250ATraditionally%2520physicians%2520have%2520manually%2520detected%2520various%2520diseases%2520and%2520biomarkers%250Afrom%2520such%2520diagnostic%2520imagery.%2520In%2520recent%2520times%252C%2520deep%2520learning%2520techniques%2520have%250Abeen%2520extensively%2520used%2520for%2520medical%2520diagnostic%2520tasks%2520enabling%2520fast%2520and%2520precise%250Adiagnosis.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520ophthalmic%2520biomarker%250Adetection%2520using%2520an%2520ensemble%2520of%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520and%2520Vision%250ATransformer.%2520While%2520CNNs%2520are%2520good%2520for%2520feature%2520extraction%2520within%2520the%2520local%250Acontext%2520of%2520the%2520image%252C%2520transformers%2520are%2520known%2520for%2520their%2520ability%2520to%2520extract%250Afeatures%2520from%2520the%2520global%2520context%2520of%2520the%2520image.%2520Using%2520an%2520ensemble%2520of%2520both%250Atechniques%2520allows%2520us%2520to%2520harness%2520the%2520best%2520of%2520both%2520worlds.%2520Our%2520method%2520has%2520been%250Aimplemented%2520on%2520the%2520OLIVES%2520dataset%2520to%2520detect%25206%2520major%2520biomarkers%2520from%2520the%2520OCT%250Aimages%2520and%2520shows%2520significant%2520improvement%2520of%2520the%2520macro%2520averaged%2520F1%2520score%2520on%2520the%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ophthalmic%20Biomarker%20Detection%20with%20Parallel%20Prediction%20of%20Transformer%0A%20%20and%20Convolutional%20Architecture&entry.906535625=Md.%20Touhidul%20Islam%20and%20Md.%20Abtahi%20Majeed%20Chowdhury%20and%20Mahmudul%20Hasan%20and%20Asif%20Quadir%20and%20Lutfa%20Aktar&entry.1292438233=%20%20Ophthalmic%20diseases%20represent%20a%20significant%20global%20health%20issue%2C%0Anecessitating%20the%20use%20of%20advanced%20precise%20diagnostic%20tools.%20Optical%20Coherence%0ATomography%20%28OCT%29%20imagery%20which%20offers%20high-resolution%20cross-sectional%20images%20of%0Athe%20retina%20has%20become%20a%20pivotal%20imaging%20modality%20in%20ophthalmology.%0ATraditionally%20physicians%20have%20manually%20detected%20various%20diseases%20and%20biomarkers%0Afrom%20such%20diagnostic%20imagery.%20In%20recent%20times%2C%20deep%20learning%20techniques%20have%0Abeen%20extensively%20used%20for%20medical%20diagnostic%20tasks%20enabling%20fast%20and%20precise%0Adiagnosis.%20This%20paper%20presents%20a%20novel%20approach%20for%20ophthalmic%20biomarker%0Adetection%20using%20an%20ensemble%20of%20Convolutional%20Neural%20Network%20%28CNN%29%20and%20Vision%0ATransformer.%20While%20CNNs%20are%20good%20for%20feature%20extraction%20within%20the%20local%0Acontext%20of%20the%20image%2C%20transformers%20are%20known%20for%20their%20ability%20to%20extract%0Afeatures%20from%20the%20global%20context%20of%20the%20image.%20Using%20an%20ensemble%20of%20both%0Atechniques%20allows%20us%20to%20harness%20the%20best%20of%20both%20worlds.%20Our%20method%20has%20been%0Aimplemented%20on%20the%20OLIVES%20dataset%20to%20detect%206%20major%20biomarkers%20from%20the%20OCT%0Aimages%20and%20shows%20significant%20improvement%20of%20the%20macro%20averaged%20F1%20score%20on%20the%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17788v1&entry.124074799=Read"},
{"title": "Learning to Receive Help: Intervention-Aware Concept Embedding Models", "author": "Mateo Espinosa Zarlenga and Katherine M. Collins and Krishnamurthy Dvijotham and Adrian Weller and Zohreh Shams and Mateja Jamnik", "abstract": "  Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures\nby constructing and explaining their predictions using a set of high-level\nconcepts. A special property of these models is that they permit concept\ninterventions, wherein users can correct mispredicted concepts and thus improve\nthe model's performance. Recent work, however, has shown that intervention\nefficacy can be highly dependent on the order in which concepts are intervened\non and on the model's architecture and training hyperparameters. We argue that\nthis is rooted in a CBM's lack of train-time incentives for the model to be\nappropriately receptive to concept interventions. To address this, we propose\nIntervention-aware Concept Embedding models (IntCEMs), a novel CBM-based\narchitecture and training paradigm that improves a model's receptiveness to\ntest-time interventions. Our model learns a concept intervention policy in an\nend-to-end fashion from where it can sample meaningful intervention\ntrajectories at train-time. This conditions IntCEMs to effectively select and\nreceive concept interventions when deployed at test-time. Our experiments show\nthat IntCEMs significantly outperform state-of-the-art concept-interpretable\nmodels when provided with test-time concept interventions, demonstrating the\neffectiveness of our approach.\n", "link": "http://arxiv.org/abs/2309.16928v3", "date": "2024-09-26", "relevancy": 2.5342, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Receive%20Help%3A%20Intervention-Aware%20Concept%20Embedding%20Models&body=Title%3A%20Learning%20to%20Receive%20Help%3A%20Intervention-Aware%20Concept%20Embedding%20Models%0AAuthor%3A%20Mateo%20Espinosa%20Zarlenga%20and%20Katherine%20M.%20Collins%20and%20Krishnamurthy%20Dvijotham%20and%20Adrian%20Weller%20and%20Zohreh%20Shams%20and%20Mateja%20Jamnik%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20tackle%20the%20opacity%20of%20neural%20architectures%0Aby%20constructing%20and%20explaining%20their%20predictions%20using%20a%20set%20of%20high-level%0Aconcepts.%20A%20special%20property%20of%20these%20models%20is%20that%20they%20permit%20concept%0Ainterventions%2C%20wherein%20users%20can%20correct%20mispredicted%20concepts%20and%20thus%20improve%0Athe%20model%27s%20performance.%20Recent%20work%2C%20however%2C%20has%20shown%20that%20intervention%0Aefficacy%20can%20be%20highly%20dependent%20on%20the%20order%20in%20which%20concepts%20are%20intervened%0Aon%20and%20on%20the%20model%27s%20architecture%20and%20training%20hyperparameters.%20We%20argue%20that%0Athis%20is%20rooted%20in%20a%20CBM%27s%20lack%20of%20train-time%20incentives%20for%20the%20model%20to%20be%0Aappropriately%20receptive%20to%20concept%20interventions.%20To%20address%20this%2C%20we%20propose%0AIntervention-aware%20Concept%20Embedding%20models%20%28IntCEMs%29%2C%20a%20novel%20CBM-based%0Aarchitecture%20and%20training%20paradigm%20that%20improves%20a%20model%27s%20receptiveness%20to%0Atest-time%20interventions.%20Our%20model%20learns%20a%20concept%20intervention%20policy%20in%20an%0Aend-to-end%20fashion%20from%20where%20it%20can%20sample%20meaningful%20intervention%0Atrajectories%20at%20train-time.%20This%20conditions%20IntCEMs%20to%20effectively%20select%20and%0Areceive%20concept%20interventions%20when%20deployed%20at%20test-time.%20Our%20experiments%20show%0Athat%20IntCEMs%20significantly%20outperform%20state-of-the-art%20concept-interpretable%0Amodels%20when%20provided%20with%20test-time%20concept%20interventions%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16928v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Receive%2520Help%253A%2520Intervention-Aware%2520Concept%2520Embedding%2520Models%26entry.906535625%3DMateo%2520Espinosa%2520Zarlenga%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Krishnamurthy%2520Dvijotham%2520and%2520Adrian%2520Weller%2520and%2520Zohreh%2520Shams%2520and%2520Mateja%2520Jamnik%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520tackle%2520the%2520opacity%2520of%2520neural%2520architectures%250Aby%2520constructing%2520and%2520explaining%2520their%2520predictions%2520using%2520a%2520set%2520of%2520high-level%250Aconcepts.%2520A%2520special%2520property%2520of%2520these%2520models%2520is%2520that%2520they%2520permit%2520concept%250Ainterventions%252C%2520wherein%2520users%2520can%2520correct%2520mispredicted%2520concepts%2520and%2520thus%2520improve%250Athe%2520model%2527s%2520performance.%2520Recent%2520work%252C%2520however%252C%2520has%2520shown%2520that%2520intervention%250Aefficacy%2520can%2520be%2520highly%2520dependent%2520on%2520the%2520order%2520in%2520which%2520concepts%2520are%2520intervened%250Aon%2520and%2520on%2520the%2520model%2527s%2520architecture%2520and%2520training%2520hyperparameters.%2520We%2520argue%2520that%250Athis%2520is%2520rooted%2520in%2520a%2520CBM%2527s%2520lack%2520of%2520train-time%2520incentives%2520for%2520the%2520model%2520to%2520be%250Aappropriately%2520receptive%2520to%2520concept%2520interventions.%2520To%2520address%2520this%252C%2520we%2520propose%250AIntervention-aware%2520Concept%2520Embedding%2520models%2520%2528IntCEMs%2529%252C%2520a%2520novel%2520CBM-based%250Aarchitecture%2520and%2520training%2520paradigm%2520that%2520improves%2520a%2520model%2527s%2520receptiveness%2520to%250Atest-time%2520interventions.%2520Our%2520model%2520learns%2520a%2520concept%2520intervention%2520policy%2520in%2520an%250Aend-to-end%2520fashion%2520from%2520where%2520it%2520can%2520sample%2520meaningful%2520intervention%250Atrajectories%2520at%2520train-time.%2520This%2520conditions%2520IntCEMs%2520to%2520effectively%2520select%2520and%250Areceive%2520concept%2520interventions%2520when%2520deployed%2520at%2520test-time.%2520Our%2520experiments%2520show%250Athat%2520IntCEMs%2520significantly%2520outperform%2520state-of-the-art%2520concept-interpretable%250Amodels%2520when%2520provided%2520with%2520test-time%2520concept%2520interventions%252C%2520demonstrating%2520the%250Aeffectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16928v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Receive%20Help%3A%20Intervention-Aware%20Concept%20Embedding%20Models&entry.906535625=Mateo%20Espinosa%20Zarlenga%20and%20Katherine%20M.%20Collins%20and%20Krishnamurthy%20Dvijotham%20and%20Adrian%20Weller%20and%20Zohreh%20Shams%20and%20Mateja%20Jamnik&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20tackle%20the%20opacity%20of%20neural%20architectures%0Aby%20constructing%20and%20explaining%20their%20predictions%20using%20a%20set%20of%20high-level%0Aconcepts.%20A%20special%20property%20of%20these%20models%20is%20that%20they%20permit%20concept%0Ainterventions%2C%20wherein%20users%20can%20correct%20mispredicted%20concepts%20and%20thus%20improve%0Athe%20model%27s%20performance.%20Recent%20work%2C%20however%2C%20has%20shown%20that%20intervention%0Aefficacy%20can%20be%20highly%20dependent%20on%20the%20order%20in%20which%20concepts%20are%20intervened%0Aon%20and%20on%20the%20model%27s%20architecture%20and%20training%20hyperparameters.%20We%20argue%20that%0Athis%20is%20rooted%20in%20a%20CBM%27s%20lack%20of%20train-time%20incentives%20for%20the%20model%20to%20be%0Aappropriately%20receptive%20to%20concept%20interventions.%20To%20address%20this%2C%20we%20propose%0AIntervention-aware%20Concept%20Embedding%20models%20%28IntCEMs%29%2C%20a%20novel%20CBM-based%0Aarchitecture%20and%20training%20paradigm%20that%20improves%20a%20model%27s%20receptiveness%20to%0Atest-time%20interventions.%20Our%20model%20learns%20a%20concept%20intervention%20policy%20in%20an%0Aend-to-end%20fashion%20from%20where%20it%20can%20sample%20meaningful%20intervention%0Atrajectories%20at%20train-time.%20This%20conditions%20IntCEMs%20to%20effectively%20select%20and%0Areceive%20concept%20interventions%20when%20deployed%20at%20test-time.%20Our%20experiments%20show%0Athat%20IntCEMs%20significantly%20outperform%20state-of-the-art%20concept-interpretable%0Amodels%20when%20provided%20with%20test-time%20concept%20interventions%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16928v3&entry.124074799=Read"},
{"title": "Neural Implicit Representation for Highly Dynamic LiDAR Mapping and\n  Odometry", "author": "Qi Zhang and He Wang and Ru Li and Wenbin Li", "abstract": "  Recent advancements in Simultaneous Localization and Mapping (SLAM) have\nincreasingly highlighted the robustness of LiDAR-based techniques. At the same\ntime, Neural Radiance Fields (NeRF) have introduced new possibilities for 3D\nscene reconstruction, exemplified by SLAM systems. Among these, NeRF-LOAM has\nshown notable performance in NeRF-based SLAM applications. However, despite its\nstrengths, these systems often encounter difficulties in dynamic outdoor\nenvironments due to their inherent static assumptions. To address these\nlimitations, this paper proposes a novel method designed to improve\nreconstruction in highly dynamic outdoor scenes. Based on NeRF-LOAM, the\nproposed approach consists of two primary components. First, we separate the\nscene into static background and dynamic foreground. By identifying and\nexcluding dynamic elements from the mapping process, this segmentation enables\nthe creation of a dense 3D map that accurately represents the static background\nonly. The second component extends the octree structure to support\nmulti-resolution representation. This extension not only enhances\nreconstruction quality but also aids in the removal of dynamic objects\nidentified by the first module. Additionally, Fourier feature encoding is\napplied to the sampled points, capturing high-frequency information and leading\nto more complete reconstruction results. Evaluations on various datasets\ndemonstrate that our method achieves more competitive results compared to\ncurrent state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2409.17729v1", "date": "2024-09-26", "relevancy": 2.5329, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6341}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Implicit%20Representation%20for%20Highly%20Dynamic%20LiDAR%20Mapping%20and%0A%20%20Odometry&body=Title%3A%20Neural%20Implicit%20Representation%20for%20Highly%20Dynamic%20LiDAR%20Mapping%20and%0A%20%20Odometry%0AAuthor%3A%20Qi%20Zhang%20and%20He%20Wang%20and%20Ru%20Li%20and%20Wenbin%20Li%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20have%0Aincreasingly%20highlighted%20the%20robustness%20of%20LiDAR-based%20techniques.%20At%20the%20same%0Atime%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20introduced%20new%20possibilities%20for%203D%0Ascene%20reconstruction%2C%20exemplified%20by%20SLAM%20systems.%20Among%20these%2C%20NeRF-LOAM%20has%0Ashown%20notable%20performance%20in%20NeRF-based%20SLAM%20applications.%20However%2C%20despite%20its%0Astrengths%2C%20these%20systems%20often%20encounter%20difficulties%20in%20dynamic%20outdoor%0Aenvironments%20due%20to%20their%20inherent%20static%20assumptions.%20To%20address%20these%0Alimitations%2C%20this%20paper%20proposes%20a%20novel%20method%20designed%20to%20improve%0Areconstruction%20in%20highly%20dynamic%20outdoor%20scenes.%20Based%20on%20NeRF-LOAM%2C%20the%0Aproposed%20approach%20consists%20of%20two%20primary%20components.%20First%2C%20we%20separate%20the%0Ascene%20into%20static%20background%20and%20dynamic%20foreground.%20By%20identifying%20and%0Aexcluding%20dynamic%20elements%20from%20the%20mapping%20process%2C%20this%20segmentation%20enables%0Athe%20creation%20of%20a%20dense%203D%20map%20that%20accurately%20represents%20the%20static%20background%0Aonly.%20The%20second%20component%20extends%20the%20octree%20structure%20to%20support%0Amulti-resolution%20representation.%20This%20extension%20not%20only%20enhances%0Areconstruction%20quality%20but%20also%20aids%20in%20the%20removal%20of%20dynamic%20objects%0Aidentified%20by%20the%20first%20module.%20Additionally%2C%20Fourier%20feature%20encoding%20is%0Aapplied%20to%20the%20sampled%20points%2C%20capturing%20high-frequency%20information%20and%20leading%0Ato%20more%20complete%20reconstruction%20results.%20Evaluations%20on%20various%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20more%20competitive%20results%20compared%20to%0Acurrent%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Implicit%2520Representation%2520for%2520Highly%2520Dynamic%2520LiDAR%2520Mapping%2520and%250A%2520%2520Odometry%26entry.906535625%3DQi%2520Zhang%2520and%2520He%2520Wang%2520and%2520Ru%2520Li%2520and%2520Wenbin%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520have%250Aincreasingly%2520highlighted%2520the%2520robustness%2520of%2520LiDAR-based%2520techniques.%2520At%2520the%2520same%250Atime%252C%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520introduced%2520new%2520possibilities%2520for%25203D%250Ascene%2520reconstruction%252C%2520exemplified%2520by%2520SLAM%2520systems.%2520Among%2520these%252C%2520NeRF-LOAM%2520has%250Ashown%2520notable%2520performance%2520in%2520NeRF-based%2520SLAM%2520applications.%2520However%252C%2520despite%2520its%250Astrengths%252C%2520these%2520systems%2520often%2520encounter%2520difficulties%2520in%2520dynamic%2520outdoor%250Aenvironments%2520due%2520to%2520their%2520inherent%2520static%2520assumptions.%2520To%2520address%2520these%250Alimitations%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520method%2520designed%2520to%2520improve%250Areconstruction%2520in%2520highly%2520dynamic%2520outdoor%2520scenes.%2520Based%2520on%2520NeRF-LOAM%252C%2520the%250Aproposed%2520approach%2520consists%2520of%2520two%2520primary%2520components.%2520First%252C%2520we%2520separate%2520the%250Ascene%2520into%2520static%2520background%2520and%2520dynamic%2520foreground.%2520By%2520identifying%2520and%250Aexcluding%2520dynamic%2520elements%2520from%2520the%2520mapping%2520process%252C%2520this%2520segmentation%2520enables%250Athe%2520creation%2520of%2520a%2520dense%25203D%2520map%2520that%2520accurately%2520represents%2520the%2520static%2520background%250Aonly.%2520The%2520second%2520component%2520extends%2520the%2520octree%2520structure%2520to%2520support%250Amulti-resolution%2520representation.%2520This%2520extension%2520not%2520only%2520enhances%250Areconstruction%2520quality%2520but%2520also%2520aids%2520in%2520the%2520removal%2520of%2520dynamic%2520objects%250Aidentified%2520by%2520the%2520first%2520module.%2520Additionally%252C%2520Fourier%2520feature%2520encoding%2520is%250Aapplied%2520to%2520the%2520sampled%2520points%252C%2520capturing%2520high-frequency%2520information%2520and%2520leading%250Ato%2520more%2520complete%2520reconstruction%2520results.%2520Evaluations%2520on%2520various%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520more%2520competitive%2520results%2520compared%2520to%250Acurrent%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Implicit%20Representation%20for%20Highly%20Dynamic%20LiDAR%20Mapping%20and%0A%20%20Odometry&entry.906535625=Qi%20Zhang%20and%20He%20Wang%20and%20Ru%20Li%20and%20Wenbin%20Li&entry.1292438233=%20%20Recent%20advancements%20in%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20have%0Aincreasingly%20highlighted%20the%20robustness%20of%20LiDAR-based%20techniques.%20At%20the%20same%0Atime%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20introduced%20new%20possibilities%20for%203D%0Ascene%20reconstruction%2C%20exemplified%20by%20SLAM%20systems.%20Among%20these%2C%20NeRF-LOAM%20has%0Ashown%20notable%20performance%20in%20NeRF-based%20SLAM%20applications.%20However%2C%20despite%20its%0Astrengths%2C%20these%20systems%20often%20encounter%20difficulties%20in%20dynamic%20outdoor%0Aenvironments%20due%20to%20their%20inherent%20static%20assumptions.%20To%20address%20these%0Alimitations%2C%20this%20paper%20proposes%20a%20novel%20method%20designed%20to%20improve%0Areconstruction%20in%20highly%20dynamic%20outdoor%20scenes.%20Based%20on%20NeRF-LOAM%2C%20the%0Aproposed%20approach%20consists%20of%20two%20primary%20components.%20First%2C%20we%20separate%20the%0Ascene%20into%20static%20background%20and%20dynamic%20foreground.%20By%20identifying%20and%0Aexcluding%20dynamic%20elements%20from%20the%20mapping%20process%2C%20this%20segmentation%20enables%0Athe%20creation%20of%20a%20dense%203D%20map%20that%20accurately%20represents%20the%20static%20background%0Aonly.%20The%20second%20component%20extends%20the%20octree%20structure%20to%20support%0Amulti-resolution%20representation.%20This%20extension%20not%20only%20enhances%0Areconstruction%20quality%20but%20also%20aids%20in%20the%20removal%20of%20dynamic%20objects%0Aidentified%20by%20the%20first%20module.%20Additionally%2C%20Fourier%20feature%20encoding%20is%0Aapplied%20to%20the%20sampled%20points%2C%20capturing%20high-frequency%20information%20and%20leading%0Ato%20more%20complete%20reconstruction%20results.%20Evaluations%20on%20various%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20more%20competitive%20results%20compared%20to%0Acurrent%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17729v1&entry.124074799=Read"},
{"title": "Two-Timescale Gradient Descent Ascent Algorithms for Nonconvex Minimax\n  Optimization", "author": "Tianyi Lin and Chi Jin and Michael. I. Jordan", "abstract": "  We provide a unified analysis of two-timescale gradient descent ascent\n(TTGDA) for solving structured nonconvex minimax optimization problems in the\nform of $\\min_\\textbf{x} \\max_{\\textbf{y} \\in Y} f(\\textbf{x}, \\textbf{y})$,\nwhere the objective function $f(\\textbf{x}, \\textbf{y})$ is nonconvex in\n$\\textbf{x}$ and concave in $\\textbf{y}$, and the constraint set $Y \\subseteq\n\\mathbb{R}^n$ is convex and bounded. In the convex-concave setting, the\nsingle-timescale gradient descent ascent (GDA) algorithm is widely used in\napplications and has been shown to have strong convergence guarantees. In more\ngeneral settings, however, it can fail to converge. Our contribution is to\ndesign TTGDA algorithms that are effective beyond the convex-concave setting,\nefficiently finding a stationary point of the function $\\Phi(\\cdot) :=\n\\max_{\\textbf{y} \\in Y} f(\\cdot, \\textbf{y})$. We also establish theoretical\nbounds on the complexity of solving both smooth and nonsmooth nonconvex-concave\nminimax optimization problems. To the best of our knowledge, this is the first\nsystematic analysis of TTGDA for nonconvex minimax optimization, shedding light\non its superior performance in training generative adversarial networks (GANs)\nand in other real-world application problems.\n", "link": "http://arxiv.org/abs/2408.11974v2", "date": "2024-09-26", "relevancy": 2.5177, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5149}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5022}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-Timescale%20Gradient%20Descent%20Ascent%20Algorithms%20for%20Nonconvex%20Minimax%0A%20%20Optimization&body=Title%3A%20Two-Timescale%20Gradient%20Descent%20Ascent%20Algorithms%20for%20Nonconvex%20Minimax%0A%20%20Optimization%0AAuthor%3A%20Tianyi%20Lin%20and%20Chi%20Jin%20and%20Michael.%20I.%20Jordan%0AAbstract%3A%20%20%20We%20provide%20a%20unified%20analysis%20of%20two-timescale%20gradient%20descent%20ascent%0A%28TTGDA%29%20for%20solving%20structured%20nonconvex%20minimax%20optimization%20problems%20in%20the%0Aform%20of%20%24%5Cmin_%5Ctextbf%7Bx%7D%20%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%2C%0Awhere%20the%20objective%20function%20%24f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%20is%20nonconvex%20in%0A%24%5Ctextbf%7Bx%7D%24%20and%20concave%20in%20%24%5Ctextbf%7By%7D%24%2C%20and%20the%20constraint%20set%20%24Y%20%5Csubseteq%0A%5Cmathbb%7BR%7D%5En%24%20is%20convex%20and%20bounded.%20In%20the%20convex-concave%20setting%2C%20the%0Asingle-timescale%20gradient%20descent%20ascent%20%28GDA%29%20algorithm%20is%20widely%20used%20in%0Aapplications%20and%20has%20been%20shown%20to%20have%20strong%20convergence%20guarantees.%20In%20more%0Ageneral%20settings%2C%20however%2C%20it%20can%20fail%20to%20converge.%20Our%20contribution%20is%20to%0Adesign%20TTGDA%20algorithms%20that%20are%20effective%20beyond%20the%20convex-concave%20setting%2C%0Aefficiently%20finding%20a%20stationary%20point%20of%20the%20function%20%24%5CPhi%28%5Ccdot%29%20%3A%3D%0A%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ccdot%2C%20%5Ctextbf%7By%7D%29%24.%20We%20also%20establish%20theoretical%0Abounds%20on%20the%20complexity%20of%20solving%20both%20smooth%20and%20nonsmooth%20nonconvex-concave%0Aminimax%20optimization%20problems.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asystematic%20analysis%20of%20TTGDA%20for%20nonconvex%20minimax%20optimization%2C%20shedding%20light%0Aon%20its%20superior%20performance%20in%20training%20generative%20adversarial%20networks%20%28GANs%29%0Aand%20in%20other%20real-world%20application%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-Timescale%2520Gradient%2520Descent%2520Ascent%2520Algorithms%2520for%2520Nonconvex%2520Minimax%250A%2520%2520Optimization%26entry.906535625%3DTianyi%2520Lin%2520and%2520Chi%2520Jin%2520and%2520Michael.%2520I.%2520Jordan%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520unified%2520analysis%2520of%2520two-timescale%2520gradient%2520descent%2520ascent%250A%2528TTGDA%2529%2520for%2520solving%2520structured%2520nonconvex%2520minimax%2520optimization%2520problems%2520in%2520the%250Aform%2520of%2520%2524%255Cmin_%255Ctextbf%257Bx%257D%2520%255Cmax_%257B%255Ctextbf%257By%257D%2520%255Cin%2520Y%257D%2520f%2528%255Ctextbf%257Bx%257D%252C%2520%255Ctextbf%257By%257D%2529%2524%252C%250Awhere%2520the%2520objective%2520function%2520%2524f%2528%255Ctextbf%257Bx%257D%252C%2520%255Ctextbf%257By%257D%2529%2524%2520is%2520nonconvex%2520in%250A%2524%255Ctextbf%257Bx%257D%2524%2520and%2520concave%2520in%2520%2524%255Ctextbf%257By%257D%2524%252C%2520and%2520the%2520constraint%2520set%2520%2524Y%2520%255Csubseteq%250A%255Cmathbb%257BR%257D%255En%2524%2520is%2520convex%2520and%2520bounded.%2520In%2520the%2520convex-concave%2520setting%252C%2520the%250Asingle-timescale%2520gradient%2520descent%2520ascent%2520%2528GDA%2529%2520algorithm%2520is%2520widely%2520used%2520in%250Aapplications%2520and%2520has%2520been%2520shown%2520to%2520have%2520strong%2520convergence%2520guarantees.%2520In%2520more%250Ageneral%2520settings%252C%2520however%252C%2520it%2520can%2520fail%2520to%2520converge.%2520Our%2520contribution%2520is%2520to%250Adesign%2520TTGDA%2520algorithms%2520that%2520are%2520effective%2520beyond%2520the%2520convex-concave%2520setting%252C%250Aefficiently%2520finding%2520a%2520stationary%2520point%2520of%2520the%2520function%2520%2524%255CPhi%2528%255Ccdot%2529%2520%253A%253D%250A%255Cmax_%257B%255Ctextbf%257By%257D%2520%255Cin%2520Y%257D%2520f%2528%255Ccdot%252C%2520%255Ctextbf%257By%257D%2529%2524.%2520We%2520also%2520establish%2520theoretical%250Abounds%2520on%2520the%2520complexity%2520of%2520solving%2520both%2520smooth%2520and%2520nonsmooth%2520nonconvex-concave%250Aminimax%2520optimization%2520problems.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Asystematic%2520analysis%2520of%2520TTGDA%2520for%2520nonconvex%2520minimax%2520optimization%252C%2520shedding%2520light%250Aon%2520its%2520superior%2520performance%2520in%2520training%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%250Aand%2520in%2520other%2520real-world%2520application%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Timescale%20Gradient%20Descent%20Ascent%20Algorithms%20for%20Nonconvex%20Minimax%0A%20%20Optimization&entry.906535625=Tianyi%20Lin%20and%20Chi%20Jin%20and%20Michael.%20I.%20Jordan&entry.1292438233=%20%20We%20provide%20a%20unified%20analysis%20of%20two-timescale%20gradient%20descent%20ascent%0A%28TTGDA%29%20for%20solving%20structured%20nonconvex%20minimax%20optimization%20problems%20in%20the%0Aform%20of%20%24%5Cmin_%5Ctextbf%7Bx%7D%20%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%2C%0Awhere%20the%20objective%20function%20%24f%28%5Ctextbf%7Bx%7D%2C%20%5Ctextbf%7By%7D%29%24%20is%20nonconvex%20in%0A%24%5Ctextbf%7Bx%7D%24%20and%20concave%20in%20%24%5Ctextbf%7By%7D%24%2C%20and%20the%20constraint%20set%20%24Y%20%5Csubseteq%0A%5Cmathbb%7BR%7D%5En%24%20is%20convex%20and%20bounded.%20In%20the%20convex-concave%20setting%2C%20the%0Asingle-timescale%20gradient%20descent%20ascent%20%28GDA%29%20algorithm%20is%20widely%20used%20in%0Aapplications%20and%20has%20been%20shown%20to%20have%20strong%20convergence%20guarantees.%20In%20more%0Ageneral%20settings%2C%20however%2C%20it%20can%20fail%20to%20converge.%20Our%20contribution%20is%20to%0Adesign%20TTGDA%20algorithms%20that%20are%20effective%20beyond%20the%20convex-concave%20setting%2C%0Aefficiently%20finding%20a%20stationary%20point%20of%20the%20function%20%24%5CPhi%28%5Ccdot%29%20%3A%3D%0A%5Cmax_%7B%5Ctextbf%7By%7D%20%5Cin%20Y%7D%20f%28%5Ccdot%2C%20%5Ctextbf%7By%7D%29%24.%20We%20also%20establish%20theoretical%0Abounds%20on%20the%20complexity%20of%20solving%20both%20smooth%20and%20nonsmooth%20nonconvex-concave%0Aminimax%20optimization%20problems.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asystematic%20analysis%20of%20TTGDA%20for%20nonconvex%20minimax%20optimization%2C%20shedding%20light%0Aon%20its%20superior%20performance%20in%20training%20generative%20adversarial%20networks%20%28GANs%29%0Aand%20in%20other%20real-world%20application%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11974v2&entry.124074799=Read"},
{"title": "LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field", "author": "Huan Wang and Feitong Tan and Ziqian Bai and Yinda Zhang and Shichen Liu and Qiangeng Xu and Menglei Chai and Anish Prabhu and Rohit Pandey and Sean Fanello and Zeng Huang and Yun Fu", "abstract": "  Recent works have shown that neural radiance fields (NeRFs) on top of\nparametric models have reached SOTA quality to build photorealistic head\navatars from a monocular video. However, one major limitation of the NeRF-based\navatars is the slow rendering speed due to the dense point sampling of NeRF,\npreventing them from broader utility on resource-constrained devices. We\nintroduce LightAvatar, the first head avatar model based on neural light fields\n(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose\nvia a single network forward pass, without using mesh or volume rendering. The\nproposed approach, while being conceptually appealing, poses a significant\nchallenge towards real-time efficiency and training stability. To resolve them,\nwe introduce dedicated network designs to obtain proper representations for the\nNeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a\ndistillation-based training strategy that uses a pretrained avatar model as\nteacher to synthesize abundant pseudo data for training. A warping field\nnetwork is introduced to correct the fitting error in the real data so that the\nmodel can learn better. Extensive experiments suggest that our method can\nachieve new SOTA image quality quantitatively or qualitatively, while being\nsignificantly faster than the counterparts, reporting 174.1 FPS (512x512\nresolution) on a consumer-grade GPU (RTX3090) with no customized optimization.\n", "link": "http://arxiv.org/abs/2409.18057v1", "date": "2024-09-26", "relevancy": 2.5038, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6423}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6423}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightAvatar%3A%20Efficient%20Head%20Avatar%20as%20Dynamic%20Neural%20Light%20Field&body=Title%3A%20LightAvatar%3A%20Efficient%20Head%20Avatar%20as%20Dynamic%20Neural%20Light%20Field%0AAuthor%3A%20Huan%20Wang%20and%20Feitong%20Tan%20and%20Ziqian%20Bai%20and%20Yinda%20Zhang%20and%20Shichen%20Liu%20and%20Qiangeng%20Xu%20and%20Menglei%20Chai%20and%20Anish%20Prabhu%20and%20Rohit%20Pandey%20and%20Sean%20Fanello%20and%20Zeng%20Huang%20and%20Yun%20Fu%0AAbstract%3A%20%20%20Recent%20works%20have%20shown%20that%20neural%20radiance%20fields%20%28NeRFs%29%20on%20top%20of%0Aparametric%20models%20have%20reached%20SOTA%20quality%20to%20build%20photorealistic%20head%0Aavatars%20from%20a%20monocular%20video.%20However%2C%20one%20major%20limitation%20of%20the%20NeRF-based%0Aavatars%20is%20the%20slow%20rendering%20speed%20due%20to%20the%20dense%20point%20sampling%20of%20NeRF%2C%0Apreventing%20them%20from%20broader%20utility%20on%20resource-constrained%20devices.%20We%0Aintroduce%20LightAvatar%2C%20the%20first%20head%20avatar%20model%20based%20on%20neural%20light%20fields%0A%28NeLFs%29.%20LightAvatar%20renders%20an%20image%20from%203DMM%20parameters%20and%20a%20camera%20pose%0Avia%20a%20single%20network%20forward%20pass%2C%20without%20using%20mesh%20or%20volume%20rendering.%20The%0Aproposed%20approach%2C%20while%20being%20conceptually%20appealing%2C%20poses%20a%20significant%0Achallenge%20towards%20real-time%20efficiency%20and%20training%20stability.%20To%20resolve%20them%2C%0Awe%20introduce%20dedicated%20network%20designs%20to%20obtain%20proper%20representations%20for%20the%0ANeLF%20model%20and%20maintain%20a%20low%20FLOPs%20budget.%20Meanwhile%2C%20we%20tap%20into%20a%0Adistillation-based%20training%20strategy%20that%20uses%20a%20pretrained%20avatar%20model%20as%0Ateacher%20to%20synthesize%20abundant%20pseudo%20data%20for%20training.%20A%20warping%20field%0Anetwork%20is%20introduced%20to%20correct%20the%20fitting%20error%20in%20the%20real%20data%20so%20that%20the%0Amodel%20can%20learn%20better.%20Extensive%20experiments%20suggest%20that%20our%20method%20can%0Aachieve%20new%20SOTA%20image%20quality%20quantitatively%20or%20qualitatively%2C%20while%20being%0Asignificantly%20faster%20than%20the%20counterparts%2C%20reporting%20174.1%20FPS%20%28512x512%0Aresolution%29%20on%20a%20consumer-grade%20GPU%20%28RTX3090%29%20with%20no%20customized%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightAvatar%253A%2520Efficient%2520Head%2520Avatar%2520as%2520Dynamic%2520Neural%2520Light%2520Field%26entry.906535625%3DHuan%2520Wang%2520and%2520Feitong%2520Tan%2520and%2520Ziqian%2520Bai%2520and%2520Yinda%2520Zhang%2520and%2520Shichen%2520Liu%2520and%2520Qiangeng%2520Xu%2520and%2520Menglei%2520Chai%2520and%2520Anish%2520Prabhu%2520and%2520Rohit%2520Pandey%2520and%2520Sean%2520Fanello%2520and%2520Zeng%2520Huang%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520shown%2520that%2520neural%2520radiance%2520fields%2520%2528NeRFs%2529%2520on%2520top%2520of%250Aparametric%2520models%2520have%2520reached%2520SOTA%2520quality%2520to%2520build%2520photorealistic%2520head%250Aavatars%2520from%2520a%2520monocular%2520video.%2520However%252C%2520one%2520major%2520limitation%2520of%2520the%2520NeRF-based%250Aavatars%2520is%2520the%2520slow%2520rendering%2520speed%2520due%2520to%2520the%2520dense%2520point%2520sampling%2520of%2520NeRF%252C%250Apreventing%2520them%2520from%2520broader%2520utility%2520on%2520resource-constrained%2520devices.%2520We%250Aintroduce%2520LightAvatar%252C%2520the%2520first%2520head%2520avatar%2520model%2520based%2520on%2520neural%2520light%2520fields%250A%2528NeLFs%2529.%2520LightAvatar%2520renders%2520an%2520image%2520from%25203DMM%2520parameters%2520and%2520a%2520camera%2520pose%250Avia%2520a%2520single%2520network%2520forward%2520pass%252C%2520without%2520using%2520mesh%2520or%2520volume%2520rendering.%2520The%250Aproposed%2520approach%252C%2520while%2520being%2520conceptually%2520appealing%252C%2520poses%2520a%2520significant%250Achallenge%2520towards%2520real-time%2520efficiency%2520and%2520training%2520stability.%2520To%2520resolve%2520them%252C%250Awe%2520introduce%2520dedicated%2520network%2520designs%2520to%2520obtain%2520proper%2520representations%2520for%2520the%250ANeLF%2520model%2520and%2520maintain%2520a%2520low%2520FLOPs%2520budget.%2520Meanwhile%252C%2520we%2520tap%2520into%2520a%250Adistillation-based%2520training%2520strategy%2520that%2520uses%2520a%2520pretrained%2520avatar%2520model%2520as%250Ateacher%2520to%2520synthesize%2520abundant%2520pseudo%2520data%2520for%2520training.%2520A%2520warping%2520field%250Anetwork%2520is%2520introduced%2520to%2520correct%2520the%2520fitting%2520error%2520in%2520the%2520real%2520data%2520so%2520that%2520the%250Amodel%2520can%2520learn%2520better.%2520Extensive%2520experiments%2520suggest%2520that%2520our%2520method%2520can%250Aachieve%2520new%2520SOTA%2520image%2520quality%2520quantitatively%2520or%2520qualitatively%252C%2520while%2520being%250Asignificantly%2520faster%2520than%2520the%2520counterparts%252C%2520reporting%2520174.1%2520FPS%2520%2528512x512%250Aresolution%2529%2520on%2520a%2520consumer-grade%2520GPU%2520%2528RTX3090%2529%2520with%2520no%2520customized%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightAvatar%3A%20Efficient%20Head%20Avatar%20as%20Dynamic%20Neural%20Light%20Field&entry.906535625=Huan%20Wang%20and%20Feitong%20Tan%20and%20Ziqian%20Bai%20and%20Yinda%20Zhang%20and%20Shichen%20Liu%20and%20Qiangeng%20Xu%20and%20Menglei%20Chai%20and%20Anish%20Prabhu%20and%20Rohit%20Pandey%20and%20Sean%20Fanello%20and%20Zeng%20Huang%20and%20Yun%20Fu&entry.1292438233=%20%20Recent%20works%20have%20shown%20that%20neural%20radiance%20fields%20%28NeRFs%29%20on%20top%20of%0Aparametric%20models%20have%20reached%20SOTA%20quality%20to%20build%20photorealistic%20head%0Aavatars%20from%20a%20monocular%20video.%20However%2C%20one%20major%20limitation%20of%20the%20NeRF-based%0Aavatars%20is%20the%20slow%20rendering%20speed%20due%20to%20the%20dense%20point%20sampling%20of%20NeRF%2C%0Apreventing%20them%20from%20broader%20utility%20on%20resource-constrained%20devices.%20We%0Aintroduce%20LightAvatar%2C%20the%20first%20head%20avatar%20model%20based%20on%20neural%20light%20fields%0A%28NeLFs%29.%20LightAvatar%20renders%20an%20image%20from%203DMM%20parameters%20and%20a%20camera%20pose%0Avia%20a%20single%20network%20forward%20pass%2C%20without%20using%20mesh%20or%20volume%20rendering.%20The%0Aproposed%20approach%2C%20while%20being%20conceptually%20appealing%2C%20poses%20a%20significant%0Achallenge%20towards%20real-time%20efficiency%20and%20training%20stability.%20To%20resolve%20them%2C%0Awe%20introduce%20dedicated%20network%20designs%20to%20obtain%20proper%20representations%20for%20the%0ANeLF%20model%20and%20maintain%20a%20low%20FLOPs%20budget.%20Meanwhile%2C%20we%20tap%20into%20a%0Adistillation-based%20training%20strategy%20that%20uses%20a%20pretrained%20avatar%20model%20as%0Ateacher%20to%20synthesize%20abundant%20pseudo%20data%20for%20training.%20A%20warping%20field%0Anetwork%20is%20introduced%20to%20correct%20the%20fitting%20error%20in%20the%20real%20data%20so%20that%20the%0Amodel%20can%20learn%20better.%20Extensive%20experiments%20suggest%20that%20our%20method%20can%0Aachieve%20new%20SOTA%20image%20quality%20quantitatively%20or%20qualitatively%2C%20while%20being%0Asignificantly%20faster%20than%20the%20counterparts%2C%20reporting%20174.1%20FPS%20%28512x512%0Aresolution%29%20on%20a%20consumer-grade%20GPU%20%28RTX3090%29%20with%20no%20customized%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18057v1&entry.124074799=Read"},
{"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "author": "Ruixin Hong and Hongming Zhang and Xiaoman Pan and Dong Yu and Changshui Zhang", "abstract": "  Abstract reasoning, the ability to reason from the abstract essence of a\nproblem, serves as a key to generalization in human reasoning. However,\neliciting language models to perform reasoning with abstraction remains\nunexplored. This paper seeks to bridge this gap by introducing a novel\nstructured reasoning format called Abstraction-of-Thought (AoT). The uniqueness\nof AoT lies in its explicit requirement for varying levels of abstraction\nwithin the reasoning process. This approach could elicit language models to\nfirst contemplate on the abstract level before incorporating concrete details,\nwhich is overlooked by the prevailing step-by-step Chain-of-Thought (CoT)\nmethod. To align models with the AoT format, we present AoT Collection, a\ngeneric finetuning dataset consisting of 348k high-quality samples with AoT\nreasoning processes, collected via an automated and scalable pipeline. We\nfinetune a wide range of language models with AoT Collection and conduct\nextensive evaluations on 23 unseen tasks from the challenging benchmark\nBig-Bench Hard. Experimental results indicate that models aligned to AoT\nreasoning format substantially outperform those aligned to CoT in many\nreasoning tasks.\n", "link": "http://arxiv.org/abs/2406.12442v2", "date": "2024-09-26", "relevancy": 2.4992, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstraction-of-Thought%20Makes%20Language%20Models%20Better%20Reasoners&body=Title%3A%20Abstraction-of-Thought%20Makes%20Language%20Models%20Better%20Reasoners%0AAuthor%3A%20Ruixin%20Hong%20and%20Hongming%20Zhang%20and%20Xiaoman%20Pan%20and%20Dong%20Yu%20and%20Changshui%20Zhang%0AAbstract%3A%20%20%20Abstract%20reasoning%2C%20the%20ability%20to%20reason%20from%20the%20abstract%20essence%20of%20a%0Aproblem%2C%20serves%20as%20a%20key%20to%20generalization%20in%20human%20reasoning.%20However%2C%0Aeliciting%20language%20models%20to%20perform%20reasoning%20with%20abstraction%20remains%0Aunexplored.%20This%20paper%20seeks%20to%20bridge%20this%20gap%20by%20introducing%20a%20novel%0Astructured%20reasoning%20format%20called%20Abstraction-of-Thought%20%28AoT%29.%20The%20uniqueness%0Aof%20AoT%20lies%20in%20its%20explicit%20requirement%20for%20varying%20levels%20of%20abstraction%0Awithin%20the%20reasoning%20process.%20This%20approach%20could%20elicit%20language%20models%20to%0Afirst%20contemplate%20on%20the%20abstract%20level%20before%20incorporating%20concrete%20details%2C%0Awhich%20is%20overlooked%20by%20the%20prevailing%20step-by-step%20Chain-of-Thought%20%28CoT%29%0Amethod.%20To%20align%20models%20with%20the%20AoT%20format%2C%20we%20present%20AoT%20Collection%2C%20a%0Ageneric%20finetuning%20dataset%20consisting%20of%20348k%20high-quality%20samples%20with%20AoT%0Areasoning%20processes%2C%20collected%20via%20an%20automated%20and%20scalable%20pipeline.%20We%0Afinetune%20a%20wide%20range%20of%20language%20models%20with%20AoT%20Collection%20and%20conduct%0Aextensive%20evaluations%20on%2023%20unseen%20tasks%20from%20the%20challenging%20benchmark%0ABig-Bench%20Hard.%20Experimental%20results%20indicate%20that%20models%20aligned%20to%20AoT%0Areasoning%20format%20substantially%20outperform%20those%20aligned%20to%20CoT%20in%20many%0Areasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstraction-of-Thought%2520Makes%2520Language%2520Models%2520Better%2520Reasoners%26entry.906535625%3DRuixin%2520Hong%2520and%2520Hongming%2520Zhang%2520and%2520Xiaoman%2520Pan%2520and%2520Dong%2520Yu%2520and%2520Changshui%2520Zhang%26entry.1292438233%3D%2520%2520Abstract%2520reasoning%252C%2520the%2520ability%2520to%2520reason%2520from%2520the%2520abstract%2520essence%2520of%2520a%250Aproblem%252C%2520serves%2520as%2520a%2520key%2520to%2520generalization%2520in%2520human%2520reasoning.%2520However%252C%250Aeliciting%2520language%2520models%2520to%2520perform%2520reasoning%2520with%2520abstraction%2520remains%250Aunexplored.%2520This%2520paper%2520seeks%2520to%2520bridge%2520this%2520gap%2520by%2520introducing%2520a%2520novel%250Astructured%2520reasoning%2520format%2520called%2520Abstraction-of-Thought%2520%2528AoT%2529.%2520The%2520uniqueness%250Aof%2520AoT%2520lies%2520in%2520its%2520explicit%2520requirement%2520for%2520varying%2520levels%2520of%2520abstraction%250Awithin%2520the%2520reasoning%2520process.%2520This%2520approach%2520could%2520elicit%2520language%2520models%2520to%250Afirst%2520contemplate%2520on%2520the%2520abstract%2520level%2520before%2520incorporating%2520concrete%2520details%252C%250Awhich%2520is%2520overlooked%2520by%2520the%2520prevailing%2520step-by-step%2520Chain-of-Thought%2520%2528CoT%2529%250Amethod.%2520To%2520align%2520models%2520with%2520the%2520AoT%2520format%252C%2520we%2520present%2520AoT%2520Collection%252C%2520a%250Ageneric%2520finetuning%2520dataset%2520consisting%2520of%2520348k%2520high-quality%2520samples%2520with%2520AoT%250Areasoning%2520processes%252C%2520collected%2520via%2520an%2520automated%2520and%2520scalable%2520pipeline.%2520We%250Afinetune%2520a%2520wide%2520range%2520of%2520language%2520models%2520with%2520AoT%2520Collection%2520and%2520conduct%250Aextensive%2520evaluations%2520on%252023%2520unseen%2520tasks%2520from%2520the%2520challenging%2520benchmark%250ABig-Bench%2520Hard.%2520Experimental%2520results%2520indicate%2520that%2520models%2520aligned%2520to%2520AoT%250Areasoning%2520format%2520substantially%2520outperform%2520those%2520aligned%2520to%2520CoT%2520in%2520many%250Areasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstraction-of-Thought%20Makes%20Language%20Models%20Better%20Reasoners&entry.906535625=Ruixin%20Hong%20and%20Hongming%20Zhang%20and%20Xiaoman%20Pan%20and%20Dong%20Yu%20and%20Changshui%20Zhang&entry.1292438233=%20%20Abstract%20reasoning%2C%20the%20ability%20to%20reason%20from%20the%20abstract%20essence%20of%20a%0Aproblem%2C%20serves%20as%20a%20key%20to%20generalization%20in%20human%20reasoning.%20However%2C%0Aeliciting%20language%20models%20to%20perform%20reasoning%20with%20abstraction%20remains%0Aunexplored.%20This%20paper%20seeks%20to%20bridge%20this%20gap%20by%20introducing%20a%20novel%0Astructured%20reasoning%20format%20called%20Abstraction-of-Thought%20%28AoT%29.%20The%20uniqueness%0Aof%20AoT%20lies%20in%20its%20explicit%20requirement%20for%20varying%20levels%20of%20abstraction%0Awithin%20the%20reasoning%20process.%20This%20approach%20could%20elicit%20language%20models%20to%0Afirst%20contemplate%20on%20the%20abstract%20level%20before%20incorporating%20concrete%20details%2C%0Awhich%20is%20overlooked%20by%20the%20prevailing%20step-by-step%20Chain-of-Thought%20%28CoT%29%0Amethod.%20To%20align%20models%20with%20the%20AoT%20format%2C%20we%20present%20AoT%20Collection%2C%20a%0Ageneric%20finetuning%20dataset%20consisting%20of%20348k%20high-quality%20samples%20with%20AoT%0Areasoning%20processes%2C%20collected%20via%20an%20automated%20and%20scalable%20pipeline.%20We%0Afinetune%20a%20wide%20range%20of%20language%20models%20with%20AoT%20Collection%20and%20conduct%0Aextensive%20evaluations%20on%2023%20unseen%20tasks%20from%20the%20challenging%20benchmark%0ABig-Bench%20Hard.%20Experimental%20results%20indicate%20that%20models%20aligned%20to%20AoT%0Areasoning%20format%20substantially%20outperform%20those%20aligned%20to%20CoT%20in%20many%0Areasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12442v2&entry.124074799=Read"},
{"title": "Low-Rank Interconnected Adaptation across Layers", "author": "Yibo Zhong and Yao Zhou", "abstract": "  Low-rank adaptation (LoRA) is a powerful parameter-efficient fine-tuning\nmethod that utilizes low-rank projectors $A$ and $B$ to learn weight updates\n$\\Delta W$ for adaptation targets $W$. Previous research has shown that LoRA is\nessentially a gradient compressor, performing random projections on the\ngradient using a fixed projection matrix $A_0$. However, this setup restricts\nthe overall weight update to be low-rank, which limits the adaptation\nperformance. In this paper, we propose low-rank interconnected adaptation\nacross layers (Lily). Specifically, we employ a hierarchical framework where\nlow-dimensional projectors (LPs) retained for downward projection at a\nparticular level, while globally-shared high-dimensional projector (HP) experts\nperform upward projection across all levels of layers. Lily uniquely connects\neach LP to all HP experts, therefore the gradient projections are no longer\ndominated by fixed projection matrices, but rather by selective combinations of\nall the projectors, thereby breaking the low-rank constraint of LoRA.\nFurthermore, Lily's cross-layer connections facilitate the capture of intricate\ninformation and dependencies across different layers, thereby enhancing the\nmodel's representational capabilities. Experiments across various modalities,\narchitectures, and model sizes underscore Lily's great performance and\nefficiency. Code is available on github https://github.com/yibozhong/lily.\n", "link": "http://arxiv.org/abs/2407.09946v2", "date": "2024-09-26", "relevancy": 2.4935, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4967}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Interconnected%20Adaptation%20across%20Layers&body=Title%3A%20Low-Rank%20Interconnected%20Adaptation%20across%20Layers%0AAuthor%3A%20Yibo%20Zhong%20and%20Yao%20Zhou%0AAbstract%3A%20%20%20Low-rank%20adaptation%20%28LoRA%29%20is%20a%20powerful%20parameter-efficient%20fine-tuning%0Amethod%20that%20utilizes%20low-rank%20projectors%20%24A%24%20and%20%24B%24%20to%20learn%20weight%20updates%0A%24%5CDelta%20W%24%20for%20adaptation%20targets%20%24W%24.%20Previous%20research%20has%20shown%20that%20LoRA%20is%0Aessentially%20a%20gradient%20compressor%2C%20performing%20random%20projections%20on%20the%0Agradient%20using%20a%20fixed%20projection%20matrix%20%24A_0%24.%20However%2C%20this%20setup%20restricts%0Athe%20overall%20weight%20update%20to%20be%20low-rank%2C%20which%20limits%20the%20adaptation%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20low-rank%20interconnected%20adaptation%0Aacross%20layers%20%28Lily%29.%20Specifically%2C%20we%20employ%20a%20hierarchical%20framework%20where%0Alow-dimensional%20projectors%20%28LPs%29%20retained%20for%20downward%20projection%20at%20a%0Aparticular%20level%2C%20while%20globally-shared%20high-dimensional%20projector%20%28HP%29%20experts%0Aperform%20upward%20projection%20across%20all%20levels%20of%20layers.%20Lily%20uniquely%20connects%0Aeach%20LP%20to%20all%20HP%20experts%2C%20therefore%20the%20gradient%20projections%20are%20no%20longer%0Adominated%20by%20fixed%20projection%20matrices%2C%20but%20rather%20by%20selective%20combinations%20of%0Aall%20the%20projectors%2C%20thereby%20breaking%20the%20low-rank%20constraint%20of%20LoRA.%0AFurthermore%2C%20Lily%27s%20cross-layer%20connections%20facilitate%20the%20capture%20of%20intricate%0Ainformation%20and%20dependencies%20across%20different%20layers%2C%20thereby%20enhancing%20the%0Amodel%27s%20representational%20capabilities.%20Experiments%20across%20various%20modalities%2C%0Aarchitectures%2C%20and%20model%20sizes%20underscore%20Lily%27s%20great%20performance%20and%0Aefficiency.%20Code%20is%20available%20on%20github%20https%3A//github.com/yibozhong/lily.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09946v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Interconnected%2520Adaptation%2520across%2520Layers%26entry.906535625%3DYibo%2520Zhong%2520and%2520Yao%2520Zhou%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%2520%2528LoRA%2529%2520is%2520a%2520powerful%2520parameter-efficient%2520fine-tuning%250Amethod%2520that%2520utilizes%2520low-rank%2520projectors%2520%2524A%2524%2520and%2520%2524B%2524%2520to%2520learn%2520weight%2520updates%250A%2524%255CDelta%2520W%2524%2520for%2520adaptation%2520targets%2520%2524W%2524.%2520Previous%2520research%2520has%2520shown%2520that%2520LoRA%2520is%250Aessentially%2520a%2520gradient%2520compressor%252C%2520performing%2520random%2520projections%2520on%2520the%250Agradient%2520using%2520a%2520fixed%2520projection%2520matrix%2520%2524A_0%2524.%2520However%252C%2520this%2520setup%2520restricts%250Athe%2520overall%2520weight%2520update%2520to%2520be%2520low-rank%252C%2520which%2520limits%2520the%2520adaptation%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520low-rank%2520interconnected%2520adaptation%250Aacross%2520layers%2520%2528Lily%2529.%2520Specifically%252C%2520we%2520employ%2520a%2520hierarchical%2520framework%2520where%250Alow-dimensional%2520projectors%2520%2528LPs%2529%2520retained%2520for%2520downward%2520projection%2520at%2520a%250Aparticular%2520level%252C%2520while%2520globally-shared%2520high-dimensional%2520projector%2520%2528HP%2529%2520experts%250Aperform%2520upward%2520projection%2520across%2520all%2520levels%2520of%2520layers.%2520Lily%2520uniquely%2520connects%250Aeach%2520LP%2520to%2520all%2520HP%2520experts%252C%2520therefore%2520the%2520gradient%2520projections%2520are%2520no%2520longer%250Adominated%2520by%2520fixed%2520projection%2520matrices%252C%2520but%2520rather%2520by%2520selective%2520combinations%2520of%250Aall%2520the%2520projectors%252C%2520thereby%2520breaking%2520the%2520low-rank%2520constraint%2520of%2520LoRA.%250AFurthermore%252C%2520Lily%2527s%2520cross-layer%2520connections%2520facilitate%2520the%2520capture%2520of%2520intricate%250Ainformation%2520and%2520dependencies%2520across%2520different%2520layers%252C%2520thereby%2520enhancing%2520the%250Amodel%2527s%2520representational%2520capabilities.%2520Experiments%2520across%2520various%2520modalities%252C%250Aarchitectures%252C%2520and%2520model%2520sizes%2520underscore%2520Lily%2527s%2520great%2520performance%2520and%250Aefficiency.%2520Code%2520is%2520available%2520on%2520github%2520https%253A//github.com/yibozhong/lily.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09946v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Interconnected%20Adaptation%20across%20Layers&entry.906535625=Yibo%20Zhong%20and%20Yao%20Zhou&entry.1292438233=%20%20Low-rank%20adaptation%20%28LoRA%29%20is%20a%20powerful%20parameter-efficient%20fine-tuning%0Amethod%20that%20utilizes%20low-rank%20projectors%20%24A%24%20and%20%24B%24%20to%20learn%20weight%20updates%0A%24%5CDelta%20W%24%20for%20adaptation%20targets%20%24W%24.%20Previous%20research%20has%20shown%20that%20LoRA%20is%0Aessentially%20a%20gradient%20compressor%2C%20performing%20random%20projections%20on%20the%0Agradient%20using%20a%20fixed%20projection%20matrix%20%24A_0%24.%20However%2C%20this%20setup%20restricts%0Athe%20overall%20weight%20update%20to%20be%20low-rank%2C%20which%20limits%20the%20adaptation%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20low-rank%20interconnected%20adaptation%0Aacross%20layers%20%28Lily%29.%20Specifically%2C%20we%20employ%20a%20hierarchical%20framework%20where%0Alow-dimensional%20projectors%20%28LPs%29%20retained%20for%20downward%20projection%20at%20a%0Aparticular%20level%2C%20while%20globally-shared%20high-dimensional%20projector%20%28HP%29%20experts%0Aperform%20upward%20projection%20across%20all%20levels%20of%20layers.%20Lily%20uniquely%20connects%0Aeach%20LP%20to%20all%20HP%20experts%2C%20therefore%20the%20gradient%20projections%20are%20no%20longer%0Adominated%20by%20fixed%20projection%20matrices%2C%20but%20rather%20by%20selective%20combinations%20of%0Aall%20the%20projectors%2C%20thereby%20breaking%20the%20low-rank%20constraint%20of%20LoRA.%0AFurthermore%2C%20Lily%27s%20cross-layer%20connections%20facilitate%20the%20capture%20of%20intricate%0Ainformation%20and%20dependencies%20across%20different%20layers%2C%20thereby%20enhancing%20the%0Amodel%27s%20representational%20capabilities.%20Experiments%20across%20various%20modalities%2C%0Aarchitectures%2C%20and%20model%20sizes%20underscore%20Lily%27s%20great%20performance%20and%0Aefficiency.%20Code%20is%20available%20on%20github%20https%3A//github.com/yibozhong/lily.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09946v2&entry.124074799=Read"},
{"title": "Unsupervisedly Learned Representations: Should the Quest be Over?", "author": "Daniel N. Nissani", "abstract": "  After four decades of research there still exists a Classification accuracy\ngap of about 20% between our best Unsupervisedly Learned Representations\nmethods and the accuracy rates achieved by intelligent animals. It thus may\nwell be that we are looking in the wrong direction. A possible solution to this\npuzzle is presented. We demonstrate that Reinforcement Learning can learn\nrepresentations which achieve the same accuracy as that of animals. Our main\nmodest contribution lies in the observations that: a. when applied to a real\nworld environment Reinforcement Learning does not require labels, and thus may\nbe legitimately considered as Unsupervised Learning, and b. in contrast, when\nReinforcement Learning is applied in a simulated environment it does inherently\nrequire labels and should thus be generally be considered as Supervised\nLearning. The corollary of these observations is that further search for\nUnsupervised Learning competitive paradigms which may be trained in simulated\nenvironments may be futile.\n", "link": "http://arxiv.org/abs/2001.07495v5", "date": "2024-09-26", "relevancy": 2.4544, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5133}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4861}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervisedly%20Learned%20Representations%3A%20Should%20the%20Quest%20be%20Over%3F&body=Title%3A%20Unsupervisedly%20Learned%20Representations%3A%20Should%20the%20Quest%20be%20Over%3F%0AAuthor%3A%20Daniel%20N.%20Nissani%0AAbstract%3A%20%20%20After%20four%20decades%20of%20research%20there%20still%20exists%20a%20Classification%20accuracy%0Agap%20of%20about%2020%25%20between%20our%20best%20Unsupervisedly%20Learned%20Representations%0Amethods%20and%20the%20accuracy%20rates%20achieved%20by%20intelligent%20animals.%20It%20thus%20may%0Awell%20be%20that%20we%20are%20looking%20in%20the%20wrong%20direction.%20A%20possible%20solution%20to%20this%0Apuzzle%20is%20presented.%20We%20demonstrate%20that%20Reinforcement%20Learning%20can%20learn%0Arepresentations%20which%20achieve%20the%20same%20accuracy%20as%20that%20of%20animals.%20Our%20main%0Amodest%20contribution%20lies%20in%20the%20observations%20that%3A%20a.%20when%20applied%20to%20a%20real%0Aworld%20environment%20Reinforcement%20Learning%20does%20not%20require%20labels%2C%20and%20thus%20may%0Abe%20legitimately%20considered%20as%20Unsupervised%20Learning%2C%20and%20b.%20in%20contrast%2C%20when%0AReinforcement%20Learning%20is%20applied%20in%20a%20simulated%20environment%20it%20does%20inherently%0Arequire%20labels%20and%20should%20thus%20be%20generally%20be%20considered%20as%20Supervised%0ALearning.%20The%20corollary%20of%20these%20observations%20is%20that%20further%20search%20for%0AUnsupervised%20Learning%20competitive%20paradigms%20which%20may%20be%20trained%20in%20simulated%0Aenvironments%20may%20be%20futile.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2001.07495v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervisedly%2520Learned%2520Representations%253A%2520Should%2520the%2520Quest%2520be%2520Over%253F%26entry.906535625%3DDaniel%2520N.%2520Nissani%26entry.1292438233%3D%2520%2520After%2520four%2520decades%2520of%2520research%2520there%2520still%2520exists%2520a%2520Classification%2520accuracy%250Agap%2520of%2520about%252020%2525%2520between%2520our%2520best%2520Unsupervisedly%2520Learned%2520Representations%250Amethods%2520and%2520the%2520accuracy%2520rates%2520achieved%2520by%2520intelligent%2520animals.%2520It%2520thus%2520may%250Awell%2520be%2520that%2520we%2520are%2520looking%2520in%2520the%2520wrong%2520direction.%2520A%2520possible%2520solution%2520to%2520this%250Apuzzle%2520is%2520presented.%2520We%2520demonstrate%2520that%2520Reinforcement%2520Learning%2520can%2520learn%250Arepresentations%2520which%2520achieve%2520the%2520same%2520accuracy%2520as%2520that%2520of%2520animals.%2520Our%2520main%250Amodest%2520contribution%2520lies%2520in%2520the%2520observations%2520that%253A%2520a.%2520when%2520applied%2520to%2520a%2520real%250Aworld%2520environment%2520Reinforcement%2520Learning%2520does%2520not%2520require%2520labels%252C%2520and%2520thus%2520may%250Abe%2520legitimately%2520considered%2520as%2520Unsupervised%2520Learning%252C%2520and%2520b.%2520in%2520contrast%252C%2520when%250AReinforcement%2520Learning%2520is%2520applied%2520in%2520a%2520simulated%2520environment%2520it%2520does%2520inherently%250Arequire%2520labels%2520and%2520should%2520thus%2520be%2520generally%2520be%2520considered%2520as%2520Supervised%250ALearning.%2520The%2520corollary%2520of%2520these%2520observations%2520is%2520that%2520further%2520search%2520for%250AUnsupervised%2520Learning%2520competitive%2520paradigms%2520which%2520may%2520be%2520trained%2520in%2520simulated%250Aenvironments%2520may%2520be%2520futile.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2001.07495v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervisedly%20Learned%20Representations%3A%20Should%20the%20Quest%20be%20Over%3F&entry.906535625=Daniel%20N.%20Nissani&entry.1292438233=%20%20After%20four%20decades%20of%20research%20there%20still%20exists%20a%20Classification%20accuracy%0Agap%20of%20about%2020%25%20between%20our%20best%20Unsupervisedly%20Learned%20Representations%0Amethods%20and%20the%20accuracy%20rates%20achieved%20by%20intelligent%20animals.%20It%20thus%20may%0Awell%20be%20that%20we%20are%20looking%20in%20the%20wrong%20direction.%20A%20possible%20solution%20to%20this%0Apuzzle%20is%20presented.%20We%20demonstrate%20that%20Reinforcement%20Learning%20can%20learn%0Arepresentations%20which%20achieve%20the%20same%20accuracy%20as%20that%20of%20animals.%20Our%20main%0Amodest%20contribution%20lies%20in%20the%20observations%20that%3A%20a.%20when%20applied%20to%20a%20real%0Aworld%20environment%20Reinforcement%20Learning%20does%20not%20require%20labels%2C%20and%20thus%20may%0Abe%20legitimately%20considered%20as%20Unsupervised%20Learning%2C%20and%20b.%20in%20contrast%2C%20when%0AReinforcement%20Learning%20is%20applied%20in%20a%20simulated%20environment%20it%20does%20inherently%0Arequire%20labels%20and%20should%20thus%20be%20generally%20be%20considered%20as%20Supervised%0ALearning.%20The%20corollary%20of%20these%20observations%20is%20that%20further%20search%20for%0AUnsupervised%20Learning%20competitive%20paradigms%20which%20may%20be%20trained%20in%20simulated%0Aenvironments%20may%20be%20futile.%0A&entry.1838667208=http%3A//arxiv.org/abs/2001.07495v5&entry.124074799=Read"},
{"title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding", "author": "Ye Liu and Zongyang Ma and Zhongang Qi and Yang Wu and Ying Shan and Chang Wen Chen", "abstract": "  Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios.\n", "link": "http://arxiv.org/abs/2409.18111v1", "date": "2024-09-26", "relevancy": 2.4401, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E.T.%20Bench%3A%20Towards%20Open-Ended%20Event-Level%20Video-Language%20Understanding&body=Title%3A%20E.T.%20Bench%3A%20Towards%20Open-Ended%20Event-Level%20Video-Language%20Understanding%0AAuthor%3A%20Ye%20Liu%20and%20Zongyang%20Ma%20and%20Zhongang%20Qi%20and%20Yang%20Wu%20and%20Ying%20Shan%20and%20Chang%20Wen%20Chen%0AAbstract%3A%20%20%20Recent%20advances%20in%20Video%20Large%20Language%20Models%20%28Video-LLMs%29%20have%20demonstrated%0Atheir%20great%20potential%20in%20general-purpose%20video%20understanding.%20To%20verify%20the%0Asignificance%20of%20these%20models%2C%20a%20number%20of%20benchmarks%20have%20been%20proposed%20to%0Adiagnose%20their%20capabilities%20in%20different%20scenarios.%20However%2C%20existing%0Abenchmarks%20merely%20evaluate%20models%20through%20video-level%20question-answering%2C%0Alacking%20fine-grained%20event-level%20assessment%20and%20task%20diversity.%20To%20fill%20this%0Agap%2C%20we%20introduce%20E.T.%20Bench%20%28Event-Level%20%26%20Time-Sensitive%20Video%20Understanding%0ABenchmark%29%2C%20a%20large-scale%20and%20high-quality%20benchmark%20for%20open-ended%20event-level%0Avideo%20understanding.%20Categorized%20within%20a%203-level%20task%20taxonomy%2C%20E.T.%20Bench%0Aencompasses%207.3K%20samples%20under%2012%20tasks%20with%207K%20videos%20%28251.4h%20total%20length%29%0Aunder%208%20domains%2C%20providing%20comprehensive%20evaluations.%20We%20extensively%20evaluated%0A8%20Image-LLMs%20and%2012%20Video-LLMs%20on%20our%20benchmark%2C%20and%20the%20results%20reveal%20that%0Astate-of-the-art%20models%20for%20coarse-level%20%28video-level%29%20understanding%20struggle%0Ato%20solve%20our%20fine-grained%20tasks%2C%20e.g.%2C%20grounding%20event-of-interests%20within%0Avideos%2C%20largely%20due%20to%20the%20short%20video%20context%20length%2C%20improper%20time%0Arepresentations%2C%20and%20lack%20of%20multi-event%20training%20data.%20Focusing%20on%20these%0Aissues%2C%20we%20further%20propose%20a%20strong%20baseline%20model%2C%20E.T.%20Chat%2C%20together%20with%20an%0Ainstruction-tuning%20dataset%20E.T.%20Instruct%20164K%20tailored%20for%20fine-grained%0Aevent-level%20understanding.%20Our%20simple%20but%20effective%20solution%20demonstrates%0Asuperior%20performance%20in%20multiple%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE.T.%2520Bench%253A%2520Towards%2520Open-Ended%2520Event-Level%2520Video-Language%2520Understanding%26entry.906535625%3DYe%2520Liu%2520and%2520Zongyang%2520Ma%2520and%2520Zhongang%2520Qi%2520and%2520Yang%2520Wu%2520and%2520Ying%2520Shan%2520and%2520Chang%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Video%2520Large%2520Language%2520Models%2520%2528Video-LLMs%2529%2520have%2520demonstrated%250Atheir%2520great%2520potential%2520in%2520general-purpose%2520video%2520understanding.%2520To%2520verify%2520the%250Asignificance%2520of%2520these%2520models%252C%2520a%2520number%2520of%2520benchmarks%2520have%2520been%2520proposed%2520to%250Adiagnose%2520their%2520capabilities%2520in%2520different%2520scenarios.%2520However%252C%2520existing%250Abenchmarks%2520merely%2520evaluate%2520models%2520through%2520video-level%2520question-answering%252C%250Alacking%2520fine-grained%2520event-level%2520assessment%2520and%2520task%2520diversity.%2520To%2520fill%2520this%250Agap%252C%2520we%2520introduce%2520E.T.%2520Bench%2520%2528Event-Level%2520%2526%2520Time-Sensitive%2520Video%2520Understanding%250ABenchmark%2529%252C%2520a%2520large-scale%2520and%2520high-quality%2520benchmark%2520for%2520open-ended%2520event-level%250Avideo%2520understanding.%2520Categorized%2520within%2520a%25203-level%2520task%2520taxonomy%252C%2520E.T.%2520Bench%250Aencompasses%25207.3K%2520samples%2520under%252012%2520tasks%2520with%25207K%2520videos%2520%2528251.4h%2520total%2520length%2529%250Aunder%25208%2520domains%252C%2520providing%2520comprehensive%2520evaluations.%2520We%2520extensively%2520evaluated%250A8%2520Image-LLMs%2520and%252012%2520Video-LLMs%2520on%2520our%2520benchmark%252C%2520and%2520the%2520results%2520reveal%2520that%250Astate-of-the-art%2520models%2520for%2520coarse-level%2520%2528video-level%2529%2520understanding%2520struggle%250Ato%2520solve%2520our%2520fine-grained%2520tasks%252C%2520e.g.%252C%2520grounding%2520event-of-interests%2520within%250Avideos%252C%2520largely%2520due%2520to%2520the%2520short%2520video%2520context%2520length%252C%2520improper%2520time%250Arepresentations%252C%2520and%2520lack%2520of%2520multi-event%2520training%2520data.%2520Focusing%2520on%2520these%250Aissues%252C%2520we%2520further%2520propose%2520a%2520strong%2520baseline%2520model%252C%2520E.T.%2520Chat%252C%2520together%2520with%2520an%250Ainstruction-tuning%2520dataset%2520E.T.%2520Instruct%2520164K%2520tailored%2520for%2520fine-grained%250Aevent-level%2520understanding.%2520Our%2520simple%2520but%2520effective%2520solution%2520demonstrates%250Asuperior%2520performance%2520in%2520multiple%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E.T.%20Bench%3A%20Towards%20Open-Ended%20Event-Level%20Video-Language%20Understanding&entry.906535625=Ye%20Liu%20and%20Zongyang%20Ma%20and%20Zhongang%20Qi%20and%20Yang%20Wu%20and%20Ying%20Shan%20and%20Chang%20Wen%20Chen&entry.1292438233=%20%20Recent%20advances%20in%20Video%20Large%20Language%20Models%20%28Video-LLMs%29%20have%20demonstrated%0Atheir%20great%20potential%20in%20general-purpose%20video%20understanding.%20To%20verify%20the%0Asignificance%20of%20these%20models%2C%20a%20number%20of%20benchmarks%20have%20been%20proposed%20to%0Adiagnose%20their%20capabilities%20in%20different%20scenarios.%20However%2C%20existing%0Abenchmarks%20merely%20evaluate%20models%20through%20video-level%20question-answering%2C%0Alacking%20fine-grained%20event-level%20assessment%20and%20task%20diversity.%20To%20fill%20this%0Agap%2C%20we%20introduce%20E.T.%20Bench%20%28Event-Level%20%26%20Time-Sensitive%20Video%20Understanding%0ABenchmark%29%2C%20a%20large-scale%20and%20high-quality%20benchmark%20for%20open-ended%20event-level%0Avideo%20understanding.%20Categorized%20within%20a%203-level%20task%20taxonomy%2C%20E.T.%20Bench%0Aencompasses%207.3K%20samples%20under%2012%20tasks%20with%207K%20videos%20%28251.4h%20total%20length%29%0Aunder%208%20domains%2C%20providing%20comprehensive%20evaluations.%20We%20extensively%20evaluated%0A8%20Image-LLMs%20and%2012%20Video-LLMs%20on%20our%20benchmark%2C%20and%20the%20results%20reveal%20that%0Astate-of-the-art%20models%20for%20coarse-level%20%28video-level%29%20understanding%20struggle%0Ato%20solve%20our%20fine-grained%20tasks%2C%20e.g.%2C%20grounding%20event-of-interests%20within%0Avideos%2C%20largely%20due%20to%20the%20short%20video%20context%20length%2C%20improper%20time%0Arepresentations%2C%20and%20lack%20of%20multi-event%20training%20data.%20Focusing%20on%20these%0Aissues%2C%20we%20further%20propose%20a%20strong%20baseline%20model%2C%20E.T.%20Chat%2C%20together%20with%20an%0Ainstruction-tuning%20dataset%20E.T.%20Instruct%20164K%20tailored%20for%20fine-grained%0Aevent-level%20understanding.%20Our%20simple%20but%20effective%20solution%20demonstrates%0Asuperior%20performance%20in%20multiple%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18111v1&entry.124074799=Read"},
{"title": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and\n  Adaptive Disambiguate based Efficient Tree Search", "author": "Linzhuang Sun and Hao Liang and Wentao Zhang", "abstract": "  Large Language Models (LLMs) have exhibited exceptional performance across a\nbroad range of tasks and domains. However, they still encounter difficulties in\nsolving mathematical problems due to the rigorous and logical nature of\nmathematics. Previous studies have employed techniques such as supervised\nfine-tuning (SFT), prompt engineering, and search-based methods to improve the\nmathematical problem-solving abilities of LLMs. Despite these efforts, their\nperformance remains suboptimal and demands substantial computational resources.\nTo address this issue, we propose a novel approach, BEATS, to enhance\nmathematical problem-solving abilities. Our method leverages newly designed\nprompts that guide the model to iteratively rewrite, advance by one step, and\ngenerate answers based on previous steps. Additionally, we introduce a new\nback-verification technique that uses LLMs to validate the correctness of the\ngenerated answers. Furthermore, we employ a pruning tree search to optimize\nsearch time while achieving strong performance. Notably, our method improves\nQwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the\nMATH benchmark.\n", "link": "http://arxiv.org/abs/2409.17972v1", "date": "2024-09-26", "relevancy": 2.4301, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEATS%3A%20Optimizing%20LLM%20Mathematical%20Capabilities%20with%20BackVerify%20and%0A%20%20Adaptive%20Disambiguate%20based%20Efficient%20Tree%20Search&body=Title%3A%20BEATS%3A%20Optimizing%20LLM%20Mathematical%20Capabilities%20with%20BackVerify%20and%0A%20%20Adaptive%20Disambiguate%20based%20Efficient%20Tree%20Search%0AAuthor%3A%20Linzhuang%20Sun%20and%20Hao%20Liang%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Abroad%20range%20of%20tasks%20and%20domains.%20However%2C%20they%20still%20encounter%20difficulties%20in%0Asolving%20mathematical%20problems%20due%20to%20the%20rigorous%20and%20logical%20nature%20of%0Amathematics.%20Previous%20studies%20have%20employed%20techniques%20such%20as%20supervised%0Afine-tuning%20%28SFT%29%2C%20prompt%20engineering%2C%20and%20search-based%20methods%20to%20improve%20the%0Amathematical%20problem-solving%20abilities%20of%20LLMs.%20Despite%20these%20efforts%2C%20their%0Aperformance%20remains%20suboptimal%20and%20demands%20substantial%20computational%20resources.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20approach%2C%20BEATS%2C%20to%20enhance%0Amathematical%20problem-solving%20abilities.%20Our%20method%20leverages%20newly%20designed%0Aprompts%20that%20guide%20the%20model%20to%20iteratively%20rewrite%2C%20advance%20by%20one%20step%2C%20and%0Agenerate%20answers%20based%20on%20previous%20steps.%20Additionally%2C%20we%20introduce%20a%20new%0Aback-verification%20technique%20that%20uses%20LLMs%20to%20validate%20the%20correctness%20of%20the%0Agenerated%20answers.%20Furthermore%2C%20we%20employ%20a%20pruning%20tree%20search%20to%20optimize%0Asearch%20time%20while%20achieving%20strong%20performance.%20Notably%2C%20our%20method%20improves%0AQwen2-7b-Instruct%27s%20score%20from%2036.94%20to%2061.52%2C%20outperforming%20GPT4%27s%2042.5%20on%20the%0AMATH%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEATS%253A%2520Optimizing%2520LLM%2520Mathematical%2520Capabilities%2520with%2520BackVerify%2520and%250A%2520%2520Adaptive%2520Disambiguate%2520based%2520Efficient%2520Tree%2520Search%26entry.906535625%3DLinzhuang%2520Sun%2520and%2520Hao%2520Liang%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520exceptional%2520performance%2520across%2520a%250Abroad%2520range%2520of%2520tasks%2520and%2520domains.%2520However%252C%2520they%2520still%2520encounter%2520difficulties%2520in%250Asolving%2520mathematical%2520problems%2520due%2520to%2520the%2520rigorous%2520and%2520logical%2520nature%2520of%250Amathematics.%2520Previous%2520studies%2520have%2520employed%2520techniques%2520such%2520as%2520supervised%250Afine-tuning%2520%2528SFT%2529%252C%2520prompt%2520engineering%252C%2520and%2520search-based%2520methods%2520to%2520improve%2520the%250Amathematical%2520problem-solving%2520abilities%2520of%2520LLMs.%2520Despite%2520these%2520efforts%252C%2520their%250Aperformance%2520remains%2520suboptimal%2520and%2520demands%2520substantial%2520computational%2520resources.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520approach%252C%2520BEATS%252C%2520to%2520enhance%250Amathematical%2520problem-solving%2520abilities.%2520Our%2520method%2520leverages%2520newly%2520designed%250Aprompts%2520that%2520guide%2520the%2520model%2520to%2520iteratively%2520rewrite%252C%2520advance%2520by%2520one%2520step%252C%2520and%250Agenerate%2520answers%2520based%2520on%2520previous%2520steps.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%250Aback-verification%2520technique%2520that%2520uses%2520LLMs%2520to%2520validate%2520the%2520correctness%2520of%2520the%250Agenerated%2520answers.%2520Furthermore%252C%2520we%2520employ%2520a%2520pruning%2520tree%2520search%2520to%2520optimize%250Asearch%2520time%2520while%2520achieving%2520strong%2520performance.%2520Notably%252C%2520our%2520method%2520improves%250AQwen2-7b-Instruct%2527s%2520score%2520from%252036.94%2520to%252061.52%252C%2520outperforming%2520GPT4%2527s%252042.5%2520on%2520the%250AMATH%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEATS%3A%20Optimizing%20LLM%20Mathematical%20Capabilities%20with%20BackVerify%20and%0A%20%20Adaptive%20Disambiguate%20based%20Efficient%20Tree%20Search&entry.906535625=Linzhuang%20Sun%20and%20Hao%20Liang%20and%20Wentao%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Abroad%20range%20of%20tasks%20and%20domains.%20However%2C%20they%20still%20encounter%20difficulties%20in%0Asolving%20mathematical%20problems%20due%20to%20the%20rigorous%20and%20logical%20nature%20of%0Amathematics.%20Previous%20studies%20have%20employed%20techniques%20such%20as%20supervised%0Afine-tuning%20%28SFT%29%2C%20prompt%20engineering%2C%20and%20search-based%20methods%20to%20improve%20the%0Amathematical%20problem-solving%20abilities%20of%20LLMs.%20Despite%20these%20efforts%2C%20their%0Aperformance%20remains%20suboptimal%20and%20demands%20substantial%20computational%20resources.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20approach%2C%20BEATS%2C%20to%20enhance%0Amathematical%20problem-solving%20abilities.%20Our%20method%20leverages%20newly%20designed%0Aprompts%20that%20guide%20the%20model%20to%20iteratively%20rewrite%2C%20advance%20by%20one%20step%2C%20and%0Agenerate%20answers%20based%20on%20previous%20steps.%20Additionally%2C%20we%20introduce%20a%20new%0Aback-verification%20technique%20that%20uses%20LLMs%20to%20validate%20the%20correctness%20of%20the%0Agenerated%20answers.%20Furthermore%2C%20we%20employ%20a%20pruning%20tree%20search%20to%20optimize%0Asearch%20time%20while%20achieving%20strong%20performance.%20Notably%2C%20our%20method%20improves%0AQwen2-7b-Instruct%27s%20score%20from%2036.94%20to%2061.52%2C%20outperforming%20GPT4%27s%2042.5%20on%20the%0AMATH%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17972v1&entry.124074799=Read"},
{"title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal\n  Imagery of South African Rhino Habitats", "author": "Lucia Gordon and Nikhil Behari and Samuel Collier and Elizabeth Bondi-Kelly and Jackson A. Killian and Catherine Ressijac and Peter Boucher and Andrew Davies and Milind Tambe", "abstract": "  Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7.\n", "link": "http://arxiv.org/abs/2409.18104v1", "date": "2024-09-26", "relevancy": 2.425, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4804}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Find%20Rhinos%20without%20Finding%20Rhinos%3A%20Active%20Learning%20with%20Multimodal%0A%20%20Imagery%20of%20South%20African%20Rhino%20Habitats&body=Title%3A%20Find%20Rhinos%20without%20Finding%20Rhinos%3A%20Active%20Learning%20with%20Multimodal%0A%20%20Imagery%20of%20South%20African%20Rhino%20Habitats%0AAuthor%3A%20Lucia%20Gordon%20and%20Nikhil%20Behari%20and%20Samuel%20Collier%20and%20Elizabeth%20Bondi-Kelly%20and%20Jackson%20A.%20Killian%20and%20Catherine%20Ressijac%20and%20Peter%20Boucher%20and%20Andrew%20Davies%20and%20Milind%20Tambe%0AAbstract%3A%20%20%20Much%20of%20Earth%27s%20charismatic%20megafauna%20is%20endangered%20by%20human%20activities%2C%0Aparticularly%20the%20rhino%2C%20which%20is%20at%20risk%20of%20extinction%20due%20to%20the%20poaching%0Acrisis%20in%20Africa.%20Monitoring%20rhinos%27%20movement%20is%20crucial%20to%20their%20protection%0Abut%20has%20unfortunately%20proven%20difficult%20because%20rhinos%20are%20elusive.%20Therefore%2C%0Ainstead%20of%20tracking%20rhinos%2C%20we%20propose%20the%20novel%20approach%20of%20mapping%20communal%0Adefecation%20sites%2C%20called%20middens%2C%20which%20give%20information%20about%20rhinos%27%20spatial%0Abehavior%20valuable%20to%20anti-poaching%2C%20management%2C%20and%20reintroduction%20efforts.%0AThis%20paper%20provides%20the%20first-ever%20mapping%20of%20rhino%20midden%20locations%20by%0Abuilding%20classifiers%20to%20detect%20them%20using%20remotely%20sensed%20thermal%2C%20RGB%2C%20and%0ALiDAR%20imagery%20in%20passive%20and%20active%20learning%20settings.%20As%20existing%20active%0Alearning%20methods%20perform%20poorly%20due%20to%20the%20extreme%20class%20imbalance%20in%20our%0Adataset%2C%20we%20design%20MultimodAL%2C%20an%20active%20learning%20system%20employing%20a%20ranking%0Atechnique%20and%20multimodality%20to%20achieve%20competitive%20performance%20with%20passive%0Alearning%20models%20with%2094%25%20fewer%20labels.%20Our%20methods%20could%20therefore%20save%20over%2076%0Ahours%20in%20labeling%20time%20when%20used%20on%20a%20similarly-sized%20dataset.%20Unexpectedly%2C%0Aour%20midden%20map%20reveals%20that%20rhino%20middens%20are%20not%20randomly%20distributed%0Athroughout%20the%20landscape%3B%20rather%2C%20they%20are%20clustered.%20Consequently%2C%20rangers%0Ashould%20be%20targeted%20at%20areas%20with%20high%20midden%20densities%20to%20strengthen%0Aanti-poaching%20efforts%2C%20in%20line%20with%20UN%20Target%2015.7.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFind%2520Rhinos%2520without%2520Finding%2520Rhinos%253A%2520Active%2520Learning%2520with%2520Multimodal%250A%2520%2520Imagery%2520of%2520South%2520African%2520Rhino%2520Habitats%26entry.906535625%3DLucia%2520Gordon%2520and%2520Nikhil%2520Behari%2520and%2520Samuel%2520Collier%2520and%2520Elizabeth%2520Bondi-Kelly%2520and%2520Jackson%2520A.%2520Killian%2520and%2520Catherine%2520Ressijac%2520and%2520Peter%2520Boucher%2520and%2520Andrew%2520Davies%2520and%2520Milind%2520Tambe%26entry.1292438233%3D%2520%2520Much%2520of%2520Earth%2527s%2520charismatic%2520megafauna%2520is%2520endangered%2520by%2520human%2520activities%252C%250Aparticularly%2520the%2520rhino%252C%2520which%2520is%2520at%2520risk%2520of%2520extinction%2520due%2520to%2520the%2520poaching%250Acrisis%2520in%2520Africa.%2520Monitoring%2520rhinos%2527%2520movement%2520is%2520crucial%2520to%2520their%2520protection%250Abut%2520has%2520unfortunately%2520proven%2520difficult%2520because%2520rhinos%2520are%2520elusive.%2520Therefore%252C%250Ainstead%2520of%2520tracking%2520rhinos%252C%2520we%2520propose%2520the%2520novel%2520approach%2520of%2520mapping%2520communal%250Adefecation%2520sites%252C%2520called%2520middens%252C%2520which%2520give%2520information%2520about%2520rhinos%2527%2520spatial%250Abehavior%2520valuable%2520to%2520anti-poaching%252C%2520management%252C%2520and%2520reintroduction%2520efforts.%250AThis%2520paper%2520provides%2520the%2520first-ever%2520mapping%2520of%2520rhino%2520midden%2520locations%2520by%250Abuilding%2520classifiers%2520to%2520detect%2520them%2520using%2520remotely%2520sensed%2520thermal%252C%2520RGB%252C%2520and%250ALiDAR%2520imagery%2520in%2520passive%2520and%2520active%2520learning%2520settings.%2520As%2520existing%2520active%250Alearning%2520methods%2520perform%2520poorly%2520due%2520to%2520the%2520extreme%2520class%2520imbalance%2520in%2520our%250Adataset%252C%2520we%2520design%2520MultimodAL%252C%2520an%2520active%2520learning%2520system%2520employing%2520a%2520ranking%250Atechnique%2520and%2520multimodality%2520to%2520achieve%2520competitive%2520performance%2520with%2520passive%250Alearning%2520models%2520with%252094%2525%2520fewer%2520labels.%2520Our%2520methods%2520could%2520therefore%2520save%2520over%252076%250Ahours%2520in%2520labeling%2520time%2520when%2520used%2520on%2520a%2520similarly-sized%2520dataset.%2520Unexpectedly%252C%250Aour%2520midden%2520map%2520reveals%2520that%2520rhino%2520middens%2520are%2520not%2520randomly%2520distributed%250Athroughout%2520the%2520landscape%253B%2520rather%252C%2520they%2520are%2520clustered.%2520Consequently%252C%2520rangers%250Ashould%2520be%2520targeted%2520at%2520areas%2520with%2520high%2520midden%2520densities%2520to%2520strengthen%250Aanti-poaching%2520efforts%252C%2520in%2520line%2520with%2520UN%2520Target%252015.7.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Find%20Rhinos%20without%20Finding%20Rhinos%3A%20Active%20Learning%20with%20Multimodal%0A%20%20Imagery%20of%20South%20African%20Rhino%20Habitats&entry.906535625=Lucia%20Gordon%20and%20Nikhil%20Behari%20and%20Samuel%20Collier%20and%20Elizabeth%20Bondi-Kelly%20and%20Jackson%20A.%20Killian%20and%20Catherine%20Ressijac%20and%20Peter%20Boucher%20and%20Andrew%20Davies%20and%20Milind%20Tambe&entry.1292438233=%20%20Much%20of%20Earth%27s%20charismatic%20megafauna%20is%20endangered%20by%20human%20activities%2C%0Aparticularly%20the%20rhino%2C%20which%20is%20at%20risk%20of%20extinction%20due%20to%20the%20poaching%0Acrisis%20in%20Africa.%20Monitoring%20rhinos%27%20movement%20is%20crucial%20to%20their%20protection%0Abut%20has%20unfortunately%20proven%20difficult%20because%20rhinos%20are%20elusive.%20Therefore%2C%0Ainstead%20of%20tracking%20rhinos%2C%20we%20propose%20the%20novel%20approach%20of%20mapping%20communal%0Adefecation%20sites%2C%20called%20middens%2C%20which%20give%20information%20about%20rhinos%27%20spatial%0Abehavior%20valuable%20to%20anti-poaching%2C%20management%2C%20and%20reintroduction%20efforts.%0AThis%20paper%20provides%20the%20first-ever%20mapping%20of%20rhino%20midden%20locations%20by%0Abuilding%20classifiers%20to%20detect%20them%20using%20remotely%20sensed%20thermal%2C%20RGB%2C%20and%0ALiDAR%20imagery%20in%20passive%20and%20active%20learning%20settings.%20As%20existing%20active%0Alearning%20methods%20perform%20poorly%20due%20to%20the%20extreme%20class%20imbalance%20in%20our%0Adataset%2C%20we%20design%20MultimodAL%2C%20an%20active%20learning%20system%20employing%20a%20ranking%0Atechnique%20and%20multimodality%20to%20achieve%20competitive%20performance%20with%20passive%0Alearning%20models%20with%2094%25%20fewer%20labels.%20Our%20methods%20could%20therefore%20save%20over%2076%0Ahours%20in%20labeling%20time%20when%20used%20on%20a%20similarly-sized%20dataset.%20Unexpectedly%2C%0Aour%20midden%20map%20reveals%20that%20rhino%20middens%20are%20not%20randomly%20distributed%0Athroughout%20the%20landscape%3B%20rather%2C%20they%20are%20clustered.%20Consequently%2C%20rangers%0Ashould%20be%20targeted%20at%20areas%20with%20high%20midden%20densities%20to%20strengthen%0Aanti-poaching%20efforts%2C%20in%20line%20with%20UN%20Target%2015.7.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18104v1&entry.124074799=Read"},
{"title": "Scene Understanding in Pick-and-Place Tasks: Analyzing Transformations\n  Between Initial and Final Scenes", "author": "Seraj Ghasemi and Hamed Hosseini and MohammadHossein Koosheshi and Mehdi Tale Masouleh and Ahmad Kalhor", "abstract": "  With robots increasingly collaborating with humans in everyday tasks, it is\nimportant to take steps toward robotic systems capable of understanding the\nenvironment. This work focuses on scene understanding to detect pick and place\ntasks given initial and final images from the scene. To this end, a dataset is\ncollected for object detection and pick and place task detection. A YOLOv5\nnetwork is subsequently trained to detect the objects in the initial and final\nscenes. Given the detected objects and their bounding boxes, two methods are\nproposed to detect the pick and place tasks which transform the initial scene\ninto the final scene. A geometric method is proposed which tracks objects'\nmovements in the two scenes and works based on the intersection of the bounding\nboxes which moved within scenes. Contrarily, the CNN-based method utilizes a\nConvolutional Neural Network to classify objects with intersected bounding\nboxes into 5 classes, showing the spatial relationship between the involved\nobjects. The performed pick and place tasks are then derived from analyzing the\nexperiments with both scenes. Results show that the CNN-based method, using a\nVGG16 backbone, outscores the geometric method by roughly 12 percentage points\nin certain scenarios, with an overall success rate of 84.3%.\n", "link": "http://arxiv.org/abs/2409.17720v1", "date": "2024-09-26", "relevancy": 2.4053, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene%20Understanding%20in%20Pick-and-Place%20Tasks%3A%20Analyzing%20Transformations%0A%20%20Between%20Initial%20and%20Final%20Scenes&body=Title%3A%20Scene%20Understanding%20in%20Pick-and-Place%20Tasks%3A%20Analyzing%20Transformations%0A%20%20Between%20Initial%20and%20Final%20Scenes%0AAuthor%3A%20Seraj%20Ghasemi%20and%20Hamed%20Hosseini%20and%20MohammadHossein%20Koosheshi%20and%20Mehdi%20Tale%20Masouleh%20and%20Ahmad%20Kalhor%0AAbstract%3A%20%20%20With%20robots%20increasingly%20collaborating%20with%20humans%20in%20everyday%20tasks%2C%20it%20is%0Aimportant%20to%20take%20steps%20toward%20robotic%20systems%20capable%20of%20understanding%20the%0Aenvironment.%20This%20work%20focuses%20on%20scene%20understanding%20to%20detect%20pick%20and%20place%0Atasks%20given%20initial%20and%20final%20images%20from%20the%20scene.%20To%20this%20end%2C%20a%20dataset%20is%0Acollected%20for%20object%20detection%20and%20pick%20and%20place%20task%20detection.%20A%20YOLOv5%0Anetwork%20is%20subsequently%20trained%20to%20detect%20the%20objects%20in%20the%20initial%20and%20final%0Ascenes.%20Given%20the%20detected%20objects%20and%20their%20bounding%20boxes%2C%20two%20methods%20are%0Aproposed%20to%20detect%20the%20pick%20and%20place%20tasks%20which%20transform%20the%20initial%20scene%0Ainto%20the%20final%20scene.%20A%20geometric%20method%20is%20proposed%20which%20tracks%20objects%27%0Amovements%20in%20the%20two%20scenes%20and%20works%20based%20on%20the%20intersection%20of%20the%20bounding%0Aboxes%20which%20moved%20within%20scenes.%20Contrarily%2C%20the%20CNN-based%20method%20utilizes%20a%0AConvolutional%20Neural%20Network%20to%20classify%20objects%20with%20intersected%20bounding%0Aboxes%20into%205%20classes%2C%20showing%20the%20spatial%20relationship%20between%20the%20involved%0Aobjects.%20The%20performed%20pick%20and%20place%20tasks%20are%20then%20derived%20from%20analyzing%20the%0Aexperiments%20with%20both%20scenes.%20Results%20show%20that%20the%20CNN-based%20method%2C%20using%20a%0AVGG16%20backbone%2C%20outscores%20the%20geometric%20method%20by%20roughly%2012%20percentage%20points%0Ain%20certain%20scenarios%2C%20with%20an%20overall%20success%20rate%20of%2084.3%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene%2520Understanding%2520in%2520Pick-and-Place%2520Tasks%253A%2520Analyzing%2520Transformations%250A%2520%2520Between%2520Initial%2520and%2520Final%2520Scenes%26entry.906535625%3DSeraj%2520Ghasemi%2520and%2520Hamed%2520Hosseini%2520and%2520MohammadHossein%2520Koosheshi%2520and%2520Mehdi%2520Tale%2520Masouleh%2520and%2520Ahmad%2520Kalhor%26entry.1292438233%3D%2520%2520With%2520robots%2520increasingly%2520collaborating%2520with%2520humans%2520in%2520everyday%2520tasks%252C%2520it%2520is%250Aimportant%2520to%2520take%2520steps%2520toward%2520robotic%2520systems%2520capable%2520of%2520understanding%2520the%250Aenvironment.%2520This%2520work%2520focuses%2520on%2520scene%2520understanding%2520to%2520detect%2520pick%2520and%2520place%250Atasks%2520given%2520initial%2520and%2520final%2520images%2520from%2520the%2520scene.%2520To%2520this%2520end%252C%2520a%2520dataset%2520is%250Acollected%2520for%2520object%2520detection%2520and%2520pick%2520and%2520place%2520task%2520detection.%2520A%2520YOLOv5%250Anetwork%2520is%2520subsequently%2520trained%2520to%2520detect%2520the%2520objects%2520in%2520the%2520initial%2520and%2520final%250Ascenes.%2520Given%2520the%2520detected%2520objects%2520and%2520their%2520bounding%2520boxes%252C%2520two%2520methods%2520are%250Aproposed%2520to%2520detect%2520the%2520pick%2520and%2520place%2520tasks%2520which%2520transform%2520the%2520initial%2520scene%250Ainto%2520the%2520final%2520scene.%2520A%2520geometric%2520method%2520is%2520proposed%2520which%2520tracks%2520objects%2527%250Amovements%2520in%2520the%2520two%2520scenes%2520and%2520works%2520based%2520on%2520the%2520intersection%2520of%2520the%2520bounding%250Aboxes%2520which%2520moved%2520within%2520scenes.%2520Contrarily%252C%2520the%2520CNN-based%2520method%2520utilizes%2520a%250AConvolutional%2520Neural%2520Network%2520to%2520classify%2520objects%2520with%2520intersected%2520bounding%250Aboxes%2520into%25205%2520classes%252C%2520showing%2520the%2520spatial%2520relationship%2520between%2520the%2520involved%250Aobjects.%2520The%2520performed%2520pick%2520and%2520place%2520tasks%2520are%2520then%2520derived%2520from%2520analyzing%2520the%250Aexperiments%2520with%2520both%2520scenes.%2520Results%2520show%2520that%2520the%2520CNN-based%2520method%252C%2520using%2520a%250AVGG16%2520backbone%252C%2520outscores%2520the%2520geometric%2520method%2520by%2520roughly%252012%2520percentage%2520points%250Ain%2520certain%2520scenarios%252C%2520with%2520an%2520overall%2520success%2520rate%2520of%252084.3%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene%20Understanding%20in%20Pick-and-Place%20Tasks%3A%20Analyzing%20Transformations%0A%20%20Between%20Initial%20and%20Final%20Scenes&entry.906535625=Seraj%20Ghasemi%20and%20Hamed%20Hosseini%20and%20MohammadHossein%20Koosheshi%20and%20Mehdi%20Tale%20Masouleh%20and%20Ahmad%20Kalhor&entry.1292438233=%20%20With%20robots%20increasingly%20collaborating%20with%20humans%20in%20everyday%20tasks%2C%20it%20is%0Aimportant%20to%20take%20steps%20toward%20robotic%20systems%20capable%20of%20understanding%20the%0Aenvironment.%20This%20work%20focuses%20on%20scene%20understanding%20to%20detect%20pick%20and%20place%0Atasks%20given%20initial%20and%20final%20images%20from%20the%20scene.%20To%20this%20end%2C%20a%20dataset%20is%0Acollected%20for%20object%20detection%20and%20pick%20and%20place%20task%20detection.%20A%20YOLOv5%0Anetwork%20is%20subsequently%20trained%20to%20detect%20the%20objects%20in%20the%20initial%20and%20final%0Ascenes.%20Given%20the%20detected%20objects%20and%20their%20bounding%20boxes%2C%20two%20methods%20are%0Aproposed%20to%20detect%20the%20pick%20and%20place%20tasks%20which%20transform%20the%20initial%20scene%0Ainto%20the%20final%20scene.%20A%20geometric%20method%20is%20proposed%20which%20tracks%20objects%27%0Amovements%20in%20the%20two%20scenes%20and%20works%20based%20on%20the%20intersection%20of%20the%20bounding%0Aboxes%20which%20moved%20within%20scenes.%20Contrarily%2C%20the%20CNN-based%20method%20utilizes%20a%0AConvolutional%20Neural%20Network%20to%20classify%20objects%20with%20intersected%20bounding%0Aboxes%20into%205%20classes%2C%20showing%20the%20spatial%20relationship%20between%20the%20involved%0Aobjects.%20The%20performed%20pick%20and%20place%20tasks%20are%20then%20derived%20from%20analyzing%20the%0Aexperiments%20with%20both%20scenes.%20Results%20show%20that%20the%20CNN-based%20method%2C%20using%20a%0AVGG16%20backbone%2C%20outscores%20the%20geometric%20method%20by%20roughly%2012%20percentage%20points%0Ain%20certain%20scenarios%2C%20with%20an%20overall%20success%20rate%20of%2084.3%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17720v1&entry.124074799=Read"},
{"title": "In-Context Ensemble Improves Video-Language Models for Low-Level\n  Workflow Understanding from Human Demonstrations", "author": "Moucheng Xu and Evangelos Chatzaroulas and Luc McCutcheon and Abdul Ahad and Hamzah Azeem and Janusz Marecki and Ammar Anwar", "abstract": "  A Standard Operating Procedure (SOP) defines a low-level, step-by-step\nwritten guide for a business software workflow based on a video demonstration.\nSOPs are a crucial step toward automating end-to-end software workflows.\nManually creating SOPs can be time-consuming. Recent advancements in large\nvideo-language models offer the potential for automating SOP generation by\nanalyzing recordings of human demonstrations. However, current large\nvideo-language models face challenges with zero-shot SOP generation. We explore\nin-context learning with video-language models for SOP generation. We report\nthat in-context learning sometimes helps video-language models at SOP\ngeneration. We then propose an in-context ensemble learning to further enhance\nthe capabilities of the models in SOP generation.\n", "link": "http://arxiv.org/abs/2409.15867v3", "date": "2024-09-26", "relevancy": 2.3841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6095}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Ensemble%20Improves%20Video-Language%20Models%20for%20Low-Level%0A%20%20Workflow%20Understanding%20from%20Human%20Demonstrations&body=Title%3A%20In-Context%20Ensemble%20Improves%20Video-Language%20Models%20for%20Low-Level%0A%20%20Workflow%20Understanding%20from%20Human%20Demonstrations%0AAuthor%3A%20Moucheng%20Xu%20and%20Evangelos%20Chatzaroulas%20and%20Luc%20McCutcheon%20and%20Abdul%20Ahad%20and%20Hamzah%20Azeem%20and%20Janusz%20Marecki%20and%20Ammar%20Anwar%0AAbstract%3A%20%20%20A%20Standard%20Operating%20Procedure%20%28SOP%29%20defines%20a%20low-level%2C%20step-by-step%0Awritten%20guide%20for%20a%20business%20software%20workflow%20based%20on%20a%20video%20demonstration.%0ASOPs%20are%20a%20crucial%20step%20toward%20automating%20end-to-end%20software%20workflows.%0AManually%20creating%20SOPs%20can%20be%20time-consuming.%20Recent%20advancements%20in%20large%0Avideo-language%20models%20offer%20the%20potential%20for%20automating%20SOP%20generation%20by%0Aanalyzing%20recordings%20of%20human%20demonstrations.%20However%2C%20current%20large%0Avideo-language%20models%20face%20challenges%20with%20zero-shot%20SOP%20generation.%20We%20explore%0Ain-context%20learning%20with%20video-language%20models%20for%20SOP%20generation.%20We%20report%0Athat%20in-context%20learning%20sometimes%20helps%20video-language%20models%20at%20SOP%0Ageneration.%20We%20then%20propose%20an%20in-context%20ensemble%20learning%20to%20further%20enhance%0Athe%20capabilities%20of%20the%20models%20in%20SOP%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15867v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Ensemble%2520Improves%2520Video-Language%2520Models%2520for%2520Low-Level%250A%2520%2520Workflow%2520Understanding%2520from%2520Human%2520Demonstrations%26entry.906535625%3DMoucheng%2520Xu%2520and%2520Evangelos%2520Chatzaroulas%2520and%2520Luc%2520McCutcheon%2520and%2520Abdul%2520Ahad%2520and%2520Hamzah%2520Azeem%2520and%2520Janusz%2520Marecki%2520and%2520Ammar%2520Anwar%26entry.1292438233%3D%2520%2520A%2520Standard%2520Operating%2520Procedure%2520%2528SOP%2529%2520defines%2520a%2520low-level%252C%2520step-by-step%250Awritten%2520guide%2520for%2520a%2520business%2520software%2520workflow%2520based%2520on%2520a%2520video%2520demonstration.%250ASOPs%2520are%2520a%2520crucial%2520step%2520toward%2520automating%2520end-to-end%2520software%2520workflows.%250AManually%2520creating%2520SOPs%2520can%2520be%2520time-consuming.%2520Recent%2520advancements%2520in%2520large%250Avideo-language%2520models%2520offer%2520the%2520potential%2520for%2520automating%2520SOP%2520generation%2520by%250Aanalyzing%2520recordings%2520of%2520human%2520demonstrations.%2520However%252C%2520current%2520large%250Avideo-language%2520models%2520face%2520challenges%2520with%2520zero-shot%2520SOP%2520generation.%2520We%2520explore%250Ain-context%2520learning%2520with%2520video-language%2520models%2520for%2520SOP%2520generation.%2520We%2520report%250Athat%2520in-context%2520learning%2520sometimes%2520helps%2520video-language%2520models%2520at%2520SOP%250Ageneration.%2520We%2520then%2520propose%2520an%2520in-context%2520ensemble%2520learning%2520to%2520further%2520enhance%250Athe%2520capabilities%2520of%2520the%2520models%2520in%2520SOP%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15867v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Ensemble%20Improves%20Video-Language%20Models%20for%20Low-Level%0A%20%20Workflow%20Understanding%20from%20Human%20Demonstrations&entry.906535625=Moucheng%20Xu%20and%20Evangelos%20Chatzaroulas%20and%20Luc%20McCutcheon%20and%20Abdul%20Ahad%20and%20Hamzah%20Azeem%20and%20Janusz%20Marecki%20and%20Ammar%20Anwar&entry.1292438233=%20%20A%20Standard%20Operating%20Procedure%20%28SOP%29%20defines%20a%20low-level%2C%20step-by-step%0Awritten%20guide%20for%20a%20business%20software%20workflow%20based%20on%20a%20video%20demonstration.%0ASOPs%20are%20a%20crucial%20step%20toward%20automating%20end-to-end%20software%20workflows.%0AManually%20creating%20SOPs%20can%20be%20time-consuming.%20Recent%20advancements%20in%20large%0Avideo-language%20models%20offer%20the%20potential%20for%20automating%20SOP%20generation%20by%0Aanalyzing%20recordings%20of%20human%20demonstrations.%20However%2C%20current%20large%0Avideo-language%20models%20face%20challenges%20with%20zero-shot%20SOP%20generation.%20We%20explore%0Ain-context%20learning%20with%20video-language%20models%20for%20SOP%20generation.%20We%20report%0Athat%20in-context%20learning%20sometimes%20helps%20video-language%20models%20at%20SOP%0Ageneration.%20We%20then%20propose%20an%20in-context%20ensemble%20learning%20to%20further%20enhance%0Athe%20capabilities%20of%20the%20models%20in%20SOP%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15867v3&entry.124074799=Read"},
{"title": "Byzantine-Robust Aggregation for Securing Decentralized Federated\n  Learning", "author": "Diego Cajaraville-Aboy and Ana Fern\u00e1ndez-Vilas and Rebeca P. D\u00edaz-Redondo and Manuel Fern\u00e1ndez-Veiga", "abstract": "  Federated Learning (FL) emerges as a distributed machine learning approach\nthat addresses privacy concerns by training AI models locally on devices.\nDecentralized Federated Learning (DFL) extends the FL paradigm by eliminating\nthe central server, thereby enhancing scalability and robustness through the\navoidance of a single point of failure. However, DFL faces significant\nchallenges in optimizing security, as most Byzantine-robust algorithms proposed\nin the literature are designed for centralized scenarios. In this paper, we\npresent a novel Byzantine-robust aggregation algorithm to enhance the security\nof Decentralized Federated Learning environments, coined WFAgg. This proposal\nhandles the adverse conditions and strength robustness of dynamic decentralized\ntopologies at the same time by employing multiple filters to identify and\nmitigate Byzantine attacks. Experimental results demonstrate the effectiveness\nof the proposed algorithm in maintaining model accuracy and convergence in the\npresence of various Byzantine attack scenarios, outperforming state-of-the-art\ncentralized Byzantine-robust aggregation schemes (such as Multi-Krum or\nClustering). These algorithms are evaluated on an IID image classification\nproblem in both centralized and decentralized scenarios.\n", "link": "http://arxiv.org/abs/2409.17754v1", "date": "2024-09-26", "relevancy": 2.371, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4735}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Byzantine-Robust%20Aggregation%20for%20Securing%20Decentralized%20Federated%0A%20%20Learning&body=Title%3A%20Byzantine-Robust%20Aggregation%20for%20Securing%20Decentralized%20Federated%0A%20%20Learning%0AAuthor%3A%20Diego%20Cajaraville-Aboy%20and%20Ana%20Fern%C3%A1ndez-Vilas%20and%20Rebeca%20P.%20D%C3%ADaz-Redondo%20and%20Manuel%20Fern%C3%A1ndez-Veiga%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20emerges%20as%20a%20distributed%20machine%20learning%20approach%0Athat%20addresses%20privacy%20concerns%20by%20training%20AI%20models%20locally%20on%20devices.%0ADecentralized%20Federated%20Learning%20%28DFL%29%20extends%20the%20FL%20paradigm%20by%20eliminating%0Athe%20central%20server%2C%20thereby%20enhancing%20scalability%20and%20robustness%20through%20the%0Aavoidance%20of%20a%20single%20point%20of%20failure.%20However%2C%20DFL%20faces%20significant%0Achallenges%20in%20optimizing%20security%2C%20as%20most%20Byzantine-robust%20algorithms%20proposed%0Ain%20the%20literature%20are%20designed%20for%20centralized%20scenarios.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20Byzantine-robust%20aggregation%20algorithm%20to%20enhance%20the%20security%0Aof%20Decentralized%20Federated%20Learning%20environments%2C%20coined%20WFAgg.%20This%20proposal%0Ahandles%20the%20adverse%20conditions%20and%20strength%20robustness%20of%20dynamic%20decentralized%0Atopologies%20at%20the%20same%20time%20by%20employing%20multiple%20filters%20to%20identify%20and%0Amitigate%20Byzantine%20attacks.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20algorithm%20in%20maintaining%20model%20accuracy%20and%20convergence%20in%20the%0Apresence%20of%20various%20Byzantine%20attack%20scenarios%2C%20outperforming%20state-of-the-art%0Acentralized%20Byzantine-robust%20aggregation%20schemes%20%28such%20as%20Multi-Krum%20or%0AClustering%29.%20These%20algorithms%20are%20evaluated%20on%20an%20IID%20image%20classification%0Aproblem%20in%20both%20centralized%20and%20decentralized%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByzantine-Robust%2520Aggregation%2520for%2520Securing%2520Decentralized%2520Federated%250A%2520%2520Learning%26entry.906535625%3DDiego%2520Cajaraville-Aboy%2520and%2520Ana%2520Fern%25C3%25A1ndez-Vilas%2520and%2520Rebeca%2520P.%2520D%25C3%25ADaz-Redondo%2520and%2520Manuel%2520Fern%25C3%25A1ndez-Veiga%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520emerges%2520as%2520a%2520distributed%2520machine%2520learning%2520approach%250Athat%2520addresses%2520privacy%2520concerns%2520by%2520training%2520AI%2520models%2520locally%2520on%2520devices.%250ADecentralized%2520Federated%2520Learning%2520%2528DFL%2529%2520extends%2520the%2520FL%2520paradigm%2520by%2520eliminating%250Athe%2520central%2520server%252C%2520thereby%2520enhancing%2520scalability%2520and%2520robustness%2520through%2520the%250Aavoidance%2520of%2520a%2520single%2520point%2520of%2520failure.%2520However%252C%2520DFL%2520faces%2520significant%250Achallenges%2520in%2520optimizing%2520security%252C%2520as%2520most%2520Byzantine-robust%2520algorithms%2520proposed%250Ain%2520the%2520literature%2520are%2520designed%2520for%2520centralized%2520scenarios.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520novel%2520Byzantine-robust%2520aggregation%2520algorithm%2520to%2520enhance%2520the%2520security%250Aof%2520Decentralized%2520Federated%2520Learning%2520environments%252C%2520coined%2520WFAgg.%2520This%2520proposal%250Ahandles%2520the%2520adverse%2520conditions%2520and%2520strength%2520robustness%2520of%2520dynamic%2520decentralized%250Atopologies%2520at%2520the%2520same%2520time%2520by%2520employing%2520multiple%2520filters%2520to%2520identify%2520and%250Amitigate%2520Byzantine%2520attacks.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520algorithm%2520in%2520maintaining%2520model%2520accuracy%2520and%2520convergence%2520in%2520the%250Apresence%2520of%2520various%2520Byzantine%2520attack%2520scenarios%252C%2520outperforming%2520state-of-the-art%250Acentralized%2520Byzantine-robust%2520aggregation%2520schemes%2520%2528such%2520as%2520Multi-Krum%2520or%250AClustering%2529.%2520These%2520algorithms%2520are%2520evaluated%2520on%2520an%2520IID%2520image%2520classification%250Aproblem%2520in%2520both%2520centralized%2520and%2520decentralized%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Byzantine-Robust%20Aggregation%20for%20Securing%20Decentralized%20Federated%0A%20%20Learning&entry.906535625=Diego%20Cajaraville-Aboy%20and%20Ana%20Fern%C3%A1ndez-Vilas%20and%20Rebeca%20P.%20D%C3%ADaz-Redondo%20and%20Manuel%20Fern%C3%A1ndez-Veiga&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20emerges%20as%20a%20distributed%20machine%20learning%20approach%0Athat%20addresses%20privacy%20concerns%20by%20training%20AI%20models%20locally%20on%20devices.%0ADecentralized%20Federated%20Learning%20%28DFL%29%20extends%20the%20FL%20paradigm%20by%20eliminating%0Athe%20central%20server%2C%20thereby%20enhancing%20scalability%20and%20robustness%20through%20the%0Aavoidance%20of%20a%20single%20point%20of%20failure.%20However%2C%20DFL%20faces%20significant%0Achallenges%20in%20optimizing%20security%2C%20as%20most%20Byzantine-robust%20algorithms%20proposed%0Ain%20the%20literature%20are%20designed%20for%20centralized%20scenarios.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20Byzantine-robust%20aggregation%20algorithm%20to%20enhance%20the%20security%0Aof%20Decentralized%20Federated%20Learning%20environments%2C%20coined%20WFAgg.%20This%20proposal%0Ahandles%20the%20adverse%20conditions%20and%20strength%20robustness%20of%20dynamic%20decentralized%0Atopologies%20at%20the%20same%20time%20by%20employing%20multiple%20filters%20to%20identify%20and%0Amitigate%20Byzantine%20attacks.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20algorithm%20in%20maintaining%20model%20accuracy%20and%20convergence%20in%20the%0Apresence%20of%20various%20Byzantine%20attack%20scenarios%2C%20outperforming%20state-of-the-art%0Acentralized%20Byzantine-robust%20aggregation%20schemes%20%28such%20as%20Multi-Krum%20or%0AClustering%29.%20These%20algorithms%20are%20evaluated%20on%20an%20IID%20image%20classification%0Aproblem%20in%20both%20centralized%20and%20decentralized%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17754v1&entry.124074799=Read"},
{"title": "LKA-ReID:Vehicle Re-Identification with Large Kernel Attention", "author": "Xuezhi Xiang and Zhushan Ma and Lei Zhang and Denis Ombati and Himaloy Himu and Xiantong Zhen", "abstract": "  With the rapid development of intelligent transportation systems and the\npopularity of smart city infrastructure, Vehicle Re-ID technology has become an\nimportant research field. The vehicle Re-ID task faces an important challenge,\nwhich is the high similarity between different vehicles. Existing methods use\nadditional detection or segmentation models to extract differentiated local\nfeatures. However, these methods either rely on additional annotations or\ngreatly increase the computational cost. Using attention mechanism to capture\nglobal and local features is crucial to solve the challenge of high similarity\nbetween classes in vehicle Re-ID tasks. In this paper, we propose LKA-ReID with\nlarge kernel attention. Specifically, the large kernel attention (LKA) utilizes\nthe advantages of self-attention and also benefits from the advantages of\nconvolution, which can extract the global and local features of the vehicle\nmore comprehensively. We also introduce hybrid channel attention (HCA) combines\nchannel attention with spatial information, so that the model can better focus\non channels and feature regions, and ignore background and other disturbing\ninformation. Experiments on VeRi-776 dataset demonstrated the effectiveness of\nLKA-ReID, with mAP reaches 86.65% and Rank-1 reaches 98.03%.\n", "link": "http://arxiv.org/abs/2409.17908v1", "date": "2024-09-26", "relevancy": 2.3616, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4851}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4752}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LKA-ReID%3AVehicle%20Re-Identification%20with%20Large%20Kernel%20Attention&body=Title%3A%20LKA-ReID%3AVehicle%20Re-Identification%20with%20Large%20Kernel%20Attention%0AAuthor%3A%20Xuezhi%20Xiang%20and%20Zhushan%20Ma%20and%20Lei%20Zhang%20and%20Denis%20Ombati%20and%20Himaloy%20Himu%20and%20Xiantong%20Zhen%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20intelligent%20transportation%20systems%20and%20the%0Apopularity%20of%20smart%20city%20infrastructure%2C%20Vehicle%20Re-ID%20technology%20has%20become%20an%0Aimportant%20research%20field.%20The%20vehicle%20Re-ID%20task%20faces%20an%20important%20challenge%2C%0Awhich%20is%20the%20high%20similarity%20between%20different%20vehicles.%20Existing%20methods%20use%0Aadditional%20detection%20or%20segmentation%20models%20to%20extract%20differentiated%20local%0Afeatures.%20However%2C%20these%20methods%20either%20rely%20on%20additional%20annotations%20or%0Agreatly%20increase%20the%20computational%20cost.%20Using%20attention%20mechanism%20to%20capture%0Aglobal%20and%20local%20features%20is%20crucial%20to%20solve%20the%20challenge%20of%20high%20similarity%0Abetween%20classes%20in%20vehicle%20Re-ID%20tasks.%20In%20this%20paper%2C%20we%20propose%20LKA-ReID%20with%0Alarge%20kernel%20attention.%20Specifically%2C%20the%20large%20kernel%20attention%20%28LKA%29%20utilizes%0Athe%20advantages%20of%20self-attention%20and%20also%20benefits%20from%20the%20advantages%20of%0Aconvolution%2C%20which%20can%20extract%20the%20global%20and%20local%20features%20of%20the%20vehicle%0Amore%20comprehensively.%20We%20also%20introduce%20hybrid%20channel%20attention%20%28HCA%29%20combines%0Achannel%20attention%20with%20spatial%20information%2C%20so%20that%20the%20model%20can%20better%20focus%0Aon%20channels%20and%20feature%20regions%2C%20and%20ignore%20background%20and%20other%20disturbing%0Ainformation.%20Experiments%20on%20VeRi-776%20dataset%20demonstrated%20the%20effectiveness%20of%0ALKA-ReID%2C%20with%20mAP%20reaches%2086.65%25%20and%20Rank-1%20reaches%2098.03%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLKA-ReID%253AVehicle%2520Re-Identification%2520with%2520Large%2520Kernel%2520Attention%26entry.906535625%3DXuezhi%2520Xiang%2520and%2520Zhushan%2520Ma%2520and%2520Lei%2520Zhang%2520and%2520Denis%2520Ombati%2520and%2520Himaloy%2520Himu%2520and%2520Xiantong%2520Zhen%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520intelligent%2520transportation%2520systems%2520and%2520the%250Apopularity%2520of%2520smart%2520city%2520infrastructure%252C%2520Vehicle%2520Re-ID%2520technology%2520has%2520become%2520an%250Aimportant%2520research%2520field.%2520The%2520vehicle%2520Re-ID%2520task%2520faces%2520an%2520important%2520challenge%252C%250Awhich%2520is%2520the%2520high%2520similarity%2520between%2520different%2520vehicles.%2520Existing%2520methods%2520use%250Aadditional%2520detection%2520or%2520segmentation%2520models%2520to%2520extract%2520differentiated%2520local%250Afeatures.%2520However%252C%2520these%2520methods%2520either%2520rely%2520on%2520additional%2520annotations%2520or%250Agreatly%2520increase%2520the%2520computational%2520cost.%2520Using%2520attention%2520mechanism%2520to%2520capture%250Aglobal%2520and%2520local%2520features%2520is%2520crucial%2520to%2520solve%2520the%2520challenge%2520of%2520high%2520similarity%250Abetween%2520classes%2520in%2520vehicle%2520Re-ID%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LKA-ReID%2520with%250Alarge%2520kernel%2520attention.%2520Specifically%252C%2520the%2520large%2520kernel%2520attention%2520%2528LKA%2529%2520utilizes%250Athe%2520advantages%2520of%2520self-attention%2520and%2520also%2520benefits%2520from%2520the%2520advantages%2520of%250Aconvolution%252C%2520which%2520can%2520extract%2520the%2520global%2520and%2520local%2520features%2520of%2520the%2520vehicle%250Amore%2520comprehensively.%2520We%2520also%2520introduce%2520hybrid%2520channel%2520attention%2520%2528HCA%2529%2520combines%250Achannel%2520attention%2520with%2520spatial%2520information%252C%2520so%2520that%2520the%2520model%2520can%2520better%2520focus%250Aon%2520channels%2520and%2520feature%2520regions%252C%2520and%2520ignore%2520background%2520and%2520other%2520disturbing%250Ainformation.%2520Experiments%2520on%2520VeRi-776%2520dataset%2520demonstrated%2520the%2520effectiveness%2520of%250ALKA-ReID%252C%2520with%2520mAP%2520reaches%252086.65%2525%2520and%2520Rank-1%2520reaches%252098.03%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LKA-ReID%3AVehicle%20Re-Identification%20with%20Large%20Kernel%20Attention&entry.906535625=Xuezhi%20Xiang%20and%20Zhushan%20Ma%20and%20Lei%20Zhang%20and%20Denis%20Ombati%20and%20Himaloy%20Himu%20and%20Xiantong%20Zhen&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20intelligent%20transportation%20systems%20and%20the%0Apopularity%20of%20smart%20city%20infrastructure%2C%20Vehicle%20Re-ID%20technology%20has%20become%20an%0Aimportant%20research%20field.%20The%20vehicle%20Re-ID%20task%20faces%20an%20important%20challenge%2C%0Awhich%20is%20the%20high%20similarity%20between%20different%20vehicles.%20Existing%20methods%20use%0Aadditional%20detection%20or%20segmentation%20models%20to%20extract%20differentiated%20local%0Afeatures.%20However%2C%20these%20methods%20either%20rely%20on%20additional%20annotations%20or%0Agreatly%20increase%20the%20computational%20cost.%20Using%20attention%20mechanism%20to%20capture%0Aglobal%20and%20local%20features%20is%20crucial%20to%20solve%20the%20challenge%20of%20high%20similarity%0Abetween%20classes%20in%20vehicle%20Re-ID%20tasks.%20In%20this%20paper%2C%20we%20propose%20LKA-ReID%20with%0Alarge%20kernel%20attention.%20Specifically%2C%20the%20large%20kernel%20attention%20%28LKA%29%20utilizes%0Athe%20advantages%20of%20self-attention%20and%20also%20benefits%20from%20the%20advantages%20of%0Aconvolution%2C%20which%20can%20extract%20the%20global%20and%20local%20features%20of%20the%20vehicle%0Amore%20comprehensively.%20We%20also%20introduce%20hybrid%20channel%20attention%20%28HCA%29%20combines%0Achannel%20attention%20with%20spatial%20information%2C%20so%20that%20the%20model%20can%20better%20focus%0Aon%20channels%20and%20feature%20regions%2C%20and%20ignore%20background%20and%20other%20disturbing%0Ainformation.%20Experiments%20on%20VeRi-776%20dataset%20demonstrated%20the%20effectiveness%20of%0ALKA-ReID%2C%20with%20mAP%20reaches%2086.65%25%20and%20Rank-1%20reaches%2098.03%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17908v1&entry.124074799=Read"},
{"title": "TabGraphs: A Benchmark and Strong Baselines for Learning on Graphs with\n  Tabular Node Features", "author": "Gleb Bazhenov and Oleg Platonov and Liudmila Prokhorenkova", "abstract": "  Tabular machine learning is an important field for industry and science. In\nthis field, table rows are usually treated as independent data samples, but\nadditional information about relations between them is sometimes available and\ncan be used to improve predictive performance. Such information can be\nnaturally modeled with a graph, thus tabular machine learning may benefit from\ngraph machine learning methods. However, graph machine learning models are\ntypically evaluated on datasets with homogeneous node features, which have\nlittle in common with heterogeneous mixtures of numerical and categorical\nfeatures present in tabular datasets. Thus, there is a critical difference\nbetween the data used in tabular and graph machine learning studies, which does\nnot allow one to understand how successfully graph models can be transferred to\ntabular data. To bridge this gap, we propose a new benchmark of diverse graphs\nwith heterogeneous tabular node features and realistic prediction tasks. We use\nthis benchmark to evaluate a vast set of models, including simple methods\npreviously overlooked in the literature. Our experiments show that graph neural\nnetworks (GNNs) can indeed often bring gains in predictive performance for\ntabular data, but standard tabular models also can be adapted to work with\ngraph data by using simple feature preprocessing, which sometimes enables them\nto compete with and even outperform GNNs. Based on our empirical study, we\nprovide insights for researchers and practitioners in both tabular and graph\nmachine learning fields.\n", "link": "http://arxiv.org/abs/2409.14500v2", "date": "2024-09-26", "relevancy": 2.3425, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4938}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4573}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabGraphs%3A%20A%20Benchmark%20and%20Strong%20Baselines%20for%20Learning%20on%20Graphs%20with%0A%20%20Tabular%20Node%20Features&body=Title%3A%20TabGraphs%3A%20A%20Benchmark%20and%20Strong%20Baselines%20for%20Learning%20on%20Graphs%20with%0A%20%20Tabular%20Node%20Features%0AAuthor%3A%20Gleb%20Bazhenov%20and%20Oleg%20Platonov%20and%20Liudmila%20Prokhorenkova%0AAbstract%3A%20%20%20Tabular%20machine%20learning%20is%20an%20important%20field%20for%20industry%20and%20science.%20In%0Athis%20field%2C%20table%20rows%20are%20usually%20treated%20as%20independent%20data%20samples%2C%20but%0Aadditional%20information%20about%20relations%20between%20them%20is%20sometimes%20available%20and%0Acan%20be%20used%20to%20improve%20predictive%20performance.%20Such%20information%20can%20be%0Anaturally%20modeled%20with%20a%20graph%2C%20thus%20tabular%20machine%20learning%20may%20benefit%20from%0Agraph%20machine%20learning%20methods.%20However%2C%20graph%20machine%20learning%20models%20are%0Atypically%20evaluated%20on%20datasets%20with%20homogeneous%20node%20features%2C%20which%20have%0Alittle%20in%20common%20with%20heterogeneous%20mixtures%20of%20numerical%20and%20categorical%0Afeatures%20present%20in%20tabular%20datasets.%20Thus%2C%20there%20is%20a%20critical%20difference%0Abetween%20the%20data%20used%20in%20tabular%20and%20graph%20machine%20learning%20studies%2C%20which%20does%0Anot%20allow%20one%20to%20understand%20how%20successfully%20graph%20models%20can%20be%20transferred%20to%0Atabular%20data.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20new%20benchmark%20of%20diverse%20graphs%0Awith%20heterogeneous%20tabular%20node%20features%20and%20realistic%20prediction%20tasks.%20We%20use%0Athis%20benchmark%20to%20evaluate%20a%20vast%20set%20of%20models%2C%20including%20simple%20methods%0Apreviously%20overlooked%20in%20the%20literature.%20Our%20experiments%20show%20that%20graph%20neural%0Anetworks%20%28GNNs%29%20can%20indeed%20often%20bring%20gains%20in%20predictive%20performance%20for%0Atabular%20data%2C%20but%20standard%20tabular%20models%20also%20can%20be%20adapted%20to%20work%20with%0Agraph%20data%20by%20using%20simple%20feature%20preprocessing%2C%20which%20sometimes%20enables%20them%0Ato%20compete%20with%20and%20even%20outperform%20GNNs.%20Based%20on%20our%20empirical%20study%2C%20we%0Aprovide%20insights%20for%20researchers%20and%20practitioners%20in%20both%20tabular%20and%20graph%0Amachine%20learning%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabGraphs%253A%2520A%2520Benchmark%2520and%2520Strong%2520Baselines%2520for%2520Learning%2520on%2520Graphs%2520with%250A%2520%2520Tabular%2520Node%2520Features%26entry.906535625%3DGleb%2520Bazhenov%2520and%2520Oleg%2520Platonov%2520and%2520Liudmila%2520Prokhorenkova%26entry.1292438233%3D%2520%2520Tabular%2520machine%2520learning%2520is%2520an%2520important%2520field%2520for%2520industry%2520and%2520science.%2520In%250Athis%2520field%252C%2520table%2520rows%2520are%2520usually%2520treated%2520as%2520independent%2520data%2520samples%252C%2520but%250Aadditional%2520information%2520about%2520relations%2520between%2520them%2520is%2520sometimes%2520available%2520and%250Acan%2520be%2520used%2520to%2520improve%2520predictive%2520performance.%2520Such%2520information%2520can%2520be%250Anaturally%2520modeled%2520with%2520a%2520graph%252C%2520thus%2520tabular%2520machine%2520learning%2520may%2520benefit%2520from%250Agraph%2520machine%2520learning%2520methods.%2520However%252C%2520graph%2520machine%2520learning%2520models%2520are%250Atypically%2520evaluated%2520on%2520datasets%2520with%2520homogeneous%2520node%2520features%252C%2520which%2520have%250Alittle%2520in%2520common%2520with%2520heterogeneous%2520mixtures%2520of%2520numerical%2520and%2520categorical%250Afeatures%2520present%2520in%2520tabular%2520datasets.%2520Thus%252C%2520there%2520is%2520a%2520critical%2520difference%250Abetween%2520the%2520data%2520used%2520in%2520tabular%2520and%2520graph%2520machine%2520learning%2520studies%252C%2520which%2520does%250Anot%2520allow%2520one%2520to%2520understand%2520how%2520successfully%2520graph%2520models%2520can%2520be%2520transferred%2520to%250Atabular%2520data.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520of%2520diverse%2520graphs%250Awith%2520heterogeneous%2520tabular%2520node%2520features%2520and%2520realistic%2520prediction%2520tasks.%2520We%2520use%250Athis%2520benchmark%2520to%2520evaluate%2520a%2520vast%2520set%2520of%2520models%252C%2520including%2520simple%2520methods%250Apreviously%2520overlooked%2520in%2520the%2520literature.%2520Our%2520experiments%2520show%2520that%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520can%2520indeed%2520often%2520bring%2520gains%2520in%2520predictive%2520performance%2520for%250Atabular%2520data%252C%2520but%2520standard%2520tabular%2520models%2520also%2520can%2520be%2520adapted%2520to%2520work%2520with%250Agraph%2520data%2520by%2520using%2520simple%2520feature%2520preprocessing%252C%2520which%2520sometimes%2520enables%2520them%250Ato%2520compete%2520with%2520and%2520even%2520outperform%2520GNNs.%2520Based%2520on%2520our%2520empirical%2520study%252C%2520we%250Aprovide%2520insights%2520for%2520researchers%2520and%2520practitioners%2520in%2520both%2520tabular%2520and%2520graph%250Amachine%2520learning%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabGraphs%3A%20A%20Benchmark%20and%20Strong%20Baselines%20for%20Learning%20on%20Graphs%20with%0A%20%20Tabular%20Node%20Features&entry.906535625=Gleb%20Bazhenov%20and%20Oleg%20Platonov%20and%20Liudmila%20Prokhorenkova&entry.1292438233=%20%20Tabular%20machine%20learning%20is%20an%20important%20field%20for%20industry%20and%20science.%20In%0Athis%20field%2C%20table%20rows%20are%20usually%20treated%20as%20independent%20data%20samples%2C%20but%0Aadditional%20information%20about%20relations%20between%20them%20is%20sometimes%20available%20and%0Acan%20be%20used%20to%20improve%20predictive%20performance.%20Such%20information%20can%20be%0Anaturally%20modeled%20with%20a%20graph%2C%20thus%20tabular%20machine%20learning%20may%20benefit%20from%0Agraph%20machine%20learning%20methods.%20However%2C%20graph%20machine%20learning%20models%20are%0Atypically%20evaluated%20on%20datasets%20with%20homogeneous%20node%20features%2C%20which%20have%0Alittle%20in%20common%20with%20heterogeneous%20mixtures%20of%20numerical%20and%20categorical%0Afeatures%20present%20in%20tabular%20datasets.%20Thus%2C%20there%20is%20a%20critical%20difference%0Abetween%20the%20data%20used%20in%20tabular%20and%20graph%20machine%20learning%20studies%2C%20which%20does%0Anot%20allow%20one%20to%20understand%20how%20successfully%20graph%20models%20can%20be%20transferred%20to%0Atabular%20data.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20new%20benchmark%20of%20diverse%20graphs%0Awith%20heterogeneous%20tabular%20node%20features%20and%20realistic%20prediction%20tasks.%20We%20use%0Athis%20benchmark%20to%20evaluate%20a%20vast%20set%20of%20models%2C%20including%20simple%20methods%0Apreviously%20overlooked%20in%20the%20literature.%20Our%20experiments%20show%20that%20graph%20neural%0Anetworks%20%28GNNs%29%20can%20indeed%20often%20bring%20gains%20in%20predictive%20performance%20for%0Atabular%20data%2C%20but%20standard%20tabular%20models%20also%20can%20be%20adapted%20to%20work%20with%0Agraph%20data%20by%20using%20simple%20feature%20preprocessing%2C%20which%20sometimes%20enables%20them%0Ato%20compete%20with%20and%20even%20outperform%20GNNs.%20Based%20on%20our%20empirical%20study%2C%20we%0Aprovide%20insights%20for%20researchers%20and%20practitioners%20in%20both%20tabular%20and%20graph%0Amachine%20learning%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14500v2&entry.124074799=Read"},
{"title": "Manydepth2: Motion-Aware Self-Supervised Monocular Depth Estimation in\n  Dynamic Scenes", "author": "Kaichen Zhou and Jia-Wang Bian and Qian Xie and Jian-Qing Zheng and Niki Trigoni and Andrew Markham", "abstract": "  Despite advancements in self-supervised monocular depth estimation,\nchallenges persist in dynamic scenarios due to the dependence on assumptions\nabout a static world. In this paper, we present Manydepth2, a Motion-Guided\nCost Volume Depth Net, to achieve precise depth estimation for both dynamic\nobjects and static backgrounds, all while maintaining computational efficiency.\nTo tackle the challenges posed by dynamic content, we incorporate optical flow\nand coarse monocular depth to create a novel static reference frame. This frame\nis then utilized to build a motion-guided cost volume in collaboration with the\ntarget frame. Additionally, to enhance the accuracy and resilience of the\nnetwork structure, we introduce an attention-based depth net architecture to\neffectively integrate information from feature maps with varying resolutions.\nCompared to methods with similar computational costs, Manydepth2 achieves a\nsignificant reduction of approximately five percent in root-mean-square error\nfor self-supervised monocular depth estimation on the KITTI-2015 dataset. The\ncode could be found: https://github.com/kaichen-z/Manydepth2\n", "link": "http://arxiv.org/abs/2312.15268v4", "date": "2024-09-26", "relevancy": 2.3368, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5926}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5808}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manydepth2%3A%20Motion-Aware%20Self-Supervised%20Monocular%20Depth%20Estimation%20in%0A%20%20Dynamic%20Scenes&body=Title%3A%20Manydepth2%3A%20Motion-Aware%20Self-Supervised%20Monocular%20Depth%20Estimation%20in%0A%20%20Dynamic%20Scenes%0AAuthor%3A%20Kaichen%20Zhou%20and%20Jia-Wang%20Bian%20and%20Qian%20Xie%20and%20Jian-Qing%20Zheng%20and%20Niki%20Trigoni%20and%20Andrew%20Markham%0AAbstract%3A%20%20%20Despite%20advancements%20in%20self-supervised%20monocular%20depth%20estimation%2C%0Achallenges%20persist%20in%20dynamic%20scenarios%20due%20to%20the%20dependence%20on%20assumptions%0Aabout%20a%20static%20world.%20In%20this%20paper%2C%20we%20present%20Manydepth2%2C%20a%20Motion-Guided%0ACost%20Volume%20Depth%20Net%2C%20to%20achieve%20precise%20depth%20estimation%20for%20both%20dynamic%0Aobjects%20and%20static%20backgrounds%2C%20all%20while%20maintaining%20computational%20efficiency.%0ATo%20tackle%20the%20challenges%20posed%20by%20dynamic%20content%2C%20we%20incorporate%20optical%20flow%0Aand%20coarse%20monocular%20depth%20to%20create%20a%20novel%20static%20reference%20frame.%20This%20frame%0Ais%20then%20utilized%20to%20build%20a%20motion-guided%20cost%20volume%20in%20collaboration%20with%20the%0Atarget%20frame.%20Additionally%2C%20to%20enhance%20the%20accuracy%20and%20resilience%20of%20the%0Anetwork%20structure%2C%20we%20introduce%20an%20attention-based%20depth%20net%20architecture%20to%0Aeffectively%20integrate%20information%20from%20feature%20maps%20with%20varying%20resolutions.%0ACompared%20to%20methods%20with%20similar%20computational%20costs%2C%20Manydepth2%20achieves%20a%0Asignificant%20reduction%20of%20approximately%20five%20percent%20in%20root-mean-square%20error%0Afor%20self-supervised%20monocular%20depth%20estimation%20on%20the%20KITTI-2015%20dataset.%20The%0Acode%20could%20be%20found%3A%20https%3A//github.com/kaichen-z/Manydepth2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15268v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManydepth2%253A%2520Motion-Aware%2520Self-Supervised%2520Monocular%2520Depth%2520Estimation%2520in%250A%2520%2520Dynamic%2520Scenes%26entry.906535625%3DKaichen%2520Zhou%2520and%2520Jia-Wang%2520Bian%2520and%2520Qian%2520Xie%2520and%2520Jian-Qing%2520Zheng%2520and%2520Niki%2520Trigoni%2520and%2520Andrew%2520Markham%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520self-supervised%2520monocular%2520depth%2520estimation%252C%250Achallenges%2520persist%2520in%2520dynamic%2520scenarios%2520due%2520to%2520the%2520dependence%2520on%2520assumptions%250Aabout%2520a%2520static%2520world.%2520In%2520this%2520paper%252C%2520we%2520present%2520Manydepth2%252C%2520a%2520Motion-Guided%250ACost%2520Volume%2520Depth%2520Net%252C%2520to%2520achieve%2520precise%2520depth%2520estimation%2520for%2520both%2520dynamic%250Aobjects%2520and%2520static%2520backgrounds%252C%2520all%2520while%2520maintaining%2520computational%2520efficiency.%250ATo%2520tackle%2520the%2520challenges%2520posed%2520by%2520dynamic%2520content%252C%2520we%2520incorporate%2520optical%2520flow%250Aand%2520coarse%2520monocular%2520depth%2520to%2520create%2520a%2520novel%2520static%2520reference%2520frame.%2520This%2520frame%250Ais%2520then%2520utilized%2520to%2520build%2520a%2520motion-guided%2520cost%2520volume%2520in%2520collaboration%2520with%2520the%250Atarget%2520frame.%2520Additionally%252C%2520to%2520enhance%2520the%2520accuracy%2520and%2520resilience%2520of%2520the%250Anetwork%2520structure%252C%2520we%2520introduce%2520an%2520attention-based%2520depth%2520net%2520architecture%2520to%250Aeffectively%2520integrate%2520information%2520from%2520feature%2520maps%2520with%2520varying%2520resolutions.%250ACompared%2520to%2520methods%2520with%2520similar%2520computational%2520costs%252C%2520Manydepth2%2520achieves%2520a%250Asignificant%2520reduction%2520of%2520approximately%2520five%2520percent%2520in%2520root-mean-square%2520error%250Afor%2520self-supervised%2520monocular%2520depth%2520estimation%2520on%2520the%2520KITTI-2015%2520dataset.%2520The%250Acode%2520could%2520be%2520found%253A%2520https%253A//github.com/kaichen-z/Manydepth2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15268v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manydepth2%3A%20Motion-Aware%20Self-Supervised%20Monocular%20Depth%20Estimation%20in%0A%20%20Dynamic%20Scenes&entry.906535625=Kaichen%20Zhou%20and%20Jia-Wang%20Bian%20and%20Qian%20Xie%20and%20Jian-Qing%20Zheng%20and%20Niki%20Trigoni%20and%20Andrew%20Markham&entry.1292438233=%20%20Despite%20advancements%20in%20self-supervised%20monocular%20depth%20estimation%2C%0Achallenges%20persist%20in%20dynamic%20scenarios%20due%20to%20the%20dependence%20on%20assumptions%0Aabout%20a%20static%20world.%20In%20this%20paper%2C%20we%20present%20Manydepth2%2C%20a%20Motion-Guided%0ACost%20Volume%20Depth%20Net%2C%20to%20achieve%20precise%20depth%20estimation%20for%20both%20dynamic%0Aobjects%20and%20static%20backgrounds%2C%20all%20while%20maintaining%20computational%20efficiency.%0ATo%20tackle%20the%20challenges%20posed%20by%20dynamic%20content%2C%20we%20incorporate%20optical%20flow%0Aand%20coarse%20monocular%20depth%20to%20create%20a%20novel%20static%20reference%20frame.%20This%20frame%0Ais%20then%20utilized%20to%20build%20a%20motion-guided%20cost%20volume%20in%20collaboration%20with%20the%0Atarget%20frame.%20Additionally%2C%20to%20enhance%20the%20accuracy%20and%20resilience%20of%20the%0Anetwork%20structure%2C%20we%20introduce%20an%20attention-based%20depth%20net%20architecture%20to%0Aeffectively%20integrate%20information%20from%20feature%20maps%20with%20varying%20resolutions.%0ACompared%20to%20methods%20with%20similar%20computational%20costs%2C%20Manydepth2%20achieves%20a%0Asignificant%20reduction%20of%20approximately%20five%20percent%20in%20root-mean-square%20error%0Afor%20self-supervised%20monocular%20depth%20estimation%20on%20the%20KITTI-2015%20dataset.%20The%0Acode%20could%20be%20found%3A%20https%3A//github.com/kaichen-z/Manydepth2%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15268v4&entry.124074799=Read"},
{"title": "A Sim-to-Real Vision-based Lane Keeping System for a 1:10-scale\n  Autonomous Vehicle", "author": "Antonio Gallina and Matteo Grandin and Angelo Cenedese and Mattia Bruschetta", "abstract": "  In recent years, several competitions have highlighted the need to\ninvestigate vision-based solutions to address scenarios with functional\ninsufficiencies in perception, world modeling and localization. This article\npresents the Vision-based Lane Keeping System (VbLKS) developed by the\nDEI-Unipd Team within the context of the Bosch Future Mobility Challenge 2022.\nThe main contribution lies in a Simulation-to-Reality (Sim2Real) GPS-denied\nVbLKS for a 1:10-scale autonomous vehicle. In this VbLKS, the input to a\ntailored Pure Pursuit (PP) based control strategy, namely the Lookahead Heading\nError (LHE), is estimated at a constant lookahead distance employing a\nConvolutional Neural Network (CNN). A training strategy for a compact CNN is\nproposed, emphasizing data generation and augmentation on simulated camera\nimages from a 3D Gazebo simulator, and enabling real-time operation on\nlow-level hardware. A tailored PP-based lateral controller equipped with a\nderivative action and a PP-based velocity reference generation are implemented.\nTuning ranges are established through a systematic time-delay stability\nanalysis. Validation in a representative controlled laboratory setting is\nprovided.\n", "link": "http://arxiv.org/abs/2409.18097v1", "date": "2024-09-26", "relevancy": 2.3233, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5917}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5828}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sim-to-Real%20Vision-based%20Lane%20Keeping%20System%20for%20a%201%3A10-scale%0A%20%20Autonomous%20Vehicle&body=Title%3A%20A%20Sim-to-Real%20Vision-based%20Lane%20Keeping%20System%20for%20a%201%3A10-scale%0A%20%20Autonomous%20Vehicle%0AAuthor%3A%20Antonio%20Gallina%20and%20Matteo%20Grandin%20and%20Angelo%20Cenedese%20and%20Mattia%20Bruschetta%0AAbstract%3A%20%20%20In%20recent%20years%2C%20several%20competitions%20have%20highlighted%20the%20need%20to%0Ainvestigate%20vision-based%20solutions%20to%20address%20scenarios%20with%20functional%0Ainsufficiencies%20in%20perception%2C%20world%20modeling%20and%20localization.%20This%20article%0Apresents%20the%20Vision-based%20Lane%20Keeping%20System%20%28VbLKS%29%20developed%20by%20the%0ADEI-Unipd%20Team%20within%20the%20context%20of%20the%20Bosch%20Future%20Mobility%20Challenge%202022.%0AThe%20main%20contribution%20lies%20in%20a%20Simulation-to-Reality%20%28Sim2Real%29%20GPS-denied%0AVbLKS%20for%20a%201%3A10-scale%20autonomous%20vehicle.%20In%20this%20VbLKS%2C%20the%20input%20to%20a%0Atailored%20Pure%20Pursuit%20%28PP%29%20based%20control%20strategy%2C%20namely%20the%20Lookahead%20Heading%0AError%20%28LHE%29%2C%20is%20estimated%20at%20a%20constant%20lookahead%20distance%20employing%20a%0AConvolutional%20Neural%20Network%20%28CNN%29.%20A%20training%20strategy%20for%20a%20compact%20CNN%20is%0Aproposed%2C%20emphasizing%20data%20generation%20and%20augmentation%20on%20simulated%20camera%0Aimages%20from%20a%203D%20Gazebo%20simulator%2C%20and%20enabling%20real-time%20operation%20on%0Alow-level%20hardware.%20A%20tailored%20PP-based%20lateral%20controller%20equipped%20with%20a%0Aderivative%20action%20and%20a%20PP-based%20velocity%20reference%20generation%20are%20implemented.%0ATuning%20ranges%20are%20established%20through%20a%20systematic%20time-delay%20stability%0Aanalysis.%20Validation%20in%20a%20representative%20controlled%20laboratory%20setting%20is%0Aprovided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sim-to-Real%2520Vision-based%2520Lane%2520Keeping%2520System%2520for%2520a%25201%253A10-scale%250A%2520%2520Autonomous%2520Vehicle%26entry.906535625%3DAntonio%2520Gallina%2520and%2520Matteo%2520Grandin%2520and%2520Angelo%2520Cenedese%2520and%2520Mattia%2520Bruschetta%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520several%2520competitions%2520have%2520highlighted%2520the%2520need%2520to%250Ainvestigate%2520vision-based%2520solutions%2520to%2520address%2520scenarios%2520with%2520functional%250Ainsufficiencies%2520in%2520perception%252C%2520world%2520modeling%2520and%2520localization.%2520This%2520article%250Apresents%2520the%2520Vision-based%2520Lane%2520Keeping%2520System%2520%2528VbLKS%2529%2520developed%2520by%2520the%250ADEI-Unipd%2520Team%2520within%2520the%2520context%2520of%2520the%2520Bosch%2520Future%2520Mobility%2520Challenge%25202022.%250AThe%2520main%2520contribution%2520lies%2520in%2520a%2520Simulation-to-Reality%2520%2528Sim2Real%2529%2520GPS-denied%250AVbLKS%2520for%2520a%25201%253A10-scale%2520autonomous%2520vehicle.%2520In%2520this%2520VbLKS%252C%2520the%2520input%2520to%2520a%250Atailored%2520Pure%2520Pursuit%2520%2528PP%2529%2520based%2520control%2520strategy%252C%2520namely%2520the%2520Lookahead%2520Heading%250AError%2520%2528LHE%2529%252C%2520is%2520estimated%2520at%2520a%2520constant%2520lookahead%2520distance%2520employing%2520a%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529.%2520A%2520training%2520strategy%2520for%2520a%2520compact%2520CNN%2520is%250Aproposed%252C%2520emphasizing%2520data%2520generation%2520and%2520augmentation%2520on%2520simulated%2520camera%250Aimages%2520from%2520a%25203D%2520Gazebo%2520simulator%252C%2520and%2520enabling%2520real-time%2520operation%2520on%250Alow-level%2520hardware.%2520A%2520tailored%2520PP-based%2520lateral%2520controller%2520equipped%2520with%2520a%250Aderivative%2520action%2520and%2520a%2520PP-based%2520velocity%2520reference%2520generation%2520are%2520implemented.%250ATuning%2520ranges%2520are%2520established%2520through%2520a%2520systematic%2520time-delay%2520stability%250Aanalysis.%2520Validation%2520in%2520a%2520representative%2520controlled%2520laboratory%2520setting%2520is%250Aprovided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sim-to-Real%20Vision-based%20Lane%20Keeping%20System%20for%20a%201%3A10-scale%0A%20%20Autonomous%20Vehicle&entry.906535625=Antonio%20Gallina%20and%20Matteo%20Grandin%20and%20Angelo%20Cenedese%20and%20Mattia%20Bruschetta&entry.1292438233=%20%20In%20recent%20years%2C%20several%20competitions%20have%20highlighted%20the%20need%20to%0Ainvestigate%20vision-based%20solutions%20to%20address%20scenarios%20with%20functional%0Ainsufficiencies%20in%20perception%2C%20world%20modeling%20and%20localization.%20This%20article%0Apresents%20the%20Vision-based%20Lane%20Keeping%20System%20%28VbLKS%29%20developed%20by%20the%0ADEI-Unipd%20Team%20within%20the%20context%20of%20the%20Bosch%20Future%20Mobility%20Challenge%202022.%0AThe%20main%20contribution%20lies%20in%20a%20Simulation-to-Reality%20%28Sim2Real%29%20GPS-denied%0AVbLKS%20for%20a%201%3A10-scale%20autonomous%20vehicle.%20In%20this%20VbLKS%2C%20the%20input%20to%20a%0Atailored%20Pure%20Pursuit%20%28PP%29%20based%20control%20strategy%2C%20namely%20the%20Lookahead%20Heading%0AError%20%28LHE%29%2C%20is%20estimated%20at%20a%20constant%20lookahead%20distance%20employing%20a%0AConvolutional%20Neural%20Network%20%28CNN%29.%20A%20training%20strategy%20for%20a%20compact%20CNN%20is%0Aproposed%2C%20emphasizing%20data%20generation%20and%20augmentation%20on%20simulated%20camera%0Aimages%20from%20a%203D%20Gazebo%20simulator%2C%20and%20enabling%20real-time%20operation%20on%0Alow-level%20hardware.%20A%20tailored%20PP-based%20lateral%20controller%20equipped%20with%20a%0Aderivative%20action%20and%20a%20PP-based%20velocity%20reference%20generation%20are%20implemented.%0ATuning%20ranges%20are%20established%20through%20a%20systematic%20time-delay%20stability%0Aanalysis.%20Validation%20in%20a%20representative%20controlled%20laboratory%20setting%20is%0Aprovided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18097v1&entry.124074799=Read"},
{"title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models", "author": "Yixi Wu and Pengfei He and Zehao Wang and Shaowei Wang and Yuan Tian and Tse-Hsun Chen", "abstract": "  Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs.\n", "link": "http://arxiv.org/abs/2409.15228v3", "date": "2024-09-26", "relevancy": 2.3217, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.475}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Framework%20for%20Evaluating%20API-oriented%20Code%20Generation%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20A%20Comprehensive%20Framework%20for%20Evaluating%20API-oriented%20Code%20Generation%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yixi%20Wu%20and%20Pengfei%20He%20and%20Zehao%20Wang%20and%20Shaowei%20Wang%20and%20Yuan%20Tian%20and%20Tse-Hsun%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20like%20GitHub%20Copilot%20and%20ChatGPT%20have%20emerged%20as%0Apowerful%20tools%20for%20code%20generation%2C%20significantly%20enhancing%20productivity%20and%0Aaccelerating%20software%20development.%20However%2C%20existing%20benchmarks%20primarily%20focus%0Aon%20general%20code%20generation%20without%20considering%20API-oriented%20code%20generation%2C%0Ai.e.%2C%20generating%20code%20that%20invokes%20APIs%20from%20specific%20libraries.%20Given%20the%0Agrowing%20demand%20for%20API-oriented%20code%20generation%2C%20there%20is%20a%20pressing%20need%20for%20a%0Asystematic%20and%20automated%20approach%20to%20evaluate%20LLM%20on%20API-oriented%20code%0Ageneration.%20To%20address%20this%20gap%2C%20we%20propose%20AutoAPIEval%2C%20a%20lightweight%20and%0Aautomated%20framework%20designed%20to%20evaluate%20the%20capabilities%20of%20LLMs%20in%0AAPI-oriented%20code%20generation.%20Our%20framework%20works%20with%20any%20library%20that%0Aprovides%20API%20documentation%20and%20focuses%20on%20two%20unit%20tasks%3A%20API%20recommendation%0Aand%20code%20example%20generation%2C%20along%20with%20four%20metrics%20to%20evaluate%20the%20generated%0AAPIs%20and%20code%20examples%2C%20such%20as%20the%20proportion%20of%20incorrect%20API%20recommendations%0Afor%20Task%201%2C%20and%20the%20proportion%20of%20code%20examples%20where%20no%20specific%20API%20is%0Ainvoked%20and%20uncompilable/unexecutable%20code%20examples%20for%20Task%202.%20In%20addition%2C%20we%0Aconducted%20a%20case%20study%20on%20three%20LLMs%20%28ChatGPT%2C%20MagiCoder%2C%20and%20DeepSeek%20Coder%29%0Aand%20Java%20Runtime%20Environment%208%20to%20demonstrate%20the%20framework%27s%20effectiveness.%0AOur%20findings%20reveal%20substantial%20variability%20in%20LLM%20performance%20across%20tasks%2C%0Awith%20ChatGPT%20adhering%20better%20to%20instructions%2C%20while%20sharing%20similar%0Aeffectiveness%20in%20code%20example%20generation%20with%20its%20counterparts%20%28i.e.%2C%20MagiCoder%0Aand%20DeekSeek%20Coder%29.%20We%20also%20identify%20key%20factors%20associated%20with%20code%20quality%2C%0Asuch%20as%20API%20popularity%20and%20model%20confidence%2C%20and%20build%20classifiers%20that%20achieve%0Ahigh%20accuracy%20in%20detecting%20incorrect%20API%20recommendations%20and%20erroneous%20code%0Aexamples.%20Retrieval-augmented%20generation%20enhances%20the%20quality%20of%20code%20generated%0Aby%20LLMs%2C%20though%20its%20effectiveness%20varies%20across%20different%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15228v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Framework%2520for%2520Evaluating%2520API-oriented%2520Code%2520Generation%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYixi%2520Wu%2520and%2520Pengfei%2520He%2520and%2520Zehao%2520Wang%2520and%2520Shaowei%2520Wang%2520and%2520Yuan%2520Tian%2520and%2520Tse-Hsun%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520like%2520GitHub%2520Copilot%2520and%2520ChatGPT%2520have%2520emerged%2520as%250Apowerful%2520tools%2520for%2520code%2520generation%252C%2520significantly%2520enhancing%2520productivity%2520and%250Aaccelerating%2520software%2520development.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520focus%250Aon%2520general%2520code%2520generation%2520without%2520considering%2520API-oriented%2520code%2520generation%252C%250Ai.e.%252C%2520generating%2520code%2520that%2520invokes%2520APIs%2520from%2520specific%2520libraries.%2520Given%2520the%250Agrowing%2520demand%2520for%2520API-oriented%2520code%2520generation%252C%2520there%2520is%2520a%2520pressing%2520need%2520for%2520a%250Asystematic%2520and%2520automated%2520approach%2520to%2520evaluate%2520LLM%2520on%2520API-oriented%2520code%250Ageneration.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520AutoAPIEval%252C%2520a%2520lightweight%2520and%250Aautomated%2520framework%2520designed%2520to%2520evaluate%2520the%2520capabilities%2520of%2520LLMs%2520in%250AAPI-oriented%2520code%2520generation.%2520Our%2520framework%2520works%2520with%2520any%2520library%2520that%250Aprovides%2520API%2520documentation%2520and%2520focuses%2520on%2520two%2520unit%2520tasks%253A%2520API%2520recommendation%250Aand%2520code%2520example%2520generation%252C%2520along%2520with%2520four%2520metrics%2520to%2520evaluate%2520the%2520generated%250AAPIs%2520and%2520code%2520examples%252C%2520such%2520as%2520the%2520proportion%2520of%2520incorrect%2520API%2520recommendations%250Afor%2520Task%25201%252C%2520and%2520the%2520proportion%2520of%2520code%2520examples%2520where%2520no%2520specific%2520API%2520is%250Ainvoked%2520and%2520uncompilable/unexecutable%2520code%2520examples%2520for%2520Task%25202.%2520In%2520addition%252C%2520we%250Aconducted%2520a%2520case%2520study%2520on%2520three%2520LLMs%2520%2528ChatGPT%252C%2520MagiCoder%252C%2520and%2520DeepSeek%2520Coder%2529%250Aand%2520Java%2520Runtime%2520Environment%25208%2520to%2520demonstrate%2520the%2520framework%2527s%2520effectiveness.%250AOur%2520findings%2520reveal%2520substantial%2520variability%2520in%2520LLM%2520performance%2520across%2520tasks%252C%250Awith%2520ChatGPT%2520adhering%2520better%2520to%2520instructions%252C%2520while%2520sharing%2520similar%250Aeffectiveness%2520in%2520code%2520example%2520generation%2520with%2520its%2520counterparts%2520%2528i.e.%252C%2520MagiCoder%250Aand%2520DeekSeek%2520Coder%2529.%2520We%2520also%2520identify%2520key%2520factors%2520associated%2520with%2520code%2520quality%252C%250Asuch%2520as%2520API%2520popularity%2520and%2520model%2520confidence%252C%2520and%2520build%2520classifiers%2520that%2520achieve%250Ahigh%2520accuracy%2520in%2520detecting%2520incorrect%2520API%2520recommendations%2520and%2520erroneous%2520code%250Aexamples.%2520Retrieval-augmented%2520generation%2520enhances%2520the%2520quality%2520of%2520code%2520generated%250Aby%2520LLMs%252C%2520though%2520its%2520effectiveness%2520varies%2520across%2520different%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15228v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Framework%20for%20Evaluating%20API-oriented%20Code%20Generation%20in%0A%20%20Large%20Language%20Models&entry.906535625=Yixi%20Wu%20and%20Pengfei%20He%20and%20Zehao%20Wang%20and%20Shaowei%20Wang%20and%20Yuan%20Tian%20and%20Tse-Hsun%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20like%20GitHub%20Copilot%20and%20ChatGPT%20have%20emerged%20as%0Apowerful%20tools%20for%20code%20generation%2C%20significantly%20enhancing%20productivity%20and%0Aaccelerating%20software%20development.%20However%2C%20existing%20benchmarks%20primarily%20focus%0Aon%20general%20code%20generation%20without%20considering%20API-oriented%20code%20generation%2C%0Ai.e.%2C%20generating%20code%20that%20invokes%20APIs%20from%20specific%20libraries.%20Given%20the%0Agrowing%20demand%20for%20API-oriented%20code%20generation%2C%20there%20is%20a%20pressing%20need%20for%20a%0Asystematic%20and%20automated%20approach%20to%20evaluate%20LLM%20on%20API-oriented%20code%0Ageneration.%20To%20address%20this%20gap%2C%20we%20propose%20AutoAPIEval%2C%20a%20lightweight%20and%0Aautomated%20framework%20designed%20to%20evaluate%20the%20capabilities%20of%20LLMs%20in%0AAPI-oriented%20code%20generation.%20Our%20framework%20works%20with%20any%20library%20that%0Aprovides%20API%20documentation%20and%20focuses%20on%20two%20unit%20tasks%3A%20API%20recommendation%0Aand%20code%20example%20generation%2C%20along%20with%20four%20metrics%20to%20evaluate%20the%20generated%0AAPIs%20and%20code%20examples%2C%20such%20as%20the%20proportion%20of%20incorrect%20API%20recommendations%0Afor%20Task%201%2C%20and%20the%20proportion%20of%20code%20examples%20where%20no%20specific%20API%20is%0Ainvoked%20and%20uncompilable/unexecutable%20code%20examples%20for%20Task%202.%20In%20addition%2C%20we%0Aconducted%20a%20case%20study%20on%20three%20LLMs%20%28ChatGPT%2C%20MagiCoder%2C%20and%20DeepSeek%20Coder%29%0Aand%20Java%20Runtime%20Environment%208%20to%20demonstrate%20the%20framework%27s%20effectiveness.%0AOur%20findings%20reveal%20substantial%20variability%20in%20LLM%20performance%20across%20tasks%2C%0Awith%20ChatGPT%20adhering%20better%20to%20instructions%2C%20while%20sharing%20similar%0Aeffectiveness%20in%20code%20example%20generation%20with%20its%20counterparts%20%28i.e.%2C%20MagiCoder%0Aand%20DeekSeek%20Coder%29.%20We%20also%20identify%20key%20factors%20associated%20with%20code%20quality%2C%0Asuch%20as%20API%20popularity%20and%20model%20confidence%2C%20and%20build%20classifiers%20that%20achieve%0Ahigh%20accuracy%20in%20detecting%20incorrect%20API%20recommendations%20and%20erroneous%20code%0Aexamples.%20Retrieval-augmented%20generation%20enhances%20the%20quality%20of%20code%20generated%0Aby%20LLMs%2C%20though%20its%20effectiveness%20varies%20across%20different%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15228v3&entry.124074799=Read"},
{"title": "DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion\n  Probabilistic Models", "author": "Helin Cao and Sven Behnke", "abstract": "  Perception systems play a crucial role in autonomous driving, incorporating\nmultiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors\nare widely used to capture sparse point clouds of the vehicle's surroundings.\nHowever, such systems struggle to perceive occluded areas and gaps in the scene\ndue to the sparsity of these point clouds and their lack of semantics. To\naddress these challenges, Semantic Scene Completion (SSC) jointly predicts\nunobserved geometry and semantics in the scene given raw LiDAR measurements,\naiming for a more complete scene representation. Building on promising results\nof diffusion models in image generation and super-resolution tasks, we propose\ntheir extension to SSC by implementing the noising and denoising diffusion\nprocesses in the point and semantic spaces individually. To control the\ngeneration, we employ semantic LiDAR point clouds as conditional input and\ndesign local and global regularization losses to stabilize the denoising\nprocess. We evaluate our approach on autonomous driving datasets and our\napproach outperforms the state-of-the-art for SSC.\n", "link": "http://arxiv.org/abs/2409.18092v1", "date": "2024-09-26", "relevancy": 2.3196, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffSSC%3A%20Semantic%20LiDAR%20Scan%20Completion%20using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models&body=Title%3A%20DiffSSC%3A%20Semantic%20LiDAR%20Scan%20Completion%20using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models%0AAuthor%3A%20Helin%20Cao%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Perception%20systems%20play%20a%20crucial%20role%20in%20autonomous%20driving%2C%20incorporating%0Amultiple%20sensors%20and%20corresponding%20computer%20vision%20algorithms.%203D%20LiDAR%20sensors%0Aare%20widely%20used%20to%20capture%20sparse%20point%20clouds%20of%20the%20vehicle%27s%20surroundings.%0AHowever%2C%20such%20systems%20struggle%20to%20perceive%20occluded%20areas%20and%20gaps%20in%20the%20scene%0Adue%20to%20the%20sparsity%20of%20these%20point%20clouds%20and%20their%20lack%20of%20semantics.%20To%0Aaddress%20these%20challenges%2C%20Semantic%20Scene%20Completion%20%28SSC%29%20jointly%20predicts%0Aunobserved%20geometry%20and%20semantics%20in%20the%20scene%20given%20raw%20LiDAR%20measurements%2C%0Aaiming%20for%20a%20more%20complete%20scene%20representation.%20Building%20on%20promising%20results%0Aof%20diffusion%20models%20in%20image%20generation%20and%20super-resolution%20tasks%2C%20we%20propose%0Atheir%20extension%20to%20SSC%20by%20implementing%20the%20noising%20and%20denoising%20diffusion%0Aprocesses%20in%20the%20point%20and%20semantic%20spaces%20individually.%20To%20control%20the%0Ageneration%2C%20we%20employ%20semantic%20LiDAR%20point%20clouds%20as%20conditional%20input%20and%0Adesign%20local%20and%20global%20regularization%20losses%20to%20stabilize%20the%20denoising%0Aprocess.%20We%20evaluate%20our%20approach%20on%20autonomous%20driving%20datasets%20and%20our%0Aapproach%20outperforms%20the%20state-of-the-art%20for%20SSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffSSC%253A%2520Semantic%2520LiDAR%2520Scan%2520Completion%2520using%2520Denoising%2520Diffusion%250A%2520%2520Probabilistic%2520Models%26entry.906535625%3DHelin%2520Cao%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Perception%2520systems%2520play%2520a%2520crucial%2520role%2520in%2520autonomous%2520driving%252C%2520incorporating%250Amultiple%2520sensors%2520and%2520corresponding%2520computer%2520vision%2520algorithms.%25203D%2520LiDAR%2520sensors%250Aare%2520widely%2520used%2520to%2520capture%2520sparse%2520point%2520clouds%2520of%2520the%2520vehicle%2527s%2520surroundings.%250AHowever%252C%2520such%2520systems%2520struggle%2520to%2520perceive%2520occluded%2520areas%2520and%2520gaps%2520in%2520the%2520scene%250Adue%2520to%2520the%2520sparsity%2520of%2520these%2520point%2520clouds%2520and%2520their%2520lack%2520of%2520semantics.%2520To%250Aaddress%2520these%2520challenges%252C%2520Semantic%2520Scene%2520Completion%2520%2528SSC%2529%2520jointly%2520predicts%250Aunobserved%2520geometry%2520and%2520semantics%2520in%2520the%2520scene%2520given%2520raw%2520LiDAR%2520measurements%252C%250Aaiming%2520for%2520a%2520more%2520complete%2520scene%2520representation.%2520Building%2520on%2520promising%2520results%250Aof%2520diffusion%2520models%2520in%2520image%2520generation%2520and%2520super-resolution%2520tasks%252C%2520we%2520propose%250Atheir%2520extension%2520to%2520SSC%2520by%2520implementing%2520the%2520noising%2520and%2520denoising%2520diffusion%250Aprocesses%2520in%2520the%2520point%2520and%2520semantic%2520spaces%2520individually.%2520To%2520control%2520the%250Ageneration%252C%2520we%2520employ%2520semantic%2520LiDAR%2520point%2520clouds%2520as%2520conditional%2520input%2520and%250Adesign%2520local%2520and%2520global%2520regularization%2520losses%2520to%2520stabilize%2520the%2520denoising%250Aprocess.%2520We%2520evaluate%2520our%2520approach%2520on%2520autonomous%2520driving%2520datasets%2520and%2520our%250Aapproach%2520outperforms%2520the%2520state-of-the-art%2520for%2520SSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffSSC%3A%20Semantic%20LiDAR%20Scan%20Completion%20using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models&entry.906535625=Helin%20Cao%20and%20Sven%20Behnke&entry.1292438233=%20%20Perception%20systems%20play%20a%20crucial%20role%20in%20autonomous%20driving%2C%20incorporating%0Amultiple%20sensors%20and%20corresponding%20computer%20vision%20algorithms.%203D%20LiDAR%20sensors%0Aare%20widely%20used%20to%20capture%20sparse%20point%20clouds%20of%20the%20vehicle%27s%20surroundings.%0AHowever%2C%20such%20systems%20struggle%20to%20perceive%20occluded%20areas%20and%20gaps%20in%20the%20scene%0Adue%20to%20the%20sparsity%20of%20these%20point%20clouds%20and%20their%20lack%20of%20semantics.%20To%0Aaddress%20these%20challenges%2C%20Semantic%20Scene%20Completion%20%28SSC%29%20jointly%20predicts%0Aunobserved%20geometry%20and%20semantics%20in%20the%20scene%20given%20raw%20LiDAR%20measurements%2C%0Aaiming%20for%20a%20more%20complete%20scene%20representation.%20Building%20on%20promising%20results%0Aof%20diffusion%20models%20in%20image%20generation%20and%20super-resolution%20tasks%2C%20we%20propose%0Atheir%20extension%20to%20SSC%20by%20implementing%20the%20noising%20and%20denoising%20diffusion%0Aprocesses%20in%20the%20point%20and%20semantic%20spaces%20individually.%20To%20control%20the%0Ageneration%2C%20we%20employ%20semantic%20LiDAR%20point%20clouds%20as%20conditional%20input%20and%0Adesign%20local%20and%20global%20regularization%20losses%20to%20stabilize%20the%20denoising%0Aprocess.%20We%20evaluate%20our%20approach%20on%20autonomous%20driving%20datasets%20and%20our%0Aapproach%20outperforms%20the%20state-of-the-art%20for%20SSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18092v1&entry.124074799=Read"},
{"title": "Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze\n  Target Detection", "author": "Andrea Toaiari and Vittorio Murino and Marco Cristani and Cigdem Beyan", "abstract": "  Gaze Target Detection (GTD), i.e., determining where a person is looking\nwithin a scene from an external viewpoint, is a challenging task, particularly\nin 3D space. Existing approaches heavily rely on analyzing the person's\nappearance, primarily focusing on their face to predict the gaze target. This\npaper presents a novel approach to tackle this problem by utilizing the\nperson's upper-body pose and available depth maps to extract a 3D gaze\ndirection and employing a multi-stage or an end-to-end pipeline to predict the\ngazed target. When predicted accurately, the human body pose can provide\nvaluable information about the head pose, which is a good approximation of the\ngaze direction, as well as the position of the arms and hands, which are linked\nto the activity the person is performing and the objects they are likely\nfocusing on. Consequently, in addition to performing gaze estimation in 3D, we\nare also able to perform GTD simultaneously. We demonstrate state-of-the-art\nresults on the most comprehensive publicly accessible 3D gaze target detection\ndataset without requiring images of the person's face, thus promoting privacy\npreservation in various application contexts. The code is available at\nhttps://github.com/intelligolabs/privacy-gtd-3D.\n", "link": "http://arxiv.org/abs/2409.17886v1", "date": "2024-09-26", "relevancy": 2.3191, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6385}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5389}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upper-Body%20Pose-based%20Gaze%20Estimation%20for%20Privacy-Preserving%203D%20Gaze%0A%20%20Target%20Detection&body=Title%3A%20Upper-Body%20Pose-based%20Gaze%20Estimation%20for%20Privacy-Preserving%203D%20Gaze%0A%20%20Target%20Detection%0AAuthor%3A%20Andrea%20Toaiari%20and%20Vittorio%20Murino%20and%20Marco%20Cristani%20and%20Cigdem%20Beyan%0AAbstract%3A%20%20%20Gaze%20Target%20Detection%20%28GTD%29%2C%20i.e.%2C%20determining%20where%20a%20person%20is%20looking%0Awithin%20a%20scene%20from%20an%20external%20viewpoint%2C%20is%20a%20challenging%20task%2C%20particularly%0Ain%203D%20space.%20Existing%20approaches%20heavily%20rely%20on%20analyzing%20the%20person%27s%0Aappearance%2C%20primarily%20focusing%20on%20their%20face%20to%20predict%20the%20gaze%20target.%20This%0Apaper%20presents%20a%20novel%20approach%20to%20tackle%20this%20problem%20by%20utilizing%20the%0Aperson%27s%20upper-body%20pose%20and%20available%20depth%20maps%20to%20extract%20a%203D%20gaze%0Adirection%20and%20employing%20a%20multi-stage%20or%20an%20end-to-end%20pipeline%20to%20predict%20the%0Agazed%20target.%20When%20predicted%20accurately%2C%20the%20human%20body%20pose%20can%20provide%0Avaluable%20information%20about%20the%20head%20pose%2C%20which%20is%20a%20good%20approximation%20of%20the%0Agaze%20direction%2C%20as%20well%20as%20the%20position%20of%20the%20arms%20and%20hands%2C%20which%20are%20linked%0Ato%20the%20activity%20the%20person%20is%20performing%20and%20the%20objects%20they%20are%20likely%0Afocusing%20on.%20Consequently%2C%20in%20addition%20to%20performing%20gaze%20estimation%20in%203D%2C%20we%0Aare%20also%20able%20to%20perform%20GTD%20simultaneously.%20We%20demonstrate%20state-of-the-art%0Aresults%20on%20the%20most%20comprehensive%20publicly%20accessible%203D%20gaze%20target%20detection%0Adataset%20without%20requiring%20images%20of%20the%20person%27s%20face%2C%20thus%20promoting%20privacy%0Apreservation%20in%20various%20application%20contexts.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/privacy-gtd-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpper-Body%2520Pose-based%2520Gaze%2520Estimation%2520for%2520Privacy-Preserving%25203D%2520Gaze%250A%2520%2520Target%2520Detection%26entry.906535625%3DAndrea%2520Toaiari%2520and%2520Vittorio%2520Murino%2520and%2520Marco%2520Cristani%2520and%2520Cigdem%2520Beyan%26entry.1292438233%3D%2520%2520Gaze%2520Target%2520Detection%2520%2528GTD%2529%252C%2520i.e.%252C%2520determining%2520where%2520a%2520person%2520is%2520looking%250Awithin%2520a%2520scene%2520from%2520an%2520external%2520viewpoint%252C%2520is%2520a%2520challenging%2520task%252C%2520particularly%250Ain%25203D%2520space.%2520Existing%2520approaches%2520heavily%2520rely%2520on%2520analyzing%2520the%2520person%2527s%250Aappearance%252C%2520primarily%2520focusing%2520on%2520their%2520face%2520to%2520predict%2520the%2520gaze%2520target.%2520This%250Apaper%2520presents%2520a%2520novel%2520approach%2520to%2520tackle%2520this%2520problem%2520by%2520utilizing%2520the%250Aperson%2527s%2520upper-body%2520pose%2520and%2520available%2520depth%2520maps%2520to%2520extract%2520a%25203D%2520gaze%250Adirection%2520and%2520employing%2520a%2520multi-stage%2520or%2520an%2520end-to-end%2520pipeline%2520to%2520predict%2520the%250Agazed%2520target.%2520When%2520predicted%2520accurately%252C%2520the%2520human%2520body%2520pose%2520can%2520provide%250Avaluable%2520information%2520about%2520the%2520head%2520pose%252C%2520which%2520is%2520a%2520good%2520approximation%2520of%2520the%250Agaze%2520direction%252C%2520as%2520well%2520as%2520the%2520position%2520of%2520the%2520arms%2520and%2520hands%252C%2520which%2520are%2520linked%250Ato%2520the%2520activity%2520the%2520person%2520is%2520performing%2520and%2520the%2520objects%2520they%2520are%2520likely%250Afocusing%2520on.%2520Consequently%252C%2520in%2520addition%2520to%2520performing%2520gaze%2520estimation%2520in%25203D%252C%2520we%250Aare%2520also%2520able%2520to%2520perform%2520GTD%2520simultaneously.%2520We%2520demonstrate%2520state-of-the-art%250Aresults%2520on%2520the%2520most%2520comprehensive%2520publicly%2520accessible%25203D%2520gaze%2520target%2520detection%250Adataset%2520without%2520requiring%2520images%2520of%2520the%2520person%2527s%2520face%252C%2520thus%2520promoting%2520privacy%250Apreservation%2520in%2520various%2520application%2520contexts.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/intelligolabs/privacy-gtd-3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upper-Body%20Pose-based%20Gaze%20Estimation%20for%20Privacy-Preserving%203D%20Gaze%0A%20%20Target%20Detection&entry.906535625=Andrea%20Toaiari%20and%20Vittorio%20Murino%20and%20Marco%20Cristani%20and%20Cigdem%20Beyan&entry.1292438233=%20%20Gaze%20Target%20Detection%20%28GTD%29%2C%20i.e.%2C%20determining%20where%20a%20person%20is%20looking%0Awithin%20a%20scene%20from%20an%20external%20viewpoint%2C%20is%20a%20challenging%20task%2C%20particularly%0Ain%203D%20space.%20Existing%20approaches%20heavily%20rely%20on%20analyzing%20the%20person%27s%0Aappearance%2C%20primarily%20focusing%20on%20their%20face%20to%20predict%20the%20gaze%20target.%20This%0Apaper%20presents%20a%20novel%20approach%20to%20tackle%20this%20problem%20by%20utilizing%20the%0Aperson%27s%20upper-body%20pose%20and%20available%20depth%20maps%20to%20extract%20a%203D%20gaze%0Adirection%20and%20employing%20a%20multi-stage%20or%20an%20end-to-end%20pipeline%20to%20predict%20the%0Agazed%20target.%20When%20predicted%20accurately%2C%20the%20human%20body%20pose%20can%20provide%0Avaluable%20information%20about%20the%20head%20pose%2C%20which%20is%20a%20good%20approximation%20of%20the%0Agaze%20direction%2C%20as%20well%20as%20the%20position%20of%20the%20arms%20and%20hands%2C%20which%20are%20linked%0Ato%20the%20activity%20the%20person%20is%20performing%20and%20the%20objects%20they%20are%20likely%0Afocusing%20on.%20Consequently%2C%20in%20addition%20to%20performing%20gaze%20estimation%20in%203D%2C%20we%0Aare%20also%20able%20to%20perform%20GTD%20simultaneously.%20We%20demonstrate%20state-of-the-art%0Aresults%20on%20the%20most%20comprehensive%20publicly%20accessible%203D%20gaze%20target%20detection%0Adataset%20without%20requiring%20images%20of%20the%20person%27s%20face%2C%20thus%20promoting%20privacy%0Apreservation%20in%20various%20application%20contexts.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/privacy-gtd-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17886v1&entry.124074799=Read"},
{"title": "Diffusion-based Generative Image Outpainting for Recovery of\n  FOV-Truncated CT Images", "author": "Michelle Espranita Liman and Daniel Rueckert and Florian J. Fintelmann and Philip M\u00fcller", "abstract": "  Field-of-view (FOV) recovery of truncated chest CT scans is crucial for\naccurate body composition analysis, which involves quantifying skeletal muscle\nand subcutaneous adipose tissue (SAT) on CT slices. This, in turn, enables\ndisease prognostication. Here, we present a method for recovering truncated CT\nslices using generative image outpainting. We train a diffusion model and apply\nit to truncated CT slices generated by simulating a small FOV. Our model\nreliably recovers the truncated anatomy and outperforms the previous\nstate-of-the-art despite being trained on 87% less data.\n", "link": "http://arxiv.org/abs/2406.04769v2", "date": "2024-09-26", "relevancy": 2.3114, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5979}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.574}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Generative%20Image%20Outpainting%20for%20Recovery%20of%0A%20%20FOV-Truncated%20CT%20Images&body=Title%3A%20Diffusion-based%20Generative%20Image%20Outpainting%20for%20Recovery%20of%0A%20%20FOV-Truncated%20CT%20Images%0AAuthor%3A%20Michelle%20Espranita%20Liman%20and%20Daniel%20Rueckert%20and%20Florian%20J.%20Fintelmann%20and%20Philip%20M%C3%BCller%0AAbstract%3A%20%20%20Field-of-view%20%28FOV%29%20recovery%20of%20truncated%20chest%20CT%20scans%20is%20crucial%20for%0Aaccurate%20body%20composition%20analysis%2C%20which%20involves%20quantifying%20skeletal%20muscle%0Aand%20subcutaneous%20adipose%20tissue%20%28SAT%29%20on%20CT%20slices.%20This%2C%20in%20turn%2C%20enables%0Adisease%20prognostication.%20Here%2C%20we%20present%20a%20method%20for%20recovering%20truncated%20CT%0Aslices%20using%20generative%20image%20outpainting.%20We%20train%20a%20diffusion%20model%20and%20apply%0Ait%20to%20truncated%20CT%20slices%20generated%20by%20simulating%20a%20small%20FOV.%20Our%20model%0Areliably%20recovers%20the%20truncated%20anatomy%20and%20outperforms%20the%20previous%0Astate-of-the-art%20despite%20being%20trained%20on%2087%25%20less%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Generative%2520Image%2520Outpainting%2520for%2520Recovery%2520of%250A%2520%2520FOV-Truncated%2520CT%2520Images%26entry.906535625%3DMichelle%2520Espranita%2520Liman%2520and%2520Daniel%2520Rueckert%2520and%2520Florian%2520J.%2520Fintelmann%2520and%2520Philip%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520Field-of-view%2520%2528FOV%2529%2520recovery%2520of%2520truncated%2520chest%2520CT%2520scans%2520is%2520crucial%2520for%250Aaccurate%2520body%2520composition%2520analysis%252C%2520which%2520involves%2520quantifying%2520skeletal%2520muscle%250Aand%2520subcutaneous%2520adipose%2520tissue%2520%2528SAT%2529%2520on%2520CT%2520slices.%2520This%252C%2520in%2520turn%252C%2520enables%250Adisease%2520prognostication.%2520Here%252C%2520we%2520present%2520a%2520method%2520for%2520recovering%2520truncated%2520CT%250Aslices%2520using%2520generative%2520image%2520outpainting.%2520We%2520train%2520a%2520diffusion%2520model%2520and%2520apply%250Ait%2520to%2520truncated%2520CT%2520slices%2520generated%2520by%2520simulating%2520a%2520small%2520FOV.%2520Our%2520model%250Areliably%2520recovers%2520the%2520truncated%2520anatomy%2520and%2520outperforms%2520the%2520previous%250Astate-of-the-art%2520despite%2520being%2520trained%2520on%252087%2525%2520less%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Generative%20Image%20Outpainting%20for%20Recovery%20of%0A%20%20FOV-Truncated%20CT%20Images&entry.906535625=Michelle%20Espranita%20Liman%20and%20Daniel%20Rueckert%20and%20Florian%20J.%20Fintelmann%20and%20Philip%20M%C3%BCller&entry.1292438233=%20%20Field-of-view%20%28FOV%29%20recovery%20of%20truncated%20chest%20CT%20scans%20is%20crucial%20for%0Aaccurate%20body%20composition%20analysis%2C%20which%20involves%20quantifying%20skeletal%20muscle%0Aand%20subcutaneous%20adipose%20tissue%20%28SAT%29%20on%20CT%20slices.%20This%2C%20in%20turn%2C%20enables%0Adisease%20prognostication.%20Here%2C%20we%20present%20a%20method%20for%20recovering%20truncated%20CT%0Aslices%20using%20generative%20image%20outpainting.%20We%20train%20a%20diffusion%20model%20and%20apply%0Ait%20to%20truncated%20CT%20slices%20generated%20by%20simulating%20a%20small%20FOV.%20Our%20model%0Areliably%20recovers%20the%20truncated%20anatomy%20and%20outperforms%20the%20previous%0Astate-of-the-art%20despite%20being%20trained%20on%2087%25%20less%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04769v2&entry.124074799=Read"},
{"title": "Joint Localization and Planning using Diffusion", "author": "L. Lao Beyer and S. Karaman", "abstract": "  Diffusion models have been successfully applied to robotics problems such as\nmanipulation and vehicle path planning. In this work, we explore their\napplication to end-to-end navigation -- including both perception and planning\n-- by considering the problem of jointly performing global localization and\npath planning in known but arbitrary 2D environments. In particular, we\nintroduce a diffusion model which produces collision-free paths in a global\nreference frame given an egocentric LIDAR scan, an arbitrary map, and a desired\ngoal position. To this end, we implement diffusion in the space of paths in\nSE(2), and describe how to condition the denoising process on both obstacles\nand sensor observations. In our evaluation, we show that the proposed\nconditioning techniques enable generalization to realistic maps of considerably\ndifferent appearance than the training environment, demonstrate our model's\nability to accurately describe ambiguous solutions, and run extensive\nsimulation experiments showcasing our model's use as a real-time, end-to-end\nlocalization and planning stack.\n", "link": "http://arxiv.org/abs/2409.17995v1", "date": "2024-09-26", "relevancy": 2.3053, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5836}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5753}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Localization%20and%20Planning%20using%20Diffusion&body=Title%3A%20Joint%20Localization%20and%20Planning%20using%20Diffusion%0AAuthor%3A%20L.%20Lao%20Beyer%20and%20S.%20Karaman%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20successfully%20applied%20to%20robotics%20problems%20such%20as%0Amanipulation%20and%20vehicle%20path%20planning.%20In%20this%20work%2C%20we%20explore%20their%0Aapplication%20to%20end-to-end%20navigation%20--%20including%20both%20perception%20and%20planning%0A--%20by%20considering%20the%20problem%20of%20jointly%20performing%20global%20localization%20and%0Apath%20planning%20in%20known%20but%20arbitrary%202D%20environments.%20In%20particular%2C%20we%0Aintroduce%20a%20diffusion%20model%20which%20produces%20collision-free%20paths%20in%20a%20global%0Areference%20frame%20given%20an%20egocentric%20LIDAR%20scan%2C%20an%20arbitrary%20map%2C%20and%20a%20desired%0Agoal%20position.%20To%20this%20end%2C%20we%20implement%20diffusion%20in%20the%20space%20of%20paths%20in%0ASE%282%29%2C%20and%20describe%20how%20to%20condition%20the%20denoising%20process%20on%20both%20obstacles%0Aand%20sensor%20observations.%20In%20our%20evaluation%2C%20we%20show%20that%20the%20proposed%0Aconditioning%20techniques%20enable%20generalization%20to%20realistic%20maps%20of%20considerably%0Adifferent%20appearance%20than%20the%20training%20environment%2C%20demonstrate%20our%20model%27s%0Aability%20to%20accurately%20describe%20ambiguous%20solutions%2C%20and%20run%20extensive%0Asimulation%20experiments%20showcasing%20our%20model%27s%20use%20as%20a%20real-time%2C%20end-to-end%0Alocalization%20and%20planning%20stack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Localization%2520and%2520Planning%2520using%2520Diffusion%26entry.906535625%3DL.%2520Lao%2520Beyer%2520and%2520S.%2520Karaman%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520been%2520successfully%2520applied%2520to%2520robotics%2520problems%2520such%2520as%250Amanipulation%2520and%2520vehicle%2520path%2520planning.%2520In%2520this%2520work%252C%2520we%2520explore%2520their%250Aapplication%2520to%2520end-to-end%2520navigation%2520--%2520including%2520both%2520perception%2520and%2520planning%250A--%2520by%2520considering%2520the%2520problem%2520of%2520jointly%2520performing%2520global%2520localization%2520and%250Apath%2520planning%2520in%2520known%2520but%2520arbitrary%25202D%2520environments.%2520In%2520particular%252C%2520we%250Aintroduce%2520a%2520diffusion%2520model%2520which%2520produces%2520collision-free%2520paths%2520in%2520a%2520global%250Areference%2520frame%2520given%2520an%2520egocentric%2520LIDAR%2520scan%252C%2520an%2520arbitrary%2520map%252C%2520and%2520a%2520desired%250Agoal%2520position.%2520To%2520this%2520end%252C%2520we%2520implement%2520diffusion%2520in%2520the%2520space%2520of%2520paths%2520in%250ASE%25282%2529%252C%2520and%2520describe%2520how%2520to%2520condition%2520the%2520denoising%2520process%2520on%2520both%2520obstacles%250Aand%2520sensor%2520observations.%2520In%2520our%2520evaluation%252C%2520we%2520show%2520that%2520the%2520proposed%250Aconditioning%2520techniques%2520enable%2520generalization%2520to%2520realistic%2520maps%2520of%2520considerably%250Adifferent%2520appearance%2520than%2520the%2520training%2520environment%252C%2520demonstrate%2520our%2520model%2527s%250Aability%2520to%2520accurately%2520describe%2520ambiguous%2520solutions%252C%2520and%2520run%2520extensive%250Asimulation%2520experiments%2520showcasing%2520our%2520model%2527s%2520use%2520as%2520a%2520real-time%252C%2520end-to-end%250Alocalization%2520and%2520planning%2520stack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Localization%20and%20Planning%20using%20Diffusion&entry.906535625=L.%20Lao%20Beyer%20and%20S.%20Karaman&entry.1292438233=%20%20Diffusion%20models%20have%20been%20successfully%20applied%20to%20robotics%20problems%20such%20as%0Amanipulation%20and%20vehicle%20path%20planning.%20In%20this%20work%2C%20we%20explore%20their%0Aapplication%20to%20end-to-end%20navigation%20--%20including%20both%20perception%20and%20planning%0A--%20by%20considering%20the%20problem%20of%20jointly%20performing%20global%20localization%20and%0Apath%20planning%20in%20known%20but%20arbitrary%202D%20environments.%20In%20particular%2C%20we%0Aintroduce%20a%20diffusion%20model%20which%20produces%20collision-free%20paths%20in%20a%20global%0Areference%20frame%20given%20an%20egocentric%20LIDAR%20scan%2C%20an%20arbitrary%20map%2C%20and%20a%20desired%0Agoal%20position.%20To%20this%20end%2C%20we%20implement%20diffusion%20in%20the%20space%20of%20paths%20in%0ASE%282%29%2C%20and%20describe%20how%20to%20condition%20the%20denoising%20process%20on%20both%20obstacles%0Aand%20sensor%20observations.%20In%20our%20evaluation%2C%20we%20show%20that%20the%20proposed%0Aconditioning%20techniques%20enable%20generalization%20to%20realistic%20maps%20of%20considerably%0Adifferent%20appearance%20than%20the%20training%20environment%2C%20demonstrate%20our%20model%27s%0Aability%20to%20accurately%20describe%20ambiguous%20solutions%2C%20and%20run%20extensive%0Asimulation%20experiments%20showcasing%20our%20model%27s%20use%20as%20a%20real-time%2C%20end-to-end%0Alocalization%20and%20planning%20stack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17995v1&entry.124074799=Read"},
{"title": "Self-supervised Monocular Depth Estimation with Large Kernel Attention", "author": "Xuezhi Xiang and Yao Wang and Lei Zhang and Denis Ombati and Himaloy Himu and Xiantong Zhen", "abstract": "  Self-supervised monocular depth estimation has emerged as a promising\napproach since it does not rely on labeled training data. Most methods combine\nconvolution and Transformer to model long-distance dependencies to estimate\ndepth accurately. However, Transformer treats 2D image features as 1D\nsequences, and positional encoding somewhat mitigates the loss of spatial\ninformation between different feature blocks, tending to overlook channel\nfeatures, which limit the performance of depth estimation. In this paper, we\npropose a self-supervised monocular depth estimation network to get finer\ndetails. Specifically, we propose a decoder based on large kernel attention,\nwhich can model long-distance dependencies without compromising the\ntwo-dimension structure of features while maintaining feature channel\nadaptivity. In addition, we introduce a up-sampling module to accurately\nrecover the fine details in the depth map. Our method achieves competitive\nresults on the KITTI dataset.\n", "link": "http://arxiv.org/abs/2409.17895v1", "date": "2024-09-26", "relevancy": 2.3051, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5807}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5764}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Monocular%20Depth%20Estimation%20with%20Large%20Kernel%20Attention&body=Title%3A%20Self-supervised%20Monocular%20Depth%20Estimation%20with%20Large%20Kernel%20Attention%0AAuthor%3A%20Xuezhi%20Xiang%20and%20Yao%20Wang%20and%20Lei%20Zhang%20and%20Denis%20Ombati%20and%20Himaloy%20Himu%20and%20Xiantong%20Zhen%0AAbstract%3A%20%20%20Self-supervised%20monocular%20depth%20estimation%20has%20emerged%20as%20a%20promising%0Aapproach%20since%20it%20does%20not%20rely%20on%20labeled%20training%20data.%20Most%20methods%20combine%0Aconvolution%20and%20Transformer%20to%20model%20long-distance%20dependencies%20to%20estimate%0Adepth%20accurately.%20However%2C%20Transformer%20treats%202D%20image%20features%20as%201D%0Asequences%2C%20and%20positional%20encoding%20somewhat%20mitigates%20the%20loss%20of%20spatial%0Ainformation%20between%20different%20feature%20blocks%2C%20tending%20to%20overlook%20channel%0Afeatures%2C%20which%20limit%20the%20performance%20of%20depth%20estimation.%20In%20this%20paper%2C%20we%0Apropose%20a%20self-supervised%20monocular%20depth%20estimation%20network%20to%20get%20finer%0Adetails.%20Specifically%2C%20we%20propose%20a%20decoder%20based%20on%20large%20kernel%20attention%2C%0Awhich%20can%20model%20long-distance%20dependencies%20without%20compromising%20the%0Atwo-dimension%20structure%20of%20features%20while%20maintaining%20feature%20channel%0Aadaptivity.%20In%20addition%2C%20we%20introduce%20a%20up-sampling%20module%20to%20accurately%0Arecover%20the%20fine%20details%20in%20the%20depth%20map.%20Our%20method%20achieves%20competitive%0Aresults%20on%20the%20KITTI%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Monocular%2520Depth%2520Estimation%2520with%2520Large%2520Kernel%2520Attention%26entry.906535625%3DXuezhi%2520Xiang%2520and%2520Yao%2520Wang%2520and%2520Lei%2520Zhang%2520and%2520Denis%2520Ombati%2520and%2520Himaloy%2520Himu%2520and%2520Xiantong%2520Zhen%26entry.1292438233%3D%2520%2520Self-supervised%2520monocular%2520depth%2520estimation%2520has%2520emerged%2520as%2520a%2520promising%250Aapproach%2520since%2520it%2520does%2520not%2520rely%2520on%2520labeled%2520training%2520data.%2520Most%2520methods%2520combine%250Aconvolution%2520and%2520Transformer%2520to%2520model%2520long-distance%2520dependencies%2520to%2520estimate%250Adepth%2520accurately.%2520However%252C%2520Transformer%2520treats%25202D%2520image%2520features%2520as%25201D%250Asequences%252C%2520and%2520positional%2520encoding%2520somewhat%2520mitigates%2520the%2520loss%2520of%2520spatial%250Ainformation%2520between%2520different%2520feature%2520blocks%252C%2520tending%2520to%2520overlook%2520channel%250Afeatures%252C%2520which%2520limit%2520the%2520performance%2520of%2520depth%2520estimation.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520self-supervised%2520monocular%2520depth%2520estimation%2520network%2520to%2520get%2520finer%250Adetails.%2520Specifically%252C%2520we%2520propose%2520a%2520decoder%2520based%2520on%2520large%2520kernel%2520attention%252C%250Awhich%2520can%2520model%2520long-distance%2520dependencies%2520without%2520compromising%2520the%250Atwo-dimension%2520structure%2520of%2520features%2520while%2520maintaining%2520feature%2520channel%250Aadaptivity.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520up-sampling%2520module%2520to%2520accurately%250Arecover%2520the%2520fine%2520details%2520in%2520the%2520depth%2520map.%2520Our%2520method%2520achieves%2520competitive%250Aresults%2520on%2520the%2520KITTI%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Monocular%20Depth%20Estimation%20with%20Large%20Kernel%20Attention&entry.906535625=Xuezhi%20Xiang%20and%20Yao%20Wang%20and%20Lei%20Zhang%20and%20Denis%20Ombati%20and%20Himaloy%20Himu%20and%20Xiantong%20Zhen&entry.1292438233=%20%20Self-supervised%20monocular%20depth%20estimation%20has%20emerged%20as%20a%20promising%0Aapproach%20since%20it%20does%20not%20rely%20on%20labeled%20training%20data.%20Most%20methods%20combine%0Aconvolution%20and%20Transformer%20to%20model%20long-distance%20dependencies%20to%20estimate%0Adepth%20accurately.%20However%2C%20Transformer%20treats%202D%20image%20features%20as%201D%0Asequences%2C%20and%20positional%20encoding%20somewhat%20mitigates%20the%20loss%20of%20spatial%0Ainformation%20between%20different%20feature%20blocks%2C%20tending%20to%20overlook%20channel%0Afeatures%2C%20which%20limit%20the%20performance%20of%20depth%20estimation.%20In%20this%20paper%2C%20we%0Apropose%20a%20self-supervised%20monocular%20depth%20estimation%20network%20to%20get%20finer%0Adetails.%20Specifically%2C%20we%20propose%20a%20decoder%20based%20on%20large%20kernel%20attention%2C%0Awhich%20can%20model%20long-distance%20dependencies%20without%20compromising%20the%0Atwo-dimension%20structure%20of%20features%20while%20maintaining%20feature%20channel%0Aadaptivity.%20In%20addition%2C%20we%20introduce%20a%20up-sampling%20module%20to%20accurately%0Arecover%20the%20fine%20details%20in%20the%20depth%20map.%20Our%20method%20achieves%20competitive%0Aresults%20on%20the%20KITTI%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17895v1&entry.124074799=Read"},
{"title": "Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or\n  Low-light Conditions", "author": "Weng Fei Low and Gim Hee Lee", "abstract": "  The stark contrast in the design philosophy of an event camera makes it\nparticularly ideal for operating under high-speed, high dynamic range and\nlow-light conditions, where standard cameras underperform. Nonetheless, event\ncameras still suffer from some amount of motion blur, especially under these\nchallenging conditions, in contrary to what most think. This is attributed to\nthe limited bandwidth of the event sensor pixel, which is mostly proportional\nto the light intensity. Thus, to ensure that event cameras can truly excel in\nsuch conditions where it has an edge over standard cameras, it is crucial to\naccount for event motion blur in downstream applications, especially\nreconstruction. However, none of the recent works on reconstructing Neural\nRadiance Fields (NeRFs) from events, nor event simulators, have considered the\nfull effects of event motion blur. To this end, we propose, Deblur e-NeRF, a\nnovel method to directly and effectively reconstruct blur-minimal NeRFs from\nmotion-blurred events generated under high-speed motion or low-light\nconditions. The core component of this work is a physically-accurate pixel\nbandwidth model proposed to account for event motion blur under arbitrary speed\nand lighting conditions. We also introduce a novel threshold-normalized total\nvariation loss to improve the regularization of large textureless patches.\nExperiments on real and novel realistically simulated sequences verify our\neffectiveness. Our code, event simulator and synthetic event dataset will be\nopen-sourced.\n", "link": "http://arxiv.org/abs/2409.17988v1", "date": "2024-09-26", "relevancy": 2.2878, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5841}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5735}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deblur%20e-NeRF%3A%20NeRF%20from%20Motion-Blurred%20Events%20under%20High-speed%20or%0A%20%20Low-light%20Conditions&body=Title%3A%20Deblur%20e-NeRF%3A%20NeRF%20from%20Motion-Blurred%20Events%20under%20High-speed%20or%0A%20%20Low-light%20Conditions%0AAuthor%3A%20Weng%20Fei%20Low%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20The%20stark%20contrast%20in%20the%20design%20philosophy%20of%20an%20event%20camera%20makes%20it%0Aparticularly%20ideal%20for%20operating%20under%20high-speed%2C%20high%20dynamic%20range%20and%0Alow-light%20conditions%2C%20where%20standard%20cameras%20underperform.%20Nonetheless%2C%20event%0Acameras%20still%20suffer%20from%20some%20amount%20of%20motion%20blur%2C%20especially%20under%20these%0Achallenging%20conditions%2C%20in%20contrary%20to%20what%20most%20think.%20This%20is%20attributed%20to%0Athe%20limited%20bandwidth%20of%20the%20event%20sensor%20pixel%2C%20which%20is%20mostly%20proportional%0Ato%20the%20light%20intensity.%20Thus%2C%20to%20ensure%20that%20event%20cameras%20can%20truly%20excel%20in%0Asuch%20conditions%20where%20it%20has%20an%20edge%20over%20standard%20cameras%2C%20it%20is%20crucial%20to%0Aaccount%20for%20event%20motion%20blur%20in%20downstream%20applications%2C%20especially%0Areconstruction.%20However%2C%20none%20of%20the%20recent%20works%20on%20reconstructing%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20from%20events%2C%20nor%20event%20simulators%2C%20have%20considered%20the%0Afull%20effects%20of%20event%20motion%20blur.%20To%20this%20end%2C%20we%20propose%2C%20Deblur%20e-NeRF%2C%20a%0Anovel%20method%20to%20directly%20and%20effectively%20reconstruct%20blur-minimal%20NeRFs%20from%0Amotion-blurred%20events%20generated%20under%20high-speed%20motion%20or%20low-light%0Aconditions.%20The%20core%20component%20of%20this%20work%20is%20a%20physically-accurate%20pixel%0Abandwidth%20model%20proposed%20to%20account%20for%20event%20motion%20blur%20under%20arbitrary%20speed%0Aand%20lighting%20conditions.%20We%20also%20introduce%20a%20novel%20threshold-normalized%20total%0Avariation%20loss%20to%20improve%20the%20regularization%20of%20large%20textureless%20patches.%0AExperiments%20on%20real%20and%20novel%20realistically%20simulated%20sequences%20verify%20our%0Aeffectiveness.%20Our%20code%2C%20event%20simulator%20and%20synthetic%20event%20dataset%20will%20be%0Aopen-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeblur%2520e-NeRF%253A%2520NeRF%2520from%2520Motion-Blurred%2520Events%2520under%2520High-speed%2520or%250A%2520%2520Low-light%2520Conditions%26entry.906535625%3DWeng%2520Fei%2520Low%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520The%2520stark%2520contrast%2520in%2520the%2520design%2520philosophy%2520of%2520an%2520event%2520camera%2520makes%2520it%250Aparticularly%2520ideal%2520for%2520operating%2520under%2520high-speed%252C%2520high%2520dynamic%2520range%2520and%250Alow-light%2520conditions%252C%2520where%2520standard%2520cameras%2520underperform.%2520Nonetheless%252C%2520event%250Acameras%2520still%2520suffer%2520from%2520some%2520amount%2520of%2520motion%2520blur%252C%2520especially%2520under%2520these%250Achallenging%2520conditions%252C%2520in%2520contrary%2520to%2520what%2520most%2520think.%2520This%2520is%2520attributed%2520to%250Athe%2520limited%2520bandwidth%2520of%2520the%2520event%2520sensor%2520pixel%252C%2520which%2520is%2520mostly%2520proportional%250Ato%2520the%2520light%2520intensity.%2520Thus%252C%2520to%2520ensure%2520that%2520event%2520cameras%2520can%2520truly%2520excel%2520in%250Asuch%2520conditions%2520where%2520it%2520has%2520an%2520edge%2520over%2520standard%2520cameras%252C%2520it%2520is%2520crucial%2520to%250Aaccount%2520for%2520event%2520motion%2520blur%2520in%2520downstream%2520applications%252C%2520especially%250Areconstruction.%2520However%252C%2520none%2520of%2520the%2520recent%2520works%2520on%2520reconstructing%2520Neural%250ARadiance%2520Fields%2520%2528NeRFs%2529%2520from%2520events%252C%2520nor%2520event%2520simulators%252C%2520have%2520considered%2520the%250Afull%2520effects%2520of%2520event%2520motion%2520blur.%2520To%2520this%2520end%252C%2520we%2520propose%252C%2520Deblur%2520e-NeRF%252C%2520a%250Anovel%2520method%2520to%2520directly%2520and%2520effectively%2520reconstruct%2520blur-minimal%2520NeRFs%2520from%250Amotion-blurred%2520events%2520generated%2520under%2520high-speed%2520motion%2520or%2520low-light%250Aconditions.%2520The%2520core%2520component%2520of%2520this%2520work%2520is%2520a%2520physically-accurate%2520pixel%250Abandwidth%2520model%2520proposed%2520to%2520account%2520for%2520event%2520motion%2520blur%2520under%2520arbitrary%2520speed%250Aand%2520lighting%2520conditions.%2520We%2520also%2520introduce%2520a%2520novel%2520threshold-normalized%2520total%250Avariation%2520loss%2520to%2520improve%2520the%2520regularization%2520of%2520large%2520textureless%2520patches.%250AExperiments%2520on%2520real%2520and%2520novel%2520realistically%2520simulated%2520sequences%2520verify%2520our%250Aeffectiveness.%2520Our%2520code%252C%2520event%2520simulator%2520and%2520synthetic%2520event%2520dataset%2520will%2520be%250Aopen-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deblur%20e-NeRF%3A%20NeRF%20from%20Motion-Blurred%20Events%20under%20High-speed%20or%0A%20%20Low-light%20Conditions&entry.906535625=Weng%20Fei%20Low%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20The%20stark%20contrast%20in%20the%20design%20philosophy%20of%20an%20event%20camera%20makes%20it%0Aparticularly%20ideal%20for%20operating%20under%20high-speed%2C%20high%20dynamic%20range%20and%0Alow-light%20conditions%2C%20where%20standard%20cameras%20underperform.%20Nonetheless%2C%20event%0Acameras%20still%20suffer%20from%20some%20amount%20of%20motion%20blur%2C%20especially%20under%20these%0Achallenging%20conditions%2C%20in%20contrary%20to%20what%20most%20think.%20This%20is%20attributed%20to%0Athe%20limited%20bandwidth%20of%20the%20event%20sensor%20pixel%2C%20which%20is%20mostly%20proportional%0Ato%20the%20light%20intensity.%20Thus%2C%20to%20ensure%20that%20event%20cameras%20can%20truly%20excel%20in%0Asuch%20conditions%20where%20it%20has%20an%20edge%20over%20standard%20cameras%2C%20it%20is%20crucial%20to%0Aaccount%20for%20event%20motion%20blur%20in%20downstream%20applications%2C%20especially%0Areconstruction.%20However%2C%20none%20of%20the%20recent%20works%20on%20reconstructing%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20from%20events%2C%20nor%20event%20simulators%2C%20have%20considered%20the%0Afull%20effects%20of%20event%20motion%20blur.%20To%20this%20end%2C%20we%20propose%2C%20Deblur%20e-NeRF%2C%20a%0Anovel%20method%20to%20directly%20and%20effectively%20reconstruct%20blur-minimal%20NeRFs%20from%0Amotion-blurred%20events%20generated%20under%20high-speed%20motion%20or%20low-light%0Aconditions.%20The%20core%20component%20of%20this%20work%20is%20a%20physically-accurate%20pixel%0Abandwidth%20model%20proposed%20to%20account%20for%20event%20motion%20blur%20under%20arbitrary%20speed%0Aand%20lighting%20conditions.%20We%20also%20introduce%20a%20novel%20threshold-normalized%20total%0Avariation%20loss%20to%20improve%20the%20regularization%20of%20large%20textureless%20patches.%0AExperiments%20on%20real%20and%20novel%20realistically%20simulated%20sequences%20verify%20our%0Aeffectiveness.%20Our%20code%2C%20event%20simulator%20and%20synthetic%20event%20dataset%20will%20be%0Aopen-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17988v1&entry.124074799=Read"},
{"title": "The application of GPT-4 in grading design university students'\n  assignment and providing feedback: An exploratory study", "author": "Qian Huang and Thijs Willems and King Wang Poon", "abstract": "  This study aims to investigate whether GPT-4 can effectively grade\nassignments for design university students and provide useful feedback. In\ndesign education, assignments do not have a single correct answer and often\ninvolve solving an open-ended design problem. This subjective nature of design\nprojects often leads to grading problems,as grades can vary between different\nraters,for instance instructor from engineering background or architecture\nbackground. This study employs an iterative research approach in developing a\nCustom GPT with the aim of achieving more reliable results and testing whether\nit can provide design students with constructive feedback. The findings\ninclude: First,through several rounds of iterations the inter-reliability\nbetween GPT and human raters reached a level that is generally accepted by\neducators. This indicates that by providing accurate prompts to GPT,and\ncontinuously iterating to build a Custom GPT, it can be used to effectively\ngrade students' design assignments, serving as a reliable complement to human\nraters. Second, the intra-reliability of GPT's scoring at different times is\nbetween 0.65 and 0.78. This indicates that, with adequate instructions, a\nCustom GPT gives consistent results which is a precondition for grading\nstudents. As consistency and comparability are the two main rules to ensure the\nreliability of educational assessment, this study has looked at whether a\nCustom GPT can be developed that adheres to these two rules. We finish the\npaper by testing whether Custom GPT can provide students with useful feedback\nand reflecting on how educators can develop and iterate a Custom GPT to serve\nas a complementary rater.\n", "link": "http://arxiv.org/abs/2409.17698v1", "date": "2024-09-26", "relevancy": 2.2776, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5042}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4467}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20application%20of%20GPT-4%20in%20grading%20design%20university%20students%27%0A%20%20assignment%20and%20providing%20feedback%3A%20An%20exploratory%20study&body=Title%3A%20The%20application%20of%20GPT-4%20in%20grading%20design%20university%20students%27%0A%20%20assignment%20and%20providing%20feedback%3A%20An%20exploratory%20study%0AAuthor%3A%20Qian%20Huang%20and%20Thijs%20Willems%20and%20King%20Wang%20Poon%0AAbstract%3A%20%20%20This%20study%20aims%20to%20investigate%20whether%20GPT-4%20can%20effectively%20grade%0Aassignments%20for%20design%20university%20students%20and%20provide%20useful%20feedback.%20In%0Adesign%20education%2C%20assignments%20do%20not%20have%20a%20single%20correct%20answer%20and%20often%0Ainvolve%20solving%20an%20open-ended%20design%20problem.%20This%20subjective%20nature%20of%20design%0Aprojects%20often%20leads%20to%20grading%20problems%2Cas%20grades%20can%20vary%20between%20different%0Araters%2Cfor%20instance%20instructor%20from%20engineering%20background%20or%20architecture%0Abackground.%20This%20study%20employs%20an%20iterative%20research%20approach%20in%20developing%20a%0ACustom%20GPT%20with%20the%20aim%20of%20achieving%20more%20reliable%20results%20and%20testing%20whether%0Ait%20can%20provide%20design%20students%20with%20constructive%20feedback.%20The%20findings%0Ainclude%3A%20First%2Cthrough%20several%20rounds%20of%20iterations%20the%20inter-reliability%0Abetween%20GPT%20and%20human%20raters%20reached%20a%20level%20that%20is%20generally%20accepted%20by%0Aeducators.%20This%20indicates%20that%20by%20providing%20accurate%20prompts%20to%20GPT%2Cand%0Acontinuously%20iterating%20to%20build%20a%20Custom%20GPT%2C%20it%20can%20be%20used%20to%20effectively%0Agrade%20students%27%20design%20assignments%2C%20serving%20as%20a%20reliable%20complement%20to%20human%0Araters.%20Second%2C%20the%20intra-reliability%20of%20GPT%27s%20scoring%20at%20different%20times%20is%0Abetween%200.65%20and%200.78.%20This%20indicates%20that%2C%20with%20adequate%20instructions%2C%20a%0ACustom%20GPT%20gives%20consistent%20results%20which%20is%20a%20precondition%20for%20grading%0Astudents.%20As%20consistency%20and%20comparability%20are%20the%20two%20main%20rules%20to%20ensure%20the%0Areliability%20of%20educational%20assessment%2C%20this%20study%20has%20looked%20at%20whether%20a%0ACustom%20GPT%20can%20be%20developed%20that%20adheres%20to%20these%20two%20rules.%20We%20finish%20the%0Apaper%20by%20testing%20whether%20Custom%20GPT%20can%20provide%20students%20with%20useful%20feedback%0Aand%20reflecting%20on%20how%20educators%20can%20develop%20and%20iterate%20a%20Custom%20GPT%20to%20serve%0Aas%20a%20complementary%20rater.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520application%2520of%2520GPT-4%2520in%2520grading%2520design%2520university%2520students%2527%250A%2520%2520assignment%2520and%2520providing%2520feedback%253A%2520An%2520exploratory%2520study%26entry.906535625%3DQian%2520Huang%2520and%2520Thijs%2520Willems%2520and%2520King%2520Wang%2520Poon%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520investigate%2520whether%2520GPT-4%2520can%2520effectively%2520grade%250Aassignments%2520for%2520design%2520university%2520students%2520and%2520provide%2520useful%2520feedback.%2520In%250Adesign%2520education%252C%2520assignments%2520do%2520not%2520have%2520a%2520single%2520correct%2520answer%2520and%2520often%250Ainvolve%2520solving%2520an%2520open-ended%2520design%2520problem.%2520This%2520subjective%2520nature%2520of%2520design%250Aprojects%2520often%2520leads%2520to%2520grading%2520problems%252Cas%2520grades%2520can%2520vary%2520between%2520different%250Araters%252Cfor%2520instance%2520instructor%2520from%2520engineering%2520background%2520or%2520architecture%250Abackground.%2520This%2520study%2520employs%2520an%2520iterative%2520research%2520approach%2520in%2520developing%2520a%250ACustom%2520GPT%2520with%2520the%2520aim%2520of%2520achieving%2520more%2520reliable%2520results%2520and%2520testing%2520whether%250Ait%2520can%2520provide%2520design%2520students%2520with%2520constructive%2520feedback.%2520The%2520findings%250Ainclude%253A%2520First%252Cthrough%2520several%2520rounds%2520of%2520iterations%2520the%2520inter-reliability%250Abetween%2520GPT%2520and%2520human%2520raters%2520reached%2520a%2520level%2520that%2520is%2520generally%2520accepted%2520by%250Aeducators.%2520This%2520indicates%2520that%2520by%2520providing%2520accurate%2520prompts%2520to%2520GPT%252Cand%250Acontinuously%2520iterating%2520to%2520build%2520a%2520Custom%2520GPT%252C%2520it%2520can%2520be%2520used%2520to%2520effectively%250Agrade%2520students%2527%2520design%2520assignments%252C%2520serving%2520as%2520a%2520reliable%2520complement%2520to%2520human%250Araters.%2520Second%252C%2520the%2520intra-reliability%2520of%2520GPT%2527s%2520scoring%2520at%2520different%2520times%2520is%250Abetween%25200.65%2520and%25200.78.%2520This%2520indicates%2520that%252C%2520with%2520adequate%2520instructions%252C%2520a%250ACustom%2520GPT%2520gives%2520consistent%2520results%2520which%2520is%2520a%2520precondition%2520for%2520grading%250Astudents.%2520As%2520consistency%2520and%2520comparability%2520are%2520the%2520two%2520main%2520rules%2520to%2520ensure%2520the%250Areliability%2520of%2520educational%2520assessment%252C%2520this%2520study%2520has%2520looked%2520at%2520whether%2520a%250ACustom%2520GPT%2520can%2520be%2520developed%2520that%2520adheres%2520to%2520these%2520two%2520rules.%2520We%2520finish%2520the%250Apaper%2520by%2520testing%2520whether%2520Custom%2520GPT%2520can%2520provide%2520students%2520with%2520useful%2520feedback%250Aand%2520reflecting%2520on%2520how%2520educators%2520can%2520develop%2520and%2520iterate%2520a%2520Custom%2520GPT%2520to%2520serve%250Aas%2520a%2520complementary%2520rater.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20application%20of%20GPT-4%20in%20grading%20design%20university%20students%27%0A%20%20assignment%20and%20providing%20feedback%3A%20An%20exploratory%20study&entry.906535625=Qian%20Huang%20and%20Thijs%20Willems%20and%20King%20Wang%20Poon&entry.1292438233=%20%20This%20study%20aims%20to%20investigate%20whether%20GPT-4%20can%20effectively%20grade%0Aassignments%20for%20design%20university%20students%20and%20provide%20useful%20feedback.%20In%0Adesign%20education%2C%20assignments%20do%20not%20have%20a%20single%20correct%20answer%20and%20often%0Ainvolve%20solving%20an%20open-ended%20design%20problem.%20This%20subjective%20nature%20of%20design%0Aprojects%20often%20leads%20to%20grading%20problems%2Cas%20grades%20can%20vary%20between%20different%0Araters%2Cfor%20instance%20instructor%20from%20engineering%20background%20or%20architecture%0Abackground.%20This%20study%20employs%20an%20iterative%20research%20approach%20in%20developing%20a%0ACustom%20GPT%20with%20the%20aim%20of%20achieving%20more%20reliable%20results%20and%20testing%20whether%0Ait%20can%20provide%20design%20students%20with%20constructive%20feedback.%20The%20findings%0Ainclude%3A%20First%2Cthrough%20several%20rounds%20of%20iterations%20the%20inter-reliability%0Abetween%20GPT%20and%20human%20raters%20reached%20a%20level%20that%20is%20generally%20accepted%20by%0Aeducators.%20This%20indicates%20that%20by%20providing%20accurate%20prompts%20to%20GPT%2Cand%0Acontinuously%20iterating%20to%20build%20a%20Custom%20GPT%2C%20it%20can%20be%20used%20to%20effectively%0Agrade%20students%27%20design%20assignments%2C%20serving%20as%20a%20reliable%20complement%20to%20human%0Araters.%20Second%2C%20the%20intra-reliability%20of%20GPT%27s%20scoring%20at%20different%20times%20is%0Abetween%200.65%20and%200.78.%20This%20indicates%20that%2C%20with%20adequate%20instructions%2C%20a%0ACustom%20GPT%20gives%20consistent%20results%20which%20is%20a%20precondition%20for%20grading%0Astudents.%20As%20consistency%20and%20comparability%20are%20the%20two%20main%20rules%20to%20ensure%20the%0Areliability%20of%20educational%20assessment%2C%20this%20study%20has%20looked%20at%20whether%20a%0ACustom%20GPT%20can%20be%20developed%20that%20adheres%20to%20these%20two%20rules.%20We%20finish%20the%0Apaper%20by%20testing%20whether%20Custom%20GPT%20can%20provide%20students%20with%20useful%20feedback%0Aand%20reflecting%20on%20how%20educators%20can%20develop%20and%20iterate%20a%20Custom%20GPT%20to%20serve%0Aas%20a%20complementary%20rater.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17698v1&entry.124074799=Read"},
{"title": "Self-Distilled Depth Refinement with Noisy Poisson Fusion", "author": "Jiaqi Li and Yiran Wang and Jinghong Zheng and Zihao Huang and Ke Xian and Zhiguo Cao and Jianming Zhang", "abstract": "  Depth refinement aims to infer high-resolution depth with fine-grained edges\nand details, refining low-resolution results of depth estimation models. The\nprevailing methods adopt tile-based manners by merging numerous patches, which\nlacks efficiency and produces inconsistency. Besides, prior arts suffer from\nfuzzy depth boundaries and limited generalizability. Analyzing the fundamental\nreasons for these limitations, we model depth refinement as a noisy Poisson\nfusion problem with local inconsistency and edge deformation noises. We propose\nthe Self-distilled Depth Refinement (SDDR) framework to enforce robustness\nagainst the noises, which mainly consists of depth edge representation and\nedge-based guidance. With noisy depth predictions as input, SDDR generates\nlow-noise depth edge representations as pseudo-labels by coarse-to-fine\nself-distillation. Edge-based guidance with edge-guided gradient loss and\nedge-based fusion loss serves as the optimization objective equivalent to\nPoisson fusion. When depth maps are better refined, the labels also become more\nnoise-free. Our model can acquire strong robustness to the noises, achieving\nsignificant improvements in accuracy, edge quality, efficiency, and\ngeneralizability on five different benchmarks. Moreover, directly training\nanother model with edge labels produced by SDDR brings improvements, suggesting\nthat our method could help with training robust refinement models in future\nworks.\n", "link": "http://arxiv.org/abs/2409.17880v1", "date": "2024-09-26", "relevancy": 2.275, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5748}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5655}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Distilled%20Depth%20Refinement%20with%20Noisy%20Poisson%20Fusion&body=Title%3A%20Self-Distilled%20Depth%20Refinement%20with%20Noisy%20Poisson%20Fusion%0AAuthor%3A%20Jiaqi%20Li%20and%20Yiran%20Wang%20and%20Jinghong%20Zheng%20and%20Zihao%20Huang%20and%20Ke%20Xian%20and%20Zhiguo%20Cao%20and%20Jianming%20Zhang%0AAbstract%3A%20%20%20Depth%20refinement%20aims%20to%20infer%20high-resolution%20depth%20with%20fine-grained%20edges%0Aand%20details%2C%20refining%20low-resolution%20results%20of%20depth%20estimation%20models.%20The%0Aprevailing%20methods%20adopt%20tile-based%20manners%20by%20merging%20numerous%20patches%2C%20which%0Alacks%20efficiency%20and%20produces%20inconsistency.%20Besides%2C%20prior%20arts%20suffer%20from%0Afuzzy%20depth%20boundaries%20and%20limited%20generalizability.%20Analyzing%20the%20fundamental%0Areasons%20for%20these%20limitations%2C%20we%20model%20depth%20refinement%20as%20a%20noisy%20Poisson%0Afusion%20problem%20with%20local%20inconsistency%20and%20edge%20deformation%20noises.%20We%20propose%0Athe%20Self-distilled%20Depth%20Refinement%20%28SDDR%29%20framework%20to%20enforce%20robustness%0Aagainst%20the%20noises%2C%20which%20mainly%20consists%20of%20depth%20edge%20representation%20and%0Aedge-based%20guidance.%20With%20noisy%20depth%20predictions%20as%20input%2C%20SDDR%20generates%0Alow-noise%20depth%20edge%20representations%20as%20pseudo-labels%20by%20coarse-to-fine%0Aself-distillation.%20Edge-based%20guidance%20with%20edge-guided%20gradient%20loss%20and%0Aedge-based%20fusion%20loss%20serves%20as%20the%20optimization%20objective%20equivalent%20to%0APoisson%20fusion.%20When%20depth%20maps%20are%20better%20refined%2C%20the%20labels%20also%20become%20more%0Anoise-free.%20Our%20model%20can%20acquire%20strong%20robustness%20to%20the%20noises%2C%20achieving%0Asignificant%20improvements%20in%20accuracy%2C%20edge%20quality%2C%20efficiency%2C%20and%0Ageneralizability%20on%20five%20different%20benchmarks.%20Moreover%2C%20directly%20training%0Aanother%20model%20with%20edge%20labels%20produced%20by%20SDDR%20brings%20improvements%2C%20suggesting%0Athat%20our%20method%20could%20help%20with%20training%20robust%20refinement%20models%20in%20future%0Aworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Distilled%2520Depth%2520Refinement%2520with%2520Noisy%2520Poisson%2520Fusion%26entry.906535625%3DJiaqi%2520Li%2520and%2520Yiran%2520Wang%2520and%2520Jinghong%2520Zheng%2520and%2520Zihao%2520Huang%2520and%2520Ke%2520Xian%2520and%2520Zhiguo%2520Cao%2520and%2520Jianming%2520Zhang%26entry.1292438233%3D%2520%2520Depth%2520refinement%2520aims%2520to%2520infer%2520high-resolution%2520depth%2520with%2520fine-grained%2520edges%250Aand%2520details%252C%2520refining%2520low-resolution%2520results%2520of%2520depth%2520estimation%2520models.%2520The%250Aprevailing%2520methods%2520adopt%2520tile-based%2520manners%2520by%2520merging%2520numerous%2520patches%252C%2520which%250Alacks%2520efficiency%2520and%2520produces%2520inconsistency.%2520Besides%252C%2520prior%2520arts%2520suffer%2520from%250Afuzzy%2520depth%2520boundaries%2520and%2520limited%2520generalizability.%2520Analyzing%2520the%2520fundamental%250Areasons%2520for%2520these%2520limitations%252C%2520we%2520model%2520depth%2520refinement%2520as%2520a%2520noisy%2520Poisson%250Afusion%2520problem%2520with%2520local%2520inconsistency%2520and%2520edge%2520deformation%2520noises.%2520We%2520propose%250Athe%2520Self-distilled%2520Depth%2520Refinement%2520%2528SDDR%2529%2520framework%2520to%2520enforce%2520robustness%250Aagainst%2520the%2520noises%252C%2520which%2520mainly%2520consists%2520of%2520depth%2520edge%2520representation%2520and%250Aedge-based%2520guidance.%2520With%2520noisy%2520depth%2520predictions%2520as%2520input%252C%2520SDDR%2520generates%250Alow-noise%2520depth%2520edge%2520representations%2520as%2520pseudo-labels%2520by%2520coarse-to-fine%250Aself-distillation.%2520Edge-based%2520guidance%2520with%2520edge-guided%2520gradient%2520loss%2520and%250Aedge-based%2520fusion%2520loss%2520serves%2520as%2520the%2520optimization%2520objective%2520equivalent%2520to%250APoisson%2520fusion.%2520When%2520depth%2520maps%2520are%2520better%2520refined%252C%2520the%2520labels%2520also%2520become%2520more%250Anoise-free.%2520Our%2520model%2520can%2520acquire%2520strong%2520robustness%2520to%2520the%2520noises%252C%2520achieving%250Asignificant%2520improvements%2520in%2520accuracy%252C%2520edge%2520quality%252C%2520efficiency%252C%2520and%250Ageneralizability%2520on%2520five%2520different%2520benchmarks.%2520Moreover%252C%2520directly%2520training%250Aanother%2520model%2520with%2520edge%2520labels%2520produced%2520by%2520SDDR%2520brings%2520improvements%252C%2520suggesting%250Athat%2520our%2520method%2520could%2520help%2520with%2520training%2520robust%2520refinement%2520models%2520in%2520future%250Aworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Distilled%20Depth%20Refinement%20with%20Noisy%20Poisson%20Fusion&entry.906535625=Jiaqi%20Li%20and%20Yiran%20Wang%20and%20Jinghong%20Zheng%20and%20Zihao%20Huang%20and%20Ke%20Xian%20and%20Zhiguo%20Cao%20and%20Jianming%20Zhang&entry.1292438233=%20%20Depth%20refinement%20aims%20to%20infer%20high-resolution%20depth%20with%20fine-grained%20edges%0Aand%20details%2C%20refining%20low-resolution%20results%20of%20depth%20estimation%20models.%20The%0Aprevailing%20methods%20adopt%20tile-based%20manners%20by%20merging%20numerous%20patches%2C%20which%0Alacks%20efficiency%20and%20produces%20inconsistency.%20Besides%2C%20prior%20arts%20suffer%20from%0Afuzzy%20depth%20boundaries%20and%20limited%20generalizability.%20Analyzing%20the%20fundamental%0Areasons%20for%20these%20limitations%2C%20we%20model%20depth%20refinement%20as%20a%20noisy%20Poisson%0Afusion%20problem%20with%20local%20inconsistency%20and%20edge%20deformation%20noises.%20We%20propose%0Athe%20Self-distilled%20Depth%20Refinement%20%28SDDR%29%20framework%20to%20enforce%20robustness%0Aagainst%20the%20noises%2C%20which%20mainly%20consists%20of%20depth%20edge%20representation%20and%0Aedge-based%20guidance.%20With%20noisy%20depth%20predictions%20as%20input%2C%20SDDR%20generates%0Alow-noise%20depth%20edge%20representations%20as%20pseudo-labels%20by%20coarse-to-fine%0Aself-distillation.%20Edge-based%20guidance%20with%20edge-guided%20gradient%20loss%20and%0Aedge-based%20fusion%20loss%20serves%20as%20the%20optimization%20objective%20equivalent%20to%0APoisson%20fusion.%20When%20depth%20maps%20are%20better%20refined%2C%20the%20labels%20also%20become%20more%0Anoise-free.%20Our%20model%20can%20acquire%20strong%20robustness%20to%20the%20noises%2C%20achieving%0Asignificant%20improvements%20in%20accuracy%2C%20edge%20quality%2C%20efficiency%2C%20and%0Ageneralizability%20on%20five%20different%20benchmarks.%20Moreover%2C%20directly%20training%0Aanother%20model%20with%20edge%20labels%20produced%20by%20SDDR%20brings%20improvements%2C%20suggesting%0Athat%20our%20method%20could%20help%20with%20training%20robust%20refinement%20models%20in%20future%0Aworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17880v1&entry.124074799=Read"},
{"title": "MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling", "author": "Weihao Yuan and Weichao Shen and Yisheng He and Yuan Dong and Xiaodong Gu and Zilong Dong and Liefeng Bo and Qixing Huang", "abstract": "  Motion generation from discrete quantization offers many advantages over\ncontinuous regression, but at the cost of inevitable approximation errors.\nPrevious methods usually quantize the entire body pose into one code, which not\nonly faces the difficulty in encoding all joints within one vector but also\nloses the spatial relationship between different joints. Differently, in this\nwork we quantize each individual joint into one vector, which i) simplifies the\nquantization process as the complexity associated with a single joint is\nmarkedly lower than that of the entire pose; ii) maintains a spatial-temporal\nstructure that preserves both the spatial relationships among joints and the\ntemporal movement patterns; iii) yields a 2D token map, which enables the\napplication of various 2D operations widely used in 2D images. Grounded in the\n2D motion quantization, we build a spatial-temporal modeling framework, where\n2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D\nattention are proposed to take advantage of spatial-temporal signals among the\n2D tokens. Extensive experiments demonstrate that our method significantly\noutperforms previous methods across different datasets, with a $26.6\\%$\ndecrease of FID on HumanML3D and a $29.9\\%$ decrease on KIT-ML.\n", "link": "http://arxiv.org/abs/2409.17686v1", "date": "2024-09-26", "relevancy": 2.2717, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.575}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5677}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoGenTS%3A%20Motion%20Generation%20based%20on%20Spatial-Temporal%20Joint%20Modeling&body=Title%3A%20MoGenTS%3A%20Motion%20Generation%20based%20on%20Spatial-Temporal%20Joint%20Modeling%0AAuthor%3A%20Weihao%20Yuan%20and%20Weichao%20Shen%20and%20Yisheng%20He%20and%20Yuan%20Dong%20and%20Xiaodong%20Gu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Qixing%20Huang%0AAbstract%3A%20%20%20Motion%20generation%20from%20discrete%20quantization%20offers%20many%20advantages%20over%0Acontinuous%20regression%2C%20but%20at%20the%20cost%20of%20inevitable%20approximation%20errors.%0APrevious%20methods%20usually%20quantize%20the%20entire%20body%20pose%20into%20one%20code%2C%20which%20not%0Aonly%20faces%20the%20difficulty%20in%20encoding%20all%20joints%20within%20one%20vector%20but%20also%0Aloses%20the%20spatial%20relationship%20between%20different%20joints.%20Differently%2C%20in%20this%0Awork%20we%20quantize%20each%20individual%20joint%20into%20one%20vector%2C%20which%20i%29%20simplifies%20the%0Aquantization%20process%20as%20the%20complexity%20associated%20with%20a%20single%20joint%20is%0Amarkedly%20lower%20than%20that%20of%20the%20entire%20pose%3B%20ii%29%20maintains%20a%20spatial-temporal%0Astructure%20that%20preserves%20both%20the%20spatial%20relationships%20among%20joints%20and%20the%0Atemporal%20movement%20patterns%3B%20iii%29%20yields%20a%202D%20token%20map%2C%20which%20enables%20the%0Aapplication%20of%20various%202D%20operations%20widely%20used%20in%202D%20images.%20Grounded%20in%20the%0A2D%20motion%20quantization%2C%20we%20build%20a%20spatial-temporal%20modeling%20framework%2C%20where%0A2D%20joint%20VQVAE%2C%20temporal-spatial%202D%20masking%20technique%2C%20and%20spatial-temporal%202D%0Aattention%20are%20proposed%20to%20take%20advantage%20of%20spatial-temporal%20signals%20among%20the%0A2D%20tokens.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20previous%20methods%20across%20different%20datasets%2C%20with%20a%20%2426.6%5C%25%24%0Adecrease%20of%20FID%20on%20HumanML3D%20and%20a%20%2429.9%5C%25%24%20decrease%20on%20KIT-ML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoGenTS%253A%2520Motion%2520Generation%2520based%2520on%2520Spatial-Temporal%2520Joint%2520Modeling%26entry.906535625%3DWeihao%2520Yuan%2520and%2520Weichao%2520Shen%2520and%2520Yisheng%2520He%2520and%2520Yuan%2520Dong%2520and%2520Xiaodong%2520Gu%2520and%2520Zilong%2520Dong%2520and%2520Liefeng%2520Bo%2520and%2520Qixing%2520Huang%26entry.1292438233%3D%2520%2520Motion%2520generation%2520from%2520discrete%2520quantization%2520offers%2520many%2520advantages%2520over%250Acontinuous%2520regression%252C%2520but%2520at%2520the%2520cost%2520of%2520inevitable%2520approximation%2520errors.%250APrevious%2520methods%2520usually%2520quantize%2520the%2520entire%2520body%2520pose%2520into%2520one%2520code%252C%2520which%2520not%250Aonly%2520faces%2520the%2520difficulty%2520in%2520encoding%2520all%2520joints%2520within%2520one%2520vector%2520but%2520also%250Aloses%2520the%2520spatial%2520relationship%2520between%2520different%2520joints.%2520Differently%252C%2520in%2520this%250Awork%2520we%2520quantize%2520each%2520individual%2520joint%2520into%2520one%2520vector%252C%2520which%2520i%2529%2520simplifies%2520the%250Aquantization%2520process%2520as%2520the%2520complexity%2520associated%2520with%2520a%2520single%2520joint%2520is%250Amarkedly%2520lower%2520than%2520that%2520of%2520the%2520entire%2520pose%253B%2520ii%2529%2520maintains%2520a%2520spatial-temporal%250Astructure%2520that%2520preserves%2520both%2520the%2520spatial%2520relationships%2520among%2520joints%2520and%2520the%250Atemporal%2520movement%2520patterns%253B%2520iii%2529%2520yields%2520a%25202D%2520token%2520map%252C%2520which%2520enables%2520the%250Aapplication%2520of%2520various%25202D%2520operations%2520widely%2520used%2520in%25202D%2520images.%2520Grounded%2520in%2520the%250A2D%2520motion%2520quantization%252C%2520we%2520build%2520a%2520spatial-temporal%2520modeling%2520framework%252C%2520where%250A2D%2520joint%2520VQVAE%252C%2520temporal-spatial%25202D%2520masking%2520technique%252C%2520and%2520spatial-temporal%25202D%250Aattention%2520are%2520proposed%2520to%2520take%2520advantage%2520of%2520spatial-temporal%2520signals%2520among%2520the%250A2D%2520tokens.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520previous%2520methods%2520across%2520different%2520datasets%252C%2520with%2520a%2520%252426.6%255C%2525%2524%250Adecrease%2520of%2520FID%2520on%2520HumanML3D%2520and%2520a%2520%252429.9%255C%2525%2524%2520decrease%2520on%2520KIT-ML.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoGenTS%3A%20Motion%20Generation%20based%20on%20Spatial-Temporal%20Joint%20Modeling&entry.906535625=Weihao%20Yuan%20and%20Weichao%20Shen%20and%20Yisheng%20He%20and%20Yuan%20Dong%20and%20Xiaodong%20Gu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Qixing%20Huang&entry.1292438233=%20%20Motion%20generation%20from%20discrete%20quantization%20offers%20many%20advantages%20over%0Acontinuous%20regression%2C%20but%20at%20the%20cost%20of%20inevitable%20approximation%20errors.%0APrevious%20methods%20usually%20quantize%20the%20entire%20body%20pose%20into%20one%20code%2C%20which%20not%0Aonly%20faces%20the%20difficulty%20in%20encoding%20all%20joints%20within%20one%20vector%20but%20also%0Aloses%20the%20spatial%20relationship%20between%20different%20joints.%20Differently%2C%20in%20this%0Awork%20we%20quantize%20each%20individual%20joint%20into%20one%20vector%2C%20which%20i%29%20simplifies%20the%0Aquantization%20process%20as%20the%20complexity%20associated%20with%20a%20single%20joint%20is%0Amarkedly%20lower%20than%20that%20of%20the%20entire%20pose%3B%20ii%29%20maintains%20a%20spatial-temporal%0Astructure%20that%20preserves%20both%20the%20spatial%20relationships%20among%20joints%20and%20the%0Atemporal%20movement%20patterns%3B%20iii%29%20yields%20a%202D%20token%20map%2C%20which%20enables%20the%0Aapplication%20of%20various%202D%20operations%20widely%20used%20in%202D%20images.%20Grounded%20in%20the%0A2D%20motion%20quantization%2C%20we%20build%20a%20spatial-temporal%20modeling%20framework%2C%20where%0A2D%20joint%20VQVAE%2C%20temporal-spatial%202D%20masking%20technique%2C%20and%20spatial-temporal%202D%0Aattention%20are%20proposed%20to%20take%20advantage%20of%20spatial-temporal%20signals%20among%20the%0A2D%20tokens.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20previous%20methods%20across%20different%20datasets%2C%20with%20a%20%2426.6%5C%25%24%0Adecrease%20of%20FID%20on%20HumanML3D%20and%20a%20%2429.9%5C%25%24%20decrease%20on%20KIT-ML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17686v1&entry.124074799=Read"},
{"title": "Multi-View and Multi-Scale Alignment for Contrastive Language-Image\n  Pre-training in Mammography", "author": "Yuexi Du and John Onofrey and Nicha C. Dvornek", "abstract": "  Contrastive Language-Image Pre-training (CLIP) shows promise in medical image\nanalysis but requires substantial data and computational resources. Due to\nthese restrictions, existing CLIP applications in medical imaging focus mainly\non modalities like chest X-rays that have abundant image-report data available,\nleaving many other important modalities under-explored. Here, we propose the\nfirst adaptation of the full CLIP model to mammography, which presents\nsignificant challenges due to labeled data scarcity, high-resolution images\nwith small regions of interest, and data imbalance. We first develop a\nspecialized supervision framework for mammography that leverages its multi-view\nnature. Furthermore, we design a symmetric local alignment module to better\nfocus on detailed features in high-resolution images. Lastly, we incorporate a\nparameter-efficient fine-tuning approach for large language models pre-trained\nwith medical knowledge to address data limitations. Our multi-view and\nmulti-scale alignment (MaMA) method outperforms state-of-the-art baselines for\nthree different tasks on two large real-world mammography datasets, EMBED and\nRSNA-Mammo, with only 52% model size compared with the largest baseline.\n", "link": "http://arxiv.org/abs/2409.18119v1", "date": "2024-09-26", "relevancy": 2.263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6446}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20and%20Multi-Scale%20Alignment%20for%20Contrastive%20Language-Image%0A%20%20Pre-training%20in%20Mammography&body=Title%3A%20Multi-View%20and%20Multi-Scale%20Alignment%20for%20Contrastive%20Language-Image%0A%20%20Pre-training%20in%20Mammography%0AAuthor%3A%20Yuexi%20Du%20and%20John%20Onofrey%20and%20Nicha%20C.%20Dvornek%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20shows%20promise%20in%20medical%20image%0Aanalysis%20but%20requires%20substantial%20data%20and%20computational%20resources.%20Due%20to%0Athese%20restrictions%2C%20existing%20CLIP%20applications%20in%20medical%20imaging%20focus%20mainly%0Aon%20modalities%20like%20chest%20X-rays%20that%20have%20abundant%20image-report%20data%20available%2C%0Aleaving%20many%20other%20important%20modalities%20under-explored.%20Here%2C%20we%20propose%20the%0Afirst%20adaptation%20of%20the%20full%20CLIP%20model%20to%20mammography%2C%20which%20presents%0Asignificant%20challenges%20due%20to%20labeled%20data%20scarcity%2C%20high-resolution%20images%0Awith%20small%20regions%20of%20interest%2C%20and%20data%20imbalance.%20We%20first%20develop%20a%0Aspecialized%20supervision%20framework%20for%20mammography%20that%20leverages%20its%20multi-view%0Anature.%20Furthermore%2C%20we%20design%20a%20symmetric%20local%20alignment%20module%20to%20better%0Afocus%20on%20detailed%20features%20in%20high-resolution%20images.%20Lastly%2C%20we%20incorporate%20a%0Aparameter-efficient%20fine-tuning%20approach%20for%20large%20language%20models%20pre-trained%0Awith%20medical%20knowledge%20to%20address%20data%20limitations.%20Our%20multi-view%20and%0Amulti-scale%20alignment%20%28MaMA%29%20method%20outperforms%20state-of-the-art%20baselines%20for%0Athree%20different%20tasks%20on%20two%20large%20real-world%20mammography%20datasets%2C%20EMBED%20and%0ARSNA-Mammo%2C%20with%20only%2052%25%20model%20size%20compared%20with%20the%20largest%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520and%2520Multi-Scale%2520Alignment%2520for%2520Contrastive%2520Language-Image%250A%2520%2520Pre-training%2520in%2520Mammography%26entry.906535625%3DYuexi%2520Du%2520and%2520John%2520Onofrey%2520and%2520Nicha%2520C.%2520Dvornek%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520shows%2520promise%2520in%2520medical%2520image%250Aanalysis%2520but%2520requires%2520substantial%2520data%2520and%2520computational%2520resources.%2520Due%2520to%250Athese%2520restrictions%252C%2520existing%2520CLIP%2520applications%2520in%2520medical%2520imaging%2520focus%2520mainly%250Aon%2520modalities%2520like%2520chest%2520X-rays%2520that%2520have%2520abundant%2520image-report%2520data%2520available%252C%250Aleaving%2520many%2520other%2520important%2520modalities%2520under-explored.%2520Here%252C%2520we%2520propose%2520the%250Afirst%2520adaptation%2520of%2520the%2520full%2520CLIP%2520model%2520to%2520mammography%252C%2520which%2520presents%250Asignificant%2520challenges%2520due%2520to%2520labeled%2520data%2520scarcity%252C%2520high-resolution%2520images%250Awith%2520small%2520regions%2520of%2520interest%252C%2520and%2520data%2520imbalance.%2520We%2520first%2520develop%2520a%250Aspecialized%2520supervision%2520framework%2520for%2520mammography%2520that%2520leverages%2520its%2520multi-view%250Anature.%2520Furthermore%252C%2520we%2520design%2520a%2520symmetric%2520local%2520alignment%2520module%2520to%2520better%250Afocus%2520on%2520detailed%2520features%2520in%2520high-resolution%2520images.%2520Lastly%252C%2520we%2520incorporate%2520a%250Aparameter-efficient%2520fine-tuning%2520approach%2520for%2520large%2520language%2520models%2520pre-trained%250Awith%2520medical%2520knowledge%2520to%2520address%2520data%2520limitations.%2520Our%2520multi-view%2520and%250Amulti-scale%2520alignment%2520%2528MaMA%2529%2520method%2520outperforms%2520state-of-the-art%2520baselines%2520for%250Athree%2520different%2520tasks%2520on%2520two%2520large%2520real-world%2520mammography%2520datasets%252C%2520EMBED%2520and%250ARSNA-Mammo%252C%2520with%2520only%252052%2525%2520model%2520size%2520compared%2520with%2520the%2520largest%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20and%20Multi-Scale%20Alignment%20for%20Contrastive%20Language-Image%0A%20%20Pre-training%20in%20Mammography&entry.906535625=Yuexi%20Du%20and%20John%20Onofrey%20and%20Nicha%20C.%20Dvornek&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20shows%20promise%20in%20medical%20image%0Aanalysis%20but%20requires%20substantial%20data%20and%20computational%20resources.%20Due%20to%0Athese%20restrictions%2C%20existing%20CLIP%20applications%20in%20medical%20imaging%20focus%20mainly%0Aon%20modalities%20like%20chest%20X-rays%20that%20have%20abundant%20image-report%20data%20available%2C%0Aleaving%20many%20other%20important%20modalities%20under-explored.%20Here%2C%20we%20propose%20the%0Afirst%20adaptation%20of%20the%20full%20CLIP%20model%20to%20mammography%2C%20which%20presents%0Asignificant%20challenges%20due%20to%20labeled%20data%20scarcity%2C%20high-resolution%20images%0Awith%20small%20regions%20of%20interest%2C%20and%20data%20imbalance.%20We%20first%20develop%20a%0Aspecialized%20supervision%20framework%20for%20mammography%20that%20leverages%20its%20multi-view%0Anature.%20Furthermore%2C%20we%20design%20a%20symmetric%20local%20alignment%20module%20to%20better%0Afocus%20on%20detailed%20features%20in%20high-resolution%20images.%20Lastly%2C%20we%20incorporate%20a%0Aparameter-efficient%20fine-tuning%20approach%20for%20large%20language%20models%20pre-trained%0Awith%20medical%20knowledge%20to%20address%20data%20limitations.%20Our%20multi-view%20and%0Amulti-scale%20alignment%20%28MaMA%29%20method%20outperforms%20state-of-the-art%20baselines%20for%0Athree%20different%20tasks%20on%20two%20large%20real-world%20mammography%20datasets%2C%20EMBED%20and%0ARSNA-Mammo%2C%20with%20only%2052%25%20model%20size%20compared%20with%20the%20largest%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18119v1&entry.124074799=Read"},
{"title": "Transferring disentangled representations: bridging the gap between\n  synthetic and real images", "author": "Jacopo Dapueto and Nicoletta Noceti and Francesca Odone", "abstract": "  Developing meaningful and efficient representations that separate the\nfundamental structure of the data generation mechanism is crucial in\nrepresentation learning. However, Disentangled Representation Learning has not\nfully shown its potential on real images, because of correlated generative\nfactors, their resolution and limited access to ground truth labels.\nSpecifically on the latter, we investigate the possibility of leveraging\nsynthetic data to learn general-purpose disentangled representations applicable\nto real data, discussing the effect of fine-tuning and what properties of\ndisentanglement are preserved after the transfer. We provide an extensive\nempirical study to address these issues. In addition, we propose a new\ninterpretable intervention-based metric, to measure the quality of factors\nencoding in the representation. Our results indicate that some level of\ndisentanglement, transferring a representation from synthetic to real data, is\npossible and effective.\n", "link": "http://arxiv.org/abs/2409.18017v1", "date": "2024-09-26", "relevancy": 2.2593, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5868}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.563}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferring%20disentangled%20representations%3A%20bridging%20the%20gap%20between%0A%20%20synthetic%20and%20real%20images&body=Title%3A%20Transferring%20disentangled%20representations%3A%20bridging%20the%20gap%20between%0A%20%20synthetic%20and%20real%20images%0AAuthor%3A%20Jacopo%20Dapueto%20and%20Nicoletta%20Noceti%20and%20Francesca%20Odone%0AAbstract%3A%20%20%20Developing%20meaningful%20and%20efficient%20representations%20that%20separate%20the%0Afundamental%20structure%20of%20the%20data%20generation%20mechanism%20is%20crucial%20in%0Arepresentation%20learning.%20However%2C%20Disentangled%20Representation%20Learning%20has%20not%0Afully%20shown%20its%20potential%20on%20real%20images%2C%20because%20of%20correlated%20generative%0Afactors%2C%20their%20resolution%20and%20limited%20access%20to%20ground%20truth%20labels.%0ASpecifically%20on%20the%20latter%2C%20we%20investigate%20the%20possibility%20of%20leveraging%0Asynthetic%20data%20to%20learn%20general-purpose%20disentangled%20representations%20applicable%0Ato%20real%20data%2C%20discussing%20the%20effect%20of%20fine-tuning%20and%20what%20properties%20of%0Adisentanglement%20are%20preserved%20after%20the%20transfer.%20We%20provide%20an%20extensive%0Aempirical%20study%20to%20address%20these%20issues.%20In%20addition%2C%20we%20propose%20a%20new%0Ainterpretable%20intervention-based%20metric%2C%20to%20measure%20the%20quality%20of%20factors%0Aencoding%20in%20the%20representation.%20Our%20results%20indicate%20that%20some%20level%20of%0Adisentanglement%2C%20transferring%20a%20representation%20from%20synthetic%20to%20real%20data%2C%20is%0Apossible%20and%20effective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferring%2520disentangled%2520representations%253A%2520bridging%2520the%2520gap%2520between%250A%2520%2520synthetic%2520and%2520real%2520images%26entry.906535625%3DJacopo%2520Dapueto%2520and%2520Nicoletta%2520Noceti%2520and%2520Francesca%2520Odone%26entry.1292438233%3D%2520%2520Developing%2520meaningful%2520and%2520efficient%2520representations%2520that%2520separate%2520the%250Afundamental%2520structure%2520of%2520the%2520data%2520generation%2520mechanism%2520is%2520crucial%2520in%250Arepresentation%2520learning.%2520However%252C%2520Disentangled%2520Representation%2520Learning%2520has%2520not%250Afully%2520shown%2520its%2520potential%2520on%2520real%2520images%252C%2520because%2520of%2520correlated%2520generative%250Afactors%252C%2520their%2520resolution%2520and%2520limited%2520access%2520to%2520ground%2520truth%2520labels.%250ASpecifically%2520on%2520the%2520latter%252C%2520we%2520investigate%2520the%2520possibility%2520of%2520leveraging%250Asynthetic%2520data%2520to%2520learn%2520general-purpose%2520disentangled%2520representations%2520applicable%250Ato%2520real%2520data%252C%2520discussing%2520the%2520effect%2520of%2520fine-tuning%2520and%2520what%2520properties%2520of%250Adisentanglement%2520are%2520preserved%2520after%2520the%2520transfer.%2520We%2520provide%2520an%2520extensive%250Aempirical%2520study%2520to%2520address%2520these%2520issues.%2520In%2520addition%252C%2520we%2520propose%2520a%2520new%250Ainterpretable%2520intervention-based%2520metric%252C%2520to%2520measure%2520the%2520quality%2520of%2520factors%250Aencoding%2520in%2520the%2520representation.%2520Our%2520results%2520indicate%2520that%2520some%2520level%2520of%250Adisentanglement%252C%2520transferring%2520a%2520representation%2520from%2520synthetic%2520to%2520real%2520data%252C%2520is%250Apossible%2520and%2520effective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferring%20disentangled%20representations%3A%20bridging%20the%20gap%20between%0A%20%20synthetic%20and%20real%20images&entry.906535625=Jacopo%20Dapueto%20and%20Nicoletta%20Noceti%20and%20Francesca%20Odone&entry.1292438233=%20%20Developing%20meaningful%20and%20efficient%20representations%20that%20separate%20the%0Afundamental%20structure%20of%20the%20data%20generation%20mechanism%20is%20crucial%20in%0Arepresentation%20learning.%20However%2C%20Disentangled%20Representation%20Learning%20has%20not%0Afully%20shown%20its%20potential%20on%20real%20images%2C%20because%20of%20correlated%20generative%0Afactors%2C%20their%20resolution%20and%20limited%20access%20to%20ground%20truth%20labels.%0ASpecifically%20on%20the%20latter%2C%20we%20investigate%20the%20possibility%20of%20leveraging%0Asynthetic%20data%20to%20learn%20general-purpose%20disentangled%20representations%20applicable%0Ato%20real%20data%2C%20discussing%20the%20effect%20of%20fine-tuning%20and%20what%20properties%20of%0Adisentanglement%20are%20preserved%20after%20the%20transfer.%20We%20provide%20an%20extensive%0Aempirical%20study%20to%20address%20these%20issues.%20In%20addition%2C%20we%20propose%20a%20new%0Ainterpretable%20intervention-based%20metric%2C%20to%20measure%20the%20quality%20of%20factors%0Aencoding%20in%20the%20representation.%20Our%20results%20indicate%20that%20some%20level%20of%0Adisentanglement%2C%20transferring%20a%20representation%20from%20synthetic%20to%20real%20data%2C%20is%0Apossible%20and%20effective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18017v1&entry.124074799=Read"},
{"title": "CollaMamba: Efficient Collaborative Perception with Cross-Agent\n  Spatial-Temporal State Space Model", "author": "Yang Li and Quan Yuan and Guiyang Luo and Xiaoyuan Fu and Xuanhan Zhu and Yujia Yang and Rui Pan and Jinglin Li", "abstract": "  By sharing complementary perceptual information, multi-agent collaborative\nperception fosters a deeper understanding of the environment. Recent studies on\ncollaborative perception mostly utilize CNNs or Transformers to learn feature\nrepresentation and fusion in the spatial dimension, which struggle to handle\nlong-range spatial-temporal features under limited computing and communication\nresources. Holistically modeling the dependencies over extensive spatial areas\nand extended temporal frames is crucial to enhancing feature quality. To this\nend, we propose a resource efficient cross-agent spatial-temporal collaborative\nstate space model (SSM), named CollaMamba. Initially, we construct a\nfoundational backbone network based on spatial SSM. This backbone adeptly\ncaptures positional causal dependencies from both single-agent and cross-agent\nviews, yielding compact and comprehensive intermediate features while\nmaintaining linear complexity. Furthermore, we devise a history-aware feature\nboosting module based on temporal SSM, extracting contextual cues from extended\nhistorical frames to refine vague features while preserving low overhead.\nExtensive experiments across several datasets demonstrate that CollaMamba\noutperforms state-of-the-art methods, achieving higher model accuracy while\nreducing computational and communication overhead by up to 71.9% and 1/64,\nrespectively. This work pioneers the exploration of the Mamba's potential in\ncollaborative perception. The source code will be made available.\n", "link": "http://arxiv.org/abs/2409.07714v2", "date": "2024-09-26", "relevancy": 2.2587, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CollaMamba%3A%20Efficient%20Collaborative%20Perception%20with%20Cross-Agent%0A%20%20Spatial-Temporal%20State%20Space%20Model&body=Title%3A%20CollaMamba%3A%20Efficient%20Collaborative%20Perception%20with%20Cross-Agent%0A%20%20Spatial-Temporal%20State%20Space%20Model%0AAuthor%3A%20Yang%20Li%20and%20Quan%20Yuan%20and%20Guiyang%20Luo%20and%20Xiaoyuan%20Fu%20and%20Xuanhan%20Zhu%20and%20Yujia%20Yang%20and%20Rui%20Pan%20and%20Jinglin%20Li%0AAbstract%3A%20%20%20By%20sharing%20complementary%20perceptual%20information%2C%20multi-agent%20collaborative%0Aperception%20fosters%20a%20deeper%20understanding%20of%20the%20environment.%20Recent%20studies%20on%0Acollaborative%20perception%20mostly%20utilize%20CNNs%20or%20Transformers%20to%20learn%20feature%0Arepresentation%20and%20fusion%20in%20the%20spatial%20dimension%2C%20which%20struggle%20to%20handle%0Along-range%20spatial-temporal%20features%20under%20limited%20computing%20and%20communication%0Aresources.%20Holistically%20modeling%20the%20dependencies%20over%20extensive%20spatial%20areas%0Aand%20extended%20temporal%20frames%20is%20crucial%20to%20enhancing%20feature%20quality.%20To%20this%0Aend%2C%20we%20propose%20a%20resource%20efficient%20cross-agent%20spatial-temporal%20collaborative%0Astate%20space%20model%20%28SSM%29%2C%20named%20CollaMamba.%20Initially%2C%20we%20construct%20a%0Afoundational%20backbone%20network%20based%20on%20spatial%20SSM.%20This%20backbone%20adeptly%0Acaptures%20positional%20causal%20dependencies%20from%20both%20single-agent%20and%20cross-agent%0Aviews%2C%20yielding%20compact%20and%20comprehensive%20intermediate%20features%20while%0Amaintaining%20linear%20complexity.%20Furthermore%2C%20we%20devise%20a%20history-aware%20feature%0Aboosting%20module%20based%20on%20temporal%20SSM%2C%20extracting%20contextual%20cues%20from%20extended%0Ahistorical%20frames%20to%20refine%20vague%20features%20while%20preserving%20low%20overhead.%0AExtensive%20experiments%20across%20several%20datasets%20demonstrate%20that%20CollaMamba%0Aoutperforms%20state-of-the-art%20methods%2C%20achieving%20higher%20model%20accuracy%20while%0Areducing%20computational%20and%20communication%20overhead%20by%20up%20to%2071.9%25%20and%201/64%2C%0Arespectively.%20This%20work%20pioneers%20the%20exploration%20of%20the%20Mamba%27s%20potential%20in%0Acollaborative%20perception.%20The%20source%20code%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaMamba%253A%2520Efficient%2520Collaborative%2520Perception%2520with%2520Cross-Agent%250A%2520%2520Spatial-Temporal%2520State%2520Space%2520Model%26entry.906535625%3DYang%2520Li%2520and%2520Quan%2520Yuan%2520and%2520Guiyang%2520Luo%2520and%2520Xiaoyuan%2520Fu%2520and%2520Xuanhan%2520Zhu%2520and%2520Yujia%2520Yang%2520and%2520Rui%2520Pan%2520and%2520Jinglin%2520Li%26entry.1292438233%3D%2520%2520By%2520sharing%2520complementary%2520perceptual%2520information%252C%2520multi-agent%2520collaborative%250Aperception%2520fosters%2520a%2520deeper%2520understanding%2520of%2520the%2520environment.%2520Recent%2520studies%2520on%250Acollaborative%2520perception%2520mostly%2520utilize%2520CNNs%2520or%2520Transformers%2520to%2520learn%2520feature%250Arepresentation%2520and%2520fusion%2520in%2520the%2520spatial%2520dimension%252C%2520which%2520struggle%2520to%2520handle%250Along-range%2520spatial-temporal%2520features%2520under%2520limited%2520computing%2520and%2520communication%250Aresources.%2520Holistically%2520modeling%2520the%2520dependencies%2520over%2520extensive%2520spatial%2520areas%250Aand%2520extended%2520temporal%2520frames%2520is%2520crucial%2520to%2520enhancing%2520feature%2520quality.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520resource%2520efficient%2520cross-agent%2520spatial-temporal%2520collaborative%250Astate%2520space%2520model%2520%2528SSM%2529%252C%2520named%2520CollaMamba.%2520Initially%252C%2520we%2520construct%2520a%250Afoundational%2520backbone%2520network%2520based%2520on%2520spatial%2520SSM.%2520This%2520backbone%2520adeptly%250Acaptures%2520positional%2520causal%2520dependencies%2520from%2520both%2520single-agent%2520and%2520cross-agent%250Aviews%252C%2520yielding%2520compact%2520and%2520comprehensive%2520intermediate%2520features%2520while%250Amaintaining%2520linear%2520complexity.%2520Furthermore%252C%2520we%2520devise%2520a%2520history-aware%2520feature%250Aboosting%2520module%2520based%2520on%2520temporal%2520SSM%252C%2520extracting%2520contextual%2520cues%2520from%2520extended%250Ahistorical%2520frames%2520to%2520refine%2520vague%2520features%2520while%2520preserving%2520low%2520overhead.%250AExtensive%2520experiments%2520across%2520several%2520datasets%2520demonstrate%2520that%2520CollaMamba%250Aoutperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520higher%2520model%2520accuracy%2520while%250Areducing%2520computational%2520and%2520communication%2520overhead%2520by%2520up%2520to%252071.9%2525%2520and%25201/64%252C%250Arespectively.%2520This%2520work%2520pioneers%2520the%2520exploration%2520of%2520the%2520Mamba%2527s%2520potential%2520in%250Acollaborative%2520perception.%2520The%2520source%2520code%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CollaMamba%3A%20Efficient%20Collaborative%20Perception%20with%20Cross-Agent%0A%20%20Spatial-Temporal%20State%20Space%20Model&entry.906535625=Yang%20Li%20and%20Quan%20Yuan%20and%20Guiyang%20Luo%20and%20Xiaoyuan%20Fu%20and%20Xuanhan%20Zhu%20and%20Yujia%20Yang%20and%20Rui%20Pan%20and%20Jinglin%20Li&entry.1292438233=%20%20By%20sharing%20complementary%20perceptual%20information%2C%20multi-agent%20collaborative%0Aperception%20fosters%20a%20deeper%20understanding%20of%20the%20environment.%20Recent%20studies%20on%0Acollaborative%20perception%20mostly%20utilize%20CNNs%20or%20Transformers%20to%20learn%20feature%0Arepresentation%20and%20fusion%20in%20the%20spatial%20dimension%2C%20which%20struggle%20to%20handle%0Along-range%20spatial-temporal%20features%20under%20limited%20computing%20and%20communication%0Aresources.%20Holistically%20modeling%20the%20dependencies%20over%20extensive%20spatial%20areas%0Aand%20extended%20temporal%20frames%20is%20crucial%20to%20enhancing%20feature%20quality.%20To%20this%0Aend%2C%20we%20propose%20a%20resource%20efficient%20cross-agent%20spatial-temporal%20collaborative%0Astate%20space%20model%20%28SSM%29%2C%20named%20CollaMamba.%20Initially%2C%20we%20construct%20a%0Afoundational%20backbone%20network%20based%20on%20spatial%20SSM.%20This%20backbone%20adeptly%0Acaptures%20positional%20causal%20dependencies%20from%20both%20single-agent%20and%20cross-agent%0Aviews%2C%20yielding%20compact%20and%20comprehensive%20intermediate%20features%20while%0Amaintaining%20linear%20complexity.%20Furthermore%2C%20we%20devise%20a%20history-aware%20feature%0Aboosting%20module%20based%20on%20temporal%20SSM%2C%20extracting%20contextual%20cues%20from%20extended%0Ahistorical%20frames%20to%20refine%20vague%20features%20while%20preserving%20low%20overhead.%0AExtensive%20experiments%20across%20several%20datasets%20demonstrate%20that%20CollaMamba%0Aoutperforms%20state-of-the-art%20methods%2C%20achieving%20higher%20model%20accuracy%20while%0Areducing%20computational%20and%20communication%20overhead%20by%20up%20to%2071.9%25%20and%201/64%2C%0Arespectively.%20This%20work%20pioneers%20the%20exploration%20of%20the%20Mamba%27s%20potential%20in%0Acollaborative%20perception.%20The%20source%20code%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07714v2&entry.124074799=Read"},
{"title": "Stable Object Placement Under Geometric Uncertainty via Differentiable\n  Contact Dynamics", "author": "Linfeng Li and Gang Yang and Lin Shao and David Hsu", "abstract": "  From serving a cup of coffee to carefully rearranging delicate items, stable\nobject placement is a crucial skill for future robots. This skill is\nchallenging due to the required accuracy, which is difficult to achieve under\ngeometric uncertainty. We leverage differentiable contact dynamics to develop a\nprincipled method for stable object placement under geometric uncertainty. We\nestimate the geometric uncertainty by minimizing the discrepancy between the\nforce-torque sensor readings and the model predictions through gradient\ndescent. We further keep track of a belief over multiple possible geometric\nparameters to mitigate the gradient-based method's sensitivity to the\ninitialization. We verify our approach in the real world on various geometric\nuncertainties, including the in-hand pose uncertainty of the grasped object,\nthe object's shape uncertainty, and the environment's shape uncertainty.\n", "link": "http://arxiv.org/abs/2409.17725v1", "date": "2024-09-26", "relevancy": 2.2468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5801}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5656}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Object%20Placement%20Under%20Geometric%20Uncertainty%20via%20Differentiable%0A%20%20Contact%20Dynamics&body=Title%3A%20Stable%20Object%20Placement%20Under%20Geometric%20Uncertainty%20via%20Differentiable%0A%20%20Contact%20Dynamics%0AAuthor%3A%20Linfeng%20Li%20and%20Gang%20Yang%20and%20Lin%20Shao%20and%20David%20Hsu%0AAbstract%3A%20%20%20From%20serving%20a%20cup%20of%20coffee%20to%20carefully%20rearranging%20delicate%20items%2C%20stable%0Aobject%20placement%20is%20a%20crucial%20skill%20for%20future%20robots.%20This%20skill%20is%0Achallenging%20due%20to%20the%20required%20accuracy%2C%20which%20is%20difficult%20to%20achieve%20under%0Ageometric%20uncertainty.%20We%20leverage%20differentiable%20contact%20dynamics%20to%20develop%20a%0Aprincipled%20method%20for%20stable%20object%20placement%20under%20geometric%20uncertainty.%20We%0Aestimate%20the%20geometric%20uncertainty%20by%20minimizing%20the%20discrepancy%20between%20the%0Aforce-torque%20sensor%20readings%20and%20the%20model%20predictions%20through%20gradient%0Adescent.%20We%20further%20keep%20track%20of%20a%20belief%20over%20multiple%20possible%20geometric%0Aparameters%20to%20mitigate%20the%20gradient-based%20method%27s%20sensitivity%20to%20the%0Ainitialization.%20We%20verify%20our%20approach%20in%20the%20real%20world%20on%20various%20geometric%0Auncertainties%2C%20including%20the%20in-hand%20pose%20uncertainty%20of%20the%20grasped%20object%2C%0Athe%20object%27s%20shape%20uncertainty%2C%20and%20the%20environment%27s%20shape%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Object%2520Placement%2520Under%2520Geometric%2520Uncertainty%2520via%2520Differentiable%250A%2520%2520Contact%2520Dynamics%26entry.906535625%3DLinfeng%2520Li%2520and%2520Gang%2520Yang%2520and%2520Lin%2520Shao%2520and%2520David%2520Hsu%26entry.1292438233%3D%2520%2520From%2520serving%2520a%2520cup%2520of%2520coffee%2520to%2520carefully%2520rearranging%2520delicate%2520items%252C%2520stable%250Aobject%2520placement%2520is%2520a%2520crucial%2520skill%2520for%2520future%2520robots.%2520This%2520skill%2520is%250Achallenging%2520due%2520to%2520the%2520required%2520accuracy%252C%2520which%2520is%2520difficult%2520to%2520achieve%2520under%250Ageometric%2520uncertainty.%2520We%2520leverage%2520differentiable%2520contact%2520dynamics%2520to%2520develop%2520a%250Aprincipled%2520method%2520for%2520stable%2520object%2520placement%2520under%2520geometric%2520uncertainty.%2520We%250Aestimate%2520the%2520geometric%2520uncertainty%2520by%2520minimizing%2520the%2520discrepancy%2520between%2520the%250Aforce-torque%2520sensor%2520readings%2520and%2520the%2520model%2520predictions%2520through%2520gradient%250Adescent.%2520We%2520further%2520keep%2520track%2520of%2520a%2520belief%2520over%2520multiple%2520possible%2520geometric%250Aparameters%2520to%2520mitigate%2520the%2520gradient-based%2520method%2527s%2520sensitivity%2520to%2520the%250Ainitialization.%2520We%2520verify%2520our%2520approach%2520in%2520the%2520real%2520world%2520on%2520various%2520geometric%250Auncertainties%252C%2520including%2520the%2520in-hand%2520pose%2520uncertainty%2520of%2520the%2520grasped%2520object%252C%250Athe%2520object%2527s%2520shape%2520uncertainty%252C%2520and%2520the%2520environment%2527s%2520shape%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Object%20Placement%20Under%20Geometric%20Uncertainty%20via%20Differentiable%0A%20%20Contact%20Dynamics&entry.906535625=Linfeng%20Li%20and%20Gang%20Yang%20and%20Lin%20Shao%20and%20David%20Hsu&entry.1292438233=%20%20From%20serving%20a%20cup%20of%20coffee%20to%20carefully%20rearranging%20delicate%20items%2C%20stable%0Aobject%20placement%20is%20a%20crucial%20skill%20for%20future%20robots.%20This%20skill%20is%0Achallenging%20due%20to%20the%20required%20accuracy%2C%20which%20is%20difficult%20to%20achieve%20under%0Ageometric%20uncertainty.%20We%20leverage%20differentiable%20contact%20dynamics%20to%20develop%20a%0Aprincipled%20method%20for%20stable%20object%20placement%20under%20geometric%20uncertainty.%20We%0Aestimate%20the%20geometric%20uncertainty%20by%20minimizing%20the%20discrepancy%20between%20the%0Aforce-torque%20sensor%20readings%20and%20the%20model%20predictions%20through%20gradient%0Adescent.%20We%20further%20keep%20track%20of%20a%20belief%20over%20multiple%20possible%20geometric%0Aparameters%20to%20mitigate%20the%20gradient-based%20method%27s%20sensitivity%20to%20the%0Ainitialization.%20We%20verify%20our%20approach%20in%20the%20real%20world%20on%20various%20geometric%0Auncertainties%2C%20including%20the%20in-hand%20pose%20uncertainty%20of%20the%20grasped%20object%2C%0Athe%20object%27s%20shape%20uncertainty%2C%20and%20the%20environment%27s%20shape%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17725v1&entry.124074799=Read"},
{"title": "Harnessing Shared Relations via Multimodal Mixup Contrastive Learning\n  for Multimodal Classification", "author": "Raja Kumar and Raghav Singhal and Pranamya Kulkarni and Deval Mehta and Kshitij Jadhav", "abstract": "  Deep multimodal learning has shown remarkable success by leveraging\ncontrastive learning to capture explicit one-to-one relations across\nmodalities. However, real-world data often exhibits shared relations beyond\nsimple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive\nLearning approach to capture nuanced shared relations inherent in multimodal\ndata. Our key contribution is a Mixup-based contrastive loss that learns robust\nrepresentations by aligning mixed samples from one modality with their\ncorresponding samples from other modalities thereby capturing shared relations\nbetween them. For multimodal classification tasks, we introduce a framework\nthat integrates a fusion module with unimodal prediction modules for auxiliary\nsupervision during training, complemented by our proposed Mixup-based\ncontrastive loss. Through extensive experiments on diverse datasets (N24News,\nROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures\nshared multimodal relations and generalizes across domains. It outperforms\nstate-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving\ncomparable performance on Food-101. Our work highlights the significance of\nlearning shared relations for robust multimodal learning, opening up promising\navenues for future research.\n", "link": "http://arxiv.org/abs/2409.17777v1", "date": "2024-09-26", "relevancy": 2.2452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6039}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5516}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Shared%20Relations%20via%20Multimodal%20Mixup%20Contrastive%20Learning%0A%20%20for%20Multimodal%20Classification&body=Title%3A%20Harnessing%20Shared%20Relations%20via%20Multimodal%20Mixup%20Contrastive%20Learning%0A%20%20for%20Multimodal%20Classification%0AAuthor%3A%20Raja%20Kumar%20and%20Raghav%20Singhal%20and%20Pranamya%20Kulkarni%20and%20Deval%20Mehta%20and%20Kshitij%20Jadhav%0AAbstract%3A%20%20%20Deep%20multimodal%20learning%20has%20shown%20remarkable%20success%20by%20leveraging%0Acontrastive%20learning%20to%20capture%20explicit%20one-to-one%20relations%20across%0Amodalities.%20However%2C%20real-world%20data%20often%20exhibits%20shared%20relations%20beyond%0Asimple%20pairwise%20associations.%20We%20propose%20M3CoL%2C%20a%20Multimodal%20Mixup%20Contrastive%0ALearning%20approach%20to%20capture%20nuanced%20shared%20relations%20inherent%20in%20multimodal%0Adata.%20Our%20key%20contribution%20is%20a%20Mixup-based%20contrastive%20loss%20that%20learns%20robust%0Arepresentations%20by%20aligning%20mixed%20samples%20from%20one%20modality%20with%20their%0Acorresponding%20samples%20from%20other%20modalities%20thereby%20capturing%20shared%20relations%0Abetween%20them.%20For%20multimodal%20classification%20tasks%2C%20we%20introduce%20a%20framework%0Athat%20integrates%20a%20fusion%20module%20with%20unimodal%20prediction%20modules%20for%20auxiliary%0Asupervision%20during%20training%2C%20complemented%20by%20our%20proposed%20Mixup-based%0Acontrastive%20loss.%20Through%20extensive%20experiments%20on%20diverse%20datasets%20%28N24News%2C%0AROSMAP%2C%20BRCA%2C%20and%20Food-101%29%2C%20we%20demonstrate%20that%20M3CoL%20effectively%20captures%0Ashared%20multimodal%20relations%20and%20generalizes%20across%20domains.%20It%20outperforms%0Astate-of-the-art%20methods%20on%20N24News%2C%20ROSMAP%2C%20and%20BRCA%2C%20while%20achieving%0Acomparable%20performance%20on%20Food-101.%20Our%20work%20highlights%20the%20significance%20of%0Alearning%20shared%20relations%20for%20robust%20multimodal%20learning%2C%20opening%20up%20promising%0Aavenues%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Shared%2520Relations%2520via%2520Multimodal%2520Mixup%2520Contrastive%2520Learning%250A%2520%2520for%2520Multimodal%2520Classification%26entry.906535625%3DRaja%2520Kumar%2520and%2520Raghav%2520Singhal%2520and%2520Pranamya%2520Kulkarni%2520and%2520Deval%2520Mehta%2520and%2520Kshitij%2520Jadhav%26entry.1292438233%3D%2520%2520Deep%2520multimodal%2520learning%2520has%2520shown%2520remarkable%2520success%2520by%2520leveraging%250Acontrastive%2520learning%2520to%2520capture%2520explicit%2520one-to-one%2520relations%2520across%250Amodalities.%2520However%252C%2520real-world%2520data%2520often%2520exhibits%2520shared%2520relations%2520beyond%250Asimple%2520pairwise%2520associations.%2520We%2520propose%2520M3CoL%252C%2520a%2520Multimodal%2520Mixup%2520Contrastive%250ALearning%2520approach%2520to%2520capture%2520nuanced%2520shared%2520relations%2520inherent%2520in%2520multimodal%250Adata.%2520Our%2520key%2520contribution%2520is%2520a%2520Mixup-based%2520contrastive%2520loss%2520that%2520learns%2520robust%250Arepresentations%2520by%2520aligning%2520mixed%2520samples%2520from%2520one%2520modality%2520with%2520their%250Acorresponding%2520samples%2520from%2520other%2520modalities%2520thereby%2520capturing%2520shared%2520relations%250Abetween%2520them.%2520For%2520multimodal%2520classification%2520tasks%252C%2520we%2520introduce%2520a%2520framework%250Athat%2520integrates%2520a%2520fusion%2520module%2520with%2520unimodal%2520prediction%2520modules%2520for%2520auxiliary%250Asupervision%2520during%2520training%252C%2520complemented%2520by%2520our%2520proposed%2520Mixup-based%250Acontrastive%2520loss.%2520Through%2520extensive%2520experiments%2520on%2520diverse%2520datasets%2520%2528N24News%252C%250AROSMAP%252C%2520BRCA%252C%2520and%2520Food-101%2529%252C%2520we%2520demonstrate%2520that%2520M3CoL%2520effectively%2520captures%250Ashared%2520multimodal%2520relations%2520and%2520generalizes%2520across%2520domains.%2520It%2520outperforms%250Astate-of-the-art%2520methods%2520on%2520N24News%252C%2520ROSMAP%252C%2520and%2520BRCA%252C%2520while%2520achieving%250Acomparable%2520performance%2520on%2520Food-101.%2520Our%2520work%2520highlights%2520the%2520significance%2520of%250Alearning%2520shared%2520relations%2520for%2520robust%2520multimodal%2520learning%252C%2520opening%2520up%2520promising%250Aavenues%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Shared%20Relations%20via%20Multimodal%20Mixup%20Contrastive%20Learning%0A%20%20for%20Multimodal%20Classification&entry.906535625=Raja%20Kumar%20and%20Raghav%20Singhal%20and%20Pranamya%20Kulkarni%20and%20Deval%20Mehta%20and%20Kshitij%20Jadhav&entry.1292438233=%20%20Deep%20multimodal%20learning%20has%20shown%20remarkable%20success%20by%20leveraging%0Acontrastive%20learning%20to%20capture%20explicit%20one-to-one%20relations%20across%0Amodalities.%20However%2C%20real-world%20data%20often%20exhibits%20shared%20relations%20beyond%0Asimple%20pairwise%20associations.%20We%20propose%20M3CoL%2C%20a%20Multimodal%20Mixup%20Contrastive%0ALearning%20approach%20to%20capture%20nuanced%20shared%20relations%20inherent%20in%20multimodal%0Adata.%20Our%20key%20contribution%20is%20a%20Mixup-based%20contrastive%20loss%20that%20learns%20robust%0Arepresentations%20by%20aligning%20mixed%20samples%20from%20one%20modality%20with%20their%0Acorresponding%20samples%20from%20other%20modalities%20thereby%20capturing%20shared%20relations%0Abetween%20them.%20For%20multimodal%20classification%20tasks%2C%20we%20introduce%20a%20framework%0Athat%20integrates%20a%20fusion%20module%20with%20unimodal%20prediction%20modules%20for%20auxiliary%0Asupervision%20during%20training%2C%20complemented%20by%20our%20proposed%20Mixup-based%0Acontrastive%20loss.%20Through%20extensive%20experiments%20on%20diverse%20datasets%20%28N24News%2C%0AROSMAP%2C%20BRCA%2C%20and%20Food-101%29%2C%20we%20demonstrate%20that%20M3CoL%20effectively%20captures%0Ashared%20multimodal%20relations%20and%20generalizes%20across%20domains.%20It%20outperforms%0Astate-of-the-art%20methods%20on%20N24News%2C%20ROSMAP%2C%20and%20BRCA%2C%20while%20achieving%0Acomparable%20performance%20on%20Food-101.%20Our%20work%20highlights%20the%20significance%20of%0Alearning%20shared%20relations%20for%20robust%20multimodal%20learning%2C%20opening%20up%20promising%0Aavenues%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17777v1&entry.124074799=Read"},
{"title": "LGFN: Lightweight Light Field Image Super-Resolution using Local\n  Convolution Modulation and Global Attention Feature Extraction", "author": "Zhongxin Yu and Liang Chen and Zhiyun Zeng and Kunping Yang and Shaofei Luo and Shaorui Chen and Cheng Zhong", "abstract": "  Capturing different intensity and directions of light rays at the same scene\nLight field (LF) can encode the 3D scene cues into a 4D LF image which has a\nwide range of applications (i.e. post-capture refocusing and depth sensing). LF\nimage super-resolution (SR) aims to improve the image resolution limited by the\nperformance of LF camera sensor. Although existing methods have achieved\npromising results the practical application of these models is limited because\nthey are not lightweight enough. In this paper we propose a lightweight model\nnamed LGFN which integrates the local and global features of different views\nand the features of different channels for LF image SR. Specifically owing to\nneighboring regions of the same pixel position in different sub-aperture images\nexhibit similar structural relationships we design a lightweight CNN-based\nfeature extraction module (namely DGCE) to extract local features better\nthrough feature modulation. Meanwhile as the position beyond the boundaries in\nthe LF image presents a large disparity we propose an efficient spatial\nattention module (namely ESAM) which uses decomposable large-kernel convolution\nto obtain an enlarged receptive field and an efficient channel attention module\n(namely ECAM). Compared with the existing LF image SR models with large\nparameter our model has a parameter of 0.45M and a FLOPs of 19.33G which has\nachieved a competitive effect. Extensive experiments with ablation studies\ndemonstrate the effectiveness of our proposed method which ranked the second\nplace in the Track 2 Fidelity & Efficiency of NTIRE2024 Light Field Super\nResolution Challenge and the seventh place in the Track 1 Fidelity.\n", "link": "http://arxiv.org/abs/2409.17759v1", "date": "2024-09-26", "relevancy": 2.2377, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5681}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5567}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGFN%3A%20Lightweight%20Light%20Field%20Image%20Super-Resolution%20using%20Local%0A%20%20Convolution%20Modulation%20and%20Global%20Attention%20Feature%20Extraction&body=Title%3A%20LGFN%3A%20Lightweight%20Light%20Field%20Image%20Super-Resolution%20using%20Local%0A%20%20Convolution%20Modulation%20and%20Global%20Attention%20Feature%20Extraction%0AAuthor%3A%20Zhongxin%20Yu%20and%20Liang%20Chen%20and%20Zhiyun%20Zeng%20and%20Kunping%20Yang%20and%20Shaofei%20Luo%20and%20Shaorui%20Chen%20and%20Cheng%20Zhong%0AAbstract%3A%20%20%20Capturing%20different%20intensity%20and%20directions%20of%20light%20rays%20at%20the%20same%20scene%0ALight%20field%20%28LF%29%20can%20encode%20the%203D%20scene%20cues%20into%20a%204D%20LF%20image%20which%20has%20a%0Awide%20range%20of%20applications%20%28i.e.%20post-capture%20refocusing%20and%20depth%20sensing%29.%20LF%0Aimage%20super-resolution%20%28SR%29%20aims%20to%20improve%20the%20image%20resolution%20limited%20by%20the%0Aperformance%20of%20LF%20camera%20sensor.%20Although%20existing%20methods%20have%20achieved%0Apromising%20results%20the%20practical%20application%20of%20these%20models%20is%20limited%20because%0Athey%20are%20not%20lightweight%20enough.%20In%20this%20paper%20we%20propose%20a%20lightweight%20model%0Anamed%20LGFN%20which%20integrates%20the%20local%20and%20global%20features%20of%20different%20views%0Aand%20the%20features%20of%20different%20channels%20for%20LF%20image%20SR.%20Specifically%20owing%20to%0Aneighboring%20regions%20of%20the%20same%20pixel%20position%20in%20different%20sub-aperture%20images%0Aexhibit%20similar%20structural%20relationships%20we%20design%20a%20lightweight%20CNN-based%0Afeature%20extraction%20module%20%28namely%20DGCE%29%20to%20extract%20local%20features%20better%0Athrough%20feature%20modulation.%20Meanwhile%20as%20the%20position%20beyond%20the%20boundaries%20in%0Athe%20LF%20image%20presents%20a%20large%20disparity%20we%20propose%20an%20efficient%20spatial%0Aattention%20module%20%28namely%20ESAM%29%20which%20uses%20decomposable%20large-kernel%20convolution%0Ato%20obtain%20an%20enlarged%20receptive%20field%20and%20an%20efficient%20channel%20attention%20module%0A%28namely%20ECAM%29.%20Compared%20with%20the%20existing%20LF%20image%20SR%20models%20with%20large%0Aparameter%20our%20model%20has%20a%20parameter%20of%200.45M%20and%20a%20FLOPs%20of%2019.33G%20which%20has%0Aachieved%20a%20competitive%20effect.%20Extensive%20experiments%20with%20ablation%20studies%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20which%20ranked%20the%20second%0Aplace%20in%20the%20Track%202%20Fidelity%20%26%20Efficiency%20of%20NTIRE2024%20Light%20Field%20Super%0AResolution%20Challenge%20and%20the%20seventh%20place%20in%20the%20Track%201%20Fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGFN%253A%2520Lightweight%2520Light%2520Field%2520Image%2520Super-Resolution%2520using%2520Local%250A%2520%2520Convolution%2520Modulation%2520and%2520Global%2520Attention%2520Feature%2520Extraction%26entry.906535625%3DZhongxin%2520Yu%2520and%2520Liang%2520Chen%2520and%2520Zhiyun%2520Zeng%2520and%2520Kunping%2520Yang%2520and%2520Shaofei%2520Luo%2520and%2520Shaorui%2520Chen%2520and%2520Cheng%2520Zhong%26entry.1292438233%3D%2520%2520Capturing%2520different%2520intensity%2520and%2520directions%2520of%2520light%2520rays%2520at%2520the%2520same%2520scene%250ALight%2520field%2520%2528LF%2529%2520can%2520encode%2520the%25203D%2520scene%2520cues%2520into%2520a%25204D%2520LF%2520image%2520which%2520has%2520a%250Awide%2520range%2520of%2520applications%2520%2528i.e.%2520post-capture%2520refocusing%2520and%2520depth%2520sensing%2529.%2520LF%250Aimage%2520super-resolution%2520%2528SR%2529%2520aims%2520to%2520improve%2520the%2520image%2520resolution%2520limited%2520by%2520the%250Aperformance%2520of%2520LF%2520camera%2520sensor.%2520Although%2520existing%2520methods%2520have%2520achieved%250Apromising%2520results%2520the%2520practical%2520application%2520of%2520these%2520models%2520is%2520limited%2520because%250Athey%2520are%2520not%2520lightweight%2520enough.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520lightweight%2520model%250Anamed%2520LGFN%2520which%2520integrates%2520the%2520local%2520and%2520global%2520features%2520of%2520different%2520views%250Aand%2520the%2520features%2520of%2520different%2520channels%2520for%2520LF%2520image%2520SR.%2520Specifically%2520owing%2520to%250Aneighboring%2520regions%2520of%2520the%2520same%2520pixel%2520position%2520in%2520different%2520sub-aperture%2520images%250Aexhibit%2520similar%2520structural%2520relationships%2520we%2520design%2520a%2520lightweight%2520CNN-based%250Afeature%2520extraction%2520module%2520%2528namely%2520DGCE%2529%2520to%2520extract%2520local%2520features%2520better%250Athrough%2520feature%2520modulation.%2520Meanwhile%2520as%2520the%2520position%2520beyond%2520the%2520boundaries%2520in%250Athe%2520LF%2520image%2520presents%2520a%2520large%2520disparity%2520we%2520propose%2520an%2520efficient%2520spatial%250Aattention%2520module%2520%2528namely%2520ESAM%2529%2520which%2520uses%2520decomposable%2520large-kernel%2520convolution%250Ato%2520obtain%2520an%2520enlarged%2520receptive%2520field%2520and%2520an%2520efficient%2520channel%2520attention%2520module%250A%2528namely%2520ECAM%2529.%2520Compared%2520with%2520the%2520existing%2520LF%2520image%2520SR%2520models%2520with%2520large%250Aparameter%2520our%2520model%2520has%2520a%2520parameter%2520of%25200.45M%2520and%2520a%2520FLOPs%2520of%252019.33G%2520which%2520has%250Aachieved%2520a%2520competitive%2520effect.%2520Extensive%2520experiments%2520with%2520ablation%2520studies%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%2520which%2520ranked%2520the%2520second%250Aplace%2520in%2520the%2520Track%25202%2520Fidelity%2520%2526%2520Efficiency%2520of%2520NTIRE2024%2520Light%2520Field%2520Super%250AResolution%2520Challenge%2520and%2520the%2520seventh%2520place%2520in%2520the%2520Track%25201%2520Fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGFN%3A%20Lightweight%20Light%20Field%20Image%20Super-Resolution%20using%20Local%0A%20%20Convolution%20Modulation%20and%20Global%20Attention%20Feature%20Extraction&entry.906535625=Zhongxin%20Yu%20and%20Liang%20Chen%20and%20Zhiyun%20Zeng%20and%20Kunping%20Yang%20and%20Shaofei%20Luo%20and%20Shaorui%20Chen%20and%20Cheng%20Zhong&entry.1292438233=%20%20Capturing%20different%20intensity%20and%20directions%20of%20light%20rays%20at%20the%20same%20scene%0ALight%20field%20%28LF%29%20can%20encode%20the%203D%20scene%20cues%20into%20a%204D%20LF%20image%20which%20has%20a%0Awide%20range%20of%20applications%20%28i.e.%20post-capture%20refocusing%20and%20depth%20sensing%29.%20LF%0Aimage%20super-resolution%20%28SR%29%20aims%20to%20improve%20the%20image%20resolution%20limited%20by%20the%0Aperformance%20of%20LF%20camera%20sensor.%20Although%20existing%20methods%20have%20achieved%0Apromising%20results%20the%20practical%20application%20of%20these%20models%20is%20limited%20because%0Athey%20are%20not%20lightweight%20enough.%20In%20this%20paper%20we%20propose%20a%20lightweight%20model%0Anamed%20LGFN%20which%20integrates%20the%20local%20and%20global%20features%20of%20different%20views%0Aand%20the%20features%20of%20different%20channels%20for%20LF%20image%20SR.%20Specifically%20owing%20to%0Aneighboring%20regions%20of%20the%20same%20pixel%20position%20in%20different%20sub-aperture%20images%0Aexhibit%20similar%20structural%20relationships%20we%20design%20a%20lightweight%20CNN-based%0Afeature%20extraction%20module%20%28namely%20DGCE%29%20to%20extract%20local%20features%20better%0Athrough%20feature%20modulation.%20Meanwhile%20as%20the%20position%20beyond%20the%20boundaries%20in%0Athe%20LF%20image%20presents%20a%20large%20disparity%20we%20propose%20an%20efficient%20spatial%0Aattention%20module%20%28namely%20ESAM%29%20which%20uses%20decomposable%20large-kernel%20convolution%0Ato%20obtain%20an%20enlarged%20receptive%20field%20and%20an%20efficient%20channel%20attention%20module%0A%28namely%20ECAM%29.%20Compared%20with%20the%20existing%20LF%20image%20SR%20models%20with%20large%0Aparameter%20our%20model%20has%20a%20parameter%20of%200.45M%20and%20a%20FLOPs%20of%2019.33G%20which%20has%0Aachieved%20a%20competitive%20effect.%20Extensive%20experiments%20with%20ablation%20studies%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20which%20ranked%20the%20second%0Aplace%20in%20the%20Track%202%20Fidelity%20%26%20Efficiency%20of%20NTIRE2024%20Light%20Field%20Super%0AResolution%20Challenge%20and%20the%20seventh%20place%20in%20the%20Track%201%20Fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17759v1&entry.124074799=Read"},
{"title": "Revisiting Acoustic Similarity in Emotional Speech and Music via\n  Self-Supervised Representations", "author": "Yujia Sun and Zeyu Zhao and Korin Richmond and Yuanchao Li", "abstract": "  Emotion recognition from speech and music shares similarities due to their\nacoustic overlap, which has led to interest in transferring knowledge between\nthese domains. However, the shared acoustic cues between speech and music,\nparticularly those encoded by Self-Supervised Learning (SSL) models, remain\nlargely unexplored, given the fact that SSL models for speech and music have\nrarely been applied in cross-domain research. In this work, we revisit the\nacoustic similarity between emotion speech and music, starting with an analysis\nof the layerwise behavior of SSL models for Speech Emotion Recognition (SER)\nand Music Emotion Recognition (MER). Furthermore, we perform cross-domain\nadaptation by comparing several approaches in a two-stage fine-tuning process,\nexamining effective ways to utilize music for SER and speech for MER. Lastly,\nwe explore the acoustic similarities between emotional speech and music using\nFrechet audio distance for individual emotions, uncovering the issue of emotion\nbias in both speech and music SSL models. Our findings reveal that while speech\nand music SSL models do capture shared acoustic features, their behaviors can\nvary depending on different emotions due to their training strategies and\ndomain-specificities. Additionally, parameter-efficient fine-tuning can enhance\nSER and MER performance by leveraging knowledge from each other. This study\nprovides new insights into the acoustic similarity between emotional speech and\nmusic, and highlights the potential for cross-domain generalization to improve\nSER and MER systems.\n", "link": "http://arxiv.org/abs/2409.17899v1", "date": "2024-09-26", "relevancy": 2.2205, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4443}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Acoustic%20Similarity%20in%20Emotional%20Speech%20and%20Music%20via%0A%20%20Self-Supervised%20Representations&body=Title%3A%20Revisiting%20Acoustic%20Similarity%20in%20Emotional%20Speech%20and%20Music%20via%0A%20%20Self-Supervised%20Representations%0AAuthor%3A%20Yujia%20Sun%20and%20Zeyu%20Zhao%20and%20Korin%20Richmond%20and%20Yuanchao%20Li%0AAbstract%3A%20%20%20Emotion%20recognition%20from%20speech%20and%20music%20shares%20similarities%20due%20to%20their%0Aacoustic%20overlap%2C%20which%20has%20led%20to%20interest%20in%20transferring%20knowledge%20between%0Athese%20domains.%20However%2C%20the%20shared%20acoustic%20cues%20between%20speech%20and%20music%2C%0Aparticularly%20those%20encoded%20by%20Self-Supervised%20Learning%20%28SSL%29%20models%2C%20remain%0Alargely%20unexplored%2C%20given%20the%20fact%20that%20SSL%20models%20for%20speech%20and%20music%20have%0Ararely%20been%20applied%20in%20cross-domain%20research.%20In%20this%20work%2C%20we%20revisit%20the%0Aacoustic%20similarity%20between%20emotion%20speech%20and%20music%2C%20starting%20with%20an%20analysis%0Aof%20the%20layerwise%20behavior%20of%20SSL%20models%20for%20Speech%20Emotion%20Recognition%20%28SER%29%0Aand%20Music%20Emotion%20Recognition%20%28MER%29.%20Furthermore%2C%20we%20perform%20cross-domain%0Aadaptation%20by%20comparing%20several%20approaches%20in%20a%20two-stage%20fine-tuning%20process%2C%0Aexamining%20effective%20ways%20to%20utilize%20music%20for%20SER%20and%20speech%20for%20MER.%20Lastly%2C%0Awe%20explore%20the%20acoustic%20similarities%20between%20emotional%20speech%20and%20music%20using%0AFrechet%20audio%20distance%20for%20individual%20emotions%2C%20uncovering%20the%20issue%20of%20emotion%0Abias%20in%20both%20speech%20and%20music%20SSL%20models.%20Our%20findings%20reveal%20that%20while%20speech%0Aand%20music%20SSL%20models%20do%20capture%20shared%20acoustic%20features%2C%20their%20behaviors%20can%0Avary%20depending%20on%20different%20emotions%20due%20to%20their%20training%20strategies%20and%0Adomain-specificities.%20Additionally%2C%20parameter-efficient%20fine-tuning%20can%20enhance%0ASER%20and%20MER%20performance%20by%20leveraging%20knowledge%20from%20each%20other.%20This%20study%0Aprovides%20new%20insights%20into%20the%20acoustic%20similarity%20between%20emotional%20speech%20and%0Amusic%2C%20and%20highlights%20the%20potential%20for%20cross-domain%20generalization%20to%20improve%0ASER%20and%20MER%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Acoustic%2520Similarity%2520in%2520Emotional%2520Speech%2520and%2520Music%2520via%250A%2520%2520Self-Supervised%2520Representations%26entry.906535625%3DYujia%2520Sun%2520and%2520Zeyu%2520Zhao%2520and%2520Korin%2520Richmond%2520and%2520Yuanchao%2520Li%26entry.1292438233%3D%2520%2520Emotion%2520recognition%2520from%2520speech%2520and%2520music%2520shares%2520similarities%2520due%2520to%2520their%250Aacoustic%2520overlap%252C%2520which%2520has%2520led%2520to%2520interest%2520in%2520transferring%2520knowledge%2520between%250Athese%2520domains.%2520However%252C%2520the%2520shared%2520acoustic%2520cues%2520between%2520speech%2520and%2520music%252C%250Aparticularly%2520those%2520encoded%2520by%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520models%252C%2520remain%250Alargely%2520unexplored%252C%2520given%2520the%2520fact%2520that%2520SSL%2520models%2520for%2520speech%2520and%2520music%2520have%250Ararely%2520been%2520applied%2520in%2520cross-domain%2520research.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%250Aacoustic%2520similarity%2520between%2520emotion%2520speech%2520and%2520music%252C%2520starting%2520with%2520an%2520analysis%250Aof%2520the%2520layerwise%2520behavior%2520of%2520SSL%2520models%2520for%2520Speech%2520Emotion%2520Recognition%2520%2528SER%2529%250Aand%2520Music%2520Emotion%2520Recognition%2520%2528MER%2529.%2520Furthermore%252C%2520we%2520perform%2520cross-domain%250Aadaptation%2520by%2520comparing%2520several%2520approaches%2520in%2520a%2520two-stage%2520fine-tuning%2520process%252C%250Aexamining%2520effective%2520ways%2520to%2520utilize%2520music%2520for%2520SER%2520and%2520speech%2520for%2520MER.%2520Lastly%252C%250Awe%2520explore%2520the%2520acoustic%2520similarities%2520between%2520emotional%2520speech%2520and%2520music%2520using%250AFrechet%2520audio%2520distance%2520for%2520individual%2520emotions%252C%2520uncovering%2520the%2520issue%2520of%2520emotion%250Abias%2520in%2520both%2520speech%2520and%2520music%2520SSL%2520models.%2520Our%2520findings%2520reveal%2520that%2520while%2520speech%250Aand%2520music%2520SSL%2520models%2520do%2520capture%2520shared%2520acoustic%2520features%252C%2520their%2520behaviors%2520can%250Avary%2520depending%2520on%2520different%2520emotions%2520due%2520to%2520their%2520training%2520strategies%2520and%250Adomain-specificities.%2520Additionally%252C%2520parameter-efficient%2520fine-tuning%2520can%2520enhance%250ASER%2520and%2520MER%2520performance%2520by%2520leveraging%2520knowledge%2520from%2520each%2520other.%2520This%2520study%250Aprovides%2520new%2520insights%2520into%2520the%2520acoustic%2520similarity%2520between%2520emotional%2520speech%2520and%250Amusic%252C%2520and%2520highlights%2520the%2520potential%2520for%2520cross-domain%2520generalization%2520to%2520improve%250ASER%2520and%2520MER%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Acoustic%20Similarity%20in%20Emotional%20Speech%20and%20Music%20via%0A%20%20Self-Supervised%20Representations&entry.906535625=Yujia%20Sun%20and%20Zeyu%20Zhao%20and%20Korin%20Richmond%20and%20Yuanchao%20Li&entry.1292438233=%20%20Emotion%20recognition%20from%20speech%20and%20music%20shares%20similarities%20due%20to%20their%0Aacoustic%20overlap%2C%20which%20has%20led%20to%20interest%20in%20transferring%20knowledge%20between%0Athese%20domains.%20However%2C%20the%20shared%20acoustic%20cues%20between%20speech%20and%20music%2C%0Aparticularly%20those%20encoded%20by%20Self-Supervised%20Learning%20%28SSL%29%20models%2C%20remain%0Alargely%20unexplored%2C%20given%20the%20fact%20that%20SSL%20models%20for%20speech%20and%20music%20have%0Ararely%20been%20applied%20in%20cross-domain%20research.%20In%20this%20work%2C%20we%20revisit%20the%0Aacoustic%20similarity%20between%20emotion%20speech%20and%20music%2C%20starting%20with%20an%20analysis%0Aof%20the%20layerwise%20behavior%20of%20SSL%20models%20for%20Speech%20Emotion%20Recognition%20%28SER%29%0Aand%20Music%20Emotion%20Recognition%20%28MER%29.%20Furthermore%2C%20we%20perform%20cross-domain%0Aadaptation%20by%20comparing%20several%20approaches%20in%20a%20two-stage%20fine-tuning%20process%2C%0Aexamining%20effective%20ways%20to%20utilize%20music%20for%20SER%20and%20speech%20for%20MER.%20Lastly%2C%0Awe%20explore%20the%20acoustic%20similarities%20between%20emotional%20speech%20and%20music%20using%0AFrechet%20audio%20distance%20for%20individual%20emotions%2C%20uncovering%20the%20issue%20of%20emotion%0Abias%20in%20both%20speech%20and%20music%20SSL%20models.%20Our%20findings%20reveal%20that%20while%20speech%0Aand%20music%20SSL%20models%20do%20capture%20shared%20acoustic%20features%2C%20their%20behaviors%20can%0Avary%20depending%20on%20different%20emotions%20due%20to%20their%20training%20strategies%20and%0Adomain-specificities.%20Additionally%2C%20parameter-efficient%20fine-tuning%20can%20enhance%0ASER%20and%20MER%20performance%20by%20leveraging%20knowledge%20from%20each%20other.%20This%20study%0Aprovides%20new%20insights%20into%20the%20acoustic%20similarity%20between%20emotional%20speech%20and%0Amusic%2C%20and%20highlights%20the%20potential%20for%20cross-domain%20generalization%20to%20improve%0ASER%20and%20MER%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17899v1&entry.124074799=Read"},
{"title": "Predicting the Stay Length of Patients in Hospitals using Convolutional\n  Gated Recurrent Deep Learning Model", "author": "Mehdi Neshat and Michael Phipps and Chris A. Browne and Nicole T. Vargas and Seyedali Mirjalili", "abstract": "  Predicting hospital length of stay (LoS) stands as a critical factor in\nshaping public health strategies. This data serves as a cornerstone for\ngovernments to discern trends, patterns, and avenues for enhancing healthcare\ndelivery. In this study, we introduce a robust hybrid deep learning model, a\ncombination of Multi-layer Convolutional (CNNs) deep learning, Gated Recurrent\nUnits (GRU), and Dense neural networks, that outperforms 11 conventional and\nstate-of-the-art Machine Learning (ML) and Deep Learning (DL) methodologies in\naccurately forecasting inpatient hospital stay duration. Our investigation\ndelves into the implementation of this hybrid model, scrutinising variables\nlike geographic indicators tied to caregiving institutions, demographic markers\nencompassing patient ethnicity, race, and age, as well as medical attributes\nsuch as the CCS diagnosis code, APR DRG code, illness severity metrics, and\nhospital stay duration. Statistical evaluations reveal the pinnacle LoS\naccuracy achieved by our proposed model (CNN-GRU-DNN), which averages at 89%\nacross a 10-fold cross-validation test, surpassing LSTM, BiLSTM, GRU, and\nConvolutional Neural Networks (CNNs) by 19%, 18.2%, 18.6%, and 7%,\nrespectively. Accurate LoS predictions not only empower hospitals to optimise\nresource allocation and curb expenses associated with prolonged stays but also\npave the way for novel strategies in hospital stay management. This avenue\nholds promise for catalysing advancements in healthcare research and\ninnovation, inspiring a new era of precision-driven healthcare practices.\n", "link": "http://arxiv.org/abs/2409.17786v1", "date": "2024-09-26", "relevancy": 2.2184, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4529}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4513}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20the%20Stay%20Length%20of%20Patients%20in%20Hospitals%20using%20Convolutional%0A%20%20Gated%20Recurrent%20Deep%20Learning%20Model&body=Title%3A%20Predicting%20the%20Stay%20Length%20of%20Patients%20in%20Hospitals%20using%20Convolutional%0A%20%20Gated%20Recurrent%20Deep%20Learning%20Model%0AAuthor%3A%20Mehdi%20Neshat%20and%20Michael%20Phipps%20and%20Chris%20A.%20Browne%20and%20Nicole%20T.%20Vargas%20and%20Seyedali%20Mirjalili%0AAbstract%3A%20%20%20Predicting%20hospital%20length%20of%20stay%20%28LoS%29%20stands%20as%20a%20critical%20factor%20in%0Ashaping%20public%20health%20strategies.%20This%20data%20serves%20as%20a%20cornerstone%20for%0Agovernments%20to%20discern%20trends%2C%20patterns%2C%20and%20avenues%20for%20enhancing%20healthcare%0Adelivery.%20In%20this%20study%2C%20we%20introduce%20a%20robust%20hybrid%20deep%20learning%20model%2C%20a%0Acombination%20of%20Multi-layer%20Convolutional%20%28CNNs%29%20deep%20learning%2C%20Gated%20Recurrent%0AUnits%20%28GRU%29%2C%20and%20Dense%20neural%20networks%2C%20that%20outperforms%2011%20conventional%20and%0Astate-of-the-art%20Machine%20Learning%20%28ML%29%20and%20Deep%20Learning%20%28DL%29%20methodologies%20in%0Aaccurately%20forecasting%20inpatient%20hospital%20stay%20duration.%20Our%20investigation%0Adelves%20into%20the%20implementation%20of%20this%20hybrid%20model%2C%20scrutinising%20variables%0Alike%20geographic%20indicators%20tied%20to%20caregiving%20institutions%2C%20demographic%20markers%0Aencompassing%20patient%20ethnicity%2C%20race%2C%20and%20age%2C%20as%20well%20as%20medical%20attributes%0Asuch%20as%20the%20CCS%20diagnosis%20code%2C%20APR%20DRG%20code%2C%20illness%20severity%20metrics%2C%20and%0Ahospital%20stay%20duration.%20Statistical%20evaluations%20reveal%20the%20pinnacle%20LoS%0Aaccuracy%20achieved%20by%20our%20proposed%20model%20%28CNN-GRU-DNN%29%2C%20which%20averages%20at%2089%25%0Aacross%20a%2010-fold%20cross-validation%20test%2C%20surpassing%20LSTM%2C%20BiLSTM%2C%20GRU%2C%20and%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20by%2019%25%2C%2018.2%25%2C%2018.6%25%2C%20and%207%25%2C%0Arespectively.%20Accurate%20LoS%20predictions%20not%20only%20empower%20hospitals%20to%20optimise%0Aresource%20allocation%20and%20curb%20expenses%20associated%20with%20prolonged%20stays%20but%20also%0Apave%20the%20way%20for%20novel%20strategies%20in%20hospital%20stay%20management.%20This%20avenue%0Aholds%20promise%20for%20catalysing%20advancements%20in%20healthcare%20research%20and%0Ainnovation%2C%20inspiring%20a%20new%20era%20of%20precision-driven%20healthcare%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520the%2520Stay%2520Length%2520of%2520Patients%2520in%2520Hospitals%2520using%2520Convolutional%250A%2520%2520Gated%2520Recurrent%2520Deep%2520Learning%2520Model%26entry.906535625%3DMehdi%2520Neshat%2520and%2520Michael%2520Phipps%2520and%2520Chris%2520A.%2520Browne%2520and%2520Nicole%2520T.%2520Vargas%2520and%2520Seyedali%2520Mirjalili%26entry.1292438233%3D%2520%2520Predicting%2520hospital%2520length%2520of%2520stay%2520%2528LoS%2529%2520stands%2520as%2520a%2520critical%2520factor%2520in%250Ashaping%2520public%2520health%2520strategies.%2520This%2520data%2520serves%2520as%2520a%2520cornerstone%2520for%250Agovernments%2520to%2520discern%2520trends%252C%2520patterns%252C%2520and%2520avenues%2520for%2520enhancing%2520healthcare%250Adelivery.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520robust%2520hybrid%2520deep%2520learning%2520model%252C%2520a%250Acombination%2520of%2520Multi-layer%2520Convolutional%2520%2528CNNs%2529%2520deep%2520learning%252C%2520Gated%2520Recurrent%250AUnits%2520%2528GRU%2529%252C%2520and%2520Dense%2520neural%2520networks%252C%2520that%2520outperforms%252011%2520conventional%2520and%250Astate-of-the-art%2520Machine%2520Learning%2520%2528ML%2529%2520and%2520Deep%2520Learning%2520%2528DL%2529%2520methodologies%2520in%250Aaccurately%2520forecasting%2520inpatient%2520hospital%2520stay%2520duration.%2520Our%2520investigation%250Adelves%2520into%2520the%2520implementation%2520of%2520this%2520hybrid%2520model%252C%2520scrutinising%2520variables%250Alike%2520geographic%2520indicators%2520tied%2520to%2520caregiving%2520institutions%252C%2520demographic%2520markers%250Aencompassing%2520patient%2520ethnicity%252C%2520race%252C%2520and%2520age%252C%2520as%2520well%2520as%2520medical%2520attributes%250Asuch%2520as%2520the%2520CCS%2520diagnosis%2520code%252C%2520APR%2520DRG%2520code%252C%2520illness%2520severity%2520metrics%252C%2520and%250Ahospital%2520stay%2520duration.%2520Statistical%2520evaluations%2520reveal%2520the%2520pinnacle%2520LoS%250Aaccuracy%2520achieved%2520by%2520our%2520proposed%2520model%2520%2528CNN-GRU-DNN%2529%252C%2520which%2520averages%2520at%252089%2525%250Aacross%2520a%252010-fold%2520cross-validation%2520test%252C%2520surpassing%2520LSTM%252C%2520BiLSTM%252C%2520GRU%252C%2520and%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520by%252019%2525%252C%252018.2%2525%252C%252018.6%2525%252C%2520and%25207%2525%252C%250Arespectively.%2520Accurate%2520LoS%2520predictions%2520not%2520only%2520empower%2520hospitals%2520to%2520optimise%250Aresource%2520allocation%2520and%2520curb%2520expenses%2520associated%2520with%2520prolonged%2520stays%2520but%2520also%250Apave%2520the%2520way%2520for%2520novel%2520strategies%2520in%2520hospital%2520stay%2520management.%2520This%2520avenue%250Aholds%2520promise%2520for%2520catalysing%2520advancements%2520in%2520healthcare%2520research%2520and%250Ainnovation%252C%2520inspiring%2520a%2520new%2520era%2520of%2520precision-driven%2520healthcare%2520practices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20the%20Stay%20Length%20of%20Patients%20in%20Hospitals%20using%20Convolutional%0A%20%20Gated%20Recurrent%20Deep%20Learning%20Model&entry.906535625=Mehdi%20Neshat%20and%20Michael%20Phipps%20and%20Chris%20A.%20Browne%20and%20Nicole%20T.%20Vargas%20and%20Seyedali%20Mirjalili&entry.1292438233=%20%20Predicting%20hospital%20length%20of%20stay%20%28LoS%29%20stands%20as%20a%20critical%20factor%20in%0Ashaping%20public%20health%20strategies.%20This%20data%20serves%20as%20a%20cornerstone%20for%0Agovernments%20to%20discern%20trends%2C%20patterns%2C%20and%20avenues%20for%20enhancing%20healthcare%0Adelivery.%20In%20this%20study%2C%20we%20introduce%20a%20robust%20hybrid%20deep%20learning%20model%2C%20a%0Acombination%20of%20Multi-layer%20Convolutional%20%28CNNs%29%20deep%20learning%2C%20Gated%20Recurrent%0AUnits%20%28GRU%29%2C%20and%20Dense%20neural%20networks%2C%20that%20outperforms%2011%20conventional%20and%0Astate-of-the-art%20Machine%20Learning%20%28ML%29%20and%20Deep%20Learning%20%28DL%29%20methodologies%20in%0Aaccurately%20forecasting%20inpatient%20hospital%20stay%20duration.%20Our%20investigation%0Adelves%20into%20the%20implementation%20of%20this%20hybrid%20model%2C%20scrutinising%20variables%0Alike%20geographic%20indicators%20tied%20to%20caregiving%20institutions%2C%20demographic%20markers%0Aencompassing%20patient%20ethnicity%2C%20race%2C%20and%20age%2C%20as%20well%20as%20medical%20attributes%0Asuch%20as%20the%20CCS%20diagnosis%20code%2C%20APR%20DRG%20code%2C%20illness%20severity%20metrics%2C%20and%0Ahospital%20stay%20duration.%20Statistical%20evaluations%20reveal%20the%20pinnacle%20LoS%0Aaccuracy%20achieved%20by%20our%20proposed%20model%20%28CNN-GRU-DNN%29%2C%20which%20averages%20at%2089%25%0Aacross%20a%2010-fold%20cross-validation%20test%2C%20surpassing%20LSTM%2C%20BiLSTM%2C%20GRU%2C%20and%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20by%2019%25%2C%2018.2%25%2C%2018.6%25%2C%20and%207%25%2C%0Arespectively.%20Accurate%20LoS%20predictions%20not%20only%20empower%20hospitals%20to%20optimise%0Aresource%20allocation%20and%20curb%20expenses%20associated%20with%20prolonged%20stays%20but%20also%0Apave%20the%20way%20for%20novel%20strategies%20in%20hospital%20stay%20management.%20This%20avenue%0Aholds%20promise%20for%20catalysing%20advancements%20in%20healthcare%20research%20and%0Ainnovation%2C%20inspiring%20a%20new%20era%20of%20precision-driven%20healthcare%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17786v1&entry.124074799=Read"},
{"title": "Enhanced Unsupervised Image-to-Image Translation Using Contrastive\n  Learning and Histogram of Oriented Gradients", "author": "Wanchen Zhao", "abstract": "  Image-to-Image Translation is a vital area of computer vision that focuses on\ntransforming images from one visual domain to another while preserving their\ncore content and structure. However, this field faces two major challenges:\nfirst, the data from the two domains are often unpaired, making it difficult to\ntrain generative adversarial networks effectively; second, existing methods\ntend to produce artifacts or hallucinations during image generation, leading to\na decline in image quality. To address these issues, this paper proposes an\nenhanced unsupervised image-to-image translation method based on the\nContrastive Unpaired Translation (CUT) model, incorporating Histogram of\nOriented Gradients (HOG) features. This novel approach ensures the preservation\nof the semantic structure of images, even without semantic labels, by\nminimizing the loss between the HOG features of input and generated images. The\nmethod was tested on translating synthetic game environments from GTA5 dataset\nto realistic urban scenes in cityscapes dataset, demonstrating significant\nimprovements in reducing hallucinations and enhancing image quality.\n", "link": "http://arxiv.org/abs/2409.16042v2", "date": "2024-09-26", "relevancy": 2.2013, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5739}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5472}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Unsupervised%20Image-to-Image%20Translation%20Using%20Contrastive%0A%20%20Learning%20and%20Histogram%20of%20Oriented%20Gradients&body=Title%3A%20Enhanced%20Unsupervised%20Image-to-Image%20Translation%20Using%20Contrastive%0A%20%20Learning%20and%20Histogram%20of%20Oriented%20Gradients%0AAuthor%3A%20Wanchen%20Zhao%0AAbstract%3A%20%20%20Image-to-Image%20Translation%20is%20a%20vital%20area%20of%20computer%20vision%20that%20focuses%20on%0Atransforming%20images%20from%20one%20visual%20domain%20to%20another%20while%20preserving%20their%0Acore%20content%20and%20structure.%20However%2C%20this%20field%20faces%20two%20major%20challenges%3A%0Afirst%2C%20the%20data%20from%20the%20two%20domains%20are%20often%20unpaired%2C%20making%20it%20difficult%20to%0Atrain%20generative%20adversarial%20networks%20effectively%3B%20second%2C%20existing%20methods%0Atend%20to%20produce%20artifacts%20or%20hallucinations%20during%20image%20generation%2C%20leading%20to%0Aa%20decline%20in%20image%20quality.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20an%0Aenhanced%20unsupervised%20image-to-image%20translation%20method%20based%20on%20the%0AContrastive%20Unpaired%20Translation%20%28CUT%29%20model%2C%20incorporating%20Histogram%20of%0AOriented%20Gradients%20%28HOG%29%20features.%20This%20novel%20approach%20ensures%20the%20preservation%0Aof%20the%20semantic%20structure%20of%20images%2C%20even%20without%20semantic%20labels%2C%20by%0Aminimizing%20the%20loss%20between%20the%20HOG%20features%20of%20input%20and%20generated%20images.%20The%0Amethod%20was%20tested%20on%20translating%20synthetic%20game%20environments%20from%20GTA5%20dataset%0Ato%20realistic%20urban%20scenes%20in%20cityscapes%20dataset%2C%20demonstrating%20significant%0Aimprovements%20in%20reducing%20hallucinations%20and%20enhancing%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Unsupervised%2520Image-to-Image%2520Translation%2520Using%2520Contrastive%250A%2520%2520Learning%2520and%2520Histogram%2520of%2520Oriented%2520Gradients%26entry.906535625%3DWanchen%2520Zhao%26entry.1292438233%3D%2520%2520Image-to-Image%2520Translation%2520is%2520a%2520vital%2520area%2520of%2520computer%2520vision%2520that%2520focuses%2520on%250Atransforming%2520images%2520from%2520one%2520visual%2520domain%2520to%2520another%2520while%2520preserving%2520their%250Acore%2520content%2520and%2520structure.%2520However%252C%2520this%2520field%2520faces%2520two%2520major%2520challenges%253A%250Afirst%252C%2520the%2520data%2520from%2520the%2520two%2520domains%2520are%2520often%2520unpaired%252C%2520making%2520it%2520difficult%2520to%250Atrain%2520generative%2520adversarial%2520networks%2520effectively%253B%2520second%252C%2520existing%2520methods%250Atend%2520to%2520produce%2520artifacts%2520or%2520hallucinations%2520during%2520image%2520generation%252C%2520leading%2520to%250Aa%2520decline%2520in%2520image%2520quality.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520proposes%2520an%250Aenhanced%2520unsupervised%2520image-to-image%2520translation%2520method%2520based%2520on%2520the%250AContrastive%2520Unpaired%2520Translation%2520%2528CUT%2529%2520model%252C%2520incorporating%2520Histogram%2520of%250AOriented%2520Gradients%2520%2528HOG%2529%2520features.%2520This%2520novel%2520approach%2520ensures%2520the%2520preservation%250Aof%2520the%2520semantic%2520structure%2520of%2520images%252C%2520even%2520without%2520semantic%2520labels%252C%2520by%250Aminimizing%2520the%2520loss%2520between%2520the%2520HOG%2520features%2520of%2520input%2520and%2520generated%2520images.%2520The%250Amethod%2520was%2520tested%2520on%2520translating%2520synthetic%2520game%2520environments%2520from%2520GTA5%2520dataset%250Ato%2520realistic%2520urban%2520scenes%2520in%2520cityscapes%2520dataset%252C%2520demonstrating%2520significant%250Aimprovements%2520in%2520reducing%2520hallucinations%2520and%2520enhancing%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Unsupervised%20Image-to-Image%20Translation%20Using%20Contrastive%0A%20%20Learning%20and%20Histogram%20of%20Oriented%20Gradients&entry.906535625=Wanchen%20Zhao&entry.1292438233=%20%20Image-to-Image%20Translation%20is%20a%20vital%20area%20of%20computer%20vision%20that%20focuses%20on%0Atransforming%20images%20from%20one%20visual%20domain%20to%20another%20while%20preserving%20their%0Acore%20content%20and%20structure.%20However%2C%20this%20field%20faces%20two%20major%20challenges%3A%0Afirst%2C%20the%20data%20from%20the%20two%20domains%20are%20often%20unpaired%2C%20making%20it%20difficult%20to%0Atrain%20generative%20adversarial%20networks%20effectively%3B%20second%2C%20existing%20methods%0Atend%20to%20produce%20artifacts%20or%20hallucinations%20during%20image%20generation%2C%20leading%20to%0Aa%20decline%20in%20image%20quality.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20an%0Aenhanced%20unsupervised%20image-to-image%20translation%20method%20based%20on%20the%0AContrastive%20Unpaired%20Translation%20%28CUT%29%20model%2C%20incorporating%20Histogram%20of%0AOriented%20Gradients%20%28HOG%29%20features.%20This%20novel%20approach%20ensures%20the%20preservation%0Aof%20the%20semantic%20structure%20of%20images%2C%20even%20without%20semantic%20labels%2C%20by%0Aminimizing%20the%20loss%20between%20the%20HOG%20features%20of%20input%20and%20generated%20images.%20The%0Amethod%20was%20tested%20on%20translating%20synthetic%20game%20environments%20from%20GTA5%20dataset%0Ato%20realistic%20urban%20scenes%20in%20cityscapes%20dataset%2C%20demonstrating%20significant%0Aimprovements%20in%20reducing%20hallucinations%20and%20enhancing%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16042v2&entry.124074799=Read"},
{"title": "LingoQA: Visual Question Answering for Autonomous Driving", "author": "Ana-Maria Marcu and Long Chen and Jan H\u00fcnermann and Alice Karnsund and Benoit Hanotte and Prajwal Chidananda and Saurabh Nair and Vijay Badrinarayanan and Alex Kendall and Jamie Shotton and Elahe Arani and Oleg Sinavski", "abstract": "  We introduce LingoQA, a novel dataset and benchmark for visual question\nanswering in autonomous driving. The dataset contains 28K unique short video\nscenarios, and 419K annotations. Evaluating state-of-the-art vision-language\nmodels on our benchmark shows that their performance is below human\ncapabilities, with GPT-4V responding truthfully to 59.6% of the questions\ncompared to 96.6% for humans. For evaluation, we propose a truthfulness\nclassifier, called Lingo-Judge, that achieves a 0.95 Spearman correlation\ncoefficient to human evaluations, surpassing existing techniques like METEOR,\nBLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and run\nextensive ablation studies to understand its performance. We release our\ndataset and benchmark as an evaluation platform for vision-language models in\nautonomous driving.\n", "link": "http://arxiv.org/abs/2312.14115v4", "date": "2024-09-26", "relevancy": 2.1877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LingoQA%3A%20Visual%20Question%20Answering%20for%20Autonomous%20Driving&body=Title%3A%20LingoQA%3A%20Visual%20Question%20Answering%20for%20Autonomous%20Driving%0AAuthor%3A%20Ana-Maria%20Marcu%20and%20Long%20Chen%20and%20Jan%20H%C3%BCnermann%20and%20Alice%20Karnsund%20and%20Benoit%20Hanotte%20and%20Prajwal%20Chidananda%20and%20Saurabh%20Nair%20and%20Vijay%20Badrinarayanan%20and%20Alex%20Kendall%20and%20Jamie%20Shotton%20and%20Elahe%20Arani%20and%20Oleg%20Sinavski%0AAbstract%3A%20%20%20We%20introduce%20LingoQA%2C%20a%20novel%20dataset%20and%20benchmark%20for%20visual%20question%0Aanswering%20in%20autonomous%20driving.%20The%20dataset%20contains%2028K%20unique%20short%20video%0Ascenarios%2C%20and%20419K%20annotations.%20Evaluating%20state-of-the-art%20vision-language%0Amodels%20on%20our%20benchmark%20shows%20that%20their%20performance%20is%20below%20human%0Acapabilities%2C%20with%20GPT-4V%20responding%20truthfully%20to%2059.6%25%20of%20the%20questions%0Acompared%20to%2096.6%25%20for%20humans.%20For%20evaluation%2C%20we%20propose%20a%20truthfulness%0Aclassifier%2C%20called%20Lingo-Judge%2C%20that%20achieves%20a%200.95%20Spearman%20correlation%0Acoefficient%20to%20human%20evaluations%2C%20surpassing%20existing%20techniques%20like%20METEOR%2C%0ABLEU%2C%20CIDEr%2C%20and%20GPT-4.%20We%20establish%20a%20baseline%20vision-language%20model%20and%20run%0Aextensive%20ablation%20studies%20to%20understand%20its%20performance.%20We%20release%20our%0Adataset%20and%20benchmark%20as%20an%20evaluation%20platform%20for%20vision-language%20models%20in%0Aautonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14115v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLingoQA%253A%2520Visual%2520Question%2520Answering%2520for%2520Autonomous%2520Driving%26entry.906535625%3DAna-Maria%2520Marcu%2520and%2520Long%2520Chen%2520and%2520Jan%2520H%25C3%25BCnermann%2520and%2520Alice%2520Karnsund%2520and%2520Benoit%2520Hanotte%2520and%2520Prajwal%2520Chidananda%2520and%2520Saurabh%2520Nair%2520and%2520Vijay%2520Badrinarayanan%2520and%2520Alex%2520Kendall%2520and%2520Jamie%2520Shotton%2520and%2520Elahe%2520Arani%2520and%2520Oleg%2520Sinavski%26entry.1292438233%3D%2520%2520We%2520introduce%2520LingoQA%252C%2520a%2520novel%2520dataset%2520and%2520benchmark%2520for%2520visual%2520question%250Aanswering%2520in%2520autonomous%2520driving.%2520The%2520dataset%2520contains%252028K%2520unique%2520short%2520video%250Ascenarios%252C%2520and%2520419K%2520annotations.%2520Evaluating%2520state-of-the-art%2520vision-language%250Amodels%2520on%2520our%2520benchmark%2520shows%2520that%2520their%2520performance%2520is%2520below%2520human%250Acapabilities%252C%2520with%2520GPT-4V%2520responding%2520truthfully%2520to%252059.6%2525%2520of%2520the%2520questions%250Acompared%2520to%252096.6%2525%2520for%2520humans.%2520For%2520evaluation%252C%2520we%2520propose%2520a%2520truthfulness%250Aclassifier%252C%2520called%2520Lingo-Judge%252C%2520that%2520achieves%2520a%25200.95%2520Spearman%2520correlation%250Acoefficient%2520to%2520human%2520evaluations%252C%2520surpassing%2520existing%2520techniques%2520like%2520METEOR%252C%250ABLEU%252C%2520CIDEr%252C%2520and%2520GPT-4.%2520We%2520establish%2520a%2520baseline%2520vision-language%2520model%2520and%2520run%250Aextensive%2520ablation%2520studies%2520to%2520understand%2520its%2520performance.%2520We%2520release%2520our%250Adataset%2520and%2520benchmark%2520as%2520an%2520evaluation%2520platform%2520for%2520vision-language%2520models%2520in%250Aautonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14115v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LingoQA%3A%20Visual%20Question%20Answering%20for%20Autonomous%20Driving&entry.906535625=Ana-Maria%20Marcu%20and%20Long%20Chen%20and%20Jan%20H%C3%BCnermann%20and%20Alice%20Karnsund%20and%20Benoit%20Hanotte%20and%20Prajwal%20Chidananda%20and%20Saurabh%20Nair%20and%20Vijay%20Badrinarayanan%20and%20Alex%20Kendall%20and%20Jamie%20Shotton%20and%20Elahe%20Arani%20and%20Oleg%20Sinavski&entry.1292438233=%20%20We%20introduce%20LingoQA%2C%20a%20novel%20dataset%20and%20benchmark%20for%20visual%20question%0Aanswering%20in%20autonomous%20driving.%20The%20dataset%20contains%2028K%20unique%20short%20video%0Ascenarios%2C%20and%20419K%20annotations.%20Evaluating%20state-of-the-art%20vision-language%0Amodels%20on%20our%20benchmark%20shows%20that%20their%20performance%20is%20below%20human%0Acapabilities%2C%20with%20GPT-4V%20responding%20truthfully%20to%2059.6%25%20of%20the%20questions%0Acompared%20to%2096.6%25%20for%20humans.%20For%20evaluation%2C%20we%20propose%20a%20truthfulness%0Aclassifier%2C%20called%20Lingo-Judge%2C%20that%20achieves%20a%200.95%20Spearman%20correlation%0Acoefficient%20to%20human%20evaluations%2C%20surpassing%20existing%20techniques%20like%20METEOR%2C%0ABLEU%2C%20CIDEr%2C%20and%20GPT-4.%20We%20establish%20a%20baseline%20vision-language%20model%20and%20run%0Aextensive%20ablation%20studies%20to%20understand%20its%20performance.%20We%20release%20our%0Adataset%20and%20benchmark%20as%20an%20evaluation%20platform%20for%20vision-language%20models%20in%0Aautonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14115v4&entry.124074799=Read"},
{"title": "Reblurring-Guided Single Image Defocus Deblurring: A Learning Framework\n  with Misaligned Training Pairs", "author": "Xinya Shu and Yu Li and Dongwei Ren and Xiaohe Wu and Jin Li and Wangmeng Zuo", "abstract": "  For single image defocus deblurring, acquiring well-aligned training pairs\n(or training triplets), i.e., a defocus blurry image, an all-in-focus sharp\nimage (and a defocus blur map), is an intricate task for the development of\ndeblurring models. Existing image defocus deblurring methods typically rely on\ntraining data collected by specialized imaging equipment, presupposing that\nthese pairs or triplets are perfectly aligned. However, in practical scenarios\ninvolving the collection of real-world data, direct acquisition of training\ntriplets is infeasible, and training pairs inevitably encounter spatial\nmisalignment issues. In this work, we introduce a reblurring-guided learning\nframework for single image defocus deblurring, enabling the learning of a\ndeblurring network even with misaligned training pairs. Specifically, we first\npropose a baseline defocus deblurring network that utilizes spatially varying\ndefocus blur map as degradation prior to enhance the deblurring performance.\nThen, to effectively learn the baseline defocus deblurring network with\nmisaligned training pairs, our reblurring module ensures spatial consistency\nbetween the deblurred image, the reblurred image and the input blurry image by\nreconstructing spatially variant isotropic blur kernels. Moreover, the\nspatially variant blur derived from the reblurring module can serve as pseudo\nsupervision for defocus blur map during training, interestingly transforming\ntraining pairs into training triplets. Additionally, we have collected a new\ndataset specifically for single image defocus deblurring (SDD) with typical\nmisalignments, which not only substantiates our proposed method but also serves\nas a benchmark for future research.\n", "link": "http://arxiv.org/abs/2409.17792v1", "date": "2024-09-26", "relevancy": 2.1757, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5786}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reblurring-Guided%20Single%20Image%20Defocus%20Deblurring%3A%20A%20Learning%20Framework%0A%20%20with%20Misaligned%20Training%20Pairs&body=Title%3A%20Reblurring-Guided%20Single%20Image%20Defocus%20Deblurring%3A%20A%20Learning%20Framework%0A%20%20with%20Misaligned%20Training%20Pairs%0AAuthor%3A%20Xinya%20Shu%20and%20Yu%20Li%20and%20Dongwei%20Ren%20and%20Xiaohe%20Wu%20and%20Jin%20Li%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20For%20single%20image%20defocus%20deblurring%2C%20acquiring%20well-aligned%20training%20pairs%0A%28or%20training%20triplets%29%2C%20i.e.%2C%20a%20defocus%20blurry%20image%2C%20an%20all-in-focus%20sharp%0Aimage%20%28and%20a%20defocus%20blur%20map%29%2C%20is%20an%20intricate%20task%20for%20the%20development%20of%0Adeblurring%20models.%20Existing%20image%20defocus%20deblurring%20methods%20typically%20rely%20on%0Atraining%20data%20collected%20by%20specialized%20imaging%20equipment%2C%20presupposing%20that%0Athese%20pairs%20or%20triplets%20are%20perfectly%20aligned.%20However%2C%20in%20practical%20scenarios%0Ainvolving%20the%20collection%20of%20real-world%20data%2C%20direct%20acquisition%20of%20training%0Atriplets%20is%20infeasible%2C%20and%20training%20pairs%20inevitably%20encounter%20spatial%0Amisalignment%20issues.%20In%20this%20work%2C%20we%20introduce%20a%20reblurring-guided%20learning%0Aframework%20for%20single%20image%20defocus%20deblurring%2C%20enabling%20the%20learning%20of%20a%0Adeblurring%20network%20even%20with%20misaligned%20training%20pairs.%20Specifically%2C%20we%20first%0Apropose%20a%20baseline%20defocus%20deblurring%20network%20that%20utilizes%20spatially%20varying%0Adefocus%20blur%20map%20as%20degradation%20prior%20to%20enhance%20the%20deblurring%20performance.%0AThen%2C%20to%20effectively%20learn%20the%20baseline%20defocus%20deblurring%20network%20with%0Amisaligned%20training%20pairs%2C%20our%20reblurring%20module%20ensures%20spatial%20consistency%0Abetween%20the%20deblurred%20image%2C%20the%20reblurred%20image%20and%20the%20input%20blurry%20image%20by%0Areconstructing%20spatially%20variant%20isotropic%20blur%20kernels.%20Moreover%2C%20the%0Aspatially%20variant%20blur%20derived%20from%20the%20reblurring%20module%20can%20serve%20as%20pseudo%0Asupervision%20for%20defocus%20blur%20map%20during%20training%2C%20interestingly%20transforming%0Atraining%20pairs%20into%20training%20triplets.%20Additionally%2C%20we%20have%20collected%20a%20new%0Adataset%20specifically%20for%20single%20image%20defocus%20deblurring%20%28SDD%29%20with%20typical%0Amisalignments%2C%20which%20not%20only%20substantiates%20our%20proposed%20method%20but%20also%20serves%0Aas%20a%20benchmark%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReblurring-Guided%2520Single%2520Image%2520Defocus%2520Deblurring%253A%2520A%2520Learning%2520Framework%250A%2520%2520with%2520Misaligned%2520Training%2520Pairs%26entry.906535625%3DXinya%2520Shu%2520and%2520Yu%2520Li%2520and%2520Dongwei%2520Ren%2520and%2520Xiaohe%2520Wu%2520and%2520Jin%2520Li%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520For%2520single%2520image%2520defocus%2520deblurring%252C%2520acquiring%2520well-aligned%2520training%2520pairs%250A%2528or%2520training%2520triplets%2529%252C%2520i.e.%252C%2520a%2520defocus%2520blurry%2520image%252C%2520an%2520all-in-focus%2520sharp%250Aimage%2520%2528and%2520a%2520defocus%2520blur%2520map%2529%252C%2520is%2520an%2520intricate%2520task%2520for%2520the%2520development%2520of%250Adeblurring%2520models.%2520Existing%2520image%2520defocus%2520deblurring%2520methods%2520typically%2520rely%2520on%250Atraining%2520data%2520collected%2520by%2520specialized%2520imaging%2520equipment%252C%2520presupposing%2520that%250Athese%2520pairs%2520or%2520triplets%2520are%2520perfectly%2520aligned.%2520However%252C%2520in%2520practical%2520scenarios%250Ainvolving%2520the%2520collection%2520of%2520real-world%2520data%252C%2520direct%2520acquisition%2520of%2520training%250Atriplets%2520is%2520infeasible%252C%2520and%2520training%2520pairs%2520inevitably%2520encounter%2520spatial%250Amisalignment%2520issues.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520reblurring-guided%2520learning%250Aframework%2520for%2520single%2520image%2520defocus%2520deblurring%252C%2520enabling%2520the%2520learning%2520of%2520a%250Adeblurring%2520network%2520even%2520with%2520misaligned%2520training%2520pairs.%2520Specifically%252C%2520we%2520first%250Apropose%2520a%2520baseline%2520defocus%2520deblurring%2520network%2520that%2520utilizes%2520spatially%2520varying%250Adefocus%2520blur%2520map%2520as%2520degradation%2520prior%2520to%2520enhance%2520the%2520deblurring%2520performance.%250AThen%252C%2520to%2520effectively%2520learn%2520the%2520baseline%2520defocus%2520deblurring%2520network%2520with%250Amisaligned%2520training%2520pairs%252C%2520our%2520reblurring%2520module%2520ensures%2520spatial%2520consistency%250Abetween%2520the%2520deblurred%2520image%252C%2520the%2520reblurred%2520image%2520and%2520the%2520input%2520blurry%2520image%2520by%250Areconstructing%2520spatially%2520variant%2520isotropic%2520blur%2520kernels.%2520Moreover%252C%2520the%250Aspatially%2520variant%2520blur%2520derived%2520from%2520the%2520reblurring%2520module%2520can%2520serve%2520as%2520pseudo%250Asupervision%2520for%2520defocus%2520blur%2520map%2520during%2520training%252C%2520interestingly%2520transforming%250Atraining%2520pairs%2520into%2520training%2520triplets.%2520Additionally%252C%2520we%2520have%2520collected%2520a%2520new%250Adataset%2520specifically%2520for%2520single%2520image%2520defocus%2520deblurring%2520%2528SDD%2529%2520with%2520typical%250Amisalignments%252C%2520which%2520not%2520only%2520substantiates%2520our%2520proposed%2520method%2520but%2520also%2520serves%250Aas%2520a%2520benchmark%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reblurring-Guided%20Single%20Image%20Defocus%20Deblurring%3A%20A%20Learning%20Framework%0A%20%20with%20Misaligned%20Training%20Pairs&entry.906535625=Xinya%20Shu%20and%20Yu%20Li%20and%20Dongwei%20Ren%20and%20Xiaohe%20Wu%20and%20Jin%20Li%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20For%20single%20image%20defocus%20deblurring%2C%20acquiring%20well-aligned%20training%20pairs%0A%28or%20training%20triplets%29%2C%20i.e.%2C%20a%20defocus%20blurry%20image%2C%20an%20all-in-focus%20sharp%0Aimage%20%28and%20a%20defocus%20blur%20map%29%2C%20is%20an%20intricate%20task%20for%20the%20development%20of%0Adeblurring%20models.%20Existing%20image%20defocus%20deblurring%20methods%20typically%20rely%20on%0Atraining%20data%20collected%20by%20specialized%20imaging%20equipment%2C%20presupposing%20that%0Athese%20pairs%20or%20triplets%20are%20perfectly%20aligned.%20However%2C%20in%20practical%20scenarios%0Ainvolving%20the%20collection%20of%20real-world%20data%2C%20direct%20acquisition%20of%20training%0Atriplets%20is%20infeasible%2C%20and%20training%20pairs%20inevitably%20encounter%20spatial%0Amisalignment%20issues.%20In%20this%20work%2C%20we%20introduce%20a%20reblurring-guided%20learning%0Aframework%20for%20single%20image%20defocus%20deblurring%2C%20enabling%20the%20learning%20of%20a%0Adeblurring%20network%20even%20with%20misaligned%20training%20pairs.%20Specifically%2C%20we%20first%0Apropose%20a%20baseline%20defocus%20deblurring%20network%20that%20utilizes%20spatially%20varying%0Adefocus%20blur%20map%20as%20degradation%20prior%20to%20enhance%20the%20deblurring%20performance.%0AThen%2C%20to%20effectively%20learn%20the%20baseline%20defocus%20deblurring%20network%20with%0Amisaligned%20training%20pairs%2C%20our%20reblurring%20module%20ensures%20spatial%20consistency%0Abetween%20the%20deblurred%20image%2C%20the%20reblurred%20image%20and%20the%20input%20blurry%20image%20by%0Areconstructing%20spatially%20variant%20isotropic%20blur%20kernels.%20Moreover%2C%20the%0Aspatially%20variant%20blur%20derived%20from%20the%20reblurring%20module%20can%20serve%20as%20pseudo%0Asupervision%20for%20defocus%20blur%20map%20during%20training%2C%20interestingly%20transforming%0Atraining%20pairs%20into%20training%20triplets.%20Additionally%2C%20we%20have%20collected%20a%20new%0Adataset%20specifically%20for%20single%20image%20defocus%20deblurring%20%28SDD%29%20with%20typical%0Amisalignments%2C%20which%20not%20only%20substantiates%20our%20proposed%20method%20but%20also%20serves%0Aas%20a%20benchmark%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17792v1&entry.124074799=Read"},
{"title": "Computational Trichromacy Reconstruction: Empowering the Color-Vision\n  Deficient to Recognize Colors Using Augmented Reality", "author": "Yuhao Zhu and Ethan Chen and Colin Hascup and Yukang Yan and Gaurav Sharma", "abstract": "  We propose an assistive technology that helps individuals with Color Vision\nDeficiencies (CVD) to recognize/name colors. A dichromat's color perception is\na reduced two-dimensional (2D) subset of a normal trichromat's three\ndimensional color (3D) perception, leading to confusion when visual stimuli\nthat appear identical to the dichromat are referred to by different color\nnames. Using our proposed system, CVD individuals can interactively induce\ndistinct perceptual changes to originally confusing colors via a computational\ncolor space transformation. By combining their original 2D precepts for colors\nwith the discriminative changes, a three dimensional color space is\nreconstructed, where the dichromat can learn to resolve color name confusions\nand accurately recognize colors. Our system is implemented as an Augmented\nReality (AR) interface on smartphones, where users interactively control the\nrotation through swipe gestures and observe the induced color shifts in the\ncamera view or in a displayed image. Through psychophysical experiments and a\nlongitudinal user study, we demonstrate that such rotational color shifts have\ndiscriminative power (initially confusing colors become distinct under\nrotation) and exhibit structured perceptual shifts dichromats can learn with\nmodest training. The AR App is also evaluated in two real-world scenarios\n(building with lego blocks and interpreting artistic works); users all report\npositive experience in using the App to recognize object colors that they\notherwise could not.\n", "link": "http://arxiv.org/abs/2408.01895v2", "date": "2024-09-26", "relevancy": 2.1692, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5482}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computational%20Trichromacy%20Reconstruction%3A%20Empowering%20the%20Color-Vision%0A%20%20Deficient%20to%20Recognize%20Colors%20Using%20Augmented%20Reality&body=Title%3A%20Computational%20Trichromacy%20Reconstruction%3A%20Empowering%20the%20Color-Vision%0A%20%20Deficient%20to%20Recognize%20Colors%20Using%20Augmented%20Reality%0AAuthor%3A%20Yuhao%20Zhu%20and%20Ethan%20Chen%20and%20Colin%20Hascup%20and%20Yukang%20Yan%20and%20Gaurav%20Sharma%0AAbstract%3A%20%20%20We%20propose%20an%20assistive%20technology%20that%20helps%20individuals%20with%20Color%20Vision%0ADeficiencies%20%28CVD%29%20to%20recognize/name%20colors.%20A%20dichromat%27s%20color%20perception%20is%0Aa%20reduced%20two-dimensional%20%282D%29%20subset%20of%20a%20normal%20trichromat%27s%20three%0Adimensional%20color%20%283D%29%20perception%2C%20leading%20to%20confusion%20when%20visual%20stimuli%0Athat%20appear%20identical%20to%20the%20dichromat%20are%20referred%20to%20by%20different%20color%0Anames.%20Using%20our%20proposed%20system%2C%20CVD%20individuals%20can%20interactively%20induce%0Adistinct%20perceptual%20changes%20to%20originally%20confusing%20colors%20via%20a%20computational%0Acolor%20space%20transformation.%20By%20combining%20their%20original%202D%20precepts%20for%20colors%0Awith%20the%20discriminative%20changes%2C%20a%20three%20dimensional%20color%20space%20is%0Areconstructed%2C%20where%20the%20dichromat%20can%20learn%20to%20resolve%20color%20name%20confusions%0Aand%20accurately%20recognize%20colors.%20Our%20system%20is%20implemented%20as%20an%20Augmented%0AReality%20%28AR%29%20interface%20on%20smartphones%2C%20where%20users%20interactively%20control%20the%0Arotation%20through%20swipe%20gestures%20and%20observe%20the%20induced%20color%20shifts%20in%20the%0Acamera%20view%20or%20in%20a%20displayed%20image.%20Through%20psychophysical%20experiments%20and%20a%0Alongitudinal%20user%20study%2C%20we%20demonstrate%20that%20such%20rotational%20color%20shifts%20have%0Adiscriminative%20power%20%28initially%20confusing%20colors%20become%20distinct%20under%0Arotation%29%20and%20exhibit%20structured%20perceptual%20shifts%20dichromats%20can%20learn%20with%0Amodest%20training.%20The%20AR%20App%20is%20also%20evaluated%20in%20two%20real-world%20scenarios%0A%28building%20with%20lego%20blocks%20and%20interpreting%20artistic%20works%29%3B%20users%20all%20report%0Apositive%20experience%20in%20using%20the%20App%20to%20recognize%20object%20colors%20that%20they%0Aotherwise%20could%20not.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01895v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputational%2520Trichromacy%2520Reconstruction%253A%2520Empowering%2520the%2520Color-Vision%250A%2520%2520Deficient%2520to%2520Recognize%2520Colors%2520Using%2520Augmented%2520Reality%26entry.906535625%3DYuhao%2520Zhu%2520and%2520Ethan%2520Chen%2520and%2520Colin%2520Hascup%2520and%2520Yukang%2520Yan%2520and%2520Gaurav%2520Sharma%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520assistive%2520technology%2520that%2520helps%2520individuals%2520with%2520Color%2520Vision%250ADeficiencies%2520%2528CVD%2529%2520to%2520recognize/name%2520colors.%2520A%2520dichromat%2527s%2520color%2520perception%2520is%250Aa%2520reduced%2520two-dimensional%2520%25282D%2529%2520subset%2520of%2520a%2520normal%2520trichromat%2527s%2520three%250Adimensional%2520color%2520%25283D%2529%2520perception%252C%2520leading%2520to%2520confusion%2520when%2520visual%2520stimuli%250Athat%2520appear%2520identical%2520to%2520the%2520dichromat%2520are%2520referred%2520to%2520by%2520different%2520color%250Anames.%2520Using%2520our%2520proposed%2520system%252C%2520CVD%2520individuals%2520can%2520interactively%2520induce%250Adistinct%2520perceptual%2520changes%2520to%2520originally%2520confusing%2520colors%2520via%2520a%2520computational%250Acolor%2520space%2520transformation.%2520By%2520combining%2520their%2520original%25202D%2520precepts%2520for%2520colors%250Awith%2520the%2520discriminative%2520changes%252C%2520a%2520three%2520dimensional%2520color%2520space%2520is%250Areconstructed%252C%2520where%2520the%2520dichromat%2520can%2520learn%2520to%2520resolve%2520color%2520name%2520confusions%250Aand%2520accurately%2520recognize%2520colors.%2520Our%2520system%2520is%2520implemented%2520as%2520an%2520Augmented%250AReality%2520%2528AR%2529%2520interface%2520on%2520smartphones%252C%2520where%2520users%2520interactively%2520control%2520the%250Arotation%2520through%2520swipe%2520gestures%2520and%2520observe%2520the%2520induced%2520color%2520shifts%2520in%2520the%250Acamera%2520view%2520or%2520in%2520a%2520displayed%2520image.%2520Through%2520psychophysical%2520experiments%2520and%2520a%250Alongitudinal%2520user%2520study%252C%2520we%2520demonstrate%2520that%2520such%2520rotational%2520color%2520shifts%2520have%250Adiscriminative%2520power%2520%2528initially%2520confusing%2520colors%2520become%2520distinct%2520under%250Arotation%2529%2520and%2520exhibit%2520structured%2520perceptual%2520shifts%2520dichromats%2520can%2520learn%2520with%250Amodest%2520training.%2520The%2520AR%2520App%2520is%2520also%2520evaluated%2520in%2520two%2520real-world%2520scenarios%250A%2528building%2520with%2520lego%2520blocks%2520and%2520interpreting%2520artistic%2520works%2529%253B%2520users%2520all%2520report%250Apositive%2520experience%2520in%2520using%2520the%2520App%2520to%2520recognize%2520object%2520colors%2520that%2520they%250Aotherwise%2520could%2520not.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01895v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Trichromacy%20Reconstruction%3A%20Empowering%20the%20Color-Vision%0A%20%20Deficient%20to%20Recognize%20Colors%20Using%20Augmented%20Reality&entry.906535625=Yuhao%20Zhu%20and%20Ethan%20Chen%20and%20Colin%20Hascup%20and%20Yukang%20Yan%20and%20Gaurav%20Sharma&entry.1292438233=%20%20We%20propose%20an%20assistive%20technology%20that%20helps%20individuals%20with%20Color%20Vision%0ADeficiencies%20%28CVD%29%20to%20recognize/name%20colors.%20A%20dichromat%27s%20color%20perception%20is%0Aa%20reduced%20two-dimensional%20%282D%29%20subset%20of%20a%20normal%20trichromat%27s%20three%0Adimensional%20color%20%283D%29%20perception%2C%20leading%20to%20confusion%20when%20visual%20stimuli%0Athat%20appear%20identical%20to%20the%20dichromat%20are%20referred%20to%20by%20different%20color%0Anames.%20Using%20our%20proposed%20system%2C%20CVD%20individuals%20can%20interactively%20induce%0Adistinct%20perceptual%20changes%20to%20originally%20confusing%20colors%20via%20a%20computational%0Acolor%20space%20transformation.%20By%20combining%20their%20original%202D%20precepts%20for%20colors%0Awith%20the%20discriminative%20changes%2C%20a%20three%20dimensional%20color%20space%20is%0Areconstructed%2C%20where%20the%20dichromat%20can%20learn%20to%20resolve%20color%20name%20confusions%0Aand%20accurately%20recognize%20colors.%20Our%20system%20is%20implemented%20as%20an%20Augmented%0AReality%20%28AR%29%20interface%20on%20smartphones%2C%20where%20users%20interactively%20control%20the%0Arotation%20through%20swipe%20gestures%20and%20observe%20the%20induced%20color%20shifts%20in%20the%0Acamera%20view%20or%20in%20a%20displayed%20image.%20Through%20psychophysical%20experiments%20and%20a%0Alongitudinal%20user%20study%2C%20we%20demonstrate%20that%20such%20rotational%20color%20shifts%20have%0Adiscriminative%20power%20%28initially%20confusing%20colors%20become%20distinct%20under%0Arotation%29%20and%20exhibit%20structured%20perceptual%20shifts%20dichromats%20can%20learn%20with%0Amodest%20training.%20The%20AR%20App%20is%20also%20evaluated%20in%20two%20real-world%20scenarios%0A%28building%20with%20lego%20blocks%20and%20interpreting%20artistic%20works%29%3B%20users%20all%20report%0Apositive%20experience%20in%20using%20the%20App%20to%20recognize%20object%20colors%20that%20they%0Aotherwise%20could%20not.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01895v2&entry.124074799=Read"},
{"title": "Transfer Learning in $\\ell_1$ Regularized Regression: Hyperparameter\n  Selection Strategy based on Sharp Asymptotic Analysis", "author": "Koki Okajima and Tomoyuki Obuchi", "abstract": "  Transfer learning techniques aim to leverage information from multiple\nrelated datasets to enhance prediction quality against a target dataset. Such\nmethods have been adopted in the context of high-dimensional sparse regression,\nand some Lasso-based algorithms have been invented: Trans-Lasso and Pretraining\nLasso are such examples. These algorithms require the statistician to select\nhyperparameters that control the extent and type of information transfer from\nrelated datasets. However, selection strategies for these hyperparameters, as\nwell as the impact of these choices on the algorithm's performance, have been\nlargely unexplored. To address this, we conduct a thorough, precise study of\nthe algorithm in a high-dimensional setting via an asymptotic analysis using\nthe replica method. Our approach reveals a surprisingly simple behavior of the\nalgorithm: Ignoring one of the two types of information transferred to the\nfine-tuning stage has little effect on generalization performance, implying\nthat efforts for hyperparameter selection can be significantly reduced. Our\ntheoretical findings are also empirically supported by real-world applications\non the IMDb dataset.\n", "link": "http://arxiv.org/abs/2409.17704v1", "date": "2024-09-26", "relevancy": 2.1604, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4379}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4295}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20in%20%24%5Cell_1%24%20Regularized%20Regression%3A%20Hyperparameter%0A%20%20Selection%20Strategy%20based%20on%20Sharp%20Asymptotic%20Analysis&body=Title%3A%20Transfer%20Learning%20in%20%24%5Cell_1%24%20Regularized%20Regression%3A%20Hyperparameter%0A%20%20Selection%20Strategy%20based%20on%20Sharp%20Asymptotic%20Analysis%0AAuthor%3A%20Koki%20Okajima%20and%20Tomoyuki%20Obuchi%0AAbstract%3A%20%20%20Transfer%20learning%20techniques%20aim%20to%20leverage%20information%20from%20multiple%0Arelated%20datasets%20to%20enhance%20prediction%20quality%20against%20a%20target%20dataset.%20Such%0Amethods%20have%20been%20adopted%20in%20the%20context%20of%20high-dimensional%20sparse%20regression%2C%0Aand%20some%20Lasso-based%20algorithms%20have%20been%20invented%3A%20Trans-Lasso%20and%20Pretraining%0ALasso%20are%20such%20examples.%20These%20algorithms%20require%20the%20statistician%20to%20select%0Ahyperparameters%20that%20control%20the%20extent%20and%20type%20of%20information%20transfer%20from%0Arelated%20datasets.%20However%2C%20selection%20strategies%20for%20these%20hyperparameters%2C%20as%0Awell%20as%20the%20impact%20of%20these%20choices%20on%20the%20algorithm%27s%20performance%2C%20have%20been%0Alargely%20unexplored.%20To%20address%20this%2C%20we%20conduct%20a%20thorough%2C%20precise%20study%20of%0Athe%20algorithm%20in%20a%20high-dimensional%20setting%20via%20an%20asymptotic%20analysis%20using%0Athe%20replica%20method.%20Our%20approach%20reveals%20a%20surprisingly%20simple%20behavior%20of%20the%0Aalgorithm%3A%20Ignoring%20one%20of%20the%20two%20types%20of%20information%20transferred%20to%20the%0Afine-tuning%20stage%20has%20little%20effect%20on%20generalization%20performance%2C%20implying%0Athat%20efforts%20for%20hyperparameter%20selection%20can%20be%20significantly%20reduced.%20Our%0Atheoretical%20findings%20are%20also%20empirically%20supported%20by%20real-world%20applications%0Aon%20the%20IMDb%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520in%2520%2524%255Cell_1%2524%2520Regularized%2520Regression%253A%2520Hyperparameter%250A%2520%2520Selection%2520Strategy%2520based%2520on%2520Sharp%2520Asymptotic%2520Analysis%26entry.906535625%3DKoki%2520Okajima%2520and%2520Tomoyuki%2520Obuchi%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520techniques%2520aim%2520to%2520leverage%2520information%2520from%2520multiple%250Arelated%2520datasets%2520to%2520enhance%2520prediction%2520quality%2520against%2520a%2520target%2520dataset.%2520Such%250Amethods%2520have%2520been%2520adopted%2520in%2520the%2520context%2520of%2520high-dimensional%2520sparse%2520regression%252C%250Aand%2520some%2520Lasso-based%2520algorithms%2520have%2520been%2520invented%253A%2520Trans-Lasso%2520and%2520Pretraining%250ALasso%2520are%2520such%2520examples.%2520These%2520algorithms%2520require%2520the%2520statistician%2520to%2520select%250Ahyperparameters%2520that%2520control%2520the%2520extent%2520and%2520type%2520of%2520information%2520transfer%2520from%250Arelated%2520datasets.%2520However%252C%2520selection%2520strategies%2520for%2520these%2520hyperparameters%252C%2520as%250Awell%2520as%2520the%2520impact%2520of%2520these%2520choices%2520on%2520the%2520algorithm%2527s%2520performance%252C%2520have%2520been%250Alargely%2520unexplored.%2520To%2520address%2520this%252C%2520we%2520conduct%2520a%2520thorough%252C%2520precise%2520study%2520of%250Athe%2520algorithm%2520in%2520a%2520high-dimensional%2520setting%2520via%2520an%2520asymptotic%2520analysis%2520using%250Athe%2520replica%2520method.%2520Our%2520approach%2520reveals%2520a%2520surprisingly%2520simple%2520behavior%2520of%2520the%250Aalgorithm%253A%2520Ignoring%2520one%2520of%2520the%2520two%2520types%2520of%2520information%2520transferred%2520to%2520the%250Afine-tuning%2520stage%2520has%2520little%2520effect%2520on%2520generalization%2520performance%252C%2520implying%250Athat%2520efforts%2520for%2520hyperparameter%2520selection%2520can%2520be%2520significantly%2520reduced.%2520Our%250Atheoretical%2520findings%2520are%2520also%2520empirically%2520supported%2520by%2520real-world%2520applications%250Aon%2520the%2520IMDb%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20in%20%24%5Cell_1%24%20Regularized%20Regression%3A%20Hyperparameter%0A%20%20Selection%20Strategy%20based%20on%20Sharp%20Asymptotic%20Analysis&entry.906535625=Koki%20Okajima%20and%20Tomoyuki%20Obuchi&entry.1292438233=%20%20Transfer%20learning%20techniques%20aim%20to%20leverage%20information%20from%20multiple%0Arelated%20datasets%20to%20enhance%20prediction%20quality%20against%20a%20target%20dataset.%20Such%0Amethods%20have%20been%20adopted%20in%20the%20context%20of%20high-dimensional%20sparse%20regression%2C%0Aand%20some%20Lasso-based%20algorithms%20have%20been%20invented%3A%20Trans-Lasso%20and%20Pretraining%0ALasso%20are%20such%20examples.%20These%20algorithms%20require%20the%20statistician%20to%20select%0Ahyperparameters%20that%20control%20the%20extent%20and%20type%20of%20information%20transfer%20from%0Arelated%20datasets.%20However%2C%20selection%20strategies%20for%20these%20hyperparameters%2C%20as%0Awell%20as%20the%20impact%20of%20these%20choices%20on%20the%20algorithm%27s%20performance%2C%20have%20been%0Alargely%20unexplored.%20To%20address%20this%2C%20we%20conduct%20a%20thorough%2C%20precise%20study%20of%0Athe%20algorithm%20in%20a%20high-dimensional%20setting%20via%20an%20asymptotic%20analysis%20using%0Athe%20replica%20method.%20Our%20approach%20reveals%20a%20surprisingly%20simple%20behavior%20of%20the%0Aalgorithm%3A%20Ignoring%20one%20of%20the%20two%20types%20of%20information%20transferred%20to%20the%0Afine-tuning%20stage%20has%20little%20effect%20on%20generalization%20performance%2C%20implying%0Athat%20efforts%20for%20hyperparameter%20selection%20can%20be%20significantly%20reduced.%20Our%0Atheoretical%20findings%20are%20also%20empirically%20supported%20by%20real-world%20applications%0Aon%20the%20IMDb%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17704v1&entry.124074799=Read"},
{"title": "Generative Modeling of Molecular Dynamics Trajectories", "author": "Bowen Jing and Hannes St\u00e4rk and Tommi Jaakkola and Bonnie Berger", "abstract": "  Molecular dynamics (MD) is a powerful technique for studying microscopic\nphenomena, but its computational cost has driven significant interest in the\ndevelopment of deep learning-based surrogate models. We introduce generative\nmodeling of molecular trajectories as a paradigm for learning flexible\nmulti-task surrogate models of MD from data. By conditioning on appropriately\nchosen frames of the trajectory, we show such generative models can be adapted\nto diverse tasks such as forward simulation, transition path sampling, and\ntrajectory upsampling. By alternatively conditioning on part of the molecular\nsystem and inpainting the rest, we also demonstrate the first steps towards\ndynamics-conditioned molecular design. We validate the full set of these\ncapabilities on tetrapeptide simulations and show that our model can produce\nreasonable ensembles of protein monomers. Altogether, our work illustrates how\ngenerative modeling can unlock value from MD data towards diverse downstream\ntasks that are not straightforward to address with existing methods or even MD\nitself. Code is available at https://github.com/bjing2016/mdgen.\n", "link": "http://arxiv.org/abs/2409.17808v1", "date": "2024-09-26", "relevancy": 2.1566, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.544}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5388}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Modeling%20of%20Molecular%20Dynamics%20Trajectories&body=Title%3A%20Generative%20Modeling%20of%20Molecular%20Dynamics%20Trajectories%0AAuthor%3A%20Bowen%20Jing%20and%20Hannes%20St%C3%A4rk%20and%20Tommi%20Jaakkola%20and%20Bonnie%20Berger%0AAbstract%3A%20%20%20Molecular%20dynamics%20%28MD%29%20is%20a%20powerful%20technique%20for%20studying%20microscopic%0Aphenomena%2C%20but%20its%20computational%20cost%20has%20driven%20significant%20interest%20in%20the%0Adevelopment%20of%20deep%20learning-based%20surrogate%20models.%20We%20introduce%20generative%0Amodeling%20of%20molecular%20trajectories%20as%20a%20paradigm%20for%20learning%20flexible%0Amulti-task%20surrogate%20models%20of%20MD%20from%20data.%20By%20conditioning%20on%20appropriately%0Achosen%20frames%20of%20the%20trajectory%2C%20we%20show%20such%20generative%20models%20can%20be%20adapted%0Ato%20diverse%20tasks%20such%20as%20forward%20simulation%2C%20transition%20path%20sampling%2C%20and%0Atrajectory%20upsampling.%20By%20alternatively%20conditioning%20on%20part%20of%20the%20molecular%0Asystem%20and%20inpainting%20the%20rest%2C%20we%20also%20demonstrate%20the%20first%20steps%20towards%0Adynamics-conditioned%20molecular%20design.%20We%20validate%20the%20full%20set%20of%20these%0Acapabilities%20on%20tetrapeptide%20simulations%20and%20show%20that%20our%20model%20can%20produce%0Areasonable%20ensembles%20of%20protein%20monomers.%20Altogether%2C%20our%20work%20illustrates%20how%0Agenerative%20modeling%20can%20unlock%20value%20from%20MD%20data%20towards%20diverse%20downstream%0Atasks%20that%20are%20not%20straightforward%20to%20address%20with%20existing%20methods%20or%20even%20MD%0Aitself.%20Code%20is%20available%20at%20https%3A//github.com/bjing2016/mdgen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Modeling%2520of%2520Molecular%2520Dynamics%2520Trajectories%26entry.906535625%3DBowen%2520Jing%2520and%2520Hannes%2520St%25C3%25A4rk%2520and%2520Tommi%2520Jaakkola%2520and%2520Bonnie%2520Berger%26entry.1292438233%3D%2520%2520Molecular%2520dynamics%2520%2528MD%2529%2520is%2520a%2520powerful%2520technique%2520for%2520studying%2520microscopic%250Aphenomena%252C%2520but%2520its%2520computational%2520cost%2520has%2520driven%2520significant%2520interest%2520in%2520the%250Adevelopment%2520of%2520deep%2520learning-based%2520surrogate%2520models.%2520We%2520introduce%2520generative%250Amodeling%2520of%2520molecular%2520trajectories%2520as%2520a%2520paradigm%2520for%2520learning%2520flexible%250Amulti-task%2520surrogate%2520models%2520of%2520MD%2520from%2520data.%2520By%2520conditioning%2520on%2520appropriately%250Achosen%2520frames%2520of%2520the%2520trajectory%252C%2520we%2520show%2520such%2520generative%2520models%2520can%2520be%2520adapted%250Ato%2520diverse%2520tasks%2520such%2520as%2520forward%2520simulation%252C%2520transition%2520path%2520sampling%252C%2520and%250Atrajectory%2520upsampling.%2520By%2520alternatively%2520conditioning%2520on%2520part%2520of%2520the%2520molecular%250Asystem%2520and%2520inpainting%2520the%2520rest%252C%2520we%2520also%2520demonstrate%2520the%2520first%2520steps%2520towards%250Adynamics-conditioned%2520molecular%2520design.%2520We%2520validate%2520the%2520full%2520set%2520of%2520these%250Acapabilities%2520on%2520tetrapeptide%2520simulations%2520and%2520show%2520that%2520our%2520model%2520can%2520produce%250Areasonable%2520ensembles%2520of%2520protein%2520monomers.%2520Altogether%252C%2520our%2520work%2520illustrates%2520how%250Agenerative%2520modeling%2520can%2520unlock%2520value%2520from%2520MD%2520data%2520towards%2520diverse%2520downstream%250Atasks%2520that%2520are%2520not%2520straightforward%2520to%2520address%2520with%2520existing%2520methods%2520or%2520even%2520MD%250Aitself.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/bjing2016/mdgen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Modeling%20of%20Molecular%20Dynamics%20Trajectories&entry.906535625=Bowen%20Jing%20and%20Hannes%20St%C3%A4rk%20and%20Tommi%20Jaakkola%20and%20Bonnie%20Berger&entry.1292438233=%20%20Molecular%20dynamics%20%28MD%29%20is%20a%20powerful%20technique%20for%20studying%20microscopic%0Aphenomena%2C%20but%20its%20computational%20cost%20has%20driven%20significant%20interest%20in%20the%0Adevelopment%20of%20deep%20learning-based%20surrogate%20models.%20We%20introduce%20generative%0Amodeling%20of%20molecular%20trajectories%20as%20a%20paradigm%20for%20learning%20flexible%0Amulti-task%20surrogate%20models%20of%20MD%20from%20data.%20By%20conditioning%20on%20appropriately%0Achosen%20frames%20of%20the%20trajectory%2C%20we%20show%20such%20generative%20models%20can%20be%20adapted%0Ato%20diverse%20tasks%20such%20as%20forward%20simulation%2C%20transition%20path%20sampling%2C%20and%0Atrajectory%20upsampling.%20By%20alternatively%20conditioning%20on%20part%20of%20the%20molecular%0Asystem%20and%20inpainting%20the%20rest%2C%20we%20also%20demonstrate%20the%20first%20steps%20towards%0Adynamics-conditioned%20molecular%20design.%20We%20validate%20the%20full%20set%20of%20these%0Acapabilities%20on%20tetrapeptide%20simulations%20and%20show%20that%20our%20model%20can%20produce%0Areasonable%20ensembles%20of%20protein%20monomers.%20Altogether%2C%20our%20work%20illustrates%20how%0Agenerative%20modeling%20can%20unlock%20value%20from%20MD%20data%20towards%20diverse%20downstream%0Atasks%20that%20are%20not%20straightforward%20to%20address%20with%20existing%20methods%20or%20even%20MD%0Aitself.%20Code%20is%20available%20at%20https%3A//github.com/bjing2016/mdgen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17808v1&entry.124074799=Read"},
{"title": "Navigation in a simplified Urban Flow through Deep Reinforcement\n  Learning", "author": "Federica Tonti and Jean Rabault and Ricardo Vinuesa", "abstract": "  The increasing number of unmanned aerial vehicles (UAVs) in urban\nenvironments requires a strategy to minimize their environmental impact, both\nin terms of energy efficiency and noise reduction. In order to reduce these\nconcerns, novel strategies for developing prediction models and optimization of\nflight planning, for instance through deep reinforcement learning (DRL), are\nneeded. Our goal is to develop DRL algorithms capable of enabling the\nautonomous navigation of UAVs in urban environments, taking into account the\npresence of buildings and other UAVs, optimizing the trajectories in order to\nreduce both energetic consumption and noise. This is achieved using fluid-flow\nsimulations which represent the environment in which UAVs navigate and training\nthe UAV as an agent interacting with an urban environment. In this work, we\nconsider a domain domain represented by a two-dimensional flow field with\nobstacles, ideally representing buildings, extracted from a three-dimensional\nhigh-fidelity numerical simulation. The presented methodology, using PPO+LSTM\ncells, was validated by reproducing a simple but fundamental problem in\nnavigation, namely the Zermelo's problem, which deals with a vessel navigating\nin a turbulent flow, travelling from a starting point to a target location,\noptimizing the trajectory. The current method shows a significant improvement\nwith respect to both a simple PPO and a TD3 algorithm, with a success rate (SR)\nof the PPO+LSTM trained policy of 98.7%, and a crash rate (CR) of 0.1%,\noutperforming both PPO (SR = 75.6%, CR=18.6%) and TD3 (SR=77.4% and CR=14.5%).\nThis is the first step towards DRL strategies which will guide UAVs in a\nthree-dimensional flow field using real-time signals, making the navigation\nefficient in terms of flight time and avoiding damages to the vehicle.\n", "link": "http://arxiv.org/abs/2409.17922v1", "date": "2024-09-26", "relevancy": 2.1525, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigation%20in%20a%20simplified%20Urban%20Flow%20through%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20Navigation%20in%20a%20simplified%20Urban%20Flow%20through%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Federica%20Tonti%20and%20Jean%20Rabault%20and%20Ricardo%20Vinuesa%0AAbstract%3A%20%20%20The%20increasing%20number%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20urban%0Aenvironments%20requires%20a%20strategy%20to%20minimize%20their%20environmental%20impact%2C%20both%0Ain%20terms%20of%20energy%20efficiency%20and%20noise%20reduction.%20In%20order%20to%20reduce%20these%0Aconcerns%2C%20novel%20strategies%20for%20developing%20prediction%20models%20and%20optimization%20of%0Aflight%20planning%2C%20for%20instance%20through%20deep%20reinforcement%20learning%20%28DRL%29%2C%20are%0Aneeded.%20Our%20goal%20is%20to%20develop%20DRL%20algorithms%20capable%20of%20enabling%20the%0Aautonomous%20navigation%20of%20UAVs%20in%20urban%20environments%2C%20taking%20into%20account%20the%0Apresence%20of%20buildings%20and%20other%20UAVs%2C%20optimizing%20the%20trajectories%20in%20order%20to%0Areduce%20both%20energetic%20consumption%20and%20noise.%20This%20is%20achieved%20using%20fluid-flow%0Asimulations%20which%20represent%20the%20environment%20in%20which%20UAVs%20navigate%20and%20training%0Athe%20UAV%20as%20an%20agent%20interacting%20with%20an%20urban%20environment.%20In%20this%20work%2C%20we%0Aconsider%20a%20domain%20domain%20represented%20by%20a%20two-dimensional%20flow%20field%20with%0Aobstacles%2C%20ideally%20representing%20buildings%2C%20extracted%20from%20a%20three-dimensional%0Ahigh-fidelity%20numerical%20simulation.%20The%20presented%20methodology%2C%20using%20PPO%2BLSTM%0Acells%2C%20was%20validated%20by%20reproducing%20a%20simple%20but%20fundamental%20problem%20in%0Anavigation%2C%20namely%20the%20Zermelo%27s%20problem%2C%20which%20deals%20with%20a%20vessel%20navigating%0Ain%20a%20turbulent%20flow%2C%20travelling%20from%20a%20starting%20point%20to%20a%20target%20location%2C%0Aoptimizing%20the%20trajectory.%20The%20current%20method%20shows%20a%20significant%20improvement%0Awith%20respect%20to%20both%20a%20simple%20PPO%20and%20a%20TD3%20algorithm%2C%20with%20a%20success%20rate%20%28SR%29%0Aof%20the%20PPO%2BLSTM%20trained%20policy%20of%2098.7%25%2C%20and%20a%20crash%20rate%20%28CR%29%20of%200.1%25%2C%0Aoutperforming%20both%20PPO%20%28SR%20%3D%2075.6%25%2C%20CR%3D18.6%25%29%20and%20TD3%20%28SR%3D77.4%25%20and%20CR%3D14.5%25%29.%0AThis%20is%20the%20first%20step%20towards%20DRL%20strategies%20which%20will%20guide%20UAVs%20in%20a%0Athree-dimensional%20flow%20field%20using%20real-time%20signals%2C%20making%20the%20navigation%0Aefficient%20in%20terms%20of%20flight%20time%20and%20avoiding%20damages%20to%20the%20vehicle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigation%2520in%2520a%2520simplified%2520Urban%2520Flow%2520through%2520Deep%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DFederica%2520Tonti%2520and%2520Jean%2520Rabault%2520and%2520Ricardo%2520Vinuesa%26entry.1292438233%3D%2520%2520The%2520increasing%2520number%2520of%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520in%2520urban%250Aenvironments%2520requires%2520a%2520strategy%2520to%2520minimize%2520their%2520environmental%2520impact%252C%2520both%250Ain%2520terms%2520of%2520energy%2520efficiency%2520and%2520noise%2520reduction.%2520In%2520order%2520to%2520reduce%2520these%250Aconcerns%252C%2520novel%2520strategies%2520for%2520developing%2520prediction%2520models%2520and%2520optimization%2520of%250Aflight%2520planning%252C%2520for%2520instance%2520through%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%252C%2520are%250Aneeded.%2520Our%2520goal%2520is%2520to%2520develop%2520DRL%2520algorithms%2520capable%2520of%2520enabling%2520the%250Aautonomous%2520navigation%2520of%2520UAVs%2520in%2520urban%2520environments%252C%2520taking%2520into%2520account%2520the%250Apresence%2520of%2520buildings%2520and%2520other%2520UAVs%252C%2520optimizing%2520the%2520trajectories%2520in%2520order%2520to%250Areduce%2520both%2520energetic%2520consumption%2520and%2520noise.%2520This%2520is%2520achieved%2520using%2520fluid-flow%250Asimulations%2520which%2520represent%2520the%2520environment%2520in%2520which%2520UAVs%2520navigate%2520and%2520training%250Athe%2520UAV%2520as%2520an%2520agent%2520interacting%2520with%2520an%2520urban%2520environment.%2520In%2520this%2520work%252C%2520we%250Aconsider%2520a%2520domain%2520domain%2520represented%2520by%2520a%2520two-dimensional%2520flow%2520field%2520with%250Aobstacles%252C%2520ideally%2520representing%2520buildings%252C%2520extracted%2520from%2520a%2520three-dimensional%250Ahigh-fidelity%2520numerical%2520simulation.%2520The%2520presented%2520methodology%252C%2520using%2520PPO%252BLSTM%250Acells%252C%2520was%2520validated%2520by%2520reproducing%2520a%2520simple%2520but%2520fundamental%2520problem%2520in%250Anavigation%252C%2520namely%2520the%2520Zermelo%2527s%2520problem%252C%2520which%2520deals%2520with%2520a%2520vessel%2520navigating%250Ain%2520a%2520turbulent%2520flow%252C%2520travelling%2520from%2520a%2520starting%2520point%2520to%2520a%2520target%2520location%252C%250Aoptimizing%2520the%2520trajectory.%2520The%2520current%2520method%2520shows%2520a%2520significant%2520improvement%250Awith%2520respect%2520to%2520both%2520a%2520simple%2520PPO%2520and%2520a%2520TD3%2520algorithm%252C%2520with%2520a%2520success%2520rate%2520%2528SR%2529%250Aof%2520the%2520PPO%252BLSTM%2520trained%2520policy%2520of%252098.7%2525%252C%2520and%2520a%2520crash%2520rate%2520%2528CR%2529%2520of%25200.1%2525%252C%250Aoutperforming%2520both%2520PPO%2520%2528SR%2520%253D%252075.6%2525%252C%2520CR%253D18.6%2525%2529%2520and%2520TD3%2520%2528SR%253D77.4%2525%2520and%2520CR%253D14.5%2525%2529.%250AThis%2520is%2520the%2520first%2520step%2520towards%2520DRL%2520strategies%2520which%2520will%2520guide%2520UAVs%2520in%2520a%250Athree-dimensional%2520flow%2520field%2520using%2520real-time%2520signals%252C%2520making%2520the%2520navigation%250Aefficient%2520in%2520terms%2520of%2520flight%2520time%2520and%2520avoiding%2520damages%2520to%2520the%2520vehicle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigation%20in%20a%20simplified%20Urban%20Flow%20through%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Federica%20Tonti%20and%20Jean%20Rabault%20and%20Ricardo%20Vinuesa&entry.1292438233=%20%20The%20increasing%20number%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20urban%0Aenvironments%20requires%20a%20strategy%20to%20minimize%20their%20environmental%20impact%2C%20both%0Ain%20terms%20of%20energy%20efficiency%20and%20noise%20reduction.%20In%20order%20to%20reduce%20these%0Aconcerns%2C%20novel%20strategies%20for%20developing%20prediction%20models%20and%20optimization%20of%0Aflight%20planning%2C%20for%20instance%20through%20deep%20reinforcement%20learning%20%28DRL%29%2C%20are%0Aneeded.%20Our%20goal%20is%20to%20develop%20DRL%20algorithms%20capable%20of%20enabling%20the%0Aautonomous%20navigation%20of%20UAVs%20in%20urban%20environments%2C%20taking%20into%20account%20the%0Apresence%20of%20buildings%20and%20other%20UAVs%2C%20optimizing%20the%20trajectories%20in%20order%20to%0Areduce%20both%20energetic%20consumption%20and%20noise.%20This%20is%20achieved%20using%20fluid-flow%0Asimulations%20which%20represent%20the%20environment%20in%20which%20UAVs%20navigate%20and%20training%0Athe%20UAV%20as%20an%20agent%20interacting%20with%20an%20urban%20environment.%20In%20this%20work%2C%20we%0Aconsider%20a%20domain%20domain%20represented%20by%20a%20two-dimensional%20flow%20field%20with%0Aobstacles%2C%20ideally%20representing%20buildings%2C%20extracted%20from%20a%20three-dimensional%0Ahigh-fidelity%20numerical%20simulation.%20The%20presented%20methodology%2C%20using%20PPO%2BLSTM%0Acells%2C%20was%20validated%20by%20reproducing%20a%20simple%20but%20fundamental%20problem%20in%0Anavigation%2C%20namely%20the%20Zermelo%27s%20problem%2C%20which%20deals%20with%20a%20vessel%20navigating%0Ain%20a%20turbulent%20flow%2C%20travelling%20from%20a%20starting%20point%20to%20a%20target%20location%2C%0Aoptimizing%20the%20trajectory.%20The%20current%20method%20shows%20a%20significant%20improvement%0Awith%20respect%20to%20both%20a%20simple%20PPO%20and%20a%20TD3%20algorithm%2C%20with%20a%20success%20rate%20%28SR%29%0Aof%20the%20PPO%2BLSTM%20trained%20policy%20of%2098.7%25%2C%20and%20a%20crash%20rate%20%28CR%29%20of%200.1%25%2C%0Aoutperforming%20both%20PPO%20%28SR%20%3D%2075.6%25%2C%20CR%3D18.6%25%29%20and%20TD3%20%28SR%3D77.4%25%20and%20CR%3D14.5%25%29.%0AThis%20is%20the%20first%20step%20towards%20DRL%20strategies%20which%20will%20guide%20UAVs%20in%20a%0Athree-dimensional%20flow%20field%20using%20real-time%20signals%2C%20making%20the%20navigation%0Aefficient%20in%20terms%20of%20flight%20time%20and%20avoiding%20damages%20to%20the%20vehicle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17922v1&entry.124074799=Read"},
{"title": "Supra-Laplacian Encoding for Transformer on Dynamic Graphs", "author": "Yannis Karmim and Marc Lafon and Rapha\u00ebl Fournier S'niehotta and Nicolas Thome", "abstract": "  Fully connected Graph Transformers (GT) have rapidly become prominent in the\nstatic graph community as an alternative to Message-Passing models, which\nsuffer from a lack of expressivity, oversquashing, and under-reaching. However,\nin a dynamic context, by interconnecting all nodes at multiple snapshots with\nself-attention, GT loose both structural and temporal information. In this\nwork, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs\n(SLATE), a new spatio-temporal encoding to leverage the GT architecture while\nkeeping spatio-temporal information. Specifically, we transform Discrete Time\nDynamic Graphs into multi-layer graphs and take advantage of the spectral\nproperties of their associated supra-Laplacian matrix. Our second contribution\nexplicitly model nodes' pairwise relationships with a cross-attention\nmechanism, providing an accurate edge representation for dynamic link\nprediction. SLATE outperforms numerous state-of-the-art methods based on\nMessage-Passing Graph Neural Networks combined with recurrent models (e.g\nLSTM), and Dynamic Graph Transformers, on 9 datasets. Code and instructions to\nreproduce our results will be open-sourced.\n", "link": "http://arxiv.org/abs/2409.17986v1", "date": "2024-09-26", "relevancy": 2.1505, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5728}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5372}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supra-Laplacian%20Encoding%20for%20Transformer%20on%20Dynamic%20Graphs&body=Title%3A%20Supra-Laplacian%20Encoding%20for%20Transformer%20on%20Dynamic%20Graphs%0AAuthor%3A%20Yannis%20Karmim%20and%20Marc%20Lafon%20and%20Rapha%C3%ABl%20Fournier%20S%27niehotta%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20Fully%20connected%20Graph%20Transformers%20%28GT%29%20have%20rapidly%20become%20prominent%20in%20the%0Astatic%20graph%20community%20as%20an%20alternative%20to%20Message-Passing%20models%2C%20which%0Asuffer%20from%20a%20lack%20of%20expressivity%2C%20oversquashing%2C%20and%20under-reaching.%20However%2C%0Ain%20a%20dynamic%20context%2C%20by%20interconnecting%20all%20nodes%20at%20multiple%20snapshots%20with%0Aself-attention%2C%20GT%20loose%20both%20structural%20and%20temporal%20information.%20In%20this%0Awork%2C%20we%20introduce%20Supra-LAplacian%20encoding%20for%20spatio-temporal%20TransformErs%0A%28SLATE%29%2C%20a%20new%20spatio-temporal%20encoding%20to%20leverage%20the%20GT%20architecture%20while%0Akeeping%20spatio-temporal%20information.%20Specifically%2C%20we%20transform%20Discrete%20Time%0ADynamic%20Graphs%20into%20multi-layer%20graphs%20and%20take%20advantage%20of%20the%20spectral%0Aproperties%20of%20their%20associated%20supra-Laplacian%20matrix.%20Our%20second%20contribution%0Aexplicitly%20model%20nodes%27%20pairwise%20relationships%20with%20a%20cross-attention%0Amechanism%2C%20providing%20an%20accurate%20edge%20representation%20for%20dynamic%20link%0Aprediction.%20SLATE%20outperforms%20numerous%20state-of-the-art%20methods%20based%20on%0AMessage-Passing%20Graph%20Neural%20Networks%20combined%20with%20recurrent%20models%20%28e.g%0ALSTM%29%2C%20and%20Dynamic%20Graph%20Transformers%2C%20on%209%20datasets.%20Code%20and%20instructions%20to%0Areproduce%20our%20results%20will%20be%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupra-Laplacian%2520Encoding%2520for%2520Transformer%2520on%2520Dynamic%2520Graphs%26entry.906535625%3DYannis%2520Karmim%2520and%2520Marc%2520Lafon%2520and%2520Rapha%25C3%25ABl%2520Fournier%2520S%2527niehotta%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520Fully%2520connected%2520Graph%2520Transformers%2520%2528GT%2529%2520have%2520rapidly%2520become%2520prominent%2520in%2520the%250Astatic%2520graph%2520community%2520as%2520an%2520alternative%2520to%2520Message-Passing%2520models%252C%2520which%250Asuffer%2520from%2520a%2520lack%2520of%2520expressivity%252C%2520oversquashing%252C%2520and%2520under-reaching.%2520However%252C%250Ain%2520a%2520dynamic%2520context%252C%2520by%2520interconnecting%2520all%2520nodes%2520at%2520multiple%2520snapshots%2520with%250Aself-attention%252C%2520GT%2520loose%2520both%2520structural%2520and%2520temporal%2520information.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520Supra-LAplacian%2520encoding%2520for%2520spatio-temporal%2520TransformErs%250A%2528SLATE%2529%252C%2520a%2520new%2520spatio-temporal%2520encoding%2520to%2520leverage%2520the%2520GT%2520architecture%2520while%250Akeeping%2520spatio-temporal%2520information.%2520Specifically%252C%2520we%2520transform%2520Discrete%2520Time%250ADynamic%2520Graphs%2520into%2520multi-layer%2520graphs%2520and%2520take%2520advantage%2520of%2520the%2520spectral%250Aproperties%2520of%2520their%2520associated%2520supra-Laplacian%2520matrix.%2520Our%2520second%2520contribution%250Aexplicitly%2520model%2520nodes%2527%2520pairwise%2520relationships%2520with%2520a%2520cross-attention%250Amechanism%252C%2520providing%2520an%2520accurate%2520edge%2520representation%2520for%2520dynamic%2520link%250Aprediction.%2520SLATE%2520outperforms%2520numerous%2520state-of-the-art%2520methods%2520based%2520on%250AMessage-Passing%2520Graph%2520Neural%2520Networks%2520combined%2520with%2520recurrent%2520models%2520%2528e.g%250ALSTM%2529%252C%2520and%2520Dynamic%2520Graph%2520Transformers%252C%2520on%25209%2520datasets.%2520Code%2520and%2520instructions%2520to%250Areproduce%2520our%2520results%2520will%2520be%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supra-Laplacian%20Encoding%20for%20Transformer%20on%20Dynamic%20Graphs&entry.906535625=Yannis%20Karmim%20and%20Marc%20Lafon%20and%20Rapha%C3%ABl%20Fournier%20S%27niehotta%20and%20Nicolas%20Thome&entry.1292438233=%20%20Fully%20connected%20Graph%20Transformers%20%28GT%29%20have%20rapidly%20become%20prominent%20in%20the%0Astatic%20graph%20community%20as%20an%20alternative%20to%20Message-Passing%20models%2C%20which%0Asuffer%20from%20a%20lack%20of%20expressivity%2C%20oversquashing%2C%20and%20under-reaching.%20However%2C%0Ain%20a%20dynamic%20context%2C%20by%20interconnecting%20all%20nodes%20at%20multiple%20snapshots%20with%0Aself-attention%2C%20GT%20loose%20both%20structural%20and%20temporal%20information.%20In%20this%0Awork%2C%20we%20introduce%20Supra-LAplacian%20encoding%20for%20spatio-temporal%20TransformErs%0A%28SLATE%29%2C%20a%20new%20spatio-temporal%20encoding%20to%20leverage%20the%20GT%20architecture%20while%0Akeeping%20spatio-temporal%20information.%20Specifically%2C%20we%20transform%20Discrete%20Time%0ADynamic%20Graphs%20into%20multi-layer%20graphs%20and%20take%20advantage%20of%20the%20spectral%0Aproperties%20of%20their%20associated%20supra-Laplacian%20matrix.%20Our%20second%20contribution%0Aexplicitly%20model%20nodes%27%20pairwise%20relationships%20with%20a%20cross-attention%0Amechanism%2C%20providing%20an%20accurate%20edge%20representation%20for%20dynamic%20link%0Aprediction.%20SLATE%20outperforms%20numerous%20state-of-the-art%20methods%20based%20on%0AMessage-Passing%20Graph%20Neural%20Networks%20combined%20with%20recurrent%20models%20%28e.g%0ALSTM%29%2C%20and%20Dynamic%20Graph%20Transformers%2C%20on%209%20datasets.%20Code%20and%20instructions%20to%0Areproduce%20our%20results%20will%20be%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17986v1&entry.124074799=Read"},
{"title": "Jumping through Local Minima: Quantization in the Loss Landscape of\n  Vision Transformers", "author": "Natalia Frumkin and Dibakar Gope and Diana Marculescu", "abstract": "  Quantization scale and bit-width are the most important parameters when\nconsidering how to quantize a neural network. Prior work focuses on optimizing\nquantization scales in a global manner through gradient methods (gradient\ndescent \\& Hessian analysis). Yet, when applying perturbations to quantization\nscales, we observe a very jagged, highly non-smooth test loss landscape. In\nfact, small perturbations in quantization scale can greatly affect accuracy,\nyielding a $0.5-0.8\\%$ accuracy boost in 4-bit quantized vision transformers\n(ViTs). In this regime, gradient methods break down, since they cannot reliably\nreach local minima. In our work, dubbed Evol-Q, we use evolutionary search to\neffectively traverse the non-smooth landscape. Additionally, we propose using\nan infoNCE loss, which not only helps combat overfitting on the small\ncalibration dataset ($1,000$ images) but also makes traversing such a highly\nnon-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully\nquantized ViT-Base by $10.30\\%$, $0.78\\%$, and $0.15\\%$ for $3$-bit, $4$-bit,\nand $8$-bit weight quantization levels. Extensive experiments on a variety of\nCNN and ViT architectures further demonstrate its robustness in extreme\nquantization scenarios. Our code is available at\nhttps://github.com/enyac-group/evol-q\n", "link": "http://arxiv.org/abs/2308.10814v3", "date": "2024-09-26", "relevancy": 2.1491, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5448}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5374}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jumping%20through%20Local%20Minima%3A%20Quantization%20in%20the%20Loss%20Landscape%20of%0A%20%20Vision%20Transformers&body=Title%3A%20Jumping%20through%20Local%20Minima%3A%20Quantization%20in%20the%20Loss%20Landscape%20of%0A%20%20Vision%20Transformers%0AAuthor%3A%20Natalia%20Frumkin%20and%20Dibakar%20Gope%20and%20Diana%20Marculescu%0AAbstract%3A%20%20%20Quantization%20scale%20and%20bit-width%20are%20the%20most%20important%20parameters%20when%0Aconsidering%20how%20to%20quantize%20a%20neural%20network.%20Prior%20work%20focuses%20on%20optimizing%0Aquantization%20scales%20in%20a%20global%20manner%20through%20gradient%20methods%20%28gradient%0Adescent%20%5C%26%20Hessian%20analysis%29.%20Yet%2C%20when%20applying%20perturbations%20to%20quantization%0Ascales%2C%20we%20observe%20a%20very%20jagged%2C%20highly%20non-smooth%20test%20loss%20landscape.%20In%0Afact%2C%20small%20perturbations%20in%20quantization%20scale%20can%20greatly%20affect%20accuracy%2C%0Ayielding%20a%20%240.5-0.8%5C%25%24%20accuracy%20boost%20in%204-bit%20quantized%20vision%20transformers%0A%28ViTs%29.%20In%20this%20regime%2C%20gradient%20methods%20break%20down%2C%20since%20they%20cannot%20reliably%0Areach%20local%20minima.%20In%20our%20work%2C%20dubbed%20Evol-Q%2C%20we%20use%20evolutionary%20search%20to%0Aeffectively%20traverse%20the%20non-smooth%20landscape.%20Additionally%2C%20we%20propose%20using%0Aan%20infoNCE%20loss%2C%20which%20not%20only%20helps%20combat%20overfitting%20on%20the%20small%0Acalibration%20dataset%20%28%241%2C000%24%20images%29%20but%20also%20makes%20traversing%20such%20a%20highly%0Anon-smooth%20surface%20easier.%20Evol-Q%20improves%20the%20top-1%20accuracy%20of%20a%20fully%0Aquantized%20ViT-Base%20by%20%2410.30%5C%25%24%2C%20%240.78%5C%25%24%2C%20and%20%240.15%5C%25%24%20for%20%243%24-bit%2C%20%244%24-bit%2C%0Aand%20%248%24-bit%20weight%20quantization%20levels.%20Extensive%20experiments%20on%20a%20variety%20of%0ACNN%20and%20ViT%20architectures%20further%20demonstrate%20its%20robustness%20in%20extreme%0Aquantization%20scenarios.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/enyac-group/evol-q%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10814v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJumping%2520through%2520Local%2520Minima%253A%2520Quantization%2520in%2520the%2520Loss%2520Landscape%2520of%250A%2520%2520Vision%2520Transformers%26entry.906535625%3DNatalia%2520Frumkin%2520and%2520Dibakar%2520Gope%2520and%2520Diana%2520Marculescu%26entry.1292438233%3D%2520%2520Quantization%2520scale%2520and%2520bit-width%2520are%2520the%2520most%2520important%2520parameters%2520when%250Aconsidering%2520how%2520to%2520quantize%2520a%2520neural%2520network.%2520Prior%2520work%2520focuses%2520on%2520optimizing%250Aquantization%2520scales%2520in%2520a%2520global%2520manner%2520through%2520gradient%2520methods%2520%2528gradient%250Adescent%2520%255C%2526%2520Hessian%2520analysis%2529.%2520Yet%252C%2520when%2520applying%2520perturbations%2520to%2520quantization%250Ascales%252C%2520we%2520observe%2520a%2520very%2520jagged%252C%2520highly%2520non-smooth%2520test%2520loss%2520landscape.%2520In%250Afact%252C%2520small%2520perturbations%2520in%2520quantization%2520scale%2520can%2520greatly%2520affect%2520accuracy%252C%250Ayielding%2520a%2520%25240.5-0.8%255C%2525%2524%2520accuracy%2520boost%2520in%25204-bit%2520quantized%2520vision%2520transformers%250A%2528ViTs%2529.%2520In%2520this%2520regime%252C%2520gradient%2520methods%2520break%2520down%252C%2520since%2520they%2520cannot%2520reliably%250Areach%2520local%2520minima.%2520In%2520our%2520work%252C%2520dubbed%2520Evol-Q%252C%2520we%2520use%2520evolutionary%2520search%2520to%250Aeffectively%2520traverse%2520the%2520non-smooth%2520landscape.%2520Additionally%252C%2520we%2520propose%2520using%250Aan%2520infoNCE%2520loss%252C%2520which%2520not%2520only%2520helps%2520combat%2520overfitting%2520on%2520the%2520small%250Acalibration%2520dataset%2520%2528%25241%252C000%2524%2520images%2529%2520but%2520also%2520makes%2520traversing%2520such%2520a%2520highly%250Anon-smooth%2520surface%2520easier.%2520Evol-Q%2520improves%2520the%2520top-1%2520accuracy%2520of%2520a%2520fully%250Aquantized%2520ViT-Base%2520by%2520%252410.30%255C%2525%2524%252C%2520%25240.78%255C%2525%2524%252C%2520and%2520%25240.15%255C%2525%2524%2520for%2520%25243%2524-bit%252C%2520%25244%2524-bit%252C%250Aand%2520%25248%2524-bit%2520weight%2520quantization%2520levels.%2520Extensive%2520experiments%2520on%2520a%2520variety%2520of%250ACNN%2520and%2520ViT%2520architectures%2520further%2520demonstrate%2520its%2520robustness%2520in%2520extreme%250Aquantization%2520scenarios.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/enyac-group/evol-q%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10814v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jumping%20through%20Local%20Minima%3A%20Quantization%20in%20the%20Loss%20Landscape%20of%0A%20%20Vision%20Transformers&entry.906535625=Natalia%20Frumkin%20and%20Dibakar%20Gope%20and%20Diana%20Marculescu&entry.1292438233=%20%20Quantization%20scale%20and%20bit-width%20are%20the%20most%20important%20parameters%20when%0Aconsidering%20how%20to%20quantize%20a%20neural%20network.%20Prior%20work%20focuses%20on%20optimizing%0Aquantization%20scales%20in%20a%20global%20manner%20through%20gradient%20methods%20%28gradient%0Adescent%20%5C%26%20Hessian%20analysis%29.%20Yet%2C%20when%20applying%20perturbations%20to%20quantization%0Ascales%2C%20we%20observe%20a%20very%20jagged%2C%20highly%20non-smooth%20test%20loss%20landscape.%20In%0Afact%2C%20small%20perturbations%20in%20quantization%20scale%20can%20greatly%20affect%20accuracy%2C%0Ayielding%20a%20%240.5-0.8%5C%25%24%20accuracy%20boost%20in%204-bit%20quantized%20vision%20transformers%0A%28ViTs%29.%20In%20this%20regime%2C%20gradient%20methods%20break%20down%2C%20since%20they%20cannot%20reliably%0Areach%20local%20minima.%20In%20our%20work%2C%20dubbed%20Evol-Q%2C%20we%20use%20evolutionary%20search%20to%0Aeffectively%20traverse%20the%20non-smooth%20landscape.%20Additionally%2C%20we%20propose%20using%0Aan%20infoNCE%20loss%2C%20which%20not%20only%20helps%20combat%20overfitting%20on%20the%20small%0Acalibration%20dataset%20%28%241%2C000%24%20images%29%20but%20also%20makes%20traversing%20such%20a%20highly%0Anon-smooth%20surface%20easier.%20Evol-Q%20improves%20the%20top-1%20accuracy%20of%20a%20fully%0Aquantized%20ViT-Base%20by%20%2410.30%5C%25%24%2C%20%240.78%5C%25%24%2C%20and%20%240.15%5C%25%24%20for%20%243%24-bit%2C%20%244%24-bit%2C%0Aand%20%248%24-bit%20weight%20quantization%20levels.%20Extensive%20experiments%20on%20a%20variety%20of%0ACNN%20and%20ViT%20architectures%20further%20demonstrate%20its%20robustness%20in%20extreme%0Aquantization%20scenarios.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/enyac-group/evol-q%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10814v3&entry.124074799=Read"},
{"title": "BlinkTrack: Feature Tracking over 100 FPS via Events and Images", "author": "Yichen Shen and Yijin Li and Shuo Chen and Guanglin Li and Zhaoyang Huang and Hujun Bao and Zhaopeng Cui and Guofeng Zhang", "abstract": "  Feature tracking is crucial for, structure from motion (SFM), simultaneous\nlocalization and mapping (SLAM), object tracking and various computer vision\ntasks. Event cameras, known for their high temporal resolution and ability to\ncapture asynchronous changes, have gained significant attention for their\npotential in feature tracking, especially in challenging conditions. However,\nevent cameras lack the fine-grained texture information that conventional\ncameras provide, leading to error accumulation in tracking. To address this, we\npropose a novel framework, BlinkTrack, which integrates event data with RGB\nimages for high-frequency feature tracking. Our method extends the traditional\nKalman filter into a learning-based framework, utilizing differentiable Kalman\nfilters in both event and image branches. This approach improves\nsingle-modality tracking, resolves ambiguities, and supports asynchronous data\nfusion. We also introduce new synthetic and augmented datasets to better\nevaluate our model. Experimental results indicate that BlinkTrack significantly\noutperforms existing event-based methods, exceeding 100 FPS with preprocessed\nevent data and 80 FPS with multi-modality data.\n", "link": "http://arxiv.org/abs/2409.17981v1", "date": "2024-09-26", "relevancy": 2.1467, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.572}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5199}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlinkTrack%3A%20Feature%20Tracking%20over%20100%20FPS%20via%20Events%20and%20Images&body=Title%3A%20BlinkTrack%3A%20Feature%20Tracking%20over%20100%20FPS%20via%20Events%20and%20Images%0AAuthor%3A%20Yichen%20Shen%20and%20Yijin%20Li%20and%20Shuo%20Chen%20and%20Guanglin%20Li%20and%20Zhaoyang%20Huang%20and%20Hujun%20Bao%20and%20Zhaopeng%20Cui%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20Feature%20tracking%20is%20crucial%20for%2C%20structure%20from%20motion%20%28SFM%29%2C%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%2C%20object%20tracking%20and%20various%20computer%20vision%0Atasks.%20Event%20cameras%2C%20known%20for%20their%20high%20temporal%20resolution%20and%20ability%20to%0Acapture%20asynchronous%20changes%2C%20have%20gained%20significant%20attention%20for%20their%0Apotential%20in%20feature%20tracking%2C%20especially%20in%20challenging%20conditions.%20However%2C%0Aevent%20cameras%20lack%20the%20fine-grained%20texture%20information%20that%20conventional%0Acameras%20provide%2C%20leading%20to%20error%20accumulation%20in%20tracking.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20framework%2C%20BlinkTrack%2C%20which%20integrates%20event%20data%20with%20RGB%0Aimages%20for%20high-frequency%20feature%20tracking.%20Our%20method%20extends%20the%20traditional%0AKalman%20filter%20into%20a%20learning-based%20framework%2C%20utilizing%20differentiable%20Kalman%0Afilters%20in%20both%20event%20and%20image%20branches.%20This%20approach%20improves%0Asingle-modality%20tracking%2C%20resolves%20ambiguities%2C%20and%20supports%20asynchronous%20data%0Afusion.%20We%20also%20introduce%20new%20synthetic%20and%20augmented%20datasets%20to%20better%0Aevaluate%20our%20model.%20Experimental%20results%20indicate%20that%20BlinkTrack%20significantly%0Aoutperforms%20existing%20event-based%20methods%2C%20exceeding%20100%20FPS%20with%20preprocessed%0Aevent%20data%20and%2080%20FPS%20with%20multi-modality%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlinkTrack%253A%2520Feature%2520Tracking%2520over%2520100%2520FPS%2520via%2520Events%2520and%2520Images%26entry.906535625%3DYichen%2520Shen%2520and%2520Yijin%2520Li%2520and%2520Shuo%2520Chen%2520and%2520Guanglin%2520Li%2520and%2520Zhaoyang%2520Huang%2520and%2520Hujun%2520Bao%2520and%2520Zhaopeng%2520Cui%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520Feature%2520tracking%2520is%2520crucial%2520for%252C%2520structure%2520from%2520motion%2520%2528SFM%2529%252C%2520simultaneous%250Alocalization%2520and%2520mapping%2520%2528SLAM%2529%252C%2520object%2520tracking%2520and%2520various%2520computer%2520vision%250Atasks.%2520Event%2520cameras%252C%2520known%2520for%2520their%2520high%2520temporal%2520resolution%2520and%2520ability%2520to%250Acapture%2520asynchronous%2520changes%252C%2520have%2520gained%2520significant%2520attention%2520for%2520their%250Apotential%2520in%2520feature%2520tracking%252C%2520especially%2520in%2520challenging%2520conditions.%2520However%252C%250Aevent%2520cameras%2520lack%2520the%2520fine-grained%2520texture%2520information%2520that%2520conventional%250Acameras%2520provide%252C%2520leading%2520to%2520error%2520accumulation%2520in%2520tracking.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520novel%2520framework%252C%2520BlinkTrack%252C%2520which%2520integrates%2520event%2520data%2520with%2520RGB%250Aimages%2520for%2520high-frequency%2520feature%2520tracking.%2520Our%2520method%2520extends%2520the%2520traditional%250AKalman%2520filter%2520into%2520a%2520learning-based%2520framework%252C%2520utilizing%2520differentiable%2520Kalman%250Afilters%2520in%2520both%2520event%2520and%2520image%2520branches.%2520This%2520approach%2520improves%250Asingle-modality%2520tracking%252C%2520resolves%2520ambiguities%252C%2520and%2520supports%2520asynchronous%2520data%250Afusion.%2520We%2520also%2520introduce%2520new%2520synthetic%2520and%2520augmented%2520datasets%2520to%2520better%250Aevaluate%2520our%2520model.%2520Experimental%2520results%2520indicate%2520that%2520BlinkTrack%2520significantly%250Aoutperforms%2520existing%2520event-based%2520methods%252C%2520exceeding%2520100%2520FPS%2520with%2520preprocessed%250Aevent%2520data%2520and%252080%2520FPS%2520with%2520multi-modality%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlinkTrack%3A%20Feature%20Tracking%20over%20100%20FPS%20via%20Events%20and%20Images&entry.906535625=Yichen%20Shen%20and%20Yijin%20Li%20and%20Shuo%20Chen%20and%20Guanglin%20Li%20and%20Zhaoyang%20Huang%20and%20Hujun%20Bao%20and%20Zhaopeng%20Cui%20and%20Guofeng%20Zhang&entry.1292438233=%20%20Feature%20tracking%20is%20crucial%20for%2C%20structure%20from%20motion%20%28SFM%29%2C%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29%2C%20object%20tracking%20and%20various%20computer%20vision%0Atasks.%20Event%20cameras%2C%20known%20for%20their%20high%20temporal%20resolution%20and%20ability%20to%0Acapture%20asynchronous%20changes%2C%20have%20gained%20significant%20attention%20for%20their%0Apotential%20in%20feature%20tracking%2C%20especially%20in%20challenging%20conditions.%20However%2C%0Aevent%20cameras%20lack%20the%20fine-grained%20texture%20information%20that%20conventional%0Acameras%20provide%2C%20leading%20to%20error%20accumulation%20in%20tracking.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20framework%2C%20BlinkTrack%2C%20which%20integrates%20event%20data%20with%20RGB%0Aimages%20for%20high-frequency%20feature%20tracking.%20Our%20method%20extends%20the%20traditional%0AKalman%20filter%20into%20a%20learning-based%20framework%2C%20utilizing%20differentiable%20Kalman%0Afilters%20in%20both%20event%20and%20image%20branches.%20This%20approach%20improves%0Asingle-modality%20tracking%2C%20resolves%20ambiguities%2C%20and%20supports%20asynchronous%20data%0Afusion.%20We%20also%20introduce%20new%20synthetic%20and%20augmented%20datasets%20to%20better%0Aevaluate%20our%20model.%20Experimental%20results%20indicate%20that%20BlinkTrack%20significantly%0Aoutperforms%20existing%20event-based%20methods%2C%20exceeding%20100%20FPS%20with%20preprocessed%0Aevent%20data%20and%2080%20FPS%20with%20multi-modality%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17981v1&entry.124074799=Read"},
{"title": "MMDVS-LF: A Multi-Modal Dynamic-Vision-Sensor Line Following Dataset", "author": "Felix Resch and M\u00f3nika Farsang and Radu Grosu", "abstract": "  Dynamic Vision Sensors (DVS), offer a unique advantage in control\napplications, due to their high temporal resolution, and asynchronous\nevent-based data. Still, their adoption in machine learning algorithms remains\nlimited. To address this gap, and promote the development of models that\nleverage the specific characteristics of DVS data, we introduce the Multi-Modal\nDynamic-Vision-Sensor Line Following dataset (MMDVS-LF). This comprehensive\ndataset, is the first to integrate multiple sensor modalities, including DVS\nrecordings, RGB video, odometry, and Inertial Measurement Unit (IMU) data, from\na small-scale standardized vehicle. Additionally, the dataset includes\neye-tracking and demographic data of drivers performing a Line Following task\non a track. With its diverse range of data, MMDVS-LF opens new opportunities\nfor developing deep learning algorithms, and conducting data science projects\nacross various domains, supporting innovation in autonomous systems and control\napplications.\n", "link": "http://arxiv.org/abs/2409.18038v1", "date": "2024-09-26", "relevancy": 2.1358, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMDVS-LF%3A%20A%20Multi-Modal%20Dynamic-Vision-Sensor%20Line%20Following%20Dataset&body=Title%3A%20MMDVS-LF%3A%20A%20Multi-Modal%20Dynamic-Vision-Sensor%20Line%20Following%20Dataset%0AAuthor%3A%20Felix%20Resch%20and%20M%C3%B3nika%20Farsang%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20Dynamic%20Vision%20Sensors%20%28DVS%29%2C%20offer%20a%20unique%20advantage%20in%20control%0Aapplications%2C%20due%20to%20their%20high%20temporal%20resolution%2C%20and%20asynchronous%0Aevent-based%20data.%20Still%2C%20their%20adoption%20in%20machine%20learning%20algorithms%20remains%0Alimited.%20To%20address%20this%20gap%2C%20and%20promote%20the%20development%20of%20models%20that%0Aleverage%20the%20specific%20characteristics%20of%20DVS%20data%2C%20we%20introduce%20the%20Multi-Modal%0ADynamic-Vision-Sensor%20Line%20Following%20dataset%20%28MMDVS-LF%29.%20This%20comprehensive%0Adataset%2C%20is%20the%20first%20to%20integrate%20multiple%20sensor%20modalities%2C%20including%20DVS%0Arecordings%2C%20RGB%20video%2C%20odometry%2C%20and%20Inertial%20Measurement%20Unit%20%28IMU%29%20data%2C%20from%0Aa%20small-scale%20standardized%20vehicle.%20Additionally%2C%20the%20dataset%20includes%0Aeye-tracking%20and%20demographic%20data%20of%20drivers%20performing%20a%20Line%20Following%20task%0Aon%20a%20track.%20With%20its%20diverse%20range%20of%20data%2C%20MMDVS-LF%20opens%20new%20opportunities%0Afor%20developing%20deep%20learning%20algorithms%2C%20and%20conducting%20data%20science%20projects%0Aacross%20various%20domains%2C%20supporting%20innovation%20in%20autonomous%20systems%20and%20control%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMDVS-LF%253A%2520A%2520Multi-Modal%2520Dynamic-Vision-Sensor%2520Line%2520Following%2520Dataset%26entry.906535625%3DFelix%2520Resch%2520and%2520M%25C3%25B3nika%2520Farsang%2520and%2520Radu%2520Grosu%26entry.1292438233%3D%2520%2520Dynamic%2520Vision%2520Sensors%2520%2528DVS%2529%252C%2520offer%2520a%2520unique%2520advantage%2520in%2520control%250Aapplications%252C%2520due%2520to%2520their%2520high%2520temporal%2520resolution%252C%2520and%2520asynchronous%250Aevent-based%2520data.%2520Still%252C%2520their%2520adoption%2520in%2520machine%2520learning%2520algorithms%2520remains%250Alimited.%2520To%2520address%2520this%2520gap%252C%2520and%2520promote%2520the%2520development%2520of%2520models%2520that%250Aleverage%2520the%2520specific%2520characteristics%2520of%2520DVS%2520data%252C%2520we%2520introduce%2520the%2520Multi-Modal%250ADynamic-Vision-Sensor%2520Line%2520Following%2520dataset%2520%2528MMDVS-LF%2529.%2520This%2520comprehensive%250Adataset%252C%2520is%2520the%2520first%2520to%2520integrate%2520multiple%2520sensor%2520modalities%252C%2520including%2520DVS%250Arecordings%252C%2520RGB%2520video%252C%2520odometry%252C%2520and%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520data%252C%2520from%250Aa%2520small-scale%2520standardized%2520vehicle.%2520Additionally%252C%2520the%2520dataset%2520includes%250Aeye-tracking%2520and%2520demographic%2520data%2520of%2520drivers%2520performing%2520a%2520Line%2520Following%2520task%250Aon%2520a%2520track.%2520With%2520its%2520diverse%2520range%2520of%2520data%252C%2520MMDVS-LF%2520opens%2520new%2520opportunities%250Afor%2520developing%2520deep%2520learning%2520algorithms%252C%2520and%2520conducting%2520data%2520science%2520projects%250Aacross%2520various%2520domains%252C%2520supporting%2520innovation%2520in%2520autonomous%2520systems%2520and%2520control%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMDVS-LF%3A%20A%20Multi-Modal%20Dynamic-Vision-Sensor%20Line%20Following%20Dataset&entry.906535625=Felix%20Resch%20and%20M%C3%B3nika%20Farsang%20and%20Radu%20Grosu&entry.1292438233=%20%20Dynamic%20Vision%20Sensors%20%28DVS%29%2C%20offer%20a%20unique%20advantage%20in%20control%0Aapplications%2C%20due%20to%20their%20high%20temporal%20resolution%2C%20and%20asynchronous%0Aevent-based%20data.%20Still%2C%20their%20adoption%20in%20machine%20learning%20algorithms%20remains%0Alimited.%20To%20address%20this%20gap%2C%20and%20promote%20the%20development%20of%20models%20that%0Aleverage%20the%20specific%20characteristics%20of%20DVS%20data%2C%20we%20introduce%20the%20Multi-Modal%0ADynamic-Vision-Sensor%20Line%20Following%20dataset%20%28MMDVS-LF%29.%20This%20comprehensive%0Adataset%2C%20is%20the%20first%20to%20integrate%20multiple%20sensor%20modalities%2C%20including%20DVS%0Arecordings%2C%20RGB%20video%2C%20odometry%2C%20and%20Inertial%20Measurement%20Unit%20%28IMU%29%20data%2C%20from%0Aa%20small-scale%20standardized%20vehicle.%20Additionally%2C%20the%20dataset%20includes%0Aeye-tracking%20and%20demographic%20data%20of%20drivers%20performing%20a%20Line%20Following%20task%0Aon%20a%20track.%20With%20its%20diverse%20range%20of%20data%2C%20MMDVS-LF%20opens%20new%20opportunities%0Afor%20developing%20deep%20learning%20algorithms%2C%20and%20conducting%20data%20science%20projects%0Aacross%20various%20domains%2C%20supporting%20innovation%20in%20autonomous%20systems%20and%20control%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18038v1&entry.124074799=Read"},
{"title": "Continual learning with task specialist", "author": "Indu Solomon and Aye Phyu Phyu Aung and Uttam Kumar and Senthilnath Jayavelu", "abstract": "  Continual learning (CL) adapt the deep learning scenarios with timely updated\ndatasets. However, existing CL models suffer from the catastrophic forgetting\nissue, where new knowledge replaces past learning. In this paper, we propose\nContinual Learning with Task Specialists (CLTS) to address the issues of\ncatastrophic forgetting and limited labelled data in real-world datasets by\nperforming class incremental learning of the incoming stream of data. The model\nconsists of Task Specialists (T S) and Task Predictor (T P ) with pre-trained\nStable Diffusion (SD) module. Here, we introduce a new specialist to handle a\nnew task sequence and each T S has three blocks; i) a variational autoencoder\n(V AE) to learn the task distribution in a low dimensional latent space, ii) a\nK-Means block to perform data clustering and iii) Bootstrapping Language-Image\nPre-training (BLIP ) model to generate a small batch of captions from the input\ndata. These captions are fed as input to the pre-trained stable diffusion model\n(SD) for the generation of task samples. The proposed model does not store any\ntask samples for replay, instead uses generated samples from SD to train the T\nP module. A comparison study with four SOTA models conducted on three\nreal-world datasets shows that the proposed model outperforms all the selected\nbaselines\n", "link": "http://arxiv.org/abs/2409.17806v1", "date": "2024-09-26", "relevancy": 2.134, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5261}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20learning%20with%20task%20specialist&body=Title%3A%20Continual%20learning%20with%20task%20specialist%0AAuthor%3A%20Indu%20Solomon%20and%20Aye%20Phyu%20Phyu%20Aung%20and%20Uttam%20Kumar%20and%20Senthilnath%20Jayavelu%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20adapt%20the%20deep%20learning%20scenarios%20with%20timely%20updated%0Adatasets.%20However%2C%20existing%20CL%20models%20suffer%20from%20the%20catastrophic%20forgetting%0Aissue%2C%20where%20new%20knowledge%20replaces%20past%20learning.%20In%20this%20paper%2C%20we%20propose%0AContinual%20Learning%20with%20Task%20Specialists%20%28CLTS%29%20to%20address%20the%20issues%20of%0Acatastrophic%20forgetting%20and%20limited%20labelled%20data%20in%20real-world%20datasets%20by%0Aperforming%20class%20incremental%20learning%20of%20the%20incoming%20stream%20of%20data.%20The%20model%0Aconsists%20of%20Task%20Specialists%20%28T%20S%29%20and%20Task%20Predictor%20%28T%20P%20%29%20with%20pre-trained%0AStable%20Diffusion%20%28SD%29%20module.%20Here%2C%20we%20introduce%20a%20new%20specialist%20to%20handle%20a%0Anew%20task%20sequence%20and%20each%20T%20S%20has%20three%20blocks%3B%20i%29%20a%20variational%20autoencoder%0A%28V%20AE%29%20to%20learn%20the%20task%20distribution%20in%20a%20low%20dimensional%20latent%20space%2C%20ii%29%20a%0AK-Means%20block%20to%20perform%20data%20clustering%20and%20iii%29%20Bootstrapping%20Language-Image%0APre-training%20%28BLIP%20%29%20model%20to%20generate%20a%20small%20batch%20of%20captions%20from%20the%20input%0Adata.%20These%20captions%20are%20fed%20as%20input%20to%20the%20pre-trained%20stable%20diffusion%20model%0A%28SD%29%20for%20the%20generation%20of%20task%20samples.%20The%20proposed%20model%20does%20not%20store%20any%0Atask%20samples%20for%20replay%2C%20instead%20uses%20generated%20samples%20from%20SD%20to%20train%20the%20T%0AP%20module.%20A%20comparison%20study%20with%20four%20SOTA%20models%20conducted%20on%20three%0Areal-world%20datasets%20shows%20that%20the%20proposed%20model%20outperforms%20all%20the%20selected%0Abaselines%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520learning%2520with%2520task%2520specialist%26entry.906535625%3DIndu%2520Solomon%2520and%2520Aye%2520Phyu%2520Phyu%2520Aung%2520and%2520Uttam%2520Kumar%2520and%2520Senthilnath%2520Jayavelu%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520adapt%2520the%2520deep%2520learning%2520scenarios%2520with%2520timely%2520updated%250Adatasets.%2520However%252C%2520existing%2520CL%2520models%2520suffer%2520from%2520the%2520catastrophic%2520forgetting%250Aissue%252C%2520where%2520new%2520knowledge%2520replaces%2520past%2520learning.%2520In%2520this%2520paper%252C%2520we%2520propose%250AContinual%2520Learning%2520with%2520Task%2520Specialists%2520%2528CLTS%2529%2520to%2520address%2520the%2520issues%2520of%250Acatastrophic%2520forgetting%2520and%2520limited%2520labelled%2520data%2520in%2520real-world%2520datasets%2520by%250Aperforming%2520class%2520incremental%2520learning%2520of%2520the%2520incoming%2520stream%2520of%2520data.%2520The%2520model%250Aconsists%2520of%2520Task%2520Specialists%2520%2528T%2520S%2529%2520and%2520Task%2520Predictor%2520%2528T%2520P%2520%2529%2520with%2520pre-trained%250AStable%2520Diffusion%2520%2528SD%2529%2520module.%2520Here%252C%2520we%2520introduce%2520a%2520new%2520specialist%2520to%2520handle%2520a%250Anew%2520task%2520sequence%2520and%2520each%2520T%2520S%2520has%2520three%2520blocks%253B%2520i%2529%2520a%2520variational%2520autoencoder%250A%2528V%2520AE%2529%2520to%2520learn%2520the%2520task%2520distribution%2520in%2520a%2520low%2520dimensional%2520latent%2520space%252C%2520ii%2529%2520a%250AK-Means%2520block%2520to%2520perform%2520data%2520clustering%2520and%2520iii%2529%2520Bootstrapping%2520Language-Image%250APre-training%2520%2528BLIP%2520%2529%2520model%2520to%2520generate%2520a%2520small%2520batch%2520of%2520captions%2520from%2520the%2520input%250Adata.%2520These%2520captions%2520are%2520fed%2520as%2520input%2520to%2520the%2520pre-trained%2520stable%2520diffusion%2520model%250A%2528SD%2529%2520for%2520the%2520generation%2520of%2520task%2520samples.%2520The%2520proposed%2520model%2520does%2520not%2520store%2520any%250Atask%2520samples%2520for%2520replay%252C%2520instead%2520uses%2520generated%2520samples%2520from%2520SD%2520to%2520train%2520the%2520T%250AP%2520module.%2520A%2520comparison%2520study%2520with%2520four%2520SOTA%2520models%2520conducted%2520on%2520three%250Areal-world%2520datasets%2520shows%2520that%2520the%2520proposed%2520model%2520outperforms%2520all%2520the%2520selected%250Abaselines%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20learning%20with%20task%20specialist&entry.906535625=Indu%20Solomon%20and%20Aye%20Phyu%20Phyu%20Aung%20and%20Uttam%20Kumar%20and%20Senthilnath%20Jayavelu&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20adapt%20the%20deep%20learning%20scenarios%20with%20timely%20updated%0Adatasets.%20However%2C%20existing%20CL%20models%20suffer%20from%20the%20catastrophic%20forgetting%0Aissue%2C%20where%20new%20knowledge%20replaces%20past%20learning.%20In%20this%20paper%2C%20we%20propose%0AContinual%20Learning%20with%20Task%20Specialists%20%28CLTS%29%20to%20address%20the%20issues%20of%0Acatastrophic%20forgetting%20and%20limited%20labelled%20data%20in%20real-world%20datasets%20by%0Aperforming%20class%20incremental%20learning%20of%20the%20incoming%20stream%20of%20data.%20The%20model%0Aconsists%20of%20Task%20Specialists%20%28T%20S%29%20and%20Task%20Predictor%20%28T%20P%20%29%20with%20pre-trained%0AStable%20Diffusion%20%28SD%29%20module.%20Here%2C%20we%20introduce%20a%20new%20specialist%20to%20handle%20a%0Anew%20task%20sequence%20and%20each%20T%20S%20has%20three%20blocks%3B%20i%29%20a%20variational%20autoencoder%0A%28V%20AE%29%20to%20learn%20the%20task%20distribution%20in%20a%20low%20dimensional%20latent%20space%2C%20ii%29%20a%0AK-Means%20block%20to%20perform%20data%20clustering%20and%20iii%29%20Bootstrapping%20Language-Image%0APre-training%20%28BLIP%20%29%20model%20to%20generate%20a%20small%20batch%20of%20captions%20from%20the%20input%0Adata.%20These%20captions%20are%20fed%20as%20input%20to%20the%20pre-trained%20stable%20diffusion%20model%0A%28SD%29%20for%20the%20generation%20of%20task%20samples.%20The%20proposed%20model%20does%20not%20store%20any%0Atask%20samples%20for%20replay%2C%20instead%20uses%20generated%20samples%20from%20SD%20to%20train%20the%20T%0AP%20module.%20A%20comparison%20study%20with%20four%20SOTA%20models%20conducted%20on%20three%0Areal-world%20datasets%20shows%20that%20the%20proposed%20model%20outperforms%20all%20the%20selected%0Abaselines%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17806v1&entry.124074799=Read"},
{"title": "HydraViT: Stacking Heads for a Scalable ViT", "author": "Janek Haberer and Ali Hojjat and Olaf Landsiedel", "abstract": "  The architecture of Vision Transformers (ViTs), particularly the Multi-head\nAttention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs\non devices with varying constraints, such as mobile phones, requires multiple\nmodels of different sizes. However, this approach has limitations, such as\ntraining and storing each required model separately. This paper introduces\nHydraViT, a novel approach that addresses these limitations by stacking\nattention heads to achieve a scalable ViT. By repeatedly changing the size of\nthe embedded dimensions throughout each layer and their corresponding number of\nattention heads in MHA during training, HydraViT induces multiple subnetworks.\nThereby, HydraViT achieves adaptability across a wide spectrum of hardware\nenvironments while maintaining performance. Our experimental results\ndemonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10\nsubnetworks, covering a wide range of resource constraints. HydraViT achieves\nup to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy\nwith the same throughput on ImageNet-1K compared to the baselines, making it an\neffective solution for scenarios where hardware availability is diverse or\nvaries over time. Source code available at https://github.com/ds-kiel/HydraViT.\n", "link": "http://arxiv.org/abs/2409.17978v1", "date": "2024-09-26", "relevancy": 2.1185, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HydraViT%3A%20Stacking%20Heads%20for%20a%20Scalable%20ViT&body=Title%3A%20HydraViT%3A%20Stacking%20Heads%20for%20a%20Scalable%20ViT%0AAuthor%3A%20Janek%20Haberer%20and%20Ali%20Hojjat%20and%20Olaf%20Landsiedel%0AAbstract%3A%20%20%20The%20architecture%20of%20Vision%20Transformers%20%28ViTs%29%2C%20particularly%20the%20Multi-head%0AAttention%20%28MHA%29%20mechanism%2C%20imposes%20substantial%20hardware%20demands.%20Deploying%20ViTs%0Aon%20devices%20with%20varying%20constraints%2C%20such%20as%20mobile%20phones%2C%20requires%20multiple%0Amodels%20of%20different%20sizes.%20However%2C%20this%20approach%20has%20limitations%2C%20such%20as%0Atraining%20and%20storing%20each%20required%20model%20separately.%20This%20paper%20introduces%0AHydraViT%2C%20a%20novel%20approach%20that%20addresses%20these%20limitations%20by%20stacking%0Aattention%20heads%20to%20achieve%20a%20scalable%20ViT.%20By%20repeatedly%20changing%20the%20size%20of%0Athe%20embedded%20dimensions%20throughout%20each%20layer%20and%20their%20corresponding%20number%20of%0Aattention%20heads%20in%20MHA%20during%20training%2C%20HydraViT%20induces%20multiple%20subnetworks.%0AThereby%2C%20HydraViT%20achieves%20adaptability%20across%20a%20wide%20spectrum%20of%20hardware%0Aenvironments%20while%20maintaining%20performance.%20Our%20experimental%20results%0Ademonstrate%20the%20efficacy%20of%20HydraViT%20in%20achieving%20a%20scalable%20ViT%20with%20up%20to%2010%0Asubnetworks%2C%20covering%20a%20wide%20range%20of%20resource%20constraints.%20HydraViT%20achieves%0Aup%20to%205%20p.p.%20more%20accuracy%20with%20the%20same%20GMACs%20and%20up%20to%207%20p.p.%20more%20accuracy%0Awith%20the%20same%20throughput%20on%20ImageNet-1K%20compared%20to%20the%20baselines%2C%20making%20it%20an%0Aeffective%20solution%20for%20scenarios%20where%20hardware%20availability%20is%20diverse%20or%0Avaries%20over%20time.%20Source%20code%20available%20at%20https%3A//github.com/ds-kiel/HydraViT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHydraViT%253A%2520Stacking%2520Heads%2520for%2520a%2520Scalable%2520ViT%26entry.906535625%3DJanek%2520Haberer%2520and%2520Ali%2520Hojjat%2520and%2520Olaf%2520Landsiedel%26entry.1292438233%3D%2520%2520The%2520architecture%2520of%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520particularly%2520the%2520Multi-head%250AAttention%2520%2528MHA%2529%2520mechanism%252C%2520imposes%2520substantial%2520hardware%2520demands.%2520Deploying%2520ViTs%250Aon%2520devices%2520with%2520varying%2520constraints%252C%2520such%2520as%2520mobile%2520phones%252C%2520requires%2520multiple%250Amodels%2520of%2520different%2520sizes.%2520However%252C%2520this%2520approach%2520has%2520limitations%252C%2520such%2520as%250Atraining%2520and%2520storing%2520each%2520required%2520model%2520separately.%2520This%2520paper%2520introduces%250AHydraViT%252C%2520a%2520novel%2520approach%2520that%2520addresses%2520these%2520limitations%2520by%2520stacking%250Aattention%2520heads%2520to%2520achieve%2520a%2520scalable%2520ViT.%2520By%2520repeatedly%2520changing%2520the%2520size%2520of%250Athe%2520embedded%2520dimensions%2520throughout%2520each%2520layer%2520and%2520their%2520corresponding%2520number%2520of%250Aattention%2520heads%2520in%2520MHA%2520during%2520training%252C%2520HydraViT%2520induces%2520multiple%2520subnetworks.%250AThereby%252C%2520HydraViT%2520achieves%2520adaptability%2520across%2520a%2520wide%2520spectrum%2520of%2520hardware%250Aenvironments%2520while%2520maintaining%2520performance.%2520Our%2520experimental%2520results%250Ademonstrate%2520the%2520efficacy%2520of%2520HydraViT%2520in%2520achieving%2520a%2520scalable%2520ViT%2520with%2520up%2520to%252010%250Asubnetworks%252C%2520covering%2520a%2520wide%2520range%2520of%2520resource%2520constraints.%2520HydraViT%2520achieves%250Aup%2520to%25205%2520p.p.%2520more%2520accuracy%2520with%2520the%2520same%2520GMACs%2520and%2520up%2520to%25207%2520p.p.%2520more%2520accuracy%250Awith%2520the%2520same%2520throughput%2520on%2520ImageNet-1K%2520compared%2520to%2520the%2520baselines%252C%2520making%2520it%2520an%250Aeffective%2520solution%2520for%2520scenarios%2520where%2520hardware%2520availability%2520is%2520diverse%2520or%250Avaries%2520over%2520time.%2520Source%2520code%2520available%2520at%2520https%253A//github.com/ds-kiel/HydraViT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HydraViT%3A%20Stacking%20Heads%20for%20a%20Scalable%20ViT&entry.906535625=Janek%20Haberer%20and%20Ali%20Hojjat%20and%20Olaf%20Landsiedel&entry.1292438233=%20%20The%20architecture%20of%20Vision%20Transformers%20%28ViTs%29%2C%20particularly%20the%20Multi-head%0AAttention%20%28MHA%29%20mechanism%2C%20imposes%20substantial%20hardware%20demands.%20Deploying%20ViTs%0Aon%20devices%20with%20varying%20constraints%2C%20such%20as%20mobile%20phones%2C%20requires%20multiple%0Amodels%20of%20different%20sizes.%20However%2C%20this%20approach%20has%20limitations%2C%20such%20as%0Atraining%20and%20storing%20each%20required%20model%20separately.%20This%20paper%20introduces%0AHydraViT%2C%20a%20novel%20approach%20that%20addresses%20these%20limitations%20by%20stacking%0Aattention%20heads%20to%20achieve%20a%20scalable%20ViT.%20By%20repeatedly%20changing%20the%20size%20of%0Athe%20embedded%20dimensions%20throughout%20each%20layer%20and%20their%20corresponding%20number%20of%0Aattention%20heads%20in%20MHA%20during%20training%2C%20HydraViT%20induces%20multiple%20subnetworks.%0AThereby%2C%20HydraViT%20achieves%20adaptability%20across%20a%20wide%20spectrum%20of%20hardware%0Aenvironments%20while%20maintaining%20performance.%20Our%20experimental%20results%0Ademonstrate%20the%20efficacy%20of%20HydraViT%20in%20achieving%20a%20scalable%20ViT%20with%20up%20to%2010%0Asubnetworks%2C%20covering%20a%20wide%20range%20of%20resource%20constraints.%20HydraViT%20achieves%0Aup%20to%205%20p.p.%20more%20accuracy%20with%20the%20same%20GMACs%20and%20up%20to%207%20p.p.%20more%20accuracy%0Awith%20the%20same%20throughput%20on%20ImageNet-1K%20compared%20to%20the%20baselines%2C%20making%20it%20an%0Aeffective%20solution%20for%20scenarios%20where%20hardware%20availability%20is%20diverse%20or%0Avaries%20over%20time.%20Source%20code%20available%20at%20https%3A//github.com/ds-kiel/HydraViT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17978v1&entry.124074799=Read"},
{"title": "Role-RL: Online Long-Context Processing with Role Reinforcement Learning\n  for Distinct LLMs in Their Optimal Roles", "author": "Lewei He and Tianyu Shi and Pengran Huang and Bingzhi Chen and Qianglong Chen and Jiahui Pan", "abstract": "  Large language models (LLMs) with long-context processing are still\nchallenging because of their implementation complexity, training efficiency and\ndata sparsity. To address this issue, a new paradigm named Online Long-context\nProcessing (OLP) is proposed when we process a document of unlimited length,\nwhich typically occurs in the information reception and organization of diverse\nstreaming media such as automated news reporting, live e-commerce, and viral\nshort videos. Moreover, a dilemma was often encountered when we tried to select\nthe most suitable LLM from a large number of LLMs amidst explosive growth\naiming for outstanding performance, affordable prices, and short response\ndelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)\nto automatically deploy different LLMs in their respective roles within the OLP\npipeline according to their actual performance. Extensive experiments are\nconducted on our OLP-MINI dataset and it is found that OLP with Role-RL\nframework achieves OLP benchmark with an average recall rate of 93.2% and the\nLLM cost saved by 79.4%. The code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL.\n", "link": "http://arxiv.org/abs/2409.18014v1", "date": "2024-09-26", "relevancy": 2.1119, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Role-RL%3A%20Online%20Long-Context%20Processing%20with%20Role%20Reinforcement%20Learning%0A%20%20for%20Distinct%20LLMs%20in%20Their%20Optimal%20Roles&body=Title%3A%20Role-RL%3A%20Online%20Long-Context%20Processing%20with%20Role%20Reinforcement%20Learning%0A%20%20for%20Distinct%20LLMs%20in%20Their%20Optimal%20Roles%0AAuthor%3A%20Lewei%20He%20and%20Tianyu%20Shi%20and%20Pengran%20Huang%20and%20Bingzhi%20Chen%20and%20Qianglong%20Chen%20and%20Jiahui%20Pan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20with%20long-context%20processing%20are%20still%0Achallenging%20because%20of%20their%20implementation%20complexity%2C%20training%20efficiency%20and%0Adata%20sparsity.%20To%20address%20this%20issue%2C%20a%20new%20paradigm%20named%20Online%20Long-context%0AProcessing%20%28OLP%29%20is%20proposed%20when%20we%20process%20a%20document%20of%20unlimited%20length%2C%0Awhich%20typically%20occurs%20in%20the%20information%20reception%20and%20organization%20of%20diverse%0Astreaming%20media%20such%20as%20automated%20news%20reporting%2C%20live%20e-commerce%2C%20and%20viral%0Ashort%20videos.%20Moreover%2C%20a%20dilemma%20was%20often%20encountered%20when%20we%20tried%20to%20select%0Athe%20most%20suitable%20LLM%20from%20a%20large%20number%20of%20LLMs%20amidst%20explosive%20growth%0Aaiming%20for%20outstanding%20performance%2C%20affordable%20prices%2C%20and%20short%20response%0Adelays.%20In%20view%20of%20this%2C%20we%20also%20develop%20Role%20Reinforcement%20Learning%20%28Role-RL%29%0Ato%20automatically%20deploy%20different%20LLMs%20in%20their%20respective%20roles%20within%20the%20OLP%0Apipeline%20according%20to%20their%20actual%20performance.%20Extensive%20experiments%20are%0Aconducted%20on%20our%20OLP-MINI%20dataset%20and%20it%20is%20found%20that%20OLP%20with%20Role-RL%0Aframework%20achieves%20OLP%20benchmark%20with%20an%20average%20recall%20rate%20of%2093.2%25%20and%20the%0ALLM%20cost%20saved%20by%2079.4%25.%20The%20code%20and%20dataset%20are%20publicly%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/Role-RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRole-RL%253A%2520Online%2520Long-Context%2520Processing%2520with%2520Role%2520Reinforcement%2520Learning%250A%2520%2520for%2520Distinct%2520LLMs%2520in%2520Their%2520Optimal%2520Roles%26entry.906535625%3DLewei%2520He%2520and%2520Tianyu%2520Shi%2520and%2520Pengran%2520Huang%2520and%2520Bingzhi%2520Chen%2520and%2520Qianglong%2520Chen%2520and%2520Jiahui%2520Pan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520with%2520long-context%2520processing%2520are%2520still%250Achallenging%2520because%2520of%2520their%2520implementation%2520complexity%252C%2520training%2520efficiency%2520and%250Adata%2520sparsity.%2520To%2520address%2520this%2520issue%252C%2520a%2520new%2520paradigm%2520named%2520Online%2520Long-context%250AProcessing%2520%2528OLP%2529%2520is%2520proposed%2520when%2520we%2520process%2520a%2520document%2520of%2520unlimited%2520length%252C%250Awhich%2520typically%2520occurs%2520in%2520the%2520information%2520reception%2520and%2520organization%2520of%2520diverse%250Astreaming%2520media%2520such%2520as%2520automated%2520news%2520reporting%252C%2520live%2520e-commerce%252C%2520and%2520viral%250Ashort%2520videos.%2520Moreover%252C%2520a%2520dilemma%2520was%2520often%2520encountered%2520when%2520we%2520tried%2520to%2520select%250Athe%2520most%2520suitable%2520LLM%2520from%2520a%2520large%2520number%2520of%2520LLMs%2520amidst%2520explosive%2520growth%250Aaiming%2520for%2520outstanding%2520performance%252C%2520affordable%2520prices%252C%2520and%2520short%2520response%250Adelays.%2520In%2520view%2520of%2520this%252C%2520we%2520also%2520develop%2520Role%2520Reinforcement%2520Learning%2520%2528Role-RL%2529%250Ato%2520automatically%2520deploy%2520different%2520LLMs%2520in%2520their%2520respective%2520roles%2520within%2520the%2520OLP%250Apipeline%2520according%2520to%2520their%2520actual%2520performance.%2520Extensive%2520experiments%2520are%250Aconducted%2520on%2520our%2520OLP-MINI%2520dataset%2520and%2520it%2520is%2520found%2520that%2520OLP%2520with%2520Role-RL%250Aframework%2520achieves%2520OLP%2520benchmark%2520with%2520an%2520average%2520recall%2520rate%2520of%252093.2%2525%2520and%2520the%250ALLM%2520cost%2520saved%2520by%252079.4%2525.%2520The%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/Role-RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Role-RL%3A%20Online%20Long-Context%20Processing%20with%20Role%20Reinforcement%20Learning%0A%20%20for%20Distinct%20LLMs%20in%20Their%20Optimal%20Roles&entry.906535625=Lewei%20He%20and%20Tianyu%20Shi%20and%20Pengran%20Huang%20and%20Bingzhi%20Chen%20and%20Qianglong%20Chen%20and%20Jiahui%20Pan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20with%20long-context%20processing%20are%20still%0Achallenging%20because%20of%20their%20implementation%20complexity%2C%20training%20efficiency%20and%0Adata%20sparsity.%20To%20address%20this%20issue%2C%20a%20new%20paradigm%20named%20Online%20Long-context%0AProcessing%20%28OLP%29%20is%20proposed%20when%20we%20process%20a%20document%20of%20unlimited%20length%2C%0Awhich%20typically%20occurs%20in%20the%20information%20reception%20and%20organization%20of%20diverse%0Astreaming%20media%20such%20as%20automated%20news%20reporting%2C%20live%20e-commerce%2C%20and%20viral%0Ashort%20videos.%20Moreover%2C%20a%20dilemma%20was%20often%20encountered%20when%20we%20tried%20to%20select%0Athe%20most%20suitable%20LLM%20from%20a%20large%20number%20of%20LLMs%20amidst%20explosive%20growth%0Aaiming%20for%20outstanding%20performance%2C%20affordable%20prices%2C%20and%20short%20response%0Adelays.%20In%20view%20of%20this%2C%20we%20also%20develop%20Role%20Reinforcement%20Learning%20%28Role-RL%29%0Ato%20automatically%20deploy%20different%20LLMs%20in%20their%20respective%20roles%20within%20the%20OLP%0Apipeline%20according%20to%20their%20actual%20performance.%20Extensive%20experiments%20are%0Aconducted%20on%20our%20OLP-MINI%20dataset%20and%20it%20is%20found%20that%20OLP%20with%20Role-RL%0Aframework%20achieves%20OLP%20benchmark%20with%20an%20average%20recall%20rate%20of%2093.2%25%20and%20the%0ALLM%20cost%20saved%20by%2079.4%25.%20The%20code%20and%20dataset%20are%20publicly%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/Role-RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18014v1&entry.124074799=Read"},
{"title": "Interpretable Vision-Language Survival Analysis with Ordinal Inductive\n  Bias for Computational Pathology", "author": "Pei Liu and Luping Ji and Jiaxiang Gou and Bo Fu and Mao Ye", "abstract": "  Histopathology Whole-Slide Images (WSIs) provide an important tool to assess\ncancer prognosis in computational pathology (CPATH). While existing survival\nanalysis (SA) approaches have made exciting progress, they are generally\nlimited to adopting highly-expressive architectures and only coarse-grained\npatient-level labels to learn prognostic visual representations from gigapixel\nWSIs. Such learning paradigm suffers from important performance bottlenecks,\nwhen facing present scarce training data and standard multi-instance learning\n(MIL) framework in CPATH. To overcome it, this paper, for the first time,\nproposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA\nis driven by pathology VL foundation models. It no longer relies on\nhigh-capability networks and shows the advantage of data efficiency. (2) In\nvision-end, VLSA encodes prognostic language prior and then employs it as\nauxiliary signals to guide the aggregating of prognostic visual features at\ninstance level, thereby compensating for the weak supervision in MIL. Moreover,\ngiven the characteristics of SA, we propose i) ordinal survival prompt learning\nto transform continuous survival labels into textual prompts; and ii) ordinal\nincidence function as prediction target to make SA compatible with VL-based\nprediction. Notably, VLSA's predictions can be interpreted intuitively by our\nShapley values-based method. The extensive experiments on five datasets confirm\nthe effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH\nby offering weakly-supervised MIL an effective means to learn valuable\nprognostic clues from gigapixel WSIs. Our source code is available at\nhttps://github.com/liupei101/VLSA.\n", "link": "http://arxiv.org/abs/2409.09369v3", "date": "2024-09-26", "relevancy": 2.1045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology&body=Title%3A%20Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology%0AAuthor%3A%20Pei%20Liu%20and%20Luping%20Ji%20and%20Jiaxiang%20Gou%20and%20Bo%20Fu%20and%20Mao%20Ye%0AAbstract%3A%20%20%20Histopathology%20Whole-Slide%20Images%20%28WSIs%29%20provide%20an%20important%20tool%20to%20assess%0Acancer%20prognosis%20in%20computational%20pathology%20%28CPATH%29.%20While%20existing%20survival%0Aanalysis%20%28SA%29%20approaches%20have%20made%20exciting%20progress%2C%20they%20are%20generally%0Alimited%20to%20adopting%20highly-expressive%20architectures%20and%20only%20coarse-grained%0Apatient-level%20labels%20to%20learn%20prognostic%20visual%20representations%20from%20gigapixel%0AWSIs.%20Such%20learning%20paradigm%20suffers%20from%20important%20performance%20bottlenecks%2C%0Awhen%20facing%20present%20scarce%20training%20data%20and%20standard%20multi-instance%20learning%0A%28MIL%29%20framework%20in%20CPATH.%20To%20overcome%20it%2C%20this%20paper%2C%20for%20the%20first%20time%2C%0Aproposes%20a%20new%20Vision-Language-based%20SA%20%28VLSA%29%20paradigm.%20Concretely%2C%20%281%29%20VLSA%0Ais%20driven%20by%20pathology%20VL%20foundation%20models.%20It%20no%20longer%20relies%20on%0Ahigh-capability%20networks%20and%20shows%20the%20advantage%20of%20data%20efficiency.%20%282%29%20In%0Avision-end%2C%20VLSA%20encodes%20prognostic%20language%20prior%20and%20then%20employs%20it%20as%0Aauxiliary%20signals%20to%20guide%20the%20aggregating%20of%20prognostic%20visual%20features%20at%0Ainstance%20level%2C%20thereby%20compensating%20for%20the%20weak%20supervision%20in%20MIL.%20Moreover%2C%0Agiven%20the%20characteristics%20of%20SA%2C%20we%20propose%20i%29%20ordinal%20survival%20prompt%20learning%0Ato%20transform%20continuous%20survival%20labels%20into%20textual%20prompts%3B%20and%20ii%29%20ordinal%0Aincidence%20function%20as%20prediction%20target%20to%20make%20SA%20compatible%20with%20VL-based%0Aprediction.%20Notably%2C%20VLSA%27s%20predictions%20can%20be%20interpreted%20intuitively%20by%20our%0AShapley%20values-based%20method.%20The%20extensive%20experiments%20on%20five%20datasets%20confirm%0Athe%20effectiveness%20of%20our%20scheme.%20Our%20VLSA%20could%20pave%20a%20new%20way%20for%20SA%20in%20CPATH%0Aby%20offering%20weakly-supervised%20MIL%20an%20effective%20means%20to%20learn%20valuable%0Aprognostic%20clues%20from%20gigapixel%20WSIs.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/liupei101/VLSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09369v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Vision-Language%2520Survival%2520Analysis%2520with%2520Ordinal%2520Inductive%250A%2520%2520Bias%2520for%2520Computational%2520Pathology%26entry.906535625%3DPei%2520Liu%2520and%2520Luping%2520Ji%2520and%2520Jiaxiang%2520Gou%2520and%2520Bo%2520Fu%2520and%2520Mao%2520Ye%26entry.1292438233%3D%2520%2520Histopathology%2520Whole-Slide%2520Images%2520%2528WSIs%2529%2520provide%2520an%2520important%2520tool%2520to%2520assess%250Acancer%2520prognosis%2520in%2520computational%2520pathology%2520%2528CPATH%2529.%2520While%2520existing%2520survival%250Aanalysis%2520%2528SA%2529%2520approaches%2520have%2520made%2520exciting%2520progress%252C%2520they%2520are%2520generally%250Alimited%2520to%2520adopting%2520highly-expressive%2520architectures%2520and%2520only%2520coarse-grained%250Apatient-level%2520labels%2520to%2520learn%2520prognostic%2520visual%2520representations%2520from%2520gigapixel%250AWSIs.%2520Such%2520learning%2520paradigm%2520suffers%2520from%2520important%2520performance%2520bottlenecks%252C%250Awhen%2520facing%2520present%2520scarce%2520training%2520data%2520and%2520standard%2520multi-instance%2520learning%250A%2528MIL%2529%2520framework%2520in%2520CPATH.%2520To%2520overcome%2520it%252C%2520this%2520paper%252C%2520for%2520the%2520first%2520time%252C%250Aproposes%2520a%2520new%2520Vision-Language-based%2520SA%2520%2528VLSA%2529%2520paradigm.%2520Concretely%252C%2520%25281%2529%2520VLSA%250Ais%2520driven%2520by%2520pathology%2520VL%2520foundation%2520models.%2520It%2520no%2520longer%2520relies%2520on%250Ahigh-capability%2520networks%2520and%2520shows%2520the%2520advantage%2520of%2520data%2520efficiency.%2520%25282%2529%2520In%250Avision-end%252C%2520VLSA%2520encodes%2520prognostic%2520language%2520prior%2520and%2520then%2520employs%2520it%2520as%250Aauxiliary%2520signals%2520to%2520guide%2520the%2520aggregating%2520of%2520prognostic%2520visual%2520features%2520at%250Ainstance%2520level%252C%2520thereby%2520compensating%2520for%2520the%2520weak%2520supervision%2520in%2520MIL.%2520Moreover%252C%250Agiven%2520the%2520characteristics%2520of%2520SA%252C%2520we%2520propose%2520i%2529%2520ordinal%2520survival%2520prompt%2520learning%250Ato%2520transform%2520continuous%2520survival%2520labels%2520into%2520textual%2520prompts%253B%2520and%2520ii%2529%2520ordinal%250Aincidence%2520function%2520as%2520prediction%2520target%2520to%2520make%2520SA%2520compatible%2520with%2520VL-based%250Aprediction.%2520Notably%252C%2520VLSA%2527s%2520predictions%2520can%2520be%2520interpreted%2520intuitively%2520by%2520our%250AShapley%2520values-based%2520method.%2520The%2520extensive%2520experiments%2520on%2520five%2520datasets%2520confirm%250Athe%2520effectiveness%2520of%2520our%2520scheme.%2520Our%2520VLSA%2520could%2520pave%2520a%2520new%2520way%2520for%2520SA%2520in%2520CPATH%250Aby%2520offering%2520weakly-supervised%2520MIL%2520an%2520effective%2520means%2520to%2520learn%2520valuable%250Aprognostic%2520clues%2520from%2520gigapixel%2520WSIs.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/liupei101/VLSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09369v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology&entry.906535625=Pei%20Liu%20and%20Luping%20Ji%20and%20Jiaxiang%20Gou%20and%20Bo%20Fu%20and%20Mao%20Ye&entry.1292438233=%20%20Histopathology%20Whole-Slide%20Images%20%28WSIs%29%20provide%20an%20important%20tool%20to%20assess%0Acancer%20prognosis%20in%20computational%20pathology%20%28CPATH%29.%20While%20existing%20survival%0Aanalysis%20%28SA%29%20approaches%20have%20made%20exciting%20progress%2C%20they%20are%20generally%0Alimited%20to%20adopting%20highly-expressive%20architectures%20and%20only%20coarse-grained%0Apatient-level%20labels%20to%20learn%20prognostic%20visual%20representations%20from%20gigapixel%0AWSIs.%20Such%20learning%20paradigm%20suffers%20from%20important%20performance%20bottlenecks%2C%0Awhen%20facing%20present%20scarce%20training%20data%20and%20standard%20multi-instance%20learning%0A%28MIL%29%20framework%20in%20CPATH.%20To%20overcome%20it%2C%20this%20paper%2C%20for%20the%20first%20time%2C%0Aproposes%20a%20new%20Vision-Language-based%20SA%20%28VLSA%29%20paradigm.%20Concretely%2C%20%281%29%20VLSA%0Ais%20driven%20by%20pathology%20VL%20foundation%20models.%20It%20no%20longer%20relies%20on%0Ahigh-capability%20networks%20and%20shows%20the%20advantage%20of%20data%20efficiency.%20%282%29%20In%0Avision-end%2C%20VLSA%20encodes%20prognostic%20language%20prior%20and%20then%20employs%20it%20as%0Aauxiliary%20signals%20to%20guide%20the%20aggregating%20of%20prognostic%20visual%20features%20at%0Ainstance%20level%2C%20thereby%20compensating%20for%20the%20weak%20supervision%20in%20MIL.%20Moreover%2C%0Agiven%20the%20characteristics%20of%20SA%2C%20we%20propose%20i%29%20ordinal%20survival%20prompt%20learning%0Ato%20transform%20continuous%20survival%20labels%20into%20textual%20prompts%3B%20and%20ii%29%20ordinal%0Aincidence%20function%20as%20prediction%20target%20to%20make%20SA%20compatible%20with%20VL-based%0Aprediction.%20Notably%2C%20VLSA%27s%20predictions%20can%20be%20interpreted%20intuitively%20by%20our%0AShapley%20values-based%20method.%20The%20extensive%20experiments%20on%20five%20datasets%20confirm%0Athe%20effectiveness%20of%20our%20scheme.%20Our%20VLSA%20could%20pave%20a%20new%20way%20for%20SA%20in%20CPATH%0Aby%20offering%20weakly-supervised%20MIL%20an%20effective%20means%20to%20learn%20valuable%0Aprognostic%20clues%20from%20gigapixel%20WSIs.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/liupei101/VLSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09369v3&entry.124074799=Read"},
{"title": "Message-Passing Monte Carlo: Generating low-discrepancy point sets via\n  Graph Neural Networks", "author": "T. Konstantin Rusch and Nathan Kirk and Michael M. Bronstein and Christiane Lemieux and Daniela Rus", "abstract": "  Discrepancy is a well-known measure for the irregularity of the distribution\nof a point set. Point sets with small discrepancy are called low-discrepancy\nand are known to efficiently fill the space in a uniform manner.\nLow-discrepancy points play a central role in many problems in science and\nengineering, including numerical integration, computer vision, machine\nperception, computer graphics, machine learning, and simulation. In this work,\nwe present the first machine learning approach to generate a new class of\nlow-discrepancy point sets named Message-Passing Monte Carlo (MPMC) points.\nMotivated by the geometric nature of generating low-discrepancy point sets, we\nleverage tools from Geometric Deep Learning and base our model on Graph Neural\nNetworks. We further provide an extension of our framework to higher\ndimensions, which flexibly allows the generation of custom-made points that\nemphasize the uniformity in specific dimensions that are primarily important\nfor the particular problem at hand. Finally, we demonstrate that our proposed\nmodel achieves state-of-the-art performance superior to previous methods by a\nsignificant margin. In fact, MPMC points are empirically shown to be either\noptimal or near-optimal with respect to the discrepancy for low dimension and\nsmall number of points, i.e., for which the optimal discrepancy can be\ndetermined. Code for generating MPMC points can be found at\nhttps://github.com/tk-rusch/MPMC.\n", "link": "http://arxiv.org/abs/2405.15059v2", "date": "2024-09-26", "relevancy": 2.1028, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5607}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5095}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Message-Passing%20Monte%20Carlo%3A%20Generating%20low-discrepancy%20point%20sets%20via%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Message-Passing%20Monte%20Carlo%3A%20Generating%20low-discrepancy%20point%20sets%20via%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20T.%20Konstantin%20Rusch%20and%20Nathan%20Kirk%20and%20Michael%20M.%20Bronstein%20and%20Christiane%20Lemieux%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20Discrepancy%20is%20a%20well-known%20measure%20for%20the%20irregularity%20of%20the%20distribution%0Aof%20a%20point%20set.%20Point%20sets%20with%20small%20discrepancy%20are%20called%20low-discrepancy%0Aand%20are%20known%20to%20efficiently%20fill%20the%20space%20in%20a%20uniform%20manner.%0ALow-discrepancy%20points%20play%20a%20central%20role%20in%20many%20problems%20in%20science%20and%0Aengineering%2C%20including%20numerical%20integration%2C%20computer%20vision%2C%20machine%0Aperception%2C%20computer%20graphics%2C%20machine%20learning%2C%20and%20simulation.%20In%20this%20work%2C%0Awe%20present%20the%20first%20machine%20learning%20approach%20to%20generate%20a%20new%20class%20of%0Alow-discrepancy%20point%20sets%20named%20Message-Passing%20Monte%20Carlo%20%28MPMC%29%20points.%0AMotivated%20by%20the%20geometric%20nature%20of%20generating%20low-discrepancy%20point%20sets%2C%20we%0Aleverage%20tools%20from%20Geometric%20Deep%20Learning%20and%20base%20our%20model%20on%20Graph%20Neural%0ANetworks.%20We%20further%20provide%20an%20extension%20of%20our%20framework%20to%20higher%0Adimensions%2C%20which%20flexibly%20allows%20the%20generation%20of%20custom-made%20points%20that%0Aemphasize%20the%20uniformity%20in%20specific%20dimensions%20that%20are%20primarily%20important%0Afor%20the%20particular%20problem%20at%20hand.%20Finally%2C%20we%20demonstrate%20that%20our%20proposed%0Amodel%20achieves%20state-of-the-art%20performance%20superior%20to%20previous%20methods%20by%20a%0Asignificant%20margin.%20In%20fact%2C%20MPMC%20points%20are%20empirically%20shown%20to%20be%20either%0Aoptimal%20or%20near-optimal%20with%20respect%20to%20the%20discrepancy%20for%20low%20dimension%20and%0Asmall%20number%20of%20points%2C%20i.e.%2C%20for%20which%20the%20optimal%20discrepancy%20can%20be%0Adetermined.%20Code%20for%20generating%20MPMC%20points%20can%20be%20found%20at%0Ahttps%3A//github.com/tk-rusch/MPMC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMessage-Passing%2520Monte%2520Carlo%253A%2520Generating%2520low-discrepancy%2520point%2520sets%2520via%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DT.%2520Konstantin%2520Rusch%2520and%2520Nathan%2520Kirk%2520and%2520Michael%2520M.%2520Bronstein%2520and%2520Christiane%2520Lemieux%2520and%2520Daniela%2520Rus%26entry.1292438233%3D%2520%2520Discrepancy%2520is%2520a%2520well-known%2520measure%2520for%2520the%2520irregularity%2520of%2520the%2520distribution%250Aof%2520a%2520point%2520set.%2520Point%2520sets%2520with%2520small%2520discrepancy%2520are%2520called%2520low-discrepancy%250Aand%2520are%2520known%2520to%2520efficiently%2520fill%2520the%2520space%2520in%2520a%2520uniform%2520manner.%250ALow-discrepancy%2520points%2520play%2520a%2520central%2520role%2520in%2520many%2520problems%2520in%2520science%2520and%250Aengineering%252C%2520including%2520numerical%2520integration%252C%2520computer%2520vision%252C%2520machine%250Aperception%252C%2520computer%2520graphics%252C%2520machine%2520learning%252C%2520and%2520simulation.%2520In%2520this%2520work%252C%250Awe%2520present%2520the%2520first%2520machine%2520learning%2520approach%2520to%2520generate%2520a%2520new%2520class%2520of%250Alow-discrepancy%2520point%2520sets%2520named%2520Message-Passing%2520Monte%2520Carlo%2520%2528MPMC%2529%2520points.%250AMotivated%2520by%2520the%2520geometric%2520nature%2520of%2520generating%2520low-discrepancy%2520point%2520sets%252C%2520we%250Aleverage%2520tools%2520from%2520Geometric%2520Deep%2520Learning%2520and%2520base%2520our%2520model%2520on%2520Graph%2520Neural%250ANetworks.%2520We%2520further%2520provide%2520an%2520extension%2520of%2520our%2520framework%2520to%2520higher%250Adimensions%252C%2520which%2520flexibly%2520allows%2520the%2520generation%2520of%2520custom-made%2520points%2520that%250Aemphasize%2520the%2520uniformity%2520in%2520specific%2520dimensions%2520that%2520are%2520primarily%2520important%250Afor%2520the%2520particular%2520problem%2520at%2520hand.%2520Finally%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%250Amodel%2520achieves%2520state-of-the-art%2520performance%2520superior%2520to%2520previous%2520methods%2520by%2520a%250Asignificant%2520margin.%2520In%2520fact%252C%2520MPMC%2520points%2520are%2520empirically%2520shown%2520to%2520be%2520either%250Aoptimal%2520or%2520near-optimal%2520with%2520respect%2520to%2520the%2520discrepancy%2520for%2520low%2520dimension%2520and%250Asmall%2520number%2520of%2520points%252C%2520i.e.%252C%2520for%2520which%2520the%2520optimal%2520discrepancy%2520can%2520be%250Adetermined.%2520Code%2520for%2520generating%2520MPMC%2520points%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/tk-rusch/MPMC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Message-Passing%20Monte%20Carlo%3A%20Generating%20low-discrepancy%20point%20sets%20via%0A%20%20Graph%20Neural%20Networks&entry.906535625=T.%20Konstantin%20Rusch%20and%20Nathan%20Kirk%20and%20Michael%20M.%20Bronstein%20and%20Christiane%20Lemieux%20and%20Daniela%20Rus&entry.1292438233=%20%20Discrepancy%20is%20a%20well-known%20measure%20for%20the%20irregularity%20of%20the%20distribution%0Aof%20a%20point%20set.%20Point%20sets%20with%20small%20discrepancy%20are%20called%20low-discrepancy%0Aand%20are%20known%20to%20efficiently%20fill%20the%20space%20in%20a%20uniform%20manner.%0ALow-discrepancy%20points%20play%20a%20central%20role%20in%20many%20problems%20in%20science%20and%0Aengineering%2C%20including%20numerical%20integration%2C%20computer%20vision%2C%20machine%0Aperception%2C%20computer%20graphics%2C%20machine%20learning%2C%20and%20simulation.%20In%20this%20work%2C%0Awe%20present%20the%20first%20machine%20learning%20approach%20to%20generate%20a%20new%20class%20of%0Alow-discrepancy%20point%20sets%20named%20Message-Passing%20Monte%20Carlo%20%28MPMC%29%20points.%0AMotivated%20by%20the%20geometric%20nature%20of%20generating%20low-discrepancy%20point%20sets%2C%20we%0Aleverage%20tools%20from%20Geometric%20Deep%20Learning%20and%20base%20our%20model%20on%20Graph%20Neural%0ANetworks.%20We%20further%20provide%20an%20extension%20of%20our%20framework%20to%20higher%0Adimensions%2C%20which%20flexibly%20allows%20the%20generation%20of%20custom-made%20points%20that%0Aemphasize%20the%20uniformity%20in%20specific%20dimensions%20that%20are%20primarily%20important%0Afor%20the%20particular%20problem%20at%20hand.%20Finally%2C%20we%20demonstrate%20that%20our%20proposed%0Amodel%20achieves%20state-of-the-art%20performance%20superior%20to%20previous%20methods%20by%20a%0Asignificant%20margin.%20In%20fact%2C%20MPMC%20points%20are%20empirically%20shown%20to%20be%20either%0Aoptimal%20or%20near-optimal%20with%20respect%20to%20the%20discrepancy%20for%20low%20dimension%20and%0Asmall%20number%20of%20points%2C%20i.e.%2C%20for%20which%20the%20optimal%20discrepancy%20can%20be%0Adetermined.%20Code%20for%20generating%20MPMC%20points%20can%20be%20found%20at%0Ahttps%3A//github.com/tk-rusch/MPMC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15059v2&entry.124074799=Read"},
{"title": "Neural Exploratory Landscape Analysis", "author": "Zeyuan Ma and Jiacheng Chen and Hongshu Guo and Yue-Jiao Gong", "abstract": "  Recent research in Meta-Black-Box Optimization (MetaBBO) have shown that\nmeta-trained neural networks can effectively guide the design of black-box\noptimizers, significantly reducing the need for expert tuning and delivering\nrobust performance across complex problem distributions. Despite their success,\na paradox remains: MetaBBO still rely on human-crafted Exploratory Landscape\nAnalysis features to inform the meta-level agent about the low-level\noptimization progress. To address the gap, this paper proposes Neural\nExploratory Landscape Analysis (NeurELA), a novel framework that dynamically\nprofiles landscape features through a two-stage, attention-based neural\nnetwork, executed in an entirely end-to-end fashion. NeurELA is pre-trained\nover a variety of MetaBBO algorithms using a multi-task neuroevolution\nstrategy. Extensive experiments show that NeurELA achieves consistently\nsuperior performance when integrated into different and even unseen MetaBBO\ntasks and can be efficiently fine-tuned for further performance boost. This\nadvancement marks a pivotal step in making MetaBBO algorithms more autonomous\nand broadly applicable.The source code of NeurELA can be accessed at\nhttps://anonymous.4open.science/r/Neur-ELA-303C.\n", "link": "http://arxiv.org/abs/2408.10672v2", "date": "2024-09-26", "relevancy": 2.0956, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5456}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Exploratory%20Landscape%20Analysis&body=Title%3A%20Neural%20Exploratory%20Landscape%20Analysis%0AAuthor%3A%20Zeyuan%20Ma%20and%20Jiacheng%20Chen%20and%20Hongshu%20Guo%20and%20Yue-Jiao%20Gong%0AAbstract%3A%20%20%20Recent%20research%20in%20Meta-Black-Box%20Optimization%20%28MetaBBO%29%20have%20shown%20that%0Ameta-trained%20neural%20networks%20can%20effectively%20guide%20the%20design%20of%20black-box%0Aoptimizers%2C%20significantly%20reducing%20the%20need%20for%20expert%20tuning%20and%20delivering%0Arobust%20performance%20across%20complex%20problem%20distributions.%20Despite%20their%20success%2C%0Aa%20paradox%20remains%3A%20MetaBBO%20still%20rely%20on%20human-crafted%20Exploratory%20Landscape%0AAnalysis%20features%20to%20inform%20the%20meta-level%20agent%20about%20the%20low-level%0Aoptimization%20progress.%20To%20address%20the%20gap%2C%20this%20paper%20proposes%20Neural%0AExploratory%20Landscape%20Analysis%20%28NeurELA%29%2C%20a%20novel%20framework%20that%20dynamically%0Aprofiles%20landscape%20features%20through%20a%20two-stage%2C%20attention-based%20neural%0Anetwork%2C%20executed%20in%20an%20entirely%20end-to-end%20fashion.%20NeurELA%20is%20pre-trained%0Aover%20a%20variety%20of%20MetaBBO%20algorithms%20using%20a%20multi-task%20neuroevolution%0Astrategy.%20Extensive%20experiments%20show%20that%20NeurELA%20achieves%20consistently%0Asuperior%20performance%20when%20integrated%20into%20different%20and%20even%20unseen%20MetaBBO%0Atasks%20and%20can%20be%20efficiently%20fine-tuned%20for%20further%20performance%20boost.%20This%0Aadvancement%20marks%20a%20pivotal%20step%20in%20making%20MetaBBO%20algorithms%20more%20autonomous%0Aand%20broadly%20applicable.The%20source%20code%20of%20NeurELA%20can%20be%20accessed%20at%0Ahttps%3A//anonymous.4open.science/r/Neur-ELA-303C.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Exploratory%2520Landscape%2520Analysis%26entry.906535625%3DZeyuan%2520Ma%2520and%2520Jiacheng%2520Chen%2520and%2520Hongshu%2520Guo%2520and%2520Yue-Jiao%2520Gong%26entry.1292438233%3D%2520%2520Recent%2520research%2520in%2520Meta-Black-Box%2520Optimization%2520%2528MetaBBO%2529%2520have%2520shown%2520that%250Ameta-trained%2520neural%2520networks%2520can%2520effectively%2520guide%2520the%2520design%2520of%2520black-box%250Aoptimizers%252C%2520significantly%2520reducing%2520the%2520need%2520for%2520expert%2520tuning%2520and%2520delivering%250Arobust%2520performance%2520across%2520complex%2520problem%2520distributions.%2520Despite%2520their%2520success%252C%250Aa%2520paradox%2520remains%253A%2520MetaBBO%2520still%2520rely%2520on%2520human-crafted%2520Exploratory%2520Landscape%250AAnalysis%2520features%2520to%2520inform%2520the%2520meta-level%2520agent%2520about%2520the%2520low-level%250Aoptimization%2520progress.%2520To%2520address%2520the%2520gap%252C%2520this%2520paper%2520proposes%2520Neural%250AExploratory%2520Landscape%2520Analysis%2520%2528NeurELA%2529%252C%2520a%2520novel%2520framework%2520that%2520dynamically%250Aprofiles%2520landscape%2520features%2520through%2520a%2520two-stage%252C%2520attention-based%2520neural%250Anetwork%252C%2520executed%2520in%2520an%2520entirely%2520end-to-end%2520fashion.%2520NeurELA%2520is%2520pre-trained%250Aover%2520a%2520variety%2520of%2520MetaBBO%2520algorithms%2520using%2520a%2520multi-task%2520neuroevolution%250Astrategy.%2520Extensive%2520experiments%2520show%2520that%2520NeurELA%2520achieves%2520consistently%250Asuperior%2520performance%2520when%2520integrated%2520into%2520different%2520and%2520even%2520unseen%2520MetaBBO%250Atasks%2520and%2520can%2520be%2520efficiently%2520fine-tuned%2520for%2520further%2520performance%2520boost.%2520This%250Aadvancement%2520marks%2520a%2520pivotal%2520step%2520in%2520making%2520MetaBBO%2520algorithms%2520more%2520autonomous%250Aand%2520broadly%2520applicable.The%2520source%2520code%2520of%2520NeurELA%2520can%2520be%2520accessed%2520at%250Ahttps%253A//anonymous.4open.science/r/Neur-ELA-303C.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Exploratory%20Landscape%20Analysis&entry.906535625=Zeyuan%20Ma%20and%20Jiacheng%20Chen%20and%20Hongshu%20Guo%20and%20Yue-Jiao%20Gong&entry.1292438233=%20%20Recent%20research%20in%20Meta-Black-Box%20Optimization%20%28MetaBBO%29%20have%20shown%20that%0Ameta-trained%20neural%20networks%20can%20effectively%20guide%20the%20design%20of%20black-box%0Aoptimizers%2C%20significantly%20reducing%20the%20need%20for%20expert%20tuning%20and%20delivering%0Arobust%20performance%20across%20complex%20problem%20distributions.%20Despite%20their%20success%2C%0Aa%20paradox%20remains%3A%20MetaBBO%20still%20rely%20on%20human-crafted%20Exploratory%20Landscape%0AAnalysis%20features%20to%20inform%20the%20meta-level%20agent%20about%20the%20low-level%0Aoptimization%20progress.%20To%20address%20the%20gap%2C%20this%20paper%20proposes%20Neural%0AExploratory%20Landscape%20Analysis%20%28NeurELA%29%2C%20a%20novel%20framework%20that%20dynamically%0Aprofiles%20landscape%20features%20through%20a%20two-stage%2C%20attention-based%20neural%0Anetwork%2C%20executed%20in%20an%20entirely%20end-to-end%20fashion.%20NeurELA%20is%20pre-trained%0Aover%20a%20variety%20of%20MetaBBO%20algorithms%20using%20a%20multi-task%20neuroevolution%0Astrategy.%20Extensive%20experiments%20show%20that%20NeurELA%20achieves%20consistently%0Asuperior%20performance%20when%20integrated%20into%20different%20and%20even%20unseen%20MetaBBO%0Atasks%20and%20can%20be%20efficiently%20fine-tuned%20for%20further%20performance%20boost.%20This%0Aadvancement%20marks%20a%20pivotal%20step%20in%20making%20MetaBBO%20algorithms%20more%20autonomous%0Aand%20broadly%20applicable.The%20source%20code%20of%20NeurELA%20can%20be%20accessed%20at%0Ahttps%3A//anonymous.4open.science/r/Neur-ELA-303C.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10672v2&entry.124074799=Read"},
{"title": "Efficient Bias Mitigation Without Privileged Information", "author": "Mateo Espinosa Zarlenga and Swami Sankaranarayanan and Jerone T. A. Andrews and Zohreh Shams and Mateja Jamnik and Alice Xiang", "abstract": "  Deep neural networks trained via empirical risk minimisation often exhibit\nsignificant performance disparities across groups, particularly when group and\ntask labels are spuriously correlated (e.g., \"grassy background\" and \"cows\").\nExisting bias mitigation methods that aim to address this issue often either\nrely on group labels for training or validation, or require an extensive\nhyperparameter search. Such data and computational requirements hinder the\npractical deployment of these methods, especially when datasets are too large\nto be group-annotated, computational resources are limited, and models are\ntrained through already complex pipelines. In this paper, we propose Targeted\nAugmentations for Bias Mitigation (TAB), a simple hyperparameter-free framework\nthat leverages the entire training history of a helper model to identify\nspurious samples, and generate a group-balanced training set from which a\nrobust model can be trained. We show that TAB improves worst-group performance\nwithout any group information or model selection, outperforming existing\nmethods while maintaining overall accuracy.\n", "link": "http://arxiv.org/abs/2409.17691v1", "date": "2024-09-26", "relevancy": 2.0956, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5318}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5203}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Bias%20Mitigation%20Without%20Privileged%20Information&body=Title%3A%20Efficient%20Bias%20Mitigation%20Without%20Privileged%20Information%0AAuthor%3A%20Mateo%20Espinosa%20Zarlenga%20and%20Swami%20Sankaranarayanan%20and%20Jerone%20T.%20A.%20Andrews%20and%20Zohreh%20Shams%20and%20Mateja%20Jamnik%20and%20Alice%20Xiang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20trained%20via%20empirical%20risk%20minimisation%20often%20exhibit%0Asignificant%20performance%20disparities%20across%20groups%2C%20particularly%20when%20group%20and%0Atask%20labels%20are%20spuriously%20correlated%20%28e.g.%2C%20%22grassy%20background%22%20and%20%22cows%22%29.%0AExisting%20bias%20mitigation%20methods%20that%20aim%20to%20address%20this%20issue%20often%20either%0Arely%20on%20group%20labels%20for%20training%20or%20validation%2C%20or%20require%20an%20extensive%0Ahyperparameter%20search.%20Such%20data%20and%20computational%20requirements%20hinder%20the%0Apractical%20deployment%20of%20these%20methods%2C%20especially%20when%20datasets%20are%20too%20large%0Ato%20be%20group-annotated%2C%20computational%20resources%20are%20limited%2C%20and%20models%20are%0Atrained%20through%20already%20complex%20pipelines.%20In%20this%20paper%2C%20we%20propose%20Targeted%0AAugmentations%20for%20Bias%20Mitigation%20%28TAB%29%2C%20a%20simple%20hyperparameter-free%20framework%0Athat%20leverages%20the%20entire%20training%20history%20of%20a%20helper%20model%20to%20identify%0Aspurious%20samples%2C%20and%20generate%20a%20group-balanced%20training%20set%20from%20which%20a%0Arobust%20model%20can%20be%20trained.%20We%20show%20that%20TAB%20improves%20worst-group%20performance%0Awithout%20any%20group%20information%20or%20model%20selection%2C%20outperforming%20existing%0Amethods%20while%20maintaining%20overall%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Bias%2520Mitigation%2520Without%2520Privileged%2520Information%26entry.906535625%3DMateo%2520Espinosa%2520Zarlenga%2520and%2520Swami%2520Sankaranarayanan%2520and%2520Jerone%2520T.%2520A.%2520Andrews%2520and%2520Zohreh%2520Shams%2520and%2520Mateja%2520Jamnik%2520and%2520Alice%2520Xiang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520trained%2520via%2520empirical%2520risk%2520minimisation%2520often%2520exhibit%250Asignificant%2520performance%2520disparities%2520across%2520groups%252C%2520particularly%2520when%2520group%2520and%250Atask%2520labels%2520are%2520spuriously%2520correlated%2520%2528e.g.%252C%2520%2522grassy%2520background%2522%2520and%2520%2522cows%2522%2529.%250AExisting%2520bias%2520mitigation%2520methods%2520that%2520aim%2520to%2520address%2520this%2520issue%2520often%2520either%250Arely%2520on%2520group%2520labels%2520for%2520training%2520or%2520validation%252C%2520or%2520require%2520an%2520extensive%250Ahyperparameter%2520search.%2520Such%2520data%2520and%2520computational%2520requirements%2520hinder%2520the%250Apractical%2520deployment%2520of%2520these%2520methods%252C%2520especially%2520when%2520datasets%2520are%2520too%2520large%250Ato%2520be%2520group-annotated%252C%2520computational%2520resources%2520are%2520limited%252C%2520and%2520models%2520are%250Atrained%2520through%2520already%2520complex%2520pipelines.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Targeted%250AAugmentations%2520for%2520Bias%2520Mitigation%2520%2528TAB%2529%252C%2520a%2520simple%2520hyperparameter-free%2520framework%250Athat%2520leverages%2520the%2520entire%2520training%2520history%2520of%2520a%2520helper%2520model%2520to%2520identify%250Aspurious%2520samples%252C%2520and%2520generate%2520a%2520group-balanced%2520training%2520set%2520from%2520which%2520a%250Arobust%2520model%2520can%2520be%2520trained.%2520We%2520show%2520that%2520TAB%2520improves%2520worst-group%2520performance%250Awithout%2520any%2520group%2520information%2520or%2520model%2520selection%252C%2520outperforming%2520existing%250Amethods%2520while%2520maintaining%2520overall%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Bias%20Mitigation%20Without%20Privileged%20Information&entry.906535625=Mateo%20Espinosa%20Zarlenga%20and%20Swami%20Sankaranarayanan%20and%20Jerone%20T.%20A.%20Andrews%20and%20Zohreh%20Shams%20and%20Mateja%20Jamnik%20and%20Alice%20Xiang&entry.1292438233=%20%20Deep%20neural%20networks%20trained%20via%20empirical%20risk%20minimisation%20often%20exhibit%0Asignificant%20performance%20disparities%20across%20groups%2C%20particularly%20when%20group%20and%0Atask%20labels%20are%20spuriously%20correlated%20%28e.g.%2C%20%22grassy%20background%22%20and%20%22cows%22%29.%0AExisting%20bias%20mitigation%20methods%20that%20aim%20to%20address%20this%20issue%20often%20either%0Arely%20on%20group%20labels%20for%20training%20or%20validation%2C%20or%20require%20an%20extensive%0Ahyperparameter%20search.%20Such%20data%20and%20computational%20requirements%20hinder%20the%0Apractical%20deployment%20of%20these%20methods%2C%20especially%20when%20datasets%20are%20too%20large%0Ato%20be%20group-annotated%2C%20computational%20resources%20are%20limited%2C%20and%20models%20are%0Atrained%20through%20already%20complex%20pipelines.%20In%20this%20paper%2C%20we%20propose%20Targeted%0AAugmentations%20for%20Bias%20Mitigation%20%28TAB%29%2C%20a%20simple%20hyperparameter-free%20framework%0Athat%20leverages%20the%20entire%20training%20history%20of%20a%20helper%20model%20to%20identify%0Aspurious%20samples%2C%20and%20generate%20a%20group-balanced%20training%20set%20from%20which%20a%0Arobust%20model%20can%20be%20trained.%20We%20show%20that%20TAB%20improves%20worst-group%20performance%0Awithout%20any%20group%20information%20or%20model%20selection%2C%20outperforming%20existing%0Amethods%20while%20maintaining%20overall%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17691v1&entry.124074799=Read"},
{"title": "2D and 3D Deep Learning Models for MRI-based Parkinson's Disease\n  Classification: A Comparative Analysis of Convolutional Kolmogorov-Arnold\n  Networks, Convolutional Neural Networks, and Graph Convolutional Networks", "author": "Salil B Patel and Vicky Goh and James F FitzGerald and Chrystalina A Antoniades", "abstract": "  Parkinson's Disease (PD) diagnosis remains challenging. This study applies\nConvolutional Kolmogorov-Arnold Networks (ConvKANs), integrating learnable\nspline-based activation functions into convolutional layers, for PD\nclassification using structural MRI. The first 3D implementation of ConvKANs\nfor medical imaging is presented, comparing their performance to Convolutional\nNeural Networks (CNNs) and Graph Convolutional Networks (GCNs) across three\nopen-source datasets. Isolated analyses assessed performance within individual\ndatasets, using cross-validation techniques. Holdout analyses evaluated\ncross-dataset generalizability by training models on two datasets and testing\non the third, mirroring real-world clinical scenarios. In isolated analyses, 2D\nConvKANs achieved the highest AUC of 0.99 (95% CI: 0.98-0.99) on the PPMI\ndataset, outperforming 2D CNNs (AUC: 0.97, p = 0.0092). 3D models showed\npromise, with 3D CNN and 3D ConvKAN reaching an AUC of 0.85 on PPMI. In holdout\nanalyses, 3D ConvKAN demonstrated superior generalization, achieving an AUC of\n0.85 on early-stage PD data. GCNs underperformed in 2D but improved in 3D\nimplementations. These findings highlight ConvKANs' potential for PD detection,\nemphasize the importance of 3D analysis in capturing subtle brain changes, and\nunderscore cross-dataset generalization challenges. This study advances\nAI-assisted PD diagnosis using structural MRI and emphasizes the need for\nlarger-scale validation.\n", "link": "http://arxiv.org/abs/2407.17380v2", "date": "2024-09-26", "relevancy": 2.0944, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202D%20and%203D%20Deep%20Learning%20Models%20for%20MRI-based%20Parkinson%27s%20Disease%0A%20%20Classification%3A%20A%20Comparative%20Analysis%20of%20Convolutional%20Kolmogorov-Arnold%0A%20%20Networks%2C%20Convolutional%20Neural%20Networks%2C%20and%20Graph%20Convolutional%20Networks&body=Title%3A%202D%20and%203D%20Deep%20Learning%20Models%20for%20MRI-based%20Parkinson%27s%20Disease%0A%20%20Classification%3A%20A%20Comparative%20Analysis%20of%20Convolutional%20Kolmogorov-Arnold%0A%20%20Networks%2C%20Convolutional%20Neural%20Networks%2C%20and%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Salil%20B%20Patel%20and%20Vicky%20Goh%20and%20James%20F%20FitzGerald%20and%20Chrystalina%20A%20Antoniades%0AAbstract%3A%20%20%20Parkinson%27s%20Disease%20%28PD%29%20diagnosis%20remains%20challenging.%20This%20study%20applies%0AConvolutional%20Kolmogorov-Arnold%20Networks%20%28ConvKANs%29%2C%20integrating%20learnable%0Aspline-based%20activation%20functions%20into%20convolutional%20layers%2C%20for%20PD%0Aclassification%20using%20structural%20MRI.%20The%20first%203D%20implementation%20of%20ConvKANs%0Afor%20medical%20imaging%20is%20presented%2C%20comparing%20their%20performance%20to%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20and%20Graph%20Convolutional%20Networks%20%28GCNs%29%20across%20three%0Aopen-source%20datasets.%20Isolated%20analyses%20assessed%20performance%20within%20individual%0Adatasets%2C%20using%20cross-validation%20techniques.%20Holdout%20analyses%20evaluated%0Across-dataset%20generalizability%20by%20training%20models%20on%20two%20datasets%20and%20testing%0Aon%20the%20third%2C%20mirroring%20real-world%20clinical%20scenarios.%20In%20isolated%20analyses%2C%202D%0AConvKANs%20achieved%20the%20highest%20AUC%20of%200.99%20%2895%25%20CI%3A%200.98-0.99%29%20on%20the%20PPMI%0Adataset%2C%20outperforming%202D%20CNNs%20%28AUC%3A%200.97%2C%20p%20%3D%200.0092%29.%203D%20models%20showed%0Apromise%2C%20with%203D%20CNN%20and%203D%20ConvKAN%20reaching%20an%20AUC%20of%200.85%20on%20PPMI.%20In%20holdout%0Aanalyses%2C%203D%20ConvKAN%20demonstrated%20superior%20generalization%2C%20achieving%20an%20AUC%20of%0A0.85%20on%20early-stage%20PD%20data.%20GCNs%20underperformed%20in%202D%20but%20improved%20in%203D%0Aimplementations.%20These%20findings%20highlight%20ConvKANs%27%20potential%20for%20PD%20detection%2C%0Aemphasize%20the%20importance%20of%203D%20analysis%20in%20capturing%20subtle%20brain%20changes%2C%20and%0Aunderscore%20cross-dataset%20generalization%20challenges.%20This%20study%20advances%0AAI-assisted%20PD%20diagnosis%20using%20structural%20MRI%20and%20emphasizes%20the%20need%20for%0Alarger-scale%20validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2D%2520and%25203D%2520Deep%2520Learning%2520Models%2520for%2520MRI-based%2520Parkinson%2527s%2520Disease%250A%2520%2520Classification%253A%2520A%2520Comparative%2520Analysis%2520of%2520Convolutional%2520Kolmogorov-Arnold%250A%2520%2520Networks%252C%2520Convolutional%2520Neural%2520Networks%252C%2520and%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DSalil%2520B%2520Patel%2520and%2520Vicky%2520Goh%2520and%2520James%2520F%2520FitzGerald%2520and%2520Chrystalina%2520A%2520Antoniades%26entry.1292438233%3D%2520%2520Parkinson%2527s%2520Disease%2520%2528PD%2529%2520diagnosis%2520remains%2520challenging.%2520This%2520study%2520applies%250AConvolutional%2520Kolmogorov-Arnold%2520Networks%2520%2528ConvKANs%2529%252C%2520integrating%2520learnable%250Aspline-based%2520activation%2520functions%2520into%2520convolutional%2520layers%252C%2520for%2520PD%250Aclassification%2520using%2520structural%2520MRI.%2520The%2520first%25203D%2520implementation%2520of%2520ConvKANs%250Afor%2520medical%2520imaging%2520is%2520presented%252C%2520comparing%2520their%2520performance%2520to%2520Convolutional%250ANeural%2520Networks%2520%2528CNNs%2529%2520and%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520across%2520three%250Aopen-source%2520datasets.%2520Isolated%2520analyses%2520assessed%2520performance%2520within%2520individual%250Adatasets%252C%2520using%2520cross-validation%2520techniques.%2520Holdout%2520analyses%2520evaluated%250Across-dataset%2520generalizability%2520by%2520training%2520models%2520on%2520two%2520datasets%2520and%2520testing%250Aon%2520the%2520third%252C%2520mirroring%2520real-world%2520clinical%2520scenarios.%2520In%2520isolated%2520analyses%252C%25202D%250AConvKANs%2520achieved%2520the%2520highest%2520AUC%2520of%25200.99%2520%252895%2525%2520CI%253A%25200.98-0.99%2529%2520on%2520the%2520PPMI%250Adataset%252C%2520outperforming%25202D%2520CNNs%2520%2528AUC%253A%25200.97%252C%2520p%2520%253D%25200.0092%2529.%25203D%2520models%2520showed%250Apromise%252C%2520with%25203D%2520CNN%2520and%25203D%2520ConvKAN%2520reaching%2520an%2520AUC%2520of%25200.85%2520on%2520PPMI.%2520In%2520holdout%250Aanalyses%252C%25203D%2520ConvKAN%2520demonstrated%2520superior%2520generalization%252C%2520achieving%2520an%2520AUC%2520of%250A0.85%2520on%2520early-stage%2520PD%2520data.%2520GCNs%2520underperformed%2520in%25202D%2520but%2520improved%2520in%25203D%250Aimplementations.%2520These%2520findings%2520highlight%2520ConvKANs%2527%2520potential%2520for%2520PD%2520detection%252C%250Aemphasize%2520the%2520importance%2520of%25203D%2520analysis%2520in%2520capturing%2520subtle%2520brain%2520changes%252C%2520and%250Aunderscore%2520cross-dataset%2520generalization%2520challenges.%2520This%2520study%2520advances%250AAI-assisted%2520PD%2520diagnosis%2520using%2520structural%2520MRI%2520and%2520emphasizes%2520the%2520need%2520for%250Alarger-scale%2520validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2D%20and%203D%20Deep%20Learning%20Models%20for%20MRI-based%20Parkinson%27s%20Disease%0A%20%20Classification%3A%20A%20Comparative%20Analysis%20of%20Convolutional%20Kolmogorov-Arnold%0A%20%20Networks%2C%20Convolutional%20Neural%20Networks%2C%20and%20Graph%20Convolutional%20Networks&entry.906535625=Salil%20B%20Patel%20and%20Vicky%20Goh%20and%20James%20F%20FitzGerald%20and%20Chrystalina%20A%20Antoniades&entry.1292438233=%20%20Parkinson%27s%20Disease%20%28PD%29%20diagnosis%20remains%20challenging.%20This%20study%20applies%0AConvolutional%20Kolmogorov-Arnold%20Networks%20%28ConvKANs%29%2C%20integrating%20learnable%0Aspline-based%20activation%20functions%20into%20convolutional%20layers%2C%20for%20PD%0Aclassification%20using%20structural%20MRI.%20The%20first%203D%20implementation%20of%20ConvKANs%0Afor%20medical%20imaging%20is%20presented%2C%20comparing%20their%20performance%20to%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20and%20Graph%20Convolutional%20Networks%20%28GCNs%29%20across%20three%0Aopen-source%20datasets.%20Isolated%20analyses%20assessed%20performance%20within%20individual%0Adatasets%2C%20using%20cross-validation%20techniques.%20Holdout%20analyses%20evaluated%0Across-dataset%20generalizability%20by%20training%20models%20on%20two%20datasets%20and%20testing%0Aon%20the%20third%2C%20mirroring%20real-world%20clinical%20scenarios.%20In%20isolated%20analyses%2C%202D%0AConvKANs%20achieved%20the%20highest%20AUC%20of%200.99%20%2895%25%20CI%3A%200.98-0.99%29%20on%20the%20PPMI%0Adataset%2C%20outperforming%202D%20CNNs%20%28AUC%3A%200.97%2C%20p%20%3D%200.0092%29.%203D%20models%20showed%0Apromise%2C%20with%203D%20CNN%20and%203D%20ConvKAN%20reaching%20an%20AUC%20of%200.85%20on%20PPMI.%20In%20holdout%0Aanalyses%2C%203D%20ConvKAN%20demonstrated%20superior%20generalization%2C%20achieving%20an%20AUC%20of%0A0.85%20on%20early-stage%20PD%20data.%20GCNs%20underperformed%20in%202D%20but%20improved%20in%203D%0Aimplementations.%20These%20findings%20highlight%20ConvKANs%27%20potential%20for%20PD%20detection%2C%0Aemphasize%20the%20importance%20of%203D%20analysis%20in%20capturing%20subtle%20brain%20changes%2C%20and%0Aunderscore%20cross-dataset%20generalization%20challenges.%20This%20study%20advances%0AAI-assisted%20PD%20diagnosis%20using%20structural%20MRI%20and%20emphasizes%20the%20need%20for%0Alarger-scale%20validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17380v2&entry.124074799=Read"},
{"title": "Detecting and Measuring Confounding Using Causal Mechanism Shifts", "author": "Abbavaram Gowtham Reddy and Vineeth N Balasubramanian", "abstract": "  Detecting and measuring confounding effects from data is a key challenge in\ncausal inference. Existing methods frequently assume causal sufficiency,\ndisregarding the presence of unobserved confounding variables. Causal\nsufficiency is both unrealistic and empirically untestable. Additionally,\nexisting methods make strong parametric assumptions about the underlying causal\ngenerative process to guarantee the identifiability of confounding variables.\nRelaxing the causal sufficiency and parametric assumptions and leveraging\nrecent advancements in causal discovery and confounding analysis with\nnon-i.i.d. data, we propose a comprehensive approach for detecting and\nmeasuring confounding. We consider various definitions of confounding and\nintroduce tailored methodologies to achieve three objectives: (i) detecting and\nmeasuring confounding among a set of variables, (ii) separating observed and\nunobserved confounding effects, and (iii) understanding the relative strengths\nof confounding bias between different sets of variables. We present useful\nproperties of a confounding measure and present measures that satisfy those\nproperties. Empirical results support the theoretical analysis.\n", "link": "http://arxiv.org/abs/2409.17840v1", "date": "2024-09-26", "relevancy": 1.3289, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4548}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4454}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Measuring%20Confounding%20Using%20Causal%20Mechanism%20Shifts&body=Title%3A%20Detecting%20and%20Measuring%20Confounding%20Using%20Causal%20Mechanism%20Shifts%0AAuthor%3A%20Abbavaram%20Gowtham%20Reddy%20and%20Vineeth%20N%20Balasubramanian%0AAbstract%3A%20%20%20Detecting%20and%20measuring%20confounding%20effects%20from%20data%20is%20a%20key%20challenge%20in%0Acausal%20inference.%20Existing%20methods%20frequently%20assume%20causal%20sufficiency%2C%0Adisregarding%20the%20presence%20of%20unobserved%20confounding%20variables.%20Causal%0Asufficiency%20is%20both%20unrealistic%20and%20empirically%20untestable.%20Additionally%2C%0Aexisting%20methods%20make%20strong%20parametric%20assumptions%20about%20the%20underlying%20causal%0Agenerative%20process%20to%20guarantee%20the%20identifiability%20of%20confounding%20variables.%0ARelaxing%20the%20causal%20sufficiency%20and%20parametric%20assumptions%20and%20leveraging%0Arecent%20advancements%20in%20causal%20discovery%20and%20confounding%20analysis%20with%0Anon-i.i.d.%20data%2C%20we%20propose%20a%20comprehensive%20approach%20for%20detecting%20and%0Ameasuring%20confounding.%20We%20consider%20various%20definitions%20of%20confounding%20and%0Aintroduce%20tailored%20methodologies%20to%20achieve%20three%20objectives%3A%20%28i%29%20detecting%20and%0Ameasuring%20confounding%20among%20a%20set%20of%20variables%2C%20%28ii%29%20separating%20observed%20and%0Aunobserved%20confounding%20effects%2C%20and%20%28iii%29%20understanding%20the%20relative%20strengths%0Aof%20confounding%20bias%20between%20different%20sets%20of%20variables.%20We%20present%20useful%0Aproperties%20of%20a%20confounding%20measure%20and%20present%20measures%20that%20satisfy%20those%0Aproperties.%20Empirical%20results%20support%20the%20theoretical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Measuring%2520Confounding%2520Using%2520Causal%2520Mechanism%2520Shifts%26entry.906535625%3DAbbavaram%2520Gowtham%2520Reddy%2520and%2520Vineeth%2520N%2520Balasubramanian%26entry.1292438233%3D%2520%2520Detecting%2520and%2520measuring%2520confounding%2520effects%2520from%2520data%2520is%2520a%2520key%2520challenge%2520in%250Acausal%2520inference.%2520Existing%2520methods%2520frequently%2520assume%2520causal%2520sufficiency%252C%250Adisregarding%2520the%2520presence%2520of%2520unobserved%2520confounding%2520variables.%2520Causal%250Asufficiency%2520is%2520both%2520unrealistic%2520and%2520empirically%2520untestable.%2520Additionally%252C%250Aexisting%2520methods%2520make%2520strong%2520parametric%2520assumptions%2520about%2520the%2520underlying%2520causal%250Agenerative%2520process%2520to%2520guarantee%2520the%2520identifiability%2520of%2520confounding%2520variables.%250ARelaxing%2520the%2520causal%2520sufficiency%2520and%2520parametric%2520assumptions%2520and%2520leveraging%250Arecent%2520advancements%2520in%2520causal%2520discovery%2520and%2520confounding%2520analysis%2520with%250Anon-i.i.d.%2520data%252C%2520we%2520propose%2520a%2520comprehensive%2520approach%2520for%2520detecting%2520and%250Ameasuring%2520confounding.%2520We%2520consider%2520various%2520definitions%2520of%2520confounding%2520and%250Aintroduce%2520tailored%2520methodologies%2520to%2520achieve%2520three%2520objectives%253A%2520%2528i%2529%2520detecting%2520and%250Ameasuring%2520confounding%2520among%2520a%2520set%2520of%2520variables%252C%2520%2528ii%2529%2520separating%2520observed%2520and%250Aunobserved%2520confounding%2520effects%252C%2520and%2520%2528iii%2529%2520understanding%2520the%2520relative%2520strengths%250Aof%2520confounding%2520bias%2520between%2520different%2520sets%2520of%2520variables.%2520We%2520present%2520useful%250Aproperties%2520of%2520a%2520confounding%2520measure%2520and%2520present%2520measures%2520that%2520satisfy%2520those%250Aproperties.%2520Empirical%2520results%2520support%2520the%2520theoretical%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Measuring%20Confounding%20Using%20Causal%20Mechanism%20Shifts&entry.906535625=Abbavaram%20Gowtham%20Reddy%20and%20Vineeth%20N%20Balasubramanian&entry.1292438233=%20%20Detecting%20and%20measuring%20confounding%20effects%20from%20data%20is%20a%20key%20challenge%20in%0Acausal%20inference.%20Existing%20methods%20frequently%20assume%20causal%20sufficiency%2C%0Adisregarding%20the%20presence%20of%20unobserved%20confounding%20variables.%20Causal%0Asufficiency%20is%20both%20unrealistic%20and%20empirically%20untestable.%20Additionally%2C%0Aexisting%20methods%20make%20strong%20parametric%20assumptions%20about%20the%20underlying%20causal%0Agenerative%20process%20to%20guarantee%20the%20identifiability%20of%20confounding%20variables.%0ARelaxing%20the%20causal%20sufficiency%20and%20parametric%20assumptions%20and%20leveraging%0Arecent%20advancements%20in%20causal%20discovery%20and%20confounding%20analysis%20with%0Anon-i.i.d.%20data%2C%20we%20propose%20a%20comprehensive%20approach%20for%20detecting%20and%0Ameasuring%20confounding.%20We%20consider%20various%20definitions%20of%20confounding%20and%0Aintroduce%20tailored%20methodologies%20to%20achieve%20three%20objectives%3A%20%28i%29%20detecting%20and%0Ameasuring%20confounding%20among%20a%20set%20of%20variables%2C%20%28ii%29%20separating%20observed%20and%0Aunobserved%20confounding%20effects%2C%20and%20%28iii%29%20understanding%20the%20relative%20strengths%0Aof%20confounding%20bias%20between%20different%20sets%20of%20variables.%20We%20present%20useful%0Aproperties%20of%20a%20confounding%20measure%20and%20present%20measures%20that%20satisfy%20those%0Aproperties.%20Empirical%20results%20support%20the%20theoretical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17840v1&entry.124074799=Read"},
{"title": "How Feature Learning Can Improve Neural Scaling Laws", "author": "Blake Bordelon and Alexander Atanasov and Cengiz Pehlevan", "abstract": "  We develop a solvable model of neural scaling laws beyond the kernel limit.\nTheoretical analysis of this model shows how performance scales with model\nsize, training time, and the total amount of available data. We identify three\nscaling regimes corresponding to varying task difficulties: hard, easy, and\nsuper easy tasks. For easy and super-easy target functions, which lie in the\nreproducing kernel Hilbert space (RKHS) defined by the initial infinite-width\nNeural Tangent Kernel (NTK), the scaling exponents remain unchanged between\nfeature learning and kernel regime models. For hard tasks, defined as those\noutside the RKHS of the initial NTK, we demonstrate both analytically and\nempirically that feature learning can improve scaling with training time and\ncompute, nearly doubling the exponent for hard tasks. This leads to a different\ncompute optimal strategy to scale parameters and training time in the feature\nlearning regime. We support our finding that feature learning improves the\nscaling law for hard tasks but not for easy and super-easy tasks with\nexperiments of nonlinear MLPs fitting functions with power-law Fourier spectra\non the circle and CNNs learning vision tasks.\n", "link": "http://arxiv.org/abs/2409.17858v1", "date": "2024-09-26", "relevancy": 1.4611, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5258}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Feature%20Learning%20Can%20Improve%20Neural%20Scaling%20Laws&body=Title%3A%20How%20Feature%20Learning%20Can%20Improve%20Neural%20Scaling%20Laws%0AAuthor%3A%20Blake%20Bordelon%20and%20Alexander%20Atanasov%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20We%20develop%20a%20solvable%20model%20of%20neural%20scaling%20laws%20beyond%20the%20kernel%20limit.%0ATheoretical%20analysis%20of%20this%20model%20shows%20how%20performance%20scales%20with%20model%0Asize%2C%20training%20time%2C%20and%20the%20total%20amount%20of%20available%20data.%20We%20identify%20three%0Ascaling%20regimes%20corresponding%20to%20varying%20task%20difficulties%3A%20hard%2C%20easy%2C%20and%0Asuper%20easy%20tasks.%20For%20easy%20and%20super-easy%20target%20functions%2C%20which%20lie%20in%20the%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20defined%20by%20the%20initial%20infinite-width%0ANeural%20Tangent%20Kernel%20%28NTK%29%2C%20the%20scaling%20exponents%20remain%20unchanged%20between%0Afeature%20learning%20and%20kernel%20regime%20models.%20For%20hard%20tasks%2C%20defined%20as%20those%0Aoutside%20the%20RKHS%20of%20the%20initial%20NTK%2C%20we%20demonstrate%20both%20analytically%20and%0Aempirically%20that%20feature%20learning%20can%20improve%20scaling%20with%20training%20time%20and%0Acompute%2C%20nearly%20doubling%20the%20exponent%20for%20hard%20tasks.%20This%20leads%20to%20a%20different%0Acompute%20optimal%20strategy%20to%20scale%20parameters%20and%20training%20time%20in%20the%20feature%0Alearning%20regime.%20We%20support%20our%20finding%20that%20feature%20learning%20improves%20the%0Ascaling%20law%20for%20hard%20tasks%20but%20not%20for%20easy%20and%20super-easy%20tasks%20with%0Aexperiments%20of%20nonlinear%20MLPs%20fitting%20functions%20with%20power-law%20Fourier%20spectra%0Aon%20the%20circle%20and%20CNNs%20learning%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Feature%2520Learning%2520Can%2520Improve%2520Neural%2520Scaling%2520Laws%26entry.906535625%3DBlake%2520Bordelon%2520and%2520Alexander%2520Atanasov%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520solvable%2520model%2520of%2520neural%2520scaling%2520laws%2520beyond%2520the%2520kernel%2520limit.%250ATheoretical%2520analysis%2520of%2520this%2520model%2520shows%2520how%2520performance%2520scales%2520with%2520model%250Asize%252C%2520training%2520time%252C%2520and%2520the%2520total%2520amount%2520of%2520available%2520data.%2520We%2520identify%2520three%250Ascaling%2520regimes%2520corresponding%2520to%2520varying%2520task%2520difficulties%253A%2520hard%252C%2520easy%252C%2520and%250Asuper%2520easy%2520tasks.%2520For%2520easy%2520and%2520super-easy%2520target%2520functions%252C%2520which%2520lie%2520in%2520the%250Areproducing%2520kernel%2520Hilbert%2520space%2520%2528RKHS%2529%2520defined%2520by%2520the%2520initial%2520infinite-width%250ANeural%2520Tangent%2520Kernel%2520%2528NTK%2529%252C%2520the%2520scaling%2520exponents%2520remain%2520unchanged%2520between%250Afeature%2520learning%2520and%2520kernel%2520regime%2520models.%2520For%2520hard%2520tasks%252C%2520defined%2520as%2520those%250Aoutside%2520the%2520RKHS%2520of%2520the%2520initial%2520NTK%252C%2520we%2520demonstrate%2520both%2520analytically%2520and%250Aempirically%2520that%2520feature%2520learning%2520can%2520improve%2520scaling%2520with%2520training%2520time%2520and%250Acompute%252C%2520nearly%2520doubling%2520the%2520exponent%2520for%2520hard%2520tasks.%2520This%2520leads%2520to%2520a%2520different%250Acompute%2520optimal%2520strategy%2520to%2520scale%2520parameters%2520and%2520training%2520time%2520in%2520the%2520feature%250Alearning%2520regime.%2520We%2520support%2520our%2520finding%2520that%2520feature%2520learning%2520improves%2520the%250Ascaling%2520law%2520for%2520hard%2520tasks%2520but%2520not%2520for%2520easy%2520and%2520super-easy%2520tasks%2520with%250Aexperiments%2520of%2520nonlinear%2520MLPs%2520fitting%2520functions%2520with%2520power-law%2520Fourier%2520spectra%250Aon%2520the%2520circle%2520and%2520CNNs%2520learning%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Feature%20Learning%20Can%20Improve%20Neural%20Scaling%20Laws&entry.906535625=Blake%20Bordelon%20and%20Alexander%20Atanasov%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20We%20develop%20a%20solvable%20model%20of%20neural%20scaling%20laws%20beyond%20the%20kernel%20limit.%0ATheoretical%20analysis%20of%20this%20model%20shows%20how%20performance%20scales%20with%20model%0Asize%2C%20training%20time%2C%20and%20the%20total%20amount%20of%20available%20data.%20We%20identify%20three%0Ascaling%20regimes%20corresponding%20to%20varying%20task%20difficulties%3A%20hard%2C%20easy%2C%20and%0Asuper%20easy%20tasks.%20For%20easy%20and%20super-easy%20target%20functions%2C%20which%20lie%20in%20the%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20defined%20by%20the%20initial%20infinite-width%0ANeural%20Tangent%20Kernel%20%28NTK%29%2C%20the%20scaling%20exponents%20remain%20unchanged%20between%0Afeature%20learning%20and%20kernel%20regime%20models.%20For%20hard%20tasks%2C%20defined%20as%20those%0Aoutside%20the%20RKHS%20of%20the%20initial%20NTK%2C%20we%20demonstrate%20both%20analytically%20and%0Aempirically%20that%20feature%20learning%20can%20improve%20scaling%20with%20training%20time%20and%0Acompute%2C%20nearly%20doubling%20the%20exponent%20for%20hard%20tasks.%20This%20leads%20to%20a%20different%0Acompute%20optimal%20strategy%20to%20scale%20parameters%20and%20training%20time%20in%20the%20feature%0Alearning%20regime.%20We%20support%20our%20finding%20that%20feature%20learning%20improves%20the%0Ascaling%20law%20for%20hard%20tasks%20but%20not%20for%20easy%20and%20super-easy%20tasks%20with%0Aexperiments%20of%20nonlinear%20MLPs%20fitting%20functions%20with%20power-law%20Fourier%20spectra%0Aon%20the%20circle%20and%20CNNs%20learning%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17858v1&entry.124074799=Read"},
{"title": "Federated Learning under Attack: Improving Gradient Inversion for Batch\n  of Images", "author": "Luiz Leite and Yuri Santo and Bruno L. Dalmazo and Andr\u00e9 Riker", "abstract": "  Federated Learning (FL) has emerged as a machine learning approach able to\npreserve the privacy of user's data. Applying FL, clients train machine\nlearning models on a local dataset and a central server aggregates the learned\nparameters coming from the clients, training a global machine learning model\nwithout sharing user's data. However, the state-of-the-art shows several\napproaches to promote attacks on FL systems. For instance, inverting or leaking\ngradient attacks can find, with high precision, the local dataset used during\nthe training phase of the FL. This paper presents an approach, called Deep\nLeakage from Gradients with Feedback Blending (DLG-FB), which is able to\nimprove the inverting gradient attack, considering the spatial correlation that\ntypically exists in batches of images. The performed evaluation shows an\nimprovement of 19.18% and 48,82% in terms of attack success rate and the number\nof iterations per attacked image, respectively.\n", "link": "http://arxiv.org/abs/2409.17767v1", "date": "2024-09-26", "relevancy": 1.4855, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5048}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4935}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20under%20Attack%3A%20Improving%20Gradient%20Inversion%20for%20Batch%0A%20%20of%20Images&body=Title%3A%20Federated%20Learning%20under%20Attack%3A%20Improving%20Gradient%20Inversion%20for%20Batch%0A%20%20of%20Images%0AAuthor%3A%20Luiz%20Leite%20and%20Yuri%20Santo%20and%20Bruno%20L.%20Dalmazo%20and%20Andr%C3%A9%20Riker%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20machine%20learning%20approach%20able%20to%0Apreserve%20the%20privacy%20of%20user%27s%20data.%20Applying%20FL%2C%20clients%20train%20machine%0Alearning%20models%20on%20a%20local%20dataset%20and%20a%20central%20server%20aggregates%20the%20learned%0Aparameters%20coming%20from%20the%20clients%2C%20training%20a%20global%20machine%20learning%20model%0Awithout%20sharing%20user%27s%20data.%20However%2C%20the%20state-of-the-art%20shows%20several%0Aapproaches%20to%20promote%20attacks%20on%20FL%20systems.%20For%20instance%2C%20inverting%20or%20leaking%0Agradient%20attacks%20can%20find%2C%20with%20high%20precision%2C%20the%20local%20dataset%20used%20during%0Athe%20training%20phase%20of%20the%20FL.%20This%20paper%20presents%20an%20approach%2C%20called%20Deep%0ALeakage%20from%20Gradients%20with%20Feedback%20Blending%20%28DLG-FB%29%2C%20which%20is%20able%20to%0Aimprove%20the%20inverting%20gradient%20attack%2C%20considering%20the%20spatial%20correlation%20that%0Atypically%20exists%20in%20batches%20of%20images.%20The%20performed%20evaluation%20shows%20an%0Aimprovement%20of%2019.18%25%20and%2048%2C82%25%20in%20terms%20of%20attack%20success%20rate%20and%20the%20number%0Aof%20iterations%20per%20attacked%20image%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520under%2520Attack%253A%2520Improving%2520Gradient%2520Inversion%2520for%2520Batch%250A%2520%2520of%2520Images%26entry.906535625%3DLuiz%2520Leite%2520and%2520Yuri%2520Santo%2520and%2520Bruno%2520L.%2520Dalmazo%2520and%2520Andr%25C3%25A9%2520Riker%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520machine%2520learning%2520approach%2520able%2520to%250Apreserve%2520the%2520privacy%2520of%2520user%2527s%2520data.%2520Applying%2520FL%252C%2520clients%2520train%2520machine%250Alearning%2520models%2520on%2520a%2520local%2520dataset%2520and%2520a%2520central%2520server%2520aggregates%2520the%2520learned%250Aparameters%2520coming%2520from%2520the%2520clients%252C%2520training%2520a%2520global%2520machine%2520learning%2520model%250Awithout%2520sharing%2520user%2527s%2520data.%2520However%252C%2520the%2520state-of-the-art%2520shows%2520several%250Aapproaches%2520to%2520promote%2520attacks%2520on%2520FL%2520systems.%2520For%2520instance%252C%2520inverting%2520or%2520leaking%250Agradient%2520attacks%2520can%2520find%252C%2520with%2520high%2520precision%252C%2520the%2520local%2520dataset%2520used%2520during%250Athe%2520training%2520phase%2520of%2520the%2520FL.%2520This%2520paper%2520presents%2520an%2520approach%252C%2520called%2520Deep%250ALeakage%2520from%2520Gradients%2520with%2520Feedback%2520Blending%2520%2528DLG-FB%2529%252C%2520which%2520is%2520able%2520to%250Aimprove%2520the%2520inverting%2520gradient%2520attack%252C%2520considering%2520the%2520spatial%2520correlation%2520that%250Atypically%2520exists%2520in%2520batches%2520of%2520images.%2520The%2520performed%2520evaluation%2520shows%2520an%250Aimprovement%2520of%252019.18%2525%2520and%252048%252C82%2525%2520in%2520terms%2520of%2520attack%2520success%2520rate%2520and%2520the%2520number%250Aof%2520iterations%2520per%2520attacked%2520image%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20under%20Attack%3A%20Improving%20Gradient%20Inversion%20for%20Batch%0A%20%20of%20Images&entry.906535625=Luiz%20Leite%20and%20Yuri%20Santo%20and%20Bruno%20L.%20Dalmazo%20and%20Andr%C3%A9%20Riker&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20machine%20learning%20approach%20able%20to%0Apreserve%20the%20privacy%20of%20user%27s%20data.%20Applying%20FL%2C%20clients%20train%20machine%0Alearning%20models%20on%20a%20local%20dataset%20and%20a%20central%20server%20aggregates%20the%20learned%0Aparameters%20coming%20from%20the%20clients%2C%20training%20a%20global%20machine%20learning%20model%0Awithout%20sharing%20user%27s%20data.%20However%2C%20the%20state-of-the-art%20shows%20several%0Aapproaches%20to%20promote%20attacks%20on%20FL%20systems.%20For%20instance%2C%20inverting%20or%20leaking%0Agradient%20attacks%20can%20find%2C%20with%20high%20precision%2C%20the%20local%20dataset%20used%20during%0Athe%20training%20phase%20of%20the%20FL.%20This%20paper%20presents%20an%20approach%2C%20called%20Deep%0ALeakage%20from%20Gradients%20with%20Feedback%20Blending%20%28DLG-FB%29%2C%20which%20is%20able%20to%0Aimprove%20the%20inverting%20gradient%20attack%2C%20considering%20the%20spatial%20correlation%20that%0Atypically%20exists%20in%20batches%20of%20images.%20The%20performed%20evaluation%20shows%20an%0Aimprovement%20of%2019.18%25%20and%2048%2C82%25%20in%20terms%20of%20attack%20success%20rate%20and%20the%20number%0Aof%20iterations%20per%20attacked%20image%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17767v1&entry.124074799=Read"},
{"title": "Investigating OCR-Sensitive Neurons to Improve Entity Recognition in\n  Historical Documents", "author": "Emanuela Boros and Maud Ehrmann", "abstract": "  This paper investigates the presence of OCR-sensitive neurons within the\nTransformer architecture and their influence on named entity recognition (NER)\nperformance on historical documents. By analysing neuron activation patterns in\nresponse to clean and noisy text inputs, we identify and then neutralise\nOCR-sensitive neurons to improve model performance. Based on two open access\nlarge language models (Llama2 and Mistral), experiments demonstrate the\nexistence of OCR-sensitive regions and show improvements in NER performance on\nhistorical newspapers and classical commentaries, highlighting the potential of\ntargeted neuron modulation to improve models' performance on noisy text.\n", "link": "http://arxiv.org/abs/2409.16934v2", "date": "2024-09-26", "relevancy": 1.9222, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5093}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents&body=Title%3A%20Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents%0AAuthor%3A%20Emanuela%20Boros%20and%20Maud%20Ehrmann%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20presence%20of%20OCR-sensitive%20neurons%20within%20the%0ATransformer%20architecture%20and%20their%20influence%20on%20named%20entity%20recognition%20%28NER%29%0Aperformance%20on%20historical%20documents.%20By%20analysing%20neuron%20activation%20patterns%20in%0Aresponse%20to%20clean%20and%20noisy%20text%20inputs%2C%20we%20identify%20and%20then%20neutralise%0AOCR-sensitive%20neurons%20to%20improve%20model%20performance.%20Based%20on%20two%20open%20access%0Alarge%20language%20models%20%28Llama2%20and%20Mistral%29%2C%20experiments%20demonstrate%20the%0Aexistence%20of%20OCR-sensitive%20regions%20and%20show%20improvements%20in%20NER%20performance%20on%0Ahistorical%20newspapers%20and%20classical%20commentaries%2C%20highlighting%20the%20potential%20of%0Atargeted%20neuron%20modulation%20to%20improve%20models%27%20performance%20on%20noisy%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16934v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520OCR-Sensitive%2520Neurons%2520to%2520Improve%2520Entity%2520Recognition%2520in%250A%2520%2520Historical%2520Documents%26entry.906535625%3DEmanuela%2520Boros%2520and%2520Maud%2520Ehrmann%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520presence%2520of%2520OCR-sensitive%2520neurons%2520within%2520the%250ATransformer%2520architecture%2520and%2520their%2520influence%2520on%2520named%2520entity%2520recognition%2520%2528NER%2529%250Aperformance%2520on%2520historical%2520documents.%2520By%2520analysing%2520neuron%2520activation%2520patterns%2520in%250Aresponse%2520to%2520clean%2520and%2520noisy%2520text%2520inputs%252C%2520we%2520identify%2520and%2520then%2520neutralise%250AOCR-sensitive%2520neurons%2520to%2520improve%2520model%2520performance.%2520Based%2520on%2520two%2520open%2520access%250Alarge%2520language%2520models%2520%2528Llama2%2520and%2520Mistral%2529%252C%2520experiments%2520demonstrate%2520the%250Aexistence%2520of%2520OCR-sensitive%2520regions%2520and%2520show%2520improvements%2520in%2520NER%2520performance%2520on%250Ahistorical%2520newspapers%2520and%2520classical%2520commentaries%252C%2520highlighting%2520the%2520potential%2520of%250Atargeted%2520neuron%2520modulation%2520to%2520improve%2520models%2527%2520performance%2520on%2520noisy%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16934v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents&entry.906535625=Emanuela%20Boros%20and%20Maud%20Ehrmann&entry.1292438233=%20%20This%20paper%20investigates%20the%20presence%20of%20OCR-sensitive%20neurons%20within%20the%0ATransformer%20architecture%20and%20their%20influence%20on%20named%20entity%20recognition%20%28NER%29%0Aperformance%20on%20historical%20documents.%20By%20analysing%20neuron%20activation%20patterns%20in%0Aresponse%20to%20clean%20and%20noisy%20text%20inputs%2C%20we%20identify%20and%20then%20neutralise%0AOCR-sensitive%20neurons%20to%20improve%20model%20performance.%20Based%20on%20two%20open%20access%0Alarge%20language%20models%20%28Llama2%20and%20Mistral%29%2C%20experiments%20demonstrate%20the%0Aexistence%20of%20OCR-sensitive%20regions%20and%20show%20improvements%20in%20NER%20performance%20on%0Ahistorical%20newspapers%20and%20classical%20commentaries%2C%20highlighting%20the%20potential%20of%0Atargeted%20neuron%20modulation%20to%20improve%20models%27%20performance%20on%20noisy%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16934v2&entry.124074799=Read"},
{"title": "UNICORN: A Deep Learning Model for Integrating Multi-Stain Data in\n  Histopathology", "author": "Valentin Koch and Sabine Bauer and Valerio Luppberger and Michael Joner and Heribert Schunkert and Julia A. Schnabel and Moritz von Scheidt and Carsten Marr", "abstract": "  Background: The integration of multi-stain histopathology images through deep\nlearning poses a significant challenge in digital histopathology. Current\nmulti-modal approaches struggle with data heterogeneity and missing data. This\nstudy aims to overcome these limitations by developing a novel transformer\nmodel for multi-stain integration that can handle missing data during training\nas well as inference. Methods: We propose UNICORN (UNiversal modality\nIntegration Network for CORonary classificatioN) a multi-modal transformer\ncapable of processing multi-stain histopathology for atherosclerosis severity\nclass prediction. The architecture comprises a two-stage, end-to-end trainable\nmodel with specialized modules utilizing transformer self-attention blocks. The\ninitial stage employs domain-specific expert modules to extract features from\neach modality. In the subsequent stage, an aggregation expert module integrates\nthese features by learning the interactions between the different data\nmodalities. Results: Evaluation was performed using a multi-class dataset of\natherosclerotic lesions from the Munich Cardiovascular Studies Biobank\n(MISSION), using over 4,000 paired multi-stain whole slide images (WSIs) from\n170 deceased individuals on 7 prespecified segments of the coronary tree, each\nstained according to four histopathological protocols. UNICORN achieved a\nclassification accuracy of 0.67, outperforming other state-of-the-art models.\nThe model effectively identifies relevant tissue phenotypes across stainings\nand implicitly models disease progression. Conclusion: Our proposed multi-modal\ntransformer model addresses key challenges in medical data analysis, including\ndata heterogeneity and missing modalities. Explainability and the model's\neffectiveness in predicting atherosclerosis progression underscores its\npotential for broader applications in medical research.\n", "link": "http://arxiv.org/abs/2409.17775v1", "date": "2024-09-26", "relevancy": 1.5936, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5085}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNICORN%3A%20A%20Deep%20Learning%20Model%20for%20Integrating%20Multi-Stain%20Data%20in%0A%20%20Histopathology&body=Title%3A%20UNICORN%3A%20A%20Deep%20Learning%20Model%20for%20Integrating%20Multi-Stain%20Data%20in%0A%20%20Histopathology%0AAuthor%3A%20Valentin%20Koch%20and%20Sabine%20Bauer%20and%20Valerio%20Luppberger%20and%20Michael%20Joner%20and%20Heribert%20Schunkert%20and%20Julia%20A.%20Schnabel%20and%20Moritz%20von%20Scheidt%20and%20Carsten%20Marr%0AAbstract%3A%20%20%20Background%3A%20The%20integration%20of%20multi-stain%20histopathology%20images%20through%20deep%0Alearning%20poses%20a%20significant%20challenge%20in%20digital%20histopathology.%20Current%0Amulti-modal%20approaches%20struggle%20with%20data%20heterogeneity%20and%20missing%20data.%20This%0Astudy%20aims%20to%20overcome%20these%20limitations%20by%20developing%20a%20novel%20transformer%0Amodel%20for%20multi-stain%20integration%20that%20can%20handle%20missing%20data%20during%20training%0Aas%20well%20as%20inference.%20Methods%3A%20We%20propose%20UNICORN%20%28UNiversal%20modality%0AIntegration%20Network%20for%20CORonary%20classificatioN%29%20a%20multi-modal%20transformer%0Acapable%20of%20processing%20multi-stain%20histopathology%20for%20atherosclerosis%20severity%0Aclass%20prediction.%20The%20architecture%20comprises%20a%20two-stage%2C%20end-to-end%20trainable%0Amodel%20with%20specialized%20modules%20utilizing%20transformer%20self-attention%20blocks.%20The%0Ainitial%20stage%20employs%20domain-specific%20expert%20modules%20to%20extract%20features%20from%0Aeach%20modality.%20In%20the%20subsequent%20stage%2C%20an%20aggregation%20expert%20module%20integrates%0Athese%20features%20by%20learning%20the%20interactions%20between%20the%20different%20data%0Amodalities.%20Results%3A%20Evaluation%20was%20performed%20using%20a%20multi-class%20dataset%20of%0Aatherosclerotic%20lesions%20from%20the%20Munich%20Cardiovascular%20Studies%20Biobank%0A%28MISSION%29%2C%20using%20over%204%2C000%20paired%20multi-stain%20whole%20slide%20images%20%28WSIs%29%20from%0A170%20deceased%20individuals%20on%207%20prespecified%20segments%20of%20the%20coronary%20tree%2C%20each%0Astained%20according%20to%20four%20histopathological%20protocols.%20UNICORN%20achieved%20a%0Aclassification%20accuracy%20of%200.67%2C%20outperforming%20other%20state-of-the-art%20models.%0AThe%20model%20effectively%20identifies%20relevant%20tissue%20phenotypes%20across%20stainings%0Aand%20implicitly%20models%20disease%20progression.%20Conclusion%3A%20Our%20proposed%20multi-modal%0Atransformer%20model%20addresses%20key%20challenges%20in%20medical%20data%20analysis%2C%20including%0Adata%20heterogeneity%20and%20missing%20modalities.%20Explainability%20and%20the%20model%27s%0Aeffectiveness%20in%20predicting%20atherosclerosis%20progression%20underscores%20its%0Apotential%20for%20broader%20applications%20in%20medical%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNICORN%253A%2520A%2520Deep%2520Learning%2520Model%2520for%2520Integrating%2520Multi-Stain%2520Data%2520in%250A%2520%2520Histopathology%26entry.906535625%3DValentin%2520Koch%2520and%2520Sabine%2520Bauer%2520and%2520Valerio%2520Luppberger%2520and%2520Michael%2520Joner%2520and%2520Heribert%2520Schunkert%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Moritz%2520von%2520Scheidt%2520and%2520Carsten%2520Marr%26entry.1292438233%3D%2520%2520Background%253A%2520The%2520integration%2520of%2520multi-stain%2520histopathology%2520images%2520through%2520deep%250Alearning%2520poses%2520a%2520significant%2520challenge%2520in%2520digital%2520histopathology.%2520Current%250Amulti-modal%2520approaches%2520struggle%2520with%2520data%2520heterogeneity%2520and%2520missing%2520data.%2520This%250Astudy%2520aims%2520to%2520overcome%2520these%2520limitations%2520by%2520developing%2520a%2520novel%2520transformer%250Amodel%2520for%2520multi-stain%2520integration%2520that%2520can%2520handle%2520missing%2520data%2520during%2520training%250Aas%2520well%2520as%2520inference.%2520Methods%253A%2520We%2520propose%2520UNICORN%2520%2528UNiversal%2520modality%250AIntegration%2520Network%2520for%2520CORonary%2520classificatioN%2529%2520a%2520multi-modal%2520transformer%250Acapable%2520of%2520processing%2520multi-stain%2520histopathology%2520for%2520atherosclerosis%2520severity%250Aclass%2520prediction.%2520The%2520architecture%2520comprises%2520a%2520two-stage%252C%2520end-to-end%2520trainable%250Amodel%2520with%2520specialized%2520modules%2520utilizing%2520transformer%2520self-attention%2520blocks.%2520The%250Ainitial%2520stage%2520employs%2520domain-specific%2520expert%2520modules%2520to%2520extract%2520features%2520from%250Aeach%2520modality.%2520In%2520the%2520subsequent%2520stage%252C%2520an%2520aggregation%2520expert%2520module%2520integrates%250Athese%2520features%2520by%2520learning%2520the%2520interactions%2520between%2520the%2520different%2520data%250Amodalities.%2520Results%253A%2520Evaluation%2520was%2520performed%2520using%2520a%2520multi-class%2520dataset%2520of%250Aatherosclerotic%2520lesions%2520from%2520the%2520Munich%2520Cardiovascular%2520Studies%2520Biobank%250A%2528MISSION%2529%252C%2520using%2520over%25204%252C000%2520paired%2520multi-stain%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520from%250A170%2520deceased%2520individuals%2520on%25207%2520prespecified%2520segments%2520of%2520the%2520coronary%2520tree%252C%2520each%250Astained%2520according%2520to%2520four%2520histopathological%2520protocols.%2520UNICORN%2520achieved%2520a%250Aclassification%2520accuracy%2520of%25200.67%252C%2520outperforming%2520other%2520state-of-the-art%2520models.%250AThe%2520model%2520effectively%2520identifies%2520relevant%2520tissue%2520phenotypes%2520across%2520stainings%250Aand%2520implicitly%2520models%2520disease%2520progression.%2520Conclusion%253A%2520Our%2520proposed%2520multi-modal%250Atransformer%2520model%2520addresses%2520key%2520challenges%2520in%2520medical%2520data%2520analysis%252C%2520including%250Adata%2520heterogeneity%2520and%2520missing%2520modalities.%2520Explainability%2520and%2520the%2520model%2527s%250Aeffectiveness%2520in%2520predicting%2520atherosclerosis%2520progression%2520underscores%2520its%250Apotential%2520for%2520broader%2520applications%2520in%2520medical%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNICORN%3A%20A%20Deep%20Learning%20Model%20for%20Integrating%20Multi-Stain%20Data%20in%0A%20%20Histopathology&entry.906535625=Valentin%20Koch%20and%20Sabine%20Bauer%20and%20Valerio%20Luppberger%20and%20Michael%20Joner%20and%20Heribert%20Schunkert%20and%20Julia%20A.%20Schnabel%20and%20Moritz%20von%20Scheidt%20and%20Carsten%20Marr&entry.1292438233=%20%20Background%3A%20The%20integration%20of%20multi-stain%20histopathology%20images%20through%20deep%0Alearning%20poses%20a%20significant%20challenge%20in%20digital%20histopathology.%20Current%0Amulti-modal%20approaches%20struggle%20with%20data%20heterogeneity%20and%20missing%20data.%20This%0Astudy%20aims%20to%20overcome%20these%20limitations%20by%20developing%20a%20novel%20transformer%0Amodel%20for%20multi-stain%20integration%20that%20can%20handle%20missing%20data%20during%20training%0Aas%20well%20as%20inference.%20Methods%3A%20We%20propose%20UNICORN%20%28UNiversal%20modality%0AIntegration%20Network%20for%20CORonary%20classificatioN%29%20a%20multi-modal%20transformer%0Acapable%20of%20processing%20multi-stain%20histopathology%20for%20atherosclerosis%20severity%0Aclass%20prediction.%20The%20architecture%20comprises%20a%20two-stage%2C%20end-to-end%20trainable%0Amodel%20with%20specialized%20modules%20utilizing%20transformer%20self-attention%20blocks.%20The%0Ainitial%20stage%20employs%20domain-specific%20expert%20modules%20to%20extract%20features%20from%0Aeach%20modality.%20In%20the%20subsequent%20stage%2C%20an%20aggregation%20expert%20module%20integrates%0Athese%20features%20by%20learning%20the%20interactions%20between%20the%20different%20data%0Amodalities.%20Results%3A%20Evaluation%20was%20performed%20using%20a%20multi-class%20dataset%20of%0Aatherosclerotic%20lesions%20from%20the%20Munich%20Cardiovascular%20Studies%20Biobank%0A%28MISSION%29%2C%20using%20over%204%2C000%20paired%20multi-stain%20whole%20slide%20images%20%28WSIs%29%20from%0A170%20deceased%20individuals%20on%207%20prespecified%20segments%20of%20the%20coronary%20tree%2C%20each%0Astained%20according%20to%20four%20histopathological%20protocols.%20UNICORN%20achieved%20a%0Aclassification%20accuracy%20of%200.67%2C%20outperforming%20other%20state-of-the-art%20models.%0AThe%20model%20effectively%20identifies%20relevant%20tissue%20phenotypes%20across%20stainings%0Aand%20implicitly%20models%20disease%20progression.%20Conclusion%3A%20Our%20proposed%20multi-modal%0Atransformer%20model%20addresses%20key%20challenges%20in%20medical%20data%20analysis%2C%20including%0Adata%20heterogeneity%20and%20missing%20modalities.%20Explainability%20and%20the%20model%27s%0Aeffectiveness%20in%20predicting%20atherosclerosis%20progression%20underscores%20its%0Apotential%20for%20broader%20applications%20in%20medical%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17775v1&entry.124074799=Read"},
{"title": "Learning to Love Edge Cases in Formative Math Assessment: Using the\n  AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy", "author": "Owen Henkel and Hannah Horne-Robinson and Maria Dyshel and Nabil Ch and Baptiste Moreau-Pernet and Ralph Abood", "abstract": "  This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment.\n", "link": "http://arxiv.org/abs/2409.17904v1", "date": "2024-09-26", "relevancy": 1.4046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4766}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4601}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Love%20Edge%20Cases%20in%20Formative%20Math%20Assessment%3A%20Using%20the%0A%20%20AMMORE%20Dataset%20and%20Chain-of-Thought%20Prompting%20to%20Improve%20Grading%20Accuracy&body=Title%3A%20Learning%20to%20Love%20Edge%20Cases%20in%20Formative%20Math%20Assessment%3A%20Using%20the%0A%20%20AMMORE%20Dataset%20and%20Chain-of-Thought%20Prompting%20to%20Improve%20Grading%20Accuracy%0AAuthor%3A%20Owen%20Henkel%20and%20Hannah%20Horne-Robinson%20and%20Maria%20Dyshel%20and%20Nabil%20Ch%20and%20Baptiste%20Moreau-Pernet%20and%20Ralph%20Abood%0AAbstract%3A%20%20%20This%20paper%20introduces%20AMMORE%2C%20a%20new%20dataset%20of%2053%2C000%20math%20open-response%0Aquestion-answer%20pairs%20from%20Rori%2C%20a%20learning%20platform%20used%20by%20students%20in%0Aseveral%20African%20countries%20and%20conducts%20two%20experiments%20to%20evaluate%20the%20use%20of%0Alarge%20language%20models%20%28LLM%29%20for%20grading%20particularly%20challenging%20student%0Aanswers.%20The%20AMMORE%20dataset%20enables%20various%20potential%20analyses%20and%20provides%20an%0Aimportant%20resource%20for%20researching%20student%20math%20acquisition%20in%20understudied%2C%0Areal-world%2C%20educational%20contexts.%20In%20experiment%201%20we%20use%20a%20variety%20of%0ALLM-driven%20approaches%2C%20including%20zero-shot%2C%20few-shot%2C%20and%20chain-of-thought%0Aprompting%2C%20to%20grade%20the%201%25%20of%20student%20answers%20that%20a%20rule-based%20classifier%0Afails%20to%20grade%20accurately.%20We%20find%20that%20the%20best-performing%20approach%20--%0Achain-of-thought%20prompting%20--%20accurately%20scored%2092%25%20of%20these%20edge%20cases%2C%0Aeffectively%20boosting%20the%20overall%20accuracy%20of%20the%20grading%20from%2098.7%25%20to%2099.9%25.%0AIn%20experiment%202%2C%20we%20aim%20to%20better%20understand%20the%20consequential%20validity%20of%20the%0Aimproved%20grading%20accuracy%2C%20by%20passing%20grades%20generated%20by%20the%20best-performing%0ALLM-based%20approach%20to%20a%20Bayesian%20Knowledge%20Tracing%20%28BKT%29%20model%2C%20which%20estimated%0Astudent%20mastery%20of%20specific%20lessons.%20We%20find%20that%20relatively%20modest%0Aimprovements%20in%20model%20accuracy%20at%20the%20individual%20question%20level%20can%20lead%20to%0Asignificant%20changes%20in%20the%20estimation%20of%20student%20mastery.%20Where%20the%20rules-based%0Aclassifier%20currently%20used%20to%20grade%20student%2C%20answers%20misclassified%20the%20mastery%0Astatus%20of%206.9%25%20of%20students%20across%20their%20completed%20lessons%2C%20using%20the%20LLM%0Achain-of-thought%20approach%20this%20misclassification%20rate%20was%20reduced%20to%202.6%25%20of%0Astudents.%20Taken%20together%2C%20these%20findings%20suggest%20that%20LLMs%20could%20be%20a%20valuable%0Atool%20for%20grading%20open-response%20questions%20in%20K-12%20mathematics%20education%2C%0Apotentially%20enabling%20encouraging%20wider%20adoption%20of%20open-ended%20questions%20in%0Aformative%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Love%2520Edge%2520Cases%2520in%2520Formative%2520Math%2520Assessment%253A%2520Using%2520the%250A%2520%2520AMMORE%2520Dataset%2520and%2520Chain-of-Thought%2520Prompting%2520to%2520Improve%2520Grading%2520Accuracy%26entry.906535625%3DOwen%2520Henkel%2520and%2520Hannah%2520Horne-Robinson%2520and%2520Maria%2520Dyshel%2520and%2520Nabil%2520Ch%2520and%2520Baptiste%2520Moreau-Pernet%2520and%2520Ralph%2520Abood%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520AMMORE%252C%2520a%2520new%2520dataset%2520of%252053%252C000%2520math%2520open-response%250Aquestion-answer%2520pairs%2520from%2520Rori%252C%2520a%2520learning%2520platform%2520used%2520by%2520students%2520in%250Aseveral%2520African%2520countries%2520and%2520conducts%2520two%2520experiments%2520to%2520evaluate%2520the%2520use%2520of%250Alarge%2520language%2520models%2520%2528LLM%2529%2520for%2520grading%2520particularly%2520challenging%2520student%250Aanswers.%2520The%2520AMMORE%2520dataset%2520enables%2520various%2520potential%2520analyses%2520and%2520provides%2520an%250Aimportant%2520resource%2520for%2520researching%2520student%2520math%2520acquisition%2520in%2520understudied%252C%250Areal-world%252C%2520educational%2520contexts.%2520In%2520experiment%25201%2520we%2520use%2520a%2520variety%2520of%250ALLM-driven%2520approaches%252C%2520including%2520zero-shot%252C%2520few-shot%252C%2520and%2520chain-of-thought%250Aprompting%252C%2520to%2520grade%2520the%25201%2525%2520of%2520student%2520answers%2520that%2520a%2520rule-based%2520classifier%250Afails%2520to%2520grade%2520accurately.%2520We%2520find%2520that%2520the%2520best-performing%2520approach%2520--%250Achain-of-thought%2520prompting%2520--%2520accurately%2520scored%252092%2525%2520of%2520these%2520edge%2520cases%252C%250Aeffectively%2520boosting%2520the%2520overall%2520accuracy%2520of%2520the%2520grading%2520from%252098.7%2525%2520to%252099.9%2525.%250AIn%2520experiment%25202%252C%2520we%2520aim%2520to%2520better%2520understand%2520the%2520consequential%2520validity%2520of%2520the%250Aimproved%2520grading%2520accuracy%252C%2520by%2520passing%2520grades%2520generated%2520by%2520the%2520best-performing%250ALLM-based%2520approach%2520to%2520a%2520Bayesian%2520Knowledge%2520Tracing%2520%2528BKT%2529%2520model%252C%2520which%2520estimated%250Astudent%2520mastery%2520of%2520specific%2520lessons.%2520We%2520find%2520that%2520relatively%2520modest%250Aimprovements%2520in%2520model%2520accuracy%2520at%2520the%2520individual%2520question%2520level%2520can%2520lead%2520to%250Asignificant%2520changes%2520in%2520the%2520estimation%2520of%2520student%2520mastery.%2520Where%2520the%2520rules-based%250Aclassifier%2520currently%2520used%2520to%2520grade%2520student%252C%2520answers%2520misclassified%2520the%2520mastery%250Astatus%2520of%25206.9%2525%2520of%2520students%2520across%2520their%2520completed%2520lessons%252C%2520using%2520the%2520LLM%250Achain-of-thought%2520approach%2520this%2520misclassification%2520rate%2520was%2520reduced%2520to%25202.6%2525%2520of%250Astudents.%2520Taken%2520together%252C%2520these%2520findings%2520suggest%2520that%2520LLMs%2520could%2520be%2520a%2520valuable%250Atool%2520for%2520grading%2520open-response%2520questions%2520in%2520K-12%2520mathematics%2520education%252C%250Apotentially%2520enabling%2520encouraging%2520wider%2520adoption%2520of%2520open-ended%2520questions%2520in%250Aformative%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Love%20Edge%20Cases%20in%20Formative%20Math%20Assessment%3A%20Using%20the%0A%20%20AMMORE%20Dataset%20and%20Chain-of-Thought%20Prompting%20to%20Improve%20Grading%20Accuracy&entry.906535625=Owen%20Henkel%20and%20Hannah%20Horne-Robinson%20and%20Maria%20Dyshel%20and%20Nabil%20Ch%20and%20Baptiste%20Moreau-Pernet%20and%20Ralph%20Abood&entry.1292438233=%20%20This%20paper%20introduces%20AMMORE%2C%20a%20new%20dataset%20of%2053%2C000%20math%20open-response%0Aquestion-answer%20pairs%20from%20Rori%2C%20a%20learning%20platform%20used%20by%20students%20in%0Aseveral%20African%20countries%20and%20conducts%20two%20experiments%20to%20evaluate%20the%20use%20of%0Alarge%20language%20models%20%28LLM%29%20for%20grading%20particularly%20challenging%20student%0Aanswers.%20The%20AMMORE%20dataset%20enables%20various%20potential%20analyses%20and%20provides%20an%0Aimportant%20resource%20for%20researching%20student%20math%20acquisition%20in%20understudied%2C%0Areal-world%2C%20educational%20contexts.%20In%20experiment%201%20we%20use%20a%20variety%20of%0ALLM-driven%20approaches%2C%20including%20zero-shot%2C%20few-shot%2C%20and%20chain-of-thought%0Aprompting%2C%20to%20grade%20the%201%25%20of%20student%20answers%20that%20a%20rule-based%20classifier%0Afails%20to%20grade%20accurately.%20We%20find%20that%20the%20best-performing%20approach%20--%0Achain-of-thought%20prompting%20--%20accurately%20scored%2092%25%20of%20these%20edge%20cases%2C%0Aeffectively%20boosting%20the%20overall%20accuracy%20of%20the%20grading%20from%2098.7%25%20to%2099.9%25.%0AIn%20experiment%202%2C%20we%20aim%20to%20better%20understand%20the%20consequential%20validity%20of%20the%0Aimproved%20grading%20accuracy%2C%20by%20passing%20grades%20generated%20by%20the%20best-performing%0ALLM-based%20approach%20to%20a%20Bayesian%20Knowledge%20Tracing%20%28BKT%29%20model%2C%20which%20estimated%0Astudent%20mastery%20of%20specific%20lessons.%20We%20find%20that%20relatively%20modest%0Aimprovements%20in%20model%20accuracy%20at%20the%20individual%20question%20level%20can%20lead%20to%0Asignificant%20changes%20in%20the%20estimation%20of%20student%20mastery.%20Where%20the%20rules-based%0Aclassifier%20currently%20used%20to%20grade%20student%2C%20answers%20misclassified%20the%20mastery%0Astatus%20of%206.9%25%20of%20students%20across%20their%20completed%20lessons%2C%20using%20the%20LLM%0Achain-of-thought%20approach%20this%20misclassification%20rate%20was%20reduced%20to%202.6%25%20of%0Astudents.%20Taken%20together%2C%20these%20findings%20suggest%20that%20LLMs%20could%20be%20a%20valuable%0Atool%20for%20grading%20open-response%20questions%20in%20K-12%20mathematics%20education%2C%0Apotentially%20enabling%20encouraging%20wider%20adoption%20of%20open-ended%20questions%20in%0Aformative%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17904v1&entry.124074799=Read"},
{"title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language\n  Models for Robotic Garment Manipulation", "author": "Xin Li and Siyuan Huang and Qiaojun Yu and Zhengkai Jiang and Ce Hao and Yimeng Zhu and Hongsheng Li and Peng Gao and Cewu Lu", "abstract": "  Automating garment manipulation poses a significant challenge for assistive\nrobotics due to the diverse and deformable nature of garments. Traditional\napproaches typically require separate models for each garment type, which\nlimits scalability and adaptability. In contrast, this paper presents a unified\napproach using vision-language models (VLMs) to improve keypoint prediction\nacross various garment categories. By interpreting both visual and semantic\ninformation, our model enables robots to manage different garment states with a\nsingle model. We created a large-scale synthetic dataset using advanced\nsimulation techniques, allowing scalable training without extensive real-world\ndata. Experimental results indicate that the VLM-based method significantly\nenhances keypoint detection accuracy and task success rates, providing a more\nflexible and general solution for robotic garment manipulation. In addition,\nthis research also underscores the potential of VLMs to unify various garment\nmanipulation tasks within a single framework, paving the way for broader\napplications in home automation and assistive robotics for future.\n", "link": "http://arxiv.org/abs/2409.18082v1", "date": "2024-09-26", "relevancy": 1.7828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6225}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5909}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKT%3A%20Integrating%20State-Aware%20Keypoint%20Trajectories%20with%20Vision-Language%0A%20%20Models%20for%20Robotic%20Garment%20Manipulation&body=Title%3A%20SKT%3A%20Integrating%20State-Aware%20Keypoint%20Trajectories%20with%20Vision-Language%0A%20%20Models%20for%20Robotic%20Garment%20Manipulation%0AAuthor%3A%20Xin%20Li%20and%20Siyuan%20Huang%20and%20Qiaojun%20Yu%20and%20Zhengkai%20Jiang%20and%20Ce%20Hao%20and%20Yimeng%20Zhu%20and%20Hongsheng%20Li%20and%20Peng%20Gao%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Automating%20garment%20manipulation%20poses%20a%20significant%20challenge%20for%20assistive%0Arobotics%20due%20to%20the%20diverse%20and%20deformable%20nature%20of%20garments.%20Traditional%0Aapproaches%20typically%20require%20separate%20models%20for%20each%20garment%20type%2C%20which%0Alimits%20scalability%20and%20adaptability.%20In%20contrast%2C%20this%20paper%20presents%20a%20unified%0Aapproach%20using%20vision-language%20models%20%28VLMs%29%20to%20improve%20keypoint%20prediction%0Aacross%20various%20garment%20categories.%20By%20interpreting%20both%20visual%20and%20semantic%0Ainformation%2C%20our%20model%20enables%20robots%20to%20manage%20different%20garment%20states%20with%20a%0Asingle%20model.%20We%20created%20a%20large-scale%20synthetic%20dataset%20using%20advanced%0Asimulation%20techniques%2C%20allowing%20scalable%20training%20without%20extensive%20real-world%0Adata.%20Experimental%20results%20indicate%20that%20the%20VLM-based%20method%20significantly%0Aenhances%20keypoint%20detection%20accuracy%20and%20task%20success%20rates%2C%20providing%20a%20more%0Aflexible%20and%20general%20solution%20for%20robotic%20garment%20manipulation.%20In%20addition%2C%0Athis%20research%20also%20underscores%20the%20potential%20of%20VLMs%20to%20unify%20various%20garment%0Amanipulation%20tasks%20within%20a%20single%20framework%2C%20paving%20the%20way%20for%20broader%0Aapplications%20in%20home%20automation%20and%20assistive%20robotics%20for%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKT%253A%2520Integrating%2520State-Aware%2520Keypoint%2520Trajectories%2520with%2520Vision-Language%250A%2520%2520Models%2520for%2520Robotic%2520Garment%2520Manipulation%26entry.906535625%3DXin%2520Li%2520and%2520Siyuan%2520Huang%2520and%2520Qiaojun%2520Yu%2520and%2520Zhengkai%2520Jiang%2520and%2520Ce%2520Hao%2520and%2520Yimeng%2520Zhu%2520and%2520Hongsheng%2520Li%2520and%2520Peng%2520Gao%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Automating%2520garment%2520manipulation%2520poses%2520a%2520significant%2520challenge%2520for%2520assistive%250Arobotics%2520due%2520to%2520the%2520diverse%2520and%2520deformable%2520nature%2520of%2520garments.%2520Traditional%250Aapproaches%2520typically%2520require%2520separate%2520models%2520for%2520each%2520garment%2520type%252C%2520which%250Alimits%2520scalability%2520and%2520adaptability.%2520In%2520contrast%252C%2520this%2520paper%2520presents%2520a%2520unified%250Aapproach%2520using%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520improve%2520keypoint%2520prediction%250Aacross%2520various%2520garment%2520categories.%2520By%2520interpreting%2520both%2520visual%2520and%2520semantic%250Ainformation%252C%2520our%2520model%2520enables%2520robots%2520to%2520manage%2520different%2520garment%2520states%2520with%2520a%250Asingle%2520model.%2520We%2520created%2520a%2520large-scale%2520synthetic%2520dataset%2520using%2520advanced%250Asimulation%2520techniques%252C%2520allowing%2520scalable%2520training%2520without%2520extensive%2520real-world%250Adata.%2520Experimental%2520results%2520indicate%2520that%2520the%2520VLM-based%2520method%2520significantly%250Aenhances%2520keypoint%2520detection%2520accuracy%2520and%2520task%2520success%2520rates%252C%2520providing%2520a%2520more%250Aflexible%2520and%2520general%2520solution%2520for%2520robotic%2520garment%2520manipulation.%2520In%2520addition%252C%250Athis%2520research%2520also%2520underscores%2520the%2520potential%2520of%2520VLMs%2520to%2520unify%2520various%2520garment%250Amanipulation%2520tasks%2520within%2520a%2520single%2520framework%252C%2520paving%2520the%2520way%2520for%2520broader%250Aapplications%2520in%2520home%2520automation%2520and%2520assistive%2520robotics%2520for%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKT%3A%20Integrating%20State-Aware%20Keypoint%20Trajectories%20with%20Vision-Language%0A%20%20Models%20for%20Robotic%20Garment%20Manipulation&entry.906535625=Xin%20Li%20and%20Siyuan%20Huang%20and%20Qiaojun%20Yu%20and%20Zhengkai%20Jiang%20and%20Ce%20Hao%20and%20Yimeng%20Zhu%20and%20Hongsheng%20Li%20and%20Peng%20Gao%20and%20Cewu%20Lu&entry.1292438233=%20%20Automating%20garment%20manipulation%20poses%20a%20significant%20challenge%20for%20assistive%0Arobotics%20due%20to%20the%20diverse%20and%20deformable%20nature%20of%20garments.%20Traditional%0Aapproaches%20typically%20require%20separate%20models%20for%20each%20garment%20type%2C%20which%0Alimits%20scalability%20and%20adaptability.%20In%20contrast%2C%20this%20paper%20presents%20a%20unified%0Aapproach%20using%20vision-language%20models%20%28VLMs%29%20to%20improve%20keypoint%20prediction%0Aacross%20various%20garment%20categories.%20By%20interpreting%20both%20visual%20and%20semantic%0Ainformation%2C%20our%20model%20enables%20robots%20to%20manage%20different%20garment%20states%20with%20a%0Asingle%20model.%20We%20created%20a%20large-scale%20synthetic%20dataset%20using%20advanced%0Asimulation%20techniques%2C%20allowing%20scalable%20training%20without%20extensive%20real-world%0Adata.%20Experimental%20results%20indicate%20that%20the%20VLM-based%20method%20significantly%0Aenhances%20keypoint%20detection%20accuracy%20and%20task%20success%20rates%2C%20providing%20a%20more%0Aflexible%20and%20general%20solution%20for%20robotic%20garment%20manipulation.%20In%20addition%2C%0Athis%20research%20also%20underscores%20the%20potential%20of%20VLMs%20to%20unify%20various%20garment%0Amanipulation%20tasks%20within%20a%20single%20framework%2C%20paving%20the%20way%20for%20broader%0Aapplications%20in%20home%20automation%20and%20assistive%20robotics%20for%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18082v1&entry.124074799=Read"},
{"title": "Dimension-independent learning rates for high-dimensional classification\n  problems", "author": "Andres Felipe Lerma-Pineda and Philipp Petersen and Simon Frieder and Thomas Lukasiewicz", "abstract": "  We study the problem of approximating and estimating classification functions\nthat have their decision boundary in the $RBV^2$ space. Functions of $RBV^2$\ntype arise naturally as solutions of regularized neural network learning\nproblems and neural networks can approximate these functions without the curse\nof dimensionality. We modify existing results to show that every $RBV^2$\nfunction can be approximated by a neural network with bounded weights.\nThereafter, we prove the existence of a neural network with bounded weights\napproximating a classification function. And we leverage these bounds to\nquantify the estimation rates. Finally, we present a numerical study that\nanalyzes the effect of different regularity conditions on the decision\nboundaries.\n", "link": "http://arxiv.org/abs/2409.17991v1", "date": "2024-09-26", "relevancy": 1.8612, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.501}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4708}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimension-independent%20learning%20rates%20for%20high-dimensional%20classification%0A%20%20problems&body=Title%3A%20Dimension-independent%20learning%20rates%20for%20high-dimensional%20classification%0A%20%20problems%0AAuthor%3A%20Andres%20Felipe%20Lerma-Pineda%20and%20Philipp%20Petersen%20and%20Simon%20Frieder%20and%20Thomas%20Lukasiewicz%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20approximating%20and%20estimating%20classification%20functions%0Athat%20have%20their%20decision%20boundary%20in%20the%20%24RBV%5E2%24%20space.%20Functions%20of%20%24RBV%5E2%24%0Atype%20arise%20naturally%20as%20solutions%20of%20regularized%20neural%20network%20learning%0Aproblems%20and%20neural%20networks%20can%20approximate%20these%20functions%20without%20the%20curse%0Aof%20dimensionality.%20We%20modify%20existing%20results%20to%20show%20that%20every%20%24RBV%5E2%24%0Afunction%20can%20be%20approximated%20by%20a%20neural%20network%20with%20bounded%20weights.%0AThereafter%2C%20we%20prove%20the%20existence%20of%20a%20neural%20network%20with%20bounded%20weights%0Aapproximating%20a%20classification%20function.%20And%20we%20leverage%20these%20bounds%20to%0Aquantify%20the%20estimation%20rates.%20Finally%2C%20we%20present%20a%20numerical%20study%20that%0Aanalyzes%20the%20effect%20of%20different%20regularity%20conditions%20on%20the%20decision%0Aboundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimension-independent%2520learning%2520rates%2520for%2520high-dimensional%2520classification%250A%2520%2520problems%26entry.906535625%3DAndres%2520Felipe%2520Lerma-Pineda%2520and%2520Philipp%2520Petersen%2520and%2520Simon%2520Frieder%2520and%2520Thomas%2520Lukasiewicz%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520approximating%2520and%2520estimating%2520classification%2520functions%250Athat%2520have%2520their%2520decision%2520boundary%2520in%2520the%2520%2524RBV%255E2%2524%2520space.%2520Functions%2520of%2520%2524RBV%255E2%2524%250Atype%2520arise%2520naturally%2520as%2520solutions%2520of%2520regularized%2520neural%2520network%2520learning%250Aproblems%2520and%2520neural%2520networks%2520can%2520approximate%2520these%2520functions%2520without%2520the%2520curse%250Aof%2520dimensionality.%2520We%2520modify%2520existing%2520results%2520to%2520show%2520that%2520every%2520%2524RBV%255E2%2524%250Afunction%2520can%2520be%2520approximated%2520by%2520a%2520neural%2520network%2520with%2520bounded%2520weights.%250AThereafter%252C%2520we%2520prove%2520the%2520existence%2520of%2520a%2520neural%2520network%2520with%2520bounded%2520weights%250Aapproximating%2520a%2520classification%2520function.%2520And%2520we%2520leverage%2520these%2520bounds%2520to%250Aquantify%2520the%2520estimation%2520rates.%2520Finally%252C%2520we%2520present%2520a%2520numerical%2520study%2520that%250Aanalyzes%2520the%2520effect%2520of%2520different%2520regularity%2520conditions%2520on%2520the%2520decision%250Aboundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimension-independent%20learning%20rates%20for%20high-dimensional%20classification%0A%20%20problems&entry.906535625=Andres%20Felipe%20Lerma-Pineda%20and%20Philipp%20Petersen%20and%20Simon%20Frieder%20and%20Thomas%20Lukasiewicz&entry.1292438233=%20%20We%20study%20the%20problem%20of%20approximating%20and%20estimating%20classification%20functions%0Athat%20have%20their%20decision%20boundary%20in%20the%20%24RBV%5E2%24%20space.%20Functions%20of%20%24RBV%5E2%24%0Atype%20arise%20naturally%20as%20solutions%20of%20regularized%20neural%20network%20learning%0Aproblems%20and%20neural%20networks%20can%20approximate%20these%20functions%20without%20the%20curse%0Aof%20dimensionality.%20We%20modify%20existing%20results%20to%20show%20that%20every%20%24RBV%5E2%24%0Afunction%20can%20be%20approximated%20by%20a%20neural%20network%20with%20bounded%20weights.%0AThereafter%2C%20we%20prove%20the%20existence%20of%20a%20neural%20network%20with%20bounded%20weights%0Aapproximating%20a%20classification%20function.%20And%20we%20leverage%20these%20bounds%20to%0Aquantify%20the%20estimation%20rates.%20Finally%2C%20we%20present%20a%20numerical%20study%20that%0Aanalyzes%20the%20effect%20of%20different%20regularity%20conditions%20on%20the%20decision%0Aboundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17991v1&entry.124074799=Read"},
{"title": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "author": "Ziyuan Zhuang and Zhiyang Zhang and Sitao Cheng and Fangkai Yang and Jia Liu and Shujian Huang and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.\n", "link": "http://arxiv.org/abs/2408.04259v2", "date": "2024-09-26", "relevancy": 1.3755, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4711}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EfficientRAG%3A%20Efficient%20Retriever%20for%20Multi-Hop%20Question%20Answering&body=Title%3A%20EfficientRAG%3A%20Efficient%20Retriever%20for%20Multi-Hop%20Question%20Answering%0AAuthor%3A%20Ziyuan%20Zhuang%20and%20Zhiyang%20Zhang%20and%20Sitao%20Cheng%20and%20Fangkai%20Yang%20and%20Jia%20Liu%20and%20Shujian%20Huang%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20methods%20encounter%20difficulties%20when%0Aaddressing%20complex%20questions%20like%20multi-hop%20queries.%20While%20iterative%20retrieval%0Amethods%20improve%20performance%20by%20gathering%20additional%20information%2C%20current%0Aapproaches%20often%20rely%20on%20multiple%20calls%20of%20large%20language%20models%20%28LLMs%29.%20In%0Athis%20paper%2C%20we%20introduce%20EfficientRAG%2C%20an%20efficient%20retriever%20for%20multi-hop%0Aquestion%20answering.%20EfficientRAG%20iteratively%20generates%20new%20queries%20without%20the%0Aneed%20for%20LLM%20calls%20at%20each%20iteration%20and%20filters%20out%20irrelevant%20information.%0AExperimental%20results%20demonstrate%20that%20EfficientRAG%20surpasses%20existing%20RAG%0Amethods%20on%20three%20open-domain%20multi-hop%20question-answering%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficientRAG%253A%2520Efficient%2520Retriever%2520for%2520Multi-Hop%2520Question%2520Answering%26entry.906535625%3DZiyuan%2520Zhuang%2520and%2520Zhiyang%2520Zhang%2520and%2520Sitao%2520Cheng%2520and%2520Fangkai%2520Yang%2520and%2520Jia%2520Liu%2520and%2520Shujian%2520Huang%2520and%2520Qingwei%2520Lin%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520methods%2520encounter%2520difficulties%2520when%250Aaddressing%2520complex%2520questions%2520like%2520multi-hop%2520queries.%2520While%2520iterative%2520retrieval%250Amethods%2520improve%2520performance%2520by%2520gathering%2520additional%2520information%252C%2520current%250Aapproaches%2520often%2520rely%2520on%2520multiple%2520calls%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520EfficientRAG%252C%2520an%2520efficient%2520retriever%2520for%2520multi-hop%250Aquestion%2520answering.%2520EfficientRAG%2520iteratively%2520generates%2520new%2520queries%2520without%2520the%250Aneed%2520for%2520LLM%2520calls%2520at%2520each%2520iteration%2520and%2520filters%2520out%2520irrelevant%2520information.%250AExperimental%2520results%2520demonstrate%2520that%2520EfficientRAG%2520surpasses%2520existing%2520RAG%250Amethods%2520on%2520three%2520open-domain%2520multi-hop%2520question-answering%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EfficientRAG%3A%20Efficient%20Retriever%20for%20Multi-Hop%20Question%20Answering&entry.906535625=Ziyuan%20Zhuang%20and%20Zhiyang%20Zhang%20and%20Sitao%20Cheng%20and%20Fangkai%20Yang%20and%20Jia%20Liu%20and%20Shujian%20Huang%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20methods%20encounter%20difficulties%20when%0Aaddressing%20complex%20questions%20like%20multi-hop%20queries.%20While%20iterative%20retrieval%0Amethods%20improve%20performance%20by%20gathering%20additional%20information%2C%20current%0Aapproaches%20often%20rely%20on%20multiple%20calls%20of%20large%20language%20models%20%28LLMs%29.%20In%0Athis%20paper%2C%20we%20introduce%20EfficientRAG%2C%20an%20efficient%20retriever%20for%20multi-hop%0Aquestion%20answering.%20EfficientRAG%20iteratively%20generates%20new%20queries%20without%20the%0Aneed%20for%20LLM%20calls%20at%20each%20iteration%20and%20filters%20out%20irrelevant%20information.%0AExperimental%20results%20demonstrate%20that%20EfficientRAG%20surpasses%20existing%20RAG%0Amethods%20on%20three%20open-domain%20multi-hop%20question-answering%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04259v2&entry.124074799=Read"},
{"title": "Inverse Reinforcement Learning with Multiple Planning Horizons", "author": "Jiayu Yao and Weiwei Pan and Finale Doshi-Velez and Barbara E Engelhardt", "abstract": "  In this work, we study an inverse reinforcement learning (IRL) problem where\nthe experts are planning under a shared reward function but with different,\nunknown planning horizons. Without the knowledge of discount factors, the\nreward function has a larger feasible solution set, which makes it harder for\nexisting IRL approaches to identify a reward function. To overcome this\nchallenge, we develop algorithms that can learn a global multi-agent reward\nfunction with agent-specific discount factors that reconstruct the expert\npolicies. We characterize the feasible solution space of the reward function\nand discount factors for both algorithms and demonstrate the generalizability\nof the learned reward function across multiple domains.\n", "link": "http://arxiv.org/abs/2409.18051v1", "date": "2024-09-26", "relevancy": 1.3173, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4325}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Reinforcement%20Learning%20with%20Multiple%20Planning%20Horizons&body=Title%3A%20Inverse%20Reinforcement%20Learning%20with%20Multiple%20Planning%20Horizons%0AAuthor%3A%20Jiayu%20Yao%20and%20Weiwei%20Pan%20and%20Finale%20Doshi-Velez%20and%20Barbara%20E%20Engelhardt%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20an%20inverse%20reinforcement%20learning%20%28IRL%29%20problem%20where%0Athe%20experts%20are%20planning%20under%20a%20shared%20reward%20function%20but%20with%20different%2C%0Aunknown%20planning%20horizons.%20Without%20the%20knowledge%20of%20discount%20factors%2C%20the%0Areward%20function%20has%20a%20larger%20feasible%20solution%20set%2C%20which%20makes%20it%20harder%20for%0Aexisting%20IRL%20approaches%20to%20identify%20a%20reward%20function.%20To%20overcome%20this%0Achallenge%2C%20we%20develop%20algorithms%20that%20can%20learn%20a%20global%20multi-agent%20reward%0Afunction%20with%20agent-specific%20discount%20factors%20that%20reconstruct%20the%20expert%0Apolicies.%20We%20characterize%20the%20feasible%20solution%20space%20of%20the%20reward%20function%0Aand%20discount%20factors%20for%20both%20algorithms%20and%20demonstrate%20the%20generalizability%0Aof%20the%20learned%20reward%20function%20across%20multiple%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Reinforcement%2520Learning%2520with%2520Multiple%2520Planning%2520Horizons%26entry.906535625%3DJiayu%2520Yao%2520and%2520Weiwei%2520Pan%2520and%2520Finale%2520Doshi-Velez%2520and%2520Barbara%2520E%2520Engelhardt%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520an%2520inverse%2520reinforcement%2520learning%2520%2528IRL%2529%2520problem%2520where%250Athe%2520experts%2520are%2520planning%2520under%2520a%2520shared%2520reward%2520function%2520but%2520with%2520different%252C%250Aunknown%2520planning%2520horizons.%2520Without%2520the%2520knowledge%2520of%2520discount%2520factors%252C%2520the%250Areward%2520function%2520has%2520a%2520larger%2520feasible%2520solution%2520set%252C%2520which%2520makes%2520it%2520harder%2520for%250Aexisting%2520IRL%2520approaches%2520to%2520identify%2520a%2520reward%2520function.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520develop%2520algorithms%2520that%2520can%2520learn%2520a%2520global%2520multi-agent%2520reward%250Afunction%2520with%2520agent-specific%2520discount%2520factors%2520that%2520reconstruct%2520the%2520expert%250Apolicies.%2520We%2520characterize%2520the%2520feasible%2520solution%2520space%2520of%2520the%2520reward%2520function%250Aand%2520discount%2520factors%2520for%2520both%2520algorithms%2520and%2520demonstrate%2520the%2520generalizability%250Aof%2520the%2520learned%2520reward%2520function%2520across%2520multiple%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Reinforcement%20Learning%20with%20Multiple%20Planning%20Horizons&entry.906535625=Jiayu%20Yao%20and%20Weiwei%20Pan%20and%20Finale%20Doshi-Velez%20and%20Barbara%20E%20Engelhardt&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20an%20inverse%20reinforcement%20learning%20%28IRL%29%20problem%20where%0Athe%20experts%20are%20planning%20under%20a%20shared%20reward%20function%20but%20with%20different%2C%0Aunknown%20planning%20horizons.%20Without%20the%20knowledge%20of%20discount%20factors%2C%20the%0Areward%20function%20has%20a%20larger%20feasible%20solution%20set%2C%20which%20makes%20it%20harder%20for%0Aexisting%20IRL%20approaches%20to%20identify%20a%20reward%20function.%20To%20overcome%20this%0Achallenge%2C%20we%20develop%20algorithms%20that%20can%20learn%20a%20global%20multi-agent%20reward%0Afunction%20with%20agent-specific%20discount%20factors%20that%20reconstruct%20the%20expert%0Apolicies.%20We%20characterize%20the%20feasible%20solution%20space%20of%20the%20reward%20function%0Aand%20discount%20factors%20for%20both%20algorithms%20and%20demonstrate%20the%20generalizability%0Aof%20the%20learned%20reward%20function%20across%20multiple%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18051v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


