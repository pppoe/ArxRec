<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241212.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D\n  Reconstruction", "author": "Jiale Xu and Shenghua Gao and Ying Shan", "abstract": "  Existing sparse-view reconstruction models heavily rely on accurate known\ncamera poses. However, deriving camera extrinsics and intrinsics from\nsparse-view images presents significant challenges. In this work, we present\nFreeSplatter, a highly scalable, feed-forward reconstruction framework capable\nof generating high-quality 3D Gaussians from uncalibrated sparse-view images\nand recovering their camera parameters in mere seconds. FreeSplatter is built\nupon a streamlined transformer architecture, comprising sequential\nself-attention blocks that facilitate information exchange among multi-view\nimage tokens and decode them into pixel-wise 3D Gaussian primitives. The\npredicted Gaussian primitives are situated in a unified reference frame,\nallowing for high-fidelity 3D modeling and instant camera parameter estimation\nusing off-the-shelf solvers. To cater to both object-centric and scene-level\nreconstruction, we train two model variants of FreeSplatter on extensive\ndatasets. In both scenarios, FreeSplatter outperforms state-of-the-art\nbaselines in terms of reconstruction quality and pose estimation accuracy.\nFurthermore, we showcase FreeSplatter's potential in enhancing the productivity\nof downstream applications, such as text/image-to-3D content creation.\n", "link": "http://arxiv.org/abs/2412.09573v1", "date": "2024-12-12", "relevancy": 3.522, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7439}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7433}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeSplatter%3A%20Pose-free%20Gaussian%20Splatting%20for%20Sparse-view%203D%0A%20%20Reconstruction&body=Title%3A%20FreeSplatter%3A%20Pose-free%20Gaussian%20Splatting%20for%20Sparse-view%203D%0A%20%20Reconstruction%0AAuthor%3A%20Jiale%20Xu%20and%20Shenghua%20Gao%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Existing%20sparse-view%20reconstruction%20models%20heavily%20rely%20on%20accurate%20known%0Acamera%20poses.%20However%2C%20deriving%20camera%20extrinsics%20and%20intrinsics%20from%0Asparse-view%20images%20presents%20significant%20challenges.%20In%20this%20work%2C%20we%20present%0AFreeSplatter%2C%20a%20highly%20scalable%2C%20feed-forward%20reconstruction%20framework%20capable%0Aof%20generating%20high-quality%203D%20Gaussians%20from%20uncalibrated%20sparse-view%20images%0Aand%20recovering%20their%20camera%20parameters%20in%20mere%20seconds.%20FreeSplatter%20is%20built%0Aupon%20a%20streamlined%20transformer%20architecture%2C%20comprising%20sequential%0Aself-attention%20blocks%20that%20facilitate%20information%20exchange%20among%20multi-view%0Aimage%20tokens%20and%20decode%20them%20into%20pixel-wise%203D%20Gaussian%20primitives.%20The%0Apredicted%20Gaussian%20primitives%20are%20situated%20in%20a%20unified%20reference%20frame%2C%0Aallowing%20for%20high-fidelity%203D%20modeling%20and%20instant%20camera%20parameter%20estimation%0Ausing%20off-the-shelf%20solvers.%20To%20cater%20to%20both%20object-centric%20and%20scene-level%0Areconstruction%2C%20we%20train%20two%20model%20variants%20of%20FreeSplatter%20on%20extensive%0Adatasets.%20In%20both%20scenarios%2C%20FreeSplatter%20outperforms%20state-of-the-art%0Abaselines%20in%20terms%20of%20reconstruction%20quality%20and%20pose%20estimation%20accuracy.%0AFurthermore%2C%20we%20showcase%20FreeSplatter%27s%20potential%20in%20enhancing%20the%20productivity%0Aof%20downstream%20applications%2C%20such%20as%20text/image-to-3D%20content%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeSplatter%253A%2520Pose-free%2520Gaussian%2520Splatting%2520for%2520Sparse-view%25203D%250A%2520%2520Reconstruction%26entry.906535625%3DJiale%2520Xu%2520and%2520Shenghua%2520Gao%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Existing%2520sparse-view%2520reconstruction%2520models%2520heavily%2520rely%2520on%2520accurate%2520known%250Acamera%2520poses.%2520However%252C%2520deriving%2520camera%2520extrinsics%2520and%2520intrinsics%2520from%250Asparse-view%2520images%2520presents%2520significant%2520challenges.%2520In%2520this%2520work%252C%2520we%2520present%250AFreeSplatter%252C%2520a%2520highly%2520scalable%252C%2520feed-forward%2520reconstruction%2520framework%2520capable%250Aof%2520generating%2520high-quality%25203D%2520Gaussians%2520from%2520uncalibrated%2520sparse-view%2520images%250Aand%2520recovering%2520their%2520camera%2520parameters%2520in%2520mere%2520seconds.%2520FreeSplatter%2520is%2520built%250Aupon%2520a%2520streamlined%2520transformer%2520architecture%252C%2520comprising%2520sequential%250Aself-attention%2520blocks%2520that%2520facilitate%2520information%2520exchange%2520among%2520multi-view%250Aimage%2520tokens%2520and%2520decode%2520them%2520into%2520pixel-wise%25203D%2520Gaussian%2520primitives.%2520The%250Apredicted%2520Gaussian%2520primitives%2520are%2520situated%2520in%2520a%2520unified%2520reference%2520frame%252C%250Aallowing%2520for%2520high-fidelity%25203D%2520modeling%2520and%2520instant%2520camera%2520parameter%2520estimation%250Ausing%2520off-the-shelf%2520solvers.%2520To%2520cater%2520to%2520both%2520object-centric%2520and%2520scene-level%250Areconstruction%252C%2520we%2520train%2520two%2520model%2520variants%2520of%2520FreeSplatter%2520on%2520extensive%250Adatasets.%2520In%2520both%2520scenarios%252C%2520FreeSplatter%2520outperforms%2520state-of-the-art%250Abaselines%2520in%2520terms%2520of%2520reconstruction%2520quality%2520and%2520pose%2520estimation%2520accuracy.%250AFurthermore%252C%2520we%2520showcase%2520FreeSplatter%2527s%2520potential%2520in%2520enhancing%2520the%2520productivity%250Aof%2520downstream%2520applications%252C%2520such%2520as%2520text/image-to-3D%2520content%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeSplatter%3A%20Pose-free%20Gaussian%20Splatting%20for%20Sparse-view%203D%0A%20%20Reconstruction&entry.906535625=Jiale%20Xu%20and%20Shenghua%20Gao%20and%20Ying%20Shan&entry.1292438233=%20%20Existing%20sparse-view%20reconstruction%20models%20heavily%20rely%20on%20accurate%20known%0Acamera%20poses.%20However%2C%20deriving%20camera%20extrinsics%20and%20intrinsics%20from%0Asparse-view%20images%20presents%20significant%20challenges.%20In%20this%20work%2C%20we%20present%0AFreeSplatter%2C%20a%20highly%20scalable%2C%20feed-forward%20reconstruction%20framework%20capable%0Aof%20generating%20high-quality%203D%20Gaussians%20from%20uncalibrated%20sparse-view%20images%0Aand%20recovering%20their%20camera%20parameters%20in%20mere%20seconds.%20FreeSplatter%20is%20built%0Aupon%20a%20streamlined%20transformer%20architecture%2C%20comprising%20sequential%0Aself-attention%20blocks%20that%20facilitate%20information%20exchange%20among%20multi-view%0Aimage%20tokens%20and%20decode%20them%20into%20pixel-wise%203D%20Gaussian%20primitives.%20The%0Apredicted%20Gaussian%20primitives%20are%20situated%20in%20a%20unified%20reference%20frame%2C%0Aallowing%20for%20high-fidelity%203D%20modeling%20and%20instant%20camera%20parameter%20estimation%0Ausing%20off-the-shelf%20solvers.%20To%20cater%20to%20both%20object-centric%20and%20scene-level%0Areconstruction%2C%20we%20train%20two%20model%20variants%20of%20FreeSplatter%20on%20extensive%0Adatasets.%20In%20both%20scenarios%2C%20FreeSplatter%20outperforms%20state-of-the-art%0Abaselines%20in%20terms%20of%20reconstruction%20quality%20and%20pose%20estimation%20accuracy.%0AFurthermore%2C%20we%20showcase%20FreeSplatter%27s%20potential%20in%20enhancing%20the%20productivity%0Aof%20downstream%20applications%2C%20such%20as%20text/image-to-3D%20content%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09573v1&entry.124074799=Read"},
{"title": "GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation\n  with Gaussian Splatting", "author": "Wanshui Gan and Fang Liu and Hongbin Xu and Ningkai Mo and Naoto Yokoya", "abstract": "  We introduce GaussianOcc, a systematic method that investigates the two\nusages of Gaussian splatting for fully self-supervised and efficient 3D\noccupancy estimation in surround views. First, traditional methods for\nself-supervised 3D occupancy estimation still require ground truth 6D poses\nfrom sensors during training. To address this limitation, we propose Gaussian\nSplatting for Projection (GSP) module to provide accurate scale information for\nfully self-supervised training from adjacent view projection. Additionally,\nexisting methods rely on volume rendering for final 3D voxel representation\nlearning using 2D signals (depth maps, semantic maps), which is both\ntime-consuming and less effective. We propose Gaussian Splatting from Voxel\nspace (GSV) to leverage the fast rendering properties of Gaussian splatting. As\na result, the proposed GaussianOcc method enables fully self-supervised (no\nground truth pose) 3D occupancy estimation in competitive performance with low\ncomputational cost (2.7 times faster in training and 5 times faster in\nrendering). The relevant code is available in\nhttps://github.com/GANWANSHUI/GaussianOcc.git.\n", "link": "http://arxiv.org/abs/2408.11447v3", "date": "2024-12-12", "relevancy": 3.3942, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7048}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7041}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianOcc%3A%20Fully%20Self-supervised%20and%20Efficient%203D%20Occupancy%20Estimation%0A%20%20with%20Gaussian%20Splatting&body=Title%3A%20GaussianOcc%3A%20Fully%20Self-supervised%20and%20Efficient%203D%20Occupancy%20Estimation%0A%20%20with%20Gaussian%20Splatting%0AAuthor%3A%20Wanshui%20Gan%20and%20Fang%20Liu%20and%20Hongbin%20Xu%20and%20Ningkai%20Mo%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20We%20introduce%20GaussianOcc%2C%20a%20systematic%20method%20that%20investigates%20the%20two%0Ausages%20of%20Gaussian%20splatting%20for%20fully%20self-supervised%20and%20efficient%203D%0Aoccupancy%20estimation%20in%20surround%20views.%20First%2C%20traditional%20methods%20for%0Aself-supervised%203D%20occupancy%20estimation%20still%20require%20ground%20truth%206D%20poses%0Afrom%20sensors%20during%20training.%20To%20address%20this%20limitation%2C%20we%20propose%20Gaussian%0ASplatting%20for%20Projection%20%28GSP%29%20module%20to%20provide%20accurate%20scale%20information%20for%0Afully%20self-supervised%20training%20from%20adjacent%20view%20projection.%20Additionally%2C%0Aexisting%20methods%20rely%20on%20volume%20rendering%20for%20final%203D%20voxel%20representation%0Alearning%20using%202D%20signals%20%28depth%20maps%2C%20semantic%20maps%29%2C%20which%20is%20both%0Atime-consuming%20and%20less%20effective.%20We%20propose%20Gaussian%20Splatting%20from%20Voxel%0Aspace%20%28GSV%29%20to%20leverage%20the%20fast%20rendering%20properties%20of%20Gaussian%20splatting.%20As%0Aa%20result%2C%20the%20proposed%20GaussianOcc%20method%20enables%20fully%20self-supervised%20%28no%0Aground%20truth%20pose%29%203D%20occupancy%20estimation%20in%20competitive%20performance%20with%20low%0Acomputational%20cost%20%282.7%20times%20faster%20in%20training%20and%205%20times%20faster%20in%0Arendering%29.%20The%20relevant%20code%20is%20available%20in%0Ahttps%3A//github.com/GANWANSHUI/GaussianOcc.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11447v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianOcc%253A%2520Fully%2520Self-supervised%2520and%2520Efficient%25203D%2520Occupancy%2520Estimation%250A%2520%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DWanshui%2520Gan%2520and%2520Fang%2520Liu%2520and%2520Hongbin%2520Xu%2520and%2520Ningkai%2520Mo%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520We%2520introduce%2520GaussianOcc%252C%2520a%2520systematic%2520method%2520that%2520investigates%2520the%2520two%250Ausages%2520of%2520Gaussian%2520splatting%2520for%2520fully%2520self-supervised%2520and%2520efficient%25203D%250Aoccupancy%2520estimation%2520in%2520surround%2520views.%2520First%252C%2520traditional%2520methods%2520for%250Aself-supervised%25203D%2520occupancy%2520estimation%2520still%2520require%2520ground%2520truth%25206D%2520poses%250Afrom%2520sensors%2520during%2520training.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Gaussian%250ASplatting%2520for%2520Projection%2520%2528GSP%2529%2520module%2520to%2520provide%2520accurate%2520scale%2520information%2520for%250Afully%2520self-supervised%2520training%2520from%2520adjacent%2520view%2520projection.%2520Additionally%252C%250Aexisting%2520methods%2520rely%2520on%2520volume%2520rendering%2520for%2520final%25203D%2520voxel%2520representation%250Alearning%2520using%25202D%2520signals%2520%2528depth%2520maps%252C%2520semantic%2520maps%2529%252C%2520which%2520is%2520both%250Atime-consuming%2520and%2520less%2520effective.%2520We%2520propose%2520Gaussian%2520Splatting%2520from%2520Voxel%250Aspace%2520%2528GSV%2529%2520to%2520leverage%2520the%2520fast%2520rendering%2520properties%2520of%2520Gaussian%2520splatting.%2520As%250Aa%2520result%252C%2520the%2520proposed%2520GaussianOcc%2520method%2520enables%2520fully%2520self-supervised%2520%2528no%250Aground%2520truth%2520pose%2529%25203D%2520occupancy%2520estimation%2520in%2520competitive%2520performance%2520with%2520low%250Acomputational%2520cost%2520%25282.7%2520times%2520faster%2520in%2520training%2520and%25205%2520times%2520faster%2520in%250Arendering%2529.%2520The%2520relevant%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/GANWANSHUI/GaussianOcc.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11447v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianOcc%3A%20Fully%20Self-supervised%20and%20Efficient%203D%20Occupancy%20Estimation%0A%20%20with%20Gaussian%20Splatting&entry.906535625=Wanshui%20Gan%20and%20Fang%20Liu%20and%20Hongbin%20Xu%20and%20Ningkai%20Mo%20and%20Naoto%20Yokoya&entry.1292438233=%20%20We%20introduce%20GaussianOcc%2C%20a%20systematic%20method%20that%20investigates%20the%20two%0Ausages%20of%20Gaussian%20splatting%20for%20fully%20self-supervised%20and%20efficient%203D%0Aoccupancy%20estimation%20in%20surround%20views.%20First%2C%20traditional%20methods%20for%0Aself-supervised%203D%20occupancy%20estimation%20still%20require%20ground%20truth%206D%20poses%0Afrom%20sensors%20during%20training.%20To%20address%20this%20limitation%2C%20we%20propose%20Gaussian%0ASplatting%20for%20Projection%20%28GSP%29%20module%20to%20provide%20accurate%20scale%20information%20for%0Afully%20self-supervised%20training%20from%20adjacent%20view%20projection.%20Additionally%2C%0Aexisting%20methods%20rely%20on%20volume%20rendering%20for%20final%203D%20voxel%20representation%0Alearning%20using%202D%20signals%20%28depth%20maps%2C%20semantic%20maps%29%2C%20which%20is%20both%0Atime-consuming%20and%20less%20effective.%20We%20propose%20Gaussian%20Splatting%20from%20Voxel%0Aspace%20%28GSV%29%20to%20leverage%20the%20fast%20rendering%20properties%20of%20Gaussian%20splatting.%20As%0Aa%20result%2C%20the%20proposed%20GaussianOcc%20method%20enables%20fully%20self-supervised%20%28no%0Aground%20truth%20pose%29%203D%20occupancy%20estimation%20in%20competitive%20performance%20with%20low%0Acomputational%20cost%20%282.7%20times%20faster%20in%20training%20and%205%20times%20faster%20in%0Arendering%29.%20The%20relevant%20code%20is%20available%20in%0Ahttps%3A//github.com/GANWANSHUI/GaussianOcc.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11447v3&entry.124074799=Read"},
{"title": "Illusion3D: 3D Multiview Illusion with 2D Diffusion Priors", "author": "Yue Feng and Vaibhav Sanjay and Spencer Lutz and Badour AlBahar and Songwei Ge and Jia-Bin Huang", "abstract": "  Automatically generating multiview illusions is a compelling challenge, where\na single piece of visual content offers distinct interpretations from different\nviewing perspectives. Traditional methods, such as shadow art and wire art,\ncreate interesting 3D illusions but are limited to simple visual outputs (i.e.,\nfigure-ground or line drawing), restricting their artistic expressiveness and\npractical versatility. Recent diffusion-based illusion generation methods can\ngenerate more intricate designs but are confined to 2D images. In this work, we\npresent a simple yet effective approach for creating 3D multiview illusions\nbased on user-provided text prompts or images. Our method leverages a\npre-trained text-to-image diffusion model to optimize the textures and geometry\nof neural 3D representations through differentiable rendering. When viewed from\nmultiple angles, this produces different interpretations. We develop several\ntechniques to improve the quality of the generated 3D multiview illusions. We\ndemonstrate the effectiveness of our approach through extensive experiments and\nshowcase illusion generation with diverse 3D forms.\n", "link": "http://arxiv.org/abs/2412.09625v1", "date": "2024-12-12", "relevancy": 3.3795, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7051}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7051}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Illusion3D%3A%203D%20Multiview%20Illusion%20with%202D%20Diffusion%20Priors&body=Title%3A%20Illusion3D%3A%203D%20Multiview%20Illusion%20with%202D%20Diffusion%20Priors%0AAuthor%3A%20Yue%20Feng%20and%20Vaibhav%20Sanjay%20and%20Spencer%20Lutz%20and%20Badour%20AlBahar%20and%20Songwei%20Ge%20and%20Jia-Bin%20Huang%0AAbstract%3A%20%20%20Automatically%20generating%20multiview%20illusions%20is%20a%20compelling%20challenge%2C%20where%0Aa%20single%20piece%20of%20visual%20content%20offers%20distinct%20interpretations%20from%20different%0Aviewing%20perspectives.%20Traditional%20methods%2C%20such%20as%20shadow%20art%20and%20wire%20art%2C%0Acreate%20interesting%203D%20illusions%20but%20are%20limited%20to%20simple%20visual%20outputs%20%28i.e.%2C%0Afigure-ground%20or%20line%20drawing%29%2C%20restricting%20their%20artistic%20expressiveness%20and%0Apractical%20versatility.%20Recent%20diffusion-based%20illusion%20generation%20methods%20can%0Agenerate%20more%20intricate%20designs%20but%20are%20confined%20to%202D%20images.%20In%20this%20work%2C%20we%0Apresent%20a%20simple%20yet%20effective%20approach%20for%20creating%203D%20multiview%20illusions%0Abased%20on%20user-provided%20text%20prompts%20or%20images.%20Our%20method%20leverages%20a%0Apre-trained%20text-to-image%20diffusion%20model%20to%20optimize%20the%20textures%20and%20geometry%0Aof%20neural%203D%20representations%20through%20differentiable%20rendering.%20When%20viewed%20from%0Amultiple%20angles%2C%20this%20produces%20different%20interpretations.%20We%20develop%20several%0Atechniques%20to%20improve%20the%20quality%20of%20the%20generated%203D%20multiview%20illusions.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20through%20extensive%20experiments%20and%0Ashowcase%20illusion%20generation%20with%20diverse%203D%20forms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIllusion3D%253A%25203D%2520Multiview%2520Illusion%2520with%25202D%2520Diffusion%2520Priors%26entry.906535625%3DYue%2520Feng%2520and%2520Vaibhav%2520Sanjay%2520and%2520Spencer%2520Lutz%2520and%2520Badour%2520AlBahar%2520and%2520Songwei%2520Ge%2520and%2520Jia-Bin%2520Huang%26entry.1292438233%3D%2520%2520Automatically%2520generating%2520multiview%2520illusions%2520is%2520a%2520compelling%2520challenge%252C%2520where%250Aa%2520single%2520piece%2520of%2520visual%2520content%2520offers%2520distinct%2520interpretations%2520from%2520different%250Aviewing%2520perspectives.%2520Traditional%2520methods%252C%2520such%2520as%2520shadow%2520art%2520and%2520wire%2520art%252C%250Acreate%2520interesting%25203D%2520illusions%2520but%2520are%2520limited%2520to%2520simple%2520visual%2520outputs%2520%2528i.e.%252C%250Afigure-ground%2520or%2520line%2520drawing%2529%252C%2520restricting%2520their%2520artistic%2520expressiveness%2520and%250Apractical%2520versatility.%2520Recent%2520diffusion-based%2520illusion%2520generation%2520methods%2520can%250Agenerate%2520more%2520intricate%2520designs%2520but%2520are%2520confined%2520to%25202D%2520images.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520simple%2520yet%2520effective%2520approach%2520for%2520creating%25203D%2520multiview%2520illusions%250Abased%2520on%2520user-provided%2520text%2520prompts%2520or%2520images.%2520Our%2520method%2520leverages%2520a%250Apre-trained%2520text-to-image%2520diffusion%2520model%2520to%2520optimize%2520the%2520textures%2520and%2520geometry%250Aof%2520neural%25203D%2520representations%2520through%2520differentiable%2520rendering.%2520When%2520viewed%2520from%250Amultiple%2520angles%252C%2520this%2520produces%2520different%2520interpretations.%2520We%2520develop%2520several%250Atechniques%2520to%2520improve%2520the%2520quality%2520of%2520the%2520generated%25203D%2520multiview%2520illusions.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%2520extensive%2520experiments%2520and%250Ashowcase%2520illusion%2520generation%2520with%2520diverse%25203D%2520forms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Illusion3D%3A%203D%20Multiview%20Illusion%20with%202D%20Diffusion%20Priors&entry.906535625=Yue%20Feng%20and%20Vaibhav%20Sanjay%20and%20Spencer%20Lutz%20and%20Badour%20AlBahar%20and%20Songwei%20Ge%20and%20Jia-Bin%20Huang&entry.1292438233=%20%20Automatically%20generating%20multiview%20illusions%20is%20a%20compelling%20challenge%2C%20where%0Aa%20single%20piece%20of%20visual%20content%20offers%20distinct%20interpretations%20from%20different%0Aviewing%20perspectives.%20Traditional%20methods%2C%20such%20as%20shadow%20art%20and%20wire%20art%2C%0Acreate%20interesting%203D%20illusions%20but%20are%20limited%20to%20simple%20visual%20outputs%20%28i.e.%2C%0Afigure-ground%20or%20line%20drawing%29%2C%20restricting%20their%20artistic%20expressiveness%20and%0Apractical%20versatility.%20Recent%20diffusion-based%20illusion%20generation%20methods%20can%0Agenerate%20more%20intricate%20designs%20but%20are%20confined%20to%202D%20images.%20In%20this%20work%2C%20we%0Apresent%20a%20simple%20yet%20effective%20approach%20for%20creating%203D%20multiview%20illusions%0Abased%20on%20user-provided%20text%20prompts%20or%20images.%20Our%20method%20leverages%20a%0Apre-trained%20text-to-image%20diffusion%20model%20to%20optimize%20the%20textures%20and%20geometry%0Aof%20neural%203D%20representations%20through%20differentiable%20rendering.%20When%20viewed%20from%0Amultiple%20angles%2C%20this%20produces%20different%20interpretations.%20We%20develop%20several%0Atechniques%20to%20improve%20the%20quality%20of%20the%20generated%203D%20multiview%20illusions.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20through%20extensive%20experiments%20and%0Ashowcase%20illusion%20generation%20with%20diverse%203D%20forms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09625v1&entry.124074799=Read"},
{"title": "SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos", "author": "Yuzheng Liu and Siyan Dong and Shuzhe Wang and Yingda Yin and Yanchao Yang and Qingnan Fan and Baoquan Chen", "abstract": "  In this paper, we introduce \\textbf{SLAM3R}, a novel and effective monocular\nRGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R\nprovides an end-to-end solution by seamlessly integrating local 3D\nreconstruction and global coordinate registration through feed-forward neural\nnetworks. Given an input video, the system first converts it into overlapping\nclips using a sliding window mechanism. Unlike traditional pose\noptimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB\nimages in each window and progressively aligns and deforms these local\npointmaps to create a globally consistent scene reconstruction - all without\nexplicitly solving any camera parameters. Experiments across datasets\nconsistently show that SLAM3R achieves state-of-the-art reconstruction accuracy\nand completeness while maintaining real-time performance at 20+ FPS. Code and\nweights at: \\url{https://github.com/PKU-VCL-3DV/SLAM3R}.\n", "link": "http://arxiv.org/abs/2412.09401v1", "date": "2024-12-12", "relevancy": 3.3173, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7625}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6219}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM3R%3A%20Real-Time%20Dense%20Scene%20Reconstruction%20from%20Monocular%20RGB%20Videos&body=Title%3A%20SLAM3R%3A%20Real-Time%20Dense%20Scene%20Reconstruction%20from%20Monocular%20RGB%20Videos%0AAuthor%3A%20Yuzheng%20Liu%20and%20Siyan%20Dong%20and%20Shuzhe%20Wang%20and%20Yingda%20Yin%20and%20Yanchao%20Yang%20and%20Qingnan%20Fan%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BSLAM3R%7D%2C%20a%20novel%20and%20effective%20monocular%0ARGB%20SLAM%20system%20for%20real-time%20and%20high-quality%20dense%203D%20reconstruction.%20SLAM3R%0Aprovides%20an%20end-to-end%20solution%20by%20seamlessly%20integrating%20local%203D%0Areconstruction%20and%20global%20coordinate%20registration%20through%20feed-forward%20neural%0Anetworks.%20Given%20an%20input%20video%2C%20the%20system%20first%20converts%20it%20into%20overlapping%0Aclips%20using%20a%20sliding%20window%20mechanism.%20Unlike%20traditional%20pose%0Aoptimization-based%20methods%2C%20SLAM3R%20directly%20regresses%203D%20pointmaps%20from%20RGB%0Aimages%20in%20each%20window%20and%20progressively%20aligns%20and%20deforms%20these%20local%0Apointmaps%20to%20create%20a%20globally%20consistent%20scene%20reconstruction%20-%20all%20without%0Aexplicitly%20solving%20any%20camera%20parameters.%20Experiments%20across%20datasets%0Aconsistently%20show%20that%20SLAM3R%20achieves%20state-of-the-art%20reconstruction%20accuracy%0Aand%20completeness%20while%20maintaining%20real-time%20performance%20at%2020%2B%20FPS.%20Code%20and%0Aweights%20at%3A%20%5Curl%7Bhttps%3A//github.com/PKU-VCL-3DV/SLAM3R%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM3R%253A%2520Real-Time%2520Dense%2520Scene%2520Reconstruction%2520from%2520Monocular%2520RGB%2520Videos%26entry.906535625%3DYuzheng%2520Liu%2520and%2520Siyan%2520Dong%2520and%2520Shuzhe%2520Wang%2520and%2520Yingda%2520Yin%2520and%2520Yanchao%2520Yang%2520and%2520Qingnan%2520Fan%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BSLAM3R%257D%252C%2520a%2520novel%2520and%2520effective%2520monocular%250ARGB%2520SLAM%2520system%2520for%2520real-time%2520and%2520high-quality%2520dense%25203D%2520reconstruction.%2520SLAM3R%250Aprovides%2520an%2520end-to-end%2520solution%2520by%2520seamlessly%2520integrating%2520local%25203D%250Areconstruction%2520and%2520global%2520coordinate%2520registration%2520through%2520feed-forward%2520neural%250Anetworks.%2520Given%2520an%2520input%2520video%252C%2520the%2520system%2520first%2520converts%2520it%2520into%2520overlapping%250Aclips%2520using%2520a%2520sliding%2520window%2520mechanism.%2520Unlike%2520traditional%2520pose%250Aoptimization-based%2520methods%252C%2520SLAM3R%2520directly%2520regresses%25203D%2520pointmaps%2520from%2520RGB%250Aimages%2520in%2520each%2520window%2520and%2520progressively%2520aligns%2520and%2520deforms%2520these%2520local%250Apointmaps%2520to%2520create%2520a%2520globally%2520consistent%2520scene%2520reconstruction%2520-%2520all%2520without%250Aexplicitly%2520solving%2520any%2520camera%2520parameters.%2520Experiments%2520across%2520datasets%250Aconsistently%2520show%2520that%2520SLAM3R%2520achieves%2520state-of-the-art%2520reconstruction%2520accuracy%250Aand%2520completeness%2520while%2520maintaining%2520real-time%2520performance%2520at%252020%252B%2520FPS.%2520Code%2520and%250Aweights%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/PKU-VCL-3DV/SLAM3R%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM3R%3A%20Real-Time%20Dense%20Scene%20Reconstruction%20from%20Monocular%20RGB%20Videos&entry.906535625=Yuzheng%20Liu%20and%20Siyan%20Dong%20and%20Shuzhe%20Wang%20and%20Yingda%20Yin%20and%20Yanchao%20Yang%20and%20Qingnan%20Fan%20and%20Baoquan%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BSLAM3R%7D%2C%20a%20novel%20and%20effective%20monocular%0ARGB%20SLAM%20system%20for%20real-time%20and%20high-quality%20dense%203D%20reconstruction.%20SLAM3R%0Aprovides%20an%20end-to-end%20solution%20by%20seamlessly%20integrating%20local%203D%0Areconstruction%20and%20global%20coordinate%20registration%20through%20feed-forward%20neural%0Anetworks.%20Given%20an%20input%20video%2C%20the%20system%20first%20converts%20it%20into%20overlapping%0Aclips%20using%20a%20sliding%20window%20mechanism.%20Unlike%20traditional%20pose%0Aoptimization-based%20methods%2C%20SLAM3R%20directly%20regresses%203D%20pointmaps%20from%20RGB%0Aimages%20in%20each%20window%20and%20progressively%20aligns%20and%20deforms%20these%20local%0Apointmaps%20to%20create%20a%20globally%20consistent%20scene%20reconstruction%20-%20all%20without%0Aexplicitly%20solving%20any%20camera%20parameters.%20Experiments%20across%20datasets%0Aconsistently%20show%20that%20SLAM3R%20achieves%20state-of-the-art%20reconstruction%20accuracy%0Aand%20completeness%20while%20maintaining%20real-time%20performance%20at%2020%2B%20FPS.%20Code%20and%0Aweights%20at%3A%20%5Curl%7Bhttps%3A//github.com/PKU-VCL-3DV/SLAM3R%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09401v1&entry.124074799=Read"},
{"title": "LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video\n  Generation Priors", "author": "Yabo Chen and Chen Yang and Jiemin Fang and Xiaopeng Zhang and Lingxi Xie and Wei Shen and Wenrui Dai and Hongkai Xiong and Qi Tian", "abstract": "  Single-image 3D reconstruction remains a fundamental challenge in computer\nvision due to inherent geometric ambiguities and limited viewpoint information.\nRecent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D\npriors learned from large-scale video data. However, leveraging these priors\neffectively faces three key challenges: (1) degradation in quality across large\ncamera motions, (2) difficulties in achieving precise camera control, and (3)\ngeometric distortions inherent to the diffusion process that damage 3D\nconsistency. We address these challenges by proposing LiftImage3D, a framework\nthat effectively releases LVDMs' generative priors while ensuring 3D\nconsistency. Specifically, we design an articulated trajectory strategy to\ngenerate video frames, which decomposes video sequences with large camera\nmotions into ones with controllable small motions. Then we use robust neural\nmatching models, i.e. MASt3R, to calibrate the camera poses of generated frames\nand produce corresponding point clouds. Finally, we propose a distortion-aware\n3D Gaussian splatting representation, which can learn independent distortions\nbetween frames and output undistorted canonical Gaussians. Extensive\nexperiments demonstrate that LiftImage3D achieves state-of-the-art performance\non two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and\ngeneralizes well to diverse in-the-wild images, from cartoon illustrations to\ncomplex real-world scenes.\n", "link": "http://arxiv.org/abs/2412.09597v1", "date": "2024-12-12", "relevancy": 3.3097, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6804}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6533}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiftImage3D%3A%20Lifting%20Any%20Single%20Image%20to%203D%20Gaussians%20with%20Video%0A%20%20Generation%20Priors&body=Title%3A%20LiftImage3D%3A%20Lifting%20Any%20Single%20Image%20to%203D%20Gaussians%20with%20Video%0A%20%20Generation%20Priors%0AAuthor%3A%20Yabo%20Chen%20and%20Chen%20Yang%20and%20Jiemin%20Fang%20and%20Xiaopeng%20Zhang%20and%20Lingxi%20Xie%20and%20Wei%20Shen%20and%20Wenrui%20Dai%20and%20Hongkai%20Xiong%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Single-image%203D%20reconstruction%20remains%20a%20fundamental%20challenge%20in%20computer%0Avision%20due%20to%20inherent%20geometric%20ambiguities%20and%20limited%20viewpoint%20information.%0ARecent%20advances%20in%20Latent%20Video%20Diffusion%20Models%20%28LVDMs%29%20offer%20promising%203D%0Apriors%20learned%20from%20large-scale%20video%20data.%20However%2C%20leveraging%20these%20priors%0Aeffectively%20faces%20three%20key%20challenges%3A%20%281%29%20degradation%20in%20quality%20across%20large%0Acamera%20motions%2C%20%282%29%20difficulties%20in%20achieving%20precise%20camera%20control%2C%20and%20%283%29%0Ageometric%20distortions%20inherent%20to%20the%20diffusion%20process%20that%20damage%203D%0Aconsistency.%20We%20address%20these%20challenges%20by%20proposing%20LiftImage3D%2C%20a%20framework%0Athat%20effectively%20releases%20LVDMs%27%20generative%20priors%20while%20ensuring%203D%0Aconsistency.%20Specifically%2C%20we%20design%20an%20articulated%20trajectory%20strategy%20to%0Agenerate%20video%20frames%2C%20which%20decomposes%20video%20sequences%20with%20large%20camera%0Amotions%20into%20ones%20with%20controllable%20small%20motions.%20Then%20we%20use%20robust%20neural%0Amatching%20models%2C%20i.e.%20MASt3R%2C%20to%20calibrate%20the%20camera%20poses%20of%20generated%20frames%0Aand%20produce%20corresponding%20point%20clouds.%20Finally%2C%20we%20propose%20a%20distortion-aware%0A3D%20Gaussian%20splatting%20representation%2C%20which%20can%20learn%20independent%20distortions%0Abetween%20frames%20and%20output%20undistorted%20canonical%20Gaussians.%20Extensive%0Aexperiments%20demonstrate%20that%20LiftImage3D%20achieves%20state-of-the-art%20performance%0Aon%20two%20challenging%20datasets%2C%20i.e.%20LLFF%2C%20DL3DV%2C%20and%20Tanks%20and%20Temples%2C%20and%0Ageneralizes%20well%20to%20diverse%20in-the-wild%20images%2C%20from%20cartoon%20illustrations%20to%0Acomplex%20real-world%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiftImage3D%253A%2520Lifting%2520Any%2520Single%2520Image%2520to%25203D%2520Gaussians%2520with%2520Video%250A%2520%2520Generation%2520Priors%26entry.906535625%3DYabo%2520Chen%2520and%2520Chen%2520Yang%2520and%2520Jiemin%2520Fang%2520and%2520Xiaopeng%2520Zhang%2520and%2520Lingxi%2520Xie%2520and%2520Wei%2520Shen%2520and%2520Wenrui%2520Dai%2520and%2520Hongkai%2520Xiong%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Single-image%25203D%2520reconstruction%2520remains%2520a%2520fundamental%2520challenge%2520in%2520computer%250Avision%2520due%2520to%2520inherent%2520geometric%2520ambiguities%2520and%2520limited%2520viewpoint%2520information.%250ARecent%2520advances%2520in%2520Latent%2520Video%2520Diffusion%2520Models%2520%2528LVDMs%2529%2520offer%2520promising%25203D%250Apriors%2520learned%2520from%2520large-scale%2520video%2520data.%2520However%252C%2520leveraging%2520these%2520priors%250Aeffectively%2520faces%2520three%2520key%2520challenges%253A%2520%25281%2529%2520degradation%2520in%2520quality%2520across%2520large%250Acamera%2520motions%252C%2520%25282%2529%2520difficulties%2520in%2520achieving%2520precise%2520camera%2520control%252C%2520and%2520%25283%2529%250Ageometric%2520distortions%2520inherent%2520to%2520the%2520diffusion%2520process%2520that%2520damage%25203D%250Aconsistency.%2520We%2520address%2520these%2520challenges%2520by%2520proposing%2520LiftImage3D%252C%2520a%2520framework%250Athat%2520effectively%2520releases%2520LVDMs%2527%2520generative%2520priors%2520while%2520ensuring%25203D%250Aconsistency.%2520Specifically%252C%2520we%2520design%2520an%2520articulated%2520trajectory%2520strategy%2520to%250Agenerate%2520video%2520frames%252C%2520which%2520decomposes%2520video%2520sequences%2520with%2520large%2520camera%250Amotions%2520into%2520ones%2520with%2520controllable%2520small%2520motions.%2520Then%2520we%2520use%2520robust%2520neural%250Amatching%2520models%252C%2520i.e.%2520MASt3R%252C%2520to%2520calibrate%2520the%2520camera%2520poses%2520of%2520generated%2520frames%250Aand%2520produce%2520corresponding%2520point%2520clouds.%2520Finally%252C%2520we%2520propose%2520a%2520distortion-aware%250A3D%2520Gaussian%2520splatting%2520representation%252C%2520which%2520can%2520learn%2520independent%2520distortions%250Abetween%2520frames%2520and%2520output%2520undistorted%2520canonical%2520Gaussians.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520LiftImage3D%2520achieves%2520state-of-the-art%2520performance%250Aon%2520two%2520challenging%2520datasets%252C%2520i.e.%2520LLFF%252C%2520DL3DV%252C%2520and%2520Tanks%2520and%2520Temples%252C%2520and%250Ageneralizes%2520well%2520to%2520diverse%2520in-the-wild%2520images%252C%2520from%2520cartoon%2520illustrations%2520to%250Acomplex%2520real-world%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiftImage3D%3A%20Lifting%20Any%20Single%20Image%20to%203D%20Gaussians%20with%20Video%0A%20%20Generation%20Priors&entry.906535625=Yabo%20Chen%20and%20Chen%20Yang%20and%20Jiemin%20Fang%20and%20Xiaopeng%20Zhang%20and%20Lingxi%20Xie%20and%20Wei%20Shen%20and%20Wenrui%20Dai%20and%20Hongkai%20Xiong%20and%20Qi%20Tian&entry.1292438233=%20%20Single-image%203D%20reconstruction%20remains%20a%20fundamental%20challenge%20in%20computer%0Avision%20due%20to%20inherent%20geometric%20ambiguities%20and%20limited%20viewpoint%20information.%0ARecent%20advances%20in%20Latent%20Video%20Diffusion%20Models%20%28LVDMs%29%20offer%20promising%203D%0Apriors%20learned%20from%20large-scale%20video%20data.%20However%2C%20leveraging%20these%20priors%0Aeffectively%20faces%20three%20key%20challenges%3A%20%281%29%20degradation%20in%20quality%20across%20large%0Acamera%20motions%2C%20%282%29%20difficulties%20in%20achieving%20precise%20camera%20control%2C%20and%20%283%29%0Ageometric%20distortions%20inherent%20to%20the%20diffusion%20process%20that%20damage%203D%0Aconsistency.%20We%20address%20these%20challenges%20by%20proposing%20LiftImage3D%2C%20a%20framework%0Athat%20effectively%20releases%20LVDMs%27%20generative%20priors%20while%20ensuring%203D%0Aconsistency.%20Specifically%2C%20we%20design%20an%20articulated%20trajectory%20strategy%20to%0Agenerate%20video%20frames%2C%20which%20decomposes%20video%20sequences%20with%20large%20camera%0Amotions%20into%20ones%20with%20controllable%20small%20motions.%20Then%20we%20use%20robust%20neural%0Amatching%20models%2C%20i.e.%20MASt3R%2C%20to%20calibrate%20the%20camera%20poses%20of%20generated%20frames%0Aand%20produce%20corresponding%20point%20clouds.%20Finally%2C%20we%20propose%20a%20distortion-aware%0A3D%20Gaussian%20splatting%20representation%2C%20which%20can%20learn%20independent%20distortions%0Abetween%20frames%20and%20output%20undistorted%20canonical%20Gaussians.%20Extensive%0Aexperiments%20demonstrate%20that%20LiftImage3D%20achieves%20state-of-the-art%20performance%0Aon%20two%20challenging%20datasets%2C%20i.e.%20LLFF%2C%20DL3DV%2C%20and%20Tanks%20and%20Temples%2C%20and%0Ageneralizes%20well%20to%20diverse%20in-the-wild%20images%2C%20from%20cartoon%20illustrations%20to%0Acomplex%20real-world%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09597v1&entry.124074799=Read"},
{"title": "Feat2GS: Probing Visual Foundation Models with Gaussian Splatting", "author": "Yue Chen and Xingyu Chen and Anpei Chen and Gerard Pons-Moll and Yuliang Xiu", "abstract": "  Given that visual foundation models (VFMs) are trained on extensive datasets\nbut often limited to 2D images, a natural question arises: how well do they\nunderstand the 3D world? With the differences in architecture and training\nprotocols (i.e., objectives, proxy tasks), a unified framework to fairly and\ncomprehensively probe their 3D awareness is urgently needed. Existing works on\n3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or\ntwo-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,\nthese tasks ignore texture awareness, and require 3D data as ground-truth,\nwhich limits the scale and diversity of their evaluation set. To address these\nissues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM\nfeatures extracted from unposed images. This allows us to probe 3D awareness\nfor geometry and texture via novel view synthesis, without requiring 3D data.\nAdditionally, the disentanglement of 3DGS parameters - geometry\n($\\boldsymbol{x}, \\alpha, \\Sigma$) and texture ($\\boldsymbol{c}$) - enables\nseparate analysis of texture and geometry awareness. Under Feat2GS, we conduct\nextensive experiments to probe the 3D awareness of several VFMs, and\ninvestigate the ingredients that lead to a 3D aware VFM. Building on these\nfindings, we develop several variants that achieve state-of-the-art across\ndiverse datasets. This makes Feat2GS useful for probing VFMs, and as a\nsimple-yet-effective baseline for novel-view synthesis. Code and data will be\nmade available at https://fanegg.github.io/Feat2GS/.\n", "link": "http://arxiv.org/abs/2412.09606v1", "date": "2024-12-12", "relevancy": 3.2645, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6799}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.642}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feat2GS%3A%20Probing%20Visual%20Foundation%20Models%20with%20Gaussian%20Splatting&body=Title%3A%20Feat2GS%3A%20Probing%20Visual%20Foundation%20Models%20with%20Gaussian%20Splatting%0AAuthor%3A%20Yue%20Chen%20and%20Xingyu%20Chen%20and%20Anpei%20Chen%20and%20Gerard%20Pons-Moll%20and%20Yuliang%20Xiu%0AAbstract%3A%20%20%20Given%20that%20visual%20foundation%20models%20%28VFMs%29%20are%20trained%20on%20extensive%20datasets%0Abut%20often%20limited%20to%202D%20images%2C%20a%20natural%20question%20arises%3A%20how%20well%20do%20they%0Aunderstand%20the%203D%20world%3F%20With%20the%20differences%20in%20architecture%20and%20training%0Aprotocols%20%28i.e.%2C%20objectives%2C%20proxy%20tasks%29%2C%20a%20unified%20framework%20to%20fairly%20and%0Acomprehensively%20probe%20their%203D%20awareness%20is%20urgently%20needed.%20Existing%20works%20on%0A3D%20probing%20suggest%20single-view%202.5D%20estimation%20%28e.g.%2C%20depth%20and%20normal%29%20or%0Atwo-view%20sparse%202D%20correspondence%20%28e.g.%2C%20matching%20and%20tracking%29.%20Unfortunately%2C%0Athese%20tasks%20ignore%20texture%20awareness%2C%20and%20require%203D%20data%20as%20ground-truth%2C%0Awhich%20limits%20the%20scale%20and%20diversity%20of%20their%20evaluation%20set.%20To%20address%20these%0Aissues%2C%20we%20introduce%20Feat2GS%2C%20which%20readout%203D%20Gaussians%20attributes%20from%20VFM%0Afeatures%20extracted%20from%20unposed%20images.%20This%20allows%20us%20to%20probe%203D%20awareness%0Afor%20geometry%20and%20texture%20via%20novel%20view%20synthesis%2C%20without%20requiring%203D%20data.%0AAdditionally%2C%20the%20disentanglement%20of%203DGS%20parameters%20-%20geometry%0A%28%24%5Cboldsymbol%7Bx%7D%2C%20%5Calpha%2C%20%5CSigma%24%29%20and%20texture%20%28%24%5Cboldsymbol%7Bc%7D%24%29%20-%20enables%0Aseparate%20analysis%20of%20texture%20and%20geometry%20awareness.%20Under%20Feat2GS%2C%20we%20conduct%0Aextensive%20experiments%20to%20probe%20the%203D%20awareness%20of%20several%20VFMs%2C%20and%0Ainvestigate%20the%20ingredients%20that%20lead%20to%20a%203D%20aware%20VFM.%20Building%20on%20these%0Afindings%2C%20we%20develop%20several%20variants%20that%20achieve%20state-of-the-art%20across%0Adiverse%20datasets.%20This%20makes%20Feat2GS%20useful%20for%20probing%20VFMs%2C%20and%20as%20a%0Asimple-yet-effective%20baseline%20for%20novel-view%20synthesis.%20Code%20and%20data%20will%20be%0Amade%20available%20at%20https%3A//fanegg.github.io/Feat2GS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeat2GS%253A%2520Probing%2520Visual%2520Foundation%2520Models%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DYue%2520Chen%2520and%2520Xingyu%2520Chen%2520and%2520Anpei%2520Chen%2520and%2520Gerard%2520Pons-Moll%2520and%2520Yuliang%2520Xiu%26entry.1292438233%3D%2520%2520Given%2520that%2520visual%2520foundation%2520models%2520%2528VFMs%2529%2520are%2520trained%2520on%2520extensive%2520datasets%250Abut%2520often%2520limited%2520to%25202D%2520images%252C%2520a%2520natural%2520question%2520arises%253A%2520how%2520well%2520do%2520they%250Aunderstand%2520the%25203D%2520world%253F%2520With%2520the%2520differences%2520in%2520architecture%2520and%2520training%250Aprotocols%2520%2528i.e.%252C%2520objectives%252C%2520proxy%2520tasks%2529%252C%2520a%2520unified%2520framework%2520to%2520fairly%2520and%250Acomprehensively%2520probe%2520their%25203D%2520awareness%2520is%2520urgently%2520needed.%2520Existing%2520works%2520on%250A3D%2520probing%2520suggest%2520single-view%25202.5D%2520estimation%2520%2528e.g.%252C%2520depth%2520and%2520normal%2529%2520or%250Atwo-view%2520sparse%25202D%2520correspondence%2520%2528e.g.%252C%2520matching%2520and%2520tracking%2529.%2520Unfortunately%252C%250Athese%2520tasks%2520ignore%2520texture%2520awareness%252C%2520and%2520require%25203D%2520data%2520as%2520ground-truth%252C%250Awhich%2520limits%2520the%2520scale%2520and%2520diversity%2520of%2520their%2520evaluation%2520set.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520Feat2GS%252C%2520which%2520readout%25203D%2520Gaussians%2520attributes%2520from%2520VFM%250Afeatures%2520extracted%2520from%2520unposed%2520images.%2520This%2520allows%2520us%2520to%2520probe%25203D%2520awareness%250Afor%2520geometry%2520and%2520texture%2520via%2520novel%2520view%2520synthesis%252C%2520without%2520requiring%25203D%2520data.%250AAdditionally%252C%2520the%2520disentanglement%2520of%25203DGS%2520parameters%2520-%2520geometry%250A%2528%2524%255Cboldsymbol%257Bx%257D%252C%2520%255Calpha%252C%2520%255CSigma%2524%2529%2520and%2520texture%2520%2528%2524%255Cboldsymbol%257Bc%257D%2524%2529%2520-%2520enables%250Aseparate%2520analysis%2520of%2520texture%2520and%2520geometry%2520awareness.%2520Under%2520Feat2GS%252C%2520we%2520conduct%250Aextensive%2520experiments%2520to%2520probe%2520the%25203D%2520awareness%2520of%2520several%2520VFMs%252C%2520and%250Ainvestigate%2520the%2520ingredients%2520that%2520lead%2520to%2520a%25203D%2520aware%2520VFM.%2520Building%2520on%2520these%250Afindings%252C%2520we%2520develop%2520several%2520variants%2520that%2520achieve%2520state-of-the-art%2520across%250Adiverse%2520datasets.%2520This%2520makes%2520Feat2GS%2520useful%2520for%2520probing%2520VFMs%252C%2520and%2520as%2520a%250Asimple-yet-effective%2520baseline%2520for%2520novel-view%2520synthesis.%2520Code%2520and%2520data%2520will%2520be%250Amade%2520available%2520at%2520https%253A//fanegg.github.io/Feat2GS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feat2GS%3A%20Probing%20Visual%20Foundation%20Models%20with%20Gaussian%20Splatting&entry.906535625=Yue%20Chen%20and%20Xingyu%20Chen%20and%20Anpei%20Chen%20and%20Gerard%20Pons-Moll%20and%20Yuliang%20Xiu&entry.1292438233=%20%20Given%20that%20visual%20foundation%20models%20%28VFMs%29%20are%20trained%20on%20extensive%20datasets%0Abut%20often%20limited%20to%202D%20images%2C%20a%20natural%20question%20arises%3A%20how%20well%20do%20they%0Aunderstand%20the%203D%20world%3F%20With%20the%20differences%20in%20architecture%20and%20training%0Aprotocols%20%28i.e.%2C%20objectives%2C%20proxy%20tasks%29%2C%20a%20unified%20framework%20to%20fairly%20and%0Acomprehensively%20probe%20their%203D%20awareness%20is%20urgently%20needed.%20Existing%20works%20on%0A3D%20probing%20suggest%20single-view%202.5D%20estimation%20%28e.g.%2C%20depth%20and%20normal%29%20or%0Atwo-view%20sparse%202D%20correspondence%20%28e.g.%2C%20matching%20and%20tracking%29.%20Unfortunately%2C%0Athese%20tasks%20ignore%20texture%20awareness%2C%20and%20require%203D%20data%20as%20ground-truth%2C%0Awhich%20limits%20the%20scale%20and%20diversity%20of%20their%20evaluation%20set.%20To%20address%20these%0Aissues%2C%20we%20introduce%20Feat2GS%2C%20which%20readout%203D%20Gaussians%20attributes%20from%20VFM%0Afeatures%20extracted%20from%20unposed%20images.%20This%20allows%20us%20to%20probe%203D%20awareness%0Afor%20geometry%20and%20texture%20via%20novel%20view%20synthesis%2C%20without%20requiring%203D%20data.%0AAdditionally%2C%20the%20disentanglement%20of%203DGS%20parameters%20-%20geometry%0A%28%24%5Cboldsymbol%7Bx%7D%2C%20%5Calpha%2C%20%5CSigma%24%29%20and%20texture%20%28%24%5Cboldsymbol%7Bc%7D%24%29%20-%20enables%0Aseparate%20analysis%20of%20texture%20and%20geometry%20awareness.%20Under%20Feat2GS%2C%20we%20conduct%0Aextensive%20experiments%20to%20probe%20the%203D%20awareness%20of%20several%20VFMs%2C%20and%0Ainvestigate%20the%20ingredients%20that%20lead%20to%20a%203D%20aware%20VFM.%20Building%20on%20these%0Afindings%2C%20we%20develop%20several%20variants%20that%20achieve%20state-of-the-art%20across%0Adiverse%20datasets.%20This%20makes%20Feat2GS%20useful%20for%20probing%20VFMs%2C%20and%20as%20a%0Asimple-yet-effective%20baseline%20for%20novel-view%20synthesis.%20Code%20and%20data%20will%20be%0Amade%20available%20at%20https%3A//fanegg.github.io/Feat2GS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09606v1&entry.124074799=Read"},
{"title": "SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing", "author": "Xueting Li and Ye Yuan and Shalini De Mello and Gilles Daviet and Jonathan Leaf and Miles Macklin and Jan Kautz and Umar Iqbal", "abstract": "  We introduce SimAvatar, a framework designed to generate simulation-ready\nclothed 3D human avatars from a text prompt. Current text-driven human avatar\ngeneration methods either model hair, clothing, and the human body using a\nunified geometry or produce hair and garments that are not easily adaptable for\nsimulation within existing simulation pipelines. The primary challenge lies in\nrepresenting the hair and garment geometry in a way that allows leveraging\nestablished prior knowledge from foundational image diffusion models (e.g.,\nStable Diffusion) while being simulation-ready using either physics or neural\nsimulators. To address this task, we propose a two-stage framework that\ncombines the flexibility of 3D Gaussians with simulation-ready hair strands and\ngarment meshes. Specifically, we first employ three text-conditioned 3D\ngenerative models to generate garment mesh, body shape and hair strands from\nthe given text prompt. To leverage prior knowledge from foundational diffusion\nmodels, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair\nstrands and learn the avatar appearance through optimization. To drive the\navatar given a pose sequence, we first apply physics simulators onto the\ngarment meshes and hair strands. We then transfer the motion onto 3D Gaussians\nthrough carefully designed mechanisms for each body part. As a result, our\nsynthesized avatars have vivid texture and realistic dynamic motion. To the\nbest of our knowledge, our method is the first to produce highly realistic,\nfully simulation-ready 3D avatars, surpassing the capabilities of current\napproaches.\n", "link": "http://arxiv.org/abs/2412.09545v1", "date": "2024-12-12", "relevancy": 3.2465, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.666}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6409}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimAvatar%3A%20Simulation-Ready%20Avatars%20with%20Layered%20Hair%20and%20Clothing&body=Title%3A%20SimAvatar%3A%20Simulation-Ready%20Avatars%20with%20Layered%20Hair%20and%20Clothing%0AAuthor%3A%20Xueting%20Li%20and%20Ye%20Yuan%20and%20Shalini%20De%20Mello%20and%20Gilles%20Daviet%20and%20Jonathan%20Leaf%20and%20Miles%20Macklin%20and%20Jan%20Kautz%20and%20Umar%20Iqbal%0AAbstract%3A%20%20%20We%20introduce%20SimAvatar%2C%20a%20framework%20designed%20to%20generate%20simulation-ready%0Aclothed%203D%20human%20avatars%20from%20a%20text%20prompt.%20Current%20text-driven%20human%20avatar%0Ageneration%20methods%20either%20model%20hair%2C%20clothing%2C%20and%20the%20human%20body%20using%20a%0Aunified%20geometry%20or%20produce%20hair%20and%20garments%20that%20are%20not%20easily%20adaptable%20for%0Asimulation%20within%20existing%20simulation%20pipelines.%20The%20primary%20challenge%20lies%20in%0Arepresenting%20the%20hair%20and%20garment%20geometry%20in%20a%20way%20that%20allows%20leveraging%0Aestablished%20prior%20knowledge%20from%20foundational%20image%20diffusion%20models%20%28e.g.%2C%0AStable%20Diffusion%29%20while%20being%20simulation-ready%20using%20either%20physics%20or%20neural%0Asimulators.%20To%20address%20this%20task%2C%20we%20propose%20a%20two-stage%20framework%20that%0Acombines%20the%20flexibility%20of%203D%20Gaussians%20with%20simulation-ready%20hair%20strands%20and%0Agarment%20meshes.%20Specifically%2C%20we%20first%20employ%20three%20text-conditioned%203D%0Agenerative%20models%20to%20generate%20garment%20mesh%2C%20body%20shape%20and%20hair%20strands%20from%0Athe%20given%20text%20prompt.%20To%20leverage%20prior%20knowledge%20from%20foundational%20diffusion%0Amodels%2C%20we%20attach%203D%20Gaussians%20to%20the%20body%20mesh%2C%20garment%20mesh%2C%20as%20well%20as%20hair%0Astrands%20and%20learn%20the%20avatar%20appearance%20through%20optimization.%20To%20drive%20the%0Aavatar%20given%20a%20pose%20sequence%2C%20we%20first%20apply%20physics%20simulators%20onto%20the%0Agarment%20meshes%20and%20hair%20strands.%20We%20then%20transfer%20the%20motion%20onto%203D%20Gaussians%0Athrough%20carefully%20designed%20mechanisms%20for%20each%20body%20part.%20As%20a%20result%2C%20our%0Asynthesized%20avatars%20have%20vivid%20texture%20and%20realistic%20dynamic%20motion.%20To%20the%0Abest%20of%20our%20knowledge%2C%20our%20method%20is%20the%20first%20to%20produce%20highly%20realistic%2C%0Afully%20simulation-ready%203D%20avatars%2C%20surpassing%20the%20capabilities%20of%20current%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimAvatar%253A%2520Simulation-Ready%2520Avatars%2520with%2520Layered%2520Hair%2520and%2520Clothing%26entry.906535625%3DXueting%2520Li%2520and%2520Ye%2520Yuan%2520and%2520Shalini%2520De%2520Mello%2520and%2520Gilles%2520Daviet%2520and%2520Jonathan%2520Leaf%2520and%2520Miles%2520Macklin%2520and%2520Jan%2520Kautz%2520and%2520Umar%2520Iqbal%26entry.1292438233%3D%2520%2520We%2520introduce%2520SimAvatar%252C%2520a%2520framework%2520designed%2520to%2520generate%2520simulation-ready%250Aclothed%25203D%2520human%2520avatars%2520from%2520a%2520text%2520prompt.%2520Current%2520text-driven%2520human%2520avatar%250Ageneration%2520methods%2520either%2520model%2520hair%252C%2520clothing%252C%2520and%2520the%2520human%2520body%2520using%2520a%250Aunified%2520geometry%2520or%2520produce%2520hair%2520and%2520garments%2520that%2520are%2520not%2520easily%2520adaptable%2520for%250Asimulation%2520within%2520existing%2520simulation%2520pipelines.%2520The%2520primary%2520challenge%2520lies%2520in%250Arepresenting%2520the%2520hair%2520and%2520garment%2520geometry%2520in%2520a%2520way%2520that%2520allows%2520leveraging%250Aestablished%2520prior%2520knowledge%2520from%2520foundational%2520image%2520diffusion%2520models%2520%2528e.g.%252C%250AStable%2520Diffusion%2529%2520while%2520being%2520simulation-ready%2520using%2520either%2520physics%2520or%2520neural%250Asimulators.%2520To%2520address%2520this%2520task%252C%2520we%2520propose%2520a%2520two-stage%2520framework%2520that%250Acombines%2520the%2520flexibility%2520of%25203D%2520Gaussians%2520with%2520simulation-ready%2520hair%2520strands%2520and%250Agarment%2520meshes.%2520Specifically%252C%2520we%2520first%2520employ%2520three%2520text-conditioned%25203D%250Agenerative%2520models%2520to%2520generate%2520garment%2520mesh%252C%2520body%2520shape%2520and%2520hair%2520strands%2520from%250Athe%2520given%2520text%2520prompt.%2520To%2520leverage%2520prior%2520knowledge%2520from%2520foundational%2520diffusion%250Amodels%252C%2520we%2520attach%25203D%2520Gaussians%2520to%2520the%2520body%2520mesh%252C%2520garment%2520mesh%252C%2520as%2520well%2520as%2520hair%250Astrands%2520and%2520learn%2520the%2520avatar%2520appearance%2520through%2520optimization.%2520To%2520drive%2520the%250Aavatar%2520given%2520a%2520pose%2520sequence%252C%2520we%2520first%2520apply%2520physics%2520simulators%2520onto%2520the%250Agarment%2520meshes%2520and%2520hair%2520strands.%2520We%2520then%2520transfer%2520the%2520motion%2520onto%25203D%2520Gaussians%250Athrough%2520carefully%2520designed%2520mechanisms%2520for%2520each%2520body%2520part.%2520As%2520a%2520result%252C%2520our%250Asynthesized%2520avatars%2520have%2520vivid%2520texture%2520and%2520realistic%2520dynamic%2520motion.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520our%2520method%2520is%2520the%2520first%2520to%2520produce%2520highly%2520realistic%252C%250Afully%2520simulation-ready%25203D%2520avatars%252C%2520surpassing%2520the%2520capabilities%2520of%2520current%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimAvatar%3A%20Simulation-Ready%20Avatars%20with%20Layered%20Hair%20and%20Clothing&entry.906535625=Xueting%20Li%20and%20Ye%20Yuan%20and%20Shalini%20De%20Mello%20and%20Gilles%20Daviet%20and%20Jonathan%20Leaf%20and%20Miles%20Macklin%20and%20Jan%20Kautz%20and%20Umar%20Iqbal&entry.1292438233=%20%20We%20introduce%20SimAvatar%2C%20a%20framework%20designed%20to%20generate%20simulation-ready%0Aclothed%203D%20human%20avatars%20from%20a%20text%20prompt.%20Current%20text-driven%20human%20avatar%0Ageneration%20methods%20either%20model%20hair%2C%20clothing%2C%20and%20the%20human%20body%20using%20a%0Aunified%20geometry%20or%20produce%20hair%20and%20garments%20that%20are%20not%20easily%20adaptable%20for%0Asimulation%20within%20existing%20simulation%20pipelines.%20The%20primary%20challenge%20lies%20in%0Arepresenting%20the%20hair%20and%20garment%20geometry%20in%20a%20way%20that%20allows%20leveraging%0Aestablished%20prior%20knowledge%20from%20foundational%20image%20diffusion%20models%20%28e.g.%2C%0AStable%20Diffusion%29%20while%20being%20simulation-ready%20using%20either%20physics%20or%20neural%0Asimulators.%20To%20address%20this%20task%2C%20we%20propose%20a%20two-stage%20framework%20that%0Acombines%20the%20flexibility%20of%203D%20Gaussians%20with%20simulation-ready%20hair%20strands%20and%0Agarment%20meshes.%20Specifically%2C%20we%20first%20employ%20three%20text-conditioned%203D%0Agenerative%20models%20to%20generate%20garment%20mesh%2C%20body%20shape%20and%20hair%20strands%20from%0Athe%20given%20text%20prompt.%20To%20leverage%20prior%20knowledge%20from%20foundational%20diffusion%0Amodels%2C%20we%20attach%203D%20Gaussians%20to%20the%20body%20mesh%2C%20garment%20mesh%2C%20as%20well%20as%20hair%0Astrands%20and%20learn%20the%20avatar%20appearance%20through%20optimization.%20To%20drive%20the%0Aavatar%20given%20a%20pose%20sequence%2C%20we%20first%20apply%20physics%20simulators%20onto%20the%0Agarment%20meshes%20and%20hair%20strands.%20We%20then%20transfer%20the%20motion%20onto%203D%20Gaussians%0Athrough%20carefully%20designed%20mechanisms%20for%20each%20body%20part.%20As%20a%20result%2C%20our%0Asynthesized%20avatars%20have%20vivid%20texture%20and%20realistic%20dynamic%20motion.%20To%20the%0Abest%20of%20our%20knowledge%2C%20our%20method%20is%20the%20first%20to%20produce%20highly%20realistic%2C%0Afully%20simulation-ready%203D%20avatars%2C%20surpassing%20the%20capabilities%20of%20current%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09545v1&entry.124074799=Read"},
{"title": "GoHD: Gaze-oriented and Highly Disentangled Portrait Animation with\n  Rhythmic Poses and Realistic Expression", "author": "Ziqi Zhou and Weize Quan and Hailin Shi and Wei Li and Lili Wang and Dong-ming Yan", "abstract": "  Audio-driven talking head generation necessitates seamless integration of\naudio and visual data amidst the challenges posed by diverse input portraits\nand intricate correlations between audio and facial motions. In response, we\npropose a robust framework GoHD designed to produce highly realistic,\nexpressive, and controllable portrait videos from any reference identity with\nany motion. GoHD innovates with three key modules: Firstly, an animation module\nutilizing latent navigation is introduced to improve the generalization ability\nacross unseen input styles. This module achieves high disentanglement of motion\nand identity, and it also incorporates gaze orientation to rectify unnatural\neye movements that were previously overlooked. Secondly, a conformer-structured\nconditional diffusion model is designed to guarantee head poses that are aware\nof prosody. Thirdly, to estimate lip-synchronized and realistic expressions\nfrom the input audio within limited training data, a two-stage training\nstrategy is devised to decouple frequent and frame-wise lip motion distillation\nfrom the generation of other more temporally dependent but less audio-related\nmotions, e.g., blinks and frowns. Extensive experiments validate GoHD's\nadvanced generalization capabilities, demonstrating its effectiveness in\ngenerating realistic talking face results on arbitrary subjects.\n", "link": "http://arxiv.org/abs/2412.09296v1", "date": "2024-12-12", "relevancy": 3.2001, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6467}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6467}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GoHD%3A%20Gaze-oriented%20and%20Highly%20Disentangled%20Portrait%20Animation%20with%0A%20%20Rhythmic%20Poses%20and%20Realistic%20Expression&body=Title%3A%20GoHD%3A%20Gaze-oriented%20and%20Highly%20Disentangled%20Portrait%20Animation%20with%0A%20%20Rhythmic%20Poses%20and%20Realistic%20Expression%0AAuthor%3A%20Ziqi%20Zhou%20and%20Weize%20Quan%20and%20Hailin%20Shi%20and%20Wei%20Li%20and%20Lili%20Wang%20and%20Dong-ming%20Yan%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20generation%20necessitates%20seamless%20integration%20of%0Aaudio%20and%20visual%20data%20amidst%20the%20challenges%20posed%20by%20diverse%20input%20portraits%0Aand%20intricate%20correlations%20between%20audio%20and%20facial%20motions.%20In%20response%2C%20we%0Apropose%20a%20robust%20framework%20GoHD%20designed%20to%20produce%20highly%20realistic%2C%0Aexpressive%2C%20and%20controllable%20portrait%20videos%20from%20any%20reference%20identity%20with%0Aany%20motion.%20GoHD%20innovates%20with%20three%20key%20modules%3A%20Firstly%2C%20an%20animation%20module%0Autilizing%20latent%20navigation%20is%20introduced%20to%20improve%20the%20generalization%20ability%0Aacross%20unseen%20input%20styles.%20This%20module%20achieves%20high%20disentanglement%20of%20motion%0Aand%20identity%2C%20and%20it%20also%20incorporates%20gaze%20orientation%20to%20rectify%20unnatural%0Aeye%20movements%20that%20were%20previously%20overlooked.%20Secondly%2C%20a%20conformer-structured%0Aconditional%20diffusion%20model%20is%20designed%20to%20guarantee%20head%20poses%20that%20are%20aware%0Aof%20prosody.%20Thirdly%2C%20to%20estimate%20lip-synchronized%20and%20realistic%20expressions%0Afrom%20the%20input%20audio%20within%20limited%20training%20data%2C%20a%20two-stage%20training%0Astrategy%20is%20devised%20to%20decouple%20frequent%20and%20frame-wise%20lip%20motion%20distillation%0Afrom%20the%20generation%20of%20other%20more%20temporally%20dependent%20but%20less%20audio-related%0Amotions%2C%20e.g.%2C%20blinks%20and%20frowns.%20Extensive%20experiments%20validate%20GoHD%27s%0Aadvanced%20generalization%20capabilities%2C%20demonstrating%20its%20effectiveness%20in%0Agenerating%20realistic%20talking%20face%20results%20on%20arbitrary%20subjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoHD%253A%2520Gaze-oriented%2520and%2520Highly%2520Disentangled%2520Portrait%2520Animation%2520with%250A%2520%2520Rhythmic%2520Poses%2520and%2520Realistic%2520Expression%26entry.906535625%3DZiqi%2520Zhou%2520and%2520Weize%2520Quan%2520and%2520Hailin%2520Shi%2520and%2520Wei%2520Li%2520and%2520Lili%2520Wang%2520and%2520Dong-ming%2520Yan%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520generation%2520necessitates%2520seamless%2520integration%2520of%250Aaudio%2520and%2520visual%2520data%2520amidst%2520the%2520challenges%2520posed%2520by%2520diverse%2520input%2520portraits%250Aand%2520intricate%2520correlations%2520between%2520audio%2520and%2520facial%2520motions.%2520In%2520response%252C%2520we%250Apropose%2520a%2520robust%2520framework%2520GoHD%2520designed%2520to%2520produce%2520highly%2520realistic%252C%250Aexpressive%252C%2520and%2520controllable%2520portrait%2520videos%2520from%2520any%2520reference%2520identity%2520with%250Aany%2520motion.%2520GoHD%2520innovates%2520with%2520three%2520key%2520modules%253A%2520Firstly%252C%2520an%2520animation%2520module%250Autilizing%2520latent%2520navigation%2520is%2520introduced%2520to%2520improve%2520the%2520generalization%2520ability%250Aacross%2520unseen%2520input%2520styles.%2520This%2520module%2520achieves%2520high%2520disentanglement%2520of%2520motion%250Aand%2520identity%252C%2520and%2520it%2520also%2520incorporates%2520gaze%2520orientation%2520to%2520rectify%2520unnatural%250Aeye%2520movements%2520that%2520were%2520previously%2520overlooked.%2520Secondly%252C%2520a%2520conformer-structured%250Aconditional%2520diffusion%2520model%2520is%2520designed%2520to%2520guarantee%2520head%2520poses%2520that%2520are%2520aware%250Aof%2520prosody.%2520Thirdly%252C%2520to%2520estimate%2520lip-synchronized%2520and%2520realistic%2520expressions%250Afrom%2520the%2520input%2520audio%2520within%2520limited%2520training%2520data%252C%2520a%2520two-stage%2520training%250Astrategy%2520is%2520devised%2520to%2520decouple%2520frequent%2520and%2520frame-wise%2520lip%2520motion%2520distillation%250Afrom%2520the%2520generation%2520of%2520other%2520more%2520temporally%2520dependent%2520but%2520less%2520audio-related%250Amotions%252C%2520e.g.%252C%2520blinks%2520and%2520frowns.%2520Extensive%2520experiments%2520validate%2520GoHD%2527s%250Aadvanced%2520generalization%2520capabilities%252C%2520demonstrating%2520its%2520effectiveness%2520in%250Agenerating%2520realistic%2520talking%2520face%2520results%2520on%2520arbitrary%2520subjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GoHD%3A%20Gaze-oriented%20and%20Highly%20Disentangled%20Portrait%20Animation%20with%0A%20%20Rhythmic%20Poses%20and%20Realistic%20Expression&entry.906535625=Ziqi%20Zhou%20and%20Weize%20Quan%20and%20Hailin%20Shi%20and%20Wei%20Li%20and%20Lili%20Wang%20and%20Dong-ming%20Yan&entry.1292438233=%20%20Audio-driven%20talking%20head%20generation%20necessitates%20seamless%20integration%20of%0Aaudio%20and%20visual%20data%20amidst%20the%20challenges%20posed%20by%20diverse%20input%20portraits%0Aand%20intricate%20correlations%20between%20audio%20and%20facial%20motions.%20In%20response%2C%20we%0Apropose%20a%20robust%20framework%20GoHD%20designed%20to%20produce%20highly%20realistic%2C%0Aexpressive%2C%20and%20controllable%20portrait%20videos%20from%20any%20reference%20identity%20with%0Aany%20motion.%20GoHD%20innovates%20with%20three%20key%20modules%3A%20Firstly%2C%20an%20animation%20module%0Autilizing%20latent%20navigation%20is%20introduced%20to%20improve%20the%20generalization%20ability%0Aacross%20unseen%20input%20styles.%20This%20module%20achieves%20high%20disentanglement%20of%20motion%0Aand%20identity%2C%20and%20it%20also%20incorporates%20gaze%20orientation%20to%20rectify%20unnatural%0Aeye%20movements%20that%20were%20previously%20overlooked.%20Secondly%2C%20a%20conformer-structured%0Aconditional%20diffusion%20model%20is%20designed%20to%20guarantee%20head%20poses%20that%20are%20aware%0Aof%20prosody.%20Thirdly%2C%20to%20estimate%20lip-synchronized%20and%20realistic%20expressions%0Afrom%20the%20input%20audio%20within%20limited%20training%20data%2C%20a%20two-stage%20training%0Astrategy%20is%20devised%20to%20decouple%20frequent%20and%20frame-wise%20lip%20motion%20distillation%0Afrom%20the%20generation%20of%20other%20more%20temporally%20dependent%20but%20less%20audio-related%0Amotions%2C%20e.g.%2C%20blinks%20and%20frowns.%20Extensive%20experiments%20validate%20GoHD%27s%0Aadvanced%20generalization%20capabilities%2C%20demonstrating%20its%20effectiveness%20in%0Agenerating%20realistic%20talking%20face%20results%20on%20arbitrary%20subjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09296v1&entry.124074799=Read"},
{"title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos", "author": "Linyi Jin and Richard Tucker and Zhengqi Li and David Fouhey and Noah Snavely and Aleksander Holynski", "abstract": "  Learning to understand dynamic 3D scenes from imagery is crucial for\napplications ranging from robotics to scene reconstruction. Yet, unlike other\nproblems where large-scale supervised training has enabled rapid progress,\ndirectly supervising methods for recovering 3D motion remains challenging due\nto the fundamental difficulty of obtaining ground truth annotations. We present\na system for mining high-quality 4D reconstructions from internet stereoscopic,\nwide-angle videos. Our system fuses and filters the outputs of camera pose\nestimation, stereo depth estimation, and temporal tracking methods into\nhigh-quality dynamic 3D reconstructions. We use this method to generate\nlarge-scale data in the form of world-consistent, pseudo-metric 3D point clouds\nwith long-term motion trajectories. We demonstrate the utility of this data by\ntraining a variant of DUSt3R to predict structure and 3D motion from real-world\nimage pairs, showing that training on our reconstructed data enables\ngeneralization to diverse real-world scenes. Project page:\nhttps://stereo4d.github.io\n", "link": "http://arxiv.org/abs/2412.09621v1", "date": "2024-12-12", "relevancy": 3.1863, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6475}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stereo4D%3A%20Learning%20How%20Things%20Move%20in%203D%20from%20Internet%20Stereo%20Videos&body=Title%3A%20Stereo4D%3A%20Learning%20How%20Things%20Move%20in%203D%20from%20Internet%20Stereo%20Videos%0AAuthor%3A%20Linyi%20Jin%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20David%20Fouhey%20and%20Noah%20Snavely%20and%20Aleksander%20Holynski%0AAbstract%3A%20%20%20Learning%20to%20understand%20dynamic%203D%20scenes%20from%20imagery%20is%20crucial%20for%0Aapplications%20ranging%20from%20robotics%20to%20scene%20reconstruction.%20Yet%2C%20unlike%20other%0Aproblems%20where%20large-scale%20supervised%20training%20has%20enabled%20rapid%20progress%2C%0Adirectly%20supervising%20methods%20for%20recovering%203D%20motion%20remains%20challenging%20due%0Ato%20the%20fundamental%20difficulty%20of%20obtaining%20ground%20truth%20annotations.%20We%20present%0Aa%20system%20for%20mining%20high-quality%204D%20reconstructions%20from%20internet%20stereoscopic%2C%0Awide-angle%20videos.%20Our%20system%20fuses%20and%20filters%20the%20outputs%20of%20camera%20pose%0Aestimation%2C%20stereo%20depth%20estimation%2C%20and%20temporal%20tracking%20methods%20into%0Ahigh-quality%20dynamic%203D%20reconstructions.%20We%20use%20this%20method%20to%20generate%0Alarge-scale%20data%20in%20the%20form%20of%20world-consistent%2C%20pseudo-metric%203D%20point%20clouds%0Awith%20long-term%20motion%20trajectories.%20We%20demonstrate%20the%20utility%20of%20this%20data%20by%0Atraining%20a%20variant%20of%20DUSt3R%20to%20predict%20structure%20and%203D%20motion%20from%20real-world%0Aimage%20pairs%2C%20showing%20that%20training%20on%20our%20reconstructed%20data%20enables%0Ageneralization%20to%20diverse%20real-world%20scenes.%20Project%20page%3A%0Ahttps%3A//stereo4d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereo4D%253A%2520Learning%2520How%2520Things%2520Move%2520in%25203D%2520from%2520Internet%2520Stereo%2520Videos%26entry.906535625%3DLinyi%2520Jin%2520and%2520Richard%2520Tucker%2520and%2520Zhengqi%2520Li%2520and%2520David%2520Fouhey%2520and%2520Noah%2520Snavely%2520and%2520Aleksander%2520Holynski%26entry.1292438233%3D%2520%2520Learning%2520to%2520understand%2520dynamic%25203D%2520scenes%2520from%2520imagery%2520is%2520crucial%2520for%250Aapplications%2520ranging%2520from%2520robotics%2520to%2520scene%2520reconstruction.%2520Yet%252C%2520unlike%2520other%250Aproblems%2520where%2520large-scale%2520supervised%2520training%2520has%2520enabled%2520rapid%2520progress%252C%250Adirectly%2520supervising%2520methods%2520for%2520recovering%25203D%2520motion%2520remains%2520challenging%2520due%250Ato%2520the%2520fundamental%2520difficulty%2520of%2520obtaining%2520ground%2520truth%2520annotations.%2520We%2520present%250Aa%2520system%2520for%2520mining%2520high-quality%25204D%2520reconstructions%2520from%2520internet%2520stereoscopic%252C%250Awide-angle%2520videos.%2520Our%2520system%2520fuses%2520and%2520filters%2520the%2520outputs%2520of%2520camera%2520pose%250Aestimation%252C%2520stereo%2520depth%2520estimation%252C%2520and%2520temporal%2520tracking%2520methods%2520into%250Ahigh-quality%2520dynamic%25203D%2520reconstructions.%2520We%2520use%2520this%2520method%2520to%2520generate%250Alarge-scale%2520data%2520in%2520the%2520form%2520of%2520world-consistent%252C%2520pseudo-metric%25203D%2520point%2520clouds%250Awith%2520long-term%2520motion%2520trajectories.%2520We%2520demonstrate%2520the%2520utility%2520of%2520this%2520data%2520by%250Atraining%2520a%2520variant%2520of%2520DUSt3R%2520to%2520predict%2520structure%2520and%25203D%2520motion%2520from%2520real-world%250Aimage%2520pairs%252C%2520showing%2520that%2520training%2520on%2520our%2520reconstructed%2520data%2520enables%250Ageneralization%2520to%2520diverse%2520real-world%2520scenes.%2520Project%2520page%253A%250Ahttps%253A//stereo4d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stereo4D%3A%20Learning%20How%20Things%20Move%20in%203D%20from%20Internet%20Stereo%20Videos&entry.906535625=Linyi%20Jin%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20David%20Fouhey%20and%20Noah%20Snavely%20and%20Aleksander%20Holynski&entry.1292438233=%20%20Learning%20to%20understand%20dynamic%203D%20scenes%20from%20imagery%20is%20crucial%20for%0Aapplications%20ranging%20from%20robotics%20to%20scene%20reconstruction.%20Yet%2C%20unlike%20other%0Aproblems%20where%20large-scale%20supervised%20training%20has%20enabled%20rapid%20progress%2C%0Adirectly%20supervising%20methods%20for%20recovering%203D%20motion%20remains%20challenging%20due%0Ato%20the%20fundamental%20difficulty%20of%20obtaining%20ground%20truth%20annotations.%20We%20present%0Aa%20system%20for%20mining%20high-quality%204D%20reconstructions%20from%20internet%20stereoscopic%2C%0Awide-angle%20videos.%20Our%20system%20fuses%20and%20filters%20the%20outputs%20of%20camera%20pose%0Aestimation%2C%20stereo%20depth%20estimation%2C%20and%20temporal%20tracking%20methods%20into%0Ahigh-quality%20dynamic%203D%20reconstructions.%20We%20use%20this%20method%20to%20generate%0Alarge-scale%20data%20in%20the%20form%20of%20world-consistent%2C%20pseudo-metric%203D%20point%20clouds%0Awith%20long-term%20motion%20trajectories.%20We%20demonstrate%20the%20utility%20of%20this%20data%20by%0Atraining%20a%20variant%20of%20DUSt3R%20to%20predict%20structure%20and%203D%20motion%20from%20real-world%0Aimage%20pairs%2C%20showing%20that%20training%20on%20our%20reconstructed%20data%20enables%0Ageneralization%20to%20diverse%20real-world%20scenes.%20Project%20page%3A%0Ahttps%3A//stereo4d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09621v1&entry.124074799=Read"},
{"title": "DisPose: Disentangling Pose Guidance for Controllable Human Image\n  Animation", "author": "Hongxiang Li and Yaowei Li and Yuhang Yang and Junjie Cao and Zhihong Zhu and Xuxin Cheng and Long Chen", "abstract": "  Controllable human image animation aims to generate videos from reference\nimages using driving videos. Due to the limited control signals provided by\nsparse guidance (e.g., skeleton pose), recent works have attempted to introduce\nadditional dense conditions (e.g., depth map) to ensure motion alignment.\nHowever, such strict dense guidance impairs the quality of the generated video\nwhen the body shape of the reference character differs significantly from that\nof the driving video. In this paper, we present DisPose to mine more\ngeneralizable and effective control signals without additional dense input,\nwhich disentangles the sparse skeleton pose in human image animation into\nmotion field guidance and keypoint correspondence. Specifically, we generate a\ndense motion field from a sparse motion field and the reference image, which\nprovides region-level dense guidance while maintaining the generalization of\nthe sparse pose control. We also extract diffusion features corresponding to\npose keypoints from the reference image, and then these point features are\ntransferred to the target pose to provide distinct identity information. To\nseamlessly integrate into existing models, we propose a plug-and-play hybrid\nControlNet that improves the quality and consistency of generated videos while\nfreezing the existing model parameters. Extensive qualitative and quantitative\nexperiments demonstrate the superiority of DisPose compared to current methods.\nCode:\n\\hyperlink{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.\n", "link": "http://arxiv.org/abs/2412.09349v1", "date": "2024-12-12", "relevancy": 3.1599, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.7006}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6039}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisPose%3A%20Disentangling%20Pose%20Guidance%20for%20Controllable%20Human%20Image%0A%20%20Animation&body=Title%3A%20DisPose%3A%20Disentangling%20Pose%20Guidance%20for%20Controllable%20Human%20Image%0A%20%20Animation%0AAuthor%3A%20Hongxiang%20Li%20and%20Yaowei%20Li%20and%20Yuhang%20Yang%20and%20Junjie%20Cao%20and%20Zhihong%20Zhu%20and%20Xuxin%20Cheng%20and%20Long%20Chen%0AAbstract%3A%20%20%20Controllable%20human%20image%20animation%20aims%20to%20generate%20videos%20from%20reference%0Aimages%20using%20driving%20videos.%20Due%20to%20the%20limited%20control%20signals%20provided%20by%0Asparse%20guidance%20%28e.g.%2C%20skeleton%20pose%29%2C%20recent%20works%20have%20attempted%20to%20introduce%0Aadditional%20dense%20conditions%20%28e.g.%2C%20depth%20map%29%20to%20ensure%20motion%20alignment.%0AHowever%2C%20such%20strict%20dense%20guidance%20impairs%20the%20quality%20of%20the%20generated%20video%0Awhen%20the%20body%20shape%20of%20the%20reference%20character%20differs%20significantly%20from%20that%0Aof%20the%20driving%20video.%20In%20this%20paper%2C%20we%20present%20DisPose%20to%20mine%20more%0Ageneralizable%20and%20effective%20control%20signals%20without%20additional%20dense%20input%2C%0Awhich%20disentangles%20the%20sparse%20skeleton%20pose%20in%20human%20image%20animation%20into%0Amotion%20field%20guidance%20and%20keypoint%20correspondence.%20Specifically%2C%20we%20generate%20a%0Adense%20motion%20field%20from%20a%20sparse%20motion%20field%20and%20the%20reference%20image%2C%20which%0Aprovides%20region-level%20dense%20guidance%20while%20maintaining%20the%20generalization%20of%0Athe%20sparse%20pose%20control.%20We%20also%20extract%20diffusion%20features%20corresponding%20to%0Apose%20keypoints%20from%20the%20reference%20image%2C%20and%20then%20these%20point%20features%20are%0Atransferred%20to%20the%20target%20pose%20to%20provide%20distinct%20identity%20information.%20To%0Aseamlessly%20integrate%20into%20existing%20models%2C%20we%20propose%20a%20plug-and-play%20hybrid%0AControlNet%20that%20improves%20the%20quality%20and%20consistency%20of%20generated%20videos%20while%0Afreezing%20the%20existing%20model%20parameters.%20Extensive%20qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20the%20superiority%20of%20DisPose%20compared%20to%20current%20methods.%0ACode%3A%0A%5Chyperlink%7Bhttps%3A//github.com/lihxxx/DisPose%7D%7Bhttps%3A//github.com/lihxxx/DisPose%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisPose%253A%2520Disentangling%2520Pose%2520Guidance%2520for%2520Controllable%2520Human%2520Image%250A%2520%2520Animation%26entry.906535625%3DHongxiang%2520Li%2520and%2520Yaowei%2520Li%2520and%2520Yuhang%2520Yang%2520and%2520Junjie%2520Cao%2520and%2520Zhihong%2520Zhu%2520and%2520Xuxin%2520Cheng%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Controllable%2520human%2520image%2520animation%2520aims%2520to%2520generate%2520videos%2520from%2520reference%250Aimages%2520using%2520driving%2520videos.%2520Due%2520to%2520the%2520limited%2520control%2520signals%2520provided%2520by%250Asparse%2520guidance%2520%2528e.g.%252C%2520skeleton%2520pose%2529%252C%2520recent%2520works%2520have%2520attempted%2520to%2520introduce%250Aadditional%2520dense%2520conditions%2520%2528e.g.%252C%2520depth%2520map%2529%2520to%2520ensure%2520motion%2520alignment.%250AHowever%252C%2520such%2520strict%2520dense%2520guidance%2520impairs%2520the%2520quality%2520of%2520the%2520generated%2520video%250Awhen%2520the%2520body%2520shape%2520of%2520the%2520reference%2520character%2520differs%2520significantly%2520from%2520that%250Aof%2520the%2520driving%2520video.%2520In%2520this%2520paper%252C%2520we%2520present%2520DisPose%2520to%2520mine%2520more%250Ageneralizable%2520and%2520effective%2520control%2520signals%2520without%2520additional%2520dense%2520input%252C%250Awhich%2520disentangles%2520the%2520sparse%2520skeleton%2520pose%2520in%2520human%2520image%2520animation%2520into%250Amotion%2520field%2520guidance%2520and%2520keypoint%2520correspondence.%2520Specifically%252C%2520we%2520generate%2520a%250Adense%2520motion%2520field%2520from%2520a%2520sparse%2520motion%2520field%2520and%2520the%2520reference%2520image%252C%2520which%250Aprovides%2520region-level%2520dense%2520guidance%2520while%2520maintaining%2520the%2520generalization%2520of%250Athe%2520sparse%2520pose%2520control.%2520We%2520also%2520extract%2520diffusion%2520features%2520corresponding%2520to%250Apose%2520keypoints%2520from%2520the%2520reference%2520image%252C%2520and%2520then%2520these%2520point%2520features%2520are%250Atransferred%2520to%2520the%2520target%2520pose%2520to%2520provide%2520distinct%2520identity%2520information.%2520To%250Aseamlessly%2520integrate%2520into%2520existing%2520models%252C%2520we%2520propose%2520a%2520plug-and-play%2520hybrid%250AControlNet%2520that%2520improves%2520the%2520quality%2520and%2520consistency%2520of%2520generated%2520videos%2520while%250Afreezing%2520the%2520existing%2520model%2520parameters.%2520Extensive%2520qualitative%2520and%2520quantitative%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520DisPose%2520compared%2520to%2520current%2520methods.%250ACode%253A%250A%255Chyperlink%257Bhttps%253A//github.com/lihxxx/DisPose%257D%257Bhttps%253A//github.com/lihxxx/DisPose%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisPose%3A%20Disentangling%20Pose%20Guidance%20for%20Controllable%20Human%20Image%0A%20%20Animation&entry.906535625=Hongxiang%20Li%20and%20Yaowei%20Li%20and%20Yuhang%20Yang%20and%20Junjie%20Cao%20and%20Zhihong%20Zhu%20and%20Xuxin%20Cheng%20and%20Long%20Chen&entry.1292438233=%20%20Controllable%20human%20image%20animation%20aims%20to%20generate%20videos%20from%20reference%0Aimages%20using%20driving%20videos.%20Due%20to%20the%20limited%20control%20signals%20provided%20by%0Asparse%20guidance%20%28e.g.%2C%20skeleton%20pose%29%2C%20recent%20works%20have%20attempted%20to%20introduce%0Aadditional%20dense%20conditions%20%28e.g.%2C%20depth%20map%29%20to%20ensure%20motion%20alignment.%0AHowever%2C%20such%20strict%20dense%20guidance%20impairs%20the%20quality%20of%20the%20generated%20video%0Awhen%20the%20body%20shape%20of%20the%20reference%20character%20differs%20significantly%20from%20that%0Aof%20the%20driving%20video.%20In%20this%20paper%2C%20we%20present%20DisPose%20to%20mine%20more%0Ageneralizable%20and%20effective%20control%20signals%20without%20additional%20dense%20input%2C%0Awhich%20disentangles%20the%20sparse%20skeleton%20pose%20in%20human%20image%20animation%20into%0Amotion%20field%20guidance%20and%20keypoint%20correspondence.%20Specifically%2C%20we%20generate%20a%0Adense%20motion%20field%20from%20a%20sparse%20motion%20field%20and%20the%20reference%20image%2C%20which%0Aprovides%20region-level%20dense%20guidance%20while%20maintaining%20the%20generalization%20of%0Athe%20sparse%20pose%20control.%20We%20also%20extract%20diffusion%20features%20corresponding%20to%0Apose%20keypoints%20from%20the%20reference%20image%2C%20and%20then%20these%20point%20features%20are%0Atransferred%20to%20the%20target%20pose%20to%20provide%20distinct%20identity%20information.%20To%0Aseamlessly%20integrate%20into%20existing%20models%2C%20we%20propose%20a%20plug-and-play%20hybrid%0AControlNet%20that%20improves%20the%20quality%20and%20consistency%20of%20generated%20videos%20while%0Afreezing%20the%20existing%20model%20parameters.%20Extensive%20qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20the%20superiority%20of%20DisPose%20compared%20to%20current%20methods.%0ACode%3A%0A%5Chyperlink%7Bhttps%3A//github.com/lihxxx/DisPose%7D%7Bhttps%3A//github.com/lihxxx/DisPose%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09349v1&entry.124074799=Read"},
{"title": "Representing Long Volumetric Video with Temporal Gaussian Hierarchy", "author": "Zhen Xu and Yinghao Xu and Zhiyuan Yu and Sida Peng and Jiaming Sun and Hujun Bao and Xiaowei Zhou", "abstract": "  This paper aims to address the challenge of reconstructing long volumetric\nvideos from multi-view RGB videos. Recent dynamic view synthesis methods\nleverage powerful 4D representations, like feature grids or point cloud\nsequences, to achieve high-quality rendering results. However, they are\ntypically limited to short (1~2s) video clips and often suffer from large\nmemory footprints when dealing with longer videos. To solve this issue, we\npropose a novel 4D representation, named Temporal Gaussian Hierarchy, to\ncompactly model long volumetric videos. Our key observation is that there are\ngenerally various degrees of temporal redundancy in dynamic scenes, which\nconsist of areas changing at different speeds. Motivated by this, our approach\nbuilds a multi-level hierarchy of 4D Gaussian primitives, where each level\nseparately describes scene regions with different degrees of content change,\nand adaptively shares Gaussian primitives to represent unchanged scene content\nover different temporal segments, thus effectively reducing the number of\nGaussian primitives. In addition, the tree-like structure of the Gaussian\nhierarchy allows us to efficiently represent the scene at a particular moment\nwith a subset of Gaussian primitives, leading to nearly constant GPU memory\nusage during the training or rendering regardless of the video length.\nExtensive experimental results demonstrate the superiority of our method over\nalternative methods in terms of training cost, rendering speed, and storage\nusage. To our knowledge, this work is the first approach capable of efficiently\nhandling minutes of volumetric video data while maintaining state-of-the-art\nrendering quality. Our project page is available at:\nhttps://zju3dv.github.io/longvolcap.\n", "link": "http://arxiv.org/abs/2412.09608v1", "date": "2024-12-12", "relevancy": 3.1174, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.665}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.631}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representing%20Long%20Volumetric%20Video%20with%20Temporal%20Gaussian%20Hierarchy&body=Title%3A%20Representing%20Long%20Volumetric%20Video%20with%20Temporal%20Gaussian%20Hierarchy%0AAuthor%3A%20Zhen%20Xu%20and%20Yinghao%20Xu%20and%20Zhiyuan%20Yu%20and%20Sida%20Peng%20and%20Jiaming%20Sun%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20address%20the%20challenge%20of%20reconstructing%20long%20volumetric%0Avideos%20from%20multi-view%20RGB%20videos.%20Recent%20dynamic%20view%20synthesis%20methods%0Aleverage%20powerful%204D%20representations%2C%20like%20feature%20grids%20or%20point%20cloud%0Asequences%2C%20to%20achieve%20high-quality%20rendering%20results.%20However%2C%20they%20are%0Atypically%20limited%20to%20short%20%281~2s%29%20video%20clips%20and%20often%20suffer%20from%20large%0Amemory%20footprints%20when%20dealing%20with%20longer%20videos.%20To%20solve%20this%20issue%2C%20we%0Apropose%20a%20novel%204D%20representation%2C%20named%20Temporal%20Gaussian%20Hierarchy%2C%20to%0Acompactly%20model%20long%20volumetric%20videos.%20Our%20key%20observation%20is%20that%20there%20are%0Agenerally%20various%20degrees%20of%20temporal%20redundancy%20in%20dynamic%20scenes%2C%20which%0Aconsist%20of%20areas%20changing%20at%20different%20speeds.%20Motivated%20by%20this%2C%20our%20approach%0Abuilds%20a%20multi-level%20hierarchy%20of%204D%20Gaussian%20primitives%2C%20where%20each%20level%0Aseparately%20describes%20scene%20regions%20with%20different%20degrees%20of%20content%20change%2C%0Aand%20adaptively%20shares%20Gaussian%20primitives%20to%20represent%20unchanged%20scene%20content%0Aover%20different%20temporal%20segments%2C%20thus%20effectively%20reducing%20the%20number%20of%0AGaussian%20primitives.%20In%20addition%2C%20the%20tree-like%20structure%20of%20the%20Gaussian%0Ahierarchy%20allows%20us%20to%20efficiently%20represent%20the%20scene%20at%20a%20particular%20moment%0Awith%20a%20subset%20of%20Gaussian%20primitives%2C%20leading%20to%20nearly%20constant%20GPU%20memory%0Ausage%20during%20the%20training%20or%20rendering%20regardless%20of%20the%20video%20length.%0AExtensive%20experimental%20results%20demonstrate%20the%20superiority%20of%20our%20method%20over%0Aalternative%20methods%20in%20terms%20of%20training%20cost%2C%20rendering%20speed%2C%20and%20storage%0Ausage.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20approach%20capable%20of%20efficiently%0Ahandling%20minutes%20of%20volumetric%20video%20data%20while%20maintaining%20state-of-the-art%0Arendering%20quality.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//zju3dv.github.io/longvolcap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresenting%2520Long%2520Volumetric%2520Video%2520with%2520Temporal%2520Gaussian%2520Hierarchy%26entry.906535625%3DZhen%2520Xu%2520and%2520Yinghao%2520Xu%2520and%2520Zhiyuan%2520Yu%2520and%2520Sida%2520Peng%2520and%2520Jiaming%2520Sun%2520and%2520Hujun%2520Bao%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520address%2520the%2520challenge%2520of%2520reconstructing%2520long%2520volumetric%250Avideos%2520from%2520multi-view%2520RGB%2520videos.%2520Recent%2520dynamic%2520view%2520synthesis%2520methods%250Aleverage%2520powerful%25204D%2520representations%252C%2520like%2520feature%2520grids%2520or%2520point%2520cloud%250Asequences%252C%2520to%2520achieve%2520high-quality%2520rendering%2520results.%2520However%252C%2520they%2520are%250Atypically%2520limited%2520to%2520short%2520%25281~2s%2529%2520video%2520clips%2520and%2520often%2520suffer%2520from%2520large%250Amemory%2520footprints%2520when%2520dealing%2520with%2520longer%2520videos.%2520To%2520solve%2520this%2520issue%252C%2520we%250Apropose%2520a%2520novel%25204D%2520representation%252C%2520named%2520Temporal%2520Gaussian%2520Hierarchy%252C%2520to%250Acompactly%2520model%2520long%2520volumetric%2520videos.%2520Our%2520key%2520observation%2520is%2520that%2520there%2520are%250Agenerally%2520various%2520degrees%2520of%2520temporal%2520redundancy%2520in%2520dynamic%2520scenes%252C%2520which%250Aconsist%2520of%2520areas%2520changing%2520at%2520different%2520speeds.%2520Motivated%2520by%2520this%252C%2520our%2520approach%250Abuilds%2520a%2520multi-level%2520hierarchy%2520of%25204D%2520Gaussian%2520primitives%252C%2520where%2520each%2520level%250Aseparately%2520describes%2520scene%2520regions%2520with%2520different%2520degrees%2520of%2520content%2520change%252C%250Aand%2520adaptively%2520shares%2520Gaussian%2520primitives%2520to%2520represent%2520unchanged%2520scene%2520content%250Aover%2520different%2520temporal%2520segments%252C%2520thus%2520effectively%2520reducing%2520the%2520number%2520of%250AGaussian%2520primitives.%2520In%2520addition%252C%2520the%2520tree-like%2520structure%2520of%2520the%2520Gaussian%250Ahierarchy%2520allows%2520us%2520to%2520efficiently%2520represent%2520the%2520scene%2520at%2520a%2520particular%2520moment%250Awith%2520a%2520subset%2520of%2520Gaussian%2520primitives%252C%2520leading%2520to%2520nearly%2520constant%2520GPU%2520memory%250Ausage%2520during%2520the%2520training%2520or%2520rendering%2520regardless%2520of%2520the%2520video%2520length.%250AExtensive%2520experimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%250Aalternative%2520methods%2520in%2520terms%2520of%2520training%2520cost%252C%2520rendering%2520speed%252C%2520and%2520storage%250Ausage.%2520To%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520approach%2520capable%2520of%2520efficiently%250Ahandling%2520minutes%2520of%2520volumetric%2520video%2520data%2520while%2520maintaining%2520state-of-the-art%250Arendering%2520quality.%2520Our%2520project%2520page%2520is%2520available%2520at%253A%250Ahttps%253A//zju3dv.github.io/longvolcap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20Long%20Volumetric%20Video%20with%20Temporal%20Gaussian%20Hierarchy&entry.906535625=Zhen%20Xu%20and%20Yinghao%20Xu%20and%20Zhiyuan%20Yu%20and%20Sida%20Peng%20and%20Jiaming%20Sun%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20This%20paper%20aims%20to%20address%20the%20challenge%20of%20reconstructing%20long%20volumetric%0Avideos%20from%20multi-view%20RGB%20videos.%20Recent%20dynamic%20view%20synthesis%20methods%0Aleverage%20powerful%204D%20representations%2C%20like%20feature%20grids%20or%20point%20cloud%0Asequences%2C%20to%20achieve%20high-quality%20rendering%20results.%20However%2C%20they%20are%0Atypically%20limited%20to%20short%20%281~2s%29%20video%20clips%20and%20often%20suffer%20from%20large%0Amemory%20footprints%20when%20dealing%20with%20longer%20videos.%20To%20solve%20this%20issue%2C%20we%0Apropose%20a%20novel%204D%20representation%2C%20named%20Temporal%20Gaussian%20Hierarchy%2C%20to%0Acompactly%20model%20long%20volumetric%20videos.%20Our%20key%20observation%20is%20that%20there%20are%0Agenerally%20various%20degrees%20of%20temporal%20redundancy%20in%20dynamic%20scenes%2C%20which%0Aconsist%20of%20areas%20changing%20at%20different%20speeds.%20Motivated%20by%20this%2C%20our%20approach%0Abuilds%20a%20multi-level%20hierarchy%20of%204D%20Gaussian%20primitives%2C%20where%20each%20level%0Aseparately%20describes%20scene%20regions%20with%20different%20degrees%20of%20content%20change%2C%0Aand%20adaptively%20shares%20Gaussian%20primitives%20to%20represent%20unchanged%20scene%20content%0Aover%20different%20temporal%20segments%2C%20thus%20effectively%20reducing%20the%20number%20of%0AGaussian%20primitives.%20In%20addition%2C%20the%20tree-like%20structure%20of%20the%20Gaussian%0Ahierarchy%20allows%20us%20to%20efficiently%20represent%20the%20scene%20at%20a%20particular%20moment%0Awith%20a%20subset%20of%20Gaussian%20primitives%2C%20leading%20to%20nearly%20constant%20GPU%20memory%0Ausage%20during%20the%20training%20or%20rendering%20regardless%20of%20the%20video%20length.%0AExtensive%20experimental%20results%20demonstrate%20the%20superiority%20of%20our%20method%20over%0Aalternative%20methods%20in%20terms%20of%20training%20cost%2C%20rendering%20speed%2C%20and%20storage%0Ausage.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20approach%20capable%20of%20efficiently%0Ahandling%20minutes%20of%20volumetric%20video%20data%20while%20maintaining%20state-of-the-art%0Arendering%20quality.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//zju3dv.github.io/longvolcap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09608v1&entry.124074799=Read"},
{"title": "LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living", "author": "Dominick Reilly and Rajatsubhra Chakraborty and Arkaprava Sinha and Manish Kumar Govind and Pu Wang and Francois Bremond and Le Xue and Srijan Das", "abstract": "  Current Large Language Vision Models (LLVMs) trained on web videos perform\nwell in general video understanding but struggle with fine-grained details,\ncomplex human-object interactions (HOI), and view-invariant representation\nlearning essential for Activities of Daily Living (ADL). This limitation stems\nfrom a lack of specialized ADL video instruction-tuning datasets and\ninsufficient modality integration to capture discriminative action\nrepresentations. To address this, we propose a semi-automated framework for\ncurating ADL datasets, creating ADL-X, a multiview, multimodal RGBS\ninstruction-tuning dataset. Additionally, we introduce LLAVIDAL, an LLVM\nintegrating videos, 3D skeletons, and HOIs to model ADL's complex\nspatiotemporal relationships. For training LLAVIDAL a simple joint alignment of\nall modalities yields suboptimal results; thus, we propose a Multimodal\nProgressive (MMPro) training strategy, incorporating modalities in stages\nfollowing a curriculum. We also establish ADL MCQ and video description\nbenchmarks to assess LLVM performance in ADL tasks. Trained on ADL-X, LLAVIDAL\nachieves state-of-the-art performance across ADL benchmarks. Code and data will\nbe made publicly available at: https://adl-x.github.io/.\n", "link": "http://arxiv.org/abs/2406.09390v2", "date": "2024-12-12", "relevancy": 3.0936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6228}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLAVIDAL%3A%20A%20Large%20LAnguage%20VIsion%20Model%20for%20Daily%20Activities%20of%20Living&body=Title%3A%20LLAVIDAL%3A%20A%20Large%20LAnguage%20VIsion%20Model%20for%20Daily%20Activities%20of%20Living%0AAuthor%3A%20Dominick%20Reilly%20and%20Rajatsubhra%20Chakraborty%20and%20Arkaprava%20Sinha%20and%20Manish%20Kumar%20Govind%20and%20Pu%20Wang%20and%20Francois%20Bremond%20and%20Le%20Xue%20and%20Srijan%20Das%0AAbstract%3A%20%20%20Current%20Large%20Language%20Vision%20Models%20%28LLVMs%29%20trained%20on%20web%20videos%20perform%0Awell%20in%20general%20video%20understanding%20but%20struggle%20with%20fine-grained%20details%2C%0Acomplex%20human-object%20interactions%20%28HOI%29%2C%20and%20view-invariant%20representation%0Alearning%20essential%20for%20Activities%20of%20Daily%20Living%20%28ADL%29.%20This%20limitation%20stems%0Afrom%20a%20lack%20of%20specialized%20ADL%20video%20instruction-tuning%20datasets%20and%0Ainsufficient%20modality%20integration%20to%20capture%20discriminative%20action%0Arepresentations.%20To%20address%20this%2C%20we%20propose%20a%20semi-automated%20framework%20for%0Acurating%20ADL%20datasets%2C%20creating%20ADL-X%2C%20a%20multiview%2C%20multimodal%20RGBS%0Ainstruction-tuning%20dataset.%20Additionally%2C%20we%20introduce%20LLAVIDAL%2C%20an%20LLVM%0Aintegrating%20videos%2C%203D%20skeletons%2C%20and%20HOIs%20to%20model%20ADL%27s%20complex%0Aspatiotemporal%20relationships.%20For%20training%20LLAVIDAL%20a%20simple%20joint%20alignment%20of%0Aall%20modalities%20yields%20suboptimal%20results%3B%20thus%2C%20we%20propose%20a%20Multimodal%0AProgressive%20%28MMPro%29%20training%20strategy%2C%20incorporating%20modalities%20in%20stages%0Afollowing%20a%20curriculum.%20We%20also%20establish%20ADL%20MCQ%20and%20video%20description%0Abenchmarks%20to%20assess%20LLVM%20performance%20in%20ADL%20tasks.%20Trained%20on%20ADL-X%2C%20LLAVIDAL%0Aachieves%20state-of-the-art%20performance%20across%20ADL%20benchmarks.%20Code%20and%20data%20will%0Abe%20made%20publicly%20available%20at%3A%20https%3A//adl-x.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLAVIDAL%253A%2520A%2520Large%2520LAnguage%2520VIsion%2520Model%2520for%2520Daily%2520Activities%2520of%2520Living%26entry.906535625%3DDominick%2520Reilly%2520and%2520Rajatsubhra%2520Chakraborty%2520and%2520Arkaprava%2520Sinha%2520and%2520Manish%2520Kumar%2520Govind%2520and%2520Pu%2520Wang%2520and%2520Francois%2520Bremond%2520and%2520Le%2520Xue%2520and%2520Srijan%2520Das%26entry.1292438233%3D%2520%2520Current%2520Large%2520Language%2520Vision%2520Models%2520%2528LLVMs%2529%2520trained%2520on%2520web%2520videos%2520perform%250Awell%2520in%2520general%2520video%2520understanding%2520but%2520struggle%2520with%2520fine-grained%2520details%252C%250Acomplex%2520human-object%2520interactions%2520%2528HOI%2529%252C%2520and%2520view-invariant%2520representation%250Alearning%2520essential%2520for%2520Activities%2520of%2520Daily%2520Living%2520%2528ADL%2529.%2520This%2520limitation%2520stems%250Afrom%2520a%2520lack%2520of%2520specialized%2520ADL%2520video%2520instruction-tuning%2520datasets%2520and%250Ainsufficient%2520modality%2520integration%2520to%2520capture%2520discriminative%2520action%250Arepresentations.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520semi-automated%2520framework%2520for%250Acurating%2520ADL%2520datasets%252C%2520creating%2520ADL-X%252C%2520a%2520multiview%252C%2520multimodal%2520RGBS%250Ainstruction-tuning%2520dataset.%2520Additionally%252C%2520we%2520introduce%2520LLAVIDAL%252C%2520an%2520LLVM%250Aintegrating%2520videos%252C%25203D%2520skeletons%252C%2520and%2520HOIs%2520to%2520model%2520ADL%2527s%2520complex%250Aspatiotemporal%2520relationships.%2520For%2520training%2520LLAVIDAL%2520a%2520simple%2520joint%2520alignment%2520of%250Aall%2520modalities%2520yields%2520suboptimal%2520results%253B%2520thus%252C%2520we%2520propose%2520a%2520Multimodal%250AProgressive%2520%2528MMPro%2529%2520training%2520strategy%252C%2520incorporating%2520modalities%2520in%2520stages%250Afollowing%2520a%2520curriculum.%2520We%2520also%2520establish%2520ADL%2520MCQ%2520and%2520video%2520description%250Abenchmarks%2520to%2520assess%2520LLVM%2520performance%2520in%2520ADL%2520tasks.%2520Trained%2520on%2520ADL-X%252C%2520LLAVIDAL%250Aachieves%2520state-of-the-art%2520performance%2520across%2520ADL%2520benchmarks.%2520Code%2520and%2520data%2520will%250Abe%2520made%2520publicly%2520available%2520at%253A%2520https%253A//adl-x.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLAVIDAL%3A%20A%20Large%20LAnguage%20VIsion%20Model%20for%20Daily%20Activities%20of%20Living&entry.906535625=Dominick%20Reilly%20and%20Rajatsubhra%20Chakraborty%20and%20Arkaprava%20Sinha%20and%20Manish%20Kumar%20Govind%20and%20Pu%20Wang%20and%20Francois%20Bremond%20and%20Le%20Xue%20and%20Srijan%20Das&entry.1292438233=%20%20Current%20Large%20Language%20Vision%20Models%20%28LLVMs%29%20trained%20on%20web%20videos%20perform%0Awell%20in%20general%20video%20understanding%20but%20struggle%20with%20fine-grained%20details%2C%0Acomplex%20human-object%20interactions%20%28HOI%29%2C%20and%20view-invariant%20representation%0Alearning%20essential%20for%20Activities%20of%20Daily%20Living%20%28ADL%29.%20This%20limitation%20stems%0Afrom%20a%20lack%20of%20specialized%20ADL%20video%20instruction-tuning%20datasets%20and%0Ainsufficient%20modality%20integration%20to%20capture%20discriminative%20action%0Arepresentations.%20To%20address%20this%2C%20we%20propose%20a%20semi-automated%20framework%20for%0Acurating%20ADL%20datasets%2C%20creating%20ADL-X%2C%20a%20multiview%2C%20multimodal%20RGBS%0Ainstruction-tuning%20dataset.%20Additionally%2C%20we%20introduce%20LLAVIDAL%2C%20an%20LLVM%0Aintegrating%20videos%2C%203D%20skeletons%2C%20and%20HOIs%20to%20model%20ADL%27s%20complex%0Aspatiotemporal%20relationships.%20For%20training%20LLAVIDAL%20a%20simple%20joint%20alignment%20of%0Aall%20modalities%20yields%20suboptimal%20results%3B%20thus%2C%20we%20propose%20a%20Multimodal%0AProgressive%20%28MMPro%29%20training%20strategy%2C%20incorporating%20modalities%20in%20stages%0Afollowing%20a%20curriculum.%20We%20also%20establish%20ADL%20MCQ%20and%20video%20description%0Abenchmarks%20to%20assess%20LLVM%20performance%20in%20ADL%20tasks.%20Trained%20on%20ADL-X%2C%20LLAVIDAL%0Aachieves%20state-of-the-art%20performance%20across%20ADL%20benchmarks.%20Code%20and%20data%20will%0Abe%20made%20publicly%20available%20at%3A%20https%3A//adl-x.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09390v2&entry.124074799=Read"},
{"title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language\n  Models with Variable Visual Position Encoding", "author": "Junqi Ge and Ziyi Chen and Jintao Lin and Jinguo Zhu and Xihui Liu and Jifeng Dai and Xizhou Zhu", "abstract": "  Vision-Language Models (VLMs) have shown promising capabilities in handling\nvarious multimodal tasks, yet they struggle in long-context scenarios,\nparticularly in tasks involving videos, high-resolution images, or lengthy\nimage-text documents. In our work, we first conduct an empirical analysis of\nthe long-context capabilities of VLMs using our augmented long-context\nmultimodal datasets. Our findings reveal that directly applying the positional\nencoding mechanism used for textual tokens to visual tokens is suboptimal, and\nVLM performance degrades sharply when the position encoding exceeds the model's\ncontext window. To address this, we propose Variable Visual Position Encoding\n(V2PE), a novel positional encoding approach that employs variable and smaller\nincrements for visual tokens, enabling more efficient management of long\nmultimodal sequences. Our experiments demonstrate the effectiveness of V2PE to\nenhances VLMs' ability to effectively understand and reason over long\nmultimodal contexts. We further integrate V2PE with our augmented long-context\nmultimodal datasets to fine-tune the open-source VLM, InternVL2. The fine-tuned\nmodel achieves strong performance on both standard and long-context multimodal\ntasks. Notably, when the sequence length of the training dataset is increased\nto 256K tokens, the model is capable of processing multimodal sequences up to\n1M tokens, highlighting its potential for real-world long-context applications.\n", "link": "http://arxiv.org/abs/2412.09616v1", "date": "2024-12-12", "relevancy": 3.0883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2PE%3A%20Improving%20Multimodal%20Long-Context%20Capability%20of%20Vision-Language%0A%20%20Models%20with%20Variable%20Visual%20Position%20Encoding&body=Title%3A%20V2PE%3A%20Improving%20Multimodal%20Long-Context%20Capability%20of%20Vision-Language%0A%20%20Models%20with%20Variable%20Visual%20Position%20Encoding%0AAuthor%3A%20Junqi%20Ge%20and%20Ziyi%20Chen%20and%20Jintao%20Lin%20and%20Jinguo%20Zhu%20and%20Xihui%20Liu%20and%20Jifeng%20Dai%20and%20Xizhou%20Zhu%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20promising%20capabilities%20in%20handling%0Avarious%20multimodal%20tasks%2C%20yet%20they%20struggle%20in%20long-context%20scenarios%2C%0Aparticularly%20in%20tasks%20involving%20videos%2C%20high-resolution%20images%2C%20or%20lengthy%0Aimage-text%20documents.%20In%20our%20work%2C%20we%20first%20conduct%20an%20empirical%20analysis%20of%0Athe%20long-context%20capabilities%20of%20VLMs%20using%20our%20augmented%20long-context%0Amultimodal%20datasets.%20Our%20findings%20reveal%20that%20directly%20applying%20the%20positional%0Aencoding%20mechanism%20used%20for%20textual%20tokens%20to%20visual%20tokens%20is%20suboptimal%2C%20and%0AVLM%20performance%20degrades%20sharply%20when%20the%20position%20encoding%20exceeds%20the%20model%27s%0Acontext%20window.%20To%20address%20this%2C%20we%20propose%20Variable%20Visual%20Position%20Encoding%0A%28V2PE%29%2C%20a%20novel%20positional%20encoding%20approach%20that%20employs%20variable%20and%20smaller%0Aincrements%20for%20visual%20tokens%2C%20enabling%20more%20efficient%20management%20of%20long%0Amultimodal%20sequences.%20Our%20experiments%20demonstrate%20the%20effectiveness%20of%20V2PE%20to%0Aenhances%20VLMs%27%20ability%20to%20effectively%20understand%20and%20reason%20over%20long%0Amultimodal%20contexts.%20We%20further%20integrate%20V2PE%20with%20our%20augmented%20long-context%0Amultimodal%20datasets%20to%20fine-tune%20the%20open-source%20VLM%2C%20InternVL2.%20The%20fine-tuned%0Amodel%20achieves%20strong%20performance%20on%20both%20standard%20and%20long-context%20multimodal%0Atasks.%20Notably%2C%20when%20the%20sequence%20length%20of%20the%20training%20dataset%20is%20increased%0Ato%20256K%20tokens%2C%20the%20model%20is%20capable%20of%20processing%20multimodal%20sequences%20up%20to%0A1M%20tokens%2C%20highlighting%20its%20potential%20for%20real-world%20long-context%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2PE%253A%2520Improving%2520Multimodal%2520Long-Context%2520Capability%2520of%2520Vision-Language%250A%2520%2520Models%2520with%2520Variable%2520Visual%2520Position%2520Encoding%26entry.906535625%3DJunqi%2520Ge%2520and%2520Ziyi%2520Chen%2520and%2520Jintao%2520Lin%2520and%2520Jinguo%2520Zhu%2520and%2520Xihui%2520Liu%2520and%2520Jifeng%2520Dai%2520and%2520Xizhou%2520Zhu%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520promising%2520capabilities%2520in%2520handling%250Avarious%2520multimodal%2520tasks%252C%2520yet%2520they%2520struggle%2520in%2520long-context%2520scenarios%252C%250Aparticularly%2520in%2520tasks%2520involving%2520videos%252C%2520high-resolution%2520images%252C%2520or%2520lengthy%250Aimage-text%2520documents.%2520In%2520our%2520work%252C%2520we%2520first%2520conduct%2520an%2520empirical%2520analysis%2520of%250Athe%2520long-context%2520capabilities%2520of%2520VLMs%2520using%2520our%2520augmented%2520long-context%250Amultimodal%2520datasets.%2520Our%2520findings%2520reveal%2520that%2520directly%2520applying%2520the%2520positional%250Aencoding%2520mechanism%2520used%2520for%2520textual%2520tokens%2520to%2520visual%2520tokens%2520is%2520suboptimal%252C%2520and%250AVLM%2520performance%2520degrades%2520sharply%2520when%2520the%2520position%2520encoding%2520exceeds%2520the%2520model%2527s%250Acontext%2520window.%2520To%2520address%2520this%252C%2520we%2520propose%2520Variable%2520Visual%2520Position%2520Encoding%250A%2528V2PE%2529%252C%2520a%2520novel%2520positional%2520encoding%2520approach%2520that%2520employs%2520variable%2520and%2520smaller%250Aincrements%2520for%2520visual%2520tokens%252C%2520enabling%2520more%2520efficient%2520management%2520of%2520long%250Amultimodal%2520sequences.%2520Our%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520V2PE%2520to%250Aenhances%2520VLMs%2527%2520ability%2520to%2520effectively%2520understand%2520and%2520reason%2520over%2520long%250Amultimodal%2520contexts.%2520We%2520further%2520integrate%2520V2PE%2520with%2520our%2520augmented%2520long-context%250Amultimodal%2520datasets%2520to%2520fine-tune%2520the%2520open-source%2520VLM%252C%2520InternVL2.%2520The%2520fine-tuned%250Amodel%2520achieves%2520strong%2520performance%2520on%2520both%2520standard%2520and%2520long-context%2520multimodal%250Atasks.%2520Notably%252C%2520when%2520the%2520sequence%2520length%2520of%2520the%2520training%2520dataset%2520is%2520increased%250Ato%2520256K%2520tokens%252C%2520the%2520model%2520is%2520capable%2520of%2520processing%2520multimodal%2520sequences%2520up%2520to%250A1M%2520tokens%252C%2520highlighting%2520its%2520potential%2520for%2520real-world%2520long-context%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2PE%3A%20Improving%20Multimodal%20Long-Context%20Capability%20of%20Vision-Language%0A%20%20Models%20with%20Variable%20Visual%20Position%20Encoding&entry.906535625=Junqi%20Ge%20and%20Ziyi%20Chen%20and%20Jintao%20Lin%20and%20Jinguo%20Zhu%20and%20Xihui%20Liu%20and%20Jifeng%20Dai%20and%20Xizhou%20Zhu&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20promising%20capabilities%20in%20handling%0Avarious%20multimodal%20tasks%2C%20yet%20they%20struggle%20in%20long-context%20scenarios%2C%0Aparticularly%20in%20tasks%20involving%20videos%2C%20high-resolution%20images%2C%20or%20lengthy%0Aimage-text%20documents.%20In%20our%20work%2C%20we%20first%20conduct%20an%20empirical%20analysis%20of%0Athe%20long-context%20capabilities%20of%20VLMs%20using%20our%20augmented%20long-context%0Amultimodal%20datasets.%20Our%20findings%20reveal%20that%20directly%20applying%20the%20positional%0Aencoding%20mechanism%20used%20for%20textual%20tokens%20to%20visual%20tokens%20is%20suboptimal%2C%20and%0AVLM%20performance%20degrades%20sharply%20when%20the%20position%20encoding%20exceeds%20the%20model%27s%0Acontext%20window.%20To%20address%20this%2C%20we%20propose%20Variable%20Visual%20Position%20Encoding%0A%28V2PE%29%2C%20a%20novel%20positional%20encoding%20approach%20that%20employs%20variable%20and%20smaller%0Aincrements%20for%20visual%20tokens%2C%20enabling%20more%20efficient%20management%20of%20long%0Amultimodal%20sequences.%20Our%20experiments%20demonstrate%20the%20effectiveness%20of%20V2PE%20to%0Aenhances%20VLMs%27%20ability%20to%20effectively%20understand%20and%20reason%20over%20long%0Amultimodal%20contexts.%20We%20further%20integrate%20V2PE%20with%20our%20augmented%20long-context%0Amultimodal%20datasets%20to%20fine-tune%20the%20open-source%20VLM%2C%20InternVL2.%20The%20fine-tuned%0Amodel%20achieves%20strong%20performance%20on%20both%20standard%20and%20long-context%20multimodal%0Atasks.%20Notably%2C%20when%20the%20sequence%20length%20of%20the%20training%20dataset%20is%20increased%0Ato%20256K%20tokens%2C%20the%20model%20is%20capable%20of%20processing%20multimodal%20sequences%20up%20to%0A1M%20tokens%2C%20highlighting%20its%20potential%20for%20real-world%20long-context%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09616v1&entry.124074799=Read"},
{"title": "Temporal Action Localization with Cross Layer Task Decoupling and\n  Refinement", "author": "Qiang Li and Di Liu and Jun Kong and Sen Li and Hui Xu and Jianzhong Wang", "abstract": "  Temporal action localization (TAL) involves dual tasks to classify and\nlocalize actions within untrimmed videos. However, the two tasks often have\nconflicting requirements for features. Existing methods typically employ\nseparate heads for classification and localization tasks but share the same\ninput feature, leading to suboptimal performance. To address this issue, we\npropose a novel TAL method with Cross Layer Task Decoupling and Refinement\n(CLTDR). Based on the feature pyramid of video, CLTDR strategy integrates\nsemantically strong features from higher pyramid layers and detailed\nboundary-aware boundary features from lower pyramid layers to effectively\ndisentangle the action classification and localization tasks. Moreover, the\nmultiple features from cross layers are also employed to refine and align the\ndisentangled classification and regression results. At last, a lightweight\nGated Multi-Granularity (GMG) module is proposed to comprehensively extract and\naggregate video features at instant, local, and global temporal granularities.\nBenefiting from the CLTDR and GMG modules, our method achieves state-of-the-art\nperformance on five challenging benchmarks: THUMOS14, MultiTHUMOS,\nEPIC-KITCHENS-100, ActivityNet-1.3, and HACS. Our code and pre-trained models\nare publicly available at: https://github.com/LiQiang0307/CLTDR-GMG.\n", "link": "http://arxiv.org/abs/2412.09202v1", "date": "2024-12-12", "relevancy": 3.0363, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6231}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6147}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Action%20Localization%20with%20Cross%20Layer%20Task%20Decoupling%20and%0A%20%20Refinement&body=Title%3A%20Temporal%20Action%20Localization%20with%20Cross%20Layer%20Task%20Decoupling%20and%0A%20%20Refinement%0AAuthor%3A%20Qiang%20Li%20and%20Di%20Liu%20and%20Jun%20Kong%20and%20Sen%20Li%20and%20Hui%20Xu%20and%20Jianzhong%20Wang%0AAbstract%3A%20%20%20Temporal%20action%20localization%20%28TAL%29%20involves%20dual%20tasks%20to%20classify%20and%0Alocalize%20actions%20within%20untrimmed%20videos.%20However%2C%20the%20two%20tasks%20often%20have%0Aconflicting%20requirements%20for%20features.%20Existing%20methods%20typically%20employ%0Aseparate%20heads%20for%20classification%20and%20localization%20tasks%20but%20share%20the%20same%0Ainput%20feature%2C%20leading%20to%20suboptimal%20performance.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20TAL%20method%20with%20Cross%20Layer%20Task%20Decoupling%20and%20Refinement%0A%28CLTDR%29.%20Based%20on%20the%20feature%20pyramid%20of%20video%2C%20CLTDR%20strategy%20integrates%0Asemantically%20strong%20features%20from%20higher%20pyramid%20layers%20and%20detailed%0Aboundary-aware%20boundary%20features%20from%20lower%20pyramid%20layers%20to%20effectively%0Adisentangle%20the%20action%20classification%20and%20localization%20tasks.%20Moreover%2C%20the%0Amultiple%20features%20from%20cross%20layers%20are%20also%20employed%20to%20refine%20and%20align%20the%0Adisentangled%20classification%20and%20regression%20results.%20At%20last%2C%20a%20lightweight%0AGated%20Multi-Granularity%20%28GMG%29%20module%20is%20proposed%20to%20comprehensively%20extract%20and%0Aaggregate%20video%20features%20at%20instant%2C%20local%2C%20and%20global%20temporal%20granularities.%0ABenefiting%20from%20the%20CLTDR%20and%20GMG%20modules%2C%20our%20method%20achieves%20state-of-the-art%0Aperformance%20on%20five%20challenging%20benchmarks%3A%20THUMOS14%2C%20MultiTHUMOS%2C%0AEPIC-KITCHENS-100%2C%20ActivityNet-1.3%2C%20and%20HACS.%20Our%20code%20and%20pre-trained%20models%0Aare%20publicly%20available%20at%3A%20https%3A//github.com/LiQiang0307/CLTDR-GMG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Action%2520Localization%2520with%2520Cross%2520Layer%2520Task%2520Decoupling%2520and%250A%2520%2520Refinement%26entry.906535625%3DQiang%2520Li%2520and%2520Di%2520Liu%2520and%2520Jun%2520Kong%2520and%2520Sen%2520Li%2520and%2520Hui%2520Xu%2520and%2520Jianzhong%2520Wang%26entry.1292438233%3D%2520%2520Temporal%2520action%2520localization%2520%2528TAL%2529%2520involves%2520dual%2520tasks%2520to%2520classify%2520and%250Alocalize%2520actions%2520within%2520untrimmed%2520videos.%2520However%252C%2520the%2520two%2520tasks%2520often%2520have%250Aconflicting%2520requirements%2520for%2520features.%2520Existing%2520methods%2520typically%2520employ%250Aseparate%2520heads%2520for%2520classification%2520and%2520localization%2520tasks%2520but%2520share%2520the%2520same%250Ainput%2520feature%252C%2520leading%2520to%2520suboptimal%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520novel%2520TAL%2520method%2520with%2520Cross%2520Layer%2520Task%2520Decoupling%2520and%2520Refinement%250A%2528CLTDR%2529.%2520Based%2520on%2520the%2520feature%2520pyramid%2520of%2520video%252C%2520CLTDR%2520strategy%2520integrates%250Asemantically%2520strong%2520features%2520from%2520higher%2520pyramid%2520layers%2520and%2520detailed%250Aboundary-aware%2520boundary%2520features%2520from%2520lower%2520pyramid%2520layers%2520to%2520effectively%250Adisentangle%2520the%2520action%2520classification%2520and%2520localization%2520tasks.%2520Moreover%252C%2520the%250Amultiple%2520features%2520from%2520cross%2520layers%2520are%2520also%2520employed%2520to%2520refine%2520and%2520align%2520the%250Adisentangled%2520classification%2520and%2520regression%2520results.%2520At%2520last%252C%2520a%2520lightweight%250AGated%2520Multi-Granularity%2520%2528GMG%2529%2520module%2520is%2520proposed%2520to%2520comprehensively%2520extract%2520and%250Aaggregate%2520video%2520features%2520at%2520instant%252C%2520local%252C%2520and%2520global%2520temporal%2520granularities.%250ABenefiting%2520from%2520the%2520CLTDR%2520and%2520GMG%2520modules%252C%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520five%2520challenging%2520benchmarks%253A%2520THUMOS14%252C%2520MultiTHUMOS%252C%250AEPIC-KITCHENS-100%252C%2520ActivityNet-1.3%252C%2520and%2520HACS.%2520Our%2520code%2520and%2520pre-trained%2520models%250Aare%2520publicly%2520available%2520at%253A%2520https%253A//github.com/LiQiang0307/CLTDR-GMG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Action%20Localization%20with%20Cross%20Layer%20Task%20Decoupling%20and%0A%20%20Refinement&entry.906535625=Qiang%20Li%20and%20Di%20Liu%20and%20Jun%20Kong%20and%20Sen%20Li%20and%20Hui%20Xu%20and%20Jianzhong%20Wang&entry.1292438233=%20%20Temporal%20action%20localization%20%28TAL%29%20involves%20dual%20tasks%20to%20classify%20and%0Alocalize%20actions%20within%20untrimmed%20videos.%20However%2C%20the%20two%20tasks%20often%20have%0Aconflicting%20requirements%20for%20features.%20Existing%20methods%20typically%20employ%0Aseparate%20heads%20for%20classification%20and%20localization%20tasks%20but%20share%20the%20same%0Ainput%20feature%2C%20leading%20to%20suboptimal%20performance.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20novel%20TAL%20method%20with%20Cross%20Layer%20Task%20Decoupling%20and%20Refinement%0A%28CLTDR%29.%20Based%20on%20the%20feature%20pyramid%20of%20video%2C%20CLTDR%20strategy%20integrates%0Asemantically%20strong%20features%20from%20higher%20pyramid%20layers%20and%20detailed%0Aboundary-aware%20boundary%20features%20from%20lower%20pyramid%20layers%20to%20effectively%0Adisentangle%20the%20action%20classification%20and%20localization%20tasks.%20Moreover%2C%20the%0Amultiple%20features%20from%20cross%20layers%20are%20also%20employed%20to%20refine%20and%20align%20the%0Adisentangled%20classification%20and%20regression%20results.%20At%20last%2C%20a%20lightweight%0AGated%20Multi-Granularity%20%28GMG%29%20module%20is%20proposed%20to%20comprehensively%20extract%20and%0Aaggregate%20video%20features%20at%20instant%2C%20local%2C%20and%20global%20temporal%20granularities.%0ABenefiting%20from%20the%20CLTDR%20and%20GMG%20modules%2C%20our%20method%20achieves%20state-of-the-art%0Aperformance%20on%20five%20challenging%20benchmarks%3A%20THUMOS14%2C%20MultiTHUMOS%2C%0AEPIC-KITCHENS-100%2C%20ActivityNet-1.3%2C%20and%20HACS.%20Our%20code%20and%20pre-trained%20models%0Aare%20publicly%20available%20at%3A%20https%3A//github.com/LiQiang0307/CLTDR-GMG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09202v1&entry.124074799=Read"},
{"title": "Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM", "author": "Han Wang and Yuxiang Nie and Yongjie Ye and Deng GuanYu and Yanjie Wang and Shuai Li and Haiyang Yu and Jinghui Lu and Can Huang", "abstract": "  The application of Large Vision-Language Models (LVLMs) for analyzing images\nand videos is an exciting and rapidly evolving field. In recent years, we've\nseen significant growth in high-quality image-text datasets for fine-tuning\nimage understanding, but there is still a lack of comparable datasets for\nvideos. Additionally, many VideoLLMs are extensions of single-image VLMs, which\nmay not efficiently handle the complexities of longer videos. In this study, we\nintroduce a large-scale synthetic dataset created from proprietary models,\nusing carefully designed prompts to tackle a wide range of questions. We also\nexplore a dynamic visual token compression architecture that strikes a balance\nbetween computational efficiency and performance. Our proposed \\model{}\nachieves state-of-the-art results across various video tasks and shows\nimpressive generalization, setting new baselines in multi-image understanding.\nNotably, \\model{} delivers an absolute improvement of 2.7\\% over\nLLaVA-OneVision on VideoMME and 10.7\\% on MuirBench. Codes are available at\nhttps://github.com/Hon-Wong/ByteVideoLLM\n", "link": "http://arxiv.org/abs/2412.09530v1", "date": "2024-12-12", "relevancy": 3.0063, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic-VLM%3A%20Simple%20Dynamic%20Visual%20Token%20Compression%20for%20VideoLLM&body=Title%3A%20Dynamic-VLM%3A%20Simple%20Dynamic%20Visual%20Token%20Compression%20for%20VideoLLM%0AAuthor%3A%20Han%20Wang%20and%20Yuxiang%20Nie%20and%20Yongjie%20Ye%20and%20Deng%20GuanYu%20and%20Yanjie%20Wang%20and%20Shuai%20Li%20and%20Haiyang%20Yu%20and%20Jinghui%20Lu%20and%20Can%20Huang%0AAbstract%3A%20%20%20The%20application%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20for%20analyzing%20images%0Aand%20videos%20is%20an%20exciting%20and%20rapidly%20evolving%20field.%20In%20recent%20years%2C%20we%27ve%0Aseen%20significant%20growth%20in%20high-quality%20image-text%20datasets%20for%20fine-tuning%0Aimage%20understanding%2C%20but%20there%20is%20still%20a%20lack%20of%20comparable%20datasets%20for%0Avideos.%20Additionally%2C%20many%20VideoLLMs%20are%20extensions%20of%20single-image%20VLMs%2C%20which%0Amay%20not%20efficiently%20handle%20the%20complexities%20of%20longer%20videos.%20In%20this%20study%2C%20we%0Aintroduce%20a%20large-scale%20synthetic%20dataset%20created%20from%20proprietary%20models%2C%0Ausing%20carefully%20designed%20prompts%20to%20tackle%20a%20wide%20range%20of%20questions.%20We%20also%0Aexplore%20a%20dynamic%20visual%20token%20compression%20architecture%20that%20strikes%20a%20balance%0Abetween%20computational%20efficiency%20and%20performance.%20Our%20proposed%20%5Cmodel%7B%7D%0Aachieves%20state-of-the-art%20results%20across%20various%20video%20tasks%20and%20shows%0Aimpressive%20generalization%2C%20setting%20new%20baselines%20in%20multi-image%20understanding.%0ANotably%2C%20%5Cmodel%7B%7D%20delivers%20an%20absolute%20improvement%20of%202.7%5C%25%20over%0ALLaVA-OneVision%20on%20VideoMME%20and%2010.7%5C%25%20on%20MuirBench.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Hon-Wong/ByteVideoLLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic-VLM%253A%2520Simple%2520Dynamic%2520Visual%2520Token%2520Compression%2520for%2520VideoLLM%26entry.906535625%3DHan%2520Wang%2520and%2520Yuxiang%2520Nie%2520and%2520Yongjie%2520Ye%2520and%2520Deng%2520GuanYu%2520and%2520Yanjie%2520Wang%2520and%2520Shuai%2520Li%2520and%2520Haiyang%2520Yu%2520and%2520Jinghui%2520Lu%2520and%2520Can%2520Huang%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520for%2520analyzing%2520images%250Aand%2520videos%2520is%2520an%2520exciting%2520and%2520rapidly%2520evolving%2520field.%2520In%2520recent%2520years%252C%2520we%2527ve%250Aseen%2520significant%2520growth%2520in%2520high-quality%2520image-text%2520datasets%2520for%2520fine-tuning%250Aimage%2520understanding%252C%2520but%2520there%2520is%2520still%2520a%2520lack%2520of%2520comparable%2520datasets%2520for%250Avideos.%2520Additionally%252C%2520many%2520VideoLLMs%2520are%2520extensions%2520of%2520single-image%2520VLMs%252C%2520which%250Amay%2520not%2520efficiently%2520handle%2520the%2520complexities%2520of%2520longer%2520videos.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520a%2520large-scale%2520synthetic%2520dataset%2520created%2520from%2520proprietary%2520models%252C%250Ausing%2520carefully%2520designed%2520prompts%2520to%2520tackle%2520a%2520wide%2520range%2520of%2520questions.%2520We%2520also%250Aexplore%2520a%2520dynamic%2520visual%2520token%2520compression%2520architecture%2520that%2520strikes%2520a%2520balance%250Abetween%2520computational%2520efficiency%2520and%2520performance.%2520Our%2520proposed%2520%255Cmodel%257B%257D%250Aachieves%2520state-of-the-art%2520results%2520across%2520various%2520video%2520tasks%2520and%2520shows%250Aimpressive%2520generalization%252C%2520setting%2520new%2520baselines%2520in%2520multi-image%2520understanding.%250ANotably%252C%2520%255Cmodel%257B%257D%2520delivers%2520an%2520absolute%2520improvement%2520of%25202.7%255C%2525%2520over%250ALLaVA-OneVision%2520on%2520VideoMME%2520and%252010.7%255C%2525%2520on%2520MuirBench.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/Hon-Wong/ByteVideoLLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic-VLM%3A%20Simple%20Dynamic%20Visual%20Token%20Compression%20for%20VideoLLM&entry.906535625=Han%20Wang%20and%20Yuxiang%20Nie%20and%20Yongjie%20Ye%20and%20Deng%20GuanYu%20and%20Yanjie%20Wang%20and%20Shuai%20Li%20and%20Haiyang%20Yu%20and%20Jinghui%20Lu%20and%20Can%20Huang&entry.1292438233=%20%20The%20application%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20for%20analyzing%20images%0Aand%20videos%20is%20an%20exciting%20and%20rapidly%20evolving%20field.%20In%20recent%20years%2C%20we%27ve%0Aseen%20significant%20growth%20in%20high-quality%20image-text%20datasets%20for%20fine-tuning%0Aimage%20understanding%2C%20but%20there%20is%20still%20a%20lack%20of%20comparable%20datasets%20for%0Avideos.%20Additionally%2C%20many%20VideoLLMs%20are%20extensions%20of%20single-image%20VLMs%2C%20which%0Amay%20not%20efficiently%20handle%20the%20complexities%20of%20longer%20videos.%20In%20this%20study%2C%20we%0Aintroduce%20a%20large-scale%20synthetic%20dataset%20created%20from%20proprietary%20models%2C%0Ausing%20carefully%20designed%20prompts%20to%20tackle%20a%20wide%20range%20of%20questions.%20We%20also%0Aexplore%20a%20dynamic%20visual%20token%20compression%20architecture%20that%20strikes%20a%20balance%0Abetween%20computational%20efficiency%20and%20performance.%20Our%20proposed%20%5Cmodel%7B%7D%0Aachieves%20state-of-the-art%20results%20across%20various%20video%20tasks%20and%20shows%0Aimpressive%20generalization%2C%20setting%20new%20baselines%20in%20multi-image%20understanding.%0ANotably%2C%20%5Cmodel%7B%7D%20delivers%20an%20absolute%20improvement%20of%202.7%5C%25%20over%0ALLaVA-OneVision%20on%20VideoMME%20and%2010.7%5C%25%20on%20MuirBench.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Hon-Wong/ByteVideoLLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09530v1&entry.124074799=Read"},
{"title": "A Plug-and-Play Algorithm for 3D Video Super-Resolution of Single-Photon\n  LiDAR data", "author": "Alice Ruget and Lewis Wilson and Jonathan Leach and Rachael Tobin and Aongus Mccarthy and Gerald S. Buller and Steve Mclaughlin and Abderrahim Halimi", "abstract": "  Single-photon avalanche diodes (SPADs) are advanced sensors capable of\ndetecting individual photons and recording their arrival times with picosecond\nresolution using time-correlated Single-Photon Counting detection techniques.\nThey are used in various applications, such as LiDAR, and can capture\nhigh-speed sequences of binary single-photon images, offering great potential\nfor reconstructing 3D environments with high motion dynamics. To complement\nsingle-photon data, they are often paired with conventional passive cameras,\nwhich capture high-resolution (HR) intensity images at a lower frame rate.\nHowever, 3D reconstruction from SPAD data faces challenges. Aggregating\nmultiple binary measurements improves precision and reduces noise but can cause\nmotion blur in dynamic scenes. Additionally, SPAD arrays often have lower\nresolution than passive cameras. To address these issues, we propose a novel\ncomputational imaging algorithm to improve the 3D reconstruction of moving\nscenes from SPAD data by addressing the motion blur and increasing the native\nspatial resolution. We adopt a plug-and-play approach within an optimization\nscheme alternating between guided video super-resolution of the 3D scene, and\nprecise image realignment using optical flow. Experiments on synthetic data\nshow significantly improved image resolutions across various signal-to-noise\nratios and photon levels. We validate our method using real-world SPAD\nmeasurements on three practical situations with dynamic objects. First on\nfast-moving scenes in laboratory conditions at short range; second very low\nresolution imaging of people with a consumer-grade SPAD sensor from\nSTMicroelectronics; and finally, HR imaging of people walking outdoors in\ndaylight at a range of 325 meters under eye-safe illumination conditions using\na short-wave infrared SPAD camera. These results demonstrate the robustness and\nversatility of our approach.\n", "link": "http://arxiv.org/abs/2412.09427v1", "date": "2024-12-12", "relevancy": 2.9979, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.616}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5914}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Plug-and-Play%20Algorithm%20for%203D%20Video%20Super-Resolution%20of%20Single-Photon%0A%20%20LiDAR%20data&body=Title%3A%20A%20Plug-and-Play%20Algorithm%20for%203D%20Video%20Super-Resolution%20of%20Single-Photon%0A%20%20LiDAR%20data%0AAuthor%3A%20Alice%20Ruget%20and%20Lewis%20Wilson%20and%20Jonathan%20Leach%20and%20Rachael%20Tobin%20and%20Aongus%20Mccarthy%20and%20Gerald%20S.%20Buller%20and%20Steve%20Mclaughlin%20and%20Abderrahim%20Halimi%0AAbstract%3A%20%20%20Single-photon%20avalanche%20diodes%20%28SPADs%29%20are%20advanced%20sensors%20capable%20of%0Adetecting%20individual%20photons%20and%20recording%20their%20arrival%20times%20with%20picosecond%0Aresolution%20using%20time-correlated%20Single-Photon%20Counting%20detection%20techniques.%0AThey%20are%20used%20in%20various%20applications%2C%20such%20as%20LiDAR%2C%20and%20can%20capture%0Ahigh-speed%20sequences%20of%20binary%20single-photon%20images%2C%20offering%20great%20potential%0Afor%20reconstructing%203D%20environments%20with%20high%20motion%20dynamics.%20To%20complement%0Asingle-photon%20data%2C%20they%20are%20often%20paired%20with%20conventional%20passive%20cameras%2C%0Awhich%20capture%20high-resolution%20%28HR%29%20intensity%20images%20at%20a%20lower%20frame%20rate.%0AHowever%2C%203D%20reconstruction%20from%20SPAD%20data%20faces%20challenges.%20Aggregating%0Amultiple%20binary%20measurements%20improves%20precision%20and%20reduces%20noise%20but%20can%20cause%0Amotion%20blur%20in%20dynamic%20scenes.%20Additionally%2C%20SPAD%20arrays%20often%20have%20lower%0Aresolution%20than%20passive%20cameras.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%0Acomputational%20imaging%20algorithm%20to%20improve%20the%203D%20reconstruction%20of%20moving%0Ascenes%20from%20SPAD%20data%20by%20addressing%20the%20motion%20blur%20and%20increasing%20the%20native%0Aspatial%20resolution.%20We%20adopt%20a%20plug-and-play%20approach%20within%20an%20optimization%0Ascheme%20alternating%20between%20guided%20video%20super-resolution%20of%20the%203D%20scene%2C%20and%0Aprecise%20image%20realignment%20using%20optical%20flow.%20Experiments%20on%20synthetic%20data%0Ashow%20significantly%20improved%20image%20resolutions%20across%20various%20signal-to-noise%0Aratios%20and%20photon%20levels.%20We%20validate%20our%20method%20using%20real-world%20SPAD%0Ameasurements%20on%20three%20practical%20situations%20with%20dynamic%20objects.%20First%20on%0Afast-moving%20scenes%20in%20laboratory%20conditions%20at%20short%20range%3B%20second%20very%20low%0Aresolution%20imaging%20of%20people%20with%20a%20consumer-grade%20SPAD%20sensor%20from%0ASTMicroelectronics%3B%20and%20finally%2C%20HR%20imaging%20of%20people%20walking%20outdoors%20in%0Adaylight%20at%20a%20range%20of%20325%20meters%20under%20eye-safe%20illumination%20conditions%20using%0Aa%20short-wave%20infrared%20SPAD%20camera.%20These%20results%20demonstrate%20the%20robustness%20and%0Aversatility%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Plug-and-Play%2520Algorithm%2520for%25203D%2520Video%2520Super-Resolution%2520of%2520Single-Photon%250A%2520%2520LiDAR%2520data%26entry.906535625%3DAlice%2520Ruget%2520and%2520Lewis%2520Wilson%2520and%2520Jonathan%2520Leach%2520and%2520Rachael%2520Tobin%2520and%2520Aongus%2520Mccarthy%2520and%2520Gerald%2520S.%2520Buller%2520and%2520Steve%2520Mclaughlin%2520and%2520Abderrahim%2520Halimi%26entry.1292438233%3D%2520%2520Single-photon%2520avalanche%2520diodes%2520%2528SPADs%2529%2520are%2520advanced%2520sensors%2520capable%2520of%250Adetecting%2520individual%2520photons%2520and%2520recording%2520their%2520arrival%2520times%2520with%2520picosecond%250Aresolution%2520using%2520time-correlated%2520Single-Photon%2520Counting%2520detection%2520techniques.%250AThey%2520are%2520used%2520in%2520various%2520applications%252C%2520such%2520as%2520LiDAR%252C%2520and%2520can%2520capture%250Ahigh-speed%2520sequences%2520of%2520binary%2520single-photon%2520images%252C%2520offering%2520great%2520potential%250Afor%2520reconstructing%25203D%2520environments%2520with%2520high%2520motion%2520dynamics.%2520To%2520complement%250Asingle-photon%2520data%252C%2520they%2520are%2520often%2520paired%2520with%2520conventional%2520passive%2520cameras%252C%250Awhich%2520capture%2520high-resolution%2520%2528HR%2529%2520intensity%2520images%2520at%2520a%2520lower%2520frame%2520rate.%250AHowever%252C%25203D%2520reconstruction%2520from%2520SPAD%2520data%2520faces%2520challenges.%2520Aggregating%250Amultiple%2520binary%2520measurements%2520improves%2520precision%2520and%2520reduces%2520noise%2520but%2520can%2520cause%250Amotion%2520blur%2520in%2520dynamic%2520scenes.%2520Additionally%252C%2520SPAD%2520arrays%2520often%2520have%2520lower%250Aresolution%2520than%2520passive%2520cameras.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%250Acomputational%2520imaging%2520algorithm%2520to%2520improve%2520the%25203D%2520reconstruction%2520of%2520moving%250Ascenes%2520from%2520SPAD%2520data%2520by%2520addressing%2520the%2520motion%2520blur%2520and%2520increasing%2520the%2520native%250Aspatial%2520resolution.%2520We%2520adopt%2520a%2520plug-and-play%2520approach%2520within%2520an%2520optimization%250Ascheme%2520alternating%2520between%2520guided%2520video%2520super-resolution%2520of%2520the%25203D%2520scene%252C%2520and%250Aprecise%2520image%2520realignment%2520using%2520optical%2520flow.%2520Experiments%2520on%2520synthetic%2520data%250Ashow%2520significantly%2520improved%2520image%2520resolutions%2520across%2520various%2520signal-to-noise%250Aratios%2520and%2520photon%2520levels.%2520We%2520validate%2520our%2520method%2520using%2520real-world%2520SPAD%250Ameasurements%2520on%2520three%2520practical%2520situations%2520with%2520dynamic%2520objects.%2520First%2520on%250Afast-moving%2520scenes%2520in%2520laboratory%2520conditions%2520at%2520short%2520range%253B%2520second%2520very%2520low%250Aresolution%2520imaging%2520of%2520people%2520with%2520a%2520consumer-grade%2520SPAD%2520sensor%2520from%250ASTMicroelectronics%253B%2520and%2520finally%252C%2520HR%2520imaging%2520of%2520people%2520walking%2520outdoors%2520in%250Adaylight%2520at%2520a%2520range%2520of%2520325%2520meters%2520under%2520eye-safe%2520illumination%2520conditions%2520using%250Aa%2520short-wave%2520infrared%2520SPAD%2520camera.%2520These%2520results%2520demonstrate%2520the%2520robustness%2520and%250Aversatility%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Plug-and-Play%20Algorithm%20for%203D%20Video%20Super-Resolution%20of%20Single-Photon%0A%20%20LiDAR%20data&entry.906535625=Alice%20Ruget%20and%20Lewis%20Wilson%20and%20Jonathan%20Leach%20and%20Rachael%20Tobin%20and%20Aongus%20Mccarthy%20and%20Gerald%20S.%20Buller%20and%20Steve%20Mclaughlin%20and%20Abderrahim%20Halimi&entry.1292438233=%20%20Single-photon%20avalanche%20diodes%20%28SPADs%29%20are%20advanced%20sensors%20capable%20of%0Adetecting%20individual%20photons%20and%20recording%20their%20arrival%20times%20with%20picosecond%0Aresolution%20using%20time-correlated%20Single-Photon%20Counting%20detection%20techniques.%0AThey%20are%20used%20in%20various%20applications%2C%20such%20as%20LiDAR%2C%20and%20can%20capture%0Ahigh-speed%20sequences%20of%20binary%20single-photon%20images%2C%20offering%20great%20potential%0Afor%20reconstructing%203D%20environments%20with%20high%20motion%20dynamics.%20To%20complement%0Asingle-photon%20data%2C%20they%20are%20often%20paired%20with%20conventional%20passive%20cameras%2C%0Awhich%20capture%20high-resolution%20%28HR%29%20intensity%20images%20at%20a%20lower%20frame%20rate.%0AHowever%2C%203D%20reconstruction%20from%20SPAD%20data%20faces%20challenges.%20Aggregating%0Amultiple%20binary%20measurements%20improves%20precision%20and%20reduces%20noise%20but%20can%20cause%0Amotion%20blur%20in%20dynamic%20scenes.%20Additionally%2C%20SPAD%20arrays%20often%20have%20lower%0Aresolution%20than%20passive%20cameras.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%0Acomputational%20imaging%20algorithm%20to%20improve%20the%203D%20reconstruction%20of%20moving%0Ascenes%20from%20SPAD%20data%20by%20addressing%20the%20motion%20blur%20and%20increasing%20the%20native%0Aspatial%20resolution.%20We%20adopt%20a%20plug-and-play%20approach%20within%20an%20optimization%0Ascheme%20alternating%20between%20guided%20video%20super-resolution%20of%20the%203D%20scene%2C%20and%0Aprecise%20image%20realignment%20using%20optical%20flow.%20Experiments%20on%20synthetic%20data%0Ashow%20significantly%20improved%20image%20resolutions%20across%20various%20signal-to-noise%0Aratios%20and%20photon%20levels.%20We%20validate%20our%20method%20using%20real-world%20SPAD%0Ameasurements%20on%20three%20practical%20situations%20with%20dynamic%20objects.%20First%20on%0Afast-moving%20scenes%20in%20laboratory%20conditions%20at%20short%20range%3B%20second%20very%20low%0Aresolution%20imaging%20of%20people%20with%20a%20consumer-grade%20SPAD%20sensor%20from%0ASTMicroelectronics%3B%20and%20finally%2C%20HR%20imaging%20of%20people%20walking%20outdoors%20in%0Adaylight%20at%20a%20range%20of%20325%20meters%20under%20eye-safe%20illumination%20conditions%20using%0Aa%20short-wave%20infrared%20SPAD%20camera.%20These%20results%20demonstrate%20the%20robustness%20and%0Aversatility%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09427v1&entry.124074799=Read"},
{"title": "Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale", "author": "Zekun Hao and David W. Romero and Tsung-Yi Lin and Ming-Yu Liu", "abstract": "  Meshes are fundamental representations of 3D surfaces. However, creating\nhigh-quality meshes is a labor-intensive task that requires significant time\nand expertise in 3D modeling. While a delicate object often requires over\n$10^4$ faces to be accurately modeled, recent attempts at generating\nartist-like meshes are limited to $1.6$K faces and heavy discretization of\nvertex coordinates. Hence, scaling both the maximum face count and vertex\ncoordinate resolution is crucial to producing high-quality meshes of realistic,\ncomplex 3D objects. We present Meshtron, a novel autoregressive mesh generation\nmodel able to generate meshes with up to 64K faces at 1024-level coordinate\nresolution --over an order of magnitude higher face count and $8{\\times}$\nhigher coordinate resolution than current state-of-the-art methods. Meshtron's\nscalability is driven by four key components: (1) an hourglass neural\narchitecture, (2) truncated sequence training, (3) sliding window inference,\n(4) a robust sampling strategy that enforces the order of mesh sequences. This\nresults in over $50{\\%}$ less training memory, $2.5{\\times}$ faster throughput,\nand better consistency than existing works. Meshtron generates meshes of\ndetailed, complex 3D objects at unprecedented levels of resolution and\nfidelity, closely resembling those created by professional artists, and opening\nthe door to more realistic generation of detailed 3D assets for animation,\ngaming, and virtual environments.\n", "link": "http://arxiv.org/abs/2412.09548v1", "date": "2024-12-12", "relevancy": 2.9542, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6446}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.564}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meshtron%3A%20High-Fidelity%2C%20Artist-Like%203D%20Mesh%20Generation%20at%20Scale&body=Title%3A%20Meshtron%3A%20High-Fidelity%2C%20Artist-Like%203D%20Mesh%20Generation%20at%20Scale%0AAuthor%3A%20Zekun%20Hao%20and%20David%20W.%20Romero%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%0AAbstract%3A%20%20%20Meshes%20are%20fundamental%20representations%20of%203D%20surfaces.%20However%2C%20creating%0Ahigh-quality%20meshes%20is%20a%20labor-intensive%20task%20that%20requires%20significant%20time%0Aand%20expertise%20in%203D%20modeling.%20While%20a%20delicate%20object%20often%20requires%20over%0A%2410%5E4%24%20faces%20to%20be%20accurately%20modeled%2C%20recent%20attempts%20at%20generating%0Aartist-like%20meshes%20are%20limited%20to%20%241.6%24K%20faces%20and%20heavy%20discretization%20of%0Avertex%20coordinates.%20Hence%2C%20scaling%20both%20the%20maximum%20face%20count%20and%20vertex%0Acoordinate%20resolution%20is%20crucial%20to%20producing%20high-quality%20meshes%20of%20realistic%2C%0Acomplex%203D%20objects.%20We%20present%20Meshtron%2C%20a%20novel%20autoregressive%20mesh%20generation%0Amodel%20able%20to%20generate%20meshes%20with%20up%20to%2064K%20faces%20at%201024-level%20coordinate%0Aresolution%20--over%20an%20order%20of%20magnitude%20higher%20face%20count%20and%20%248%7B%5Ctimes%7D%24%0Ahigher%20coordinate%20resolution%20than%20current%20state-of-the-art%20methods.%20Meshtron%27s%0Ascalability%20is%20driven%20by%20four%20key%20components%3A%20%281%29%20an%20hourglass%20neural%0Aarchitecture%2C%20%282%29%20truncated%20sequence%20training%2C%20%283%29%20sliding%20window%20inference%2C%0A%284%29%20a%20robust%20sampling%20strategy%20that%20enforces%20the%20order%20of%20mesh%20sequences.%20This%0Aresults%20in%20over%20%2450%7B%5C%25%7D%24%20less%20training%20memory%2C%20%242.5%7B%5Ctimes%7D%24%20faster%20throughput%2C%0Aand%20better%20consistency%20than%20existing%20works.%20Meshtron%20generates%20meshes%20of%0Adetailed%2C%20complex%203D%20objects%20at%20unprecedented%20levels%20of%20resolution%20and%0Afidelity%2C%20closely%20resembling%20those%20created%20by%20professional%20artists%2C%20and%20opening%0Athe%20door%20to%20more%20realistic%20generation%20of%20detailed%203D%20assets%20for%20animation%2C%0Agaming%2C%20and%20virtual%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshtron%253A%2520High-Fidelity%252C%2520Artist-Like%25203D%2520Mesh%2520Generation%2520at%2520Scale%26entry.906535625%3DZekun%2520Hao%2520and%2520David%2520W.%2520Romero%2520and%2520Tsung-Yi%2520Lin%2520and%2520Ming-Yu%2520Liu%26entry.1292438233%3D%2520%2520Meshes%2520are%2520fundamental%2520representations%2520of%25203D%2520surfaces.%2520However%252C%2520creating%250Ahigh-quality%2520meshes%2520is%2520a%2520labor-intensive%2520task%2520that%2520requires%2520significant%2520time%250Aand%2520expertise%2520in%25203D%2520modeling.%2520While%2520a%2520delicate%2520object%2520often%2520requires%2520over%250A%252410%255E4%2524%2520faces%2520to%2520be%2520accurately%2520modeled%252C%2520recent%2520attempts%2520at%2520generating%250Aartist-like%2520meshes%2520are%2520limited%2520to%2520%25241.6%2524K%2520faces%2520and%2520heavy%2520discretization%2520of%250Avertex%2520coordinates.%2520Hence%252C%2520scaling%2520both%2520the%2520maximum%2520face%2520count%2520and%2520vertex%250Acoordinate%2520resolution%2520is%2520crucial%2520to%2520producing%2520high-quality%2520meshes%2520of%2520realistic%252C%250Acomplex%25203D%2520objects.%2520We%2520present%2520Meshtron%252C%2520a%2520novel%2520autoregressive%2520mesh%2520generation%250Amodel%2520able%2520to%2520generate%2520meshes%2520with%2520up%2520to%252064K%2520faces%2520at%25201024-level%2520coordinate%250Aresolution%2520--over%2520an%2520order%2520of%2520magnitude%2520higher%2520face%2520count%2520and%2520%25248%257B%255Ctimes%257D%2524%250Ahigher%2520coordinate%2520resolution%2520than%2520current%2520state-of-the-art%2520methods.%2520Meshtron%2527s%250Ascalability%2520is%2520driven%2520by%2520four%2520key%2520components%253A%2520%25281%2529%2520an%2520hourglass%2520neural%250Aarchitecture%252C%2520%25282%2529%2520truncated%2520sequence%2520training%252C%2520%25283%2529%2520sliding%2520window%2520inference%252C%250A%25284%2529%2520a%2520robust%2520sampling%2520strategy%2520that%2520enforces%2520the%2520order%2520of%2520mesh%2520sequences.%2520This%250Aresults%2520in%2520over%2520%252450%257B%255C%2525%257D%2524%2520less%2520training%2520memory%252C%2520%25242.5%257B%255Ctimes%257D%2524%2520faster%2520throughput%252C%250Aand%2520better%2520consistency%2520than%2520existing%2520works.%2520Meshtron%2520generates%2520meshes%2520of%250Adetailed%252C%2520complex%25203D%2520objects%2520at%2520unprecedented%2520levels%2520of%2520resolution%2520and%250Afidelity%252C%2520closely%2520resembling%2520those%2520created%2520by%2520professional%2520artists%252C%2520and%2520opening%250Athe%2520door%2520to%2520more%2520realistic%2520generation%2520of%2520detailed%25203D%2520assets%2520for%2520animation%252C%250Agaming%252C%2520and%2520virtual%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meshtron%3A%20High-Fidelity%2C%20Artist-Like%203D%20Mesh%20Generation%20at%20Scale&entry.906535625=Zekun%20Hao%20and%20David%20W.%20Romero%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu&entry.1292438233=%20%20Meshes%20are%20fundamental%20representations%20of%203D%20surfaces.%20However%2C%20creating%0Ahigh-quality%20meshes%20is%20a%20labor-intensive%20task%20that%20requires%20significant%20time%0Aand%20expertise%20in%203D%20modeling.%20While%20a%20delicate%20object%20often%20requires%20over%0A%2410%5E4%24%20faces%20to%20be%20accurately%20modeled%2C%20recent%20attempts%20at%20generating%0Aartist-like%20meshes%20are%20limited%20to%20%241.6%24K%20faces%20and%20heavy%20discretization%20of%0Avertex%20coordinates.%20Hence%2C%20scaling%20both%20the%20maximum%20face%20count%20and%20vertex%0Acoordinate%20resolution%20is%20crucial%20to%20producing%20high-quality%20meshes%20of%20realistic%2C%0Acomplex%203D%20objects.%20We%20present%20Meshtron%2C%20a%20novel%20autoregressive%20mesh%20generation%0Amodel%20able%20to%20generate%20meshes%20with%20up%20to%2064K%20faces%20at%201024-level%20coordinate%0Aresolution%20--over%20an%20order%20of%20magnitude%20higher%20face%20count%20and%20%248%7B%5Ctimes%7D%24%0Ahigher%20coordinate%20resolution%20than%20current%20state-of-the-art%20methods.%20Meshtron%27s%0Ascalability%20is%20driven%20by%20four%20key%20components%3A%20%281%29%20an%20hourglass%20neural%0Aarchitecture%2C%20%282%29%20truncated%20sequence%20training%2C%20%283%29%20sliding%20window%20inference%2C%0A%284%29%20a%20robust%20sampling%20strategy%20that%20enforces%20the%20order%20of%20mesh%20sequences.%20This%0Aresults%20in%20over%20%2450%7B%5C%25%7D%24%20less%20training%20memory%2C%20%242.5%7B%5Ctimes%7D%24%20faster%20throughput%2C%0Aand%20better%20consistency%20than%20existing%20works.%20Meshtron%20generates%20meshes%20of%0Adetailed%2C%20complex%203D%20objects%20at%20unprecedented%20levels%20of%20resolution%20and%0Afidelity%2C%20closely%20resembling%20those%20created%20by%20professional%20artists%2C%20and%20opening%0Athe%20door%20to%20more%20realistic%20generation%20of%20detailed%203D%20assets%20for%20animation%2C%0Agaming%2C%20and%20virtual%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09548v1&entry.124074799=Read"},
{"title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with\n  Vision Experts and Token Folding", "author": "Hao Li and Changyao Tian and Jie Shao and Xizhou Zhu and Zhaokai Wang and Jinguo Zhu and Wenhan Dou and Xiaogang Wang and Hongsheng Li and Lewei Lu and Jifeng Dai", "abstract": "  The remarkable success of Large Language Models (LLMs) has extended to the\nmultimodal domain, achieving outstanding performance in image understanding and\ngeneration. Recent efforts to develop unified Multimodal Large Language Models\n(MLLMs) that integrate these capabilities have shown promising results.\nHowever, existing approaches often involve complex designs in model\narchitecture or training pipeline, increasing the difficulty of model training\nand scaling. In this paper, we propose SynerGen-VL, a simple yet powerful\nencoder-free MLLM capable of both image understanding and generation. To\naddress challenges identified in existing encoder-free unified MLLMs, we\nintroduce the token folding mechanism and the vision-expert-based progressive\nalignment pretraining strategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After being trained on\nlarge-scale mixed image-text data with a unified next-token prediction\nobjective, SynerGen-VL achieves or surpasses the performance of existing\nencoder-free unified MLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art models, highlighting a\npromising path toward future unified MLLMs. Our code and models shall be\nreleased.\n", "link": "http://arxiv.org/abs/2412.09604v1", "date": "2024-12-12", "relevancy": 2.949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6027}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynerGen-VL%3A%20Towards%20Synergistic%20Image%20Understanding%20and%20Generation%20with%0A%20%20Vision%20Experts%20and%20Token%20Folding&body=Title%3A%20SynerGen-VL%3A%20Towards%20Synergistic%20Image%20Understanding%20and%20Generation%20with%0A%20%20Vision%20Experts%20and%20Token%20Folding%0AAuthor%3A%20Hao%20Li%20and%20Changyao%20Tian%20and%20Jie%20Shao%20and%20Xizhou%20Zhu%20and%20Zhaokai%20Wang%20and%20Jinguo%20Zhu%20and%20Wenhan%20Dou%20and%20Xiaogang%20Wang%20and%20Hongsheng%20Li%20and%20Lewei%20Lu%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20The%20remarkable%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20extended%20to%20the%0Amultimodal%20domain%2C%20achieving%20outstanding%20performance%20in%20image%20understanding%20and%0Ageneration.%20Recent%20efforts%20to%20develop%20unified%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20that%20integrate%20these%20capabilities%20have%20shown%20promising%20results.%0AHowever%2C%20existing%20approaches%20often%20involve%20complex%20designs%20in%20model%0Aarchitecture%20or%20training%20pipeline%2C%20increasing%20the%20difficulty%20of%20model%20training%0Aand%20scaling.%20In%20this%20paper%2C%20we%20propose%20SynerGen-VL%2C%20a%20simple%20yet%20powerful%0Aencoder-free%20MLLM%20capable%20of%20both%20image%20understanding%20and%20generation.%20To%0Aaddress%20challenges%20identified%20in%20existing%20encoder-free%20unified%20MLLMs%2C%20we%0Aintroduce%20the%20token%20folding%20mechanism%20and%20the%20vision-expert-based%20progressive%0Aalignment%20pretraining%20strategy%2C%20which%20effectively%20support%20high-resolution%20image%0Aunderstanding%20while%20reducing%20training%20complexity.%20After%20being%20trained%20on%0Alarge-scale%20mixed%20image-text%20data%20with%20a%20unified%20next-token%20prediction%0Aobjective%2C%20SynerGen-VL%20achieves%20or%20surpasses%20the%20performance%20of%20existing%0Aencoder-free%20unified%20MLLMs%20with%20comparable%20or%20smaller%20parameter%20sizes%2C%20and%0Anarrows%20the%20gap%20with%20task-specific%20state-of-the-art%20models%2C%20highlighting%20a%0Apromising%20path%20toward%20future%20unified%20MLLMs.%20Our%20code%20and%20models%20shall%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynerGen-VL%253A%2520Towards%2520Synergistic%2520Image%2520Understanding%2520and%2520Generation%2520with%250A%2520%2520Vision%2520Experts%2520and%2520Token%2520Folding%26entry.906535625%3DHao%2520Li%2520and%2520Changyao%2520Tian%2520and%2520Jie%2520Shao%2520and%2520Xizhou%2520Zhu%2520and%2520Zhaokai%2520Wang%2520and%2520Jinguo%2520Zhu%2520and%2520Wenhan%2520Dou%2520and%2520Xiaogang%2520Wang%2520and%2520Hongsheng%2520Li%2520and%2520Lewei%2520Lu%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520The%2520remarkable%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520extended%2520to%2520the%250Amultimodal%2520domain%252C%2520achieving%2520outstanding%2520performance%2520in%2520image%2520understanding%2520and%250Ageneration.%2520Recent%2520efforts%2520to%2520develop%2520unified%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520that%2520integrate%2520these%2520capabilities%2520have%2520shown%2520promising%2520results.%250AHowever%252C%2520existing%2520approaches%2520often%2520involve%2520complex%2520designs%2520in%2520model%250Aarchitecture%2520or%2520training%2520pipeline%252C%2520increasing%2520the%2520difficulty%2520of%2520model%2520training%250Aand%2520scaling.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SynerGen-VL%252C%2520a%2520simple%2520yet%2520powerful%250Aencoder-free%2520MLLM%2520capable%2520of%2520both%2520image%2520understanding%2520and%2520generation.%2520To%250Aaddress%2520challenges%2520identified%2520in%2520existing%2520encoder-free%2520unified%2520MLLMs%252C%2520we%250Aintroduce%2520the%2520token%2520folding%2520mechanism%2520and%2520the%2520vision-expert-based%2520progressive%250Aalignment%2520pretraining%2520strategy%252C%2520which%2520effectively%2520support%2520high-resolution%2520image%250Aunderstanding%2520while%2520reducing%2520training%2520complexity.%2520After%2520being%2520trained%2520on%250Alarge-scale%2520mixed%2520image-text%2520data%2520with%2520a%2520unified%2520next-token%2520prediction%250Aobjective%252C%2520SynerGen-VL%2520achieves%2520or%2520surpasses%2520the%2520performance%2520of%2520existing%250Aencoder-free%2520unified%2520MLLMs%2520with%2520comparable%2520or%2520smaller%2520parameter%2520sizes%252C%2520and%250Anarrows%2520the%2520gap%2520with%2520task-specific%2520state-of-the-art%2520models%252C%2520highlighting%2520a%250Apromising%2520path%2520toward%2520future%2520unified%2520MLLMs.%2520Our%2520code%2520and%2520models%2520shall%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynerGen-VL%3A%20Towards%20Synergistic%20Image%20Understanding%20and%20Generation%20with%0A%20%20Vision%20Experts%20and%20Token%20Folding&entry.906535625=Hao%20Li%20and%20Changyao%20Tian%20and%20Jie%20Shao%20and%20Xizhou%20Zhu%20and%20Zhaokai%20Wang%20and%20Jinguo%20Zhu%20and%20Wenhan%20Dou%20and%20Xiaogang%20Wang%20and%20Hongsheng%20Li%20and%20Lewei%20Lu%20and%20Jifeng%20Dai&entry.1292438233=%20%20The%20remarkable%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20extended%20to%20the%0Amultimodal%20domain%2C%20achieving%20outstanding%20performance%20in%20image%20understanding%20and%0Ageneration.%20Recent%20efforts%20to%20develop%20unified%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20that%20integrate%20these%20capabilities%20have%20shown%20promising%20results.%0AHowever%2C%20existing%20approaches%20often%20involve%20complex%20designs%20in%20model%0Aarchitecture%20or%20training%20pipeline%2C%20increasing%20the%20difficulty%20of%20model%20training%0Aand%20scaling.%20In%20this%20paper%2C%20we%20propose%20SynerGen-VL%2C%20a%20simple%20yet%20powerful%0Aencoder-free%20MLLM%20capable%20of%20both%20image%20understanding%20and%20generation.%20To%0Aaddress%20challenges%20identified%20in%20existing%20encoder-free%20unified%20MLLMs%2C%20we%0Aintroduce%20the%20token%20folding%20mechanism%20and%20the%20vision-expert-based%20progressive%0Aalignment%20pretraining%20strategy%2C%20which%20effectively%20support%20high-resolution%20image%0Aunderstanding%20while%20reducing%20training%20complexity.%20After%20being%20trained%20on%0Alarge-scale%20mixed%20image-text%20data%20with%20a%20unified%20next-token%20prediction%0Aobjective%2C%20SynerGen-VL%20achieves%20or%20surpasses%20the%20performance%20of%20existing%0Aencoder-free%20unified%20MLLMs%20with%20comparable%20or%20smaller%20parameter%20sizes%2C%20and%0Anarrows%20the%20gap%20with%20task-specific%20state-of-the-art%20models%2C%20highlighting%20a%0Apromising%20path%20toward%20future%20unified%20MLLMs.%20Our%20code%20and%20models%20shall%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09604v1&entry.124074799=Read"},
{"title": "Doe-1: Closed-Loop Autonomous Driving with Large World Model", "author": "Wenzhao Zheng and Zetian Xia and Yuanhui Huang and Sicheng Zuo and Jie Zhou and Jiwen Lu", "abstract": "  End-to-end autonomous driving has received increasing attention due to its\npotential to learn from large amounts of data. However, most existing methods\nare still open-loop and suffer from weak scalability, lack of high-order\ninteractions, and inefficient decision-making. In this paper, we explore a\nclosed-loop framework for autonomous driving and propose a large Driving wOrld\nmodEl (Doe-1) for unified perception, prediction, and planning. We formulate\nautonomous driving as a next-token generation problem and use multi-modal\ntokens to accomplish different tasks. Specifically, we use free-form texts\n(i.e., scene descriptions) for perception and generate future predictions\ndirectly in the RGB space with image tokens. For planning, we employ a\nposition-aware tokenizer to effectively encode action into discrete tokens. We\ntrain a multi-modal transformer to autoregressively generate perception,\nprediction, and planning tokens in an end-to-end and unified manner.\nExperiments on the widely used nuScenes dataset demonstrate the effectiveness\nof Doe-1 in various tasks including visual question-answering,\naction-conditioned video generation, and motion planning. Code:\nhttps://github.com/wzzheng/Doe.\n", "link": "http://arxiv.org/abs/2412.09627v1", "date": "2024-12-12", "relevancy": 2.9337, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doe-1%3A%20Closed-Loop%20Autonomous%20Driving%20with%20Large%20World%20Model&body=Title%3A%20Doe-1%3A%20Closed-Loop%20Autonomous%20Driving%20with%20Large%20World%20Model%0AAuthor%3A%20Wenzhao%20Zheng%20and%20Zetian%20Xia%20and%20Yuanhui%20Huang%20and%20Sicheng%20Zuo%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20has%20received%20increasing%20attention%20due%20to%20its%0Apotential%20to%20learn%20from%20large%20amounts%20of%20data.%20However%2C%20most%20existing%20methods%0Aare%20still%20open-loop%20and%20suffer%20from%20weak%20scalability%2C%20lack%20of%20high-order%0Ainteractions%2C%20and%20inefficient%20decision-making.%20In%20this%20paper%2C%20we%20explore%20a%0Aclosed-loop%20framework%20for%20autonomous%20driving%20and%20propose%20a%20large%20Driving%20wOrld%0AmodEl%20%28Doe-1%29%20for%20unified%20perception%2C%20prediction%2C%20and%20planning.%20We%20formulate%0Aautonomous%20driving%20as%20a%20next-token%20generation%20problem%20and%20use%20multi-modal%0Atokens%20to%20accomplish%20different%20tasks.%20Specifically%2C%20we%20use%20free-form%20texts%0A%28i.e.%2C%20scene%20descriptions%29%20for%20perception%20and%20generate%20future%20predictions%0Adirectly%20in%20the%20RGB%20space%20with%20image%20tokens.%20For%20planning%2C%20we%20employ%20a%0Aposition-aware%20tokenizer%20to%20effectively%20encode%20action%20into%20discrete%20tokens.%20We%0Atrain%20a%20multi-modal%20transformer%20to%20autoregressively%20generate%20perception%2C%0Aprediction%2C%20and%20planning%20tokens%20in%20an%20end-to-end%20and%20unified%20manner.%0AExperiments%20on%20the%20widely%20used%20nuScenes%20dataset%20demonstrate%20the%20effectiveness%0Aof%20Doe-1%20in%20various%20tasks%20including%20visual%20question-answering%2C%0Aaction-conditioned%20video%20generation%2C%20and%20motion%20planning.%20Code%3A%0Ahttps%3A//github.com/wzzheng/Doe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoe-1%253A%2520Closed-Loop%2520Autonomous%2520Driving%2520with%2520Large%2520World%2520Model%26entry.906535625%3DWenzhao%2520Zheng%2520and%2520Zetian%2520Xia%2520and%2520Yuanhui%2520Huang%2520and%2520Sicheng%2520Zuo%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520has%2520received%2520increasing%2520attention%2520due%2520to%2520its%250Apotential%2520to%2520learn%2520from%2520large%2520amounts%2520of%2520data.%2520However%252C%2520most%2520existing%2520methods%250Aare%2520still%2520open-loop%2520and%2520suffer%2520from%2520weak%2520scalability%252C%2520lack%2520of%2520high-order%250Ainteractions%252C%2520and%2520inefficient%2520decision-making.%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%250Aclosed-loop%2520framework%2520for%2520autonomous%2520driving%2520and%2520propose%2520a%2520large%2520Driving%2520wOrld%250AmodEl%2520%2528Doe-1%2529%2520for%2520unified%2520perception%252C%2520prediction%252C%2520and%2520planning.%2520We%2520formulate%250Aautonomous%2520driving%2520as%2520a%2520next-token%2520generation%2520problem%2520and%2520use%2520multi-modal%250Atokens%2520to%2520accomplish%2520different%2520tasks.%2520Specifically%252C%2520we%2520use%2520free-form%2520texts%250A%2528i.e.%252C%2520scene%2520descriptions%2529%2520for%2520perception%2520and%2520generate%2520future%2520predictions%250Adirectly%2520in%2520the%2520RGB%2520space%2520with%2520image%2520tokens.%2520For%2520planning%252C%2520we%2520employ%2520a%250Aposition-aware%2520tokenizer%2520to%2520effectively%2520encode%2520action%2520into%2520discrete%2520tokens.%2520We%250Atrain%2520a%2520multi-modal%2520transformer%2520to%2520autoregressively%2520generate%2520perception%252C%250Aprediction%252C%2520and%2520planning%2520tokens%2520in%2520an%2520end-to-end%2520and%2520unified%2520manner.%250AExperiments%2520on%2520the%2520widely%2520used%2520nuScenes%2520dataset%2520demonstrate%2520the%2520effectiveness%250Aof%2520Doe-1%2520in%2520various%2520tasks%2520including%2520visual%2520question-answering%252C%250Aaction-conditioned%2520video%2520generation%252C%2520and%2520motion%2520planning.%2520Code%253A%250Ahttps%253A//github.com/wzzheng/Doe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doe-1%3A%20Closed-Loop%20Autonomous%20Driving%20with%20Large%20World%20Model&entry.906535625=Wenzhao%20Zheng%20and%20Zetian%20Xia%20and%20Yuanhui%20Huang%20and%20Sicheng%20Zuo%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20has%20received%20increasing%20attention%20due%20to%20its%0Apotential%20to%20learn%20from%20large%20amounts%20of%20data.%20However%2C%20most%20existing%20methods%0Aare%20still%20open-loop%20and%20suffer%20from%20weak%20scalability%2C%20lack%20of%20high-order%0Ainteractions%2C%20and%20inefficient%20decision-making.%20In%20this%20paper%2C%20we%20explore%20a%0Aclosed-loop%20framework%20for%20autonomous%20driving%20and%20propose%20a%20large%20Driving%20wOrld%0AmodEl%20%28Doe-1%29%20for%20unified%20perception%2C%20prediction%2C%20and%20planning.%20We%20formulate%0Aautonomous%20driving%20as%20a%20next-token%20generation%20problem%20and%20use%20multi-modal%0Atokens%20to%20accomplish%20different%20tasks.%20Specifically%2C%20we%20use%20free-form%20texts%0A%28i.e.%2C%20scene%20descriptions%29%20for%20perception%20and%20generate%20future%20predictions%0Adirectly%20in%20the%20RGB%20space%20with%20image%20tokens.%20For%20planning%2C%20we%20employ%20a%0Aposition-aware%20tokenizer%20to%20effectively%20encode%20action%20into%20discrete%20tokens.%20We%0Atrain%20a%20multi-modal%20transformer%20to%20autoregressively%20generate%20perception%2C%0Aprediction%2C%20and%20planning%20tokens%20in%20an%20end-to-end%20and%20unified%20manner.%0AExperiments%20on%20the%20widely%20used%20nuScenes%20dataset%20demonstrate%20the%20effectiveness%0Aof%20Doe-1%20in%20various%20tasks%20including%20visual%20question-answering%2C%0Aaction-conditioned%20video%20generation%2C%20and%20motion%20planning.%20Code%3A%0Ahttps%3A//github.com/wzzheng/Doe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09627v1&entry.124074799=Read"},
{"title": "DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose\n  Reconstruction", "author": "Ben Kaye and Tomas Jakab and Shangzhe Wu and Christian Rupprecht and Andrea Vedaldi", "abstract": "  The choice of data representation is a key factor in the success of deep\nlearning in geometric tasks. For instance, DUSt3R has recently introduced the\nconcept of viewpoint-invariant point maps, generalizing depth prediction, and\nshowing that one can reduce all the key problems in the 3D reconstruction of\nstatic scenes to predicting such point maps. In this paper, we develop an\nanalogous concept for a very different problem, namely, the reconstruction of\nthe 3D shape and pose of deformable objects. To this end, we introduce the Dual\nPoint Maps (DualPM), where a pair of point maps is extracted from the same\nimage, one associating pixels to their 3D locations on the object, and the\nother to a canonical version of the object at rest pose. We also extend point\nmaps to amodal reconstruction, seeing through self-occlusions to obtain the\ncomplete shape of the object. We show that 3D reconstruction and 3D pose\nestimation reduce to the prediction of the DualPMs. We demonstrate empirically\nthat this representation is a good target for a deep network to predict;\nspecifically, we consider modeling horses, showing that DualPMs can be trained\npurely on 3D synthetic data, consisting of a single model of a horse, while\ngeneralizing very well to real images. With this, we improve by a large margin\nprevious methods for the 3D analysis and reconstruction of this type of\nobjects.\n", "link": "http://arxiv.org/abs/2412.04464v2", "date": "2024-12-12", "relevancy": 2.9322, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6009}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5937}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualPM%3A%20Dual%20Posed-Canonical%20Point%20Maps%20for%203D%20Shape%20and%20Pose%0A%20%20Reconstruction&body=Title%3A%20DualPM%3A%20Dual%20Posed-Canonical%20Point%20Maps%20for%203D%20Shape%20and%20Pose%0A%20%20Reconstruction%0AAuthor%3A%20Ben%20Kaye%20and%20Tomas%20Jakab%20and%20Shangzhe%20Wu%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20The%20choice%20of%20data%20representation%20is%20a%20key%20factor%20in%20the%20success%20of%20deep%0Alearning%20in%20geometric%20tasks.%20For%20instance%2C%20DUSt3R%20has%20recently%20introduced%20the%0Aconcept%20of%20viewpoint-invariant%20point%20maps%2C%20generalizing%20depth%20prediction%2C%20and%0Ashowing%20that%20one%20can%20reduce%20all%20the%20key%20problems%20in%20the%203D%20reconstruction%20of%0Astatic%20scenes%20to%20predicting%20such%20point%20maps.%20In%20this%20paper%2C%20we%20develop%20an%0Aanalogous%20concept%20for%20a%20very%20different%20problem%2C%20namely%2C%20the%20reconstruction%20of%0Athe%203D%20shape%20and%20pose%20of%20deformable%20objects.%20To%20this%20end%2C%20we%20introduce%20the%20Dual%0APoint%20Maps%20%28DualPM%29%2C%20where%20a%20pair%20of%20point%20maps%20is%20extracted%20from%20the%20same%0Aimage%2C%20one%20associating%20pixels%20to%20their%203D%20locations%20on%20the%20object%2C%20and%20the%0Aother%20to%20a%20canonical%20version%20of%20the%20object%20at%20rest%20pose.%20We%20also%20extend%20point%0Amaps%20to%20amodal%20reconstruction%2C%20seeing%20through%20self-occlusions%20to%20obtain%20the%0Acomplete%20shape%20of%20the%20object.%20We%20show%20that%203D%20reconstruction%20and%203D%20pose%0Aestimation%20reduce%20to%20the%20prediction%20of%20the%20DualPMs.%20We%20demonstrate%20empirically%0Athat%20this%20representation%20is%20a%20good%20target%20for%20a%20deep%20network%20to%20predict%3B%0Aspecifically%2C%20we%20consider%20modeling%20horses%2C%20showing%20that%20DualPMs%20can%20be%20trained%0Apurely%20on%203D%20synthetic%20data%2C%20consisting%20of%20a%20single%20model%20of%20a%20horse%2C%20while%0Ageneralizing%20very%20well%20to%20real%20images.%20With%20this%2C%20we%20improve%20by%20a%20large%20margin%0Aprevious%20methods%20for%20the%203D%20analysis%20and%20reconstruction%20of%20this%20type%20of%0Aobjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualPM%253A%2520Dual%2520Posed-Canonical%2520Point%2520Maps%2520for%25203D%2520Shape%2520and%2520Pose%250A%2520%2520Reconstruction%26entry.906535625%3DBen%2520Kaye%2520and%2520Tomas%2520Jakab%2520and%2520Shangzhe%2520Wu%2520and%2520Christian%2520Rupprecht%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520The%2520choice%2520of%2520data%2520representation%2520is%2520a%2520key%2520factor%2520in%2520the%2520success%2520of%2520deep%250Alearning%2520in%2520geometric%2520tasks.%2520For%2520instance%252C%2520DUSt3R%2520has%2520recently%2520introduced%2520the%250Aconcept%2520of%2520viewpoint-invariant%2520point%2520maps%252C%2520generalizing%2520depth%2520prediction%252C%2520and%250Ashowing%2520that%2520one%2520can%2520reduce%2520all%2520the%2520key%2520problems%2520in%2520the%25203D%2520reconstruction%2520of%250Astatic%2520scenes%2520to%2520predicting%2520such%2520point%2520maps.%2520In%2520this%2520paper%252C%2520we%2520develop%2520an%250Aanalogous%2520concept%2520for%2520a%2520very%2520different%2520problem%252C%2520namely%252C%2520the%2520reconstruction%2520of%250Athe%25203D%2520shape%2520and%2520pose%2520of%2520deformable%2520objects.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520Dual%250APoint%2520Maps%2520%2528DualPM%2529%252C%2520where%2520a%2520pair%2520of%2520point%2520maps%2520is%2520extracted%2520from%2520the%2520same%250Aimage%252C%2520one%2520associating%2520pixels%2520to%2520their%25203D%2520locations%2520on%2520the%2520object%252C%2520and%2520the%250Aother%2520to%2520a%2520canonical%2520version%2520of%2520the%2520object%2520at%2520rest%2520pose.%2520We%2520also%2520extend%2520point%250Amaps%2520to%2520amodal%2520reconstruction%252C%2520seeing%2520through%2520self-occlusions%2520to%2520obtain%2520the%250Acomplete%2520shape%2520of%2520the%2520object.%2520We%2520show%2520that%25203D%2520reconstruction%2520and%25203D%2520pose%250Aestimation%2520reduce%2520to%2520the%2520prediction%2520of%2520the%2520DualPMs.%2520We%2520demonstrate%2520empirically%250Athat%2520this%2520representation%2520is%2520a%2520good%2520target%2520for%2520a%2520deep%2520network%2520to%2520predict%253B%250Aspecifically%252C%2520we%2520consider%2520modeling%2520horses%252C%2520showing%2520that%2520DualPMs%2520can%2520be%2520trained%250Apurely%2520on%25203D%2520synthetic%2520data%252C%2520consisting%2520of%2520a%2520single%2520model%2520of%2520a%2520horse%252C%2520while%250Ageneralizing%2520very%2520well%2520to%2520real%2520images.%2520With%2520this%252C%2520we%2520improve%2520by%2520a%2520large%2520margin%250Aprevious%2520methods%2520for%2520the%25203D%2520analysis%2520and%2520reconstruction%2520of%2520this%2520type%2520of%250Aobjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualPM%3A%20Dual%20Posed-Canonical%20Point%20Maps%20for%203D%20Shape%20and%20Pose%0A%20%20Reconstruction&entry.906535625=Ben%20Kaye%20and%20Tomas%20Jakab%20and%20Shangzhe%20Wu%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20The%20choice%20of%20data%20representation%20is%20a%20key%20factor%20in%20the%20success%20of%20deep%0Alearning%20in%20geometric%20tasks.%20For%20instance%2C%20DUSt3R%20has%20recently%20introduced%20the%0Aconcept%20of%20viewpoint-invariant%20point%20maps%2C%20generalizing%20depth%20prediction%2C%20and%0Ashowing%20that%20one%20can%20reduce%20all%20the%20key%20problems%20in%20the%203D%20reconstruction%20of%0Astatic%20scenes%20to%20predicting%20such%20point%20maps.%20In%20this%20paper%2C%20we%20develop%20an%0Aanalogous%20concept%20for%20a%20very%20different%20problem%2C%20namely%2C%20the%20reconstruction%20of%0Athe%203D%20shape%20and%20pose%20of%20deformable%20objects.%20To%20this%20end%2C%20we%20introduce%20the%20Dual%0APoint%20Maps%20%28DualPM%29%2C%20where%20a%20pair%20of%20point%20maps%20is%20extracted%20from%20the%20same%0Aimage%2C%20one%20associating%20pixels%20to%20their%203D%20locations%20on%20the%20object%2C%20and%20the%0Aother%20to%20a%20canonical%20version%20of%20the%20object%20at%20rest%20pose.%20We%20also%20extend%20point%0Amaps%20to%20amodal%20reconstruction%2C%20seeing%20through%20self-occlusions%20to%20obtain%20the%0Acomplete%20shape%20of%20the%20object.%20We%20show%20that%203D%20reconstruction%20and%203D%20pose%0Aestimation%20reduce%20to%20the%20prediction%20of%20the%20DualPMs.%20We%20demonstrate%20empirically%0Athat%20this%20representation%20is%20a%20good%20target%20for%20a%20deep%20network%20to%20predict%3B%0Aspecifically%2C%20we%20consider%20modeling%20horses%2C%20showing%20that%20DualPMs%20can%20be%20trained%0Apurely%20on%203D%20synthetic%20data%2C%20consisting%20of%20a%20single%20model%20of%20a%20horse%2C%20while%0Ageneralizing%20very%20well%20to%20real%20images.%20With%20this%2C%20we%20improve%20by%20a%20large%20margin%0Aprevious%20methods%20for%20the%203D%20analysis%20and%20reconstruction%20of%20this%20type%20of%0Aobjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04464v2&entry.124074799=Read"},
{"title": "Humans as Checkerboards: Calibrating Camera Motion Scale for\n  World-Coordinate Human Mesh Recovery", "author": "Fengyuan Yang and Kerui Gu and Ha Linh Nguyen and Tze Ho Elden Tse and Angela Yao", "abstract": "  Accurate camera motion estimation is essential for recovering global human\nmotion in world coordinates from RGB video inputs. SLAM is widely used for\nestimating camera trajectory and point cloud, but monocular SLAM does so only\nup to an unknown scale factor. Previous works estimate the scale factor through\noptimization, but this is unreliable and time-consuming. This paper presents an\noptimization-free scale calibration framework, Human as Checkerboard (HAC). HAC\ninnovatively leverages the human body predicted by human mesh recovery model as\na calibration reference. Specifically, it uses the absolute depth of\nhuman-scene contact joints as references to calibrate the corresponding\nrelative scene depth from SLAM. HAC benefits from geometric priors encoded in\nhuman mesh recovery models to estimate the SLAM scale and achieves precise\nglobal human motion estimation. Simple yet powerful, our method sets a new\nstate-of-the-art performance for global human mesh estimation tasks, reducing\nmotion errors by 50% over prior local-to-global methods while using 100$\\times$\nless inference time than optimization-based methods. Project page:\nhttps://martayang.github.io/HAC.\n", "link": "http://arxiv.org/abs/2407.00574v2", "date": "2024-12-12", "relevancy": 2.9263, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5932}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5829}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Humans%20as%20Checkerboards%3A%20Calibrating%20Camera%20Motion%20Scale%20for%0A%20%20World-Coordinate%20Human%20Mesh%20Recovery&body=Title%3A%20Humans%20as%20Checkerboards%3A%20Calibrating%20Camera%20Motion%20Scale%20for%0A%20%20World-Coordinate%20Human%20Mesh%20Recovery%0AAuthor%3A%20Fengyuan%20Yang%20and%20Kerui%20Gu%20and%20Ha%20Linh%20Nguyen%20and%20Tze%20Ho%20Elden%20Tse%20and%20Angela%20Yao%0AAbstract%3A%20%20%20Accurate%20camera%20motion%20estimation%20is%20essential%20for%20recovering%20global%20human%0Amotion%20in%20world%20coordinates%20from%20RGB%20video%20inputs.%20SLAM%20is%20widely%20used%20for%0Aestimating%20camera%20trajectory%20and%20point%20cloud%2C%20but%20monocular%20SLAM%20does%20so%20only%0Aup%20to%20an%20unknown%20scale%20factor.%20Previous%20works%20estimate%20the%20scale%20factor%20through%0Aoptimization%2C%20but%20this%20is%20unreliable%20and%20time-consuming.%20This%20paper%20presents%20an%0Aoptimization-free%20scale%20calibration%20framework%2C%20Human%20as%20Checkerboard%20%28HAC%29.%20HAC%0Ainnovatively%20leverages%20the%20human%20body%20predicted%20by%20human%20mesh%20recovery%20model%20as%0Aa%20calibration%20reference.%20Specifically%2C%20it%20uses%20the%20absolute%20depth%20of%0Ahuman-scene%20contact%20joints%20as%20references%20to%20calibrate%20the%20corresponding%0Arelative%20scene%20depth%20from%20SLAM.%20HAC%20benefits%20from%20geometric%20priors%20encoded%20in%0Ahuman%20mesh%20recovery%20models%20to%20estimate%20the%20SLAM%20scale%20and%20achieves%20precise%0Aglobal%20human%20motion%20estimation.%20Simple%20yet%20powerful%2C%20our%20method%20sets%20a%20new%0Astate-of-the-art%20performance%20for%20global%20human%20mesh%20estimation%20tasks%2C%20reducing%0Amotion%20errors%20by%2050%25%20over%20prior%20local-to-global%20methods%20while%20using%20100%24%5Ctimes%24%0Aless%20inference%20time%20than%20optimization-based%20methods.%20Project%20page%3A%0Ahttps%3A//martayang.github.io/HAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumans%2520as%2520Checkerboards%253A%2520Calibrating%2520Camera%2520Motion%2520Scale%2520for%250A%2520%2520World-Coordinate%2520Human%2520Mesh%2520Recovery%26entry.906535625%3DFengyuan%2520Yang%2520and%2520Kerui%2520Gu%2520and%2520Ha%2520Linh%2520Nguyen%2520and%2520Tze%2520Ho%2520Elden%2520Tse%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520Accurate%2520camera%2520motion%2520estimation%2520is%2520essential%2520for%2520recovering%2520global%2520human%250Amotion%2520in%2520world%2520coordinates%2520from%2520RGB%2520video%2520inputs.%2520SLAM%2520is%2520widely%2520used%2520for%250Aestimating%2520camera%2520trajectory%2520and%2520point%2520cloud%252C%2520but%2520monocular%2520SLAM%2520does%2520so%2520only%250Aup%2520to%2520an%2520unknown%2520scale%2520factor.%2520Previous%2520works%2520estimate%2520the%2520scale%2520factor%2520through%250Aoptimization%252C%2520but%2520this%2520is%2520unreliable%2520and%2520time-consuming.%2520This%2520paper%2520presents%2520an%250Aoptimization-free%2520scale%2520calibration%2520framework%252C%2520Human%2520as%2520Checkerboard%2520%2528HAC%2529.%2520HAC%250Ainnovatively%2520leverages%2520the%2520human%2520body%2520predicted%2520by%2520human%2520mesh%2520recovery%2520model%2520as%250Aa%2520calibration%2520reference.%2520Specifically%252C%2520it%2520uses%2520the%2520absolute%2520depth%2520of%250Ahuman-scene%2520contact%2520joints%2520as%2520references%2520to%2520calibrate%2520the%2520corresponding%250Arelative%2520scene%2520depth%2520from%2520SLAM.%2520HAC%2520benefits%2520from%2520geometric%2520priors%2520encoded%2520in%250Ahuman%2520mesh%2520recovery%2520models%2520to%2520estimate%2520the%2520SLAM%2520scale%2520and%2520achieves%2520precise%250Aglobal%2520human%2520motion%2520estimation.%2520Simple%2520yet%2520powerful%252C%2520our%2520method%2520sets%2520a%2520new%250Astate-of-the-art%2520performance%2520for%2520global%2520human%2520mesh%2520estimation%2520tasks%252C%2520reducing%250Amotion%2520errors%2520by%252050%2525%2520over%2520prior%2520local-to-global%2520methods%2520while%2520using%2520100%2524%255Ctimes%2524%250Aless%2520inference%2520time%2520than%2520optimization-based%2520methods.%2520Project%2520page%253A%250Ahttps%253A//martayang.github.io/HAC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humans%20as%20Checkerboards%3A%20Calibrating%20Camera%20Motion%20Scale%20for%0A%20%20World-Coordinate%20Human%20Mesh%20Recovery&entry.906535625=Fengyuan%20Yang%20and%20Kerui%20Gu%20and%20Ha%20Linh%20Nguyen%20and%20Tze%20Ho%20Elden%20Tse%20and%20Angela%20Yao&entry.1292438233=%20%20Accurate%20camera%20motion%20estimation%20is%20essential%20for%20recovering%20global%20human%0Amotion%20in%20world%20coordinates%20from%20RGB%20video%20inputs.%20SLAM%20is%20widely%20used%20for%0Aestimating%20camera%20trajectory%20and%20point%20cloud%2C%20but%20monocular%20SLAM%20does%20so%20only%0Aup%20to%20an%20unknown%20scale%20factor.%20Previous%20works%20estimate%20the%20scale%20factor%20through%0Aoptimization%2C%20but%20this%20is%20unreliable%20and%20time-consuming.%20This%20paper%20presents%20an%0Aoptimization-free%20scale%20calibration%20framework%2C%20Human%20as%20Checkerboard%20%28HAC%29.%20HAC%0Ainnovatively%20leverages%20the%20human%20body%20predicted%20by%20human%20mesh%20recovery%20model%20as%0Aa%20calibration%20reference.%20Specifically%2C%20it%20uses%20the%20absolute%20depth%20of%0Ahuman-scene%20contact%20joints%20as%20references%20to%20calibrate%20the%20corresponding%0Arelative%20scene%20depth%20from%20SLAM.%20HAC%20benefits%20from%20geometric%20priors%20encoded%20in%0Ahuman%20mesh%20recovery%20models%20to%20estimate%20the%20SLAM%20scale%20and%20achieves%20precise%0Aglobal%20human%20motion%20estimation.%20Simple%20yet%20powerful%2C%20our%20method%20sets%20a%20new%0Astate-of-the-art%20performance%20for%20global%20human%20mesh%20estimation%20tasks%2C%20reducing%0Amotion%20errors%20by%2050%25%20over%20prior%20local-to-global%20methods%20while%20using%20100%24%5Ctimes%24%0Aless%20inference%20time%20than%20optimization-based%20methods.%20Project%20page%3A%0Ahttps%3A//martayang.github.io/HAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00574v2&entry.124074799=Read"},
{"title": "Do Multimodal Large Language Models See Like Humans?", "author": "Jiaying Lin and Shuquan Ye and Rynson W. H. Lau", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved impressive results on\nvarious vision tasks, leveraging recent advancements in large language models.\nHowever, a critical question remains unaddressed: do MLLMs perceive visual\ninformation similarly to humans? Current benchmarks lack the ability to\nevaluate MLLMs from this perspective. To address this challenge, we introduce\nHVSBench, a large-scale benchmark designed to assess the alignment between\nMLLMs and the human visual system (HVS) on fundamental vision tasks that mirror\nhuman vision. HVSBench curated over 85K multimodal samples, spanning 13\ncategories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,\nFree-Viewing, and Searching. Extensive experiments demonstrate the\neffectiveness of our benchmark in providing a comprehensive evaluation of\nMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best models\nshow significant room for improvement, with most achieving only moderate\nresults. Our experiments reveal that HVSBench presents a new and significant\nchallenge for cutting-edge MLLMs. We believe that HVSBench will facilitate\nresearch on human-aligned and explainable MLLMs, marking a key step in\nunderstanding how MLLMs perceive and process visual information.\n", "link": "http://arxiv.org/abs/2412.09603v1", "date": "2024-12-12", "relevancy": 2.9033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Multimodal%20Large%20Language%20Models%20See%20Like%20Humans%3F&body=Title%3A%20Do%20Multimodal%20Large%20Language%20Models%20See%20Like%20Humans%3F%0AAuthor%3A%20Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20impressive%20results%20on%0Avarious%20vision%20tasks%2C%20leveraging%20recent%20advancements%20in%20large%20language%20models.%0AHowever%2C%20a%20critical%20question%20remains%20unaddressed%3A%20do%20MLLMs%20perceive%20visual%0Ainformation%20similarly%20to%20humans%3F%20Current%20benchmarks%20lack%20the%20ability%20to%0Aevaluate%20MLLMs%20from%20this%20perspective.%20To%20address%20this%20challenge%2C%20we%20introduce%0AHVSBench%2C%20a%20large-scale%20benchmark%20designed%20to%20assess%20the%20alignment%20between%0AMLLMs%20and%20the%20human%20visual%20system%20%28HVS%29%20on%20fundamental%20vision%20tasks%20that%20mirror%0Ahuman%20vision.%20HVSBench%20curated%20over%2085K%20multimodal%20samples%2C%20spanning%2013%0Acategories%20and%205%20fields%20in%20HVS%2C%20including%20Prominence%2C%20Subitizing%2C%20Prioritizing%2C%0AFree-Viewing%2C%20and%20Searching.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20benchmark%20in%20providing%20a%20comprehensive%20evaluation%20of%0AMLLMs.%20Specifically%2C%20we%20evaluate%2013%20MLLMs%2C%20revealing%20that%20even%20the%20best%20models%0Ashow%20significant%20room%20for%20improvement%2C%20with%20most%20achieving%20only%20moderate%0Aresults.%20Our%20experiments%20reveal%20that%20HVSBench%20presents%20a%20new%20and%20significant%0Achallenge%20for%20cutting-edge%20MLLMs.%20We%20believe%20that%20HVSBench%20will%20facilitate%0Aresearch%20on%20human-aligned%20and%20explainable%20MLLMs%2C%20marking%20a%20key%20step%20in%0Aunderstanding%20how%20MLLMs%20perceive%20and%20process%20visual%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Multimodal%2520Large%2520Language%2520Models%2520See%2520Like%2520Humans%253F%26entry.906535625%3DJiaying%2520Lin%2520and%2520Shuquan%2520Ye%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520impressive%2520results%2520on%250Avarious%2520vision%2520tasks%252C%2520leveraging%2520recent%2520advancements%2520in%2520large%2520language%2520models.%250AHowever%252C%2520a%2520critical%2520question%2520remains%2520unaddressed%253A%2520do%2520MLLMs%2520perceive%2520visual%250Ainformation%2520similarly%2520to%2520humans%253F%2520Current%2520benchmarks%2520lack%2520the%2520ability%2520to%250Aevaluate%2520MLLMs%2520from%2520this%2520perspective.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%250AHVSBench%252C%2520a%2520large-scale%2520benchmark%2520designed%2520to%2520assess%2520the%2520alignment%2520between%250AMLLMs%2520and%2520the%2520human%2520visual%2520system%2520%2528HVS%2529%2520on%2520fundamental%2520vision%2520tasks%2520that%2520mirror%250Ahuman%2520vision.%2520HVSBench%2520curated%2520over%252085K%2520multimodal%2520samples%252C%2520spanning%252013%250Acategories%2520and%25205%2520fields%2520in%2520HVS%252C%2520including%2520Prominence%252C%2520Subitizing%252C%2520Prioritizing%252C%250AFree-Viewing%252C%2520and%2520Searching.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520benchmark%2520in%2520providing%2520a%2520comprehensive%2520evaluation%2520of%250AMLLMs.%2520Specifically%252C%2520we%2520evaluate%252013%2520MLLMs%252C%2520revealing%2520that%2520even%2520the%2520best%2520models%250Ashow%2520significant%2520room%2520for%2520improvement%252C%2520with%2520most%2520achieving%2520only%2520moderate%250Aresults.%2520Our%2520experiments%2520reveal%2520that%2520HVSBench%2520presents%2520a%2520new%2520and%2520significant%250Achallenge%2520for%2520cutting-edge%2520MLLMs.%2520We%2520believe%2520that%2520HVSBench%2520will%2520facilitate%250Aresearch%2520on%2520human-aligned%2520and%2520explainable%2520MLLMs%252C%2520marking%2520a%2520key%2520step%2520in%250Aunderstanding%2520how%2520MLLMs%2520perceive%2520and%2520process%2520visual%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Multimodal%20Large%20Language%20Models%20See%20Like%20Humans%3F&entry.906535625=Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20impressive%20results%20on%0Avarious%20vision%20tasks%2C%20leveraging%20recent%20advancements%20in%20large%20language%20models.%0AHowever%2C%20a%20critical%20question%20remains%20unaddressed%3A%20do%20MLLMs%20perceive%20visual%0Ainformation%20similarly%20to%20humans%3F%20Current%20benchmarks%20lack%20the%20ability%20to%0Aevaluate%20MLLMs%20from%20this%20perspective.%20To%20address%20this%20challenge%2C%20we%20introduce%0AHVSBench%2C%20a%20large-scale%20benchmark%20designed%20to%20assess%20the%20alignment%20between%0AMLLMs%20and%20the%20human%20visual%20system%20%28HVS%29%20on%20fundamental%20vision%20tasks%20that%20mirror%0Ahuman%20vision.%20HVSBench%20curated%20over%2085K%20multimodal%20samples%2C%20spanning%2013%0Acategories%20and%205%20fields%20in%20HVS%2C%20including%20Prominence%2C%20Subitizing%2C%20Prioritizing%2C%0AFree-Viewing%2C%20and%20Searching.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20benchmark%20in%20providing%20a%20comprehensive%20evaluation%20of%0AMLLMs.%20Specifically%2C%20we%20evaluate%2013%20MLLMs%2C%20revealing%20that%20even%20the%20best%20models%0Ashow%20significant%20room%20for%20improvement%2C%20with%20most%20achieving%20only%20moderate%0Aresults.%20Our%20experiments%20reveal%20that%20HVSBench%20presents%20a%20new%20and%20significant%0Achallenge%20for%20cutting-edge%20MLLMs.%20We%20believe%20that%20HVSBench%20will%20facilitate%0Aresearch%20on%20human-aligned%20and%20explainable%20MLLMs%2C%20marking%20a%20key%20step%20in%0Aunderstanding%20how%20MLLMs%20perceive%20and%20process%20visual%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09603v1&entry.124074799=Read"},
{"title": "Riemann-based Multi-scale Attention Reasoning Network for Text-3D\n  Retrieval", "author": "Wenrui Li and Wei Han and Yandu Chen and Yeyu Chai and Yidan Lu and Xingtao Wang and Xiaopeng Fan", "abstract": "  Due to the challenges in acquiring paired Text-3D data and the inherent\nirregularity of 3D data structures, combined representation learning of 3D\npoint clouds and text remains unexplored. In this paper, we propose a novel\nRiemann-based Multi-scale Attention Reasoning Network (RMARN) for text-3D\nretrieval. Specifically, the extracted text and point cloud features are\nrefined by their respective Adaptive Feature Refiner (AFR). Furthermore, we\nintroduce the innovative Riemann Local Similarity (RLS) module and the Global\nPooling Similarity (GPS) module. However, as 3D point cloud data and text data\noften possess complex geometric structures in high-dimensional space, the\nproposed RLS employs a novel Riemann Attention Mechanism to reflect the\nintrinsic geometric relationships of the data. Without explicitly defining the\nmanifold, RMARN learns the manifold parameters to better represent the\ndistances between text-point cloud samples. To address the challenges of\nlacking paired text-3D data, we have created the large-scale Text-3D Retrieval\ndataset T3DR-HIT, which comprises over 3,380 pairs of text and point cloud\ndata. T3DR-HIT contains coarse-grained indoor 3D scenes and fine-grained\nChinese artifact scenes, consisting of 1,380 and over 2,000 text-3D pairs,\nrespectively. Experiments on our custom datasets demonstrate the superior\nperformance of the proposed method. Our code and proposed datasets are\navailable at \\url{https://github.com/liwrui/RMARN}.\n", "link": "http://arxiv.org/abs/2408.13712v2", "date": "2024-12-12", "relevancy": 2.9013, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6189}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5716}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemann-based%20Multi-scale%20Attention%20Reasoning%20Network%20for%20Text-3D%0A%20%20Retrieval&body=Title%3A%20Riemann-based%20Multi-scale%20Attention%20Reasoning%20Network%20for%20Text-3D%0A%20%20Retrieval%0AAuthor%3A%20Wenrui%20Li%20and%20Wei%20Han%20and%20Yandu%20Chen%20and%20Yeyu%20Chai%20and%20Yidan%20Lu%20and%20Xingtao%20Wang%20and%20Xiaopeng%20Fan%0AAbstract%3A%20%20%20Due%20to%20the%20challenges%20in%20acquiring%20paired%20Text-3D%20data%20and%20the%20inherent%0Airregularity%20of%203D%20data%20structures%2C%20combined%20representation%20learning%20of%203D%0Apoint%20clouds%20and%20text%20remains%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ARiemann-based%20Multi-scale%20Attention%20Reasoning%20Network%20%28RMARN%29%20for%20text-3D%0Aretrieval.%20Specifically%2C%20the%20extracted%20text%20and%20point%20cloud%20features%20are%0Arefined%20by%20their%20respective%20Adaptive%20Feature%20Refiner%20%28AFR%29.%20Furthermore%2C%20we%0Aintroduce%20the%20innovative%20Riemann%20Local%20Similarity%20%28RLS%29%20module%20and%20the%20Global%0APooling%20Similarity%20%28GPS%29%20module.%20However%2C%20as%203D%20point%20cloud%20data%20and%20text%20data%0Aoften%20possess%20complex%20geometric%20structures%20in%20high-dimensional%20space%2C%20the%0Aproposed%20RLS%20employs%20a%20novel%20Riemann%20Attention%20Mechanism%20to%20reflect%20the%0Aintrinsic%20geometric%20relationships%20of%20the%20data.%20Without%20explicitly%20defining%20the%0Amanifold%2C%20RMARN%20learns%20the%20manifold%20parameters%20to%20better%20represent%20the%0Adistances%20between%20text-point%20cloud%20samples.%20To%20address%20the%20challenges%20of%0Alacking%20paired%20text-3D%20data%2C%20we%20have%20created%20the%20large-scale%20Text-3D%20Retrieval%0Adataset%20T3DR-HIT%2C%20which%20comprises%20over%203%2C380%20pairs%20of%20text%20and%20point%20cloud%0Adata.%20T3DR-HIT%20contains%20coarse-grained%20indoor%203D%20scenes%20and%20fine-grained%0AChinese%20artifact%20scenes%2C%20consisting%20of%201%2C380%20and%20over%202%2C000%20text-3D%20pairs%2C%0Arespectively.%20Experiments%20on%20our%20custom%20datasets%20demonstrate%20the%20superior%0Aperformance%20of%20the%20proposed%20method.%20Our%20code%20and%20proposed%20datasets%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/liwrui/RMARN%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemann-based%2520Multi-scale%2520Attention%2520Reasoning%2520Network%2520for%2520Text-3D%250A%2520%2520Retrieval%26entry.906535625%3DWenrui%2520Li%2520and%2520Wei%2520Han%2520and%2520Yandu%2520Chen%2520and%2520Yeyu%2520Chai%2520and%2520Yidan%2520Lu%2520and%2520Xingtao%2520Wang%2520and%2520Xiaopeng%2520Fan%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520challenges%2520in%2520acquiring%2520paired%2520Text-3D%2520data%2520and%2520the%2520inherent%250Airregularity%2520of%25203D%2520data%2520structures%252C%2520combined%2520representation%2520learning%2520of%25203D%250Apoint%2520clouds%2520and%2520text%2520remains%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250ARiemann-based%2520Multi-scale%2520Attention%2520Reasoning%2520Network%2520%2528RMARN%2529%2520for%2520text-3D%250Aretrieval.%2520Specifically%252C%2520the%2520extracted%2520text%2520and%2520point%2520cloud%2520features%2520are%250Arefined%2520by%2520their%2520respective%2520Adaptive%2520Feature%2520Refiner%2520%2528AFR%2529.%2520Furthermore%252C%2520we%250Aintroduce%2520the%2520innovative%2520Riemann%2520Local%2520Similarity%2520%2528RLS%2529%2520module%2520and%2520the%2520Global%250APooling%2520Similarity%2520%2528GPS%2529%2520module.%2520However%252C%2520as%25203D%2520point%2520cloud%2520data%2520and%2520text%2520data%250Aoften%2520possess%2520complex%2520geometric%2520structures%2520in%2520high-dimensional%2520space%252C%2520the%250Aproposed%2520RLS%2520employs%2520a%2520novel%2520Riemann%2520Attention%2520Mechanism%2520to%2520reflect%2520the%250Aintrinsic%2520geometric%2520relationships%2520of%2520the%2520data.%2520Without%2520explicitly%2520defining%2520the%250Amanifold%252C%2520RMARN%2520learns%2520the%2520manifold%2520parameters%2520to%2520better%2520represent%2520the%250Adistances%2520between%2520text-point%2520cloud%2520samples.%2520To%2520address%2520the%2520challenges%2520of%250Alacking%2520paired%2520text-3D%2520data%252C%2520we%2520have%2520created%2520the%2520large-scale%2520Text-3D%2520Retrieval%250Adataset%2520T3DR-HIT%252C%2520which%2520comprises%2520over%25203%252C380%2520pairs%2520of%2520text%2520and%2520point%2520cloud%250Adata.%2520T3DR-HIT%2520contains%2520coarse-grained%2520indoor%25203D%2520scenes%2520and%2520fine-grained%250AChinese%2520artifact%2520scenes%252C%2520consisting%2520of%25201%252C380%2520and%2520over%25202%252C000%2520text-3D%2520pairs%252C%250Arespectively.%2520Experiments%2520on%2520our%2520custom%2520datasets%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520the%2520proposed%2520method.%2520Our%2520code%2520and%2520proposed%2520datasets%2520are%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/liwrui/RMARN%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemann-based%20Multi-scale%20Attention%20Reasoning%20Network%20for%20Text-3D%0A%20%20Retrieval&entry.906535625=Wenrui%20Li%20and%20Wei%20Han%20and%20Yandu%20Chen%20and%20Yeyu%20Chai%20and%20Yidan%20Lu%20and%20Xingtao%20Wang%20and%20Xiaopeng%20Fan&entry.1292438233=%20%20Due%20to%20the%20challenges%20in%20acquiring%20paired%20Text-3D%20data%20and%20the%20inherent%0Airregularity%20of%203D%20data%20structures%2C%20combined%20representation%20learning%20of%203D%0Apoint%20clouds%20and%20text%20remains%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ARiemann-based%20Multi-scale%20Attention%20Reasoning%20Network%20%28RMARN%29%20for%20text-3D%0Aretrieval.%20Specifically%2C%20the%20extracted%20text%20and%20point%20cloud%20features%20are%0Arefined%20by%20their%20respective%20Adaptive%20Feature%20Refiner%20%28AFR%29.%20Furthermore%2C%20we%0Aintroduce%20the%20innovative%20Riemann%20Local%20Similarity%20%28RLS%29%20module%20and%20the%20Global%0APooling%20Similarity%20%28GPS%29%20module.%20However%2C%20as%203D%20point%20cloud%20data%20and%20text%20data%0Aoften%20possess%20complex%20geometric%20structures%20in%20high-dimensional%20space%2C%20the%0Aproposed%20RLS%20employs%20a%20novel%20Riemann%20Attention%20Mechanism%20to%20reflect%20the%0Aintrinsic%20geometric%20relationships%20of%20the%20data.%20Without%20explicitly%20defining%20the%0Amanifold%2C%20RMARN%20learns%20the%20manifold%20parameters%20to%20better%20represent%20the%0Adistances%20between%20text-point%20cloud%20samples.%20To%20address%20the%20challenges%20of%0Alacking%20paired%20text-3D%20data%2C%20we%20have%20created%20the%20large-scale%20Text-3D%20Retrieval%0Adataset%20T3DR-HIT%2C%20which%20comprises%20over%203%2C380%20pairs%20of%20text%20and%20point%20cloud%0Adata.%20T3DR-HIT%20contains%20coarse-grained%20indoor%203D%20scenes%20and%20fine-grained%0AChinese%20artifact%20scenes%2C%20consisting%20of%201%2C380%20and%20over%202%2C000%20text-3D%20pairs%2C%0Arespectively.%20Experiments%20on%20our%20custom%20datasets%20demonstrate%20the%20superior%0Aperformance%20of%20the%20proposed%20method.%20Our%20code%20and%20proposed%20datasets%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/liwrui/RMARN%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13712v2&entry.124074799=Read"},
{"title": "PVC: Progressive Visual Token Compression for Unified Image and Video\n  Processing in Large Vision-Language Models", "author": "Chenyu Yang and Xuan Dong and Xizhou Zhu and Weijie Su and Jiahao Wang and Hao Tian and Zhe Chen and Wenhai Wang and Lewei Lu and Jifeng Dai", "abstract": "  Large Vision-Language Models (VLMs) have been extended to understand both\nimages and videos. Visual token compression is leveraged to reduce the\nconsiderable token length of visual inputs. To meet the needs of different\ntasks, existing high-performance models usually process images and videos\nseparately with different token compression strategies, limiting the\ncapabilities of combining images and videos. To this end, we extend each image\ninto a \"static\" video and introduce a unified token compression strategy called\nProgressive Visual Token Compression (PVC), where the tokens of each frame are\nprogressively encoded and adaptively compressed to supplement the information\nnot extracted from previous frames. Video tokens are efficiently compressed\nwith exploiting the inherent temporal redundancy. Images are repeated as static\nvideos, and the spatial details can be gradually supplemented in multiple\nframes. PVC unifies the token compressing of images and videos. With a limited\nnumber of tokens per frame (64 tokens by default), spatial details and temporal\nchanges can still be preserved. Experiments show that our model achieves\nstate-of-the-art performance across various video understanding benchmarks,\nincluding long video tasks and fine-grained short video tasks. Meanwhile, our\nunified token compression strategy incurs no performance loss on image\nbenchmarks, particularly in detail-sensitive tasks.\n", "link": "http://arxiv.org/abs/2412.09613v1", "date": "2024-12-12", "relevancy": 2.8917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PVC%3A%20Progressive%20Visual%20Token%20Compression%20for%20Unified%20Image%20and%20Video%0A%20%20Processing%20in%20Large%20Vision-Language%20Models&body=Title%3A%20PVC%3A%20Progressive%20Visual%20Token%20Compression%20for%20Unified%20Image%20and%20Video%0A%20%20Processing%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Chenyu%20Yang%20and%20Xuan%20Dong%20and%20Xizhou%20Zhu%20and%20Weijie%20Su%20and%20Jiahao%20Wang%20and%20Hao%20Tian%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20extended%20to%20understand%20both%0Aimages%20and%20videos.%20Visual%20token%20compression%20is%20leveraged%20to%20reduce%20the%0Aconsiderable%20token%20length%20of%20visual%20inputs.%20To%20meet%20the%20needs%20of%20different%0Atasks%2C%20existing%20high-performance%20models%20usually%20process%20images%20and%20videos%0Aseparately%20with%20different%20token%20compression%20strategies%2C%20limiting%20the%0Acapabilities%20of%20combining%20images%20and%20videos.%20To%20this%20end%2C%20we%20extend%20each%20image%0Ainto%20a%20%22static%22%20video%20and%20introduce%20a%20unified%20token%20compression%20strategy%20called%0AProgressive%20Visual%20Token%20Compression%20%28PVC%29%2C%20where%20the%20tokens%20of%20each%20frame%20are%0Aprogressively%20encoded%20and%20adaptively%20compressed%20to%20supplement%20the%20information%0Anot%20extracted%20from%20previous%20frames.%20Video%20tokens%20are%20efficiently%20compressed%0Awith%20exploiting%20the%20inherent%20temporal%20redundancy.%20Images%20are%20repeated%20as%20static%0Avideos%2C%20and%20the%20spatial%20details%20can%20be%20gradually%20supplemented%20in%20multiple%0Aframes.%20PVC%20unifies%20the%20token%20compressing%20of%20images%20and%20videos.%20With%20a%20limited%0Anumber%20of%20tokens%20per%20frame%20%2864%20tokens%20by%20default%29%2C%20spatial%20details%20and%20temporal%0Achanges%20can%20still%20be%20preserved.%20Experiments%20show%20that%20our%20model%20achieves%0Astate-of-the-art%20performance%20across%20various%20video%20understanding%20benchmarks%2C%0Aincluding%20long%20video%20tasks%20and%20fine-grained%20short%20video%20tasks.%20Meanwhile%2C%20our%0Aunified%20token%20compression%20strategy%20incurs%20no%20performance%20loss%20on%20image%0Abenchmarks%2C%20particularly%20in%20detail-sensitive%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPVC%253A%2520Progressive%2520Visual%2520Token%2520Compression%2520for%2520Unified%2520Image%2520and%2520Video%250A%2520%2520Processing%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DChenyu%2520Yang%2520and%2520Xuan%2520Dong%2520and%2520Xizhou%2520Zhu%2520and%2520Weijie%2520Su%2520and%2520Jiahao%2520Wang%2520and%2520Hao%2520Tian%2520and%2520Zhe%2520Chen%2520and%2520Wenhai%2520Wang%2520and%2520Lewei%2520Lu%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520been%2520extended%2520to%2520understand%2520both%250Aimages%2520and%2520videos.%2520Visual%2520token%2520compression%2520is%2520leveraged%2520to%2520reduce%2520the%250Aconsiderable%2520token%2520length%2520of%2520visual%2520inputs.%2520To%2520meet%2520the%2520needs%2520of%2520different%250Atasks%252C%2520existing%2520high-performance%2520models%2520usually%2520process%2520images%2520and%2520videos%250Aseparately%2520with%2520different%2520token%2520compression%2520strategies%252C%2520limiting%2520the%250Acapabilities%2520of%2520combining%2520images%2520and%2520videos.%2520To%2520this%2520end%252C%2520we%2520extend%2520each%2520image%250Ainto%2520a%2520%2522static%2522%2520video%2520and%2520introduce%2520a%2520unified%2520token%2520compression%2520strategy%2520called%250AProgressive%2520Visual%2520Token%2520Compression%2520%2528PVC%2529%252C%2520where%2520the%2520tokens%2520of%2520each%2520frame%2520are%250Aprogressively%2520encoded%2520and%2520adaptively%2520compressed%2520to%2520supplement%2520the%2520information%250Anot%2520extracted%2520from%2520previous%2520frames.%2520Video%2520tokens%2520are%2520efficiently%2520compressed%250Awith%2520exploiting%2520the%2520inherent%2520temporal%2520redundancy.%2520Images%2520are%2520repeated%2520as%2520static%250Avideos%252C%2520and%2520the%2520spatial%2520details%2520can%2520be%2520gradually%2520supplemented%2520in%2520multiple%250Aframes.%2520PVC%2520unifies%2520the%2520token%2520compressing%2520of%2520images%2520and%2520videos.%2520With%2520a%2520limited%250Anumber%2520of%2520tokens%2520per%2520frame%2520%252864%2520tokens%2520by%2520default%2529%252C%2520spatial%2520details%2520and%2520temporal%250Achanges%2520can%2520still%2520be%2520preserved.%2520Experiments%2520show%2520that%2520our%2520model%2520achieves%250Astate-of-the-art%2520performance%2520across%2520various%2520video%2520understanding%2520benchmarks%252C%250Aincluding%2520long%2520video%2520tasks%2520and%2520fine-grained%2520short%2520video%2520tasks.%2520Meanwhile%252C%2520our%250Aunified%2520token%2520compression%2520strategy%2520incurs%2520no%2520performance%2520loss%2520on%2520image%250Abenchmarks%252C%2520particularly%2520in%2520detail-sensitive%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PVC%3A%20Progressive%20Visual%20Token%20Compression%20for%20Unified%20Image%20and%20Video%0A%20%20Processing%20in%20Large%20Vision-Language%20Models&entry.906535625=Chenyu%20Yang%20and%20Xuan%20Dong%20and%20Xizhou%20Zhu%20and%20Weijie%20Su%20and%20Jiahao%20Wang%20and%20Hao%20Tian%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Jifeng%20Dai&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20extended%20to%20understand%20both%0Aimages%20and%20videos.%20Visual%20token%20compression%20is%20leveraged%20to%20reduce%20the%0Aconsiderable%20token%20length%20of%20visual%20inputs.%20To%20meet%20the%20needs%20of%20different%0Atasks%2C%20existing%20high-performance%20models%20usually%20process%20images%20and%20videos%0Aseparately%20with%20different%20token%20compression%20strategies%2C%20limiting%20the%0Acapabilities%20of%20combining%20images%20and%20videos.%20To%20this%20end%2C%20we%20extend%20each%20image%0Ainto%20a%20%22static%22%20video%20and%20introduce%20a%20unified%20token%20compression%20strategy%20called%0AProgressive%20Visual%20Token%20Compression%20%28PVC%29%2C%20where%20the%20tokens%20of%20each%20frame%20are%0Aprogressively%20encoded%20and%20adaptively%20compressed%20to%20supplement%20the%20information%0Anot%20extracted%20from%20previous%20frames.%20Video%20tokens%20are%20efficiently%20compressed%0Awith%20exploiting%20the%20inherent%20temporal%20redundancy.%20Images%20are%20repeated%20as%20static%0Avideos%2C%20and%20the%20spatial%20details%20can%20be%20gradually%20supplemented%20in%20multiple%0Aframes.%20PVC%20unifies%20the%20token%20compressing%20of%20images%20and%20videos.%20With%20a%20limited%0Anumber%20of%20tokens%20per%20frame%20%2864%20tokens%20by%20default%29%2C%20spatial%20details%20and%20temporal%0Achanges%20can%20still%20be%20preserved.%20Experiments%20show%20that%20our%20model%20achieves%0Astate-of-the-art%20performance%20across%20various%20video%20understanding%20benchmarks%2C%0Aincluding%20long%20video%20tasks%20and%20fine-grained%20short%20video%20tasks.%20Meanwhile%2C%20our%0Aunified%20token%20compression%20strategy%20incurs%20no%20performance%20loss%20on%20image%0Abenchmarks%2C%20particularly%20in%20detail-sensitive%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09613v1&entry.124074799=Read"},
{"title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary\n  Embedding Distillation", "author": "Jitesh Jain and Zhengyuan Yang and Humphrey Shi and Jianfeng Gao and Jianwei Yang", "abstract": "  The standard practice for developing contemporary MLLMs is to feed features\nfrom vision encoder(s) into the LLM and train with natural language\nsupervision. In this work, we posit an overlooked opportunity to optimize the\nintermediate LLM representations through a vision perspective (objective),\ni.e., solely natural language supervision is sub-optimal for the MLLM's visual\nunderstanding ability. To that end, we propose OLA-VLM, the first approach\ndistilling knowledge into the LLM's hidden representations from a set of target\nvisual representations. Firstly, we formulate the objective during the\npretraining stage in MLLMs as a coupled optimization of predictive visual\nembedding and next text-token prediction. Secondly, we investigate MLLMs\ntrained solely with natural language supervision and identify a positive\ncorrelation between the quality of visual representations within these models\nand their downstream performance. Moreover, upon probing our OLA-VLM, we\nobserve improved representation quality owing to the embedding optimization.\nThirdly, we demonstrate that our OLA-VLM outperforms the single and\nmulti-encoder baselines, proving our approach's superiority over explicitly\nfeeding the corresponding features to the LLM. Particularly, OLA-VLM boosts\nperformance by an average margin of up to 2.5% on various benchmarks, with a\nnotable improvement of 8.7% on the Depth task in CV-Bench. Our code is\nopen-sourced at https://github.com/SHI-Labs/OLA-VLM .\n", "link": "http://arxiv.org/abs/2412.09585v1", "date": "2024-12-12", "relevancy": 2.8911, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLA-VLM%3A%20Elevating%20Visual%20Perception%20in%20Multimodal%20LLMs%20with%20Auxiliary%0A%20%20Embedding%20Distillation&body=Title%3A%20OLA-VLM%3A%20Elevating%20Visual%20Perception%20in%20Multimodal%20LLMs%20with%20Auxiliary%0A%20%20Embedding%20Distillation%0AAuthor%3A%20Jitesh%20Jain%20and%20Zhengyuan%20Yang%20and%20Humphrey%20Shi%20and%20Jianfeng%20Gao%20and%20Jianwei%20Yang%0AAbstract%3A%20%20%20The%20standard%20practice%20for%20developing%20contemporary%20MLLMs%20is%20to%20feed%20features%0Afrom%20vision%20encoder%28s%29%20into%20the%20LLM%20and%20train%20with%20natural%20language%0Asupervision.%20In%20this%20work%2C%20we%20posit%20an%20overlooked%20opportunity%20to%20optimize%20the%0Aintermediate%20LLM%20representations%20through%20a%20vision%20perspective%20%28objective%29%2C%0Ai.e.%2C%20solely%20natural%20language%20supervision%20is%20sub-optimal%20for%20the%20MLLM%27s%20visual%0Aunderstanding%20ability.%20To%20that%20end%2C%20we%20propose%20OLA-VLM%2C%20the%20first%20approach%0Adistilling%20knowledge%20into%20the%20LLM%27s%20hidden%20representations%20from%20a%20set%20of%20target%0Avisual%20representations.%20Firstly%2C%20we%20formulate%20the%20objective%20during%20the%0Apretraining%20stage%20in%20MLLMs%20as%20a%20coupled%20optimization%20of%20predictive%20visual%0Aembedding%20and%20next%20text-token%20prediction.%20Secondly%2C%20we%20investigate%20MLLMs%0Atrained%20solely%20with%20natural%20language%20supervision%20and%20identify%20a%20positive%0Acorrelation%20between%20the%20quality%20of%20visual%20representations%20within%20these%20models%0Aand%20their%20downstream%20performance.%20Moreover%2C%20upon%20probing%20our%20OLA-VLM%2C%20we%0Aobserve%20improved%20representation%20quality%20owing%20to%20the%20embedding%20optimization.%0AThirdly%2C%20we%20demonstrate%20that%20our%20OLA-VLM%20outperforms%20the%20single%20and%0Amulti-encoder%20baselines%2C%20proving%20our%20approach%27s%20superiority%20over%20explicitly%0Afeeding%20the%20corresponding%20features%20to%20the%20LLM.%20Particularly%2C%20OLA-VLM%20boosts%0Aperformance%20by%20an%20average%20margin%20of%20up%20to%202.5%25%20on%20various%20benchmarks%2C%20with%20a%0Anotable%20improvement%20of%208.7%25%20on%20the%20Depth%20task%20in%20CV-Bench.%20Our%20code%20is%0Aopen-sourced%20at%20https%3A//github.com/SHI-Labs/OLA-VLM%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLA-VLM%253A%2520Elevating%2520Visual%2520Perception%2520in%2520Multimodal%2520LLMs%2520with%2520Auxiliary%250A%2520%2520Embedding%2520Distillation%26entry.906535625%3DJitesh%2520Jain%2520and%2520Zhengyuan%2520Yang%2520and%2520Humphrey%2520Shi%2520and%2520Jianfeng%2520Gao%2520and%2520Jianwei%2520Yang%26entry.1292438233%3D%2520%2520The%2520standard%2520practice%2520for%2520developing%2520contemporary%2520MLLMs%2520is%2520to%2520feed%2520features%250Afrom%2520vision%2520encoder%2528s%2529%2520into%2520the%2520LLM%2520and%2520train%2520with%2520natural%2520language%250Asupervision.%2520In%2520this%2520work%252C%2520we%2520posit%2520an%2520overlooked%2520opportunity%2520to%2520optimize%2520the%250Aintermediate%2520LLM%2520representations%2520through%2520a%2520vision%2520perspective%2520%2528objective%2529%252C%250Ai.e.%252C%2520solely%2520natural%2520language%2520supervision%2520is%2520sub-optimal%2520for%2520the%2520MLLM%2527s%2520visual%250Aunderstanding%2520ability.%2520To%2520that%2520end%252C%2520we%2520propose%2520OLA-VLM%252C%2520the%2520first%2520approach%250Adistilling%2520knowledge%2520into%2520the%2520LLM%2527s%2520hidden%2520representations%2520from%2520a%2520set%2520of%2520target%250Avisual%2520representations.%2520Firstly%252C%2520we%2520formulate%2520the%2520objective%2520during%2520the%250Apretraining%2520stage%2520in%2520MLLMs%2520as%2520a%2520coupled%2520optimization%2520of%2520predictive%2520visual%250Aembedding%2520and%2520next%2520text-token%2520prediction.%2520Secondly%252C%2520we%2520investigate%2520MLLMs%250Atrained%2520solely%2520with%2520natural%2520language%2520supervision%2520and%2520identify%2520a%2520positive%250Acorrelation%2520between%2520the%2520quality%2520of%2520visual%2520representations%2520within%2520these%2520models%250Aand%2520their%2520downstream%2520performance.%2520Moreover%252C%2520upon%2520probing%2520our%2520OLA-VLM%252C%2520we%250Aobserve%2520improved%2520representation%2520quality%2520owing%2520to%2520the%2520embedding%2520optimization.%250AThirdly%252C%2520we%2520demonstrate%2520that%2520our%2520OLA-VLM%2520outperforms%2520the%2520single%2520and%250Amulti-encoder%2520baselines%252C%2520proving%2520our%2520approach%2527s%2520superiority%2520over%2520explicitly%250Afeeding%2520the%2520corresponding%2520features%2520to%2520the%2520LLM.%2520Particularly%252C%2520OLA-VLM%2520boosts%250Aperformance%2520by%2520an%2520average%2520margin%2520of%2520up%2520to%25202.5%2525%2520on%2520various%2520benchmarks%252C%2520with%2520a%250Anotable%2520improvement%2520of%25208.7%2525%2520on%2520the%2520Depth%2520task%2520in%2520CV-Bench.%2520Our%2520code%2520is%250Aopen-sourced%2520at%2520https%253A//github.com/SHI-Labs/OLA-VLM%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLA-VLM%3A%20Elevating%20Visual%20Perception%20in%20Multimodal%20LLMs%20with%20Auxiliary%0A%20%20Embedding%20Distillation&entry.906535625=Jitesh%20Jain%20and%20Zhengyuan%20Yang%20and%20Humphrey%20Shi%20and%20Jianfeng%20Gao%20and%20Jianwei%20Yang&entry.1292438233=%20%20The%20standard%20practice%20for%20developing%20contemporary%20MLLMs%20is%20to%20feed%20features%0Afrom%20vision%20encoder%28s%29%20into%20the%20LLM%20and%20train%20with%20natural%20language%0Asupervision.%20In%20this%20work%2C%20we%20posit%20an%20overlooked%20opportunity%20to%20optimize%20the%0Aintermediate%20LLM%20representations%20through%20a%20vision%20perspective%20%28objective%29%2C%0Ai.e.%2C%20solely%20natural%20language%20supervision%20is%20sub-optimal%20for%20the%20MLLM%27s%20visual%0Aunderstanding%20ability.%20To%20that%20end%2C%20we%20propose%20OLA-VLM%2C%20the%20first%20approach%0Adistilling%20knowledge%20into%20the%20LLM%27s%20hidden%20representations%20from%20a%20set%20of%20target%0Avisual%20representations.%20Firstly%2C%20we%20formulate%20the%20objective%20during%20the%0Apretraining%20stage%20in%20MLLMs%20as%20a%20coupled%20optimization%20of%20predictive%20visual%0Aembedding%20and%20next%20text-token%20prediction.%20Secondly%2C%20we%20investigate%20MLLMs%0Atrained%20solely%20with%20natural%20language%20supervision%20and%20identify%20a%20positive%0Acorrelation%20between%20the%20quality%20of%20visual%20representations%20within%20these%20models%0Aand%20their%20downstream%20performance.%20Moreover%2C%20upon%20probing%20our%20OLA-VLM%2C%20we%0Aobserve%20improved%20representation%20quality%20owing%20to%20the%20embedding%20optimization.%0AThirdly%2C%20we%20demonstrate%20that%20our%20OLA-VLM%20outperforms%20the%20single%20and%0Amulti-encoder%20baselines%2C%20proving%20our%20approach%27s%20superiority%20over%20explicitly%0Afeeding%20the%20corresponding%20features%20to%20the%20LLM.%20Particularly%2C%20OLA-VLM%20boosts%0Aperformance%20by%20an%20average%20margin%20of%20up%20to%202.5%25%20on%20various%20benchmarks%2C%20with%20a%0Anotable%20improvement%20of%208.7%25%20on%20the%20Depth%20task%20in%20CV-Bench.%20Our%20code%20is%0Aopen-sourced%20at%20https%3A//github.com/SHI-Labs/OLA-VLM%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09585v1&entry.124074799=Read"},
{"title": "VDB-GPDF: Online Gaussian Process Distance Field with VDB Structure", "author": "Lan Wu and Cedric Le Gentil and Teresa Vidal-Calleja", "abstract": "  Robots reason about the environment through dedicated representations.\nPopular choices for dense representations exploit Truncated Signed Distance\nFunctions (TSDF) and Octree data structures. However, TSDF provides a\nprojective or non-projective signed distance obtained directly from depth\nmeasurements that overestimate the Euclidean distance. Octrees, despite being\nmemory efficient, require tree traversal and can lead to increased runtime in\nlarge scenarios. Other representations based on the Gaussian Process (GP)\ndistance fields are appealing due to their probabilistic and continuous nature,\nbut the computational complexity is a concern. In this paper, we present an\nonline efficient mapping framework that seamlessly couples GP distance fields\nand the fast-access OpenVDB data structure. The key aspect is a latent Local GP\nSigned Distance Field (L-GPDF) contained in a local VDB structure that allows\nfast queries of the Euclidean distance, surface properties and their\nuncertainties for arbitrary points in the field of view. Probabilistic fusion\nis then performed by merging the inferred values of these points into a global\nVDB structure that is efficiently maintained over time. After fusion, the\nsurface mesh is recovered, and a global GP Signed Distance Field (G-GPDF) is\ngenerated and made available for downstream applications to query accurate\ndistance and gradients. A comparison with the state-of-the-art frameworks shows\nsuperior efficiency and accuracy of the inferred distance field and comparable\nreconstruction performance. https://github.com/UTS-RI/VDB_GPDF\n", "link": "http://arxiv.org/abs/2407.09649v3", "date": "2024-12-12", "relevancy": 2.8689, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6042}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5754}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VDB-GPDF%3A%20Online%20Gaussian%20Process%20Distance%20Field%20with%20VDB%20Structure&body=Title%3A%20VDB-GPDF%3A%20Online%20Gaussian%20Process%20Distance%20Field%20with%20VDB%20Structure%0AAuthor%3A%20Lan%20Wu%20and%20Cedric%20Le%20Gentil%20and%20Teresa%20Vidal-Calleja%0AAbstract%3A%20%20%20Robots%20reason%20about%20the%20environment%20through%20dedicated%20representations.%0APopular%20choices%20for%20dense%20representations%20exploit%20Truncated%20Signed%20Distance%0AFunctions%20%28TSDF%29%20and%20Octree%20data%20structures.%20However%2C%20TSDF%20provides%20a%0Aprojective%20or%20non-projective%20signed%20distance%20obtained%20directly%20from%20depth%0Ameasurements%20that%20overestimate%20the%20Euclidean%20distance.%20Octrees%2C%20despite%20being%0Amemory%20efficient%2C%20require%20tree%20traversal%20and%20can%20lead%20to%20increased%20runtime%20in%0Alarge%20scenarios.%20Other%20representations%20based%20on%20the%20Gaussian%20Process%20%28GP%29%0Adistance%20fields%20are%20appealing%20due%20to%20their%20probabilistic%20and%20continuous%20nature%2C%0Abut%20the%20computational%20complexity%20is%20a%20concern.%20In%20this%20paper%2C%20we%20present%20an%0Aonline%20efficient%20mapping%20framework%20that%20seamlessly%20couples%20GP%20distance%20fields%0Aand%20the%20fast-access%20OpenVDB%20data%20structure.%20The%20key%20aspect%20is%20a%20latent%20Local%20GP%0ASigned%20Distance%20Field%20%28L-GPDF%29%20contained%20in%20a%20local%20VDB%20structure%20that%20allows%0Afast%20queries%20of%20the%20Euclidean%20distance%2C%20surface%20properties%20and%20their%0Auncertainties%20for%20arbitrary%20points%20in%20the%20field%20of%20view.%20Probabilistic%20fusion%0Ais%20then%20performed%20by%20merging%20the%20inferred%20values%20of%20these%20points%20into%20a%20global%0AVDB%20structure%20that%20is%20efficiently%20maintained%20over%20time.%20After%20fusion%2C%20the%0Asurface%20mesh%20is%20recovered%2C%20and%20a%20global%20GP%20Signed%20Distance%20Field%20%28G-GPDF%29%20is%0Agenerated%20and%20made%20available%20for%20downstream%20applications%20to%20query%20accurate%0Adistance%20and%20gradients.%20A%20comparison%20with%20the%20state-of-the-art%20frameworks%20shows%0Asuperior%20efficiency%20and%20accuracy%20of%20the%20inferred%20distance%20field%20and%20comparable%0Areconstruction%20performance.%20https%3A//github.com/UTS-RI/VDB_GPDF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09649v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVDB-GPDF%253A%2520Online%2520Gaussian%2520Process%2520Distance%2520Field%2520with%2520VDB%2520Structure%26entry.906535625%3DLan%2520Wu%2520and%2520Cedric%2520Le%2520Gentil%2520and%2520Teresa%2520Vidal-Calleja%26entry.1292438233%3D%2520%2520Robots%2520reason%2520about%2520the%2520environment%2520through%2520dedicated%2520representations.%250APopular%2520choices%2520for%2520dense%2520representations%2520exploit%2520Truncated%2520Signed%2520Distance%250AFunctions%2520%2528TSDF%2529%2520and%2520Octree%2520data%2520structures.%2520However%252C%2520TSDF%2520provides%2520a%250Aprojective%2520or%2520non-projective%2520signed%2520distance%2520obtained%2520directly%2520from%2520depth%250Ameasurements%2520that%2520overestimate%2520the%2520Euclidean%2520distance.%2520Octrees%252C%2520despite%2520being%250Amemory%2520efficient%252C%2520require%2520tree%2520traversal%2520and%2520can%2520lead%2520to%2520increased%2520runtime%2520in%250Alarge%2520scenarios.%2520Other%2520representations%2520based%2520on%2520the%2520Gaussian%2520Process%2520%2528GP%2529%250Adistance%2520fields%2520are%2520appealing%2520due%2520to%2520their%2520probabilistic%2520and%2520continuous%2520nature%252C%250Abut%2520the%2520computational%2520complexity%2520is%2520a%2520concern.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%250Aonline%2520efficient%2520mapping%2520framework%2520that%2520seamlessly%2520couples%2520GP%2520distance%2520fields%250Aand%2520the%2520fast-access%2520OpenVDB%2520data%2520structure.%2520The%2520key%2520aspect%2520is%2520a%2520latent%2520Local%2520GP%250ASigned%2520Distance%2520Field%2520%2528L-GPDF%2529%2520contained%2520in%2520a%2520local%2520VDB%2520structure%2520that%2520allows%250Afast%2520queries%2520of%2520the%2520Euclidean%2520distance%252C%2520surface%2520properties%2520and%2520their%250Auncertainties%2520for%2520arbitrary%2520points%2520in%2520the%2520field%2520of%2520view.%2520Probabilistic%2520fusion%250Ais%2520then%2520performed%2520by%2520merging%2520the%2520inferred%2520values%2520of%2520these%2520points%2520into%2520a%2520global%250AVDB%2520structure%2520that%2520is%2520efficiently%2520maintained%2520over%2520time.%2520After%2520fusion%252C%2520the%250Asurface%2520mesh%2520is%2520recovered%252C%2520and%2520a%2520global%2520GP%2520Signed%2520Distance%2520Field%2520%2528G-GPDF%2529%2520is%250Agenerated%2520and%2520made%2520available%2520for%2520downstream%2520applications%2520to%2520query%2520accurate%250Adistance%2520and%2520gradients.%2520A%2520comparison%2520with%2520the%2520state-of-the-art%2520frameworks%2520shows%250Asuperior%2520efficiency%2520and%2520accuracy%2520of%2520the%2520inferred%2520distance%2520field%2520and%2520comparable%250Areconstruction%2520performance.%2520https%253A//github.com/UTS-RI/VDB_GPDF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09649v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VDB-GPDF%3A%20Online%20Gaussian%20Process%20Distance%20Field%20with%20VDB%20Structure&entry.906535625=Lan%20Wu%20and%20Cedric%20Le%20Gentil%20and%20Teresa%20Vidal-Calleja&entry.1292438233=%20%20Robots%20reason%20about%20the%20environment%20through%20dedicated%20representations.%0APopular%20choices%20for%20dense%20representations%20exploit%20Truncated%20Signed%20Distance%0AFunctions%20%28TSDF%29%20and%20Octree%20data%20structures.%20However%2C%20TSDF%20provides%20a%0Aprojective%20or%20non-projective%20signed%20distance%20obtained%20directly%20from%20depth%0Ameasurements%20that%20overestimate%20the%20Euclidean%20distance.%20Octrees%2C%20despite%20being%0Amemory%20efficient%2C%20require%20tree%20traversal%20and%20can%20lead%20to%20increased%20runtime%20in%0Alarge%20scenarios.%20Other%20representations%20based%20on%20the%20Gaussian%20Process%20%28GP%29%0Adistance%20fields%20are%20appealing%20due%20to%20their%20probabilistic%20and%20continuous%20nature%2C%0Abut%20the%20computational%20complexity%20is%20a%20concern.%20In%20this%20paper%2C%20we%20present%20an%0Aonline%20efficient%20mapping%20framework%20that%20seamlessly%20couples%20GP%20distance%20fields%0Aand%20the%20fast-access%20OpenVDB%20data%20structure.%20The%20key%20aspect%20is%20a%20latent%20Local%20GP%0ASigned%20Distance%20Field%20%28L-GPDF%29%20contained%20in%20a%20local%20VDB%20structure%20that%20allows%0Afast%20queries%20of%20the%20Euclidean%20distance%2C%20surface%20properties%20and%20their%0Auncertainties%20for%20arbitrary%20points%20in%20the%20field%20of%20view.%20Probabilistic%20fusion%0Ais%20then%20performed%20by%20merging%20the%20inferred%20values%20of%20these%20points%20into%20a%20global%0AVDB%20structure%20that%20is%20efficiently%20maintained%20over%20time.%20After%20fusion%2C%20the%0Asurface%20mesh%20is%20recovered%2C%20and%20a%20global%20GP%20Signed%20Distance%20Field%20%28G-GPDF%29%20is%0Agenerated%20and%20made%20available%20for%20downstream%20applications%20to%20query%20accurate%0Adistance%20and%20gradients.%20A%20comparison%20with%20the%20state-of-the-art%20frameworks%20shows%0Asuperior%20efficiency%20and%20accuracy%20of%20the%20inferred%20distance%20field%20and%20comparable%0Areconstruction%20performance.%20https%3A//github.com/UTS-RI/VDB_GPDF%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09649v3&entry.124074799=Read"},
{"title": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders", "author": "Fiona Ryan and Ajay Bati and Sangmin Lee and Daniel Bolya and Judy Hoffman and James M. Rehg", "abstract": "  We address the problem of gaze target estimation, which aims to predict where\na person is looking in a scene. Predicting a person's gaze target requires\nreasoning both about the person's appearance and the contents of the scene.\nPrior works have developed increasingly complex, hand-crafted pipelines for\ngaze target estimation that carefully fuse features from separate scene\nencoders, head encoders, and auxiliary models for signals like depth and pose.\nMotivated by the success of general-purpose feature extractors on a variety of\nvisual tasks, we propose Gaze-LLE, a novel transformer framework that\nstreamlines gaze target estimation by leveraging features from a frozen DINOv2\nencoder. We extract a single feature representation for the scene, and apply a\nperson-specific positional prompt to decode gaze with a lightweight module. We\ndemonstrate state-of-the-art performance across several gaze benchmarks and\nprovide extensive analysis to validate our design choices. Our code is\navailable at: http://github.com/fkryan/gazelle .\n", "link": "http://arxiv.org/abs/2412.09586v1", "date": "2024-12-12", "relevancy": 2.8655, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze-LLE%3A%20Gaze%20Target%20Estimation%20via%20Large-Scale%20Learned%20Encoders&body=Title%3A%20Gaze-LLE%3A%20Gaze%20Target%20Estimation%20via%20Large-Scale%20Learned%20Encoders%0AAuthor%3A%20Fiona%20Ryan%20and%20Ajay%20Bati%20and%20Sangmin%20Lee%20and%20Daniel%20Bolya%20and%20Judy%20Hoffman%20and%20James%20M.%20Rehg%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20gaze%20target%20estimation%2C%20which%20aims%20to%20predict%20where%0Aa%20person%20is%20looking%20in%20a%20scene.%20Predicting%20a%20person%27s%20gaze%20target%20requires%0Areasoning%20both%20about%20the%20person%27s%20appearance%20and%20the%20contents%20of%20the%20scene.%0APrior%20works%20have%20developed%20increasingly%20complex%2C%20hand-crafted%20pipelines%20for%0Agaze%20target%20estimation%20that%20carefully%20fuse%20features%20from%20separate%20scene%0Aencoders%2C%20head%20encoders%2C%20and%20auxiliary%20models%20for%20signals%20like%20depth%20and%20pose.%0AMotivated%20by%20the%20success%20of%20general-purpose%20feature%20extractors%20on%20a%20variety%20of%0Avisual%20tasks%2C%20we%20propose%20Gaze-LLE%2C%20a%20novel%20transformer%20framework%20that%0Astreamlines%20gaze%20target%20estimation%20by%20leveraging%20features%20from%20a%20frozen%20DINOv2%0Aencoder.%20We%20extract%20a%20single%20feature%20representation%20for%20the%20scene%2C%20and%20apply%20a%0Aperson-specific%20positional%20prompt%20to%20decode%20gaze%20with%20a%20lightweight%20module.%20We%0Ademonstrate%20state-of-the-art%20performance%20across%20several%20gaze%20benchmarks%20and%0Aprovide%20extensive%20analysis%20to%20validate%20our%20design%20choices.%20Our%20code%20is%0Aavailable%20at%3A%20http%3A//github.com/fkryan/gazelle%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze-LLE%253A%2520Gaze%2520Target%2520Estimation%2520via%2520Large-Scale%2520Learned%2520Encoders%26entry.906535625%3DFiona%2520Ryan%2520and%2520Ajay%2520Bati%2520and%2520Sangmin%2520Lee%2520and%2520Daniel%2520Bolya%2520and%2520Judy%2520Hoffman%2520and%2520James%2520M.%2520Rehg%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520gaze%2520target%2520estimation%252C%2520which%2520aims%2520to%2520predict%2520where%250Aa%2520person%2520is%2520looking%2520in%2520a%2520scene.%2520Predicting%2520a%2520person%2527s%2520gaze%2520target%2520requires%250Areasoning%2520both%2520about%2520the%2520person%2527s%2520appearance%2520and%2520the%2520contents%2520of%2520the%2520scene.%250APrior%2520works%2520have%2520developed%2520increasingly%2520complex%252C%2520hand-crafted%2520pipelines%2520for%250Agaze%2520target%2520estimation%2520that%2520carefully%2520fuse%2520features%2520from%2520separate%2520scene%250Aencoders%252C%2520head%2520encoders%252C%2520and%2520auxiliary%2520models%2520for%2520signals%2520like%2520depth%2520and%2520pose.%250AMotivated%2520by%2520the%2520success%2520of%2520general-purpose%2520feature%2520extractors%2520on%2520a%2520variety%2520of%250Avisual%2520tasks%252C%2520we%2520propose%2520Gaze-LLE%252C%2520a%2520novel%2520transformer%2520framework%2520that%250Astreamlines%2520gaze%2520target%2520estimation%2520by%2520leveraging%2520features%2520from%2520a%2520frozen%2520DINOv2%250Aencoder.%2520We%2520extract%2520a%2520single%2520feature%2520representation%2520for%2520the%2520scene%252C%2520and%2520apply%2520a%250Aperson-specific%2520positional%2520prompt%2520to%2520decode%2520gaze%2520with%2520a%2520lightweight%2520module.%2520We%250Ademonstrate%2520state-of-the-art%2520performance%2520across%2520several%2520gaze%2520benchmarks%2520and%250Aprovide%2520extensive%2520analysis%2520to%2520validate%2520our%2520design%2520choices.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520http%253A//github.com/fkryan/gazelle%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze-LLE%3A%20Gaze%20Target%20Estimation%20via%20Large-Scale%20Learned%20Encoders&entry.906535625=Fiona%20Ryan%20and%20Ajay%20Bati%20and%20Sangmin%20Lee%20and%20Daniel%20Bolya%20and%20Judy%20Hoffman%20and%20James%20M.%20Rehg&entry.1292438233=%20%20We%20address%20the%20problem%20of%20gaze%20target%20estimation%2C%20which%20aims%20to%20predict%20where%0Aa%20person%20is%20looking%20in%20a%20scene.%20Predicting%20a%20person%27s%20gaze%20target%20requires%0Areasoning%20both%20about%20the%20person%27s%20appearance%20and%20the%20contents%20of%20the%20scene.%0APrior%20works%20have%20developed%20increasingly%20complex%2C%20hand-crafted%20pipelines%20for%0Agaze%20target%20estimation%20that%20carefully%20fuse%20features%20from%20separate%20scene%0Aencoders%2C%20head%20encoders%2C%20and%20auxiliary%20models%20for%20signals%20like%20depth%20and%20pose.%0AMotivated%20by%20the%20success%20of%20general-purpose%20feature%20extractors%20on%20a%20variety%20of%0Avisual%20tasks%2C%20we%20propose%20Gaze-LLE%2C%20a%20novel%20transformer%20framework%20that%0Astreamlines%20gaze%20target%20estimation%20by%20leveraging%20features%20from%20a%20frozen%20DINOv2%0Aencoder.%20We%20extract%20a%20single%20feature%20representation%20for%20the%20scene%2C%20and%20apply%20a%0Aperson-specific%20positional%20prompt%20to%20decode%20gaze%20with%20a%20lightweight%20module.%20We%0Ademonstrate%20state-of-the-art%20performance%20across%20several%20gaze%20benchmarks%20and%0Aprovide%20extensive%20analysis%20to%20validate%20our%20design%20choices.%20Our%20code%20is%0Aavailable%20at%3A%20http%3A//github.com/fkryan/gazelle%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09586v1&entry.124074799=Read"},
{"title": "Causal Graphical Models for Vision-Language Compositional Understanding", "author": "Fiorenzo Parascandolo and Nicholas Moratelli and Enver Sangineto and Lorenzo Baraldi and Rita Cucchiara", "abstract": "  Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.\n", "link": "http://arxiv.org/abs/2412.09353v1", "date": "2024-12-12", "relevancy": 2.8595, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Graphical%20Models%20for%20Vision-Language%20Compositional%20Understanding&body=Title%3A%20Causal%20Graphical%20Models%20for%20Vision-Language%20Compositional%20Understanding%0AAuthor%3A%20Fiorenzo%20Parascandolo%20and%20Nicholas%20Moratelli%20and%20Enver%20Sangineto%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Recent%20work%20has%20empirically%20shown%20that%20Vision-Language%20Models%20%28VLMs%29%20struggle%0Ato%20fully%20understand%20the%20compositional%20properties%20of%20the%20human%20language%2C%20usually%0Amodeling%20an%20image%20caption%20as%20a%20%22bag%20of%20words%22.%20As%20a%20result%2C%20they%20perform%20poorly%0Aon%20compositional%20tasks%2C%20which%20require%20a%20deeper%20understanding%20of%20the%20different%0Aentities%20of%20a%20sentence%20%28subject%2C%20verb%2C%20etc.%29%20jointly%20with%20their%20mutual%0Arelationships%20in%20order%20to%20be%20solved.%20In%20this%20paper%2C%20we%20model%20the%20dependency%0Arelations%20among%20textual%20and%20visual%20tokens%20using%20a%20Causal%20Graphical%20Model%20%28CGM%29%2C%0Abuilt%20using%20a%20dependency%20parser%2C%20and%20we%20train%20a%20decoder%20conditioned%20by%20the%20VLM%0Avisual%20encoder.%20Differently%20from%20standard%20autoregressive%20or%20parallel%0Apredictions%2C%20our%20decoder%27s%20generative%20process%20is%20partially-ordered%20following%0Athe%20CGM%20structure.%20This%20structure%20encourages%20the%20decoder%20to%20learn%20only%20the%20main%0Acausal%20dependencies%20in%20a%20sentence%20discarding%20spurious%20correlations.%20Using%0Aextensive%20experiments%20on%20five%20compositional%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20all%20the%20state-of-the-art%20compositional%20approaches%20by%0Aa%20large%20margin%2C%20and%20it%20also%20improves%20over%20methods%20trained%20using%20much%20larger%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Graphical%2520Models%2520for%2520Vision-Language%2520Compositional%2520Understanding%26entry.906535625%3DFiorenzo%2520Parascandolo%2520and%2520Nicholas%2520Moratelli%2520and%2520Enver%2520Sangineto%2520and%2520Lorenzo%2520Baraldi%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520empirically%2520shown%2520that%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520struggle%250Ato%2520fully%2520understand%2520the%2520compositional%2520properties%2520of%2520the%2520human%2520language%252C%2520usually%250Amodeling%2520an%2520image%2520caption%2520as%2520a%2520%2522bag%2520of%2520words%2522.%2520As%2520a%2520result%252C%2520they%2520perform%2520poorly%250Aon%2520compositional%2520tasks%252C%2520which%2520require%2520a%2520deeper%2520understanding%2520of%2520the%2520different%250Aentities%2520of%2520a%2520sentence%2520%2528subject%252C%2520verb%252C%2520etc.%2529%2520jointly%2520with%2520their%2520mutual%250Arelationships%2520in%2520order%2520to%2520be%2520solved.%2520In%2520this%2520paper%252C%2520we%2520model%2520the%2520dependency%250Arelations%2520among%2520textual%2520and%2520visual%2520tokens%2520using%2520a%2520Causal%2520Graphical%2520Model%2520%2528CGM%2529%252C%250Abuilt%2520using%2520a%2520dependency%2520parser%252C%2520and%2520we%2520train%2520a%2520decoder%2520conditioned%2520by%2520the%2520VLM%250Avisual%2520encoder.%2520Differently%2520from%2520standard%2520autoregressive%2520or%2520parallel%250Apredictions%252C%2520our%2520decoder%2527s%2520generative%2520process%2520is%2520partially-ordered%2520following%250Athe%2520CGM%2520structure.%2520This%2520structure%2520encourages%2520the%2520decoder%2520to%2520learn%2520only%2520the%2520main%250Acausal%2520dependencies%2520in%2520a%2520sentence%2520discarding%2520spurious%2520correlations.%2520Using%250Aextensive%2520experiments%2520on%2520five%2520compositional%2520benchmarks%252C%2520we%2520show%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520all%2520the%2520state-of-the-art%2520compositional%2520approaches%2520by%250Aa%2520large%2520margin%252C%2520and%2520it%2520also%2520improves%2520over%2520methods%2520trained%2520using%2520much%2520larger%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Graphical%20Models%20for%20Vision-Language%20Compositional%20Understanding&entry.906535625=Fiorenzo%20Parascandolo%20and%20Nicholas%20Moratelli%20and%20Enver%20Sangineto%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Recent%20work%20has%20empirically%20shown%20that%20Vision-Language%20Models%20%28VLMs%29%20struggle%0Ato%20fully%20understand%20the%20compositional%20properties%20of%20the%20human%20language%2C%20usually%0Amodeling%20an%20image%20caption%20as%20a%20%22bag%20of%20words%22.%20As%20a%20result%2C%20they%20perform%20poorly%0Aon%20compositional%20tasks%2C%20which%20require%20a%20deeper%20understanding%20of%20the%20different%0Aentities%20of%20a%20sentence%20%28subject%2C%20verb%2C%20etc.%29%20jointly%20with%20their%20mutual%0Arelationships%20in%20order%20to%20be%20solved.%20In%20this%20paper%2C%20we%20model%20the%20dependency%0Arelations%20among%20textual%20and%20visual%20tokens%20using%20a%20Causal%20Graphical%20Model%20%28CGM%29%2C%0Abuilt%20using%20a%20dependency%20parser%2C%20and%20we%20train%20a%20decoder%20conditioned%20by%20the%20VLM%0Avisual%20encoder.%20Differently%20from%20standard%20autoregressive%20or%20parallel%0Apredictions%2C%20our%20decoder%27s%20generative%20process%20is%20partially-ordered%20following%0Athe%20CGM%20structure.%20This%20structure%20encourages%20the%20decoder%20to%20learn%20only%20the%20main%0Acausal%20dependencies%20in%20a%20sentence%20discarding%20spurious%20correlations.%20Using%0Aextensive%20experiments%20on%20five%20compositional%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20all%20the%20state-of-the-art%20compositional%20approaches%20by%0Aa%20large%20margin%2C%20and%20it%20also%20improves%20over%20methods%20trained%20using%20much%20larger%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09353v1&entry.124074799=Read"},
{"title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for\n  Long-term Streaming Video and Audio Interactions", "author": "Pan Zhang and Xiaoyi Dong and Yuhang Cao and Yuhang Zang and Rui Qian and Xilin Wei and Lin Chen and Yifei Li and Junbo Niu and Shuangrui Ding and Qipeng Guo and Haodong Duan and Xin Chen and Han Lv and Zheng Nie and Min Zhang and Bin Wang and Wenwei Zhang and Xinyue Zhang and Jiaye Ge and Wei Li and Jingwen Li and Zhongying Tu and Conghui He and Xingcheng Zhang and Kai Chen and Yu Qiao and Dahua Lin and Jiaqi Wang", "abstract": "  Creating AI systems that can interact with environments over long periods,\nsimilar to human cognition, has been a longstanding research goal. Recent\nadvancements in multimodal large language models (MLLMs) have made significant\nstrides in open-world understanding. However, the challenge of continuous and\nsimultaneous streaming perception, memory, and reasoning remains largely\nunexplored. Current MLLMs are constrained by their sequence-to-sequence\narchitecture, which limits their ability to process inputs and generate\nresponses simultaneously, akin to being unable to think while perceiving.\nFurthermore, relying on long contexts to store historical data is impractical\nfor long-term interactions, as retaining all information becomes costly and\ninefficient. Therefore, rather than relying on a single foundation model to\nperform all functions, this project draws inspiration from the concept of the\nSpecialized Generalist AI and introduces disentangled streaming perception,\nreasoning, and memory mechanisms, enabling real-time interaction with streaming\nvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive\n(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:\nProcesses multimodal information in real-time, storing key details in memory\nand triggering reasoning in response to user queries. (2) Multi-modal Long\nMemory Module: Integrates short-term and long-term memory, compressing\nshort-term memories into long-term ones for efficient retrieval and improved\naccuracy. (3) Reasoning Module: Responds to queries and executes reasoning\ntasks, coordinating with the perception and memory modules. This project\nsimulates human-like cognition, enabling multimodal large language models to\nprovide continuous and adaptive service over time.\n", "link": "http://arxiv.org/abs/2412.09596v1", "date": "2024-12-12", "relevancy": 2.8129, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternLM-XComposer2.5-OmniLive%3A%20A%20Comprehensive%20Multimodal%20System%20for%0A%20%20Long-term%20Streaming%20Video%20and%20Audio%20Interactions&body=Title%3A%20InternLM-XComposer2.5-OmniLive%3A%20A%20Comprehensive%20Multimodal%20System%20for%0A%20%20Long-term%20Streaming%20Video%20and%20Audio%20Interactions%0AAuthor%3A%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Yuhang%20Zang%20and%20Rui%20Qian%20and%20Xilin%20Wei%20and%20Lin%20Chen%20and%20Yifei%20Li%20and%20Junbo%20Niu%20and%20Shuangrui%20Ding%20and%20Qipeng%20Guo%20and%20Haodong%20Duan%20and%20Xin%20Chen%20and%20Han%20Lv%20and%20Zheng%20Nie%20and%20Min%20Zhang%20and%20Bin%20Wang%20and%20Wenwei%20Zhang%20and%20Xinyue%20Zhang%20and%20Jiaye%20Ge%20and%20Wei%20Li%20and%20Jingwen%20Li%20and%20Zhongying%20Tu%20and%20Conghui%20He%20and%20Xingcheng%20Zhang%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Creating%20AI%20systems%20that%20can%20interact%20with%20environments%20over%20long%20periods%2C%0Asimilar%20to%20human%20cognition%2C%20has%20been%20a%20longstanding%20research%20goal.%20Recent%0Aadvancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%0Astrides%20in%20open-world%20understanding.%20However%2C%20the%20challenge%20of%20continuous%20and%0Asimultaneous%20streaming%20perception%2C%20memory%2C%20and%20reasoning%20remains%20largely%0Aunexplored.%20Current%20MLLMs%20are%20constrained%20by%20their%20sequence-to-sequence%0Aarchitecture%2C%20which%20limits%20their%20ability%20to%20process%20inputs%20and%20generate%0Aresponses%20simultaneously%2C%20akin%20to%20being%20unable%20to%20think%20while%20perceiving.%0AFurthermore%2C%20relying%20on%20long%20contexts%20to%20store%20historical%20data%20is%20impractical%0Afor%20long-term%20interactions%2C%20as%20retaining%20all%20information%20becomes%20costly%20and%0Ainefficient.%20Therefore%2C%20rather%20than%20relying%20on%20a%20single%20foundation%20model%20to%0Aperform%20all%20functions%2C%20this%20project%20draws%20inspiration%20from%20the%20concept%20of%20the%0ASpecialized%20Generalist%20AI%20and%20introduces%20disentangled%20streaming%20perception%2C%0Areasoning%2C%20and%20memory%20mechanisms%2C%20enabling%20real-time%20interaction%20with%20streaming%0Avideo%20and%20audio%20input.%20The%20proposed%20framework%20InternLM-XComposer2.5-OmniLive%0A%28IXC2.5-OL%29%20consists%20of%20three%20key%20modules%3A%20%281%29%20Streaming%20Perception%20Module%3A%0AProcesses%20multimodal%20information%20in%20real-time%2C%20storing%20key%20details%20in%20memory%0Aand%20triggering%20reasoning%20in%20response%20to%20user%20queries.%20%282%29%20Multi-modal%20Long%0AMemory%20Module%3A%20Integrates%20short-term%20and%20long-term%20memory%2C%20compressing%0Ashort-term%20memories%20into%20long-term%20ones%20for%20efficient%20retrieval%20and%20improved%0Aaccuracy.%20%283%29%20Reasoning%20Module%3A%20Responds%20to%20queries%20and%20executes%20reasoning%0Atasks%2C%20coordinating%20with%20the%20perception%20and%20memory%20modules.%20This%20project%0Asimulates%20human-like%20cognition%2C%20enabling%20multimodal%20large%20language%20models%20to%0Aprovide%20continuous%20and%20adaptive%20service%20over%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternLM-XComposer2.5-OmniLive%253A%2520A%2520Comprehensive%2520Multimodal%2520System%2520for%250A%2520%2520Long-term%2520Streaming%2520Video%2520and%2520Audio%2520Interactions%26entry.906535625%3DPan%2520Zhang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Cao%2520and%2520Yuhang%2520Zang%2520and%2520Rui%2520Qian%2520and%2520Xilin%2520Wei%2520and%2520Lin%2520Chen%2520and%2520Yifei%2520Li%2520and%2520Junbo%2520Niu%2520and%2520Shuangrui%2520Ding%2520and%2520Qipeng%2520Guo%2520and%2520Haodong%2520Duan%2520and%2520Xin%2520Chen%2520and%2520Han%2520Lv%2520and%2520Zheng%2520Nie%2520and%2520Min%2520Zhang%2520and%2520Bin%2520Wang%2520and%2520Wenwei%2520Zhang%2520and%2520Xinyue%2520Zhang%2520and%2520Jiaye%2520Ge%2520and%2520Wei%2520Li%2520and%2520Jingwen%2520Li%2520and%2520Zhongying%2520Tu%2520and%2520Conghui%2520He%2520and%2520Xingcheng%2520Zhang%2520and%2520Kai%2520Chen%2520and%2520Yu%2520Qiao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Creating%2520AI%2520systems%2520that%2520can%2520interact%2520with%2520environments%2520over%2520long%2520periods%252C%250Asimilar%2520to%2520human%2520cognition%252C%2520has%2520been%2520a%2520longstanding%2520research%2520goal.%2520Recent%250Aadvancements%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520made%2520significant%250Astrides%2520in%2520open-world%2520understanding.%2520However%252C%2520the%2520challenge%2520of%2520continuous%2520and%250Asimultaneous%2520streaming%2520perception%252C%2520memory%252C%2520and%2520reasoning%2520remains%2520largely%250Aunexplored.%2520Current%2520MLLMs%2520are%2520constrained%2520by%2520their%2520sequence-to-sequence%250Aarchitecture%252C%2520which%2520limits%2520their%2520ability%2520to%2520process%2520inputs%2520and%2520generate%250Aresponses%2520simultaneously%252C%2520akin%2520to%2520being%2520unable%2520to%2520think%2520while%2520perceiving.%250AFurthermore%252C%2520relying%2520on%2520long%2520contexts%2520to%2520store%2520historical%2520data%2520is%2520impractical%250Afor%2520long-term%2520interactions%252C%2520as%2520retaining%2520all%2520information%2520becomes%2520costly%2520and%250Ainefficient.%2520Therefore%252C%2520rather%2520than%2520relying%2520on%2520a%2520single%2520foundation%2520model%2520to%250Aperform%2520all%2520functions%252C%2520this%2520project%2520draws%2520inspiration%2520from%2520the%2520concept%2520of%2520the%250ASpecialized%2520Generalist%2520AI%2520and%2520introduces%2520disentangled%2520streaming%2520perception%252C%250Areasoning%252C%2520and%2520memory%2520mechanisms%252C%2520enabling%2520real-time%2520interaction%2520with%2520streaming%250Avideo%2520and%2520audio%2520input.%2520The%2520proposed%2520framework%2520InternLM-XComposer2.5-OmniLive%250A%2528IXC2.5-OL%2529%2520consists%2520of%2520three%2520key%2520modules%253A%2520%25281%2529%2520Streaming%2520Perception%2520Module%253A%250AProcesses%2520multimodal%2520information%2520in%2520real-time%252C%2520storing%2520key%2520details%2520in%2520memory%250Aand%2520triggering%2520reasoning%2520in%2520response%2520to%2520user%2520queries.%2520%25282%2529%2520Multi-modal%2520Long%250AMemory%2520Module%253A%2520Integrates%2520short-term%2520and%2520long-term%2520memory%252C%2520compressing%250Ashort-term%2520memories%2520into%2520long-term%2520ones%2520for%2520efficient%2520retrieval%2520and%2520improved%250Aaccuracy.%2520%25283%2529%2520Reasoning%2520Module%253A%2520Responds%2520to%2520queries%2520and%2520executes%2520reasoning%250Atasks%252C%2520coordinating%2520with%2520the%2520perception%2520and%2520memory%2520modules.%2520This%2520project%250Asimulates%2520human-like%2520cognition%252C%2520enabling%2520multimodal%2520large%2520language%2520models%2520to%250Aprovide%2520continuous%2520and%2520adaptive%2520service%2520over%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternLM-XComposer2.5-OmniLive%3A%20A%20Comprehensive%20Multimodal%20System%20for%0A%20%20Long-term%20Streaming%20Video%20and%20Audio%20Interactions&entry.906535625=Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Yuhang%20Zang%20and%20Rui%20Qian%20and%20Xilin%20Wei%20and%20Lin%20Chen%20and%20Yifei%20Li%20and%20Junbo%20Niu%20and%20Shuangrui%20Ding%20and%20Qipeng%20Guo%20and%20Haodong%20Duan%20and%20Xin%20Chen%20and%20Han%20Lv%20and%20Zheng%20Nie%20and%20Min%20Zhang%20and%20Bin%20Wang%20and%20Wenwei%20Zhang%20and%20Xinyue%20Zhang%20and%20Jiaye%20Ge%20and%20Wei%20Li%20and%20Jingwen%20Li%20and%20Zhongying%20Tu%20and%20Conghui%20He%20and%20Xingcheng%20Zhang%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Creating%20AI%20systems%20that%20can%20interact%20with%20environments%20over%20long%20periods%2C%0Asimilar%20to%20human%20cognition%2C%20has%20been%20a%20longstanding%20research%20goal.%20Recent%0Aadvancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%0Astrides%20in%20open-world%20understanding.%20However%2C%20the%20challenge%20of%20continuous%20and%0Asimultaneous%20streaming%20perception%2C%20memory%2C%20and%20reasoning%20remains%20largely%0Aunexplored.%20Current%20MLLMs%20are%20constrained%20by%20their%20sequence-to-sequence%0Aarchitecture%2C%20which%20limits%20their%20ability%20to%20process%20inputs%20and%20generate%0Aresponses%20simultaneously%2C%20akin%20to%20being%20unable%20to%20think%20while%20perceiving.%0AFurthermore%2C%20relying%20on%20long%20contexts%20to%20store%20historical%20data%20is%20impractical%0Afor%20long-term%20interactions%2C%20as%20retaining%20all%20information%20becomes%20costly%20and%0Ainefficient.%20Therefore%2C%20rather%20than%20relying%20on%20a%20single%20foundation%20model%20to%0Aperform%20all%20functions%2C%20this%20project%20draws%20inspiration%20from%20the%20concept%20of%20the%0ASpecialized%20Generalist%20AI%20and%20introduces%20disentangled%20streaming%20perception%2C%0Areasoning%2C%20and%20memory%20mechanisms%2C%20enabling%20real-time%20interaction%20with%20streaming%0Avideo%20and%20audio%20input.%20The%20proposed%20framework%20InternLM-XComposer2.5-OmniLive%0A%28IXC2.5-OL%29%20consists%20of%20three%20key%20modules%3A%20%281%29%20Streaming%20Perception%20Module%3A%0AProcesses%20multimodal%20information%20in%20real-time%2C%20storing%20key%20details%20in%20memory%0Aand%20triggering%20reasoning%20in%20response%20to%20user%20queries.%20%282%29%20Multi-modal%20Long%0AMemory%20Module%3A%20Integrates%20short-term%20and%20long-term%20memory%2C%20compressing%0Ashort-term%20memories%20into%20long-term%20ones%20for%20efficient%20retrieval%20and%20improved%0Aaccuracy.%20%283%29%20Reasoning%20Module%3A%20Responds%20to%20queries%20and%20executes%20reasoning%0Atasks%2C%20coordinating%20with%20the%20perception%20and%20memory%20modules.%20This%20project%0Asimulates%20human-like%20cognition%2C%20enabling%20multimodal%20large%20language%20models%20to%0Aprovide%20continuous%20and%20adaptive%20service%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09596v1&entry.124074799=Read"},
{"title": "Game4Loc: A UAV Geo-Localization Benchmark from Game Data", "author": "Yuxiang Ji and Boyong He and Zhuoyue Tan and Liaoni Wu", "abstract": "  The vision-based geo-localization technology for UAV, serving as a secondary\nsource of GPS information in addition to the global navigation satellite\nsystems (GNSS), can still operate independently in the GPS-denied environment.\nRecent deep learning based methods attribute this as the task of image matching\nand retrieval. By retrieving drone-view images in geo-tagged satellite image\ndatabase, approximate localization information can be obtained. However, due to\nhigh costs and privacy concerns, it is usually difficult to obtain large\nquantities of drone-view images from a continuous area. Existing drone-view\ndatasets are mostly composed of small-scale aerial photography with a strong\nassumption that there exists a perfect one-to-one aligned reference image for\nany query, leaving a significant gap from the practical localization scenario.\nIn this work, we construct a large-range contiguous area UAV geo-localization\ndataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,\nand targets using modern computer games. Based on this dataset, we introduce a\nmore practical UAV geo-localization task including partial matches of\ncross-view paired data, and expand the image-level retrieval to the actual\nlocalization in terms of distance (meters). For the construction of drone-view\nand satellite-view pairs, we adopt a weight-based contrastive learning\napproach, which allows for effective learning while avoiding additional\npost-processing matching steps. Experiments demonstrate the effectiveness of\nour data and training method for UAV geo-localization, as well as the\ngeneralization capabilities to real-world scenarios.\n", "link": "http://arxiv.org/abs/2409.16925v2", "date": "2024-12-12", "relevancy": 2.7982, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5928}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Game4Loc%3A%20A%20UAV%20Geo-Localization%20Benchmark%20from%20Game%20Data&body=Title%3A%20Game4Loc%3A%20A%20UAV%20Geo-Localization%20Benchmark%20from%20Game%20Data%0AAuthor%3A%20Yuxiang%20Ji%20and%20Boyong%20He%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu%0AAbstract%3A%20%20%20The%20vision-based%20geo-localization%20technology%20for%20UAV%2C%20serving%20as%20a%20secondary%0Asource%20of%20GPS%20information%20in%20addition%20to%20the%20global%20navigation%20satellite%0Asystems%20%28GNSS%29%2C%20can%20still%20operate%20independently%20in%20the%20GPS-denied%20environment.%0ARecent%20deep%20learning%20based%20methods%20attribute%20this%20as%20the%20task%20of%20image%20matching%0Aand%20retrieval.%20By%20retrieving%20drone-view%20images%20in%20geo-tagged%20satellite%20image%0Adatabase%2C%20approximate%20localization%20information%20can%20be%20obtained.%20However%2C%20due%20to%0Ahigh%20costs%20and%20privacy%20concerns%2C%20it%20is%20usually%20difficult%20to%20obtain%20large%0Aquantities%20of%20drone-view%20images%20from%20a%20continuous%20area.%20Existing%20drone-view%0Adatasets%20are%20mostly%20composed%20of%20small-scale%20aerial%20photography%20with%20a%20strong%0Aassumption%20that%20there%20exists%20a%20perfect%20one-to-one%20aligned%20reference%20image%20for%0Aany%20query%2C%20leaving%20a%20significant%20gap%20from%20the%20practical%20localization%20scenario.%0AIn%20this%20work%2C%20we%20construct%20a%20large-range%20contiguous%20area%20UAV%20geo-localization%0Adataset%20named%20GTA-UAV%2C%20featuring%20multiple%20flight%20altitudes%2C%20attitudes%2C%20scenes%2C%0Aand%20targets%20using%20modern%20computer%20games.%20Based%20on%20this%20dataset%2C%20we%20introduce%20a%0Amore%20practical%20UAV%20geo-localization%20task%20including%20partial%20matches%20of%0Across-view%20paired%20data%2C%20and%20expand%20the%20image-level%20retrieval%20to%20the%20actual%0Alocalization%20in%20terms%20of%20distance%20%28meters%29.%20For%20the%20construction%20of%20drone-view%0Aand%20satellite-view%20pairs%2C%20we%20adopt%20a%20weight-based%20contrastive%20learning%0Aapproach%2C%20which%20allows%20for%20effective%20learning%20while%20avoiding%20additional%0Apost-processing%20matching%20steps.%20Experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20data%20and%20training%20method%20for%20UAV%20geo-localization%2C%20as%20well%20as%20the%0Ageneralization%20capabilities%20to%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGame4Loc%253A%2520A%2520UAV%2520Geo-Localization%2520Benchmark%2520from%2520Game%2520Data%26entry.906535625%3DYuxiang%2520Ji%2520and%2520Boyong%2520He%2520and%2520Zhuoyue%2520Tan%2520and%2520Liaoni%2520Wu%26entry.1292438233%3D%2520%2520The%2520vision-based%2520geo-localization%2520technology%2520for%2520UAV%252C%2520serving%2520as%2520a%2520secondary%250Asource%2520of%2520GPS%2520information%2520in%2520addition%2520to%2520the%2520global%2520navigation%2520satellite%250Asystems%2520%2528GNSS%2529%252C%2520can%2520still%2520operate%2520independently%2520in%2520the%2520GPS-denied%2520environment.%250ARecent%2520deep%2520learning%2520based%2520methods%2520attribute%2520this%2520as%2520the%2520task%2520of%2520image%2520matching%250Aand%2520retrieval.%2520By%2520retrieving%2520drone-view%2520images%2520in%2520geo-tagged%2520satellite%2520image%250Adatabase%252C%2520approximate%2520localization%2520information%2520can%2520be%2520obtained.%2520However%252C%2520due%2520to%250Ahigh%2520costs%2520and%2520privacy%2520concerns%252C%2520it%2520is%2520usually%2520difficult%2520to%2520obtain%2520large%250Aquantities%2520of%2520drone-view%2520images%2520from%2520a%2520continuous%2520area.%2520Existing%2520drone-view%250Adatasets%2520are%2520mostly%2520composed%2520of%2520small-scale%2520aerial%2520photography%2520with%2520a%2520strong%250Aassumption%2520that%2520there%2520exists%2520a%2520perfect%2520one-to-one%2520aligned%2520reference%2520image%2520for%250Aany%2520query%252C%2520leaving%2520a%2520significant%2520gap%2520from%2520the%2520practical%2520localization%2520scenario.%250AIn%2520this%2520work%252C%2520we%2520construct%2520a%2520large-range%2520contiguous%2520area%2520UAV%2520geo-localization%250Adataset%2520named%2520GTA-UAV%252C%2520featuring%2520multiple%2520flight%2520altitudes%252C%2520attitudes%252C%2520scenes%252C%250Aand%2520targets%2520using%2520modern%2520computer%2520games.%2520Based%2520on%2520this%2520dataset%252C%2520we%2520introduce%2520a%250Amore%2520practical%2520UAV%2520geo-localization%2520task%2520including%2520partial%2520matches%2520of%250Across-view%2520paired%2520data%252C%2520and%2520expand%2520the%2520image-level%2520retrieval%2520to%2520the%2520actual%250Alocalization%2520in%2520terms%2520of%2520distance%2520%2528meters%2529.%2520For%2520the%2520construction%2520of%2520drone-view%250Aand%2520satellite-view%2520pairs%252C%2520we%2520adopt%2520a%2520weight-based%2520contrastive%2520learning%250Aapproach%252C%2520which%2520allows%2520for%2520effective%2520learning%2520while%2520avoiding%2520additional%250Apost-processing%2520matching%2520steps.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520data%2520and%2520training%2520method%2520for%2520UAV%2520geo-localization%252C%2520as%2520well%2520as%2520the%250Ageneralization%2520capabilities%2520to%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Game4Loc%3A%20A%20UAV%20Geo-Localization%20Benchmark%20from%20Game%20Data&entry.906535625=Yuxiang%20Ji%20and%20Boyong%20He%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu&entry.1292438233=%20%20The%20vision-based%20geo-localization%20technology%20for%20UAV%2C%20serving%20as%20a%20secondary%0Asource%20of%20GPS%20information%20in%20addition%20to%20the%20global%20navigation%20satellite%0Asystems%20%28GNSS%29%2C%20can%20still%20operate%20independently%20in%20the%20GPS-denied%20environment.%0ARecent%20deep%20learning%20based%20methods%20attribute%20this%20as%20the%20task%20of%20image%20matching%0Aand%20retrieval.%20By%20retrieving%20drone-view%20images%20in%20geo-tagged%20satellite%20image%0Adatabase%2C%20approximate%20localization%20information%20can%20be%20obtained.%20However%2C%20due%20to%0Ahigh%20costs%20and%20privacy%20concerns%2C%20it%20is%20usually%20difficult%20to%20obtain%20large%0Aquantities%20of%20drone-view%20images%20from%20a%20continuous%20area.%20Existing%20drone-view%0Adatasets%20are%20mostly%20composed%20of%20small-scale%20aerial%20photography%20with%20a%20strong%0Aassumption%20that%20there%20exists%20a%20perfect%20one-to-one%20aligned%20reference%20image%20for%0Aany%20query%2C%20leaving%20a%20significant%20gap%20from%20the%20practical%20localization%20scenario.%0AIn%20this%20work%2C%20we%20construct%20a%20large-range%20contiguous%20area%20UAV%20geo-localization%0Adataset%20named%20GTA-UAV%2C%20featuring%20multiple%20flight%20altitudes%2C%20attitudes%2C%20scenes%2C%0Aand%20targets%20using%20modern%20computer%20games.%20Based%20on%20this%20dataset%2C%20we%20introduce%20a%0Amore%20practical%20UAV%20geo-localization%20task%20including%20partial%20matches%20of%0Across-view%20paired%20data%2C%20and%20expand%20the%20image-level%20retrieval%20to%20the%20actual%0Alocalization%20in%20terms%20of%20distance%20%28meters%29.%20For%20the%20construction%20of%20drone-view%0Aand%20satellite-view%20pairs%2C%20we%20adopt%20a%20weight-based%20contrastive%20learning%0Aapproach%2C%20which%20allows%20for%20effective%20learning%20while%20avoiding%20additional%0Apost-processing%20matching%20steps.%20Experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20data%20and%20training%20method%20for%20UAV%20geo-localization%2C%20as%20well%20as%20the%0Ageneralization%20capabilities%20to%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16925v2&entry.124074799=Read"},
{"title": "Model Developmental Safety: A Retention-Centric Method and Applications\n  in Vision-Language Models", "author": "Gang Li and Wendi Yu and Yao Yao and Wei Tong and Yingbin Liang and Qihang Lin and Tianbao Yang", "abstract": "  In the real world, a learning-enabled system usually undergoes multiple\ncycles of model development to enhance the system's ability to handle difficult\nor emerging tasks. This continual model development process raises a\nsignificant issue that the model development for acquiring new or improving\nexisting capabilities may inadvertently lose capabilities of the old model,\nalso known as catastrophic forgetting. Existing continual learning studies\nfocus on mitigating catastrophic forgetting by trading off performance on\nprevious tasks and new tasks to ensure good average performance. However, they\nare inadequate for many applications especially in safety-critical domains, as\nfailure to strictly preserve the good performance of the old model not only\nintroduces safety risks and uncertainties but also imposes substantial expenses\nin the re-improving and re-validation of existing properties. To address this\nissue, we introduce model developmental safety as a guarantee of a learning\nsystem such that in the model development process the new model should strictly\npreserve the existing protected capabilities of the old model while improving\nits performance on target tasks. To ensure the model developmental safety, we\npresent a retention-centric framework by formulating the model developmental\nsafety as data-dependent constraints. Under this framework, we study how to\ndevelop a pretrained vision-language model, specifically the CLIP model, for\nacquiring new capabilities or improving existing capabilities of image\nclassification. We propose an efficient constrained optimization algorithm with\ntheoretical guarantee and use its insights to finetune a CLIP model with\ntask-dependent heads for promoting the model developmental safety. Our\nexperiments on improving vision perception capabilities on autonomous driving\nand scene recognition datasets demonstrate the efficacy of the proposed\napproach.\n", "link": "http://arxiv.org/abs/2410.03955v3", "date": "2024-12-12", "relevancy": 2.7981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Developmental%20Safety%3A%20A%20Retention-Centric%20Method%20and%20Applications%0A%20%20in%20Vision-Language%20Models&body=Title%3A%20Model%20Developmental%20Safety%3A%20A%20Retention-Centric%20Method%20and%20Applications%0A%20%20in%20Vision-Language%20Models%0AAuthor%3A%20Gang%20Li%20and%20Wendi%20Yu%20and%20Yao%20Yao%20and%20Wei%20Tong%20and%20Yingbin%20Liang%20and%20Qihang%20Lin%20and%20Tianbao%20Yang%0AAbstract%3A%20%20%20In%20the%20real%20world%2C%20a%20learning-enabled%20system%20usually%20undergoes%20multiple%0Acycles%20of%20model%20development%20to%20enhance%20the%20system%27s%20ability%20to%20handle%20difficult%0Aor%20emerging%20tasks.%20This%20continual%20model%20development%20process%20raises%20a%0Asignificant%20issue%20that%20the%20model%20development%20for%20acquiring%20new%20or%20improving%0Aexisting%20capabilities%20may%20inadvertently%20lose%20capabilities%20of%20the%20old%20model%2C%0Aalso%20known%20as%20catastrophic%20forgetting.%20Existing%20continual%20learning%20studies%0Afocus%20on%20mitigating%20catastrophic%20forgetting%20by%20trading%20off%20performance%20on%0Aprevious%20tasks%20and%20new%20tasks%20to%20ensure%20good%20average%20performance.%20However%2C%20they%0Aare%20inadequate%20for%20many%20applications%20especially%20in%20safety-critical%20domains%2C%20as%0Afailure%20to%20strictly%20preserve%20the%20good%20performance%20of%20the%20old%20model%20not%20only%0Aintroduces%20safety%20risks%20and%20uncertainties%20but%20also%20imposes%20substantial%20expenses%0Ain%20the%20re-improving%20and%20re-validation%20of%20existing%20properties.%20To%20address%20this%0Aissue%2C%20we%20introduce%20model%20developmental%20safety%20as%20a%20guarantee%20of%20a%20learning%0Asystem%20such%20that%20in%20the%20model%20development%20process%20the%20new%20model%20should%20strictly%0Apreserve%20the%20existing%20protected%20capabilities%20of%20the%20old%20model%20while%20improving%0Aits%20performance%20on%20target%20tasks.%20To%20ensure%20the%20model%20developmental%20safety%2C%20we%0Apresent%20a%20retention-centric%20framework%20by%20formulating%20the%20model%20developmental%0Asafety%20as%20data-dependent%20constraints.%20Under%20this%20framework%2C%20we%20study%20how%20to%0Adevelop%20a%20pretrained%20vision-language%20model%2C%20specifically%20the%20CLIP%20model%2C%20for%0Aacquiring%20new%20capabilities%20or%20improving%20existing%20capabilities%20of%20image%0Aclassification.%20We%20propose%20an%20efficient%20constrained%20optimization%20algorithm%20with%0Atheoretical%20guarantee%20and%20use%20its%20insights%20to%20finetune%20a%20CLIP%20model%20with%0Atask-dependent%20heads%20for%20promoting%20the%20model%20developmental%20safety.%20Our%0Aexperiments%20on%20improving%20vision%20perception%20capabilities%20on%20autonomous%20driving%0Aand%20scene%20recognition%20datasets%20demonstrate%20the%20efficacy%20of%20the%20proposed%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03955v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Developmental%2520Safety%253A%2520A%2520Retention-Centric%2520Method%2520and%2520Applications%250A%2520%2520in%2520Vision-Language%2520Models%26entry.906535625%3DGang%2520Li%2520and%2520Wendi%2520Yu%2520and%2520Yao%2520Yao%2520and%2520Wei%2520Tong%2520and%2520Yingbin%2520Liang%2520and%2520Qihang%2520Lin%2520and%2520Tianbao%2520Yang%26entry.1292438233%3D%2520%2520In%2520the%2520real%2520world%252C%2520a%2520learning-enabled%2520system%2520usually%2520undergoes%2520multiple%250Acycles%2520of%2520model%2520development%2520to%2520enhance%2520the%2520system%2527s%2520ability%2520to%2520handle%2520difficult%250Aor%2520emerging%2520tasks.%2520This%2520continual%2520model%2520development%2520process%2520raises%2520a%250Asignificant%2520issue%2520that%2520the%2520model%2520development%2520for%2520acquiring%2520new%2520or%2520improving%250Aexisting%2520capabilities%2520may%2520inadvertently%2520lose%2520capabilities%2520of%2520the%2520old%2520model%252C%250Aalso%2520known%2520as%2520catastrophic%2520forgetting.%2520Existing%2520continual%2520learning%2520studies%250Afocus%2520on%2520mitigating%2520catastrophic%2520forgetting%2520by%2520trading%2520off%2520performance%2520on%250Aprevious%2520tasks%2520and%2520new%2520tasks%2520to%2520ensure%2520good%2520average%2520performance.%2520However%252C%2520they%250Aare%2520inadequate%2520for%2520many%2520applications%2520especially%2520in%2520safety-critical%2520domains%252C%2520as%250Afailure%2520to%2520strictly%2520preserve%2520the%2520good%2520performance%2520of%2520the%2520old%2520model%2520not%2520only%250Aintroduces%2520safety%2520risks%2520and%2520uncertainties%2520but%2520also%2520imposes%2520substantial%2520expenses%250Ain%2520the%2520re-improving%2520and%2520re-validation%2520of%2520existing%2520properties.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520model%2520developmental%2520safety%2520as%2520a%2520guarantee%2520of%2520a%2520learning%250Asystem%2520such%2520that%2520in%2520the%2520model%2520development%2520process%2520the%2520new%2520model%2520should%2520strictly%250Apreserve%2520the%2520existing%2520protected%2520capabilities%2520of%2520the%2520old%2520model%2520while%2520improving%250Aits%2520performance%2520on%2520target%2520tasks.%2520To%2520ensure%2520the%2520model%2520developmental%2520safety%252C%2520we%250Apresent%2520a%2520retention-centric%2520framework%2520by%2520formulating%2520the%2520model%2520developmental%250Asafety%2520as%2520data-dependent%2520constraints.%2520Under%2520this%2520framework%252C%2520we%2520study%2520how%2520to%250Adevelop%2520a%2520pretrained%2520vision-language%2520model%252C%2520specifically%2520the%2520CLIP%2520model%252C%2520for%250Aacquiring%2520new%2520capabilities%2520or%2520improving%2520existing%2520capabilities%2520of%2520image%250Aclassification.%2520We%2520propose%2520an%2520efficient%2520constrained%2520optimization%2520algorithm%2520with%250Atheoretical%2520guarantee%2520and%2520use%2520its%2520insights%2520to%2520finetune%2520a%2520CLIP%2520model%2520with%250Atask-dependent%2520heads%2520for%2520promoting%2520the%2520model%2520developmental%2520safety.%2520Our%250Aexperiments%2520on%2520improving%2520vision%2520perception%2520capabilities%2520on%2520autonomous%2520driving%250Aand%2520scene%2520recognition%2520datasets%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520proposed%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03955v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Developmental%20Safety%3A%20A%20Retention-Centric%20Method%20and%20Applications%0A%20%20in%20Vision-Language%20Models&entry.906535625=Gang%20Li%20and%20Wendi%20Yu%20and%20Yao%20Yao%20and%20Wei%20Tong%20and%20Yingbin%20Liang%20and%20Qihang%20Lin%20and%20Tianbao%20Yang&entry.1292438233=%20%20In%20the%20real%20world%2C%20a%20learning-enabled%20system%20usually%20undergoes%20multiple%0Acycles%20of%20model%20development%20to%20enhance%20the%20system%27s%20ability%20to%20handle%20difficult%0Aor%20emerging%20tasks.%20This%20continual%20model%20development%20process%20raises%20a%0Asignificant%20issue%20that%20the%20model%20development%20for%20acquiring%20new%20or%20improving%0Aexisting%20capabilities%20may%20inadvertently%20lose%20capabilities%20of%20the%20old%20model%2C%0Aalso%20known%20as%20catastrophic%20forgetting.%20Existing%20continual%20learning%20studies%0Afocus%20on%20mitigating%20catastrophic%20forgetting%20by%20trading%20off%20performance%20on%0Aprevious%20tasks%20and%20new%20tasks%20to%20ensure%20good%20average%20performance.%20However%2C%20they%0Aare%20inadequate%20for%20many%20applications%20especially%20in%20safety-critical%20domains%2C%20as%0Afailure%20to%20strictly%20preserve%20the%20good%20performance%20of%20the%20old%20model%20not%20only%0Aintroduces%20safety%20risks%20and%20uncertainties%20but%20also%20imposes%20substantial%20expenses%0Ain%20the%20re-improving%20and%20re-validation%20of%20existing%20properties.%20To%20address%20this%0Aissue%2C%20we%20introduce%20model%20developmental%20safety%20as%20a%20guarantee%20of%20a%20learning%0Asystem%20such%20that%20in%20the%20model%20development%20process%20the%20new%20model%20should%20strictly%0Apreserve%20the%20existing%20protected%20capabilities%20of%20the%20old%20model%20while%20improving%0Aits%20performance%20on%20target%20tasks.%20To%20ensure%20the%20model%20developmental%20safety%2C%20we%0Apresent%20a%20retention-centric%20framework%20by%20formulating%20the%20model%20developmental%0Asafety%20as%20data-dependent%20constraints.%20Under%20this%20framework%2C%20we%20study%20how%20to%0Adevelop%20a%20pretrained%20vision-language%20model%2C%20specifically%20the%20CLIP%20model%2C%20for%0Aacquiring%20new%20capabilities%20or%20improving%20existing%20capabilities%20of%20image%0Aclassification.%20We%20propose%20an%20efficient%20constrained%20optimization%20algorithm%20with%0Atheoretical%20guarantee%20and%20use%20its%20insights%20to%20finetune%20a%20CLIP%20model%20with%0Atask-dependent%20heads%20for%20promoting%20the%20model%20developmental%20safety.%20Our%0Aexperiments%20on%20improving%20vision%20perception%20capabilities%20on%20autonomous%20driving%0Aand%20scene%20recognition%20datasets%20demonstrate%20the%20efficacy%20of%20the%20proposed%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03955v3&entry.124074799=Read"},
{"title": "VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation\n  with Unsupervised Domain Adaptation", "author": "Roberto Alcover-Couso and Marcos Escudero-Vi\u00f1olo and Juan C. SanMiguel and Jesus Bescos", "abstract": "  Segmentation models are typically constrained by the categories defined\nduring training. To address this, researchers have explored two independent\napproaches: adapting Vision-Language Models (VLMs) and leveraging synthetic\ndata. However, VLMs often struggle with granularity, failing to disentangle\nfine-grained concepts, while synthetic data-based methods remain limited by the\nscope of available datasets.\n  This paper proposes enhancing segmentation accuracy across diverse domains by\nintegrating Vision-Language reasoning with key strategies for Unsupervised\nDomain Adaptation (UDA). First, we improve the fine-grained segmentation\ncapabilities of VLMs through multi-scale contextual data, robust text\nembeddings with prompt augmentation, and layer-wise fine-tuning in our proposed\nFoundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS)\nframework. Next, we incorporate these enhancements into a UDA framework by\nemploying distillation to stabilize training and cross-domain mixed sampling to\nboost adaptability without compromising generalization. The resulting\nUDA-FROVSS framework is the first UDA approach to effectively adapt across\ndomains without requiring shared categories.\n", "link": "http://arxiv.org/abs/2412.09240v1", "date": "2024-12-12", "relevancy": 2.7874, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLMs%20meet%20UDA%3A%20Boosting%20Transferability%20of%20Open%20Vocabulary%20Segmentation%0A%20%20with%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20VLMs%20meet%20UDA%3A%20Boosting%20Transferability%20of%20Open%20Vocabulary%20Segmentation%0A%20%20with%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Roberto%20Alcover-Couso%20and%20Marcos%20Escudero-Vi%C3%B1olo%20and%20Juan%20C.%20SanMiguel%20and%20Jesus%20Bescos%0AAbstract%3A%20%20%20Segmentation%20models%20are%20typically%20constrained%20by%20the%20categories%20defined%0Aduring%20training.%20To%20address%20this%2C%20researchers%20have%20explored%20two%20independent%0Aapproaches%3A%20adapting%20Vision-Language%20Models%20%28VLMs%29%20and%20leveraging%20synthetic%0Adata.%20However%2C%20VLMs%20often%20struggle%20with%20granularity%2C%20failing%20to%20disentangle%0Afine-grained%20concepts%2C%20while%20synthetic%20data-based%20methods%20remain%20limited%20by%20the%0Ascope%20of%20available%20datasets.%0A%20%20This%20paper%20proposes%20enhancing%20segmentation%20accuracy%20across%20diverse%20domains%20by%0Aintegrating%20Vision-Language%20reasoning%20with%20key%20strategies%20for%20Unsupervised%0ADomain%20Adaptation%20%28UDA%29.%20First%2C%20we%20improve%20the%20fine-grained%20segmentation%0Acapabilities%20of%20VLMs%20through%20multi-scale%20contextual%20data%2C%20robust%20text%0Aembeddings%20with%20prompt%20augmentation%2C%20and%20layer-wise%20fine-tuning%20in%20our%20proposed%0AFoundational-Retaining%20Open%20Vocabulary%20Semantic%20Segmentation%20%28FROVSS%29%0Aframework.%20Next%2C%20we%20incorporate%20these%20enhancements%20into%20a%20UDA%20framework%20by%0Aemploying%20distillation%20to%20stabilize%20training%20and%20cross-domain%20mixed%20sampling%20to%0Aboost%20adaptability%20without%20compromising%20generalization.%20The%20resulting%0AUDA-FROVSS%20framework%20is%20the%20first%20UDA%20approach%20to%20effectively%20adapt%20across%0Adomains%20without%20requiring%20shared%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLMs%2520meet%2520UDA%253A%2520Boosting%2520Transferability%2520of%2520Open%2520Vocabulary%2520Segmentation%250A%2520%2520with%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DRoberto%2520Alcover-Couso%2520and%2520Marcos%2520Escudero-Vi%25C3%25B1olo%2520and%2520Juan%2520C.%2520SanMiguel%2520and%2520Jesus%2520Bescos%26entry.1292438233%3D%2520%2520Segmentation%2520models%2520are%2520typically%2520constrained%2520by%2520the%2520categories%2520defined%250Aduring%2520training.%2520To%2520address%2520this%252C%2520researchers%2520have%2520explored%2520two%2520independent%250Aapproaches%253A%2520adapting%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520and%2520leveraging%2520synthetic%250Adata.%2520However%252C%2520VLMs%2520often%2520struggle%2520with%2520granularity%252C%2520failing%2520to%2520disentangle%250Afine-grained%2520concepts%252C%2520while%2520synthetic%2520data-based%2520methods%2520remain%2520limited%2520by%2520the%250Ascope%2520of%2520available%2520datasets.%250A%2520%2520This%2520paper%2520proposes%2520enhancing%2520segmentation%2520accuracy%2520across%2520diverse%2520domains%2520by%250Aintegrating%2520Vision-Language%2520reasoning%2520with%2520key%2520strategies%2520for%2520Unsupervised%250ADomain%2520Adaptation%2520%2528UDA%2529.%2520First%252C%2520we%2520improve%2520the%2520fine-grained%2520segmentation%250Acapabilities%2520of%2520VLMs%2520through%2520multi-scale%2520contextual%2520data%252C%2520robust%2520text%250Aembeddings%2520with%2520prompt%2520augmentation%252C%2520and%2520layer-wise%2520fine-tuning%2520in%2520our%2520proposed%250AFoundational-Retaining%2520Open%2520Vocabulary%2520Semantic%2520Segmentation%2520%2528FROVSS%2529%250Aframework.%2520Next%252C%2520we%2520incorporate%2520these%2520enhancements%2520into%2520a%2520UDA%2520framework%2520by%250Aemploying%2520distillation%2520to%2520stabilize%2520training%2520and%2520cross-domain%2520mixed%2520sampling%2520to%250Aboost%2520adaptability%2520without%2520compromising%2520generalization.%2520The%2520resulting%250AUDA-FROVSS%2520framework%2520is%2520the%2520first%2520UDA%2520approach%2520to%2520effectively%2520adapt%2520across%250Adomains%2520without%2520requiring%2520shared%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLMs%20meet%20UDA%3A%20Boosting%20Transferability%20of%20Open%20Vocabulary%20Segmentation%0A%20%20with%20Unsupervised%20Domain%20Adaptation&entry.906535625=Roberto%20Alcover-Couso%20and%20Marcos%20Escudero-Vi%C3%B1olo%20and%20Juan%20C.%20SanMiguel%20and%20Jesus%20Bescos&entry.1292438233=%20%20Segmentation%20models%20are%20typically%20constrained%20by%20the%20categories%20defined%0Aduring%20training.%20To%20address%20this%2C%20researchers%20have%20explored%20two%20independent%0Aapproaches%3A%20adapting%20Vision-Language%20Models%20%28VLMs%29%20and%20leveraging%20synthetic%0Adata.%20However%2C%20VLMs%20often%20struggle%20with%20granularity%2C%20failing%20to%20disentangle%0Afine-grained%20concepts%2C%20while%20synthetic%20data-based%20methods%20remain%20limited%20by%20the%0Ascope%20of%20available%20datasets.%0A%20%20This%20paper%20proposes%20enhancing%20segmentation%20accuracy%20across%20diverse%20domains%20by%0Aintegrating%20Vision-Language%20reasoning%20with%20key%20strategies%20for%20Unsupervised%0ADomain%20Adaptation%20%28UDA%29.%20First%2C%20we%20improve%20the%20fine-grained%20segmentation%0Acapabilities%20of%20VLMs%20through%20multi-scale%20contextual%20data%2C%20robust%20text%0Aembeddings%20with%20prompt%20augmentation%2C%20and%20layer-wise%20fine-tuning%20in%20our%20proposed%0AFoundational-Retaining%20Open%20Vocabulary%20Semantic%20Segmentation%20%28FROVSS%29%0Aframework.%20Next%2C%20we%20incorporate%20these%20enhancements%20into%20a%20UDA%20framework%20by%0Aemploying%20distillation%20to%20stabilize%20training%20and%20cross-domain%20mixed%20sampling%20to%0Aboost%20adaptability%20without%20compromising%20generalization.%20The%20resulting%0AUDA-FROVSS%20framework%20is%20the%20first%20UDA%20approach%20to%20effectively%20adapt%20across%0Adomains%20without%20requiring%20shared%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09240v1&entry.124074799=Read"},
{"title": "Towards Robust and Fair Vision Learning in Open-World Environments", "author": "Thanh-Dat Truong", "abstract": "  The dissertation presents four key contributions toward fairness and\nrobustness in vision learning. First, to address the problem of large-scale\ndata requirements, the dissertation presents a novel Fairness Domain Adaptation\napproach derived from two major novel research findings of Bijective Maximum\nLikelihood and Fairness Adaptation Learning. Second, to enable the capability\nof open-world modeling of vision learning, this dissertation presents a novel\nOpen-world Fairness Continual Learning Framework. The success of this research\ndirection is the result of two research lines, i.e., Fairness Continual\nLearning and Open-world Continual Learning. Third, since visual data are often\ncaptured from multiple camera views, robust vision learning methods should be\ncapable of modeling invariant features across views. To achieve this desired\ngoal, the research in this thesis will present a novel Geometry-based\nCross-view Adaptation framework to learn robust feature representations across\nviews. Finally, with the recent increase in large-scale videos and multimodal\ndata, understanding the feature representations and improving the robustness of\nlarge-scale visual foundation models is critical. Therefore, this thesis will\npresent novel Transformer-based approaches to improve the robust feature\nrepresentations against multimodal and temporal data. Then, a novel Domain\nGeneralization Approach will be presented to improve the robustness of visual\nfoundation models. The research's theoretical analysis and experimental results\nhave shown the effectiveness of the proposed approaches, demonstrating their\nsuperior performance compared to prior studies. The contributions in this\ndissertation have advanced the fairness and robustness of machine vision\nlearning.\n", "link": "http://arxiv.org/abs/2412.09439v1", "date": "2024-12-12", "relevancy": 2.7852, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20and%20Fair%20Vision%20Learning%20in%20Open-World%20Environments&body=Title%3A%20Towards%20Robust%20and%20Fair%20Vision%20Learning%20in%20Open-World%20Environments%0AAuthor%3A%20Thanh-Dat%20Truong%0AAbstract%3A%20%20%20The%20dissertation%20presents%20four%20key%20contributions%20toward%20fairness%20and%0Arobustness%20in%20vision%20learning.%20First%2C%20to%20address%20the%20problem%20of%20large-scale%0Adata%20requirements%2C%20the%20dissertation%20presents%20a%20novel%20Fairness%20Domain%20Adaptation%0Aapproach%20derived%20from%20two%20major%20novel%20research%20findings%20of%20Bijective%20Maximum%0ALikelihood%20and%20Fairness%20Adaptation%20Learning.%20Second%2C%20to%20enable%20the%20capability%0Aof%20open-world%20modeling%20of%20vision%20learning%2C%20this%20dissertation%20presents%20a%20novel%0AOpen-world%20Fairness%20Continual%20Learning%20Framework.%20The%20success%20of%20this%20research%0Adirection%20is%20the%20result%20of%20two%20research%20lines%2C%20i.e.%2C%20Fairness%20Continual%0ALearning%20and%20Open-world%20Continual%20Learning.%20Third%2C%20since%20visual%20data%20are%20often%0Acaptured%20from%20multiple%20camera%20views%2C%20robust%20vision%20learning%20methods%20should%20be%0Acapable%20of%20modeling%20invariant%20features%20across%20views.%20To%20achieve%20this%20desired%0Agoal%2C%20the%20research%20in%20this%20thesis%20will%20present%20a%20novel%20Geometry-based%0ACross-view%20Adaptation%20framework%20to%20learn%20robust%20feature%20representations%20across%0Aviews.%20Finally%2C%20with%20the%20recent%20increase%20in%20large-scale%20videos%20and%20multimodal%0Adata%2C%20understanding%20the%20feature%20representations%20and%20improving%20the%20robustness%20of%0Alarge-scale%20visual%20foundation%20models%20is%20critical.%20Therefore%2C%20this%20thesis%20will%0Apresent%20novel%20Transformer-based%20approaches%20to%20improve%20the%20robust%20feature%0Arepresentations%20against%20multimodal%20and%20temporal%20data.%20Then%2C%20a%20novel%20Domain%0AGeneralization%20Approach%20will%20be%20presented%20to%20improve%20the%20robustness%20of%20visual%0Afoundation%20models.%20The%20research%27s%20theoretical%20analysis%20and%20experimental%20results%0Ahave%20shown%20the%20effectiveness%20of%20the%20proposed%20approaches%2C%20demonstrating%20their%0Asuperior%20performance%20compared%20to%20prior%20studies.%20The%20contributions%20in%20this%0Adissertation%20have%20advanced%20the%20fairness%20and%20robustness%20of%20machine%20vision%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520and%2520Fair%2520Vision%2520Learning%2520in%2520Open-World%2520Environments%26entry.906535625%3DThanh-Dat%2520Truong%26entry.1292438233%3D%2520%2520The%2520dissertation%2520presents%2520four%2520key%2520contributions%2520toward%2520fairness%2520and%250Arobustness%2520in%2520vision%2520learning.%2520First%252C%2520to%2520address%2520the%2520problem%2520of%2520large-scale%250Adata%2520requirements%252C%2520the%2520dissertation%2520presents%2520a%2520novel%2520Fairness%2520Domain%2520Adaptation%250Aapproach%2520derived%2520from%2520two%2520major%2520novel%2520research%2520findings%2520of%2520Bijective%2520Maximum%250ALikelihood%2520and%2520Fairness%2520Adaptation%2520Learning.%2520Second%252C%2520to%2520enable%2520the%2520capability%250Aof%2520open-world%2520modeling%2520of%2520vision%2520learning%252C%2520this%2520dissertation%2520presents%2520a%2520novel%250AOpen-world%2520Fairness%2520Continual%2520Learning%2520Framework.%2520The%2520success%2520of%2520this%2520research%250Adirection%2520is%2520the%2520result%2520of%2520two%2520research%2520lines%252C%2520i.e.%252C%2520Fairness%2520Continual%250ALearning%2520and%2520Open-world%2520Continual%2520Learning.%2520Third%252C%2520since%2520visual%2520data%2520are%2520often%250Acaptured%2520from%2520multiple%2520camera%2520views%252C%2520robust%2520vision%2520learning%2520methods%2520should%2520be%250Acapable%2520of%2520modeling%2520invariant%2520features%2520across%2520views.%2520To%2520achieve%2520this%2520desired%250Agoal%252C%2520the%2520research%2520in%2520this%2520thesis%2520will%2520present%2520a%2520novel%2520Geometry-based%250ACross-view%2520Adaptation%2520framework%2520to%2520learn%2520robust%2520feature%2520representations%2520across%250Aviews.%2520Finally%252C%2520with%2520the%2520recent%2520increase%2520in%2520large-scale%2520videos%2520and%2520multimodal%250Adata%252C%2520understanding%2520the%2520feature%2520representations%2520and%2520improving%2520the%2520robustness%2520of%250Alarge-scale%2520visual%2520foundation%2520models%2520is%2520critical.%2520Therefore%252C%2520this%2520thesis%2520will%250Apresent%2520novel%2520Transformer-based%2520approaches%2520to%2520improve%2520the%2520robust%2520feature%250Arepresentations%2520against%2520multimodal%2520and%2520temporal%2520data.%2520Then%252C%2520a%2520novel%2520Domain%250AGeneralization%2520Approach%2520will%2520be%2520presented%2520to%2520improve%2520the%2520robustness%2520of%2520visual%250Afoundation%2520models.%2520The%2520research%2527s%2520theoretical%2520analysis%2520and%2520experimental%2520results%250Ahave%2520shown%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approaches%252C%2520demonstrating%2520their%250Asuperior%2520performance%2520compared%2520to%2520prior%2520studies.%2520The%2520contributions%2520in%2520this%250Adissertation%2520have%2520advanced%2520the%2520fairness%2520and%2520robustness%2520of%2520machine%2520vision%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20and%20Fair%20Vision%20Learning%20in%20Open-World%20Environments&entry.906535625=Thanh-Dat%20Truong&entry.1292438233=%20%20The%20dissertation%20presents%20four%20key%20contributions%20toward%20fairness%20and%0Arobustness%20in%20vision%20learning.%20First%2C%20to%20address%20the%20problem%20of%20large-scale%0Adata%20requirements%2C%20the%20dissertation%20presents%20a%20novel%20Fairness%20Domain%20Adaptation%0Aapproach%20derived%20from%20two%20major%20novel%20research%20findings%20of%20Bijective%20Maximum%0ALikelihood%20and%20Fairness%20Adaptation%20Learning.%20Second%2C%20to%20enable%20the%20capability%0Aof%20open-world%20modeling%20of%20vision%20learning%2C%20this%20dissertation%20presents%20a%20novel%0AOpen-world%20Fairness%20Continual%20Learning%20Framework.%20The%20success%20of%20this%20research%0Adirection%20is%20the%20result%20of%20two%20research%20lines%2C%20i.e.%2C%20Fairness%20Continual%0ALearning%20and%20Open-world%20Continual%20Learning.%20Third%2C%20since%20visual%20data%20are%20often%0Acaptured%20from%20multiple%20camera%20views%2C%20robust%20vision%20learning%20methods%20should%20be%0Acapable%20of%20modeling%20invariant%20features%20across%20views.%20To%20achieve%20this%20desired%0Agoal%2C%20the%20research%20in%20this%20thesis%20will%20present%20a%20novel%20Geometry-based%0ACross-view%20Adaptation%20framework%20to%20learn%20robust%20feature%20representations%20across%0Aviews.%20Finally%2C%20with%20the%20recent%20increase%20in%20large-scale%20videos%20and%20multimodal%0Adata%2C%20understanding%20the%20feature%20representations%20and%20improving%20the%20robustness%20of%0Alarge-scale%20visual%20foundation%20models%20is%20critical.%20Therefore%2C%20this%20thesis%20will%0Apresent%20novel%20Transformer-based%20approaches%20to%20improve%20the%20robust%20feature%0Arepresentations%20against%20multimodal%20and%20temporal%20data.%20Then%2C%20a%20novel%20Domain%0AGeneralization%20Approach%20will%20be%20presented%20to%20improve%20the%20robustness%20of%20visual%0Afoundation%20models.%20The%20research%27s%20theoretical%20analysis%20and%20experimental%20results%0Ahave%20shown%20the%20effectiveness%20of%20the%20proposed%20approaches%2C%20demonstrating%20their%0Asuperior%20performance%20compared%20to%20prior%20studies.%20The%20contributions%20in%20this%0Adissertation%20have%20advanced%20the%20fairness%20and%20robustness%20of%20machine%20vision%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09439v1&entry.124074799=Read"},
{"title": "USDRL: Unified Skeleton-Based Dense Representation Learning with\n  Multi-Grained Feature Decorrelation", "author": "Wanjiang Weng and Hongsong Wang and Junbo He and Lei He and Guosen Xie", "abstract": "  Contrastive learning has achieved great success in skeleton-based\nrepresentation learning recently. However, the prevailing methods are\npredominantly negative-based, necessitating additional momentum encoder and\nmemory bank to get negative samples, which increases the difficulty of model\ntraining. Furthermore, these methods primarily concentrate on learning a global\nrepresentation for recognition and retrieval tasks, while overlooking the rich\nand detailed local representations that are crucial for dense prediction tasks.\nTo alleviate these issues, we introduce a Unified Skeleton-based Dense\nRepresentation Learning framework based on feature decorrelation, called USDRL,\nwhich employs feature decorrelation across temporal, spatial, and instance\ndomains in a multi-grained manner to reduce redundancy among dimensions of the\nrepresentations to maximize information extraction from features. Additionally,\nwe design a Dense Spatio-Temporal Encoder (DSTE) to capture fine-grained action\nrepresentations effectively, thereby enhancing the performance of dense\nprediction tasks. Comprehensive experiments, conducted on the benchmarks\nNTU-60, NTU-120, PKU-MMD I, and PKU-MMD II, across diverse downstream tasks\nincluding action recognition, action retrieval, and action detection,\nconclusively demonstrate that our approach significantly outperforms the\ncurrent state-of-the-art (SOTA) approaches. Our code and models are available\nat https://github.com/wengwanjiang/USDRL.\n", "link": "http://arxiv.org/abs/2412.09220v1", "date": "2024-12-12", "relevancy": 2.7838, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USDRL%3A%20Unified%20Skeleton-Based%20Dense%20Representation%20Learning%20with%0A%20%20Multi-Grained%20Feature%20Decorrelation&body=Title%3A%20USDRL%3A%20Unified%20Skeleton-Based%20Dense%20Representation%20Learning%20with%0A%20%20Multi-Grained%20Feature%20Decorrelation%0AAuthor%3A%20Wanjiang%20Weng%20and%20Hongsong%20Wang%20and%20Junbo%20He%20and%20Lei%20He%20and%20Guosen%20Xie%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20achieved%20great%20success%20in%20skeleton-based%0Arepresentation%20learning%20recently.%20However%2C%20the%20prevailing%20methods%20are%0Apredominantly%20negative-based%2C%20necessitating%20additional%20momentum%20encoder%20and%0Amemory%20bank%20to%20get%20negative%20samples%2C%20which%20increases%20the%20difficulty%20of%20model%0Atraining.%20Furthermore%2C%20these%20methods%20primarily%20concentrate%20on%20learning%20a%20global%0Arepresentation%20for%20recognition%20and%20retrieval%20tasks%2C%20while%20overlooking%20the%20rich%0Aand%20detailed%20local%20representations%20that%20are%20crucial%20for%20dense%20prediction%20tasks.%0ATo%20alleviate%20these%20issues%2C%20we%20introduce%20a%20Unified%20Skeleton-based%20Dense%0ARepresentation%20Learning%20framework%20based%20on%20feature%20decorrelation%2C%20called%20USDRL%2C%0Awhich%20employs%20feature%20decorrelation%20across%20temporal%2C%20spatial%2C%20and%20instance%0Adomains%20in%20a%20multi-grained%20manner%20to%20reduce%20redundancy%20among%20dimensions%20of%20the%0Arepresentations%20to%20maximize%20information%20extraction%20from%20features.%20Additionally%2C%0Awe%20design%20a%20Dense%20Spatio-Temporal%20Encoder%20%28DSTE%29%20to%20capture%20fine-grained%20action%0Arepresentations%20effectively%2C%20thereby%20enhancing%20the%20performance%20of%20dense%0Aprediction%20tasks.%20Comprehensive%20experiments%2C%20conducted%20on%20the%20benchmarks%0ANTU-60%2C%20NTU-120%2C%20PKU-MMD%20I%2C%20and%20PKU-MMD%20II%2C%20across%20diverse%20downstream%20tasks%0Aincluding%20action%20recognition%2C%20action%20retrieval%2C%20and%20action%20detection%2C%0Aconclusively%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20the%0Acurrent%20state-of-the-art%20%28SOTA%29%20approaches.%20Our%20code%20and%20models%20are%20available%0Aat%20https%3A//github.com/wengwanjiang/USDRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSDRL%253A%2520Unified%2520Skeleton-Based%2520Dense%2520Representation%2520Learning%2520with%250A%2520%2520Multi-Grained%2520Feature%2520Decorrelation%26entry.906535625%3DWanjiang%2520Weng%2520and%2520Hongsong%2520Wang%2520and%2520Junbo%2520He%2520and%2520Lei%2520He%2520and%2520Guosen%2520Xie%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520has%2520achieved%2520great%2520success%2520in%2520skeleton-based%250Arepresentation%2520learning%2520recently.%2520However%252C%2520the%2520prevailing%2520methods%2520are%250Apredominantly%2520negative-based%252C%2520necessitating%2520additional%2520momentum%2520encoder%2520and%250Amemory%2520bank%2520to%2520get%2520negative%2520samples%252C%2520which%2520increases%2520the%2520difficulty%2520of%2520model%250Atraining.%2520Furthermore%252C%2520these%2520methods%2520primarily%2520concentrate%2520on%2520learning%2520a%2520global%250Arepresentation%2520for%2520recognition%2520and%2520retrieval%2520tasks%252C%2520while%2520overlooking%2520the%2520rich%250Aand%2520detailed%2520local%2520representations%2520that%2520are%2520crucial%2520for%2520dense%2520prediction%2520tasks.%250ATo%2520alleviate%2520these%2520issues%252C%2520we%2520introduce%2520a%2520Unified%2520Skeleton-based%2520Dense%250ARepresentation%2520Learning%2520framework%2520based%2520on%2520feature%2520decorrelation%252C%2520called%2520USDRL%252C%250Awhich%2520employs%2520feature%2520decorrelation%2520across%2520temporal%252C%2520spatial%252C%2520and%2520instance%250Adomains%2520in%2520a%2520multi-grained%2520manner%2520to%2520reduce%2520redundancy%2520among%2520dimensions%2520of%2520the%250Arepresentations%2520to%2520maximize%2520information%2520extraction%2520from%2520features.%2520Additionally%252C%250Awe%2520design%2520a%2520Dense%2520Spatio-Temporal%2520Encoder%2520%2528DSTE%2529%2520to%2520capture%2520fine-grained%2520action%250Arepresentations%2520effectively%252C%2520thereby%2520enhancing%2520the%2520performance%2520of%2520dense%250Aprediction%2520tasks.%2520Comprehensive%2520experiments%252C%2520conducted%2520on%2520the%2520benchmarks%250ANTU-60%252C%2520NTU-120%252C%2520PKU-MMD%2520I%252C%2520and%2520PKU-MMD%2520II%252C%2520across%2520diverse%2520downstream%2520tasks%250Aincluding%2520action%2520recognition%252C%2520action%2520retrieval%252C%2520and%2520action%2520detection%252C%250Aconclusively%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520the%250Acurrent%2520state-of-the-art%2520%2528SOTA%2529%2520approaches.%2520Our%2520code%2520and%2520models%2520are%2520available%250Aat%2520https%253A//github.com/wengwanjiang/USDRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USDRL%3A%20Unified%20Skeleton-Based%20Dense%20Representation%20Learning%20with%0A%20%20Multi-Grained%20Feature%20Decorrelation&entry.906535625=Wanjiang%20Weng%20and%20Hongsong%20Wang%20and%20Junbo%20He%20and%20Lei%20He%20and%20Guosen%20Xie&entry.1292438233=%20%20Contrastive%20learning%20has%20achieved%20great%20success%20in%20skeleton-based%0Arepresentation%20learning%20recently.%20However%2C%20the%20prevailing%20methods%20are%0Apredominantly%20negative-based%2C%20necessitating%20additional%20momentum%20encoder%20and%0Amemory%20bank%20to%20get%20negative%20samples%2C%20which%20increases%20the%20difficulty%20of%20model%0Atraining.%20Furthermore%2C%20these%20methods%20primarily%20concentrate%20on%20learning%20a%20global%0Arepresentation%20for%20recognition%20and%20retrieval%20tasks%2C%20while%20overlooking%20the%20rich%0Aand%20detailed%20local%20representations%20that%20are%20crucial%20for%20dense%20prediction%20tasks.%0ATo%20alleviate%20these%20issues%2C%20we%20introduce%20a%20Unified%20Skeleton-based%20Dense%0ARepresentation%20Learning%20framework%20based%20on%20feature%20decorrelation%2C%20called%20USDRL%2C%0Awhich%20employs%20feature%20decorrelation%20across%20temporal%2C%20spatial%2C%20and%20instance%0Adomains%20in%20a%20multi-grained%20manner%20to%20reduce%20redundancy%20among%20dimensions%20of%20the%0Arepresentations%20to%20maximize%20information%20extraction%20from%20features.%20Additionally%2C%0Awe%20design%20a%20Dense%20Spatio-Temporal%20Encoder%20%28DSTE%29%20to%20capture%20fine-grained%20action%0Arepresentations%20effectively%2C%20thereby%20enhancing%20the%20performance%20of%20dense%0Aprediction%20tasks.%20Comprehensive%20experiments%2C%20conducted%20on%20the%20benchmarks%0ANTU-60%2C%20NTU-120%2C%20PKU-MMD%20I%2C%20and%20PKU-MMD%20II%2C%20across%20diverse%20downstream%20tasks%0Aincluding%20action%20recognition%2C%20action%20retrieval%2C%20and%20action%20detection%2C%0Aconclusively%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20the%0Acurrent%20state-of-the-art%20%28SOTA%29%20approaches.%20Our%20code%20and%20models%20are%20available%0Aat%20https%3A//github.com/wengwanjiang/USDRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09220v1&entry.124074799=Read"},
{"title": "Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition", "author": "Zhisheng Zhong and Chengyao Wang and Yuqi Liu and Senqiao Yang and Longxiang Tang and Yuechen Zhang and Jingyao Li and Tianyuan Qu and Yanwei Li and Yukang Chen and Shaozuo Yu and Sitong Wu and Eric Lo and Shu Liu and Jiaya Jia", "abstract": "  As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond\nsingle-domain capabilities is essential to meet the demands for more versatile\nand efficient AI. However, previous omni-models have insufficiently explored\nspeech, neglecting its integration with multi-modality. We introduce Lyra, an\nefficient MLLM that enhances multimodal abilities, including advanced\nlong-speech comprehension, sound understanding, cross-modality efficiency, and\nseamless speech interaction. To achieve efficiency and speech-centric\ncapabilities, Lyra employs three strategies: (1) leveraging existing\nopen-source large models and a proposed multi-modality LoRA to reduce training\ncosts and data requirements; (2) using a latent multi-modality regularizer and\nextractor to strengthen the relationship between speech and other modalities,\nthereby enhancing model performance; and (3) constructing a high-quality,\nextensive dataset that includes 1.5M multi-modal (language, vision, audio) data\nsamples and 12K long speech samples, enabling Lyra to handle complex long\nspeech inputs and achieve more robust omni-cognition. Compared to other\nomni-methods, Lyra achieves state-of-the-art performance on various\nvision-language, vision-speech, and speech-language benchmarks, while also\nusing fewer computational resources and less training data.\n", "link": "http://arxiv.org/abs/2412.09501v1", "date": "2024-12-12", "relevancy": 2.7641, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lyra%3A%20An%20Efficient%20and%20Speech-Centric%20Framework%20for%20Omni-Cognition&body=Title%3A%20Lyra%3A%20An%20Efficient%20and%20Speech-Centric%20Framework%20for%20Omni-Cognition%0AAuthor%3A%20Zhisheng%20Zhong%20and%20Chengyao%20Wang%20and%20Yuqi%20Liu%20and%20Senqiao%20Yang%20and%20Longxiang%20Tang%20and%20Yuechen%20Zhang%20and%20Jingyao%20Li%20and%20Tianyuan%20Qu%20and%20Yanwei%20Li%20and%20Yukang%20Chen%20and%20Shaozuo%20Yu%20and%20Sitong%20Wu%20and%20Eric%20Lo%20and%20Shu%20Liu%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20As%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20evolve%2C%20expanding%20beyond%0Asingle-domain%20capabilities%20is%20essential%20to%20meet%20the%20demands%20for%20more%20versatile%0Aand%20efficient%20AI.%20However%2C%20previous%20omni-models%20have%20insufficiently%20explored%0Aspeech%2C%20neglecting%20its%20integration%20with%20multi-modality.%20We%20introduce%20Lyra%2C%20an%0Aefficient%20MLLM%20that%20enhances%20multimodal%20abilities%2C%20including%20advanced%0Along-speech%20comprehension%2C%20sound%20understanding%2C%20cross-modality%20efficiency%2C%20and%0Aseamless%20speech%20interaction.%20To%20achieve%20efficiency%20and%20speech-centric%0Acapabilities%2C%20Lyra%20employs%20three%20strategies%3A%20%281%29%20leveraging%20existing%0Aopen-source%20large%20models%20and%20a%20proposed%20multi-modality%20LoRA%20to%20reduce%20training%0Acosts%20and%20data%20requirements%3B%20%282%29%20using%20a%20latent%20multi-modality%20regularizer%20and%0Aextractor%20to%20strengthen%20the%20relationship%20between%20speech%20and%20other%20modalities%2C%0Athereby%20enhancing%20model%20performance%3B%20and%20%283%29%20constructing%20a%20high-quality%2C%0Aextensive%20dataset%20that%20includes%201.5M%20multi-modal%20%28language%2C%20vision%2C%20audio%29%20data%0Asamples%20and%2012K%20long%20speech%20samples%2C%20enabling%20Lyra%20to%20handle%20complex%20long%0Aspeech%20inputs%20and%20achieve%20more%20robust%20omni-cognition.%20Compared%20to%20other%0Aomni-methods%2C%20Lyra%20achieves%20state-of-the-art%20performance%20on%20various%0Avision-language%2C%20vision-speech%2C%20and%20speech-language%20benchmarks%2C%20while%20also%0Ausing%20fewer%20computational%20resources%20and%20less%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyra%253A%2520An%2520Efficient%2520and%2520Speech-Centric%2520Framework%2520for%2520Omni-Cognition%26entry.906535625%3DZhisheng%2520Zhong%2520and%2520Chengyao%2520Wang%2520and%2520Yuqi%2520Liu%2520and%2520Senqiao%2520Yang%2520and%2520Longxiang%2520Tang%2520and%2520Yuechen%2520Zhang%2520and%2520Jingyao%2520Li%2520and%2520Tianyuan%2520Qu%2520and%2520Yanwei%2520Li%2520and%2520Yukang%2520Chen%2520and%2520Shaozuo%2520Yu%2520and%2520Sitong%2520Wu%2520and%2520Eric%2520Lo%2520and%2520Shu%2520Liu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520As%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520evolve%252C%2520expanding%2520beyond%250Asingle-domain%2520capabilities%2520is%2520essential%2520to%2520meet%2520the%2520demands%2520for%2520more%2520versatile%250Aand%2520efficient%2520AI.%2520However%252C%2520previous%2520omni-models%2520have%2520insufficiently%2520explored%250Aspeech%252C%2520neglecting%2520its%2520integration%2520with%2520multi-modality.%2520We%2520introduce%2520Lyra%252C%2520an%250Aefficient%2520MLLM%2520that%2520enhances%2520multimodal%2520abilities%252C%2520including%2520advanced%250Along-speech%2520comprehension%252C%2520sound%2520understanding%252C%2520cross-modality%2520efficiency%252C%2520and%250Aseamless%2520speech%2520interaction.%2520To%2520achieve%2520efficiency%2520and%2520speech-centric%250Acapabilities%252C%2520Lyra%2520employs%2520three%2520strategies%253A%2520%25281%2529%2520leveraging%2520existing%250Aopen-source%2520large%2520models%2520and%2520a%2520proposed%2520multi-modality%2520LoRA%2520to%2520reduce%2520training%250Acosts%2520and%2520data%2520requirements%253B%2520%25282%2529%2520using%2520a%2520latent%2520multi-modality%2520regularizer%2520and%250Aextractor%2520to%2520strengthen%2520the%2520relationship%2520between%2520speech%2520and%2520other%2520modalities%252C%250Athereby%2520enhancing%2520model%2520performance%253B%2520and%2520%25283%2529%2520constructing%2520a%2520high-quality%252C%250Aextensive%2520dataset%2520that%2520includes%25201.5M%2520multi-modal%2520%2528language%252C%2520vision%252C%2520audio%2529%2520data%250Asamples%2520and%252012K%2520long%2520speech%2520samples%252C%2520enabling%2520Lyra%2520to%2520handle%2520complex%2520long%250Aspeech%2520inputs%2520and%2520achieve%2520more%2520robust%2520omni-cognition.%2520Compared%2520to%2520other%250Aomni-methods%252C%2520Lyra%2520achieves%2520state-of-the-art%2520performance%2520on%2520various%250Avision-language%252C%2520vision-speech%252C%2520and%2520speech-language%2520benchmarks%252C%2520while%2520also%250Ausing%2520fewer%2520computational%2520resources%2520and%2520less%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lyra%3A%20An%20Efficient%20and%20Speech-Centric%20Framework%20for%20Omni-Cognition&entry.906535625=Zhisheng%20Zhong%20and%20Chengyao%20Wang%20and%20Yuqi%20Liu%20and%20Senqiao%20Yang%20and%20Longxiang%20Tang%20and%20Yuechen%20Zhang%20and%20Jingyao%20Li%20and%20Tianyuan%20Qu%20and%20Yanwei%20Li%20and%20Yukang%20Chen%20and%20Shaozuo%20Yu%20and%20Sitong%20Wu%20and%20Eric%20Lo%20and%20Shu%20Liu%20and%20Jiaya%20Jia&entry.1292438233=%20%20As%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20evolve%2C%20expanding%20beyond%0Asingle-domain%20capabilities%20is%20essential%20to%20meet%20the%20demands%20for%20more%20versatile%0Aand%20efficient%20AI.%20However%2C%20previous%20omni-models%20have%20insufficiently%20explored%0Aspeech%2C%20neglecting%20its%20integration%20with%20multi-modality.%20We%20introduce%20Lyra%2C%20an%0Aefficient%20MLLM%20that%20enhances%20multimodal%20abilities%2C%20including%20advanced%0Along-speech%20comprehension%2C%20sound%20understanding%2C%20cross-modality%20efficiency%2C%20and%0Aseamless%20speech%20interaction.%20To%20achieve%20efficiency%20and%20speech-centric%0Acapabilities%2C%20Lyra%20employs%20three%20strategies%3A%20%281%29%20leveraging%20existing%0Aopen-source%20large%20models%20and%20a%20proposed%20multi-modality%20LoRA%20to%20reduce%20training%0Acosts%20and%20data%20requirements%3B%20%282%29%20using%20a%20latent%20multi-modality%20regularizer%20and%0Aextractor%20to%20strengthen%20the%20relationship%20between%20speech%20and%20other%20modalities%2C%0Athereby%20enhancing%20model%20performance%3B%20and%20%283%29%20constructing%20a%20high-quality%2C%0Aextensive%20dataset%20that%20includes%201.5M%20multi-modal%20%28language%2C%20vision%2C%20audio%29%20data%0Asamples%20and%2012K%20long%20speech%20samples%2C%20enabling%20Lyra%20to%20handle%20complex%20long%0Aspeech%20inputs%20and%20achieve%20more%20robust%20omni-cognition.%20Compared%20to%20other%0Aomni-methods%2C%20Lyra%20achieves%20state-of-the-art%20performance%20on%20various%0Avision-language%2C%20vision-speech%2C%20and%20speech-language%20benchmarks%2C%20while%20also%0Ausing%20fewer%20computational%20resources%20and%20less%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09501v1&entry.124074799=Read"},
{"title": "MVC-VPR: Mutual Learning of Viewpoint Classification and Visual Place\n  Recognition", "author": "Qiwen Gu and Xufei Wang and Fenglin Zhang and Junqiao Zhao and Siyue Tao and Chen Ye and Tiantian Feng and Changjun Jiang", "abstract": "  Visual Place Recognition (VPR) aims to robustly identify locations by\nleveraging image retrieval based on descriptors encoded from environmental\nimages. However, drastic appearance changes of images captured from different\nviewpoints at the same location pose incoherent supervision signals for\ndescriptor learning, which severely hinder the performance of VPR. Previous\nwork proposes classifying images based on manually defined rules or ground\ntruth labels for viewpoints, followed by descriptor training based on the\nclassification results. However, not all datasets have ground truth labels of\nviewpoints and manually defined rules may be suboptimal, leading to degraded\ndescriptor performance.To address these challenges, we introduce the mutual\nlearning of viewpoint self-classification and VPR. Starting from coarse\nclassification based on geographical coordinates, we progress to finer\nclassification of viewpoints using simple clustering techniques. The dataset is\npartitioned in an unsupervised manner while simultaneously training a\ndescriptor extractor for place recognition. Experimental results show that this\napproach almost perfectly partitions the dataset based on viewpoints, thus\nachieving mutually reinforcing effects. Our method even excels state-of-the-art\n(SOTA) methods that partition datasets using ground truth labels.\n", "link": "http://arxiv.org/abs/2412.09199v1", "date": "2024-12-12", "relevancy": 2.763, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5574}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVC-VPR%3A%20Mutual%20Learning%20of%20Viewpoint%20Classification%20and%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20MVC-VPR%3A%20Mutual%20Learning%20of%20Viewpoint%20Classification%20and%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Qiwen%20Gu%20and%20Xufei%20Wang%20and%20Fenglin%20Zhang%20and%20Junqiao%20Zhao%20and%20Siyue%20Tao%20and%20Chen%20Ye%20and%20Tiantian%20Feng%20and%20Changjun%20Jiang%0AAbstract%3A%20%20%20Visual%20Place%20Recognition%20%28VPR%29%20aims%20to%20robustly%20identify%20locations%20by%0Aleveraging%20image%20retrieval%20based%20on%20descriptors%20encoded%20from%20environmental%0Aimages.%20However%2C%20drastic%20appearance%20changes%20of%20images%20captured%20from%20different%0Aviewpoints%20at%20the%20same%20location%20pose%20incoherent%20supervision%20signals%20for%0Adescriptor%20learning%2C%20which%20severely%20hinder%20the%20performance%20of%20VPR.%20Previous%0Awork%20proposes%20classifying%20images%20based%20on%20manually%20defined%20rules%20or%20ground%0Atruth%20labels%20for%20viewpoints%2C%20followed%20by%20descriptor%20training%20based%20on%20the%0Aclassification%20results.%20However%2C%20not%20all%20datasets%20have%20ground%20truth%20labels%20of%0Aviewpoints%20and%20manually%20defined%20rules%20may%20be%20suboptimal%2C%20leading%20to%20degraded%0Adescriptor%20performance.To%20address%20these%20challenges%2C%20we%20introduce%20the%20mutual%0Alearning%20of%20viewpoint%20self-classification%20and%20VPR.%20Starting%20from%20coarse%0Aclassification%20based%20on%20geographical%20coordinates%2C%20we%20progress%20to%20finer%0Aclassification%20of%20viewpoints%20using%20simple%20clustering%20techniques.%20The%20dataset%20is%0Apartitioned%20in%20an%20unsupervised%20manner%20while%20simultaneously%20training%20a%0Adescriptor%20extractor%20for%20place%20recognition.%20Experimental%20results%20show%20that%20this%0Aapproach%20almost%20perfectly%20partitions%20the%20dataset%20based%20on%20viewpoints%2C%20thus%0Aachieving%20mutually%20reinforcing%20effects.%20Our%20method%20even%20excels%20state-of-the-art%0A%28SOTA%29%20methods%20that%20partition%20datasets%20using%20ground%20truth%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVC-VPR%253A%2520Mutual%2520Learning%2520of%2520Viewpoint%2520Classification%2520and%2520Visual%2520Place%250A%2520%2520Recognition%26entry.906535625%3DQiwen%2520Gu%2520and%2520Xufei%2520Wang%2520and%2520Fenglin%2520Zhang%2520and%2520Junqiao%2520Zhao%2520and%2520Siyue%2520Tao%2520and%2520Chen%2520Ye%2520and%2520Tiantian%2520Feng%2520and%2520Changjun%2520Jiang%26entry.1292438233%3D%2520%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520aims%2520to%2520robustly%2520identify%2520locations%2520by%250Aleveraging%2520image%2520retrieval%2520based%2520on%2520descriptors%2520encoded%2520from%2520environmental%250Aimages.%2520However%252C%2520drastic%2520appearance%2520changes%2520of%2520images%2520captured%2520from%2520different%250Aviewpoints%2520at%2520the%2520same%2520location%2520pose%2520incoherent%2520supervision%2520signals%2520for%250Adescriptor%2520learning%252C%2520which%2520severely%2520hinder%2520the%2520performance%2520of%2520VPR.%2520Previous%250Awork%2520proposes%2520classifying%2520images%2520based%2520on%2520manually%2520defined%2520rules%2520or%2520ground%250Atruth%2520labels%2520for%2520viewpoints%252C%2520followed%2520by%2520descriptor%2520training%2520based%2520on%2520the%250Aclassification%2520results.%2520However%252C%2520not%2520all%2520datasets%2520have%2520ground%2520truth%2520labels%2520of%250Aviewpoints%2520and%2520manually%2520defined%2520rules%2520may%2520be%2520suboptimal%252C%2520leading%2520to%2520degraded%250Adescriptor%2520performance.To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520mutual%250Alearning%2520of%2520viewpoint%2520self-classification%2520and%2520VPR.%2520Starting%2520from%2520coarse%250Aclassification%2520based%2520on%2520geographical%2520coordinates%252C%2520we%2520progress%2520to%2520finer%250Aclassification%2520of%2520viewpoints%2520using%2520simple%2520clustering%2520techniques.%2520The%2520dataset%2520is%250Apartitioned%2520in%2520an%2520unsupervised%2520manner%2520while%2520simultaneously%2520training%2520a%250Adescriptor%2520extractor%2520for%2520place%2520recognition.%2520Experimental%2520results%2520show%2520that%2520this%250Aapproach%2520almost%2520perfectly%2520partitions%2520the%2520dataset%2520based%2520on%2520viewpoints%252C%2520thus%250Aachieving%2520mutually%2520reinforcing%2520effects.%2520Our%2520method%2520even%2520excels%2520state-of-the-art%250A%2528SOTA%2529%2520methods%2520that%2520partition%2520datasets%2520using%2520ground%2520truth%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVC-VPR%3A%20Mutual%20Learning%20of%20Viewpoint%20Classification%20and%20Visual%20Place%0A%20%20Recognition&entry.906535625=Qiwen%20Gu%20and%20Xufei%20Wang%20and%20Fenglin%20Zhang%20and%20Junqiao%20Zhao%20and%20Siyue%20Tao%20and%20Chen%20Ye%20and%20Tiantian%20Feng%20and%20Changjun%20Jiang&entry.1292438233=%20%20Visual%20Place%20Recognition%20%28VPR%29%20aims%20to%20robustly%20identify%20locations%20by%0Aleveraging%20image%20retrieval%20based%20on%20descriptors%20encoded%20from%20environmental%0Aimages.%20However%2C%20drastic%20appearance%20changes%20of%20images%20captured%20from%20different%0Aviewpoints%20at%20the%20same%20location%20pose%20incoherent%20supervision%20signals%20for%0Adescriptor%20learning%2C%20which%20severely%20hinder%20the%20performance%20of%20VPR.%20Previous%0Awork%20proposes%20classifying%20images%20based%20on%20manually%20defined%20rules%20or%20ground%0Atruth%20labels%20for%20viewpoints%2C%20followed%20by%20descriptor%20training%20based%20on%20the%0Aclassification%20results.%20However%2C%20not%20all%20datasets%20have%20ground%20truth%20labels%20of%0Aviewpoints%20and%20manually%20defined%20rules%20may%20be%20suboptimal%2C%20leading%20to%20degraded%0Adescriptor%20performance.To%20address%20these%20challenges%2C%20we%20introduce%20the%20mutual%0Alearning%20of%20viewpoint%20self-classification%20and%20VPR.%20Starting%20from%20coarse%0Aclassification%20based%20on%20geographical%20coordinates%2C%20we%20progress%20to%20finer%0Aclassification%20of%20viewpoints%20using%20simple%20clustering%20techniques.%20The%20dataset%20is%0Apartitioned%20in%20an%20unsupervised%20manner%20while%20simultaneously%20training%20a%0Adescriptor%20extractor%20for%20place%20recognition.%20Experimental%20results%20show%20that%20this%0Aapproach%20almost%20perfectly%20partitions%20the%20dataset%20based%20on%20viewpoints%2C%20thus%0Aachieving%20mutually%20reinforcing%20effects.%20Our%20method%20even%20excels%20state-of-the-art%0A%28SOTA%29%20methods%20that%20partition%20datasets%20using%20ground%20truth%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09199v1&entry.124074799=Read"},
{"title": "Hyperspectral Imaging-Based Perception in Autonomous Driving Scenarios:\n  Benchmarking Baseline Semantic Segmentation Models", "author": "Imad Ali Shah and Jiarong Li and Martin Glavin and Edward Jones and Enda Ward and Brian Deegan", "abstract": "  Hyperspectral Imaging (HSI) is known for its advantages over traditional RGB\nimaging in remote sensing, agriculture, and medicine. Recently, it has gained\nattention for enhancing Advanced Driving Assistance Systems (ADAS) perception.\nSeveral HSI datasets such as HyKo, HSI-Drive, HSI-Road, and Hyperspectral City\nhave been made available. However, a comprehensive evaluation of semantic\nsegmentation models (SSM) using these datasets is lacking. To address this gap,\nwe evaluated the available annotated HSI datasets on four deep learning-based\nbaseline SSMs: DeepLab v3+, HRNet, PSPNet, and U-Net, along with its two\nvariants: Coordinate Attention (UNet-CA) and Convolutional Block-Attention\nModule (UNet-CBAM). The original model architectures were adapted to handle the\nvarying spatial and spectral dimensions of the datasets. These baseline SSMs\nwere trained using a class-weighted loss function for individual HSI datasets\nand evaluated using mean-based metrics such as intersection over union (IoU),\nrecall, precision, F1 score, specificity, and accuracy. Our results indicate\nthat UNet-CBAM, which extracts channel-wise features, outperforms other SSMs\nand shows potential to leverage spectral information for enhanced semantic\nsegmentation. This study establishes a baseline SSM benchmark on available\nannotated datasets for future evaluation of HSI-based ADAS perception. However,\nlimitations of current HSI datasets, such as limited dataset size, high class\nimbalance, and lack of fine-grained annotations, remain significant constraints\nfor developing robust SSMs for ADAS applications.\n", "link": "http://arxiv.org/abs/2410.22101v2", "date": "2024-12-12", "relevancy": 2.6974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5404}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Imaging-Based%20Perception%20in%20Autonomous%20Driving%20Scenarios%3A%0A%20%20Benchmarking%20Baseline%20Semantic%20Segmentation%20Models&body=Title%3A%20Hyperspectral%20Imaging-Based%20Perception%20in%20Autonomous%20Driving%20Scenarios%3A%0A%20%20Benchmarking%20Baseline%20Semantic%20Segmentation%20Models%0AAuthor%3A%20Imad%20Ali%20Shah%20and%20Jiarong%20Li%20and%20Martin%20Glavin%20and%20Edward%20Jones%20and%20Enda%20Ward%20and%20Brian%20Deegan%0AAbstract%3A%20%20%20Hyperspectral%20Imaging%20%28HSI%29%20is%20known%20for%20its%20advantages%20over%20traditional%20RGB%0Aimaging%20in%20remote%20sensing%2C%20agriculture%2C%20and%20medicine.%20Recently%2C%20it%20has%20gained%0Aattention%20for%20enhancing%20Advanced%20Driving%20Assistance%20Systems%20%28ADAS%29%20perception.%0ASeveral%20HSI%20datasets%20such%20as%20HyKo%2C%20HSI-Drive%2C%20HSI-Road%2C%20and%20Hyperspectral%20City%0Ahave%20been%20made%20available.%20However%2C%20a%20comprehensive%20evaluation%20of%20semantic%0Asegmentation%20models%20%28SSM%29%20using%20these%20datasets%20is%20lacking.%20To%20address%20this%20gap%2C%0Awe%20evaluated%20the%20available%20annotated%20HSI%20datasets%20on%20four%20deep%20learning-based%0Abaseline%20SSMs%3A%20DeepLab%20v3%2B%2C%20HRNet%2C%20PSPNet%2C%20and%20U-Net%2C%20along%20with%20its%20two%0Avariants%3A%20Coordinate%20Attention%20%28UNet-CA%29%20and%20Convolutional%20Block-Attention%0AModule%20%28UNet-CBAM%29.%20The%20original%20model%20architectures%20were%20adapted%20to%20handle%20the%0Avarying%20spatial%20and%20spectral%20dimensions%20of%20the%20datasets.%20These%20baseline%20SSMs%0Awere%20trained%20using%20a%20class-weighted%20loss%20function%20for%20individual%20HSI%20datasets%0Aand%20evaluated%20using%20mean-based%20metrics%20such%20as%20intersection%20over%20union%20%28IoU%29%2C%0Arecall%2C%20precision%2C%20F1%20score%2C%20specificity%2C%20and%20accuracy.%20Our%20results%20indicate%0Athat%20UNet-CBAM%2C%20which%20extracts%20channel-wise%20features%2C%20outperforms%20other%20SSMs%0Aand%20shows%20potential%20to%20leverage%20spectral%20information%20for%20enhanced%20semantic%0Asegmentation.%20This%20study%20establishes%20a%20baseline%20SSM%20benchmark%20on%20available%0Aannotated%20datasets%20for%20future%20evaluation%20of%20HSI-based%20ADAS%20perception.%20However%2C%0Alimitations%20of%20current%20HSI%20datasets%2C%20such%20as%20limited%20dataset%20size%2C%20high%20class%0Aimbalance%2C%20and%20lack%20of%20fine-grained%20annotations%2C%20remain%20significant%20constraints%0Afor%20developing%20robust%20SSMs%20for%20ADAS%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Imaging-Based%2520Perception%2520in%2520Autonomous%2520Driving%2520Scenarios%253A%250A%2520%2520Benchmarking%2520Baseline%2520Semantic%2520Segmentation%2520Models%26entry.906535625%3DImad%2520Ali%2520Shah%2520and%2520Jiarong%2520Li%2520and%2520Martin%2520Glavin%2520and%2520Edward%2520Jones%2520and%2520Enda%2520Ward%2520and%2520Brian%2520Deegan%26entry.1292438233%3D%2520%2520Hyperspectral%2520Imaging%2520%2528HSI%2529%2520is%2520known%2520for%2520its%2520advantages%2520over%2520traditional%2520RGB%250Aimaging%2520in%2520remote%2520sensing%252C%2520agriculture%252C%2520and%2520medicine.%2520Recently%252C%2520it%2520has%2520gained%250Aattention%2520for%2520enhancing%2520Advanced%2520Driving%2520Assistance%2520Systems%2520%2528ADAS%2529%2520perception.%250ASeveral%2520HSI%2520datasets%2520such%2520as%2520HyKo%252C%2520HSI-Drive%252C%2520HSI-Road%252C%2520and%2520Hyperspectral%2520City%250Ahave%2520been%2520made%2520available.%2520However%252C%2520a%2520comprehensive%2520evaluation%2520of%2520semantic%250Asegmentation%2520models%2520%2528SSM%2529%2520using%2520these%2520datasets%2520is%2520lacking.%2520To%2520address%2520this%2520gap%252C%250Awe%2520evaluated%2520the%2520available%2520annotated%2520HSI%2520datasets%2520on%2520four%2520deep%2520learning-based%250Abaseline%2520SSMs%253A%2520DeepLab%2520v3%252B%252C%2520HRNet%252C%2520PSPNet%252C%2520and%2520U-Net%252C%2520along%2520with%2520its%2520two%250Avariants%253A%2520Coordinate%2520Attention%2520%2528UNet-CA%2529%2520and%2520Convolutional%2520Block-Attention%250AModule%2520%2528UNet-CBAM%2529.%2520The%2520original%2520model%2520architectures%2520were%2520adapted%2520to%2520handle%2520the%250Avarying%2520spatial%2520and%2520spectral%2520dimensions%2520of%2520the%2520datasets.%2520These%2520baseline%2520SSMs%250Awere%2520trained%2520using%2520a%2520class-weighted%2520loss%2520function%2520for%2520individual%2520HSI%2520datasets%250Aand%2520evaluated%2520using%2520mean-based%2520metrics%2520such%2520as%2520intersection%2520over%2520union%2520%2528IoU%2529%252C%250Arecall%252C%2520precision%252C%2520F1%2520score%252C%2520specificity%252C%2520and%2520accuracy.%2520Our%2520results%2520indicate%250Athat%2520UNet-CBAM%252C%2520which%2520extracts%2520channel-wise%2520features%252C%2520outperforms%2520other%2520SSMs%250Aand%2520shows%2520potential%2520to%2520leverage%2520spectral%2520information%2520for%2520enhanced%2520semantic%250Asegmentation.%2520This%2520study%2520establishes%2520a%2520baseline%2520SSM%2520benchmark%2520on%2520available%250Aannotated%2520datasets%2520for%2520future%2520evaluation%2520of%2520HSI-based%2520ADAS%2520perception.%2520However%252C%250Alimitations%2520of%2520current%2520HSI%2520datasets%252C%2520such%2520as%2520limited%2520dataset%2520size%252C%2520high%2520class%250Aimbalance%252C%2520and%2520lack%2520of%2520fine-grained%2520annotations%252C%2520remain%2520significant%2520constraints%250Afor%2520developing%2520robust%2520SSMs%2520for%2520ADAS%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Imaging-Based%20Perception%20in%20Autonomous%20Driving%20Scenarios%3A%0A%20%20Benchmarking%20Baseline%20Semantic%20Segmentation%20Models&entry.906535625=Imad%20Ali%20Shah%20and%20Jiarong%20Li%20and%20Martin%20Glavin%20and%20Edward%20Jones%20and%20Enda%20Ward%20and%20Brian%20Deegan&entry.1292438233=%20%20Hyperspectral%20Imaging%20%28HSI%29%20is%20known%20for%20its%20advantages%20over%20traditional%20RGB%0Aimaging%20in%20remote%20sensing%2C%20agriculture%2C%20and%20medicine.%20Recently%2C%20it%20has%20gained%0Aattention%20for%20enhancing%20Advanced%20Driving%20Assistance%20Systems%20%28ADAS%29%20perception.%0ASeveral%20HSI%20datasets%20such%20as%20HyKo%2C%20HSI-Drive%2C%20HSI-Road%2C%20and%20Hyperspectral%20City%0Ahave%20been%20made%20available.%20However%2C%20a%20comprehensive%20evaluation%20of%20semantic%0Asegmentation%20models%20%28SSM%29%20using%20these%20datasets%20is%20lacking.%20To%20address%20this%20gap%2C%0Awe%20evaluated%20the%20available%20annotated%20HSI%20datasets%20on%20four%20deep%20learning-based%0Abaseline%20SSMs%3A%20DeepLab%20v3%2B%2C%20HRNet%2C%20PSPNet%2C%20and%20U-Net%2C%20along%20with%20its%20two%0Avariants%3A%20Coordinate%20Attention%20%28UNet-CA%29%20and%20Convolutional%20Block-Attention%0AModule%20%28UNet-CBAM%29.%20The%20original%20model%20architectures%20were%20adapted%20to%20handle%20the%0Avarying%20spatial%20and%20spectral%20dimensions%20of%20the%20datasets.%20These%20baseline%20SSMs%0Awere%20trained%20using%20a%20class-weighted%20loss%20function%20for%20individual%20HSI%20datasets%0Aand%20evaluated%20using%20mean-based%20metrics%20such%20as%20intersection%20over%20union%20%28IoU%29%2C%0Arecall%2C%20precision%2C%20F1%20score%2C%20specificity%2C%20and%20accuracy.%20Our%20results%20indicate%0Athat%20UNet-CBAM%2C%20which%20extracts%20channel-wise%20features%2C%20outperforms%20other%20SSMs%0Aand%20shows%20potential%20to%20leverage%20spectral%20information%20for%20enhanced%20semantic%0Asegmentation.%20This%20study%20establishes%20a%20baseline%20SSM%20benchmark%20on%20available%0Aannotated%20datasets%20for%20future%20evaluation%20of%20HSI-based%20ADAS%20perception.%20However%2C%0Alimitations%20of%20current%20HSI%20datasets%2C%20such%20as%20limited%20dataset%20size%2C%20high%20class%0Aimbalance%2C%20and%20lack%20of%20fine-grained%20annotations%2C%20remain%20significant%20constraints%0Afor%20developing%20robust%20SSMs%20for%20ADAS%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22101v2&entry.124074799=Read"},
{"title": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images", "author": "Virmarie Maquiling and Sean Anthony Byrne and Diederick C. Niehorster and Marco Carminati and Enkelejda Kasneci", "abstract": "  We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research.\n", "link": "http://arxiv.org/abs/2410.08926v2", "date": "2024-12-12", "relevancy": 2.6579, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5413}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5392}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Pupil%20Segmentation%20with%20SAM%202%3A%20A%20Case%20Study%20of%20Over%2014%20Million%0A%20%20Images&body=Title%3A%20Zero-Shot%20Pupil%20Segmentation%20with%20SAM%202%3A%20A%20Case%20Study%20of%20Over%2014%20Million%0A%20%20Images%0AAuthor%3A%20Virmarie%20Maquiling%20and%20Sean%20Anthony%20Byrne%20and%20Diederick%20C.%20Niehorster%20and%20Marco%20Carminati%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20We%20explore%20the%20transformative%20potential%20of%20SAM%202%2C%20a%20vision%20foundation%20model%2C%0Ain%20advancing%20gaze%20estimation%20and%20eye%20tracking%20technologies.%20By%20significantly%0Areducing%20annotation%20time%2C%20lowering%20technical%20barriers%20through%20its%20ease%20of%0Adeployment%2C%20and%20enhancing%20segmentation%20accuracy%2C%20SAM%202%20addresses%20critical%0Achallenges%20faced%20by%20researchers%20and%20practitioners.%20Utilizing%20its%20zero-shot%0Asegmentation%20capabilities%20with%20minimal%20user%20input-a%20single%20click%20per%20video-we%0Atested%20SAM%202%20on%20over%2014%20million%20eye%20images%20from%20diverse%20datasets%2C%20including%0Avirtual%20reality%20setups%20and%20the%20world%27s%20largest%20unified%20dataset%20recorded%20using%0Awearable%20eye%20trackers.%20Remarkably%2C%20in%20pupil%20segmentation%20tasks%2C%20SAM%202%20matches%0Athe%20performance%20of%20domain-specific%20models%20trained%20solely%20on%20eye%20images%2C%0Aachieving%20competitive%20mean%20Intersection%20over%20Union%20%28mIoU%29%20scores%20of%20up%20to%2093%25%0Awithout%20fine-tuning.%20Additionally%2C%20we%20provide%20our%20code%20and%20segmentation%20masks%0Afor%20these%20widely%20used%20datasets%20to%20promote%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Pupil%2520Segmentation%2520with%2520SAM%25202%253A%2520A%2520Case%2520Study%2520of%2520Over%252014%2520Million%250A%2520%2520Images%26entry.906535625%3DVirmarie%2520Maquiling%2520and%2520Sean%2520Anthony%2520Byrne%2520and%2520Diederick%2520C.%2520Niehorster%2520and%2520Marco%2520Carminati%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520transformative%2520potential%2520of%2520SAM%25202%252C%2520a%2520vision%2520foundation%2520model%252C%250Ain%2520advancing%2520gaze%2520estimation%2520and%2520eye%2520tracking%2520technologies.%2520By%2520significantly%250Areducing%2520annotation%2520time%252C%2520lowering%2520technical%2520barriers%2520through%2520its%2520ease%2520of%250Adeployment%252C%2520and%2520enhancing%2520segmentation%2520accuracy%252C%2520SAM%25202%2520addresses%2520critical%250Achallenges%2520faced%2520by%2520researchers%2520and%2520practitioners.%2520Utilizing%2520its%2520zero-shot%250Asegmentation%2520capabilities%2520with%2520minimal%2520user%2520input-a%2520single%2520click%2520per%2520video-we%250Atested%2520SAM%25202%2520on%2520over%252014%2520million%2520eye%2520images%2520from%2520diverse%2520datasets%252C%2520including%250Avirtual%2520reality%2520setups%2520and%2520the%2520world%2527s%2520largest%2520unified%2520dataset%2520recorded%2520using%250Awearable%2520eye%2520trackers.%2520Remarkably%252C%2520in%2520pupil%2520segmentation%2520tasks%252C%2520SAM%25202%2520matches%250Athe%2520performance%2520of%2520domain-specific%2520models%2520trained%2520solely%2520on%2520eye%2520images%252C%250Aachieving%2520competitive%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520scores%2520of%2520up%2520to%252093%2525%250Awithout%2520fine-tuning.%2520Additionally%252C%2520we%2520provide%2520our%2520code%2520and%2520segmentation%2520masks%250Afor%2520these%2520widely%2520used%2520datasets%2520to%2520promote%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Pupil%20Segmentation%20with%20SAM%202%3A%20A%20Case%20Study%20of%20Over%2014%20Million%0A%20%20Images&entry.906535625=Virmarie%20Maquiling%20and%20Sean%20Anthony%20Byrne%20and%20Diederick%20C.%20Niehorster%20and%20Marco%20Carminati%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20We%20explore%20the%20transformative%20potential%20of%20SAM%202%2C%20a%20vision%20foundation%20model%2C%0Ain%20advancing%20gaze%20estimation%20and%20eye%20tracking%20technologies.%20By%20significantly%0Areducing%20annotation%20time%2C%20lowering%20technical%20barriers%20through%20its%20ease%20of%0Adeployment%2C%20and%20enhancing%20segmentation%20accuracy%2C%20SAM%202%20addresses%20critical%0Achallenges%20faced%20by%20researchers%20and%20practitioners.%20Utilizing%20its%20zero-shot%0Asegmentation%20capabilities%20with%20minimal%20user%20input-a%20single%20click%20per%20video-we%0Atested%20SAM%202%20on%20over%2014%20million%20eye%20images%20from%20diverse%20datasets%2C%20including%0Avirtual%20reality%20setups%20and%20the%20world%27s%20largest%20unified%20dataset%20recorded%20using%0Awearable%20eye%20trackers.%20Remarkably%2C%20in%20pupil%20segmentation%20tasks%2C%20SAM%202%20matches%0Athe%20performance%20of%20domain-specific%20models%20trained%20solely%20on%20eye%20images%2C%0Aachieving%20competitive%20mean%20Intersection%20over%20Union%20%28mIoU%29%20scores%20of%20up%20to%2093%25%0Awithout%20fine-tuning.%20Additionally%2C%20we%20provide%20our%20code%20and%20segmentation%20masks%0Afor%20these%20widely%20used%20datasets%20to%20promote%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08926v2&entry.124074799=Read"},
{"title": "GenEx: Generating an Explorable World", "author": "Taiming Lu and Tianmin Shu and Junfei Xiao and Luoxin Ye and Jiahao Wang and Cheng Peng and Chen Wei and Daniel Khashabi and Rama Chellappa and Alan Yuille and Jieneng Chen", "abstract": "  Understanding, navigating, and exploring the 3D physical real world has long\nbeen a central challenge in the development of artificial intelligence. In this\nwork, we take a step toward this goal by introducing GenEx, a system capable of\nplanning complex embodied world exploration, guided by its generative\nimagination that forms priors (expectations) about the surrounding\nenvironments. GenEx generates an entire 3D-consistent imaginative environment\nfrom as little as a single RGB image, bringing it to life through panoramic\nvideo streams. Leveraging scalable 3D world data curated from Unreal Engine,\nour generative model is rounded in the physical world. It captures a continuous\n360-degree environment with little effort, offering a boundless landscape for\nAI agents to explore and interact with. GenEx achieves high-quality world\ngeneration, robust loop consistency over long trajectories, and demonstrates\nstrong 3D capabilities such as consistency and active 3D mapping. Powered by\ngenerative imagination of the world, GPT-assisted agents are equipped to\nperform complex embodied tasks, including both goal-agnostic exploration and\ngoal-driven navigation. These agents utilize predictive expectation regarding\nunseen parts of the physical world to refine their beliefs, simulate different\noutcomes based on potential decisions, and make more informed choices. In\nsummary, we demonstrate that GenEx provides a transformative platform for\nadvancing embodied AI in imaginative spaces and brings potential for extending\nthese capabilities to real-world exploration.\n", "link": "http://arxiv.org/abs/2412.09624v1", "date": "2024-12-12", "relevancy": 2.6505, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.7311}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6509}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenEx%3A%20Generating%20an%20Explorable%20World&body=Title%3A%20GenEx%3A%20Generating%20an%20Explorable%20World%0AAuthor%3A%20Taiming%20Lu%20and%20Tianmin%20Shu%20and%20Junfei%20Xiao%20and%20Luoxin%20Ye%20and%20Jiahao%20Wang%20and%20Cheng%20Peng%20and%20Chen%20Wei%20and%20Daniel%20Khashabi%20and%20Rama%20Chellappa%20and%20Alan%20Yuille%20and%20Jieneng%20Chen%0AAbstract%3A%20%20%20Understanding%2C%20navigating%2C%20and%20exploring%20the%203D%20physical%20real%20world%20has%20long%0Abeen%20a%20central%20challenge%20in%20the%20development%20of%20artificial%20intelligence.%20In%20this%0Awork%2C%20we%20take%20a%20step%20toward%20this%20goal%20by%20introducing%20GenEx%2C%20a%20system%20capable%20of%0Aplanning%20complex%20embodied%20world%20exploration%2C%20guided%20by%20its%20generative%0Aimagination%20that%20forms%20priors%20%28expectations%29%20about%20the%20surrounding%0Aenvironments.%20GenEx%20generates%20an%20entire%203D-consistent%20imaginative%20environment%0Afrom%20as%20little%20as%20a%20single%20RGB%20image%2C%20bringing%20it%20to%20life%20through%20panoramic%0Avideo%20streams.%20Leveraging%20scalable%203D%20world%20data%20curated%20from%20Unreal%20Engine%2C%0Aour%20generative%20model%20is%20rounded%20in%20the%20physical%20world.%20It%20captures%20a%20continuous%0A360-degree%20environment%20with%20little%20effort%2C%20offering%20a%20boundless%20landscape%20for%0AAI%20agents%20to%20explore%20and%20interact%20with.%20GenEx%20achieves%20high-quality%20world%0Ageneration%2C%20robust%20loop%20consistency%20over%20long%20trajectories%2C%20and%20demonstrates%0Astrong%203D%20capabilities%20such%20as%20consistency%20and%20active%203D%20mapping.%20Powered%20by%0Agenerative%20imagination%20of%20the%20world%2C%20GPT-assisted%20agents%20are%20equipped%20to%0Aperform%20complex%20embodied%20tasks%2C%20including%20both%20goal-agnostic%20exploration%20and%0Agoal-driven%20navigation.%20These%20agents%20utilize%20predictive%20expectation%20regarding%0Aunseen%20parts%20of%20the%20physical%20world%20to%20refine%20their%20beliefs%2C%20simulate%20different%0Aoutcomes%20based%20on%20potential%20decisions%2C%20and%20make%20more%20informed%20choices.%20In%0Asummary%2C%20we%20demonstrate%20that%20GenEx%20provides%20a%20transformative%20platform%20for%0Aadvancing%20embodied%20AI%20in%20imaginative%20spaces%20and%20brings%20potential%20for%20extending%0Athese%20capabilities%20to%20real-world%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenEx%253A%2520Generating%2520an%2520Explorable%2520World%26entry.906535625%3DTaiming%2520Lu%2520and%2520Tianmin%2520Shu%2520and%2520Junfei%2520Xiao%2520and%2520Luoxin%2520Ye%2520and%2520Jiahao%2520Wang%2520and%2520Cheng%2520Peng%2520and%2520Chen%2520Wei%2520and%2520Daniel%2520Khashabi%2520and%2520Rama%2520Chellappa%2520and%2520Alan%2520Yuille%2520and%2520Jieneng%2520Chen%26entry.1292438233%3D%2520%2520Understanding%252C%2520navigating%252C%2520and%2520exploring%2520the%25203D%2520physical%2520real%2520world%2520has%2520long%250Abeen%2520a%2520central%2520challenge%2520in%2520the%2520development%2520of%2520artificial%2520intelligence.%2520In%2520this%250Awork%252C%2520we%2520take%2520a%2520step%2520toward%2520this%2520goal%2520by%2520introducing%2520GenEx%252C%2520a%2520system%2520capable%2520of%250Aplanning%2520complex%2520embodied%2520world%2520exploration%252C%2520guided%2520by%2520its%2520generative%250Aimagination%2520that%2520forms%2520priors%2520%2528expectations%2529%2520about%2520the%2520surrounding%250Aenvironments.%2520GenEx%2520generates%2520an%2520entire%25203D-consistent%2520imaginative%2520environment%250Afrom%2520as%2520little%2520as%2520a%2520single%2520RGB%2520image%252C%2520bringing%2520it%2520to%2520life%2520through%2520panoramic%250Avideo%2520streams.%2520Leveraging%2520scalable%25203D%2520world%2520data%2520curated%2520from%2520Unreal%2520Engine%252C%250Aour%2520generative%2520model%2520is%2520rounded%2520in%2520the%2520physical%2520world.%2520It%2520captures%2520a%2520continuous%250A360-degree%2520environment%2520with%2520little%2520effort%252C%2520offering%2520a%2520boundless%2520landscape%2520for%250AAI%2520agents%2520to%2520explore%2520and%2520interact%2520with.%2520GenEx%2520achieves%2520high-quality%2520world%250Ageneration%252C%2520robust%2520loop%2520consistency%2520over%2520long%2520trajectories%252C%2520and%2520demonstrates%250Astrong%25203D%2520capabilities%2520such%2520as%2520consistency%2520and%2520active%25203D%2520mapping.%2520Powered%2520by%250Agenerative%2520imagination%2520of%2520the%2520world%252C%2520GPT-assisted%2520agents%2520are%2520equipped%2520to%250Aperform%2520complex%2520embodied%2520tasks%252C%2520including%2520both%2520goal-agnostic%2520exploration%2520and%250Agoal-driven%2520navigation.%2520These%2520agents%2520utilize%2520predictive%2520expectation%2520regarding%250Aunseen%2520parts%2520of%2520the%2520physical%2520world%2520to%2520refine%2520their%2520beliefs%252C%2520simulate%2520different%250Aoutcomes%2520based%2520on%2520potential%2520decisions%252C%2520and%2520make%2520more%2520informed%2520choices.%2520In%250Asummary%252C%2520we%2520demonstrate%2520that%2520GenEx%2520provides%2520a%2520transformative%2520platform%2520for%250Aadvancing%2520embodied%2520AI%2520in%2520imaginative%2520spaces%2520and%2520brings%2520potential%2520for%2520extending%250Athese%2520capabilities%2520to%2520real-world%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenEx%3A%20Generating%20an%20Explorable%20World&entry.906535625=Taiming%20Lu%20and%20Tianmin%20Shu%20and%20Junfei%20Xiao%20and%20Luoxin%20Ye%20and%20Jiahao%20Wang%20and%20Cheng%20Peng%20and%20Chen%20Wei%20and%20Daniel%20Khashabi%20and%20Rama%20Chellappa%20and%20Alan%20Yuille%20and%20Jieneng%20Chen&entry.1292438233=%20%20Understanding%2C%20navigating%2C%20and%20exploring%20the%203D%20physical%20real%20world%20has%20long%0Abeen%20a%20central%20challenge%20in%20the%20development%20of%20artificial%20intelligence.%20In%20this%0Awork%2C%20we%20take%20a%20step%20toward%20this%20goal%20by%20introducing%20GenEx%2C%20a%20system%20capable%20of%0Aplanning%20complex%20embodied%20world%20exploration%2C%20guided%20by%20its%20generative%0Aimagination%20that%20forms%20priors%20%28expectations%29%20about%20the%20surrounding%0Aenvironments.%20GenEx%20generates%20an%20entire%203D-consistent%20imaginative%20environment%0Afrom%20as%20little%20as%20a%20single%20RGB%20image%2C%20bringing%20it%20to%20life%20through%20panoramic%0Avideo%20streams.%20Leveraging%20scalable%203D%20world%20data%20curated%20from%20Unreal%20Engine%2C%0Aour%20generative%20model%20is%20rounded%20in%20the%20physical%20world.%20It%20captures%20a%20continuous%0A360-degree%20environment%20with%20little%20effort%2C%20offering%20a%20boundless%20landscape%20for%0AAI%20agents%20to%20explore%20and%20interact%20with.%20GenEx%20achieves%20high-quality%20world%0Ageneration%2C%20robust%20loop%20consistency%20over%20long%20trajectories%2C%20and%20demonstrates%0Astrong%203D%20capabilities%20such%20as%20consistency%20and%20active%203D%20mapping.%20Powered%20by%0Agenerative%20imagination%20of%20the%20world%2C%20GPT-assisted%20agents%20are%20equipped%20to%0Aperform%20complex%20embodied%20tasks%2C%20including%20both%20goal-agnostic%20exploration%20and%0Agoal-driven%20navigation.%20These%20agents%20utilize%20predictive%20expectation%20regarding%0Aunseen%20parts%20of%20the%20physical%20world%20to%20refine%20their%20beliefs%2C%20simulate%20different%0Aoutcomes%20based%20on%20potential%20decisions%2C%20and%20make%20more%20informed%20choices.%20In%0Asummary%2C%20we%20demonstrate%20that%20GenEx%20provides%20a%20transformative%20platform%20for%0Aadvancing%20embodied%20AI%20in%20imaginative%20spaces%20and%20brings%20potential%20for%20extending%0Athese%20capabilities%20to%20real-world%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09624v1&entry.124074799=Read"},
{"title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey", "author": "Zhijie Nie and Zhangchi Feng and Mingxin Li and Cunwang Zhang and Yanzhao Zhang and Dingkun Long and Richong Zhang", "abstract": "  Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications, such as semantic matching, clustering,\nand information retrieval, continue to rely on text embeddings for their\nefficiency and effectiveness. In this survey, we categorize the interplay\nbetween LLMs and text embeddings into three overarching themes: (1)\nLLM-augmented text embedding, enhancing traditional embedding methods with\nLLMs; (2) LLMs as text embedders, utilizing their innate capabilities for\nembedding generation; and (3) Text embedding understanding with LLMs,\nleveraging LLMs to analyze and interpret embeddings. By organizing these\nefforts based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.\n", "link": "http://arxiv.org/abs/2412.09165v1", "date": "2024-12-12", "relevancy": 2.6501, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5521}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Text%20Embedding%20Meets%20Large%20Language%20Model%3A%20A%20Comprehensive%20Survey&body=Title%3A%20When%20Text%20Embedding%20Meets%20Large%20Language%20Model%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Zhijie%20Nie%20and%20Zhangchi%20Feng%20and%20Mingxin%20Li%20and%20Cunwang%20Zhang%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Richong%20Zhang%0AAbstract%3A%20%20%20Text%20embedding%20has%20become%20a%20foundational%20technology%20in%20natural%20language%0Aprocessing%20%28NLP%29%20during%20the%20deep%20learning%20era%2C%20driving%20advancements%20across%20a%0Awide%20array%20of%20downstream%20tasks.%20While%20many%20natural%20language%20understanding%0Achallenges%20can%20now%20be%20modeled%20using%20generative%20paradigms%20and%20leverage%20the%0Arobust%20generative%20and%20comprehension%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%2C%20numerous%20practical%20applications%2C%20such%20as%20semantic%20matching%2C%20clustering%2C%0Aand%20information%20retrieval%2C%20continue%20to%20rely%20on%20text%20embeddings%20for%20their%0Aefficiency%20and%20effectiveness.%20In%20this%20survey%2C%20we%20categorize%20the%20interplay%0Abetween%20LLMs%20and%20text%20embeddings%20into%20three%20overarching%20themes%3A%20%281%29%0ALLM-augmented%20text%20embedding%2C%20enhancing%20traditional%20embedding%20methods%20with%0ALLMs%3B%20%282%29%20LLMs%20as%20text%20embedders%2C%20utilizing%20their%20innate%20capabilities%20for%0Aembedding%20generation%3B%20and%20%283%29%20Text%20embedding%20understanding%20with%20LLMs%2C%0Aleveraging%20LLMs%20to%20analyze%20and%20interpret%20embeddings.%20By%20organizing%20these%0Aefforts%20based%20on%20interaction%20patterns%20rather%20than%20specific%20downstream%0Aapplications%2C%20we%20offer%20a%20novel%20and%20systematic%20overview%20of%20contributions%20from%0Avarious%20research%20and%20application%20domains%20in%20the%20era%20of%20LLMs.%20Furthermore%2C%20we%0Ahighlight%20the%20unresolved%20challenges%20that%20persisted%20in%20the%20pre-LLM%20era%20with%0Apre-trained%20language%20models%20%28PLMs%29%20and%20explore%20the%20emerging%20obstacles%20brought%0Aforth%20by%20LLMs.%20Building%20on%20this%20analysis%2C%20we%20outline%20prospective%20directions%20for%0Athe%20evolution%20of%20text%20embedding%2C%20addressing%20both%20theoretical%20and%20practical%0Aopportunities%20in%20the%20rapidly%20advancing%20landscape%20of%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Text%2520Embedding%2520Meets%2520Large%2520Language%2520Model%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DZhijie%2520Nie%2520and%2520Zhangchi%2520Feng%2520and%2520Mingxin%2520Li%2520and%2520Cunwang%2520Zhang%2520and%2520Yanzhao%2520Zhang%2520and%2520Dingkun%2520Long%2520and%2520Richong%2520Zhang%26entry.1292438233%3D%2520%2520Text%2520embedding%2520has%2520become%2520a%2520foundational%2520technology%2520in%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520during%2520the%2520deep%2520learning%2520era%252C%2520driving%2520advancements%2520across%2520a%250Awide%2520array%2520of%2520downstream%2520tasks.%2520While%2520many%2520natural%2520language%2520understanding%250Achallenges%2520can%2520now%2520be%2520modeled%2520using%2520generative%2520paradigms%2520and%2520leverage%2520the%250Arobust%2520generative%2520and%2520comprehension%2520capabilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520numerous%2520practical%2520applications%252C%2520such%2520as%2520semantic%2520matching%252C%2520clustering%252C%250Aand%2520information%2520retrieval%252C%2520continue%2520to%2520rely%2520on%2520text%2520embeddings%2520for%2520their%250Aefficiency%2520and%2520effectiveness.%2520In%2520this%2520survey%252C%2520we%2520categorize%2520the%2520interplay%250Abetween%2520LLMs%2520and%2520text%2520embeddings%2520into%2520three%2520overarching%2520themes%253A%2520%25281%2529%250ALLM-augmented%2520text%2520embedding%252C%2520enhancing%2520traditional%2520embedding%2520methods%2520with%250ALLMs%253B%2520%25282%2529%2520LLMs%2520as%2520text%2520embedders%252C%2520utilizing%2520their%2520innate%2520capabilities%2520for%250Aembedding%2520generation%253B%2520and%2520%25283%2529%2520Text%2520embedding%2520understanding%2520with%2520LLMs%252C%250Aleveraging%2520LLMs%2520to%2520analyze%2520and%2520interpret%2520embeddings.%2520By%2520organizing%2520these%250Aefforts%2520based%2520on%2520interaction%2520patterns%2520rather%2520than%2520specific%2520downstream%250Aapplications%252C%2520we%2520offer%2520a%2520novel%2520and%2520systematic%2520overview%2520of%2520contributions%2520from%250Avarious%2520research%2520and%2520application%2520domains%2520in%2520the%2520era%2520of%2520LLMs.%2520Furthermore%252C%2520we%250Ahighlight%2520the%2520unresolved%2520challenges%2520that%2520persisted%2520in%2520the%2520pre-LLM%2520era%2520with%250Apre-trained%2520language%2520models%2520%2528PLMs%2529%2520and%2520explore%2520the%2520emerging%2520obstacles%2520brought%250Aforth%2520by%2520LLMs.%2520Building%2520on%2520this%2520analysis%252C%2520we%2520outline%2520prospective%2520directions%2520for%250Athe%2520evolution%2520of%2520text%2520embedding%252C%2520addressing%2520both%2520theoretical%2520and%2520practical%250Aopportunities%2520in%2520the%2520rapidly%2520advancing%2520landscape%2520of%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Text%20Embedding%20Meets%20Large%20Language%20Model%3A%20A%20Comprehensive%20Survey&entry.906535625=Zhijie%20Nie%20and%20Zhangchi%20Feng%20and%20Mingxin%20Li%20and%20Cunwang%20Zhang%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Richong%20Zhang&entry.1292438233=%20%20Text%20embedding%20has%20become%20a%20foundational%20technology%20in%20natural%20language%0Aprocessing%20%28NLP%29%20during%20the%20deep%20learning%20era%2C%20driving%20advancements%20across%20a%0Awide%20array%20of%20downstream%20tasks.%20While%20many%20natural%20language%20understanding%0Achallenges%20can%20now%20be%20modeled%20using%20generative%20paradigms%20and%20leverage%20the%0Arobust%20generative%20and%20comprehension%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%2C%20numerous%20practical%20applications%2C%20such%20as%20semantic%20matching%2C%20clustering%2C%0Aand%20information%20retrieval%2C%20continue%20to%20rely%20on%20text%20embeddings%20for%20their%0Aefficiency%20and%20effectiveness.%20In%20this%20survey%2C%20we%20categorize%20the%20interplay%0Abetween%20LLMs%20and%20text%20embeddings%20into%20three%20overarching%20themes%3A%20%281%29%0ALLM-augmented%20text%20embedding%2C%20enhancing%20traditional%20embedding%20methods%20with%0ALLMs%3B%20%282%29%20LLMs%20as%20text%20embedders%2C%20utilizing%20their%20innate%20capabilities%20for%0Aembedding%20generation%3B%20and%20%283%29%20Text%20embedding%20understanding%20with%20LLMs%2C%0Aleveraging%20LLMs%20to%20analyze%20and%20interpret%20embeddings.%20By%20organizing%20these%0Aefforts%20based%20on%20interaction%20patterns%20rather%20than%20specific%20downstream%0Aapplications%2C%20we%20offer%20a%20novel%20and%20systematic%20overview%20of%20contributions%20from%0Avarious%20research%20and%20application%20domains%20in%20the%20era%20of%20LLMs.%20Furthermore%2C%20we%0Ahighlight%20the%20unresolved%20challenges%20that%20persisted%20in%20the%20pre-LLM%20era%20with%0Apre-trained%20language%20models%20%28PLMs%29%20and%20explore%20the%20emerging%20obstacles%20brought%0Aforth%20by%20LLMs.%20Building%20on%20this%20analysis%2C%20we%20outline%20prospective%20directions%20for%0Athe%20evolution%20of%20text%20embedding%2C%20addressing%20both%20theoretical%20and%20practical%0Aopportunities%20in%20the%20rapidly%20advancing%20landscape%20of%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09165v1&entry.124074799=Read"},
{"title": "FD2-Net: Frequency-Driven Feature Decomposition Network for\n  Infrared-Visible Object Detection", "author": "Ke Li and Di Wang and Zhangyuan Hu and Shaofeng Li and Weiping Ni and Lin Zhao and Quan Wang", "abstract": "  Infrared-visible object detection (IVOD) seeks to harness the complementary\ninformation in infrared and visible images, thereby enhancing the performance\nof detectors in complex environments. However, existing methods often neglect\nthe frequency characteristics of complementary information, such as the\nabundant high-frequency details in visible images and the valuable\nlow-frequency thermal information in infrared images, thus constraining\ndetection performance. To solve this problem, we introduce a novel\nFrequency-Driven Feature Decomposition Network for IVOD, called FD2-Net, which\neffectively captures the unique frequency representations of complementary\ninformation across multimodal visual spaces. Specifically, we propose a feature\ndecomposition encoder, wherein the high-frequency unit (HFU) utilizes discrete\ncosine transform to capture representative high-frequency features, while the\nlow-frequency unit (LFU) employs dynamic receptive fields to model the\nmulti-scale context of diverse objects. Next, we adopt a parameter-free\ncomplementary strengths strategy to enhance multimodal features through\nseamless inter-frequency recoupling. Furthermore, we innovatively design a\nmultimodal reconstruction mechanism that recovers image details lost during\nfeature extraction, further leveraging the complementary information from\ninfrared and visible images to enhance overall representational capacity.\nExtensive experiments demonstrate that FD2-Net outperforms state-of-the-art\n(SOTA) models across various IVOD benchmarks, i.e. LLVIP (96.2% mAP), FLIR\n(82.9% mAP), and M3FD (83.5% mAP).\n", "link": "http://arxiv.org/abs/2412.09258v1", "date": "2024-12-12", "relevancy": 2.6265, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FD2-Net%3A%20Frequency-Driven%20Feature%20Decomposition%20Network%20for%0A%20%20Infrared-Visible%20Object%20Detection&body=Title%3A%20FD2-Net%3A%20Frequency-Driven%20Feature%20Decomposition%20Network%20for%0A%20%20Infrared-Visible%20Object%20Detection%0AAuthor%3A%20Ke%20Li%20and%20Di%20Wang%20and%20Zhangyuan%20Hu%20and%20Shaofeng%20Li%20and%20Weiping%20Ni%20and%20Lin%20Zhao%20and%20Quan%20Wang%0AAbstract%3A%20%20%20Infrared-visible%20object%20detection%20%28IVOD%29%20seeks%20to%20harness%20the%20complementary%0Ainformation%20in%20infrared%20and%20visible%20images%2C%20thereby%20enhancing%20the%20performance%0Aof%20detectors%20in%20complex%20environments.%20However%2C%20existing%20methods%20often%20neglect%0Athe%20frequency%20characteristics%20of%20complementary%20information%2C%20such%20as%20the%0Aabundant%20high-frequency%20details%20in%20visible%20images%20and%20the%20valuable%0Alow-frequency%20thermal%20information%20in%20infrared%20images%2C%20thus%20constraining%0Adetection%20performance.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%0AFrequency-Driven%20Feature%20Decomposition%20Network%20for%20IVOD%2C%20called%20FD2-Net%2C%20which%0Aeffectively%20captures%20the%20unique%20frequency%20representations%20of%20complementary%0Ainformation%20across%20multimodal%20visual%20spaces.%20Specifically%2C%20we%20propose%20a%20feature%0Adecomposition%20encoder%2C%20wherein%20the%20high-frequency%20unit%20%28HFU%29%20utilizes%20discrete%0Acosine%20transform%20to%20capture%20representative%20high-frequency%20features%2C%20while%20the%0Alow-frequency%20unit%20%28LFU%29%20employs%20dynamic%20receptive%20fields%20to%20model%20the%0Amulti-scale%20context%20of%20diverse%20objects.%20Next%2C%20we%20adopt%20a%20parameter-free%0Acomplementary%20strengths%20strategy%20to%20enhance%20multimodal%20features%20through%0Aseamless%20inter-frequency%20recoupling.%20Furthermore%2C%20we%20innovatively%20design%20a%0Amultimodal%20reconstruction%20mechanism%20that%20recovers%20image%20details%20lost%20during%0Afeature%20extraction%2C%20further%20leveraging%20the%20complementary%20information%20from%0Ainfrared%20and%20visible%20images%20to%20enhance%20overall%20representational%20capacity.%0AExtensive%20experiments%20demonstrate%20that%20FD2-Net%20outperforms%20state-of-the-art%0A%28SOTA%29%20models%20across%20various%20IVOD%20benchmarks%2C%20i.e.%20LLVIP%20%2896.2%25%20mAP%29%2C%20FLIR%0A%2882.9%25%20mAP%29%2C%20and%20M3FD%20%2883.5%25%20mAP%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFD2-Net%253A%2520Frequency-Driven%2520Feature%2520Decomposition%2520Network%2520for%250A%2520%2520Infrared-Visible%2520Object%2520Detection%26entry.906535625%3DKe%2520Li%2520and%2520Di%2520Wang%2520and%2520Zhangyuan%2520Hu%2520and%2520Shaofeng%2520Li%2520and%2520Weiping%2520Ni%2520and%2520Lin%2520Zhao%2520and%2520Quan%2520Wang%26entry.1292438233%3D%2520%2520Infrared-visible%2520object%2520detection%2520%2528IVOD%2529%2520seeks%2520to%2520harness%2520the%2520complementary%250Ainformation%2520in%2520infrared%2520and%2520visible%2520images%252C%2520thereby%2520enhancing%2520the%2520performance%250Aof%2520detectors%2520in%2520complex%2520environments.%2520However%252C%2520existing%2520methods%2520often%2520neglect%250Athe%2520frequency%2520characteristics%2520of%2520complementary%2520information%252C%2520such%2520as%2520the%250Aabundant%2520high-frequency%2520details%2520in%2520visible%2520images%2520and%2520the%2520valuable%250Alow-frequency%2520thermal%2520information%2520in%2520infrared%2520images%252C%2520thus%2520constraining%250Adetection%2520performance.%2520To%2520solve%2520this%2520problem%252C%2520we%2520introduce%2520a%2520novel%250AFrequency-Driven%2520Feature%2520Decomposition%2520Network%2520for%2520IVOD%252C%2520called%2520FD2-Net%252C%2520which%250Aeffectively%2520captures%2520the%2520unique%2520frequency%2520representations%2520of%2520complementary%250Ainformation%2520across%2520multimodal%2520visual%2520spaces.%2520Specifically%252C%2520we%2520propose%2520a%2520feature%250Adecomposition%2520encoder%252C%2520wherein%2520the%2520high-frequency%2520unit%2520%2528HFU%2529%2520utilizes%2520discrete%250Acosine%2520transform%2520to%2520capture%2520representative%2520high-frequency%2520features%252C%2520while%2520the%250Alow-frequency%2520unit%2520%2528LFU%2529%2520employs%2520dynamic%2520receptive%2520fields%2520to%2520model%2520the%250Amulti-scale%2520context%2520of%2520diverse%2520objects.%2520Next%252C%2520we%2520adopt%2520a%2520parameter-free%250Acomplementary%2520strengths%2520strategy%2520to%2520enhance%2520multimodal%2520features%2520through%250Aseamless%2520inter-frequency%2520recoupling.%2520Furthermore%252C%2520we%2520innovatively%2520design%2520a%250Amultimodal%2520reconstruction%2520mechanism%2520that%2520recovers%2520image%2520details%2520lost%2520during%250Afeature%2520extraction%252C%2520further%2520leveraging%2520the%2520complementary%2520information%2520from%250Ainfrared%2520and%2520visible%2520images%2520to%2520enhance%2520overall%2520representational%2520capacity.%250AExtensive%2520experiments%2520demonstrate%2520that%2520FD2-Net%2520outperforms%2520state-of-the-art%250A%2528SOTA%2529%2520models%2520across%2520various%2520IVOD%2520benchmarks%252C%2520i.e.%2520LLVIP%2520%252896.2%2525%2520mAP%2529%252C%2520FLIR%250A%252882.9%2525%2520mAP%2529%252C%2520and%2520M3FD%2520%252883.5%2525%2520mAP%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FD2-Net%3A%20Frequency-Driven%20Feature%20Decomposition%20Network%20for%0A%20%20Infrared-Visible%20Object%20Detection&entry.906535625=Ke%20Li%20and%20Di%20Wang%20and%20Zhangyuan%20Hu%20and%20Shaofeng%20Li%20and%20Weiping%20Ni%20and%20Lin%20Zhao%20and%20Quan%20Wang&entry.1292438233=%20%20Infrared-visible%20object%20detection%20%28IVOD%29%20seeks%20to%20harness%20the%20complementary%0Ainformation%20in%20infrared%20and%20visible%20images%2C%20thereby%20enhancing%20the%20performance%0Aof%20detectors%20in%20complex%20environments.%20However%2C%20existing%20methods%20often%20neglect%0Athe%20frequency%20characteristics%20of%20complementary%20information%2C%20such%20as%20the%0Aabundant%20high-frequency%20details%20in%20visible%20images%20and%20the%20valuable%0Alow-frequency%20thermal%20information%20in%20infrared%20images%2C%20thus%20constraining%0Adetection%20performance.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%0AFrequency-Driven%20Feature%20Decomposition%20Network%20for%20IVOD%2C%20called%20FD2-Net%2C%20which%0Aeffectively%20captures%20the%20unique%20frequency%20representations%20of%20complementary%0Ainformation%20across%20multimodal%20visual%20spaces.%20Specifically%2C%20we%20propose%20a%20feature%0Adecomposition%20encoder%2C%20wherein%20the%20high-frequency%20unit%20%28HFU%29%20utilizes%20discrete%0Acosine%20transform%20to%20capture%20representative%20high-frequency%20features%2C%20while%20the%0Alow-frequency%20unit%20%28LFU%29%20employs%20dynamic%20receptive%20fields%20to%20model%20the%0Amulti-scale%20context%20of%20diverse%20objects.%20Next%2C%20we%20adopt%20a%20parameter-free%0Acomplementary%20strengths%20strategy%20to%20enhance%20multimodal%20features%20through%0Aseamless%20inter-frequency%20recoupling.%20Furthermore%2C%20we%20innovatively%20design%20a%0Amultimodal%20reconstruction%20mechanism%20that%20recovers%20image%20details%20lost%20during%0Afeature%20extraction%2C%20further%20leveraging%20the%20complementary%20information%20from%0Ainfrared%20and%20visible%20images%20to%20enhance%20overall%20representational%20capacity.%0AExtensive%20experiments%20demonstrate%20that%20FD2-Net%20outperforms%20state-of-the-art%0A%28SOTA%29%20models%20across%20various%20IVOD%20benchmarks%2C%20i.e.%20LLVIP%20%2896.2%25%20mAP%29%2C%20FLIR%0A%2882.9%25%20mAP%29%2C%20and%20M3FD%20%2883.5%25%20mAP%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09258v1&entry.124074799=Read"},
{"title": "Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large\n  Language Models Reasoning", "author": "Xinlu Zhang and Zhiyu Zoey Chen and Xi Ye and Xianjun Yang and Lichang Chen and William Yang Wang and Linda Ruth Petzold", "abstract": "  Instruction Fine-Tuning (IFT) significantly enhances the zero-shot\ncapabilities of pretrained Large Language Models (LLMs). While coding data is\nknown to boost LLM reasoning abilities during pretraining, its role in\nactivating internal reasoning capacities during IFT remains understudied. This\npaper investigates a key question: How does coding data impact LLMs' reasoning\ncapacities during IFT stage? To explore this, we thoroughly examine the impact\nof coding data across different coding data proportions, model families, sizes,\nand reasoning domains, from various perspectives. Specifically, we create three\nIFT datasets with increasing coding data proportions, fine-tune six LLM\nbackbones across different families and scales on these datasets, evaluate the\ntuned models' performance across twelve tasks in three reasoning domains, and\nanalyze the outcomes from three broad-to-granular perspectives: overall,\ndomain-level, and task-specific. Our holistic analysis provides valuable\ninsights into each perspective. First, coding data tuning enhances the overall\nreasoning capabilities of LLMs across different model families and scales.\nMoreover, while the impact of coding data varies by domain, it shows consistent\ntrends within each domain across different model families and scales.\nAdditionally, coding data generally provides comparable task-specific benefits\nacross model families, with optimal proportions in IFT datasets being\ntask-dependent.\n", "link": "http://arxiv.org/abs/2405.20535v2", "date": "2024-12-12", "relevancy": 2.6076, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Impact%20of%20Coding%20Data%20Instruction%20Fine-Tuning%20on%20Large%0A%20%20Language%20Models%20Reasoning&body=Title%3A%20Unveiling%20the%20Impact%20of%20Coding%20Data%20Instruction%20Fine-Tuning%20on%20Large%0A%20%20Language%20Models%20Reasoning%0AAuthor%3A%20Xinlu%20Zhang%20and%20Zhiyu%20Zoey%20Chen%20and%20Xi%20Ye%20and%20Xianjun%20Yang%20and%20Lichang%20Chen%20and%20William%20Yang%20Wang%20and%20Linda%20Ruth%20Petzold%0AAbstract%3A%20%20%20Instruction%20Fine-Tuning%20%28IFT%29%20significantly%20enhances%20the%20zero-shot%0Acapabilities%20of%20pretrained%20Large%20Language%20Models%20%28LLMs%29.%20While%20coding%20data%20is%0Aknown%20to%20boost%20LLM%20reasoning%20abilities%20during%20pretraining%2C%20its%20role%20in%0Aactivating%20internal%20reasoning%20capacities%20during%20IFT%20remains%20understudied.%20This%0Apaper%20investigates%20a%20key%20question%3A%20How%20does%20coding%20data%20impact%20LLMs%27%20reasoning%0Acapacities%20during%20IFT%20stage%3F%20To%20explore%20this%2C%20we%20thoroughly%20examine%20the%20impact%0Aof%20coding%20data%20across%20different%20coding%20data%20proportions%2C%20model%20families%2C%20sizes%2C%0Aand%20reasoning%20domains%2C%20from%20various%20perspectives.%20Specifically%2C%20we%20create%20three%0AIFT%20datasets%20with%20increasing%20coding%20data%20proportions%2C%20fine-tune%20six%20LLM%0Abackbones%20across%20different%20families%20and%20scales%20on%20these%20datasets%2C%20evaluate%20the%0Atuned%20models%27%20performance%20across%20twelve%20tasks%20in%20three%20reasoning%20domains%2C%20and%0Aanalyze%20the%20outcomes%20from%20three%20broad-to-granular%20perspectives%3A%20overall%2C%0Adomain-level%2C%20and%20task-specific.%20Our%20holistic%20analysis%20provides%20valuable%0Ainsights%20into%20each%20perspective.%20First%2C%20coding%20data%20tuning%20enhances%20the%20overall%0Areasoning%20capabilities%20of%20LLMs%20across%20different%20model%20families%20and%20scales.%0AMoreover%2C%20while%20the%20impact%20of%20coding%20data%20varies%20by%20domain%2C%20it%20shows%20consistent%0Atrends%20within%20each%20domain%20across%20different%20model%20families%20and%20scales.%0AAdditionally%2C%20coding%20data%20generally%20provides%20comparable%20task-specific%20benefits%0Aacross%20model%20families%2C%20with%20optimal%20proportions%20in%20IFT%20datasets%20being%0Atask-dependent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Impact%2520of%2520Coding%2520Data%2520Instruction%2520Fine-Tuning%2520on%2520Large%250A%2520%2520Language%2520Models%2520Reasoning%26entry.906535625%3DXinlu%2520Zhang%2520and%2520Zhiyu%2520Zoey%2520Chen%2520and%2520Xi%2520Ye%2520and%2520Xianjun%2520Yang%2520and%2520Lichang%2520Chen%2520and%2520William%2520Yang%2520Wang%2520and%2520Linda%2520Ruth%2520Petzold%26entry.1292438233%3D%2520%2520Instruction%2520Fine-Tuning%2520%2528IFT%2529%2520significantly%2520enhances%2520the%2520zero-shot%250Acapabilities%2520of%2520pretrained%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520While%2520coding%2520data%2520is%250Aknown%2520to%2520boost%2520LLM%2520reasoning%2520abilities%2520during%2520pretraining%252C%2520its%2520role%2520in%250Aactivating%2520internal%2520reasoning%2520capacities%2520during%2520IFT%2520remains%2520understudied.%2520This%250Apaper%2520investigates%2520a%2520key%2520question%253A%2520How%2520does%2520coding%2520data%2520impact%2520LLMs%2527%2520reasoning%250Acapacities%2520during%2520IFT%2520stage%253F%2520To%2520explore%2520this%252C%2520we%2520thoroughly%2520examine%2520the%2520impact%250Aof%2520coding%2520data%2520across%2520different%2520coding%2520data%2520proportions%252C%2520model%2520families%252C%2520sizes%252C%250Aand%2520reasoning%2520domains%252C%2520from%2520various%2520perspectives.%2520Specifically%252C%2520we%2520create%2520three%250AIFT%2520datasets%2520with%2520increasing%2520coding%2520data%2520proportions%252C%2520fine-tune%2520six%2520LLM%250Abackbones%2520across%2520different%2520families%2520and%2520scales%2520on%2520these%2520datasets%252C%2520evaluate%2520the%250Atuned%2520models%2527%2520performance%2520across%2520twelve%2520tasks%2520in%2520three%2520reasoning%2520domains%252C%2520and%250Aanalyze%2520the%2520outcomes%2520from%2520three%2520broad-to-granular%2520perspectives%253A%2520overall%252C%250Adomain-level%252C%2520and%2520task-specific.%2520Our%2520holistic%2520analysis%2520provides%2520valuable%250Ainsights%2520into%2520each%2520perspective.%2520First%252C%2520coding%2520data%2520tuning%2520enhances%2520the%2520overall%250Areasoning%2520capabilities%2520of%2520LLMs%2520across%2520different%2520model%2520families%2520and%2520scales.%250AMoreover%252C%2520while%2520the%2520impact%2520of%2520coding%2520data%2520varies%2520by%2520domain%252C%2520it%2520shows%2520consistent%250Atrends%2520within%2520each%2520domain%2520across%2520different%2520model%2520families%2520and%2520scales.%250AAdditionally%252C%2520coding%2520data%2520generally%2520provides%2520comparable%2520task-specific%2520benefits%250Aacross%2520model%2520families%252C%2520with%2520optimal%2520proportions%2520in%2520IFT%2520datasets%2520being%250Atask-dependent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Impact%20of%20Coding%20Data%20Instruction%20Fine-Tuning%20on%20Large%0A%20%20Language%20Models%20Reasoning&entry.906535625=Xinlu%20Zhang%20and%20Zhiyu%20Zoey%20Chen%20and%20Xi%20Ye%20and%20Xianjun%20Yang%20and%20Lichang%20Chen%20and%20William%20Yang%20Wang%20and%20Linda%20Ruth%20Petzold&entry.1292438233=%20%20Instruction%20Fine-Tuning%20%28IFT%29%20significantly%20enhances%20the%20zero-shot%0Acapabilities%20of%20pretrained%20Large%20Language%20Models%20%28LLMs%29.%20While%20coding%20data%20is%0Aknown%20to%20boost%20LLM%20reasoning%20abilities%20during%20pretraining%2C%20its%20role%20in%0Aactivating%20internal%20reasoning%20capacities%20during%20IFT%20remains%20understudied.%20This%0Apaper%20investigates%20a%20key%20question%3A%20How%20does%20coding%20data%20impact%20LLMs%27%20reasoning%0Acapacities%20during%20IFT%20stage%3F%20To%20explore%20this%2C%20we%20thoroughly%20examine%20the%20impact%0Aof%20coding%20data%20across%20different%20coding%20data%20proportions%2C%20model%20families%2C%20sizes%2C%0Aand%20reasoning%20domains%2C%20from%20various%20perspectives.%20Specifically%2C%20we%20create%20three%0AIFT%20datasets%20with%20increasing%20coding%20data%20proportions%2C%20fine-tune%20six%20LLM%0Abackbones%20across%20different%20families%20and%20scales%20on%20these%20datasets%2C%20evaluate%20the%0Atuned%20models%27%20performance%20across%20twelve%20tasks%20in%20three%20reasoning%20domains%2C%20and%0Aanalyze%20the%20outcomes%20from%20three%20broad-to-granular%20perspectives%3A%20overall%2C%0Adomain-level%2C%20and%20task-specific.%20Our%20holistic%20analysis%20provides%20valuable%0Ainsights%20into%20each%20perspective.%20First%2C%20coding%20data%20tuning%20enhances%20the%20overall%0Areasoning%20capabilities%20of%20LLMs%20across%20different%20model%20families%20and%20scales.%0AMoreover%2C%20while%20the%20impact%20of%20coding%20data%20varies%20by%20domain%2C%20it%20shows%20consistent%0Atrends%20within%20each%20domain%20across%20different%20model%20families%20and%20scales.%0AAdditionally%2C%20coding%20data%20generally%20provides%20comparable%20task-specific%20benefits%0Aacross%20model%20families%2C%20with%20optimal%20proportions%20in%20IFT%20datasets%20being%0Atask-dependent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20535v2&entry.124074799=Read"},
{"title": "Olympus: A Universal Task Router for Computer Vision Tasks", "author": "Yuanze Lin and Yunsheng Li and Dongdong Chen and Weijian Xu and Ronald Clark and Philip H. S. Torr", "abstract": "  We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttps://github.com/yuanze-lin/Olympus_page\n", "link": "http://arxiv.org/abs/2412.09612v1", "date": "2024-12-12", "relevancy": 2.6039, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Olympus%3A%20A%20Universal%20Task%20Router%20for%20Computer%20Vision%20Tasks&body=Title%3A%20Olympus%3A%20A%20Universal%20Task%20Router%20for%20Computer%20Vision%20Tasks%0AAuthor%3A%20Yuanze%20Lin%20and%20Yunsheng%20Li%20and%20Dongdong%20Chen%20and%20Weijian%20Xu%20and%20Ronald%20Clark%20and%20Philip%20H.%20S.%20Torr%0AAbstract%3A%20%20%20We%20introduce%20Olympus%2C%20a%20new%20approach%20that%20transforms%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20into%20a%20unified%20framework%20capable%20of%20handling%20a%20wide%0Aarray%20of%20computer%20vision%20tasks.%20Utilizing%20a%20controller%20MLLM%2C%20Olympus%20delegates%0Aover%2020%20specialized%20tasks%20across%20images%2C%20videos%2C%20and%203D%20objects%20to%20dedicated%0Amodules.%20This%20instruction-based%20routing%20enables%20complex%20workflows%20through%0Achained%20actions%20without%20the%20need%20for%20training%20heavy%20generative%20models.%20Olympus%0Aeasily%20integrates%20with%20existing%20MLLMs%2C%20expanding%20their%20capabilities%20with%0Acomparable%20performance.%20Experimental%20results%20demonstrate%20that%20Olympus%20achieves%0Aan%20average%20routing%20accuracy%20of%2094.75%25%20across%2020%20tasks%20and%20precision%20of%2091.82%25%0Ain%20chained%20action%20scenarios%2C%20showcasing%20its%20effectiveness%20as%20a%20universal%20task%0Arouter%20that%20can%20solve%20a%20diverse%20range%20of%20computer%20vision%20tasks.%20Project%20page%3A%0Ahttps%3A//github.com/yuanze-lin/Olympus_page%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOlympus%253A%2520A%2520Universal%2520Task%2520Router%2520for%2520Computer%2520Vision%2520Tasks%26entry.906535625%3DYuanze%2520Lin%2520and%2520Yunsheng%2520Li%2520and%2520Dongdong%2520Chen%2520and%2520Weijian%2520Xu%2520and%2520Ronald%2520Clark%2520and%2520Philip%2520H.%2520S.%2520Torr%26entry.1292438233%3D%2520%2520We%2520introduce%2520Olympus%252C%2520a%2520new%2520approach%2520that%2520transforms%2520Multimodal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529%2520into%2520a%2520unified%2520framework%2520capable%2520of%2520handling%2520a%2520wide%250Aarray%2520of%2520computer%2520vision%2520tasks.%2520Utilizing%2520a%2520controller%2520MLLM%252C%2520Olympus%2520delegates%250Aover%252020%2520specialized%2520tasks%2520across%2520images%252C%2520videos%252C%2520and%25203D%2520objects%2520to%2520dedicated%250Amodules.%2520This%2520instruction-based%2520routing%2520enables%2520complex%2520workflows%2520through%250Achained%2520actions%2520without%2520the%2520need%2520for%2520training%2520heavy%2520generative%2520models.%2520Olympus%250Aeasily%2520integrates%2520with%2520existing%2520MLLMs%252C%2520expanding%2520their%2520capabilities%2520with%250Acomparable%2520performance.%2520Experimental%2520results%2520demonstrate%2520that%2520Olympus%2520achieves%250Aan%2520average%2520routing%2520accuracy%2520of%252094.75%2525%2520across%252020%2520tasks%2520and%2520precision%2520of%252091.82%2525%250Ain%2520chained%2520action%2520scenarios%252C%2520showcasing%2520its%2520effectiveness%2520as%2520a%2520universal%2520task%250Arouter%2520that%2520can%2520solve%2520a%2520diverse%2520range%2520of%2520computer%2520vision%2520tasks.%2520Project%2520page%253A%250Ahttps%253A//github.com/yuanze-lin/Olympus_page%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Olympus%3A%20A%20Universal%20Task%20Router%20for%20Computer%20Vision%20Tasks&entry.906535625=Yuanze%20Lin%20and%20Yunsheng%20Li%20and%20Dongdong%20Chen%20and%20Weijian%20Xu%20and%20Ronald%20Clark%20and%20Philip%20H.%20S.%20Torr&entry.1292438233=%20%20We%20introduce%20Olympus%2C%20a%20new%20approach%20that%20transforms%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20into%20a%20unified%20framework%20capable%20of%20handling%20a%20wide%0Aarray%20of%20computer%20vision%20tasks.%20Utilizing%20a%20controller%20MLLM%2C%20Olympus%20delegates%0Aover%2020%20specialized%20tasks%20across%20images%2C%20videos%2C%20and%203D%20objects%20to%20dedicated%0Amodules.%20This%20instruction-based%20routing%20enables%20complex%20workflows%20through%0Achained%20actions%20without%20the%20need%20for%20training%20heavy%20generative%20models.%20Olympus%0Aeasily%20integrates%20with%20existing%20MLLMs%2C%20expanding%20their%20capabilities%20with%0Acomparable%20performance.%20Experimental%20results%20demonstrate%20that%20Olympus%20achieves%0Aan%20average%20routing%20accuracy%20of%2094.75%25%20across%2020%20tasks%20and%20precision%20of%2091.82%25%0Ain%20chained%20action%20scenarios%2C%20showcasing%20its%20effectiveness%20as%20a%20universal%20task%0Arouter%20that%20can%20solve%20a%20diverse%20range%20of%20computer%20vision%20tasks.%20Project%20page%3A%0Ahttps%3A//github.com/yuanze-lin/Olympus_page%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09612v1&entry.124074799=Read"},
{"title": "Video Creation by Demonstration", "author": "Yihong Sun and Hao Zhou and Liangzhe Yuan and Jennifer J. Sun and Yandong Li and Xuhui Jia and Hartwig Adam and Bharath Hariharan and Long Zhao and Ting Liu", "abstract": "  We explore a novel video creation experience, namely Video Creation by\nDemonstration. Given a demonstration video and a context image from a different\nscene, we generate a physically plausible video that continues naturally from\nthe context image and carries out the action concepts from the demonstration.\nTo enable this capability, we present $\\delta$-Diffusion, a self-supervised\ntraining approach that learns from unlabeled videos by conditional future frame\nprediction. Unlike most existing video generation controls that are based on\nexplicit signals, we adopts the form of implicit latent control for maximal\nflexibility and expressiveness required by general videos. By leveraging a\nvideo foundation model with an appearance bottleneck design on top, we extract\naction latents from demonstration videos for conditioning the generation\nprocess with minimal appearance leakage. Empirically, $\\delta$-Diffusion\noutperforms related baselines in terms of both human preference and large-scale\nmachine evaluations, and demonstrates potentials towards interactive world\nsimulation. Sampled video generation results are available at\nhttps://delta-diffusion.github.io/.\n", "link": "http://arxiv.org/abs/2412.09551v1", "date": "2024-12-12", "relevancy": 2.5961, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6614}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6513}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Creation%20by%20Demonstration&body=Title%3A%20Video%20Creation%20by%20Demonstration%0AAuthor%3A%20Yihong%20Sun%20and%20Hao%20Zhou%20and%20Liangzhe%20Yuan%20and%20Jennifer%20J.%20Sun%20and%20Yandong%20Li%20and%20Xuhui%20Jia%20and%20Hartwig%20Adam%20and%20Bharath%20Hariharan%20and%20Long%20Zhao%20and%20Ting%20Liu%0AAbstract%3A%20%20%20We%20explore%20a%20novel%20video%20creation%20experience%2C%20namely%20Video%20Creation%20by%0ADemonstration.%20Given%20a%20demonstration%20video%20and%20a%20context%20image%20from%20a%20different%0Ascene%2C%20we%20generate%20a%20physically%20plausible%20video%20that%20continues%20naturally%20from%0Athe%20context%20image%20and%20carries%20out%20the%20action%20concepts%20from%20the%20demonstration.%0ATo%20enable%20this%20capability%2C%20we%20present%20%24%5Cdelta%24-Diffusion%2C%20a%20self-supervised%0Atraining%20approach%20that%20learns%20from%20unlabeled%20videos%20by%20conditional%20future%20frame%0Aprediction.%20Unlike%20most%20existing%20video%20generation%20controls%20that%20are%20based%20on%0Aexplicit%20signals%2C%20we%20adopts%20the%20form%20of%20implicit%20latent%20control%20for%20maximal%0Aflexibility%20and%20expressiveness%20required%20by%20general%20videos.%20By%20leveraging%20a%0Avideo%20foundation%20model%20with%20an%20appearance%20bottleneck%20design%20on%20top%2C%20we%20extract%0Aaction%20latents%20from%20demonstration%20videos%20for%20conditioning%20the%20generation%0Aprocess%20with%20minimal%20appearance%20leakage.%20Empirically%2C%20%24%5Cdelta%24-Diffusion%0Aoutperforms%20related%20baselines%20in%20terms%20of%20both%20human%20preference%20and%20large-scale%0Amachine%20evaluations%2C%20and%20demonstrates%20potentials%20towards%20interactive%20world%0Asimulation.%20Sampled%20video%20generation%20results%20are%20available%20at%0Ahttps%3A//delta-diffusion.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Creation%2520by%2520Demonstration%26entry.906535625%3DYihong%2520Sun%2520and%2520Hao%2520Zhou%2520and%2520Liangzhe%2520Yuan%2520and%2520Jennifer%2520J.%2520Sun%2520and%2520Yandong%2520Li%2520and%2520Xuhui%2520Jia%2520and%2520Hartwig%2520Adam%2520and%2520Bharath%2520Hariharan%2520and%2520Long%2520Zhao%2520and%2520Ting%2520Liu%26entry.1292438233%3D%2520%2520We%2520explore%2520a%2520novel%2520video%2520creation%2520experience%252C%2520namely%2520Video%2520Creation%2520by%250ADemonstration.%2520Given%2520a%2520demonstration%2520video%2520and%2520a%2520context%2520image%2520from%2520a%2520different%250Ascene%252C%2520we%2520generate%2520a%2520physically%2520plausible%2520video%2520that%2520continues%2520naturally%2520from%250Athe%2520context%2520image%2520and%2520carries%2520out%2520the%2520action%2520concepts%2520from%2520the%2520demonstration.%250ATo%2520enable%2520this%2520capability%252C%2520we%2520present%2520%2524%255Cdelta%2524-Diffusion%252C%2520a%2520self-supervised%250Atraining%2520approach%2520that%2520learns%2520from%2520unlabeled%2520videos%2520by%2520conditional%2520future%2520frame%250Aprediction.%2520Unlike%2520most%2520existing%2520video%2520generation%2520controls%2520that%2520are%2520based%2520on%250Aexplicit%2520signals%252C%2520we%2520adopts%2520the%2520form%2520of%2520implicit%2520latent%2520control%2520for%2520maximal%250Aflexibility%2520and%2520expressiveness%2520required%2520by%2520general%2520videos.%2520By%2520leveraging%2520a%250Avideo%2520foundation%2520model%2520with%2520an%2520appearance%2520bottleneck%2520design%2520on%2520top%252C%2520we%2520extract%250Aaction%2520latents%2520from%2520demonstration%2520videos%2520for%2520conditioning%2520the%2520generation%250Aprocess%2520with%2520minimal%2520appearance%2520leakage.%2520Empirically%252C%2520%2524%255Cdelta%2524-Diffusion%250Aoutperforms%2520related%2520baselines%2520in%2520terms%2520of%2520both%2520human%2520preference%2520and%2520large-scale%250Amachine%2520evaluations%252C%2520and%2520demonstrates%2520potentials%2520towards%2520interactive%2520world%250Asimulation.%2520Sampled%2520video%2520generation%2520results%2520are%2520available%2520at%250Ahttps%253A//delta-diffusion.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Creation%20by%20Demonstration&entry.906535625=Yihong%20Sun%20and%20Hao%20Zhou%20and%20Liangzhe%20Yuan%20and%20Jennifer%20J.%20Sun%20and%20Yandong%20Li%20and%20Xuhui%20Jia%20and%20Hartwig%20Adam%20and%20Bharath%20Hariharan%20and%20Long%20Zhao%20and%20Ting%20Liu&entry.1292438233=%20%20We%20explore%20a%20novel%20video%20creation%20experience%2C%20namely%20Video%20Creation%20by%0ADemonstration.%20Given%20a%20demonstration%20video%20and%20a%20context%20image%20from%20a%20different%0Ascene%2C%20we%20generate%20a%20physically%20plausible%20video%20that%20continues%20naturally%20from%0Athe%20context%20image%20and%20carries%20out%20the%20action%20concepts%20from%20the%20demonstration.%0ATo%20enable%20this%20capability%2C%20we%20present%20%24%5Cdelta%24-Diffusion%2C%20a%20self-supervised%0Atraining%20approach%20that%20learns%20from%20unlabeled%20videos%20by%20conditional%20future%20frame%0Aprediction.%20Unlike%20most%20existing%20video%20generation%20controls%20that%20are%20based%20on%0Aexplicit%20signals%2C%20we%20adopts%20the%20form%20of%20implicit%20latent%20control%20for%20maximal%0Aflexibility%20and%20expressiveness%20required%20by%20general%20videos.%20By%20leveraging%20a%0Avideo%20foundation%20model%20with%20an%20appearance%20bottleneck%20design%20on%20top%2C%20we%20extract%0Aaction%20latents%20from%20demonstration%20videos%20for%20conditioning%20the%20generation%0Aprocess%20with%20minimal%20appearance%20leakage.%20Empirically%2C%20%24%5Cdelta%24-Diffusion%0Aoutperforms%20related%20baselines%20in%20terms%20of%20both%20human%20preference%20and%20large-scale%0Amachine%20evaluations%2C%20and%20demonstrates%20potentials%20towards%20interactive%20world%0Asimulation.%20Sampled%20video%20generation%20results%20are%20available%20at%0Ahttps%3A//delta-diffusion.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09551v1&entry.124074799=Read"},
{"title": "Dynamic Prompt Allocation and Tuning for Continual Test-Time Adaptation", "author": "Chaoran Cui and Yongrui Zhen and Shuai Gong and Chunyun Zhang and Hui Liu and Yilong Yin", "abstract": "  Continual test-time adaptation (CTTA) has recently emerged to adapt a\npre-trained source model to continuously evolving target distributions, which\naccommodates the dynamic nature of real-world environments. To mitigate the\nrisk of catastrophic forgetting in CTTA, existing methods typically incorporate\nexplicit regularization terms to constrain the variation of model parameters.\nHowever, they cannot fundamentally resolve catastrophic forgetting because they\nrely on a single shared model to adapt across all target domains, which\ninevitably leads to severe inter-domain interference. In this paper, we\nintroduce learnable domain-specific prompts that guide the model to adapt to\ncorresponding target domains, thereby partially disentangling the parameter\nspace of different domains. In the absence of domain identity for target\nsamples, we propose a novel dynamic Prompt AllocatIon aNd Tuning (PAINT)\nmethod, which utilizes a query mechanism to dynamically determine whether the\ncurrent samples come from a known domain or an unexplored one. For known\ndomains, the corresponding domain-specific prompt is directly selected, while\nfor previously unseen domains, a new prompt is allocated. Prompt tuning is\nsubsequently performed using mutual information maximization along with\nstructural regularization. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of our PAINT method for CTTA. We have released\nour code at https://github.com/Cadezzyr/PAINT.\n", "link": "http://arxiv.org/abs/2412.09308v1", "date": "2024-12-12", "relevancy": 2.5814, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5202}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Prompt%20Allocation%20and%20Tuning%20for%20Continual%20Test-Time%20Adaptation&body=Title%3A%20Dynamic%20Prompt%20Allocation%20and%20Tuning%20for%20Continual%20Test-Time%20Adaptation%0AAuthor%3A%20Chaoran%20Cui%20and%20Yongrui%20Zhen%20and%20Shuai%20Gong%20and%20Chunyun%20Zhang%20and%20Hui%20Liu%20and%20Yilong%20Yin%0AAbstract%3A%20%20%20Continual%20test-time%20adaptation%20%28CTTA%29%20has%20recently%20emerged%20to%20adapt%20a%0Apre-trained%20source%20model%20to%20continuously%20evolving%20target%20distributions%2C%20which%0Aaccommodates%20the%20dynamic%20nature%20of%20real-world%20environments.%20To%20mitigate%20the%0Arisk%20of%20catastrophic%20forgetting%20in%20CTTA%2C%20existing%20methods%20typically%20incorporate%0Aexplicit%20regularization%20terms%20to%20constrain%20the%20variation%20of%20model%20parameters.%0AHowever%2C%20they%20cannot%20fundamentally%20resolve%20catastrophic%20forgetting%20because%20they%0Arely%20on%20a%20single%20shared%20model%20to%20adapt%20across%20all%20target%20domains%2C%20which%0Ainevitably%20leads%20to%20severe%20inter-domain%20interference.%20In%20this%20paper%2C%20we%0Aintroduce%20learnable%20domain-specific%20prompts%20that%20guide%20the%20model%20to%20adapt%20to%0Acorresponding%20target%20domains%2C%20thereby%20partially%20disentangling%20the%20parameter%0Aspace%20of%20different%20domains.%20In%20the%20absence%20of%20domain%20identity%20for%20target%0Asamples%2C%20we%20propose%20a%20novel%20dynamic%20Prompt%20AllocatIon%20aNd%20Tuning%20%28PAINT%29%0Amethod%2C%20which%20utilizes%20a%20query%20mechanism%20to%20dynamically%20determine%20whether%20the%0Acurrent%20samples%20come%20from%20a%20known%20domain%20or%20an%20unexplored%20one.%20For%20known%0Adomains%2C%20the%20corresponding%20domain-specific%20prompt%20is%20directly%20selected%2C%20while%0Afor%20previously%20unseen%20domains%2C%20a%20new%20prompt%20is%20allocated.%20Prompt%20tuning%20is%0Asubsequently%20performed%20using%20mutual%20information%20maximization%20along%20with%0Astructural%20regularization.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20PAINT%20method%20for%20CTTA.%20We%20have%20released%0Aour%20code%20at%20https%3A//github.com/Cadezzyr/PAINT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Prompt%2520Allocation%2520and%2520Tuning%2520for%2520Continual%2520Test-Time%2520Adaptation%26entry.906535625%3DChaoran%2520Cui%2520and%2520Yongrui%2520Zhen%2520and%2520Shuai%2520Gong%2520and%2520Chunyun%2520Zhang%2520and%2520Hui%2520Liu%2520and%2520Yilong%2520Yin%26entry.1292438233%3D%2520%2520Continual%2520test-time%2520adaptation%2520%2528CTTA%2529%2520has%2520recently%2520emerged%2520to%2520adapt%2520a%250Apre-trained%2520source%2520model%2520to%2520continuously%2520evolving%2520target%2520distributions%252C%2520which%250Aaccommodates%2520the%2520dynamic%2520nature%2520of%2520real-world%2520environments.%2520To%2520mitigate%2520the%250Arisk%2520of%2520catastrophic%2520forgetting%2520in%2520CTTA%252C%2520existing%2520methods%2520typically%2520incorporate%250Aexplicit%2520regularization%2520terms%2520to%2520constrain%2520the%2520variation%2520of%2520model%2520parameters.%250AHowever%252C%2520they%2520cannot%2520fundamentally%2520resolve%2520catastrophic%2520forgetting%2520because%2520they%250Arely%2520on%2520a%2520single%2520shared%2520model%2520to%2520adapt%2520across%2520all%2520target%2520domains%252C%2520which%250Ainevitably%2520leads%2520to%2520severe%2520inter-domain%2520interference.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520learnable%2520domain-specific%2520prompts%2520that%2520guide%2520the%2520model%2520to%2520adapt%2520to%250Acorresponding%2520target%2520domains%252C%2520thereby%2520partially%2520disentangling%2520the%2520parameter%250Aspace%2520of%2520different%2520domains.%2520In%2520the%2520absence%2520of%2520domain%2520identity%2520for%2520target%250Asamples%252C%2520we%2520propose%2520a%2520novel%2520dynamic%2520Prompt%2520AllocatIon%2520aNd%2520Tuning%2520%2528PAINT%2529%250Amethod%252C%2520which%2520utilizes%2520a%2520query%2520mechanism%2520to%2520dynamically%2520determine%2520whether%2520the%250Acurrent%2520samples%2520come%2520from%2520a%2520known%2520domain%2520or%2520an%2520unexplored%2520one.%2520For%2520known%250Adomains%252C%2520the%2520corresponding%2520domain-specific%2520prompt%2520is%2520directly%2520selected%252C%2520while%250Afor%2520previously%2520unseen%2520domains%252C%2520a%2520new%2520prompt%2520is%2520allocated.%2520Prompt%2520tuning%2520is%250Asubsequently%2520performed%2520using%2520mutual%2520information%2520maximization%2520along%2520with%250Astructural%2520regularization.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520PAINT%2520method%2520for%2520CTTA.%2520We%2520have%2520released%250Aour%2520code%2520at%2520https%253A//github.com/Cadezzyr/PAINT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Prompt%20Allocation%20and%20Tuning%20for%20Continual%20Test-Time%20Adaptation&entry.906535625=Chaoran%20Cui%20and%20Yongrui%20Zhen%20and%20Shuai%20Gong%20and%20Chunyun%20Zhang%20and%20Hui%20Liu%20and%20Yilong%20Yin&entry.1292438233=%20%20Continual%20test-time%20adaptation%20%28CTTA%29%20has%20recently%20emerged%20to%20adapt%20a%0Apre-trained%20source%20model%20to%20continuously%20evolving%20target%20distributions%2C%20which%0Aaccommodates%20the%20dynamic%20nature%20of%20real-world%20environments.%20To%20mitigate%20the%0Arisk%20of%20catastrophic%20forgetting%20in%20CTTA%2C%20existing%20methods%20typically%20incorporate%0Aexplicit%20regularization%20terms%20to%20constrain%20the%20variation%20of%20model%20parameters.%0AHowever%2C%20they%20cannot%20fundamentally%20resolve%20catastrophic%20forgetting%20because%20they%0Arely%20on%20a%20single%20shared%20model%20to%20adapt%20across%20all%20target%20domains%2C%20which%0Ainevitably%20leads%20to%20severe%20inter-domain%20interference.%20In%20this%20paper%2C%20we%0Aintroduce%20learnable%20domain-specific%20prompts%20that%20guide%20the%20model%20to%20adapt%20to%0Acorresponding%20target%20domains%2C%20thereby%20partially%20disentangling%20the%20parameter%0Aspace%20of%20different%20domains.%20In%20the%20absence%20of%20domain%20identity%20for%20target%0Asamples%2C%20we%20propose%20a%20novel%20dynamic%20Prompt%20AllocatIon%20aNd%20Tuning%20%28PAINT%29%0Amethod%2C%20which%20utilizes%20a%20query%20mechanism%20to%20dynamically%20determine%20whether%20the%0Acurrent%20samples%20come%20from%20a%20known%20domain%20or%20an%20unexplored%20one.%20For%20known%0Adomains%2C%20the%20corresponding%20domain-specific%20prompt%20is%20directly%20selected%2C%20while%0Afor%20previously%20unseen%20domains%2C%20a%20new%20prompt%20is%20allocated.%20Prompt%20tuning%20is%0Asubsequently%20performed%20using%20mutual%20information%20maximization%20along%20with%0Astructural%20regularization.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20PAINT%20method%20for%20CTTA.%20We%20have%20released%0Aour%20code%20at%20https%3A//github.com/Cadezzyr/PAINT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09308v1&entry.124074799=Read"},
{"title": "Leveraging Medical Foundation Model Features in Graph Neural\n  Network-Based Retrieval of Breast Histopathology Images", "author": "Nematollah Saeidi and Hossein Karshenas and Bijan Shoushtarian and Sepideh Hatamikia and Ramona Woitek and Amirreza Mahbod", "abstract": "  Breast cancer is the most common cancer type in women worldwide. Early\ndetection and appropriate treatment can significantly reduce its impact. While\nhistopathology examinations play a vital role in rapid and accurate diagnosis,\nthey often require experienced medical experts for proper recognition and\ncancer grading. Automated image retrieval systems have the potential to assist\npathologists in identifying cancerous tissues, thereby accelerating the\ndiagnostic process. Nevertheless, proposing an accurate image retrieval model\nis challenging due to considerable variability among the tissue and cell\npatterns in histological images. In this work, we leverage the features from\nfoundation models in a novel attention-based adversarially regularized\nvariational graph autoencoder model for breast histological image retrieval.\nOur results confirm the superior performance of models trained with foundation\nmodel features compared to those using pre-trained convolutional neural\nnetworks (up to 7.7% and 15.5% for mAP and mMV, respectively), with the\npre-trained general-purpose self-supervised model for computational pathology\n(UNI) delivering the best overall performance. By evaluating two publicly\navailable histology image datasets of breast cancer, our top-performing model,\ntrained with UNI features, achieved average mAP/mMV scores of 96.7%/91.5% and\n97.6%/94.2% for the BreakHis and BACH datasets, respectively. Our proposed\nretrieval model has the potential to be used in clinical settings to enhance\ndiagnostic performance and ultimately benefit patients.\n", "link": "http://arxiv.org/abs/2405.04211v3", "date": "2024-12-12", "relevancy": 2.5789, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Medical%20Foundation%20Model%20Features%20in%20Graph%20Neural%0A%20%20Network-Based%20Retrieval%20of%20Breast%20Histopathology%20Images&body=Title%3A%20Leveraging%20Medical%20Foundation%20Model%20Features%20in%20Graph%20Neural%0A%20%20Network-Based%20Retrieval%20of%20Breast%20Histopathology%20Images%0AAuthor%3A%20Nematollah%20Saeidi%20and%20Hossein%20Karshenas%20and%20Bijan%20Shoushtarian%20and%20Sepideh%20Hatamikia%20and%20Ramona%20Woitek%20and%20Amirreza%20Mahbod%0AAbstract%3A%20%20%20Breast%20cancer%20is%20the%20most%20common%20cancer%20type%20in%20women%20worldwide.%20Early%0Adetection%20and%20appropriate%20treatment%20can%20significantly%20reduce%20its%20impact.%20While%0Ahistopathology%20examinations%20play%20a%20vital%20role%20in%20rapid%20and%20accurate%20diagnosis%2C%0Athey%20often%20require%20experienced%20medical%20experts%20for%20proper%20recognition%20and%0Acancer%20grading.%20Automated%20image%20retrieval%20systems%20have%20the%20potential%20to%20assist%0Apathologists%20in%20identifying%20cancerous%20tissues%2C%20thereby%20accelerating%20the%0Adiagnostic%20process.%20Nevertheless%2C%20proposing%20an%20accurate%20image%20retrieval%20model%0Ais%20challenging%20due%20to%20considerable%20variability%20among%20the%20tissue%20and%20cell%0Apatterns%20in%20histological%20images.%20In%20this%20work%2C%20we%20leverage%20the%20features%20from%0Afoundation%20models%20in%20a%20novel%20attention-based%20adversarially%20regularized%0Avariational%20graph%20autoencoder%20model%20for%20breast%20histological%20image%20retrieval.%0AOur%20results%20confirm%20the%20superior%20performance%20of%20models%20trained%20with%20foundation%0Amodel%20features%20compared%20to%20those%20using%20pre-trained%20convolutional%20neural%0Anetworks%20%28up%20to%207.7%25%20and%2015.5%25%20for%20mAP%20and%20mMV%2C%20respectively%29%2C%20with%20the%0Apre-trained%20general-purpose%20self-supervised%20model%20for%20computational%20pathology%0A%28UNI%29%20delivering%20the%20best%20overall%20performance.%20By%20evaluating%20two%20publicly%0Aavailable%20histology%20image%20datasets%20of%20breast%20cancer%2C%20our%20top-performing%20model%2C%0Atrained%20with%20UNI%20features%2C%20achieved%20average%20mAP/mMV%20scores%20of%2096.7%25/91.5%25%20and%0A97.6%25/94.2%25%20for%20the%20BreakHis%20and%20BACH%20datasets%2C%20respectively.%20Our%20proposed%0Aretrieval%20model%20has%20the%20potential%20to%20be%20used%20in%20clinical%20settings%20to%20enhance%0Adiagnostic%20performance%20and%20ultimately%20benefit%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04211v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Medical%2520Foundation%2520Model%2520Features%2520in%2520Graph%2520Neural%250A%2520%2520Network-Based%2520Retrieval%2520of%2520Breast%2520Histopathology%2520Images%26entry.906535625%3DNematollah%2520Saeidi%2520and%2520Hossein%2520Karshenas%2520and%2520Bijan%2520Shoushtarian%2520and%2520Sepideh%2520Hatamikia%2520and%2520Ramona%2520Woitek%2520and%2520Amirreza%2520Mahbod%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520is%2520the%2520most%2520common%2520cancer%2520type%2520in%2520women%2520worldwide.%2520Early%250Adetection%2520and%2520appropriate%2520treatment%2520can%2520significantly%2520reduce%2520its%2520impact.%2520While%250Ahistopathology%2520examinations%2520play%2520a%2520vital%2520role%2520in%2520rapid%2520and%2520accurate%2520diagnosis%252C%250Athey%2520often%2520require%2520experienced%2520medical%2520experts%2520for%2520proper%2520recognition%2520and%250Acancer%2520grading.%2520Automated%2520image%2520retrieval%2520systems%2520have%2520the%2520potential%2520to%2520assist%250Apathologists%2520in%2520identifying%2520cancerous%2520tissues%252C%2520thereby%2520accelerating%2520the%250Adiagnostic%2520process.%2520Nevertheless%252C%2520proposing%2520an%2520accurate%2520image%2520retrieval%2520model%250Ais%2520challenging%2520due%2520to%2520considerable%2520variability%2520among%2520the%2520tissue%2520and%2520cell%250Apatterns%2520in%2520histological%2520images.%2520In%2520this%2520work%252C%2520we%2520leverage%2520the%2520features%2520from%250Afoundation%2520models%2520in%2520a%2520novel%2520attention-based%2520adversarially%2520regularized%250Avariational%2520graph%2520autoencoder%2520model%2520for%2520breast%2520histological%2520image%2520retrieval.%250AOur%2520results%2520confirm%2520the%2520superior%2520performance%2520of%2520models%2520trained%2520with%2520foundation%250Amodel%2520features%2520compared%2520to%2520those%2520using%2520pre-trained%2520convolutional%2520neural%250Anetworks%2520%2528up%2520to%25207.7%2525%2520and%252015.5%2525%2520for%2520mAP%2520and%2520mMV%252C%2520respectively%2529%252C%2520with%2520the%250Apre-trained%2520general-purpose%2520self-supervised%2520model%2520for%2520computational%2520pathology%250A%2528UNI%2529%2520delivering%2520the%2520best%2520overall%2520performance.%2520By%2520evaluating%2520two%2520publicly%250Aavailable%2520histology%2520image%2520datasets%2520of%2520breast%2520cancer%252C%2520our%2520top-performing%2520model%252C%250Atrained%2520with%2520UNI%2520features%252C%2520achieved%2520average%2520mAP/mMV%2520scores%2520of%252096.7%2525/91.5%2525%2520and%250A97.6%2525/94.2%2525%2520for%2520the%2520BreakHis%2520and%2520BACH%2520datasets%252C%2520respectively.%2520Our%2520proposed%250Aretrieval%2520model%2520has%2520the%2520potential%2520to%2520be%2520used%2520in%2520clinical%2520settings%2520to%2520enhance%250Adiagnostic%2520performance%2520and%2520ultimately%2520benefit%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04211v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Medical%20Foundation%20Model%20Features%20in%20Graph%20Neural%0A%20%20Network-Based%20Retrieval%20of%20Breast%20Histopathology%20Images&entry.906535625=Nematollah%20Saeidi%20and%20Hossein%20Karshenas%20and%20Bijan%20Shoushtarian%20and%20Sepideh%20Hatamikia%20and%20Ramona%20Woitek%20and%20Amirreza%20Mahbod&entry.1292438233=%20%20Breast%20cancer%20is%20the%20most%20common%20cancer%20type%20in%20women%20worldwide.%20Early%0Adetection%20and%20appropriate%20treatment%20can%20significantly%20reduce%20its%20impact.%20While%0Ahistopathology%20examinations%20play%20a%20vital%20role%20in%20rapid%20and%20accurate%20diagnosis%2C%0Athey%20often%20require%20experienced%20medical%20experts%20for%20proper%20recognition%20and%0Acancer%20grading.%20Automated%20image%20retrieval%20systems%20have%20the%20potential%20to%20assist%0Apathologists%20in%20identifying%20cancerous%20tissues%2C%20thereby%20accelerating%20the%0Adiagnostic%20process.%20Nevertheless%2C%20proposing%20an%20accurate%20image%20retrieval%20model%0Ais%20challenging%20due%20to%20considerable%20variability%20among%20the%20tissue%20and%20cell%0Apatterns%20in%20histological%20images.%20In%20this%20work%2C%20we%20leverage%20the%20features%20from%0Afoundation%20models%20in%20a%20novel%20attention-based%20adversarially%20regularized%0Avariational%20graph%20autoencoder%20model%20for%20breast%20histological%20image%20retrieval.%0AOur%20results%20confirm%20the%20superior%20performance%20of%20models%20trained%20with%20foundation%0Amodel%20features%20compared%20to%20those%20using%20pre-trained%20convolutional%20neural%0Anetworks%20%28up%20to%207.7%25%20and%2015.5%25%20for%20mAP%20and%20mMV%2C%20respectively%29%2C%20with%20the%0Apre-trained%20general-purpose%20self-supervised%20model%20for%20computational%20pathology%0A%28UNI%29%20delivering%20the%20best%20overall%20performance.%20By%20evaluating%20two%20publicly%0Aavailable%20histology%20image%20datasets%20of%20breast%20cancer%2C%20our%20top-performing%20model%2C%0Atrained%20with%20UNI%20features%2C%20achieved%20average%20mAP/mMV%20scores%20of%2096.7%25/91.5%25%20and%0A97.6%25/94.2%25%20for%20the%20BreakHis%20and%20BACH%20datasets%2C%20respectively.%20Our%20proposed%0Aretrieval%20model%20has%20the%20potential%20to%20be%20used%20in%20clinical%20settings%20to%20enhance%0Adiagnostic%20performance%20and%20ultimately%20benefit%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04211v3&entry.124074799=Read"},
{"title": "Embeddings are all you need! Achieving High Performance Medical Image\n  Classification through Training-Free Embedding Analysis", "author": "Raj Hansini Khoiwal and Alan B. McMillan", "abstract": "  Developing artificial intelligence (AI) and machine learning (ML) models for\nmedical imaging typically involves extensive training and testing on large\ndatasets, consuming significant computational time, energy, and resources.\nThere is a need for more efficient methods that can achieve comparable or\nsuperior diagnostic performance without the associated resource burden. We\ninvestigated the feasibility of replacing conventional training procedures with\nan embedding-based approach that leverages concise and semantically meaningful\nrepresentations of medical images. Using pre-trained foundational\nmodels-specifically, convolutional neural networks (CNN) like ResNet and\nmultimodal models like Contrastive Language-Image Pre-training (CLIP)-we\ngenerated image embeddings for multi-class classification tasks. Simple linear\nclassifiers were then applied to these embeddings. The approach was evaluated\nacross diverse medical imaging modalities, including retinal images,\nmammography, dermatoscopic images, and chest radiographs. Performance was\ncompared to benchmark models trained and tested using traditional methods. The\nembedding-based models surpassed the benchmark area under the receiver\noperating characteristic curve (AUC-ROC) scores by up to 87 percentage in\nmulti-class classification tasks across the various medical imaging modalities.\nNotably, CLIP embedding models achieved the highest AUC-ROC scores,\ndemonstrating superior classification performance while significantly reducing\ncomputational demands. Our study indicates that leveraging embeddings from\npre-trained foundational models can effectively replace conventional,\nresource-intensive training and testing procedures in medical image analysis.\nThis embedding-based approach offers a more efficient alternative for image\nsegmentation, classification, and prediction, potentially accelerating AI\ntechnology integration into clinical practice.\n", "link": "http://arxiv.org/abs/2412.09445v1", "date": "2024-12-12", "relevancy": 2.5718, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embeddings%20are%20all%20you%20need%21%20Achieving%20High%20Performance%20Medical%20Image%0A%20%20Classification%20through%20Training-Free%20Embedding%20Analysis&body=Title%3A%20Embeddings%20are%20all%20you%20need%21%20Achieving%20High%20Performance%20Medical%20Image%0A%20%20Classification%20through%20Training-Free%20Embedding%20Analysis%0AAuthor%3A%20Raj%20Hansini%20Khoiwal%20and%20Alan%20B.%20McMillan%0AAbstract%3A%20%20%20Developing%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%20%28ML%29%20models%20for%0Amedical%20imaging%20typically%20involves%20extensive%20training%20and%20testing%20on%20large%0Adatasets%2C%20consuming%20significant%20computational%20time%2C%20energy%2C%20and%20resources.%0AThere%20is%20a%20need%20for%20more%20efficient%20methods%20that%20can%20achieve%20comparable%20or%0Asuperior%20diagnostic%20performance%20without%20the%20associated%20resource%20burden.%20We%0Ainvestigated%20the%20feasibility%20of%20replacing%20conventional%20training%20procedures%20with%0Aan%20embedding-based%20approach%20that%20leverages%20concise%20and%20semantically%20meaningful%0Arepresentations%20of%20medical%20images.%20Using%20pre-trained%20foundational%0Amodels-specifically%2C%20convolutional%20neural%20networks%20%28CNN%29%20like%20ResNet%20and%0Amultimodal%20models%20like%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29-we%0Agenerated%20image%20embeddings%20for%20multi-class%20classification%20tasks.%20Simple%20linear%0Aclassifiers%20were%20then%20applied%20to%20these%20embeddings.%20The%20approach%20was%20evaluated%0Aacross%20diverse%20medical%20imaging%20modalities%2C%20including%20retinal%20images%2C%0Amammography%2C%20dermatoscopic%20images%2C%20and%20chest%20radiographs.%20Performance%20was%0Acompared%20to%20benchmark%20models%20trained%20and%20tested%20using%20traditional%20methods.%20The%0Aembedding-based%20models%20surpassed%20the%20benchmark%20area%20under%20the%20receiver%0Aoperating%20characteristic%20curve%20%28AUC-ROC%29%20scores%20by%20up%20to%2087%20percentage%20in%0Amulti-class%20classification%20tasks%20across%20the%20various%20medical%20imaging%20modalities.%0ANotably%2C%20CLIP%20embedding%20models%20achieved%20the%20highest%20AUC-ROC%20scores%2C%0Ademonstrating%20superior%20classification%20performance%20while%20significantly%20reducing%0Acomputational%20demands.%20Our%20study%20indicates%20that%20leveraging%20embeddings%20from%0Apre-trained%20foundational%20models%20can%20effectively%20replace%20conventional%2C%0Aresource-intensive%20training%20and%20testing%20procedures%20in%20medical%20image%20analysis.%0AThis%20embedding-based%20approach%20offers%20a%20more%20efficient%20alternative%20for%20image%0Asegmentation%2C%20classification%2C%20and%20prediction%2C%20potentially%20accelerating%20AI%0Atechnology%20integration%20into%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbeddings%2520are%2520all%2520you%2520need%2521%2520Achieving%2520High%2520Performance%2520Medical%2520Image%250A%2520%2520Classification%2520through%2520Training-Free%2520Embedding%2520Analysis%26entry.906535625%3DRaj%2520Hansini%2520Khoiwal%2520and%2520Alan%2520B.%2520McMillan%26entry.1292438233%3D%2520%2520Developing%2520artificial%2520intelligence%2520%2528AI%2529%2520and%2520machine%2520learning%2520%2528ML%2529%2520models%2520for%250Amedical%2520imaging%2520typically%2520involves%2520extensive%2520training%2520and%2520testing%2520on%2520large%250Adatasets%252C%2520consuming%2520significant%2520computational%2520time%252C%2520energy%252C%2520and%2520resources.%250AThere%2520is%2520a%2520need%2520for%2520more%2520efficient%2520methods%2520that%2520can%2520achieve%2520comparable%2520or%250Asuperior%2520diagnostic%2520performance%2520without%2520the%2520associated%2520resource%2520burden.%2520We%250Ainvestigated%2520the%2520feasibility%2520of%2520replacing%2520conventional%2520training%2520procedures%2520with%250Aan%2520embedding-based%2520approach%2520that%2520leverages%2520concise%2520and%2520semantically%2520meaningful%250Arepresentations%2520of%2520medical%2520images.%2520Using%2520pre-trained%2520foundational%250Amodels-specifically%252C%2520convolutional%2520neural%2520networks%2520%2528CNN%2529%2520like%2520ResNet%2520and%250Amultimodal%2520models%2520like%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529-we%250Agenerated%2520image%2520embeddings%2520for%2520multi-class%2520classification%2520tasks.%2520Simple%2520linear%250Aclassifiers%2520were%2520then%2520applied%2520to%2520these%2520embeddings.%2520The%2520approach%2520was%2520evaluated%250Aacross%2520diverse%2520medical%2520imaging%2520modalities%252C%2520including%2520retinal%2520images%252C%250Amammography%252C%2520dermatoscopic%2520images%252C%2520and%2520chest%2520radiographs.%2520Performance%2520was%250Acompared%2520to%2520benchmark%2520models%2520trained%2520and%2520tested%2520using%2520traditional%2520methods.%2520The%250Aembedding-based%2520models%2520surpassed%2520the%2520benchmark%2520area%2520under%2520the%2520receiver%250Aoperating%2520characteristic%2520curve%2520%2528AUC-ROC%2529%2520scores%2520by%2520up%2520to%252087%2520percentage%2520in%250Amulti-class%2520classification%2520tasks%2520across%2520the%2520various%2520medical%2520imaging%2520modalities.%250ANotably%252C%2520CLIP%2520embedding%2520models%2520achieved%2520the%2520highest%2520AUC-ROC%2520scores%252C%250Ademonstrating%2520superior%2520classification%2520performance%2520while%2520significantly%2520reducing%250Acomputational%2520demands.%2520Our%2520study%2520indicates%2520that%2520leveraging%2520embeddings%2520from%250Apre-trained%2520foundational%2520models%2520can%2520effectively%2520replace%2520conventional%252C%250Aresource-intensive%2520training%2520and%2520testing%2520procedures%2520in%2520medical%2520image%2520analysis.%250AThis%2520embedding-based%2520approach%2520offers%2520a%2520more%2520efficient%2520alternative%2520for%2520image%250Asegmentation%252C%2520classification%252C%2520and%2520prediction%252C%2520potentially%2520accelerating%2520AI%250Atechnology%2520integration%2520into%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embeddings%20are%20all%20you%20need%21%20Achieving%20High%20Performance%20Medical%20Image%0A%20%20Classification%20through%20Training-Free%20Embedding%20Analysis&entry.906535625=Raj%20Hansini%20Khoiwal%20and%20Alan%20B.%20McMillan&entry.1292438233=%20%20Developing%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%20%28ML%29%20models%20for%0Amedical%20imaging%20typically%20involves%20extensive%20training%20and%20testing%20on%20large%0Adatasets%2C%20consuming%20significant%20computational%20time%2C%20energy%2C%20and%20resources.%0AThere%20is%20a%20need%20for%20more%20efficient%20methods%20that%20can%20achieve%20comparable%20or%0Asuperior%20diagnostic%20performance%20without%20the%20associated%20resource%20burden.%20We%0Ainvestigated%20the%20feasibility%20of%20replacing%20conventional%20training%20procedures%20with%0Aan%20embedding-based%20approach%20that%20leverages%20concise%20and%20semantically%20meaningful%0Arepresentations%20of%20medical%20images.%20Using%20pre-trained%20foundational%0Amodels-specifically%2C%20convolutional%20neural%20networks%20%28CNN%29%20like%20ResNet%20and%0Amultimodal%20models%20like%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29-we%0Agenerated%20image%20embeddings%20for%20multi-class%20classification%20tasks.%20Simple%20linear%0Aclassifiers%20were%20then%20applied%20to%20these%20embeddings.%20The%20approach%20was%20evaluated%0Aacross%20diverse%20medical%20imaging%20modalities%2C%20including%20retinal%20images%2C%0Amammography%2C%20dermatoscopic%20images%2C%20and%20chest%20radiographs.%20Performance%20was%0Acompared%20to%20benchmark%20models%20trained%20and%20tested%20using%20traditional%20methods.%20The%0Aembedding-based%20models%20surpassed%20the%20benchmark%20area%20under%20the%20receiver%0Aoperating%20characteristic%20curve%20%28AUC-ROC%29%20scores%20by%20up%20to%2087%20percentage%20in%0Amulti-class%20classification%20tasks%20across%20the%20various%20medical%20imaging%20modalities.%0ANotably%2C%20CLIP%20embedding%20models%20achieved%20the%20highest%20AUC-ROC%20scores%2C%0Ademonstrating%20superior%20classification%20performance%20while%20significantly%20reducing%0Acomputational%20demands.%20Our%20study%20indicates%20that%20leveraging%20embeddings%20from%0Apre-trained%20foundational%20models%20can%20effectively%20replace%20conventional%2C%0Aresource-intensive%20training%20and%20testing%20procedures%20in%20medical%20image%20analysis.%0AThis%20embedding-based%20approach%20offers%20a%20more%20efficient%20alternative%20for%20image%0Asegmentation%2C%20classification%2C%20and%20prediction%2C%20potentially%20accelerating%20AI%0Atechnology%20integration%20into%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09445v1&entry.124074799=Read"},
{"title": "Perturb and Recover: Fine-tuning for Effective Backdoor Removal from\n  CLIP", "author": "Naman Deep Singh and Francesco Croce and Matthias Hein", "abstract": "  Vision-Language models like CLIP have been shown to be highly effective at\nlinking visual perception and natural language understanding, enabling\nsophisticated image-text capabilities, including strong retrieval and zero-shot\nclassification performance. Their widespread use, as well as the fact that CLIP\nmodels are trained on image-text pairs from the web, make them both a\nworthwhile and relatively easy target for backdoor attacks. As training\nfoundational models, such as CLIP, from scratch is very expensive, this paper\nfocuses on cleaning potentially poisoned models via fine-tuning. We first show\nthat existing cleaning techniques are not effective against simple structured\ntriggers used in Blended or BadNet backdoor attacks, exposing a critical\nvulnerability for potential real-world deployment of these models. Then, we\nintroduce PAR, Perturb and Recover, a surprisingly simple yet effective\nmechanism to remove backdoors from CLIP models. Through extensive experiments\nacross different encoders and types of backdoor attacks, we show that PAR\nachieves high backdoor removal rate while preserving good standard performance.\nFinally, we illustrate that our approach is effective even only with synthetic\ntext-image pairs, i.e. without access to real training data. The code and\nmodels are available at https://github.com/nmndeep/PerturbAndRecover.\n", "link": "http://arxiv.org/abs/2412.00727v2", "date": "2024-12-12", "relevancy": 2.5634, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perturb%20and%20Recover%3A%20Fine-tuning%20for%20Effective%20Backdoor%20Removal%20from%0A%20%20CLIP&body=Title%3A%20Perturb%20and%20Recover%3A%20Fine-tuning%20for%20Effective%20Backdoor%20Removal%20from%0A%20%20CLIP%0AAuthor%3A%20Naman%20Deep%20Singh%20and%20Francesco%20Croce%20and%20Matthias%20Hein%0AAbstract%3A%20%20%20Vision-Language%20models%20like%20CLIP%20have%20been%20shown%20to%20be%20highly%20effective%20at%0Alinking%20visual%20perception%20and%20natural%20language%20understanding%2C%20enabling%0Asophisticated%20image-text%20capabilities%2C%20including%20strong%20retrieval%20and%20zero-shot%0Aclassification%20performance.%20Their%20widespread%20use%2C%20as%20well%20as%20the%20fact%20that%20CLIP%0Amodels%20are%20trained%20on%20image-text%20pairs%20from%20the%20web%2C%20make%20them%20both%20a%0Aworthwhile%20and%20relatively%20easy%20target%20for%20backdoor%20attacks.%20As%20training%0Afoundational%20models%2C%20such%20as%20CLIP%2C%20from%20scratch%20is%20very%20expensive%2C%20this%20paper%0Afocuses%20on%20cleaning%20potentially%20poisoned%20models%20via%20fine-tuning.%20We%20first%20show%0Athat%20existing%20cleaning%20techniques%20are%20not%20effective%20against%20simple%20structured%0Atriggers%20used%20in%20Blended%20or%20BadNet%20backdoor%20attacks%2C%20exposing%20a%20critical%0Avulnerability%20for%20potential%20real-world%20deployment%20of%20these%20models.%20Then%2C%20we%0Aintroduce%20PAR%2C%20Perturb%20and%20Recover%2C%20a%20surprisingly%20simple%20yet%20effective%0Amechanism%20to%20remove%20backdoors%20from%20CLIP%20models.%20Through%20extensive%20experiments%0Aacross%20different%20encoders%20and%20types%20of%20backdoor%20attacks%2C%20we%20show%20that%20PAR%0Aachieves%20high%20backdoor%20removal%20rate%20while%20preserving%20good%20standard%20performance.%0AFinally%2C%20we%20illustrate%20that%20our%20approach%20is%20effective%20even%20only%20with%20synthetic%0Atext-image%20pairs%2C%20i.e.%20without%20access%20to%20real%20training%20data.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/nmndeep/PerturbAndRecover.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerturb%2520and%2520Recover%253A%2520Fine-tuning%2520for%2520Effective%2520Backdoor%2520Removal%2520from%250A%2520%2520CLIP%26entry.906535625%3DNaman%2520Deep%2520Singh%2520and%2520Francesco%2520Croce%2520and%2520Matthias%2520Hein%26entry.1292438233%3D%2520%2520Vision-Language%2520models%2520like%2520CLIP%2520have%2520been%2520shown%2520to%2520be%2520highly%2520effective%2520at%250Alinking%2520visual%2520perception%2520and%2520natural%2520language%2520understanding%252C%2520enabling%250Asophisticated%2520image-text%2520capabilities%252C%2520including%2520strong%2520retrieval%2520and%2520zero-shot%250Aclassification%2520performance.%2520Their%2520widespread%2520use%252C%2520as%2520well%2520as%2520the%2520fact%2520that%2520CLIP%250Amodels%2520are%2520trained%2520on%2520image-text%2520pairs%2520from%2520the%2520web%252C%2520make%2520them%2520both%2520a%250Aworthwhile%2520and%2520relatively%2520easy%2520target%2520for%2520backdoor%2520attacks.%2520As%2520training%250Afoundational%2520models%252C%2520such%2520as%2520CLIP%252C%2520from%2520scratch%2520is%2520very%2520expensive%252C%2520this%2520paper%250Afocuses%2520on%2520cleaning%2520potentially%2520poisoned%2520models%2520via%2520fine-tuning.%2520We%2520first%2520show%250Athat%2520existing%2520cleaning%2520techniques%2520are%2520not%2520effective%2520against%2520simple%2520structured%250Atriggers%2520used%2520in%2520Blended%2520or%2520BadNet%2520backdoor%2520attacks%252C%2520exposing%2520a%2520critical%250Avulnerability%2520for%2520potential%2520real-world%2520deployment%2520of%2520these%2520models.%2520Then%252C%2520we%250Aintroduce%2520PAR%252C%2520Perturb%2520and%2520Recover%252C%2520a%2520surprisingly%2520simple%2520yet%2520effective%250Amechanism%2520to%2520remove%2520backdoors%2520from%2520CLIP%2520models.%2520Through%2520extensive%2520experiments%250Aacross%2520different%2520encoders%2520and%2520types%2520of%2520backdoor%2520attacks%252C%2520we%2520show%2520that%2520PAR%250Aachieves%2520high%2520backdoor%2520removal%2520rate%2520while%2520preserving%2520good%2520standard%2520performance.%250AFinally%252C%2520we%2520illustrate%2520that%2520our%2520approach%2520is%2520effective%2520even%2520only%2520with%2520synthetic%250Atext-image%2520pairs%252C%2520i.e.%2520without%2520access%2520to%2520real%2520training%2520data.%2520The%2520code%2520and%250Amodels%2520are%2520available%2520at%2520https%253A//github.com/nmndeep/PerturbAndRecover.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perturb%20and%20Recover%3A%20Fine-tuning%20for%20Effective%20Backdoor%20Removal%20from%0A%20%20CLIP&entry.906535625=Naman%20Deep%20Singh%20and%20Francesco%20Croce%20and%20Matthias%20Hein&entry.1292438233=%20%20Vision-Language%20models%20like%20CLIP%20have%20been%20shown%20to%20be%20highly%20effective%20at%0Alinking%20visual%20perception%20and%20natural%20language%20understanding%2C%20enabling%0Asophisticated%20image-text%20capabilities%2C%20including%20strong%20retrieval%20and%20zero-shot%0Aclassification%20performance.%20Their%20widespread%20use%2C%20as%20well%20as%20the%20fact%20that%20CLIP%0Amodels%20are%20trained%20on%20image-text%20pairs%20from%20the%20web%2C%20make%20them%20both%20a%0Aworthwhile%20and%20relatively%20easy%20target%20for%20backdoor%20attacks.%20As%20training%0Afoundational%20models%2C%20such%20as%20CLIP%2C%20from%20scratch%20is%20very%20expensive%2C%20this%20paper%0Afocuses%20on%20cleaning%20potentially%20poisoned%20models%20via%20fine-tuning.%20We%20first%20show%0Athat%20existing%20cleaning%20techniques%20are%20not%20effective%20against%20simple%20structured%0Atriggers%20used%20in%20Blended%20or%20BadNet%20backdoor%20attacks%2C%20exposing%20a%20critical%0Avulnerability%20for%20potential%20real-world%20deployment%20of%20these%20models.%20Then%2C%20we%0Aintroduce%20PAR%2C%20Perturb%20and%20Recover%2C%20a%20surprisingly%20simple%20yet%20effective%0Amechanism%20to%20remove%20backdoors%20from%20CLIP%20models.%20Through%20extensive%20experiments%0Aacross%20different%20encoders%20and%20types%20of%20backdoor%20attacks%2C%20we%20show%20that%20PAR%0Aachieves%20high%20backdoor%20removal%20rate%20while%20preserving%20good%20standard%20performance.%0AFinally%2C%20we%20illustrate%20that%20our%20approach%20is%20effective%20even%20only%20with%20synthetic%0Atext-image%20pairs%2C%20i.e.%20without%20access%20to%20real%20training%20data.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/nmndeep/PerturbAndRecover.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00727v2&entry.124074799=Read"},
{"title": "UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame\n  Organizer", "author": "Delong Liu and Zhaohui Hou and Mingjie Zhan and Shihao Han and Zhicheng Zhao and Fei Su", "abstract": "  Recently, diffusion-based video generation models have achieved significant\nsuccess. However, existing models often suffer from issues like weak\nconsistency and declining image quality over time. To overcome these\nchallenges, inspired by aesthetic principles, we propose a non-invasive plug-in\ncalled Uniform Frame Organizer (UFO), which is compatible with any\ndiffusion-based video generation model. The UFO comprises a series of adaptive\nadapters with adjustable intensities, which can significantly enhance the\nconsistency between the foreground and background of videos and improve image\nquality without altering the original model parameters when integrated. The\ntraining for UFO is simple, efficient, requires minimal resources, and supports\nstylized training. Its modular design allows for the combination of multiple\nUFOs, enabling the customization of personalized video generation models.\nFurthermore, the UFO also supports direct transferability across different\nmodels of the same specification without the need for specific retraining. The\nexperimental results indicate that UFO effectively enhances video generation\nquality and demonstrates its superiority in public video generation benchmarks.\nThe code will be publicly available at https://github.com/Delong-liu-bupt/UFO.\n", "link": "http://arxiv.org/abs/2412.09389v1", "date": "2024-12-12", "relevancy": 2.5396, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6426}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6306}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UFO%3A%20Enhancing%20Diffusion-Based%20Video%20Generation%20with%20a%20Uniform%20Frame%0A%20%20Organizer&body=Title%3A%20UFO%3A%20Enhancing%20Diffusion-Based%20Video%20Generation%20with%20a%20Uniform%20Frame%0A%20%20Organizer%0AAuthor%3A%20Delong%20Liu%20and%20Zhaohui%20Hou%20and%20Mingjie%20Zhan%20and%20Shihao%20Han%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%0AAbstract%3A%20%20%20Recently%2C%20diffusion-based%20video%20generation%20models%20have%20achieved%20significant%0Asuccess.%20However%2C%20existing%20models%20often%20suffer%20from%20issues%20like%20weak%0Aconsistency%20and%20declining%20image%20quality%20over%20time.%20To%20overcome%20these%0Achallenges%2C%20inspired%20by%20aesthetic%20principles%2C%20we%20propose%20a%20non-invasive%20plug-in%0Acalled%20Uniform%20Frame%20Organizer%20%28UFO%29%2C%20which%20is%20compatible%20with%20any%0Adiffusion-based%20video%20generation%20model.%20The%20UFO%20comprises%20a%20series%20of%20adaptive%0Aadapters%20with%20adjustable%20intensities%2C%20which%20can%20significantly%20enhance%20the%0Aconsistency%20between%20the%20foreground%20and%20background%20of%20videos%20and%20improve%20image%0Aquality%20without%20altering%20the%20original%20model%20parameters%20when%20integrated.%20The%0Atraining%20for%20UFO%20is%20simple%2C%20efficient%2C%20requires%20minimal%20resources%2C%20and%20supports%0Astylized%20training.%20Its%20modular%20design%20allows%20for%20the%20combination%20of%20multiple%0AUFOs%2C%20enabling%20the%20customization%20of%20personalized%20video%20generation%20models.%0AFurthermore%2C%20the%20UFO%20also%20supports%20direct%20transferability%20across%20different%0Amodels%20of%20the%20same%20specification%20without%20the%20need%20for%20specific%20retraining.%20The%0Aexperimental%20results%20indicate%20that%20UFO%20effectively%20enhances%20video%20generation%0Aquality%20and%20demonstrates%20its%20superiority%20in%20public%20video%20generation%20benchmarks.%0AThe%20code%20will%20be%20publicly%20available%20at%20https%3A//github.com/Delong-liu-bupt/UFO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUFO%253A%2520Enhancing%2520Diffusion-Based%2520Video%2520Generation%2520with%2520a%2520Uniform%2520Frame%250A%2520%2520Organizer%26entry.906535625%3DDelong%2520Liu%2520and%2520Zhaohui%2520Hou%2520and%2520Mingjie%2520Zhan%2520and%2520Shihao%2520Han%2520and%2520Zhicheng%2520Zhao%2520and%2520Fei%2520Su%26entry.1292438233%3D%2520%2520Recently%252C%2520diffusion-based%2520video%2520generation%2520models%2520have%2520achieved%2520significant%250Asuccess.%2520However%252C%2520existing%2520models%2520often%2520suffer%2520from%2520issues%2520like%2520weak%250Aconsistency%2520and%2520declining%2520image%2520quality%2520over%2520time.%2520To%2520overcome%2520these%250Achallenges%252C%2520inspired%2520by%2520aesthetic%2520principles%252C%2520we%2520propose%2520a%2520non-invasive%2520plug-in%250Acalled%2520Uniform%2520Frame%2520Organizer%2520%2528UFO%2529%252C%2520which%2520is%2520compatible%2520with%2520any%250Adiffusion-based%2520video%2520generation%2520model.%2520The%2520UFO%2520comprises%2520a%2520series%2520of%2520adaptive%250Aadapters%2520with%2520adjustable%2520intensities%252C%2520which%2520can%2520significantly%2520enhance%2520the%250Aconsistency%2520between%2520the%2520foreground%2520and%2520background%2520of%2520videos%2520and%2520improve%2520image%250Aquality%2520without%2520altering%2520the%2520original%2520model%2520parameters%2520when%2520integrated.%2520The%250Atraining%2520for%2520UFO%2520is%2520simple%252C%2520efficient%252C%2520requires%2520minimal%2520resources%252C%2520and%2520supports%250Astylized%2520training.%2520Its%2520modular%2520design%2520allows%2520for%2520the%2520combination%2520of%2520multiple%250AUFOs%252C%2520enabling%2520the%2520customization%2520of%2520personalized%2520video%2520generation%2520models.%250AFurthermore%252C%2520the%2520UFO%2520also%2520supports%2520direct%2520transferability%2520across%2520different%250Amodels%2520of%2520the%2520same%2520specification%2520without%2520the%2520need%2520for%2520specific%2520retraining.%2520The%250Aexperimental%2520results%2520indicate%2520that%2520UFO%2520effectively%2520enhances%2520video%2520generation%250Aquality%2520and%2520demonstrates%2520its%2520superiority%2520in%2520public%2520video%2520generation%2520benchmarks.%250AThe%2520code%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/Delong-liu-bupt/UFO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UFO%3A%20Enhancing%20Diffusion-Based%20Video%20Generation%20with%20a%20Uniform%20Frame%0A%20%20Organizer&entry.906535625=Delong%20Liu%20and%20Zhaohui%20Hou%20and%20Mingjie%20Zhan%20and%20Shihao%20Han%20and%20Zhicheng%20Zhao%20and%20Fei%20Su&entry.1292438233=%20%20Recently%2C%20diffusion-based%20video%20generation%20models%20have%20achieved%20significant%0Asuccess.%20However%2C%20existing%20models%20often%20suffer%20from%20issues%20like%20weak%0Aconsistency%20and%20declining%20image%20quality%20over%20time.%20To%20overcome%20these%0Achallenges%2C%20inspired%20by%20aesthetic%20principles%2C%20we%20propose%20a%20non-invasive%20plug-in%0Acalled%20Uniform%20Frame%20Organizer%20%28UFO%29%2C%20which%20is%20compatible%20with%20any%0Adiffusion-based%20video%20generation%20model.%20The%20UFO%20comprises%20a%20series%20of%20adaptive%0Aadapters%20with%20adjustable%20intensities%2C%20which%20can%20significantly%20enhance%20the%0Aconsistency%20between%20the%20foreground%20and%20background%20of%20videos%20and%20improve%20image%0Aquality%20without%20altering%20the%20original%20model%20parameters%20when%20integrated.%20The%0Atraining%20for%20UFO%20is%20simple%2C%20efficient%2C%20requires%20minimal%20resources%2C%20and%20supports%0Astylized%20training.%20Its%20modular%20design%20allows%20for%20the%20combination%20of%20multiple%0AUFOs%2C%20enabling%20the%20customization%20of%20personalized%20video%20generation%20models.%0AFurthermore%2C%20the%20UFO%20also%20supports%20direct%20transferability%20across%20different%0Amodels%20of%20the%20same%20specification%20without%20the%20need%20for%20specific%20retraining.%20The%0Aexperimental%20results%20indicate%20that%20UFO%20effectively%20enhances%20video%20generation%0Aquality%20and%20demonstrates%20its%20superiority%20in%20public%20video%20generation%20benchmarks.%0AThe%20code%20will%20be%20publicly%20available%20at%20https%3A//github.com/Delong-liu-bupt/UFO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09389v1&entry.124074799=Read"},
{"title": "Weighted Poisson-disk Resampling on Large-Scale Point Clouds", "author": "Xianhe Jiao and Chenlei Lv and Junli Zhao and Ran Yi and Yu-Hui Wen and Zhenkuan Pan and Zhongke Wu and Yong-jin Liu", "abstract": "  For large-scale point cloud processing, resampling takes the important role\nof controlling the point number and density while keeping the geometric\nconsistency. % in related tasks. However, current methods cannot balance such\ndifferent requirements. Particularly with large-scale point clouds, classical\nmethods often struggle with decreased efficiency and accuracy. To address such\nissues, we propose a weighted Poisson-disk (WPD) resampling method to improve\nthe usability and efficiency for the processing. We first design an initial\nPoisson resampling with a voxel-based estimation strategy. It is able to\nestimate a more accurate radius of the Poisson-disk while maintaining high\nefficiency. Then, we design a weighted tangent smoothing step to further\noptimize the Voronoi diagram for each point. At the same time, sharp features\nare detected and kept in the optimized results with isotropic property.\nFinally, we achieve a resampling copy from the original point cloud with the\nspecified point number, uniform density, and high-quality geometric\nconsistency. Experiments show that our method significantly improves the\nperformance of large-scale point cloud resampling for different applications,\nand provides a highly practical solution.\n", "link": "http://arxiv.org/abs/2412.09177v1", "date": "2024-12-12", "relevancy": 2.5328, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5227}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5094}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weighted%20Poisson-disk%20Resampling%20on%20Large-Scale%20Point%20Clouds&body=Title%3A%20Weighted%20Poisson-disk%20Resampling%20on%20Large-Scale%20Point%20Clouds%0AAuthor%3A%20Xianhe%20Jiao%20and%20Chenlei%20Lv%20and%20Junli%20Zhao%20and%20Ran%20Yi%20and%20Yu-Hui%20Wen%20and%20Zhenkuan%20Pan%20and%20Zhongke%20Wu%20and%20Yong-jin%20Liu%0AAbstract%3A%20%20%20For%20large-scale%20point%20cloud%20processing%2C%20resampling%20takes%20the%20important%20role%0Aof%20controlling%20the%20point%20number%20and%20density%20while%20keeping%20the%20geometric%0Aconsistency.%20%25%20in%20related%20tasks.%20However%2C%20current%20methods%20cannot%20balance%20such%0Adifferent%20requirements.%20Particularly%20with%20large-scale%20point%20clouds%2C%20classical%0Amethods%20often%20struggle%20with%20decreased%20efficiency%20and%20accuracy.%20To%20address%20such%0Aissues%2C%20we%20propose%20a%20weighted%20Poisson-disk%20%28WPD%29%20resampling%20method%20to%20improve%0Athe%20usability%20and%20efficiency%20for%20the%20processing.%20We%20first%20design%20an%20initial%0APoisson%20resampling%20with%20a%20voxel-based%20estimation%20strategy.%20It%20is%20able%20to%0Aestimate%20a%20more%20accurate%20radius%20of%20the%20Poisson-disk%20while%20maintaining%20high%0Aefficiency.%20Then%2C%20we%20design%20a%20weighted%20tangent%20smoothing%20step%20to%20further%0Aoptimize%20the%20Voronoi%20diagram%20for%20each%20point.%20At%20the%20same%20time%2C%20sharp%20features%0Aare%20detected%20and%20kept%20in%20the%20optimized%20results%20with%20isotropic%20property.%0AFinally%2C%20we%20achieve%20a%20resampling%20copy%20from%20the%20original%20point%20cloud%20with%20the%0Aspecified%20point%20number%2C%20uniform%20density%2C%20and%20high-quality%20geometric%0Aconsistency.%20Experiments%20show%20that%20our%20method%20significantly%20improves%20the%0Aperformance%20of%20large-scale%20point%20cloud%20resampling%20for%20different%20applications%2C%0Aand%20provides%20a%20highly%20practical%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeighted%2520Poisson-disk%2520Resampling%2520on%2520Large-Scale%2520Point%2520Clouds%26entry.906535625%3DXianhe%2520Jiao%2520and%2520Chenlei%2520Lv%2520and%2520Junli%2520Zhao%2520and%2520Ran%2520Yi%2520and%2520Yu-Hui%2520Wen%2520and%2520Zhenkuan%2520Pan%2520and%2520Zhongke%2520Wu%2520and%2520Yong-jin%2520Liu%26entry.1292438233%3D%2520%2520For%2520large-scale%2520point%2520cloud%2520processing%252C%2520resampling%2520takes%2520the%2520important%2520role%250Aof%2520controlling%2520the%2520point%2520number%2520and%2520density%2520while%2520keeping%2520the%2520geometric%250Aconsistency.%2520%2525%2520in%2520related%2520tasks.%2520However%252C%2520current%2520methods%2520cannot%2520balance%2520such%250Adifferent%2520requirements.%2520Particularly%2520with%2520large-scale%2520point%2520clouds%252C%2520classical%250Amethods%2520often%2520struggle%2520with%2520decreased%2520efficiency%2520and%2520accuracy.%2520To%2520address%2520such%250Aissues%252C%2520we%2520propose%2520a%2520weighted%2520Poisson-disk%2520%2528WPD%2529%2520resampling%2520method%2520to%2520improve%250Athe%2520usability%2520and%2520efficiency%2520for%2520the%2520processing.%2520We%2520first%2520design%2520an%2520initial%250APoisson%2520resampling%2520with%2520a%2520voxel-based%2520estimation%2520strategy.%2520It%2520is%2520able%2520to%250Aestimate%2520a%2520more%2520accurate%2520radius%2520of%2520the%2520Poisson-disk%2520while%2520maintaining%2520high%250Aefficiency.%2520Then%252C%2520we%2520design%2520a%2520weighted%2520tangent%2520smoothing%2520step%2520to%2520further%250Aoptimize%2520the%2520Voronoi%2520diagram%2520for%2520each%2520point.%2520At%2520the%2520same%2520time%252C%2520sharp%2520features%250Aare%2520detected%2520and%2520kept%2520in%2520the%2520optimized%2520results%2520with%2520isotropic%2520property.%250AFinally%252C%2520we%2520achieve%2520a%2520resampling%2520copy%2520from%2520the%2520original%2520point%2520cloud%2520with%2520the%250Aspecified%2520point%2520number%252C%2520uniform%2520density%252C%2520and%2520high-quality%2520geometric%250Aconsistency.%2520Experiments%2520show%2520that%2520our%2520method%2520significantly%2520improves%2520the%250Aperformance%2520of%2520large-scale%2520point%2520cloud%2520resampling%2520for%2520different%2520applications%252C%250Aand%2520provides%2520a%2520highly%2520practical%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20Poisson-disk%20Resampling%20on%20Large-Scale%20Point%20Clouds&entry.906535625=Xianhe%20Jiao%20and%20Chenlei%20Lv%20and%20Junli%20Zhao%20and%20Ran%20Yi%20and%20Yu-Hui%20Wen%20and%20Zhenkuan%20Pan%20and%20Zhongke%20Wu%20and%20Yong-jin%20Liu&entry.1292438233=%20%20For%20large-scale%20point%20cloud%20processing%2C%20resampling%20takes%20the%20important%20role%0Aof%20controlling%20the%20point%20number%20and%20density%20while%20keeping%20the%20geometric%0Aconsistency.%20%25%20in%20related%20tasks.%20However%2C%20current%20methods%20cannot%20balance%20such%0Adifferent%20requirements.%20Particularly%20with%20large-scale%20point%20clouds%2C%20classical%0Amethods%20often%20struggle%20with%20decreased%20efficiency%20and%20accuracy.%20To%20address%20such%0Aissues%2C%20we%20propose%20a%20weighted%20Poisson-disk%20%28WPD%29%20resampling%20method%20to%20improve%0Athe%20usability%20and%20efficiency%20for%20the%20processing.%20We%20first%20design%20an%20initial%0APoisson%20resampling%20with%20a%20voxel-based%20estimation%20strategy.%20It%20is%20able%20to%0Aestimate%20a%20more%20accurate%20radius%20of%20the%20Poisson-disk%20while%20maintaining%20high%0Aefficiency.%20Then%2C%20we%20design%20a%20weighted%20tangent%20smoothing%20step%20to%20further%0Aoptimize%20the%20Voronoi%20diagram%20for%20each%20point.%20At%20the%20same%20time%2C%20sharp%20features%0Aare%20detected%20and%20kept%20in%20the%20optimized%20results%20with%20isotropic%20property.%0AFinally%2C%20we%20achieve%20a%20resampling%20copy%20from%20the%20original%20point%20cloud%20with%20the%0Aspecified%20point%20number%2C%20uniform%20density%2C%20and%20high-quality%20geometric%0Aconsistency.%20Experiments%20show%20that%20our%20method%20significantly%20improves%20the%0Aperformance%20of%20large-scale%20point%20cloud%20resampling%20for%20different%20applications%2C%0Aand%20provides%20a%20highly%20practical%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09177v1&entry.124074799=Read"},
{"title": "Localizing Memorization in SSL Vision Encoders", "author": "Wenhao Wang and Adam Dziedzic and Michael Backes and Franziska Boenisch", "abstract": "  Recent work on studying memorization in self-supervised learning (SSL)\nsuggests that even though SSL encoders are trained on millions of images, they\nstill memorize individual data points. While effort has been put into\ncharacterizing the memorized data and linking encoder memorization to\ndownstream utility, little is known about where the memorization happens inside\nSSL encoders. To close this gap, we propose two metrics for localizing\nmemorization in SSL encoders on a per-layer (layermem) and per-unit basis\n(unitmem). Our localization methods are independent of the downstream task, do\nnot require any label information, and can be performed in a forward pass. By\nlocalizing memorization in various encoder architectures (convolutional and\ntransformer-based) trained on diverse datasets with contrastive and\nnon-contrastive SSL frameworks, we find that (1) while SSL memorization\nincreases with layer depth, highly memorizing units are distributed across the\nentire encoder, (2) a significant fraction of units in SSL encoders experiences\nsurprisingly high memorization of individual data points, which is in contrast\nto models trained under supervision, (3) atypical (or outlier) data points\ncause much higher layer and unit memorization than standard data points, and\n(4) in vision transformers, most memorization happens in the fully-connected\nlayers. Finally, we show that localizing memorization in SSL has the potential\nto improve fine-tuning and to inform pruning strategies.\n", "link": "http://arxiv.org/abs/2409.19069v3", "date": "2024-12-12", "relevancy": 2.5261, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localizing%20Memorization%20in%20SSL%20Vision%20Encoders&body=Title%3A%20Localizing%20Memorization%20in%20SSL%20Vision%20Encoders%0AAuthor%3A%20Wenhao%20Wang%20and%20Adam%20Dziedzic%20and%20Michael%20Backes%20and%20Franziska%20Boenisch%0AAbstract%3A%20%20%20Recent%20work%20on%20studying%20memorization%20in%20self-supervised%20learning%20%28SSL%29%0Asuggests%20that%20even%20though%20SSL%20encoders%20are%20trained%20on%20millions%20of%20images%2C%20they%0Astill%20memorize%20individual%20data%20points.%20While%20effort%20has%20been%20put%20into%0Acharacterizing%20the%20memorized%20data%20and%20linking%20encoder%20memorization%20to%0Adownstream%20utility%2C%20little%20is%20known%20about%20where%20the%20memorization%20happens%20inside%0ASSL%20encoders.%20To%20close%20this%20gap%2C%20we%20propose%20two%20metrics%20for%20localizing%0Amemorization%20in%20SSL%20encoders%20on%20a%20per-layer%20%28layermem%29%20and%20per-unit%20basis%0A%28unitmem%29.%20Our%20localization%20methods%20are%20independent%20of%20the%20downstream%20task%2C%20do%0Anot%20require%20any%20label%20information%2C%20and%20can%20be%20performed%20in%20a%20forward%20pass.%20By%0Alocalizing%20memorization%20in%20various%20encoder%20architectures%20%28convolutional%20and%0Atransformer-based%29%20trained%20on%20diverse%20datasets%20with%20contrastive%20and%0Anon-contrastive%20SSL%20frameworks%2C%20we%20find%20that%20%281%29%20while%20SSL%20memorization%0Aincreases%20with%20layer%20depth%2C%20highly%20memorizing%20units%20are%20distributed%20across%20the%0Aentire%20encoder%2C%20%282%29%20a%20significant%20fraction%20of%20units%20in%20SSL%20encoders%20experiences%0Asurprisingly%20high%20memorization%20of%20individual%20data%20points%2C%20which%20is%20in%20contrast%0Ato%20models%20trained%20under%20supervision%2C%20%283%29%20atypical%20%28or%20outlier%29%20data%20points%0Acause%20much%20higher%20layer%20and%20unit%20memorization%20than%20standard%20data%20points%2C%20and%0A%284%29%20in%20vision%20transformers%2C%20most%20memorization%20happens%20in%20the%20fully-connected%0Alayers.%20Finally%2C%20we%20show%20that%20localizing%20memorization%20in%20SSL%20has%20the%20potential%0Ato%20improve%20fine-tuning%20and%20to%20inform%20pruning%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19069v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalizing%2520Memorization%2520in%2520SSL%2520Vision%2520Encoders%26entry.906535625%3DWenhao%2520Wang%2520and%2520Adam%2520Dziedzic%2520and%2520Michael%2520Backes%2520and%2520Franziska%2520Boenisch%26entry.1292438233%3D%2520%2520Recent%2520work%2520on%2520studying%2520memorization%2520in%2520self-supervised%2520learning%2520%2528SSL%2529%250Asuggests%2520that%2520even%2520though%2520SSL%2520encoders%2520are%2520trained%2520on%2520millions%2520of%2520images%252C%2520they%250Astill%2520memorize%2520individual%2520data%2520points.%2520While%2520effort%2520has%2520been%2520put%2520into%250Acharacterizing%2520the%2520memorized%2520data%2520and%2520linking%2520encoder%2520memorization%2520to%250Adownstream%2520utility%252C%2520little%2520is%2520known%2520about%2520where%2520the%2520memorization%2520happens%2520inside%250ASSL%2520encoders.%2520To%2520close%2520this%2520gap%252C%2520we%2520propose%2520two%2520metrics%2520for%2520localizing%250Amemorization%2520in%2520SSL%2520encoders%2520on%2520a%2520per-layer%2520%2528layermem%2529%2520and%2520per-unit%2520basis%250A%2528unitmem%2529.%2520Our%2520localization%2520methods%2520are%2520independent%2520of%2520the%2520downstream%2520task%252C%2520do%250Anot%2520require%2520any%2520label%2520information%252C%2520and%2520can%2520be%2520performed%2520in%2520a%2520forward%2520pass.%2520By%250Alocalizing%2520memorization%2520in%2520various%2520encoder%2520architectures%2520%2528convolutional%2520and%250Atransformer-based%2529%2520trained%2520on%2520diverse%2520datasets%2520with%2520contrastive%2520and%250Anon-contrastive%2520SSL%2520frameworks%252C%2520we%2520find%2520that%2520%25281%2529%2520while%2520SSL%2520memorization%250Aincreases%2520with%2520layer%2520depth%252C%2520highly%2520memorizing%2520units%2520are%2520distributed%2520across%2520the%250Aentire%2520encoder%252C%2520%25282%2529%2520a%2520significant%2520fraction%2520of%2520units%2520in%2520SSL%2520encoders%2520experiences%250Asurprisingly%2520high%2520memorization%2520of%2520individual%2520data%2520points%252C%2520which%2520is%2520in%2520contrast%250Ato%2520models%2520trained%2520under%2520supervision%252C%2520%25283%2529%2520atypical%2520%2528or%2520outlier%2529%2520data%2520points%250Acause%2520much%2520higher%2520layer%2520and%2520unit%2520memorization%2520than%2520standard%2520data%2520points%252C%2520and%250A%25284%2529%2520in%2520vision%2520transformers%252C%2520most%2520memorization%2520happens%2520in%2520the%2520fully-connected%250Alayers.%2520Finally%252C%2520we%2520show%2520that%2520localizing%2520memorization%2520in%2520SSL%2520has%2520the%2520potential%250Ato%2520improve%2520fine-tuning%2520and%2520to%2520inform%2520pruning%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19069v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localizing%20Memorization%20in%20SSL%20Vision%20Encoders&entry.906535625=Wenhao%20Wang%20and%20Adam%20Dziedzic%20and%20Michael%20Backes%20and%20Franziska%20Boenisch&entry.1292438233=%20%20Recent%20work%20on%20studying%20memorization%20in%20self-supervised%20learning%20%28SSL%29%0Asuggests%20that%20even%20though%20SSL%20encoders%20are%20trained%20on%20millions%20of%20images%2C%20they%0Astill%20memorize%20individual%20data%20points.%20While%20effort%20has%20been%20put%20into%0Acharacterizing%20the%20memorized%20data%20and%20linking%20encoder%20memorization%20to%0Adownstream%20utility%2C%20little%20is%20known%20about%20where%20the%20memorization%20happens%20inside%0ASSL%20encoders.%20To%20close%20this%20gap%2C%20we%20propose%20two%20metrics%20for%20localizing%0Amemorization%20in%20SSL%20encoders%20on%20a%20per-layer%20%28layermem%29%20and%20per-unit%20basis%0A%28unitmem%29.%20Our%20localization%20methods%20are%20independent%20of%20the%20downstream%20task%2C%20do%0Anot%20require%20any%20label%20information%2C%20and%20can%20be%20performed%20in%20a%20forward%20pass.%20By%0Alocalizing%20memorization%20in%20various%20encoder%20architectures%20%28convolutional%20and%0Atransformer-based%29%20trained%20on%20diverse%20datasets%20with%20contrastive%20and%0Anon-contrastive%20SSL%20frameworks%2C%20we%20find%20that%20%281%29%20while%20SSL%20memorization%0Aincreases%20with%20layer%20depth%2C%20highly%20memorizing%20units%20are%20distributed%20across%20the%0Aentire%20encoder%2C%20%282%29%20a%20significant%20fraction%20of%20units%20in%20SSL%20encoders%20experiences%0Asurprisingly%20high%20memorization%20of%20individual%20data%20points%2C%20which%20is%20in%20contrast%0Ato%20models%20trained%20under%20supervision%2C%20%283%29%20atypical%20%28or%20outlier%29%20data%20points%0Acause%20much%20higher%20layer%20and%20unit%20memorization%20than%20standard%20data%20points%2C%20and%0A%284%29%20in%20vision%20transformers%2C%20most%20memorization%20happens%20in%20the%20fully-connected%0Alayers.%20Finally%2C%20we%20show%20that%20localizing%20memorization%20in%20SSL%20has%20the%20potential%0Ato%20improve%20fine-tuning%20and%20to%20inform%20pruning%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19069v3&entry.124074799=Read"},
{"title": "BEACON: Benchmark for Comprehensive RNA Tasks and Language Models", "author": "Yuchen Ren and Zhiyuan Chen and Lifeng Qiao and Hongtai Jing and Yuchen Cai and Sheng Xu and Peng Ye and Xinzhu Ma and Siqi Sun and Hongliang Yan and Dong Yuan and Wanli Ouyang and Xihui Liu", "abstract": "  RNA plays a pivotal role in translating genetic instructions into functional\noutcomes, underscoring its importance in biological processes and disease\nmechanisms. Despite the emergence of numerous deep learning approaches for RNA,\nparticularly universal RNA language models, there remains a significant lack of\nstandardized benchmarks to assess the effectiveness of these methods. In this\nstudy, we introduce the first comprehensive RNA benchmark BEACON\n(\\textbf{BE}nchm\\textbf{A}rk for \\textbf{CO}mprehensive R\\textbf{N}A Task and\nLanguage Models). First, BEACON comprises 13 distinct tasks derived from\nextensive previous work covering structural analysis, functional studies, and\nengineering applications, enabling a comprehensive assessment of the\nperformance of methods on various RNA understanding tasks. Second, we examine a\nrange of models, including traditional approaches like CNNs, as well as\nadvanced RNA foundation models based on language models, offering valuable\ninsights into the task-specific performances of these models. Third, we\ninvestigate the vital RNA language model components from the tokenizer and\npositional encoding aspects. Notably, our findings emphasize the superiority of\nsingle nucleotide tokenization and the effectiveness of Attention with Linear\nBiases (ALiBi) over traditional positional encoding methods. Based on these\ninsights, a simple yet strong baseline called BEACON-B is proposed, which can\nachieve outstanding performance with limited data and computational resources.\nThe datasets and source code of our benchmark are available at\nhttps://github.com/terry-r123/RNABenchmark.\n", "link": "http://arxiv.org/abs/2406.10391v2", "date": "2024-12-12", "relevancy": 2.5026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEACON%3A%20Benchmark%20for%20Comprehensive%20RNA%20Tasks%20and%20Language%20Models&body=Title%3A%20BEACON%3A%20Benchmark%20for%20Comprehensive%20RNA%20Tasks%20and%20Language%20Models%0AAuthor%3A%20Yuchen%20Ren%20and%20Zhiyuan%20Chen%20and%20Lifeng%20Qiao%20and%20Hongtai%20Jing%20and%20Yuchen%20Cai%20and%20Sheng%20Xu%20and%20Peng%20Ye%20and%20Xinzhu%20Ma%20and%20Siqi%20Sun%20and%20Hongliang%20Yan%20and%20Dong%20Yuan%20and%20Wanli%20Ouyang%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20RNA%20plays%20a%20pivotal%20role%20in%20translating%20genetic%20instructions%20into%20functional%0Aoutcomes%2C%20underscoring%20its%20importance%20in%20biological%20processes%20and%20disease%0Amechanisms.%20Despite%20the%20emergence%20of%20numerous%20deep%20learning%20approaches%20for%20RNA%2C%0Aparticularly%20universal%20RNA%20language%20models%2C%20there%20remains%20a%20significant%20lack%20of%0Astandardized%20benchmarks%20to%20assess%20the%20effectiveness%20of%20these%20methods.%20In%20this%0Astudy%2C%20we%20introduce%20the%20first%20comprehensive%20RNA%20benchmark%20BEACON%0A%28%5Ctextbf%7BBE%7Dnchm%5Ctextbf%7BA%7Drk%20for%20%5Ctextbf%7BCO%7Dmprehensive%20R%5Ctextbf%7BN%7DA%20Task%20and%0ALanguage%20Models%29.%20First%2C%20BEACON%20comprises%2013%20distinct%20tasks%20derived%20from%0Aextensive%20previous%20work%20covering%20structural%20analysis%2C%20functional%20studies%2C%20and%0Aengineering%20applications%2C%20enabling%20a%20comprehensive%20assessment%20of%20the%0Aperformance%20of%20methods%20on%20various%20RNA%20understanding%20tasks.%20Second%2C%20we%20examine%20a%0Arange%20of%20models%2C%20including%20traditional%20approaches%20like%20CNNs%2C%20as%20well%20as%0Aadvanced%20RNA%20foundation%20models%20based%20on%20language%20models%2C%20offering%20valuable%0Ainsights%20into%20the%20task-specific%20performances%20of%20these%20models.%20Third%2C%20we%0Ainvestigate%20the%20vital%20RNA%20language%20model%20components%20from%20the%20tokenizer%20and%0Apositional%20encoding%20aspects.%20Notably%2C%20our%20findings%20emphasize%20the%20superiority%20of%0Asingle%20nucleotide%20tokenization%20and%20the%20effectiveness%20of%20Attention%20with%20Linear%0ABiases%20%28ALiBi%29%20over%20traditional%20positional%20encoding%20methods.%20Based%20on%20these%0Ainsights%2C%20a%20simple%20yet%20strong%20baseline%20called%20BEACON-B%20is%20proposed%2C%20which%20can%0Aachieve%20outstanding%20performance%20with%20limited%20data%20and%20computational%20resources.%0AThe%20datasets%20and%20source%20code%20of%20our%20benchmark%20are%20available%20at%0Ahttps%3A//github.com/terry-r123/RNABenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEACON%253A%2520Benchmark%2520for%2520Comprehensive%2520RNA%2520Tasks%2520and%2520Language%2520Models%26entry.906535625%3DYuchen%2520Ren%2520and%2520Zhiyuan%2520Chen%2520and%2520Lifeng%2520Qiao%2520and%2520Hongtai%2520Jing%2520and%2520Yuchen%2520Cai%2520and%2520Sheng%2520Xu%2520and%2520Peng%2520Ye%2520and%2520Xinzhu%2520Ma%2520and%2520Siqi%2520Sun%2520and%2520Hongliang%2520Yan%2520and%2520Dong%2520Yuan%2520and%2520Wanli%2520Ouyang%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520RNA%2520plays%2520a%2520pivotal%2520role%2520in%2520translating%2520genetic%2520instructions%2520into%2520functional%250Aoutcomes%252C%2520underscoring%2520its%2520importance%2520in%2520biological%2520processes%2520and%2520disease%250Amechanisms.%2520Despite%2520the%2520emergence%2520of%2520numerous%2520deep%2520learning%2520approaches%2520for%2520RNA%252C%250Aparticularly%2520universal%2520RNA%2520language%2520models%252C%2520there%2520remains%2520a%2520significant%2520lack%2520of%250Astandardized%2520benchmarks%2520to%2520assess%2520the%2520effectiveness%2520of%2520these%2520methods.%2520In%2520this%250Astudy%252C%2520we%2520introduce%2520the%2520first%2520comprehensive%2520RNA%2520benchmark%2520BEACON%250A%2528%255Ctextbf%257BBE%257Dnchm%255Ctextbf%257BA%257Drk%2520for%2520%255Ctextbf%257BCO%257Dmprehensive%2520R%255Ctextbf%257BN%257DA%2520Task%2520and%250ALanguage%2520Models%2529.%2520First%252C%2520BEACON%2520comprises%252013%2520distinct%2520tasks%2520derived%2520from%250Aextensive%2520previous%2520work%2520covering%2520structural%2520analysis%252C%2520functional%2520studies%252C%2520and%250Aengineering%2520applications%252C%2520enabling%2520a%2520comprehensive%2520assessment%2520of%2520the%250Aperformance%2520of%2520methods%2520on%2520various%2520RNA%2520understanding%2520tasks.%2520Second%252C%2520we%2520examine%2520a%250Arange%2520of%2520models%252C%2520including%2520traditional%2520approaches%2520like%2520CNNs%252C%2520as%2520well%2520as%250Aadvanced%2520RNA%2520foundation%2520models%2520based%2520on%2520language%2520models%252C%2520offering%2520valuable%250Ainsights%2520into%2520the%2520task-specific%2520performances%2520of%2520these%2520models.%2520Third%252C%2520we%250Ainvestigate%2520the%2520vital%2520RNA%2520language%2520model%2520components%2520from%2520the%2520tokenizer%2520and%250Apositional%2520encoding%2520aspects.%2520Notably%252C%2520our%2520findings%2520emphasize%2520the%2520superiority%2520of%250Asingle%2520nucleotide%2520tokenization%2520and%2520the%2520effectiveness%2520of%2520Attention%2520with%2520Linear%250ABiases%2520%2528ALiBi%2529%2520over%2520traditional%2520positional%2520encoding%2520methods.%2520Based%2520on%2520these%250Ainsights%252C%2520a%2520simple%2520yet%2520strong%2520baseline%2520called%2520BEACON-B%2520is%2520proposed%252C%2520which%2520can%250Aachieve%2520outstanding%2520performance%2520with%2520limited%2520data%2520and%2520computational%2520resources.%250AThe%2520datasets%2520and%2520source%2520code%2520of%2520our%2520benchmark%2520are%2520available%2520at%250Ahttps%253A//github.com/terry-r123/RNABenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEACON%3A%20Benchmark%20for%20Comprehensive%20RNA%20Tasks%20and%20Language%20Models&entry.906535625=Yuchen%20Ren%20and%20Zhiyuan%20Chen%20and%20Lifeng%20Qiao%20and%20Hongtai%20Jing%20and%20Yuchen%20Cai%20and%20Sheng%20Xu%20and%20Peng%20Ye%20and%20Xinzhu%20Ma%20and%20Siqi%20Sun%20and%20Hongliang%20Yan%20and%20Dong%20Yuan%20and%20Wanli%20Ouyang%20and%20Xihui%20Liu&entry.1292438233=%20%20RNA%20plays%20a%20pivotal%20role%20in%20translating%20genetic%20instructions%20into%20functional%0Aoutcomes%2C%20underscoring%20its%20importance%20in%20biological%20processes%20and%20disease%0Amechanisms.%20Despite%20the%20emergence%20of%20numerous%20deep%20learning%20approaches%20for%20RNA%2C%0Aparticularly%20universal%20RNA%20language%20models%2C%20there%20remains%20a%20significant%20lack%20of%0Astandardized%20benchmarks%20to%20assess%20the%20effectiveness%20of%20these%20methods.%20In%20this%0Astudy%2C%20we%20introduce%20the%20first%20comprehensive%20RNA%20benchmark%20BEACON%0A%28%5Ctextbf%7BBE%7Dnchm%5Ctextbf%7BA%7Drk%20for%20%5Ctextbf%7BCO%7Dmprehensive%20R%5Ctextbf%7BN%7DA%20Task%20and%0ALanguage%20Models%29.%20First%2C%20BEACON%20comprises%2013%20distinct%20tasks%20derived%20from%0Aextensive%20previous%20work%20covering%20structural%20analysis%2C%20functional%20studies%2C%20and%0Aengineering%20applications%2C%20enabling%20a%20comprehensive%20assessment%20of%20the%0Aperformance%20of%20methods%20on%20various%20RNA%20understanding%20tasks.%20Second%2C%20we%20examine%20a%0Arange%20of%20models%2C%20including%20traditional%20approaches%20like%20CNNs%2C%20as%20well%20as%0Aadvanced%20RNA%20foundation%20models%20based%20on%20language%20models%2C%20offering%20valuable%0Ainsights%20into%20the%20task-specific%20performances%20of%20these%20models.%20Third%2C%20we%0Ainvestigate%20the%20vital%20RNA%20language%20model%20components%20from%20the%20tokenizer%20and%0Apositional%20encoding%20aspects.%20Notably%2C%20our%20findings%20emphasize%20the%20superiority%20of%0Asingle%20nucleotide%20tokenization%20and%20the%20effectiveness%20of%20Attention%20with%20Linear%0ABiases%20%28ALiBi%29%20over%20traditional%20positional%20encoding%20methods.%20Based%20on%20these%0Ainsights%2C%20a%20simple%20yet%20strong%20baseline%20called%20BEACON-B%20is%20proposed%2C%20which%20can%0Aachieve%20outstanding%20performance%20with%20limited%20data%20and%20computational%20resources.%0AThe%20datasets%20and%20source%20code%20of%20our%20benchmark%20are%20available%20at%0Ahttps%3A//github.com/terry-r123/RNABenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10391v2&entry.124074799=Read"},
{"title": "Single-View Graph Contrastive Learning with Soft Neighborhood Awareness", "author": "Qingqiang Sun and Chaoqi Chen and Ziyue Qiao and Xubin Zheng and Kai Wang", "abstract": "  Most graph contrastive learning (GCL) methods heavily rely on cross-view\ncontrast, thus facing several concomitant challenges, such as the complexity of\ndesigning effective augmentations, the potential for information loss between\nviews, and increased computational costs. To mitigate reliance on cross-view\ncontrasts, we propose \\ttt{SIGNA}, a novel single-view graph contrastive\nlearning framework. Regarding the inconsistency between structural connection\nand semantic similarity of neighborhoods, we resort to soft neighborhood\nawareness for GCL. Specifically, we leverage dropout to obtain\nstructurally-related yet randomly-noised embedding pairs for neighbors, which\nserve as potential positive samples. At each epoch, the role of partial\nneighbors is switched from positive to negative, leading to probabilistic\nneighborhood contrastive learning effect. Furthermore, we propose a normalized\nJensen-Shannon divergence estimator for a better effect of contrastive\nlearning. Surprisingly, experiments on diverse node-level tasks demonstrate\nthat our simple single-view GCL framework consistently outperforms existing\nmethods by margins of up to 21.74% (PPI). In particular, with soft neighborhood\nawareness, SIGNA can adopt MLPs instead of complicated GCNs as the encoder to\ngenerate representations in transductive learning tasks, thus speeding up its\ninference process by 109 times to 331 times. The source code is available at\nhttps://github.com/sunisfighting/SIGNA.\n", "link": "http://arxiv.org/abs/2412.09261v1", "date": "2024-12-12", "relevancy": 2.479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-View%20Graph%20Contrastive%20Learning%20with%20Soft%20Neighborhood%20Awareness&body=Title%3A%20Single-View%20Graph%20Contrastive%20Learning%20with%20Soft%20Neighborhood%20Awareness%0AAuthor%3A%20Qingqiang%20Sun%20and%20Chaoqi%20Chen%20and%20Ziyue%20Qiao%20and%20Xubin%20Zheng%20and%20Kai%20Wang%0AAbstract%3A%20%20%20Most%20graph%20contrastive%20learning%20%28GCL%29%20methods%20heavily%20rely%20on%20cross-view%0Acontrast%2C%20thus%20facing%20several%20concomitant%20challenges%2C%20such%20as%20the%20complexity%20of%0Adesigning%20effective%20augmentations%2C%20the%20potential%20for%20information%20loss%20between%0Aviews%2C%20and%20increased%20computational%20costs.%20To%20mitigate%20reliance%20on%20cross-view%0Acontrasts%2C%20we%20propose%20%5Cttt%7BSIGNA%7D%2C%20a%20novel%20single-view%20graph%20contrastive%0Alearning%20framework.%20Regarding%20the%20inconsistency%20between%20structural%20connection%0Aand%20semantic%20similarity%20of%20neighborhoods%2C%20we%20resort%20to%20soft%20neighborhood%0Aawareness%20for%20GCL.%20Specifically%2C%20we%20leverage%20dropout%20to%20obtain%0Astructurally-related%20yet%20randomly-noised%20embedding%20pairs%20for%20neighbors%2C%20which%0Aserve%20as%20potential%20positive%20samples.%20At%20each%20epoch%2C%20the%20role%20of%20partial%0Aneighbors%20is%20switched%20from%20positive%20to%20negative%2C%20leading%20to%20probabilistic%0Aneighborhood%20contrastive%20learning%20effect.%20Furthermore%2C%20we%20propose%20a%20normalized%0AJensen-Shannon%20divergence%20estimator%20for%20a%20better%20effect%20of%20contrastive%0Alearning.%20Surprisingly%2C%20experiments%20on%20diverse%20node-level%20tasks%20demonstrate%0Athat%20our%20simple%20single-view%20GCL%20framework%20consistently%20outperforms%20existing%0Amethods%20by%20margins%20of%20up%20to%2021.74%25%20%28PPI%29.%20In%20particular%2C%20with%20soft%20neighborhood%0Aawareness%2C%20SIGNA%20can%20adopt%20MLPs%20instead%20of%20complicated%20GCNs%20as%20the%20encoder%20to%0Agenerate%20representations%20in%20transductive%20learning%20tasks%2C%20thus%20speeding%20up%20its%0Ainference%20process%20by%20109%20times%20to%20331%20times.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/sunisfighting/SIGNA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-View%2520Graph%2520Contrastive%2520Learning%2520with%2520Soft%2520Neighborhood%2520Awareness%26entry.906535625%3DQingqiang%2520Sun%2520and%2520Chaoqi%2520Chen%2520and%2520Ziyue%2520Qiao%2520and%2520Xubin%2520Zheng%2520and%2520Kai%2520Wang%26entry.1292438233%3D%2520%2520Most%2520graph%2520contrastive%2520learning%2520%2528GCL%2529%2520methods%2520heavily%2520rely%2520on%2520cross-view%250Acontrast%252C%2520thus%2520facing%2520several%2520concomitant%2520challenges%252C%2520such%2520as%2520the%2520complexity%2520of%250Adesigning%2520effective%2520augmentations%252C%2520the%2520potential%2520for%2520information%2520loss%2520between%250Aviews%252C%2520and%2520increased%2520computational%2520costs.%2520To%2520mitigate%2520reliance%2520on%2520cross-view%250Acontrasts%252C%2520we%2520propose%2520%255Cttt%257BSIGNA%257D%252C%2520a%2520novel%2520single-view%2520graph%2520contrastive%250Alearning%2520framework.%2520Regarding%2520the%2520inconsistency%2520between%2520structural%2520connection%250Aand%2520semantic%2520similarity%2520of%2520neighborhoods%252C%2520we%2520resort%2520to%2520soft%2520neighborhood%250Aawareness%2520for%2520GCL.%2520Specifically%252C%2520we%2520leverage%2520dropout%2520to%2520obtain%250Astructurally-related%2520yet%2520randomly-noised%2520embedding%2520pairs%2520for%2520neighbors%252C%2520which%250Aserve%2520as%2520potential%2520positive%2520samples.%2520At%2520each%2520epoch%252C%2520the%2520role%2520of%2520partial%250Aneighbors%2520is%2520switched%2520from%2520positive%2520to%2520negative%252C%2520leading%2520to%2520probabilistic%250Aneighborhood%2520contrastive%2520learning%2520effect.%2520Furthermore%252C%2520we%2520propose%2520a%2520normalized%250AJensen-Shannon%2520divergence%2520estimator%2520for%2520a%2520better%2520effect%2520of%2520contrastive%250Alearning.%2520Surprisingly%252C%2520experiments%2520on%2520diverse%2520node-level%2520tasks%2520demonstrate%250Athat%2520our%2520simple%2520single-view%2520GCL%2520framework%2520consistently%2520outperforms%2520existing%250Amethods%2520by%2520margins%2520of%2520up%2520to%252021.74%2525%2520%2528PPI%2529.%2520In%2520particular%252C%2520with%2520soft%2520neighborhood%250Aawareness%252C%2520SIGNA%2520can%2520adopt%2520MLPs%2520instead%2520of%2520complicated%2520GCNs%2520as%2520the%2520encoder%2520to%250Agenerate%2520representations%2520in%2520transductive%2520learning%2520tasks%252C%2520thus%2520speeding%2520up%2520its%250Ainference%2520process%2520by%2520109%2520times%2520to%2520331%2520times.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/sunisfighting/SIGNA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-View%20Graph%20Contrastive%20Learning%20with%20Soft%20Neighborhood%20Awareness&entry.906535625=Qingqiang%20Sun%20and%20Chaoqi%20Chen%20and%20Ziyue%20Qiao%20and%20Xubin%20Zheng%20and%20Kai%20Wang&entry.1292438233=%20%20Most%20graph%20contrastive%20learning%20%28GCL%29%20methods%20heavily%20rely%20on%20cross-view%0Acontrast%2C%20thus%20facing%20several%20concomitant%20challenges%2C%20such%20as%20the%20complexity%20of%0Adesigning%20effective%20augmentations%2C%20the%20potential%20for%20information%20loss%20between%0Aviews%2C%20and%20increased%20computational%20costs.%20To%20mitigate%20reliance%20on%20cross-view%0Acontrasts%2C%20we%20propose%20%5Cttt%7BSIGNA%7D%2C%20a%20novel%20single-view%20graph%20contrastive%0Alearning%20framework.%20Regarding%20the%20inconsistency%20between%20structural%20connection%0Aand%20semantic%20similarity%20of%20neighborhoods%2C%20we%20resort%20to%20soft%20neighborhood%0Aawareness%20for%20GCL.%20Specifically%2C%20we%20leverage%20dropout%20to%20obtain%0Astructurally-related%20yet%20randomly-noised%20embedding%20pairs%20for%20neighbors%2C%20which%0Aserve%20as%20potential%20positive%20samples.%20At%20each%20epoch%2C%20the%20role%20of%20partial%0Aneighbors%20is%20switched%20from%20positive%20to%20negative%2C%20leading%20to%20probabilistic%0Aneighborhood%20contrastive%20learning%20effect.%20Furthermore%2C%20we%20propose%20a%20normalized%0AJensen-Shannon%20divergence%20estimator%20for%20a%20better%20effect%20of%20contrastive%0Alearning.%20Surprisingly%2C%20experiments%20on%20diverse%20node-level%20tasks%20demonstrate%0Athat%20our%20simple%20single-view%20GCL%20framework%20consistently%20outperforms%20existing%0Amethods%20by%20margins%20of%20up%20to%2021.74%25%20%28PPI%29.%20In%20particular%2C%20with%20soft%20neighborhood%0Aawareness%2C%20SIGNA%20can%20adopt%20MLPs%20instead%20of%20complicated%20GCNs%20as%20the%20encoder%20to%0Agenerate%20representations%20in%20transductive%20learning%20tasks%2C%20thus%20speeding%20up%20its%0Ainference%20process%20by%20109%20times%20to%20331%20times.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/sunisfighting/SIGNA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09261v1&entry.124074799=Read"},
{"title": "Optimized 3D Point Labeling with Leaders Using the Beams Displacement\n  Method", "author": "Zhiwei Wei and Nai Yang and Wenjia Xu and Su Ding and Li Minmin and Li You and Guo Renzhong", "abstract": "  In three-dimensional geographical scenes, adding labels with leader lines to\npoint features can significantly improve their visibility. Leadered labels have\na large degree of freedom in position con-figuration, but existing methods are\nmostly based on limited position candidate models, which not only fail to\neffectively utilize the map space but also make it difficult to consider the\nrelative relationships between labels. Therefore, we conceptualize the dynamic\nconfiguration process of computing label positions as akin to solving a map\ndisplacement problem. We use a triangulated graph to delineate spatial\nrelationships among labels and calculate the forces exerted on labels\nconsidering the constraints associated with point feature labels. Then we use\nthe Beams Displacement Method to iteratively calculate new positions for the\nlabels. Our experimental outcomes demonstrate that this method effectively\nmitigates label overlay issues while maintaining minimal average directional\ndeviation between adjacent labels. Furthermore, this method is adaptable to\nvarious types of leader line labels. Meanwhile, we also discuss the block\nprocessing strategy to improve the efficiency of label configuration and\nanalyze the impact of different proximity graphs.\n", "link": "http://arxiv.org/abs/2407.09552v2", "date": "2024-12-12", "relevancy": 2.4644, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5117}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4911}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimized%203D%20Point%20Labeling%20with%20Leaders%20Using%20the%20Beams%20Displacement%0A%20%20Method&body=Title%3A%20Optimized%203D%20Point%20Labeling%20with%20Leaders%20Using%20the%20Beams%20Displacement%0A%20%20Method%0AAuthor%3A%20Zhiwei%20Wei%20and%20Nai%20Yang%20and%20Wenjia%20Xu%20and%20Su%20Ding%20and%20Li%20Minmin%20and%20Li%20You%20and%20Guo%20Renzhong%0AAbstract%3A%20%20%20In%20three-dimensional%20geographical%20scenes%2C%20adding%20labels%20with%20leader%20lines%20to%0Apoint%20features%20can%20significantly%20improve%20their%20visibility.%20Leadered%20labels%20have%0Aa%20large%20degree%20of%20freedom%20in%20position%20con-figuration%2C%20but%20existing%20methods%20are%0Amostly%20based%20on%20limited%20position%20candidate%20models%2C%20which%20not%20only%20fail%20to%0Aeffectively%20utilize%20the%20map%20space%20but%20also%20make%20it%20difficult%20to%20consider%20the%0Arelative%20relationships%20between%20labels.%20Therefore%2C%20we%20conceptualize%20the%20dynamic%0Aconfiguration%20process%20of%20computing%20label%20positions%20as%20akin%20to%20solving%20a%20map%0Adisplacement%20problem.%20We%20use%20a%20triangulated%20graph%20to%20delineate%20spatial%0Arelationships%20among%20labels%20and%20calculate%20the%20forces%20exerted%20on%20labels%0Aconsidering%20the%20constraints%20associated%20with%20point%20feature%20labels.%20Then%20we%20use%0Athe%20Beams%20Displacement%20Method%20to%20iteratively%20calculate%20new%20positions%20for%20the%0Alabels.%20Our%20experimental%20outcomes%20demonstrate%20that%20this%20method%20effectively%0Amitigates%20label%20overlay%20issues%20while%20maintaining%20minimal%20average%20directional%0Adeviation%20between%20adjacent%20labels.%20Furthermore%2C%20this%20method%20is%20adaptable%20to%0Avarious%20types%20of%20leader%20line%20labels.%20Meanwhile%2C%20we%20also%20discuss%20the%20block%0Aprocessing%20strategy%20to%20improve%20the%20efficiency%20of%20label%20configuration%20and%0Aanalyze%20the%20impact%20of%20different%20proximity%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09552v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimized%25203D%2520Point%2520Labeling%2520with%2520Leaders%2520Using%2520the%2520Beams%2520Displacement%250A%2520%2520Method%26entry.906535625%3DZhiwei%2520Wei%2520and%2520Nai%2520Yang%2520and%2520Wenjia%2520Xu%2520and%2520Su%2520Ding%2520and%2520Li%2520Minmin%2520and%2520Li%2520You%2520and%2520Guo%2520Renzhong%26entry.1292438233%3D%2520%2520In%2520three-dimensional%2520geographical%2520scenes%252C%2520adding%2520labels%2520with%2520leader%2520lines%2520to%250Apoint%2520features%2520can%2520significantly%2520improve%2520their%2520visibility.%2520Leadered%2520labels%2520have%250Aa%2520large%2520degree%2520of%2520freedom%2520in%2520position%2520con-figuration%252C%2520but%2520existing%2520methods%2520are%250Amostly%2520based%2520on%2520limited%2520position%2520candidate%2520models%252C%2520which%2520not%2520only%2520fail%2520to%250Aeffectively%2520utilize%2520the%2520map%2520space%2520but%2520also%2520make%2520it%2520difficult%2520to%2520consider%2520the%250Arelative%2520relationships%2520between%2520labels.%2520Therefore%252C%2520we%2520conceptualize%2520the%2520dynamic%250Aconfiguration%2520process%2520of%2520computing%2520label%2520positions%2520as%2520akin%2520to%2520solving%2520a%2520map%250Adisplacement%2520problem.%2520We%2520use%2520a%2520triangulated%2520graph%2520to%2520delineate%2520spatial%250Arelationships%2520among%2520labels%2520and%2520calculate%2520the%2520forces%2520exerted%2520on%2520labels%250Aconsidering%2520the%2520constraints%2520associated%2520with%2520point%2520feature%2520labels.%2520Then%2520we%2520use%250Athe%2520Beams%2520Displacement%2520Method%2520to%2520iteratively%2520calculate%2520new%2520positions%2520for%2520the%250Alabels.%2520Our%2520experimental%2520outcomes%2520demonstrate%2520that%2520this%2520method%2520effectively%250Amitigates%2520label%2520overlay%2520issues%2520while%2520maintaining%2520minimal%2520average%2520directional%250Adeviation%2520between%2520adjacent%2520labels.%2520Furthermore%252C%2520this%2520method%2520is%2520adaptable%2520to%250Avarious%2520types%2520of%2520leader%2520line%2520labels.%2520Meanwhile%252C%2520we%2520also%2520discuss%2520the%2520block%250Aprocessing%2520strategy%2520to%2520improve%2520the%2520efficiency%2520of%2520label%2520configuration%2520and%250Aanalyze%2520the%2520impact%2520of%2520different%2520proximity%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09552v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimized%203D%20Point%20Labeling%20with%20Leaders%20Using%20the%20Beams%20Displacement%0A%20%20Method&entry.906535625=Zhiwei%20Wei%20and%20Nai%20Yang%20and%20Wenjia%20Xu%20and%20Su%20Ding%20and%20Li%20Minmin%20and%20Li%20You%20and%20Guo%20Renzhong&entry.1292438233=%20%20In%20three-dimensional%20geographical%20scenes%2C%20adding%20labels%20with%20leader%20lines%20to%0Apoint%20features%20can%20significantly%20improve%20their%20visibility.%20Leadered%20labels%20have%0Aa%20large%20degree%20of%20freedom%20in%20position%20con-figuration%2C%20but%20existing%20methods%20are%0Amostly%20based%20on%20limited%20position%20candidate%20models%2C%20which%20not%20only%20fail%20to%0Aeffectively%20utilize%20the%20map%20space%20but%20also%20make%20it%20difficult%20to%20consider%20the%0Arelative%20relationships%20between%20labels.%20Therefore%2C%20we%20conceptualize%20the%20dynamic%0Aconfiguration%20process%20of%20computing%20label%20positions%20as%20akin%20to%20solving%20a%20map%0Adisplacement%20problem.%20We%20use%20a%20triangulated%20graph%20to%20delineate%20spatial%0Arelationships%20among%20labels%20and%20calculate%20the%20forces%20exerted%20on%20labels%0Aconsidering%20the%20constraints%20associated%20with%20point%20feature%20labels.%20Then%20we%20use%0Athe%20Beams%20Displacement%20Method%20to%20iteratively%20calculate%20new%20positions%20for%20the%0Alabels.%20Our%20experimental%20outcomes%20demonstrate%20that%20this%20method%20effectively%0Amitigates%20label%20overlay%20issues%20while%20maintaining%20minimal%20average%20directional%0Adeviation%20between%20adjacent%20labels.%20Furthermore%2C%20this%20method%20is%20adaptable%20to%0Avarious%20types%20of%20leader%20line%20labels.%20Meanwhile%2C%20we%20also%20discuss%20the%20block%0Aprocessing%20strategy%20to%20improve%20the%20efficiency%20of%20label%20configuration%20and%0Aanalyze%20the%20impact%20of%20different%20proximity%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09552v2&entry.124074799=Read"},
{"title": "Foundation Models and Adaptive Feature Selection: A Synergistic Approach\n  to Video Question Answering", "author": "Sai Bhargav Rongali and Mohamad Hassan N C and Ankit Jha and Neha Bhargava and Saurabh Prasad and Biplab Banerjee", "abstract": "  This paper tackles the intricate challenge of video question-answering\n(VideoQA). Despite notable progress, current methods fall short of effectively\nintegrating questions with video frames and semantic object-level abstractions\nto create question-aware video representations. We introduce Local-Global\nQuestion Aware Video Embedding (LGQAVE), which incorporates three major\ninnovations to integrate multi-modal knowledge better and emphasize semantic\nvisual concepts relevant to specific questions. LGQAVE moves beyond traditional\nad-hoc frame sampling by utilizing a cross-attention mechanism that precisely\nidentifies the most relevant frames concerning the questions. It captures the\ndynamics of objects within these frames using distinct graphs, grounding them\nin question semantics with the miniGPT model. These graphs are processed by a\nquestion-aware dynamic graph transformer (Q-DGT), which refines the outputs to\ndevelop nuanced global and local video representations. An additional\ncross-attention module integrates these local and global embeddings to generate\nthe final video embeddings, which a language model uses to generate answers.\nExtensive evaluations across multiple benchmarks demonstrate that LGQAVE\nsignificantly outperforms existing models in delivering accurate multi-choice\nand open-ended answers.\n", "link": "http://arxiv.org/abs/2412.09230v1", "date": "2024-12-12", "relevancy": 2.4582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6202}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20and%20Adaptive%20Feature%20Selection%3A%20A%20Synergistic%20Approach%0A%20%20to%20Video%20Question%20Answering&body=Title%3A%20Foundation%20Models%20and%20Adaptive%20Feature%20Selection%3A%20A%20Synergistic%20Approach%0A%20%20to%20Video%20Question%20Answering%0AAuthor%3A%20Sai%20Bhargav%20Rongali%20and%20Mohamad%20Hassan%20N%20C%20and%20Ankit%20Jha%20and%20Neha%20Bhargava%20and%20Saurabh%20Prasad%20and%20Biplab%20Banerjee%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20intricate%20challenge%20of%20video%20question-answering%0A%28VideoQA%29.%20Despite%20notable%20progress%2C%20current%20methods%20fall%20short%20of%20effectively%0Aintegrating%20questions%20with%20video%20frames%20and%20semantic%20object-level%20abstractions%0Ato%20create%20question-aware%20video%20representations.%20We%20introduce%20Local-Global%0AQuestion%20Aware%20Video%20Embedding%20%28LGQAVE%29%2C%20which%20incorporates%20three%20major%0Ainnovations%20to%20integrate%20multi-modal%20knowledge%20better%20and%20emphasize%20semantic%0Avisual%20concepts%20relevant%20to%20specific%20questions.%20LGQAVE%20moves%20beyond%20traditional%0Aad-hoc%20frame%20sampling%20by%20utilizing%20a%20cross-attention%20mechanism%20that%20precisely%0Aidentifies%20the%20most%20relevant%20frames%20concerning%20the%20questions.%20It%20captures%20the%0Adynamics%20of%20objects%20within%20these%20frames%20using%20distinct%20graphs%2C%20grounding%20them%0Ain%20question%20semantics%20with%20the%20miniGPT%20model.%20These%20graphs%20are%20processed%20by%20a%0Aquestion-aware%20dynamic%20graph%20transformer%20%28Q-DGT%29%2C%20which%20refines%20the%20outputs%20to%0Adevelop%20nuanced%20global%20and%20local%20video%20representations.%20An%20additional%0Across-attention%20module%20integrates%20these%20local%20and%20global%20embeddings%20to%20generate%0Athe%20final%20video%20embeddings%2C%20which%20a%20language%20model%20uses%20to%20generate%20answers.%0AExtensive%20evaluations%20across%20multiple%20benchmarks%20demonstrate%20that%20LGQAVE%0Asignificantly%20outperforms%20existing%20models%20in%20delivering%20accurate%20multi-choice%0Aand%20open-ended%20answers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520and%2520Adaptive%2520Feature%2520Selection%253A%2520A%2520Synergistic%2520Approach%250A%2520%2520to%2520Video%2520Question%2520Answering%26entry.906535625%3DSai%2520Bhargav%2520Rongali%2520and%2520Mohamad%2520Hassan%2520N%2520C%2520and%2520Ankit%2520Jha%2520and%2520Neha%2520Bhargava%2520and%2520Saurabh%2520Prasad%2520and%2520Biplab%2520Banerjee%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520intricate%2520challenge%2520of%2520video%2520question-answering%250A%2528VideoQA%2529.%2520Despite%2520notable%2520progress%252C%2520current%2520methods%2520fall%2520short%2520of%2520effectively%250Aintegrating%2520questions%2520with%2520video%2520frames%2520and%2520semantic%2520object-level%2520abstractions%250Ato%2520create%2520question-aware%2520video%2520representations.%2520We%2520introduce%2520Local-Global%250AQuestion%2520Aware%2520Video%2520Embedding%2520%2528LGQAVE%2529%252C%2520which%2520incorporates%2520three%2520major%250Ainnovations%2520to%2520integrate%2520multi-modal%2520knowledge%2520better%2520and%2520emphasize%2520semantic%250Avisual%2520concepts%2520relevant%2520to%2520specific%2520questions.%2520LGQAVE%2520moves%2520beyond%2520traditional%250Aad-hoc%2520frame%2520sampling%2520by%2520utilizing%2520a%2520cross-attention%2520mechanism%2520that%2520precisely%250Aidentifies%2520the%2520most%2520relevant%2520frames%2520concerning%2520the%2520questions.%2520It%2520captures%2520the%250Adynamics%2520of%2520objects%2520within%2520these%2520frames%2520using%2520distinct%2520graphs%252C%2520grounding%2520them%250Ain%2520question%2520semantics%2520with%2520the%2520miniGPT%2520model.%2520These%2520graphs%2520are%2520processed%2520by%2520a%250Aquestion-aware%2520dynamic%2520graph%2520transformer%2520%2528Q-DGT%2529%252C%2520which%2520refines%2520the%2520outputs%2520to%250Adevelop%2520nuanced%2520global%2520and%2520local%2520video%2520representations.%2520An%2520additional%250Across-attention%2520module%2520integrates%2520these%2520local%2520and%2520global%2520embeddings%2520to%2520generate%250Athe%2520final%2520video%2520embeddings%252C%2520which%2520a%2520language%2520model%2520uses%2520to%2520generate%2520answers.%250AExtensive%2520evaluations%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520LGQAVE%250Asignificantly%2520outperforms%2520existing%2520models%2520in%2520delivering%2520accurate%2520multi-choice%250Aand%2520open-ended%2520answers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20and%20Adaptive%20Feature%20Selection%3A%20A%20Synergistic%20Approach%0A%20%20to%20Video%20Question%20Answering&entry.906535625=Sai%20Bhargav%20Rongali%20and%20Mohamad%20Hassan%20N%20C%20and%20Ankit%20Jha%20and%20Neha%20Bhargava%20and%20Saurabh%20Prasad%20and%20Biplab%20Banerjee&entry.1292438233=%20%20This%20paper%20tackles%20the%20intricate%20challenge%20of%20video%20question-answering%0A%28VideoQA%29.%20Despite%20notable%20progress%2C%20current%20methods%20fall%20short%20of%20effectively%0Aintegrating%20questions%20with%20video%20frames%20and%20semantic%20object-level%20abstractions%0Ato%20create%20question-aware%20video%20representations.%20We%20introduce%20Local-Global%0AQuestion%20Aware%20Video%20Embedding%20%28LGQAVE%29%2C%20which%20incorporates%20three%20major%0Ainnovations%20to%20integrate%20multi-modal%20knowledge%20better%20and%20emphasize%20semantic%0Avisual%20concepts%20relevant%20to%20specific%20questions.%20LGQAVE%20moves%20beyond%20traditional%0Aad-hoc%20frame%20sampling%20by%20utilizing%20a%20cross-attention%20mechanism%20that%20precisely%0Aidentifies%20the%20most%20relevant%20frames%20concerning%20the%20questions.%20It%20captures%20the%0Adynamics%20of%20objects%20within%20these%20frames%20using%20distinct%20graphs%2C%20grounding%20them%0Ain%20question%20semantics%20with%20the%20miniGPT%20model.%20These%20graphs%20are%20processed%20by%20a%0Aquestion-aware%20dynamic%20graph%20transformer%20%28Q-DGT%29%2C%20which%20refines%20the%20outputs%20to%0Adevelop%20nuanced%20global%20and%20local%20video%20representations.%20An%20additional%0Across-attention%20module%20integrates%20these%20local%20and%20global%20embeddings%20to%20generate%0Athe%20final%20video%20embeddings%2C%20which%20a%20language%20model%20uses%20to%20generate%20answers.%0AExtensive%20evaluations%20across%20multiple%20benchmarks%20demonstrate%20that%20LGQAVE%0Asignificantly%20outperforms%20existing%20models%20in%20delivering%20accurate%20multi-choice%0Aand%20open-ended%20answers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09230v1&entry.124074799=Read"},
{"title": "Towards Open-Vocabulary Video Semantic Segmentation", "author": "Xinhao Li and Yun Liu and Guolei Sun and Min Wu and Le Zhang and Ce Zhu", "abstract": "  Semantic segmentation in videos has been a focal point of recent research.\nHowever, existing models encounter challenges when faced with unfamiliar\ncategories. To address this, we introduce the Open Vocabulary Video Semantic\nSegmentation (OV-VSS) task, designed to accurately segment every pixel across a\nwide range of open-vocabulary categories, including those that are novel or\npreviously unexplored. To enhance OV-VSS performance, we propose a robust\nbaseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing\nthe model to utilize temporal relationships across consecutive frames.\nAdditionally, we incorporate a random frame enhancement module, broadening the\nmodel's understanding of semantic context throughout the entire video sequence.\nOur approach also includes video text encoding, which strengthens the model's\ncapability to interpret textual information within the video context.\nComprehensive evaluations on benchmark datasets such as VSPW and Cityscapes\nhighlight OV-VSS's zero-shot generalization capabilities, especially in\nhandling novel categories. The results validate OV2VSS's effectiveness,\ndemonstrating improved performance in semantic segmentation tasks across\ndiverse video datasets.\n", "link": "http://arxiv.org/abs/2412.09329v1", "date": "2024-12-12", "relevancy": 2.4292, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6161}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Open-Vocabulary%20Video%20Semantic%20Segmentation&body=Title%3A%20Towards%20Open-Vocabulary%20Video%20Semantic%20Segmentation%0AAuthor%3A%20Xinhao%20Li%20and%20Yun%20Liu%20and%20Guolei%20Sun%20and%20Min%20Wu%20and%20Le%20Zhang%20and%20Ce%20Zhu%0AAbstract%3A%20%20%20Semantic%20segmentation%20in%20videos%20has%20been%20a%20focal%20point%20of%20recent%20research.%0AHowever%2C%20existing%20models%20encounter%20challenges%20when%20faced%20with%20unfamiliar%0Acategories.%20To%20address%20this%2C%20we%20introduce%20the%20Open%20Vocabulary%20Video%20Semantic%0ASegmentation%20%28OV-VSS%29%20task%2C%20designed%20to%20accurately%20segment%20every%20pixel%20across%20a%0Awide%20range%20of%20open-vocabulary%20categories%2C%20including%20those%20that%20are%20novel%20or%0Apreviously%20unexplored.%20To%20enhance%20OV-VSS%20performance%2C%20we%20propose%20a%20robust%0Abaseline%2C%20OV2VSS%2C%20which%20integrates%20a%20spatial-temporal%20fusion%20module%2C%20allowing%0Athe%20model%20to%20utilize%20temporal%20relationships%20across%20consecutive%20frames.%0AAdditionally%2C%20we%20incorporate%20a%20random%20frame%20enhancement%20module%2C%20broadening%20the%0Amodel%27s%20understanding%20of%20semantic%20context%20throughout%20the%20entire%20video%20sequence.%0AOur%20approach%20also%20includes%20video%20text%20encoding%2C%20which%20strengthens%20the%20model%27s%0Acapability%20to%20interpret%20textual%20information%20within%20the%20video%20context.%0AComprehensive%20evaluations%20on%20benchmark%20datasets%20such%20as%20VSPW%20and%20Cityscapes%0Ahighlight%20OV-VSS%27s%20zero-shot%20generalization%20capabilities%2C%20especially%20in%0Ahandling%20novel%20categories.%20The%20results%20validate%20OV2VSS%27s%20effectiveness%2C%0Ademonstrating%20improved%20performance%20in%20semantic%20segmentation%20tasks%20across%0Adiverse%20video%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Open-Vocabulary%2520Video%2520Semantic%2520Segmentation%26entry.906535625%3DXinhao%2520Li%2520and%2520Yun%2520Liu%2520and%2520Guolei%2520Sun%2520and%2520Min%2520Wu%2520and%2520Le%2520Zhang%2520and%2520Ce%2520Zhu%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520in%2520videos%2520has%2520been%2520a%2520focal%2520point%2520of%2520recent%2520research.%250AHowever%252C%2520existing%2520models%2520encounter%2520challenges%2520when%2520faced%2520with%2520unfamiliar%250Acategories.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Open%2520Vocabulary%2520Video%2520Semantic%250ASegmentation%2520%2528OV-VSS%2529%2520task%252C%2520designed%2520to%2520accurately%2520segment%2520every%2520pixel%2520across%2520a%250Awide%2520range%2520of%2520open-vocabulary%2520categories%252C%2520including%2520those%2520that%2520are%2520novel%2520or%250Apreviously%2520unexplored.%2520To%2520enhance%2520OV-VSS%2520performance%252C%2520we%2520propose%2520a%2520robust%250Abaseline%252C%2520OV2VSS%252C%2520which%2520integrates%2520a%2520spatial-temporal%2520fusion%2520module%252C%2520allowing%250Athe%2520model%2520to%2520utilize%2520temporal%2520relationships%2520across%2520consecutive%2520frames.%250AAdditionally%252C%2520we%2520incorporate%2520a%2520random%2520frame%2520enhancement%2520module%252C%2520broadening%2520the%250Amodel%2527s%2520understanding%2520of%2520semantic%2520context%2520throughout%2520the%2520entire%2520video%2520sequence.%250AOur%2520approach%2520also%2520includes%2520video%2520text%2520encoding%252C%2520which%2520strengthens%2520the%2520model%2527s%250Acapability%2520to%2520interpret%2520textual%2520information%2520within%2520the%2520video%2520context.%250AComprehensive%2520evaluations%2520on%2520benchmark%2520datasets%2520such%2520as%2520VSPW%2520and%2520Cityscapes%250Ahighlight%2520OV-VSS%2527s%2520zero-shot%2520generalization%2520capabilities%252C%2520especially%2520in%250Ahandling%2520novel%2520categories.%2520The%2520results%2520validate%2520OV2VSS%2527s%2520effectiveness%252C%250Ademonstrating%2520improved%2520performance%2520in%2520semantic%2520segmentation%2520tasks%2520across%250Adiverse%2520video%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Open-Vocabulary%20Video%20Semantic%20Segmentation&entry.906535625=Xinhao%20Li%20and%20Yun%20Liu%20and%20Guolei%20Sun%20and%20Min%20Wu%20and%20Le%20Zhang%20and%20Ce%20Zhu&entry.1292438233=%20%20Semantic%20segmentation%20in%20videos%20has%20been%20a%20focal%20point%20of%20recent%20research.%0AHowever%2C%20existing%20models%20encounter%20challenges%20when%20faced%20with%20unfamiliar%0Acategories.%20To%20address%20this%2C%20we%20introduce%20the%20Open%20Vocabulary%20Video%20Semantic%0ASegmentation%20%28OV-VSS%29%20task%2C%20designed%20to%20accurately%20segment%20every%20pixel%20across%20a%0Awide%20range%20of%20open-vocabulary%20categories%2C%20including%20those%20that%20are%20novel%20or%0Apreviously%20unexplored.%20To%20enhance%20OV-VSS%20performance%2C%20we%20propose%20a%20robust%0Abaseline%2C%20OV2VSS%2C%20which%20integrates%20a%20spatial-temporal%20fusion%20module%2C%20allowing%0Athe%20model%20to%20utilize%20temporal%20relationships%20across%20consecutive%20frames.%0AAdditionally%2C%20we%20incorporate%20a%20random%20frame%20enhancement%20module%2C%20broadening%20the%0Amodel%27s%20understanding%20of%20semantic%20context%20throughout%20the%20entire%20video%20sequence.%0AOur%20approach%20also%20includes%20video%20text%20encoding%2C%20which%20strengthens%20the%20model%27s%0Acapability%20to%20interpret%20textual%20information%20within%20the%20video%20context.%0AComprehensive%20evaluations%20on%20benchmark%20datasets%20such%20as%20VSPW%20and%20Cityscapes%0Ahighlight%20OV-VSS%27s%20zero-shot%20generalization%20capabilities%2C%20especially%20in%0Ahandling%20novel%20categories.%20The%20results%20validate%20OV2VSS%27s%20effectiveness%2C%0Ademonstrating%20improved%20performance%20in%20semantic%20segmentation%20tasks%20across%0Adiverse%20video%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09329v1&entry.124074799=Read"},
{"title": "OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video\n  Generation", "author": "Weiqi Li and Shijie Zhao and Chong Mou and Xuhan Sheng and Zhenyu Zhang and Qian Wang and Junlin Li and Li Zhang and Jian Zhang", "abstract": "  As virtual reality gains popularity, the demand for controllable creation of\nimmersive and dynamic omnidirectional videos (ODVs) is increasing. While\nprevious text-to-ODV generation methods achieve impressive results, they\nstruggle with content inaccuracies and inconsistencies due to reliance solely\non textual inputs. Although recent motion control techniques provide\nfine-grained control for video generation, directly applying these methods to\nODVs often results in spatial distortion and unsatisfactory performance,\nespecially with complex spherical motions. To tackle these challenges, we\npropose OmniDrag, the first approach enabling both scene- and object-level\nmotion control for accurate, high-quality omnidirectional image-to-video\ngeneration. Building on pretrained video diffusion models, we introduce an\nomnidirectional control module, which is jointly fine-tuned with temporal\nattention layers to effectively handle complex spherical motion. In addition,\nwe develop a novel spherical motion estimator that accurately extracts\nmotion-control signals and allows users to perform drag-style ODV generation by\nsimply drawing handle and target points. We also present a new dataset, named\nMove360, addressing the scarcity of ODV data with large scene and object\nmotions. Experiments demonstrate the significant superiority of OmniDrag in\nachieving holistic scene-level and fine-grained object-level control for ODV\ngeneration. The project page is available at\nhttps://lwq20020127.github.io/OmniDrag.\n", "link": "http://arxiv.org/abs/2412.09623v1", "date": "2024-12-12", "relevancy": 2.3632, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6009}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5907}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniDrag%3A%20Enabling%20Motion%20Control%20for%20Omnidirectional%20Image-to-Video%0A%20%20Generation&body=Title%3A%20OmniDrag%3A%20Enabling%20Motion%20Control%20for%20Omnidirectional%20Image-to-Video%0A%20%20Generation%0AAuthor%3A%20Weiqi%20Li%20and%20Shijie%20Zhao%20and%20Chong%20Mou%20and%20Xuhan%20Sheng%20and%20Zhenyu%20Zhang%20and%20Qian%20Wang%20and%20Junlin%20Li%20and%20Li%20Zhang%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20As%20virtual%20reality%20gains%20popularity%2C%20the%20demand%20for%20controllable%20creation%20of%0Aimmersive%20and%20dynamic%20omnidirectional%20videos%20%28ODVs%29%20is%20increasing.%20While%0Aprevious%20text-to-ODV%20generation%20methods%20achieve%20impressive%20results%2C%20they%0Astruggle%20with%20content%20inaccuracies%20and%20inconsistencies%20due%20to%20reliance%20solely%0Aon%20textual%20inputs.%20Although%20recent%20motion%20control%20techniques%20provide%0Afine-grained%20control%20for%20video%20generation%2C%20directly%20applying%20these%20methods%20to%0AODVs%20often%20results%20in%20spatial%20distortion%20and%20unsatisfactory%20performance%2C%0Aespecially%20with%20complex%20spherical%20motions.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20OmniDrag%2C%20the%20first%20approach%20enabling%20both%20scene-%20and%20object-level%0Amotion%20control%20for%20accurate%2C%20high-quality%20omnidirectional%20image-to-video%0Ageneration.%20Building%20on%20pretrained%20video%20diffusion%20models%2C%20we%20introduce%20an%0Aomnidirectional%20control%20module%2C%20which%20is%20jointly%20fine-tuned%20with%20temporal%0Aattention%20layers%20to%20effectively%20handle%20complex%20spherical%20motion.%20In%20addition%2C%0Awe%20develop%20a%20novel%20spherical%20motion%20estimator%20that%20accurately%20extracts%0Amotion-control%20signals%20and%20allows%20users%20to%20perform%20drag-style%20ODV%20generation%20by%0Asimply%20drawing%20handle%20and%20target%20points.%20We%20also%20present%20a%20new%20dataset%2C%20named%0AMove360%2C%20addressing%20the%20scarcity%20of%20ODV%20data%20with%20large%20scene%20and%20object%0Amotions.%20Experiments%20demonstrate%20the%20significant%20superiority%20of%20OmniDrag%20in%0Aachieving%20holistic%20scene-level%20and%20fine-grained%20object-level%20control%20for%20ODV%0Ageneration.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//lwq20020127.github.io/OmniDrag.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniDrag%253A%2520Enabling%2520Motion%2520Control%2520for%2520Omnidirectional%2520Image-to-Video%250A%2520%2520Generation%26entry.906535625%3DWeiqi%2520Li%2520and%2520Shijie%2520Zhao%2520and%2520Chong%2520Mou%2520and%2520Xuhan%2520Sheng%2520and%2520Zhenyu%2520Zhang%2520and%2520Qian%2520Wang%2520and%2520Junlin%2520Li%2520and%2520Li%2520Zhang%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520As%2520virtual%2520reality%2520gains%2520popularity%252C%2520the%2520demand%2520for%2520controllable%2520creation%2520of%250Aimmersive%2520and%2520dynamic%2520omnidirectional%2520videos%2520%2528ODVs%2529%2520is%2520increasing.%2520While%250Aprevious%2520text-to-ODV%2520generation%2520methods%2520achieve%2520impressive%2520results%252C%2520they%250Astruggle%2520with%2520content%2520inaccuracies%2520and%2520inconsistencies%2520due%2520to%2520reliance%2520solely%250Aon%2520textual%2520inputs.%2520Although%2520recent%2520motion%2520control%2520techniques%2520provide%250Afine-grained%2520control%2520for%2520video%2520generation%252C%2520directly%2520applying%2520these%2520methods%2520to%250AODVs%2520often%2520results%2520in%2520spatial%2520distortion%2520and%2520unsatisfactory%2520performance%252C%250Aespecially%2520with%2520complex%2520spherical%2520motions.%2520To%2520tackle%2520these%2520challenges%252C%2520we%250Apropose%2520OmniDrag%252C%2520the%2520first%2520approach%2520enabling%2520both%2520scene-%2520and%2520object-level%250Amotion%2520control%2520for%2520accurate%252C%2520high-quality%2520omnidirectional%2520image-to-video%250Ageneration.%2520Building%2520on%2520pretrained%2520video%2520diffusion%2520models%252C%2520we%2520introduce%2520an%250Aomnidirectional%2520control%2520module%252C%2520which%2520is%2520jointly%2520fine-tuned%2520with%2520temporal%250Aattention%2520layers%2520to%2520effectively%2520handle%2520complex%2520spherical%2520motion.%2520In%2520addition%252C%250Awe%2520develop%2520a%2520novel%2520spherical%2520motion%2520estimator%2520that%2520accurately%2520extracts%250Amotion-control%2520signals%2520and%2520allows%2520users%2520to%2520perform%2520drag-style%2520ODV%2520generation%2520by%250Asimply%2520drawing%2520handle%2520and%2520target%2520points.%2520We%2520also%2520present%2520a%2520new%2520dataset%252C%2520named%250AMove360%252C%2520addressing%2520the%2520scarcity%2520of%2520ODV%2520data%2520with%2520large%2520scene%2520and%2520object%250Amotions.%2520Experiments%2520demonstrate%2520the%2520significant%2520superiority%2520of%2520OmniDrag%2520in%250Aachieving%2520holistic%2520scene-level%2520and%2520fine-grained%2520object-level%2520control%2520for%2520ODV%250Ageneration.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//lwq20020127.github.io/OmniDrag.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDrag%3A%20Enabling%20Motion%20Control%20for%20Omnidirectional%20Image-to-Video%0A%20%20Generation&entry.906535625=Weiqi%20Li%20and%20Shijie%20Zhao%20and%20Chong%20Mou%20and%20Xuhan%20Sheng%20and%20Zhenyu%20Zhang%20and%20Qian%20Wang%20and%20Junlin%20Li%20and%20Li%20Zhang%20and%20Jian%20Zhang&entry.1292438233=%20%20As%20virtual%20reality%20gains%20popularity%2C%20the%20demand%20for%20controllable%20creation%20of%0Aimmersive%20and%20dynamic%20omnidirectional%20videos%20%28ODVs%29%20is%20increasing.%20While%0Aprevious%20text-to-ODV%20generation%20methods%20achieve%20impressive%20results%2C%20they%0Astruggle%20with%20content%20inaccuracies%20and%20inconsistencies%20due%20to%20reliance%20solely%0Aon%20textual%20inputs.%20Although%20recent%20motion%20control%20techniques%20provide%0Afine-grained%20control%20for%20video%20generation%2C%20directly%20applying%20these%20methods%20to%0AODVs%20often%20results%20in%20spatial%20distortion%20and%20unsatisfactory%20performance%2C%0Aespecially%20with%20complex%20spherical%20motions.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20OmniDrag%2C%20the%20first%20approach%20enabling%20both%20scene-%20and%20object-level%0Amotion%20control%20for%20accurate%2C%20high-quality%20omnidirectional%20image-to-video%0Ageneration.%20Building%20on%20pretrained%20video%20diffusion%20models%2C%20we%20introduce%20an%0Aomnidirectional%20control%20module%2C%20which%20is%20jointly%20fine-tuned%20with%20temporal%0Aattention%20layers%20to%20effectively%20handle%20complex%20spherical%20motion.%20In%20addition%2C%0Awe%20develop%20a%20novel%20spherical%20motion%20estimator%20that%20accurately%20extracts%0Amotion-control%20signals%20and%20allows%20users%20to%20perform%20drag-style%20ODV%20generation%20by%0Asimply%20drawing%20handle%20and%20target%20points.%20We%20also%20present%20a%20new%20dataset%2C%20named%0AMove360%2C%20addressing%20the%20scarcity%20of%20ODV%20data%20with%20large%20scene%20and%20object%0Amotions.%20Experiments%20demonstrate%20the%20significant%20superiority%20of%20OmniDrag%20in%0Aachieving%20holistic%20scene-level%20and%20fine-grained%20object-level%20control%20for%20ODV%0Ageneration.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//lwq20020127.github.io/OmniDrag.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09623v1&entry.124074799=Read"},
{"title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine", "author": "Xiaoshuang Huang and Lingdong Shen and Jia Liu and Fangxin Shang and Hongxiang Li and Haifeng Huang and Yehui Yang", "abstract": "  In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.\n", "link": "http://arxiv.org/abs/2412.09278v1", "date": "2024-12-12", "relevancy": 2.3569, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Multimodal%20Large%20Language%20Model%20with%20Pixel-Level%20Insight%20for%0A%20%20Biomedicine&body=Title%3A%20Towards%20a%20Multimodal%20Large%20Language%20Model%20with%20Pixel-Level%20Insight%20for%0A%20%20Biomedicine%0AAuthor%3A%20Xiaoshuang%20Huang%20and%20Lingdong%20Shen%20and%20Jia%20Liu%20and%20Fangxin%20Shang%20and%20Hongxiang%20Li%20and%20Haifeng%20Huang%20and%20Yehui%20Yang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Multimodal%20Large%20Language%20Models%20%28MLLM%29%20have%20achieved%0Anotable%20advancements%2C%20demonstrating%20the%20feasibility%20of%20developing%20an%0Aintelligent%20biomedical%20assistant.%20However%2C%20current%20biomedical%20MLLMs%0Apredominantly%20focus%20on%20image-level%20understanding%20and%20restrict%20interactions%20to%0Atextual%20commands%2C%20thus%20limiting%20their%20capability%20boundaries%20and%20the%20flexibility%0Aof%20usage.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20end-to-end%20multimodal%20large%0Alanguage%20model%20for%20the%20biomedical%20domain%2C%20named%20MedPLIB%2C%20which%20possesses%0Apixel-level%20understanding.%20Excitingly%2C%20it%20supports%20visual%20question%20answering%0A%28VQA%29%2C%20arbitrary%20pixel-level%20prompts%20%28points%2C%20bounding%20boxes%2C%20and%20free-form%0Ashapes%29%2C%20and%20pixel-level%20grounding.%20We%20propose%20a%20novel%20Mixture-of-Experts%20%28MoE%29%0Amulti-stage%20training%20strategy%2C%20which%20divides%20MoE%20into%20separate%20training%20phases%0Afor%20a%20visual-language%20expert%20model%20and%20a%20pixel-grounding%20expert%20model%2C%20followed%0Aby%20fine-tuning%20using%20MoE.%20This%20strategy%20effectively%20coordinates%20multitask%0Alearning%20while%20maintaining%20the%20computational%20cost%20at%20inference%20equivalent%20to%0Athat%20of%20a%20single%20expert%20model.%20To%20advance%20the%20research%20of%20biomedical%20MLLMs%2C%20we%0Aintroduce%20the%20Medical%20Complex%20Vision%20Question%20Answering%20Dataset%20%28MeCoVQA%29%2C%0Awhich%20comprises%20an%20array%20of%208%20modalities%20for%20complex%20medical%20imaging%20question%0Aanswering%20and%20image%20region%20understanding.%20Experimental%20results%20indicate%20that%0AMedPLIB%20has%20achieved%20state-of-the-art%20outcomes%20across%20multiple%20medical%20visual%0Alanguage%20tasks.%20More%20importantly%2C%20in%20zero-shot%20evaluations%20for%20the%20pixel%0Agrounding%20task%2C%20MedPLIB%20leads%20the%20best%20small%20and%20large%20models%20by%20margins%20of%0A19.7%20and%2015.6%20respectively%20on%20the%20mDice%20metric.%20The%20codes%2C%20data%2C%20and%20model%0Acheckpoints%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/ShawnHuang497/MedPLIB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520with%2520Pixel-Level%2520Insight%2520for%250A%2520%2520Biomedicine%26entry.906535625%3DXiaoshuang%2520Huang%2520and%2520Lingdong%2520Shen%2520and%2520Jia%2520Liu%2520and%2520Fangxin%2520Shang%2520and%2520Hongxiang%2520Li%2520and%2520Haifeng%2520Huang%2520and%2520Yehui%2520Yang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLM%2529%2520have%2520achieved%250Anotable%2520advancements%252C%2520demonstrating%2520the%2520feasibility%2520of%2520developing%2520an%250Aintelligent%2520biomedical%2520assistant.%2520However%252C%2520current%2520biomedical%2520MLLMs%250Apredominantly%2520focus%2520on%2520image-level%2520understanding%2520and%2520restrict%2520interactions%2520to%250Atextual%2520commands%252C%2520thus%2520limiting%2520their%2520capability%2520boundaries%2520and%2520the%2520flexibility%250Aof%2520usage.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520end-to-end%2520multimodal%2520large%250Alanguage%2520model%2520for%2520the%2520biomedical%2520domain%252C%2520named%2520MedPLIB%252C%2520which%2520possesses%250Apixel-level%2520understanding.%2520Excitingly%252C%2520it%2520supports%2520visual%2520question%2520answering%250A%2528VQA%2529%252C%2520arbitrary%2520pixel-level%2520prompts%2520%2528points%252C%2520bounding%2520boxes%252C%2520and%2520free-form%250Ashapes%2529%252C%2520and%2520pixel-level%2520grounding.%2520We%2520propose%2520a%2520novel%2520Mixture-of-Experts%2520%2528MoE%2529%250Amulti-stage%2520training%2520strategy%252C%2520which%2520divides%2520MoE%2520into%2520separate%2520training%2520phases%250Afor%2520a%2520visual-language%2520expert%2520model%2520and%2520a%2520pixel-grounding%2520expert%2520model%252C%2520followed%250Aby%2520fine-tuning%2520using%2520MoE.%2520This%2520strategy%2520effectively%2520coordinates%2520multitask%250Alearning%2520while%2520maintaining%2520the%2520computational%2520cost%2520at%2520inference%2520equivalent%2520to%250Athat%2520of%2520a%2520single%2520expert%2520model.%2520To%2520advance%2520the%2520research%2520of%2520biomedical%2520MLLMs%252C%2520we%250Aintroduce%2520the%2520Medical%2520Complex%2520Vision%2520Question%2520Answering%2520Dataset%2520%2528MeCoVQA%2529%252C%250Awhich%2520comprises%2520an%2520array%2520of%25208%2520modalities%2520for%2520complex%2520medical%2520imaging%2520question%250Aanswering%2520and%2520image%2520region%2520understanding.%2520Experimental%2520results%2520indicate%2520that%250AMedPLIB%2520has%2520achieved%2520state-of-the-art%2520outcomes%2520across%2520multiple%2520medical%2520visual%250Alanguage%2520tasks.%2520More%2520importantly%252C%2520in%2520zero-shot%2520evaluations%2520for%2520the%2520pixel%250Agrounding%2520task%252C%2520MedPLIB%2520leads%2520the%2520best%2520small%2520and%2520large%2520models%2520by%2520margins%2520of%250A19.7%2520and%252015.6%2520respectively%2520on%2520the%2520mDice%2520metric.%2520The%2520codes%252C%2520data%252C%2520and%2520model%250Acheckpoints%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/ShawnHuang497/MedPLIB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Multimodal%20Large%20Language%20Model%20with%20Pixel-Level%20Insight%20for%0A%20%20Biomedicine&entry.906535625=Xiaoshuang%20Huang%20and%20Lingdong%20Shen%20and%20Jia%20Liu%20and%20Fangxin%20Shang%20and%20Hongxiang%20Li%20and%20Haifeng%20Huang%20and%20Yehui%20Yang&entry.1292438233=%20%20In%20recent%20years%2C%20Multimodal%20Large%20Language%20Models%20%28MLLM%29%20have%20achieved%0Anotable%20advancements%2C%20demonstrating%20the%20feasibility%20of%20developing%20an%0Aintelligent%20biomedical%20assistant.%20However%2C%20current%20biomedical%20MLLMs%0Apredominantly%20focus%20on%20image-level%20understanding%20and%20restrict%20interactions%20to%0Atextual%20commands%2C%20thus%20limiting%20their%20capability%20boundaries%20and%20the%20flexibility%0Aof%20usage.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20end-to-end%20multimodal%20large%0Alanguage%20model%20for%20the%20biomedical%20domain%2C%20named%20MedPLIB%2C%20which%20possesses%0Apixel-level%20understanding.%20Excitingly%2C%20it%20supports%20visual%20question%20answering%0A%28VQA%29%2C%20arbitrary%20pixel-level%20prompts%20%28points%2C%20bounding%20boxes%2C%20and%20free-form%0Ashapes%29%2C%20and%20pixel-level%20grounding.%20We%20propose%20a%20novel%20Mixture-of-Experts%20%28MoE%29%0Amulti-stage%20training%20strategy%2C%20which%20divides%20MoE%20into%20separate%20training%20phases%0Afor%20a%20visual-language%20expert%20model%20and%20a%20pixel-grounding%20expert%20model%2C%20followed%0Aby%20fine-tuning%20using%20MoE.%20This%20strategy%20effectively%20coordinates%20multitask%0Alearning%20while%20maintaining%20the%20computational%20cost%20at%20inference%20equivalent%20to%0Athat%20of%20a%20single%20expert%20model.%20To%20advance%20the%20research%20of%20biomedical%20MLLMs%2C%20we%0Aintroduce%20the%20Medical%20Complex%20Vision%20Question%20Answering%20Dataset%20%28MeCoVQA%29%2C%0Awhich%20comprises%20an%20array%20of%208%20modalities%20for%20complex%20medical%20imaging%20question%0Aanswering%20and%20image%20region%20understanding.%20Experimental%20results%20indicate%20that%0AMedPLIB%20has%20achieved%20state-of-the-art%20outcomes%20across%20multiple%20medical%20visual%0Alanguage%20tasks.%20More%20importantly%2C%20in%20zero-shot%20evaluations%20for%20the%20pixel%0Agrounding%20task%2C%20MedPLIB%20leads%20the%20best%20small%20and%20large%20models%20by%20margins%20of%0A19.7%20and%2015.6%20respectively%20on%20the%20mDice%20metric.%20The%20codes%2C%20data%2C%20and%20model%0Acheckpoints%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/ShawnHuang497/MedPLIB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09278v1&entry.124074799=Read"},
{"title": "Word Sense Linking: Disambiguating Outside the Sandbox", "author": "Andrei Stefan Bejgu and Edoardo Barba and Luigi Procopio and Alberte Fern\u00e1ndez-Castro and Roberto Navigli", "abstract": "  Word Sense Disambiguation (WSD) is the task of associating a word in a given\ncontext with its most suitable meaning among a set of possible candidates.\nWhile the task has recently witnessed renewed interest, with systems achieving\nperformances above the estimated inter-annotator agreement, at the time of\nwriting it still struggles to find downstream applications. We argue that one\nof the reasons behind this is the difficulty of applying WSD to plain text.\nIndeed, in the standard formulation, models work under the assumptions that a)\nall the spans to disambiguate have already been identified, and b) all the\npossible candidate senses of each span are provided, both of which are\nrequirements that are far from trivial. In this work, we present a new task\ncalled Word Sense Linking (WSL) where, given an input text and a reference\nsense inventory, systems have to both identify which spans to disambiguate and\nthen link them to their most suitable meaning.We put forward a\ntransformer-based architecture for the task and thoroughly evaluate both its\nperformance and those of state-of-the-art WSD systems scaled to WSL,\niteratively relaxing the assumptions of WSD. We hope that our work will foster\neasier integration of lexical semantics into downstream applications.\n", "link": "http://arxiv.org/abs/2412.09370v1", "date": "2024-12-12", "relevancy": 2.3493, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Word%20Sense%20Linking%3A%20Disambiguating%20Outside%20the%20Sandbox&body=Title%3A%20Word%20Sense%20Linking%3A%20Disambiguating%20Outside%20the%20Sandbox%0AAuthor%3A%20Andrei%20Stefan%20Bejgu%20and%20Edoardo%20Barba%20and%20Luigi%20Procopio%20and%20Alberte%20Fern%C3%A1ndez-Castro%20and%20Roberto%20Navigli%0AAbstract%3A%20%20%20Word%20Sense%20Disambiguation%20%28WSD%29%20is%20the%20task%20of%20associating%20a%20word%20in%20a%20given%0Acontext%20with%20its%20most%20suitable%20meaning%20among%20a%20set%20of%20possible%20candidates.%0AWhile%20the%20task%20has%20recently%20witnessed%20renewed%20interest%2C%20with%20systems%20achieving%0Aperformances%20above%20the%20estimated%20inter-annotator%20agreement%2C%20at%20the%20time%20of%0Awriting%20it%20still%20struggles%20to%20find%20downstream%20applications.%20We%20argue%20that%20one%0Aof%20the%20reasons%20behind%20this%20is%20the%20difficulty%20of%20applying%20WSD%20to%20plain%20text.%0AIndeed%2C%20in%20the%20standard%20formulation%2C%20models%20work%20under%20the%20assumptions%20that%20a%29%0Aall%20the%20spans%20to%20disambiguate%20have%20already%20been%20identified%2C%20and%20b%29%20all%20the%0Apossible%20candidate%20senses%20of%20each%20span%20are%20provided%2C%20both%20of%20which%20are%0Arequirements%20that%20are%20far%20from%20trivial.%20In%20this%20work%2C%20we%20present%20a%20new%20task%0Acalled%20Word%20Sense%20Linking%20%28WSL%29%20where%2C%20given%20an%20input%20text%20and%20a%20reference%0Asense%20inventory%2C%20systems%20have%20to%20both%20identify%20which%20spans%20to%20disambiguate%20and%0Athen%20link%20them%20to%20their%20most%20suitable%20meaning.We%20put%20forward%20a%0Atransformer-based%20architecture%20for%20the%20task%20and%20thoroughly%20evaluate%20both%20its%0Aperformance%20and%20those%20of%20state-of-the-art%20WSD%20systems%20scaled%20to%20WSL%2C%0Aiteratively%20relaxing%20the%20assumptions%20of%20WSD.%20We%20hope%20that%20our%20work%20will%20foster%0Aeasier%20integration%20of%20lexical%20semantics%20into%20downstream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWord%2520Sense%2520Linking%253A%2520Disambiguating%2520Outside%2520the%2520Sandbox%26entry.906535625%3DAndrei%2520Stefan%2520Bejgu%2520and%2520Edoardo%2520Barba%2520and%2520Luigi%2520Procopio%2520and%2520Alberte%2520Fern%25C3%25A1ndez-Castro%2520and%2520Roberto%2520Navigli%26entry.1292438233%3D%2520%2520Word%2520Sense%2520Disambiguation%2520%2528WSD%2529%2520is%2520the%2520task%2520of%2520associating%2520a%2520word%2520in%2520a%2520given%250Acontext%2520with%2520its%2520most%2520suitable%2520meaning%2520among%2520a%2520set%2520of%2520possible%2520candidates.%250AWhile%2520the%2520task%2520has%2520recently%2520witnessed%2520renewed%2520interest%252C%2520with%2520systems%2520achieving%250Aperformances%2520above%2520the%2520estimated%2520inter-annotator%2520agreement%252C%2520at%2520the%2520time%2520of%250Awriting%2520it%2520still%2520struggles%2520to%2520find%2520downstream%2520applications.%2520We%2520argue%2520that%2520one%250Aof%2520the%2520reasons%2520behind%2520this%2520is%2520the%2520difficulty%2520of%2520applying%2520WSD%2520to%2520plain%2520text.%250AIndeed%252C%2520in%2520the%2520standard%2520formulation%252C%2520models%2520work%2520under%2520the%2520assumptions%2520that%2520a%2529%250Aall%2520the%2520spans%2520to%2520disambiguate%2520have%2520already%2520been%2520identified%252C%2520and%2520b%2529%2520all%2520the%250Apossible%2520candidate%2520senses%2520of%2520each%2520span%2520are%2520provided%252C%2520both%2520of%2520which%2520are%250Arequirements%2520that%2520are%2520far%2520from%2520trivial.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%2520task%250Acalled%2520Word%2520Sense%2520Linking%2520%2528WSL%2529%2520where%252C%2520given%2520an%2520input%2520text%2520and%2520a%2520reference%250Asense%2520inventory%252C%2520systems%2520have%2520to%2520both%2520identify%2520which%2520spans%2520to%2520disambiguate%2520and%250Athen%2520link%2520them%2520to%2520their%2520most%2520suitable%2520meaning.We%2520put%2520forward%2520a%250Atransformer-based%2520architecture%2520for%2520the%2520task%2520and%2520thoroughly%2520evaluate%2520both%2520its%250Aperformance%2520and%2520those%2520of%2520state-of-the-art%2520WSD%2520systems%2520scaled%2520to%2520WSL%252C%250Aiteratively%2520relaxing%2520the%2520assumptions%2520of%2520WSD.%2520We%2520hope%2520that%2520our%2520work%2520will%2520foster%250Aeasier%2520integration%2520of%2520lexical%2520semantics%2520into%2520downstream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Word%20Sense%20Linking%3A%20Disambiguating%20Outside%20the%20Sandbox&entry.906535625=Andrei%20Stefan%20Bejgu%20and%20Edoardo%20Barba%20and%20Luigi%20Procopio%20and%20Alberte%20Fern%C3%A1ndez-Castro%20and%20Roberto%20Navigli&entry.1292438233=%20%20Word%20Sense%20Disambiguation%20%28WSD%29%20is%20the%20task%20of%20associating%20a%20word%20in%20a%20given%0Acontext%20with%20its%20most%20suitable%20meaning%20among%20a%20set%20of%20possible%20candidates.%0AWhile%20the%20task%20has%20recently%20witnessed%20renewed%20interest%2C%20with%20systems%20achieving%0Aperformances%20above%20the%20estimated%20inter-annotator%20agreement%2C%20at%20the%20time%20of%0Awriting%20it%20still%20struggles%20to%20find%20downstream%20applications.%20We%20argue%20that%20one%0Aof%20the%20reasons%20behind%20this%20is%20the%20difficulty%20of%20applying%20WSD%20to%20plain%20text.%0AIndeed%2C%20in%20the%20standard%20formulation%2C%20models%20work%20under%20the%20assumptions%20that%20a%29%0Aall%20the%20spans%20to%20disambiguate%20have%20already%20been%20identified%2C%20and%20b%29%20all%20the%0Apossible%20candidate%20senses%20of%20each%20span%20are%20provided%2C%20both%20of%20which%20are%0Arequirements%20that%20are%20far%20from%20trivial.%20In%20this%20work%2C%20we%20present%20a%20new%20task%0Acalled%20Word%20Sense%20Linking%20%28WSL%29%20where%2C%20given%20an%20input%20text%20and%20a%20reference%0Asense%20inventory%2C%20systems%20have%20to%20both%20identify%20which%20spans%20to%20disambiguate%20and%0Athen%20link%20them%20to%20their%20most%20suitable%20meaning.We%20put%20forward%20a%0Atransformer-based%20architecture%20for%20the%20task%20and%20thoroughly%20evaluate%20both%20its%0Aperformance%20and%20those%20of%20state-of-the-art%20WSD%20systems%20scaled%20to%20WSL%2C%0Aiteratively%20relaxing%20the%20assumptions%20of%20WSD.%20We%20hope%20that%20our%20work%20will%20foster%0Aeasier%20integration%20of%20lexical%20semantics%20into%20downstream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09370v1&entry.124074799=Read"},
{"title": "T-SVG: Text-Driven Stereoscopic Video Generation", "author": "Qiao Jin and Xiaodong Chen and Wu Liu and Tao Mei and Yongdong Zhang", "abstract": "  The advent of stereoscopic videos has opened new horizons in multimedia,\nparticularly in extended reality (XR) and virtual reality (VR) applications,\nwhere immersive content captivates audiences across various platforms. Despite\nits growing popularity, producing stereoscopic videos remains challenging due\nto the technical complexities involved in generating stereo parallax. This\nrefers to the positional differences of objects viewed from two distinct\nperspectives and is crucial for creating depth perception. This complex process\nposes significant challenges for creators aiming to deliver convincing and\nengaging presentations. To address these challenges, this paper introduces the\nText-driven Stereoscopic Video Generation (T-SVG) system. This innovative,\nmodel-agnostic, zero-shot approach streamlines video generation by using text\nprompts to create reference videos. These videos are transformed into 3D point\ncloud sequences, which are rendered from two perspectives with subtle parallax\ndifferences, achieving a natural stereoscopic effect. T-SVG represents a\nsignificant advancement in stereoscopic content creation by integrating\nstate-of-the-art, training-free techniques in text-to-video generation, depth\nestimation, and video inpainting. Its flexible architecture ensures high\nefficiency and user-friendliness, allowing seamless updates with newer models\nwithout retraining. By simplifying the production pipeline, T-SVG makes\nstereoscopic video generation accessible to a broader audience, demonstrating\nits potential to revolutionize the field.\n", "link": "http://arxiv.org/abs/2412.09323v1", "date": "2024-12-12", "relevancy": 2.3444, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5904}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5879}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-SVG%3A%20Text-Driven%20Stereoscopic%20Video%20Generation&body=Title%3A%20T-SVG%3A%20Text-Driven%20Stereoscopic%20Video%20Generation%0AAuthor%3A%20Qiao%20Jin%20and%20Xiaodong%20Chen%20and%20Wu%20Liu%20and%20Tao%20Mei%20and%20Yongdong%20Zhang%0AAbstract%3A%20%20%20The%20advent%20of%20stereoscopic%20videos%20has%20opened%20new%20horizons%20in%20multimedia%2C%0Aparticularly%20in%20extended%20reality%20%28XR%29%20and%20virtual%20reality%20%28VR%29%20applications%2C%0Awhere%20immersive%20content%20captivates%20audiences%20across%20various%20platforms.%20Despite%0Aits%20growing%20popularity%2C%20producing%20stereoscopic%20videos%20remains%20challenging%20due%0Ato%20the%20technical%20complexities%20involved%20in%20generating%20stereo%20parallax.%20This%0Arefers%20to%20the%20positional%20differences%20of%20objects%20viewed%20from%20two%20distinct%0Aperspectives%20and%20is%20crucial%20for%20creating%20depth%20perception.%20This%20complex%20process%0Aposes%20significant%20challenges%20for%20creators%20aiming%20to%20deliver%20convincing%20and%0Aengaging%20presentations.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%20the%0AText-driven%20Stereoscopic%20Video%20Generation%20%28T-SVG%29%20system.%20This%20innovative%2C%0Amodel-agnostic%2C%20zero-shot%20approach%20streamlines%20video%20generation%20by%20using%20text%0Aprompts%20to%20create%20reference%20videos.%20These%20videos%20are%20transformed%20into%203D%20point%0Acloud%20sequences%2C%20which%20are%20rendered%20from%20two%20perspectives%20with%20subtle%20parallax%0Adifferences%2C%20achieving%20a%20natural%20stereoscopic%20effect.%20T-SVG%20represents%20a%0Asignificant%20advancement%20in%20stereoscopic%20content%20creation%20by%20integrating%0Astate-of-the-art%2C%20training-free%20techniques%20in%20text-to-video%20generation%2C%20depth%0Aestimation%2C%20and%20video%20inpainting.%20Its%20flexible%20architecture%20ensures%20high%0Aefficiency%20and%20user-friendliness%2C%20allowing%20seamless%20updates%20with%20newer%20models%0Awithout%20retraining.%20By%20simplifying%20the%20production%20pipeline%2C%20T-SVG%20makes%0Astereoscopic%20video%20generation%20accessible%20to%20a%20broader%20audience%2C%20demonstrating%0Aits%20potential%20to%20revolutionize%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-SVG%253A%2520Text-Driven%2520Stereoscopic%2520Video%2520Generation%26entry.906535625%3DQiao%2520Jin%2520and%2520Xiaodong%2520Chen%2520and%2520Wu%2520Liu%2520and%2520Tao%2520Mei%2520and%2520Yongdong%2520Zhang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520stereoscopic%2520videos%2520has%2520opened%2520new%2520horizons%2520in%2520multimedia%252C%250Aparticularly%2520in%2520extended%2520reality%2520%2528XR%2529%2520and%2520virtual%2520reality%2520%2528VR%2529%2520applications%252C%250Awhere%2520immersive%2520content%2520captivates%2520audiences%2520across%2520various%2520platforms.%2520Despite%250Aits%2520growing%2520popularity%252C%2520producing%2520stereoscopic%2520videos%2520remains%2520challenging%2520due%250Ato%2520the%2520technical%2520complexities%2520involved%2520in%2520generating%2520stereo%2520parallax.%2520This%250Arefers%2520to%2520the%2520positional%2520differences%2520of%2520objects%2520viewed%2520from%2520two%2520distinct%250Aperspectives%2520and%2520is%2520crucial%2520for%2520creating%2520depth%2520perception.%2520This%2520complex%2520process%250Aposes%2520significant%2520challenges%2520for%2520creators%2520aiming%2520to%2520deliver%2520convincing%2520and%250Aengaging%2520presentations.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520introduces%2520the%250AText-driven%2520Stereoscopic%2520Video%2520Generation%2520%2528T-SVG%2529%2520system.%2520This%2520innovative%252C%250Amodel-agnostic%252C%2520zero-shot%2520approach%2520streamlines%2520video%2520generation%2520by%2520using%2520text%250Aprompts%2520to%2520create%2520reference%2520videos.%2520These%2520videos%2520are%2520transformed%2520into%25203D%2520point%250Acloud%2520sequences%252C%2520which%2520are%2520rendered%2520from%2520two%2520perspectives%2520with%2520subtle%2520parallax%250Adifferences%252C%2520achieving%2520a%2520natural%2520stereoscopic%2520effect.%2520T-SVG%2520represents%2520a%250Asignificant%2520advancement%2520in%2520stereoscopic%2520content%2520creation%2520by%2520integrating%250Astate-of-the-art%252C%2520training-free%2520techniques%2520in%2520text-to-video%2520generation%252C%2520depth%250Aestimation%252C%2520and%2520video%2520inpainting.%2520Its%2520flexible%2520architecture%2520ensures%2520high%250Aefficiency%2520and%2520user-friendliness%252C%2520allowing%2520seamless%2520updates%2520with%2520newer%2520models%250Awithout%2520retraining.%2520By%2520simplifying%2520the%2520production%2520pipeline%252C%2520T-SVG%2520makes%250Astereoscopic%2520video%2520generation%2520accessible%2520to%2520a%2520broader%2520audience%252C%2520demonstrating%250Aits%2520potential%2520to%2520revolutionize%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-SVG%3A%20Text-Driven%20Stereoscopic%20Video%20Generation&entry.906535625=Qiao%20Jin%20and%20Xiaodong%20Chen%20and%20Wu%20Liu%20and%20Tao%20Mei%20and%20Yongdong%20Zhang&entry.1292438233=%20%20The%20advent%20of%20stereoscopic%20videos%20has%20opened%20new%20horizons%20in%20multimedia%2C%0Aparticularly%20in%20extended%20reality%20%28XR%29%20and%20virtual%20reality%20%28VR%29%20applications%2C%0Awhere%20immersive%20content%20captivates%20audiences%20across%20various%20platforms.%20Despite%0Aits%20growing%20popularity%2C%20producing%20stereoscopic%20videos%20remains%20challenging%20due%0Ato%20the%20technical%20complexities%20involved%20in%20generating%20stereo%20parallax.%20This%0Arefers%20to%20the%20positional%20differences%20of%20objects%20viewed%20from%20two%20distinct%0Aperspectives%20and%20is%20crucial%20for%20creating%20depth%20perception.%20This%20complex%20process%0Aposes%20significant%20challenges%20for%20creators%20aiming%20to%20deliver%20convincing%20and%0Aengaging%20presentations.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%20the%0AText-driven%20Stereoscopic%20Video%20Generation%20%28T-SVG%29%20system.%20This%20innovative%2C%0Amodel-agnostic%2C%20zero-shot%20approach%20streamlines%20video%20generation%20by%20using%20text%0Aprompts%20to%20create%20reference%20videos.%20These%20videos%20are%20transformed%20into%203D%20point%0Acloud%20sequences%2C%20which%20are%20rendered%20from%20two%20perspectives%20with%20subtle%20parallax%0Adifferences%2C%20achieving%20a%20natural%20stereoscopic%20effect.%20T-SVG%20represents%20a%0Asignificant%20advancement%20in%20stereoscopic%20content%20creation%20by%20integrating%0Astate-of-the-art%2C%20training-free%20techniques%20in%20text-to-video%20generation%2C%20depth%0Aestimation%2C%20and%20video%20inpainting.%20Its%20flexible%20architecture%20ensures%20high%0Aefficiency%20and%20user-friendliness%2C%20allowing%20seamless%20updates%20with%20newer%20models%0Awithout%20retraining.%20By%20simplifying%20the%20production%20pipeline%2C%20T-SVG%20makes%0Astereoscopic%20video%20generation%20accessible%20to%20a%20broader%20audience%2C%20demonstrating%0Aits%20potential%20to%20revolutionize%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09323v1&entry.124074799=Read"},
{"title": "Text-Video Multi-Grained Integration for Video Moment Montage", "author": "Zhihui Yin and Ye Ma and Xipeng Cao and Bo Wang and Quan Chen and Peng Jiang", "abstract": "  The proliferation of online short video platforms has driven a surge in user\ndemand for short video editing. However, manually selecting, cropping, and\nassembling raw footage into a coherent, high-quality video remains laborious\nand time-consuming. To accelerate this process, we focus on a user-friendly new\ntask called Video Moment Montage (VMM), which aims to accurately locate the\ncorresponding video segments based on a pre-provided narration text and then\narrange these video clips to create a complete video that aligns with the\ncorresponding descriptions. The challenge lies in extracting precise temporal\nsegments while ensuring intra-sentence and inter-sentence context consistency,\nas a single script sentence may require trimming and assembling multiple video\nclips. To address this problem, we present a novel \\textit{Text-Video\nMulti-Grained Integration} method (TV-MGI) that efficiently fuses text features\nfrom the script with both shot-level and frame-level video features, which\nenables the global and fine-grained alignment between the video content and the\ncorresponding textual descriptions in the script. To facilitate further\nresearch in this area, we introduce the Multiple Sentences with Shots Dataset\n(MSSD), a large-scale dataset designed explicitly for the VMM task. We conduct\nextensive experiments on the MSSD dataset to demonstrate the effectiveness of\nour framework compared to baseline methods.\n", "link": "http://arxiv.org/abs/2412.09276v1", "date": "2024-12-12", "relevancy": 2.3412, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.634}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Video%20Multi-Grained%20Integration%20for%20Video%20Moment%20Montage&body=Title%3A%20Text-Video%20Multi-Grained%20Integration%20for%20Video%20Moment%20Montage%0AAuthor%3A%20Zhihui%20Yin%20and%20Ye%20Ma%20and%20Xipeng%20Cao%20and%20Bo%20Wang%20and%20Quan%20Chen%20and%20Peng%20Jiang%0AAbstract%3A%20%20%20The%20proliferation%20of%20online%20short%20video%20platforms%20has%20driven%20a%20surge%20in%20user%0Ademand%20for%20short%20video%20editing.%20However%2C%20manually%20selecting%2C%20cropping%2C%20and%0Aassembling%20raw%20footage%20into%20a%20coherent%2C%20high-quality%20video%20remains%20laborious%0Aand%20time-consuming.%20To%20accelerate%20this%20process%2C%20we%20focus%20on%20a%20user-friendly%20new%0Atask%20called%20Video%20Moment%20Montage%20%28VMM%29%2C%20which%20aims%20to%20accurately%20locate%20the%0Acorresponding%20video%20segments%20based%20on%20a%20pre-provided%20narration%20text%20and%20then%0Aarrange%20these%20video%20clips%20to%20create%20a%20complete%20video%20that%20aligns%20with%20the%0Acorresponding%20descriptions.%20The%20challenge%20lies%20in%20extracting%20precise%20temporal%0Asegments%20while%20ensuring%20intra-sentence%20and%20inter-sentence%20context%20consistency%2C%0Aas%20a%20single%20script%20sentence%20may%20require%20trimming%20and%20assembling%20multiple%20video%0Aclips.%20To%20address%20this%20problem%2C%20we%20present%20a%20novel%20%5Ctextit%7BText-Video%0AMulti-Grained%20Integration%7D%20method%20%28TV-MGI%29%20that%20efficiently%20fuses%20text%20features%0Afrom%20the%20script%20with%20both%20shot-level%20and%20frame-level%20video%20features%2C%20which%0Aenables%20the%20global%20and%20fine-grained%20alignment%20between%20the%20video%20content%20and%20the%0Acorresponding%20textual%20descriptions%20in%20the%20script.%20To%20facilitate%20further%0Aresearch%20in%20this%20area%2C%20we%20introduce%20the%20Multiple%20Sentences%20with%20Shots%20Dataset%0A%28MSSD%29%2C%20a%20large-scale%20dataset%20designed%20explicitly%20for%20the%20VMM%20task.%20We%20conduct%0Aextensive%20experiments%20on%20the%20MSSD%20dataset%20to%20demonstrate%20the%20effectiveness%20of%0Aour%20framework%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Video%2520Multi-Grained%2520Integration%2520for%2520Video%2520Moment%2520Montage%26entry.906535625%3DZhihui%2520Yin%2520and%2520Ye%2520Ma%2520and%2520Xipeng%2520Cao%2520and%2520Bo%2520Wang%2520and%2520Quan%2520Chen%2520and%2520Peng%2520Jiang%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520online%2520short%2520video%2520platforms%2520has%2520driven%2520a%2520surge%2520in%2520user%250Ademand%2520for%2520short%2520video%2520editing.%2520However%252C%2520manually%2520selecting%252C%2520cropping%252C%2520and%250Aassembling%2520raw%2520footage%2520into%2520a%2520coherent%252C%2520high-quality%2520video%2520remains%2520laborious%250Aand%2520time-consuming.%2520To%2520accelerate%2520this%2520process%252C%2520we%2520focus%2520on%2520a%2520user-friendly%2520new%250Atask%2520called%2520Video%2520Moment%2520Montage%2520%2528VMM%2529%252C%2520which%2520aims%2520to%2520accurately%2520locate%2520the%250Acorresponding%2520video%2520segments%2520based%2520on%2520a%2520pre-provided%2520narration%2520text%2520and%2520then%250Aarrange%2520these%2520video%2520clips%2520to%2520create%2520a%2520complete%2520video%2520that%2520aligns%2520with%2520the%250Acorresponding%2520descriptions.%2520The%2520challenge%2520lies%2520in%2520extracting%2520precise%2520temporal%250Asegments%2520while%2520ensuring%2520intra-sentence%2520and%2520inter-sentence%2520context%2520consistency%252C%250Aas%2520a%2520single%2520script%2520sentence%2520may%2520require%2520trimming%2520and%2520assembling%2520multiple%2520video%250Aclips.%2520To%2520address%2520this%2520problem%252C%2520we%2520present%2520a%2520novel%2520%255Ctextit%257BText-Video%250AMulti-Grained%2520Integration%257D%2520method%2520%2528TV-MGI%2529%2520that%2520efficiently%2520fuses%2520text%2520features%250Afrom%2520the%2520script%2520with%2520both%2520shot-level%2520and%2520frame-level%2520video%2520features%252C%2520which%250Aenables%2520the%2520global%2520and%2520fine-grained%2520alignment%2520between%2520the%2520video%2520content%2520and%2520the%250Acorresponding%2520textual%2520descriptions%2520in%2520the%2520script.%2520To%2520facilitate%2520further%250Aresearch%2520in%2520this%2520area%252C%2520we%2520introduce%2520the%2520Multiple%2520Sentences%2520with%2520Shots%2520Dataset%250A%2528MSSD%2529%252C%2520a%2520large-scale%2520dataset%2520designed%2520explicitly%2520for%2520the%2520VMM%2520task.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520the%2520MSSD%2520dataset%2520to%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520framework%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Video%20Multi-Grained%20Integration%20for%20Video%20Moment%20Montage&entry.906535625=Zhihui%20Yin%20and%20Ye%20Ma%20and%20Xipeng%20Cao%20and%20Bo%20Wang%20and%20Quan%20Chen%20and%20Peng%20Jiang&entry.1292438233=%20%20The%20proliferation%20of%20online%20short%20video%20platforms%20has%20driven%20a%20surge%20in%20user%0Ademand%20for%20short%20video%20editing.%20However%2C%20manually%20selecting%2C%20cropping%2C%20and%0Aassembling%20raw%20footage%20into%20a%20coherent%2C%20high-quality%20video%20remains%20laborious%0Aand%20time-consuming.%20To%20accelerate%20this%20process%2C%20we%20focus%20on%20a%20user-friendly%20new%0Atask%20called%20Video%20Moment%20Montage%20%28VMM%29%2C%20which%20aims%20to%20accurately%20locate%20the%0Acorresponding%20video%20segments%20based%20on%20a%20pre-provided%20narration%20text%20and%20then%0Aarrange%20these%20video%20clips%20to%20create%20a%20complete%20video%20that%20aligns%20with%20the%0Acorresponding%20descriptions.%20The%20challenge%20lies%20in%20extracting%20precise%20temporal%0Asegments%20while%20ensuring%20intra-sentence%20and%20inter-sentence%20context%20consistency%2C%0Aas%20a%20single%20script%20sentence%20may%20require%20trimming%20and%20assembling%20multiple%20video%0Aclips.%20To%20address%20this%20problem%2C%20we%20present%20a%20novel%20%5Ctextit%7BText-Video%0AMulti-Grained%20Integration%7D%20method%20%28TV-MGI%29%20that%20efficiently%20fuses%20text%20features%0Afrom%20the%20script%20with%20both%20shot-level%20and%20frame-level%20video%20features%2C%20which%0Aenables%20the%20global%20and%20fine-grained%20alignment%20between%20the%20video%20content%20and%20the%0Acorresponding%20textual%20descriptions%20in%20the%20script.%20To%20facilitate%20further%0Aresearch%20in%20this%20area%2C%20we%20introduce%20the%20Multiple%20Sentences%20with%20Shots%20Dataset%0A%28MSSD%29%2C%20a%20large-scale%20dataset%20designed%20explicitly%20for%20the%20VMM%20task.%20We%20conduct%0Aextensive%20experiments%20on%20the%20MSSD%20dataset%20to%20demonstrate%20the%20effectiveness%20of%0Aour%20framework%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09276v1&entry.124074799=Read"},
{"title": "Capturing the Temporal Dependence of Training Data Influence", "author": "Jiachen T. Wang and Dawn Song and James Zou and Prateek Mittal and Ruoxi Jia", "abstract": "  Traditional data influence estimation methods, like influence function,\nassume that learning algorithms are permutation-invariant with respect to\ntraining data. However, modern training paradigms, especially for foundation\nmodels using stochastic algorithms and multi-stage curricula, are sensitive to\ndata ordering, thus violating this assumption. This mismatch renders influence\nfunctions inadequate for answering a critical question in machine learning: How\ncan we capture the dependence of data influence on the optimization trajectory\nduring training? To address this gap, we formalize the concept of\ntrajectory-specific leave-one-out (LOO) influence, which quantifies the impact\nof removing a data point from a specific iteration during training, accounting\nfor the exact sequence of data encountered and the model's optimization\ntrajectory. However, exactly evaluating the trajectory-specific LOO presents a\nsignificant computational challenge. To address this, we propose data value\nembedding, a novel technique enabling efficient approximation of\ntrajectory-specific LOO. Specifically, we compute a training data embedding\nthat encapsulates the cumulative interactions between data and the evolving\nmodel parameters. The LOO can then be efficiently approximated through a simple\ndot-product between the data value embedding and the gradient of the given test\ndata. As data value embedding captures training data ordering, it offers\nvaluable insights into model training dynamics. In particular, we uncover\ndistinct phases of data influence, revealing that data points in the early and\nlate stages of training exert a greater impact on the final model. These\ninsights translate into actionable strategies for managing the computational\noverhead of data selection by strategically timing the selection process,\npotentially opening new avenues in data curation research.\n", "link": "http://arxiv.org/abs/2412.09538v1", "date": "2024-12-12", "relevancy": 2.3406, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4706}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20the%20Temporal%20Dependence%20of%20Training%20Data%20Influence&body=Title%3A%20Capturing%20the%20Temporal%20Dependence%20of%20Training%20Data%20Influence%0AAuthor%3A%20Jiachen%20T.%20Wang%20and%20Dawn%20Song%20and%20James%20Zou%20and%20Prateek%20Mittal%20and%20Ruoxi%20Jia%0AAbstract%3A%20%20%20Traditional%20data%20influence%20estimation%20methods%2C%20like%20influence%20function%2C%0Aassume%20that%20learning%20algorithms%20are%20permutation-invariant%20with%20respect%20to%0Atraining%20data.%20However%2C%20modern%20training%20paradigms%2C%20especially%20for%20foundation%0Amodels%20using%20stochastic%20algorithms%20and%20multi-stage%20curricula%2C%20are%20sensitive%20to%0Adata%20ordering%2C%20thus%20violating%20this%20assumption.%20This%20mismatch%20renders%20influence%0Afunctions%20inadequate%20for%20answering%20a%20critical%20question%20in%20machine%20learning%3A%20How%0Acan%20we%20capture%20the%20dependence%20of%20data%20influence%20on%20the%20optimization%20trajectory%0Aduring%20training%3F%20To%20address%20this%20gap%2C%20we%20formalize%20the%20concept%20of%0Atrajectory-specific%20leave-one-out%20%28LOO%29%20influence%2C%20which%20quantifies%20the%20impact%0Aof%20removing%20a%20data%20point%20from%20a%20specific%20iteration%20during%20training%2C%20accounting%0Afor%20the%20exact%20sequence%20of%20data%20encountered%20and%20the%20model%27s%20optimization%0Atrajectory.%20However%2C%20exactly%20evaluating%20the%20trajectory-specific%20LOO%20presents%20a%0Asignificant%20computational%20challenge.%20To%20address%20this%2C%20we%20propose%20data%20value%0Aembedding%2C%20a%20novel%20technique%20enabling%20efficient%20approximation%20of%0Atrajectory-specific%20LOO.%20Specifically%2C%20we%20compute%20a%20training%20data%20embedding%0Athat%20encapsulates%20the%20cumulative%20interactions%20between%20data%20and%20the%20evolving%0Amodel%20parameters.%20The%20LOO%20can%20then%20be%20efficiently%20approximated%20through%20a%20simple%0Adot-product%20between%20the%20data%20value%20embedding%20and%20the%20gradient%20of%20the%20given%20test%0Adata.%20As%20data%20value%20embedding%20captures%20training%20data%20ordering%2C%20it%20offers%0Avaluable%20insights%20into%20model%20training%20dynamics.%20In%20particular%2C%20we%20uncover%0Adistinct%20phases%20of%20data%20influence%2C%20revealing%20that%20data%20points%20in%20the%20early%20and%0Alate%20stages%20of%20training%20exert%20a%20greater%20impact%20on%20the%20final%20model.%20These%0Ainsights%20translate%20into%20actionable%20strategies%20for%20managing%20the%20computational%0Aoverhead%20of%20data%20selection%20by%20strategically%20timing%20the%20selection%20process%2C%0Apotentially%20opening%20new%20avenues%20in%20data%20curation%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520the%2520Temporal%2520Dependence%2520of%2520Training%2520Data%2520Influence%26entry.906535625%3DJiachen%2520T.%2520Wang%2520and%2520Dawn%2520Song%2520and%2520James%2520Zou%2520and%2520Prateek%2520Mittal%2520and%2520Ruoxi%2520Jia%26entry.1292438233%3D%2520%2520Traditional%2520data%2520influence%2520estimation%2520methods%252C%2520like%2520influence%2520function%252C%250Aassume%2520that%2520learning%2520algorithms%2520are%2520permutation-invariant%2520with%2520respect%2520to%250Atraining%2520data.%2520However%252C%2520modern%2520training%2520paradigms%252C%2520especially%2520for%2520foundation%250Amodels%2520using%2520stochastic%2520algorithms%2520and%2520multi-stage%2520curricula%252C%2520are%2520sensitive%2520to%250Adata%2520ordering%252C%2520thus%2520violating%2520this%2520assumption.%2520This%2520mismatch%2520renders%2520influence%250Afunctions%2520inadequate%2520for%2520answering%2520a%2520critical%2520question%2520in%2520machine%2520learning%253A%2520How%250Acan%2520we%2520capture%2520the%2520dependence%2520of%2520data%2520influence%2520on%2520the%2520optimization%2520trajectory%250Aduring%2520training%253F%2520To%2520address%2520this%2520gap%252C%2520we%2520formalize%2520the%2520concept%2520of%250Atrajectory-specific%2520leave-one-out%2520%2528LOO%2529%2520influence%252C%2520which%2520quantifies%2520the%2520impact%250Aof%2520removing%2520a%2520data%2520point%2520from%2520a%2520specific%2520iteration%2520during%2520training%252C%2520accounting%250Afor%2520the%2520exact%2520sequence%2520of%2520data%2520encountered%2520and%2520the%2520model%2527s%2520optimization%250Atrajectory.%2520However%252C%2520exactly%2520evaluating%2520the%2520trajectory-specific%2520LOO%2520presents%2520a%250Asignificant%2520computational%2520challenge.%2520To%2520address%2520this%252C%2520we%2520propose%2520data%2520value%250Aembedding%252C%2520a%2520novel%2520technique%2520enabling%2520efficient%2520approximation%2520of%250Atrajectory-specific%2520LOO.%2520Specifically%252C%2520we%2520compute%2520a%2520training%2520data%2520embedding%250Athat%2520encapsulates%2520the%2520cumulative%2520interactions%2520between%2520data%2520and%2520the%2520evolving%250Amodel%2520parameters.%2520The%2520LOO%2520can%2520then%2520be%2520efficiently%2520approximated%2520through%2520a%2520simple%250Adot-product%2520between%2520the%2520data%2520value%2520embedding%2520and%2520the%2520gradient%2520of%2520the%2520given%2520test%250Adata.%2520As%2520data%2520value%2520embedding%2520captures%2520training%2520data%2520ordering%252C%2520it%2520offers%250Avaluable%2520insights%2520into%2520model%2520training%2520dynamics.%2520In%2520particular%252C%2520we%2520uncover%250Adistinct%2520phases%2520of%2520data%2520influence%252C%2520revealing%2520that%2520data%2520points%2520in%2520the%2520early%2520and%250Alate%2520stages%2520of%2520training%2520exert%2520a%2520greater%2520impact%2520on%2520the%2520final%2520model.%2520These%250Ainsights%2520translate%2520into%2520actionable%2520strategies%2520for%2520managing%2520the%2520computational%250Aoverhead%2520of%2520data%2520selection%2520by%2520strategically%2520timing%2520the%2520selection%2520process%252C%250Apotentially%2520opening%2520new%2520avenues%2520in%2520data%2520curation%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20the%20Temporal%20Dependence%20of%20Training%20Data%20Influence&entry.906535625=Jiachen%20T.%20Wang%20and%20Dawn%20Song%20and%20James%20Zou%20and%20Prateek%20Mittal%20and%20Ruoxi%20Jia&entry.1292438233=%20%20Traditional%20data%20influence%20estimation%20methods%2C%20like%20influence%20function%2C%0Aassume%20that%20learning%20algorithms%20are%20permutation-invariant%20with%20respect%20to%0Atraining%20data.%20However%2C%20modern%20training%20paradigms%2C%20especially%20for%20foundation%0Amodels%20using%20stochastic%20algorithms%20and%20multi-stage%20curricula%2C%20are%20sensitive%20to%0Adata%20ordering%2C%20thus%20violating%20this%20assumption.%20This%20mismatch%20renders%20influence%0Afunctions%20inadequate%20for%20answering%20a%20critical%20question%20in%20machine%20learning%3A%20How%0Acan%20we%20capture%20the%20dependence%20of%20data%20influence%20on%20the%20optimization%20trajectory%0Aduring%20training%3F%20To%20address%20this%20gap%2C%20we%20formalize%20the%20concept%20of%0Atrajectory-specific%20leave-one-out%20%28LOO%29%20influence%2C%20which%20quantifies%20the%20impact%0Aof%20removing%20a%20data%20point%20from%20a%20specific%20iteration%20during%20training%2C%20accounting%0Afor%20the%20exact%20sequence%20of%20data%20encountered%20and%20the%20model%27s%20optimization%0Atrajectory.%20However%2C%20exactly%20evaluating%20the%20trajectory-specific%20LOO%20presents%20a%0Asignificant%20computational%20challenge.%20To%20address%20this%2C%20we%20propose%20data%20value%0Aembedding%2C%20a%20novel%20technique%20enabling%20efficient%20approximation%20of%0Atrajectory-specific%20LOO.%20Specifically%2C%20we%20compute%20a%20training%20data%20embedding%0Athat%20encapsulates%20the%20cumulative%20interactions%20between%20data%20and%20the%20evolving%0Amodel%20parameters.%20The%20LOO%20can%20then%20be%20efficiently%20approximated%20through%20a%20simple%0Adot-product%20between%20the%20data%20value%20embedding%20and%20the%20gradient%20of%20the%20given%20test%0Adata.%20As%20data%20value%20embedding%20captures%20training%20data%20ordering%2C%20it%20offers%0Avaluable%20insights%20into%20model%20training%20dynamics.%20In%20particular%2C%20we%20uncover%0Adistinct%20phases%20of%20data%20influence%2C%20revealing%20that%20data%20points%20in%20the%20early%20and%0Alate%20stages%20of%20training%20exert%20a%20greater%20impact%20on%20the%20final%20model.%20These%0Ainsights%20translate%20into%20actionable%20strategies%20for%20managing%20the%20computational%0Aoverhead%20of%20data%20selection%20by%20strategically%20timing%20the%20selection%20process%2C%0Apotentially%20opening%20new%20avenues%20in%20data%20curation%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09538v1&entry.124074799=Read"},
{"title": "Owl-1: Omni World Model for Consistent Long Video Generation", "author": "Yuanhui Huang and Wenzhao Zheng and Yuan Gao and Xin Tao and Pengfei Wan and Di Zhang and Jie Zhou and Jiwen Lu", "abstract": "  Video generation models (VGMs) have received extensive attention recently and\nserve as promising candidates for general-purpose large vision models. While\nthey can only generate short videos each time, existing methods achieve long\nvideo generation by iteratively calling the VGMs, using the last-frame output\nas the condition for the next-round generation. However, the last frame only\ncontains short-term fine-grained information about the scene, resulting in\ninconsistency in the long horizon. To address this, we propose an Omni World\nmodeL (Owl-1) to produce long-term coherent and comprehensive conditions for\nconsistent long video generation. As videos are observations of the underlying\nevolving world, we propose to model the long-term developments in a latent\nspace and use VGMs to film them into videos. Specifically, we represent the\nworld with a latent state variable which can be decoded into explicit video\nobservations. These observations serve as a basis for anticipating temporal\ndynamics which in turn update the state variable. The interaction between\nevolving dynamics and persistent state enhances the diversity and consistency\nof the long videos. Extensive experiments show that Owl-1 achieves comparable\nperformance with SOTA methods on VBench-I2V and VBench-Long, validating its\nability to generate high-quality video observations. Code:\nhttps://github.com/huang-yh/Owl.\n", "link": "http://arxiv.org/abs/2412.09600v1", "date": "2024-12-12", "relevancy": 2.3287, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5982}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5719}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Owl-1%3A%20Omni%20World%20Model%20for%20Consistent%20Long%20Video%20Generation&body=Title%3A%20Owl-1%3A%20Omni%20World%20Model%20for%20Consistent%20Long%20Video%20Generation%0AAuthor%3A%20Yuanhui%20Huang%20and%20Wenzhao%20Zheng%20and%20Yuan%20Gao%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Video%20generation%20models%20%28VGMs%29%20have%20received%20extensive%20attention%20recently%20and%0Aserve%20as%20promising%20candidates%20for%20general-purpose%20large%20vision%20models.%20While%0Athey%20can%20only%20generate%20short%20videos%20each%20time%2C%20existing%20methods%20achieve%20long%0Avideo%20generation%20by%20iteratively%20calling%20the%20VGMs%2C%20using%20the%20last-frame%20output%0Aas%20the%20condition%20for%20the%20next-round%20generation.%20However%2C%20the%20last%20frame%20only%0Acontains%20short-term%20fine-grained%20information%20about%20the%20scene%2C%20resulting%20in%0Ainconsistency%20in%20the%20long%20horizon.%20To%20address%20this%2C%20we%20propose%20an%20Omni%20World%0AmodeL%20%28Owl-1%29%20to%20produce%20long-term%20coherent%20and%20comprehensive%20conditions%20for%0Aconsistent%20long%20video%20generation.%20As%20videos%20are%20observations%20of%20the%20underlying%0Aevolving%20world%2C%20we%20propose%20to%20model%20the%20long-term%20developments%20in%20a%20latent%0Aspace%20and%20use%20VGMs%20to%20film%20them%20into%20videos.%20Specifically%2C%20we%20represent%20the%0Aworld%20with%20a%20latent%20state%20variable%20which%20can%20be%20decoded%20into%20explicit%20video%0Aobservations.%20These%20observations%20serve%20as%20a%20basis%20for%20anticipating%20temporal%0Adynamics%20which%20in%20turn%20update%20the%20state%20variable.%20The%20interaction%20between%0Aevolving%20dynamics%20and%20persistent%20state%20enhances%20the%20diversity%20and%20consistency%0Aof%20the%20long%20videos.%20Extensive%20experiments%20show%20that%20Owl-1%20achieves%20comparable%0Aperformance%20with%20SOTA%20methods%20on%20VBench-I2V%20and%20VBench-Long%2C%20validating%20its%0Aability%20to%20generate%20high-quality%20video%20observations.%20Code%3A%0Ahttps%3A//github.com/huang-yh/Owl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOwl-1%253A%2520Omni%2520World%2520Model%2520for%2520Consistent%2520Long%2520Video%2520Generation%26entry.906535625%3DYuanhui%2520Huang%2520and%2520Wenzhao%2520Zheng%2520and%2520Yuan%2520Gao%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Video%2520generation%2520models%2520%2528VGMs%2529%2520have%2520received%2520extensive%2520attention%2520recently%2520and%250Aserve%2520as%2520promising%2520candidates%2520for%2520general-purpose%2520large%2520vision%2520models.%2520While%250Athey%2520can%2520only%2520generate%2520short%2520videos%2520each%2520time%252C%2520existing%2520methods%2520achieve%2520long%250Avideo%2520generation%2520by%2520iteratively%2520calling%2520the%2520VGMs%252C%2520using%2520the%2520last-frame%2520output%250Aas%2520the%2520condition%2520for%2520the%2520next-round%2520generation.%2520However%252C%2520the%2520last%2520frame%2520only%250Acontains%2520short-term%2520fine-grained%2520information%2520about%2520the%2520scene%252C%2520resulting%2520in%250Ainconsistency%2520in%2520the%2520long%2520horizon.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520Omni%2520World%250AmodeL%2520%2528Owl-1%2529%2520to%2520produce%2520long-term%2520coherent%2520and%2520comprehensive%2520conditions%2520for%250Aconsistent%2520long%2520video%2520generation.%2520As%2520videos%2520are%2520observations%2520of%2520the%2520underlying%250Aevolving%2520world%252C%2520we%2520propose%2520to%2520model%2520the%2520long-term%2520developments%2520in%2520a%2520latent%250Aspace%2520and%2520use%2520VGMs%2520to%2520film%2520them%2520into%2520videos.%2520Specifically%252C%2520we%2520represent%2520the%250Aworld%2520with%2520a%2520latent%2520state%2520variable%2520which%2520can%2520be%2520decoded%2520into%2520explicit%2520video%250Aobservations.%2520These%2520observations%2520serve%2520as%2520a%2520basis%2520for%2520anticipating%2520temporal%250Adynamics%2520which%2520in%2520turn%2520update%2520the%2520state%2520variable.%2520The%2520interaction%2520between%250Aevolving%2520dynamics%2520and%2520persistent%2520state%2520enhances%2520the%2520diversity%2520and%2520consistency%250Aof%2520the%2520long%2520videos.%2520Extensive%2520experiments%2520show%2520that%2520Owl-1%2520achieves%2520comparable%250Aperformance%2520with%2520SOTA%2520methods%2520on%2520VBench-I2V%2520and%2520VBench-Long%252C%2520validating%2520its%250Aability%2520to%2520generate%2520high-quality%2520video%2520observations.%2520Code%253A%250Ahttps%253A//github.com/huang-yh/Owl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Owl-1%3A%20Omni%20World%20Model%20for%20Consistent%20Long%20Video%20Generation&entry.906535625=Yuanhui%20Huang%20and%20Wenzhao%20Zheng%20and%20Yuan%20Gao%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Video%20generation%20models%20%28VGMs%29%20have%20received%20extensive%20attention%20recently%20and%0Aserve%20as%20promising%20candidates%20for%20general-purpose%20large%20vision%20models.%20While%0Athey%20can%20only%20generate%20short%20videos%20each%20time%2C%20existing%20methods%20achieve%20long%0Avideo%20generation%20by%20iteratively%20calling%20the%20VGMs%2C%20using%20the%20last-frame%20output%0Aas%20the%20condition%20for%20the%20next-round%20generation.%20However%2C%20the%20last%20frame%20only%0Acontains%20short-term%20fine-grained%20information%20about%20the%20scene%2C%20resulting%20in%0Ainconsistency%20in%20the%20long%20horizon.%20To%20address%20this%2C%20we%20propose%20an%20Omni%20World%0AmodeL%20%28Owl-1%29%20to%20produce%20long-term%20coherent%20and%20comprehensive%20conditions%20for%0Aconsistent%20long%20video%20generation.%20As%20videos%20are%20observations%20of%20the%20underlying%0Aevolving%20world%2C%20we%20propose%20to%20model%20the%20long-term%20developments%20in%20a%20latent%0Aspace%20and%20use%20VGMs%20to%20film%20them%20into%20videos.%20Specifically%2C%20we%20represent%20the%0Aworld%20with%20a%20latent%20state%20variable%20which%20can%20be%20decoded%20into%20explicit%20video%0Aobservations.%20These%20observations%20serve%20as%20a%20basis%20for%20anticipating%20temporal%0Adynamics%20which%20in%20turn%20update%20the%20state%20variable.%20The%20interaction%20between%0Aevolving%20dynamics%20and%20persistent%20state%20enhances%20the%20diversity%20and%20consistency%0Aof%20the%20long%20videos.%20Extensive%20experiments%20show%20that%20Owl-1%20achieves%20comparable%0Aperformance%20with%20SOTA%20methods%20on%20VBench-I2V%20and%20VBench-Long%2C%20validating%20its%0Aability%20to%20generate%20high-quality%20video%20observations.%20Code%3A%0Ahttps%3A//github.com/huang-yh/Owl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09600v1&entry.124074799=Read"},
{"title": "Video Summarization using Denoising Diffusion Probabilistic Model", "author": "Zirui Shang and Yubo Zhu and Hongxi Li and Shuo Yang and Xinxiao Wu", "abstract": "  Video summarization aims to eliminate visual redundancy while retaining key\nparts of video to construct concise and comprehensive synopses. Most existing\nmethods use discriminative models to predict the importance scores of video\nframes. However, these methods are susceptible to annotation inconsistency\ncaused by the inherent subjectivity of different annotators when annotating the\nsame video. In this paper, we introduce a generative framework for video\nsummarization that learns how to generate summaries from a probability\ndistribution perspective, effectively reducing the interference of subjective\nannotation noise. Specifically, we propose a novel diffusion summarization\nmethod based on the Denoising Diffusion Probabilistic Model (DDPM), which\nlearns the probability distribution of training data through noise prediction,\nand generates summaries by iterative denoising. Our method is more resistant to\nsubjective annotation noise, and is less prone to overfitting the training data\nthan discriminative methods, with strong generalization ability. Moreover, to\nfacilitate training DDPM with limited data, we employ an unsupervised video\nsummarization model to implement the earlier denoising process. Extensive\nexperiments on various datasets (TVSum, SumMe, and FPVSum) demonstrate the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2412.08357v2", "date": "2024-12-12", "relevancy": 2.3237, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6011}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5673}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Summarization%20using%20Denoising%20Diffusion%20Probabilistic%20Model&body=Title%3A%20Video%20Summarization%20using%20Denoising%20Diffusion%20Probabilistic%20Model%0AAuthor%3A%20Zirui%20Shang%20and%20Yubo%20Zhu%20and%20Hongxi%20Li%20and%20Shuo%20Yang%20and%20Xinxiao%20Wu%0AAbstract%3A%20%20%20Video%20summarization%20aims%20to%20eliminate%20visual%20redundancy%20while%20retaining%20key%0Aparts%20of%20video%20to%20construct%20concise%20and%20comprehensive%20synopses.%20Most%20existing%0Amethods%20use%20discriminative%20models%20to%20predict%20the%20importance%20scores%20of%20video%0Aframes.%20However%2C%20these%20methods%20are%20susceptible%20to%20annotation%20inconsistency%0Acaused%20by%20the%20inherent%20subjectivity%20of%20different%20annotators%20when%20annotating%20the%0Asame%20video.%20In%20this%20paper%2C%20we%20introduce%20a%20generative%20framework%20for%20video%0Asummarization%20that%20learns%20how%20to%20generate%20summaries%20from%20a%20probability%0Adistribution%20perspective%2C%20effectively%20reducing%20the%20interference%20of%20subjective%0Aannotation%20noise.%20Specifically%2C%20we%20propose%20a%20novel%20diffusion%20summarization%0Amethod%20based%20on%20the%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%2C%20which%0Alearns%20the%20probability%20distribution%20of%20training%20data%20through%20noise%20prediction%2C%0Aand%20generates%20summaries%20by%20iterative%20denoising.%20Our%20method%20is%20more%20resistant%20to%0Asubjective%20annotation%20noise%2C%20and%20is%20less%20prone%20to%20overfitting%20the%20training%20data%0Athan%20discriminative%20methods%2C%20with%20strong%20generalization%20ability.%20Moreover%2C%20to%0Afacilitate%20training%20DDPM%20with%20limited%20data%2C%20we%20employ%20an%20unsupervised%20video%0Asummarization%20model%20to%20implement%20the%20earlier%20denoising%20process.%20Extensive%0Aexperiments%20on%20various%20datasets%20%28TVSum%2C%20SumMe%2C%20and%20FPVSum%29%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08357v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Summarization%2520using%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%26entry.906535625%3DZirui%2520Shang%2520and%2520Yubo%2520Zhu%2520and%2520Hongxi%2520Li%2520and%2520Shuo%2520Yang%2520and%2520Xinxiao%2520Wu%26entry.1292438233%3D%2520%2520Video%2520summarization%2520aims%2520to%2520eliminate%2520visual%2520redundancy%2520while%2520retaining%2520key%250Aparts%2520of%2520video%2520to%2520construct%2520concise%2520and%2520comprehensive%2520synopses.%2520Most%2520existing%250Amethods%2520use%2520discriminative%2520models%2520to%2520predict%2520the%2520importance%2520scores%2520of%2520video%250Aframes.%2520However%252C%2520these%2520methods%2520are%2520susceptible%2520to%2520annotation%2520inconsistency%250Acaused%2520by%2520the%2520inherent%2520subjectivity%2520of%2520different%2520annotators%2520when%2520annotating%2520the%250Asame%2520video.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520generative%2520framework%2520for%2520video%250Asummarization%2520that%2520learns%2520how%2520to%2520generate%2520summaries%2520from%2520a%2520probability%250Adistribution%2520perspective%252C%2520effectively%2520reducing%2520the%2520interference%2520of%2520subjective%250Aannotation%2520noise.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520diffusion%2520summarization%250Amethod%2520based%2520on%2520the%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%2520%2528DDPM%2529%252C%2520which%250Alearns%2520the%2520probability%2520distribution%2520of%2520training%2520data%2520through%2520noise%2520prediction%252C%250Aand%2520generates%2520summaries%2520by%2520iterative%2520denoising.%2520Our%2520method%2520is%2520more%2520resistant%2520to%250Asubjective%2520annotation%2520noise%252C%2520and%2520is%2520less%2520prone%2520to%2520overfitting%2520the%2520training%2520data%250Athan%2520discriminative%2520methods%252C%2520with%2520strong%2520generalization%2520ability.%2520Moreover%252C%2520to%250Afacilitate%2520training%2520DDPM%2520with%2520limited%2520data%252C%2520we%2520employ%2520an%2520unsupervised%2520video%250Asummarization%2520model%2520to%2520implement%2520the%2520earlier%2520denoising%2520process.%2520Extensive%250Aexperiments%2520on%2520various%2520datasets%2520%2528TVSum%252C%2520SumMe%252C%2520and%2520FPVSum%2529%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08357v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Summarization%20using%20Denoising%20Diffusion%20Probabilistic%20Model&entry.906535625=Zirui%20Shang%20and%20Yubo%20Zhu%20and%20Hongxi%20Li%20and%20Shuo%20Yang%20and%20Xinxiao%20Wu&entry.1292438233=%20%20Video%20summarization%20aims%20to%20eliminate%20visual%20redundancy%20while%20retaining%20key%0Aparts%20of%20video%20to%20construct%20concise%20and%20comprehensive%20synopses.%20Most%20existing%0Amethods%20use%20discriminative%20models%20to%20predict%20the%20importance%20scores%20of%20video%0Aframes.%20However%2C%20these%20methods%20are%20susceptible%20to%20annotation%20inconsistency%0Acaused%20by%20the%20inherent%20subjectivity%20of%20different%20annotators%20when%20annotating%20the%0Asame%20video.%20In%20this%20paper%2C%20we%20introduce%20a%20generative%20framework%20for%20video%0Asummarization%20that%20learns%20how%20to%20generate%20summaries%20from%20a%20probability%0Adistribution%20perspective%2C%20effectively%20reducing%20the%20interference%20of%20subjective%0Aannotation%20noise.%20Specifically%2C%20we%20propose%20a%20novel%20diffusion%20summarization%0Amethod%20based%20on%20the%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%2C%20which%0Alearns%20the%20probability%20distribution%20of%20training%20data%20through%20noise%20prediction%2C%0Aand%20generates%20summaries%20by%20iterative%20denoising.%20Our%20method%20is%20more%20resistant%20to%0Asubjective%20annotation%20noise%2C%20and%20is%20less%20prone%20to%20overfitting%20the%20training%20data%0Athan%20discriminative%20methods%2C%20with%20strong%20generalization%20ability.%20Moreover%2C%20to%0Afacilitate%20training%20DDPM%20with%20limited%20data%2C%20we%20employ%20an%20unsupervised%20video%0Asummarization%20model%20to%20implement%20the%20earlier%20denoising%20process.%20Extensive%0Aexperiments%20on%20various%20datasets%20%28TVSum%2C%20SumMe%2C%20and%20FPVSum%29%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08357v2&entry.124074799=Read"},
{"title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language\n  Model for Clinical Pathology Analysis", "author": "Shengxuming Zhang and Weihan Li and Tianhong Gao and Jiacong Hu and Haoming Luo and Mingli Song and Xiuming Zhang and Zunlei Feng", "abstract": "  Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\ntraditional pure vision models face challenges of redundant feature extraction,\nwhereas existing large vision-language models (LVLMs) are limited by input\nresolution constraints, hindering their efficiency and accuracy. To overcome\nthese issues, we propose two innovative strategies: the mixed task-guided\nfeature enhancement, which directs feature extraction toward lesion-related\ndetails across scales, and the prompt-guided detail feature completion, which\nintegrates coarse- and fine-grained features from WSI based on specific prompts\nwithout compromising inference speed. Leveraging a comprehensive dataset of\n490,000 samples from diverse pathology tasks-including cancer detection,\ngrading, vascular and neural invasion identification, and so on-we trained the\npathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that\nthis model significantly outperforms existing methods in diagnostic accuracy\nand efficiency, offering an interactive, clinically aligned approach for\nauxiliary diagnosis in a wide range of pathology applications.\n", "link": "http://arxiv.org/abs/2412.09521v1", "date": "2024-12-12", "relevancy": 2.3068, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Comprehensive%20Feature%20Extraction%20in%20Large%20Vision-Language%0A%20%20Model%20for%20Clinical%20Pathology%20Analysis&body=Title%3A%20Efficient%20and%20Comprehensive%20Feature%20Extraction%20in%20Large%20Vision-Language%0A%20%20Model%20for%20Clinical%20Pathology%20Analysis%0AAuthor%3A%20Shengxuming%20Zhang%20and%20Weihan%20Li%20and%20Tianhong%20Gao%20and%20Jiacong%20Hu%20and%20Haoming%20Luo%20and%20Mingli%20Song%20and%20Xiuming%20Zhang%20and%20Zunlei%20Feng%0AAbstract%3A%20%20%20Pathological%20diagnosis%20is%20vital%20for%20determining%20disease%20characteristics%2C%0Aguiding%20treatment%2C%20and%20assessing%20prognosis%2C%20relying%20heavily%20on%20detailed%2C%0Amulti-scale%20analysis%20of%20high-resolution%20whole%20slide%20images%20%28WSI%29.%20However%2C%0Atraditional%20pure%20vision%20models%20face%20challenges%20of%20redundant%20feature%20extraction%2C%0Awhereas%20existing%20large%20vision-language%20models%20%28LVLMs%29%20are%20limited%20by%20input%0Aresolution%20constraints%2C%20hindering%20their%20efficiency%20and%20accuracy.%20To%20overcome%0Athese%20issues%2C%20we%20propose%20two%20innovative%20strategies%3A%20the%20mixed%20task-guided%0Afeature%20enhancement%2C%20which%20directs%20feature%20extraction%20toward%20lesion-related%0Adetails%20across%20scales%2C%20and%20the%20prompt-guided%20detail%20feature%20completion%2C%20which%0Aintegrates%20coarse-%20and%20fine-grained%20features%20from%20WSI%20based%20on%20specific%20prompts%0Awithout%20compromising%20inference%20speed.%20Leveraging%20a%20comprehensive%20dataset%20of%0A490%2C000%20samples%20from%20diverse%20pathology%20tasks-including%20cancer%20detection%2C%0Agrading%2C%20vascular%20and%20neural%20invasion%20identification%2C%20and%20so%20on-we%20trained%20the%0Apathology-specialized%20LVLM%2C%20OmniPath.%20Extensive%20experiments%20demonstrate%20that%0Athis%20model%20significantly%20outperforms%20existing%20methods%20in%20diagnostic%20accuracy%0Aand%20efficiency%2C%20offering%20an%20interactive%2C%20clinically%20aligned%20approach%20for%0Aauxiliary%20diagnosis%20in%20a%20wide%20range%20of%20pathology%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Comprehensive%2520Feature%2520Extraction%2520in%2520Large%2520Vision-Language%250A%2520%2520Model%2520for%2520Clinical%2520Pathology%2520Analysis%26entry.906535625%3DShengxuming%2520Zhang%2520and%2520Weihan%2520Li%2520and%2520Tianhong%2520Gao%2520and%2520Jiacong%2520Hu%2520and%2520Haoming%2520Luo%2520and%2520Mingli%2520Song%2520and%2520Xiuming%2520Zhang%2520and%2520Zunlei%2520Feng%26entry.1292438233%3D%2520%2520Pathological%2520diagnosis%2520is%2520vital%2520for%2520determining%2520disease%2520characteristics%252C%250Aguiding%2520treatment%252C%2520and%2520assessing%2520prognosis%252C%2520relying%2520heavily%2520on%2520detailed%252C%250Amulti-scale%2520analysis%2520of%2520high-resolution%2520whole%2520slide%2520images%2520%2528WSI%2529.%2520However%252C%250Atraditional%2520pure%2520vision%2520models%2520face%2520challenges%2520of%2520redundant%2520feature%2520extraction%252C%250Awhereas%2520existing%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520limited%2520by%2520input%250Aresolution%2520constraints%252C%2520hindering%2520their%2520efficiency%2520and%2520accuracy.%2520To%2520overcome%250Athese%2520issues%252C%2520we%2520propose%2520two%2520innovative%2520strategies%253A%2520the%2520mixed%2520task-guided%250Afeature%2520enhancement%252C%2520which%2520directs%2520feature%2520extraction%2520toward%2520lesion-related%250Adetails%2520across%2520scales%252C%2520and%2520the%2520prompt-guided%2520detail%2520feature%2520completion%252C%2520which%250Aintegrates%2520coarse-%2520and%2520fine-grained%2520features%2520from%2520WSI%2520based%2520on%2520specific%2520prompts%250Awithout%2520compromising%2520inference%2520speed.%2520Leveraging%2520a%2520comprehensive%2520dataset%2520of%250A490%252C000%2520samples%2520from%2520diverse%2520pathology%2520tasks-including%2520cancer%2520detection%252C%250Agrading%252C%2520vascular%2520and%2520neural%2520invasion%2520identification%252C%2520and%2520so%2520on-we%2520trained%2520the%250Apathology-specialized%2520LVLM%252C%2520OmniPath.%2520Extensive%2520experiments%2520demonstrate%2520that%250Athis%2520model%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520diagnostic%2520accuracy%250Aand%2520efficiency%252C%2520offering%2520an%2520interactive%252C%2520clinically%2520aligned%2520approach%2520for%250Aauxiliary%2520diagnosis%2520in%2520a%2520wide%2520range%2520of%2520pathology%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Comprehensive%20Feature%20Extraction%20in%20Large%20Vision-Language%0A%20%20Model%20for%20Clinical%20Pathology%20Analysis&entry.906535625=Shengxuming%20Zhang%20and%20Weihan%20Li%20and%20Tianhong%20Gao%20and%20Jiacong%20Hu%20and%20Haoming%20Luo%20and%20Mingli%20Song%20and%20Xiuming%20Zhang%20and%20Zunlei%20Feng&entry.1292438233=%20%20Pathological%20diagnosis%20is%20vital%20for%20determining%20disease%20characteristics%2C%0Aguiding%20treatment%2C%20and%20assessing%20prognosis%2C%20relying%20heavily%20on%20detailed%2C%0Amulti-scale%20analysis%20of%20high-resolution%20whole%20slide%20images%20%28WSI%29.%20However%2C%0Atraditional%20pure%20vision%20models%20face%20challenges%20of%20redundant%20feature%20extraction%2C%0Awhereas%20existing%20large%20vision-language%20models%20%28LVLMs%29%20are%20limited%20by%20input%0Aresolution%20constraints%2C%20hindering%20their%20efficiency%20and%20accuracy.%20To%20overcome%0Athese%20issues%2C%20we%20propose%20two%20innovative%20strategies%3A%20the%20mixed%20task-guided%0Afeature%20enhancement%2C%20which%20directs%20feature%20extraction%20toward%20lesion-related%0Adetails%20across%20scales%2C%20and%20the%20prompt-guided%20detail%20feature%20completion%2C%20which%0Aintegrates%20coarse-%20and%20fine-grained%20features%20from%20WSI%20based%20on%20specific%20prompts%0Awithout%20compromising%20inference%20speed.%20Leveraging%20a%20comprehensive%20dataset%20of%0A490%2C000%20samples%20from%20diverse%20pathology%20tasks-including%20cancer%20detection%2C%0Agrading%2C%20vascular%20and%20neural%20invasion%20identification%2C%20and%20so%20on-we%20trained%20the%0Apathology-specialized%20LVLM%2C%20OmniPath.%20Extensive%20experiments%20demonstrate%20that%0Athis%20model%20significantly%20outperforms%20existing%20methods%20in%20diagnostic%20accuracy%0Aand%20efficiency%2C%20offering%20an%20interactive%2C%20clinically%20aligned%20approach%20for%0Aauxiliary%20diagnosis%20in%20a%20wide%20range%20of%20pathology%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09521v1&entry.124074799=Read"},
{"title": "Learning Camera Movement Control from Real-World Drone Videos", "author": "Yunzhong Hou and Liang Zheng and Philip Torr", "abstract": "  This study seeks to automate camera movement control for filming existing\nsubjects into attractive videos, contrasting with the creation of non-existent\ncontent by directly generating the pixels. We select drone videos as our test\ncase due to their rich and challenging motion patterns, distinctive viewing\nangles, and precise controls. Existing AI videography methods struggle with\nlimited appearance diversity in simulation training, high costs of recording\nexpert operations, and difficulties in designing heuristic-based goals to cover\nall scenarios. To avoid these issues, we propose a scalable method that\ninvolves collecting real-world training data to improve diversity, extracting\ncamera trajectories automatically to minimize annotation costs, and training an\neffective architecture that does not rely on heuristics. Specifically, we\ncollect 99k high-quality trajectories by running 3D reconstruction on online\nvideos, connecting camera poses from consecutive frames to formulate 3D camera\npaths, and using Kalman filter to identify and remove low-quality data.\nMoreover, we introduce DVGFormer, an auto-regressive transformer that leverages\nthe camera path and images from all past frames to predict camera movement in\nthe next frame. We evaluate our system across 38 synthetic natural scenes and 7\nreal city 3D scans. We show that our system effectively learns to perform\nchallenging camera movements such as navigating through obstacles, maintaining\nlow altitude to increase perceived speed, and orbiting towers and buildings,\nwhich are very useful for recording high-quality videos. Data and code are\navailable at dvgformer.github.io.\n", "link": "http://arxiv.org/abs/2412.09620v1", "date": "2024-12-12", "relevancy": 2.3013, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5806}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5743}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Camera%20Movement%20Control%20from%20Real-World%20Drone%20Videos&body=Title%3A%20Learning%20Camera%20Movement%20Control%20from%20Real-World%20Drone%20Videos%0AAuthor%3A%20Yunzhong%20Hou%20and%20Liang%20Zheng%20and%20Philip%20Torr%0AAbstract%3A%20%20%20This%20study%20seeks%20to%20automate%20camera%20movement%20control%20for%20filming%20existing%0Asubjects%20into%20attractive%20videos%2C%20contrasting%20with%20the%20creation%20of%20non-existent%0Acontent%20by%20directly%20generating%20the%20pixels.%20We%20select%20drone%20videos%20as%20our%20test%0Acase%20due%20to%20their%20rich%20and%20challenging%20motion%20patterns%2C%20distinctive%20viewing%0Aangles%2C%20and%20precise%20controls.%20Existing%20AI%20videography%20methods%20struggle%20with%0Alimited%20appearance%20diversity%20in%20simulation%20training%2C%20high%20costs%20of%20recording%0Aexpert%20operations%2C%20and%20difficulties%20in%20designing%20heuristic-based%20goals%20to%20cover%0Aall%20scenarios.%20To%20avoid%20these%20issues%2C%20we%20propose%20a%20scalable%20method%20that%0Ainvolves%20collecting%20real-world%20training%20data%20to%20improve%20diversity%2C%20extracting%0Acamera%20trajectories%20automatically%20to%20minimize%20annotation%20costs%2C%20and%20training%20an%0Aeffective%20architecture%20that%20does%20not%20rely%20on%20heuristics.%20Specifically%2C%20we%0Acollect%2099k%20high-quality%20trajectories%20by%20running%203D%20reconstruction%20on%20online%0Avideos%2C%20connecting%20camera%20poses%20from%20consecutive%20frames%20to%20formulate%203D%20camera%0Apaths%2C%20and%20using%20Kalman%20filter%20to%20identify%20and%20remove%20low-quality%20data.%0AMoreover%2C%20we%20introduce%20DVGFormer%2C%20an%20auto-regressive%20transformer%20that%20leverages%0Athe%20camera%20path%20and%20images%20from%20all%20past%20frames%20to%20predict%20camera%20movement%20in%0Athe%20next%20frame.%20We%20evaluate%20our%20system%20across%2038%20synthetic%20natural%20scenes%20and%207%0Areal%20city%203D%20scans.%20We%20show%20that%20our%20system%20effectively%20learns%20to%20perform%0Achallenging%20camera%20movements%20such%20as%20navigating%20through%20obstacles%2C%20maintaining%0Alow%20altitude%20to%20increase%20perceived%20speed%2C%20and%20orbiting%20towers%20and%20buildings%2C%0Awhich%20are%20very%20useful%20for%20recording%20high-quality%20videos.%20Data%20and%20code%20are%0Aavailable%20at%20dvgformer.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Camera%2520Movement%2520Control%2520from%2520Real-World%2520Drone%2520Videos%26entry.906535625%3DYunzhong%2520Hou%2520and%2520Liang%2520Zheng%2520and%2520Philip%2520Torr%26entry.1292438233%3D%2520%2520This%2520study%2520seeks%2520to%2520automate%2520camera%2520movement%2520control%2520for%2520filming%2520existing%250Asubjects%2520into%2520attractive%2520videos%252C%2520contrasting%2520with%2520the%2520creation%2520of%2520non-existent%250Acontent%2520by%2520directly%2520generating%2520the%2520pixels.%2520We%2520select%2520drone%2520videos%2520as%2520our%2520test%250Acase%2520due%2520to%2520their%2520rich%2520and%2520challenging%2520motion%2520patterns%252C%2520distinctive%2520viewing%250Aangles%252C%2520and%2520precise%2520controls.%2520Existing%2520AI%2520videography%2520methods%2520struggle%2520with%250Alimited%2520appearance%2520diversity%2520in%2520simulation%2520training%252C%2520high%2520costs%2520of%2520recording%250Aexpert%2520operations%252C%2520and%2520difficulties%2520in%2520designing%2520heuristic-based%2520goals%2520to%2520cover%250Aall%2520scenarios.%2520To%2520avoid%2520these%2520issues%252C%2520we%2520propose%2520a%2520scalable%2520method%2520that%250Ainvolves%2520collecting%2520real-world%2520training%2520data%2520to%2520improve%2520diversity%252C%2520extracting%250Acamera%2520trajectories%2520automatically%2520to%2520minimize%2520annotation%2520costs%252C%2520and%2520training%2520an%250Aeffective%2520architecture%2520that%2520does%2520not%2520rely%2520on%2520heuristics.%2520Specifically%252C%2520we%250Acollect%252099k%2520high-quality%2520trajectories%2520by%2520running%25203D%2520reconstruction%2520on%2520online%250Avideos%252C%2520connecting%2520camera%2520poses%2520from%2520consecutive%2520frames%2520to%2520formulate%25203D%2520camera%250Apaths%252C%2520and%2520using%2520Kalman%2520filter%2520to%2520identify%2520and%2520remove%2520low-quality%2520data.%250AMoreover%252C%2520we%2520introduce%2520DVGFormer%252C%2520an%2520auto-regressive%2520transformer%2520that%2520leverages%250Athe%2520camera%2520path%2520and%2520images%2520from%2520all%2520past%2520frames%2520to%2520predict%2520camera%2520movement%2520in%250Athe%2520next%2520frame.%2520We%2520evaluate%2520our%2520system%2520across%252038%2520synthetic%2520natural%2520scenes%2520and%25207%250Areal%2520city%25203D%2520scans.%2520We%2520show%2520that%2520our%2520system%2520effectively%2520learns%2520to%2520perform%250Achallenging%2520camera%2520movements%2520such%2520as%2520navigating%2520through%2520obstacles%252C%2520maintaining%250Alow%2520altitude%2520to%2520increase%2520perceived%2520speed%252C%2520and%2520orbiting%2520towers%2520and%2520buildings%252C%250Awhich%2520are%2520very%2520useful%2520for%2520recording%2520high-quality%2520videos.%2520Data%2520and%2520code%2520are%250Aavailable%2520at%2520dvgformer.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Camera%20Movement%20Control%20from%20Real-World%20Drone%20Videos&entry.906535625=Yunzhong%20Hou%20and%20Liang%20Zheng%20and%20Philip%20Torr&entry.1292438233=%20%20This%20study%20seeks%20to%20automate%20camera%20movement%20control%20for%20filming%20existing%0Asubjects%20into%20attractive%20videos%2C%20contrasting%20with%20the%20creation%20of%20non-existent%0Acontent%20by%20directly%20generating%20the%20pixels.%20We%20select%20drone%20videos%20as%20our%20test%0Acase%20due%20to%20their%20rich%20and%20challenging%20motion%20patterns%2C%20distinctive%20viewing%0Aangles%2C%20and%20precise%20controls.%20Existing%20AI%20videography%20methods%20struggle%20with%0Alimited%20appearance%20diversity%20in%20simulation%20training%2C%20high%20costs%20of%20recording%0Aexpert%20operations%2C%20and%20difficulties%20in%20designing%20heuristic-based%20goals%20to%20cover%0Aall%20scenarios.%20To%20avoid%20these%20issues%2C%20we%20propose%20a%20scalable%20method%20that%0Ainvolves%20collecting%20real-world%20training%20data%20to%20improve%20diversity%2C%20extracting%0Acamera%20trajectories%20automatically%20to%20minimize%20annotation%20costs%2C%20and%20training%20an%0Aeffective%20architecture%20that%20does%20not%20rely%20on%20heuristics.%20Specifically%2C%20we%0Acollect%2099k%20high-quality%20trajectories%20by%20running%203D%20reconstruction%20on%20online%0Avideos%2C%20connecting%20camera%20poses%20from%20consecutive%20frames%20to%20formulate%203D%20camera%0Apaths%2C%20and%20using%20Kalman%20filter%20to%20identify%20and%20remove%20low-quality%20data.%0AMoreover%2C%20we%20introduce%20DVGFormer%2C%20an%20auto-regressive%20transformer%20that%20leverages%0Athe%20camera%20path%20and%20images%20from%20all%20past%20frames%20to%20predict%20camera%20movement%20in%0Athe%20next%20frame.%20We%20evaluate%20our%20system%20across%2038%20synthetic%20natural%20scenes%20and%207%0Areal%20city%203D%20scans.%20We%20show%20that%20our%20system%20effectively%20learns%20to%20perform%0Achallenging%20camera%20movements%20such%20as%20navigating%20through%20obstacles%2C%20maintaining%0Alow%20altitude%20to%20increase%20perceived%20speed%2C%20and%20orbiting%20towers%20and%20buildings%2C%0Awhich%20are%20very%20useful%20for%20recording%20high-quality%20videos.%20Data%20and%20code%20are%0Aavailable%20at%20dvgformer.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09620v1&entry.124074799=Read"},
{"title": "Video Seal: Open and Efficient Video Watermarking", "author": "Pierre Fernandez and Hady Elsahar and I. Zeki Yalniz and Alexandre Mourachko", "abstract": "  The proliferation of AI-generated content and sophisticated video editing\ntools has made it both important and challenging to moderate digital platforms.\nVideo watermarking addresses these challenges by embedding imperceptible\nsignals into videos, allowing for identification. However, the rare open tools\nand methods often fall short on efficiency, robustness, and flexibility. To\nreduce these gaps, this paper introduces Video Seal, a comprehensive framework\nfor neural video watermarking and a competitive open-sourced model. Our\napproach jointly trains an embedder and an extractor, while ensuring the\nwatermark robustness by applying transformations in-between, e.g., video\ncodecs. This training is multistage and includes image pre-training, hybrid\npost-training and extractor fine-tuning. We also introduce temporal watermark\npropagation, a technique to convert any image watermarking model to an\nefficient video watermarking model without the need to watermark every\nhigh-resolution frame. We present experimental results demonstrating the\neffectiveness of the approach in terms of speed, imperceptibility, and\nrobustness. Video Seal achieves higher robustness compared to strong baselines\nespecially under challenging distortions combining geometric transformations\nand video compression. Additionally, we provide new insights such as the impact\nof video compression during training, and how to compare methods operating on\ndifferent payloads. Contributions in this work - including the codebase,\nmodels, and a public demo - are open-sourced under permissive licenses to\nfoster further research and development in the field.\n", "link": "http://arxiv.org/abs/2412.09492v1", "date": "2024-12-12", "relevancy": 2.2953, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5831}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5735}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Seal%3A%20Open%20and%20Efficient%20Video%20Watermarking&body=Title%3A%20Video%20Seal%3A%20Open%20and%20Efficient%20Video%20Watermarking%0AAuthor%3A%20Pierre%20Fernandez%20and%20Hady%20Elsahar%20and%20I.%20Zeki%20Yalniz%20and%20Alexandre%20Mourachko%0AAbstract%3A%20%20%20The%20proliferation%20of%20AI-generated%20content%20and%20sophisticated%20video%20editing%0Atools%20has%20made%20it%20both%20important%20and%20challenging%20to%20moderate%20digital%20platforms.%0AVideo%20watermarking%20addresses%20these%20challenges%20by%20embedding%20imperceptible%0Asignals%20into%20videos%2C%20allowing%20for%20identification.%20However%2C%20the%20rare%20open%20tools%0Aand%20methods%20often%20fall%20short%20on%20efficiency%2C%20robustness%2C%20and%20flexibility.%20To%0Areduce%20these%20gaps%2C%20this%20paper%20introduces%20Video%20Seal%2C%20a%20comprehensive%20framework%0Afor%20neural%20video%20watermarking%20and%20a%20competitive%20open-sourced%20model.%20Our%0Aapproach%20jointly%20trains%20an%20embedder%20and%20an%20extractor%2C%20while%20ensuring%20the%0Awatermark%20robustness%20by%20applying%20transformations%20in-between%2C%20e.g.%2C%20video%0Acodecs.%20This%20training%20is%20multistage%20and%20includes%20image%20pre-training%2C%20hybrid%0Apost-training%20and%20extractor%20fine-tuning.%20We%20also%20introduce%20temporal%20watermark%0Apropagation%2C%20a%20technique%20to%20convert%20any%20image%20watermarking%20model%20to%20an%0Aefficient%20video%20watermarking%20model%20without%20the%20need%20to%20watermark%20every%0Ahigh-resolution%20frame.%20We%20present%20experimental%20results%20demonstrating%20the%0Aeffectiveness%20of%20the%20approach%20in%20terms%20of%20speed%2C%20imperceptibility%2C%20and%0Arobustness.%20Video%20Seal%20achieves%20higher%20robustness%20compared%20to%20strong%20baselines%0Aespecially%20under%20challenging%20distortions%20combining%20geometric%20transformations%0Aand%20video%20compression.%20Additionally%2C%20we%20provide%20new%20insights%20such%20as%20the%20impact%0Aof%20video%20compression%20during%20training%2C%20and%20how%20to%20compare%20methods%20operating%20on%0Adifferent%20payloads.%20Contributions%20in%20this%20work%20-%20including%20the%20codebase%2C%0Amodels%2C%20and%20a%20public%20demo%20-%20are%20open-sourced%20under%20permissive%20licenses%20to%0Afoster%20further%20research%20and%20development%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Seal%253A%2520Open%2520and%2520Efficient%2520Video%2520Watermarking%26entry.906535625%3DPierre%2520Fernandez%2520and%2520Hady%2520Elsahar%2520and%2520I.%2520Zeki%2520Yalniz%2520and%2520Alexandre%2520Mourachko%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520AI-generated%2520content%2520and%2520sophisticated%2520video%2520editing%250Atools%2520has%2520made%2520it%2520both%2520important%2520and%2520challenging%2520to%2520moderate%2520digital%2520platforms.%250AVideo%2520watermarking%2520addresses%2520these%2520challenges%2520by%2520embedding%2520imperceptible%250Asignals%2520into%2520videos%252C%2520allowing%2520for%2520identification.%2520However%252C%2520the%2520rare%2520open%2520tools%250Aand%2520methods%2520often%2520fall%2520short%2520on%2520efficiency%252C%2520robustness%252C%2520and%2520flexibility.%2520To%250Areduce%2520these%2520gaps%252C%2520this%2520paper%2520introduces%2520Video%2520Seal%252C%2520a%2520comprehensive%2520framework%250Afor%2520neural%2520video%2520watermarking%2520and%2520a%2520competitive%2520open-sourced%2520model.%2520Our%250Aapproach%2520jointly%2520trains%2520an%2520embedder%2520and%2520an%2520extractor%252C%2520while%2520ensuring%2520the%250Awatermark%2520robustness%2520by%2520applying%2520transformations%2520in-between%252C%2520e.g.%252C%2520video%250Acodecs.%2520This%2520training%2520is%2520multistage%2520and%2520includes%2520image%2520pre-training%252C%2520hybrid%250Apost-training%2520and%2520extractor%2520fine-tuning.%2520We%2520also%2520introduce%2520temporal%2520watermark%250Apropagation%252C%2520a%2520technique%2520to%2520convert%2520any%2520image%2520watermarking%2520model%2520to%2520an%250Aefficient%2520video%2520watermarking%2520model%2520without%2520the%2520need%2520to%2520watermark%2520every%250Ahigh-resolution%2520frame.%2520We%2520present%2520experimental%2520results%2520demonstrating%2520the%250Aeffectiveness%2520of%2520the%2520approach%2520in%2520terms%2520of%2520speed%252C%2520imperceptibility%252C%2520and%250Arobustness.%2520Video%2520Seal%2520achieves%2520higher%2520robustness%2520compared%2520to%2520strong%2520baselines%250Aespecially%2520under%2520challenging%2520distortions%2520combining%2520geometric%2520transformations%250Aand%2520video%2520compression.%2520Additionally%252C%2520we%2520provide%2520new%2520insights%2520such%2520as%2520the%2520impact%250Aof%2520video%2520compression%2520during%2520training%252C%2520and%2520how%2520to%2520compare%2520methods%2520operating%2520on%250Adifferent%2520payloads.%2520Contributions%2520in%2520this%2520work%2520-%2520including%2520the%2520codebase%252C%250Amodels%252C%2520and%2520a%2520public%2520demo%2520-%2520are%2520open-sourced%2520under%2520permissive%2520licenses%2520to%250Afoster%2520further%2520research%2520and%2520development%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Seal%3A%20Open%20and%20Efficient%20Video%20Watermarking&entry.906535625=Pierre%20Fernandez%20and%20Hady%20Elsahar%20and%20I.%20Zeki%20Yalniz%20and%20Alexandre%20Mourachko&entry.1292438233=%20%20The%20proliferation%20of%20AI-generated%20content%20and%20sophisticated%20video%20editing%0Atools%20has%20made%20it%20both%20important%20and%20challenging%20to%20moderate%20digital%20platforms.%0AVideo%20watermarking%20addresses%20these%20challenges%20by%20embedding%20imperceptible%0Asignals%20into%20videos%2C%20allowing%20for%20identification.%20However%2C%20the%20rare%20open%20tools%0Aand%20methods%20often%20fall%20short%20on%20efficiency%2C%20robustness%2C%20and%20flexibility.%20To%0Areduce%20these%20gaps%2C%20this%20paper%20introduces%20Video%20Seal%2C%20a%20comprehensive%20framework%0Afor%20neural%20video%20watermarking%20and%20a%20competitive%20open-sourced%20model.%20Our%0Aapproach%20jointly%20trains%20an%20embedder%20and%20an%20extractor%2C%20while%20ensuring%20the%0Awatermark%20robustness%20by%20applying%20transformations%20in-between%2C%20e.g.%2C%20video%0Acodecs.%20This%20training%20is%20multistage%20and%20includes%20image%20pre-training%2C%20hybrid%0Apost-training%20and%20extractor%20fine-tuning.%20We%20also%20introduce%20temporal%20watermark%0Apropagation%2C%20a%20technique%20to%20convert%20any%20image%20watermarking%20model%20to%20an%0Aefficient%20video%20watermarking%20model%20without%20the%20need%20to%20watermark%20every%0Ahigh-resolution%20frame.%20We%20present%20experimental%20results%20demonstrating%20the%0Aeffectiveness%20of%20the%20approach%20in%20terms%20of%20speed%2C%20imperceptibility%2C%20and%0Arobustness.%20Video%20Seal%20achieves%20higher%20robustness%20compared%20to%20strong%20baselines%0Aespecially%20under%20challenging%20distortions%20combining%20geometric%20transformations%0Aand%20video%20compression.%20Additionally%2C%20we%20provide%20new%20insights%20such%20as%20the%20impact%0Aof%20video%20compression%20during%20training%2C%20and%20how%20to%20compare%20methods%20operating%20on%0Adifferent%20payloads.%20Contributions%20in%20this%20work%20-%20including%20the%20codebase%2C%0Amodels%2C%20and%20a%20public%20demo%20-%20are%20open-sourced%20under%20permissive%20licenses%20to%0Afoster%20further%20research%20and%20development%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09492v1&entry.124074799=Read"},
{"title": "UADet: A Remarkably Simple Yet Effective Uncertainty-Aware Open-Set\n  Object Detection Framework", "author": "Silin Cheng and Yuanpei Liu and Kai Han", "abstract": "  We tackle the challenging problem of Open-Set Object Detection (OSOD), which\naims to detect both known and unknown objects in unlabelled images. The main\ndifficulty arises from the absence of supervision for these unknown classes,\nmaking it challenging to distinguish them from the background. Existing OSOD\ndetectors either fail to properly exploit or inadequately leverage the abundant\nunlabeled unknown objects in training data, restricting their performance. To\naddress these limitations, we propose UADet, an Uncertainty-Aware Open-Set\nObject Detector that considers appearance and geometric uncertainty. By\nintegrating these uncertainty measures, UADet effectively reduces the number of\nunannotated instances incorrectly utilized or omitted by previous methods.\nExtensive experiments on OSOD benchmarks demonstrate that UADet substantially\noutperforms previous state-of-the-art (SOTA) methods in detecting both known\nand unknown objects, achieving a 1.8x improvement in unknown recall while\nmaintaining high performance on known classes. When extended to Open World\nObject Detection (OWOD), our method shows significant advantages over the\ncurrent SOTA method, with average improvements of 13.8% and 6.9% in unknown\nrecall on M-OWODB and S-OWODB benchmarks, respectively. Extensive results\nvalidate the effectiveness of our uncertainty-aware approach across different\nopen-set scenarios.\n", "link": "http://arxiv.org/abs/2412.09229v1", "date": "2024-12-12", "relevancy": 2.2824, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6538}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5899}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UADet%3A%20A%20Remarkably%20Simple%20Yet%20Effective%20Uncertainty-Aware%20Open-Set%0A%20%20Object%20Detection%20Framework&body=Title%3A%20UADet%3A%20A%20Remarkably%20Simple%20Yet%20Effective%20Uncertainty-Aware%20Open-Set%0A%20%20Object%20Detection%20Framework%0AAuthor%3A%20Silin%20Cheng%20and%20Yuanpei%20Liu%20and%20Kai%20Han%0AAbstract%3A%20%20%20We%20tackle%20the%20challenging%20problem%20of%20Open-Set%20Object%20Detection%20%28OSOD%29%2C%20which%0Aaims%20to%20detect%20both%20known%20and%20unknown%20objects%20in%20unlabelled%20images.%20The%20main%0Adifficulty%20arises%20from%20the%20absence%20of%20supervision%20for%20these%20unknown%20classes%2C%0Amaking%20it%20challenging%20to%20distinguish%20them%20from%20the%20background.%20Existing%20OSOD%0Adetectors%20either%20fail%20to%20properly%20exploit%20or%20inadequately%20leverage%20the%20abundant%0Aunlabeled%20unknown%20objects%20in%20training%20data%2C%20restricting%20their%20performance.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20UADet%2C%20an%20Uncertainty-Aware%20Open-Set%0AObject%20Detector%20that%20considers%20appearance%20and%20geometric%20uncertainty.%20By%0Aintegrating%20these%20uncertainty%20measures%2C%20UADet%20effectively%20reduces%20the%20number%20of%0Aunannotated%20instances%20incorrectly%20utilized%20or%20omitted%20by%20previous%20methods.%0AExtensive%20experiments%20on%20OSOD%20benchmarks%20demonstrate%20that%20UADet%20substantially%0Aoutperforms%20previous%20state-of-the-art%20%28SOTA%29%20methods%20in%20detecting%20both%20known%0Aand%20unknown%20objects%2C%20achieving%20a%201.8x%20improvement%20in%20unknown%20recall%20while%0Amaintaining%20high%20performance%20on%20known%20classes.%20When%20extended%20to%20Open%20World%0AObject%20Detection%20%28OWOD%29%2C%20our%20method%20shows%20significant%20advantages%20over%20the%0Acurrent%20SOTA%20method%2C%20with%20average%20improvements%20of%2013.8%25%20and%206.9%25%20in%20unknown%0Arecall%20on%20M-OWODB%20and%20S-OWODB%20benchmarks%2C%20respectively.%20Extensive%20results%0Avalidate%20the%20effectiveness%20of%20our%20uncertainty-aware%20approach%20across%20different%0Aopen-set%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUADet%253A%2520A%2520Remarkably%2520Simple%2520Yet%2520Effective%2520Uncertainty-Aware%2520Open-Set%250A%2520%2520Object%2520Detection%2520Framework%26entry.906535625%3DSilin%2520Cheng%2520and%2520Yuanpei%2520Liu%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520challenging%2520problem%2520of%2520Open-Set%2520Object%2520Detection%2520%2528OSOD%2529%252C%2520which%250Aaims%2520to%2520detect%2520both%2520known%2520and%2520unknown%2520objects%2520in%2520unlabelled%2520images.%2520The%2520main%250Adifficulty%2520arises%2520from%2520the%2520absence%2520of%2520supervision%2520for%2520these%2520unknown%2520classes%252C%250Amaking%2520it%2520challenging%2520to%2520distinguish%2520them%2520from%2520the%2520background.%2520Existing%2520OSOD%250Adetectors%2520either%2520fail%2520to%2520properly%2520exploit%2520or%2520inadequately%2520leverage%2520the%2520abundant%250Aunlabeled%2520unknown%2520objects%2520in%2520training%2520data%252C%2520restricting%2520their%2520performance.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520UADet%252C%2520an%2520Uncertainty-Aware%2520Open-Set%250AObject%2520Detector%2520that%2520considers%2520appearance%2520and%2520geometric%2520uncertainty.%2520By%250Aintegrating%2520these%2520uncertainty%2520measures%252C%2520UADet%2520effectively%2520reduces%2520the%2520number%2520of%250Aunannotated%2520instances%2520incorrectly%2520utilized%2520or%2520omitted%2520by%2520previous%2520methods.%250AExtensive%2520experiments%2520on%2520OSOD%2520benchmarks%2520demonstrate%2520that%2520UADet%2520substantially%250Aoutperforms%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520in%2520detecting%2520both%2520known%250Aand%2520unknown%2520objects%252C%2520achieving%2520a%25201.8x%2520improvement%2520in%2520unknown%2520recall%2520while%250Amaintaining%2520high%2520performance%2520on%2520known%2520classes.%2520When%2520extended%2520to%2520Open%2520World%250AObject%2520Detection%2520%2528OWOD%2529%252C%2520our%2520method%2520shows%2520significant%2520advantages%2520over%2520the%250Acurrent%2520SOTA%2520method%252C%2520with%2520average%2520improvements%2520of%252013.8%2525%2520and%25206.9%2525%2520in%2520unknown%250Arecall%2520on%2520M-OWODB%2520and%2520S-OWODB%2520benchmarks%252C%2520respectively.%2520Extensive%2520results%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520uncertainty-aware%2520approach%2520across%2520different%250Aopen-set%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UADet%3A%20A%20Remarkably%20Simple%20Yet%20Effective%20Uncertainty-Aware%20Open-Set%0A%20%20Object%20Detection%20Framework&entry.906535625=Silin%20Cheng%20and%20Yuanpei%20Liu%20and%20Kai%20Han&entry.1292438233=%20%20We%20tackle%20the%20challenging%20problem%20of%20Open-Set%20Object%20Detection%20%28OSOD%29%2C%20which%0Aaims%20to%20detect%20both%20known%20and%20unknown%20objects%20in%20unlabelled%20images.%20The%20main%0Adifficulty%20arises%20from%20the%20absence%20of%20supervision%20for%20these%20unknown%20classes%2C%0Amaking%20it%20challenging%20to%20distinguish%20them%20from%20the%20background.%20Existing%20OSOD%0Adetectors%20either%20fail%20to%20properly%20exploit%20or%20inadequately%20leverage%20the%20abundant%0Aunlabeled%20unknown%20objects%20in%20training%20data%2C%20restricting%20their%20performance.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20UADet%2C%20an%20Uncertainty-Aware%20Open-Set%0AObject%20Detector%20that%20considers%20appearance%20and%20geometric%20uncertainty.%20By%0Aintegrating%20these%20uncertainty%20measures%2C%20UADet%20effectively%20reduces%20the%20number%20of%0Aunannotated%20instances%20incorrectly%20utilized%20or%20omitted%20by%20previous%20methods.%0AExtensive%20experiments%20on%20OSOD%20benchmarks%20demonstrate%20that%20UADet%20substantially%0Aoutperforms%20previous%20state-of-the-art%20%28SOTA%29%20methods%20in%20detecting%20both%20known%0Aand%20unknown%20objects%2C%20achieving%20a%201.8x%20improvement%20in%20unknown%20recall%20while%0Amaintaining%20high%20performance%20on%20known%20classes.%20When%20extended%20to%20Open%20World%0AObject%20Detection%20%28OWOD%29%2C%20our%20method%20shows%20significant%20advantages%20over%20the%0Acurrent%20SOTA%20method%2C%20with%20average%20improvements%20of%2013.8%25%20and%206.9%25%20in%20unknown%0Arecall%20on%20M-OWODB%20and%20S-OWODB%20benchmarks%2C%20respectively.%20Extensive%20results%0Avalidate%20the%20effectiveness%20of%20our%20uncertainty-aware%20approach%20across%20different%0Aopen-set%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09229v1&entry.124074799=Read"},
{"title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware\n  Structured Caption", "author": "Tiehan Fan and Kepan Nan and Rui Xie and Penghao Zhou and Zhenheng Yang and Chaoyou Fu and Xiang Li and Jian Yang and Ying Tai", "abstract": "  Text-to-video generation has evolved rapidly in recent years, delivering\nremarkable results. Training typically relies on video-caption paired data,\nwhich plays a crucial role in enhancing generation performance. However,\ncurrent video captions often suffer from insufficient details, hallucinations\nand imprecise motion depiction, affecting the fidelity and consistency of\ngenerated videos. In this work, we propose a novel instance-aware structured\ncaption framework, termed InstanceCap, to achieve instance-level and\nfine-grained video caption for the first time. Based on this scheme, we design\nan auxiliary models cluster to convert original video into instances to enhance\ninstance fidelity. Video instances are further used to refine dense prompts\ninto structured phrases, achieving concise yet precise descriptions.\nFurthermore, a 22K InstanceVid dataset is curated for training, and an\nenhancement pipeline that tailored to InstanceCap structure is proposed for\ninference. Experimental results demonstrate that our proposed InstanceCap\nsignificantly outperform previous models, ensuring high fidelity between\ncaptions and videos while reducing hallucinations.\n", "link": "http://arxiv.org/abs/2412.09283v1", "date": "2024-12-12", "relevancy": 2.2709, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6019}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5832}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstanceCap%3A%20Improving%20Text-to-Video%20Generation%20via%20Instance-aware%0A%20%20Structured%20Caption&body=Title%3A%20InstanceCap%3A%20Improving%20Text-to-Video%20Generation%20via%20Instance-aware%0A%20%20Structured%20Caption%0AAuthor%3A%20Tiehan%20Fan%20and%20Kepan%20Nan%20and%20Rui%20Xie%20and%20Penghao%20Zhou%20and%20Zhenheng%20Yang%20and%20Chaoyou%20Fu%20and%20Xiang%20Li%20and%20Jian%20Yang%20and%20Ying%20Tai%0AAbstract%3A%20%20%20Text-to-video%20generation%20has%20evolved%20rapidly%20in%20recent%20years%2C%20delivering%0Aremarkable%20results.%20Training%20typically%20relies%20on%20video-caption%20paired%20data%2C%0Awhich%20plays%20a%20crucial%20role%20in%20enhancing%20generation%20performance.%20However%2C%0Acurrent%20video%20captions%20often%20suffer%20from%20insufficient%20details%2C%20hallucinations%0Aand%20imprecise%20motion%20depiction%2C%20affecting%20the%20fidelity%20and%20consistency%20of%0Agenerated%20videos.%20In%20this%20work%2C%20we%20propose%20a%20novel%20instance-aware%20structured%0Acaption%20framework%2C%20termed%20InstanceCap%2C%20to%20achieve%20instance-level%20and%0Afine-grained%20video%20caption%20for%20the%20first%20time.%20Based%20on%20this%20scheme%2C%20we%20design%0Aan%20auxiliary%20models%20cluster%20to%20convert%20original%20video%20into%20instances%20to%20enhance%0Ainstance%20fidelity.%20Video%20instances%20are%20further%20used%20to%20refine%20dense%20prompts%0Ainto%20structured%20phrases%2C%20achieving%20concise%20yet%20precise%20descriptions.%0AFurthermore%2C%20a%2022K%20InstanceVid%20dataset%20is%20curated%20for%20training%2C%20and%20an%0Aenhancement%20pipeline%20that%20tailored%20to%20InstanceCap%20structure%20is%20proposed%20for%0Ainference.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20InstanceCap%0Asignificantly%20outperform%20previous%20models%2C%20ensuring%20high%20fidelity%20between%0Acaptions%20and%20videos%20while%20reducing%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstanceCap%253A%2520Improving%2520Text-to-Video%2520Generation%2520via%2520Instance-aware%250A%2520%2520Structured%2520Caption%26entry.906535625%3DTiehan%2520Fan%2520and%2520Kepan%2520Nan%2520and%2520Rui%2520Xie%2520and%2520Penghao%2520Zhou%2520and%2520Zhenheng%2520Yang%2520and%2520Chaoyou%2520Fu%2520and%2520Xiang%2520Li%2520and%2520Jian%2520Yang%2520and%2520Ying%2520Tai%26entry.1292438233%3D%2520%2520Text-to-video%2520generation%2520has%2520evolved%2520rapidly%2520in%2520recent%2520years%252C%2520delivering%250Aremarkable%2520results.%2520Training%2520typically%2520relies%2520on%2520video-caption%2520paired%2520data%252C%250Awhich%2520plays%2520a%2520crucial%2520role%2520in%2520enhancing%2520generation%2520performance.%2520However%252C%250Acurrent%2520video%2520captions%2520often%2520suffer%2520from%2520insufficient%2520details%252C%2520hallucinations%250Aand%2520imprecise%2520motion%2520depiction%252C%2520affecting%2520the%2520fidelity%2520and%2520consistency%2520of%250Agenerated%2520videos.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520instance-aware%2520structured%250Acaption%2520framework%252C%2520termed%2520InstanceCap%252C%2520to%2520achieve%2520instance-level%2520and%250Afine-grained%2520video%2520caption%2520for%2520the%2520first%2520time.%2520Based%2520on%2520this%2520scheme%252C%2520we%2520design%250Aan%2520auxiliary%2520models%2520cluster%2520to%2520convert%2520original%2520video%2520into%2520instances%2520to%2520enhance%250Ainstance%2520fidelity.%2520Video%2520instances%2520are%2520further%2520used%2520to%2520refine%2520dense%2520prompts%250Ainto%2520structured%2520phrases%252C%2520achieving%2520concise%2520yet%2520precise%2520descriptions.%250AFurthermore%252C%2520a%252022K%2520InstanceVid%2520dataset%2520is%2520curated%2520for%2520training%252C%2520and%2520an%250Aenhancement%2520pipeline%2520that%2520tailored%2520to%2520InstanceCap%2520structure%2520is%2520proposed%2520for%250Ainference.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520InstanceCap%250Asignificantly%2520outperform%2520previous%2520models%252C%2520ensuring%2520high%2520fidelity%2520between%250Acaptions%2520and%2520videos%2520while%2520reducing%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstanceCap%3A%20Improving%20Text-to-Video%20Generation%20via%20Instance-aware%0A%20%20Structured%20Caption&entry.906535625=Tiehan%20Fan%20and%20Kepan%20Nan%20and%20Rui%20Xie%20and%20Penghao%20Zhou%20and%20Zhenheng%20Yang%20and%20Chaoyou%20Fu%20and%20Xiang%20Li%20and%20Jian%20Yang%20and%20Ying%20Tai&entry.1292438233=%20%20Text-to-video%20generation%20has%20evolved%20rapidly%20in%20recent%20years%2C%20delivering%0Aremarkable%20results.%20Training%20typically%20relies%20on%20video-caption%20paired%20data%2C%0Awhich%20plays%20a%20crucial%20role%20in%20enhancing%20generation%20performance.%20However%2C%0Acurrent%20video%20captions%20often%20suffer%20from%20insufficient%20details%2C%20hallucinations%0Aand%20imprecise%20motion%20depiction%2C%20affecting%20the%20fidelity%20and%20consistency%20of%0Agenerated%20videos.%20In%20this%20work%2C%20we%20propose%20a%20novel%20instance-aware%20structured%0Acaption%20framework%2C%20termed%20InstanceCap%2C%20to%20achieve%20instance-level%20and%0Afine-grained%20video%20caption%20for%20the%20first%20time.%20Based%20on%20this%20scheme%2C%20we%20design%0Aan%20auxiliary%20models%20cluster%20to%20convert%20original%20video%20into%20instances%20to%20enhance%0Ainstance%20fidelity.%20Video%20instances%20are%20further%20used%20to%20refine%20dense%20prompts%0Ainto%20structured%20phrases%2C%20achieving%20concise%20yet%20precise%20descriptions.%0AFurthermore%2C%20a%2022K%20InstanceVid%20dataset%20is%20curated%20for%20training%2C%20and%20an%0Aenhancement%20pipeline%20that%20tailored%20to%20InstanceCap%20structure%20is%20proposed%20for%0Ainference.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20InstanceCap%0Asignificantly%20outperform%20previous%20models%2C%20ensuring%20high%20fidelity%20between%0Acaptions%20and%20videos%20while%20reducing%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09283v1&entry.124074799=Read"},
{"title": "A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony\n  in Talking Head Generation", "author": "Louis Airale and Dominique Vaufreydaz and Xavier Alameda-Pineda", "abstract": "  Animating still face images with deep generative models using a speech input\nsignal is an active research topic and has seen important recent\nprogress.However, much of the effort has been put into lip syncing and\nrendering quality while the generation of natural head motion, let alone the\naudio-visual correlation between head motion and speech, has often been\nneglected.In this work, we propose a multi-scale audio-visual synchrony loss\nand a multi-scale autoregressive GAN to better handle short and long-term\ncorrelation between speech and the dynamics of the head and lips.In particular,\nwe train a stack of syncer models on multimodal input pyramids and use these\nmodels as guidance in a multi-scale generator network to produce audio-aligned\nmotion unfolding over diverse time scales.Both the pyramid of audio-visual\nsyncers and the generative models are trained in a low-dimensional space that\nfully preserves dynamics cues.The experiments show significant improvements\nover the state-of-the-art in head motion dynamics quality and especially in\nmulti-scale audio-visual synchrony on a collection of benchmark datasets.\n", "link": "http://arxiv.org/abs/2307.03270v2", "date": "2024-12-12", "relevancy": 2.2571, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5666}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5638}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Multi-scale%20Approach%20for%20Speech%20and%20Dynamics%20Synchrony%0A%20%20in%20Talking%20Head%20Generation&body=Title%3A%20A%20Comprehensive%20Multi-scale%20Approach%20for%20Speech%20and%20Dynamics%20Synchrony%0A%20%20in%20Talking%20Head%20Generation%0AAuthor%3A%20Louis%20Airale%20and%20Dominique%20Vaufreydaz%20and%20Xavier%20Alameda-Pineda%0AAbstract%3A%20%20%20Animating%20still%20face%20images%20with%20deep%20generative%20models%20using%20a%20speech%20input%0Asignal%20is%20an%20active%20research%20topic%20and%20has%20seen%20important%20recent%0Aprogress.However%2C%20much%20of%20the%20effort%20has%20been%20put%20into%20lip%20syncing%20and%0Arendering%20quality%20while%20the%20generation%20of%20natural%20head%20motion%2C%20let%20alone%20the%0Aaudio-visual%20correlation%20between%20head%20motion%20and%20speech%2C%20has%20often%20been%0Aneglected.In%20this%20work%2C%20we%20propose%20a%20multi-scale%20audio-visual%20synchrony%20loss%0Aand%20a%20multi-scale%20autoregressive%20GAN%20to%20better%20handle%20short%20and%20long-term%0Acorrelation%20between%20speech%20and%20the%20dynamics%20of%20the%20head%20and%20lips.In%20particular%2C%0Awe%20train%20a%20stack%20of%20syncer%20models%20on%20multimodal%20input%20pyramids%20and%20use%20these%0Amodels%20as%20guidance%20in%20a%20multi-scale%20generator%20network%20to%20produce%20audio-aligned%0Amotion%20unfolding%20over%20diverse%20time%20scales.Both%20the%20pyramid%20of%20audio-visual%0Asyncers%20and%20the%20generative%20models%20are%20trained%20in%20a%20low-dimensional%20space%20that%0Afully%20preserves%20dynamics%20cues.The%20experiments%20show%20significant%20improvements%0Aover%20the%20state-of-the-art%20in%20head%20motion%20dynamics%20quality%20and%20especially%20in%0Amulti-scale%20audio-visual%20synchrony%20on%20a%20collection%20of%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.03270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Multi-scale%2520Approach%2520for%2520Speech%2520and%2520Dynamics%2520Synchrony%250A%2520%2520in%2520Talking%2520Head%2520Generation%26entry.906535625%3DLouis%2520Airale%2520and%2520Dominique%2520Vaufreydaz%2520and%2520Xavier%2520Alameda-Pineda%26entry.1292438233%3D%2520%2520Animating%2520still%2520face%2520images%2520with%2520deep%2520generative%2520models%2520using%2520a%2520speech%2520input%250Asignal%2520is%2520an%2520active%2520research%2520topic%2520and%2520has%2520seen%2520important%2520recent%250Aprogress.However%252C%2520much%2520of%2520the%2520effort%2520has%2520been%2520put%2520into%2520lip%2520syncing%2520and%250Arendering%2520quality%2520while%2520the%2520generation%2520of%2520natural%2520head%2520motion%252C%2520let%2520alone%2520the%250Aaudio-visual%2520correlation%2520between%2520head%2520motion%2520and%2520speech%252C%2520has%2520often%2520been%250Aneglected.In%2520this%2520work%252C%2520we%2520propose%2520a%2520multi-scale%2520audio-visual%2520synchrony%2520loss%250Aand%2520a%2520multi-scale%2520autoregressive%2520GAN%2520to%2520better%2520handle%2520short%2520and%2520long-term%250Acorrelation%2520between%2520speech%2520and%2520the%2520dynamics%2520of%2520the%2520head%2520and%2520lips.In%2520particular%252C%250Awe%2520train%2520a%2520stack%2520of%2520syncer%2520models%2520on%2520multimodal%2520input%2520pyramids%2520and%2520use%2520these%250Amodels%2520as%2520guidance%2520in%2520a%2520multi-scale%2520generator%2520network%2520to%2520produce%2520audio-aligned%250Amotion%2520unfolding%2520over%2520diverse%2520time%2520scales.Both%2520the%2520pyramid%2520of%2520audio-visual%250Asyncers%2520and%2520the%2520generative%2520models%2520are%2520trained%2520in%2520a%2520low-dimensional%2520space%2520that%250Afully%2520preserves%2520dynamics%2520cues.The%2520experiments%2520show%2520significant%2520improvements%250Aover%2520the%2520state-of-the-art%2520in%2520head%2520motion%2520dynamics%2520quality%2520and%2520especially%2520in%250Amulti-scale%2520audio-visual%2520synchrony%2520on%2520a%2520collection%2520of%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.03270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Multi-scale%20Approach%20for%20Speech%20and%20Dynamics%20Synchrony%0A%20%20in%20Talking%20Head%20Generation&entry.906535625=Louis%20Airale%20and%20Dominique%20Vaufreydaz%20and%20Xavier%20Alameda-Pineda&entry.1292438233=%20%20Animating%20still%20face%20images%20with%20deep%20generative%20models%20using%20a%20speech%20input%0Asignal%20is%20an%20active%20research%20topic%20and%20has%20seen%20important%20recent%0Aprogress.However%2C%20much%20of%20the%20effort%20has%20been%20put%20into%20lip%20syncing%20and%0Arendering%20quality%20while%20the%20generation%20of%20natural%20head%20motion%2C%20let%20alone%20the%0Aaudio-visual%20correlation%20between%20head%20motion%20and%20speech%2C%20has%20often%20been%0Aneglected.In%20this%20work%2C%20we%20propose%20a%20multi-scale%20audio-visual%20synchrony%20loss%0Aand%20a%20multi-scale%20autoregressive%20GAN%20to%20better%20handle%20short%20and%20long-term%0Acorrelation%20between%20speech%20and%20the%20dynamics%20of%20the%20head%20and%20lips.In%20particular%2C%0Awe%20train%20a%20stack%20of%20syncer%20models%20on%20multimodal%20input%20pyramids%20and%20use%20these%0Amodels%20as%20guidance%20in%20a%20multi-scale%20generator%20network%20to%20produce%20audio-aligned%0Amotion%20unfolding%20over%20diverse%20time%20scales.Both%20the%20pyramid%20of%20audio-visual%0Asyncers%20and%20the%20generative%20models%20are%20trained%20in%20a%20low-dimensional%20space%20that%0Afully%20preserves%20dynamics%20cues.The%20experiments%20show%20significant%20improvements%0Aover%20the%20state-of-the-art%20in%20head%20motion%20dynamics%20quality%20and%20especially%20in%0Amulti-scale%20audio-visual%20synchrony%20on%20a%20collection%20of%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.03270v2&entry.124074799=Read"},
{"title": "Neptune: The Long Orbit to Benchmarking Long Video Understanding", "author": "Arsha Nagrani and Mingda Zhang and Ramin Mehran and Rachel Hornung and Nitesh Bharadwaj Gundavarapu and Nilpa Jha and Austin Myers and Xingyi Zhou and Boqing Gong and Cordelia Schmid and Mikhail Sirotenko and Yukun Zhu and Tobias Weyand", "abstract": "  This paper describes a semi-automatic pipeline to generate challenging\nquestion-answer-decoy sets for understanding long videos. Many existing video\ndatasets and models are focused on short clips (10s-30s). While some long video\ndatasets do exist, they can often be solved by powerful image models applied\nper frame (and often to very few frames) in a video, and are usually manually\nannotated at high cost. In order to mitigate both these problems, we propose a\nscalable dataset creation pipeline which leverages large models (VLMs and\nLLMs), to automatically generate dense, time-aligned video captions, as well as\ntough question answer decoy sets for video segments (up to 15 minutes in\nlength). Our dataset Neptune covers a broad range of long video reasoning\nabilities and consists of a subset that emphasizes multimodal reasoning. Since\nexisting metrics for open-ended question answering are either rule-based or may\nrely on proprietary models, we provide a new open source model-based metric GEM\nto score open-ended responses on Neptune. Benchmark evaluations reveal that\nmost current open-source long video models perform poorly on Neptune,\nparticularly on questions testing temporal ordering, counting and state\nchanges. Through Neptune, we aim to spur the development of more advanced\nmodels capable of understanding long videos. The dataset is available at\nhttps://github.com/google-deepmind/neptune\n", "link": "http://arxiv.org/abs/2412.09582v1", "date": "2024-12-12", "relevancy": 2.2302, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neptune%3A%20The%20Long%20Orbit%20to%20Benchmarking%20Long%20Video%20Understanding&body=Title%3A%20Neptune%3A%20The%20Long%20Orbit%20to%20Benchmarking%20Long%20Video%20Understanding%0AAuthor%3A%20Arsha%20Nagrani%20and%20Mingda%20Zhang%20and%20Ramin%20Mehran%20and%20Rachel%20Hornung%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Nilpa%20Jha%20and%20Austin%20Myers%20and%20Xingyi%20Zhou%20and%20Boqing%20Gong%20and%20Cordelia%20Schmid%20and%20Mikhail%20Sirotenko%20and%20Yukun%20Zhu%20and%20Tobias%20Weyand%0AAbstract%3A%20%20%20This%20paper%20describes%20a%20semi-automatic%20pipeline%20to%20generate%20challenging%0Aquestion-answer-decoy%20sets%20for%20understanding%20long%20videos.%20Many%20existing%20video%0Adatasets%20and%20models%20are%20focused%20on%20short%20clips%20%2810s-30s%29.%20While%20some%20long%20video%0Adatasets%20do%20exist%2C%20they%20can%20often%20be%20solved%20by%20powerful%20image%20models%20applied%0Aper%20frame%20%28and%20often%20to%20very%20few%20frames%29%20in%20a%20video%2C%20and%20are%20usually%20manually%0Aannotated%20at%20high%20cost.%20In%20order%20to%20mitigate%20both%20these%20problems%2C%20we%20propose%20a%0Ascalable%20dataset%20creation%20pipeline%20which%20leverages%20large%20models%20%28VLMs%20and%0ALLMs%29%2C%20to%20automatically%20generate%20dense%2C%20time-aligned%20video%20captions%2C%20as%20well%20as%0Atough%20question%20answer%20decoy%20sets%20for%20video%20segments%20%28up%20to%2015%20minutes%20in%0Alength%29.%20Our%20dataset%20Neptune%20covers%20a%20broad%20range%20of%20long%20video%20reasoning%0Aabilities%20and%20consists%20of%20a%20subset%20that%20emphasizes%20multimodal%20reasoning.%20Since%0Aexisting%20metrics%20for%20open-ended%20question%20answering%20are%20either%20rule-based%20or%20may%0Arely%20on%20proprietary%20models%2C%20we%20provide%20a%20new%20open%20source%20model-based%20metric%20GEM%0Ato%20score%20open-ended%20responses%20on%20Neptune.%20Benchmark%20evaluations%20reveal%20that%0Amost%20current%20open-source%20long%20video%20models%20perform%20poorly%20on%20Neptune%2C%0Aparticularly%20on%20questions%20testing%20temporal%20ordering%2C%20counting%20and%20state%0Achanges.%20Through%20Neptune%2C%20we%20aim%20to%20spur%20the%20development%20of%20more%20advanced%0Amodels%20capable%20of%20understanding%20long%20videos.%20The%20dataset%20is%20available%20at%0Ahttps%3A//github.com/google-deepmind/neptune%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeptune%253A%2520The%2520Long%2520Orbit%2520to%2520Benchmarking%2520Long%2520Video%2520Understanding%26entry.906535625%3DArsha%2520Nagrani%2520and%2520Mingda%2520Zhang%2520and%2520Ramin%2520Mehran%2520and%2520Rachel%2520Hornung%2520and%2520Nitesh%2520Bharadwaj%2520Gundavarapu%2520and%2520Nilpa%2520Jha%2520and%2520Austin%2520Myers%2520and%2520Xingyi%2520Zhou%2520and%2520Boqing%2520Gong%2520and%2520Cordelia%2520Schmid%2520and%2520Mikhail%2520Sirotenko%2520and%2520Yukun%2520Zhu%2520and%2520Tobias%2520Weyand%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520a%2520semi-automatic%2520pipeline%2520to%2520generate%2520challenging%250Aquestion-answer-decoy%2520sets%2520for%2520understanding%2520long%2520videos.%2520Many%2520existing%2520video%250Adatasets%2520and%2520models%2520are%2520focused%2520on%2520short%2520clips%2520%252810s-30s%2529.%2520While%2520some%2520long%2520video%250Adatasets%2520do%2520exist%252C%2520they%2520can%2520often%2520be%2520solved%2520by%2520powerful%2520image%2520models%2520applied%250Aper%2520frame%2520%2528and%2520often%2520to%2520very%2520few%2520frames%2529%2520in%2520a%2520video%252C%2520and%2520are%2520usually%2520manually%250Aannotated%2520at%2520high%2520cost.%2520In%2520order%2520to%2520mitigate%2520both%2520these%2520problems%252C%2520we%2520propose%2520a%250Ascalable%2520dataset%2520creation%2520pipeline%2520which%2520leverages%2520large%2520models%2520%2528VLMs%2520and%250ALLMs%2529%252C%2520to%2520automatically%2520generate%2520dense%252C%2520time-aligned%2520video%2520captions%252C%2520as%2520well%2520as%250Atough%2520question%2520answer%2520decoy%2520sets%2520for%2520video%2520segments%2520%2528up%2520to%252015%2520minutes%2520in%250Alength%2529.%2520Our%2520dataset%2520Neptune%2520covers%2520a%2520broad%2520range%2520of%2520long%2520video%2520reasoning%250Aabilities%2520and%2520consists%2520of%2520a%2520subset%2520that%2520emphasizes%2520multimodal%2520reasoning.%2520Since%250Aexisting%2520metrics%2520for%2520open-ended%2520question%2520answering%2520are%2520either%2520rule-based%2520or%2520may%250Arely%2520on%2520proprietary%2520models%252C%2520we%2520provide%2520a%2520new%2520open%2520source%2520model-based%2520metric%2520GEM%250Ato%2520score%2520open-ended%2520responses%2520on%2520Neptune.%2520Benchmark%2520evaluations%2520reveal%2520that%250Amost%2520current%2520open-source%2520long%2520video%2520models%2520perform%2520poorly%2520on%2520Neptune%252C%250Aparticularly%2520on%2520questions%2520testing%2520temporal%2520ordering%252C%2520counting%2520and%2520state%250Achanges.%2520Through%2520Neptune%252C%2520we%2520aim%2520to%2520spur%2520the%2520development%2520of%2520more%2520advanced%250Amodels%2520capable%2520of%2520understanding%2520long%2520videos.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//github.com/google-deepmind/neptune%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neptune%3A%20The%20Long%20Orbit%20to%20Benchmarking%20Long%20Video%20Understanding&entry.906535625=Arsha%20Nagrani%20and%20Mingda%20Zhang%20and%20Ramin%20Mehran%20and%20Rachel%20Hornung%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Nilpa%20Jha%20and%20Austin%20Myers%20and%20Xingyi%20Zhou%20and%20Boqing%20Gong%20and%20Cordelia%20Schmid%20and%20Mikhail%20Sirotenko%20and%20Yukun%20Zhu%20and%20Tobias%20Weyand&entry.1292438233=%20%20This%20paper%20describes%20a%20semi-automatic%20pipeline%20to%20generate%20challenging%0Aquestion-answer-decoy%20sets%20for%20understanding%20long%20videos.%20Many%20existing%20video%0Adatasets%20and%20models%20are%20focused%20on%20short%20clips%20%2810s-30s%29.%20While%20some%20long%20video%0Adatasets%20do%20exist%2C%20they%20can%20often%20be%20solved%20by%20powerful%20image%20models%20applied%0Aper%20frame%20%28and%20often%20to%20very%20few%20frames%29%20in%20a%20video%2C%20and%20are%20usually%20manually%0Aannotated%20at%20high%20cost.%20In%20order%20to%20mitigate%20both%20these%20problems%2C%20we%20propose%20a%0Ascalable%20dataset%20creation%20pipeline%20which%20leverages%20large%20models%20%28VLMs%20and%0ALLMs%29%2C%20to%20automatically%20generate%20dense%2C%20time-aligned%20video%20captions%2C%20as%20well%20as%0Atough%20question%20answer%20decoy%20sets%20for%20video%20segments%20%28up%20to%2015%20minutes%20in%0Alength%29.%20Our%20dataset%20Neptune%20covers%20a%20broad%20range%20of%20long%20video%20reasoning%0Aabilities%20and%20consists%20of%20a%20subset%20that%20emphasizes%20multimodal%20reasoning.%20Since%0Aexisting%20metrics%20for%20open-ended%20question%20answering%20are%20either%20rule-based%20or%20may%0Arely%20on%20proprietary%20models%2C%20we%20provide%20a%20new%20open%20source%20model-based%20metric%20GEM%0Ato%20score%20open-ended%20responses%20on%20Neptune.%20Benchmark%20evaluations%20reveal%20that%0Amost%20current%20open-source%20long%20video%20models%20perform%20poorly%20on%20Neptune%2C%0Aparticularly%20on%20questions%20testing%20temporal%20ordering%2C%20counting%20and%20state%0Achanges.%20Through%20Neptune%2C%20we%20aim%20to%20spur%20the%20development%20of%20more%20advanced%0Amodels%20capable%20of%20understanding%20long%20videos.%20The%20dataset%20is%20available%20at%0Ahttps%3A//github.com/google-deepmind/neptune%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09582v1&entry.124074799=Read"},
{"title": "Liquid: Language Models are Scalable Multi-modal Generators", "author": "Junfeng Wu and Yi Jiang and Chuofan Ma and Yuliang Liu and Hengshuang Zhao and Zehuan Yuan and Song Bai and Xiang Bai", "abstract": "  We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid.\n", "link": "http://arxiv.org/abs/2412.04332v2", "date": "2024-12-12", "relevancy": 2.2251, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5753}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Liquid%3A%20Language%20Models%20are%20Scalable%20Multi-modal%20Generators&body=Title%3A%20Liquid%3A%20Language%20Models%20are%20Scalable%20Multi-modal%20Generators%0AAuthor%3A%20Junfeng%20Wu%20and%20Yi%20Jiang%20and%20Chuofan%20Ma%20and%20Yuliang%20Liu%20and%20Hengshuang%20Zhao%20and%20Zehuan%20Yuan%20and%20Song%20Bai%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20We%20present%20Liquid%2C%20an%20auto-regressive%20generation%20paradigm%20that%20seamlessly%0Aintegrates%20visual%20comprehension%20and%20generation%20by%20tokenizing%20images%20into%0Adiscrete%20codes%20and%20learning%20these%20code%20embeddings%20alongside%20text%20tokens%20within%0Aa%20shared%20feature%20space%20for%20both%20vision%20and%20language.%20Unlike%20previous%20multimodal%0Alarge%20language%20model%20%28MLLM%29%2C%20Liquid%20achieves%20this%20integration%20using%20a%20single%0Alarge%20language%20model%20%28LLM%29%2C%20eliminating%20the%20need%20for%20external%20pretrained%20visual%0Aembeddings%20such%20as%20CLIP.%20For%20the%20first%20time%2C%20Liquid%20uncovers%20a%20scaling%20law%20that%0Aperformance%20drop%20unavoidably%20brought%20by%20the%20unified%20training%20of%20visual%20and%0Alanguage%20tasks%20diminishes%20as%20the%20model%20size%20increases.%20Furthermore%2C%20the%20unified%0Atoken%20space%20enables%20visual%20generation%20and%20comprehension%20tasks%20to%20mutually%0Aenhance%20each%20other%2C%20effectively%20removing%20the%20typical%20interference%20seen%20in%0Aearlier%20models.%20We%20show%20that%20existing%20LLMs%20can%20serve%20as%20strong%20foundations%20for%0ALiquid%2C%20saving%20100x%20in%20training%20costs%20while%20outperforming%20Chameleon%20in%0Amultimodal%20capabilities%20and%20maintaining%20language%20performance%20comparable%20to%0Amainstream%20LLMs%20like%20LLAMA2.%20Liquid%20also%20outperforms%20models%20like%20SD%20v2.1%20and%0ASD-XL%20%28FID%20of%205.47%20on%20MJHQ-30K%29%2C%20excelling%20in%20both%20vision-language%20and%0Atext-only%20tasks.%20This%20work%20demonstrates%20that%20LLMs%20such%20as%20LLAMA3.2%20and%20GEMMA2%0Aare%20powerful%20multimodal%20generators%2C%20offering%20a%20scalable%20solution%20for%20enhancing%0Aboth%20vision-language%20understanding%20and%20generation.%20The%20code%20and%20models%20will%20be%0Areleased%20at%20https%3A//github.com/FoundationVision/Liquid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04332v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiquid%253A%2520Language%2520Models%2520are%2520Scalable%2520Multi-modal%2520Generators%26entry.906535625%3DJunfeng%2520Wu%2520and%2520Yi%2520Jiang%2520and%2520Chuofan%2520Ma%2520and%2520Yuliang%2520Liu%2520and%2520Hengshuang%2520Zhao%2520and%2520Zehuan%2520Yuan%2520and%2520Song%2520Bai%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520We%2520present%2520Liquid%252C%2520an%2520auto-regressive%2520generation%2520paradigm%2520that%2520seamlessly%250Aintegrates%2520visual%2520comprehension%2520and%2520generation%2520by%2520tokenizing%2520images%2520into%250Adiscrete%2520codes%2520and%2520learning%2520these%2520code%2520embeddings%2520alongside%2520text%2520tokens%2520within%250Aa%2520shared%2520feature%2520space%2520for%2520both%2520vision%2520and%2520language.%2520Unlike%2520previous%2520multimodal%250Alarge%2520language%2520model%2520%2528MLLM%2529%252C%2520Liquid%2520achieves%2520this%2520integration%2520using%2520a%2520single%250Alarge%2520language%2520model%2520%2528LLM%2529%252C%2520eliminating%2520the%2520need%2520for%2520external%2520pretrained%2520visual%250Aembeddings%2520such%2520as%2520CLIP.%2520For%2520the%2520first%2520time%252C%2520Liquid%2520uncovers%2520a%2520scaling%2520law%2520that%250Aperformance%2520drop%2520unavoidably%2520brought%2520by%2520the%2520unified%2520training%2520of%2520visual%2520and%250Alanguage%2520tasks%2520diminishes%2520as%2520the%2520model%2520size%2520increases.%2520Furthermore%252C%2520the%2520unified%250Atoken%2520space%2520enables%2520visual%2520generation%2520and%2520comprehension%2520tasks%2520to%2520mutually%250Aenhance%2520each%2520other%252C%2520effectively%2520removing%2520the%2520typical%2520interference%2520seen%2520in%250Aearlier%2520models.%2520We%2520show%2520that%2520existing%2520LLMs%2520can%2520serve%2520as%2520strong%2520foundations%2520for%250ALiquid%252C%2520saving%2520100x%2520in%2520training%2520costs%2520while%2520outperforming%2520Chameleon%2520in%250Amultimodal%2520capabilities%2520and%2520maintaining%2520language%2520performance%2520comparable%2520to%250Amainstream%2520LLMs%2520like%2520LLAMA2.%2520Liquid%2520also%2520outperforms%2520models%2520like%2520SD%2520v2.1%2520and%250ASD-XL%2520%2528FID%2520of%25205.47%2520on%2520MJHQ-30K%2529%252C%2520excelling%2520in%2520both%2520vision-language%2520and%250Atext-only%2520tasks.%2520This%2520work%2520demonstrates%2520that%2520LLMs%2520such%2520as%2520LLAMA3.2%2520and%2520GEMMA2%250Aare%2520powerful%2520multimodal%2520generators%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520enhancing%250Aboth%2520vision-language%2520understanding%2520and%2520generation.%2520The%2520code%2520and%2520models%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/FoundationVision/Liquid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04332v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Liquid%3A%20Language%20Models%20are%20Scalable%20Multi-modal%20Generators&entry.906535625=Junfeng%20Wu%20and%20Yi%20Jiang%20and%20Chuofan%20Ma%20and%20Yuliang%20Liu%20and%20Hengshuang%20Zhao%20and%20Zehuan%20Yuan%20and%20Song%20Bai%20and%20Xiang%20Bai&entry.1292438233=%20%20We%20present%20Liquid%2C%20an%20auto-regressive%20generation%20paradigm%20that%20seamlessly%0Aintegrates%20visual%20comprehension%20and%20generation%20by%20tokenizing%20images%20into%0Adiscrete%20codes%20and%20learning%20these%20code%20embeddings%20alongside%20text%20tokens%20within%0Aa%20shared%20feature%20space%20for%20both%20vision%20and%20language.%20Unlike%20previous%20multimodal%0Alarge%20language%20model%20%28MLLM%29%2C%20Liquid%20achieves%20this%20integration%20using%20a%20single%0Alarge%20language%20model%20%28LLM%29%2C%20eliminating%20the%20need%20for%20external%20pretrained%20visual%0Aembeddings%20such%20as%20CLIP.%20For%20the%20first%20time%2C%20Liquid%20uncovers%20a%20scaling%20law%20that%0Aperformance%20drop%20unavoidably%20brought%20by%20the%20unified%20training%20of%20visual%20and%0Alanguage%20tasks%20diminishes%20as%20the%20model%20size%20increases.%20Furthermore%2C%20the%20unified%0Atoken%20space%20enables%20visual%20generation%20and%20comprehension%20tasks%20to%20mutually%0Aenhance%20each%20other%2C%20effectively%20removing%20the%20typical%20interference%20seen%20in%0Aearlier%20models.%20We%20show%20that%20existing%20LLMs%20can%20serve%20as%20strong%20foundations%20for%0ALiquid%2C%20saving%20100x%20in%20training%20costs%20while%20outperforming%20Chameleon%20in%0Amultimodal%20capabilities%20and%20maintaining%20language%20performance%20comparable%20to%0Amainstream%20LLMs%20like%20LLAMA2.%20Liquid%20also%20outperforms%20models%20like%20SD%20v2.1%20and%0ASD-XL%20%28FID%20of%205.47%20on%20MJHQ-30K%29%2C%20excelling%20in%20both%20vision-language%20and%0Atext-only%20tasks.%20This%20work%20demonstrates%20that%20LLMs%20such%20as%20LLAMA3.2%20and%20GEMMA2%0Aare%20powerful%20multimodal%20generators%2C%20offering%20a%20scalable%20solution%20for%20enhancing%0Aboth%20vision-language%20understanding%20and%20generation.%20The%20code%20and%20models%20will%20be%0Areleased%20at%20https%3A//github.com/FoundationVision/Liquid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04332v2&entry.124074799=Read"},
{"title": "YingSound: Video-Guided Sound Effects Generation with Multi-modal\n  Chain-of-Thought Controls", "author": "Zihao Chen and Haomin Zhang and Xinhan Di and Haoyu Wang and Sizhe Shan and Junjie Zheng and Yunming Liang and Yihan Fan and Xinfa Zhu and Wenjie Tian and Yihua Wang and Chaofan Ding and Lei Xie", "abstract": "  Generating sound effects for product-level videos, where only a small amount\nof labeled data is available for diverse scenes, requires the production of\nhigh-quality sounds in few-shot settings. To tackle the challenge of limited\nlabeled data in real-world scenes, we introduce YingSound, a foundation model\ndesigned for video-guided sound generation that supports high-quality audio\ngeneration in few-shot settings. Specifically, YingSound consists of two major\nmodules. The first module uses a conditional flow matching transformer to\nachieve effective semantic alignment in sound generation across audio and\nvisual modalities. This module aims to build a learnable audio-visual\naggregator (AVA) that integrates high-resolution visual features with\ncorresponding audio features at multiple stages. The second module is developed\nwith a proposed multi-modal visual-audio chain-of-thought (CoT) approach to\ngenerate finer sound effects in few-shot settings. Finally, an\nindustry-standard video-to-audio (V2A) dataset that encompasses various\nreal-world scenarios is presented. We show that YingSound effectively generates\nhigh-quality synchronized sounds across diverse conditional inputs through\nautomated evaluations and human studies. Project Page:\n\\url{https://giantailab.github.io/yingsound/}\n", "link": "http://arxiv.org/abs/2412.09168v1", "date": "2024-12-12", "relevancy": 2.224, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5672}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5509}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YingSound%3A%20Video-Guided%20Sound%20Effects%20Generation%20with%20Multi-modal%0A%20%20Chain-of-Thought%20Controls&body=Title%3A%20YingSound%3A%20Video-Guided%20Sound%20Effects%20Generation%20with%20Multi-modal%0A%20%20Chain-of-Thought%20Controls%0AAuthor%3A%20Zihao%20Chen%20and%20Haomin%20Zhang%20and%20Xinhan%20Di%20and%20Haoyu%20Wang%20and%20Sizhe%20Shan%20and%20Junjie%20Zheng%20and%20Yunming%20Liang%20and%20Yihan%20Fan%20and%20Xinfa%20Zhu%20and%20Wenjie%20Tian%20and%20Yihua%20Wang%20and%20Chaofan%20Ding%20and%20Lei%20Xie%0AAbstract%3A%20%20%20Generating%20sound%20effects%20for%20product-level%20videos%2C%20where%20only%20a%20small%20amount%0Aof%20labeled%20data%20is%20available%20for%20diverse%20scenes%2C%20requires%20the%20production%20of%0Ahigh-quality%20sounds%20in%20few-shot%20settings.%20To%20tackle%20the%20challenge%20of%20limited%0Alabeled%20data%20in%20real-world%20scenes%2C%20we%20introduce%20YingSound%2C%20a%20foundation%20model%0Adesigned%20for%20video-guided%20sound%20generation%20that%20supports%20high-quality%20audio%0Ageneration%20in%20few-shot%20settings.%20Specifically%2C%20YingSound%20consists%20of%20two%20major%0Amodules.%20The%20first%20module%20uses%20a%20conditional%20flow%20matching%20transformer%20to%0Aachieve%20effective%20semantic%20alignment%20in%20sound%20generation%20across%20audio%20and%0Avisual%20modalities.%20This%20module%20aims%20to%20build%20a%20learnable%20audio-visual%0Aaggregator%20%28AVA%29%20that%20integrates%20high-resolution%20visual%20features%20with%0Acorresponding%20audio%20features%20at%20multiple%20stages.%20The%20second%20module%20is%20developed%0Awith%20a%20proposed%20multi-modal%20visual-audio%20chain-of-thought%20%28CoT%29%20approach%20to%0Agenerate%20finer%20sound%20effects%20in%20few-shot%20settings.%20Finally%2C%20an%0Aindustry-standard%20video-to-audio%20%28V2A%29%20dataset%20that%20encompasses%20various%0Areal-world%20scenarios%20is%20presented.%20We%20show%20that%20YingSound%20effectively%20generates%0Ahigh-quality%20synchronized%20sounds%20across%20diverse%20conditional%20inputs%20through%0Aautomated%20evaluations%20and%20human%20studies.%20Project%20Page%3A%0A%5Curl%7Bhttps%3A//giantailab.github.io/yingsound/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYingSound%253A%2520Video-Guided%2520Sound%2520Effects%2520Generation%2520with%2520Multi-modal%250A%2520%2520Chain-of-Thought%2520Controls%26entry.906535625%3DZihao%2520Chen%2520and%2520Haomin%2520Zhang%2520and%2520Xinhan%2520Di%2520and%2520Haoyu%2520Wang%2520and%2520Sizhe%2520Shan%2520and%2520Junjie%2520Zheng%2520and%2520Yunming%2520Liang%2520and%2520Yihan%2520Fan%2520and%2520Xinfa%2520Zhu%2520and%2520Wenjie%2520Tian%2520and%2520Yihua%2520Wang%2520and%2520Chaofan%2520Ding%2520and%2520Lei%2520Xie%26entry.1292438233%3D%2520%2520Generating%2520sound%2520effects%2520for%2520product-level%2520videos%252C%2520where%2520only%2520a%2520small%2520amount%250Aof%2520labeled%2520data%2520is%2520available%2520for%2520diverse%2520scenes%252C%2520requires%2520the%2520production%2520of%250Ahigh-quality%2520sounds%2520in%2520few-shot%2520settings.%2520To%2520tackle%2520the%2520challenge%2520of%2520limited%250Alabeled%2520data%2520in%2520real-world%2520scenes%252C%2520we%2520introduce%2520YingSound%252C%2520a%2520foundation%2520model%250Adesigned%2520for%2520video-guided%2520sound%2520generation%2520that%2520supports%2520high-quality%2520audio%250Ageneration%2520in%2520few-shot%2520settings.%2520Specifically%252C%2520YingSound%2520consists%2520of%2520two%2520major%250Amodules.%2520The%2520first%2520module%2520uses%2520a%2520conditional%2520flow%2520matching%2520transformer%2520to%250Aachieve%2520effective%2520semantic%2520alignment%2520in%2520sound%2520generation%2520across%2520audio%2520and%250Avisual%2520modalities.%2520This%2520module%2520aims%2520to%2520build%2520a%2520learnable%2520audio-visual%250Aaggregator%2520%2528AVA%2529%2520that%2520integrates%2520high-resolution%2520visual%2520features%2520with%250Acorresponding%2520audio%2520features%2520at%2520multiple%2520stages.%2520The%2520second%2520module%2520is%2520developed%250Awith%2520a%2520proposed%2520multi-modal%2520visual-audio%2520chain-of-thought%2520%2528CoT%2529%2520approach%2520to%250Agenerate%2520finer%2520sound%2520effects%2520in%2520few-shot%2520settings.%2520Finally%252C%2520an%250Aindustry-standard%2520video-to-audio%2520%2528V2A%2529%2520dataset%2520that%2520encompasses%2520various%250Areal-world%2520scenarios%2520is%2520presented.%2520We%2520show%2520that%2520YingSound%2520effectively%2520generates%250Ahigh-quality%2520synchronized%2520sounds%2520across%2520diverse%2520conditional%2520inputs%2520through%250Aautomated%2520evaluations%2520and%2520human%2520studies.%2520Project%2520Page%253A%250A%255Curl%257Bhttps%253A//giantailab.github.io/yingsound/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YingSound%3A%20Video-Guided%20Sound%20Effects%20Generation%20with%20Multi-modal%0A%20%20Chain-of-Thought%20Controls&entry.906535625=Zihao%20Chen%20and%20Haomin%20Zhang%20and%20Xinhan%20Di%20and%20Haoyu%20Wang%20and%20Sizhe%20Shan%20and%20Junjie%20Zheng%20and%20Yunming%20Liang%20and%20Yihan%20Fan%20and%20Xinfa%20Zhu%20and%20Wenjie%20Tian%20and%20Yihua%20Wang%20and%20Chaofan%20Ding%20and%20Lei%20Xie&entry.1292438233=%20%20Generating%20sound%20effects%20for%20product-level%20videos%2C%20where%20only%20a%20small%20amount%0Aof%20labeled%20data%20is%20available%20for%20diverse%20scenes%2C%20requires%20the%20production%20of%0Ahigh-quality%20sounds%20in%20few-shot%20settings.%20To%20tackle%20the%20challenge%20of%20limited%0Alabeled%20data%20in%20real-world%20scenes%2C%20we%20introduce%20YingSound%2C%20a%20foundation%20model%0Adesigned%20for%20video-guided%20sound%20generation%20that%20supports%20high-quality%20audio%0Ageneration%20in%20few-shot%20settings.%20Specifically%2C%20YingSound%20consists%20of%20two%20major%0Amodules.%20The%20first%20module%20uses%20a%20conditional%20flow%20matching%20transformer%20to%0Aachieve%20effective%20semantic%20alignment%20in%20sound%20generation%20across%20audio%20and%0Avisual%20modalities.%20This%20module%20aims%20to%20build%20a%20learnable%20audio-visual%0Aaggregator%20%28AVA%29%20that%20integrates%20high-resolution%20visual%20features%20with%0Acorresponding%20audio%20features%20at%20multiple%20stages.%20The%20second%20module%20is%20developed%0Awith%20a%20proposed%20multi-modal%20visual-audio%20chain-of-thought%20%28CoT%29%20approach%20to%0Agenerate%20finer%20sound%20effects%20in%20few-shot%20settings.%20Finally%2C%20an%0Aindustry-standard%20video-to-audio%20%28V2A%29%20dataset%20that%20encompasses%20various%0Areal-world%20scenarios%20is%20presented.%20We%20show%20that%20YingSound%20effectively%20generates%0Ahigh-quality%20synchronized%20sounds%20across%20diverse%20conditional%20inputs%20through%0Aautomated%20evaluations%20and%20human%20studies.%20Project%20Page%3A%0A%5Curl%7Bhttps%3A//giantailab.github.io/yingsound/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09168v1&entry.124074799=Read"},
{"title": "A Multi-Stage Framework for Joint Chest X-Ray Diagnosis and Visual\n  Attention Prediction Using Deep Learning", "author": "Zirui Qiu and Hassan Rivaz and Yiming Xiao", "abstract": "  Purpose: As visual inspection is an inherent process during radiological\nscreening, the associated eye gaze data can provide valuable insights into\nrelevant clinical decisions. As deep learning has become the state-of-the-art\nfor computer-assisted diagnosis, integrating human behavior, such as eye gaze\ndata, into these systems is instrumental to help align machine predictions with\nclinical diagnostic criteria, thus enhancing the quality of automatic\nradiological diagnosis. Methods: We propose a novel deep learning framework for\njoint disease diagnosis and prediction of corresponding clinical visual\nattention maps for chest X-ray scans. Specifically, we introduce a new\ndual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a\nResidual and Squeeze-and-Excitation block-based encoder to extract diverse\nfeatures for visual attention map prediction, and a multi-scale feature-fusion\nclassifier to perform disease classification. To tackle the issue of\nasynchronous training schedules of individual tasks in multi-task learning, we\nproposed a multi-stage cooperative learning strategy, with contrastive learning\nfor feature encoder pretraining to boost performance. Results: Our proposed\nmethod is shown to significantly outperform existing techniques for chest X-ray\ndiagnosis (AUC=0.93) and the quality of visual attention map prediction\n(Correlation coefficient=0.58). Conclusion: Benefiting from the proposed\nmulti-task multi-stage cooperative learning, our technique demonstrates the\nbenefit of integrating clinicians' eye gaze into clinical AI systems to boost\nperformance and potentially explainability.\n", "link": "http://arxiv.org/abs/2403.16970v4", "date": "2024-12-12", "relevancy": 2.2235, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5639}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5581}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stage%20Framework%20for%20Joint%20Chest%20X-Ray%20Diagnosis%20and%20Visual%0A%20%20Attention%20Prediction%20Using%20Deep%20Learning&body=Title%3A%20A%20Multi-Stage%20Framework%20for%20Joint%20Chest%20X-Ray%20Diagnosis%20and%20Visual%0A%20%20Attention%20Prediction%20Using%20Deep%20Learning%0AAuthor%3A%20Zirui%20Qiu%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao%0AAbstract%3A%20%20%20Purpose%3A%20As%20visual%20inspection%20is%20an%20inherent%20process%20during%20radiological%0Ascreening%2C%20the%20associated%20eye%20gaze%20data%20can%20provide%20valuable%20insights%20into%0Arelevant%20clinical%20decisions.%20As%20deep%20learning%20has%20become%20the%20state-of-the-art%0Afor%20computer-assisted%20diagnosis%2C%20integrating%20human%20behavior%2C%20such%20as%20eye%20gaze%0Adata%2C%20into%20these%20systems%20is%20instrumental%20to%20help%20align%20machine%20predictions%20with%0Aclinical%20diagnostic%20criteria%2C%20thus%20enhancing%20the%20quality%20of%20automatic%0Aradiological%20diagnosis.%20Methods%3A%20We%20propose%20a%20novel%20deep%20learning%20framework%20for%0Ajoint%20disease%20diagnosis%20and%20prediction%20of%20corresponding%20clinical%20visual%0Aattention%20maps%20for%20chest%20X-ray%20scans.%20Specifically%2C%20we%20introduce%20a%20new%0Adual-encoder%20multi-task%20UNet%2C%20which%20leverages%20both%20a%20DenseNet201%20backbone%20and%20a%0AResidual%20and%20Squeeze-and-Excitation%20block-based%20encoder%20to%20extract%20diverse%0Afeatures%20for%20visual%20attention%20map%20prediction%2C%20and%20a%20multi-scale%20feature-fusion%0Aclassifier%20to%20perform%20disease%20classification.%20To%20tackle%20the%20issue%20of%0Aasynchronous%20training%20schedules%20of%20individual%20tasks%20in%20multi-task%20learning%2C%20we%0Aproposed%20a%20multi-stage%20cooperative%20learning%20strategy%2C%20with%20contrastive%20learning%0Afor%20feature%20encoder%20pretraining%20to%20boost%20performance.%20Results%3A%20Our%20proposed%0Amethod%20is%20shown%20to%20significantly%20outperform%20existing%20techniques%20for%20chest%20X-ray%0Adiagnosis%20%28AUC%3D0.93%29%20and%20the%20quality%20of%20visual%20attention%20map%20prediction%0A%28Correlation%20coefficient%3D0.58%29.%20Conclusion%3A%20Benefiting%20from%20the%20proposed%0Amulti-task%20multi-stage%20cooperative%20learning%2C%20our%20technique%20demonstrates%20the%0Abenefit%20of%20integrating%20clinicians%27%20eye%20gaze%20into%20clinical%20AI%20systems%20to%20boost%0Aperformance%20and%20potentially%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16970v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stage%2520Framework%2520for%2520Joint%2520Chest%2520X-Ray%2520Diagnosis%2520and%2520Visual%250A%2520%2520Attention%2520Prediction%2520Using%2520Deep%2520Learning%26entry.906535625%3DZirui%2520Qiu%2520and%2520Hassan%2520Rivaz%2520and%2520Yiming%2520Xiao%26entry.1292438233%3D%2520%2520Purpose%253A%2520As%2520visual%2520inspection%2520is%2520an%2520inherent%2520process%2520during%2520radiological%250Ascreening%252C%2520the%2520associated%2520eye%2520gaze%2520data%2520can%2520provide%2520valuable%2520insights%2520into%250Arelevant%2520clinical%2520decisions.%2520As%2520deep%2520learning%2520has%2520become%2520the%2520state-of-the-art%250Afor%2520computer-assisted%2520diagnosis%252C%2520integrating%2520human%2520behavior%252C%2520such%2520as%2520eye%2520gaze%250Adata%252C%2520into%2520these%2520systems%2520is%2520instrumental%2520to%2520help%2520align%2520machine%2520predictions%2520with%250Aclinical%2520diagnostic%2520criteria%252C%2520thus%2520enhancing%2520the%2520quality%2520of%2520automatic%250Aradiological%2520diagnosis.%2520Methods%253A%2520We%2520propose%2520a%2520novel%2520deep%2520learning%2520framework%2520for%250Ajoint%2520disease%2520diagnosis%2520and%2520prediction%2520of%2520corresponding%2520clinical%2520visual%250Aattention%2520maps%2520for%2520chest%2520X-ray%2520scans.%2520Specifically%252C%2520we%2520introduce%2520a%2520new%250Adual-encoder%2520multi-task%2520UNet%252C%2520which%2520leverages%2520both%2520a%2520DenseNet201%2520backbone%2520and%2520a%250AResidual%2520and%2520Squeeze-and-Excitation%2520block-based%2520encoder%2520to%2520extract%2520diverse%250Afeatures%2520for%2520visual%2520attention%2520map%2520prediction%252C%2520and%2520a%2520multi-scale%2520feature-fusion%250Aclassifier%2520to%2520perform%2520disease%2520classification.%2520To%2520tackle%2520the%2520issue%2520of%250Aasynchronous%2520training%2520schedules%2520of%2520individual%2520tasks%2520in%2520multi-task%2520learning%252C%2520we%250Aproposed%2520a%2520multi-stage%2520cooperative%2520learning%2520strategy%252C%2520with%2520contrastive%2520learning%250Afor%2520feature%2520encoder%2520pretraining%2520to%2520boost%2520performance.%2520Results%253A%2520Our%2520proposed%250Amethod%2520is%2520shown%2520to%2520significantly%2520outperform%2520existing%2520techniques%2520for%2520chest%2520X-ray%250Adiagnosis%2520%2528AUC%253D0.93%2529%2520and%2520the%2520quality%2520of%2520visual%2520attention%2520map%2520prediction%250A%2528Correlation%2520coefficient%253D0.58%2529.%2520Conclusion%253A%2520Benefiting%2520from%2520the%2520proposed%250Amulti-task%2520multi-stage%2520cooperative%2520learning%252C%2520our%2520technique%2520demonstrates%2520the%250Abenefit%2520of%2520integrating%2520clinicians%2527%2520eye%2520gaze%2520into%2520clinical%2520AI%2520systems%2520to%2520boost%250Aperformance%2520and%2520potentially%2520explainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16970v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stage%20Framework%20for%20Joint%20Chest%20X-Ray%20Diagnosis%20and%20Visual%0A%20%20Attention%20Prediction%20Using%20Deep%20Learning&entry.906535625=Zirui%20Qiu%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao&entry.1292438233=%20%20Purpose%3A%20As%20visual%20inspection%20is%20an%20inherent%20process%20during%20radiological%0Ascreening%2C%20the%20associated%20eye%20gaze%20data%20can%20provide%20valuable%20insights%20into%0Arelevant%20clinical%20decisions.%20As%20deep%20learning%20has%20become%20the%20state-of-the-art%0Afor%20computer-assisted%20diagnosis%2C%20integrating%20human%20behavior%2C%20such%20as%20eye%20gaze%0Adata%2C%20into%20these%20systems%20is%20instrumental%20to%20help%20align%20machine%20predictions%20with%0Aclinical%20diagnostic%20criteria%2C%20thus%20enhancing%20the%20quality%20of%20automatic%0Aradiological%20diagnosis.%20Methods%3A%20We%20propose%20a%20novel%20deep%20learning%20framework%20for%0Ajoint%20disease%20diagnosis%20and%20prediction%20of%20corresponding%20clinical%20visual%0Aattention%20maps%20for%20chest%20X-ray%20scans.%20Specifically%2C%20we%20introduce%20a%20new%0Adual-encoder%20multi-task%20UNet%2C%20which%20leverages%20both%20a%20DenseNet201%20backbone%20and%20a%0AResidual%20and%20Squeeze-and-Excitation%20block-based%20encoder%20to%20extract%20diverse%0Afeatures%20for%20visual%20attention%20map%20prediction%2C%20and%20a%20multi-scale%20feature-fusion%0Aclassifier%20to%20perform%20disease%20classification.%20To%20tackle%20the%20issue%20of%0Aasynchronous%20training%20schedules%20of%20individual%20tasks%20in%20multi-task%20learning%2C%20we%0Aproposed%20a%20multi-stage%20cooperative%20learning%20strategy%2C%20with%20contrastive%20learning%0Afor%20feature%20encoder%20pretraining%20to%20boost%20performance.%20Results%3A%20Our%20proposed%0Amethod%20is%20shown%20to%20significantly%20outperform%20existing%20techniques%20for%20chest%20X-ray%0Adiagnosis%20%28AUC%3D0.93%29%20and%20the%20quality%20of%20visual%20attention%20map%20prediction%0A%28Correlation%20coefficient%3D0.58%29.%20Conclusion%3A%20Benefiting%20from%20the%20proposed%0Amulti-task%20multi-stage%20cooperative%20learning%2C%20our%20technique%20demonstrates%20the%0Abenefit%20of%20integrating%20clinicians%27%20eye%20gaze%20into%20clinical%20AI%20systems%20to%20boost%0Aperformance%20and%20potentially%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16970v4&entry.124074799=Read"},
{"title": "Advancing Attribution-Based Neural Network Explainability through\n  Relative Absolute Magnitude Layer-Wise Relevance Propagation and\n  Multi-Component Evaluation", "author": "Davor Vukadin and Petar Afri\u0107 and Marin \u0160ili\u0107 and Goran Dela\u010d", "abstract": "  Recent advancement in deep-neural network performance led to the development\nof new state-of-the-art approaches in numerous areas. However, the black-box\nnature of neural networks often prohibits their use in areas where model\nexplainability and model transparency are crucial. Over the years, researchers\nproposed many algorithms to aid neural network understanding and provide\nadditional information to the human expert. One of the most popular methods\nbeing Layer-Wise Relevance Propagation (LRP). This method assigns local\nrelevance based on the pixel-wise decomposition of nonlinear classifiers. With\nthe rise of attribution method research, there has emerged a pressing need to\nassess and evaluate their performance. Numerous metrics have been proposed,\neach assessing an individual property of attribution methods such as\nfaithfulness, robustness or localization. Unfortunately, no single metric is\ndeemed optimal for every case, and researchers often use several metrics to\ntest the quality of the attribution maps. In this work, we address the\nshortcomings of the current LRP formulations and introduce a novel method for\ndetermining the relevance of input neurons through layer-wise relevance\npropagation. Furthermore, we apply this approach to the recently developed\nVision Transformer architecture and evaluate its performance against existing\nmethods on two image classification datasets, namely ImageNet and PascalVOC.\nOur results clearly demonstrate the advantage of our proposed method.\nFurthermore, we discuss the insufficiencies of current evaluation metrics for\nattribution-based explainability and propose a new evaluation metric that\ncombines the notions of faithfulness, robustness and contrastiveness. We\nutilize this new metric to evaluate the performance of various\nattribution-based methods. Our code is available at:\nhttps://github.com/davor10105/relative-absolute-magnitude-propagation\n", "link": "http://arxiv.org/abs/2412.09311v1", "date": "2024-12-12", "relevancy": 2.2177, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Attribution-Based%20Neural%20Network%20Explainability%20through%0A%20%20Relative%20Absolute%20Magnitude%20Layer-Wise%20Relevance%20Propagation%20and%0A%20%20Multi-Component%20Evaluation&body=Title%3A%20Advancing%20Attribution-Based%20Neural%20Network%20Explainability%20through%0A%20%20Relative%20Absolute%20Magnitude%20Layer-Wise%20Relevance%20Propagation%20and%0A%20%20Multi-Component%20Evaluation%0AAuthor%3A%20Davor%20Vukadin%20and%20Petar%20Afri%C4%87%20and%20Marin%20%C5%A0ili%C4%87%20and%20Goran%20Dela%C4%8D%0AAbstract%3A%20%20%20Recent%20advancement%20in%20deep-neural%20network%20performance%20led%20to%20the%20development%0Aof%20new%20state-of-the-art%20approaches%20in%20numerous%20areas.%20However%2C%20the%20black-box%0Anature%20of%20neural%20networks%20often%20prohibits%20their%20use%20in%20areas%20where%20model%0Aexplainability%20and%20model%20transparency%20are%20crucial.%20Over%20the%20years%2C%20researchers%0Aproposed%20many%20algorithms%20to%20aid%20neural%20network%20understanding%20and%20provide%0Aadditional%20information%20to%20the%20human%20expert.%20One%20of%20the%20most%20popular%20methods%0Abeing%20Layer-Wise%20Relevance%20Propagation%20%28LRP%29.%20This%20method%20assigns%20local%0Arelevance%20based%20on%20the%20pixel-wise%20decomposition%20of%20nonlinear%20classifiers.%20With%0Athe%20rise%20of%20attribution%20method%20research%2C%20there%20has%20emerged%20a%20pressing%20need%20to%0Aassess%20and%20evaluate%20their%20performance.%20Numerous%20metrics%20have%20been%20proposed%2C%0Aeach%20assessing%20an%20individual%20property%20of%20attribution%20methods%20such%20as%0Afaithfulness%2C%20robustness%20or%20localization.%20Unfortunately%2C%20no%20single%20metric%20is%0Adeemed%20optimal%20for%20every%20case%2C%20and%20researchers%20often%20use%20several%20metrics%20to%0Atest%20the%20quality%20of%20the%20attribution%20maps.%20In%20this%20work%2C%20we%20address%20the%0Ashortcomings%20of%20the%20current%20LRP%20formulations%20and%20introduce%20a%20novel%20method%20for%0Adetermining%20the%20relevance%20of%20input%20neurons%20through%20layer-wise%20relevance%0Apropagation.%20Furthermore%2C%20we%20apply%20this%20approach%20to%20the%20recently%20developed%0AVision%20Transformer%20architecture%20and%20evaluate%20its%20performance%20against%20existing%0Amethods%20on%20two%20image%20classification%20datasets%2C%20namely%20ImageNet%20and%20PascalVOC.%0AOur%20results%20clearly%20demonstrate%20the%20advantage%20of%20our%20proposed%20method.%0AFurthermore%2C%20we%20discuss%20the%20insufficiencies%20of%20current%20evaluation%20metrics%20for%0Aattribution-based%20explainability%20and%20propose%20a%20new%20evaluation%20metric%20that%0Acombines%20the%20notions%20of%20faithfulness%2C%20robustness%20and%20contrastiveness.%20We%0Autilize%20this%20new%20metric%20to%20evaluate%20the%20performance%20of%20various%0Aattribution-based%20methods.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/davor10105/relative-absolute-magnitude-propagation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Attribution-Based%2520Neural%2520Network%2520Explainability%2520through%250A%2520%2520Relative%2520Absolute%2520Magnitude%2520Layer-Wise%2520Relevance%2520Propagation%2520and%250A%2520%2520Multi-Component%2520Evaluation%26entry.906535625%3DDavor%2520Vukadin%2520and%2520Petar%2520Afri%25C4%2587%2520and%2520Marin%2520%25C5%25A0ili%25C4%2587%2520and%2520Goran%2520Dela%25C4%258D%26entry.1292438233%3D%2520%2520Recent%2520advancement%2520in%2520deep-neural%2520network%2520performance%2520led%2520to%2520the%2520development%250Aof%2520new%2520state-of-the-art%2520approaches%2520in%2520numerous%2520areas.%2520However%252C%2520the%2520black-box%250Anature%2520of%2520neural%2520networks%2520often%2520prohibits%2520their%2520use%2520in%2520areas%2520where%2520model%250Aexplainability%2520and%2520model%2520transparency%2520are%2520crucial.%2520Over%2520the%2520years%252C%2520researchers%250Aproposed%2520many%2520algorithms%2520to%2520aid%2520neural%2520network%2520understanding%2520and%2520provide%250Aadditional%2520information%2520to%2520the%2520human%2520expert.%2520One%2520of%2520the%2520most%2520popular%2520methods%250Abeing%2520Layer-Wise%2520Relevance%2520Propagation%2520%2528LRP%2529.%2520This%2520method%2520assigns%2520local%250Arelevance%2520based%2520on%2520the%2520pixel-wise%2520decomposition%2520of%2520nonlinear%2520classifiers.%2520With%250Athe%2520rise%2520of%2520attribution%2520method%2520research%252C%2520there%2520has%2520emerged%2520a%2520pressing%2520need%2520to%250Aassess%2520and%2520evaluate%2520their%2520performance.%2520Numerous%2520metrics%2520have%2520been%2520proposed%252C%250Aeach%2520assessing%2520an%2520individual%2520property%2520of%2520attribution%2520methods%2520such%2520as%250Afaithfulness%252C%2520robustness%2520or%2520localization.%2520Unfortunately%252C%2520no%2520single%2520metric%2520is%250Adeemed%2520optimal%2520for%2520every%2520case%252C%2520and%2520researchers%2520often%2520use%2520several%2520metrics%2520to%250Atest%2520the%2520quality%2520of%2520the%2520attribution%2520maps.%2520In%2520this%2520work%252C%2520we%2520address%2520the%250Ashortcomings%2520of%2520the%2520current%2520LRP%2520formulations%2520and%2520introduce%2520a%2520novel%2520method%2520for%250Adetermining%2520the%2520relevance%2520of%2520input%2520neurons%2520through%2520layer-wise%2520relevance%250Apropagation.%2520Furthermore%252C%2520we%2520apply%2520this%2520approach%2520to%2520the%2520recently%2520developed%250AVision%2520Transformer%2520architecture%2520and%2520evaluate%2520its%2520performance%2520against%2520existing%250Amethods%2520on%2520two%2520image%2520classification%2520datasets%252C%2520namely%2520ImageNet%2520and%2520PascalVOC.%250AOur%2520results%2520clearly%2520demonstrate%2520the%2520advantage%2520of%2520our%2520proposed%2520method.%250AFurthermore%252C%2520we%2520discuss%2520the%2520insufficiencies%2520of%2520current%2520evaluation%2520metrics%2520for%250Aattribution-based%2520explainability%2520and%2520propose%2520a%2520new%2520evaluation%2520metric%2520that%250Acombines%2520the%2520notions%2520of%2520faithfulness%252C%2520robustness%2520and%2520contrastiveness.%2520We%250Autilize%2520this%2520new%2520metric%2520to%2520evaluate%2520the%2520performance%2520of%2520various%250Aattribution-based%2520methods.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/davor10105/relative-absolute-magnitude-propagation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Attribution-Based%20Neural%20Network%20Explainability%20through%0A%20%20Relative%20Absolute%20Magnitude%20Layer-Wise%20Relevance%20Propagation%20and%0A%20%20Multi-Component%20Evaluation&entry.906535625=Davor%20Vukadin%20and%20Petar%20Afri%C4%87%20and%20Marin%20%C5%A0ili%C4%87%20and%20Goran%20Dela%C4%8D&entry.1292438233=%20%20Recent%20advancement%20in%20deep-neural%20network%20performance%20led%20to%20the%20development%0Aof%20new%20state-of-the-art%20approaches%20in%20numerous%20areas.%20However%2C%20the%20black-box%0Anature%20of%20neural%20networks%20often%20prohibits%20their%20use%20in%20areas%20where%20model%0Aexplainability%20and%20model%20transparency%20are%20crucial.%20Over%20the%20years%2C%20researchers%0Aproposed%20many%20algorithms%20to%20aid%20neural%20network%20understanding%20and%20provide%0Aadditional%20information%20to%20the%20human%20expert.%20One%20of%20the%20most%20popular%20methods%0Abeing%20Layer-Wise%20Relevance%20Propagation%20%28LRP%29.%20This%20method%20assigns%20local%0Arelevance%20based%20on%20the%20pixel-wise%20decomposition%20of%20nonlinear%20classifiers.%20With%0Athe%20rise%20of%20attribution%20method%20research%2C%20there%20has%20emerged%20a%20pressing%20need%20to%0Aassess%20and%20evaluate%20their%20performance.%20Numerous%20metrics%20have%20been%20proposed%2C%0Aeach%20assessing%20an%20individual%20property%20of%20attribution%20methods%20such%20as%0Afaithfulness%2C%20robustness%20or%20localization.%20Unfortunately%2C%20no%20single%20metric%20is%0Adeemed%20optimal%20for%20every%20case%2C%20and%20researchers%20often%20use%20several%20metrics%20to%0Atest%20the%20quality%20of%20the%20attribution%20maps.%20In%20this%20work%2C%20we%20address%20the%0Ashortcomings%20of%20the%20current%20LRP%20formulations%20and%20introduce%20a%20novel%20method%20for%0Adetermining%20the%20relevance%20of%20input%20neurons%20through%20layer-wise%20relevance%0Apropagation.%20Furthermore%2C%20we%20apply%20this%20approach%20to%20the%20recently%20developed%0AVision%20Transformer%20architecture%20and%20evaluate%20its%20performance%20against%20existing%0Amethods%20on%20two%20image%20classification%20datasets%2C%20namely%20ImageNet%20and%20PascalVOC.%0AOur%20results%20clearly%20demonstrate%20the%20advantage%20of%20our%20proposed%20method.%0AFurthermore%2C%20we%20discuss%20the%20insufficiencies%20of%20current%20evaluation%20metrics%20for%0Aattribution-based%20explainability%20and%20propose%20a%20new%20evaluation%20metric%20that%0Acombines%20the%20notions%20of%20faithfulness%2C%20robustness%20and%20contrastiveness.%20We%0Autilize%20this%20new%20metric%20to%20evaluate%20the%20performance%20of%20various%0Aattribution-based%20methods.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/davor10105/relative-absolute-magnitude-propagation%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09311v1&entry.124074799=Read"},
{"title": "iKap: Kinematics-aware Planning with Imperative Learning", "author": "Qihang Li and Zhuoqun Chen and Haoze Zheng and Haonan He and Shaoshu Su and Junyi Geng and Chen Wang", "abstract": "  Trajectory planning in robotics aims to generate collision-free pose\nsequences that can be reliably executed. Recently, vision-to-planning systems\nhave garnered increasing attention for their efficiency and ability to\ninterpret and adapt to surrounding environments. However, traditional modular\nsystems suffer from increased latency and error propagation, while purely\ndata-driven approaches often overlook the robot's kinematic constraints. This\noversight leads to discrepancies between planned trajectories and those that\nare executable. To address these challenges, we propose iKap, a novel\nvision-to-planning system that integrates the robot's kinematic model directly\ninto the learning pipeline. iKap employs a self-supervised learning approach\nand incorporates the state transition model within a differentiable bi-level\noptimization framework. This integration ensures the network learns\ncollision-free waypoints while satisfying kinematic constraints, enabling\ngradient back-propagation for end-to-end training. Our experimental results\ndemonstrate that iKap achieves higher success rates and reduced latency\ncompared to the state-of-the-art methods. Besides the complete system, iKap\noffers a visual-to-planning network that seamlessly integrates kinematics into\nvarious controllers, providing a robust solution for robots navigating complex\nand dynamic environments.\n", "link": "http://arxiv.org/abs/2412.09496v1", "date": "2024-12-12", "relevancy": 2.215, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5753}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iKap%3A%20Kinematics-aware%20Planning%20with%20Imperative%20Learning&body=Title%3A%20iKap%3A%20Kinematics-aware%20Planning%20with%20Imperative%20Learning%0AAuthor%3A%20Qihang%20Li%20and%20Zhuoqun%20Chen%20and%20Haoze%20Zheng%20and%20Haonan%20He%20and%20Shaoshu%20Su%20and%20Junyi%20Geng%20and%20Chen%20Wang%0AAbstract%3A%20%20%20Trajectory%20planning%20in%20robotics%20aims%20to%20generate%20collision-free%20pose%0Asequences%20that%20can%20be%20reliably%20executed.%20Recently%2C%20vision-to-planning%20systems%0Ahave%20garnered%20increasing%20attention%20for%20their%20efficiency%20and%20ability%20to%0Ainterpret%20and%20adapt%20to%20surrounding%20environments.%20However%2C%20traditional%20modular%0Asystems%20suffer%20from%20increased%20latency%20and%20error%20propagation%2C%20while%20purely%0Adata-driven%20approaches%20often%20overlook%20the%20robot%27s%20kinematic%20constraints.%20This%0Aoversight%20leads%20to%20discrepancies%20between%20planned%20trajectories%20and%20those%20that%0Aare%20executable.%20To%20address%20these%20challenges%2C%20we%20propose%20iKap%2C%20a%20novel%0Avision-to-planning%20system%20that%20integrates%20the%20robot%27s%20kinematic%20model%20directly%0Ainto%20the%20learning%20pipeline.%20iKap%20employs%20a%20self-supervised%20learning%20approach%0Aand%20incorporates%20the%20state%20transition%20model%20within%20a%20differentiable%20bi-level%0Aoptimization%20framework.%20This%20integration%20ensures%20the%20network%20learns%0Acollision-free%20waypoints%20while%20satisfying%20kinematic%20constraints%2C%20enabling%0Agradient%20back-propagation%20for%20end-to-end%20training.%20Our%20experimental%20results%0Ademonstrate%20that%20iKap%20achieves%20higher%20success%20rates%20and%20reduced%20latency%0Acompared%20to%20the%20state-of-the-art%20methods.%20Besides%20the%20complete%20system%2C%20iKap%0Aoffers%20a%20visual-to-planning%20network%20that%20seamlessly%20integrates%20kinematics%20into%0Avarious%20controllers%2C%20providing%20a%20robust%20solution%20for%20robots%20navigating%20complex%0Aand%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiKap%253A%2520Kinematics-aware%2520Planning%2520with%2520Imperative%2520Learning%26entry.906535625%3DQihang%2520Li%2520and%2520Zhuoqun%2520Chen%2520and%2520Haoze%2520Zheng%2520and%2520Haonan%2520He%2520and%2520Shaoshu%2520Su%2520and%2520Junyi%2520Geng%2520and%2520Chen%2520Wang%26entry.1292438233%3D%2520%2520Trajectory%2520planning%2520in%2520robotics%2520aims%2520to%2520generate%2520collision-free%2520pose%250Asequences%2520that%2520can%2520be%2520reliably%2520executed.%2520Recently%252C%2520vision-to-planning%2520systems%250Ahave%2520garnered%2520increasing%2520attention%2520for%2520their%2520efficiency%2520and%2520ability%2520to%250Ainterpret%2520and%2520adapt%2520to%2520surrounding%2520environments.%2520However%252C%2520traditional%2520modular%250Asystems%2520suffer%2520from%2520increased%2520latency%2520and%2520error%2520propagation%252C%2520while%2520purely%250Adata-driven%2520approaches%2520often%2520overlook%2520the%2520robot%2527s%2520kinematic%2520constraints.%2520This%250Aoversight%2520leads%2520to%2520discrepancies%2520between%2520planned%2520trajectories%2520and%2520those%2520that%250Aare%2520executable.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520iKap%252C%2520a%2520novel%250Avision-to-planning%2520system%2520that%2520integrates%2520the%2520robot%2527s%2520kinematic%2520model%2520directly%250Ainto%2520the%2520learning%2520pipeline.%2520iKap%2520employs%2520a%2520self-supervised%2520learning%2520approach%250Aand%2520incorporates%2520the%2520state%2520transition%2520model%2520within%2520a%2520differentiable%2520bi-level%250Aoptimization%2520framework.%2520This%2520integration%2520ensures%2520the%2520network%2520learns%250Acollision-free%2520waypoints%2520while%2520satisfying%2520kinematic%2520constraints%252C%2520enabling%250Agradient%2520back-propagation%2520for%2520end-to-end%2520training.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520iKap%2520achieves%2520higher%2520success%2520rates%2520and%2520reduced%2520latency%250Acompared%2520to%2520the%2520state-of-the-art%2520methods.%2520Besides%2520the%2520complete%2520system%252C%2520iKap%250Aoffers%2520a%2520visual-to-planning%2520network%2520that%2520seamlessly%2520integrates%2520kinematics%2520into%250Avarious%2520controllers%252C%2520providing%2520a%2520robust%2520solution%2520for%2520robots%2520navigating%2520complex%250Aand%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iKap%3A%20Kinematics-aware%20Planning%20with%20Imperative%20Learning&entry.906535625=Qihang%20Li%20and%20Zhuoqun%20Chen%20and%20Haoze%20Zheng%20and%20Haonan%20He%20and%20Shaoshu%20Su%20and%20Junyi%20Geng%20and%20Chen%20Wang&entry.1292438233=%20%20Trajectory%20planning%20in%20robotics%20aims%20to%20generate%20collision-free%20pose%0Asequences%20that%20can%20be%20reliably%20executed.%20Recently%2C%20vision-to-planning%20systems%0Ahave%20garnered%20increasing%20attention%20for%20their%20efficiency%20and%20ability%20to%0Ainterpret%20and%20adapt%20to%20surrounding%20environments.%20However%2C%20traditional%20modular%0Asystems%20suffer%20from%20increased%20latency%20and%20error%20propagation%2C%20while%20purely%0Adata-driven%20approaches%20often%20overlook%20the%20robot%27s%20kinematic%20constraints.%20This%0Aoversight%20leads%20to%20discrepancies%20between%20planned%20trajectories%20and%20those%20that%0Aare%20executable.%20To%20address%20these%20challenges%2C%20we%20propose%20iKap%2C%20a%20novel%0Avision-to-planning%20system%20that%20integrates%20the%20robot%27s%20kinematic%20model%20directly%0Ainto%20the%20learning%20pipeline.%20iKap%20employs%20a%20self-supervised%20learning%20approach%0Aand%20incorporates%20the%20state%20transition%20model%20within%20a%20differentiable%20bi-level%0Aoptimization%20framework.%20This%20integration%20ensures%20the%20network%20learns%0Acollision-free%20waypoints%20while%20satisfying%20kinematic%20constraints%2C%20enabling%0Agradient%20back-propagation%20for%20end-to-end%20training.%20Our%20experimental%20results%0Ademonstrate%20that%20iKap%20achieves%20higher%20success%20rates%20and%20reduced%20latency%0Acompared%20to%20the%20state-of-the-art%20methods.%20Besides%20the%20complete%20system%2C%20iKap%0Aoffers%20a%20visual-to-planning%20network%20that%20seamlessly%20integrates%20kinematics%20into%0Avarious%20controllers%2C%20providing%20a%20robust%20solution%20for%20robots%20navigating%20complex%0Aand%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09496v1&entry.124074799=Read"},
{"title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction", "author": "Jing Liu and Abdellah Fourtassi", "abstract": "  LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.\n", "link": "http://arxiv.org/abs/2412.09318v1", "date": "2024-12-12", "relevancy": 2.2045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4432}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20LLMs%20for%20Mimicking%20Child-Caregiver%20Language%20in%20Interaction&body=Title%3A%20Benchmarking%20LLMs%20for%20Mimicking%20Child-Caregiver%20Language%20in%20Interaction%0AAuthor%3A%20Jing%20Liu%20and%20Abdellah%20Fourtassi%0AAbstract%3A%20%20%20LLMs%20can%20generate%20human-like%20dialogues%2C%20yet%20their%20ability%20to%20simulate%20early%0Achild-adult%20interactions%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20examined%0Ahow%20effectively%20LLMs%20can%20capture%20the%20distinctive%20features%20of%20child-caregiver%0Alanguage%20in%20interaction%2C%20using%20both%20static%20and%20interactive%20benchmarking%0Amethods.%20We%20found%20that%20state-of-the-art%20LLMs%20like%20Llama%203%20and%20GPT-4o%20can%0Aapproximate%20child-caregiver%20dialogues%20at%20the%20word%20and%20utterance%20level%2C%20but%20they%0Astruggle%20to%20reproduce%20the%20child%20and%20caregiver%27s%20discursive%20patterns%2C%20exaggerate%0Aalignment%2C%20and%20fail%20to%20reach%20the%20level%20of%20diversity%20shown%20by%20humans.%20The%0Abroader%20goal%20of%20this%20work%20is%20to%20initiate%20the%20development%20of%20a%20comprehensive%0Abenchmark%20for%20LLMs%20in%20child-oriented%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520LLMs%2520for%2520Mimicking%2520Child-Caregiver%2520Language%2520in%2520Interaction%26entry.906535625%3DJing%2520Liu%2520and%2520Abdellah%2520Fourtassi%26entry.1292438233%3D%2520%2520LLMs%2520can%2520generate%2520human-like%2520dialogues%252C%2520yet%2520their%2520ability%2520to%2520simulate%2520early%250Achild-adult%2520interactions%2520remains%2520largely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520examined%250Ahow%2520effectively%2520LLMs%2520can%2520capture%2520the%2520distinctive%2520features%2520of%2520child-caregiver%250Alanguage%2520in%2520interaction%252C%2520using%2520both%2520static%2520and%2520interactive%2520benchmarking%250Amethods.%2520We%2520found%2520that%2520state-of-the-art%2520LLMs%2520like%2520Llama%25203%2520and%2520GPT-4o%2520can%250Aapproximate%2520child-caregiver%2520dialogues%2520at%2520the%2520word%2520and%2520utterance%2520level%252C%2520but%2520they%250Astruggle%2520to%2520reproduce%2520the%2520child%2520and%2520caregiver%2527s%2520discursive%2520patterns%252C%2520exaggerate%250Aalignment%252C%2520and%2520fail%2520to%2520reach%2520the%2520level%2520of%2520diversity%2520shown%2520by%2520humans.%2520The%250Abroader%2520goal%2520of%2520this%2520work%2520is%2520to%2520initiate%2520the%2520development%2520of%2520a%2520comprehensive%250Abenchmark%2520for%2520LLMs%2520in%2520child-oriented%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20LLMs%20for%20Mimicking%20Child-Caregiver%20Language%20in%20Interaction&entry.906535625=Jing%20Liu%20and%20Abdellah%20Fourtassi&entry.1292438233=%20%20LLMs%20can%20generate%20human-like%20dialogues%2C%20yet%20their%20ability%20to%20simulate%20early%0Achild-adult%20interactions%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20examined%0Ahow%20effectively%20LLMs%20can%20capture%20the%20distinctive%20features%20of%20child-caregiver%0Alanguage%20in%20interaction%2C%20using%20both%20static%20and%20interactive%20benchmarking%0Amethods.%20We%20found%20that%20state-of-the-art%20LLMs%20like%20Llama%203%20and%20GPT-4o%20can%0Aapproximate%20child-caregiver%20dialogues%20at%20the%20word%20and%20utterance%20level%2C%20but%20they%0Astruggle%20to%20reproduce%20the%20child%20and%20caregiver%27s%20discursive%20patterns%2C%20exaggerate%0Aalignment%2C%20and%20fail%20to%20reach%20the%20level%20of%20diversity%20shown%20by%20humans.%20The%0Abroader%20goal%20of%20this%20work%20is%20to%20initiate%20the%20development%20of%20a%20comprehensive%0Abenchmark%20for%20LLMs%20in%20child-oriented%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09318v1&entry.124074799=Read"},
{"title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs", "author": "Rong Wang and Kun Sun and Jonas Kuhn", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems.\n", "link": "http://arxiv.org/abs/2411.18564v2", "date": "2024-12-12", "relevancy": 2.199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dspy-based%20Neural-Symbolic%20Pipeline%20to%20Enhance%20Spatial%20Reasoning%20in%20LLMs&body=Title%3A%20Dspy-based%20Neural-Symbolic%20Pipeline%20to%20Enhance%20Spatial%20Reasoning%20in%20LLMs%0AAuthor%3A%20Rong%20Wang%20and%20Kun%20Sun%20and%20Jonas%20Kuhn%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20tasks%2C%20yet%20they%20often%20struggle%20with%20spatial%20reasoning.%20This%20paper%0Apresents%20a%20novel%20neural-symbolic%20framework%20that%20enhances%20LLMs%27%20spatial%0Areasoning%20abilities%20through%20iterative%20feedback%20between%20LLMs%20and%20Answer%20Set%0AProgramming%20%28ASP%29.%20We%20evaluate%20our%20approach%20on%20two%20benchmark%20datasets%3A%20StepGame%0Aand%20SparQA%2C%20implementing%20three%20distinct%20strategies%3A%20%281%29%20direct%20prompting%0Abaseline%2C%20%282%29%20Facts%2BRules%20prompting%2C%20and%20%283%29%20DSPy-based%20LLM%2BASP%20pipeline%20with%0Aiterative%20refinement.%20Our%20experimental%20results%20demonstrate%20that%20the%20LLM%2BASP%0Apipeline%20significantly%20outperforms%20baseline%20methods%2C%20achieving%20an%20average%2082%25%0Aaccuracy%20on%20StepGame%20and%2069%25%20on%20SparQA%2C%20marking%20improvements%20of%2040-50%25%20and%0A8-15%25%20respectively%20over%20direct%20prompting.%20The%20success%20stems%20from%20three%20key%0Ainnovations%3A%20%281%29%20effective%20separation%20of%20semantic%20parsing%20and%20logical%20reasoning%0Athrough%20a%20modular%20pipeline%2C%20%282%29%20iterative%20feedback%20mechanism%20between%20LLMs%20and%0AASP%20solvers%20that%20improves%20program%20rate%2C%20and%20%283%29%20robust%20error%20handling%20that%0Aaddresses%20parsing%2C%20grounding%2C%20and%20solving%20failures.%20Additionally%2C%20we%20propose%0AFacts%2BRules%20as%20a%20lightweight%20alternative%20that%20achieves%20comparable%20performance%0Aon%20complex%20SparQA%20dataset%2C%20while%20reducing%20computational%20overhead.Our%20analysis%0Aacross%20different%20LLM%20architectures%20%28Deepseek%2C%20Llama3-70B%2C%20GPT-4.0%20mini%29%0Ademonstrates%20the%20framework%27s%20generalizability%20and%20provides%20insights%20into%20the%0Atrade-offs%20between%20implementation%20complexity%20and%20reasoning%20capability%2C%0Acontributing%20to%20the%20development%20of%20more%20interpretable%20and%20reliable%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18564v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDspy-based%2520Neural-Symbolic%2520Pipeline%2520to%2520Enhance%2520Spatial%2520Reasoning%2520in%2520LLMs%26entry.906535625%3DRong%2520Wang%2520and%2520Kun%2520Sun%2520and%2520Jonas%2520Kuhn%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Avarious%2520tasks%252C%2520yet%2520they%2520often%2520struggle%2520with%2520spatial%2520reasoning.%2520This%2520paper%250Apresents%2520a%2520novel%2520neural-symbolic%2520framework%2520that%2520enhances%2520LLMs%2527%2520spatial%250Areasoning%2520abilities%2520through%2520iterative%2520feedback%2520between%2520LLMs%2520and%2520Answer%2520Set%250AProgramming%2520%2528ASP%2529.%2520We%2520evaluate%2520our%2520approach%2520on%2520two%2520benchmark%2520datasets%253A%2520StepGame%250Aand%2520SparQA%252C%2520implementing%2520three%2520distinct%2520strategies%253A%2520%25281%2529%2520direct%2520prompting%250Abaseline%252C%2520%25282%2529%2520Facts%252BRules%2520prompting%252C%2520and%2520%25283%2529%2520DSPy-based%2520LLM%252BASP%2520pipeline%2520with%250Aiterative%2520refinement.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520the%2520LLM%252BASP%250Apipeline%2520significantly%2520outperforms%2520baseline%2520methods%252C%2520achieving%2520an%2520average%252082%2525%250Aaccuracy%2520on%2520StepGame%2520and%252069%2525%2520on%2520SparQA%252C%2520marking%2520improvements%2520of%252040-50%2525%2520and%250A8-15%2525%2520respectively%2520over%2520direct%2520prompting.%2520The%2520success%2520stems%2520from%2520three%2520key%250Ainnovations%253A%2520%25281%2529%2520effective%2520separation%2520of%2520semantic%2520parsing%2520and%2520logical%2520reasoning%250Athrough%2520a%2520modular%2520pipeline%252C%2520%25282%2529%2520iterative%2520feedback%2520mechanism%2520between%2520LLMs%2520and%250AASP%2520solvers%2520that%2520improves%2520program%2520rate%252C%2520and%2520%25283%2529%2520robust%2520error%2520handling%2520that%250Aaddresses%2520parsing%252C%2520grounding%252C%2520and%2520solving%2520failures.%2520Additionally%252C%2520we%2520propose%250AFacts%252BRules%2520as%2520a%2520lightweight%2520alternative%2520that%2520achieves%2520comparable%2520performance%250Aon%2520complex%2520SparQA%2520dataset%252C%2520while%2520reducing%2520computational%2520overhead.Our%2520analysis%250Aacross%2520different%2520LLM%2520architectures%2520%2528Deepseek%252C%2520Llama3-70B%252C%2520GPT-4.0%2520mini%2529%250Ademonstrates%2520the%2520framework%2527s%2520generalizability%2520and%2520provides%2520insights%2520into%2520the%250Atrade-offs%2520between%2520implementation%2520complexity%2520and%2520reasoning%2520capability%252C%250Acontributing%2520to%2520the%2520development%2520of%2520more%2520interpretable%2520and%2520reliable%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18564v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dspy-based%20Neural-Symbolic%20Pipeline%20to%20Enhance%20Spatial%20Reasoning%20in%20LLMs&entry.906535625=Rong%20Wang%20and%20Kun%20Sun%20and%20Jonas%20Kuhn&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20tasks%2C%20yet%20they%20often%20struggle%20with%20spatial%20reasoning.%20This%20paper%0Apresents%20a%20novel%20neural-symbolic%20framework%20that%20enhances%20LLMs%27%20spatial%0Areasoning%20abilities%20through%20iterative%20feedback%20between%20LLMs%20and%20Answer%20Set%0AProgramming%20%28ASP%29.%20We%20evaluate%20our%20approach%20on%20two%20benchmark%20datasets%3A%20StepGame%0Aand%20SparQA%2C%20implementing%20three%20distinct%20strategies%3A%20%281%29%20direct%20prompting%0Abaseline%2C%20%282%29%20Facts%2BRules%20prompting%2C%20and%20%283%29%20DSPy-based%20LLM%2BASP%20pipeline%20with%0Aiterative%20refinement.%20Our%20experimental%20results%20demonstrate%20that%20the%20LLM%2BASP%0Apipeline%20significantly%20outperforms%20baseline%20methods%2C%20achieving%20an%20average%2082%25%0Aaccuracy%20on%20StepGame%20and%2069%25%20on%20SparQA%2C%20marking%20improvements%20of%2040-50%25%20and%0A8-15%25%20respectively%20over%20direct%20prompting.%20The%20success%20stems%20from%20three%20key%0Ainnovations%3A%20%281%29%20effective%20separation%20of%20semantic%20parsing%20and%20logical%20reasoning%0Athrough%20a%20modular%20pipeline%2C%20%282%29%20iterative%20feedback%20mechanism%20between%20LLMs%20and%0AASP%20solvers%20that%20improves%20program%20rate%2C%20and%20%283%29%20robust%20error%20handling%20that%0Aaddresses%20parsing%2C%20grounding%2C%20and%20solving%20failures.%20Additionally%2C%20we%20propose%0AFacts%2BRules%20as%20a%20lightweight%20alternative%20that%20achieves%20comparable%20performance%0Aon%20complex%20SparQA%20dataset%2C%20while%20reducing%20computational%20overhead.Our%20analysis%0Aacross%20different%20LLM%20architectures%20%28Deepseek%2C%20Llama3-70B%2C%20GPT-4.0%20mini%29%0Ademonstrates%20the%20framework%27s%20generalizability%20and%20provides%20insights%20into%20the%0Atrade-offs%20between%20implementation%20complexity%20and%20reasoning%20capability%2C%0Acontributing%20to%20the%20development%20of%20more%20interpretable%20and%20reliable%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18564v2&entry.124074799=Read"},
{"title": "MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple\n  Object Tracking", "author": "Mufeng Yao and Jinlong Peng and Qingdong He and Bo Peng and Hao Chen and Mingmin Chi and Chao Liu and Jon Atli Benediktsson", "abstract": "  Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms\nrequires efficient motion modeling. This is because UAV-MOT faces both local\nobject motion and global camera motion. Motion blur also increases the\ndifficulty of detecting large moving objects. Previous UAV motion modeling\napproaches either focus only on local motion or ignore motion blurring effects,\nthus limiting their tracking performance and speed. To address these issues, we\npropose the Motion Mamba Module, which explores both local and global motion\nfeatures through cross-correlation and bi-directional Mamba Modules for better\nmotion modeling. To address the detection difficulties caused by motion blur,\nwe also design motion margin loss to effectively improve the detection accuracy\nof motion blurred objects. Based on the Motion Mamba module and motion margin\nloss, our proposed MM-Tracker surpasses the state-of-the-art in two widely\nopen-source UAV-MOT datasets. Code will be available.\n", "link": "http://arxiv.org/abs/2407.10485v3", "date": "2024-12-12", "relevancy": 2.1856, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5794}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5421}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Tracker%3A%20Motion%20Mamba%20with%20Margin%20Loss%20for%20UAV-platform%20Multiple%0A%20%20Object%20Tracking&body=Title%3A%20MM-Tracker%3A%20Motion%20Mamba%20with%20Margin%20Loss%20for%20UAV-platform%20Multiple%0A%20%20Object%20Tracking%0AAuthor%3A%20Mufeng%20Yao%20and%20Jinlong%20Peng%20and%20Qingdong%20He%20and%20Bo%20Peng%20and%20Hao%20Chen%20and%20Mingmin%20Chi%20and%20Chao%20Liu%20and%20Jon%20Atli%20Benediktsson%0AAbstract%3A%20%20%20Multiple%20object%20tracking%20%28MOT%29%20from%20unmanned%20aerial%20vehicle%20%28UAV%29%20platforms%0Arequires%20efficient%20motion%20modeling.%20This%20is%20because%20UAV-MOT%20faces%20both%20local%0Aobject%20motion%20and%20global%20camera%20motion.%20Motion%20blur%20also%20increases%20the%0Adifficulty%20of%20detecting%20large%20moving%20objects.%20Previous%20UAV%20motion%20modeling%0Aapproaches%20either%20focus%20only%20on%20local%20motion%20or%20ignore%20motion%20blurring%20effects%2C%0Athus%20limiting%20their%20tracking%20performance%20and%20speed.%20To%20address%20these%20issues%2C%20we%0Apropose%20the%20Motion%20Mamba%20Module%2C%20which%20explores%20both%20local%20and%20global%20motion%0Afeatures%20through%20cross-correlation%20and%20bi-directional%20Mamba%20Modules%20for%20better%0Amotion%20modeling.%20To%20address%20the%20detection%20difficulties%20caused%20by%20motion%20blur%2C%0Awe%20also%20design%20motion%20margin%20loss%20to%20effectively%20improve%20the%20detection%20accuracy%0Aof%20motion%20blurred%20objects.%20Based%20on%20the%20Motion%20Mamba%20module%20and%20motion%20margin%0Aloss%2C%20our%20proposed%20MM-Tracker%20surpasses%20the%20state-of-the-art%20in%20two%20widely%0Aopen-source%20UAV-MOT%20datasets.%20Code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Tracker%253A%2520Motion%2520Mamba%2520with%2520Margin%2520Loss%2520for%2520UAV-platform%2520Multiple%250A%2520%2520Object%2520Tracking%26entry.906535625%3DMufeng%2520Yao%2520and%2520Jinlong%2520Peng%2520and%2520Qingdong%2520He%2520and%2520Bo%2520Peng%2520and%2520Hao%2520Chen%2520and%2520Mingmin%2520Chi%2520and%2520Chao%2520Liu%2520and%2520Jon%2520Atli%2520Benediktsson%26entry.1292438233%3D%2520%2520Multiple%2520object%2520tracking%2520%2528MOT%2529%2520from%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520platforms%250Arequires%2520efficient%2520motion%2520modeling.%2520This%2520is%2520because%2520UAV-MOT%2520faces%2520both%2520local%250Aobject%2520motion%2520and%2520global%2520camera%2520motion.%2520Motion%2520blur%2520also%2520increases%2520the%250Adifficulty%2520of%2520detecting%2520large%2520moving%2520objects.%2520Previous%2520UAV%2520motion%2520modeling%250Aapproaches%2520either%2520focus%2520only%2520on%2520local%2520motion%2520or%2520ignore%2520motion%2520blurring%2520effects%252C%250Athus%2520limiting%2520their%2520tracking%2520performance%2520and%2520speed.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520the%2520Motion%2520Mamba%2520Module%252C%2520which%2520explores%2520both%2520local%2520and%2520global%2520motion%250Afeatures%2520through%2520cross-correlation%2520and%2520bi-directional%2520Mamba%2520Modules%2520for%2520better%250Amotion%2520modeling.%2520To%2520address%2520the%2520detection%2520difficulties%2520caused%2520by%2520motion%2520blur%252C%250Awe%2520also%2520design%2520motion%2520margin%2520loss%2520to%2520effectively%2520improve%2520the%2520detection%2520accuracy%250Aof%2520motion%2520blurred%2520objects.%2520Based%2520on%2520the%2520Motion%2520Mamba%2520module%2520and%2520motion%2520margin%250Aloss%252C%2520our%2520proposed%2520MM-Tracker%2520surpasses%2520the%2520state-of-the-art%2520in%2520two%2520widely%250Aopen-source%2520UAV-MOT%2520datasets.%2520Code%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Tracker%3A%20Motion%20Mamba%20with%20Margin%20Loss%20for%20UAV-platform%20Multiple%0A%20%20Object%20Tracking&entry.906535625=Mufeng%20Yao%20and%20Jinlong%20Peng%20and%20Qingdong%20He%20and%20Bo%20Peng%20and%20Hao%20Chen%20and%20Mingmin%20Chi%20and%20Chao%20Liu%20and%20Jon%20Atli%20Benediktsson&entry.1292438233=%20%20Multiple%20object%20tracking%20%28MOT%29%20from%20unmanned%20aerial%20vehicle%20%28UAV%29%20platforms%0Arequires%20efficient%20motion%20modeling.%20This%20is%20because%20UAV-MOT%20faces%20both%20local%0Aobject%20motion%20and%20global%20camera%20motion.%20Motion%20blur%20also%20increases%20the%0Adifficulty%20of%20detecting%20large%20moving%20objects.%20Previous%20UAV%20motion%20modeling%0Aapproaches%20either%20focus%20only%20on%20local%20motion%20or%20ignore%20motion%20blurring%20effects%2C%0Athus%20limiting%20their%20tracking%20performance%20and%20speed.%20To%20address%20these%20issues%2C%20we%0Apropose%20the%20Motion%20Mamba%20Module%2C%20which%20explores%20both%20local%20and%20global%20motion%0Afeatures%20through%20cross-correlation%20and%20bi-directional%20Mamba%20Modules%20for%20better%0Amotion%20modeling.%20To%20address%20the%20detection%20difficulties%20caused%20by%20motion%20blur%2C%0Awe%20also%20design%20motion%20margin%20loss%20to%20effectively%20improve%20the%20detection%20accuracy%0Aof%20motion%20blurred%20objects.%20Based%20on%20the%20Motion%20Mamba%20module%20and%20motion%20margin%0Aloss%2C%20our%20proposed%20MM-Tracker%20surpasses%20the%20state-of-the-art%20in%20two%20widely%0Aopen-source%20UAV-MOT%20datasets.%20Code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10485v3&entry.124074799=Read"},
{"title": "Does Representation Matter? Exploring Intermediate Layers in Large\n  Language Models", "author": "Oscar Skean and Md Rifat Arefin and Yann LeCun and Ravid Shwartz-Ziv", "abstract": "  Understanding what defines a good representation in large language models\n(LLMs) is fundamental to both theoretical understanding and practical\napplications. In this paper, we investigate the quality of intermediate\nrepresentations in various LLM architectures, including Transformers and State\nSpace Models (SSMs). We find that intermediate layers often yield more\ninformative representations for downstream tasks than the final layers. To\nmeasure the representation quality, we adapt and apply a suite of metrics -\nsuch as prompt entropy, curvature, and augmentation-invariance - originally\nproposed in other contexts. Our empirical study reveals significant\narchitectural differences, how representations evolve throughout training, and\nhow factors like input randomness and prompt length affect each layer. Notably,\nwe observe a bimodal pattern in the entropy of some intermediate layers and\nconsider potential explanations tied to training data. Overall, our results\nilluminate the internal mechanics of LLMs and guide strategies for\narchitectural optimization and training.\n", "link": "http://arxiv.org/abs/2412.09563v1", "date": "2024-12-12", "relevancy": 2.1612, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Representation%20Matter%3F%20Exploring%20Intermediate%20Layers%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Does%20Representation%20Matter%3F%20Exploring%20Intermediate%20Layers%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Oscar%20Skean%20and%20Md%20Rifat%20Arefin%20and%20Yann%20LeCun%20and%20Ravid%20Shwartz-Ziv%0AAbstract%3A%20%20%20Understanding%20what%20defines%20a%20good%20representation%20in%20large%20language%20models%0A%28LLMs%29%20is%20fundamental%20to%20both%20theoretical%20understanding%20and%20practical%0Aapplications.%20In%20this%20paper%2C%20we%20investigate%20the%20quality%20of%20intermediate%0Arepresentations%20in%20various%20LLM%20architectures%2C%20including%20Transformers%20and%20State%0ASpace%20Models%20%28SSMs%29.%20We%20find%20that%20intermediate%20layers%20often%20yield%20more%0Ainformative%20representations%20for%20downstream%20tasks%20than%20the%20final%20layers.%20To%0Ameasure%20the%20representation%20quality%2C%20we%20adapt%20and%20apply%20a%20suite%20of%20metrics%20-%0Asuch%20as%20prompt%20entropy%2C%20curvature%2C%20and%20augmentation-invariance%20-%20originally%0Aproposed%20in%20other%20contexts.%20Our%20empirical%20study%20reveals%20significant%0Aarchitectural%20differences%2C%20how%20representations%20evolve%20throughout%20training%2C%20and%0Ahow%20factors%20like%20input%20randomness%20and%20prompt%20length%20affect%20each%20layer.%20Notably%2C%0Awe%20observe%20a%20bimodal%20pattern%20in%20the%20entropy%20of%20some%20intermediate%20layers%20and%0Aconsider%20potential%20explanations%20tied%20to%20training%20data.%20Overall%2C%20our%20results%0Ailluminate%20the%20internal%20mechanics%20of%20LLMs%20and%20guide%20strategies%20for%0Aarchitectural%20optimization%20and%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Representation%2520Matter%253F%2520Exploring%2520Intermediate%2520Layers%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DOscar%2520Skean%2520and%2520Md%2520Rifat%2520Arefin%2520and%2520Yann%2520LeCun%2520and%2520Ravid%2520Shwartz-Ziv%26entry.1292438233%3D%2520%2520Understanding%2520what%2520defines%2520a%2520good%2520representation%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%2520is%2520fundamental%2520to%2520both%2520theoretical%2520understanding%2520and%2520practical%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520quality%2520of%2520intermediate%250Arepresentations%2520in%2520various%2520LLM%2520architectures%252C%2520including%2520Transformers%2520and%2520State%250ASpace%2520Models%2520%2528SSMs%2529.%2520We%2520find%2520that%2520intermediate%2520layers%2520often%2520yield%2520more%250Ainformative%2520representations%2520for%2520downstream%2520tasks%2520than%2520the%2520final%2520layers.%2520To%250Ameasure%2520the%2520representation%2520quality%252C%2520we%2520adapt%2520and%2520apply%2520a%2520suite%2520of%2520metrics%2520-%250Asuch%2520as%2520prompt%2520entropy%252C%2520curvature%252C%2520and%2520augmentation-invariance%2520-%2520originally%250Aproposed%2520in%2520other%2520contexts.%2520Our%2520empirical%2520study%2520reveals%2520significant%250Aarchitectural%2520differences%252C%2520how%2520representations%2520evolve%2520throughout%2520training%252C%2520and%250Ahow%2520factors%2520like%2520input%2520randomness%2520and%2520prompt%2520length%2520affect%2520each%2520layer.%2520Notably%252C%250Awe%2520observe%2520a%2520bimodal%2520pattern%2520in%2520the%2520entropy%2520of%2520some%2520intermediate%2520layers%2520and%250Aconsider%2520potential%2520explanations%2520tied%2520to%2520training%2520data.%2520Overall%252C%2520our%2520results%250Ailluminate%2520the%2520internal%2520mechanics%2520of%2520LLMs%2520and%2520guide%2520strategies%2520for%250Aarchitectural%2520optimization%2520and%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Representation%20Matter%3F%20Exploring%20Intermediate%20Layers%20in%20Large%0A%20%20Language%20Models&entry.906535625=Oscar%20Skean%20and%20Md%20Rifat%20Arefin%20and%20Yann%20LeCun%20and%20Ravid%20Shwartz-Ziv&entry.1292438233=%20%20Understanding%20what%20defines%20a%20good%20representation%20in%20large%20language%20models%0A%28LLMs%29%20is%20fundamental%20to%20both%20theoretical%20understanding%20and%20practical%0Aapplications.%20In%20this%20paper%2C%20we%20investigate%20the%20quality%20of%20intermediate%0Arepresentations%20in%20various%20LLM%20architectures%2C%20including%20Transformers%20and%20State%0ASpace%20Models%20%28SSMs%29.%20We%20find%20that%20intermediate%20layers%20often%20yield%20more%0Ainformative%20representations%20for%20downstream%20tasks%20than%20the%20final%20layers.%20To%0Ameasure%20the%20representation%20quality%2C%20we%20adapt%20and%20apply%20a%20suite%20of%20metrics%20-%0Asuch%20as%20prompt%20entropy%2C%20curvature%2C%20and%20augmentation-invariance%20-%20originally%0Aproposed%20in%20other%20contexts.%20Our%20empirical%20study%20reveals%20significant%0Aarchitectural%20differences%2C%20how%20representations%20evolve%20throughout%20training%2C%20and%0Ahow%20factors%20like%20input%20randomness%20and%20prompt%20length%20affect%20each%20layer.%20Notably%2C%0Awe%20observe%20a%20bimodal%20pattern%20in%20the%20entropy%20of%20some%20intermediate%20layers%20and%0Aconsider%20potential%20explanations%20tied%20to%20training%20data.%20Overall%2C%20our%20results%0Ailluminate%20the%20internal%20mechanics%20of%20LLMs%20and%20guide%20strategies%20for%0Aarchitectural%20optimization%20and%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09563v1&entry.124074799=Read"},
{"title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them\n  Optimally", "author": "Manon Verbockhaven and Sylvain Chevallier and Guillaume Charpiat and Th\u00e9o Rudkiewicz", "abstract": "  Machine learning tasks are generally formulated as optimization problems,\nwhere one searches for an optimal function within a certain functional space.\nIn practice, parameterized functional spaces are considered, in order to be\nable to perform gradient descent. Typically, a neural network architecture is\nchosen and fixed, and its parameters (connection weights) are optimized,\nyielding an architecture-dependent result. This way of proceeding however\nforces the evolution of the function during training to lie within the realm of\nwhat is expressible with the chosen architecture, and prevents any optimization\nacross architectures. Costly architectural hyper-parameter optimization is\noften performed to compensate for this. Instead, we propose to adapt the\narchitecture on the fly during training. We show that the information about\ndesirable architectural changes, due to expressivity bottlenecks when\nattempting to follow the functional gradient, can be extracted from\nbackpropagation. To do this, we propose a mathematical definition of\nexpressivity bottlenecks, which enables us to detect, quantify and solve them\nwhile training, by adding suitable neurons. Thus, while the standard approach\nrequires large networks, in terms of number of neurons per layer, for\nexpressivity and optimization reasons, we provide tools and properties to\ndevelop an architecture starting with a very small number of neurons. As a\nproof of concept, we show results~on the CIFAR dataset, matching large neural\nnetwork accuracy, with competitive training time, while removing the need for\nstandard architectural hyper-parameter search.\n", "link": "http://arxiv.org/abs/2405.19816v2", "date": "2024-12-12", "relevancy": 2.1562, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5659}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5221}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Growing%20Tiny%20Networks%3A%20Spotting%20Expressivity%20Bottlenecks%20and%20Fixing%20Them%0A%20%20Optimally&body=Title%3A%20Growing%20Tiny%20Networks%3A%20Spotting%20Expressivity%20Bottlenecks%20and%20Fixing%20Them%0A%20%20Optimally%0AAuthor%3A%20Manon%20Verbockhaven%20and%20Sylvain%20Chevallier%20and%20Guillaume%20Charpiat%20and%20Th%C3%A9o%20Rudkiewicz%0AAbstract%3A%20%20%20Machine%20learning%20tasks%20are%20generally%20formulated%20as%20optimization%20problems%2C%0Awhere%20one%20searches%20for%20an%20optimal%20function%20within%20a%20certain%20functional%20space.%0AIn%20practice%2C%20parameterized%20functional%20spaces%20are%20considered%2C%20in%20order%20to%20be%0Aable%20to%20perform%20gradient%20descent.%20Typically%2C%20a%20neural%20network%20architecture%20is%0Achosen%20and%20fixed%2C%20and%20its%20parameters%20%28connection%20weights%29%20are%20optimized%2C%0Ayielding%20an%20architecture-dependent%20result.%20This%20way%20of%20proceeding%20however%0Aforces%20the%20evolution%20of%20the%20function%20during%20training%20to%20lie%20within%20the%20realm%20of%0Awhat%20is%20expressible%20with%20the%20chosen%20architecture%2C%20and%20prevents%20any%20optimization%0Aacross%20architectures.%20Costly%20architectural%20hyper-parameter%20optimization%20is%0Aoften%20performed%20to%20compensate%20for%20this.%20Instead%2C%20we%20propose%20to%20adapt%20the%0Aarchitecture%20on%20the%20fly%20during%20training.%20We%20show%20that%20the%20information%20about%0Adesirable%20architectural%20changes%2C%20due%20to%20expressivity%20bottlenecks%20when%0Aattempting%20to%20follow%20the%20functional%20gradient%2C%20can%20be%20extracted%20from%0Abackpropagation.%20To%20do%20this%2C%20we%20propose%20a%20mathematical%20definition%20of%0Aexpressivity%20bottlenecks%2C%20which%20enables%20us%20to%20detect%2C%20quantify%20and%20solve%20them%0Awhile%20training%2C%20by%20adding%20suitable%20neurons.%20Thus%2C%20while%20the%20standard%20approach%0Arequires%20large%20networks%2C%20in%20terms%20of%20number%20of%20neurons%20per%20layer%2C%20for%0Aexpressivity%20and%20optimization%20reasons%2C%20we%20provide%20tools%20and%20properties%20to%0Adevelop%20an%20architecture%20starting%20with%20a%20very%20small%20number%20of%20neurons.%20As%20a%0Aproof%20of%20concept%2C%20we%20show%20results~on%20the%20CIFAR%20dataset%2C%20matching%20large%20neural%0Anetwork%20accuracy%2C%20with%20competitive%20training%20time%2C%20while%20removing%20the%20need%20for%0Astandard%20architectural%20hyper-parameter%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrowing%2520Tiny%2520Networks%253A%2520Spotting%2520Expressivity%2520Bottlenecks%2520and%2520Fixing%2520Them%250A%2520%2520Optimally%26entry.906535625%3DManon%2520Verbockhaven%2520and%2520Sylvain%2520Chevallier%2520and%2520Guillaume%2520Charpiat%2520and%2520Th%25C3%25A9o%2520Rudkiewicz%26entry.1292438233%3D%2520%2520Machine%2520learning%2520tasks%2520are%2520generally%2520formulated%2520as%2520optimization%2520problems%252C%250Awhere%2520one%2520searches%2520for%2520an%2520optimal%2520function%2520within%2520a%2520certain%2520functional%2520space.%250AIn%2520practice%252C%2520parameterized%2520functional%2520spaces%2520are%2520considered%252C%2520in%2520order%2520to%2520be%250Aable%2520to%2520perform%2520gradient%2520descent.%2520Typically%252C%2520a%2520neural%2520network%2520architecture%2520is%250Achosen%2520and%2520fixed%252C%2520and%2520its%2520parameters%2520%2528connection%2520weights%2529%2520are%2520optimized%252C%250Ayielding%2520an%2520architecture-dependent%2520result.%2520This%2520way%2520of%2520proceeding%2520however%250Aforces%2520the%2520evolution%2520of%2520the%2520function%2520during%2520training%2520to%2520lie%2520within%2520the%2520realm%2520of%250Awhat%2520is%2520expressible%2520with%2520the%2520chosen%2520architecture%252C%2520and%2520prevents%2520any%2520optimization%250Aacross%2520architectures.%2520Costly%2520architectural%2520hyper-parameter%2520optimization%2520is%250Aoften%2520performed%2520to%2520compensate%2520for%2520this.%2520Instead%252C%2520we%2520propose%2520to%2520adapt%2520the%250Aarchitecture%2520on%2520the%2520fly%2520during%2520training.%2520We%2520show%2520that%2520the%2520information%2520about%250Adesirable%2520architectural%2520changes%252C%2520due%2520to%2520expressivity%2520bottlenecks%2520when%250Aattempting%2520to%2520follow%2520the%2520functional%2520gradient%252C%2520can%2520be%2520extracted%2520from%250Abackpropagation.%2520To%2520do%2520this%252C%2520we%2520propose%2520a%2520mathematical%2520definition%2520of%250Aexpressivity%2520bottlenecks%252C%2520which%2520enables%2520us%2520to%2520detect%252C%2520quantify%2520and%2520solve%2520them%250Awhile%2520training%252C%2520by%2520adding%2520suitable%2520neurons.%2520Thus%252C%2520while%2520the%2520standard%2520approach%250Arequires%2520large%2520networks%252C%2520in%2520terms%2520of%2520number%2520of%2520neurons%2520per%2520layer%252C%2520for%250Aexpressivity%2520and%2520optimization%2520reasons%252C%2520we%2520provide%2520tools%2520and%2520properties%2520to%250Adevelop%2520an%2520architecture%2520starting%2520with%2520a%2520very%2520small%2520number%2520of%2520neurons.%2520As%2520a%250Aproof%2520of%2520concept%252C%2520we%2520show%2520results~on%2520the%2520CIFAR%2520dataset%252C%2520matching%2520large%2520neural%250Anetwork%2520accuracy%252C%2520with%2520competitive%2520training%2520time%252C%2520while%2520removing%2520the%2520need%2520for%250Astandard%2520architectural%2520hyper-parameter%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Growing%20Tiny%20Networks%3A%20Spotting%20Expressivity%20Bottlenecks%20and%20Fixing%20Them%0A%20%20Optimally&entry.906535625=Manon%20Verbockhaven%20and%20Sylvain%20Chevallier%20and%20Guillaume%20Charpiat%20and%20Th%C3%A9o%20Rudkiewicz&entry.1292438233=%20%20Machine%20learning%20tasks%20are%20generally%20formulated%20as%20optimization%20problems%2C%0Awhere%20one%20searches%20for%20an%20optimal%20function%20within%20a%20certain%20functional%20space.%0AIn%20practice%2C%20parameterized%20functional%20spaces%20are%20considered%2C%20in%20order%20to%20be%0Aable%20to%20perform%20gradient%20descent.%20Typically%2C%20a%20neural%20network%20architecture%20is%0Achosen%20and%20fixed%2C%20and%20its%20parameters%20%28connection%20weights%29%20are%20optimized%2C%0Ayielding%20an%20architecture-dependent%20result.%20This%20way%20of%20proceeding%20however%0Aforces%20the%20evolution%20of%20the%20function%20during%20training%20to%20lie%20within%20the%20realm%20of%0Awhat%20is%20expressible%20with%20the%20chosen%20architecture%2C%20and%20prevents%20any%20optimization%0Aacross%20architectures.%20Costly%20architectural%20hyper-parameter%20optimization%20is%0Aoften%20performed%20to%20compensate%20for%20this.%20Instead%2C%20we%20propose%20to%20adapt%20the%0Aarchitecture%20on%20the%20fly%20during%20training.%20We%20show%20that%20the%20information%20about%0Adesirable%20architectural%20changes%2C%20due%20to%20expressivity%20bottlenecks%20when%0Aattempting%20to%20follow%20the%20functional%20gradient%2C%20can%20be%20extracted%20from%0Abackpropagation.%20To%20do%20this%2C%20we%20propose%20a%20mathematical%20definition%20of%0Aexpressivity%20bottlenecks%2C%20which%20enables%20us%20to%20detect%2C%20quantify%20and%20solve%20them%0Awhile%20training%2C%20by%20adding%20suitable%20neurons.%20Thus%2C%20while%20the%20standard%20approach%0Arequires%20large%20networks%2C%20in%20terms%20of%20number%20of%20neurons%20per%20layer%2C%20for%0Aexpressivity%20and%20optimization%20reasons%2C%20we%20provide%20tools%20and%20properties%20to%0Adevelop%20an%20architecture%20starting%20with%20a%20very%20small%20number%20of%20neurons.%20As%20a%0Aproof%20of%20concept%2C%20we%20show%20results~on%20the%20CIFAR%20dataset%2C%20matching%20large%20neural%0Anetwork%20accuracy%2C%20with%20competitive%20training%20time%2C%20while%20removing%20the%20need%20for%0Astandard%20architectural%20hyper-parameter%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19816v2&entry.124074799=Read"},
{"title": "Agent-based Video Trimming", "author": "Lingfeng Yang and Zhenyuan Chen and Xiang Li and Peiyang Jia and Liangqu Long and Jian Yang", "abstract": "  As information becomes more accessible, user-generated videos are increasing\nin length, placing a burden on viewers to sift through vast content for\nvaluable insights. This trend underscores the need for an algorithm to extract\nkey video information efficiently. Despite significant advancements in\nhighlight detection, moment retrieval, and video summarization, current\napproaches primarily focus on selecting specific time intervals, often\noverlooking the relevance between segments and the potential for segment\narranging. In this paper, we introduce a novel task called Video Trimming (VT),\nwhich focuses on detecting wasted footage, selecting valuable segments, and\ncomposing them into a final video with a coherent story. To address this task,\nwe propose Agent-based Video Trimming (AVT), structured into three phases:\nVideo Structuring, Clip Filtering, and Story Composition. Specifically, we\nemploy a Video Captioning Agent to convert video slices into structured textual\ndescriptions, a Filtering Module to dynamically discard low-quality footage\nbased on the structured information of each clip, and a Video Arrangement Agent\nto select and compile valid clips into a coherent final narrative. For\nevaluation, we develop a Video Evaluation Agent to assess trimmed videos,\nconducting assessments in parallel with human evaluations. Additionally, we\ncurate a new benchmark dataset for video trimming using raw user videos from\nthe internet. As a result, AVT received more favorable evaluations in user\nstudies and demonstrated superior mAP and precision on the YouTube Highlights,\nTVSum, and our own dataset for the highlight detection task. The code and\nmodels are available at https://ylingfeng.github.io/AVT.\n", "link": "http://arxiv.org/abs/2412.09513v1", "date": "2024-12-12", "relevancy": 2.1413, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5629}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5412}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent-based%20Video%20Trimming&body=Title%3A%20Agent-based%20Video%20Trimming%0AAuthor%3A%20Lingfeng%20Yang%20and%20Zhenyuan%20Chen%20and%20Xiang%20Li%20and%20Peiyang%20Jia%20and%20Liangqu%20Long%20and%20Jian%20Yang%0AAbstract%3A%20%20%20As%20information%20becomes%20more%20accessible%2C%20user-generated%20videos%20are%20increasing%0Ain%20length%2C%20placing%20a%20burden%20on%20viewers%20to%20sift%20through%20vast%20content%20for%0Avaluable%20insights.%20This%20trend%20underscores%20the%20need%20for%20an%20algorithm%20to%20extract%0Akey%20video%20information%20efficiently.%20Despite%20significant%20advancements%20in%0Ahighlight%20detection%2C%20moment%20retrieval%2C%20and%20video%20summarization%2C%20current%0Aapproaches%20primarily%20focus%20on%20selecting%20specific%20time%20intervals%2C%20often%0Aoverlooking%20the%20relevance%20between%20segments%20and%20the%20potential%20for%20segment%0Aarranging.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20task%20called%20Video%20Trimming%20%28VT%29%2C%0Awhich%20focuses%20on%20detecting%20wasted%20footage%2C%20selecting%20valuable%20segments%2C%20and%0Acomposing%20them%20into%20a%20final%20video%20with%20a%20coherent%20story.%20To%20address%20this%20task%2C%0Awe%20propose%20Agent-based%20Video%20Trimming%20%28AVT%29%2C%20structured%20into%20three%20phases%3A%0AVideo%20Structuring%2C%20Clip%20Filtering%2C%20and%20Story%20Composition.%20Specifically%2C%20we%0Aemploy%20a%20Video%20Captioning%20Agent%20to%20convert%20video%20slices%20into%20structured%20textual%0Adescriptions%2C%20a%20Filtering%20Module%20to%20dynamically%20discard%20low-quality%20footage%0Abased%20on%20the%20structured%20information%20of%20each%20clip%2C%20and%20a%20Video%20Arrangement%20Agent%0Ato%20select%20and%20compile%20valid%20clips%20into%20a%20coherent%20final%20narrative.%20For%0Aevaluation%2C%20we%20develop%20a%20Video%20Evaluation%20Agent%20to%20assess%20trimmed%20videos%2C%0Aconducting%20assessments%20in%20parallel%20with%20human%20evaluations.%20Additionally%2C%20we%0Acurate%20a%20new%20benchmark%20dataset%20for%20video%20trimming%20using%20raw%20user%20videos%20from%0Athe%20internet.%20As%20a%20result%2C%20AVT%20received%20more%20favorable%20evaluations%20in%20user%0Astudies%20and%20demonstrated%20superior%20mAP%20and%20precision%20on%20the%20YouTube%20Highlights%2C%0ATVSum%2C%20and%20our%20own%20dataset%20for%20the%20highlight%20detection%20task.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//ylingfeng.github.io/AVT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent-based%2520Video%2520Trimming%26entry.906535625%3DLingfeng%2520Yang%2520and%2520Zhenyuan%2520Chen%2520and%2520Xiang%2520Li%2520and%2520Peiyang%2520Jia%2520and%2520Liangqu%2520Long%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520As%2520information%2520becomes%2520more%2520accessible%252C%2520user-generated%2520videos%2520are%2520increasing%250Ain%2520length%252C%2520placing%2520a%2520burden%2520on%2520viewers%2520to%2520sift%2520through%2520vast%2520content%2520for%250Avaluable%2520insights.%2520This%2520trend%2520underscores%2520the%2520need%2520for%2520an%2520algorithm%2520to%2520extract%250Akey%2520video%2520information%2520efficiently.%2520Despite%2520significant%2520advancements%2520in%250Ahighlight%2520detection%252C%2520moment%2520retrieval%252C%2520and%2520video%2520summarization%252C%2520current%250Aapproaches%2520primarily%2520focus%2520on%2520selecting%2520specific%2520time%2520intervals%252C%2520often%250Aoverlooking%2520the%2520relevance%2520between%2520segments%2520and%2520the%2520potential%2520for%2520segment%250Aarranging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520task%2520called%2520Video%2520Trimming%2520%2528VT%2529%252C%250Awhich%2520focuses%2520on%2520detecting%2520wasted%2520footage%252C%2520selecting%2520valuable%2520segments%252C%2520and%250Acomposing%2520them%2520into%2520a%2520final%2520video%2520with%2520a%2520coherent%2520story.%2520To%2520address%2520this%2520task%252C%250Awe%2520propose%2520Agent-based%2520Video%2520Trimming%2520%2528AVT%2529%252C%2520structured%2520into%2520three%2520phases%253A%250AVideo%2520Structuring%252C%2520Clip%2520Filtering%252C%2520and%2520Story%2520Composition.%2520Specifically%252C%2520we%250Aemploy%2520a%2520Video%2520Captioning%2520Agent%2520to%2520convert%2520video%2520slices%2520into%2520structured%2520textual%250Adescriptions%252C%2520a%2520Filtering%2520Module%2520to%2520dynamically%2520discard%2520low-quality%2520footage%250Abased%2520on%2520the%2520structured%2520information%2520of%2520each%2520clip%252C%2520and%2520a%2520Video%2520Arrangement%2520Agent%250Ato%2520select%2520and%2520compile%2520valid%2520clips%2520into%2520a%2520coherent%2520final%2520narrative.%2520For%250Aevaluation%252C%2520we%2520develop%2520a%2520Video%2520Evaluation%2520Agent%2520to%2520assess%2520trimmed%2520videos%252C%250Aconducting%2520assessments%2520in%2520parallel%2520with%2520human%2520evaluations.%2520Additionally%252C%2520we%250Acurate%2520a%2520new%2520benchmark%2520dataset%2520for%2520video%2520trimming%2520using%2520raw%2520user%2520videos%2520from%250Athe%2520internet.%2520As%2520a%2520result%252C%2520AVT%2520received%2520more%2520favorable%2520evaluations%2520in%2520user%250Astudies%2520and%2520demonstrated%2520superior%2520mAP%2520and%2520precision%2520on%2520the%2520YouTube%2520Highlights%252C%250ATVSum%252C%2520and%2520our%2520own%2520dataset%2520for%2520the%2520highlight%2520detection%2520task.%2520The%2520code%2520and%250Amodels%2520are%2520available%2520at%2520https%253A//ylingfeng.github.io/AVT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent-based%20Video%20Trimming&entry.906535625=Lingfeng%20Yang%20and%20Zhenyuan%20Chen%20and%20Xiang%20Li%20and%20Peiyang%20Jia%20and%20Liangqu%20Long%20and%20Jian%20Yang&entry.1292438233=%20%20As%20information%20becomes%20more%20accessible%2C%20user-generated%20videos%20are%20increasing%0Ain%20length%2C%20placing%20a%20burden%20on%20viewers%20to%20sift%20through%20vast%20content%20for%0Avaluable%20insights.%20This%20trend%20underscores%20the%20need%20for%20an%20algorithm%20to%20extract%0Akey%20video%20information%20efficiently.%20Despite%20significant%20advancements%20in%0Ahighlight%20detection%2C%20moment%20retrieval%2C%20and%20video%20summarization%2C%20current%0Aapproaches%20primarily%20focus%20on%20selecting%20specific%20time%20intervals%2C%20often%0Aoverlooking%20the%20relevance%20between%20segments%20and%20the%20potential%20for%20segment%0Aarranging.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20task%20called%20Video%20Trimming%20%28VT%29%2C%0Awhich%20focuses%20on%20detecting%20wasted%20footage%2C%20selecting%20valuable%20segments%2C%20and%0Acomposing%20them%20into%20a%20final%20video%20with%20a%20coherent%20story.%20To%20address%20this%20task%2C%0Awe%20propose%20Agent-based%20Video%20Trimming%20%28AVT%29%2C%20structured%20into%20three%20phases%3A%0AVideo%20Structuring%2C%20Clip%20Filtering%2C%20and%20Story%20Composition.%20Specifically%2C%20we%0Aemploy%20a%20Video%20Captioning%20Agent%20to%20convert%20video%20slices%20into%20structured%20textual%0Adescriptions%2C%20a%20Filtering%20Module%20to%20dynamically%20discard%20low-quality%20footage%0Abased%20on%20the%20structured%20information%20of%20each%20clip%2C%20and%20a%20Video%20Arrangement%20Agent%0Ato%20select%20and%20compile%20valid%20clips%20into%20a%20coherent%20final%20narrative.%20For%0Aevaluation%2C%20we%20develop%20a%20Video%20Evaluation%20Agent%20to%20assess%20trimmed%20videos%2C%0Aconducting%20assessments%20in%20parallel%20with%20human%20evaluations.%20Additionally%2C%20we%0Acurate%20a%20new%20benchmark%20dataset%20for%20video%20trimming%20using%20raw%20user%20videos%20from%0Athe%20internet.%20As%20a%20result%2C%20AVT%20received%20more%20favorable%20evaluations%20in%20user%0Astudies%20and%20demonstrated%20superior%20mAP%20and%20precision%20on%20the%20YouTube%20Highlights%2C%0ATVSum%2C%20and%20our%20own%20dataset%20for%20the%20highlight%20detection%20task.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//ylingfeng.github.io/AVT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09513v1&entry.124074799=Read"},
{"title": "Enhancing Implicit Neural Representations via Symmetric Power\n  Transformation", "author": "Weixiang Zhang and Shuzhao Xie and Chengwei Ren and Shijia Ge and Mingzi Wang and Zhi Wang", "abstract": "  We propose symmetric power transformation to enhance the capacity of Implicit\nNeural Representation~(INR) from the perspective of data transformation. Unlike\nprior work utilizing random permutation or index rearrangement, our method\nfeatures a reversible operation that does not require additional storage\nconsumption. Specifically, we first investigate the characteristics of data\nthat can benefit the training of INR, proposing the Range-Defined Symmetric\nHypothesis, which posits that specific range and symmetry can improve the\nexpressive ability of INR. Based on this hypothesis, we propose a nonlinear\nsymmetric power transformation to achieve both range-defined and symmetric\nproperties simultaneously. We use the power coefficient to redistribute data to\napproximate symmetry within the target range. To improve the robustness of the\ntransformation, we further design deviation-aware calibration and adaptive soft\nboundary to address issues of extreme deviation boosting and continuity\nbreaking. Extensive experiments are conducted to verify the performance of the\nproposed method, demonstrating that our transformation can reliably improve INR\ncompared with other data transformations. We also conduct 1D audio, 2D image\nand 3D video fitting tasks to demonstrate the effectiveness and applicability\nof our method.\n", "link": "http://arxiv.org/abs/2412.09213v1", "date": "2024-12-12", "relevancy": 2.1353, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.551}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.537}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Implicit%20Neural%20Representations%20via%20Symmetric%20Power%0A%20%20Transformation&body=Title%3A%20Enhancing%20Implicit%20Neural%20Representations%20via%20Symmetric%20Power%0A%20%20Transformation%0AAuthor%3A%20Weixiang%20Zhang%20and%20Shuzhao%20Xie%20and%20Chengwei%20Ren%20and%20Shijia%20Ge%20and%20Mingzi%20Wang%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20We%20propose%20symmetric%20power%20transformation%20to%20enhance%20the%20capacity%20of%20Implicit%0ANeural%20Representation~%28INR%29%20from%20the%20perspective%20of%20data%20transformation.%20Unlike%0Aprior%20work%20utilizing%20random%20permutation%20or%20index%20rearrangement%2C%20our%20method%0Afeatures%20a%20reversible%20operation%20that%20does%20not%20require%20additional%20storage%0Aconsumption.%20Specifically%2C%20we%20first%20investigate%20the%20characteristics%20of%20data%0Athat%20can%20benefit%20the%20training%20of%20INR%2C%20proposing%20the%20Range-Defined%20Symmetric%0AHypothesis%2C%20which%20posits%20that%20specific%20range%20and%20symmetry%20can%20improve%20the%0Aexpressive%20ability%20of%20INR.%20Based%20on%20this%20hypothesis%2C%20we%20propose%20a%20nonlinear%0Asymmetric%20power%20transformation%20to%20achieve%20both%20range-defined%20and%20symmetric%0Aproperties%20simultaneously.%20We%20use%20the%20power%20coefficient%20to%20redistribute%20data%20to%0Aapproximate%20symmetry%20within%20the%20target%20range.%20To%20improve%20the%20robustness%20of%20the%0Atransformation%2C%20we%20further%20design%20deviation-aware%20calibration%20and%20adaptive%20soft%0Aboundary%20to%20address%20issues%20of%20extreme%20deviation%20boosting%20and%20continuity%0Abreaking.%20Extensive%20experiments%20are%20conducted%20to%20verify%20the%20performance%20of%20the%0Aproposed%20method%2C%20demonstrating%20that%20our%20transformation%20can%20reliably%20improve%20INR%0Acompared%20with%20other%20data%20transformations.%20We%20also%20conduct%201D%20audio%2C%202D%20image%0Aand%203D%20video%20fitting%20tasks%20to%20demonstrate%20the%20effectiveness%20and%20applicability%0Aof%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Implicit%2520Neural%2520Representations%2520via%2520Symmetric%2520Power%250A%2520%2520Transformation%26entry.906535625%3DWeixiang%2520Zhang%2520and%2520Shuzhao%2520Xie%2520and%2520Chengwei%2520Ren%2520and%2520Shijia%2520Ge%2520and%2520Mingzi%2520Wang%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520symmetric%2520power%2520transformation%2520to%2520enhance%2520the%2520capacity%2520of%2520Implicit%250ANeural%2520Representation~%2528INR%2529%2520from%2520the%2520perspective%2520of%2520data%2520transformation.%2520Unlike%250Aprior%2520work%2520utilizing%2520random%2520permutation%2520or%2520index%2520rearrangement%252C%2520our%2520method%250Afeatures%2520a%2520reversible%2520operation%2520that%2520does%2520not%2520require%2520additional%2520storage%250Aconsumption.%2520Specifically%252C%2520we%2520first%2520investigate%2520the%2520characteristics%2520of%2520data%250Athat%2520can%2520benefit%2520the%2520training%2520of%2520INR%252C%2520proposing%2520the%2520Range-Defined%2520Symmetric%250AHypothesis%252C%2520which%2520posits%2520that%2520specific%2520range%2520and%2520symmetry%2520can%2520improve%2520the%250Aexpressive%2520ability%2520of%2520INR.%2520Based%2520on%2520this%2520hypothesis%252C%2520we%2520propose%2520a%2520nonlinear%250Asymmetric%2520power%2520transformation%2520to%2520achieve%2520both%2520range-defined%2520and%2520symmetric%250Aproperties%2520simultaneously.%2520We%2520use%2520the%2520power%2520coefficient%2520to%2520redistribute%2520data%2520to%250Aapproximate%2520symmetry%2520within%2520the%2520target%2520range.%2520To%2520improve%2520the%2520robustness%2520of%2520the%250Atransformation%252C%2520we%2520further%2520design%2520deviation-aware%2520calibration%2520and%2520adaptive%2520soft%250Aboundary%2520to%2520address%2520issues%2520of%2520extreme%2520deviation%2520boosting%2520and%2520continuity%250Abreaking.%2520Extensive%2520experiments%2520are%2520conducted%2520to%2520verify%2520the%2520performance%2520of%2520the%250Aproposed%2520method%252C%2520demonstrating%2520that%2520our%2520transformation%2520can%2520reliably%2520improve%2520INR%250Acompared%2520with%2520other%2520data%2520transformations.%2520We%2520also%2520conduct%25201D%2520audio%252C%25202D%2520image%250Aand%25203D%2520video%2520fitting%2520tasks%2520to%2520demonstrate%2520the%2520effectiveness%2520and%2520applicability%250Aof%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Implicit%20Neural%20Representations%20via%20Symmetric%20Power%0A%20%20Transformation&entry.906535625=Weixiang%20Zhang%20and%20Shuzhao%20Xie%20and%20Chengwei%20Ren%20and%20Shijia%20Ge%20and%20Mingzi%20Wang%20and%20Zhi%20Wang&entry.1292438233=%20%20We%20propose%20symmetric%20power%20transformation%20to%20enhance%20the%20capacity%20of%20Implicit%0ANeural%20Representation~%28INR%29%20from%20the%20perspective%20of%20data%20transformation.%20Unlike%0Aprior%20work%20utilizing%20random%20permutation%20or%20index%20rearrangement%2C%20our%20method%0Afeatures%20a%20reversible%20operation%20that%20does%20not%20require%20additional%20storage%0Aconsumption.%20Specifically%2C%20we%20first%20investigate%20the%20characteristics%20of%20data%0Athat%20can%20benefit%20the%20training%20of%20INR%2C%20proposing%20the%20Range-Defined%20Symmetric%0AHypothesis%2C%20which%20posits%20that%20specific%20range%20and%20symmetry%20can%20improve%20the%0Aexpressive%20ability%20of%20INR.%20Based%20on%20this%20hypothesis%2C%20we%20propose%20a%20nonlinear%0Asymmetric%20power%20transformation%20to%20achieve%20both%20range-defined%20and%20symmetric%0Aproperties%20simultaneously.%20We%20use%20the%20power%20coefficient%20to%20redistribute%20data%20to%0Aapproximate%20symmetry%20within%20the%20target%20range.%20To%20improve%20the%20robustness%20of%20the%0Atransformation%2C%20we%20further%20design%20deviation-aware%20calibration%20and%20adaptive%20soft%0Aboundary%20to%20address%20issues%20of%20extreme%20deviation%20boosting%20and%20continuity%0Abreaking.%20Extensive%20experiments%20are%20conducted%20to%20verify%20the%20performance%20of%20the%0Aproposed%20method%2C%20demonstrating%20that%20our%20transformation%20can%20reliably%20improve%20INR%0Acompared%20with%20other%20data%20transformations.%20We%20also%20conduct%201D%20audio%2C%202D%20image%0Aand%203D%20video%20fitting%20tasks%20to%20demonstrate%20the%20effectiveness%20and%20applicability%0Aof%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09213v1&entry.124074799=Read"},
{"title": "NormalFlow: Fast, Robust, and Accurate Contact-based Object 6DoF Pose\n  Tracking with Vision-based Tactile Sensors", "author": "Hung-Jui Huang and Michael Kaess and Wenzhen Yuan", "abstract": "  Tactile sensing is crucial for robots aiming to achieve human-level\ndexterity. Among tactile-dependent skills, tactile-based object tracking serves\nas the cornerstone for many tasks, including manipulation, in-hand\nmanipulation, and 3D reconstruction. In this work, we introduce NormalFlow, a\nfast, robust, and real-time tactile-based 6DoF tracking algorithm. Leveraging\nthe precise surface normal estimation of vision-based tactile sensors,\nNormalFlow determines object movements by minimizing discrepancies between the\ntactile-derived surface normals. Our results show that NormalFlow consistently\noutperforms competitive baselines and can track low-texture objects like table\nsurfaces. For long-horizon tracking, we demonstrate when rolling the sensor\naround a bead for 360 degrees, NormalFlow maintains a rotational tracking error\nof 2.5 degrees. Additionally, we present state-of-the-art tactile-based 3D\nreconstruction results, showcasing the high accuracy of NormalFlow. We believe\nNormalFlow unlocks new possibilities for high-precision perception and\nmanipulation tasks that involve interacting with objects using hands. The video\ndemo, code, and dataset are available on our website:\nhttps://joehjhuang.github.io/normalflow.\n", "link": "http://arxiv.org/abs/2412.09617v1", "date": "2024-12-12", "relevancy": 2.1349, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5271}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NormalFlow%3A%20Fast%2C%20Robust%2C%20and%20Accurate%20Contact-based%20Object%206DoF%20Pose%0A%20%20Tracking%20with%20Vision-based%20Tactile%20Sensors&body=Title%3A%20NormalFlow%3A%20Fast%2C%20Robust%2C%20and%20Accurate%20Contact-based%20Object%206DoF%20Pose%0A%20%20Tracking%20with%20Vision-based%20Tactile%20Sensors%0AAuthor%3A%20Hung-Jui%20Huang%20and%20Michael%20Kaess%20and%20Wenzhen%20Yuan%0AAbstract%3A%20%20%20Tactile%20sensing%20is%20crucial%20for%20robots%20aiming%20to%20achieve%20human-level%0Adexterity.%20Among%20tactile-dependent%20skills%2C%20tactile-based%20object%20tracking%20serves%0Aas%20the%20cornerstone%20for%20many%20tasks%2C%20including%20manipulation%2C%20in-hand%0Amanipulation%2C%20and%203D%20reconstruction.%20In%20this%20work%2C%20we%20introduce%20NormalFlow%2C%20a%0Afast%2C%20robust%2C%20and%20real-time%20tactile-based%206DoF%20tracking%20algorithm.%20Leveraging%0Athe%20precise%20surface%20normal%20estimation%20of%20vision-based%20tactile%20sensors%2C%0ANormalFlow%20determines%20object%20movements%20by%20minimizing%20discrepancies%20between%20the%0Atactile-derived%20surface%20normals.%20Our%20results%20show%20that%20NormalFlow%20consistently%0Aoutperforms%20competitive%20baselines%20and%20can%20track%20low-texture%20objects%20like%20table%0Asurfaces.%20For%20long-horizon%20tracking%2C%20we%20demonstrate%20when%20rolling%20the%20sensor%0Aaround%20a%20bead%20for%20360%20degrees%2C%20NormalFlow%20maintains%20a%20rotational%20tracking%20error%0Aof%202.5%20degrees.%20Additionally%2C%20we%20present%20state-of-the-art%20tactile-based%203D%0Areconstruction%20results%2C%20showcasing%20the%20high%20accuracy%20of%20NormalFlow.%20We%20believe%0ANormalFlow%20unlocks%20new%20possibilities%20for%20high-precision%20perception%20and%0Amanipulation%20tasks%20that%20involve%20interacting%20with%20objects%20using%20hands.%20The%20video%0Ademo%2C%20code%2C%20and%20dataset%20are%20available%20on%20our%20website%3A%0Ahttps%3A//joehjhuang.github.io/normalflow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalFlow%253A%2520Fast%252C%2520Robust%252C%2520and%2520Accurate%2520Contact-based%2520Object%25206DoF%2520Pose%250A%2520%2520Tracking%2520with%2520Vision-based%2520Tactile%2520Sensors%26entry.906535625%3DHung-Jui%2520Huang%2520and%2520Michael%2520Kaess%2520and%2520Wenzhen%2520Yuan%26entry.1292438233%3D%2520%2520Tactile%2520sensing%2520is%2520crucial%2520for%2520robots%2520aiming%2520to%2520achieve%2520human-level%250Adexterity.%2520Among%2520tactile-dependent%2520skills%252C%2520tactile-based%2520object%2520tracking%2520serves%250Aas%2520the%2520cornerstone%2520for%2520many%2520tasks%252C%2520including%2520manipulation%252C%2520in-hand%250Amanipulation%252C%2520and%25203D%2520reconstruction.%2520In%2520this%2520work%252C%2520we%2520introduce%2520NormalFlow%252C%2520a%250Afast%252C%2520robust%252C%2520and%2520real-time%2520tactile-based%25206DoF%2520tracking%2520algorithm.%2520Leveraging%250Athe%2520precise%2520surface%2520normal%2520estimation%2520of%2520vision-based%2520tactile%2520sensors%252C%250ANormalFlow%2520determines%2520object%2520movements%2520by%2520minimizing%2520discrepancies%2520between%2520the%250Atactile-derived%2520surface%2520normals.%2520Our%2520results%2520show%2520that%2520NormalFlow%2520consistently%250Aoutperforms%2520competitive%2520baselines%2520and%2520can%2520track%2520low-texture%2520objects%2520like%2520table%250Asurfaces.%2520For%2520long-horizon%2520tracking%252C%2520we%2520demonstrate%2520when%2520rolling%2520the%2520sensor%250Aaround%2520a%2520bead%2520for%2520360%2520degrees%252C%2520NormalFlow%2520maintains%2520a%2520rotational%2520tracking%2520error%250Aof%25202.5%2520degrees.%2520Additionally%252C%2520we%2520present%2520state-of-the-art%2520tactile-based%25203D%250Areconstruction%2520results%252C%2520showcasing%2520the%2520high%2520accuracy%2520of%2520NormalFlow.%2520We%2520believe%250ANormalFlow%2520unlocks%2520new%2520possibilities%2520for%2520high-precision%2520perception%2520and%250Amanipulation%2520tasks%2520that%2520involve%2520interacting%2520with%2520objects%2520using%2520hands.%2520The%2520video%250Ademo%252C%2520code%252C%2520and%2520dataset%2520are%2520available%2520on%2520our%2520website%253A%250Ahttps%253A//joehjhuang.github.io/normalflow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NormalFlow%3A%20Fast%2C%20Robust%2C%20and%20Accurate%20Contact-based%20Object%206DoF%20Pose%0A%20%20Tracking%20with%20Vision-based%20Tactile%20Sensors&entry.906535625=Hung-Jui%20Huang%20and%20Michael%20Kaess%20and%20Wenzhen%20Yuan&entry.1292438233=%20%20Tactile%20sensing%20is%20crucial%20for%20robots%20aiming%20to%20achieve%20human-level%0Adexterity.%20Among%20tactile-dependent%20skills%2C%20tactile-based%20object%20tracking%20serves%0Aas%20the%20cornerstone%20for%20many%20tasks%2C%20including%20manipulation%2C%20in-hand%0Amanipulation%2C%20and%203D%20reconstruction.%20In%20this%20work%2C%20we%20introduce%20NormalFlow%2C%20a%0Afast%2C%20robust%2C%20and%20real-time%20tactile-based%206DoF%20tracking%20algorithm.%20Leveraging%0Athe%20precise%20surface%20normal%20estimation%20of%20vision-based%20tactile%20sensors%2C%0ANormalFlow%20determines%20object%20movements%20by%20minimizing%20discrepancies%20between%20the%0Atactile-derived%20surface%20normals.%20Our%20results%20show%20that%20NormalFlow%20consistently%0Aoutperforms%20competitive%20baselines%20and%20can%20track%20low-texture%20objects%20like%20table%0Asurfaces.%20For%20long-horizon%20tracking%2C%20we%20demonstrate%20when%20rolling%20the%20sensor%0Aaround%20a%20bead%20for%20360%20degrees%2C%20NormalFlow%20maintains%20a%20rotational%20tracking%20error%0Aof%202.5%20degrees.%20Additionally%2C%20we%20present%20state-of-the-art%20tactile-based%203D%0Areconstruction%20results%2C%20showcasing%20the%20high%20accuracy%20of%20NormalFlow.%20We%20believe%0ANormalFlow%20unlocks%20new%20possibilities%20for%20high-precision%20perception%20and%0Amanipulation%20tasks%20that%20involve%20interacting%20with%20objects%20using%20hands.%20The%20video%0Ademo%2C%20code%2C%20and%20dataset%20are%20available%20on%20our%20website%3A%0Ahttps%3A//joehjhuang.github.io/normalflow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09617v1&entry.124074799=Read"},
{"title": "CaDRE: Controllable and Diverse Generation of Safety-Critical Driving\n  Scenarios using Real-World Trajectories", "author": "Peide Huang and Wenhao Ding and Benjamin Stoler and Jonathan Francis and Bingqing Chen and Ding Zhao", "abstract": "  Simulation is an indispensable tool in the development and testing of\nautonomous vehicles (AVs), offering an efficient and safe alternative to road\ntesting. An outstanding challenge with simulation-based testing is the\ngeneration of safety-critical scenarios, which are essential to ensure that AVs\ncan handle rare but potentially fatal situations. This paper addresses this\nchallenge by introducing a novel framework, CaDRE, to generate realistic,\ndiverse, and controllable safety-critical scenarios. Our approach optimizes for\nboth the quality and diversity of scenarios by employing a unique formulation\nand algorithm that integrates real-world scenarios, domain knowledge, and\nblack-box optimization. We validate the effectiveness of our framework through\nextensive testing in three representative types of traffic scenarios. The\nresults demonstrate superior performance in generating diverse and high-quality\nscenarios with greater sample efficiency than existing reinforcement learning\n(RL) and sampling-based methods.\n", "link": "http://arxiv.org/abs/2403.13208v2", "date": "2024-12-12", "relevancy": 2.127, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5285}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaDRE%3A%20Controllable%20and%20Diverse%20Generation%20of%20Safety-Critical%20Driving%0A%20%20Scenarios%20using%20Real-World%20Trajectories&body=Title%3A%20CaDRE%3A%20Controllable%20and%20Diverse%20Generation%20of%20Safety-Critical%20Driving%0A%20%20Scenarios%20using%20Real-World%20Trajectories%0AAuthor%3A%20Peide%20Huang%20and%20Wenhao%20Ding%20and%20Benjamin%20Stoler%20and%20Jonathan%20Francis%20and%20Bingqing%20Chen%20and%20Ding%20Zhao%0AAbstract%3A%20%20%20Simulation%20is%20an%20indispensable%20tool%20in%20the%20development%20and%20testing%20of%0Aautonomous%20vehicles%20%28AVs%29%2C%20offering%20an%20efficient%20and%20safe%20alternative%20to%20road%0Atesting.%20An%20outstanding%20challenge%20with%20simulation-based%20testing%20is%20the%0Ageneration%20of%20safety-critical%20scenarios%2C%20which%20are%20essential%20to%20ensure%20that%20AVs%0Acan%20handle%20rare%20but%20potentially%20fatal%20situations.%20This%20paper%20addresses%20this%0Achallenge%20by%20introducing%20a%20novel%20framework%2C%20CaDRE%2C%20to%20generate%20realistic%2C%0Adiverse%2C%20and%20controllable%20safety-critical%20scenarios.%20Our%20approach%20optimizes%20for%0Aboth%20the%20quality%20and%20diversity%20of%20scenarios%20by%20employing%20a%20unique%20formulation%0Aand%20algorithm%20that%20integrates%20real-world%20scenarios%2C%20domain%20knowledge%2C%20and%0Ablack-box%20optimization.%20We%20validate%20the%20effectiveness%20of%20our%20framework%20through%0Aextensive%20testing%20in%20three%20representative%20types%20of%20traffic%20scenarios.%20The%0Aresults%20demonstrate%20superior%20performance%20in%20generating%20diverse%20and%20high-quality%0Ascenarios%20with%20greater%20sample%20efficiency%20than%20existing%20reinforcement%20learning%0A%28RL%29%20and%20sampling-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaDRE%253A%2520Controllable%2520and%2520Diverse%2520Generation%2520of%2520Safety-Critical%2520Driving%250A%2520%2520Scenarios%2520using%2520Real-World%2520Trajectories%26entry.906535625%3DPeide%2520Huang%2520and%2520Wenhao%2520Ding%2520and%2520Benjamin%2520Stoler%2520and%2520Jonathan%2520Francis%2520and%2520Bingqing%2520Chen%2520and%2520Ding%2520Zhao%26entry.1292438233%3D%2520%2520Simulation%2520is%2520an%2520indispensable%2520tool%2520in%2520the%2520development%2520and%2520testing%2520of%250Aautonomous%2520vehicles%2520%2528AVs%2529%252C%2520offering%2520an%2520efficient%2520and%2520safe%2520alternative%2520to%2520road%250Atesting.%2520An%2520outstanding%2520challenge%2520with%2520simulation-based%2520testing%2520is%2520the%250Ageneration%2520of%2520safety-critical%2520scenarios%252C%2520which%2520are%2520essential%2520to%2520ensure%2520that%2520AVs%250Acan%2520handle%2520rare%2520but%2520potentially%2520fatal%2520situations.%2520This%2520paper%2520addresses%2520this%250Achallenge%2520by%2520introducing%2520a%2520novel%2520framework%252C%2520CaDRE%252C%2520to%2520generate%2520realistic%252C%250Adiverse%252C%2520and%2520controllable%2520safety-critical%2520scenarios.%2520Our%2520approach%2520optimizes%2520for%250Aboth%2520the%2520quality%2520and%2520diversity%2520of%2520scenarios%2520by%2520employing%2520a%2520unique%2520formulation%250Aand%2520algorithm%2520that%2520integrates%2520real-world%2520scenarios%252C%2520domain%2520knowledge%252C%2520and%250Ablack-box%2520optimization.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520framework%2520through%250Aextensive%2520testing%2520in%2520three%2520representative%2520types%2520of%2520traffic%2520scenarios.%2520The%250Aresults%2520demonstrate%2520superior%2520performance%2520in%2520generating%2520diverse%2520and%2520high-quality%250Ascenarios%2520with%2520greater%2520sample%2520efficiency%2520than%2520existing%2520reinforcement%2520learning%250A%2528RL%2529%2520and%2520sampling-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaDRE%3A%20Controllable%20and%20Diverse%20Generation%20of%20Safety-Critical%20Driving%0A%20%20Scenarios%20using%20Real-World%20Trajectories&entry.906535625=Peide%20Huang%20and%20Wenhao%20Ding%20and%20Benjamin%20Stoler%20and%20Jonathan%20Francis%20and%20Bingqing%20Chen%20and%20Ding%20Zhao&entry.1292438233=%20%20Simulation%20is%20an%20indispensable%20tool%20in%20the%20development%20and%20testing%20of%0Aautonomous%20vehicles%20%28AVs%29%2C%20offering%20an%20efficient%20and%20safe%20alternative%20to%20road%0Atesting.%20An%20outstanding%20challenge%20with%20simulation-based%20testing%20is%20the%0Ageneration%20of%20safety-critical%20scenarios%2C%20which%20are%20essential%20to%20ensure%20that%20AVs%0Acan%20handle%20rare%20but%20potentially%20fatal%20situations.%20This%20paper%20addresses%20this%0Achallenge%20by%20introducing%20a%20novel%20framework%2C%20CaDRE%2C%20to%20generate%20realistic%2C%0Adiverse%2C%20and%20controllable%20safety-critical%20scenarios.%20Our%20approach%20optimizes%20for%0Aboth%20the%20quality%20and%20diversity%20of%20scenarios%20by%20employing%20a%20unique%20formulation%0Aand%20algorithm%20that%20integrates%20real-world%20scenarios%2C%20domain%20knowledge%2C%20and%0Ablack-box%20optimization.%20We%20validate%20the%20effectiveness%20of%20our%20framework%20through%0Aextensive%20testing%20in%20three%20representative%20types%20of%20traffic%20scenarios.%20The%0Aresults%20demonstrate%20superior%20performance%20in%20generating%20diverse%20and%20high-quality%0Ascenarios%20with%20greater%20sample%20efficiency%20than%20existing%20reinforcement%20learning%0A%28RL%29%20and%20sampling-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13208v2&entry.124074799=Read"},
{"title": "Integrating Vision Systems and STPA for Robust Landing and Take-Off in\n  VTOL Aircraft", "author": "Sandeep Banik and Jinrae Kim and Naira Hovakimyan and Luca Carlone and John P. Thomas and Nancy G. Leveson", "abstract": "  Vertical take-off and landing (VTOL) unmanned aerial vehicles (UAVs) are\nversatile platforms widely used in applications such as surveillance, search\nand rescue, and urban air mobility. Despite their potential, the critical\nphases of take-off and landing in uncertain and dynamic environments pose\nsignificant safety challenges due to environmental uncertainties, sensor noise,\nand system-level interactions. This paper presents an integrated approach\ncombining vision-based sensor fusion with System-Theoretic Process Analysis\n(STPA) to enhance the safety and robustness of VTOL UAV operations during\ntake-off and landing. By incorporating fiducial markers, such as AprilTags,\ninto the control architecture, and performing comprehensive hazard analysis, we\nidentify unsafe control actions and propose mitigation strategies. Key\ncontributions include developing the control structure with vision system\ncapable of identifying a fiducial marker, multirotor controller and\ncorresponding unsafe control actions and mitigation strategies. The proposed\nsolution is expected to improve the reliability and safety of VTOL UAV\noperations, paving the way for resilient autonomous systems.\n", "link": "http://arxiv.org/abs/2412.09505v1", "date": "2024-12-12", "relevancy": 2.1212, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Vision%20Systems%20and%20STPA%20for%20Robust%20Landing%20and%20Take-Off%20in%0A%20%20VTOL%20Aircraft&body=Title%3A%20Integrating%20Vision%20Systems%20and%20STPA%20for%20Robust%20Landing%20and%20Take-Off%20in%0A%20%20VTOL%20Aircraft%0AAuthor%3A%20Sandeep%20Banik%20and%20Jinrae%20Kim%20and%20Naira%20Hovakimyan%20and%20Luca%20Carlone%20and%20John%20P.%20Thomas%20and%20Nancy%20G.%20Leveson%0AAbstract%3A%20%20%20Vertical%20take-off%20and%20landing%20%28VTOL%29%20unmanned%20aerial%20vehicles%20%28UAVs%29%20are%0Aversatile%20platforms%20widely%20used%20in%20applications%20such%20as%20surveillance%2C%20search%0Aand%20rescue%2C%20and%20urban%20air%20mobility.%20Despite%20their%20potential%2C%20the%20critical%0Aphases%20of%20take-off%20and%20landing%20in%20uncertain%20and%20dynamic%20environments%20pose%0Asignificant%20safety%20challenges%20due%20to%20environmental%20uncertainties%2C%20sensor%20noise%2C%0Aand%20system-level%20interactions.%20This%20paper%20presents%20an%20integrated%20approach%0Acombining%20vision-based%20sensor%20fusion%20with%20System-Theoretic%20Process%20Analysis%0A%28STPA%29%20to%20enhance%20the%20safety%20and%20robustness%20of%20VTOL%20UAV%20operations%20during%0Atake-off%20and%20landing.%20By%20incorporating%20fiducial%20markers%2C%20such%20as%20AprilTags%2C%0Ainto%20the%20control%20architecture%2C%20and%20performing%20comprehensive%20hazard%20analysis%2C%20we%0Aidentify%20unsafe%20control%20actions%20and%20propose%20mitigation%20strategies.%20Key%0Acontributions%20include%20developing%20the%20control%20structure%20with%20vision%20system%0Acapable%20of%20identifying%20a%20fiducial%20marker%2C%20multirotor%20controller%20and%0Acorresponding%20unsafe%20control%20actions%20and%20mitigation%20strategies.%20The%20proposed%0Asolution%20is%20expected%20to%20improve%20the%20reliability%20and%20safety%20of%20VTOL%20UAV%0Aoperations%2C%20paving%20the%20way%20for%20resilient%20autonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Vision%2520Systems%2520and%2520STPA%2520for%2520Robust%2520Landing%2520and%2520Take-Off%2520in%250A%2520%2520VTOL%2520Aircraft%26entry.906535625%3DSandeep%2520Banik%2520and%2520Jinrae%2520Kim%2520and%2520Naira%2520Hovakimyan%2520and%2520Luca%2520Carlone%2520and%2520John%2520P.%2520Thomas%2520and%2520Nancy%2520G.%2520Leveson%26entry.1292438233%3D%2520%2520Vertical%2520take-off%2520and%2520landing%2520%2528VTOL%2529%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520are%250Aversatile%2520platforms%2520widely%2520used%2520in%2520applications%2520such%2520as%2520surveillance%252C%2520search%250Aand%2520rescue%252C%2520and%2520urban%2520air%2520mobility.%2520Despite%2520their%2520potential%252C%2520the%2520critical%250Aphases%2520of%2520take-off%2520and%2520landing%2520in%2520uncertain%2520and%2520dynamic%2520environments%2520pose%250Asignificant%2520safety%2520challenges%2520due%2520to%2520environmental%2520uncertainties%252C%2520sensor%2520noise%252C%250Aand%2520system-level%2520interactions.%2520This%2520paper%2520presents%2520an%2520integrated%2520approach%250Acombining%2520vision-based%2520sensor%2520fusion%2520with%2520System-Theoretic%2520Process%2520Analysis%250A%2528STPA%2529%2520to%2520enhance%2520the%2520safety%2520and%2520robustness%2520of%2520VTOL%2520UAV%2520operations%2520during%250Atake-off%2520and%2520landing.%2520By%2520incorporating%2520fiducial%2520markers%252C%2520such%2520as%2520AprilTags%252C%250Ainto%2520the%2520control%2520architecture%252C%2520and%2520performing%2520comprehensive%2520hazard%2520analysis%252C%2520we%250Aidentify%2520unsafe%2520control%2520actions%2520and%2520propose%2520mitigation%2520strategies.%2520Key%250Acontributions%2520include%2520developing%2520the%2520control%2520structure%2520with%2520vision%2520system%250Acapable%2520of%2520identifying%2520a%2520fiducial%2520marker%252C%2520multirotor%2520controller%2520and%250Acorresponding%2520unsafe%2520control%2520actions%2520and%2520mitigation%2520strategies.%2520The%2520proposed%250Asolution%2520is%2520expected%2520to%2520improve%2520the%2520reliability%2520and%2520safety%2520of%2520VTOL%2520UAV%250Aoperations%252C%2520paving%2520the%2520way%2520for%2520resilient%2520autonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Vision%20Systems%20and%20STPA%20for%20Robust%20Landing%20and%20Take-Off%20in%0A%20%20VTOL%20Aircraft&entry.906535625=Sandeep%20Banik%20and%20Jinrae%20Kim%20and%20Naira%20Hovakimyan%20and%20Luca%20Carlone%20and%20John%20P.%20Thomas%20and%20Nancy%20G.%20Leveson&entry.1292438233=%20%20Vertical%20take-off%20and%20landing%20%28VTOL%29%20unmanned%20aerial%20vehicles%20%28UAVs%29%20are%0Aversatile%20platforms%20widely%20used%20in%20applications%20such%20as%20surveillance%2C%20search%0Aand%20rescue%2C%20and%20urban%20air%20mobility.%20Despite%20their%20potential%2C%20the%20critical%0Aphases%20of%20take-off%20and%20landing%20in%20uncertain%20and%20dynamic%20environments%20pose%0Asignificant%20safety%20challenges%20due%20to%20environmental%20uncertainties%2C%20sensor%20noise%2C%0Aand%20system-level%20interactions.%20This%20paper%20presents%20an%20integrated%20approach%0Acombining%20vision-based%20sensor%20fusion%20with%20System-Theoretic%20Process%20Analysis%0A%28STPA%29%20to%20enhance%20the%20safety%20and%20robustness%20of%20VTOL%20UAV%20operations%20during%0Atake-off%20and%20landing.%20By%20incorporating%20fiducial%20markers%2C%20such%20as%20AprilTags%2C%0Ainto%20the%20control%20architecture%2C%20and%20performing%20comprehensive%20hazard%20analysis%2C%20we%0Aidentify%20unsafe%20control%20actions%20and%20propose%20mitigation%20strategies.%20Key%0Acontributions%20include%20developing%20the%20control%20structure%20with%20vision%20system%0Acapable%20of%20identifying%20a%20fiducial%20marker%2C%20multirotor%20controller%20and%0Acorresponding%20unsafe%20control%20actions%20and%20mitigation%20strategies.%20The%20proposed%0Asolution%20is%20expected%20to%20improve%20the%20reliability%20and%20safety%20of%20VTOL%20UAV%0Aoperations%2C%20paving%20the%20way%20for%20resilient%20autonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09505v1&entry.124074799=Read"},
{"title": "TimeRefine: Temporal Grounding with Time Refining Video LLM", "author": "Xizi Wang and Feng Cheng and Ziyang Wang and Huiyu Wang and Md Mohaiminul Islam and Lorenzo Torresani and Mohit Bansal and Gedas Bertasius and David Crandall", "abstract": "  Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.\n", "link": "http://arxiv.org/abs/2412.09601v1", "date": "2024-12-12", "relevancy": 2.1192, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5612}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.522}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeRefine%3A%20Temporal%20Grounding%20with%20Time%20Refining%20Video%20LLM&body=Title%3A%20TimeRefine%3A%20Temporal%20Grounding%20with%20Time%20Refining%20Video%20LLM%0AAuthor%3A%20Xizi%20Wang%20and%20Feng%20Cheng%20and%20Ziyang%20Wang%20and%20Huiyu%20Wang%20and%20Md%20Mohaiminul%20Islam%20and%20Lorenzo%20Torresani%20and%20Mohit%20Bansal%20and%20Gedas%20Bertasius%20and%20David%20Crandall%0AAbstract%3A%20%20%20Video%20temporal%20grounding%20aims%20to%20localize%20relevant%20temporal%20boundaries%20in%20a%0Avideo%20given%20a%20textual%20prompt.%20Recent%20work%20has%20focused%20on%20enabling%20Video%20LLMs%20to%0Aperform%20video%20temporal%20grounding%20via%20next-token%20prediction%20of%20temporal%0Atimestamps.%20However%2C%20accurately%20localizing%20timestamps%20in%20videos%20remains%0Achallenging%20for%20Video%20LLMs%20when%20relying%20solely%20on%20temporal%20token%20prediction.%0AOur%20proposed%20TimeRefine%20addresses%20this%20challenge%20in%20two%20ways.%20First%2C%20instead%20of%0Adirectly%20predicting%20the%20start%20and%20end%20timestamps%2C%20we%20reformulate%20the%20temporal%0Agrounding%20task%20as%20a%20temporal%20refining%20task%3A%20the%20model%20first%20makes%20rough%0Apredictions%20and%20then%20refines%20them%20by%20predicting%20offsets%20to%20the%20target%20segment.%0AThis%20refining%20process%20is%20repeated%20multiple%20times%2C%20through%20which%20the%20model%0Aprogressively%20self-improves%20its%20temporal%20localization%20accuracy.%20Second%2C%20to%0Aenhance%20the%20model%27s%20temporal%20perception%20capabilities%2C%20we%20incorporate%20an%0Aauxiliary%20prediction%20head%20that%20penalizes%20the%20model%20more%20if%20a%20predicted%20segment%0Adeviates%20further%20from%20the%20ground%20truth%2C%20thus%20encouraging%20the%20model%20to%20make%0Acloser%20and%20more%20accurate%20predictions.%20Our%20plug-and-play%20method%20can%20be%0Aintegrated%20into%20most%20LLM-based%20temporal%20grounding%20approaches.%20The%20experimental%0Aresults%20demonstrate%20that%20TimeRefine%20achieves%203.6%25%20and%205.0%25%20mIoU%20improvements%20on%0Athe%20ActivityNet%20and%20Charades-STA%20datasets%2C%20respectively.%20Code%20and%20pretrained%0Amodels%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeRefine%253A%2520Temporal%2520Grounding%2520with%2520Time%2520Refining%2520Video%2520LLM%26entry.906535625%3DXizi%2520Wang%2520and%2520Feng%2520Cheng%2520and%2520Ziyang%2520Wang%2520and%2520Huiyu%2520Wang%2520and%2520Md%2520Mohaiminul%2520Islam%2520and%2520Lorenzo%2520Torresani%2520and%2520Mohit%2520Bansal%2520and%2520Gedas%2520Bertasius%2520and%2520David%2520Crandall%26entry.1292438233%3D%2520%2520Video%2520temporal%2520grounding%2520aims%2520to%2520localize%2520relevant%2520temporal%2520boundaries%2520in%2520a%250Avideo%2520given%2520a%2520textual%2520prompt.%2520Recent%2520work%2520has%2520focused%2520on%2520enabling%2520Video%2520LLMs%2520to%250Aperform%2520video%2520temporal%2520grounding%2520via%2520next-token%2520prediction%2520of%2520temporal%250Atimestamps.%2520However%252C%2520accurately%2520localizing%2520timestamps%2520in%2520videos%2520remains%250Achallenging%2520for%2520Video%2520LLMs%2520when%2520relying%2520solely%2520on%2520temporal%2520token%2520prediction.%250AOur%2520proposed%2520TimeRefine%2520addresses%2520this%2520challenge%2520in%2520two%2520ways.%2520First%252C%2520instead%2520of%250Adirectly%2520predicting%2520the%2520start%2520and%2520end%2520timestamps%252C%2520we%2520reformulate%2520the%2520temporal%250Agrounding%2520task%2520as%2520a%2520temporal%2520refining%2520task%253A%2520the%2520model%2520first%2520makes%2520rough%250Apredictions%2520and%2520then%2520refines%2520them%2520by%2520predicting%2520offsets%2520to%2520the%2520target%2520segment.%250AThis%2520refining%2520process%2520is%2520repeated%2520multiple%2520times%252C%2520through%2520which%2520the%2520model%250Aprogressively%2520self-improves%2520its%2520temporal%2520localization%2520accuracy.%2520Second%252C%2520to%250Aenhance%2520the%2520model%2527s%2520temporal%2520perception%2520capabilities%252C%2520we%2520incorporate%2520an%250Aauxiliary%2520prediction%2520head%2520that%2520penalizes%2520the%2520model%2520more%2520if%2520a%2520predicted%2520segment%250Adeviates%2520further%2520from%2520the%2520ground%2520truth%252C%2520thus%2520encouraging%2520the%2520model%2520to%2520make%250Acloser%2520and%2520more%2520accurate%2520predictions.%2520Our%2520plug-and-play%2520method%2520can%2520be%250Aintegrated%2520into%2520most%2520LLM-based%2520temporal%2520grounding%2520approaches.%2520The%2520experimental%250Aresults%2520demonstrate%2520that%2520TimeRefine%2520achieves%25203.6%2525%2520and%25205.0%2525%2520mIoU%2520improvements%2520on%250Athe%2520ActivityNet%2520and%2520Charades-STA%2520datasets%252C%2520respectively.%2520Code%2520and%2520pretrained%250Amodels%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeRefine%3A%20Temporal%20Grounding%20with%20Time%20Refining%20Video%20LLM&entry.906535625=Xizi%20Wang%20and%20Feng%20Cheng%20and%20Ziyang%20Wang%20and%20Huiyu%20Wang%20and%20Md%20Mohaiminul%20Islam%20and%20Lorenzo%20Torresani%20and%20Mohit%20Bansal%20and%20Gedas%20Bertasius%20and%20David%20Crandall&entry.1292438233=%20%20Video%20temporal%20grounding%20aims%20to%20localize%20relevant%20temporal%20boundaries%20in%20a%0Avideo%20given%20a%20textual%20prompt.%20Recent%20work%20has%20focused%20on%20enabling%20Video%20LLMs%20to%0Aperform%20video%20temporal%20grounding%20via%20next-token%20prediction%20of%20temporal%0Atimestamps.%20However%2C%20accurately%20localizing%20timestamps%20in%20videos%20remains%0Achallenging%20for%20Video%20LLMs%20when%20relying%20solely%20on%20temporal%20token%20prediction.%0AOur%20proposed%20TimeRefine%20addresses%20this%20challenge%20in%20two%20ways.%20First%2C%20instead%20of%0Adirectly%20predicting%20the%20start%20and%20end%20timestamps%2C%20we%20reformulate%20the%20temporal%0Agrounding%20task%20as%20a%20temporal%20refining%20task%3A%20the%20model%20first%20makes%20rough%0Apredictions%20and%20then%20refines%20them%20by%20predicting%20offsets%20to%20the%20target%20segment.%0AThis%20refining%20process%20is%20repeated%20multiple%20times%2C%20through%20which%20the%20model%0Aprogressively%20self-improves%20its%20temporal%20localization%20accuracy.%20Second%2C%20to%0Aenhance%20the%20model%27s%20temporal%20perception%20capabilities%2C%20we%20incorporate%20an%0Aauxiliary%20prediction%20head%20that%20penalizes%20the%20model%20more%20if%20a%20predicted%20segment%0Adeviates%20further%20from%20the%20ground%20truth%2C%20thus%20encouraging%20the%20model%20to%20make%0Acloser%20and%20more%20accurate%20predictions.%20Our%20plug-and-play%20method%20can%20be%0Aintegrated%20into%20most%20LLM-based%20temporal%20grounding%20approaches.%20The%20experimental%0Aresults%20demonstrate%20that%20TimeRefine%20achieves%203.6%25%20and%205.0%25%20mIoU%20improvements%20on%0Athe%20ActivityNet%20and%20Charades-STA%20datasets%2C%20respectively.%20Code%20and%20pretrained%0Amodels%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09601v1&entry.124074799=Read"},
{"title": "Hidden Biases of End-to-End Driving Datasets", "author": "Julian Zimmerlin and Jens Bei\u00dfwenger and Bernhard Jaeger and Andreas Geiger and Kashyap Chitta", "abstract": "  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n", "link": "http://arxiv.org/abs/2412.09602v1", "date": "2024-12-12", "relevancy": 2.1144, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5427}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hidden%20Biases%20of%20End-to-End%20Driving%20Datasets&body=Title%3A%20Hidden%20Biases%20of%20End-to-End%20Driving%20Datasets%0AAuthor%3A%20Julian%20Zimmerlin%20and%20Jens%20Bei%C3%9Fwenger%20and%20Bernhard%20Jaeger%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta%0AAbstract%3A%20%20%20End-to-end%20driving%20systems%20have%20made%20rapid%20progress%2C%20but%20have%20so%20far%20not%20been%0Aapplied%20to%20the%20challenging%20new%20CARLA%20Leaderboard%202.0.%20Further%2C%20while%20there%20is%20a%0Alarge%20body%20of%20literature%20on%20end-to-end%20architectures%20and%20training%20strategies%2C%0Athe%20impact%20of%20the%20training%20dataset%20is%20often%20overlooked.%20In%20this%20work%2C%20we%20make%20a%0Afirst%20attempt%20at%20end-to-end%20driving%20for%20Leaderboard%202.0.%20Instead%20of%0Ainvestigating%20architectures%2C%20we%20systematically%20analyze%20the%20training%20dataset%2C%0Aleading%20to%20new%20insights%3A%20%281%29%20Expert%20style%20significantly%20affects%20downstream%0Apolicy%20performance.%20%282%29%20In%20complex%20data%20sets%2C%20the%20frames%20should%20not%20be%20weighted%0Aon%20the%20basis%20of%20simplistic%20criteria%20such%20as%20class%20frequencies.%20%283%29%20Instead%2C%0Aestimating%20whether%20a%20frame%20changes%20the%20target%20labels%20compared%20to%20previous%0Aframes%20can%20reduce%20the%20size%20of%20the%20dataset%20without%20removing%20important%0Ainformation.%20By%20incorporating%20these%20findings%2C%20our%20model%20ranks%20first%20and%20second%0Arespectively%20on%20the%20map%20and%20sensors%20tracks%20of%20the%202024%20CARLA%20Challenge%2C%20and%0Asets%20a%20new%20state-of-the-art%20on%20the%20Bench2Drive%20test%20routes.%20Finally%2C%20we%20uncover%0Aa%20design%20flaw%20in%20the%20current%20evaluation%20metrics%20and%20propose%20a%20modification%20for%0Afuture%20challenges.%20Our%20dataset%2C%20code%2C%20and%20pre-trained%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/autonomousvision/carla_garage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHidden%2520Biases%2520of%2520End-to-End%2520Driving%2520Datasets%26entry.906535625%3DJulian%2520Zimmerlin%2520and%2520Jens%2520Bei%25C3%259Fwenger%2520and%2520Bernhard%2520Jaeger%2520and%2520Andreas%2520Geiger%2520and%2520Kashyap%2520Chitta%26entry.1292438233%3D%2520%2520End-to-end%2520driving%2520systems%2520have%2520made%2520rapid%2520progress%252C%2520but%2520have%2520so%2520far%2520not%2520been%250Aapplied%2520to%2520the%2520challenging%2520new%2520CARLA%2520Leaderboard%25202.0.%2520Further%252C%2520while%2520there%2520is%2520a%250Alarge%2520body%2520of%2520literature%2520on%2520end-to-end%2520architectures%2520and%2520training%2520strategies%252C%250Athe%2520impact%2520of%2520the%2520training%2520dataset%2520is%2520often%2520overlooked.%2520In%2520this%2520work%252C%2520we%2520make%2520a%250Afirst%2520attempt%2520at%2520end-to-end%2520driving%2520for%2520Leaderboard%25202.0.%2520Instead%2520of%250Ainvestigating%2520architectures%252C%2520we%2520systematically%2520analyze%2520the%2520training%2520dataset%252C%250Aleading%2520to%2520new%2520insights%253A%2520%25281%2529%2520Expert%2520style%2520significantly%2520affects%2520downstream%250Apolicy%2520performance.%2520%25282%2529%2520In%2520complex%2520data%2520sets%252C%2520the%2520frames%2520should%2520not%2520be%2520weighted%250Aon%2520the%2520basis%2520of%2520simplistic%2520criteria%2520such%2520as%2520class%2520frequencies.%2520%25283%2529%2520Instead%252C%250Aestimating%2520whether%2520a%2520frame%2520changes%2520the%2520target%2520labels%2520compared%2520to%2520previous%250Aframes%2520can%2520reduce%2520the%2520size%2520of%2520the%2520dataset%2520without%2520removing%2520important%250Ainformation.%2520By%2520incorporating%2520these%2520findings%252C%2520our%2520model%2520ranks%2520first%2520and%2520second%250Arespectively%2520on%2520the%2520map%2520and%2520sensors%2520tracks%2520of%2520the%25202024%2520CARLA%2520Challenge%252C%2520and%250Asets%2520a%2520new%2520state-of-the-art%2520on%2520the%2520Bench2Drive%2520test%2520routes.%2520Finally%252C%2520we%2520uncover%250Aa%2520design%2520flaw%2520in%2520the%2520current%2520evaluation%2520metrics%2520and%2520propose%2520a%2520modification%2520for%250Afuture%2520challenges.%2520Our%2520dataset%252C%2520code%252C%2520and%2520pre-trained%2520models%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/autonomousvision/carla_garage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hidden%20Biases%20of%20End-to-End%20Driving%20Datasets&entry.906535625=Julian%20Zimmerlin%20and%20Jens%20Bei%C3%9Fwenger%20and%20Bernhard%20Jaeger%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta&entry.1292438233=%20%20End-to-end%20driving%20systems%20have%20made%20rapid%20progress%2C%20but%20have%20so%20far%20not%20been%0Aapplied%20to%20the%20challenging%20new%20CARLA%20Leaderboard%202.0.%20Further%2C%20while%20there%20is%20a%0Alarge%20body%20of%20literature%20on%20end-to-end%20architectures%20and%20training%20strategies%2C%0Athe%20impact%20of%20the%20training%20dataset%20is%20often%20overlooked.%20In%20this%20work%2C%20we%20make%20a%0Afirst%20attempt%20at%20end-to-end%20driving%20for%20Leaderboard%202.0.%20Instead%20of%0Ainvestigating%20architectures%2C%20we%20systematically%20analyze%20the%20training%20dataset%2C%0Aleading%20to%20new%20insights%3A%20%281%29%20Expert%20style%20significantly%20affects%20downstream%0Apolicy%20performance.%20%282%29%20In%20complex%20data%20sets%2C%20the%20frames%20should%20not%20be%20weighted%0Aon%20the%20basis%20of%20simplistic%20criteria%20such%20as%20class%20frequencies.%20%283%29%20Instead%2C%0Aestimating%20whether%20a%20frame%20changes%20the%20target%20labels%20compared%20to%20previous%0Aframes%20can%20reduce%20the%20size%20of%20the%20dataset%20without%20removing%20important%0Ainformation.%20By%20incorporating%20these%20findings%2C%20our%20model%20ranks%20first%20and%20second%0Arespectively%20on%20the%20map%20and%20sensors%20tracks%20of%20the%202024%20CARLA%20Challenge%2C%20and%0Asets%20a%20new%20state-of-the-art%20on%20the%20Bench2Drive%20test%20routes.%20Finally%2C%20we%20uncover%0Aa%20design%20flaw%20in%20the%20current%20evaluation%20metrics%20and%20propose%20a%20modification%20for%0Afuture%20challenges.%20Our%20dataset%2C%20code%2C%20and%20pre-trained%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/autonomousvision/carla_garage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09602v1&entry.124074799=Read"},
{"title": "ATPrompt: Textual Prompt Learning with Embedded Attributes", "author": "Zheng Li and Yibing Song and Penghai Zhao and Ming-Ming Cheng and Xiang Li and Jian Yang", "abstract": "  Textual-based prompt learning methods primarily employ multiple learnable\nsoft prompts and hard class tokens in a cascading manner as text prompt inputs,\naiming to align image and text (category) spaces for downstream tasks. However,\ncurrent training is restricted to aligning images with predefined known\ncategories and cannot be associated with unknown categories. In this work, we\npropose utilizing universal attributes as a bridge to enhance the alignment\nbetween images and unknown categories. Specifically, we introduce an\nAttribute-embedded Textual Prompt learning method for vision-language models,\nnamed ATPrompt. This approach expands the learning space of soft prompts from\nthe original one-dimensional category level into the multi-dimensional\nattribute level by incorporating multiple universal attribute tokens into the\nlearnable soft prompts. Through this modification, we transform the text prompt\nfrom a category-centric form to an attribute-category hybrid form. To finalize\nthe attributes for downstream tasks, we propose a differentiable attribute\nsearch method that learns to identify representative and suitable attributes\nfrom a candidate pool summarized by a large language model. As an easy-to-use\nplug-in technique, ATPrompt can seamlessly replace the existing prompt format\nof textual-based methods, offering general improvements at a negligible\ncomputational cost. Extensive experiments on 11 datasets demonstrate the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2412.09442v1", "date": "2024-12-12", "relevancy": 2.0972, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5494}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5162}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATPrompt%3A%20Textual%20Prompt%20Learning%20with%20Embedded%20Attributes&body=Title%3A%20ATPrompt%3A%20Textual%20Prompt%20Learning%20with%20Embedded%20Attributes%0AAuthor%3A%20Zheng%20Li%20and%20Yibing%20Song%20and%20Penghai%20Zhao%20and%20Ming-Ming%20Cheng%20and%20Xiang%20Li%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Textual-based%20prompt%20learning%20methods%20primarily%20employ%20multiple%20learnable%0Asoft%20prompts%20and%20hard%20class%20tokens%20in%20a%20cascading%20manner%20as%20text%20prompt%20inputs%2C%0Aaiming%20to%20align%20image%20and%20text%20%28category%29%20spaces%20for%20downstream%20tasks.%20However%2C%0Acurrent%20training%20is%20restricted%20to%20aligning%20images%20with%20predefined%20known%0Acategories%20and%20cannot%20be%20associated%20with%20unknown%20categories.%20In%20this%20work%2C%20we%0Apropose%20utilizing%20universal%20attributes%20as%20a%20bridge%20to%20enhance%20the%20alignment%0Abetween%20images%20and%20unknown%20categories.%20Specifically%2C%20we%20introduce%20an%0AAttribute-embedded%20Textual%20Prompt%20learning%20method%20for%20vision-language%20models%2C%0Anamed%20ATPrompt.%20This%20approach%20expands%20the%20learning%20space%20of%20soft%20prompts%20from%0Athe%20original%20one-dimensional%20category%20level%20into%20the%20multi-dimensional%0Aattribute%20level%20by%20incorporating%20multiple%20universal%20attribute%20tokens%20into%20the%0Alearnable%20soft%20prompts.%20Through%20this%20modification%2C%20we%20transform%20the%20text%20prompt%0Afrom%20a%20category-centric%20form%20to%20an%20attribute-category%20hybrid%20form.%20To%20finalize%0Athe%20attributes%20for%20downstream%20tasks%2C%20we%20propose%20a%20differentiable%20attribute%0Asearch%20method%20that%20learns%20to%20identify%20representative%20and%20suitable%20attributes%0Afrom%20a%20candidate%20pool%20summarized%20by%20a%20large%20language%20model.%20As%20an%20easy-to-use%0Aplug-in%20technique%2C%20ATPrompt%20can%20seamlessly%20replace%20the%20existing%20prompt%20format%0Aof%20textual-based%20methods%2C%20offering%20general%20improvements%20at%20a%20negligible%0Acomputational%20cost.%20Extensive%20experiments%20on%2011%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATPrompt%253A%2520Textual%2520Prompt%2520Learning%2520with%2520Embedded%2520Attributes%26entry.906535625%3DZheng%2520Li%2520and%2520Yibing%2520Song%2520and%2520Penghai%2520Zhao%2520and%2520Ming-Ming%2520Cheng%2520and%2520Xiang%2520Li%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Textual-based%2520prompt%2520learning%2520methods%2520primarily%2520employ%2520multiple%2520learnable%250Asoft%2520prompts%2520and%2520hard%2520class%2520tokens%2520in%2520a%2520cascading%2520manner%2520as%2520text%2520prompt%2520inputs%252C%250Aaiming%2520to%2520align%2520image%2520and%2520text%2520%2528category%2529%2520spaces%2520for%2520downstream%2520tasks.%2520However%252C%250Acurrent%2520training%2520is%2520restricted%2520to%2520aligning%2520images%2520with%2520predefined%2520known%250Acategories%2520and%2520cannot%2520be%2520associated%2520with%2520unknown%2520categories.%2520In%2520this%2520work%252C%2520we%250Apropose%2520utilizing%2520universal%2520attributes%2520as%2520a%2520bridge%2520to%2520enhance%2520the%2520alignment%250Abetween%2520images%2520and%2520unknown%2520categories.%2520Specifically%252C%2520we%2520introduce%2520an%250AAttribute-embedded%2520Textual%2520Prompt%2520learning%2520method%2520for%2520vision-language%2520models%252C%250Anamed%2520ATPrompt.%2520This%2520approach%2520expands%2520the%2520learning%2520space%2520of%2520soft%2520prompts%2520from%250Athe%2520original%2520one-dimensional%2520category%2520level%2520into%2520the%2520multi-dimensional%250Aattribute%2520level%2520by%2520incorporating%2520multiple%2520universal%2520attribute%2520tokens%2520into%2520the%250Alearnable%2520soft%2520prompts.%2520Through%2520this%2520modification%252C%2520we%2520transform%2520the%2520text%2520prompt%250Afrom%2520a%2520category-centric%2520form%2520to%2520an%2520attribute-category%2520hybrid%2520form.%2520To%2520finalize%250Athe%2520attributes%2520for%2520downstream%2520tasks%252C%2520we%2520propose%2520a%2520differentiable%2520attribute%250Asearch%2520method%2520that%2520learns%2520to%2520identify%2520representative%2520and%2520suitable%2520attributes%250Afrom%2520a%2520candidate%2520pool%2520summarized%2520by%2520a%2520large%2520language%2520model.%2520As%2520an%2520easy-to-use%250Aplug-in%2520technique%252C%2520ATPrompt%2520can%2520seamlessly%2520replace%2520the%2520existing%2520prompt%2520format%250Aof%2520textual-based%2520methods%252C%2520offering%2520general%2520improvements%2520at%2520a%2520negligible%250Acomputational%2520cost.%2520Extensive%2520experiments%2520on%252011%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATPrompt%3A%20Textual%20Prompt%20Learning%20with%20Embedded%20Attributes&entry.906535625=Zheng%20Li%20and%20Yibing%20Song%20and%20Penghai%20Zhao%20and%20Ming-Ming%20Cheng%20and%20Xiang%20Li%20and%20Jian%20Yang&entry.1292438233=%20%20Textual-based%20prompt%20learning%20methods%20primarily%20employ%20multiple%20learnable%0Asoft%20prompts%20and%20hard%20class%20tokens%20in%20a%20cascading%20manner%20as%20text%20prompt%20inputs%2C%0Aaiming%20to%20align%20image%20and%20text%20%28category%29%20spaces%20for%20downstream%20tasks.%20However%2C%0Acurrent%20training%20is%20restricted%20to%20aligning%20images%20with%20predefined%20known%0Acategories%20and%20cannot%20be%20associated%20with%20unknown%20categories.%20In%20this%20work%2C%20we%0Apropose%20utilizing%20universal%20attributes%20as%20a%20bridge%20to%20enhance%20the%20alignment%0Abetween%20images%20and%20unknown%20categories.%20Specifically%2C%20we%20introduce%20an%0AAttribute-embedded%20Textual%20Prompt%20learning%20method%20for%20vision-language%20models%2C%0Anamed%20ATPrompt.%20This%20approach%20expands%20the%20learning%20space%20of%20soft%20prompts%20from%0Athe%20original%20one-dimensional%20category%20level%20into%20the%20multi-dimensional%0Aattribute%20level%20by%20incorporating%20multiple%20universal%20attribute%20tokens%20into%20the%0Alearnable%20soft%20prompts.%20Through%20this%20modification%2C%20we%20transform%20the%20text%20prompt%0Afrom%20a%20category-centric%20form%20to%20an%20attribute-category%20hybrid%20form.%20To%20finalize%0Athe%20attributes%20for%20downstream%20tasks%2C%20we%20propose%20a%20differentiable%20attribute%0Asearch%20method%20that%20learns%20to%20identify%20representative%20and%20suitable%20attributes%0Afrom%20a%20candidate%20pool%20summarized%20by%20a%20large%20language%20model.%20As%20an%20easy-to-use%0Aplug-in%20technique%2C%20ATPrompt%20can%20seamlessly%20replace%20the%20existing%20prompt%20format%0Aof%20textual-based%20methods%2C%20offering%20general%20improvements%20at%20a%20negligible%0Acomputational%20cost.%20Extensive%20experiments%20on%2011%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09442v1&entry.124074799=Read"},
{"title": "Opinion de-polarization of social networks with GNNs", "author": "Konstantinos Mylonas and Thrasyvoulos Spyropoulos", "abstract": "  Nowadays, social media is the ground for political debate and exchange of\nopinions. There is a significant amount of research that suggests that social\nmedia are highly polarized. A phenomenon that is commonly observed is the echo\nchamber structure, where users are organized in polarized communities and form\nconnections only with similar-minded individuals, limiting themselves to\nconsume specific content. In this paper we explore a way to decrease the\npolarization of networks with two echo chambers. Particularly, we observe that\nif some users adopt a moderate opinion about a topic, the polarization of the\nnetwork decreases. Based on this observation, we propose an efficient algorithm\nto identify a good set of K users, such that if they adopt a moderate stance\naround a topic, the polarization is minimized. Our algorithm employs a Graph\nNeural Network and thus it can handle large graphs more effectively than other\napproaches\n", "link": "http://arxiv.org/abs/2412.09404v1", "date": "2024-12-12", "relevancy": 2.0962, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4548}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4036}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opinion%20de-polarization%20of%20social%20networks%20with%20GNNs&body=Title%3A%20Opinion%20de-polarization%20of%20social%20networks%20with%20GNNs%0AAuthor%3A%20Konstantinos%20Mylonas%20and%20Thrasyvoulos%20Spyropoulos%0AAbstract%3A%20%20%20Nowadays%2C%20social%20media%20is%20the%20ground%20for%20political%20debate%20and%20exchange%20of%0Aopinions.%20There%20is%20a%20significant%20amount%20of%20research%20that%20suggests%20that%20social%0Amedia%20are%20highly%20polarized.%20A%20phenomenon%20that%20is%20commonly%20observed%20is%20the%20echo%0Achamber%20structure%2C%20where%20users%20are%20organized%20in%20polarized%20communities%20and%20form%0Aconnections%20only%20with%20similar-minded%20individuals%2C%20limiting%20themselves%20to%0Aconsume%20specific%20content.%20In%20this%20paper%20we%20explore%20a%20way%20to%20decrease%20the%0Apolarization%20of%20networks%20with%20two%20echo%20chambers.%20Particularly%2C%20we%20observe%20that%0Aif%20some%20users%20adopt%20a%20moderate%20opinion%20about%20a%20topic%2C%20the%20polarization%20of%20the%0Anetwork%20decreases.%20Based%20on%20this%20observation%2C%20we%20propose%20an%20efficient%20algorithm%0Ato%20identify%20a%20good%20set%20of%20K%20users%2C%20such%20that%20if%20they%20adopt%20a%20moderate%20stance%0Aaround%20a%20topic%2C%20the%20polarization%20is%20minimized.%20Our%20algorithm%20employs%20a%20Graph%0ANeural%20Network%20and%20thus%20it%20can%20handle%20large%20graphs%20more%20effectively%20than%20other%0Aapproaches%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpinion%2520de-polarization%2520of%2520social%2520networks%2520with%2520GNNs%26entry.906535625%3DKonstantinos%2520Mylonas%2520and%2520Thrasyvoulos%2520Spyropoulos%26entry.1292438233%3D%2520%2520Nowadays%252C%2520social%2520media%2520is%2520the%2520ground%2520for%2520political%2520debate%2520and%2520exchange%2520of%250Aopinions.%2520There%2520is%2520a%2520significant%2520amount%2520of%2520research%2520that%2520suggests%2520that%2520social%250Amedia%2520are%2520highly%2520polarized.%2520A%2520phenomenon%2520that%2520is%2520commonly%2520observed%2520is%2520the%2520echo%250Achamber%2520structure%252C%2520where%2520users%2520are%2520organized%2520in%2520polarized%2520communities%2520and%2520form%250Aconnections%2520only%2520with%2520similar-minded%2520individuals%252C%2520limiting%2520themselves%2520to%250Aconsume%2520specific%2520content.%2520In%2520this%2520paper%2520we%2520explore%2520a%2520way%2520to%2520decrease%2520the%250Apolarization%2520of%2520networks%2520with%2520two%2520echo%2520chambers.%2520Particularly%252C%2520we%2520observe%2520that%250Aif%2520some%2520users%2520adopt%2520a%2520moderate%2520opinion%2520about%2520a%2520topic%252C%2520the%2520polarization%2520of%2520the%250Anetwork%2520decreases.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520an%2520efficient%2520algorithm%250Ato%2520identify%2520a%2520good%2520set%2520of%2520K%2520users%252C%2520such%2520that%2520if%2520they%2520adopt%2520a%2520moderate%2520stance%250Aaround%2520a%2520topic%252C%2520the%2520polarization%2520is%2520minimized.%2520Our%2520algorithm%2520employs%2520a%2520Graph%250ANeural%2520Network%2520and%2520thus%2520it%2520can%2520handle%2520large%2520graphs%2520more%2520effectively%2520than%2520other%250Aapproaches%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opinion%20de-polarization%20of%20social%20networks%20with%20GNNs&entry.906535625=Konstantinos%20Mylonas%20and%20Thrasyvoulos%20Spyropoulos&entry.1292438233=%20%20Nowadays%2C%20social%20media%20is%20the%20ground%20for%20political%20debate%20and%20exchange%20of%0Aopinions.%20There%20is%20a%20significant%20amount%20of%20research%20that%20suggests%20that%20social%0Amedia%20are%20highly%20polarized.%20A%20phenomenon%20that%20is%20commonly%20observed%20is%20the%20echo%0Achamber%20structure%2C%20where%20users%20are%20organized%20in%20polarized%20communities%20and%20form%0Aconnections%20only%20with%20similar-minded%20individuals%2C%20limiting%20themselves%20to%0Aconsume%20specific%20content.%20In%20this%20paper%20we%20explore%20a%20way%20to%20decrease%20the%0Apolarization%20of%20networks%20with%20two%20echo%20chambers.%20Particularly%2C%20we%20observe%20that%0Aif%20some%20users%20adopt%20a%20moderate%20opinion%20about%20a%20topic%2C%20the%20polarization%20of%20the%0Anetwork%20decreases.%20Based%20on%20this%20observation%2C%20we%20propose%20an%20efficient%20algorithm%0Ato%20identify%20a%20good%20set%20of%20K%20users%2C%20such%20that%20if%20they%20adopt%20a%20moderate%20stance%0Aaround%20a%20topic%2C%20the%20polarization%20is%20minimized.%20Our%20algorithm%20employs%20a%20Graph%0ANeural%20Network%20and%20thus%20it%20can%20handle%20large%20graphs%20more%20effectively%20than%20other%0Aapproaches%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09404v1&entry.124074799=Read"},
{"title": "Optimising TinyML with Quantization and Distillation of Transformer and\n  Mamba Models for Indoor Localisation on Edge Devices", "author": "Thanaphon Suwannaphong and Ferdian Jovan and Ian Craddock and Ryan McConville", "abstract": "  This paper proposes small and efficient machine learning models (TinyML) for\nresource-constrained edge devices, specifically for on-device indoor\nlocalisation. Typical approaches for indoor localisation rely on centralised\nremote processing of data transmitted from lower powered devices such as\nwearables. However, there are several benefits for moving this to the edge\ndevice itself, including increased battery life, enhanced privacy, reduced\nlatency and lowered operational costs, all of which are key for common\napplications such as health monitoring. The work focuses on model compression\ntechniques, including quantization and knowledge distillation, to significantly\nreduce the model size while maintaining high predictive performance. We base\nour work on a large state-of-the-art transformer-based model and seek to deploy\nit within low-power MCUs. We also propose a state-space-based architecture\nusing Mamba as a more compact alternative to the transformer. Our results show\nthat the quantized transformer model performs well within a 64 KB RAM\nconstraint, achieving an effective balance between model size and localisation\nprecision. Additionally, the compact Mamba model has strong performance under\neven tighter constraints, such as a 32 KB of RAM, without the need for model\ncompression, making it a viable option for more resource-limited environments.\nWe demonstrate that, through our framework, it is feasible to deploy advanced\nindoor localisation models onto low-power MCUs with restricted memory\nlimitations. The application of these TinyML models in healthcare has the\npotential to revolutionize patient monitoring by providing accurate, real-time\nlocation data while minimizing power consumption, increasing data privacy,\nimproving latency and reducing infrastructure costs.\n", "link": "http://arxiv.org/abs/2412.09289v1", "date": "2024-12-12", "relevancy": 2.0177, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5329}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimising%20TinyML%20with%20Quantization%20and%20Distillation%20of%20Transformer%20and%0A%20%20Mamba%20Models%20for%20Indoor%20Localisation%20on%20Edge%20Devices&body=Title%3A%20Optimising%20TinyML%20with%20Quantization%20and%20Distillation%20of%20Transformer%20and%0A%20%20Mamba%20Models%20for%20Indoor%20Localisation%20on%20Edge%20Devices%0AAuthor%3A%20Thanaphon%20Suwannaphong%20and%20Ferdian%20Jovan%20and%20Ian%20Craddock%20and%20Ryan%20McConville%0AAbstract%3A%20%20%20This%20paper%20proposes%20small%20and%20efficient%20machine%20learning%20models%20%28TinyML%29%20for%0Aresource-constrained%20edge%20devices%2C%20specifically%20for%20on-device%20indoor%0Alocalisation.%20Typical%20approaches%20for%20indoor%20localisation%20rely%20on%20centralised%0Aremote%20processing%20of%20data%20transmitted%20from%20lower%20powered%20devices%20such%20as%0Awearables.%20However%2C%20there%20are%20several%20benefits%20for%20moving%20this%20to%20the%20edge%0Adevice%20itself%2C%20including%20increased%20battery%20life%2C%20enhanced%20privacy%2C%20reduced%0Alatency%20and%20lowered%20operational%20costs%2C%20all%20of%20which%20are%20key%20for%20common%0Aapplications%20such%20as%20health%20monitoring.%20The%20work%20focuses%20on%20model%20compression%0Atechniques%2C%20including%20quantization%20and%20knowledge%20distillation%2C%20to%20significantly%0Areduce%20the%20model%20size%20while%20maintaining%20high%20predictive%20performance.%20We%20base%0Aour%20work%20on%20a%20large%20state-of-the-art%20transformer-based%20model%20and%20seek%20to%20deploy%0Ait%20within%20low-power%20MCUs.%20We%20also%20propose%20a%20state-space-based%20architecture%0Ausing%20Mamba%20as%20a%20more%20compact%20alternative%20to%20the%20transformer.%20Our%20results%20show%0Athat%20the%20quantized%20transformer%20model%20performs%20well%20within%20a%2064%20KB%20RAM%0Aconstraint%2C%20achieving%20an%20effective%20balance%20between%20model%20size%20and%20localisation%0Aprecision.%20Additionally%2C%20the%20compact%20Mamba%20model%20has%20strong%20performance%20under%0Aeven%20tighter%20constraints%2C%20such%20as%20a%2032%20KB%20of%20RAM%2C%20without%20the%20need%20for%20model%0Acompression%2C%20making%20it%20a%20viable%20option%20for%20more%20resource-limited%20environments.%0AWe%20demonstrate%20that%2C%20through%20our%20framework%2C%20it%20is%20feasible%20to%20deploy%20advanced%0Aindoor%20localisation%20models%20onto%20low-power%20MCUs%20with%20restricted%20memory%0Alimitations.%20The%20application%20of%20these%20TinyML%20models%20in%20healthcare%20has%20the%0Apotential%20to%20revolutionize%20patient%20monitoring%20by%20providing%20accurate%2C%20real-time%0Alocation%20data%20while%20minimizing%20power%20consumption%2C%20increasing%20data%20privacy%2C%0Aimproving%20latency%20and%20reducing%20infrastructure%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimising%2520TinyML%2520with%2520Quantization%2520and%2520Distillation%2520of%2520Transformer%2520and%250A%2520%2520Mamba%2520Models%2520for%2520Indoor%2520Localisation%2520on%2520Edge%2520Devices%26entry.906535625%3DThanaphon%2520Suwannaphong%2520and%2520Ferdian%2520Jovan%2520and%2520Ian%2520Craddock%2520and%2520Ryan%2520McConville%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520small%2520and%2520efficient%2520machine%2520learning%2520models%2520%2528TinyML%2529%2520for%250Aresource-constrained%2520edge%2520devices%252C%2520specifically%2520for%2520on-device%2520indoor%250Alocalisation.%2520Typical%2520approaches%2520for%2520indoor%2520localisation%2520rely%2520on%2520centralised%250Aremote%2520processing%2520of%2520data%2520transmitted%2520from%2520lower%2520powered%2520devices%2520such%2520as%250Awearables.%2520However%252C%2520there%2520are%2520several%2520benefits%2520for%2520moving%2520this%2520to%2520the%2520edge%250Adevice%2520itself%252C%2520including%2520increased%2520battery%2520life%252C%2520enhanced%2520privacy%252C%2520reduced%250Alatency%2520and%2520lowered%2520operational%2520costs%252C%2520all%2520of%2520which%2520are%2520key%2520for%2520common%250Aapplications%2520such%2520as%2520health%2520monitoring.%2520The%2520work%2520focuses%2520on%2520model%2520compression%250Atechniques%252C%2520including%2520quantization%2520and%2520knowledge%2520distillation%252C%2520to%2520significantly%250Areduce%2520the%2520model%2520size%2520while%2520maintaining%2520high%2520predictive%2520performance.%2520We%2520base%250Aour%2520work%2520on%2520a%2520large%2520state-of-the-art%2520transformer-based%2520model%2520and%2520seek%2520to%2520deploy%250Ait%2520within%2520low-power%2520MCUs.%2520We%2520also%2520propose%2520a%2520state-space-based%2520architecture%250Ausing%2520Mamba%2520as%2520a%2520more%2520compact%2520alternative%2520to%2520the%2520transformer.%2520Our%2520results%2520show%250Athat%2520the%2520quantized%2520transformer%2520model%2520performs%2520well%2520within%2520a%252064%2520KB%2520RAM%250Aconstraint%252C%2520achieving%2520an%2520effective%2520balance%2520between%2520model%2520size%2520and%2520localisation%250Aprecision.%2520Additionally%252C%2520the%2520compact%2520Mamba%2520model%2520has%2520strong%2520performance%2520under%250Aeven%2520tighter%2520constraints%252C%2520such%2520as%2520a%252032%2520KB%2520of%2520RAM%252C%2520without%2520the%2520need%2520for%2520model%250Acompression%252C%2520making%2520it%2520a%2520viable%2520option%2520for%2520more%2520resource-limited%2520environments.%250AWe%2520demonstrate%2520that%252C%2520through%2520our%2520framework%252C%2520it%2520is%2520feasible%2520to%2520deploy%2520advanced%250Aindoor%2520localisation%2520models%2520onto%2520low-power%2520MCUs%2520with%2520restricted%2520memory%250Alimitations.%2520The%2520application%2520of%2520these%2520TinyML%2520models%2520in%2520healthcare%2520has%2520the%250Apotential%2520to%2520revolutionize%2520patient%2520monitoring%2520by%2520providing%2520accurate%252C%2520real-time%250Alocation%2520data%2520while%2520minimizing%2520power%2520consumption%252C%2520increasing%2520data%2520privacy%252C%250Aimproving%2520latency%2520and%2520reducing%2520infrastructure%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimising%20TinyML%20with%20Quantization%20and%20Distillation%20of%20Transformer%20and%0A%20%20Mamba%20Models%20for%20Indoor%20Localisation%20on%20Edge%20Devices&entry.906535625=Thanaphon%20Suwannaphong%20and%20Ferdian%20Jovan%20and%20Ian%20Craddock%20and%20Ryan%20McConville&entry.1292438233=%20%20This%20paper%20proposes%20small%20and%20efficient%20machine%20learning%20models%20%28TinyML%29%20for%0Aresource-constrained%20edge%20devices%2C%20specifically%20for%20on-device%20indoor%0Alocalisation.%20Typical%20approaches%20for%20indoor%20localisation%20rely%20on%20centralised%0Aremote%20processing%20of%20data%20transmitted%20from%20lower%20powered%20devices%20such%20as%0Awearables.%20However%2C%20there%20are%20several%20benefits%20for%20moving%20this%20to%20the%20edge%0Adevice%20itself%2C%20including%20increased%20battery%20life%2C%20enhanced%20privacy%2C%20reduced%0Alatency%20and%20lowered%20operational%20costs%2C%20all%20of%20which%20are%20key%20for%20common%0Aapplications%20such%20as%20health%20monitoring.%20The%20work%20focuses%20on%20model%20compression%0Atechniques%2C%20including%20quantization%20and%20knowledge%20distillation%2C%20to%20significantly%0Areduce%20the%20model%20size%20while%20maintaining%20high%20predictive%20performance.%20We%20base%0Aour%20work%20on%20a%20large%20state-of-the-art%20transformer-based%20model%20and%20seek%20to%20deploy%0Ait%20within%20low-power%20MCUs.%20We%20also%20propose%20a%20state-space-based%20architecture%0Ausing%20Mamba%20as%20a%20more%20compact%20alternative%20to%20the%20transformer.%20Our%20results%20show%0Athat%20the%20quantized%20transformer%20model%20performs%20well%20within%20a%2064%20KB%20RAM%0Aconstraint%2C%20achieving%20an%20effective%20balance%20between%20model%20size%20and%20localisation%0Aprecision.%20Additionally%2C%20the%20compact%20Mamba%20model%20has%20strong%20performance%20under%0Aeven%20tighter%20constraints%2C%20such%20as%20a%2032%20KB%20of%20RAM%2C%20without%20the%20need%20for%20model%0Acompression%2C%20making%20it%20a%20viable%20option%20for%20more%20resource-limited%20environments.%0AWe%20demonstrate%20that%2C%20through%20our%20framework%2C%20it%20is%20feasible%20to%20deploy%20advanced%0Aindoor%20localisation%20models%20onto%20low-power%20MCUs%20with%20restricted%20memory%0Alimitations.%20The%20application%20of%20these%20TinyML%20models%20in%20healthcare%20has%20the%0Apotential%20to%20revolutionize%20patient%20monitoring%20by%20providing%20accurate%2C%20real-time%0Alocation%20data%20while%20minimizing%20power%20consumption%2C%20increasing%20data%20privacy%2C%0Aimproving%20latency%20and%20reducing%20infrastructure%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09289v1&entry.124074799=Read"},
{"title": "Parallel simulation for sampling under isoperimetry and score-based\n  diffusion models", "author": "Huanjian Zhou and Masashi Sugiyama", "abstract": "  In recent years, there has been a surge of interest in proving discretization\nbounds for sampling under isoperimetry and for diffusion models. As data size\ngrows, reducing the iteration cost becomes an important goal. Inspired by the\ngreat success of the parallel simulation of the initial value problem in\nscientific computation, we propose parallel Picard methods for sampling tasks.\nRigorous theoretical analysis reveals that our algorithm achieves better\ndependence on dimension $d$ than prior works in iteration complexity (i.e.,\nreduced from $\\widetilde{O}(\\log^2 d)$ to $\\widetilde{O}(\\log d)$), which is\neven optimal for sampling under isoperimetry with specific iteration\ncomplexity. Our work highlights the potential advantages of simulation methods\nin scientific computation for dynamics-based sampling and diffusion models.\n", "link": "http://arxiv.org/abs/2412.07435v2", "date": "2024-12-12", "relevancy": 2.0948, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5609}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5163}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20simulation%20for%20sampling%20under%20isoperimetry%20and%20score-based%0A%20%20diffusion%20models&body=Title%3A%20Parallel%20simulation%20for%20sampling%20under%20isoperimetry%20and%20score-based%0A%20%20diffusion%20models%0AAuthor%3A%20Huanjian%20Zhou%20and%20Masashi%20Sugiyama%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20a%20surge%20of%20interest%20in%20proving%20discretization%0Abounds%20for%20sampling%20under%20isoperimetry%20and%20for%20diffusion%20models.%20As%20data%20size%0Agrows%2C%20reducing%20the%20iteration%20cost%20becomes%20an%20important%20goal.%20Inspired%20by%20the%0Agreat%20success%20of%20the%20parallel%20simulation%20of%20the%20initial%20value%20problem%20in%0Ascientific%20computation%2C%20we%20propose%20parallel%20Picard%20methods%20for%20sampling%20tasks.%0ARigorous%20theoretical%20analysis%20reveals%20that%20our%20algorithm%20achieves%20better%0Adependence%20on%20dimension%20%24d%24%20than%20prior%20works%20in%20iteration%20complexity%20%28i.e.%2C%0Areduced%20from%20%24%5Cwidetilde%7BO%7D%28%5Clog%5E2%20d%29%24%20to%20%24%5Cwidetilde%7BO%7D%28%5Clog%20d%29%24%29%2C%20which%20is%0Aeven%20optimal%20for%20sampling%20under%20isoperimetry%20with%20specific%20iteration%0Acomplexity.%20Our%20work%20highlights%20the%20potential%20advantages%20of%20simulation%20methods%0Ain%20scientific%20computation%20for%20dynamics-based%20sampling%20and%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520simulation%2520for%2520sampling%2520under%2520isoperimetry%2520and%2520score-based%250A%2520%2520diffusion%2520models%26entry.906535625%3DHuanjian%2520Zhou%2520and%2520Masashi%2520Sugiyama%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520a%2520surge%2520of%2520interest%2520in%2520proving%2520discretization%250Abounds%2520for%2520sampling%2520under%2520isoperimetry%2520and%2520for%2520diffusion%2520models.%2520As%2520data%2520size%250Agrows%252C%2520reducing%2520the%2520iteration%2520cost%2520becomes%2520an%2520important%2520goal.%2520Inspired%2520by%2520the%250Agreat%2520success%2520of%2520the%2520parallel%2520simulation%2520of%2520the%2520initial%2520value%2520problem%2520in%250Ascientific%2520computation%252C%2520we%2520propose%2520parallel%2520Picard%2520methods%2520for%2520sampling%2520tasks.%250ARigorous%2520theoretical%2520analysis%2520reveals%2520that%2520our%2520algorithm%2520achieves%2520better%250Adependence%2520on%2520dimension%2520%2524d%2524%2520than%2520prior%2520works%2520in%2520iteration%2520complexity%2520%2528i.e.%252C%250Areduced%2520from%2520%2524%255Cwidetilde%257BO%257D%2528%255Clog%255E2%2520d%2529%2524%2520to%2520%2524%255Cwidetilde%257BO%257D%2528%255Clog%2520d%2529%2524%2529%252C%2520which%2520is%250Aeven%2520optimal%2520for%2520sampling%2520under%2520isoperimetry%2520with%2520specific%2520iteration%250Acomplexity.%2520Our%2520work%2520highlights%2520the%2520potential%2520advantages%2520of%2520simulation%2520methods%250Ain%2520scientific%2520computation%2520for%2520dynamics-based%2520sampling%2520and%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20simulation%20for%20sampling%20under%20isoperimetry%20and%20score-based%0A%20%20diffusion%20models&entry.906535625=Huanjian%20Zhou%20and%20Masashi%20Sugiyama&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20a%20surge%20of%20interest%20in%20proving%20discretization%0Abounds%20for%20sampling%20under%20isoperimetry%20and%20for%20diffusion%20models.%20As%20data%20size%0Agrows%2C%20reducing%20the%20iteration%20cost%20becomes%20an%20important%20goal.%20Inspired%20by%20the%0Agreat%20success%20of%20the%20parallel%20simulation%20of%20the%20initial%20value%20problem%20in%0Ascientific%20computation%2C%20we%20propose%20parallel%20Picard%20methods%20for%20sampling%20tasks.%0ARigorous%20theoretical%20analysis%20reveals%20that%20our%20algorithm%20achieves%20better%0Adependence%20on%20dimension%20%24d%24%20than%20prior%20works%20in%20iteration%20complexity%20%28i.e.%2C%0Areduced%20from%20%24%5Cwidetilde%7BO%7D%28%5Clog%5E2%20d%29%24%20to%20%24%5Cwidetilde%7BO%7D%28%5Clog%20d%29%24%29%2C%20which%20is%0Aeven%20optimal%20for%20sampling%20under%20isoperimetry%20with%20specific%20iteration%0Acomplexity.%20Our%20work%20highlights%20the%20potential%20advantages%20of%20simulation%20methods%0Ain%20scientific%20computation%20for%20dynamics-based%20sampling%20and%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07435v2&entry.124074799=Read"},
{"title": "Auto-Regressive Moving Diffusion Models for Time Series Forecasting", "author": "Jiaxin Gao and Qinglong Cao and Yuntian Chen", "abstract": "  Time series forecasting (TSF) is essential in various domains, and recent\nadvancements in diffusion-based TSF models have shown considerable promise.\nHowever, these models typically adopt traditional diffusion patterns, treating\nTSF as a noise-based conditional generation task. This approach neglects the\ninherent continuous sequential nature of time series, leading to a fundamental\nmisalignment between diffusion mechanisms and the TSF objective, thereby\nseverely impairing performance. To bridge this misalignment, and inspired by\nthe classic Auto-Regressive Moving Average (ARMA) theory, which views time\nseries as continuous sequential progressions evolving from previous data\npoints, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to\nfirst achieve the continuous sequential diffusion-based TSF. Unlike previous\nmethods that start from white Gaussian noise, our model employs chain-based\ndiffusion with priors, accurately modeling the evolution of time series and\nleveraging intermediate state information to improve forecasting accuracy and\nstability. Specifically, our approach reinterprets the diffusion process by\nconsidering future series as the initial state and historical series as the\nfinal state, with intermediate series generated using a sliding-based technique\nduring the forward process. This design aligns the diffusion model's sampling\nprocedure with the forecasting objective, resulting in an unconditional,\ncontinuous sequential diffusion TSF model. Extensive experiments conducted on\nseven widely used datasets demonstrate that our model achieves state-of-the-art\nperformance, significantly outperforming existing diffusion-based TSF models.\nOur code is available on GitHub: https://github.com/daxin007/ARMD.\n", "link": "http://arxiv.org/abs/2412.09328v1", "date": "2024-12-12", "relevancy": 1.0629, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5888}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5147}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Regressive%20Moving%20Diffusion%20Models%20for%20Time%20Series%20Forecasting&body=Title%3A%20Auto-Regressive%20Moving%20Diffusion%20Models%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Jiaxin%20Gao%20and%20Qinglong%20Cao%20and%20Yuntian%20Chen%0AAbstract%3A%20%20%20Time%20series%20forecasting%20%28TSF%29%20is%20essential%20in%20various%20domains%2C%20and%20recent%0Aadvancements%20in%20diffusion-based%20TSF%20models%20have%20shown%20considerable%20promise.%0AHowever%2C%20these%20models%20typically%20adopt%20traditional%20diffusion%20patterns%2C%20treating%0ATSF%20as%20a%20noise-based%20conditional%20generation%20task.%20This%20approach%20neglects%20the%0Ainherent%20continuous%20sequential%20nature%20of%20time%20series%2C%20leading%20to%20a%20fundamental%0Amisalignment%20between%20diffusion%20mechanisms%20and%20the%20TSF%20objective%2C%20thereby%0Aseverely%20impairing%20performance.%20To%20bridge%20this%20misalignment%2C%20and%20inspired%20by%0Athe%20classic%20Auto-Regressive%20Moving%20Average%20%28ARMA%29%20theory%2C%20which%20views%20time%0Aseries%20as%20continuous%20sequential%20progressions%20evolving%20from%20previous%20data%0Apoints%2C%20we%20propose%20a%20novel%20Auto-Regressive%20Moving%20Diffusion%20%28ARMD%29%20model%20to%0Afirst%20achieve%20the%20continuous%20sequential%20diffusion-based%20TSF.%20Unlike%20previous%0Amethods%20that%20start%20from%20white%20Gaussian%20noise%2C%20our%20model%20employs%20chain-based%0Adiffusion%20with%20priors%2C%20accurately%20modeling%20the%20evolution%20of%20time%20series%20and%0Aleveraging%20intermediate%20state%20information%20to%20improve%20forecasting%20accuracy%20and%0Astability.%20Specifically%2C%20our%20approach%20reinterprets%20the%20diffusion%20process%20by%0Aconsidering%20future%20series%20as%20the%20initial%20state%20and%20historical%20series%20as%20the%0Afinal%20state%2C%20with%20intermediate%20series%20generated%20using%20a%20sliding-based%20technique%0Aduring%20the%20forward%20process.%20This%20design%20aligns%20the%20diffusion%20model%27s%20sampling%0Aprocedure%20with%20the%20forecasting%20objective%2C%20resulting%20in%20an%20unconditional%2C%0Acontinuous%20sequential%20diffusion%20TSF%20model.%20Extensive%20experiments%20conducted%20on%0Aseven%20widely%20used%20datasets%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%0Aperformance%2C%20significantly%20outperforming%20existing%20diffusion-based%20TSF%20models.%0AOur%20code%20is%20available%20on%20GitHub%3A%20https%3A//github.com/daxin007/ARMD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Regressive%2520Moving%2520Diffusion%2520Models%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DJiaxin%2520Gao%2520and%2520Qinglong%2520Cao%2520and%2520Yuntian%2520Chen%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasting%2520%2528TSF%2529%2520is%2520essential%2520in%2520various%2520domains%252C%2520and%2520recent%250Aadvancements%2520in%2520diffusion-based%2520TSF%2520models%2520have%2520shown%2520considerable%2520promise.%250AHowever%252C%2520these%2520models%2520typically%2520adopt%2520traditional%2520diffusion%2520patterns%252C%2520treating%250ATSF%2520as%2520a%2520noise-based%2520conditional%2520generation%2520task.%2520This%2520approach%2520neglects%2520the%250Ainherent%2520continuous%2520sequential%2520nature%2520of%2520time%2520series%252C%2520leading%2520to%2520a%2520fundamental%250Amisalignment%2520between%2520diffusion%2520mechanisms%2520and%2520the%2520TSF%2520objective%252C%2520thereby%250Aseverely%2520impairing%2520performance.%2520To%2520bridge%2520this%2520misalignment%252C%2520and%2520inspired%2520by%250Athe%2520classic%2520Auto-Regressive%2520Moving%2520Average%2520%2528ARMA%2529%2520theory%252C%2520which%2520views%2520time%250Aseries%2520as%2520continuous%2520sequential%2520progressions%2520evolving%2520from%2520previous%2520data%250Apoints%252C%2520we%2520propose%2520a%2520novel%2520Auto-Regressive%2520Moving%2520Diffusion%2520%2528ARMD%2529%2520model%2520to%250Afirst%2520achieve%2520the%2520continuous%2520sequential%2520diffusion-based%2520TSF.%2520Unlike%2520previous%250Amethods%2520that%2520start%2520from%2520white%2520Gaussian%2520noise%252C%2520our%2520model%2520employs%2520chain-based%250Adiffusion%2520with%2520priors%252C%2520accurately%2520modeling%2520the%2520evolution%2520of%2520time%2520series%2520and%250Aleveraging%2520intermediate%2520state%2520information%2520to%2520improve%2520forecasting%2520accuracy%2520and%250Astability.%2520Specifically%252C%2520our%2520approach%2520reinterprets%2520the%2520diffusion%2520process%2520by%250Aconsidering%2520future%2520series%2520as%2520the%2520initial%2520state%2520and%2520historical%2520series%2520as%2520the%250Afinal%2520state%252C%2520with%2520intermediate%2520series%2520generated%2520using%2520a%2520sliding-based%2520technique%250Aduring%2520the%2520forward%2520process.%2520This%2520design%2520aligns%2520the%2520diffusion%2520model%2527s%2520sampling%250Aprocedure%2520with%2520the%2520forecasting%2520objective%252C%2520resulting%2520in%2520an%2520unconditional%252C%250Acontinuous%2520sequential%2520diffusion%2520TSF%2520model.%2520Extensive%2520experiments%2520conducted%2520on%250Aseven%2520widely%2520used%2520datasets%2520demonstrate%2520that%2520our%2520model%2520achieves%2520state-of-the-art%250Aperformance%252C%2520significantly%2520outperforming%2520existing%2520diffusion-based%2520TSF%2520models.%250AOur%2520code%2520is%2520available%2520on%2520GitHub%253A%2520https%253A//github.com/daxin007/ARMD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Regressive%20Moving%20Diffusion%20Models%20for%20Time%20Series%20Forecasting&entry.906535625=Jiaxin%20Gao%20and%20Qinglong%20Cao%20and%20Yuntian%20Chen&entry.1292438233=%20%20Time%20series%20forecasting%20%28TSF%29%20is%20essential%20in%20various%20domains%2C%20and%20recent%0Aadvancements%20in%20diffusion-based%20TSF%20models%20have%20shown%20considerable%20promise.%0AHowever%2C%20these%20models%20typically%20adopt%20traditional%20diffusion%20patterns%2C%20treating%0ATSF%20as%20a%20noise-based%20conditional%20generation%20task.%20This%20approach%20neglects%20the%0Ainherent%20continuous%20sequential%20nature%20of%20time%20series%2C%20leading%20to%20a%20fundamental%0Amisalignment%20between%20diffusion%20mechanisms%20and%20the%20TSF%20objective%2C%20thereby%0Aseverely%20impairing%20performance.%20To%20bridge%20this%20misalignment%2C%20and%20inspired%20by%0Athe%20classic%20Auto-Regressive%20Moving%20Average%20%28ARMA%29%20theory%2C%20which%20views%20time%0Aseries%20as%20continuous%20sequential%20progressions%20evolving%20from%20previous%20data%0Apoints%2C%20we%20propose%20a%20novel%20Auto-Regressive%20Moving%20Diffusion%20%28ARMD%29%20model%20to%0Afirst%20achieve%20the%20continuous%20sequential%20diffusion-based%20TSF.%20Unlike%20previous%0Amethods%20that%20start%20from%20white%20Gaussian%20noise%2C%20our%20model%20employs%20chain-based%0Adiffusion%20with%20priors%2C%20accurately%20modeling%20the%20evolution%20of%20time%20series%20and%0Aleveraging%20intermediate%20state%20information%20to%20improve%20forecasting%20accuracy%20and%0Astability.%20Specifically%2C%20our%20approach%20reinterprets%20the%20diffusion%20process%20by%0Aconsidering%20future%20series%20as%20the%20initial%20state%20and%20historical%20series%20as%20the%0Afinal%20state%2C%20with%20intermediate%20series%20generated%20using%20a%20sliding-based%20technique%0Aduring%20the%20forward%20process.%20This%20design%20aligns%20the%20diffusion%20model%27s%20sampling%0Aprocedure%20with%20the%20forecasting%20objective%2C%20resulting%20in%20an%20unconditional%2C%0Acontinuous%20sequential%20diffusion%20TSF%20model.%20Extensive%20experiments%20conducted%20on%0Aseven%20widely%20used%20datasets%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%0Aperformance%2C%20significantly%20outperforming%20existing%20diffusion-based%20TSF%20models.%0AOur%20code%20is%20available%20on%20GitHub%3A%20https%3A//github.com/daxin007/ARMD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09328v1&entry.124074799=Read"},
{"title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation", "author": "Konstantin Burlachenko and Peter Richt\u00e1rik", "abstract": "  Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.\n", "link": "http://arxiv.org/abs/2410.08760v2", "date": "2024-12-12", "relevancy": 1.9742, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5164}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.48}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20FedNL%3A%20Self-Contained%20Compute-Optimized%20Implementation&body=Title%3A%20Unlocking%20FedNL%3A%20Self-Contained%20Compute-Optimized%20Implementation%0AAuthor%3A%20Konstantin%20Burlachenko%20and%20Peter%20Richt%C3%A1rik%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20an%20emerging%20paradigm%20that%20enables%20intelligent%0Aagents%20to%20collaboratively%20train%20Machine%20Learning%20%28ML%29%20models%20in%20a%20distributed%0Amanner%2C%20eliminating%20the%20need%20for%20sharing%20their%20local%20data.%20The%20recent%20work%0A%28arXiv%3A2106.02969%29%20introduces%20a%20family%20of%20Federated%20Newton%20Learn%20%28FedNL%29%0Aalgorithms%2C%20marking%20a%20significant%20step%20towards%20applying%20second-order%20methods%20to%0AFL%20and%20large-scale%20optimization.%20However%2C%20the%20reference%20FedNL%20prototype%0Aexhibits%20three%20serious%20practical%20drawbacks%3A%20%28i%29%20It%20requires%204.8%20hours%20to%20launch%0Aa%20single%20experiment%20in%20a%20sever-grade%20workstation%3B%20%28ii%29%20The%20prototype%20only%0Asimulates%20multi-node%20setting%3B%20%28iii%29%20Prototype%20integration%20into%0Aresource-constrained%20applications%20is%20challenging.%20To%20bridge%20the%20gap%20between%0Atheory%20and%20practice%2C%20we%20present%20a%20self-contained%20implementation%20of%20FedNL%2C%0AFedNL-LS%2C%20FedNL-PP%20for%20single-node%20and%20multi-node%20settings.%20Our%20work%20resolves%0Athe%20aforementioned%20issues%20and%20reduces%20the%20wall%20clock%20time%20by%20x1000.%20With%20this%0AFedNL%20outperforms%20alternatives%20for%20training%20logistic%20regression%20in%20a%0Asingle-node%20--%20CVXPY%20%28arXiv%3A1603.00943%29%2C%20and%20in%20a%20multi-node%20--%20Apache%20Spark%0A%28arXiv%3A1505.06807%29%2C%20Ray/Scikit-Learn%20%28arXiv%3A1712.05889%29.%20Finally%2C%20we%20propose%0Atwo%20practical-orientated%20compressors%20for%20FedNL%20-%20adaptive%20TopLEK%20and%0Acache-aware%20RandSeqK%2C%20which%20fulfill%20the%20theory%20of%20FedNL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520FedNL%253A%2520Self-Contained%2520Compute-Optimized%2520Implementation%26entry.906535625%3DKonstantin%2520Burlachenko%2520and%2520Peter%2520Richt%25C3%25A1rik%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520an%2520emerging%2520paradigm%2520that%2520enables%2520intelligent%250Aagents%2520to%2520collaboratively%2520train%2520Machine%2520Learning%2520%2528ML%2529%2520models%2520in%2520a%2520distributed%250Amanner%252C%2520eliminating%2520the%2520need%2520for%2520sharing%2520their%2520local%2520data.%2520The%2520recent%2520work%250A%2528arXiv%253A2106.02969%2529%2520introduces%2520a%2520family%2520of%2520Federated%2520Newton%2520Learn%2520%2528FedNL%2529%250Aalgorithms%252C%2520marking%2520a%2520significant%2520step%2520towards%2520applying%2520second-order%2520methods%2520to%250AFL%2520and%2520large-scale%2520optimization.%2520However%252C%2520the%2520reference%2520FedNL%2520prototype%250Aexhibits%2520three%2520serious%2520practical%2520drawbacks%253A%2520%2528i%2529%2520It%2520requires%25204.8%2520hours%2520to%2520launch%250Aa%2520single%2520experiment%2520in%2520a%2520sever-grade%2520workstation%253B%2520%2528ii%2529%2520The%2520prototype%2520only%250Asimulates%2520multi-node%2520setting%253B%2520%2528iii%2529%2520Prototype%2520integration%2520into%250Aresource-constrained%2520applications%2520is%2520challenging.%2520To%2520bridge%2520the%2520gap%2520between%250Atheory%2520and%2520practice%252C%2520we%2520present%2520a%2520self-contained%2520implementation%2520of%2520FedNL%252C%250AFedNL-LS%252C%2520FedNL-PP%2520for%2520single-node%2520and%2520multi-node%2520settings.%2520Our%2520work%2520resolves%250Athe%2520aforementioned%2520issues%2520and%2520reduces%2520the%2520wall%2520clock%2520time%2520by%2520x1000.%2520With%2520this%250AFedNL%2520outperforms%2520alternatives%2520for%2520training%2520logistic%2520regression%2520in%2520a%250Asingle-node%2520--%2520CVXPY%2520%2528arXiv%253A1603.00943%2529%252C%2520and%2520in%2520a%2520multi-node%2520--%2520Apache%2520Spark%250A%2528arXiv%253A1505.06807%2529%252C%2520Ray/Scikit-Learn%2520%2528arXiv%253A1712.05889%2529.%2520Finally%252C%2520we%2520propose%250Atwo%2520practical-orientated%2520compressors%2520for%2520FedNL%2520-%2520adaptive%2520TopLEK%2520and%250Acache-aware%2520RandSeqK%252C%2520which%2520fulfill%2520the%2520theory%2520of%2520FedNL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20FedNL%3A%20Self-Contained%20Compute-Optimized%20Implementation&entry.906535625=Konstantin%20Burlachenko%20and%20Peter%20Richt%C3%A1rik&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20an%20emerging%20paradigm%20that%20enables%20intelligent%0Aagents%20to%20collaboratively%20train%20Machine%20Learning%20%28ML%29%20models%20in%20a%20distributed%0Amanner%2C%20eliminating%20the%20need%20for%20sharing%20their%20local%20data.%20The%20recent%20work%0A%28arXiv%3A2106.02969%29%20introduces%20a%20family%20of%20Federated%20Newton%20Learn%20%28FedNL%29%0Aalgorithms%2C%20marking%20a%20significant%20step%20towards%20applying%20second-order%20methods%20to%0AFL%20and%20large-scale%20optimization.%20However%2C%20the%20reference%20FedNL%20prototype%0Aexhibits%20three%20serious%20practical%20drawbacks%3A%20%28i%29%20It%20requires%204.8%20hours%20to%20launch%0Aa%20single%20experiment%20in%20a%20sever-grade%20workstation%3B%20%28ii%29%20The%20prototype%20only%0Asimulates%20multi-node%20setting%3B%20%28iii%29%20Prototype%20integration%20into%0Aresource-constrained%20applications%20is%20challenging.%20To%20bridge%20the%20gap%20between%0Atheory%20and%20practice%2C%20we%20present%20a%20self-contained%20implementation%20of%20FedNL%2C%0AFedNL-LS%2C%20FedNL-PP%20for%20single-node%20and%20multi-node%20settings.%20Our%20work%20resolves%0Athe%20aforementioned%20issues%20and%20reduces%20the%20wall%20clock%20time%20by%20x1000.%20With%20this%0AFedNL%20outperforms%20alternatives%20for%20training%20logistic%20regression%20in%20a%0Asingle-node%20--%20CVXPY%20%28arXiv%3A1603.00943%29%2C%20and%20in%20a%20multi-node%20--%20Apache%20Spark%0A%28arXiv%3A1505.06807%29%2C%20Ray/Scikit-Learn%20%28arXiv%3A1712.05889%29.%20Finally%2C%20we%20propose%0Atwo%20practical-orientated%20compressors%20for%20FedNL%20-%20adaptive%20TopLEK%20and%0Acache-aware%20RandSeqK%2C%20which%20fulfill%20the%20theory%20of%20FedNL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08760v2&entry.124074799=Read"},
{"title": "Solving Multiagent Path Finding on Highly Centralized Networks", "author": "Foivos Fioravantes and Du\u0161an Knop and Jan Maty\u00e1\u0161 K\u0159i\u0161\u0165an and Nikolaos Melissinos and Michal Opler and Tung Anh Vu", "abstract": "  The Mutliagent Path Finding (MAPF) problem consists of identifying the\ntrajectories that a set of agents should follow inside a given network in order\nto reach their desired destinations as soon as possible, but without colliding\nwith each other. We aim to minimize the maximum time any agent takes to reach\ntheir goal, ensuring optimal path length. In this work, we complement a recent\nthread of results that aim to systematically study the algorithmic behavior of\nthis problem, through the parameterized complexity point of view.\n  First, we show that MAPF is NP-hard when the given network has a star-like\ntopology (bounded vertex cover number) or is a tree with $11$ leaves. Both of\nthese results fill important gaps in our understanding of the tractability of\nthis problem that were left untreated in the recent work of [Fioravantes et al.\nExact Algorithms and Lowerbounds for Multiagent Path Finding: Power of Treelike\nTopology. AAAI'24]. Nevertheless, our main contribution is an exact algorithm\nthat scales well as the input grows (FPT) when the topology of the given\nnetwork is highly centralized (bounded distance to clique). This parameter is\nsignificant as it mirrors real-world networks. In such environments, a bunch of\ncentral hubs (e.g., processing areas) are connected to only few peripheral\nnodes.\n", "link": "http://arxiv.org/abs/2412.09433v1", "date": "2024-12-12", "relevancy": 1.8748, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4923}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.467}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Multiagent%20Path%20Finding%20on%20Highly%20Centralized%20Networks&body=Title%3A%20Solving%20Multiagent%20Path%20Finding%20on%20Highly%20Centralized%20Networks%0AAuthor%3A%20Foivos%20Fioravantes%20and%20Du%C5%A1an%20Knop%20and%20Jan%20Maty%C3%A1%C5%A1%20K%C5%99i%C5%A1%C5%A5an%20and%20Nikolaos%20Melissinos%20and%20Michal%20Opler%20and%20Tung%20Anh%20Vu%0AAbstract%3A%20%20%20The%20Mutliagent%20Path%20Finding%20%28MAPF%29%20problem%20consists%20of%20identifying%20the%0Atrajectories%20that%20a%20set%20of%20agents%20should%20follow%20inside%20a%20given%20network%20in%20order%0Ato%20reach%20their%20desired%20destinations%20as%20soon%20as%20possible%2C%20but%20without%20colliding%0Awith%20each%20other.%20We%20aim%20to%20minimize%20the%20maximum%20time%20any%20agent%20takes%20to%20reach%0Atheir%20goal%2C%20ensuring%20optimal%20path%20length.%20In%20this%20work%2C%20we%20complement%20a%20recent%0Athread%20of%20results%20that%20aim%20to%20systematically%20study%20the%20algorithmic%20behavior%20of%0Athis%20problem%2C%20through%20the%20parameterized%20complexity%20point%20of%20view.%0A%20%20First%2C%20we%20show%20that%20MAPF%20is%20NP-hard%20when%20the%20given%20network%20has%20a%20star-like%0Atopology%20%28bounded%20vertex%20cover%20number%29%20or%20is%20a%20tree%20with%20%2411%24%20leaves.%20Both%20of%0Athese%20results%20fill%20important%20gaps%20in%20our%20understanding%20of%20the%20tractability%20of%0Athis%20problem%20that%20were%20left%20untreated%20in%20the%20recent%20work%20of%20%5BFioravantes%20et%20al.%0AExact%20Algorithms%20and%20Lowerbounds%20for%20Multiagent%20Path%20Finding%3A%20Power%20of%20Treelike%0ATopology.%20AAAI%2724%5D.%20Nevertheless%2C%20our%20main%20contribution%20is%20an%20exact%20algorithm%0Athat%20scales%20well%20as%20the%20input%20grows%20%28FPT%29%20when%20the%20topology%20of%20the%20given%0Anetwork%20is%20highly%20centralized%20%28bounded%20distance%20to%20clique%29.%20This%20parameter%20is%0Asignificant%20as%20it%20mirrors%20real-world%20networks.%20In%20such%20environments%2C%20a%20bunch%20of%0Acentral%20hubs%20%28e.g.%2C%20processing%20areas%29%20are%20connected%20to%20only%20few%20peripheral%0Anodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Multiagent%2520Path%2520Finding%2520on%2520Highly%2520Centralized%2520Networks%26entry.906535625%3DFoivos%2520Fioravantes%2520and%2520Du%25C5%25A1an%2520Knop%2520and%2520Jan%2520Maty%25C3%25A1%25C5%25A1%2520K%25C5%2599i%25C5%25A1%25C5%25A5an%2520and%2520Nikolaos%2520Melissinos%2520and%2520Michal%2520Opler%2520and%2520Tung%2520Anh%2520Vu%26entry.1292438233%3D%2520%2520The%2520Mutliagent%2520Path%2520Finding%2520%2528MAPF%2529%2520problem%2520consists%2520of%2520identifying%2520the%250Atrajectories%2520that%2520a%2520set%2520of%2520agents%2520should%2520follow%2520inside%2520a%2520given%2520network%2520in%2520order%250Ato%2520reach%2520their%2520desired%2520destinations%2520as%2520soon%2520as%2520possible%252C%2520but%2520without%2520colliding%250Awith%2520each%2520other.%2520We%2520aim%2520to%2520minimize%2520the%2520maximum%2520time%2520any%2520agent%2520takes%2520to%2520reach%250Atheir%2520goal%252C%2520ensuring%2520optimal%2520path%2520length.%2520In%2520this%2520work%252C%2520we%2520complement%2520a%2520recent%250Athread%2520of%2520results%2520that%2520aim%2520to%2520systematically%2520study%2520the%2520algorithmic%2520behavior%2520of%250Athis%2520problem%252C%2520through%2520the%2520parameterized%2520complexity%2520point%2520of%2520view.%250A%2520%2520First%252C%2520we%2520show%2520that%2520MAPF%2520is%2520NP-hard%2520when%2520the%2520given%2520network%2520has%2520a%2520star-like%250Atopology%2520%2528bounded%2520vertex%2520cover%2520number%2529%2520or%2520is%2520a%2520tree%2520with%2520%252411%2524%2520leaves.%2520Both%2520of%250Athese%2520results%2520fill%2520important%2520gaps%2520in%2520our%2520understanding%2520of%2520the%2520tractability%2520of%250Athis%2520problem%2520that%2520were%2520left%2520untreated%2520in%2520the%2520recent%2520work%2520of%2520%255BFioravantes%2520et%2520al.%250AExact%2520Algorithms%2520and%2520Lowerbounds%2520for%2520Multiagent%2520Path%2520Finding%253A%2520Power%2520of%2520Treelike%250ATopology.%2520AAAI%252724%255D.%2520Nevertheless%252C%2520our%2520main%2520contribution%2520is%2520an%2520exact%2520algorithm%250Athat%2520scales%2520well%2520as%2520the%2520input%2520grows%2520%2528FPT%2529%2520when%2520the%2520topology%2520of%2520the%2520given%250Anetwork%2520is%2520highly%2520centralized%2520%2528bounded%2520distance%2520to%2520clique%2529.%2520This%2520parameter%2520is%250Asignificant%2520as%2520it%2520mirrors%2520real-world%2520networks.%2520In%2520such%2520environments%252C%2520a%2520bunch%2520of%250Acentral%2520hubs%2520%2528e.g.%252C%2520processing%2520areas%2529%2520are%2520connected%2520to%2520only%2520few%2520peripheral%250Anodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Multiagent%20Path%20Finding%20on%20Highly%20Centralized%20Networks&entry.906535625=Foivos%20Fioravantes%20and%20Du%C5%A1an%20Knop%20and%20Jan%20Maty%C3%A1%C5%A1%20K%C5%99i%C5%A1%C5%A5an%20and%20Nikolaos%20Melissinos%20and%20Michal%20Opler%20and%20Tung%20Anh%20Vu&entry.1292438233=%20%20The%20Mutliagent%20Path%20Finding%20%28MAPF%29%20problem%20consists%20of%20identifying%20the%0Atrajectories%20that%20a%20set%20of%20agents%20should%20follow%20inside%20a%20given%20network%20in%20order%0Ato%20reach%20their%20desired%20destinations%20as%20soon%20as%20possible%2C%20but%20without%20colliding%0Awith%20each%20other.%20We%20aim%20to%20minimize%20the%20maximum%20time%20any%20agent%20takes%20to%20reach%0Atheir%20goal%2C%20ensuring%20optimal%20path%20length.%20In%20this%20work%2C%20we%20complement%20a%20recent%0Athread%20of%20results%20that%20aim%20to%20systematically%20study%20the%20algorithmic%20behavior%20of%0Athis%20problem%2C%20through%20the%20parameterized%20complexity%20point%20of%20view.%0A%20%20First%2C%20we%20show%20that%20MAPF%20is%20NP-hard%20when%20the%20given%20network%20has%20a%20star-like%0Atopology%20%28bounded%20vertex%20cover%20number%29%20or%20is%20a%20tree%20with%20%2411%24%20leaves.%20Both%20of%0Athese%20results%20fill%20important%20gaps%20in%20our%20understanding%20of%20the%20tractability%20of%0Athis%20problem%20that%20were%20left%20untreated%20in%20the%20recent%20work%20of%20%5BFioravantes%20et%20al.%0AExact%20Algorithms%20and%20Lowerbounds%20for%20Multiagent%20Path%20Finding%3A%20Power%20of%20Treelike%0ATopology.%20AAAI%2724%5D.%20Nevertheless%2C%20our%20main%20contribution%20is%20an%20exact%20algorithm%0Athat%20scales%20well%20as%20the%20input%20grows%20%28FPT%29%20when%20the%20topology%20of%20the%20given%0Anetwork%20is%20highly%20centralized%20%28bounded%20distance%20to%20clique%29.%20This%20parameter%20is%0Asignificant%20as%20it%20mirrors%20real-world%20networks.%20In%20such%20environments%2C%20a%20bunch%20of%0Acentral%20hubs%20%28e.g.%2C%20processing%20areas%29%20are%20connected%20to%20only%20few%20peripheral%0Anodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09433v1&entry.124074799=Read"},
{"title": "FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot\n  Medical Image Segmentation", "author": "Yuntian Bo and Yazhou Zhu and Lunbo Li and Haofeng Zhang", "abstract": "  Existing few-shot medical image segmentation (FSMIS) models fail to address a\npractical issue in medical imaging: the domain shift caused by different\nimaging techniques, which limits the applicability to current FSMIS tasks. To\novercome this limitation, we focus on the cross-domain few-shot medical image\nsegmentation (CD-FSMIS) task, aiming to develop a generalized model capable of\nadapting to a broader range of medical image segmentation scenarios with\nlimited labeled data from the novel target domain. Inspired by the\ncharacteristics of frequency domain similarity across different domains, we\npropose a Frequency-aware Matching Network (FAMNet), which includes two key\ncomponents: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion\n(MSF) module. The FAM module tackles two problems during the meta-learning\nphase: 1) intra-domain variance caused by the inherent support-query bias, due\nto the different appearances of organs and lesions, and 2) inter-domain\nvariance caused by different medical imaging techniques. Additionally, we\ndesign an MSF module to integrate the different frequency features decoupled by\nthe FAM module, and further mitigate the impact of inter-domain variance on the\nmodel's segmentation performance. Combining these two modules, our FAMNet\nsurpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation\nmodels on three cross-domain datasets, achieving state-of-the-art performance\nin the CD-FSMIS task.\n", "link": "http://arxiv.org/abs/2412.09319v1", "date": "2024-12-12", "relevancy": 2.0643, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5082}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAMNet%3A%20Frequency-aware%20Matching%20Network%20for%20Cross-domain%20Few-shot%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20FAMNet%3A%20Frequency-aware%20Matching%20Network%20for%20Cross-domain%20Few-shot%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yuntian%20Bo%20and%20Yazhou%20Zhu%20and%20Lunbo%20Li%20and%20Haofeng%20Zhang%0AAbstract%3A%20%20%20Existing%20few-shot%20medical%20image%20segmentation%20%28FSMIS%29%20models%20fail%20to%20address%20a%0Apractical%20issue%20in%20medical%20imaging%3A%20the%20domain%20shift%20caused%20by%20different%0Aimaging%20techniques%2C%20which%20limits%20the%20applicability%20to%20current%20FSMIS%20tasks.%20To%0Aovercome%20this%20limitation%2C%20we%20focus%20on%20the%20cross-domain%20few-shot%20medical%20image%0Asegmentation%20%28CD-FSMIS%29%20task%2C%20aiming%20to%20develop%20a%20generalized%20model%20capable%20of%0Aadapting%20to%20a%20broader%20range%20of%20medical%20image%20segmentation%20scenarios%20with%0Alimited%20labeled%20data%20from%20the%20novel%20target%20domain.%20Inspired%20by%20the%0Acharacteristics%20of%20frequency%20domain%20similarity%20across%20different%20domains%2C%20we%0Apropose%20a%20Frequency-aware%20Matching%20Network%20%28FAMNet%29%2C%20which%20includes%20two%20key%0Acomponents%3A%20a%20Frequency-aware%20Matching%20%28FAM%29%20module%20and%20a%20Multi-Spectral%20Fusion%0A%28MSF%29%20module.%20The%20FAM%20module%20tackles%20two%20problems%20during%20the%20meta-learning%0Aphase%3A%201%29%20intra-domain%20variance%20caused%20by%20the%20inherent%20support-query%20bias%2C%20due%0Ato%20the%20different%20appearances%20of%20organs%20and%20lesions%2C%20and%202%29%20inter-domain%0Avariance%20caused%20by%20different%20medical%20imaging%20techniques.%20Additionally%2C%20we%0Adesign%20an%20MSF%20module%20to%20integrate%20the%20different%20frequency%20features%20decoupled%20by%0Athe%20FAM%20module%2C%20and%20further%20mitigate%20the%20impact%20of%20inter-domain%20variance%20on%20the%0Amodel%27s%20segmentation%20performance.%20Combining%20these%20two%20modules%2C%20our%20FAMNet%0Asurpasses%20existing%20FSMIS%20models%20and%20Cross-domain%20Few-shot%20Semantic%20Segmentation%0Amodels%20on%20three%20cross-domain%20datasets%2C%20achieving%20state-of-the-art%20performance%0Ain%20the%20CD-FSMIS%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAMNet%253A%2520Frequency-aware%2520Matching%2520Network%2520for%2520Cross-domain%2520Few-shot%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DYuntian%2520Bo%2520and%2520Yazhou%2520Zhu%2520and%2520Lunbo%2520Li%2520and%2520Haofeng%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520few-shot%2520medical%2520image%2520segmentation%2520%2528FSMIS%2529%2520models%2520fail%2520to%2520address%2520a%250Apractical%2520issue%2520in%2520medical%2520imaging%253A%2520the%2520domain%2520shift%2520caused%2520by%2520different%250Aimaging%2520techniques%252C%2520which%2520limits%2520the%2520applicability%2520to%2520current%2520FSMIS%2520tasks.%2520To%250Aovercome%2520this%2520limitation%252C%2520we%2520focus%2520on%2520the%2520cross-domain%2520few-shot%2520medical%2520image%250Asegmentation%2520%2528CD-FSMIS%2529%2520task%252C%2520aiming%2520to%2520develop%2520a%2520generalized%2520model%2520capable%2520of%250Aadapting%2520to%2520a%2520broader%2520range%2520of%2520medical%2520image%2520segmentation%2520scenarios%2520with%250Alimited%2520labeled%2520data%2520from%2520the%2520novel%2520target%2520domain.%2520Inspired%2520by%2520the%250Acharacteristics%2520of%2520frequency%2520domain%2520similarity%2520across%2520different%2520domains%252C%2520we%250Apropose%2520a%2520Frequency-aware%2520Matching%2520Network%2520%2528FAMNet%2529%252C%2520which%2520includes%2520two%2520key%250Acomponents%253A%2520a%2520Frequency-aware%2520Matching%2520%2528FAM%2529%2520module%2520and%2520a%2520Multi-Spectral%2520Fusion%250A%2528MSF%2529%2520module.%2520The%2520FAM%2520module%2520tackles%2520two%2520problems%2520during%2520the%2520meta-learning%250Aphase%253A%25201%2529%2520intra-domain%2520variance%2520caused%2520by%2520the%2520inherent%2520support-query%2520bias%252C%2520due%250Ato%2520the%2520different%2520appearances%2520of%2520organs%2520and%2520lesions%252C%2520and%25202%2529%2520inter-domain%250Avariance%2520caused%2520by%2520different%2520medical%2520imaging%2520techniques.%2520Additionally%252C%2520we%250Adesign%2520an%2520MSF%2520module%2520to%2520integrate%2520the%2520different%2520frequency%2520features%2520decoupled%2520by%250Athe%2520FAM%2520module%252C%2520and%2520further%2520mitigate%2520the%2520impact%2520of%2520inter-domain%2520variance%2520on%2520the%250Amodel%2527s%2520segmentation%2520performance.%2520Combining%2520these%2520two%2520modules%252C%2520our%2520FAMNet%250Asurpasses%2520existing%2520FSMIS%2520models%2520and%2520Cross-domain%2520Few-shot%2520Semantic%2520Segmentation%250Amodels%2520on%2520three%2520cross-domain%2520datasets%252C%2520achieving%2520state-of-the-art%2520performance%250Ain%2520the%2520CD-FSMIS%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAMNet%3A%20Frequency-aware%20Matching%20Network%20for%20Cross-domain%20Few-shot%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Yuntian%20Bo%20and%20Yazhou%20Zhu%20and%20Lunbo%20Li%20and%20Haofeng%20Zhang&entry.1292438233=%20%20Existing%20few-shot%20medical%20image%20segmentation%20%28FSMIS%29%20models%20fail%20to%20address%20a%0Apractical%20issue%20in%20medical%20imaging%3A%20the%20domain%20shift%20caused%20by%20different%0Aimaging%20techniques%2C%20which%20limits%20the%20applicability%20to%20current%20FSMIS%20tasks.%20To%0Aovercome%20this%20limitation%2C%20we%20focus%20on%20the%20cross-domain%20few-shot%20medical%20image%0Asegmentation%20%28CD-FSMIS%29%20task%2C%20aiming%20to%20develop%20a%20generalized%20model%20capable%20of%0Aadapting%20to%20a%20broader%20range%20of%20medical%20image%20segmentation%20scenarios%20with%0Alimited%20labeled%20data%20from%20the%20novel%20target%20domain.%20Inspired%20by%20the%0Acharacteristics%20of%20frequency%20domain%20similarity%20across%20different%20domains%2C%20we%0Apropose%20a%20Frequency-aware%20Matching%20Network%20%28FAMNet%29%2C%20which%20includes%20two%20key%0Acomponents%3A%20a%20Frequency-aware%20Matching%20%28FAM%29%20module%20and%20a%20Multi-Spectral%20Fusion%0A%28MSF%29%20module.%20The%20FAM%20module%20tackles%20two%20problems%20during%20the%20meta-learning%0Aphase%3A%201%29%20intra-domain%20variance%20caused%20by%20the%20inherent%20support-query%20bias%2C%20due%0Ato%20the%20different%20appearances%20of%20organs%20and%20lesions%2C%20and%202%29%20inter-domain%0Avariance%20caused%20by%20different%20medical%20imaging%20techniques.%20Additionally%2C%20we%0Adesign%20an%20MSF%20module%20to%20integrate%20the%20different%20frequency%20features%20decoupled%20by%0Athe%20FAM%20module%2C%20and%20further%20mitigate%20the%20impact%20of%20inter-domain%20variance%20on%20the%0Amodel%27s%20segmentation%20performance.%20Combining%20these%20two%20modules%2C%20our%20FAMNet%0Asurpasses%20existing%20FSMIS%20models%20and%20Cross-domain%20Few-shot%20Semantic%20Segmentation%0Amodels%20on%20three%20cross-domain%20datasets%2C%20achieving%20state-of-the-art%20performance%0Ain%20the%20CD-FSMIS%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09319v1&entry.124074799=Read"},
{"title": "Finite-PINN: A Physics-Informed Neural Network Architecture for Solving\n  Solid Mechanics Problems with General Geometries", "author": "Haolin Li and Yuyang Miao and Zahra Sharif Khodaei and M. H. Aliabadi", "abstract": "  PINN models have demonstrated impressive capabilities in addressing fluid PDE\nproblems, and their potential in solid mechanics is beginning to emerge. This\nstudy identifies two key challenges when using PINN to solve general solid\nmechanics problems. These challenges become evident when comparing the\nlimitations of PINN with the well-established numerical methods commonly used\nin solid mechanics, such as the finite element method (FEM). Specifically: a)\nPINN models generate solutions over an infinite domain, which conflicts with\nthe finite boundaries typical of most solid structures; and b) the solution\nspace utilised by PINN is Euclidean, which is inadequate for addressing the\ncomplex geometries often present in solid structures.\n  This work proposes a PINN architecture used for general solid mechanics\nproblems, termed the Finite-PINN model. The proposed model aims to effectively\naddress these two challenges while preserving as much of the original\nimplementation of PINN as possible. The unique architecture of the Finite-PINN\nmodel addresses these challenges by separating the approximation of stress and\ndisplacement fields, and by transforming the solution space from the\ntraditional Euclidean space to a Euclidean-topological joint space. Several\ncase studies presented in this paper demonstrate that the Finite-PINN model\nprovides satisfactory results for a variety of problem types, including both\nforward and inverse problems, in both 2D and 3D contexts. The developed\nFinite-PINN model offers a promising tool for addressing general solid\nmechanics problems, particularly those not yet well-explored in current\nresearch.\n", "link": "http://arxiv.org/abs/2412.09453v1", "date": "2024-12-12", "relevancy": 1.8604, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4851}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4541}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite-PINN%3A%20A%20Physics-Informed%20Neural%20Network%20Architecture%20for%20Solving%0A%20%20Solid%20Mechanics%20Problems%20with%20General%20Geometries&body=Title%3A%20Finite-PINN%3A%20A%20Physics-Informed%20Neural%20Network%20Architecture%20for%20Solving%0A%20%20Solid%20Mechanics%20Problems%20with%20General%20Geometries%0AAuthor%3A%20Haolin%20Li%20and%20Yuyang%20Miao%20and%20Zahra%20Sharif%20Khodaei%20and%20M.%20H.%20Aliabadi%0AAbstract%3A%20%20%20PINN%20models%20have%20demonstrated%20impressive%20capabilities%20in%20addressing%20fluid%20PDE%0Aproblems%2C%20and%20their%20potential%20in%20solid%20mechanics%20is%20beginning%20to%20emerge.%20This%0Astudy%20identifies%20two%20key%20challenges%20when%20using%20PINN%20to%20solve%20general%20solid%0Amechanics%20problems.%20These%20challenges%20become%20evident%20when%20comparing%20the%0Alimitations%20of%20PINN%20with%20the%20well-established%20numerical%20methods%20commonly%20used%0Ain%20solid%20mechanics%2C%20such%20as%20the%20finite%20element%20method%20%28FEM%29.%20Specifically%3A%20a%29%0APINN%20models%20generate%20solutions%20over%20an%20infinite%20domain%2C%20which%20conflicts%20with%0Athe%20finite%20boundaries%20typical%20of%20most%20solid%20structures%3B%20and%20b%29%20the%20solution%0Aspace%20utilised%20by%20PINN%20is%20Euclidean%2C%20which%20is%20inadequate%20for%20addressing%20the%0Acomplex%20geometries%20often%20present%20in%20solid%20structures.%0A%20%20This%20work%20proposes%20a%20PINN%20architecture%20used%20for%20general%20solid%20mechanics%0Aproblems%2C%20termed%20the%20Finite-PINN%20model.%20The%20proposed%20model%20aims%20to%20effectively%0Aaddress%20these%20two%20challenges%20while%20preserving%20as%20much%20of%20the%20original%0Aimplementation%20of%20PINN%20as%20possible.%20The%20unique%20architecture%20of%20the%20Finite-PINN%0Amodel%20addresses%20these%20challenges%20by%20separating%20the%20approximation%20of%20stress%20and%0Adisplacement%20fields%2C%20and%20by%20transforming%20the%20solution%20space%20from%20the%0Atraditional%20Euclidean%20space%20to%20a%20Euclidean-topological%20joint%20space.%20Several%0Acase%20studies%20presented%20in%20this%20paper%20demonstrate%20that%20the%20Finite-PINN%20model%0Aprovides%20satisfactory%20results%20for%20a%20variety%20of%20problem%20types%2C%20including%20both%0Aforward%20and%20inverse%20problems%2C%20in%20both%202D%20and%203D%20contexts.%20The%20developed%0AFinite-PINN%20model%20offers%20a%20promising%20tool%20for%20addressing%20general%20solid%0Amechanics%20problems%2C%20particularly%20those%20not%20yet%20well-explored%20in%20current%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite-PINN%253A%2520A%2520Physics-Informed%2520Neural%2520Network%2520Architecture%2520for%2520Solving%250A%2520%2520Solid%2520Mechanics%2520Problems%2520with%2520General%2520Geometries%26entry.906535625%3DHaolin%2520Li%2520and%2520Yuyang%2520Miao%2520and%2520Zahra%2520Sharif%2520Khodaei%2520and%2520M.%2520H.%2520Aliabadi%26entry.1292438233%3D%2520%2520PINN%2520models%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520addressing%2520fluid%2520PDE%250Aproblems%252C%2520and%2520their%2520potential%2520in%2520solid%2520mechanics%2520is%2520beginning%2520to%2520emerge.%2520This%250Astudy%2520identifies%2520two%2520key%2520challenges%2520when%2520using%2520PINN%2520to%2520solve%2520general%2520solid%250Amechanics%2520problems.%2520These%2520challenges%2520become%2520evident%2520when%2520comparing%2520the%250Alimitations%2520of%2520PINN%2520with%2520the%2520well-established%2520numerical%2520methods%2520commonly%2520used%250Ain%2520solid%2520mechanics%252C%2520such%2520as%2520the%2520finite%2520element%2520method%2520%2528FEM%2529.%2520Specifically%253A%2520a%2529%250APINN%2520models%2520generate%2520solutions%2520over%2520an%2520infinite%2520domain%252C%2520which%2520conflicts%2520with%250Athe%2520finite%2520boundaries%2520typical%2520of%2520most%2520solid%2520structures%253B%2520and%2520b%2529%2520the%2520solution%250Aspace%2520utilised%2520by%2520PINN%2520is%2520Euclidean%252C%2520which%2520is%2520inadequate%2520for%2520addressing%2520the%250Acomplex%2520geometries%2520often%2520present%2520in%2520solid%2520structures.%250A%2520%2520This%2520work%2520proposes%2520a%2520PINN%2520architecture%2520used%2520for%2520general%2520solid%2520mechanics%250Aproblems%252C%2520termed%2520the%2520Finite-PINN%2520model.%2520The%2520proposed%2520model%2520aims%2520to%2520effectively%250Aaddress%2520these%2520two%2520challenges%2520while%2520preserving%2520as%2520much%2520of%2520the%2520original%250Aimplementation%2520of%2520PINN%2520as%2520possible.%2520The%2520unique%2520architecture%2520of%2520the%2520Finite-PINN%250Amodel%2520addresses%2520these%2520challenges%2520by%2520separating%2520the%2520approximation%2520of%2520stress%2520and%250Adisplacement%2520fields%252C%2520and%2520by%2520transforming%2520the%2520solution%2520space%2520from%2520the%250Atraditional%2520Euclidean%2520space%2520to%2520a%2520Euclidean-topological%2520joint%2520space.%2520Several%250Acase%2520studies%2520presented%2520in%2520this%2520paper%2520demonstrate%2520that%2520the%2520Finite-PINN%2520model%250Aprovides%2520satisfactory%2520results%2520for%2520a%2520variety%2520of%2520problem%2520types%252C%2520including%2520both%250Aforward%2520and%2520inverse%2520problems%252C%2520in%2520both%25202D%2520and%25203D%2520contexts.%2520The%2520developed%250AFinite-PINN%2520model%2520offers%2520a%2520promising%2520tool%2520for%2520addressing%2520general%2520solid%250Amechanics%2520problems%252C%2520particularly%2520those%2520not%2520yet%2520well-explored%2520in%2520current%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite-PINN%3A%20A%20Physics-Informed%20Neural%20Network%20Architecture%20for%20Solving%0A%20%20Solid%20Mechanics%20Problems%20with%20General%20Geometries&entry.906535625=Haolin%20Li%20and%20Yuyang%20Miao%20and%20Zahra%20Sharif%20Khodaei%20and%20M.%20H.%20Aliabadi&entry.1292438233=%20%20PINN%20models%20have%20demonstrated%20impressive%20capabilities%20in%20addressing%20fluid%20PDE%0Aproblems%2C%20and%20their%20potential%20in%20solid%20mechanics%20is%20beginning%20to%20emerge.%20This%0Astudy%20identifies%20two%20key%20challenges%20when%20using%20PINN%20to%20solve%20general%20solid%0Amechanics%20problems.%20These%20challenges%20become%20evident%20when%20comparing%20the%0Alimitations%20of%20PINN%20with%20the%20well-established%20numerical%20methods%20commonly%20used%0Ain%20solid%20mechanics%2C%20such%20as%20the%20finite%20element%20method%20%28FEM%29.%20Specifically%3A%20a%29%0APINN%20models%20generate%20solutions%20over%20an%20infinite%20domain%2C%20which%20conflicts%20with%0Athe%20finite%20boundaries%20typical%20of%20most%20solid%20structures%3B%20and%20b%29%20the%20solution%0Aspace%20utilised%20by%20PINN%20is%20Euclidean%2C%20which%20is%20inadequate%20for%20addressing%20the%0Acomplex%20geometries%20often%20present%20in%20solid%20structures.%0A%20%20This%20work%20proposes%20a%20PINN%20architecture%20used%20for%20general%20solid%20mechanics%0Aproblems%2C%20termed%20the%20Finite-PINN%20model.%20The%20proposed%20model%20aims%20to%20effectively%0Aaddress%20these%20two%20challenges%20while%20preserving%20as%20much%20of%20the%20original%0Aimplementation%20of%20PINN%20as%20possible.%20The%20unique%20architecture%20of%20the%20Finite-PINN%0Amodel%20addresses%20these%20challenges%20by%20separating%20the%20approximation%20of%20stress%20and%0Adisplacement%20fields%2C%20and%20by%20transforming%20the%20solution%20space%20from%20the%0Atraditional%20Euclidean%20space%20to%20a%20Euclidean-topological%20joint%20space.%20Several%0Acase%20studies%20presented%20in%20this%20paper%20demonstrate%20that%20the%20Finite-PINN%20model%0Aprovides%20satisfactory%20results%20for%20a%20variety%20of%20problem%20types%2C%20including%20both%0Aforward%20and%20inverse%20problems%2C%20in%20both%202D%20and%203D%20contexts.%20The%20developed%0AFinite-PINN%20model%20offers%20a%20promising%20tool%20for%20addressing%20general%20solid%0Amechanics%20problems%2C%20particularly%20those%20not%20yet%20well-explored%20in%20current%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09453v1&entry.124074799=Read"},
{"title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots", "author": "Dongge Han and Trevor McInroe and Adam Jelley and Stefano V. Albrecht and Peter Bell and Amos Storkey", "abstract": "  Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/.\n", "link": "http://arxiv.org/abs/2404.14285v2", "date": "2024-12-12", "relevancy": 1.6548, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5527}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Personalize%3A%20Aligning%20LLM%20Planners%20with%20Human%20Preferences%20via%0A%20%20Reinforced%20Self-Training%20for%20Housekeeping%20Robots&body=Title%3A%20LLM-Personalize%3A%20Aligning%20LLM%20Planners%20with%20Human%20Preferences%20via%0A%20%20Reinforced%20Self-Training%20for%20Housekeeping%20Robots%0AAuthor%3A%20Dongge%20Han%20and%20Trevor%20McInroe%20and%20Adam%20Jelley%20and%20Stefano%20V.%20Albrecht%20and%20Peter%20Bell%20and%20Amos%20Storkey%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20potential%20for%20robotics%0Aapplications%2C%20particularly%20task%20planning%2C%20by%20harnessing%20their%20language%0Acomprehension%20and%20text%20generation%20capabilities.%20However%2C%20in%20applications%20such%0Aas%20household%20robotics%2C%20a%20critical%20gap%20remains%20in%20the%20personalization%20of%20these%0Amodels%20to%20individual%20user%20preferences.%20We%20introduce%20LLM-Personalize%2C%20a%20novel%0Aframework%20with%20an%20optimization%20pipeline%20designed%20to%20personalize%20LLM%20planners%0Afor%20household%20robotics.%20Our%20LLM-Personalize%20framework%20features%20an%20LLM%20planner%0Athat%20performs%20iterative%20planning%20in%20multi-room%2C%20partially-observable%20household%0Ascenarios%2C%20making%20use%20of%20a%20scene%20graph%20constructed%20with%20local%20observations.%20The%0Agenerated%20plan%20consists%20of%20a%20sequence%20of%20high-level%20actions%20which%20are%0Asubsequently%20executed%20by%20a%20controller.%20Central%20to%20our%20approach%20is%20the%0Aoptimization%20pipeline%2C%20which%20combines%20imitation%20learning%20and%20iterative%0Aself-training%20to%20personalize%20the%20LLM%20planner.%20In%20particular%2C%20the%20imitation%0Alearning%20phase%20performs%20initial%20LLM%20alignment%20from%20demonstrations%2C%20and%0Abootstraps%20the%20model%20to%20facilitate%20effective%20iterative%20self-training%2C%20which%0Afurther%20explores%20and%20aligns%20the%20model%20to%20user%20preferences.%20We%20evaluate%0ALLM-Personalize%20on%20Housekeep%2C%20a%20challenging%20simulated%20real-world%203D%20benchmark%0Afor%20household%20rearrangements%2C%20and%20show%20that%20LLM-Personalize%20achieves%20more%20than%0Aa%2030%20percent%20increase%20in%20success%20rate%20over%20existing%20LLM%20planners%2C%20showcasing%0Asignificantly%20improved%20alignment%20with%20human%20preferences.%20Project%20page%3A%0Ahttps%3A//gdg94.github.io/projectllmpersonalize/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Personalize%253A%2520Aligning%2520LLM%2520Planners%2520with%2520Human%2520Preferences%2520via%250A%2520%2520Reinforced%2520Self-Training%2520for%2520Housekeeping%2520Robots%26entry.906535625%3DDongge%2520Han%2520and%2520Trevor%2520McInroe%2520and%2520Adam%2520Jelley%2520and%2520Stefano%2520V.%2520Albrecht%2520and%2520Peter%2520Bell%2520and%2520Amos%2520Storkey%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%2520potential%2520for%2520robotics%250Aapplications%252C%2520particularly%2520task%2520planning%252C%2520by%2520harnessing%2520their%2520language%250Acomprehension%2520and%2520text%2520generation%2520capabilities.%2520However%252C%2520in%2520applications%2520such%250Aas%2520household%2520robotics%252C%2520a%2520critical%2520gap%2520remains%2520in%2520the%2520personalization%2520of%2520these%250Amodels%2520to%2520individual%2520user%2520preferences.%2520We%2520introduce%2520LLM-Personalize%252C%2520a%2520novel%250Aframework%2520with%2520an%2520optimization%2520pipeline%2520designed%2520to%2520personalize%2520LLM%2520planners%250Afor%2520household%2520robotics.%2520Our%2520LLM-Personalize%2520framework%2520features%2520an%2520LLM%2520planner%250Athat%2520performs%2520iterative%2520planning%2520in%2520multi-room%252C%2520partially-observable%2520household%250Ascenarios%252C%2520making%2520use%2520of%2520a%2520scene%2520graph%2520constructed%2520with%2520local%2520observations.%2520The%250Agenerated%2520plan%2520consists%2520of%2520a%2520sequence%2520of%2520high-level%2520actions%2520which%2520are%250Asubsequently%2520executed%2520by%2520a%2520controller.%2520Central%2520to%2520our%2520approach%2520is%2520the%250Aoptimization%2520pipeline%252C%2520which%2520combines%2520imitation%2520learning%2520and%2520iterative%250Aself-training%2520to%2520personalize%2520the%2520LLM%2520planner.%2520In%2520particular%252C%2520the%2520imitation%250Alearning%2520phase%2520performs%2520initial%2520LLM%2520alignment%2520from%2520demonstrations%252C%2520and%250Abootstraps%2520the%2520model%2520to%2520facilitate%2520effective%2520iterative%2520self-training%252C%2520which%250Afurther%2520explores%2520and%2520aligns%2520the%2520model%2520to%2520user%2520preferences.%2520We%2520evaluate%250ALLM-Personalize%2520on%2520Housekeep%252C%2520a%2520challenging%2520simulated%2520real-world%25203D%2520benchmark%250Afor%2520household%2520rearrangements%252C%2520and%2520show%2520that%2520LLM-Personalize%2520achieves%2520more%2520than%250Aa%252030%2520percent%2520increase%2520in%2520success%2520rate%2520over%2520existing%2520LLM%2520planners%252C%2520showcasing%250Asignificantly%2520improved%2520alignment%2520with%2520human%2520preferences.%2520Project%2520page%253A%250Ahttps%253A//gdg94.github.io/projectllmpersonalize/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Personalize%3A%20Aligning%20LLM%20Planners%20with%20Human%20Preferences%20via%0A%20%20Reinforced%20Self-Training%20for%20Housekeeping%20Robots&entry.906535625=Dongge%20Han%20and%20Trevor%20McInroe%20and%20Adam%20Jelley%20and%20Stefano%20V.%20Albrecht%20and%20Peter%20Bell%20and%20Amos%20Storkey&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20potential%20for%20robotics%0Aapplications%2C%20particularly%20task%20planning%2C%20by%20harnessing%20their%20language%0Acomprehension%20and%20text%20generation%20capabilities.%20However%2C%20in%20applications%20such%0Aas%20household%20robotics%2C%20a%20critical%20gap%20remains%20in%20the%20personalization%20of%20these%0Amodels%20to%20individual%20user%20preferences.%20We%20introduce%20LLM-Personalize%2C%20a%20novel%0Aframework%20with%20an%20optimization%20pipeline%20designed%20to%20personalize%20LLM%20planners%0Afor%20household%20robotics.%20Our%20LLM-Personalize%20framework%20features%20an%20LLM%20planner%0Athat%20performs%20iterative%20planning%20in%20multi-room%2C%20partially-observable%20household%0Ascenarios%2C%20making%20use%20of%20a%20scene%20graph%20constructed%20with%20local%20observations.%20The%0Agenerated%20plan%20consists%20of%20a%20sequence%20of%20high-level%20actions%20which%20are%0Asubsequently%20executed%20by%20a%20controller.%20Central%20to%20our%20approach%20is%20the%0Aoptimization%20pipeline%2C%20which%20combines%20imitation%20learning%20and%20iterative%0Aself-training%20to%20personalize%20the%20LLM%20planner.%20In%20particular%2C%20the%20imitation%0Alearning%20phase%20performs%20initial%20LLM%20alignment%20from%20demonstrations%2C%20and%0Abootstraps%20the%20model%20to%20facilitate%20effective%20iterative%20self-training%2C%20which%0Afurther%20explores%20and%20aligns%20the%20model%20to%20user%20preferences.%20We%20evaluate%0ALLM-Personalize%20on%20Housekeep%2C%20a%20challenging%20simulated%20real-world%203D%20benchmark%0Afor%20household%20rearrangements%2C%20and%20show%20that%20LLM-Personalize%20achieves%20more%20than%0Aa%2030%20percent%20increase%20in%20success%20rate%20over%20existing%20LLM%20planners%2C%20showcasing%0Asignificantly%20improved%20alignment%20with%20human%20preferences.%20Project%20page%3A%0Ahttps%3A//gdg94.github.io/projectllmpersonalize/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14285v2&entry.124074799=Read"},
{"title": "FedAA: A Reinforcement Learning Perspective on Adaptive Aggregation for\n  Fair and Robust Federated Learning", "author": "Jialuo He and Wei Chen and Xiaojin Zhang", "abstract": "  Federated Learning (FL) has emerged as a promising approach for\nprivacy-preserving model training across decentralized devices. However, it\nfaces challenges such as statistical heterogeneity and susceptibility to\nadversarial attacks, which can impact model robustness and fairness.\nPersonalized FL attempts to provide some relief by customizing models for\nindividual clients. However, it falls short in addressing server-side\naggregation vulnerabilities. We introduce a novel method called \\textbf{FedAA},\nwhich optimizes client contributions via \\textbf{A}daptive \\textbf{A}ggregation\nto enhance model robustness against malicious clients and ensure fairness\nacross participants in non-identically distributed settings. To achieve this\ngoal, we propose an approach involving a Deep Deterministic Policy\nGradient-based algorithm for continuous control of aggregation weights, an\ninnovative client selection method based on model parameter distances, and a\nreward mechanism guided by validation set performance. Empirically, extensive\nexperiments demonstrate that, in terms of robustness, \\textbf{FedAA}\noutperforms the state-of-the-art methods, while maintaining comparable levels\nof fairness, offering a promising solution to build resilient and fair\nfederated systems. Our code is available at https://github.com/Gp1g/FedAA.\n", "link": "http://arxiv.org/abs/2402.05541v2", "date": "2024-12-12", "relevancy": 1.9201, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4821}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedAA%3A%20A%20Reinforcement%20Learning%20Perspective%20on%20Adaptive%20Aggregation%20for%0A%20%20Fair%20and%20Robust%20Federated%20Learning&body=Title%3A%20FedAA%3A%20A%20Reinforcement%20Learning%20Perspective%20on%20Adaptive%20Aggregation%20for%0A%20%20Fair%20and%20Robust%20Federated%20Learning%0AAuthor%3A%20Jialuo%20He%20and%20Wei%20Chen%20and%20Xiaojin%20Zhang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20approach%20for%0Aprivacy-preserving%20model%20training%20across%20decentralized%20devices.%20However%2C%20it%0Afaces%20challenges%20such%20as%20statistical%20heterogeneity%20and%20susceptibility%20to%0Aadversarial%20attacks%2C%20which%20can%20impact%20model%20robustness%20and%20fairness.%0APersonalized%20FL%20attempts%20to%20provide%20some%20relief%20by%20customizing%20models%20for%0Aindividual%20clients.%20However%2C%20it%20falls%20short%20in%20addressing%20server-side%0Aaggregation%20vulnerabilities.%20We%20introduce%20a%20novel%20method%20called%20%5Ctextbf%7BFedAA%7D%2C%0Awhich%20optimizes%20client%20contributions%20via%20%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BA%7Dggregation%0Ato%20enhance%20model%20robustness%20against%20malicious%20clients%20and%20ensure%20fairness%0Aacross%20participants%20in%20non-identically%20distributed%20settings.%20To%20achieve%20this%0Agoal%2C%20we%20propose%20an%20approach%20involving%20a%20Deep%20Deterministic%20Policy%0AGradient-based%20algorithm%20for%20continuous%20control%20of%20aggregation%20weights%2C%20an%0Ainnovative%20client%20selection%20method%20based%20on%20model%20parameter%20distances%2C%20and%20a%0Areward%20mechanism%20guided%20by%20validation%20set%20performance.%20Empirically%2C%20extensive%0Aexperiments%20demonstrate%20that%2C%20in%20terms%20of%20robustness%2C%20%5Ctextbf%7BFedAA%7D%0Aoutperforms%20the%20state-of-the-art%20methods%2C%20while%20maintaining%20comparable%20levels%0Aof%20fairness%2C%20offering%20a%20promising%20solution%20to%20build%20resilient%20and%20fair%0Afederated%20systems.%20Our%20code%20is%20available%20at%20https%3A//github.com/Gp1g/FedAA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedAA%253A%2520A%2520Reinforcement%2520Learning%2520Perspective%2520on%2520Adaptive%2520Aggregation%2520for%250A%2520%2520Fair%2520and%2520Robust%2520Federated%2520Learning%26entry.906535625%3DJialuo%2520He%2520and%2520Wei%2520Chen%2520and%2520Xiaojin%2520Zhang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%250Aprivacy-preserving%2520model%2520training%2520across%2520decentralized%2520devices.%2520However%252C%2520it%250Afaces%2520challenges%2520such%2520as%2520statistical%2520heterogeneity%2520and%2520susceptibility%2520to%250Aadversarial%2520attacks%252C%2520which%2520can%2520impact%2520model%2520robustness%2520and%2520fairness.%250APersonalized%2520FL%2520attempts%2520to%2520provide%2520some%2520relief%2520by%2520customizing%2520models%2520for%250Aindividual%2520clients.%2520However%252C%2520it%2520falls%2520short%2520in%2520addressing%2520server-side%250Aaggregation%2520vulnerabilities.%2520We%2520introduce%2520a%2520novel%2520method%2520called%2520%255Ctextbf%257BFedAA%257D%252C%250Awhich%2520optimizes%2520client%2520contributions%2520via%2520%255Ctextbf%257BA%257Ddaptive%2520%255Ctextbf%257BA%257Dggregation%250Ato%2520enhance%2520model%2520robustness%2520against%2520malicious%2520clients%2520and%2520ensure%2520fairness%250Aacross%2520participants%2520in%2520non-identically%2520distributed%2520settings.%2520To%2520achieve%2520this%250Agoal%252C%2520we%2520propose%2520an%2520approach%2520involving%2520a%2520Deep%2520Deterministic%2520Policy%250AGradient-based%2520algorithm%2520for%2520continuous%2520control%2520of%2520aggregation%2520weights%252C%2520an%250Ainnovative%2520client%2520selection%2520method%2520based%2520on%2520model%2520parameter%2520distances%252C%2520and%2520a%250Areward%2520mechanism%2520guided%2520by%2520validation%2520set%2520performance.%2520Empirically%252C%2520extensive%250Aexperiments%2520demonstrate%2520that%252C%2520in%2520terms%2520of%2520robustness%252C%2520%255Ctextbf%257BFedAA%257D%250Aoutperforms%2520the%2520state-of-the-art%2520methods%252C%2520while%2520maintaining%2520comparable%2520levels%250Aof%2520fairness%252C%2520offering%2520a%2520promising%2520solution%2520to%2520build%2520resilient%2520and%2520fair%250Afederated%2520systems.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Gp1g/FedAA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedAA%3A%20A%20Reinforcement%20Learning%20Perspective%20on%20Adaptive%20Aggregation%20for%0A%20%20Fair%20and%20Robust%20Federated%20Learning&entry.906535625=Jialuo%20He%20and%20Wei%20Chen%20and%20Xiaojin%20Zhang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20approach%20for%0Aprivacy-preserving%20model%20training%20across%20decentralized%20devices.%20However%2C%20it%0Afaces%20challenges%20such%20as%20statistical%20heterogeneity%20and%20susceptibility%20to%0Aadversarial%20attacks%2C%20which%20can%20impact%20model%20robustness%20and%20fairness.%0APersonalized%20FL%20attempts%20to%20provide%20some%20relief%20by%20customizing%20models%20for%0Aindividual%20clients.%20However%2C%20it%20falls%20short%20in%20addressing%20server-side%0Aaggregation%20vulnerabilities.%20We%20introduce%20a%20novel%20method%20called%20%5Ctextbf%7BFedAA%7D%2C%0Awhich%20optimizes%20client%20contributions%20via%20%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BA%7Dggregation%0Ato%20enhance%20model%20robustness%20against%20malicious%20clients%20and%20ensure%20fairness%0Aacross%20participants%20in%20non-identically%20distributed%20settings.%20To%20achieve%20this%0Agoal%2C%20we%20propose%20an%20approach%20involving%20a%20Deep%20Deterministic%20Policy%0AGradient-based%20algorithm%20for%20continuous%20control%20of%20aggregation%20weights%2C%20an%0Ainnovative%20client%20selection%20method%20based%20on%20model%20parameter%20distances%2C%20and%20a%0Areward%20mechanism%20guided%20by%20validation%20set%20performance.%20Empirically%2C%20extensive%0Aexperiments%20demonstrate%20that%2C%20in%20terms%20of%20robustness%2C%20%5Ctextbf%7BFedAA%7D%0Aoutperforms%20the%20state-of-the-art%20methods%2C%20while%20maintaining%20comparable%20levels%0Aof%20fairness%2C%20offering%20a%20promising%20solution%20to%20build%20resilient%20and%20fair%0Afederated%20systems.%20Our%20code%20is%20available%20at%20https%3A//github.com/Gp1g/FedAA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05541v2&entry.124074799=Read"},
{"title": "Accuracy Improvements for Convolutional and Differential Distance\n  Function Approximations", "author": "Alexander Belyaev and Pierre-Alain Fayolle", "abstract": "  Given a bounded domain, we deal with the problem of estimating the distance\nfunction from the internal points of the domain to the boundary of the domain.\nConvolutional and differential distance estimation schemes are considered and,\nfor both the schemes, accuracy improvements are proposed and evaluated.\nAsymptotics of Laplace integrals and Taylor series extrapolations are used to\nachieve the improvements.\n", "link": "http://arxiv.org/abs/2412.09200v1", "date": "2024-12-12", "relevancy": 1.7384, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4431}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4335}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy%20Improvements%20for%20Convolutional%20and%20Differential%20Distance%0A%20%20Function%20Approximations&body=Title%3A%20Accuracy%20Improvements%20for%20Convolutional%20and%20Differential%20Distance%0A%20%20Function%20Approximations%0AAuthor%3A%20Alexander%20Belyaev%20and%20Pierre-Alain%20Fayolle%0AAbstract%3A%20%20%20Given%20a%20bounded%20domain%2C%20we%20deal%20with%20the%20problem%20of%20estimating%20the%20distance%0Afunction%20from%20the%20internal%20points%20of%20the%20domain%20to%20the%20boundary%20of%20the%20domain.%0AConvolutional%20and%20differential%20distance%20estimation%20schemes%20are%20considered%20and%2C%0Afor%20both%20the%20schemes%2C%20accuracy%20improvements%20are%20proposed%20and%20evaluated.%0AAsymptotics%20of%20Laplace%20integrals%20and%20Taylor%20series%20extrapolations%20are%20used%20to%0Aachieve%20the%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy%2520Improvements%2520for%2520Convolutional%2520and%2520Differential%2520Distance%250A%2520%2520Function%2520Approximations%26entry.906535625%3DAlexander%2520Belyaev%2520and%2520Pierre-Alain%2520Fayolle%26entry.1292438233%3D%2520%2520Given%2520a%2520bounded%2520domain%252C%2520we%2520deal%2520with%2520the%2520problem%2520of%2520estimating%2520the%2520distance%250Afunction%2520from%2520the%2520internal%2520points%2520of%2520the%2520domain%2520to%2520the%2520boundary%2520of%2520the%2520domain.%250AConvolutional%2520and%2520differential%2520distance%2520estimation%2520schemes%2520are%2520considered%2520and%252C%250Afor%2520both%2520the%2520schemes%252C%2520accuracy%2520improvements%2520are%2520proposed%2520and%2520evaluated.%250AAsymptotics%2520of%2520Laplace%2520integrals%2520and%2520Taylor%2520series%2520extrapolations%2520are%2520used%2520to%250Aachieve%2520the%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy%20Improvements%20for%20Convolutional%20and%20Differential%20Distance%0A%20%20Function%20Approximations&entry.906535625=Alexander%20Belyaev%20and%20Pierre-Alain%20Fayolle&entry.1292438233=%20%20Given%20a%20bounded%20domain%2C%20we%20deal%20with%20the%20problem%20of%20estimating%20the%20distance%0Afunction%20from%20the%20internal%20points%20of%20the%20domain%20to%20the%20boundary%20of%20the%20domain.%0AConvolutional%20and%20differential%20distance%20estimation%20schemes%20are%20considered%20and%2C%0Afor%20both%20the%20schemes%2C%20accuracy%20improvements%20are%20proposed%20and%20evaluated.%0AAsymptotics%20of%20Laplace%20integrals%20and%20Taylor%20series%20extrapolations%20are%20used%20to%0Aachieve%20the%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09200v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


