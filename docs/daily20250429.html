<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250428.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep\n  Learning Refined SLAM", "author": "Leon Davies and Baihua Li and Mohamad Saada and Simon S\u00f8lvsten and Qinggang Meng", "abstract": "  SLAM (Simultaneous Localisation and Mapping) is a crucial component for\nrobotic systems, providing a map of an environment, the current location and\nprevious trajectory of a robot. While 3D LiDAR SLAM has received notable\nimprovements in recent years, 2D SLAM lags behind. Gradual drifts in odometry\nand pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in\nlarge complex environments. Dynamic robotic motion coupled with inherent\nestimation based SLAM processes introduce noise and errors, degrading map\nquality. Occupancy Grid Mapping (OGM) produces results that are often noisy and\nunclear. This is due to the fact that evidence based mapping represents maps\naccording to uncertain observations. This is why OGMs are so popular in\nexploration or navigation tasks. However, this also limits OGMs' effectiveness\nfor specific mapping based tasks such as floor plan creation in complex scenes.\nTo address this, we propose our novel Transformation and Translation Occupancy\nGrid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation\ntechniques from 3D SLAM to the world of 2D and mitigate errors to improve map\nquality using Generative Adversarial Networks (GANs). We introduce a novel data\ngeneration method via deep reinforcement learning (DRL) to build datasets large\nenough for training a GAN for SLAM error correction. We demonstrate our SLAM in\nreal-time on data collected at Loughborough University. We also prove its\ngeneralisability on a variety of large complex environments on a collection of\nlarge scale well-known 2D occupancy maps. Our novel approach enables the\ncreation of high quality OGMs in complex scenes, far surpassing the\ncapabilities of current SLAM algorithms in terms of quality, accuracy and\nreliability.\n", "link": "http://arxiv.org/abs/2504.19654v1", "date": "2025-04-28", "relevancy": 3.1192, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6725}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6106}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformation%20%26%20Translation%20Occupancy%20Grid%20Mapping%3A%202-Dimensional%20Deep%0A%20%20Learning%20Refined%20SLAM&body=Title%3A%20Transformation%20%26%20Translation%20Occupancy%20Grid%20Mapping%3A%202-Dimensional%20Deep%0A%20%20Learning%20Refined%20SLAM%0AAuthor%3A%20Leon%20Davies%20and%20Baihua%20Li%20and%20Mohamad%20Saada%20and%20Simon%20S%C3%B8lvsten%20and%20Qinggang%20Meng%0AAbstract%3A%20%20%20SLAM%20%28Simultaneous%20Localisation%20and%20Mapping%29%20is%20a%20crucial%20component%20for%0Arobotic%20systems%2C%20providing%20a%20map%20of%20an%20environment%2C%20the%20current%20location%20and%0Aprevious%20trajectory%20of%20a%20robot.%20While%203D%20LiDAR%20SLAM%20has%20received%20notable%0Aimprovements%20in%20recent%20years%2C%202D%20SLAM%20lags%20behind.%20Gradual%20drifts%20in%20odometry%0Aand%20pose%20estimation%20inaccuracies%20hinder%20modern%202D%20LiDAR-odometry%20algorithms%20in%0Alarge%20complex%20environments.%20Dynamic%20robotic%20motion%20coupled%20with%20inherent%0Aestimation%20based%20SLAM%20processes%20introduce%20noise%20and%20errors%2C%20degrading%20map%0Aquality.%20Occupancy%20Grid%20Mapping%20%28OGM%29%20produces%20results%20that%20are%20often%20noisy%20and%0Aunclear.%20This%20is%20due%20to%20the%20fact%20that%20evidence%20based%20mapping%20represents%20maps%0Aaccording%20to%20uncertain%20observations.%20This%20is%20why%20OGMs%20are%20so%20popular%20in%0Aexploration%20or%20navigation%20tasks.%20However%2C%20this%20also%20limits%20OGMs%27%20effectiveness%0Afor%20specific%20mapping%20based%20tasks%20such%20as%20floor%20plan%20creation%20in%20complex%20scenes.%0ATo%20address%20this%2C%20we%20propose%20our%20novel%20Transformation%20and%20Translation%20Occupancy%0AGrid%20Mapping%20%28TT-OGM%29.%20We%20adapt%20and%20enable%20accurate%20and%20robust%20pose%20estimation%0Atechniques%20from%203D%20SLAM%20to%20the%20world%20of%202D%20and%20mitigate%20errors%20to%20improve%20map%0Aquality%20using%20Generative%20Adversarial%20Networks%20%28GANs%29.%20We%20introduce%20a%20novel%20data%0Ageneration%20method%20via%20deep%20reinforcement%20learning%20%28DRL%29%20to%20build%20datasets%20large%0Aenough%20for%20training%20a%20GAN%20for%20SLAM%20error%20correction.%20We%20demonstrate%20our%20SLAM%20in%0Areal-time%20on%20data%20collected%20at%20Loughborough%20University.%20We%20also%20prove%20its%0Ageneralisability%20on%20a%20variety%20of%20large%20complex%20environments%20on%20a%20collection%20of%0Alarge%20scale%20well-known%202D%20occupancy%20maps.%20Our%20novel%20approach%20enables%20the%0Acreation%20of%20high%20quality%20OGMs%20in%20complex%20scenes%2C%20far%20surpassing%20the%0Acapabilities%20of%20current%20SLAM%20algorithms%20in%20terms%20of%20quality%2C%20accuracy%20and%0Areliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformation%2520%2526%2520Translation%2520Occupancy%2520Grid%2520Mapping%253A%25202-Dimensional%2520Deep%250A%2520%2520Learning%2520Refined%2520SLAM%26entry.906535625%3DLeon%2520Davies%2520and%2520Baihua%2520Li%2520and%2520Mohamad%2520Saada%2520and%2520Simon%2520S%25C3%25B8lvsten%2520and%2520Qinggang%2520Meng%26entry.1292438233%3D%2520%2520SLAM%2520%2528Simultaneous%2520Localisation%2520and%2520Mapping%2529%2520is%2520a%2520crucial%2520component%2520for%250Arobotic%2520systems%252C%2520providing%2520a%2520map%2520of%2520an%2520environment%252C%2520the%2520current%2520location%2520and%250Aprevious%2520trajectory%2520of%2520a%2520robot.%2520While%25203D%2520LiDAR%2520SLAM%2520has%2520received%2520notable%250Aimprovements%2520in%2520recent%2520years%252C%25202D%2520SLAM%2520lags%2520behind.%2520Gradual%2520drifts%2520in%2520odometry%250Aand%2520pose%2520estimation%2520inaccuracies%2520hinder%2520modern%25202D%2520LiDAR-odometry%2520algorithms%2520in%250Alarge%2520complex%2520environments.%2520Dynamic%2520robotic%2520motion%2520coupled%2520with%2520inherent%250Aestimation%2520based%2520SLAM%2520processes%2520introduce%2520noise%2520and%2520errors%252C%2520degrading%2520map%250Aquality.%2520Occupancy%2520Grid%2520Mapping%2520%2528OGM%2529%2520produces%2520results%2520that%2520are%2520often%2520noisy%2520and%250Aunclear.%2520This%2520is%2520due%2520to%2520the%2520fact%2520that%2520evidence%2520based%2520mapping%2520represents%2520maps%250Aaccording%2520to%2520uncertain%2520observations.%2520This%2520is%2520why%2520OGMs%2520are%2520so%2520popular%2520in%250Aexploration%2520or%2520navigation%2520tasks.%2520However%252C%2520this%2520also%2520limits%2520OGMs%2527%2520effectiveness%250Afor%2520specific%2520mapping%2520based%2520tasks%2520such%2520as%2520floor%2520plan%2520creation%2520in%2520complex%2520scenes.%250ATo%2520address%2520this%252C%2520we%2520propose%2520our%2520novel%2520Transformation%2520and%2520Translation%2520Occupancy%250AGrid%2520Mapping%2520%2528TT-OGM%2529.%2520We%2520adapt%2520and%2520enable%2520accurate%2520and%2520robust%2520pose%2520estimation%250Atechniques%2520from%25203D%2520SLAM%2520to%2520the%2520world%2520of%25202D%2520and%2520mitigate%2520errors%2520to%2520improve%2520map%250Aquality%2520using%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529.%2520We%2520introduce%2520a%2520novel%2520data%250Ageneration%2520method%2520via%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520to%2520build%2520datasets%2520large%250Aenough%2520for%2520training%2520a%2520GAN%2520for%2520SLAM%2520error%2520correction.%2520We%2520demonstrate%2520our%2520SLAM%2520in%250Areal-time%2520on%2520data%2520collected%2520at%2520Loughborough%2520University.%2520We%2520also%2520prove%2520its%250Ageneralisability%2520on%2520a%2520variety%2520of%2520large%2520complex%2520environments%2520on%2520a%2520collection%2520of%250Alarge%2520scale%2520well-known%25202D%2520occupancy%2520maps.%2520Our%2520novel%2520approach%2520enables%2520the%250Acreation%2520of%2520high%2520quality%2520OGMs%2520in%2520complex%2520scenes%252C%2520far%2520surpassing%2520the%250Acapabilities%2520of%2520current%2520SLAM%2520algorithms%2520in%2520terms%2520of%2520quality%252C%2520accuracy%2520and%250Areliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformation%20%26%20Translation%20Occupancy%20Grid%20Mapping%3A%202-Dimensional%20Deep%0A%20%20Learning%20Refined%20SLAM&entry.906535625=Leon%20Davies%20and%20Baihua%20Li%20and%20Mohamad%20Saada%20and%20Simon%20S%C3%B8lvsten%20and%20Qinggang%20Meng&entry.1292438233=%20%20SLAM%20%28Simultaneous%20Localisation%20and%20Mapping%29%20is%20a%20crucial%20component%20for%0Arobotic%20systems%2C%20providing%20a%20map%20of%20an%20environment%2C%20the%20current%20location%20and%0Aprevious%20trajectory%20of%20a%20robot.%20While%203D%20LiDAR%20SLAM%20has%20received%20notable%0Aimprovements%20in%20recent%20years%2C%202D%20SLAM%20lags%20behind.%20Gradual%20drifts%20in%20odometry%0Aand%20pose%20estimation%20inaccuracies%20hinder%20modern%202D%20LiDAR-odometry%20algorithms%20in%0Alarge%20complex%20environments.%20Dynamic%20robotic%20motion%20coupled%20with%20inherent%0Aestimation%20based%20SLAM%20processes%20introduce%20noise%20and%20errors%2C%20degrading%20map%0Aquality.%20Occupancy%20Grid%20Mapping%20%28OGM%29%20produces%20results%20that%20are%20often%20noisy%20and%0Aunclear.%20This%20is%20due%20to%20the%20fact%20that%20evidence%20based%20mapping%20represents%20maps%0Aaccording%20to%20uncertain%20observations.%20This%20is%20why%20OGMs%20are%20so%20popular%20in%0Aexploration%20or%20navigation%20tasks.%20However%2C%20this%20also%20limits%20OGMs%27%20effectiveness%0Afor%20specific%20mapping%20based%20tasks%20such%20as%20floor%20plan%20creation%20in%20complex%20scenes.%0ATo%20address%20this%2C%20we%20propose%20our%20novel%20Transformation%20and%20Translation%20Occupancy%0AGrid%20Mapping%20%28TT-OGM%29.%20We%20adapt%20and%20enable%20accurate%20and%20robust%20pose%20estimation%0Atechniques%20from%203D%20SLAM%20to%20the%20world%20of%202D%20and%20mitigate%20errors%20to%20improve%20map%0Aquality%20using%20Generative%20Adversarial%20Networks%20%28GANs%29.%20We%20introduce%20a%20novel%20data%0Ageneration%20method%20via%20deep%20reinforcement%20learning%20%28DRL%29%20to%20build%20datasets%20large%0Aenough%20for%20training%20a%20GAN%20for%20SLAM%20error%20correction.%20We%20demonstrate%20our%20SLAM%20in%0Areal-time%20on%20data%20collected%20at%20Loughborough%20University.%20We%20also%20prove%20its%0Ageneralisability%20on%20a%20variety%20of%20large%20complex%20environments%20on%20a%20collection%20of%0Alarge%20scale%20well-known%202D%20occupancy%20maps.%20Our%20novel%20approach%20enables%20the%0Acreation%20of%20high%20quality%20OGMs%20in%20complex%20scenes%2C%20far%20surpassing%20the%0Acapabilities%20of%20current%20SLAM%20algorithms%20in%20terms%20of%20quality%2C%20accuracy%20and%0Areliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19654v1&entry.124074799=Read"},
{"title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning", "author": "Wufei Ma and Yu-Cheng Chou and Qihao Liu and Xingrui Wang and Celso de Melo and Jieneng Chen and Jianwen Xie and Alan Yuille", "abstract": "  Recent studies in 3D spatial reasoning explore data-driven approaches and\nachieve enhanced spatial reasoning performance with reinforcement learning\n(RL). However, these methods typically perform spatial reasoning in an implicit\nmanner, and it remains underexplored whether the acquired 3D knowledge\ngeneralizes to unseen question types at any stage of the training. In this work\nwe introduce SpatialReasoner, a novel large vision-language model (LVLM) that\naddress 3D spatial reasoning with explicit 3D representations shared between\nstages -- 3D perception, computation, and reasoning. Explicit 3D\nrepresentations provide a coherent interface that supports advanced 3D spatial\nreasoning and enable us to study the factual errors made by LVLMs. Results show\nthat our SpatialReasoner achieve improved performance on a variety of spatial\nreasoning benchmarks and generalizes better when evaluating on novel 3D spatial\nreasoning questions. Our study bridges the 3D parsing capabilities of prior\nvisual foundation models with the powerful reasoning abilities of large\nlanguage models, opening new directions for 3D spatial reasoning.\n", "link": "http://arxiv.org/abs/2504.20024v1", "date": "2025-04-28", "relevancy": 3.0602, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialReasoner%3A%20Towards%20Explicit%20and%20Generalizable%203D%20Spatial%20Reasoning&body=Title%3A%20SpatialReasoner%3A%20Towards%20Explicit%20and%20Generalizable%203D%20Spatial%20Reasoning%0AAuthor%3A%20Wufei%20Ma%20and%20Yu-Cheng%20Chou%20and%20Qihao%20Liu%20and%20Xingrui%20Wang%20and%20Celso%20de%20Melo%20and%20Jieneng%20Chen%20and%20Jianwen%20Xie%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20Recent%20studies%20in%203D%20spatial%20reasoning%20explore%20data-driven%20approaches%20and%0Aachieve%20enhanced%20spatial%20reasoning%20performance%20with%20reinforcement%20learning%0A%28RL%29.%20However%2C%20these%20methods%20typically%20perform%20spatial%20reasoning%20in%20an%20implicit%0Amanner%2C%20and%20it%20remains%20underexplored%20whether%20the%20acquired%203D%20knowledge%0Ageneralizes%20to%20unseen%20question%20types%20at%20any%20stage%20of%20the%20training.%20In%20this%20work%0Awe%20introduce%20SpatialReasoner%2C%20a%20novel%20large%20vision-language%20model%20%28LVLM%29%20that%0Aaddress%203D%20spatial%20reasoning%20with%20explicit%203D%20representations%20shared%20between%0Astages%20--%203D%20perception%2C%20computation%2C%20and%20reasoning.%20Explicit%203D%0Arepresentations%20provide%20a%20coherent%20interface%20that%20supports%20advanced%203D%20spatial%0Areasoning%20and%20enable%20us%20to%20study%20the%20factual%20errors%20made%20by%20LVLMs.%20Results%20show%0Athat%20our%20SpatialReasoner%20achieve%20improved%20performance%20on%20a%20variety%20of%20spatial%0Areasoning%20benchmarks%20and%20generalizes%20better%20when%20evaluating%20on%20novel%203D%20spatial%0Areasoning%20questions.%20Our%20study%20bridges%20the%203D%20parsing%20capabilities%20of%20prior%0Avisual%20foundation%20models%20with%20the%20powerful%20reasoning%20abilities%20of%20large%0Alanguage%20models%2C%20opening%20new%20directions%20for%203D%20spatial%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialReasoner%253A%2520Towards%2520Explicit%2520and%2520Generalizable%25203D%2520Spatial%2520Reasoning%26entry.906535625%3DWufei%2520Ma%2520and%2520Yu-Cheng%2520Chou%2520and%2520Qihao%2520Liu%2520and%2520Xingrui%2520Wang%2520and%2520Celso%2520de%2520Melo%2520and%2520Jieneng%2520Chen%2520and%2520Jianwen%2520Xie%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520Recent%2520studies%2520in%25203D%2520spatial%2520reasoning%2520explore%2520data-driven%2520approaches%2520and%250Aachieve%2520enhanced%2520spatial%2520reasoning%2520performance%2520with%2520reinforcement%2520learning%250A%2528RL%2529.%2520However%252C%2520these%2520methods%2520typically%2520perform%2520spatial%2520reasoning%2520in%2520an%2520implicit%250Amanner%252C%2520and%2520it%2520remains%2520underexplored%2520whether%2520the%2520acquired%25203D%2520knowledge%250Ageneralizes%2520to%2520unseen%2520question%2520types%2520at%2520any%2520stage%2520of%2520the%2520training.%2520In%2520this%2520work%250Awe%2520introduce%2520SpatialReasoner%252C%2520a%2520novel%2520large%2520vision-language%2520model%2520%2528LVLM%2529%2520that%250Aaddress%25203D%2520spatial%2520reasoning%2520with%2520explicit%25203D%2520representations%2520shared%2520between%250Astages%2520--%25203D%2520perception%252C%2520computation%252C%2520and%2520reasoning.%2520Explicit%25203D%250Arepresentations%2520provide%2520a%2520coherent%2520interface%2520that%2520supports%2520advanced%25203D%2520spatial%250Areasoning%2520and%2520enable%2520us%2520to%2520study%2520the%2520factual%2520errors%2520made%2520by%2520LVLMs.%2520Results%2520show%250Athat%2520our%2520SpatialReasoner%2520achieve%2520improved%2520performance%2520on%2520a%2520variety%2520of%2520spatial%250Areasoning%2520benchmarks%2520and%2520generalizes%2520better%2520when%2520evaluating%2520on%2520novel%25203D%2520spatial%250Areasoning%2520questions.%2520Our%2520study%2520bridges%2520the%25203D%2520parsing%2520capabilities%2520of%2520prior%250Avisual%2520foundation%2520models%2520with%2520the%2520powerful%2520reasoning%2520abilities%2520of%2520large%250Alanguage%2520models%252C%2520opening%2520new%2520directions%2520for%25203D%2520spatial%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialReasoner%3A%20Towards%20Explicit%20and%20Generalizable%203D%20Spatial%20Reasoning&entry.906535625=Wufei%20Ma%20and%20Yu-Cheng%20Chou%20and%20Qihao%20Liu%20and%20Xingrui%20Wang%20and%20Celso%20de%20Melo%20and%20Jieneng%20Chen%20and%20Jianwen%20Xie%20and%20Alan%20Yuille&entry.1292438233=%20%20Recent%20studies%20in%203D%20spatial%20reasoning%20explore%20data-driven%20approaches%20and%0Aachieve%20enhanced%20spatial%20reasoning%20performance%20with%20reinforcement%20learning%0A%28RL%29.%20However%2C%20these%20methods%20typically%20perform%20spatial%20reasoning%20in%20an%20implicit%0Amanner%2C%20and%20it%20remains%20underexplored%20whether%20the%20acquired%203D%20knowledge%0Ageneralizes%20to%20unseen%20question%20types%20at%20any%20stage%20of%20the%20training.%20In%20this%20work%0Awe%20introduce%20SpatialReasoner%2C%20a%20novel%20large%20vision-language%20model%20%28LVLM%29%20that%0Aaddress%203D%20spatial%20reasoning%20with%20explicit%203D%20representations%20shared%20between%0Astages%20--%203D%20perception%2C%20computation%2C%20and%20reasoning.%20Explicit%203D%0Arepresentations%20provide%20a%20coherent%20interface%20that%20supports%20advanced%203D%20spatial%0Areasoning%20and%20enable%20us%20to%20study%20the%20factual%20errors%20made%20by%20LVLMs.%20Results%20show%0Athat%20our%20SpatialReasoner%20achieve%20improved%20performance%20on%20a%20variety%20of%20spatial%0Areasoning%20benchmarks%20and%20generalizes%20better%20when%20evaluating%20on%20novel%203D%20spatial%0Areasoning%20questions.%20Our%20study%20bridges%20the%203D%20parsing%20capabilities%20of%20prior%0Avisual%20foundation%20models%20with%20the%20powerful%20reasoning%20abilities%20of%20large%0Alanguage%20models%2C%20opening%20new%20directions%20for%203D%20spatial%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20024v1&entry.124074799=Read"},
{"title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of\n  Shape, Materials and View-dependent Radiance Fields", "author": "Zhengqin Li and Dilin Wang and Ka Chen and Zhaoyang Lv and Thu Nguyen-Phuoc and Milim Lee and Jia-Bin Huang and Lei Xiao and Cheng Zhang and Yufeng Zhu and Carl S. Marshall and Yufeng Ren and Richard Newcombe and Zhao Dong", "abstract": "  We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time.\n", "link": "http://arxiv.org/abs/2504.20026v1", "date": "2025-04-28", "relevancy": 2.9668, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6111}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5845}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIRM%3A%20Large%20Inverse%20Rendering%20Model%20for%20Progressive%20Reconstruction%20of%0A%20%20Shape%2C%20Materials%20and%20View-dependent%20Radiance%20Fields&body=Title%3A%20LIRM%3A%20Large%20Inverse%20Rendering%20Model%20for%20Progressive%20Reconstruction%20of%0A%20%20Shape%2C%20Materials%20and%20View-dependent%20Radiance%20Fields%0AAuthor%3A%20Zhengqin%20Li%20and%20Dilin%20Wang%20and%20Ka%20Chen%20and%20Zhaoyang%20Lv%20and%20Thu%20Nguyen-Phuoc%20and%20Milim%20Lee%20and%20Jia-Bin%20Huang%20and%20Lei%20Xiao%20and%20Cheng%20Zhang%20and%20Yufeng%20Zhu%20and%20Carl%20S.%20Marshall%20and%20Yufeng%20Ren%20and%20Richard%20Newcombe%20and%20Zhao%20Dong%0AAbstract%3A%20%20%20We%20present%20Large%20Inverse%20Rendering%20Model%20%28LIRM%29%2C%20a%20transformer%20architecture%0Athat%20jointly%20reconstructs%20high-quality%20shape%2C%20materials%2C%20and%20radiance%20fields%0Awith%20view-dependent%20effects%20in%20less%20than%20a%20second.%20Our%20model%20builds%20upon%20the%0Arecent%20Large%20Reconstruction%20Models%20%28LRMs%29%20that%20achieve%20state-of-the-art%0Asparse-view%20reconstruction%20quality.%20However%2C%20existing%20LRMs%20struggle%20to%0Areconstruct%20unseen%20parts%20accurately%20and%20cannot%20recover%20glossy%20appearance%20or%0Agenerate%20relightable%203D%20contents%20that%20can%20be%20consumed%20by%20standard%20Graphics%0Aengines.%20To%20address%20these%20limitations%2C%20we%20make%20three%20key%20technical%0Acontributions%20to%20build%20a%20more%20practical%20multi-view%203D%20reconstruction%20framework.%0AFirst%2C%20we%20introduce%20an%20update%20model%20that%20allows%20us%20to%20progressively%20add%20more%0Ainput%20views%20to%20improve%20our%20reconstruction.%20Second%2C%20we%20propose%20a%20hexa-plane%0Aneural%20SDF%20representation%20to%20better%20recover%20detailed%20textures%2C%20geometry%20and%0Amaterial%20parameters.%20Third%2C%20we%20develop%20a%20novel%20neural%20directional-embedding%0Amechanism%20to%20handle%20view-dependent%20effects.%20Trained%20on%20a%20large-scale%20shape%20and%0Amaterial%20dataset%20with%20a%20tailored%20coarse-to-fine%20training%20scheme%2C%20our%20model%0Aachieves%20compelling%20results.%20It%20compares%20favorably%20to%20optimization-based%0Adense-view%20inverse%20rendering%20methods%20in%20terms%20of%20geometry%20and%20relighting%0Aaccuracy%2C%20while%20requiring%20only%20a%20fraction%20of%20the%20inference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIRM%253A%2520Large%2520Inverse%2520Rendering%2520Model%2520for%2520Progressive%2520Reconstruction%2520of%250A%2520%2520Shape%252C%2520Materials%2520and%2520View-dependent%2520Radiance%2520Fields%26entry.906535625%3DZhengqin%2520Li%2520and%2520Dilin%2520Wang%2520and%2520Ka%2520Chen%2520and%2520Zhaoyang%2520Lv%2520and%2520Thu%2520Nguyen-Phuoc%2520and%2520Milim%2520Lee%2520and%2520Jia-Bin%2520Huang%2520and%2520Lei%2520Xiao%2520and%2520Cheng%2520Zhang%2520and%2520Yufeng%2520Zhu%2520and%2520Carl%2520S.%2520Marshall%2520and%2520Yufeng%2520Ren%2520and%2520Richard%2520Newcombe%2520and%2520Zhao%2520Dong%26entry.1292438233%3D%2520%2520We%2520present%2520Large%2520Inverse%2520Rendering%2520Model%2520%2528LIRM%2529%252C%2520a%2520transformer%2520architecture%250Athat%2520jointly%2520reconstructs%2520high-quality%2520shape%252C%2520materials%252C%2520and%2520radiance%2520fields%250Awith%2520view-dependent%2520effects%2520in%2520less%2520than%2520a%2520second.%2520Our%2520model%2520builds%2520upon%2520the%250Arecent%2520Large%2520Reconstruction%2520Models%2520%2528LRMs%2529%2520that%2520achieve%2520state-of-the-art%250Asparse-view%2520reconstruction%2520quality.%2520However%252C%2520existing%2520LRMs%2520struggle%2520to%250Areconstruct%2520unseen%2520parts%2520accurately%2520and%2520cannot%2520recover%2520glossy%2520appearance%2520or%250Agenerate%2520relightable%25203D%2520contents%2520that%2520can%2520be%2520consumed%2520by%2520standard%2520Graphics%250Aengines.%2520To%2520address%2520these%2520limitations%252C%2520we%2520make%2520three%2520key%2520technical%250Acontributions%2520to%2520build%2520a%2520more%2520practical%2520multi-view%25203D%2520reconstruction%2520framework.%250AFirst%252C%2520we%2520introduce%2520an%2520update%2520model%2520that%2520allows%2520us%2520to%2520progressively%2520add%2520more%250Ainput%2520views%2520to%2520improve%2520our%2520reconstruction.%2520Second%252C%2520we%2520propose%2520a%2520hexa-plane%250Aneural%2520SDF%2520representation%2520to%2520better%2520recover%2520detailed%2520textures%252C%2520geometry%2520and%250Amaterial%2520parameters.%2520Third%252C%2520we%2520develop%2520a%2520novel%2520neural%2520directional-embedding%250Amechanism%2520to%2520handle%2520view-dependent%2520effects.%2520Trained%2520on%2520a%2520large-scale%2520shape%2520and%250Amaterial%2520dataset%2520with%2520a%2520tailored%2520coarse-to-fine%2520training%2520scheme%252C%2520our%2520model%250Aachieves%2520compelling%2520results.%2520It%2520compares%2520favorably%2520to%2520optimization-based%250Adense-view%2520inverse%2520rendering%2520methods%2520in%2520terms%2520of%2520geometry%2520and%2520relighting%250Aaccuracy%252C%2520while%2520requiring%2520only%2520a%2520fraction%2520of%2520the%2520inference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIRM%3A%20Large%20Inverse%20Rendering%20Model%20for%20Progressive%20Reconstruction%20of%0A%20%20Shape%2C%20Materials%20and%20View-dependent%20Radiance%20Fields&entry.906535625=Zhengqin%20Li%20and%20Dilin%20Wang%20and%20Ka%20Chen%20and%20Zhaoyang%20Lv%20and%20Thu%20Nguyen-Phuoc%20and%20Milim%20Lee%20and%20Jia-Bin%20Huang%20and%20Lei%20Xiao%20and%20Cheng%20Zhang%20and%20Yufeng%20Zhu%20and%20Carl%20S.%20Marshall%20and%20Yufeng%20Ren%20and%20Richard%20Newcombe%20and%20Zhao%20Dong&entry.1292438233=%20%20We%20present%20Large%20Inverse%20Rendering%20Model%20%28LIRM%29%2C%20a%20transformer%20architecture%0Athat%20jointly%20reconstructs%20high-quality%20shape%2C%20materials%2C%20and%20radiance%20fields%0Awith%20view-dependent%20effects%20in%20less%20than%20a%20second.%20Our%20model%20builds%20upon%20the%0Arecent%20Large%20Reconstruction%20Models%20%28LRMs%29%20that%20achieve%20state-of-the-art%0Asparse-view%20reconstruction%20quality.%20However%2C%20existing%20LRMs%20struggle%20to%0Areconstruct%20unseen%20parts%20accurately%20and%20cannot%20recover%20glossy%20appearance%20or%0Agenerate%20relightable%203D%20contents%20that%20can%20be%20consumed%20by%20standard%20Graphics%0Aengines.%20To%20address%20these%20limitations%2C%20we%20make%20three%20key%20technical%0Acontributions%20to%20build%20a%20more%20practical%20multi-view%203D%20reconstruction%20framework.%0AFirst%2C%20we%20introduce%20an%20update%20model%20that%20allows%20us%20to%20progressively%20add%20more%0Ainput%20views%20to%20improve%20our%20reconstruction.%20Second%2C%20we%20propose%20a%20hexa-plane%0Aneural%20SDF%20representation%20to%20better%20recover%20detailed%20textures%2C%20geometry%20and%0Amaterial%20parameters.%20Third%2C%20we%20develop%20a%20novel%20neural%20directional-embedding%0Amechanism%20to%20handle%20view-dependent%20effects.%20Trained%20on%20a%20large-scale%20shape%20and%0Amaterial%20dataset%20with%20a%20tailored%20coarse-to-fine%20training%20scheme%2C%20our%20model%0Aachieves%20compelling%20results.%20It%20compares%20favorably%20to%20optimization-based%0Adense-view%20inverse%20rendering%20methods%20in%20terms%20of%20geometry%20and%20relighting%0Aaccuracy%2C%20while%20requiring%20only%20a%20fraction%20of%20the%20inference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20026v1&entry.124074799=Read"},
{"title": "Mesh-Learner: Texturing Mesh with Spherical Harmonics", "author": "Yunfei Wan and Jianheng Liu and Jiarong Lin and Fu Zhang", "abstract": "  In this paper, we present a 3D reconstruction and rendering framework termed\nMesh-Learner that is natively compatible with traditional rasterization\npipelines. It integrates mesh and spherical harmonic (SH) texture (i.e.,\ntexture filled with SH coefficients) into the learning process to learn each\nmesh s view-dependent radiance end-to-end. Images are rendered by interpolating\nsurrounding SH Texels at each pixel s sampling point using a novel\ninterpolation method. Conversely, gradients from each pixel are back-propagated\nto the related SH Texels in SH textures. Mesh-Learner exploits graphic features\nof rasterization pipeline (texture sampling, deferred rendering) to render,\nwhich makes Mesh-Learner naturally compatible with tools (e.g., Blender) and\ntasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for\nrobotics) that are based on rasterization pipelines. Our system can train vast,\nunlimited scenes because we transfer only the SH textures within the frustum to\nthe GPU for training. At other times, the SH textures are stored in CPU RAM,\nwhich results in moderate GPU memory usage. The rendering results on\ninterpolation and extrapolation sequences in the Replica and FAST-LIVO2\ndatasets achieve state-of-the-art performance compared to existing\nstate-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To\nbenefit the society, the code will be available at\nhttps://github.com/hku-mars/Mesh-Learner.\n", "link": "http://arxiv.org/abs/2504.19938v1", "date": "2025-04-28", "relevancy": 2.9476, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6442}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5672}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh-Learner%3A%20Texturing%20Mesh%20with%20Spherical%20Harmonics&body=Title%3A%20Mesh-Learner%3A%20Texturing%20Mesh%20with%20Spherical%20Harmonics%0AAuthor%3A%20Yunfei%20Wan%20and%20Jianheng%20Liu%20and%20Jiarong%20Lin%20and%20Fu%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%203D%20reconstruction%20and%20rendering%20framework%20termed%0AMesh-Learner%20that%20is%20natively%20compatible%20with%20traditional%20rasterization%0Apipelines.%20It%20integrates%20mesh%20and%20spherical%20harmonic%20%28SH%29%20texture%20%28i.e.%2C%0Atexture%20filled%20with%20SH%20coefficients%29%20into%20the%20learning%20process%20to%20learn%20each%0Amesh%20s%20view-dependent%20radiance%20end-to-end.%20Images%20are%20rendered%20by%20interpolating%0Asurrounding%20SH%20Texels%20at%20each%20pixel%20s%20sampling%20point%20using%20a%20novel%0Ainterpolation%20method.%20Conversely%2C%20gradients%20from%20each%20pixel%20are%20back-propagated%0Ato%20the%20related%20SH%20Texels%20in%20SH%20textures.%20Mesh-Learner%20exploits%20graphic%20features%0Aof%20rasterization%20pipeline%20%28texture%20sampling%2C%20deferred%20rendering%29%20to%20render%2C%0Awhich%20makes%20Mesh-Learner%20naturally%20compatible%20with%20tools%20%28e.g.%2C%20Blender%29%20and%0Atasks%20%28e.g.%2C%203D%20reconstruction%2C%20scene%20rendering%2C%20reinforcement%20learning%20for%0Arobotics%29%20that%20are%20based%20on%20rasterization%20pipelines.%20Our%20system%20can%20train%20vast%2C%0Aunlimited%20scenes%20because%20we%20transfer%20only%20the%20SH%20textures%20within%20the%20frustum%20to%0Athe%20GPU%20for%20training.%20At%20other%20times%2C%20the%20SH%20textures%20are%20stored%20in%20CPU%20RAM%2C%0Awhich%20results%20in%20moderate%20GPU%20memory%20usage.%20The%20rendering%20results%20on%0Ainterpolation%20and%20extrapolation%20sequences%20in%20the%20Replica%20and%20FAST-LIVO2%0Adatasets%20achieve%20state-of-the-art%20performance%20compared%20to%20existing%0Astate-of-the-art%20methods%20%28e.g.%2C%203D%20Gaussian%20Splatting%20and%20M2-Mapping%29.%20To%0Abenefit%20the%20society%2C%20the%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/hku-mars/Mesh-Learner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh-Learner%253A%2520Texturing%2520Mesh%2520with%2520Spherical%2520Harmonics%26entry.906535625%3DYunfei%2520Wan%2520and%2520Jianheng%2520Liu%2520and%2520Jiarong%2520Lin%2520and%2520Fu%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%25203D%2520reconstruction%2520and%2520rendering%2520framework%2520termed%250AMesh-Learner%2520that%2520is%2520natively%2520compatible%2520with%2520traditional%2520rasterization%250Apipelines.%2520It%2520integrates%2520mesh%2520and%2520spherical%2520harmonic%2520%2528SH%2529%2520texture%2520%2528i.e.%252C%250Atexture%2520filled%2520with%2520SH%2520coefficients%2529%2520into%2520the%2520learning%2520process%2520to%2520learn%2520each%250Amesh%2520s%2520view-dependent%2520radiance%2520end-to-end.%2520Images%2520are%2520rendered%2520by%2520interpolating%250Asurrounding%2520SH%2520Texels%2520at%2520each%2520pixel%2520s%2520sampling%2520point%2520using%2520a%2520novel%250Ainterpolation%2520method.%2520Conversely%252C%2520gradients%2520from%2520each%2520pixel%2520are%2520back-propagated%250Ato%2520the%2520related%2520SH%2520Texels%2520in%2520SH%2520textures.%2520Mesh-Learner%2520exploits%2520graphic%2520features%250Aof%2520rasterization%2520pipeline%2520%2528texture%2520sampling%252C%2520deferred%2520rendering%2529%2520to%2520render%252C%250Awhich%2520makes%2520Mesh-Learner%2520naturally%2520compatible%2520with%2520tools%2520%2528e.g.%252C%2520Blender%2529%2520and%250Atasks%2520%2528e.g.%252C%25203D%2520reconstruction%252C%2520scene%2520rendering%252C%2520reinforcement%2520learning%2520for%250Arobotics%2529%2520that%2520are%2520based%2520on%2520rasterization%2520pipelines.%2520Our%2520system%2520can%2520train%2520vast%252C%250Aunlimited%2520scenes%2520because%2520we%2520transfer%2520only%2520the%2520SH%2520textures%2520within%2520the%2520frustum%2520to%250Athe%2520GPU%2520for%2520training.%2520At%2520other%2520times%252C%2520the%2520SH%2520textures%2520are%2520stored%2520in%2520CPU%2520RAM%252C%250Awhich%2520results%2520in%2520moderate%2520GPU%2520memory%2520usage.%2520The%2520rendering%2520results%2520on%250Ainterpolation%2520and%2520extrapolation%2520sequences%2520in%2520the%2520Replica%2520and%2520FAST-LIVO2%250Adatasets%2520achieve%2520state-of-the-art%2520performance%2520compared%2520to%2520existing%250Astate-of-the-art%2520methods%2520%2528e.g.%252C%25203D%2520Gaussian%2520Splatting%2520and%2520M2-Mapping%2529.%2520To%250Abenefit%2520the%2520society%252C%2520the%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/hku-mars/Mesh-Learner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh-Learner%3A%20Texturing%20Mesh%20with%20Spherical%20Harmonics&entry.906535625=Yunfei%20Wan%20and%20Jianheng%20Liu%20and%20Jiarong%20Lin%20and%20Fu%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%203D%20reconstruction%20and%20rendering%20framework%20termed%0AMesh-Learner%20that%20is%20natively%20compatible%20with%20traditional%20rasterization%0Apipelines.%20It%20integrates%20mesh%20and%20spherical%20harmonic%20%28SH%29%20texture%20%28i.e.%2C%0Atexture%20filled%20with%20SH%20coefficients%29%20into%20the%20learning%20process%20to%20learn%20each%0Amesh%20s%20view-dependent%20radiance%20end-to-end.%20Images%20are%20rendered%20by%20interpolating%0Asurrounding%20SH%20Texels%20at%20each%20pixel%20s%20sampling%20point%20using%20a%20novel%0Ainterpolation%20method.%20Conversely%2C%20gradients%20from%20each%20pixel%20are%20back-propagated%0Ato%20the%20related%20SH%20Texels%20in%20SH%20textures.%20Mesh-Learner%20exploits%20graphic%20features%0Aof%20rasterization%20pipeline%20%28texture%20sampling%2C%20deferred%20rendering%29%20to%20render%2C%0Awhich%20makes%20Mesh-Learner%20naturally%20compatible%20with%20tools%20%28e.g.%2C%20Blender%29%20and%0Atasks%20%28e.g.%2C%203D%20reconstruction%2C%20scene%20rendering%2C%20reinforcement%20learning%20for%0Arobotics%29%20that%20are%20based%20on%20rasterization%20pipelines.%20Our%20system%20can%20train%20vast%2C%0Aunlimited%20scenes%20because%20we%20transfer%20only%20the%20SH%20textures%20within%20the%20frustum%20to%0Athe%20GPU%20for%20training.%20At%20other%20times%2C%20the%20SH%20textures%20are%20stored%20in%20CPU%20RAM%2C%0Awhich%20results%20in%20moderate%20GPU%20memory%20usage.%20The%20rendering%20results%20on%0Ainterpolation%20and%20extrapolation%20sequences%20in%20the%20Replica%20and%20FAST-LIVO2%0Adatasets%20achieve%20state-of-the-art%20performance%20compared%20to%20existing%0Astate-of-the-art%20methods%20%28e.g.%2C%203D%20Gaussian%20Splatting%20and%20M2-Mapping%29.%20To%0Abenefit%20the%20society%2C%20the%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/hku-mars/Mesh-Learner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19938v1&entry.124074799=Read"},
{"title": "GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM", "author": "Leon Davies and Baihua Li and Mohamad Saada and Simon S\u00f8lvsten and Qinggang Meng", "abstract": "  SLAM is a fundamental component of modern autonomous systems, providing\nrobots and their operators with a deeper understanding of their environment.\nSLAM systems often encounter challenges due to the dynamic nature of robotic\nmotion, leading to inaccuracies in mapping quality, particularly in 2D\nrepresentations such as Occupancy Grid Maps. These errors can significantly\ndegrade map quality, hindering the effectiveness of specific downstream tasks\nsuch as floor plan creation. To address this challenge, we introduce our novel\n'GAN-SLAM', a new SLAM approach that leverages Generative Adversarial Networks\nto clean and complete occupancy grids during the SLAM process, reducing the\nimpact of noise and inaccuracies introduced on the output map. We adapt and\nintegrate accurate pose estimation techniques typically used for 3D SLAM into a\n2D form. This enables the quality improvement 3D LiDAR-odometry has seen in\nrecent years to be effective for 2D representations. Our results demonstrate\nsubstantial improvements in map fidelity and quality, with minimal noise and\nerrors, affirming the effectiveness of GAN-SLAM for real-world mapping\napplications within large-scale complex environments. We validate our approach\non real-world data operating in real-time, and on famous examples of 2D maps.\nThe improved quality of the output map enables new downstream tasks, such as\nfloor plan drafting, further enhancing the capabilities of autonomous systems.\nOur novel approach to SLAM offers a significant step forward in the field,\nimproving the usability for SLAM in mapping-based tasks, and offers insight\ninto the usage of GANs for OGM error correction.\n", "link": "http://arxiv.org/abs/2504.19653v1", "date": "2025-04-28", "relevancy": 2.9403, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.662}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5762}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAN-SLAM%3A%20Real-Time%20GAN%20Aided%20Floor%20Plan%20Creation%20Through%20SLAM&body=Title%3A%20GAN-SLAM%3A%20Real-Time%20GAN%20Aided%20Floor%20Plan%20Creation%20Through%20SLAM%0AAuthor%3A%20Leon%20Davies%20and%20Baihua%20Li%20and%20Mohamad%20Saada%20and%20Simon%20S%C3%B8lvsten%20and%20Qinggang%20Meng%0AAbstract%3A%20%20%20SLAM%20is%20a%20fundamental%20component%20of%20modern%20autonomous%20systems%2C%20providing%0Arobots%20and%20their%20operators%20with%20a%20deeper%20understanding%20of%20their%20environment.%0ASLAM%20systems%20often%20encounter%20challenges%20due%20to%20the%20dynamic%20nature%20of%20robotic%0Amotion%2C%20leading%20to%20inaccuracies%20in%20mapping%20quality%2C%20particularly%20in%202D%0Arepresentations%20such%20as%20Occupancy%20Grid%20Maps.%20These%20errors%20can%20significantly%0Adegrade%20map%20quality%2C%20hindering%20the%20effectiveness%20of%20specific%20downstream%20tasks%0Asuch%20as%20floor%20plan%20creation.%20To%20address%20this%20challenge%2C%20we%20introduce%20our%20novel%0A%27GAN-SLAM%27%2C%20a%20new%20SLAM%20approach%20that%20leverages%20Generative%20Adversarial%20Networks%0Ato%20clean%20and%20complete%20occupancy%20grids%20during%20the%20SLAM%20process%2C%20reducing%20the%0Aimpact%20of%20noise%20and%20inaccuracies%20introduced%20on%20the%20output%20map.%20We%20adapt%20and%0Aintegrate%20accurate%20pose%20estimation%20techniques%20typically%20used%20for%203D%20SLAM%20into%20a%0A2D%20form.%20This%20enables%20the%20quality%20improvement%203D%20LiDAR-odometry%20has%20seen%20in%0Arecent%20years%20to%20be%20effective%20for%202D%20representations.%20Our%20results%20demonstrate%0Asubstantial%20improvements%20in%20map%20fidelity%20and%20quality%2C%20with%20minimal%20noise%20and%0Aerrors%2C%20affirming%20the%20effectiveness%20of%20GAN-SLAM%20for%20real-world%20mapping%0Aapplications%20within%20large-scale%20complex%20environments.%20We%20validate%20our%20approach%0Aon%20real-world%20data%20operating%20in%20real-time%2C%20and%20on%20famous%20examples%20of%202D%20maps.%0AThe%20improved%20quality%20of%20the%20output%20map%20enables%20new%20downstream%20tasks%2C%20such%20as%0Afloor%20plan%20drafting%2C%20further%20enhancing%20the%20capabilities%20of%20autonomous%20systems.%0AOur%20novel%20approach%20to%20SLAM%20offers%20a%20significant%20step%20forward%20in%20the%20field%2C%0Aimproving%20the%20usability%20for%20SLAM%20in%20mapping-based%20tasks%2C%20and%20offers%20insight%0Ainto%20the%20usage%20of%20GANs%20for%20OGM%20error%20correction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAN-SLAM%253A%2520Real-Time%2520GAN%2520Aided%2520Floor%2520Plan%2520Creation%2520Through%2520SLAM%26entry.906535625%3DLeon%2520Davies%2520and%2520Baihua%2520Li%2520and%2520Mohamad%2520Saada%2520and%2520Simon%2520S%25C3%25B8lvsten%2520and%2520Qinggang%2520Meng%26entry.1292438233%3D%2520%2520SLAM%2520is%2520a%2520fundamental%2520component%2520of%2520modern%2520autonomous%2520systems%252C%2520providing%250Arobots%2520and%2520their%2520operators%2520with%2520a%2520deeper%2520understanding%2520of%2520their%2520environment.%250ASLAM%2520systems%2520often%2520encounter%2520challenges%2520due%2520to%2520the%2520dynamic%2520nature%2520of%2520robotic%250Amotion%252C%2520leading%2520to%2520inaccuracies%2520in%2520mapping%2520quality%252C%2520particularly%2520in%25202D%250Arepresentations%2520such%2520as%2520Occupancy%2520Grid%2520Maps.%2520These%2520errors%2520can%2520significantly%250Adegrade%2520map%2520quality%252C%2520hindering%2520the%2520effectiveness%2520of%2520specific%2520downstream%2520tasks%250Asuch%2520as%2520floor%2520plan%2520creation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520our%2520novel%250A%2527GAN-SLAM%2527%252C%2520a%2520new%2520SLAM%2520approach%2520that%2520leverages%2520Generative%2520Adversarial%2520Networks%250Ato%2520clean%2520and%2520complete%2520occupancy%2520grids%2520during%2520the%2520SLAM%2520process%252C%2520reducing%2520the%250Aimpact%2520of%2520noise%2520and%2520inaccuracies%2520introduced%2520on%2520the%2520output%2520map.%2520We%2520adapt%2520and%250Aintegrate%2520accurate%2520pose%2520estimation%2520techniques%2520typically%2520used%2520for%25203D%2520SLAM%2520into%2520a%250A2D%2520form.%2520This%2520enables%2520the%2520quality%2520improvement%25203D%2520LiDAR-odometry%2520has%2520seen%2520in%250Arecent%2520years%2520to%2520be%2520effective%2520for%25202D%2520representations.%2520Our%2520results%2520demonstrate%250Asubstantial%2520improvements%2520in%2520map%2520fidelity%2520and%2520quality%252C%2520with%2520minimal%2520noise%2520and%250Aerrors%252C%2520affirming%2520the%2520effectiveness%2520of%2520GAN-SLAM%2520for%2520real-world%2520mapping%250Aapplications%2520within%2520large-scale%2520complex%2520environments.%2520We%2520validate%2520our%2520approach%250Aon%2520real-world%2520data%2520operating%2520in%2520real-time%252C%2520and%2520on%2520famous%2520examples%2520of%25202D%2520maps.%250AThe%2520improved%2520quality%2520of%2520the%2520output%2520map%2520enables%2520new%2520downstream%2520tasks%252C%2520such%2520as%250Afloor%2520plan%2520drafting%252C%2520further%2520enhancing%2520the%2520capabilities%2520of%2520autonomous%2520systems.%250AOur%2520novel%2520approach%2520to%2520SLAM%2520offers%2520a%2520significant%2520step%2520forward%2520in%2520the%2520field%252C%250Aimproving%2520the%2520usability%2520for%2520SLAM%2520in%2520mapping-based%2520tasks%252C%2520and%2520offers%2520insight%250Ainto%2520the%2520usage%2520of%2520GANs%2520for%2520OGM%2520error%2520correction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAN-SLAM%3A%20Real-Time%20GAN%20Aided%20Floor%20Plan%20Creation%20Through%20SLAM&entry.906535625=Leon%20Davies%20and%20Baihua%20Li%20and%20Mohamad%20Saada%20and%20Simon%20S%C3%B8lvsten%20and%20Qinggang%20Meng&entry.1292438233=%20%20SLAM%20is%20a%20fundamental%20component%20of%20modern%20autonomous%20systems%2C%20providing%0Arobots%20and%20their%20operators%20with%20a%20deeper%20understanding%20of%20their%20environment.%0ASLAM%20systems%20often%20encounter%20challenges%20due%20to%20the%20dynamic%20nature%20of%20robotic%0Amotion%2C%20leading%20to%20inaccuracies%20in%20mapping%20quality%2C%20particularly%20in%202D%0Arepresentations%20such%20as%20Occupancy%20Grid%20Maps.%20These%20errors%20can%20significantly%0Adegrade%20map%20quality%2C%20hindering%20the%20effectiveness%20of%20specific%20downstream%20tasks%0Asuch%20as%20floor%20plan%20creation.%20To%20address%20this%20challenge%2C%20we%20introduce%20our%20novel%0A%27GAN-SLAM%27%2C%20a%20new%20SLAM%20approach%20that%20leverages%20Generative%20Adversarial%20Networks%0Ato%20clean%20and%20complete%20occupancy%20grids%20during%20the%20SLAM%20process%2C%20reducing%20the%0Aimpact%20of%20noise%20and%20inaccuracies%20introduced%20on%20the%20output%20map.%20We%20adapt%20and%0Aintegrate%20accurate%20pose%20estimation%20techniques%20typically%20used%20for%203D%20SLAM%20into%20a%0A2D%20form.%20This%20enables%20the%20quality%20improvement%203D%20LiDAR-odometry%20has%20seen%20in%0Arecent%20years%20to%20be%20effective%20for%202D%20representations.%20Our%20results%20demonstrate%0Asubstantial%20improvements%20in%20map%20fidelity%20and%20quality%2C%20with%20minimal%20noise%20and%0Aerrors%2C%20affirming%20the%20effectiveness%20of%20GAN-SLAM%20for%20real-world%20mapping%0Aapplications%20within%20large-scale%20complex%20environments.%20We%20validate%20our%20approach%0Aon%20real-world%20data%20operating%20in%20real-time%2C%20and%20on%20famous%20examples%20of%202D%20maps.%0AThe%20improved%20quality%20of%20the%20output%20map%20enables%20new%20downstream%20tasks%2C%20such%20as%0Afloor%20plan%20drafting%2C%20further%20enhancing%20the%20capabilities%20of%20autonomous%20systems.%0AOur%20novel%20approach%20to%20SLAM%20offers%20a%20significant%20step%20forward%20in%20the%20field%2C%0Aimproving%20the%20usability%20for%20SLAM%20in%20mapping-based%20tasks%2C%20and%20offers%20insight%0Ainto%20the%20usage%20of%20GANs%20for%20OGM%20error%20correction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19653v1&entry.124074799=Read"},
{"title": "Contextures: The Mechanism of Representation Learning", "author": "Runtian Zhai", "abstract": "  This dissertation establishes the contexture theory to mathematically\ncharacterize the mechanism of representation learning, or pretraining. Despite\nthe remarkable empirical success of foundation models, it is not very clear\nwhat representations they learn, and why these representations are useful for\nvarious downstream tasks. A scientific understanding of representation learning\nis critical, especially at this point when scaling up the model size is\nproducing diminishing returns, and designing new pretraining methods is\nimperative for further progress.\n  Prior work treated different representation learning methods quite\ndifferently, whereas the contexture theory provides a unified framework for\nanalyzing these methods. The central argument is that a representation is\nlearned from the association between the input X and a context variable A. We\nprove that if an encoder captures the maximum information of this association,\nin which case we say that the encoder learns the contexture, then it will be\noptimal on the class of tasks that are compatible with the context. We also\nshow that a context is the most useful when the association between X and A is\nneither too strong nor too weak. The important implication of the contexture\ntheory is that increasing the model size alone will achieve diminishing\nreturns, and further advancements require better contexts.\n  We demonstrate that many pretraining objectives can learn the contexture,\nincluding supervised learning, self-supervised learning, generative models,\netc. Then, we introduce two general objectives -- SVME and KISE, for learning\nthe contexture. We also show how to mix multiple contexts together, an\neffortless way to create better contexts from existing ones. Then, we prove\nstatistical learning bounds for representation learning. Finally, we discuss\nthe effect of the data distribution shift from pretraining to the downstream\ntask.\n", "link": "http://arxiv.org/abs/2504.19792v1", "date": "2025-04-28", "relevancy": 2.9396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextures%3A%20The%20Mechanism%20of%20Representation%20Learning&body=Title%3A%20Contextures%3A%20The%20Mechanism%20of%20Representation%20Learning%0AAuthor%3A%20Runtian%20Zhai%0AAbstract%3A%20%20%20This%20dissertation%20establishes%20the%20contexture%20theory%20to%20mathematically%0Acharacterize%20the%20mechanism%20of%20representation%20learning%2C%20or%20pretraining.%20Despite%0Athe%20remarkable%20empirical%20success%20of%20foundation%20models%2C%20it%20is%20not%20very%20clear%0Awhat%20representations%20they%20learn%2C%20and%20why%20these%20representations%20are%20useful%20for%0Avarious%20downstream%20tasks.%20A%20scientific%20understanding%20of%20representation%20learning%0Ais%20critical%2C%20especially%20at%20this%20point%20when%20scaling%20up%20the%20model%20size%20is%0Aproducing%20diminishing%20returns%2C%20and%20designing%20new%20pretraining%20methods%20is%0Aimperative%20for%20further%20progress.%0A%20%20Prior%20work%20treated%20different%20representation%20learning%20methods%20quite%0Adifferently%2C%20whereas%20the%20contexture%20theory%20provides%20a%20unified%20framework%20for%0Aanalyzing%20these%20methods.%20The%20central%20argument%20is%20that%20a%20representation%20is%0Alearned%20from%20the%20association%20between%20the%20input%20X%20and%20a%20context%20variable%20A.%20We%0Aprove%20that%20if%20an%20encoder%20captures%20the%20maximum%20information%20of%20this%20association%2C%0Ain%20which%20case%20we%20say%20that%20the%20encoder%20learns%20the%20contexture%2C%20then%20it%20will%20be%0Aoptimal%20on%20the%20class%20of%20tasks%20that%20are%20compatible%20with%20the%20context.%20We%20also%0Ashow%20that%20a%20context%20is%20the%20most%20useful%20when%20the%20association%20between%20X%20and%20A%20is%0Aneither%20too%20strong%20nor%20too%20weak.%20The%20important%20implication%20of%20the%20contexture%0Atheory%20is%20that%20increasing%20the%20model%20size%20alone%20will%20achieve%20diminishing%0Areturns%2C%20and%20further%20advancements%20require%20better%20contexts.%0A%20%20We%20demonstrate%20that%20many%20pretraining%20objectives%20can%20learn%20the%20contexture%2C%0Aincluding%20supervised%20learning%2C%20self-supervised%20learning%2C%20generative%20models%2C%0Aetc.%20Then%2C%20we%20introduce%20two%20general%20objectives%20--%20SVME%20and%20KISE%2C%20for%20learning%0Athe%20contexture.%20We%20also%20show%20how%20to%20mix%20multiple%20contexts%20together%2C%20an%0Aeffortless%20way%20to%20create%20better%20contexts%20from%20existing%20ones.%20Then%2C%20we%20prove%0Astatistical%20learning%20bounds%20for%20representation%20learning.%20Finally%2C%20we%20discuss%0Athe%20effect%20of%20the%20data%20distribution%20shift%20from%20pretraining%20to%20the%20downstream%0Atask.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextures%253A%2520The%2520Mechanism%2520of%2520Representation%2520Learning%26entry.906535625%3DRuntian%2520Zhai%26entry.1292438233%3D%2520%2520This%2520dissertation%2520establishes%2520the%2520contexture%2520theory%2520to%2520mathematically%250Acharacterize%2520the%2520mechanism%2520of%2520representation%2520learning%252C%2520or%2520pretraining.%2520Despite%250Athe%2520remarkable%2520empirical%2520success%2520of%2520foundation%2520models%252C%2520it%2520is%2520not%2520very%2520clear%250Awhat%2520representations%2520they%2520learn%252C%2520and%2520why%2520these%2520representations%2520are%2520useful%2520for%250Avarious%2520downstream%2520tasks.%2520A%2520scientific%2520understanding%2520of%2520representation%2520learning%250Ais%2520critical%252C%2520especially%2520at%2520this%2520point%2520when%2520scaling%2520up%2520the%2520model%2520size%2520is%250Aproducing%2520diminishing%2520returns%252C%2520and%2520designing%2520new%2520pretraining%2520methods%2520is%250Aimperative%2520for%2520further%2520progress.%250A%2520%2520Prior%2520work%2520treated%2520different%2520representation%2520learning%2520methods%2520quite%250Adifferently%252C%2520whereas%2520the%2520contexture%2520theory%2520provides%2520a%2520unified%2520framework%2520for%250Aanalyzing%2520these%2520methods.%2520The%2520central%2520argument%2520is%2520that%2520a%2520representation%2520is%250Alearned%2520from%2520the%2520association%2520between%2520the%2520input%2520X%2520and%2520a%2520context%2520variable%2520A.%2520We%250Aprove%2520that%2520if%2520an%2520encoder%2520captures%2520the%2520maximum%2520information%2520of%2520this%2520association%252C%250Ain%2520which%2520case%2520we%2520say%2520that%2520the%2520encoder%2520learns%2520the%2520contexture%252C%2520then%2520it%2520will%2520be%250Aoptimal%2520on%2520the%2520class%2520of%2520tasks%2520that%2520are%2520compatible%2520with%2520the%2520context.%2520We%2520also%250Ashow%2520that%2520a%2520context%2520is%2520the%2520most%2520useful%2520when%2520the%2520association%2520between%2520X%2520and%2520A%2520is%250Aneither%2520too%2520strong%2520nor%2520too%2520weak.%2520The%2520important%2520implication%2520of%2520the%2520contexture%250Atheory%2520is%2520that%2520increasing%2520the%2520model%2520size%2520alone%2520will%2520achieve%2520diminishing%250Areturns%252C%2520and%2520further%2520advancements%2520require%2520better%2520contexts.%250A%2520%2520We%2520demonstrate%2520that%2520many%2520pretraining%2520objectives%2520can%2520learn%2520the%2520contexture%252C%250Aincluding%2520supervised%2520learning%252C%2520self-supervised%2520learning%252C%2520generative%2520models%252C%250Aetc.%2520Then%252C%2520we%2520introduce%2520two%2520general%2520objectives%2520--%2520SVME%2520and%2520KISE%252C%2520for%2520learning%250Athe%2520contexture.%2520We%2520also%2520show%2520how%2520to%2520mix%2520multiple%2520contexts%2520together%252C%2520an%250Aeffortless%2520way%2520to%2520create%2520better%2520contexts%2520from%2520existing%2520ones.%2520Then%252C%2520we%2520prove%250Astatistical%2520learning%2520bounds%2520for%2520representation%2520learning.%2520Finally%252C%2520we%2520discuss%250Athe%2520effect%2520of%2520the%2520data%2520distribution%2520shift%2520from%2520pretraining%2520to%2520the%2520downstream%250Atask.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextures%3A%20The%20Mechanism%20of%20Representation%20Learning&entry.906535625=Runtian%20Zhai&entry.1292438233=%20%20This%20dissertation%20establishes%20the%20contexture%20theory%20to%20mathematically%0Acharacterize%20the%20mechanism%20of%20representation%20learning%2C%20or%20pretraining.%20Despite%0Athe%20remarkable%20empirical%20success%20of%20foundation%20models%2C%20it%20is%20not%20very%20clear%0Awhat%20representations%20they%20learn%2C%20and%20why%20these%20representations%20are%20useful%20for%0Avarious%20downstream%20tasks.%20A%20scientific%20understanding%20of%20representation%20learning%0Ais%20critical%2C%20especially%20at%20this%20point%20when%20scaling%20up%20the%20model%20size%20is%0Aproducing%20diminishing%20returns%2C%20and%20designing%20new%20pretraining%20methods%20is%0Aimperative%20for%20further%20progress.%0A%20%20Prior%20work%20treated%20different%20representation%20learning%20methods%20quite%0Adifferently%2C%20whereas%20the%20contexture%20theory%20provides%20a%20unified%20framework%20for%0Aanalyzing%20these%20methods.%20The%20central%20argument%20is%20that%20a%20representation%20is%0Alearned%20from%20the%20association%20between%20the%20input%20X%20and%20a%20context%20variable%20A.%20We%0Aprove%20that%20if%20an%20encoder%20captures%20the%20maximum%20information%20of%20this%20association%2C%0Ain%20which%20case%20we%20say%20that%20the%20encoder%20learns%20the%20contexture%2C%20then%20it%20will%20be%0Aoptimal%20on%20the%20class%20of%20tasks%20that%20are%20compatible%20with%20the%20context.%20We%20also%0Ashow%20that%20a%20context%20is%20the%20most%20useful%20when%20the%20association%20between%20X%20and%20A%20is%0Aneither%20too%20strong%20nor%20too%20weak.%20The%20important%20implication%20of%20the%20contexture%0Atheory%20is%20that%20increasing%20the%20model%20size%20alone%20will%20achieve%20diminishing%0Areturns%2C%20and%20further%20advancements%20require%20better%20contexts.%0A%20%20We%20demonstrate%20that%20many%20pretraining%20objectives%20can%20learn%20the%20contexture%2C%0Aincluding%20supervised%20learning%2C%20self-supervised%20learning%2C%20generative%20models%2C%0Aetc.%20Then%2C%20we%20introduce%20two%20general%20objectives%20--%20SVME%20and%20KISE%2C%20for%20learning%0Athe%20contexture.%20We%20also%20show%20how%20to%20mix%20multiple%20contexts%20together%2C%20an%0Aeffortless%20way%20to%20create%20better%20contexts%20from%20existing%20ones.%20Then%2C%20we%20prove%0Astatistical%20learning%20bounds%20for%20representation%20learning.%20Finally%2C%20we%20discuss%0Athe%20effect%20of%20the%20data%20distribution%20shift%20from%20pretraining%20to%20the%20downstream%0Atask.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19792v1&entry.124074799=Read"},
{"title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback", "author": "Chenhan Jiang and Yihan Zeng and Hang Xu and Dit-Yan Yeung", "abstract": "  Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Comprehensive evaluations demonstrate\nthat our framework, CoherenDream, establishes state-of-the-art performance in\ntext-aligned 3D generation across multiple benchmarks, including T$^3$Bench and\nTIFA subset. Qualitative results showcase the superior performance of\nCoherenDream in preserving textual consistency and semantic interactions. As\nthe first study to incorporate MLLMs into SDS optimization, we also conduct\nextensive ablation studies to explore optimal MLLM adaptations for 3D\ngeneration tasks.\n", "link": "http://arxiv.org/abs/2504.19860v1", "date": "2025-04-28", "relevancy": 2.9278, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoherenDream%3A%20Boosting%20Holistic%20Text%20Coherence%20in%203D%20Generation%20via%0A%20%20Multimodal%20Large%20Language%20Models%20Feedback&body=Title%3A%20CoherenDream%3A%20Boosting%20Holistic%20Text%20Coherence%20in%203D%20Generation%20via%0A%20%20Multimodal%20Large%20Language%20Models%20Feedback%0AAuthor%3A%20Chenhan%20Jiang%20and%20Yihan%20Zeng%20and%20Hang%20Xu%20and%20Dit-Yan%20Yeung%0AAbstract%3A%20%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20achieved%20remarkable%20success%20in%0Atext-to-3D%20content%20generation.%20However%2C%20SDS-based%20methods%20struggle%20to%20maintain%0Asemantic%20fidelity%20for%20user%20prompts%2C%20particularly%20when%20involving%20multiple%0Aobjects%20with%20intricate%20interactions.%20While%20existing%20approaches%20often%20address%203D%0Aconsistency%20through%20multiview%20diffusion%20model%20fine-tuning%20on%203D%20datasets%2C%20this%0Astrategy%20inadvertently%20exacerbates%20text-3D%20alignment%20degradation.%20The%0Alimitation%20stems%20from%20SDS%27s%20inherent%20accumulation%20of%20view-independent%20biases%0Aduring%20optimization%2C%20which%20progressively%20diverges%20from%20the%20ideal%20text%20alignment%0Adirection.%20To%20alleviate%20this%20limitation%2C%20we%20propose%20a%20novel%20SDS%20objective%2C%0Adubbed%20as%20Textual%20Coherent%20Score%20Distillation%20%28TCSD%29%2C%20which%20integrates%0Aalignment%20feedback%20from%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20TCSD%0Aleverages%20cross-modal%20understanding%20capabilities%20of%20MLLMs%20to%20assess%20and%20guide%0Athe%20text-3D%20correspondence%20during%20the%20optimization.%20We%20further%20develop%0A3DLLaVA-CRITIC%20-%20a%20fine-tuned%20MLLM%20specialized%20for%20evaluating%20multiview%20text%0Aalignment%20in%203D%20generations.%20Additionally%2C%20we%20introduce%20an%20LLM-layout%0Ainitialization%20that%20significantly%20accelerates%20optimization%20convergence%20through%0Asemantic-aware%20spatial%20configuration.%20Comprehensive%20evaluations%20demonstrate%0Athat%20our%20framework%2C%20CoherenDream%2C%20establishes%20state-of-the-art%20performance%20in%0Atext-aligned%203D%20generation%20across%20multiple%20benchmarks%2C%20including%20T%24%5E3%24Bench%20and%0ATIFA%20subset.%20Qualitative%20results%20showcase%20the%20superior%20performance%20of%0ACoherenDream%20in%20preserving%20textual%20consistency%20and%20semantic%20interactions.%20As%0Athe%20first%20study%20to%20incorporate%20MLLMs%20into%20SDS%20optimization%2C%20we%20also%20conduct%0Aextensive%20ablation%20studies%20to%20explore%20optimal%20MLLM%20adaptations%20for%203D%0Ageneration%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoherenDream%253A%2520Boosting%2520Holistic%2520Text%2520Coherence%2520in%25203D%2520Generation%2520via%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%2520Feedback%26entry.906535625%3DChenhan%2520Jiang%2520and%2520Yihan%2520Zeng%2520and%2520Hang%2520Xu%2520and%2520Dit-Yan%2520Yeung%26entry.1292438233%3D%2520%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520has%2520achieved%2520remarkable%2520success%2520in%250Atext-to-3D%2520content%2520generation.%2520However%252C%2520SDS-based%2520methods%2520struggle%2520to%2520maintain%250Asemantic%2520fidelity%2520for%2520user%2520prompts%252C%2520particularly%2520when%2520involving%2520multiple%250Aobjects%2520with%2520intricate%2520interactions.%2520While%2520existing%2520approaches%2520often%2520address%25203D%250Aconsistency%2520through%2520multiview%2520diffusion%2520model%2520fine-tuning%2520on%25203D%2520datasets%252C%2520this%250Astrategy%2520inadvertently%2520exacerbates%2520text-3D%2520alignment%2520degradation.%2520The%250Alimitation%2520stems%2520from%2520SDS%2527s%2520inherent%2520accumulation%2520of%2520view-independent%2520biases%250Aduring%2520optimization%252C%2520which%2520progressively%2520diverges%2520from%2520the%2520ideal%2520text%2520alignment%250Adirection.%2520To%2520alleviate%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520SDS%2520objective%252C%250Adubbed%2520as%2520Textual%2520Coherent%2520Score%2520Distillation%2520%2528TCSD%2529%252C%2520which%2520integrates%250Aalignment%2520feedback%2520from%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Our%2520TCSD%250Aleverages%2520cross-modal%2520understanding%2520capabilities%2520of%2520MLLMs%2520to%2520assess%2520and%2520guide%250Athe%2520text-3D%2520correspondence%2520during%2520the%2520optimization.%2520We%2520further%2520develop%250A3DLLaVA-CRITIC%2520-%2520a%2520fine-tuned%2520MLLM%2520specialized%2520for%2520evaluating%2520multiview%2520text%250Aalignment%2520in%25203D%2520generations.%2520Additionally%252C%2520we%2520introduce%2520an%2520LLM-layout%250Ainitialization%2520that%2520significantly%2520accelerates%2520optimization%2520convergence%2520through%250Asemantic-aware%2520spatial%2520configuration.%2520Comprehensive%2520evaluations%2520demonstrate%250Athat%2520our%2520framework%252C%2520CoherenDream%252C%2520establishes%2520state-of-the-art%2520performance%2520in%250Atext-aligned%25203D%2520generation%2520across%2520multiple%2520benchmarks%252C%2520including%2520T%2524%255E3%2524Bench%2520and%250ATIFA%2520subset.%2520Qualitative%2520results%2520showcase%2520the%2520superior%2520performance%2520of%250ACoherenDream%2520in%2520preserving%2520textual%2520consistency%2520and%2520semantic%2520interactions.%2520As%250Athe%2520first%2520study%2520to%2520incorporate%2520MLLMs%2520into%2520SDS%2520optimization%252C%2520we%2520also%2520conduct%250Aextensive%2520ablation%2520studies%2520to%2520explore%2520optimal%2520MLLM%2520adaptations%2520for%25203D%250Ageneration%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoherenDream%3A%20Boosting%20Holistic%20Text%20Coherence%20in%203D%20Generation%20via%0A%20%20Multimodal%20Large%20Language%20Models%20Feedback&entry.906535625=Chenhan%20Jiang%20and%20Yihan%20Zeng%20and%20Hang%20Xu%20and%20Dit-Yan%20Yeung&entry.1292438233=%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20achieved%20remarkable%20success%20in%0Atext-to-3D%20content%20generation.%20However%2C%20SDS-based%20methods%20struggle%20to%20maintain%0Asemantic%20fidelity%20for%20user%20prompts%2C%20particularly%20when%20involving%20multiple%0Aobjects%20with%20intricate%20interactions.%20While%20existing%20approaches%20often%20address%203D%0Aconsistency%20through%20multiview%20diffusion%20model%20fine-tuning%20on%203D%20datasets%2C%20this%0Astrategy%20inadvertently%20exacerbates%20text-3D%20alignment%20degradation.%20The%0Alimitation%20stems%20from%20SDS%27s%20inherent%20accumulation%20of%20view-independent%20biases%0Aduring%20optimization%2C%20which%20progressively%20diverges%20from%20the%20ideal%20text%20alignment%0Adirection.%20To%20alleviate%20this%20limitation%2C%20we%20propose%20a%20novel%20SDS%20objective%2C%0Adubbed%20as%20Textual%20Coherent%20Score%20Distillation%20%28TCSD%29%2C%20which%20integrates%0Aalignment%20feedback%20from%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20TCSD%0Aleverages%20cross-modal%20understanding%20capabilities%20of%20MLLMs%20to%20assess%20and%20guide%0Athe%20text-3D%20correspondence%20during%20the%20optimization.%20We%20further%20develop%0A3DLLaVA-CRITIC%20-%20a%20fine-tuned%20MLLM%20specialized%20for%20evaluating%20multiview%20text%0Aalignment%20in%203D%20generations.%20Additionally%2C%20we%20introduce%20an%20LLM-layout%0Ainitialization%20that%20significantly%20accelerates%20optimization%20convergence%20through%0Asemantic-aware%20spatial%20configuration.%20Comprehensive%20evaluations%20demonstrate%0Athat%20our%20framework%2C%20CoherenDream%2C%20establishes%20state-of-the-art%20performance%20in%0Atext-aligned%203D%20generation%20across%20multiple%20benchmarks%2C%20including%20T%24%5E3%24Bench%20and%0ATIFA%20subset.%20Qualitative%20results%20showcase%20the%20superior%20performance%20of%0ACoherenDream%20in%20preserving%20textual%20consistency%20and%20semantic%20interactions.%20As%0Athe%20first%20study%20to%20incorporate%20MLLMs%20into%20SDS%20optimization%2C%20we%20also%20conduct%0Aextensive%20ablation%20studies%20to%20explore%20optimal%20MLLM%20adaptations%20for%203D%0Ageneration%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19860v1&entry.124074799=Read"},
{"title": "Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation", "author": "Victoria Yue Chen and Daoye Wang and Stephan Garbin and Sebastian Winberg and Timo Bolkart and Thabo Beeler", "abstract": "  Face registration deforms a template mesh to closely fit a 3D face scan, the\nquality of which commonly degrades in non-skin regions (e.g., hair, beard,\naccessories), because the optimized template-to-scan distance pulls the\ntemplate mesh towards the noisy scan surface. Improving registration quality\nrequires a clean separation of skin and non-skin regions on the scan mesh.\nExisting image-based (2D) or scan-based (3D) segmentation methods however\nperform poorly. Image-based segmentation outputs multi-view inconsistent masks,\nand they cannot account for scan inaccuracies or scan-image misalignment, while\nscan-based methods suffer from lower spatial resolution compared to images. In\nthis work, we introduce a novel method that accurately separates skin from\nnon-skin geometry on 3D human head scans. For this, our method extracts\nfeatures from multi-view images using a frozen image foundation model and\naggregates these features in 3D. These lifted 2D features are then fused with\n3D geometric features extracted from the scan mesh, to then predict a\nsegmentation mask directly on the scan mesh. We show that our segmentations\nimprove the registration accuracy over pure 2D or 3D segmentation methods by\n8.89% and 14.3%, respectively. Although trained only on synthetic data, our\nmodel generalizes well to real data.\n", "link": "http://arxiv.org/abs/2504.19718v1", "date": "2025-04-28", "relevancy": 2.9208, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6011}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5845}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixels2Points%3A%20Fusing%202D%20and%203D%20Features%20for%20Facial%20Skin%20Segmentation&body=Title%3A%20Pixels2Points%3A%20Fusing%202D%20and%203D%20Features%20for%20Facial%20Skin%20Segmentation%0AAuthor%3A%20Victoria%20Yue%20Chen%20and%20Daoye%20Wang%20and%20Stephan%20Garbin%20and%20Sebastian%20Winberg%20and%20Timo%20Bolkart%20and%20Thabo%20Beeler%0AAbstract%3A%20%20%20Face%20registration%20deforms%20a%20template%20mesh%20to%20closely%20fit%20a%203D%20face%20scan%2C%20the%0Aquality%20of%20which%20commonly%20degrades%20in%20non-skin%20regions%20%28e.g.%2C%20hair%2C%20beard%2C%0Aaccessories%29%2C%20because%20the%20optimized%20template-to-scan%20distance%20pulls%20the%0Atemplate%20mesh%20towards%20the%20noisy%20scan%20surface.%20Improving%20registration%20quality%0Arequires%20a%20clean%20separation%20of%20skin%20and%20non-skin%20regions%20on%20the%20scan%20mesh.%0AExisting%20image-based%20%282D%29%20or%20scan-based%20%283D%29%20segmentation%20methods%20however%0Aperform%20poorly.%20Image-based%20segmentation%20outputs%20multi-view%20inconsistent%20masks%2C%0Aand%20they%20cannot%20account%20for%20scan%20inaccuracies%20or%20scan-image%20misalignment%2C%20while%0Ascan-based%20methods%20suffer%20from%20lower%20spatial%20resolution%20compared%20to%20images.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20method%20that%20accurately%20separates%20skin%20from%0Anon-skin%20geometry%20on%203D%20human%20head%20scans.%20For%20this%2C%20our%20method%20extracts%0Afeatures%20from%20multi-view%20images%20using%20a%20frozen%20image%20foundation%20model%20and%0Aaggregates%20these%20features%20in%203D.%20These%20lifted%202D%20features%20are%20then%20fused%20with%0A3D%20geometric%20features%20extracted%20from%20the%20scan%20mesh%2C%20to%20then%20predict%20a%0Asegmentation%20mask%20directly%20on%20the%20scan%20mesh.%20We%20show%20that%20our%20segmentations%0Aimprove%20the%20registration%20accuracy%20over%20pure%202D%20or%203D%20segmentation%20methods%20by%0A8.89%25%20and%2014.3%25%2C%20respectively.%20Although%20trained%20only%20on%20synthetic%20data%2C%20our%0Amodel%20generalizes%20well%20to%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixels2Points%253A%2520Fusing%25202D%2520and%25203D%2520Features%2520for%2520Facial%2520Skin%2520Segmentation%26entry.906535625%3DVictoria%2520Yue%2520Chen%2520and%2520Daoye%2520Wang%2520and%2520Stephan%2520Garbin%2520and%2520Sebastian%2520Winberg%2520and%2520Timo%2520Bolkart%2520and%2520Thabo%2520Beeler%26entry.1292438233%3D%2520%2520Face%2520registration%2520deforms%2520a%2520template%2520mesh%2520to%2520closely%2520fit%2520a%25203D%2520face%2520scan%252C%2520the%250Aquality%2520of%2520which%2520commonly%2520degrades%2520in%2520non-skin%2520regions%2520%2528e.g.%252C%2520hair%252C%2520beard%252C%250Aaccessories%2529%252C%2520because%2520the%2520optimized%2520template-to-scan%2520distance%2520pulls%2520the%250Atemplate%2520mesh%2520towards%2520the%2520noisy%2520scan%2520surface.%2520Improving%2520registration%2520quality%250Arequires%2520a%2520clean%2520separation%2520of%2520skin%2520and%2520non-skin%2520regions%2520on%2520the%2520scan%2520mesh.%250AExisting%2520image-based%2520%25282D%2529%2520or%2520scan-based%2520%25283D%2529%2520segmentation%2520methods%2520however%250Aperform%2520poorly.%2520Image-based%2520segmentation%2520outputs%2520multi-view%2520inconsistent%2520masks%252C%250Aand%2520they%2520cannot%2520account%2520for%2520scan%2520inaccuracies%2520or%2520scan-image%2520misalignment%252C%2520while%250Ascan-based%2520methods%2520suffer%2520from%2520lower%2520spatial%2520resolution%2520compared%2520to%2520images.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520accurately%2520separates%2520skin%2520from%250Anon-skin%2520geometry%2520on%25203D%2520human%2520head%2520scans.%2520For%2520this%252C%2520our%2520method%2520extracts%250Afeatures%2520from%2520multi-view%2520images%2520using%2520a%2520frozen%2520image%2520foundation%2520model%2520and%250Aaggregates%2520these%2520features%2520in%25203D.%2520These%2520lifted%25202D%2520features%2520are%2520then%2520fused%2520with%250A3D%2520geometric%2520features%2520extracted%2520from%2520the%2520scan%2520mesh%252C%2520to%2520then%2520predict%2520a%250Asegmentation%2520mask%2520directly%2520on%2520the%2520scan%2520mesh.%2520We%2520show%2520that%2520our%2520segmentations%250Aimprove%2520the%2520registration%2520accuracy%2520over%2520pure%25202D%2520or%25203D%2520segmentation%2520methods%2520by%250A8.89%2525%2520and%252014.3%2525%252C%2520respectively.%2520Although%2520trained%2520only%2520on%2520synthetic%2520data%252C%2520our%250Amodel%2520generalizes%2520well%2520to%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixels2Points%3A%20Fusing%202D%20and%203D%20Features%20for%20Facial%20Skin%20Segmentation&entry.906535625=Victoria%20Yue%20Chen%20and%20Daoye%20Wang%20and%20Stephan%20Garbin%20and%20Sebastian%20Winberg%20and%20Timo%20Bolkart%20and%20Thabo%20Beeler&entry.1292438233=%20%20Face%20registration%20deforms%20a%20template%20mesh%20to%20closely%20fit%20a%203D%20face%20scan%2C%20the%0Aquality%20of%20which%20commonly%20degrades%20in%20non-skin%20regions%20%28e.g.%2C%20hair%2C%20beard%2C%0Aaccessories%29%2C%20because%20the%20optimized%20template-to-scan%20distance%20pulls%20the%0Atemplate%20mesh%20towards%20the%20noisy%20scan%20surface.%20Improving%20registration%20quality%0Arequires%20a%20clean%20separation%20of%20skin%20and%20non-skin%20regions%20on%20the%20scan%20mesh.%0AExisting%20image-based%20%282D%29%20or%20scan-based%20%283D%29%20segmentation%20methods%20however%0Aperform%20poorly.%20Image-based%20segmentation%20outputs%20multi-view%20inconsistent%20masks%2C%0Aand%20they%20cannot%20account%20for%20scan%20inaccuracies%20or%20scan-image%20misalignment%2C%20while%0Ascan-based%20methods%20suffer%20from%20lower%20spatial%20resolution%20compared%20to%20images.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20method%20that%20accurately%20separates%20skin%20from%0Anon-skin%20geometry%20on%203D%20human%20head%20scans.%20For%20this%2C%20our%20method%20extracts%0Afeatures%20from%20multi-view%20images%20using%20a%20frozen%20image%20foundation%20model%20and%0Aaggregates%20these%20features%20in%203D.%20These%20lifted%202D%20features%20are%20then%20fused%20with%0A3D%20geometric%20features%20extracted%20from%20the%20scan%20mesh%2C%20to%20then%20predict%20a%0Asegmentation%20mask%20directly%20on%20the%20scan%20mesh.%20We%20show%20that%20our%20segmentations%0Aimprove%20the%20registration%20accuracy%20over%20pure%202D%20or%203D%20segmentation%20methods%20by%0A8.89%25%20and%2014.3%25%2C%20respectively.%20Although%20trained%20only%20on%20synthetic%20data%2C%20our%0Amodel%20generalizes%20well%20to%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19718v1&entry.124074799=Read"},
{"title": "Foundation Model-Driven Framework for Human-Object Interaction\n  Prediction with Segmentation Mask Integration", "author": "Juhan Park and Kyungjae Lee and Hyung Jin Chang and Jungchan Cho", "abstract": "  In this work, we introduce Segmentation to Human-Object Interaction\n(\\textit{\\textbf{Seg2HOI}}) approach, a novel framework that integrates\nsegmentation-based vision foundation models with the human-object interaction\ntask, distinguished from traditional detection-based Human-Object Interaction\n(HOI) methods. Our approach enhances HOI detection by not only predicting the\nstandard triplets but also introducing quadruplets, which extend HOI triplets\nby including segmentation masks for human-object pairs. More specifically,\nSeg2HOI inherits the properties of the vision foundation model (e.g.,\npromptable and interactive mechanisms) and incorporates a decoder that applies\nthese attributes to HOI task. Despite training only for HOI, without additional\ntraining mechanisms for these properties, the framework demonstrates that such\nfeatures still operate efficiently. Extensive experiments on two public\nbenchmark datasets demonstrate that Seg2HOI achieves performance comparable to\nstate-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that\nSeg2HOI can generate HOI quadruplets and interactive HOI segmentation from\nnovel text and visual prompts that were not used during training, making it\nversatile for a wide range of applications by leveraging this flexibility.\n", "link": "http://arxiv.org/abs/2504.19847v1", "date": "2025-04-28", "relevancy": 2.9044, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Model-Driven%20Framework%20for%20Human-Object%20Interaction%0A%20%20Prediction%20with%20Segmentation%20Mask%20Integration&body=Title%3A%20Foundation%20Model-Driven%20Framework%20for%20Human-Object%20Interaction%0A%20%20Prediction%20with%20Segmentation%20Mask%20Integration%0AAuthor%3A%20Juhan%20Park%20and%20Kyungjae%20Lee%20and%20Hyung%20Jin%20Chang%20and%20Jungchan%20Cho%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Segmentation%20to%20Human-Object%20Interaction%0A%28%5Ctextit%7B%5Ctextbf%7BSeg2HOI%7D%7D%29%20approach%2C%20a%20novel%20framework%20that%20integrates%0Asegmentation-based%20vision%20foundation%20models%20with%20the%20human-object%20interaction%0Atask%2C%20distinguished%20from%20traditional%20detection-based%20Human-Object%20Interaction%0A%28HOI%29%20methods.%20Our%20approach%20enhances%20HOI%20detection%20by%20not%20only%20predicting%20the%0Astandard%20triplets%20but%20also%20introducing%20quadruplets%2C%20which%20extend%20HOI%20triplets%0Aby%20including%20segmentation%20masks%20for%20human-object%20pairs.%20More%20specifically%2C%0ASeg2HOI%20inherits%20the%20properties%20of%20the%20vision%20foundation%20model%20%28e.g.%2C%0Apromptable%20and%20interactive%20mechanisms%29%20and%20incorporates%20a%20decoder%20that%20applies%0Athese%20attributes%20to%20HOI%20task.%20Despite%20training%20only%20for%20HOI%2C%20without%20additional%0Atraining%20mechanisms%20for%20these%20properties%2C%20the%20framework%20demonstrates%20that%20such%0Afeatures%20still%20operate%20efficiently.%20Extensive%20experiments%20on%20two%20public%0Abenchmark%20datasets%20demonstrate%20that%20Seg2HOI%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20methods%2C%20even%20in%20zero-shot%20scenarios.%20Lastly%2C%20we%20propose%20that%0ASeg2HOI%20can%20generate%20HOI%20quadruplets%20and%20interactive%20HOI%20segmentation%20from%0Anovel%20text%20and%20visual%20prompts%20that%20were%20not%20used%20during%20training%2C%20making%20it%0Aversatile%20for%20a%20wide%20range%20of%20applications%20by%20leveraging%20this%20flexibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Model-Driven%2520Framework%2520for%2520Human-Object%2520Interaction%250A%2520%2520Prediction%2520with%2520Segmentation%2520Mask%2520Integration%26entry.906535625%3DJuhan%2520Park%2520and%2520Kyungjae%2520Lee%2520and%2520Hyung%2520Jin%2520Chang%2520and%2520Jungchan%2520Cho%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Segmentation%2520to%2520Human-Object%2520Interaction%250A%2528%255Ctextit%257B%255Ctextbf%257BSeg2HOI%257D%257D%2529%2520approach%252C%2520a%2520novel%2520framework%2520that%2520integrates%250Asegmentation-based%2520vision%2520foundation%2520models%2520with%2520the%2520human-object%2520interaction%250Atask%252C%2520distinguished%2520from%2520traditional%2520detection-based%2520Human-Object%2520Interaction%250A%2528HOI%2529%2520methods.%2520Our%2520approach%2520enhances%2520HOI%2520detection%2520by%2520not%2520only%2520predicting%2520the%250Astandard%2520triplets%2520but%2520also%2520introducing%2520quadruplets%252C%2520which%2520extend%2520HOI%2520triplets%250Aby%2520including%2520segmentation%2520masks%2520for%2520human-object%2520pairs.%2520More%2520specifically%252C%250ASeg2HOI%2520inherits%2520the%2520properties%2520of%2520the%2520vision%2520foundation%2520model%2520%2528e.g.%252C%250Apromptable%2520and%2520interactive%2520mechanisms%2529%2520and%2520incorporates%2520a%2520decoder%2520that%2520applies%250Athese%2520attributes%2520to%2520HOI%2520task.%2520Despite%2520training%2520only%2520for%2520HOI%252C%2520without%2520additional%250Atraining%2520mechanisms%2520for%2520these%2520properties%252C%2520the%2520framework%2520demonstrates%2520that%2520such%250Afeatures%2520still%2520operate%2520efficiently.%2520Extensive%2520experiments%2520on%2520two%2520public%250Abenchmark%2520datasets%2520demonstrate%2520that%2520Seg2HOI%2520achieves%2520performance%2520comparable%2520to%250Astate-of-the-art%2520methods%252C%2520even%2520in%2520zero-shot%2520scenarios.%2520Lastly%252C%2520we%2520propose%2520that%250ASeg2HOI%2520can%2520generate%2520HOI%2520quadruplets%2520and%2520interactive%2520HOI%2520segmentation%2520from%250Anovel%2520text%2520and%2520visual%2520prompts%2520that%2520were%2520not%2520used%2520during%2520training%252C%2520making%2520it%250Aversatile%2520for%2520a%2520wide%2520range%2520of%2520applications%2520by%2520leveraging%2520this%2520flexibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Model-Driven%20Framework%20for%20Human-Object%20Interaction%0A%20%20Prediction%20with%20Segmentation%20Mask%20Integration&entry.906535625=Juhan%20Park%20and%20Kyungjae%20Lee%20and%20Hyung%20Jin%20Chang%20and%20Jungchan%20Cho&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Segmentation%20to%20Human-Object%20Interaction%0A%28%5Ctextit%7B%5Ctextbf%7BSeg2HOI%7D%7D%29%20approach%2C%20a%20novel%20framework%20that%20integrates%0Asegmentation-based%20vision%20foundation%20models%20with%20the%20human-object%20interaction%0Atask%2C%20distinguished%20from%20traditional%20detection-based%20Human-Object%20Interaction%0A%28HOI%29%20methods.%20Our%20approach%20enhances%20HOI%20detection%20by%20not%20only%20predicting%20the%0Astandard%20triplets%20but%20also%20introducing%20quadruplets%2C%20which%20extend%20HOI%20triplets%0Aby%20including%20segmentation%20masks%20for%20human-object%20pairs.%20More%20specifically%2C%0ASeg2HOI%20inherits%20the%20properties%20of%20the%20vision%20foundation%20model%20%28e.g.%2C%0Apromptable%20and%20interactive%20mechanisms%29%20and%20incorporates%20a%20decoder%20that%20applies%0Athese%20attributes%20to%20HOI%20task.%20Despite%20training%20only%20for%20HOI%2C%20without%20additional%0Atraining%20mechanisms%20for%20these%20properties%2C%20the%20framework%20demonstrates%20that%20such%0Afeatures%20still%20operate%20efficiently.%20Extensive%20experiments%20on%20two%20public%0Abenchmark%20datasets%20demonstrate%20that%20Seg2HOI%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20methods%2C%20even%20in%20zero-shot%20scenarios.%20Lastly%2C%20we%20propose%20that%0ASeg2HOI%20can%20generate%20HOI%20quadruplets%20and%20interactive%20HOI%20segmentation%20from%0Anovel%20text%20and%20visual%20prompts%20that%20were%20not%20used%20during%20training%2C%20making%20it%0Aversatile%20for%20a%20wide%20range%20of%20applications%20by%20leveraging%20this%20flexibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19847v1&entry.124074799=Read"},
{"title": "DeeCLIP: A Robust and Generalizable Transformer-Based Framework for\n  Detecting AI-Generated Images", "author": "Mamadou Keita and Wassim Hamidouche and Hessen Bougueffa Eutamene and Abdelmalik Taleb-Ahmed and Abdenour Hadid", "abstract": "  This paper introduces DeeCLIP, a novel framework for detecting AI-generated\nimages using CLIP-ViT and fusion learning. Despite significant advancements in\ngenerative models capable of creating highly photorealistic images, existing\ndetection methods often struggle to generalize across different models and are\nhighly sensitive to minor perturbations. To address these challenges, DeeCLIP\nincorporates DeeFuser, a fusion module that combines high-level and low-level\nfeatures, improving robustness against degradations such as compression and\nblurring. Additionally, we apply triplet loss to refine the embedding space,\nenhancing the model's ability to distinguish between real and synthetic\ncontent. To further enable lightweight adaptation while preserving pre-trained\nknowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation\n(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot\nlearning without sacrificing generalization. Trained exclusively on 4-class\nProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets\ncomposed of generative adversarial network (GAN) and diffusion models. Despite\nhaving fewer trainable parameters, DeeCLIP outperforms existing methods,\ndemonstrating superior robustness against various generative models and\nreal-world distortions. The code is publicly available at\nhttps://github.com/Mamadou-Keita/DeeCLIP for research purposes.\n", "link": "http://arxiv.org/abs/2504.19876v1", "date": "2025-04-28", "relevancy": 2.8837, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5728}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeeCLIP%3A%20A%20Robust%20and%20Generalizable%20Transformer-Based%20Framework%20for%0A%20%20Detecting%20AI-Generated%20Images&body=Title%3A%20DeeCLIP%3A%20A%20Robust%20and%20Generalizable%20Transformer-Based%20Framework%20for%0A%20%20Detecting%20AI-Generated%20Images%0AAuthor%3A%20Mamadou%20Keita%20and%20Wassim%20Hamidouche%20and%20Hessen%20Bougueffa%20Eutamene%20and%20Abdelmalik%20Taleb-Ahmed%20and%20Abdenour%20Hadid%0AAbstract%3A%20%20%20This%20paper%20introduces%20DeeCLIP%2C%20a%20novel%20framework%20for%20detecting%20AI-generated%0Aimages%20using%20CLIP-ViT%20and%20fusion%20learning.%20Despite%20significant%20advancements%20in%0Agenerative%20models%20capable%20of%20creating%20highly%20photorealistic%20images%2C%20existing%0Adetection%20methods%20often%20struggle%20to%20generalize%20across%20different%20models%20and%20are%0Ahighly%20sensitive%20to%20minor%20perturbations.%20To%20address%20these%20challenges%2C%20DeeCLIP%0Aincorporates%20DeeFuser%2C%20a%20fusion%20module%20that%20combines%20high-level%20and%20low-level%0Afeatures%2C%20improving%20robustness%20against%20degradations%20such%20as%20compression%20and%0Ablurring.%20Additionally%2C%20we%20apply%20triplet%20loss%20to%20refine%20the%20embedding%20space%2C%0Aenhancing%20the%20model%27s%20ability%20to%20distinguish%20between%20real%20and%20synthetic%0Acontent.%20To%20further%20enable%20lightweight%20adaptation%20while%20preserving%20pre-trained%0Aknowledge%2C%20we%20adopt%20parameter-efficient%20fine-tuning%20using%20low-rank%20adaptation%0A%28LoRA%29%20within%20the%20CLIP-ViT%20backbone.%20This%20approach%20supports%20effective%20zero-shot%0Alearning%20without%20sacrificing%20generalization.%20Trained%20exclusively%20on%204-class%0AProGAN%20data%2C%20DeeCLIP%20achieves%20an%20average%20accuracy%20of%2089.00%25%20on%2019%20test%20subsets%0Acomposed%20of%20generative%20adversarial%20network%20%28GAN%29%20and%20diffusion%20models.%20Despite%0Ahaving%20fewer%20trainable%20parameters%2C%20DeeCLIP%20outperforms%20existing%20methods%2C%0Ademonstrating%20superior%20robustness%20against%20various%20generative%20models%20and%0Areal-world%20distortions.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Mamadou-Keita/DeeCLIP%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeeCLIP%253A%2520A%2520Robust%2520and%2520Generalizable%2520Transformer-Based%2520Framework%2520for%250A%2520%2520Detecting%2520AI-Generated%2520Images%26entry.906535625%3DMamadou%2520Keita%2520and%2520Wassim%2520Hamidouche%2520and%2520Hessen%2520Bougueffa%2520Eutamene%2520and%2520Abdelmalik%2520Taleb-Ahmed%2520and%2520Abdenour%2520Hadid%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520DeeCLIP%252C%2520a%2520novel%2520framework%2520for%2520detecting%2520AI-generated%250Aimages%2520using%2520CLIP-ViT%2520and%2520fusion%2520learning.%2520Despite%2520significant%2520advancements%2520in%250Agenerative%2520models%2520capable%2520of%2520creating%2520highly%2520photorealistic%2520images%252C%2520existing%250Adetection%2520methods%2520often%2520struggle%2520to%2520generalize%2520across%2520different%2520models%2520and%2520are%250Ahighly%2520sensitive%2520to%2520minor%2520perturbations.%2520To%2520address%2520these%2520challenges%252C%2520DeeCLIP%250Aincorporates%2520DeeFuser%252C%2520a%2520fusion%2520module%2520that%2520combines%2520high-level%2520and%2520low-level%250Afeatures%252C%2520improving%2520robustness%2520against%2520degradations%2520such%2520as%2520compression%2520and%250Ablurring.%2520Additionally%252C%2520we%2520apply%2520triplet%2520loss%2520to%2520refine%2520the%2520embedding%2520space%252C%250Aenhancing%2520the%2520model%2527s%2520ability%2520to%2520distinguish%2520between%2520real%2520and%2520synthetic%250Acontent.%2520To%2520further%2520enable%2520lightweight%2520adaptation%2520while%2520preserving%2520pre-trained%250Aknowledge%252C%2520we%2520adopt%2520parameter-efficient%2520fine-tuning%2520using%2520low-rank%2520adaptation%250A%2528LoRA%2529%2520within%2520the%2520CLIP-ViT%2520backbone.%2520This%2520approach%2520supports%2520effective%2520zero-shot%250Alearning%2520without%2520sacrificing%2520generalization.%2520Trained%2520exclusively%2520on%25204-class%250AProGAN%2520data%252C%2520DeeCLIP%2520achieves%2520an%2520average%2520accuracy%2520of%252089.00%2525%2520on%252019%2520test%2520subsets%250Acomposed%2520of%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520and%2520diffusion%2520models.%2520Despite%250Ahaving%2520fewer%2520trainable%2520parameters%252C%2520DeeCLIP%2520outperforms%2520existing%2520methods%252C%250Ademonstrating%2520superior%2520robustness%2520against%2520various%2520generative%2520models%2520and%250Areal-world%2520distortions.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Mamadou-Keita/DeeCLIP%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeeCLIP%3A%20A%20Robust%20and%20Generalizable%20Transformer-Based%20Framework%20for%0A%20%20Detecting%20AI-Generated%20Images&entry.906535625=Mamadou%20Keita%20and%20Wassim%20Hamidouche%20and%20Hessen%20Bougueffa%20Eutamene%20and%20Abdelmalik%20Taleb-Ahmed%20and%20Abdenour%20Hadid&entry.1292438233=%20%20This%20paper%20introduces%20DeeCLIP%2C%20a%20novel%20framework%20for%20detecting%20AI-generated%0Aimages%20using%20CLIP-ViT%20and%20fusion%20learning.%20Despite%20significant%20advancements%20in%0Agenerative%20models%20capable%20of%20creating%20highly%20photorealistic%20images%2C%20existing%0Adetection%20methods%20often%20struggle%20to%20generalize%20across%20different%20models%20and%20are%0Ahighly%20sensitive%20to%20minor%20perturbations.%20To%20address%20these%20challenges%2C%20DeeCLIP%0Aincorporates%20DeeFuser%2C%20a%20fusion%20module%20that%20combines%20high-level%20and%20low-level%0Afeatures%2C%20improving%20robustness%20against%20degradations%20such%20as%20compression%20and%0Ablurring.%20Additionally%2C%20we%20apply%20triplet%20loss%20to%20refine%20the%20embedding%20space%2C%0Aenhancing%20the%20model%27s%20ability%20to%20distinguish%20between%20real%20and%20synthetic%0Acontent.%20To%20further%20enable%20lightweight%20adaptation%20while%20preserving%20pre-trained%0Aknowledge%2C%20we%20adopt%20parameter-efficient%20fine-tuning%20using%20low-rank%20adaptation%0A%28LoRA%29%20within%20the%20CLIP-ViT%20backbone.%20This%20approach%20supports%20effective%20zero-shot%0Alearning%20without%20sacrificing%20generalization.%20Trained%20exclusively%20on%204-class%0AProGAN%20data%2C%20DeeCLIP%20achieves%20an%20average%20accuracy%20of%2089.00%25%20on%2019%20test%20subsets%0Acomposed%20of%20generative%20adversarial%20network%20%28GAN%29%20and%20diffusion%20models.%20Despite%0Ahaving%20fewer%20trainable%20parameters%2C%20DeeCLIP%20outperforms%20existing%20methods%2C%0Ademonstrating%20superior%20robustness%20against%20various%20generative%20models%20and%0Areal-world%20distortions.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Mamadou-Keita/DeeCLIP%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19876v1&entry.124074799=Read"},
{"title": "Interpretable Dynamic Graph Neural Networks for Small Occluded Object\n  Detection and Tracking", "author": "Shahriar Soudeep and Md Abrar Jahin and M. F. Mridha", "abstract": "  The detection and tracking of small, occluded objects such as pedestrians,\ncyclists, and motorbikes pose significant challenges for traffic surveillance\nsystems because of their erratic movement, frequent occlusion, and poor\nvisibility in dynamic urban environments. Traditional methods like YOLO11,\nwhile proficient in spatial feature extraction for precise detection, often\nstruggle with these small and dynamically moving objects, particularly in\nhandling real-time data updates and resource efficiency. This paper introduces\nDGNN-YOLO, a novel framework that integrates dynamic graph neural networks\n(DGNNs) with YOLO11 to address these limitations. Unlike standard GNNs, DGNNs\nare chosen for their superior ability to dynamically update graph structures in\nreal-time, which enables adaptive and robust tracking of objects in highly\nvariable urban traffic scenarios. This framework constructs and regularly\nupdates its graph representations, capturing objects as nodes and their\ninteractions as edges, thus effectively responding to rapidly changing\nconditions. Additionally, DGNN-YOLO incorporates Grad-CAM, Grad-CAM++, and\nEigen-CAM visualization techniques to enhance interpretability and foster\ntrust, offering insights into the model's decision-making process. Extensive\nexperiments validate the framework's performance, achieving a precision of\n0.8382, recall of 0.6875, and mAP@0.5:0.95 of 0.6476, significantly\noutperforming existing methods. This study offers a scalable and interpretable\nsolution for real-time traffic surveillance and significantly advances\nintelligent transportation systems' capabilities by addressing the critical\nchallenge of detecting and tracking small, occluded objects.\n", "link": "http://arxiv.org/abs/2411.17251v7", "date": "2025-04-28", "relevancy": 2.8589, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5915}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5636}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20for%20Small%20Occluded%20Object%0A%20%20Detection%20and%20Tracking&body=Title%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20for%20Small%20Occluded%20Object%0A%20%20Detection%20and%20Tracking%0AAuthor%3A%20Shahriar%20Soudeep%20and%20Md%20Abrar%20Jahin%20and%20M.%20F.%20Mridha%0AAbstract%3A%20%20%20The%20detection%20and%20tracking%20of%20small%2C%20occluded%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20pose%20significant%20challenges%20for%20traffic%20surveillance%0Asystems%20because%20of%20their%20erratic%20movement%2C%20frequent%20occlusion%2C%20and%20poor%0Avisibility%20in%20dynamic%20urban%20environments.%20Traditional%20methods%20like%20YOLO11%2C%0Awhile%20proficient%20in%20spatial%20feature%20extraction%20for%20precise%20detection%2C%20often%0Astruggle%20with%20these%20small%20and%20dynamically%20moving%20objects%2C%20particularly%20in%0Ahandling%20real-time%20data%20updates%20and%20resource%20efficiency.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20that%20integrates%20dynamic%20graph%20neural%20networks%0A%28DGNNs%29%20with%20YOLO11%20to%20address%20these%20limitations.%20Unlike%20standard%20GNNs%2C%20DGNNs%0Aare%20chosen%20for%20their%20superior%20ability%20to%20dynamically%20update%20graph%20structures%20in%0Areal-time%2C%20which%20enables%20adaptive%20and%20robust%20tracking%20of%20objects%20in%20highly%0Avariable%20urban%20traffic%20scenarios.%20This%20framework%20constructs%20and%20regularly%0Aupdates%20its%20graph%20representations%2C%20capturing%20objects%20as%20nodes%20and%20their%0Ainteractions%20as%20edges%2C%20thus%20effectively%20responding%20to%20rapidly%20changing%0Aconditions.%20Additionally%2C%20DGNN-YOLO%20incorporates%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20and%0AEigen-CAM%20visualization%20techniques%20to%20enhance%20interpretability%20and%20foster%0Atrust%2C%20offering%20insights%20into%20the%20model%27s%20decision-making%20process.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20performance%2C%20achieving%20a%20precision%20of%0A0.8382%2C%20recall%20of%200.6875%2C%20and%20mAP%400.5%3A0.95%20of%200.6476%2C%20significantly%0Aoutperforming%20existing%20methods.%20This%20study%20offers%20a%20scalable%20and%20interpretable%0Asolution%20for%20real-time%20traffic%20surveillance%20and%20significantly%20advances%0Aintelligent%20transportation%20systems%27%20capabilities%20by%20addressing%20the%20critical%0Achallenge%20of%20detecting%20and%20tracking%20small%2C%20occluded%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17251v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Dynamic%2520Graph%2520Neural%2520Networks%2520for%2520Small%2520Occluded%2520Object%250A%2520%2520Detection%2520and%2520Tracking%26entry.906535625%3DShahriar%2520Soudeep%2520and%2520Md%2520Abrar%2520Jahin%2520and%2520M.%2520F.%2520Mridha%26entry.1292438233%3D%2520%2520The%2520detection%2520and%2520tracking%2520of%2520small%252C%2520occluded%2520objects%2520such%2520as%2520pedestrians%252C%250Acyclists%252C%2520and%2520motorbikes%2520pose%2520significant%2520challenges%2520for%2520traffic%2520surveillance%250Asystems%2520because%2520of%2520their%2520erratic%2520movement%252C%2520frequent%2520occlusion%252C%2520and%2520poor%250Avisibility%2520in%2520dynamic%2520urban%2520environments.%2520Traditional%2520methods%2520like%2520YOLO11%252C%250Awhile%2520proficient%2520in%2520spatial%2520feature%2520extraction%2520for%2520precise%2520detection%252C%2520often%250Astruggle%2520with%2520these%2520small%2520and%2520dynamically%2520moving%2520objects%252C%2520particularly%2520in%250Ahandling%2520real-time%2520data%2520updates%2520and%2520resource%2520efficiency.%2520This%2520paper%2520introduces%250ADGNN-YOLO%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520dynamic%2520graph%2520neural%2520networks%250A%2528DGNNs%2529%2520with%2520YOLO11%2520to%2520address%2520these%2520limitations.%2520Unlike%2520standard%2520GNNs%252C%2520DGNNs%250Aare%2520chosen%2520for%2520their%2520superior%2520ability%2520to%2520dynamically%2520update%2520graph%2520structures%2520in%250Areal-time%252C%2520which%2520enables%2520adaptive%2520and%2520robust%2520tracking%2520of%2520objects%2520in%2520highly%250Avariable%2520urban%2520traffic%2520scenarios.%2520This%2520framework%2520constructs%2520and%2520regularly%250Aupdates%2520its%2520graph%2520representations%252C%2520capturing%2520objects%2520as%2520nodes%2520and%2520their%250Ainteractions%2520as%2520edges%252C%2520thus%2520effectively%2520responding%2520to%2520rapidly%2520changing%250Aconditions.%2520Additionally%252C%2520DGNN-YOLO%2520incorporates%2520Grad-CAM%252C%2520Grad-CAM%252B%252B%252C%2520and%250AEigen-CAM%2520visualization%2520techniques%2520to%2520enhance%2520interpretability%2520and%2520foster%250Atrust%252C%2520offering%2520insights%2520into%2520the%2520model%2527s%2520decision-making%2520process.%2520Extensive%250Aexperiments%2520validate%2520the%2520framework%2527s%2520performance%252C%2520achieving%2520a%2520precision%2520of%250A0.8382%252C%2520recall%2520of%25200.6875%252C%2520and%2520mAP%25400.5%253A0.95%2520of%25200.6476%252C%2520significantly%250Aoutperforming%2520existing%2520methods.%2520This%2520study%2520offers%2520a%2520scalable%2520and%2520interpretable%250Asolution%2520for%2520real-time%2520traffic%2520surveillance%2520and%2520significantly%2520advances%250Aintelligent%2520transportation%2520systems%2527%2520capabilities%2520by%2520addressing%2520the%2520critical%250Achallenge%2520of%2520detecting%2520and%2520tracking%2520small%252C%2520occluded%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17251v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Dynamic%20Graph%20Neural%20Networks%20for%20Small%20Occluded%20Object%0A%20%20Detection%20and%20Tracking&entry.906535625=Shahriar%20Soudeep%20and%20Md%20Abrar%20Jahin%20and%20M.%20F.%20Mridha&entry.1292438233=%20%20The%20detection%20and%20tracking%20of%20small%2C%20occluded%20objects%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%20pose%20significant%20challenges%20for%20traffic%20surveillance%0Asystems%20because%20of%20their%20erratic%20movement%2C%20frequent%20occlusion%2C%20and%20poor%0Avisibility%20in%20dynamic%20urban%20environments.%20Traditional%20methods%20like%20YOLO11%2C%0Awhile%20proficient%20in%20spatial%20feature%20extraction%20for%20precise%20detection%2C%20often%0Astruggle%20with%20these%20small%20and%20dynamically%20moving%20objects%2C%20particularly%20in%0Ahandling%20real-time%20data%20updates%20and%20resource%20efficiency.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20that%20integrates%20dynamic%20graph%20neural%20networks%0A%28DGNNs%29%20with%20YOLO11%20to%20address%20these%20limitations.%20Unlike%20standard%20GNNs%2C%20DGNNs%0Aare%20chosen%20for%20their%20superior%20ability%20to%20dynamically%20update%20graph%20structures%20in%0Areal-time%2C%20which%20enables%20adaptive%20and%20robust%20tracking%20of%20objects%20in%20highly%0Avariable%20urban%20traffic%20scenarios.%20This%20framework%20constructs%20and%20regularly%0Aupdates%20its%20graph%20representations%2C%20capturing%20objects%20as%20nodes%20and%20their%0Ainteractions%20as%20edges%2C%20thus%20effectively%20responding%20to%20rapidly%20changing%0Aconditions.%20Additionally%2C%20DGNN-YOLO%20incorporates%20Grad-CAM%2C%20Grad-CAM%2B%2B%2C%20and%0AEigen-CAM%20visualization%20techniques%20to%20enhance%20interpretability%20and%20foster%0Atrust%2C%20offering%20insights%20into%20the%20model%27s%20decision-making%20process.%20Extensive%0Aexperiments%20validate%20the%20framework%27s%20performance%2C%20achieving%20a%20precision%20of%0A0.8382%2C%20recall%20of%200.6875%2C%20and%20mAP%400.5%3A0.95%20of%200.6476%2C%20significantly%0Aoutperforming%20existing%20methods.%20This%20study%20offers%20a%20scalable%20and%20interpretable%0Asolution%20for%20real-time%20traffic%20surveillance%20and%20significantly%20advances%0Aintelligent%20transportation%20systems%27%20capabilities%20by%20addressing%20the%20critical%0Achallenge%20of%20detecting%20and%20tracking%20small%2C%20occluded%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17251v7&entry.124074799=Read"},
{"title": "SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail\n  UHR Satellite Image Segmentation", "author": "Yulong Guo and Zilun Zhang and Yongheng Shang and Tiancheng Zhao and Shuiguang Deng and Yingchun Yang and Jianwei Yin", "abstract": "  The long-tail problem presents a significant challenge to the advancement of\nsemantic segmentation in ultra-high-resolution (UHR) satellite imagery. While\nprevious efforts in UHR semantic segmentation have largely focused on\nmulti-branch network architectures that emphasize multi-scale feature\nextraction and fusion, they have often overlooked the importance of addressing\nthe long-tail issue. In contrast to prior UHR methods that focused on\nindependent feature extraction, we emphasize data augmentation and multimodal\nfeature fusion to alleviate the long-tail problem. In this paper, we introduce\nSRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our\napproach addresses the long-tail class distribution by incorporating a\nmulti-scale cropping technique alongside a data augmentation strategy based on\nsemantic reordering and resampling. To further enhance model performance, we\npropose a multimodal fusion-based general representation knowledge injection\nmethod, which, for the first time, fuses text and visual features without the\nneed for individual region text descriptions, extracting more robust features.\nExtensive experiments on the URUR, GID, and FBP datasets demonstrate that our\nmethod improves mIoU by 3.33\\%, 0.66\\%, and 0.98\\%, respectively, achieving\nstate-of-the-art performance. Code is available at:\nhttps://github.com/BinSpa/SRMF.git.\n", "link": "http://arxiv.org/abs/2504.19839v1", "date": "2025-04-28", "relevancy": 2.8424, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5823}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5674}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRMF%3A%20A%20Data%20Augmentation%20and%20Multimodal%20Fusion%20Approach%20for%20Long-Tail%0A%20%20UHR%20Satellite%20Image%20Segmentation&body=Title%3A%20SRMF%3A%20A%20Data%20Augmentation%20and%20Multimodal%20Fusion%20Approach%20for%20Long-Tail%0A%20%20UHR%20Satellite%20Image%20Segmentation%0AAuthor%3A%20Yulong%20Guo%20and%20Zilun%20Zhang%20and%20Yongheng%20Shang%20and%20Tiancheng%20Zhao%20and%20Shuiguang%20Deng%20and%20Yingchun%20Yang%20and%20Jianwei%20Yin%0AAbstract%3A%20%20%20The%20long-tail%20problem%20presents%20a%20significant%20challenge%20to%20the%20advancement%20of%0Asemantic%20segmentation%20in%20ultra-high-resolution%20%28UHR%29%20satellite%20imagery.%20While%0Aprevious%20efforts%20in%20UHR%20semantic%20segmentation%20have%20largely%20focused%20on%0Amulti-branch%20network%20architectures%20that%20emphasize%20multi-scale%20feature%0Aextraction%20and%20fusion%2C%20they%20have%20often%20overlooked%20the%20importance%20of%20addressing%0Athe%20long-tail%20issue.%20In%20contrast%20to%20prior%20UHR%20methods%20that%20focused%20on%0Aindependent%20feature%20extraction%2C%20we%20emphasize%20data%20augmentation%20and%20multimodal%0Afeature%20fusion%20to%20alleviate%20the%20long-tail%20problem.%20In%20this%20paper%2C%20we%20introduce%0ASRMF%2C%20a%20novel%20framework%20for%20semantic%20segmentation%20in%20UHR%20satellite%20imagery.%20Our%0Aapproach%20addresses%20the%20long-tail%20class%20distribution%20by%20incorporating%20a%0Amulti-scale%20cropping%20technique%20alongside%20a%20data%20augmentation%20strategy%20based%20on%0Asemantic%20reordering%20and%20resampling.%20To%20further%20enhance%20model%20performance%2C%20we%0Apropose%20a%20multimodal%20fusion-based%20general%20representation%20knowledge%20injection%0Amethod%2C%20which%2C%20for%20the%20first%20time%2C%20fuses%20text%20and%20visual%20features%20without%20the%0Aneed%20for%20individual%20region%20text%20descriptions%2C%20extracting%20more%20robust%20features.%0AExtensive%20experiments%20on%20the%20URUR%2C%20GID%2C%20and%20FBP%20datasets%20demonstrate%20that%20our%0Amethod%20improves%20mIoU%20by%203.33%5C%25%2C%200.66%5C%25%2C%20and%200.98%5C%25%2C%20respectively%2C%20achieving%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/BinSpa/SRMF.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRMF%253A%2520A%2520Data%2520Augmentation%2520and%2520Multimodal%2520Fusion%2520Approach%2520for%2520Long-Tail%250A%2520%2520UHR%2520Satellite%2520Image%2520Segmentation%26entry.906535625%3DYulong%2520Guo%2520and%2520Zilun%2520Zhang%2520and%2520Yongheng%2520Shang%2520and%2520Tiancheng%2520Zhao%2520and%2520Shuiguang%2520Deng%2520and%2520Yingchun%2520Yang%2520and%2520Jianwei%2520Yin%26entry.1292438233%3D%2520%2520The%2520long-tail%2520problem%2520presents%2520a%2520significant%2520challenge%2520to%2520the%2520advancement%2520of%250Asemantic%2520segmentation%2520in%2520ultra-high-resolution%2520%2528UHR%2529%2520satellite%2520imagery.%2520While%250Aprevious%2520efforts%2520in%2520UHR%2520semantic%2520segmentation%2520have%2520largely%2520focused%2520on%250Amulti-branch%2520network%2520architectures%2520that%2520emphasize%2520multi-scale%2520feature%250Aextraction%2520and%2520fusion%252C%2520they%2520have%2520often%2520overlooked%2520the%2520importance%2520of%2520addressing%250Athe%2520long-tail%2520issue.%2520In%2520contrast%2520to%2520prior%2520UHR%2520methods%2520that%2520focused%2520on%250Aindependent%2520feature%2520extraction%252C%2520we%2520emphasize%2520data%2520augmentation%2520and%2520multimodal%250Afeature%2520fusion%2520to%2520alleviate%2520the%2520long-tail%2520problem.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ASRMF%252C%2520a%2520novel%2520framework%2520for%2520semantic%2520segmentation%2520in%2520UHR%2520satellite%2520imagery.%2520Our%250Aapproach%2520addresses%2520the%2520long-tail%2520class%2520distribution%2520by%2520incorporating%2520a%250Amulti-scale%2520cropping%2520technique%2520alongside%2520a%2520data%2520augmentation%2520strategy%2520based%2520on%250Asemantic%2520reordering%2520and%2520resampling.%2520To%2520further%2520enhance%2520model%2520performance%252C%2520we%250Apropose%2520a%2520multimodal%2520fusion-based%2520general%2520representation%2520knowledge%2520injection%250Amethod%252C%2520which%252C%2520for%2520the%2520first%2520time%252C%2520fuses%2520text%2520and%2520visual%2520features%2520without%2520the%250Aneed%2520for%2520individual%2520region%2520text%2520descriptions%252C%2520extracting%2520more%2520robust%2520features.%250AExtensive%2520experiments%2520on%2520the%2520URUR%252C%2520GID%252C%2520and%2520FBP%2520datasets%2520demonstrate%2520that%2520our%250Amethod%2520improves%2520mIoU%2520by%25203.33%255C%2525%252C%25200.66%255C%2525%252C%2520and%25200.98%255C%2525%252C%2520respectively%252C%2520achieving%250Astate-of-the-art%2520performance.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/BinSpa/SRMF.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRMF%3A%20A%20Data%20Augmentation%20and%20Multimodal%20Fusion%20Approach%20for%20Long-Tail%0A%20%20UHR%20Satellite%20Image%20Segmentation&entry.906535625=Yulong%20Guo%20and%20Zilun%20Zhang%20and%20Yongheng%20Shang%20and%20Tiancheng%20Zhao%20and%20Shuiguang%20Deng%20and%20Yingchun%20Yang%20and%20Jianwei%20Yin&entry.1292438233=%20%20The%20long-tail%20problem%20presents%20a%20significant%20challenge%20to%20the%20advancement%20of%0Asemantic%20segmentation%20in%20ultra-high-resolution%20%28UHR%29%20satellite%20imagery.%20While%0Aprevious%20efforts%20in%20UHR%20semantic%20segmentation%20have%20largely%20focused%20on%0Amulti-branch%20network%20architectures%20that%20emphasize%20multi-scale%20feature%0Aextraction%20and%20fusion%2C%20they%20have%20often%20overlooked%20the%20importance%20of%20addressing%0Athe%20long-tail%20issue.%20In%20contrast%20to%20prior%20UHR%20methods%20that%20focused%20on%0Aindependent%20feature%20extraction%2C%20we%20emphasize%20data%20augmentation%20and%20multimodal%0Afeature%20fusion%20to%20alleviate%20the%20long-tail%20problem.%20In%20this%20paper%2C%20we%20introduce%0ASRMF%2C%20a%20novel%20framework%20for%20semantic%20segmentation%20in%20UHR%20satellite%20imagery.%20Our%0Aapproach%20addresses%20the%20long-tail%20class%20distribution%20by%20incorporating%20a%0Amulti-scale%20cropping%20technique%20alongside%20a%20data%20augmentation%20strategy%20based%20on%0Asemantic%20reordering%20and%20resampling.%20To%20further%20enhance%20model%20performance%2C%20we%0Apropose%20a%20multimodal%20fusion-based%20general%20representation%20knowledge%20injection%0Amethod%2C%20which%2C%20for%20the%20first%20time%2C%20fuses%20text%20and%20visual%20features%20without%20the%0Aneed%20for%20individual%20region%20text%20descriptions%2C%20extracting%20more%20robust%20features.%0AExtensive%20experiments%20on%20the%20URUR%2C%20GID%2C%20and%20FBP%20datasets%20demonstrate%20that%20our%0Amethod%20improves%20mIoU%20by%203.33%5C%25%2C%200.66%5C%25%2C%20and%200.98%5C%25%2C%20respectively%2C%20achieving%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/BinSpa/SRMF.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19839v1&entry.124074799=Read"},
{"title": "Enhancing Quality for VVC Compressed Videos with Omniscient Quality\n  Enhancement Model", "author": "Xiem HoangVan and Hieu Bui Minh and Sang NguyenQuang and Wen-Hsiao Peng", "abstract": "  The latest video coding standard H.266/VVC has shown its great improvement in\nterms of compression performance when compared to its predecessor HEVC\nstandard. Though VVC was implemented with many advanced techniques, it still\nmet the same challenges as its predecessor due to the need for even higher\nperceptual quality demand at the decoder side as well as the compression\nperformance at the encoder side. The advancement of Artificial Intelligence\n(AI) technology, notably the deep learning-based video quality enhancement\nmethods, was shown to be a promising approach to improving the perceptual\nquality experience. In this paper, we propose a novel Omniscient video quality\nenhancement Network for VVC compressed Videos. The Omniscient Network for\ncompressed video quality enhancement was originally designed for HEVC\ncompressed videos in which not only the spatial-temporal features but also\ncross-frequencies information were employed to augment the visual quality.\nInspired by this work, we propose a modification of the OVQE model and\nintegrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder\narchitecture. As assessed in a rich set of test conditions, the proposed\nOVQE-VVC solution is able to achieve significant PSNR improvement, notably\naround 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec.\nThis also corresponds to around 19.6% of bitrate saving while keeping a similar\nquality observation.\n", "link": "http://arxiv.org/abs/2504.19935v1", "date": "2025-04-28", "relevancy": 2.7907, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5851}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Quality%20for%20VVC%20Compressed%20Videos%20with%20Omniscient%20Quality%0A%20%20Enhancement%20Model&body=Title%3A%20Enhancing%20Quality%20for%20VVC%20Compressed%20Videos%20with%20Omniscient%20Quality%0A%20%20Enhancement%20Model%0AAuthor%3A%20Xiem%20HoangVan%20and%20Hieu%20Bui%20Minh%20and%20Sang%20NguyenQuang%20and%20Wen-Hsiao%20Peng%0AAbstract%3A%20%20%20The%20latest%20video%20coding%20standard%20H.266/VVC%20has%20shown%20its%20great%20improvement%20in%0Aterms%20of%20compression%20performance%20when%20compared%20to%20its%20predecessor%20HEVC%0Astandard.%20Though%20VVC%20was%20implemented%20with%20many%20advanced%20techniques%2C%20it%20still%0Amet%20the%20same%20challenges%20as%20its%20predecessor%20due%20to%20the%20need%20for%20even%20higher%0Aperceptual%20quality%20demand%20at%20the%20decoder%20side%20as%20well%20as%20the%20compression%0Aperformance%20at%20the%20encoder%20side.%20The%20advancement%20of%20Artificial%20Intelligence%0A%28AI%29%20technology%2C%20notably%20the%20deep%20learning-based%20video%20quality%20enhancement%0Amethods%2C%20was%20shown%20to%20be%20a%20promising%20approach%20to%20improving%20the%20perceptual%0Aquality%20experience.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Omniscient%20video%20quality%0Aenhancement%20Network%20for%20VVC%20compressed%20Videos.%20The%20Omniscient%20Network%20for%0Acompressed%20video%20quality%20enhancement%20was%20originally%20designed%20for%20HEVC%0Acompressed%20videos%20in%20which%20not%20only%20the%20spatial-temporal%20features%20but%20also%0Across-frequencies%20information%20were%20employed%20to%20augment%20the%20visual%20quality.%0AInspired%20by%20this%20work%2C%20we%20propose%20a%20modification%20of%20the%20OVQE%20model%20and%0Aintegrate%20it%20into%20the%20lasted%20STD-VVC%20%28Standard%20Versatile%20Video%20Coding%29%20decoder%0Aarchitecture.%20As%20assessed%20in%20a%20rich%20set%20of%20test%20conditions%2C%20the%20proposed%0AOVQE-VVC%20solution%20is%20able%20to%20achieve%20significant%20PSNR%20improvement%2C%20notably%0Aaround%200.74%20dB%20and%20up%20to%201.2%20dB%20with%20respect%20to%20the%20original%20STD-VVC%20codec.%0AThis%20also%20corresponds%20to%20around%2019.6%25%20of%20bitrate%20saving%20while%20keeping%20a%20similar%0Aquality%20observation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Quality%2520for%2520VVC%2520Compressed%2520Videos%2520with%2520Omniscient%2520Quality%250A%2520%2520Enhancement%2520Model%26entry.906535625%3DXiem%2520HoangVan%2520and%2520Hieu%2520Bui%2520Minh%2520and%2520Sang%2520NguyenQuang%2520and%2520Wen-Hsiao%2520Peng%26entry.1292438233%3D%2520%2520The%2520latest%2520video%2520coding%2520standard%2520H.266/VVC%2520has%2520shown%2520its%2520great%2520improvement%2520in%250Aterms%2520of%2520compression%2520performance%2520when%2520compared%2520to%2520its%2520predecessor%2520HEVC%250Astandard.%2520Though%2520VVC%2520was%2520implemented%2520with%2520many%2520advanced%2520techniques%252C%2520it%2520still%250Amet%2520the%2520same%2520challenges%2520as%2520its%2520predecessor%2520due%2520to%2520the%2520need%2520for%2520even%2520higher%250Aperceptual%2520quality%2520demand%2520at%2520the%2520decoder%2520side%2520as%2520well%2520as%2520the%2520compression%250Aperformance%2520at%2520the%2520encoder%2520side.%2520The%2520advancement%2520of%2520Artificial%2520Intelligence%250A%2528AI%2529%2520technology%252C%2520notably%2520the%2520deep%2520learning-based%2520video%2520quality%2520enhancement%250Amethods%252C%2520was%2520shown%2520to%2520be%2520a%2520promising%2520approach%2520to%2520improving%2520the%2520perceptual%250Aquality%2520experience.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Omniscient%2520video%2520quality%250Aenhancement%2520Network%2520for%2520VVC%2520compressed%2520Videos.%2520The%2520Omniscient%2520Network%2520for%250Acompressed%2520video%2520quality%2520enhancement%2520was%2520originally%2520designed%2520for%2520HEVC%250Acompressed%2520videos%2520in%2520which%2520not%2520only%2520the%2520spatial-temporal%2520features%2520but%2520also%250Across-frequencies%2520information%2520were%2520employed%2520to%2520augment%2520the%2520visual%2520quality.%250AInspired%2520by%2520this%2520work%252C%2520we%2520propose%2520a%2520modification%2520of%2520the%2520OVQE%2520model%2520and%250Aintegrate%2520it%2520into%2520the%2520lasted%2520STD-VVC%2520%2528Standard%2520Versatile%2520Video%2520Coding%2529%2520decoder%250Aarchitecture.%2520As%2520assessed%2520in%2520a%2520rich%2520set%2520of%2520test%2520conditions%252C%2520the%2520proposed%250AOVQE-VVC%2520solution%2520is%2520able%2520to%2520achieve%2520significant%2520PSNR%2520improvement%252C%2520notably%250Aaround%25200.74%2520dB%2520and%2520up%2520to%25201.2%2520dB%2520with%2520respect%2520to%2520the%2520original%2520STD-VVC%2520codec.%250AThis%2520also%2520corresponds%2520to%2520around%252019.6%2525%2520of%2520bitrate%2520saving%2520while%2520keeping%2520a%2520similar%250Aquality%2520observation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Quality%20for%20VVC%20Compressed%20Videos%20with%20Omniscient%20Quality%0A%20%20Enhancement%20Model&entry.906535625=Xiem%20HoangVan%20and%20Hieu%20Bui%20Minh%20and%20Sang%20NguyenQuang%20and%20Wen-Hsiao%20Peng&entry.1292438233=%20%20The%20latest%20video%20coding%20standard%20H.266/VVC%20has%20shown%20its%20great%20improvement%20in%0Aterms%20of%20compression%20performance%20when%20compared%20to%20its%20predecessor%20HEVC%0Astandard.%20Though%20VVC%20was%20implemented%20with%20many%20advanced%20techniques%2C%20it%20still%0Amet%20the%20same%20challenges%20as%20its%20predecessor%20due%20to%20the%20need%20for%20even%20higher%0Aperceptual%20quality%20demand%20at%20the%20decoder%20side%20as%20well%20as%20the%20compression%0Aperformance%20at%20the%20encoder%20side.%20The%20advancement%20of%20Artificial%20Intelligence%0A%28AI%29%20technology%2C%20notably%20the%20deep%20learning-based%20video%20quality%20enhancement%0Amethods%2C%20was%20shown%20to%20be%20a%20promising%20approach%20to%20improving%20the%20perceptual%0Aquality%20experience.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Omniscient%20video%20quality%0Aenhancement%20Network%20for%20VVC%20compressed%20Videos.%20The%20Omniscient%20Network%20for%0Acompressed%20video%20quality%20enhancement%20was%20originally%20designed%20for%20HEVC%0Acompressed%20videos%20in%20which%20not%20only%20the%20spatial-temporal%20features%20but%20also%0Across-frequencies%20information%20were%20employed%20to%20augment%20the%20visual%20quality.%0AInspired%20by%20this%20work%2C%20we%20propose%20a%20modification%20of%20the%20OVQE%20model%20and%0Aintegrate%20it%20into%20the%20lasted%20STD-VVC%20%28Standard%20Versatile%20Video%20Coding%29%20decoder%0Aarchitecture.%20As%20assessed%20in%20a%20rich%20set%20of%20test%20conditions%2C%20the%20proposed%0AOVQE-VVC%20solution%20is%20able%20to%20achieve%20significant%20PSNR%20improvement%2C%20notably%0Aaround%200.74%20dB%20and%20up%20to%201.2%20dB%20with%20respect%20to%20the%20original%20STD-VVC%20codec.%0AThis%20also%20corresponds%20to%20around%2019.6%25%20of%20bitrate%20saving%20while%20keeping%20a%20similar%0Aquality%20observation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19935v1&entry.124074799=Read"},
{"title": "Shopformer: Transformer-Based Framework for Detecting Shoplifting via\n  Human Pose", "author": "Narges Rashvand and Ghazal Alinezhad Noghre and Armin Danesh Pazho and Babak Rahimi Ardabili and Hamed Tabkhi", "abstract": "  Shoplifting remains a costly issue for the retail sector, but traditional\nsurveillance systems, which are mostly based on human monitoring, are still\nlargely ineffective, with only about 2% of shoplifters being arrested. Existing\nAI-based approaches rely on pixel-level video analysis which raises privacy\nconcerns, is sensitive to environmental variations, and demands significant\ncomputational resources. To address these limitations, we introduce Shopformer,\na novel transformer-based model that detects shoplifting by analyzing pose\nsequences rather than raw video. We propose a custom tokenization strategy that\nconverts pose sequences into compact embeddings for efficient transformer\nprocessing. To the best of our knowledge, this is the first pose-sequence-based\ntransformer model for shoplifting detection. Evaluated on real-world pose data,\nour method outperforms state-of-the-art anomaly detection models, offering a\nprivacy-preserving, and scalable solution for real-time retail surveillance.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Shopformer.\n", "link": "http://arxiv.org/abs/2504.19970v1", "date": "2025-04-28", "relevancy": 2.7555, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5847}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5355}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shopformer%3A%20Transformer-Based%20Framework%20for%20Detecting%20Shoplifting%20via%0A%20%20Human%20Pose&body=Title%3A%20Shopformer%3A%20Transformer-Based%20Framework%20for%20Detecting%20Shoplifting%20via%0A%20%20Human%20Pose%0AAuthor%3A%20Narges%20Rashvand%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Babak%20Rahimi%20Ardabili%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20Shoplifting%20remains%20a%20costly%20issue%20for%20the%20retail%20sector%2C%20but%20traditional%0Asurveillance%20systems%2C%20which%20are%20mostly%20based%20on%20human%20monitoring%2C%20are%20still%0Alargely%20ineffective%2C%20with%20only%20about%202%25%20of%20shoplifters%20being%20arrested.%20Existing%0AAI-based%20approaches%20rely%20on%20pixel-level%20video%20analysis%20which%20raises%20privacy%0Aconcerns%2C%20is%20sensitive%20to%20environmental%20variations%2C%20and%20demands%20significant%0Acomputational%20resources.%20To%20address%20these%20limitations%2C%20we%20introduce%20Shopformer%2C%0Aa%20novel%20transformer-based%20model%20that%20detects%20shoplifting%20by%20analyzing%20pose%0Asequences%20rather%20than%20raw%20video.%20We%20propose%20a%20custom%20tokenization%20strategy%20that%0Aconverts%20pose%20sequences%20into%20compact%20embeddings%20for%20efficient%20transformer%0Aprocessing.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20pose-sequence-based%0Atransformer%20model%20for%20shoplifting%20detection.%20Evaluated%20on%20real-world%20pose%20data%2C%0Aour%20method%20outperforms%20state-of-the-art%20anomaly%20detection%20models%2C%20offering%20a%0Aprivacy-preserving%2C%20and%20scalable%20solution%20for%20real-time%20retail%20surveillance.%0AThe%20code%20base%20for%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/TeCSAR-UNCC/Shopformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShopformer%253A%2520Transformer-Based%2520Framework%2520for%2520Detecting%2520Shoplifting%2520via%250A%2520%2520Human%2520Pose%26entry.906535625%3DNarges%2520Rashvand%2520and%2520Ghazal%2520Alinezhad%2520Noghre%2520and%2520Armin%2520Danesh%2520Pazho%2520and%2520Babak%2520Rahimi%2520Ardabili%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520Shoplifting%2520remains%2520a%2520costly%2520issue%2520for%2520the%2520retail%2520sector%252C%2520but%2520traditional%250Asurveillance%2520systems%252C%2520which%2520are%2520mostly%2520based%2520on%2520human%2520monitoring%252C%2520are%2520still%250Alargely%2520ineffective%252C%2520with%2520only%2520about%25202%2525%2520of%2520shoplifters%2520being%2520arrested.%2520Existing%250AAI-based%2520approaches%2520rely%2520on%2520pixel-level%2520video%2520analysis%2520which%2520raises%2520privacy%250Aconcerns%252C%2520is%2520sensitive%2520to%2520environmental%2520variations%252C%2520and%2520demands%2520significant%250Acomputational%2520resources.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Shopformer%252C%250Aa%2520novel%2520transformer-based%2520model%2520that%2520detects%2520shoplifting%2520by%2520analyzing%2520pose%250Asequences%2520rather%2520than%2520raw%2520video.%2520We%2520propose%2520a%2520custom%2520tokenization%2520strategy%2520that%250Aconverts%2520pose%2520sequences%2520into%2520compact%2520embeddings%2520for%2520efficient%2520transformer%250Aprocessing.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520pose-sequence-based%250Atransformer%2520model%2520for%2520shoplifting%2520detection.%2520Evaluated%2520on%2520real-world%2520pose%2520data%252C%250Aour%2520method%2520outperforms%2520state-of-the-art%2520anomaly%2520detection%2520models%252C%2520offering%2520a%250Aprivacy-preserving%252C%2520and%2520scalable%2520solution%2520for%2520real-time%2520retail%2520surveillance.%250AThe%2520code%2520base%2520for%2520this%2520work%2520is%2520available%2520at%250Ahttps%253A//github.com/TeCSAR-UNCC/Shopformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shopformer%3A%20Transformer-Based%20Framework%20for%20Detecting%20Shoplifting%20via%0A%20%20Human%20Pose&entry.906535625=Narges%20Rashvand%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Babak%20Rahimi%20Ardabili%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20Shoplifting%20remains%20a%20costly%20issue%20for%20the%20retail%20sector%2C%20but%20traditional%0Asurveillance%20systems%2C%20which%20are%20mostly%20based%20on%20human%20monitoring%2C%20are%20still%0Alargely%20ineffective%2C%20with%20only%20about%202%25%20of%20shoplifters%20being%20arrested.%20Existing%0AAI-based%20approaches%20rely%20on%20pixel-level%20video%20analysis%20which%20raises%20privacy%0Aconcerns%2C%20is%20sensitive%20to%20environmental%20variations%2C%20and%20demands%20significant%0Acomputational%20resources.%20To%20address%20these%20limitations%2C%20we%20introduce%20Shopformer%2C%0Aa%20novel%20transformer-based%20model%20that%20detects%20shoplifting%20by%20analyzing%20pose%0Asequences%20rather%20than%20raw%20video.%20We%20propose%20a%20custom%20tokenization%20strategy%20that%0Aconverts%20pose%20sequences%20into%20compact%20embeddings%20for%20efficient%20transformer%0Aprocessing.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20pose-sequence-based%0Atransformer%20model%20for%20shoplifting%20detection.%20Evaluated%20on%20real-world%20pose%20data%2C%0Aour%20method%20outperforms%20state-of-the-art%20anomaly%20detection%20models%2C%20offering%20a%0Aprivacy-preserving%2C%20and%20scalable%20solution%20for%20real-time%20retail%20surveillance.%0AThe%20code%20base%20for%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/TeCSAR-UNCC/Shopformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19970v1&entry.124074799=Read"},
{"title": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models", "author": "Xin Wang and Haoyang Li and Zeyang Zhang and Haibo Chen and Wenwu Zhu", "abstract": "  Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications.\n", "link": "http://arxiv.org/abs/2504.20020v1", "date": "2025-04-28", "relevancy": 2.7497, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.55}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.55}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Machine%20Learning%3A%20An%20Indispensable%20Path%20towards%20New-Generation%0A%20%20Large%20Language%20Models&body=Title%3A%20Modular%20Machine%20Learning%3A%20An%20Indispensable%20Path%20towards%20New-Generation%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Xin%20Wang%20and%20Haoyang%20Li%20and%20Zeyang%20Zhang%20and%20Haibo%20Chen%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20dramatically%20advanced%20machine%20learning%0Aresearch%20including%20natural%20language%20processing%2C%20computer%20vision%2C%20data%20mining%2C%0Aetc.%2C%20yet%20they%20still%20exhibit%20critical%20limitations%20in%20reasoning%2C%20factual%0Aconsistency%2C%20and%20interpretability.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20learning%0Aparadigm%20--%20Modular%20Machine%20Learning%20%28MML%29%20--%20as%20an%20essential%20approach%20toward%0Anew-generation%20LLMs.%20MML%20decomposes%20the%20complex%20structure%20of%20LLMs%20into%20three%0Ainterdependent%20components%3A%20modular%20representation%2C%20modular%20model%2C%20and%20modular%0Areasoning%2C%20aiming%20to%20enhance%20LLMs%27%20capability%20of%20counterfactual%20reasoning%2C%0Amitigating%20hallucinations%2C%20as%20well%20as%20promoting%20fairness%2C%20safety%2C%20and%0Atransparency.%20Specifically%2C%20the%20proposed%20MML%20paradigm%20can%3A%20i%29%20clarify%20the%0Ainternal%20working%20mechanism%20of%20LLMs%20through%20the%20disentanglement%20of%20semantic%0Acomponents%3B%20ii%29%20allow%20for%20flexible%20and%20task-adaptive%20model%20design%3B%20iii%29%20enable%0Ainterpretable%20and%20logic-driven%20decision-making%20process.%20We%20present%20a%20feasible%0Aimplementation%20of%20MML-based%20LLMs%20via%20leveraging%20advanced%20techniques%20such%20as%0Adisentangled%20representation%20learning%2C%20neural%20architecture%20search%20and%0Aneuro-symbolic%20learning.%20We%20critically%20identify%20key%20challenges%2C%20such%20as%20the%0Aintegration%20of%20continuous%20neural%20and%20discrete%20symbolic%20processes%2C%20joint%0Aoptimization%2C%20and%20computational%20scalability%2C%20present%20promising%20future%20research%0Adirections%20that%20deserve%20further%20exploration.%20Ultimately%2C%20the%20integration%20of%20the%0AMML%20paradigm%20with%20LLMs%20has%20the%20potential%20to%20bridge%20the%20gap%20between%20statistical%0A%28deep%29%20learning%20and%20formal%20%28logical%29%20reasoning%2C%20thereby%20paving%20the%20way%20for%0Arobust%2C%20adaptable%2C%20and%20trustworthy%20AI%20systems%20across%20a%20wide%20range%20of%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Machine%2520Learning%253A%2520An%2520Indispensable%2520Path%2520towards%2520New-Generation%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DXin%2520Wang%2520and%2520Haoyang%2520Li%2520and%2520Zeyang%2520Zhang%2520and%2520Haibo%2520Chen%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520dramatically%2520advanced%2520machine%2520learning%250Aresearch%2520including%2520natural%2520language%2520processing%252C%2520computer%2520vision%252C%2520data%2520mining%252C%250Aetc.%252C%2520yet%2520they%2520still%2520exhibit%2520critical%2520limitations%2520in%2520reasoning%252C%2520factual%250Aconsistency%252C%2520and%2520interpretability.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520learning%250Aparadigm%2520--%2520Modular%2520Machine%2520Learning%2520%2528MML%2529%2520--%2520as%2520an%2520essential%2520approach%2520toward%250Anew-generation%2520LLMs.%2520MML%2520decomposes%2520the%2520complex%2520structure%2520of%2520LLMs%2520into%2520three%250Ainterdependent%2520components%253A%2520modular%2520representation%252C%2520modular%2520model%252C%2520and%2520modular%250Areasoning%252C%2520aiming%2520to%2520enhance%2520LLMs%2527%2520capability%2520of%2520counterfactual%2520reasoning%252C%250Amitigating%2520hallucinations%252C%2520as%2520well%2520as%2520promoting%2520fairness%252C%2520safety%252C%2520and%250Atransparency.%2520Specifically%252C%2520the%2520proposed%2520MML%2520paradigm%2520can%253A%2520i%2529%2520clarify%2520the%250Ainternal%2520working%2520mechanism%2520of%2520LLMs%2520through%2520the%2520disentanglement%2520of%2520semantic%250Acomponents%253B%2520ii%2529%2520allow%2520for%2520flexible%2520and%2520task-adaptive%2520model%2520design%253B%2520iii%2529%2520enable%250Ainterpretable%2520and%2520logic-driven%2520decision-making%2520process.%2520We%2520present%2520a%2520feasible%250Aimplementation%2520of%2520MML-based%2520LLMs%2520via%2520leveraging%2520advanced%2520techniques%2520such%2520as%250Adisentangled%2520representation%2520learning%252C%2520neural%2520architecture%2520search%2520and%250Aneuro-symbolic%2520learning.%2520We%2520critically%2520identify%2520key%2520challenges%252C%2520such%2520as%2520the%250Aintegration%2520of%2520continuous%2520neural%2520and%2520discrete%2520symbolic%2520processes%252C%2520joint%250Aoptimization%252C%2520and%2520computational%2520scalability%252C%2520present%2520promising%2520future%2520research%250Adirections%2520that%2520deserve%2520further%2520exploration.%2520Ultimately%252C%2520the%2520integration%2520of%2520the%250AMML%2520paradigm%2520with%2520LLMs%2520has%2520the%2520potential%2520to%2520bridge%2520the%2520gap%2520between%2520statistical%250A%2528deep%2529%2520learning%2520and%2520formal%2520%2528logical%2529%2520reasoning%252C%2520thereby%2520paving%2520the%2520way%2520for%250Arobust%252C%2520adaptable%252C%2520and%2520trustworthy%2520AI%2520systems%2520across%2520a%2520wide%2520range%2520of%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Machine%20Learning%3A%20An%20Indispensable%20Path%20towards%20New-Generation%0A%20%20Large%20Language%20Models&entry.906535625=Xin%20Wang%20and%20Haoyang%20Li%20and%20Zeyang%20Zhang%20and%20Haibo%20Chen%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20dramatically%20advanced%20machine%20learning%0Aresearch%20including%20natural%20language%20processing%2C%20computer%20vision%2C%20data%20mining%2C%0Aetc.%2C%20yet%20they%20still%20exhibit%20critical%20limitations%20in%20reasoning%2C%20factual%0Aconsistency%2C%20and%20interpretability.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20learning%0Aparadigm%20--%20Modular%20Machine%20Learning%20%28MML%29%20--%20as%20an%20essential%20approach%20toward%0Anew-generation%20LLMs.%20MML%20decomposes%20the%20complex%20structure%20of%20LLMs%20into%20three%0Ainterdependent%20components%3A%20modular%20representation%2C%20modular%20model%2C%20and%20modular%0Areasoning%2C%20aiming%20to%20enhance%20LLMs%27%20capability%20of%20counterfactual%20reasoning%2C%0Amitigating%20hallucinations%2C%20as%20well%20as%20promoting%20fairness%2C%20safety%2C%20and%0Atransparency.%20Specifically%2C%20the%20proposed%20MML%20paradigm%20can%3A%20i%29%20clarify%20the%0Ainternal%20working%20mechanism%20of%20LLMs%20through%20the%20disentanglement%20of%20semantic%0Acomponents%3B%20ii%29%20allow%20for%20flexible%20and%20task-adaptive%20model%20design%3B%20iii%29%20enable%0Ainterpretable%20and%20logic-driven%20decision-making%20process.%20We%20present%20a%20feasible%0Aimplementation%20of%20MML-based%20LLMs%20via%20leveraging%20advanced%20techniques%20such%20as%0Adisentangled%20representation%20learning%2C%20neural%20architecture%20search%20and%0Aneuro-symbolic%20learning.%20We%20critically%20identify%20key%20challenges%2C%20such%20as%20the%0Aintegration%20of%20continuous%20neural%20and%20discrete%20symbolic%20processes%2C%20joint%0Aoptimization%2C%20and%20computational%20scalability%2C%20present%20promising%20future%20research%0Adirections%20that%20deserve%20further%20exploration.%20Ultimately%2C%20the%20integration%20of%20the%0AMML%20paradigm%20with%20LLMs%20has%20the%20potential%20to%20bridge%20the%20gap%20between%20statistical%0A%28deep%29%20learning%20and%20formal%20%28logical%29%20reasoning%2C%20thereby%20paving%20the%20way%20for%0Arobust%2C%20adaptable%2C%20and%20trustworthy%20AI%20systems%20across%20a%20wide%20range%20of%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20020v1&entry.124074799=Read"},
{"title": "HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended\n  Reality Exploiting Eye-Hand-Head Coordination", "author": "Zhiming Hu and Daniel Haeufle and Syn Schmitt and Andreas Bulling", "abstract": "  We present HOIGaze - a novel learning-based approach for gaze estimation\nduring hand-object interactions (HOI) in extended reality (XR). HOIGaze\naddresses the challenging HOI setting by building on one key insight: The eye,\nhand, and head movements are closely coordinated during HOIs and this\ncoordination can be exploited to identify samples that are most useful for gaze\nestimator training - as such, effectively denoising the training data. This\ndenoising approach is in stark contrast to previous gaze estimation methods\nthat treated all training samples as equal. Specifically, we propose: 1) a\nnovel hierarchical framework that first recognises the hand currently visually\nattended to and then estimates gaze direction based on the attended hand; 2) a\nnew gaze estimator that uses cross-modal Transformers to fuse head and\nhand-object features extracted using a convolutional neural network and a\nspatio-temporal graph convolutional network; and 3) a novel eye-head\ncoordination loss that upgrades training samples belonging to the coordinated\neye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin\n(ADT) datasets and show that it significantly outperforms state-of-the-art\nmethods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in\nmean angular error. To demonstrate the potential of our method, we further\nreport significant performance improvements for the sample downstream task of\neye-based activity recognition on ADT. Taken together, our results underline\nthe significant information content available in eye-hand-head coordination\nand, as such, open up an exciting new direction for learning-based gaze\nestimation.\n", "link": "http://arxiv.org/abs/2504.19828v1", "date": "2025-04-28", "relevancy": 2.7353, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5522}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5459}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOIGaze%3A%20Gaze%20Estimation%20During%20Hand-Object%20Interactions%20in%20Extended%0A%20%20Reality%20Exploiting%20Eye-Hand-Head%20Coordination&body=Title%3A%20HOIGaze%3A%20Gaze%20Estimation%20During%20Hand-Object%20Interactions%20in%20Extended%0A%20%20Reality%20Exploiting%20Eye-Hand-Head%20Coordination%0AAuthor%3A%20Zhiming%20Hu%20and%20Daniel%20Haeufle%20and%20Syn%20Schmitt%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20We%20present%20HOIGaze%20-%20a%20novel%20learning-based%20approach%20for%20gaze%20estimation%0Aduring%20hand-object%20interactions%20%28HOI%29%20in%20extended%20reality%20%28XR%29.%20HOIGaze%0Aaddresses%20the%20challenging%20HOI%20setting%20by%20building%20on%20one%20key%20insight%3A%20The%20eye%2C%0Ahand%2C%20and%20head%20movements%20are%20closely%20coordinated%20during%20HOIs%20and%20this%0Acoordination%20can%20be%20exploited%20to%20identify%20samples%20that%20are%20most%20useful%20for%20gaze%0Aestimator%20training%20-%20as%20such%2C%20effectively%20denoising%20the%20training%20data.%20This%0Adenoising%20approach%20is%20in%20stark%20contrast%20to%20previous%20gaze%20estimation%20methods%0Athat%20treated%20all%20training%20samples%20as%20equal.%20Specifically%2C%20we%20propose%3A%201%29%20a%0Anovel%20hierarchical%20framework%20that%20first%20recognises%20the%20hand%20currently%20visually%0Aattended%20to%20and%20then%20estimates%20gaze%20direction%20based%20on%20the%20attended%20hand%3B%202%29%20a%0Anew%20gaze%20estimator%20that%20uses%20cross-modal%20Transformers%20to%20fuse%20head%20and%0Ahand-object%20features%20extracted%20using%20a%20convolutional%20neural%20network%20and%20a%0Aspatio-temporal%20graph%20convolutional%20network%3B%20and%203%29%20a%20novel%20eye-head%0Acoordination%20loss%20that%20upgrades%20training%20samples%20belonging%20to%20the%20coordinated%0Aeye-head%20movements.%20We%20evaluate%20HOIGaze%20on%20the%20HOT3D%20and%20Aria%20digital%20twin%0A%28ADT%29%20datasets%20and%20show%20that%20it%20significantly%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20an%20average%20improvement%20of%2015.6%25%20on%20HOT3D%20and%206.0%25%20on%20ADT%20in%0Amean%20angular%20error.%20To%20demonstrate%20the%20potential%20of%20our%20method%2C%20we%20further%0Areport%20significant%20performance%20improvements%20for%20the%20sample%20downstream%20task%20of%0Aeye-based%20activity%20recognition%20on%20ADT.%20Taken%20together%2C%20our%20results%20underline%0Athe%20significant%20information%20content%20available%20in%20eye-hand-head%20coordination%0Aand%2C%20as%20such%2C%20open%20up%20an%20exciting%20new%20direction%20for%20learning-based%20gaze%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOIGaze%253A%2520Gaze%2520Estimation%2520During%2520Hand-Object%2520Interactions%2520in%2520Extended%250A%2520%2520Reality%2520Exploiting%2520Eye-Hand-Head%2520Coordination%26entry.906535625%3DZhiming%2520Hu%2520and%2520Daniel%2520Haeufle%2520and%2520Syn%2520Schmitt%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520We%2520present%2520HOIGaze%2520-%2520a%2520novel%2520learning-based%2520approach%2520for%2520gaze%2520estimation%250Aduring%2520hand-object%2520interactions%2520%2528HOI%2529%2520in%2520extended%2520reality%2520%2528XR%2529.%2520HOIGaze%250Aaddresses%2520the%2520challenging%2520HOI%2520setting%2520by%2520building%2520on%2520one%2520key%2520insight%253A%2520The%2520eye%252C%250Ahand%252C%2520and%2520head%2520movements%2520are%2520closely%2520coordinated%2520during%2520HOIs%2520and%2520this%250Acoordination%2520can%2520be%2520exploited%2520to%2520identify%2520samples%2520that%2520are%2520most%2520useful%2520for%2520gaze%250Aestimator%2520training%2520-%2520as%2520such%252C%2520effectively%2520denoising%2520the%2520training%2520data.%2520This%250Adenoising%2520approach%2520is%2520in%2520stark%2520contrast%2520to%2520previous%2520gaze%2520estimation%2520methods%250Athat%2520treated%2520all%2520training%2520samples%2520as%2520equal.%2520Specifically%252C%2520we%2520propose%253A%25201%2529%2520a%250Anovel%2520hierarchical%2520framework%2520that%2520first%2520recognises%2520the%2520hand%2520currently%2520visually%250Aattended%2520to%2520and%2520then%2520estimates%2520gaze%2520direction%2520based%2520on%2520the%2520attended%2520hand%253B%25202%2529%2520a%250Anew%2520gaze%2520estimator%2520that%2520uses%2520cross-modal%2520Transformers%2520to%2520fuse%2520head%2520and%250Ahand-object%2520features%2520extracted%2520using%2520a%2520convolutional%2520neural%2520network%2520and%2520a%250Aspatio-temporal%2520graph%2520convolutional%2520network%253B%2520and%25203%2529%2520a%2520novel%2520eye-head%250Acoordination%2520loss%2520that%2520upgrades%2520training%2520samples%2520belonging%2520to%2520the%2520coordinated%250Aeye-head%2520movements.%2520We%2520evaluate%2520HOIGaze%2520on%2520the%2520HOT3D%2520and%2520Aria%2520digital%2520twin%250A%2528ADT%2529%2520datasets%2520and%2520show%2520that%2520it%2520significantly%2520outperforms%2520state-of-the-art%250Amethods%252C%2520achieving%2520an%2520average%2520improvement%2520of%252015.6%2525%2520on%2520HOT3D%2520and%25206.0%2525%2520on%2520ADT%2520in%250Amean%2520angular%2520error.%2520To%2520demonstrate%2520the%2520potential%2520of%2520our%2520method%252C%2520we%2520further%250Areport%2520significant%2520performance%2520improvements%2520for%2520the%2520sample%2520downstream%2520task%2520of%250Aeye-based%2520activity%2520recognition%2520on%2520ADT.%2520Taken%2520together%252C%2520our%2520results%2520underline%250Athe%2520significant%2520information%2520content%2520available%2520in%2520eye-hand-head%2520coordination%250Aand%252C%2520as%2520such%252C%2520open%2520up%2520an%2520exciting%2520new%2520direction%2520for%2520learning-based%2520gaze%250Aestimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOIGaze%3A%20Gaze%20Estimation%20During%20Hand-Object%20Interactions%20in%20Extended%0A%20%20Reality%20Exploiting%20Eye-Hand-Head%20Coordination&entry.906535625=Zhiming%20Hu%20and%20Daniel%20Haeufle%20and%20Syn%20Schmitt%20and%20Andreas%20Bulling&entry.1292438233=%20%20We%20present%20HOIGaze%20-%20a%20novel%20learning-based%20approach%20for%20gaze%20estimation%0Aduring%20hand-object%20interactions%20%28HOI%29%20in%20extended%20reality%20%28XR%29.%20HOIGaze%0Aaddresses%20the%20challenging%20HOI%20setting%20by%20building%20on%20one%20key%20insight%3A%20The%20eye%2C%0Ahand%2C%20and%20head%20movements%20are%20closely%20coordinated%20during%20HOIs%20and%20this%0Acoordination%20can%20be%20exploited%20to%20identify%20samples%20that%20are%20most%20useful%20for%20gaze%0Aestimator%20training%20-%20as%20such%2C%20effectively%20denoising%20the%20training%20data.%20This%0Adenoising%20approach%20is%20in%20stark%20contrast%20to%20previous%20gaze%20estimation%20methods%0Athat%20treated%20all%20training%20samples%20as%20equal.%20Specifically%2C%20we%20propose%3A%201%29%20a%0Anovel%20hierarchical%20framework%20that%20first%20recognises%20the%20hand%20currently%20visually%0Aattended%20to%20and%20then%20estimates%20gaze%20direction%20based%20on%20the%20attended%20hand%3B%202%29%20a%0Anew%20gaze%20estimator%20that%20uses%20cross-modal%20Transformers%20to%20fuse%20head%20and%0Ahand-object%20features%20extracted%20using%20a%20convolutional%20neural%20network%20and%20a%0Aspatio-temporal%20graph%20convolutional%20network%3B%20and%203%29%20a%20novel%20eye-head%0Acoordination%20loss%20that%20upgrades%20training%20samples%20belonging%20to%20the%20coordinated%0Aeye-head%20movements.%20We%20evaluate%20HOIGaze%20on%20the%20HOT3D%20and%20Aria%20digital%20twin%0A%28ADT%29%20datasets%20and%20show%20that%20it%20significantly%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20an%20average%20improvement%20of%2015.6%25%20on%20HOT3D%20and%206.0%25%20on%20ADT%20in%0Amean%20angular%20error.%20To%20demonstrate%20the%20potential%20of%20our%20method%2C%20we%20further%0Areport%20significant%20performance%20improvements%20for%20the%20sample%20downstream%20task%20of%0Aeye-based%20activity%20recognition%20on%20ADT.%20Taken%20together%2C%20our%20results%20underline%0Athe%20significant%20information%20content%20available%20in%20eye-hand-head%20coordination%0Aand%2C%20as%20such%2C%20open%20up%20an%20exciting%20new%20direction%20for%20learning-based%20gaze%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19828v1&entry.124074799=Read"},
{"title": "Transfer Learning Under High-Dimensional Network Convolutional\n  Regression Model", "author": "Liyuan Wang and Jiachen Chen and Kathryn L. Lunetta and Danyang Huang and Huimin Cheng and Debarghya Mukherjee", "abstract": "  Transfer learning enhances model performance by utilizing knowledge from\nrelated domains, particularly when labeled data is scarce. While existing\nresearch addresses transfer learning under various distribution shifts in\nindependent settings, handling dependencies in networked data remains\nchallenging. To address this challenge, we propose a high-dimensional transfer\nlearning framework based on network convolutional regression (NCR), inspired by\nthe success of graph convolutional networks (GCNs). The NCR model incorporates\nrandom network structure by allowing each node's response to depend on its\nfeatures and the aggregated features of its neighbors, capturing local\ndependencies effectively. Our methodology includes a two-step transfer learning\nalgorithm that addresses domain shift between source and target networks, along\nwith a source detection mechanism to identify informative domains.\nTheoretically, we analyze the lasso estimator in the context of a random graph\nbased on the Erdos-Renyi model assumption, demonstrating that transfer learning\nimproves convergence rates when informative sources are present. Empirical\nevaluations, including simulations and a real-world application using Sina\nWeibo data, demonstrate substantial improvements in prediction accuracy,\nparticularly when labeled data in the target domain is limited.\n", "link": "http://arxiv.org/abs/2504.19979v1", "date": "2025-04-28", "relevancy": 2.7277, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5634}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5392}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20Under%20High-Dimensional%20Network%20Convolutional%0A%20%20Regression%20Model&body=Title%3A%20Transfer%20Learning%20Under%20High-Dimensional%20Network%20Convolutional%0A%20%20Regression%20Model%0AAuthor%3A%20Liyuan%20Wang%20and%20Jiachen%20Chen%20and%20Kathryn%20L.%20Lunetta%20and%20Danyang%20Huang%20and%20Huimin%20Cheng%20and%20Debarghya%20Mukherjee%0AAbstract%3A%20%20%20Transfer%20learning%20enhances%20model%20performance%20by%20utilizing%20knowledge%20from%0Arelated%20domains%2C%20particularly%20when%20labeled%20data%20is%20scarce.%20While%20existing%0Aresearch%20addresses%20transfer%20learning%20under%20various%20distribution%20shifts%20in%0Aindependent%20settings%2C%20handling%20dependencies%20in%20networked%20data%20remains%0Achallenging.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20high-dimensional%20transfer%0Alearning%20framework%20based%20on%20network%20convolutional%20regression%20%28NCR%29%2C%20inspired%20by%0Athe%20success%20of%20graph%20convolutional%20networks%20%28GCNs%29.%20The%20NCR%20model%20incorporates%0Arandom%20network%20structure%20by%20allowing%20each%20node%27s%20response%20to%20depend%20on%20its%0Afeatures%20and%20the%20aggregated%20features%20of%20its%20neighbors%2C%20capturing%20local%0Adependencies%20effectively.%20Our%20methodology%20includes%20a%20two-step%20transfer%20learning%0Aalgorithm%20that%20addresses%20domain%20shift%20between%20source%20and%20target%20networks%2C%20along%0Awith%20a%20source%20detection%20mechanism%20to%20identify%20informative%20domains.%0ATheoretically%2C%20we%20analyze%20the%20lasso%20estimator%20in%20the%20context%20of%20a%20random%20graph%0Abased%20on%20the%20Erdos-Renyi%20model%20assumption%2C%20demonstrating%20that%20transfer%20learning%0Aimproves%20convergence%20rates%20when%20informative%20sources%20are%20present.%20Empirical%0Aevaluations%2C%20including%20simulations%20and%20a%20real-world%20application%20using%20Sina%0AWeibo%20data%2C%20demonstrate%20substantial%20improvements%20in%20prediction%20accuracy%2C%0Aparticularly%20when%20labeled%20data%20in%20the%20target%20domain%20is%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520Under%2520High-Dimensional%2520Network%2520Convolutional%250A%2520%2520Regression%2520Model%26entry.906535625%3DLiyuan%2520Wang%2520and%2520Jiachen%2520Chen%2520and%2520Kathryn%2520L.%2520Lunetta%2520and%2520Danyang%2520Huang%2520and%2520Huimin%2520Cheng%2520and%2520Debarghya%2520Mukherjee%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520enhances%2520model%2520performance%2520by%2520utilizing%2520knowledge%2520from%250Arelated%2520domains%252C%2520particularly%2520when%2520labeled%2520data%2520is%2520scarce.%2520While%2520existing%250Aresearch%2520addresses%2520transfer%2520learning%2520under%2520various%2520distribution%2520shifts%2520in%250Aindependent%2520settings%252C%2520handling%2520dependencies%2520in%2520networked%2520data%2520remains%250Achallenging.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520high-dimensional%2520transfer%250Alearning%2520framework%2520based%2520on%2520network%2520convolutional%2520regression%2520%2528NCR%2529%252C%2520inspired%2520by%250Athe%2520success%2520of%2520graph%2520convolutional%2520networks%2520%2528GCNs%2529.%2520The%2520NCR%2520model%2520incorporates%250Arandom%2520network%2520structure%2520by%2520allowing%2520each%2520node%2527s%2520response%2520to%2520depend%2520on%2520its%250Afeatures%2520and%2520the%2520aggregated%2520features%2520of%2520its%2520neighbors%252C%2520capturing%2520local%250Adependencies%2520effectively.%2520Our%2520methodology%2520includes%2520a%2520two-step%2520transfer%2520learning%250Aalgorithm%2520that%2520addresses%2520domain%2520shift%2520between%2520source%2520and%2520target%2520networks%252C%2520along%250Awith%2520a%2520source%2520detection%2520mechanism%2520to%2520identify%2520informative%2520domains.%250ATheoretically%252C%2520we%2520analyze%2520the%2520lasso%2520estimator%2520in%2520the%2520context%2520of%2520a%2520random%2520graph%250Abased%2520on%2520the%2520Erdos-Renyi%2520model%2520assumption%252C%2520demonstrating%2520that%2520transfer%2520learning%250Aimproves%2520convergence%2520rates%2520when%2520informative%2520sources%2520are%2520present.%2520Empirical%250Aevaluations%252C%2520including%2520simulations%2520and%2520a%2520real-world%2520application%2520using%2520Sina%250AWeibo%2520data%252C%2520demonstrate%2520substantial%2520improvements%2520in%2520prediction%2520accuracy%252C%250Aparticularly%2520when%2520labeled%2520data%2520in%2520the%2520target%2520domain%2520is%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20Under%20High-Dimensional%20Network%20Convolutional%0A%20%20Regression%20Model&entry.906535625=Liyuan%20Wang%20and%20Jiachen%20Chen%20and%20Kathryn%20L.%20Lunetta%20and%20Danyang%20Huang%20and%20Huimin%20Cheng%20and%20Debarghya%20Mukherjee&entry.1292438233=%20%20Transfer%20learning%20enhances%20model%20performance%20by%20utilizing%20knowledge%20from%0Arelated%20domains%2C%20particularly%20when%20labeled%20data%20is%20scarce.%20While%20existing%0Aresearch%20addresses%20transfer%20learning%20under%20various%20distribution%20shifts%20in%0Aindependent%20settings%2C%20handling%20dependencies%20in%20networked%20data%20remains%0Achallenging.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20high-dimensional%20transfer%0Alearning%20framework%20based%20on%20network%20convolutional%20regression%20%28NCR%29%2C%20inspired%20by%0Athe%20success%20of%20graph%20convolutional%20networks%20%28GCNs%29.%20The%20NCR%20model%20incorporates%0Arandom%20network%20structure%20by%20allowing%20each%20node%27s%20response%20to%20depend%20on%20its%0Afeatures%20and%20the%20aggregated%20features%20of%20its%20neighbors%2C%20capturing%20local%0Adependencies%20effectively.%20Our%20methodology%20includes%20a%20two-step%20transfer%20learning%0Aalgorithm%20that%20addresses%20domain%20shift%20between%20source%20and%20target%20networks%2C%20along%0Awith%20a%20source%20detection%20mechanism%20to%20identify%20informative%20domains.%0ATheoretically%2C%20we%20analyze%20the%20lasso%20estimator%20in%20the%20context%20of%20a%20random%20graph%0Abased%20on%20the%20Erdos-Renyi%20model%20assumption%2C%20demonstrating%20that%20transfer%20learning%0Aimproves%20convergence%20rates%20when%20informative%20sources%20are%20present.%20Empirical%0Aevaluations%2C%20including%20simulations%20and%20a%20real-world%20application%20using%20Sina%0AWeibo%20data%2C%20demonstrate%20substantial%20improvements%20in%20prediction%20accuracy%2C%0Aparticularly%20when%20labeled%20data%20in%20the%20target%20domain%20is%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19979v1&entry.124074799=Read"},
{"title": "Quaternion Domain Super MDS for 3D Localization", "author": "Keigo Masuoka and Takumi Takahashi and Giuseppe Thadeu Freitas de Abreu and Hideki Ochiai", "abstract": "  We propose a novel low-complexity three-dimensional (3D) localization\nalgorithm for wireless sensor networks, termed quaternion-domain super\nmultidimensional scaling (QD-SMDS). This algorithm reformulates the\nconventional SMDS, which was originally developed in the real domain, into the\nquaternion domain. By representing 3D coordinates as quaternions, the method\nenables the construction of a rank-1 Gram edge kernel (GEK) matrix that\nintegrates both relative distance and angular (phase) information between\nnodes, maximizing the noise reduction effect achieved through low-rank\ntruncation via singular value decomposition (SVD). The simulation results\nindicate that the proposed method demonstrates a notable enhancement in\nlocalization accuracy relative to the conventional SMDS algorithm, particularly\nin scenarios characterized by substantial measurement errors.\n", "link": "http://arxiv.org/abs/2504.17890v2", "date": "2025-04-28", "relevancy": 2.7106, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5902}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5199}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quaternion%20Domain%20Super%20MDS%20for%203D%20Localization&body=Title%3A%20Quaternion%20Domain%20Super%20MDS%20for%203D%20Localization%0AAuthor%3A%20Keigo%20Masuoka%20and%20Takumi%20Takahashi%20and%20Giuseppe%20Thadeu%20Freitas%20de%20Abreu%20and%20Hideki%20Ochiai%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20low-complexity%20three-dimensional%20%283D%29%20localization%0Aalgorithm%20for%20wireless%20sensor%20networks%2C%20termed%20quaternion-domain%20super%0Amultidimensional%20scaling%20%28QD-SMDS%29.%20This%20algorithm%20reformulates%20the%0Aconventional%20SMDS%2C%20which%20was%20originally%20developed%20in%20the%20real%20domain%2C%20into%20the%0Aquaternion%20domain.%20By%20representing%203D%20coordinates%20as%20quaternions%2C%20the%20method%0Aenables%20the%20construction%20of%20a%20rank-1%20Gram%20edge%20kernel%20%28GEK%29%20matrix%20that%0Aintegrates%20both%20relative%20distance%20and%20angular%20%28phase%29%20information%20between%0Anodes%2C%20maximizing%20the%20noise%20reduction%20effect%20achieved%20through%20low-rank%0Atruncation%20via%20singular%20value%20decomposition%20%28SVD%29.%20The%20simulation%20results%0Aindicate%20that%20the%20proposed%20method%20demonstrates%20a%20notable%20enhancement%20in%0Alocalization%20accuracy%20relative%20to%20the%20conventional%20SMDS%20algorithm%2C%20particularly%0Ain%20scenarios%20characterized%20by%20substantial%20measurement%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuaternion%2520Domain%2520Super%2520MDS%2520for%25203D%2520Localization%26entry.906535625%3DKeigo%2520Masuoka%2520and%2520Takumi%2520Takahashi%2520and%2520Giuseppe%2520Thadeu%2520Freitas%2520de%2520Abreu%2520and%2520Hideki%2520Ochiai%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520low-complexity%2520three-dimensional%2520%25283D%2529%2520localization%250Aalgorithm%2520for%2520wireless%2520sensor%2520networks%252C%2520termed%2520quaternion-domain%2520super%250Amultidimensional%2520scaling%2520%2528QD-SMDS%2529.%2520This%2520algorithm%2520reformulates%2520the%250Aconventional%2520SMDS%252C%2520which%2520was%2520originally%2520developed%2520in%2520the%2520real%2520domain%252C%2520into%2520the%250Aquaternion%2520domain.%2520By%2520representing%25203D%2520coordinates%2520as%2520quaternions%252C%2520the%2520method%250Aenables%2520the%2520construction%2520of%2520a%2520rank-1%2520Gram%2520edge%2520kernel%2520%2528GEK%2529%2520matrix%2520that%250Aintegrates%2520both%2520relative%2520distance%2520and%2520angular%2520%2528phase%2529%2520information%2520between%250Anodes%252C%2520maximizing%2520the%2520noise%2520reduction%2520effect%2520achieved%2520through%2520low-rank%250Atruncation%2520via%2520singular%2520value%2520decomposition%2520%2528SVD%2529.%2520The%2520simulation%2520results%250Aindicate%2520that%2520the%2520proposed%2520method%2520demonstrates%2520a%2520notable%2520enhancement%2520in%250Alocalization%2520accuracy%2520relative%2520to%2520the%2520conventional%2520SMDS%2520algorithm%252C%2520particularly%250Ain%2520scenarios%2520characterized%2520by%2520substantial%2520measurement%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quaternion%20Domain%20Super%20MDS%20for%203D%20Localization&entry.906535625=Keigo%20Masuoka%20and%20Takumi%20Takahashi%20and%20Giuseppe%20Thadeu%20Freitas%20de%20Abreu%20and%20Hideki%20Ochiai&entry.1292438233=%20%20We%20propose%20a%20novel%20low-complexity%20three-dimensional%20%283D%29%20localization%0Aalgorithm%20for%20wireless%20sensor%20networks%2C%20termed%20quaternion-domain%20super%0Amultidimensional%20scaling%20%28QD-SMDS%29.%20This%20algorithm%20reformulates%20the%0Aconventional%20SMDS%2C%20which%20was%20originally%20developed%20in%20the%20real%20domain%2C%20into%20the%0Aquaternion%20domain.%20By%20representing%203D%20coordinates%20as%20quaternions%2C%20the%20method%0Aenables%20the%20construction%20of%20a%20rank-1%20Gram%20edge%20kernel%20%28GEK%29%20matrix%20that%0Aintegrates%20both%20relative%20distance%20and%20angular%20%28phase%29%20information%20between%0Anodes%2C%20maximizing%20the%20noise%20reduction%20effect%20achieved%20through%20low-rank%0Atruncation%20via%20singular%20value%20decomposition%20%28SVD%29.%20The%20simulation%20results%0Aindicate%20that%20the%20proposed%20method%20demonstrates%20a%20notable%20enhancement%20in%0Alocalization%20accuracy%20relative%20to%20the%20conventional%20SMDS%20algorithm%2C%20particularly%0Ain%20scenarios%20characterized%20by%20substantial%20measurement%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17890v2&entry.124074799=Read"},
{"title": "Intelligent4DSE: Optimizing High-Level Synthesis Design Space\n  Exploration with Graph Neural Networks and Large Language Models", "author": "Lei Xu and Shanshan Wang and Emmanuel Casseau and Chenglong Xiao", "abstract": "  High-level synthesis (HLS) design space exploration (DSE) is an optimization\nprocess in electronic design automation (EDA) that systematically explores\nhigh-level design configurations to achieve Pareto-optimal hardware\nimplementations balancing performance, area, and power (PPA). To optimize this\nprocess, HLS prediction tasks often employ message-passing neural networks\n(MPNNs), leveraging complex architectures to achieve high accuracy. These\npredictors serve as evaluators in the DSE process, effectively bypassing the\ntime-consuming estimations traditionally required by HLS tools. However,\nexisting models often prioritize structural complexity and minimization of\ntraining loss, overlooking task-specific characteristics. Additionally, while\nevolutionary algorithms are widely used in DSE, they typically require\nextensive domain-specific knowledge to design effective crossover and mutation\noperators. To address these limitations, we propose CoGNNs-LLMEA, a framework\nthat integrates a graph neural network with task-adaptive message passing and a\nlarge language model-enhanced evolutionary algorithm. As a predictive model,\nCoGNNs directly leverages intermediate representations generated from source\ncode after compiler front-end processing, enabling prediction of quality of\nresults (QoR) without invoking HLS tools. Due to its strong adaptability to\ntasks, CoGNNs can be tuned to predict post-HLS and post-implementation\noutcomes, effectively bridging the gap between high-level abstractions and\nphysical implementation characteristics. CoGNNs achieves state-of-the-art\nprediction accuracy in post-HLS QoR prediction, reducing mean prediction errors\nby 2.8$\\times$ for latency and 3.4$\\times$ for resource utilization compared to\nbaseline models.\n", "link": "http://arxiv.org/abs/2504.19649v1", "date": "2025-04-28", "relevancy": 2.6897, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5398}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent4DSE%3A%20Optimizing%20High-Level%20Synthesis%20Design%20Space%0A%20%20Exploration%20with%20Graph%20Neural%20Networks%20and%20Large%20Language%20Models&body=Title%3A%20Intelligent4DSE%3A%20Optimizing%20High-Level%20Synthesis%20Design%20Space%0A%20%20Exploration%20with%20Graph%20Neural%20Networks%20and%20Large%20Language%20Models%0AAuthor%3A%20Lei%20Xu%20and%20Shanshan%20Wang%20and%20Emmanuel%20Casseau%20and%20Chenglong%20Xiao%0AAbstract%3A%20%20%20High-level%20synthesis%20%28HLS%29%20design%20space%20exploration%20%28DSE%29%20is%20an%20optimization%0Aprocess%20in%20electronic%20design%20automation%20%28EDA%29%20that%20systematically%20explores%0Ahigh-level%20design%20configurations%20to%20achieve%20Pareto-optimal%20hardware%0Aimplementations%20balancing%20performance%2C%20area%2C%20and%20power%20%28PPA%29.%20To%20optimize%20this%0Aprocess%2C%20HLS%20prediction%20tasks%20often%20employ%20message-passing%20neural%20networks%0A%28MPNNs%29%2C%20leveraging%20complex%20architectures%20to%20achieve%20high%20accuracy.%20These%0Apredictors%20serve%20as%20evaluators%20in%20the%20DSE%20process%2C%20effectively%20bypassing%20the%0Atime-consuming%20estimations%20traditionally%20required%20by%20HLS%20tools.%20However%2C%0Aexisting%20models%20often%20prioritize%20structural%20complexity%20and%20minimization%20of%0Atraining%20loss%2C%20overlooking%20task-specific%20characteristics.%20Additionally%2C%20while%0Aevolutionary%20algorithms%20are%20widely%20used%20in%20DSE%2C%20they%20typically%20require%0Aextensive%20domain-specific%20knowledge%20to%20design%20effective%20crossover%20and%20mutation%0Aoperators.%20To%20address%20these%20limitations%2C%20we%20propose%20CoGNNs-LLMEA%2C%20a%20framework%0Athat%20integrates%20a%20graph%20neural%20network%20with%20task-adaptive%20message%20passing%20and%20a%0Alarge%20language%20model-enhanced%20evolutionary%20algorithm.%20As%20a%20predictive%20model%2C%0ACoGNNs%20directly%20leverages%20intermediate%20representations%20generated%20from%20source%0Acode%20after%20compiler%20front-end%20processing%2C%20enabling%20prediction%20of%20quality%20of%0Aresults%20%28QoR%29%20without%20invoking%20HLS%20tools.%20Due%20to%20its%20strong%20adaptability%20to%0Atasks%2C%20CoGNNs%20can%20be%20tuned%20to%20predict%20post-HLS%20and%20post-implementation%0Aoutcomes%2C%20effectively%20bridging%20the%20gap%20between%20high-level%20abstractions%20and%0Aphysical%20implementation%20characteristics.%20CoGNNs%20achieves%20state-of-the-art%0Aprediction%20accuracy%20in%20post-HLS%20QoR%20prediction%2C%20reducing%20mean%20prediction%20errors%0Aby%202.8%24%5Ctimes%24%20for%20latency%20and%203.4%24%5Ctimes%24%20for%20resource%20utilization%20compared%20to%0Abaseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent4DSE%253A%2520Optimizing%2520High-Level%2520Synthesis%2520Design%2520Space%250A%2520%2520Exploration%2520with%2520Graph%2520Neural%2520Networks%2520and%2520Large%2520Language%2520Models%26entry.906535625%3DLei%2520Xu%2520and%2520Shanshan%2520Wang%2520and%2520Emmanuel%2520Casseau%2520and%2520Chenglong%2520Xiao%26entry.1292438233%3D%2520%2520High-level%2520synthesis%2520%2528HLS%2529%2520design%2520space%2520exploration%2520%2528DSE%2529%2520is%2520an%2520optimization%250Aprocess%2520in%2520electronic%2520design%2520automation%2520%2528EDA%2529%2520that%2520systematically%2520explores%250Ahigh-level%2520design%2520configurations%2520to%2520achieve%2520Pareto-optimal%2520hardware%250Aimplementations%2520balancing%2520performance%252C%2520area%252C%2520and%2520power%2520%2528PPA%2529.%2520To%2520optimize%2520this%250Aprocess%252C%2520HLS%2520prediction%2520tasks%2520often%2520employ%2520message-passing%2520neural%2520networks%250A%2528MPNNs%2529%252C%2520leveraging%2520complex%2520architectures%2520to%2520achieve%2520high%2520accuracy.%2520These%250Apredictors%2520serve%2520as%2520evaluators%2520in%2520the%2520DSE%2520process%252C%2520effectively%2520bypassing%2520the%250Atime-consuming%2520estimations%2520traditionally%2520required%2520by%2520HLS%2520tools.%2520However%252C%250Aexisting%2520models%2520often%2520prioritize%2520structural%2520complexity%2520and%2520minimization%2520of%250Atraining%2520loss%252C%2520overlooking%2520task-specific%2520characteristics.%2520Additionally%252C%2520while%250Aevolutionary%2520algorithms%2520are%2520widely%2520used%2520in%2520DSE%252C%2520they%2520typically%2520require%250Aextensive%2520domain-specific%2520knowledge%2520to%2520design%2520effective%2520crossover%2520and%2520mutation%250Aoperators.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520CoGNNs-LLMEA%252C%2520a%2520framework%250Athat%2520integrates%2520a%2520graph%2520neural%2520network%2520with%2520task-adaptive%2520message%2520passing%2520and%2520a%250Alarge%2520language%2520model-enhanced%2520evolutionary%2520algorithm.%2520As%2520a%2520predictive%2520model%252C%250ACoGNNs%2520directly%2520leverages%2520intermediate%2520representations%2520generated%2520from%2520source%250Acode%2520after%2520compiler%2520front-end%2520processing%252C%2520enabling%2520prediction%2520of%2520quality%2520of%250Aresults%2520%2528QoR%2529%2520without%2520invoking%2520HLS%2520tools.%2520Due%2520to%2520its%2520strong%2520adaptability%2520to%250Atasks%252C%2520CoGNNs%2520can%2520be%2520tuned%2520to%2520predict%2520post-HLS%2520and%2520post-implementation%250Aoutcomes%252C%2520effectively%2520bridging%2520the%2520gap%2520between%2520high-level%2520abstractions%2520and%250Aphysical%2520implementation%2520characteristics.%2520CoGNNs%2520achieves%2520state-of-the-art%250Aprediction%2520accuracy%2520in%2520post-HLS%2520QoR%2520prediction%252C%2520reducing%2520mean%2520prediction%2520errors%250Aby%25202.8%2524%255Ctimes%2524%2520for%2520latency%2520and%25203.4%2524%255Ctimes%2524%2520for%2520resource%2520utilization%2520compared%2520to%250Abaseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent4DSE%3A%20Optimizing%20High-Level%20Synthesis%20Design%20Space%0A%20%20Exploration%20with%20Graph%20Neural%20Networks%20and%20Large%20Language%20Models&entry.906535625=Lei%20Xu%20and%20Shanshan%20Wang%20and%20Emmanuel%20Casseau%20and%20Chenglong%20Xiao&entry.1292438233=%20%20High-level%20synthesis%20%28HLS%29%20design%20space%20exploration%20%28DSE%29%20is%20an%20optimization%0Aprocess%20in%20electronic%20design%20automation%20%28EDA%29%20that%20systematically%20explores%0Ahigh-level%20design%20configurations%20to%20achieve%20Pareto-optimal%20hardware%0Aimplementations%20balancing%20performance%2C%20area%2C%20and%20power%20%28PPA%29.%20To%20optimize%20this%0Aprocess%2C%20HLS%20prediction%20tasks%20often%20employ%20message-passing%20neural%20networks%0A%28MPNNs%29%2C%20leveraging%20complex%20architectures%20to%20achieve%20high%20accuracy.%20These%0Apredictors%20serve%20as%20evaluators%20in%20the%20DSE%20process%2C%20effectively%20bypassing%20the%0Atime-consuming%20estimations%20traditionally%20required%20by%20HLS%20tools.%20However%2C%0Aexisting%20models%20often%20prioritize%20structural%20complexity%20and%20minimization%20of%0Atraining%20loss%2C%20overlooking%20task-specific%20characteristics.%20Additionally%2C%20while%0Aevolutionary%20algorithms%20are%20widely%20used%20in%20DSE%2C%20they%20typically%20require%0Aextensive%20domain-specific%20knowledge%20to%20design%20effective%20crossover%20and%20mutation%0Aoperators.%20To%20address%20these%20limitations%2C%20we%20propose%20CoGNNs-LLMEA%2C%20a%20framework%0Athat%20integrates%20a%20graph%20neural%20network%20with%20task-adaptive%20message%20passing%20and%20a%0Alarge%20language%20model-enhanced%20evolutionary%20algorithm.%20As%20a%20predictive%20model%2C%0ACoGNNs%20directly%20leverages%20intermediate%20representations%20generated%20from%20source%0Acode%20after%20compiler%20front-end%20processing%2C%20enabling%20prediction%20of%20quality%20of%0Aresults%20%28QoR%29%20without%20invoking%20HLS%20tools.%20Due%20to%20its%20strong%20adaptability%20to%0Atasks%2C%20CoGNNs%20can%20be%20tuned%20to%20predict%20post-HLS%20and%20post-implementation%0Aoutcomes%2C%20effectively%20bridging%20the%20gap%20between%20high-level%20abstractions%20and%0Aphysical%20implementation%20characteristics.%20CoGNNs%20achieves%20state-of-the-art%0Aprediction%20accuracy%20in%20post-HLS%20QoR%20prediction%2C%20reducing%20mean%20prediction%20errors%0Aby%202.8%24%5Ctimes%24%20for%20latency%20and%203.4%24%5Ctimes%24%20for%20resource%20utilization%20compared%20to%0Abaseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19649v1&entry.124074799=Read"},
{"title": "Taming the Randomness: Towards Label-Preserving Cropping in Contrastive\n  Learning", "author": "Mohamed Hassan and Mohammad Wasil and Sebastian Houben", "abstract": "  Contrastive learning (CL) approaches have gained great recognition as a very\nsuccessful subset of self-supervised learning (SSL) methods. SSL enables\nlearning from unlabeled data, a crucial step in the advancement of deep\nlearning, particularly in computer vision (CV), given the plethora of unlabeled\nimage data. CL works by comparing different random augmentations (e.g.,\ndifferent crops) of the same image, thus achieving self-labeling. Nevertheless,\nrandomly augmenting images and especially random cropping can result in an\nimage that is semantically very distant from the original and therefore leads\nto false labeling, hence undermining the efficacy of the methods. In this\nresearch, two novel parameterized cropping methods are introduced that increase\nthe robustness of self-labeling and consequently increase the efficacy. The\nresults show that the use of these methods significantly improves the accuracy\nof the model by between 2.7\\% and 12.4\\% on the downstream task of classifying\nCIFAR-10, depending on the crop size compared to that of the non-parameterized\nrandom cropping method.\n", "link": "http://arxiv.org/abs/2504.19824v1", "date": "2025-04-28", "relevancy": 2.6674, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5584}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20the%20Randomness%3A%20Towards%20Label-Preserving%20Cropping%20in%20Contrastive%0A%20%20Learning&body=Title%3A%20Taming%20the%20Randomness%3A%20Towards%20Label-Preserving%20Cropping%20in%20Contrastive%0A%20%20Learning%0AAuthor%3A%20Mohamed%20Hassan%20and%20Mohammad%20Wasil%20and%20Sebastian%20Houben%0AAbstract%3A%20%20%20Contrastive%20learning%20%28CL%29%20approaches%20have%20gained%20great%20recognition%20as%20a%20very%0Asuccessful%20subset%20of%20self-supervised%20learning%20%28SSL%29%20methods.%20SSL%20enables%0Alearning%20from%20unlabeled%20data%2C%20a%20crucial%20step%20in%20the%20advancement%20of%20deep%0Alearning%2C%20particularly%20in%20computer%20vision%20%28CV%29%2C%20given%20the%20plethora%20of%20unlabeled%0Aimage%20data.%20CL%20works%20by%20comparing%20different%20random%20augmentations%20%28e.g.%2C%0Adifferent%20crops%29%20of%20the%20same%20image%2C%20thus%20achieving%20self-labeling.%20Nevertheless%2C%0Arandomly%20augmenting%20images%20and%20especially%20random%20cropping%20can%20result%20in%20an%0Aimage%20that%20is%20semantically%20very%20distant%20from%20the%20original%20and%20therefore%20leads%0Ato%20false%20labeling%2C%20hence%20undermining%20the%20efficacy%20of%20the%20methods.%20In%20this%0Aresearch%2C%20two%20novel%20parameterized%20cropping%20methods%20are%20introduced%20that%20increase%0Athe%20robustness%20of%20self-labeling%20and%20consequently%20increase%20the%20efficacy.%20The%0Aresults%20show%20that%20the%20use%20of%20these%20methods%20significantly%20improves%20the%20accuracy%0Aof%20the%20model%20by%20between%202.7%5C%25%20and%2012.4%5C%25%20on%20the%20downstream%20task%20of%20classifying%0ACIFAR-10%2C%20depending%20on%20the%20crop%20size%20compared%20to%20that%20of%20the%20non-parameterized%0Arandom%20cropping%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520the%2520Randomness%253A%2520Towards%2520Label-Preserving%2520Cropping%2520in%2520Contrastive%250A%2520%2520Learning%26entry.906535625%3DMohamed%2520Hassan%2520and%2520Mohammad%2520Wasil%2520and%2520Sebastian%2520Houben%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520%2528CL%2529%2520approaches%2520have%2520gained%2520great%2520recognition%2520as%2520a%2520very%250Asuccessful%2520subset%2520of%2520self-supervised%2520learning%2520%2528SSL%2529%2520methods.%2520SSL%2520enables%250Alearning%2520from%2520unlabeled%2520data%252C%2520a%2520crucial%2520step%2520in%2520the%2520advancement%2520of%2520deep%250Alearning%252C%2520particularly%2520in%2520computer%2520vision%2520%2528CV%2529%252C%2520given%2520the%2520plethora%2520of%2520unlabeled%250Aimage%2520data.%2520CL%2520works%2520by%2520comparing%2520different%2520random%2520augmentations%2520%2528e.g.%252C%250Adifferent%2520crops%2529%2520of%2520the%2520same%2520image%252C%2520thus%2520achieving%2520self-labeling.%2520Nevertheless%252C%250Arandomly%2520augmenting%2520images%2520and%2520especially%2520random%2520cropping%2520can%2520result%2520in%2520an%250Aimage%2520that%2520is%2520semantically%2520very%2520distant%2520from%2520the%2520original%2520and%2520therefore%2520leads%250Ato%2520false%2520labeling%252C%2520hence%2520undermining%2520the%2520efficacy%2520of%2520the%2520methods.%2520In%2520this%250Aresearch%252C%2520two%2520novel%2520parameterized%2520cropping%2520methods%2520are%2520introduced%2520that%2520increase%250Athe%2520robustness%2520of%2520self-labeling%2520and%2520consequently%2520increase%2520the%2520efficacy.%2520The%250Aresults%2520show%2520that%2520the%2520use%2520of%2520these%2520methods%2520significantly%2520improves%2520the%2520accuracy%250Aof%2520the%2520model%2520by%2520between%25202.7%255C%2525%2520and%252012.4%255C%2525%2520on%2520the%2520downstream%2520task%2520of%2520classifying%250ACIFAR-10%252C%2520depending%2520on%2520the%2520crop%2520size%2520compared%2520to%2520that%2520of%2520the%2520non-parameterized%250Arandom%2520cropping%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20the%20Randomness%3A%20Towards%20Label-Preserving%20Cropping%20in%20Contrastive%0A%20%20Learning&entry.906535625=Mohamed%20Hassan%20and%20Mohammad%20Wasil%20and%20Sebastian%20Houben&entry.1292438233=%20%20Contrastive%20learning%20%28CL%29%20approaches%20have%20gained%20great%20recognition%20as%20a%20very%0Asuccessful%20subset%20of%20self-supervised%20learning%20%28SSL%29%20methods.%20SSL%20enables%0Alearning%20from%20unlabeled%20data%2C%20a%20crucial%20step%20in%20the%20advancement%20of%20deep%0Alearning%2C%20particularly%20in%20computer%20vision%20%28CV%29%2C%20given%20the%20plethora%20of%20unlabeled%0Aimage%20data.%20CL%20works%20by%20comparing%20different%20random%20augmentations%20%28e.g.%2C%0Adifferent%20crops%29%20of%20the%20same%20image%2C%20thus%20achieving%20self-labeling.%20Nevertheless%2C%0Arandomly%20augmenting%20images%20and%20especially%20random%20cropping%20can%20result%20in%20an%0Aimage%20that%20is%20semantically%20very%20distant%20from%20the%20original%20and%20therefore%20leads%0Ato%20false%20labeling%2C%20hence%20undermining%20the%20efficacy%20of%20the%20methods.%20In%20this%0Aresearch%2C%20two%20novel%20parameterized%20cropping%20methods%20are%20introduced%20that%20increase%0Athe%20robustness%20of%20self-labeling%20and%20consequently%20increase%20the%20efficacy.%20The%0Aresults%20show%20that%20the%20use%20of%20these%20methods%20significantly%20improves%20the%20accuracy%0Aof%20the%20model%20by%20between%202.7%5C%25%20and%2012.4%5C%25%20on%20the%20downstream%20task%20of%20classifying%0ACIFAR-10%2C%20depending%20on%20the%20crop%20size%20compared%20to%20that%20of%20the%20non-parameterized%0Arandom%20cropping%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19824v1&entry.124074799=Read"},
{"title": "DD-rPPGNet: De-interfering and Descriptive Feature Learning for\n  Unsupervised rPPG Estimation", "author": "Pei-Kai Huang and Tzu-Hsien Chen and Ya-Ting Chan and Kuan-Wen Chen and Chiou-Ting Hsu", "abstract": "  Remote Photoplethysmography (rPPG) aims to measure physiological signals and\nHeart Rate (HR) from facial videos. Recent unsupervised rPPG estimation methods\nhave shown promising potential in estimating rPPG signals from facial regions\nwithout relying on ground truth rPPG signals. However, these methods seem\noblivious to interference existing in rPPG signals and still result in\nunsatisfactory performance. In this paper, we propose a novel De-interfered and\nDescriptive rPPG Estimation Network (DD-rPPGNet) to eliminate the interference\nwithin rPPG features for learning genuine rPPG signals. First, we investigate\nthe characteristics of local spatial-temporal similarities of interference and\ndesign a novel unsupervised model to estimate the interference. Next, we\npropose an unsupervised de-interfered method to learn genuine rPPG signals with\ntwo stages. In the first stage, we estimate the initial rPPG signals by\ncontrastive learning from both the training data and their augmented\ncounterparts. In the second stage, we use the estimated interference features\nto derive de-interfered rPPG features and encourage the rPPG signals to be\ndistinct from the interference. In addition, we propose an effective\ndescriptive rPPG feature learning by developing a strong 3D Learnable\nDescriptive Convolution (3DLDC) to capture the subtle chrominance changes for\nenhancing rPPG estimation. Extensive experiments conducted on five rPPG\nbenchmark datasets demonstrate that the proposed DD-rPPGNet outperforms\nprevious unsupervised rPPG estimation methods and achieves competitive\nperformances with state-of-the-art supervised rPPG methods. The code is\navailable at: https://github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet\n", "link": "http://arxiv.org/abs/2407.21402v3", "date": "2025-04-28", "relevancy": 2.6596, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5623}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5224}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DD-rPPGNet%3A%20De-interfering%20and%20Descriptive%20Feature%20Learning%20for%0A%20%20Unsupervised%20rPPG%20Estimation&body=Title%3A%20DD-rPPGNet%3A%20De-interfering%20and%20Descriptive%20Feature%20Learning%20for%0A%20%20Unsupervised%20rPPG%20Estimation%0AAuthor%3A%20Pei-Kai%20Huang%20and%20Tzu-Hsien%20Chen%20and%20Ya-Ting%20Chan%20and%20Kuan-Wen%20Chen%20and%20Chiou-Ting%20Hsu%0AAbstract%3A%20%20%20Remote%20Photoplethysmography%20%28rPPG%29%20aims%20to%20measure%20physiological%20signals%20and%0AHeart%20Rate%20%28HR%29%20from%20facial%20videos.%20Recent%20unsupervised%20rPPG%20estimation%20methods%0Ahave%20shown%20promising%20potential%20in%20estimating%20rPPG%20signals%20from%20facial%20regions%0Awithout%20relying%20on%20ground%20truth%20rPPG%20signals.%20However%2C%20these%20methods%20seem%0Aoblivious%20to%20interference%20existing%20in%20rPPG%20signals%20and%20still%20result%20in%0Aunsatisfactory%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20De-interfered%20and%0ADescriptive%20rPPG%20Estimation%20Network%20%28DD-rPPGNet%29%20to%20eliminate%20the%20interference%0Awithin%20rPPG%20features%20for%20learning%20genuine%20rPPG%20signals.%20First%2C%20we%20investigate%0Athe%20characteristics%20of%20local%20spatial-temporal%20similarities%20of%20interference%20and%0Adesign%20a%20novel%20unsupervised%20model%20to%20estimate%20the%20interference.%20Next%2C%20we%0Apropose%20an%20unsupervised%20de-interfered%20method%20to%20learn%20genuine%20rPPG%20signals%20with%0Atwo%20stages.%20In%20the%20first%20stage%2C%20we%20estimate%20the%20initial%20rPPG%20signals%20by%0Acontrastive%20learning%20from%20both%20the%20training%20data%20and%20their%20augmented%0Acounterparts.%20In%20the%20second%20stage%2C%20we%20use%20the%20estimated%20interference%20features%0Ato%20derive%20de-interfered%20rPPG%20features%20and%20encourage%20the%20rPPG%20signals%20to%20be%0Adistinct%20from%20the%20interference.%20In%20addition%2C%20we%20propose%20an%20effective%0Adescriptive%20rPPG%20feature%20learning%20by%20developing%20a%20strong%203D%20Learnable%0ADescriptive%20Convolution%20%283DLDC%29%20to%20capture%20the%20subtle%20chrominance%20changes%20for%0Aenhancing%20rPPG%20estimation.%20Extensive%20experiments%20conducted%20on%20five%20rPPG%0Abenchmark%20datasets%20demonstrate%20that%20the%20proposed%20DD-rPPGNet%20outperforms%0Aprevious%20unsupervised%20rPPG%20estimation%20methods%20and%20achieves%20competitive%0Aperformances%20with%20state-of-the-art%20supervised%20rPPG%20methods.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21402v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDD-rPPGNet%253A%2520De-interfering%2520and%2520Descriptive%2520Feature%2520Learning%2520for%250A%2520%2520Unsupervised%2520rPPG%2520Estimation%26entry.906535625%3DPei-Kai%2520Huang%2520and%2520Tzu-Hsien%2520Chen%2520and%2520Ya-Ting%2520Chan%2520and%2520Kuan-Wen%2520Chen%2520and%2520Chiou-Ting%2520Hsu%26entry.1292438233%3D%2520%2520Remote%2520Photoplethysmography%2520%2528rPPG%2529%2520aims%2520to%2520measure%2520physiological%2520signals%2520and%250AHeart%2520Rate%2520%2528HR%2529%2520from%2520facial%2520videos.%2520Recent%2520unsupervised%2520rPPG%2520estimation%2520methods%250Ahave%2520shown%2520promising%2520potential%2520in%2520estimating%2520rPPG%2520signals%2520from%2520facial%2520regions%250Awithout%2520relying%2520on%2520ground%2520truth%2520rPPG%2520signals.%2520However%252C%2520these%2520methods%2520seem%250Aoblivious%2520to%2520interference%2520existing%2520in%2520rPPG%2520signals%2520and%2520still%2520result%2520in%250Aunsatisfactory%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520De-interfered%2520and%250ADescriptive%2520rPPG%2520Estimation%2520Network%2520%2528DD-rPPGNet%2529%2520to%2520eliminate%2520the%2520interference%250Awithin%2520rPPG%2520features%2520for%2520learning%2520genuine%2520rPPG%2520signals.%2520First%252C%2520we%2520investigate%250Athe%2520characteristics%2520of%2520local%2520spatial-temporal%2520similarities%2520of%2520interference%2520and%250Adesign%2520a%2520novel%2520unsupervised%2520model%2520to%2520estimate%2520the%2520interference.%2520Next%252C%2520we%250Apropose%2520an%2520unsupervised%2520de-interfered%2520method%2520to%2520learn%2520genuine%2520rPPG%2520signals%2520with%250Atwo%2520stages.%2520In%2520the%2520first%2520stage%252C%2520we%2520estimate%2520the%2520initial%2520rPPG%2520signals%2520by%250Acontrastive%2520learning%2520from%2520both%2520the%2520training%2520data%2520and%2520their%2520augmented%250Acounterparts.%2520In%2520the%2520second%2520stage%252C%2520we%2520use%2520the%2520estimated%2520interference%2520features%250Ato%2520derive%2520de-interfered%2520rPPG%2520features%2520and%2520encourage%2520the%2520rPPG%2520signals%2520to%2520be%250Adistinct%2520from%2520the%2520interference.%2520In%2520addition%252C%2520we%2520propose%2520an%2520effective%250Adescriptive%2520rPPG%2520feature%2520learning%2520by%2520developing%2520a%2520strong%25203D%2520Learnable%250ADescriptive%2520Convolution%2520%25283DLDC%2529%2520to%2520capture%2520the%2520subtle%2520chrominance%2520changes%2520for%250Aenhancing%2520rPPG%2520estimation.%2520Extensive%2520experiments%2520conducted%2520on%2520five%2520rPPG%250Abenchmark%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520DD-rPPGNet%2520outperforms%250Aprevious%2520unsupervised%2520rPPG%2520estimation%2520methods%2520and%2520achieves%2520competitive%250Aperformances%2520with%2520state-of-the-art%2520supervised%2520rPPG%2520methods.%2520The%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21402v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DD-rPPGNet%3A%20De-interfering%20and%20Descriptive%20Feature%20Learning%20for%0A%20%20Unsupervised%20rPPG%20Estimation&entry.906535625=Pei-Kai%20Huang%20and%20Tzu-Hsien%20Chen%20and%20Ya-Ting%20Chan%20and%20Kuan-Wen%20Chen%20and%20Chiou-Ting%20Hsu&entry.1292438233=%20%20Remote%20Photoplethysmography%20%28rPPG%29%20aims%20to%20measure%20physiological%20signals%20and%0AHeart%20Rate%20%28HR%29%20from%20facial%20videos.%20Recent%20unsupervised%20rPPG%20estimation%20methods%0Ahave%20shown%20promising%20potential%20in%20estimating%20rPPG%20signals%20from%20facial%20regions%0Awithout%20relying%20on%20ground%20truth%20rPPG%20signals.%20However%2C%20these%20methods%20seem%0Aoblivious%20to%20interference%20existing%20in%20rPPG%20signals%20and%20still%20result%20in%0Aunsatisfactory%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20De-interfered%20and%0ADescriptive%20rPPG%20Estimation%20Network%20%28DD-rPPGNet%29%20to%20eliminate%20the%20interference%0Awithin%20rPPG%20features%20for%20learning%20genuine%20rPPG%20signals.%20First%2C%20we%20investigate%0Athe%20characteristics%20of%20local%20spatial-temporal%20similarities%20of%20interference%20and%0Adesign%20a%20novel%20unsupervised%20model%20to%20estimate%20the%20interference.%20Next%2C%20we%0Apropose%20an%20unsupervised%20de-interfered%20method%20to%20learn%20genuine%20rPPG%20signals%20with%0Atwo%20stages.%20In%20the%20first%20stage%2C%20we%20estimate%20the%20initial%20rPPG%20signals%20by%0Acontrastive%20learning%20from%20both%20the%20training%20data%20and%20their%20augmented%0Acounterparts.%20In%20the%20second%20stage%2C%20we%20use%20the%20estimated%20interference%20features%0Ato%20derive%20de-interfered%20rPPG%20features%20and%20encourage%20the%20rPPG%20signals%20to%20be%0Adistinct%20from%20the%20interference.%20In%20addition%2C%20we%20propose%20an%20effective%0Adescriptive%20rPPG%20feature%20learning%20by%20developing%20a%20strong%203D%20Learnable%0ADescriptive%20Convolution%20%283DLDC%29%20to%20capture%20the%20subtle%20chrominance%20changes%20for%0Aenhancing%20rPPG%20estimation.%20Extensive%20experiments%20conducted%20on%20five%20rPPG%0Abenchmark%20datasets%20demonstrate%20that%20the%20proposed%20DD-rPPGNet%20outperforms%0Aprevious%20unsupervised%20rPPG%20estimation%20methods%20and%20achieves%20competitive%0Aperformances%20with%20state-of-the-art%20supervised%20rPPG%20methods.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21402v3&entry.124074799=Read"},
{"title": "Hierarchical Uncertainty-Aware Graph Neural Network", "author": "Yoonhyuk Choi and Chong-Kwon Kim", "abstract": "  Recent research on graph neural networks (GNNs) has explored mechanisms for\ncapturing local uncertainty and exploiting graph hierarchies to mitigate data\nsparsity and leverage structural properties. However, the synergistic\nintegration of these two approaches remains underexplored. In this work, we\nintroduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural\nNetwork (HU-GNN), which unifies multi-scale representation learning, principled\nuncertainty estimation, and self-supervised embedding diversity within a single\nend-to-end framework. Specifically, HU-GNN adaptively forms node clusters and\nestimates uncertainty at multiple structural scales from individual nodes to\nhigher levels. These uncertainty estimates guide a robust message-passing\nmechanism and attention weighting, effectively mitigating noise and adversarial\nperturbations while preserving predictive accuracy on both node- and\ngraph-level tasks. We also offer key theoretical contributions, including a\nprobabilistic formulation, rigorous uncertainty-calibration guarantees, and\nformal robustness bounds. Finally, by incorporating recent advances in graph\ncontrastive learning, HU-GNN maintains diverse, structurally faithful\nembeddings. Extensive experiments on standard benchmarks demonstrate that our\nmodel achieves state-of-the-art robustness and interpretability.\n", "link": "http://arxiv.org/abs/2504.19820v1", "date": "2025-04-28", "relevancy": 2.6572, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5385}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5309}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Uncertainty-Aware%20Graph%20Neural%20Network&body=Title%3A%20Hierarchical%20Uncertainty-Aware%20Graph%20Neural%20Network%0AAuthor%3A%20Yoonhyuk%20Choi%20and%20Chong-Kwon%20Kim%0AAbstract%3A%20%20%20Recent%20research%20on%20graph%20neural%20networks%20%28GNNs%29%20has%20explored%20mechanisms%20for%0Acapturing%20local%20uncertainty%20and%20exploiting%20graph%20hierarchies%20to%20mitigate%20data%0Asparsity%20and%20leverage%20structural%20properties.%20However%2C%20the%20synergistic%0Aintegration%20of%20these%20two%20approaches%20remains%20underexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20novel%20architecture%2C%20the%20Hierarchical%20Uncertainty-Aware%20Graph%20Neural%0ANetwork%20%28HU-GNN%29%2C%20which%20unifies%20multi-scale%20representation%20learning%2C%20principled%0Auncertainty%20estimation%2C%20and%20self-supervised%20embedding%20diversity%20within%20a%20single%0Aend-to-end%20framework.%20Specifically%2C%20HU-GNN%20adaptively%20forms%20node%20clusters%20and%0Aestimates%20uncertainty%20at%20multiple%20structural%20scales%20from%20individual%20nodes%20to%0Ahigher%20levels.%20These%20uncertainty%20estimates%20guide%20a%20robust%20message-passing%0Amechanism%20and%20attention%20weighting%2C%20effectively%20mitigating%20noise%20and%20adversarial%0Aperturbations%20while%20preserving%20predictive%20accuracy%20on%20both%20node-%20and%0Agraph-level%20tasks.%20We%20also%20offer%20key%20theoretical%20contributions%2C%20including%20a%0Aprobabilistic%20formulation%2C%20rigorous%20uncertainty-calibration%20guarantees%2C%20and%0Aformal%20robustness%20bounds.%20Finally%2C%20by%20incorporating%20recent%20advances%20in%20graph%0Acontrastive%20learning%2C%20HU-GNN%20maintains%20diverse%2C%20structurally%20faithful%0Aembeddings.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20that%20our%0Amodel%20achieves%20state-of-the-art%20robustness%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Uncertainty-Aware%2520Graph%2520Neural%2520Network%26entry.906535625%3DYoonhyuk%2520Choi%2520and%2520Chong-Kwon%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520has%2520explored%2520mechanisms%2520for%250Acapturing%2520local%2520uncertainty%2520and%2520exploiting%2520graph%2520hierarchies%2520to%2520mitigate%2520data%250Asparsity%2520and%2520leverage%2520structural%2520properties.%2520However%252C%2520the%2520synergistic%250Aintegration%2520of%2520these%2520two%2520approaches%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520novel%2520architecture%252C%2520the%2520Hierarchical%2520Uncertainty-Aware%2520Graph%2520Neural%250ANetwork%2520%2528HU-GNN%2529%252C%2520which%2520unifies%2520multi-scale%2520representation%2520learning%252C%2520principled%250Auncertainty%2520estimation%252C%2520and%2520self-supervised%2520embedding%2520diversity%2520within%2520a%2520single%250Aend-to-end%2520framework.%2520Specifically%252C%2520HU-GNN%2520adaptively%2520forms%2520node%2520clusters%2520and%250Aestimates%2520uncertainty%2520at%2520multiple%2520structural%2520scales%2520from%2520individual%2520nodes%2520to%250Ahigher%2520levels.%2520These%2520uncertainty%2520estimates%2520guide%2520a%2520robust%2520message-passing%250Amechanism%2520and%2520attention%2520weighting%252C%2520effectively%2520mitigating%2520noise%2520and%2520adversarial%250Aperturbations%2520while%2520preserving%2520predictive%2520accuracy%2520on%2520both%2520node-%2520and%250Agraph-level%2520tasks.%2520We%2520also%2520offer%2520key%2520theoretical%2520contributions%252C%2520including%2520a%250Aprobabilistic%2520formulation%252C%2520rigorous%2520uncertainty-calibration%2520guarantees%252C%2520and%250Aformal%2520robustness%2520bounds.%2520Finally%252C%2520by%2520incorporating%2520recent%2520advances%2520in%2520graph%250Acontrastive%2520learning%252C%2520HU-GNN%2520maintains%2520diverse%252C%2520structurally%2520faithful%250Aembeddings.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520our%250Amodel%2520achieves%2520state-of-the-art%2520robustness%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Uncertainty-Aware%20Graph%20Neural%20Network&entry.906535625=Yoonhyuk%20Choi%20and%20Chong-Kwon%20Kim&entry.1292438233=%20%20Recent%20research%20on%20graph%20neural%20networks%20%28GNNs%29%20has%20explored%20mechanisms%20for%0Acapturing%20local%20uncertainty%20and%20exploiting%20graph%20hierarchies%20to%20mitigate%20data%0Asparsity%20and%20leverage%20structural%20properties.%20However%2C%20the%20synergistic%0Aintegration%20of%20these%20two%20approaches%20remains%20underexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20novel%20architecture%2C%20the%20Hierarchical%20Uncertainty-Aware%20Graph%20Neural%0ANetwork%20%28HU-GNN%29%2C%20which%20unifies%20multi-scale%20representation%20learning%2C%20principled%0Auncertainty%20estimation%2C%20and%20self-supervised%20embedding%20diversity%20within%20a%20single%0Aend-to-end%20framework.%20Specifically%2C%20HU-GNN%20adaptively%20forms%20node%20clusters%20and%0Aestimates%20uncertainty%20at%20multiple%20structural%20scales%20from%20individual%20nodes%20to%0Ahigher%20levels.%20These%20uncertainty%20estimates%20guide%20a%20robust%20message-passing%0Amechanism%20and%20attention%20weighting%2C%20effectively%20mitigating%20noise%20and%20adversarial%0Aperturbations%20while%20preserving%20predictive%20accuracy%20on%20both%20node-%20and%0Agraph-level%20tasks.%20We%20also%20offer%20key%20theoretical%20contributions%2C%20including%20a%0Aprobabilistic%20formulation%2C%20rigorous%20uncertainty-calibration%20guarantees%2C%20and%0Aformal%20robustness%20bounds.%20Finally%2C%20by%20incorporating%20recent%20advances%20in%20graph%0Acontrastive%20learning%2C%20HU-GNN%20maintains%20diverse%2C%20structurally%20faithful%0Aembeddings.%20Extensive%20experiments%20on%20standard%20benchmarks%20demonstrate%20that%20our%0Amodel%20achieves%20state-of-the-art%20robustness%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19820v1&entry.124074799=Read"},
{"title": "A prototype-based model for set classification", "author": "Mohammad Mohammadi and Sreejita Ghosh", "abstract": "  Classification of sets of inputs (e.g., images and texts) is an active area\nof research within both computer vision (CV) and natural language processing\n(NLP). A common way to represent a set of vectors is to model them as linear\nsubspaces. In this contribution, we present a prototype-based approach for\nlearning on the manifold formed from such linear subspaces, the Grassmann\nmanifold. Our proposed method learns a set of subspace prototypes capturing the\nrepresentative characteristics of classes and a set of relevance factors\nautomating the selection of the dimensionality of the subspaces. This leads to\na transparent classifier model which presents the computed impact of each input\nvector on its decision. Through experiments on benchmark image and text\ndatasets, we have demonstrated the efficiency of our proposed classifier,\ncompared to the transformer-based models in terms of not only performance and\nexplainability but also computational resource requirements.\n", "link": "http://arxiv.org/abs/2408.13720v2", "date": "2025-04-28", "relevancy": 2.6291, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20prototype-based%20model%20for%20set%20classification&body=Title%3A%20A%20prototype-based%20model%20for%20set%20classification%0AAuthor%3A%20Mohammad%20Mohammadi%20and%20Sreejita%20Ghosh%0AAbstract%3A%20%20%20Classification%20of%20sets%20of%20inputs%20%28e.g.%2C%20images%20and%20texts%29%20is%20an%20active%20area%0Aof%20research%20within%20both%20computer%20vision%20%28CV%29%20and%20natural%20language%20processing%0A%28NLP%29.%20A%20common%20way%20to%20represent%20a%20set%20of%20vectors%20is%20to%20model%20them%20as%20linear%0Asubspaces.%20In%20this%20contribution%2C%20we%20present%20a%20prototype-based%20approach%20for%0Alearning%20on%20the%20manifold%20formed%20from%20such%20linear%20subspaces%2C%20the%20Grassmann%0Amanifold.%20Our%20proposed%20method%20learns%20a%20set%20of%20subspace%20prototypes%20capturing%20the%0Arepresentative%20characteristics%20of%20classes%20and%20a%20set%20of%20relevance%20factors%0Aautomating%20the%20selection%20of%20the%20dimensionality%20of%20the%20subspaces.%20This%20leads%20to%0Aa%20transparent%20classifier%20model%20which%20presents%20the%20computed%20impact%20of%20each%20input%0Avector%20on%20its%20decision.%20Through%20experiments%20on%20benchmark%20image%20and%20text%0Adatasets%2C%20we%20have%20demonstrated%20the%20efficiency%20of%20our%20proposed%20classifier%2C%0Acompared%20to%20the%20transformer-based%20models%20in%20terms%20of%20not%20only%20performance%20and%0Aexplainability%20but%20also%20computational%20resource%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520prototype-based%2520model%2520for%2520set%2520classification%26entry.906535625%3DMohammad%2520Mohammadi%2520and%2520Sreejita%2520Ghosh%26entry.1292438233%3D%2520%2520Classification%2520of%2520sets%2520of%2520inputs%2520%2528e.g.%252C%2520images%2520and%2520texts%2529%2520is%2520an%2520active%2520area%250Aof%2520research%2520within%2520both%2520computer%2520vision%2520%2528CV%2529%2520and%2520natural%2520language%2520processing%250A%2528NLP%2529.%2520A%2520common%2520way%2520to%2520represent%2520a%2520set%2520of%2520vectors%2520is%2520to%2520model%2520them%2520as%2520linear%250Asubspaces.%2520In%2520this%2520contribution%252C%2520we%2520present%2520a%2520prototype-based%2520approach%2520for%250Alearning%2520on%2520the%2520manifold%2520formed%2520from%2520such%2520linear%2520subspaces%252C%2520the%2520Grassmann%250Amanifold.%2520Our%2520proposed%2520method%2520learns%2520a%2520set%2520of%2520subspace%2520prototypes%2520capturing%2520the%250Arepresentative%2520characteristics%2520of%2520classes%2520and%2520a%2520set%2520of%2520relevance%2520factors%250Aautomating%2520the%2520selection%2520of%2520the%2520dimensionality%2520of%2520the%2520subspaces.%2520This%2520leads%2520to%250Aa%2520transparent%2520classifier%2520model%2520which%2520presents%2520the%2520computed%2520impact%2520of%2520each%2520input%250Avector%2520on%2520its%2520decision.%2520Through%2520experiments%2520on%2520benchmark%2520image%2520and%2520text%250Adatasets%252C%2520we%2520have%2520demonstrated%2520the%2520efficiency%2520of%2520our%2520proposed%2520classifier%252C%250Acompared%2520to%2520the%2520transformer-based%2520models%2520in%2520terms%2520of%2520not%2520only%2520performance%2520and%250Aexplainability%2520but%2520also%2520computational%2520resource%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20prototype-based%20model%20for%20set%20classification&entry.906535625=Mohammad%20Mohammadi%20and%20Sreejita%20Ghosh&entry.1292438233=%20%20Classification%20of%20sets%20of%20inputs%20%28e.g.%2C%20images%20and%20texts%29%20is%20an%20active%20area%0Aof%20research%20within%20both%20computer%20vision%20%28CV%29%20and%20natural%20language%20processing%0A%28NLP%29.%20A%20common%20way%20to%20represent%20a%20set%20of%20vectors%20is%20to%20model%20them%20as%20linear%0Asubspaces.%20In%20this%20contribution%2C%20we%20present%20a%20prototype-based%20approach%20for%0Alearning%20on%20the%20manifold%20formed%20from%20such%20linear%20subspaces%2C%20the%20Grassmann%0Amanifold.%20Our%20proposed%20method%20learns%20a%20set%20of%20subspace%20prototypes%20capturing%20the%0Arepresentative%20characteristics%20of%20classes%20and%20a%20set%20of%20relevance%20factors%0Aautomating%20the%20selection%20of%20the%20dimensionality%20of%20the%20subspaces.%20This%20leads%20to%0Aa%20transparent%20classifier%20model%20which%20presents%20the%20computed%20impact%20of%20each%20input%0Avector%20on%20its%20decision.%20Through%20experiments%20on%20benchmark%20image%20and%20text%0Adatasets%2C%20we%20have%20demonstrated%20the%20efficiency%20of%20our%20proposed%20classifier%2C%0Acompared%20to%20the%20transformer-based%20models%20in%20terms%20of%20not%20only%20performance%20and%0Aexplainability%20but%20also%20computational%20resource%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13720v2&entry.124074799=Read"},
{"title": "AnimateAnywhere: Rouse the Background in Human Image Animation", "author": "Xiaoyu Liu and Mingshuai Yao and Yabo Zhang and Xianhui Lin and Peiran Ren and Xiaoming Li and Ming Liu and Wangmeng Zuo", "abstract": "  Human image animation aims to generate human videos of given characters and\nbackgrounds that adhere to the desired pose sequence. However, existing methods\nfocus more on human actions while neglecting the generation of background,\nwhich typically leads to static results or inharmonious movements. The\ncommunity has explored camera pose-guided animation tasks, yet preparing the\ncamera trajectory is impractical for most entertainment applications and\nordinary users. As a remedy, we present an AnimateAnywhere framework, rousing\nthe background in human image animation without requirements on camera\ntrajectories. In particular, based on our key insight that the movement of the\nhuman body often reflects the motion of the background, we introduce a\nbackground motion learner (BML) to learn background motions from human pose\nsequences. To encourage the model to learn more accurate cross-frame\ncorrespondences, we further deploy an epipolar constraint on the 3D attention\nmap. Specifically, the mask used to suppress geometrically unreasonable\nattention is carefully constructed by combining an epipolar mask and the\ncurrent 3D attention map. Extensive experiments demonstrate that our\nAnimateAnywhere effectively learns the background motion from human pose\nsequences, achieving state-of-the-art performance in generating human animation\nresults with vivid and realistic backgrounds. The source code and model will be\navailable at https://github.com/liuxiaoyu1104/AnimateAnywhere.\n", "link": "http://arxiv.org/abs/2504.19834v1", "date": "2025-04-28", "relevancy": 2.6196, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6871}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6609}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnimateAnywhere%3A%20Rouse%20the%20Background%20in%20Human%20Image%20Animation&body=Title%3A%20AnimateAnywhere%3A%20Rouse%20the%20Background%20in%20Human%20Image%20Animation%0AAuthor%3A%20Xiaoyu%20Liu%20and%20Mingshuai%20Yao%20and%20Yabo%20Zhang%20and%20Xianhui%20Lin%20and%20Peiran%20Ren%20and%20Xiaoming%20Li%20and%20Ming%20Liu%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Human%20image%20animation%20aims%20to%20generate%20human%20videos%20of%20given%20characters%20and%0Abackgrounds%20that%20adhere%20to%20the%20desired%20pose%20sequence.%20However%2C%20existing%20methods%0Afocus%20more%20on%20human%20actions%20while%20neglecting%20the%20generation%20of%20background%2C%0Awhich%20typically%20leads%20to%20static%20results%20or%20inharmonious%20movements.%20The%0Acommunity%20has%20explored%20camera%20pose-guided%20animation%20tasks%2C%20yet%20preparing%20the%0Acamera%20trajectory%20is%20impractical%20for%20most%20entertainment%20applications%20and%0Aordinary%20users.%20As%20a%20remedy%2C%20we%20present%20an%20AnimateAnywhere%20framework%2C%20rousing%0Athe%20background%20in%20human%20image%20animation%20without%20requirements%20on%20camera%0Atrajectories.%20In%20particular%2C%20based%20on%20our%20key%20insight%20that%20the%20movement%20of%20the%0Ahuman%20body%20often%20reflects%20the%20motion%20of%20the%20background%2C%20we%20introduce%20a%0Abackground%20motion%20learner%20%28BML%29%20to%20learn%20background%20motions%20from%20human%20pose%0Asequences.%20To%20encourage%20the%20model%20to%20learn%20more%20accurate%20cross-frame%0Acorrespondences%2C%20we%20further%20deploy%20an%20epipolar%20constraint%20on%20the%203D%20attention%0Amap.%20Specifically%2C%20the%20mask%20used%20to%20suppress%20geometrically%20unreasonable%0Aattention%20is%20carefully%20constructed%20by%20combining%20an%20epipolar%20mask%20and%20the%0Acurrent%203D%20attention%20map.%20Extensive%20experiments%20demonstrate%20that%20our%0AAnimateAnywhere%20effectively%20learns%20the%20background%20motion%20from%20human%20pose%0Asequences%2C%20achieving%20state-of-the-art%20performance%20in%20generating%20human%20animation%0Aresults%20with%20vivid%20and%20realistic%20backgrounds.%20The%20source%20code%20and%20model%20will%20be%0Aavailable%20at%20https%3A//github.com/liuxiaoyu1104/AnimateAnywhere.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnimateAnywhere%253A%2520Rouse%2520the%2520Background%2520in%2520Human%2520Image%2520Animation%26entry.906535625%3DXiaoyu%2520Liu%2520and%2520Mingshuai%2520Yao%2520and%2520Yabo%2520Zhang%2520and%2520Xianhui%2520Lin%2520and%2520Peiran%2520Ren%2520and%2520Xiaoming%2520Li%2520and%2520Ming%2520Liu%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Human%2520image%2520animation%2520aims%2520to%2520generate%2520human%2520videos%2520of%2520given%2520characters%2520and%250Abackgrounds%2520that%2520adhere%2520to%2520the%2520desired%2520pose%2520sequence.%2520However%252C%2520existing%2520methods%250Afocus%2520more%2520on%2520human%2520actions%2520while%2520neglecting%2520the%2520generation%2520of%2520background%252C%250Awhich%2520typically%2520leads%2520to%2520static%2520results%2520or%2520inharmonious%2520movements.%2520The%250Acommunity%2520has%2520explored%2520camera%2520pose-guided%2520animation%2520tasks%252C%2520yet%2520preparing%2520the%250Acamera%2520trajectory%2520is%2520impractical%2520for%2520most%2520entertainment%2520applications%2520and%250Aordinary%2520users.%2520As%2520a%2520remedy%252C%2520we%2520present%2520an%2520AnimateAnywhere%2520framework%252C%2520rousing%250Athe%2520background%2520in%2520human%2520image%2520animation%2520without%2520requirements%2520on%2520camera%250Atrajectories.%2520In%2520particular%252C%2520based%2520on%2520our%2520key%2520insight%2520that%2520the%2520movement%2520of%2520the%250Ahuman%2520body%2520often%2520reflects%2520the%2520motion%2520of%2520the%2520background%252C%2520we%2520introduce%2520a%250Abackground%2520motion%2520learner%2520%2528BML%2529%2520to%2520learn%2520background%2520motions%2520from%2520human%2520pose%250Asequences.%2520To%2520encourage%2520the%2520model%2520to%2520learn%2520more%2520accurate%2520cross-frame%250Acorrespondences%252C%2520we%2520further%2520deploy%2520an%2520epipolar%2520constraint%2520on%2520the%25203D%2520attention%250Amap.%2520Specifically%252C%2520the%2520mask%2520used%2520to%2520suppress%2520geometrically%2520unreasonable%250Aattention%2520is%2520carefully%2520constructed%2520by%2520combining%2520an%2520epipolar%2520mask%2520and%2520the%250Acurrent%25203D%2520attention%2520map.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250AAnimateAnywhere%2520effectively%2520learns%2520the%2520background%2520motion%2520from%2520human%2520pose%250Asequences%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520generating%2520human%2520animation%250Aresults%2520with%2520vivid%2520and%2520realistic%2520backgrounds.%2520The%2520source%2520code%2520and%2520model%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/liuxiaoyu1104/AnimateAnywhere.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnimateAnywhere%3A%20Rouse%20the%20Background%20in%20Human%20Image%20Animation&entry.906535625=Xiaoyu%20Liu%20and%20Mingshuai%20Yao%20and%20Yabo%20Zhang%20and%20Xianhui%20Lin%20and%20Peiran%20Ren%20and%20Xiaoming%20Li%20and%20Ming%20Liu%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Human%20image%20animation%20aims%20to%20generate%20human%20videos%20of%20given%20characters%20and%0Abackgrounds%20that%20adhere%20to%20the%20desired%20pose%20sequence.%20However%2C%20existing%20methods%0Afocus%20more%20on%20human%20actions%20while%20neglecting%20the%20generation%20of%20background%2C%0Awhich%20typically%20leads%20to%20static%20results%20or%20inharmonious%20movements.%20The%0Acommunity%20has%20explored%20camera%20pose-guided%20animation%20tasks%2C%20yet%20preparing%20the%0Acamera%20trajectory%20is%20impractical%20for%20most%20entertainment%20applications%20and%0Aordinary%20users.%20As%20a%20remedy%2C%20we%20present%20an%20AnimateAnywhere%20framework%2C%20rousing%0Athe%20background%20in%20human%20image%20animation%20without%20requirements%20on%20camera%0Atrajectories.%20In%20particular%2C%20based%20on%20our%20key%20insight%20that%20the%20movement%20of%20the%0Ahuman%20body%20often%20reflects%20the%20motion%20of%20the%20background%2C%20we%20introduce%20a%0Abackground%20motion%20learner%20%28BML%29%20to%20learn%20background%20motions%20from%20human%20pose%0Asequences.%20To%20encourage%20the%20model%20to%20learn%20more%20accurate%20cross-frame%0Acorrespondences%2C%20we%20further%20deploy%20an%20epipolar%20constraint%20on%20the%203D%20attention%0Amap.%20Specifically%2C%20the%20mask%20used%20to%20suppress%20geometrically%20unreasonable%0Aattention%20is%20carefully%20constructed%20by%20combining%20an%20epipolar%20mask%20and%20the%0Acurrent%203D%20attention%20map.%20Extensive%20experiments%20demonstrate%20that%20our%0AAnimateAnywhere%20effectively%20learns%20the%20background%20motion%20from%20human%20pose%0Asequences%2C%20achieving%20state-of-the-art%20performance%20in%20generating%20human%20animation%0Aresults%20with%20vivid%20and%20realistic%20backgrounds.%20The%20source%20code%20and%20model%20will%20be%0Aavailable%20at%20https%3A//github.com/liuxiaoyu1104/AnimateAnywhere.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19834v1&entry.124074799=Read"},
{"title": "More Clear, More Flexible, More Precise: A Comprehensive Oriented Object\n  Detection benchmark for UAV", "author": "Kai Ye and Haidi Tang and Bowen Liu and Pingyang Dai and Liujuan Cao and Rongrong Ji", "abstract": "  Applications of unmanned aerial vehicle (UAV) in logistics, agricultural\nautomation, urban management, and emergency response are highly dependent on\noriented object detection (OOD) to enhance visual perception. Although existing\ndatasets for OOD in UAV provide valuable resources, they are often designed for\nspecific downstream tasks.Consequently, they exhibit limited generalization\nperformance in real flight scenarios and fail to thoroughly demonstrate\nalgorithm effectiveness in practical environments. To bridge this critical gap,\nwe introduce CODrone, a comprehensive oriented object detection dataset for\nUAVs that accurately reflects real-world conditions. It also serves as a new\nbenchmark designed to align with downstream task requirements, ensuring greater\napplicability and robustness in UAV-based OOD.Based on application\nrequirements, we identify four key limitations in current UAV OOD datasets-low\nimage resolution, limited object categories, single-view imaging, and\nrestricted flight altitudes-and propose corresponding improvements to enhance\ntheir applicability and robustness.Furthermore, CODrone contains a broad\nspectrum of annotated images collected from multiple cities under various\nlighting conditions, enhancing the realism of the benchmark. To rigorously\nevaluate CODrone as a new benchmark and gain deeper insights into the novel\nchallenges it presents, we conduct a series of experiments based on 22\nclassical or SOTA methods.Our evaluation not only assesses the effectiveness of\nCODrone in real-world scenarios but also highlights key bottlenecks and\nopportunities to advance OOD in UAV applications.Overall, CODrone fills the\ndata gap in OOD from UAV perspective and provides a benchmark with enhanced\ngeneralization capability, better aligning with practical applications and\nfuture algorithm development.\n", "link": "http://arxiv.org/abs/2504.20032v1", "date": "2025-04-28", "relevancy": 2.6164, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Clear%2C%20More%20Flexible%2C%20More%20Precise%3A%20A%20Comprehensive%20Oriented%20Object%0A%20%20Detection%20benchmark%20for%20UAV&body=Title%3A%20More%20Clear%2C%20More%20Flexible%2C%20More%20Precise%3A%20A%20Comprehensive%20Oriented%20Object%0A%20%20Detection%20benchmark%20for%20UAV%0AAuthor%3A%20Kai%20Ye%20and%20Haidi%20Tang%20and%20Bowen%20Liu%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Applications%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20in%20logistics%2C%20agricultural%0Aautomation%2C%20urban%20management%2C%20and%20emergency%20response%20are%20highly%20dependent%20on%0Aoriented%20object%20detection%20%28OOD%29%20to%20enhance%20visual%20perception.%20Although%20existing%0Adatasets%20for%20OOD%20in%20UAV%20provide%20valuable%20resources%2C%20they%20are%20often%20designed%20for%0Aspecific%20downstream%20tasks.Consequently%2C%20they%20exhibit%20limited%20generalization%0Aperformance%20in%20real%20flight%20scenarios%20and%20fail%20to%20thoroughly%20demonstrate%0Aalgorithm%20effectiveness%20in%20practical%20environments.%20To%20bridge%20this%20critical%20gap%2C%0Awe%20introduce%20CODrone%2C%20a%20comprehensive%20oriented%20object%20detection%20dataset%20for%0AUAVs%20that%20accurately%20reflects%20real-world%20conditions.%20It%20also%20serves%20as%20a%20new%0Abenchmark%20designed%20to%20align%20with%20downstream%20task%20requirements%2C%20ensuring%20greater%0Aapplicability%20and%20robustness%20in%20UAV-based%20OOD.Based%20on%20application%0Arequirements%2C%20we%20identify%20four%20key%20limitations%20in%20current%20UAV%20OOD%20datasets-low%0Aimage%20resolution%2C%20limited%20object%20categories%2C%20single-view%20imaging%2C%20and%0Arestricted%20flight%20altitudes-and%20propose%20corresponding%20improvements%20to%20enhance%0Atheir%20applicability%20and%20robustness.Furthermore%2C%20CODrone%20contains%20a%20broad%0Aspectrum%20of%20annotated%20images%20collected%20from%20multiple%20cities%20under%20various%0Alighting%20conditions%2C%20enhancing%20the%20realism%20of%20the%20benchmark.%20To%20rigorously%0Aevaluate%20CODrone%20as%20a%20new%20benchmark%20and%20gain%20deeper%20insights%20into%20the%20novel%0Achallenges%20it%20presents%2C%20we%20conduct%20a%20series%20of%20experiments%20based%20on%2022%0Aclassical%20or%20SOTA%20methods.Our%20evaluation%20not%20only%20assesses%20the%20effectiveness%20of%0ACODrone%20in%20real-world%20scenarios%20but%20also%20highlights%20key%20bottlenecks%20and%0Aopportunities%20to%20advance%20OOD%20in%20UAV%20applications.Overall%2C%20CODrone%20fills%20the%0Adata%20gap%20in%20OOD%20from%20UAV%20perspective%20and%20provides%20a%20benchmark%20with%20enhanced%0Ageneralization%20capability%2C%20better%20aligning%20with%20practical%20applications%20and%0Afuture%20algorithm%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Clear%252C%2520More%2520Flexible%252C%2520More%2520Precise%253A%2520A%2520Comprehensive%2520Oriented%2520Object%250A%2520%2520Detection%2520benchmark%2520for%2520UAV%26entry.906535625%3DKai%2520Ye%2520and%2520Haidi%2520Tang%2520and%2520Bowen%2520Liu%2520and%2520Pingyang%2520Dai%2520and%2520Liujuan%2520Cao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Applications%2520of%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520in%2520logistics%252C%2520agricultural%250Aautomation%252C%2520urban%2520management%252C%2520and%2520emergency%2520response%2520are%2520highly%2520dependent%2520on%250Aoriented%2520object%2520detection%2520%2528OOD%2529%2520to%2520enhance%2520visual%2520perception.%2520Although%2520existing%250Adatasets%2520for%2520OOD%2520in%2520UAV%2520provide%2520valuable%2520resources%252C%2520they%2520are%2520often%2520designed%2520for%250Aspecific%2520downstream%2520tasks.Consequently%252C%2520they%2520exhibit%2520limited%2520generalization%250Aperformance%2520in%2520real%2520flight%2520scenarios%2520and%2520fail%2520to%2520thoroughly%2520demonstrate%250Aalgorithm%2520effectiveness%2520in%2520practical%2520environments.%2520To%2520bridge%2520this%2520critical%2520gap%252C%250Awe%2520introduce%2520CODrone%252C%2520a%2520comprehensive%2520oriented%2520object%2520detection%2520dataset%2520for%250AUAVs%2520that%2520accurately%2520reflects%2520real-world%2520conditions.%2520It%2520also%2520serves%2520as%2520a%2520new%250Abenchmark%2520designed%2520to%2520align%2520with%2520downstream%2520task%2520requirements%252C%2520ensuring%2520greater%250Aapplicability%2520and%2520robustness%2520in%2520UAV-based%2520OOD.Based%2520on%2520application%250Arequirements%252C%2520we%2520identify%2520four%2520key%2520limitations%2520in%2520current%2520UAV%2520OOD%2520datasets-low%250Aimage%2520resolution%252C%2520limited%2520object%2520categories%252C%2520single-view%2520imaging%252C%2520and%250Arestricted%2520flight%2520altitudes-and%2520propose%2520corresponding%2520improvements%2520to%2520enhance%250Atheir%2520applicability%2520and%2520robustness.Furthermore%252C%2520CODrone%2520contains%2520a%2520broad%250Aspectrum%2520of%2520annotated%2520images%2520collected%2520from%2520multiple%2520cities%2520under%2520various%250Alighting%2520conditions%252C%2520enhancing%2520the%2520realism%2520of%2520the%2520benchmark.%2520To%2520rigorously%250Aevaluate%2520CODrone%2520as%2520a%2520new%2520benchmark%2520and%2520gain%2520deeper%2520insights%2520into%2520the%2520novel%250Achallenges%2520it%2520presents%252C%2520we%2520conduct%2520a%2520series%2520of%2520experiments%2520based%2520on%252022%250Aclassical%2520or%2520SOTA%2520methods.Our%2520evaluation%2520not%2520only%2520assesses%2520the%2520effectiveness%2520of%250ACODrone%2520in%2520real-world%2520scenarios%2520but%2520also%2520highlights%2520key%2520bottlenecks%2520and%250Aopportunities%2520to%2520advance%2520OOD%2520in%2520UAV%2520applications.Overall%252C%2520CODrone%2520fills%2520the%250Adata%2520gap%2520in%2520OOD%2520from%2520UAV%2520perspective%2520and%2520provides%2520a%2520benchmark%2520with%2520enhanced%250Ageneralization%2520capability%252C%2520better%2520aligning%2520with%2520practical%2520applications%2520and%250Afuture%2520algorithm%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Clear%2C%20More%20Flexible%2C%20More%20Precise%3A%20A%20Comprehensive%20Oriented%20Object%0A%20%20Detection%20benchmark%20for%20UAV&entry.906535625=Kai%20Ye%20and%20Haidi%20Tang%20and%20Bowen%20Liu%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji&entry.1292438233=%20%20Applications%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20in%20logistics%2C%20agricultural%0Aautomation%2C%20urban%20management%2C%20and%20emergency%20response%20are%20highly%20dependent%20on%0Aoriented%20object%20detection%20%28OOD%29%20to%20enhance%20visual%20perception.%20Although%20existing%0Adatasets%20for%20OOD%20in%20UAV%20provide%20valuable%20resources%2C%20they%20are%20often%20designed%20for%0Aspecific%20downstream%20tasks.Consequently%2C%20they%20exhibit%20limited%20generalization%0Aperformance%20in%20real%20flight%20scenarios%20and%20fail%20to%20thoroughly%20demonstrate%0Aalgorithm%20effectiveness%20in%20practical%20environments.%20To%20bridge%20this%20critical%20gap%2C%0Awe%20introduce%20CODrone%2C%20a%20comprehensive%20oriented%20object%20detection%20dataset%20for%0AUAVs%20that%20accurately%20reflects%20real-world%20conditions.%20It%20also%20serves%20as%20a%20new%0Abenchmark%20designed%20to%20align%20with%20downstream%20task%20requirements%2C%20ensuring%20greater%0Aapplicability%20and%20robustness%20in%20UAV-based%20OOD.Based%20on%20application%0Arequirements%2C%20we%20identify%20four%20key%20limitations%20in%20current%20UAV%20OOD%20datasets-low%0Aimage%20resolution%2C%20limited%20object%20categories%2C%20single-view%20imaging%2C%20and%0Arestricted%20flight%20altitudes-and%20propose%20corresponding%20improvements%20to%20enhance%0Atheir%20applicability%20and%20robustness.Furthermore%2C%20CODrone%20contains%20a%20broad%0Aspectrum%20of%20annotated%20images%20collected%20from%20multiple%20cities%20under%20various%0Alighting%20conditions%2C%20enhancing%20the%20realism%20of%20the%20benchmark.%20To%20rigorously%0Aevaluate%20CODrone%20as%20a%20new%20benchmark%20and%20gain%20deeper%20insights%20into%20the%20novel%0Achallenges%20it%20presents%2C%20we%20conduct%20a%20series%20of%20experiments%20based%20on%2022%0Aclassical%20or%20SOTA%20methods.Our%20evaluation%20not%20only%20assesses%20the%20effectiveness%20of%0ACODrone%20in%20real-world%20scenarios%20but%20also%20highlights%20key%20bottlenecks%20and%0Aopportunities%20to%20advance%20OOD%20in%20UAV%20applications.Overall%2C%20CODrone%20fills%20the%0Adata%20gap%20in%20OOD%20from%20UAV%20perspective%20and%20provides%20a%20benchmark%20with%20enhanced%0Ageneralization%20capability%2C%20better%20aligning%20with%20practical%20applications%20and%0Afuture%20algorithm%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20032v1&entry.124074799=Read"},
{"title": "Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for\n  Partially Relevant Video Retrieval", "author": "Junlong Ren and Gangjian Zhang and Yu Hu and Jian Shu and Hao Wang", "abstract": "  Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video\nthat is partially relevant to the text query. The primary challenge in PRVR\narises from the semantic asymmetry between textual and visual modalities, as\nvideos often contain substantial content irrelevant to the query. Existing\nmethods coarsely align paired videos and text queries to construct the semantic\nspace, neglecting the critical cross-modal dual nature inherent in this task:\ninter-sample correlation and intra-sample redundancy. To this end, we propose a\nnovel PRVR framework to systematically exploit these two characteristics. Our\nframework consists of three core modules. First, the Inter Correlation\nEnhancement (ICE) module captures inter-sample correlation by identifying\nsemantically similar yet unpaired text queries and video moments, combining\nthem to form pseudo-positive pairs for more robust semantic space construction.\nSecond, the Intra Redundancy Mining (IRM) module mitigates intra-sample\nredundancy by mining redundant video moment features and treating them as hard\nnegative samples, thereby encouraging the model to learn more discriminative\nrepresentations. Finally, to reinforce these modules, we introduce the Temporal\nCoherence Prediction (TCP) module, which enhances feature discrimination by\ntraining the model to predict the original temporal order of randomly shuffled\nvideo frames and moments. Extensive experiments on three datasets demonstrate\nthe superiority of our approach compared to previous methods, achieving\nstate-of-the-art results.\n", "link": "http://arxiv.org/abs/2504.19637v1", "date": "2025-04-28", "relevancy": 2.6015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Inter-Sample%20Correlation%20and%20Intra-Sample%20Redundancy%20for%0A%20%20Partially%20Relevant%20Video%20Retrieval&body=Title%3A%20Exploiting%20Inter-Sample%20Correlation%20and%20Intra-Sample%20Redundancy%20for%0A%20%20Partially%20Relevant%20Video%20Retrieval%0AAuthor%3A%20Junlong%20Ren%20and%20Gangjian%20Zhang%20and%20Yu%20Hu%20and%20Jian%20Shu%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20aims%20to%20retrieve%20the%20target%20video%0Athat%20is%20partially%20relevant%20to%20the%20text%20query.%20The%20primary%20challenge%20in%20PRVR%0Aarises%20from%20the%20semantic%20asymmetry%20between%20textual%20and%20visual%20modalities%2C%20as%0Avideos%20often%20contain%20substantial%20content%20irrelevant%20to%20the%20query.%20Existing%0Amethods%20coarsely%20align%20paired%20videos%20and%20text%20queries%20to%20construct%20the%20semantic%0Aspace%2C%20neglecting%20the%20critical%20cross-modal%20dual%20nature%20inherent%20in%20this%20task%3A%0Ainter-sample%20correlation%20and%20intra-sample%20redundancy.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20PRVR%20framework%20to%20systematically%20exploit%20these%20two%20characteristics.%20Our%0Aframework%20consists%20of%20three%20core%20modules.%20First%2C%20the%20Inter%20Correlation%0AEnhancement%20%28ICE%29%20module%20captures%20inter-sample%20correlation%20by%20identifying%0Asemantically%20similar%20yet%20unpaired%20text%20queries%20and%20video%20moments%2C%20combining%0Athem%20to%20form%20pseudo-positive%20pairs%20for%20more%20robust%20semantic%20space%20construction.%0ASecond%2C%20the%20Intra%20Redundancy%20Mining%20%28IRM%29%20module%20mitigates%20intra-sample%0Aredundancy%20by%20mining%20redundant%20video%20moment%20features%20and%20treating%20them%20as%20hard%0Anegative%20samples%2C%20thereby%20encouraging%20the%20model%20to%20learn%20more%20discriminative%0Arepresentations.%20Finally%2C%20to%20reinforce%20these%20modules%2C%20we%20introduce%20the%20Temporal%0ACoherence%20Prediction%20%28TCP%29%20module%2C%20which%20enhances%20feature%20discrimination%20by%0Atraining%20the%20model%20to%20predict%20the%20original%20temporal%20order%20of%20randomly%20shuffled%0Avideo%20frames%20and%20moments.%20Extensive%20experiments%20on%20three%20datasets%20demonstrate%0Athe%20superiority%20of%20our%20approach%20compared%20to%20previous%20methods%2C%20achieving%0Astate-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Inter-Sample%2520Correlation%2520and%2520Intra-Sample%2520Redundancy%2520for%250A%2520%2520Partially%2520Relevant%2520Video%2520Retrieval%26entry.906535625%3DJunlong%2520Ren%2520and%2520Gangjian%2520Zhang%2520and%2520Yu%2520Hu%2520and%2520Jian%2520Shu%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Partially%2520Relevant%2520Video%2520Retrieval%2520%2528PRVR%2529%2520aims%2520to%2520retrieve%2520the%2520target%2520video%250Athat%2520is%2520partially%2520relevant%2520to%2520the%2520text%2520query.%2520The%2520primary%2520challenge%2520in%2520PRVR%250Aarises%2520from%2520the%2520semantic%2520asymmetry%2520between%2520textual%2520and%2520visual%2520modalities%252C%2520as%250Avideos%2520often%2520contain%2520substantial%2520content%2520irrelevant%2520to%2520the%2520query.%2520Existing%250Amethods%2520coarsely%2520align%2520paired%2520videos%2520and%2520text%2520queries%2520to%2520construct%2520the%2520semantic%250Aspace%252C%2520neglecting%2520the%2520critical%2520cross-modal%2520dual%2520nature%2520inherent%2520in%2520this%2520task%253A%250Ainter-sample%2520correlation%2520and%2520intra-sample%2520redundancy.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Anovel%2520PRVR%2520framework%2520to%2520systematically%2520exploit%2520these%2520two%2520characteristics.%2520Our%250Aframework%2520consists%2520of%2520three%2520core%2520modules.%2520First%252C%2520the%2520Inter%2520Correlation%250AEnhancement%2520%2528ICE%2529%2520module%2520captures%2520inter-sample%2520correlation%2520by%2520identifying%250Asemantically%2520similar%2520yet%2520unpaired%2520text%2520queries%2520and%2520video%2520moments%252C%2520combining%250Athem%2520to%2520form%2520pseudo-positive%2520pairs%2520for%2520more%2520robust%2520semantic%2520space%2520construction.%250ASecond%252C%2520the%2520Intra%2520Redundancy%2520Mining%2520%2528IRM%2529%2520module%2520mitigates%2520intra-sample%250Aredundancy%2520by%2520mining%2520redundant%2520video%2520moment%2520features%2520and%2520treating%2520them%2520as%2520hard%250Anegative%2520samples%252C%2520thereby%2520encouraging%2520the%2520model%2520to%2520learn%2520more%2520discriminative%250Arepresentations.%2520Finally%252C%2520to%2520reinforce%2520these%2520modules%252C%2520we%2520introduce%2520the%2520Temporal%250ACoherence%2520Prediction%2520%2528TCP%2529%2520module%252C%2520which%2520enhances%2520feature%2520discrimination%2520by%250Atraining%2520the%2520model%2520to%2520predict%2520the%2520original%2520temporal%2520order%2520of%2520randomly%2520shuffled%250Avideo%2520frames%2520and%2520moments.%2520Extensive%2520experiments%2520on%2520three%2520datasets%2520demonstrate%250Athe%2520superiority%2520of%2520our%2520approach%2520compared%2520to%2520previous%2520methods%252C%2520achieving%250Astate-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Inter-Sample%20Correlation%20and%20Intra-Sample%20Redundancy%20for%0A%20%20Partially%20Relevant%20Video%20Retrieval&entry.906535625=Junlong%20Ren%20and%20Gangjian%20Zhang%20and%20Yu%20Hu%20and%20Jian%20Shu%20and%20Hao%20Wang&entry.1292438233=%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20aims%20to%20retrieve%20the%20target%20video%0Athat%20is%20partially%20relevant%20to%20the%20text%20query.%20The%20primary%20challenge%20in%20PRVR%0Aarises%20from%20the%20semantic%20asymmetry%20between%20textual%20and%20visual%20modalities%2C%20as%0Avideos%20often%20contain%20substantial%20content%20irrelevant%20to%20the%20query.%20Existing%0Amethods%20coarsely%20align%20paired%20videos%20and%20text%20queries%20to%20construct%20the%20semantic%0Aspace%2C%20neglecting%20the%20critical%20cross-modal%20dual%20nature%20inherent%20in%20this%20task%3A%0Ainter-sample%20correlation%20and%20intra-sample%20redundancy.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20PRVR%20framework%20to%20systematically%20exploit%20these%20two%20characteristics.%20Our%0Aframework%20consists%20of%20three%20core%20modules.%20First%2C%20the%20Inter%20Correlation%0AEnhancement%20%28ICE%29%20module%20captures%20inter-sample%20correlation%20by%20identifying%0Asemantically%20similar%20yet%20unpaired%20text%20queries%20and%20video%20moments%2C%20combining%0Athem%20to%20form%20pseudo-positive%20pairs%20for%20more%20robust%20semantic%20space%20construction.%0ASecond%2C%20the%20Intra%20Redundancy%20Mining%20%28IRM%29%20module%20mitigates%20intra-sample%0Aredundancy%20by%20mining%20redundant%20video%20moment%20features%20and%20treating%20them%20as%20hard%0Anegative%20samples%2C%20thereby%20encouraging%20the%20model%20to%20learn%20more%20discriminative%0Arepresentations.%20Finally%2C%20to%20reinforce%20these%20modules%2C%20we%20introduce%20the%20Temporal%0ACoherence%20Prediction%20%28TCP%29%20module%2C%20which%20enhances%20feature%20discrimination%20by%0Atraining%20the%20model%20to%20predict%20the%20original%20temporal%20order%20of%20randomly%20shuffled%0Avideo%20frames%20and%20moments.%20Extensive%20experiments%20on%20three%20datasets%20demonstrate%0Athe%20superiority%20of%20our%20approach%20compared%20to%20previous%20methods%2C%20achieving%0Astate-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19637v1&entry.124074799=Read"},
{"title": "Breast Cancer Detection from Multi-View Screening Mammograms with Visual\n  Prompt Tuning", "author": "Han Chen and Anne L. Martel", "abstract": "  Accurate detection of breast cancer from high-resolution mammograms is\ncrucial for early diagnosis and effective treatment planning. Previous studies\nhave shown the potential of using single-view mammograms for breast cancer\ndetection. However, incorporating multi-view data can provide more\ncomprehensive insights. Multi-view classification, especially in medical\nimaging, presents unique challenges, particularly when dealing with\nlarge-scale, high-resolution data. In this work, we propose a novel Multi-view\nVisual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening\nmammograms. We first pretrain a robust single-view classification model on\nhigh-resolution mammograms and then innovatively adapt multi-view feature\nlearning into a task-specific prompt tuning process. This technique selectively\ntunes a minimal set of trainable parameters (7\\%) while retaining the\nrobustness of the pre-trained single-view model, enabling efficient integration\nof multi-view data without the need for aggressive downsampling. Our approach\noffers an efficient alternative to traditional feature fusion methods,\nproviding a more robust, scalable, and efficient solution for high-resolution\nmammogram analysis. Experimental results on a large multi-institution dataset\ndemonstrate that our method outperforms conventional approaches while\nmaintaining detection efficiency, achieving an AUROC of 0.852 for\ndistinguishing between Benign, DCIS, and Invasive classes. This work highlights\nthe potential of MVPT-NET for medical imaging tasks and provides a scalable\nsolution for integrating multi-view data in breast cancer detection.\n", "link": "http://arxiv.org/abs/2504.19900v1", "date": "2025-04-28", "relevancy": 2.5731, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5122}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breast%20Cancer%20Detection%20from%20Multi-View%20Screening%20Mammograms%20with%20Visual%0A%20%20Prompt%20Tuning&body=Title%3A%20Breast%20Cancer%20Detection%20from%20Multi-View%20Screening%20Mammograms%20with%20Visual%0A%20%20Prompt%20Tuning%0AAuthor%3A%20Han%20Chen%20and%20Anne%20L.%20Martel%0AAbstract%3A%20%20%20Accurate%20detection%20of%20breast%20cancer%20from%20high-resolution%20mammograms%20is%0Acrucial%20for%20early%20diagnosis%20and%20effective%20treatment%20planning.%20Previous%20studies%0Ahave%20shown%20the%20potential%20of%20using%20single-view%20mammograms%20for%20breast%20cancer%0Adetection.%20However%2C%20incorporating%20multi-view%20data%20can%20provide%20more%0Acomprehensive%20insights.%20Multi-view%20classification%2C%20especially%20in%20medical%0Aimaging%2C%20presents%20unique%20challenges%2C%20particularly%20when%20dealing%20with%0Alarge-scale%2C%20high-resolution%20data.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Multi-view%0AVisual%20Prompt%20Tuning%20Network%20%28MVPT-NET%29%20for%20analyzing%20multiple%20screening%0Amammograms.%20We%20first%20pretrain%20a%20robust%20single-view%20classification%20model%20on%0Ahigh-resolution%20mammograms%20and%20then%20innovatively%20adapt%20multi-view%20feature%0Alearning%20into%20a%20task-specific%20prompt%20tuning%20process.%20This%20technique%20selectively%0Atunes%20a%20minimal%20set%20of%20trainable%20parameters%20%287%5C%25%29%20while%20retaining%20the%0Arobustness%20of%20the%20pre-trained%20single-view%20model%2C%20enabling%20efficient%20integration%0Aof%20multi-view%20data%20without%20the%20need%20for%20aggressive%20downsampling.%20Our%20approach%0Aoffers%20an%20efficient%20alternative%20to%20traditional%20feature%20fusion%20methods%2C%0Aproviding%20a%20more%20robust%2C%20scalable%2C%20and%20efficient%20solution%20for%20high-resolution%0Amammogram%20analysis.%20Experimental%20results%20on%20a%20large%20multi-institution%20dataset%0Ademonstrate%20that%20our%20method%20outperforms%20conventional%20approaches%20while%0Amaintaining%20detection%20efficiency%2C%20achieving%20an%20AUROC%20of%200.852%20for%0Adistinguishing%20between%20Benign%2C%20DCIS%2C%20and%20Invasive%20classes.%20This%20work%20highlights%0Athe%20potential%20of%20MVPT-NET%20for%20medical%20imaging%20tasks%20and%20provides%20a%20scalable%0Asolution%20for%20integrating%20multi-view%20data%20in%20breast%20cancer%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreast%2520Cancer%2520Detection%2520from%2520Multi-View%2520Screening%2520Mammograms%2520with%2520Visual%250A%2520%2520Prompt%2520Tuning%26entry.906535625%3DHan%2520Chen%2520and%2520Anne%2520L.%2520Martel%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520of%2520breast%2520cancer%2520from%2520high-resolution%2520mammograms%2520is%250Acrucial%2520for%2520early%2520diagnosis%2520and%2520effective%2520treatment%2520planning.%2520Previous%2520studies%250Ahave%2520shown%2520the%2520potential%2520of%2520using%2520single-view%2520mammograms%2520for%2520breast%2520cancer%250Adetection.%2520However%252C%2520incorporating%2520multi-view%2520data%2520can%2520provide%2520more%250Acomprehensive%2520insights.%2520Multi-view%2520classification%252C%2520especially%2520in%2520medical%250Aimaging%252C%2520presents%2520unique%2520challenges%252C%2520particularly%2520when%2520dealing%2520with%250Alarge-scale%252C%2520high-resolution%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520Multi-view%250AVisual%2520Prompt%2520Tuning%2520Network%2520%2528MVPT-NET%2529%2520for%2520analyzing%2520multiple%2520screening%250Amammograms.%2520We%2520first%2520pretrain%2520a%2520robust%2520single-view%2520classification%2520model%2520on%250Ahigh-resolution%2520mammograms%2520and%2520then%2520innovatively%2520adapt%2520multi-view%2520feature%250Alearning%2520into%2520a%2520task-specific%2520prompt%2520tuning%2520process.%2520This%2520technique%2520selectively%250Atunes%2520a%2520minimal%2520set%2520of%2520trainable%2520parameters%2520%25287%255C%2525%2529%2520while%2520retaining%2520the%250Arobustness%2520of%2520the%2520pre-trained%2520single-view%2520model%252C%2520enabling%2520efficient%2520integration%250Aof%2520multi-view%2520data%2520without%2520the%2520need%2520for%2520aggressive%2520downsampling.%2520Our%2520approach%250Aoffers%2520an%2520efficient%2520alternative%2520to%2520traditional%2520feature%2520fusion%2520methods%252C%250Aproviding%2520a%2520more%2520robust%252C%2520scalable%252C%2520and%2520efficient%2520solution%2520for%2520high-resolution%250Amammogram%2520analysis.%2520Experimental%2520results%2520on%2520a%2520large%2520multi-institution%2520dataset%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520conventional%2520approaches%2520while%250Amaintaining%2520detection%2520efficiency%252C%2520achieving%2520an%2520AUROC%2520of%25200.852%2520for%250Adistinguishing%2520between%2520Benign%252C%2520DCIS%252C%2520and%2520Invasive%2520classes.%2520This%2520work%2520highlights%250Athe%2520potential%2520of%2520MVPT-NET%2520for%2520medical%2520imaging%2520tasks%2520and%2520provides%2520a%2520scalable%250Asolution%2520for%2520integrating%2520multi-view%2520data%2520in%2520breast%2520cancer%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Cancer%20Detection%20from%20Multi-View%20Screening%20Mammograms%20with%20Visual%0A%20%20Prompt%20Tuning&entry.906535625=Han%20Chen%20and%20Anne%20L.%20Martel&entry.1292438233=%20%20Accurate%20detection%20of%20breast%20cancer%20from%20high-resolution%20mammograms%20is%0Acrucial%20for%20early%20diagnosis%20and%20effective%20treatment%20planning.%20Previous%20studies%0Ahave%20shown%20the%20potential%20of%20using%20single-view%20mammograms%20for%20breast%20cancer%0Adetection.%20However%2C%20incorporating%20multi-view%20data%20can%20provide%20more%0Acomprehensive%20insights.%20Multi-view%20classification%2C%20especially%20in%20medical%0Aimaging%2C%20presents%20unique%20challenges%2C%20particularly%20when%20dealing%20with%0Alarge-scale%2C%20high-resolution%20data.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Multi-view%0AVisual%20Prompt%20Tuning%20Network%20%28MVPT-NET%29%20for%20analyzing%20multiple%20screening%0Amammograms.%20We%20first%20pretrain%20a%20robust%20single-view%20classification%20model%20on%0Ahigh-resolution%20mammograms%20and%20then%20innovatively%20adapt%20multi-view%20feature%0Alearning%20into%20a%20task-specific%20prompt%20tuning%20process.%20This%20technique%20selectively%0Atunes%20a%20minimal%20set%20of%20trainable%20parameters%20%287%5C%25%29%20while%20retaining%20the%0Arobustness%20of%20the%20pre-trained%20single-view%20model%2C%20enabling%20efficient%20integration%0Aof%20multi-view%20data%20without%20the%20need%20for%20aggressive%20downsampling.%20Our%20approach%0Aoffers%20an%20efficient%20alternative%20to%20traditional%20feature%20fusion%20methods%2C%0Aproviding%20a%20more%20robust%2C%20scalable%2C%20and%20efficient%20solution%20for%20high-resolution%0Amammogram%20analysis.%20Experimental%20results%20on%20a%20large%20multi-institution%20dataset%0Ademonstrate%20that%20our%20method%20outperforms%20conventional%20approaches%20while%0Amaintaining%20detection%20efficiency%2C%20achieving%20an%20AUROC%20of%200.852%20for%0Adistinguishing%20between%20Benign%2C%20DCIS%2C%20and%20Invasive%20classes.%20This%20work%20highlights%0Athe%20potential%20of%20MVPT-NET%20for%20medical%20imaging%20tasks%20and%20provides%20a%20scalable%0Asolution%20for%20integrating%20multi-view%20data%20in%20breast%20cancer%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19900v1&entry.124074799=Read"},
{"title": "Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based\n  Image Classification", "author": "Nikolaos Chaidos and Angeliki Dimitriou and Nikolaos Spanos and Athanasios Voulodimos and Giorgos Stamou", "abstract": "  Graph Neural Networks (GNNs) have emerged as an efficient alternative to\nconvolutional approaches for vision tasks such as image classification,\nleveraging patch-based representations instead of raw pixels. These methods\nconstruct graphs where image patches serve as nodes, and edges are established\nbased on patch similarity or classification relevance. Despite their\nefficiency, the explainability of GNN-based vision models remains\nunderexplored, even though graphs are naturally interpretable. In this work, we\nanalyze the semantic consistency of the graphs formed at different layers of\nGNN-based image classifiers, focusing on how well they preserve object\nstructures and meaningful relationships. A comprehensive analysis is presented\nby quantifying the extent to which inter-layer graph connections reflect\nsemantic similarity and spatial coherence. Explanations from standard and\nadversarial settings are also compared to assess whether they reflect the\nclassifiers' robustness. Additionally, we visualize the flow of information\nacross layers through heatmap-based visualization techniques, thereby\nhighlighting the models' explainability. Our findings demonstrate that the\ndecision-making processes of these models can be effectively explained, while\nalso revealing that their reasoning does not necessarily align with human\nperception, especially in deeper layers.\n", "link": "http://arxiv.org/abs/2504.19682v1", "date": "2025-04-28", "relevancy": 2.5684, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Vision%20GNNs%3A%20A%20Semantic%20and%20Visual%20Analysis%20of%20Graph-based%0A%20%20Image%20Classification&body=Title%3A%20Explaining%20Vision%20GNNs%3A%20A%20Semantic%20and%20Visual%20Analysis%20of%20Graph-based%0A%20%20Image%20Classification%0AAuthor%3A%20Nikolaos%20Chaidos%20and%20Angeliki%20Dimitriou%20and%20Nikolaos%20Spanos%20and%20Athanasios%20Voulodimos%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20an%20efficient%20alternative%20to%0Aconvolutional%20approaches%20for%20vision%20tasks%20such%20as%20image%20classification%2C%0Aleveraging%20patch-based%20representations%20instead%20of%20raw%20pixels.%20These%20methods%0Aconstruct%20graphs%20where%20image%20patches%20serve%20as%20nodes%2C%20and%20edges%20are%20established%0Abased%20on%20patch%20similarity%20or%20classification%20relevance.%20Despite%20their%0Aefficiency%2C%20the%20explainability%20of%20GNN-based%20vision%20models%20remains%0Aunderexplored%2C%20even%20though%20graphs%20are%20naturally%20interpretable.%20In%20this%20work%2C%20we%0Aanalyze%20the%20semantic%20consistency%20of%20the%20graphs%20formed%20at%20different%20layers%20of%0AGNN-based%20image%20classifiers%2C%20focusing%20on%20how%20well%20they%20preserve%20object%0Astructures%20and%20meaningful%20relationships.%20A%20comprehensive%20analysis%20is%20presented%0Aby%20quantifying%20the%20extent%20to%20which%20inter-layer%20graph%20connections%20reflect%0Asemantic%20similarity%20and%20spatial%20coherence.%20Explanations%20from%20standard%20and%0Aadversarial%20settings%20are%20also%20compared%20to%20assess%20whether%20they%20reflect%20the%0Aclassifiers%27%20robustness.%20Additionally%2C%20we%20visualize%20the%20flow%20of%20information%0Aacross%20layers%20through%20heatmap-based%20visualization%20techniques%2C%20thereby%0Ahighlighting%20the%20models%27%20explainability.%20Our%20findings%20demonstrate%20that%20the%0Adecision-making%20processes%20of%20these%20models%20can%20be%20effectively%20explained%2C%20while%0Aalso%20revealing%20that%20their%20reasoning%20does%20not%20necessarily%20align%20with%20human%0Aperception%2C%20especially%20in%20deeper%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Vision%2520GNNs%253A%2520A%2520Semantic%2520and%2520Visual%2520Analysis%2520of%2520Graph-based%250A%2520%2520Image%2520Classification%26entry.906535625%3DNikolaos%2520Chaidos%2520and%2520Angeliki%2520Dimitriou%2520and%2520Nikolaos%2520Spanos%2520and%2520Athanasios%2520Voulodimos%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520an%2520efficient%2520alternative%2520to%250Aconvolutional%2520approaches%2520for%2520vision%2520tasks%2520such%2520as%2520image%2520classification%252C%250Aleveraging%2520patch-based%2520representations%2520instead%2520of%2520raw%2520pixels.%2520These%2520methods%250Aconstruct%2520graphs%2520where%2520image%2520patches%2520serve%2520as%2520nodes%252C%2520and%2520edges%2520are%2520established%250Abased%2520on%2520patch%2520similarity%2520or%2520classification%2520relevance.%2520Despite%2520their%250Aefficiency%252C%2520the%2520explainability%2520of%2520GNN-based%2520vision%2520models%2520remains%250Aunderexplored%252C%2520even%2520though%2520graphs%2520are%2520naturally%2520interpretable.%2520In%2520this%2520work%252C%2520we%250Aanalyze%2520the%2520semantic%2520consistency%2520of%2520the%2520graphs%2520formed%2520at%2520different%2520layers%2520of%250AGNN-based%2520image%2520classifiers%252C%2520focusing%2520on%2520how%2520well%2520they%2520preserve%2520object%250Astructures%2520and%2520meaningful%2520relationships.%2520A%2520comprehensive%2520analysis%2520is%2520presented%250Aby%2520quantifying%2520the%2520extent%2520to%2520which%2520inter-layer%2520graph%2520connections%2520reflect%250Asemantic%2520similarity%2520and%2520spatial%2520coherence.%2520Explanations%2520from%2520standard%2520and%250Aadversarial%2520settings%2520are%2520also%2520compared%2520to%2520assess%2520whether%2520they%2520reflect%2520the%250Aclassifiers%2527%2520robustness.%2520Additionally%252C%2520we%2520visualize%2520the%2520flow%2520of%2520information%250Aacross%2520layers%2520through%2520heatmap-based%2520visualization%2520techniques%252C%2520thereby%250Ahighlighting%2520the%2520models%2527%2520explainability.%2520Our%2520findings%2520demonstrate%2520that%2520the%250Adecision-making%2520processes%2520of%2520these%2520models%2520can%2520be%2520effectively%2520explained%252C%2520while%250Aalso%2520revealing%2520that%2520their%2520reasoning%2520does%2520not%2520necessarily%2520align%2520with%2520human%250Aperception%252C%2520especially%2520in%2520deeper%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Vision%20GNNs%3A%20A%20Semantic%20and%20Visual%20Analysis%20of%20Graph-based%0A%20%20Image%20Classification&entry.906535625=Nikolaos%20Chaidos%20and%20Angeliki%20Dimitriou%20and%20Nikolaos%20Spanos%20and%20Athanasios%20Voulodimos%20and%20Giorgos%20Stamou&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20an%20efficient%20alternative%20to%0Aconvolutional%20approaches%20for%20vision%20tasks%20such%20as%20image%20classification%2C%0Aleveraging%20patch-based%20representations%20instead%20of%20raw%20pixels.%20These%20methods%0Aconstruct%20graphs%20where%20image%20patches%20serve%20as%20nodes%2C%20and%20edges%20are%20established%0Abased%20on%20patch%20similarity%20or%20classification%20relevance.%20Despite%20their%0Aefficiency%2C%20the%20explainability%20of%20GNN-based%20vision%20models%20remains%0Aunderexplored%2C%20even%20though%20graphs%20are%20naturally%20interpretable.%20In%20this%20work%2C%20we%0Aanalyze%20the%20semantic%20consistency%20of%20the%20graphs%20formed%20at%20different%20layers%20of%0AGNN-based%20image%20classifiers%2C%20focusing%20on%20how%20well%20they%20preserve%20object%0Astructures%20and%20meaningful%20relationships.%20A%20comprehensive%20analysis%20is%20presented%0Aby%20quantifying%20the%20extent%20to%20which%20inter-layer%20graph%20connections%20reflect%0Asemantic%20similarity%20and%20spatial%20coherence.%20Explanations%20from%20standard%20and%0Aadversarial%20settings%20are%20also%20compared%20to%20assess%20whether%20they%20reflect%20the%0Aclassifiers%27%20robustness.%20Additionally%2C%20we%20visualize%20the%20flow%20of%20information%0Aacross%20layers%20through%20heatmap-based%20visualization%20techniques%2C%20thereby%0Ahighlighting%20the%20models%27%20explainability.%20Our%20findings%20demonstrate%20that%20the%0Adecision-making%20processes%20of%20these%20models%20can%20be%20effectively%20explained%2C%20while%0Aalso%20revealing%20that%20their%20reasoning%20does%20not%20necessarily%20align%20with%20human%0Aperception%2C%20especially%20in%20deeper%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19682v1&entry.124074799=Read"},
{"title": "Better artificial intelligence does not mean better models of biology", "author": "Drew Linsley and Pinyuan Feng and Thomas Serre", "abstract": "  Deep neural networks (DNNs) once showed increasing alignment with primate\nperception and neural responses as they improved on vision benchmarks, raising\nhopes that advances in AI would yield better models of biological vision.\nHowever, we show across three benchmarks that this alignment is now plateauing\n- and in some cases worsening - as DNNs scale to human or superhuman accuracy.\nThis divergence may reflect the adoption of visual strategies that differ from\nthose used by primates. These findings challenge the view that progress in\nartificial intelligence will naturally translate to neuroscience. We argue that\nvision science must chart its own course, developing algorithms grounded in\nbiological visual systems rather than optimizing for benchmarks based on\ninternet-scale datasets.\n", "link": "http://arxiv.org/abs/2504.16940v3", "date": "2025-04-28", "relevancy": 2.5568, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20artificial%20intelligence%20does%20not%20mean%20better%20models%20of%20biology&body=Title%3A%20Better%20artificial%20intelligence%20does%20not%20mean%20better%20models%20of%20biology%0AAuthor%3A%20Drew%20Linsley%20and%20Pinyuan%20Feng%20and%20Thomas%20Serre%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20once%20showed%20increasing%20alignment%20with%20primate%0Aperception%20and%20neural%20responses%20as%20they%20improved%20on%20vision%20benchmarks%2C%20raising%0Ahopes%20that%20advances%20in%20AI%20would%20yield%20better%20models%20of%20biological%20vision.%0AHowever%2C%20we%20show%20across%20three%20benchmarks%20that%20this%20alignment%20is%20now%20plateauing%0A-%20and%20in%20some%20cases%20worsening%20-%20as%20DNNs%20scale%20to%20human%20or%20superhuman%20accuracy.%0AThis%20divergence%20may%20reflect%20the%20adoption%20of%20visual%20strategies%20that%20differ%20from%0Athose%20used%20by%20primates.%20These%20findings%20challenge%20the%20view%20that%20progress%20in%0Aartificial%20intelligence%20will%20naturally%20translate%20to%20neuroscience.%20We%20argue%20that%0Avision%20science%20must%20chart%20its%20own%20course%2C%20developing%20algorithms%20grounded%20in%0Abiological%20visual%20systems%20rather%20than%20optimizing%20for%20benchmarks%20based%20on%0Ainternet-scale%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520artificial%2520intelligence%2520does%2520not%2520mean%2520better%2520models%2520of%2520biology%26entry.906535625%3DDrew%2520Linsley%2520and%2520Pinyuan%2520Feng%2520and%2520Thomas%2520Serre%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520once%2520showed%2520increasing%2520alignment%2520with%2520primate%250Aperception%2520and%2520neural%2520responses%2520as%2520they%2520improved%2520on%2520vision%2520benchmarks%252C%2520raising%250Ahopes%2520that%2520advances%2520in%2520AI%2520would%2520yield%2520better%2520models%2520of%2520biological%2520vision.%250AHowever%252C%2520we%2520show%2520across%2520three%2520benchmarks%2520that%2520this%2520alignment%2520is%2520now%2520plateauing%250A-%2520and%2520in%2520some%2520cases%2520worsening%2520-%2520as%2520DNNs%2520scale%2520to%2520human%2520or%2520superhuman%2520accuracy.%250AThis%2520divergence%2520may%2520reflect%2520the%2520adoption%2520of%2520visual%2520strategies%2520that%2520differ%2520from%250Athose%2520used%2520by%2520primates.%2520These%2520findings%2520challenge%2520the%2520view%2520that%2520progress%2520in%250Aartificial%2520intelligence%2520will%2520naturally%2520translate%2520to%2520neuroscience.%2520We%2520argue%2520that%250Avision%2520science%2520must%2520chart%2520its%2520own%2520course%252C%2520developing%2520algorithms%2520grounded%2520in%250Abiological%2520visual%2520systems%2520rather%2520than%2520optimizing%2520for%2520benchmarks%2520based%2520on%250Ainternet-scale%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20artificial%20intelligence%20does%20not%20mean%20better%20models%20of%20biology&entry.906535625=Drew%20Linsley%20and%20Pinyuan%20Feng%20and%20Thomas%20Serre&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20once%20showed%20increasing%20alignment%20with%20primate%0Aperception%20and%20neural%20responses%20as%20they%20improved%20on%20vision%20benchmarks%2C%20raising%0Ahopes%20that%20advances%20in%20AI%20would%20yield%20better%20models%20of%20biological%20vision.%0AHowever%2C%20we%20show%20across%20three%20benchmarks%20that%20this%20alignment%20is%20now%20plateauing%0A-%20and%20in%20some%20cases%20worsening%20-%20as%20DNNs%20scale%20to%20human%20or%20superhuman%20accuracy.%0AThis%20divergence%20may%20reflect%20the%20adoption%20of%20visual%20strategies%20that%20differ%20from%0Athose%20used%20by%20primates.%20These%20findings%20challenge%20the%20view%20that%20progress%20in%0Aartificial%20intelligence%20will%20naturally%20translate%20to%20neuroscience.%20We%20argue%20that%0Avision%20science%20must%20chart%20its%20own%20course%2C%20developing%20algorithms%20grounded%20in%0Abiological%20visual%20systems%20rather%20than%20optimizing%20for%20benchmarks%20based%20on%0Ainternet-scale%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16940v3&entry.124074799=Read"},
{"title": "Enhancing breast cancer detection on screening mammogram using\n  self-supervised learning and a hybrid deep model of Swin Transformer and\n  Convolutional Neural Network", "author": "Han Chen and Anne L. Martel", "abstract": "  Purpose: The scarcity of high-quality curated labeled medical training data\nremains one of the major limitations in applying artificial intelligence (AI)\nsystems to breast cancer diagnosis. Deep models for mammogram analysis and mass\n(or micro-calcification) detection require training with a large volume of\nlabeled images, which are often expensive and time-consuming to collect. To\nreduce this challenge, we proposed a novel method that leverages\nself-supervised learning (SSL) and a deep hybrid model, named \\textbf{HybMNet},\nwhich combines local self-attention and fine-grained feature extraction to\nenhance breast cancer detection on screening mammograms.\n  Approach: Our method employs a two-stage learning process: (1) SSL\nPretraining: We utilize EsViT, a SSL technique, to pretrain a Swin Transformer\n(Swin-T) using a limited set of mammograms. The pretrained Swin-T then serves\nas the backbone for the downstream task. (2) Downstream Training: The proposed\nHybMNet combines the Swin-T backbone with a CNN-based network and a novel\nfusion strategy. The Swin-T employs local self-attention to identify\ninformative patch regions from the high-resolution mammogram, while the\nCNN-based network extracts fine-grained local features from the selected\npatches. A fusion module then integrates global and local information from both\nnetworks to generate robust predictions. The HybMNet is trained end-to-end,\nwith the loss function combining the outputs of the Swin-T and CNN modules to\noptimize feature extraction and classification performance.\n  Results: The proposed method was evaluated for its ability to detect breast\ncancer by distinguishing between benign (normal) and malignant mammograms.\nLeveraging SSL pretraining and the HybMNet model, it achieved AUC of 0.864 (95%\nCI: 0.852, 0.875) on the CMMD dataset and 0.889 (95% CI: 0.875, 0.903) on the\nINbreast dataset, highlighting its effectiveness.\n", "link": "http://arxiv.org/abs/2504.19888v1", "date": "2025-04-28", "relevancy": 2.5428, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5064}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20breast%20cancer%20detection%20on%20screening%20mammogram%20using%0A%20%20self-supervised%20learning%20and%20a%20hybrid%20deep%20model%20of%20Swin%20Transformer%20and%0A%20%20Convolutional%20Neural%20Network&body=Title%3A%20Enhancing%20breast%20cancer%20detection%20on%20screening%20mammogram%20using%0A%20%20self-supervised%20learning%20and%20a%20hybrid%20deep%20model%20of%20Swin%20Transformer%20and%0A%20%20Convolutional%20Neural%20Network%0AAuthor%3A%20Han%20Chen%20and%20Anne%20L.%20Martel%0AAbstract%3A%20%20%20Purpose%3A%20The%20scarcity%20of%20high-quality%20curated%20labeled%20medical%20training%20data%0Aremains%20one%20of%20the%20major%20limitations%20in%20applying%20artificial%20intelligence%20%28AI%29%0Asystems%20to%20breast%20cancer%20diagnosis.%20Deep%20models%20for%20mammogram%20analysis%20and%20mass%0A%28or%20micro-calcification%29%20detection%20require%20training%20with%20a%20large%20volume%20of%0Alabeled%20images%2C%20which%20are%20often%20expensive%20and%20time-consuming%20to%20collect.%20To%0Areduce%20this%20challenge%2C%20we%20proposed%20a%20novel%20method%20that%20leverages%0Aself-supervised%20learning%20%28SSL%29%20and%20a%20deep%20hybrid%20model%2C%20named%20%5Ctextbf%7BHybMNet%7D%2C%0Awhich%20combines%20local%20self-attention%20and%20fine-grained%20feature%20extraction%20to%0Aenhance%20breast%20cancer%20detection%20on%20screening%20mammograms.%0A%20%20Approach%3A%20Our%20method%20employs%20a%20two-stage%20learning%20process%3A%20%281%29%20SSL%0APretraining%3A%20We%20utilize%20EsViT%2C%20a%20SSL%20technique%2C%20to%20pretrain%20a%20Swin%20Transformer%0A%28Swin-T%29%20using%20a%20limited%20set%20of%20mammograms.%20The%20pretrained%20Swin-T%20then%20serves%0Aas%20the%20backbone%20for%20the%20downstream%20task.%20%282%29%20Downstream%20Training%3A%20The%20proposed%0AHybMNet%20combines%20the%20Swin-T%20backbone%20with%20a%20CNN-based%20network%20and%20a%20novel%0Afusion%20strategy.%20The%20Swin-T%20employs%20local%20self-attention%20to%20identify%0Ainformative%20patch%20regions%20from%20the%20high-resolution%20mammogram%2C%20while%20the%0ACNN-based%20network%20extracts%20fine-grained%20local%20features%20from%20the%20selected%0Apatches.%20A%20fusion%20module%20then%20integrates%20global%20and%20local%20information%20from%20both%0Anetworks%20to%20generate%20robust%20predictions.%20The%20HybMNet%20is%20trained%20end-to-end%2C%0Awith%20the%20loss%20function%20combining%20the%20outputs%20of%20the%20Swin-T%20and%20CNN%20modules%20to%0Aoptimize%20feature%20extraction%20and%20classification%20performance.%0A%20%20Results%3A%20The%20proposed%20method%20was%20evaluated%20for%20its%20ability%20to%20detect%20breast%0Acancer%20by%20distinguishing%20between%20benign%20%28normal%29%20and%20malignant%20mammograms.%0ALeveraging%20SSL%20pretraining%20and%20the%20HybMNet%20model%2C%20it%20achieved%20AUC%20of%200.864%20%2895%25%0ACI%3A%200.852%2C%200.875%29%20on%20the%20CMMD%20dataset%20and%200.889%20%2895%25%20CI%3A%200.875%2C%200.903%29%20on%20the%0AINbreast%20dataset%2C%20highlighting%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520breast%2520cancer%2520detection%2520on%2520screening%2520mammogram%2520using%250A%2520%2520self-supervised%2520learning%2520and%2520a%2520hybrid%2520deep%2520model%2520of%2520Swin%2520Transformer%2520and%250A%2520%2520Convolutional%2520Neural%2520Network%26entry.906535625%3DHan%2520Chen%2520and%2520Anne%2520L.%2520Martel%26entry.1292438233%3D%2520%2520Purpose%253A%2520The%2520scarcity%2520of%2520high-quality%2520curated%2520labeled%2520medical%2520training%2520data%250Aremains%2520one%2520of%2520the%2520major%2520limitations%2520in%2520applying%2520artificial%2520intelligence%2520%2528AI%2529%250Asystems%2520to%2520breast%2520cancer%2520diagnosis.%2520Deep%2520models%2520for%2520mammogram%2520analysis%2520and%2520mass%250A%2528or%2520micro-calcification%2529%2520detection%2520require%2520training%2520with%2520a%2520large%2520volume%2520of%250Alabeled%2520images%252C%2520which%2520are%2520often%2520expensive%2520and%2520time-consuming%2520to%2520collect.%2520To%250Areduce%2520this%2520challenge%252C%2520we%2520proposed%2520a%2520novel%2520method%2520that%2520leverages%250Aself-supervised%2520learning%2520%2528SSL%2529%2520and%2520a%2520deep%2520hybrid%2520model%252C%2520named%2520%255Ctextbf%257BHybMNet%257D%252C%250Awhich%2520combines%2520local%2520self-attention%2520and%2520fine-grained%2520feature%2520extraction%2520to%250Aenhance%2520breast%2520cancer%2520detection%2520on%2520screening%2520mammograms.%250A%2520%2520Approach%253A%2520Our%2520method%2520employs%2520a%2520two-stage%2520learning%2520process%253A%2520%25281%2529%2520SSL%250APretraining%253A%2520We%2520utilize%2520EsViT%252C%2520a%2520SSL%2520technique%252C%2520to%2520pretrain%2520a%2520Swin%2520Transformer%250A%2528Swin-T%2529%2520using%2520a%2520limited%2520set%2520of%2520mammograms.%2520The%2520pretrained%2520Swin-T%2520then%2520serves%250Aas%2520the%2520backbone%2520for%2520the%2520downstream%2520task.%2520%25282%2529%2520Downstream%2520Training%253A%2520The%2520proposed%250AHybMNet%2520combines%2520the%2520Swin-T%2520backbone%2520with%2520a%2520CNN-based%2520network%2520and%2520a%2520novel%250Afusion%2520strategy.%2520The%2520Swin-T%2520employs%2520local%2520self-attention%2520to%2520identify%250Ainformative%2520patch%2520regions%2520from%2520the%2520high-resolution%2520mammogram%252C%2520while%2520the%250ACNN-based%2520network%2520extracts%2520fine-grained%2520local%2520features%2520from%2520the%2520selected%250Apatches.%2520A%2520fusion%2520module%2520then%2520integrates%2520global%2520and%2520local%2520information%2520from%2520both%250Anetworks%2520to%2520generate%2520robust%2520predictions.%2520The%2520HybMNet%2520is%2520trained%2520end-to-end%252C%250Awith%2520the%2520loss%2520function%2520combining%2520the%2520outputs%2520of%2520the%2520Swin-T%2520and%2520CNN%2520modules%2520to%250Aoptimize%2520feature%2520extraction%2520and%2520classification%2520performance.%250A%2520%2520Results%253A%2520The%2520proposed%2520method%2520was%2520evaluated%2520for%2520its%2520ability%2520to%2520detect%2520breast%250Acancer%2520by%2520distinguishing%2520between%2520benign%2520%2528normal%2529%2520and%2520malignant%2520mammograms.%250ALeveraging%2520SSL%2520pretraining%2520and%2520the%2520HybMNet%2520model%252C%2520it%2520achieved%2520AUC%2520of%25200.864%2520%252895%2525%250ACI%253A%25200.852%252C%25200.875%2529%2520on%2520the%2520CMMD%2520dataset%2520and%25200.889%2520%252895%2525%2520CI%253A%25200.875%252C%25200.903%2529%2520on%2520the%250AINbreast%2520dataset%252C%2520highlighting%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20breast%20cancer%20detection%20on%20screening%20mammogram%20using%0A%20%20self-supervised%20learning%20and%20a%20hybrid%20deep%20model%20of%20Swin%20Transformer%20and%0A%20%20Convolutional%20Neural%20Network&entry.906535625=Han%20Chen%20and%20Anne%20L.%20Martel&entry.1292438233=%20%20Purpose%3A%20The%20scarcity%20of%20high-quality%20curated%20labeled%20medical%20training%20data%0Aremains%20one%20of%20the%20major%20limitations%20in%20applying%20artificial%20intelligence%20%28AI%29%0Asystems%20to%20breast%20cancer%20diagnosis.%20Deep%20models%20for%20mammogram%20analysis%20and%20mass%0A%28or%20micro-calcification%29%20detection%20require%20training%20with%20a%20large%20volume%20of%0Alabeled%20images%2C%20which%20are%20often%20expensive%20and%20time-consuming%20to%20collect.%20To%0Areduce%20this%20challenge%2C%20we%20proposed%20a%20novel%20method%20that%20leverages%0Aself-supervised%20learning%20%28SSL%29%20and%20a%20deep%20hybrid%20model%2C%20named%20%5Ctextbf%7BHybMNet%7D%2C%0Awhich%20combines%20local%20self-attention%20and%20fine-grained%20feature%20extraction%20to%0Aenhance%20breast%20cancer%20detection%20on%20screening%20mammograms.%0A%20%20Approach%3A%20Our%20method%20employs%20a%20two-stage%20learning%20process%3A%20%281%29%20SSL%0APretraining%3A%20We%20utilize%20EsViT%2C%20a%20SSL%20technique%2C%20to%20pretrain%20a%20Swin%20Transformer%0A%28Swin-T%29%20using%20a%20limited%20set%20of%20mammograms.%20The%20pretrained%20Swin-T%20then%20serves%0Aas%20the%20backbone%20for%20the%20downstream%20task.%20%282%29%20Downstream%20Training%3A%20The%20proposed%0AHybMNet%20combines%20the%20Swin-T%20backbone%20with%20a%20CNN-based%20network%20and%20a%20novel%0Afusion%20strategy.%20The%20Swin-T%20employs%20local%20self-attention%20to%20identify%0Ainformative%20patch%20regions%20from%20the%20high-resolution%20mammogram%2C%20while%20the%0ACNN-based%20network%20extracts%20fine-grained%20local%20features%20from%20the%20selected%0Apatches.%20A%20fusion%20module%20then%20integrates%20global%20and%20local%20information%20from%20both%0Anetworks%20to%20generate%20robust%20predictions.%20The%20HybMNet%20is%20trained%20end-to-end%2C%0Awith%20the%20loss%20function%20combining%20the%20outputs%20of%20the%20Swin-T%20and%20CNN%20modules%20to%0Aoptimize%20feature%20extraction%20and%20classification%20performance.%0A%20%20Results%3A%20The%20proposed%20method%20was%20evaluated%20for%20its%20ability%20to%20detect%20breast%0Acancer%20by%20distinguishing%20between%20benign%20%28normal%29%20and%20malignant%20mammograms.%0ALeveraging%20SSL%20pretraining%20and%20the%20HybMNet%20model%2C%20it%20achieved%20AUC%20of%200.864%20%2895%25%0ACI%3A%200.852%2C%200.875%29%20on%20the%20CMMD%20dataset%20and%200.889%20%2895%25%20CI%3A%200.875%2C%200.903%29%20on%20the%0AINbreast%20dataset%2C%20highlighting%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19888v1&entry.124074799=Read"},
{"title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic\n  Dataset Curation", "author": "Thomas Kerdreux and Alexandre Tuel and Quentin Febvre and Alexis Mouche and Bertrand Chapron", "abstract": "  Self-supervised learning (SSL) has enabled the development of vision\nfoundation models for Earth Observation (EO), demonstrating strong\ntransferability across diverse remote sensing tasks. While prior work has\nfocused on network architectures and training strategies, the role of dataset\ncuration, especially in balancing and diversifying pre-training datasets,\nremains underexplored. In EO, this challenge is amplified by the redundancy and\nheavy-tailed distributions common in satellite imagery, which can lead to\nbiased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\nmethod iteratively refines the training set without requiring a pre-existing\nfeature extractor, making it well-suited for domains where curated datasets are\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\nocean observations. We train models from scratch on the entire Sentinel-1 WV\narchive spanning 10 years. Across three downstream tasks, our results show that\ndynamic pruning improves both computational efficiency and representation\nquality, leading to stronger transferability.\n  We also release the weights of OceanSAR-1, the first model in the OceanSAR\nfamily, a series of foundation models for ocean observation and analysis using\nSAR imagery, at github.com/galeio-research/OceanSAR-models/.\n", "link": "http://arxiv.org/abs/2504.06962v2", "date": "2025-04-28", "relevancy": 2.5302, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Self-Supervised%20Learning%20for%20Earth%20Observation%20via%20Dynamic%0A%20%20Dataset%20Curation&body=Title%3A%20Efficient%20Self-Supervised%20Learning%20for%20Earth%20Observation%20via%20Dynamic%0A%20%20Dataset%20Curation%0AAuthor%3A%20Thomas%20Kerdreux%20and%20Alexandre%20Tuel%20and%20Quentin%20Febvre%20and%20Alexis%20Mouche%20and%20Bertrand%20Chapron%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20enabled%20the%20development%20of%20vision%0Afoundation%20models%20for%20Earth%20Observation%20%28EO%29%2C%20demonstrating%20strong%0Atransferability%20across%20diverse%20remote%20sensing%20tasks.%20While%20prior%20work%20has%0Afocused%20on%20network%20architectures%20and%20training%20strategies%2C%20the%20role%20of%20dataset%0Acuration%2C%20especially%20in%20balancing%20and%20diversifying%20pre-training%20datasets%2C%0Aremains%20underexplored.%20In%20EO%2C%20this%20challenge%20is%20amplified%20by%20the%20redundancy%20and%0Aheavy-tailed%20distributions%20common%20in%20satellite%20imagery%2C%20which%20can%20lead%20to%0Abiased%20representations%20and%20inefficient%20training.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20dynamic%20dataset%20pruning%20strategy%20designed%20to%0Aimprove%20SSL%20pre-training%20by%20maximizing%20dataset%20diversity%20and%20balance.%20Our%0Amethod%20iteratively%20refines%20the%20training%20set%20without%20requiring%20a%20pre-existing%0Afeature%20extractor%2C%20making%20it%20well-suited%20for%20domains%20where%20curated%20datasets%20are%0Alimited%20or%20unavailable.%20We%20demonstrate%20our%20approach%20on%20the%20Sentinel-1%20Wave%20Mode%0A%28WV%29%20Synthetic%20Aperture%20Radar%20%28SAR%29%20archive%2C%20a%20challenging%20dataset%20dominated%20by%0Aocean%20observations.%20We%20train%20models%20from%20scratch%20on%20the%20entire%20Sentinel-1%20WV%0Aarchive%20spanning%2010%20years.%20Across%20three%20downstream%20tasks%2C%20our%20results%20show%20that%0Adynamic%20pruning%20improves%20both%20computational%20efficiency%20and%20representation%0Aquality%2C%20leading%20to%20stronger%20transferability.%0A%20%20We%20also%20release%20the%20weights%20of%20OceanSAR-1%2C%20the%20first%20model%20in%20the%20OceanSAR%0Afamily%2C%20a%20series%20of%20foundation%20models%20for%20ocean%20observation%20and%20analysis%20using%0ASAR%20imagery%2C%20at%20github.com/galeio-research/OceanSAR-models/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06962v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Self-Supervised%2520Learning%2520for%2520Earth%2520Observation%2520via%2520Dynamic%250A%2520%2520Dataset%2520Curation%26entry.906535625%3DThomas%2520Kerdreux%2520and%2520Alexandre%2520Tuel%2520and%2520Quentin%2520Febvre%2520and%2520Alexis%2520Mouche%2520and%2520Bertrand%2520Chapron%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520enabled%2520the%2520development%2520of%2520vision%250Afoundation%2520models%2520for%2520Earth%2520Observation%2520%2528EO%2529%252C%2520demonstrating%2520strong%250Atransferability%2520across%2520diverse%2520remote%2520sensing%2520tasks.%2520While%2520prior%2520work%2520has%250Afocused%2520on%2520network%2520architectures%2520and%2520training%2520strategies%252C%2520the%2520role%2520of%2520dataset%250Acuration%252C%2520especially%2520in%2520balancing%2520and%2520diversifying%2520pre-training%2520datasets%252C%250Aremains%2520underexplored.%2520In%2520EO%252C%2520this%2520challenge%2520is%2520amplified%2520by%2520the%2520redundancy%2520and%250Aheavy-tailed%2520distributions%2520common%2520in%2520satellite%2520imagery%252C%2520which%2520can%2520lead%2520to%250Abiased%2520representations%2520and%2520inefficient%2520training.%250A%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520dynamic%2520dataset%2520pruning%2520strategy%2520designed%2520to%250Aimprove%2520SSL%2520pre-training%2520by%2520maximizing%2520dataset%2520diversity%2520and%2520balance.%2520Our%250Amethod%2520iteratively%2520refines%2520the%2520training%2520set%2520without%2520requiring%2520a%2520pre-existing%250Afeature%2520extractor%252C%2520making%2520it%2520well-suited%2520for%2520domains%2520where%2520curated%2520datasets%2520are%250Alimited%2520or%2520unavailable.%2520We%2520demonstrate%2520our%2520approach%2520on%2520the%2520Sentinel-1%2520Wave%2520Mode%250A%2528WV%2529%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520archive%252C%2520a%2520challenging%2520dataset%2520dominated%2520by%250Aocean%2520observations.%2520We%2520train%2520models%2520from%2520scratch%2520on%2520the%2520entire%2520Sentinel-1%2520WV%250Aarchive%2520spanning%252010%2520years.%2520Across%2520three%2520downstream%2520tasks%252C%2520our%2520results%2520show%2520that%250Adynamic%2520pruning%2520improves%2520both%2520computational%2520efficiency%2520and%2520representation%250Aquality%252C%2520leading%2520to%2520stronger%2520transferability.%250A%2520%2520We%2520also%2520release%2520the%2520weights%2520of%2520OceanSAR-1%252C%2520the%2520first%2520model%2520in%2520the%2520OceanSAR%250Afamily%252C%2520a%2520series%2520of%2520foundation%2520models%2520for%2520ocean%2520observation%2520and%2520analysis%2520using%250ASAR%2520imagery%252C%2520at%2520github.com/galeio-research/OceanSAR-models/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06962v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Self-Supervised%20Learning%20for%20Earth%20Observation%20via%20Dynamic%0A%20%20Dataset%20Curation&entry.906535625=Thomas%20Kerdreux%20and%20Alexandre%20Tuel%20and%20Quentin%20Febvre%20and%20Alexis%20Mouche%20and%20Bertrand%20Chapron&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20enabled%20the%20development%20of%20vision%0Afoundation%20models%20for%20Earth%20Observation%20%28EO%29%2C%20demonstrating%20strong%0Atransferability%20across%20diverse%20remote%20sensing%20tasks.%20While%20prior%20work%20has%0Afocused%20on%20network%20architectures%20and%20training%20strategies%2C%20the%20role%20of%20dataset%0Acuration%2C%20especially%20in%20balancing%20and%20diversifying%20pre-training%20datasets%2C%0Aremains%20underexplored.%20In%20EO%2C%20this%20challenge%20is%20amplified%20by%20the%20redundancy%20and%0Aheavy-tailed%20distributions%20common%20in%20satellite%20imagery%2C%20which%20can%20lead%20to%0Abiased%20representations%20and%20inefficient%20training.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20dynamic%20dataset%20pruning%20strategy%20designed%20to%0Aimprove%20SSL%20pre-training%20by%20maximizing%20dataset%20diversity%20and%20balance.%20Our%0Amethod%20iteratively%20refines%20the%20training%20set%20without%20requiring%20a%20pre-existing%0Afeature%20extractor%2C%20making%20it%20well-suited%20for%20domains%20where%20curated%20datasets%20are%0Alimited%20or%20unavailable.%20We%20demonstrate%20our%20approach%20on%20the%20Sentinel-1%20Wave%20Mode%0A%28WV%29%20Synthetic%20Aperture%20Radar%20%28SAR%29%20archive%2C%20a%20challenging%20dataset%20dominated%20by%0Aocean%20observations.%20We%20train%20models%20from%20scratch%20on%20the%20entire%20Sentinel-1%20WV%0Aarchive%20spanning%2010%20years.%20Across%20three%20downstream%20tasks%2C%20our%20results%20show%20that%0Adynamic%20pruning%20improves%20both%20computational%20efficiency%20and%20representation%0Aquality%2C%20leading%20to%20stronger%20transferability.%0A%20%20We%20also%20release%20the%20weights%20of%20OceanSAR-1%2C%20the%20first%20model%20in%20the%20OceanSAR%0Afamily%2C%20a%20series%20of%20foundation%20models%20for%20ocean%20observation%20and%20analysis%20using%0ASAR%20imagery%2C%20at%20github.com/galeio-research/OceanSAR-models/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06962v2&entry.124074799=Read"},
{"title": "Accelerating Mixture-of-Experts Training with Adaptive Expert\n  Replication", "author": "Athinagoras Skiadopoulos and Mark Zhao and Swapnil Gandhi and Thomas Norrie and Shrijeet Mukherjee and Christos Kozyrakis", "abstract": "  Mixture-of-Experts (MoE) models have become a widely adopted solution to\ncontinue scaling model sizes without a corresponding linear increase in\ncompute. During MoE model training, each input token is dynamically routed to a\nsubset of experts -- sparsely-activated feed-forward networks -- within each\ntransformer layer. The distribution of tokens assigned to each expert varies\nwidely and rapidly over the course of training. To handle the wide load\nimbalance across experts, current systems are forced to either drop tokens\nassigned to popular experts, degrading convergence, or frequently rebalance\nresources allocated to each expert based on popularity, incurring high state\nmigration overheads.\n  To break this performance-accuracy tradeoff, we introduce SwiftMoE, an\nadaptive MoE training system. The key insight of SwiftMoE is to decouple the\nplacement of expert parameters from their large optimizer state. SwiftMoE\nstatically partitions the optimizer of each expert across all training nodes.\nMeanwhile, SwiftMoE dynamically adjusts the placement of expert parameters by\nrepurposing existing weight updates, avoiding migration overheads. In doing so,\nSwiftMoE right-sizes the GPU resources allocated to each expert, on a\nper-iteration basis, with minimal overheads. Compared to state-of-the-art MoE\ntraining systems, DeepSpeed and FlexMoE, SwiftMoE is able to achieve a 30.5%\nand 25.9% faster time-to-convergence, respectively.\n", "link": "http://arxiv.org/abs/2504.19925v1", "date": "2025-04-28", "relevancy": 2.5, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5193}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4983}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Mixture-of-Experts%20Training%20with%20Adaptive%20Expert%0A%20%20Replication&body=Title%3A%20Accelerating%20Mixture-of-Experts%20Training%20with%20Adaptive%20Expert%0A%20%20Replication%0AAuthor%3A%20Athinagoras%20Skiadopoulos%20and%20Mark%20Zhao%20and%20Swapnil%20Gandhi%20and%20Thomas%20Norrie%20and%20Shrijeet%20Mukherjee%20and%20Christos%20Kozyrakis%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20models%20have%20become%20a%20widely%20adopted%20solution%20to%0Acontinue%20scaling%20model%20sizes%20without%20a%20corresponding%20linear%20increase%20in%0Acompute.%20During%20MoE%20model%20training%2C%20each%20input%20token%20is%20dynamically%20routed%20to%20a%0Asubset%20of%20experts%20--%20sparsely-activated%20feed-forward%20networks%20--%20within%20each%0Atransformer%20layer.%20The%20distribution%20of%20tokens%20assigned%20to%20each%20expert%20varies%0Awidely%20and%20rapidly%20over%20the%20course%20of%20training.%20To%20handle%20the%20wide%20load%0Aimbalance%20across%20experts%2C%20current%20systems%20are%20forced%20to%20either%20drop%20tokens%0Aassigned%20to%20popular%20experts%2C%20degrading%20convergence%2C%20or%20frequently%20rebalance%0Aresources%20allocated%20to%20each%20expert%20based%20on%20popularity%2C%20incurring%20high%20state%0Amigration%20overheads.%0A%20%20To%20break%20this%20performance-accuracy%20tradeoff%2C%20we%20introduce%20SwiftMoE%2C%20an%0Aadaptive%20MoE%20training%20system.%20The%20key%20insight%20of%20SwiftMoE%20is%20to%20decouple%20the%0Aplacement%20of%20expert%20parameters%20from%20their%20large%20optimizer%20state.%20SwiftMoE%0Astatically%20partitions%20the%20optimizer%20of%20each%20expert%20across%20all%20training%20nodes.%0AMeanwhile%2C%20SwiftMoE%20dynamically%20adjusts%20the%20placement%20of%20expert%20parameters%20by%0Arepurposing%20existing%20weight%20updates%2C%20avoiding%20migration%20overheads.%20In%20doing%20so%2C%0ASwiftMoE%20right-sizes%20the%20GPU%20resources%20allocated%20to%20each%20expert%2C%20on%20a%0Aper-iteration%20basis%2C%20with%20minimal%20overheads.%20Compared%20to%20state-of-the-art%20MoE%0Atraining%20systems%2C%20DeepSpeed%20and%20FlexMoE%2C%20SwiftMoE%20is%20able%20to%20achieve%20a%2030.5%25%0Aand%2025.9%25%20faster%20time-to-convergence%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Mixture-of-Experts%2520Training%2520with%2520Adaptive%2520Expert%250A%2520%2520Replication%26entry.906535625%3DAthinagoras%2520Skiadopoulos%2520and%2520Mark%2520Zhao%2520and%2520Swapnil%2520Gandhi%2520and%2520Thomas%2520Norrie%2520and%2520Shrijeet%2520Mukherjee%2520and%2520Christos%2520Kozyrakis%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520models%2520have%2520become%2520a%2520widely%2520adopted%2520solution%2520to%250Acontinue%2520scaling%2520model%2520sizes%2520without%2520a%2520corresponding%2520linear%2520increase%2520in%250Acompute.%2520During%2520MoE%2520model%2520training%252C%2520each%2520input%2520token%2520is%2520dynamically%2520routed%2520to%2520a%250Asubset%2520of%2520experts%2520--%2520sparsely-activated%2520feed-forward%2520networks%2520--%2520within%2520each%250Atransformer%2520layer.%2520The%2520distribution%2520of%2520tokens%2520assigned%2520to%2520each%2520expert%2520varies%250Awidely%2520and%2520rapidly%2520over%2520the%2520course%2520of%2520training.%2520To%2520handle%2520the%2520wide%2520load%250Aimbalance%2520across%2520experts%252C%2520current%2520systems%2520are%2520forced%2520to%2520either%2520drop%2520tokens%250Aassigned%2520to%2520popular%2520experts%252C%2520degrading%2520convergence%252C%2520or%2520frequently%2520rebalance%250Aresources%2520allocated%2520to%2520each%2520expert%2520based%2520on%2520popularity%252C%2520incurring%2520high%2520state%250Amigration%2520overheads.%250A%2520%2520To%2520break%2520this%2520performance-accuracy%2520tradeoff%252C%2520we%2520introduce%2520SwiftMoE%252C%2520an%250Aadaptive%2520MoE%2520training%2520system.%2520The%2520key%2520insight%2520of%2520SwiftMoE%2520is%2520to%2520decouple%2520the%250Aplacement%2520of%2520expert%2520parameters%2520from%2520their%2520large%2520optimizer%2520state.%2520SwiftMoE%250Astatically%2520partitions%2520the%2520optimizer%2520of%2520each%2520expert%2520across%2520all%2520training%2520nodes.%250AMeanwhile%252C%2520SwiftMoE%2520dynamically%2520adjusts%2520the%2520placement%2520of%2520expert%2520parameters%2520by%250Arepurposing%2520existing%2520weight%2520updates%252C%2520avoiding%2520migration%2520overheads.%2520In%2520doing%2520so%252C%250ASwiftMoE%2520right-sizes%2520the%2520GPU%2520resources%2520allocated%2520to%2520each%2520expert%252C%2520on%2520a%250Aper-iteration%2520basis%252C%2520with%2520minimal%2520overheads.%2520Compared%2520to%2520state-of-the-art%2520MoE%250Atraining%2520systems%252C%2520DeepSpeed%2520and%2520FlexMoE%252C%2520SwiftMoE%2520is%2520able%2520to%2520achieve%2520a%252030.5%2525%250Aand%252025.9%2525%2520faster%2520time-to-convergence%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Mixture-of-Experts%20Training%20with%20Adaptive%20Expert%0A%20%20Replication&entry.906535625=Athinagoras%20Skiadopoulos%20and%20Mark%20Zhao%20and%20Swapnil%20Gandhi%20and%20Thomas%20Norrie%20and%20Shrijeet%20Mukherjee%20and%20Christos%20Kozyrakis&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20models%20have%20become%20a%20widely%20adopted%20solution%20to%0Acontinue%20scaling%20model%20sizes%20without%20a%20corresponding%20linear%20increase%20in%0Acompute.%20During%20MoE%20model%20training%2C%20each%20input%20token%20is%20dynamically%20routed%20to%20a%0Asubset%20of%20experts%20--%20sparsely-activated%20feed-forward%20networks%20--%20within%20each%0Atransformer%20layer.%20The%20distribution%20of%20tokens%20assigned%20to%20each%20expert%20varies%0Awidely%20and%20rapidly%20over%20the%20course%20of%20training.%20To%20handle%20the%20wide%20load%0Aimbalance%20across%20experts%2C%20current%20systems%20are%20forced%20to%20either%20drop%20tokens%0Aassigned%20to%20popular%20experts%2C%20degrading%20convergence%2C%20or%20frequently%20rebalance%0Aresources%20allocated%20to%20each%20expert%20based%20on%20popularity%2C%20incurring%20high%20state%0Amigration%20overheads.%0A%20%20To%20break%20this%20performance-accuracy%20tradeoff%2C%20we%20introduce%20SwiftMoE%2C%20an%0Aadaptive%20MoE%20training%20system.%20The%20key%20insight%20of%20SwiftMoE%20is%20to%20decouple%20the%0Aplacement%20of%20expert%20parameters%20from%20their%20large%20optimizer%20state.%20SwiftMoE%0Astatically%20partitions%20the%20optimizer%20of%20each%20expert%20across%20all%20training%20nodes.%0AMeanwhile%2C%20SwiftMoE%20dynamically%20adjusts%20the%20placement%20of%20expert%20parameters%20by%0Arepurposing%20existing%20weight%20updates%2C%20avoiding%20migration%20overheads.%20In%20doing%20so%2C%0ASwiftMoE%20right-sizes%20the%20GPU%20resources%20allocated%20to%20each%20expert%2C%20on%20a%0Aper-iteration%20basis%2C%20with%20minimal%20overheads.%20Compared%20to%20state-of-the-art%20MoE%0Atraining%20systems%2C%20DeepSpeed%20and%20FlexMoE%2C%20SwiftMoE%20is%20able%20to%20achieve%20a%2030.5%25%0Aand%2025.9%25%20faster%20time-to-convergence%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19925v1&entry.124074799=Read"},
{"title": "Generative AI in Education: Student Skills and Lecturer Roles", "author": "Stefanie Krause and Ashish Dalvi and Syed Khubaib Zaidi", "abstract": "  Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging\nas a revolutionary tool in education that brings both positive aspects and\nchallenges for educators and students, reshaping how learning and teaching are\napproached. This study aims to identify and evaluate the key competencies\nstudents need to effectively engage with GenAI in education and to provide\nstrategies for lecturers to integrate GenAI into teaching practices. The study\napplied a mixed method approach with a combination of a literature review and a\nquantitative survey involving 130 students from South Asia and Europe to obtain\nits findings. The literature review identified 14 essential student skills for\nGenAI engagement, with AI literacy, critical thinking, and ethical AI practices\nemerging as the most critical. The student survey revealed gaps in prompt\nengineering, bias awareness, and AI output management. In our study of lecturer\nstrategies, we identified six key areas, with GenAI Integration and Curriculum\nDesign being the most emphasised. Our findings highlight the importance of\nincorporating GenAI into education. While literature prioritized ethics and\npolicy development, students favour hands-on, project-based learning and\npractical AI applications. To foster inclusive and responsible GenAI adoption,\ninstitutions should ensure equitable access to GenAI tools, establish clear\nacademic integrity policies, and advocate for global GenAI research\ninitiatives.\n", "link": "http://arxiv.org/abs/2504.19673v1", "date": "2025-04-28", "relevancy": 2.4844, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5485}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4947}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Education%3A%20Student%20Skills%20and%20Lecturer%20Roles&body=Title%3A%20Generative%20AI%20in%20Education%3A%20Student%20Skills%20and%20Lecturer%20Roles%0AAuthor%3A%20Stefanie%20Krause%20and%20Ashish%20Dalvi%20and%20Syed%20Khubaib%20Zaidi%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20tools%20such%20as%20ChatGPT%20are%20emerging%0Aas%20a%20revolutionary%20tool%20in%20education%20that%20brings%20both%20positive%20aspects%20and%0Achallenges%20for%20educators%20and%20students%2C%20reshaping%20how%20learning%20and%20teaching%20are%0Aapproached.%20This%20study%20aims%20to%20identify%20and%20evaluate%20the%20key%20competencies%0Astudents%20need%20to%20effectively%20engage%20with%20GenAI%20in%20education%20and%20to%20provide%0Astrategies%20for%20lecturers%20to%20integrate%20GenAI%20into%20teaching%20practices.%20The%20study%0Aapplied%20a%20mixed%20method%20approach%20with%20a%20combination%20of%20a%20literature%20review%20and%20a%0Aquantitative%20survey%20involving%20130%20students%20from%20South%20Asia%20and%20Europe%20to%20obtain%0Aits%20findings.%20The%20literature%20review%20identified%2014%20essential%20student%20skills%20for%0AGenAI%20engagement%2C%20with%20AI%20literacy%2C%20critical%20thinking%2C%20and%20ethical%20AI%20practices%0Aemerging%20as%20the%20most%20critical.%20The%20student%20survey%20revealed%20gaps%20in%20prompt%0Aengineering%2C%20bias%20awareness%2C%20and%20AI%20output%20management.%20In%20our%20study%20of%20lecturer%0Astrategies%2C%20we%20identified%20six%20key%20areas%2C%20with%20GenAI%20Integration%20and%20Curriculum%0ADesign%20being%20the%20most%20emphasised.%20Our%20findings%20highlight%20the%20importance%20of%0Aincorporating%20GenAI%20into%20education.%20While%20literature%20prioritized%20ethics%20and%0Apolicy%20development%2C%20students%20favour%20hands-on%2C%20project-based%20learning%20and%0Apractical%20AI%20applications.%20To%20foster%20inclusive%20and%20responsible%20GenAI%20adoption%2C%0Ainstitutions%20should%20ensure%20equitable%20access%20to%20GenAI%20tools%2C%20establish%20clear%0Aacademic%20integrity%20policies%2C%20and%20advocate%20for%20global%20GenAI%20research%0Ainitiatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Education%253A%2520Student%2520Skills%2520and%2520Lecturer%2520Roles%26entry.906535625%3DStefanie%2520Krause%2520and%2520Ashish%2520Dalvi%2520and%2520Syed%2520Khubaib%2520Zaidi%26entry.1292438233%3D%2520%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520tools%2520such%2520as%2520ChatGPT%2520are%2520emerging%250Aas%2520a%2520revolutionary%2520tool%2520in%2520education%2520that%2520brings%2520both%2520positive%2520aspects%2520and%250Achallenges%2520for%2520educators%2520and%2520students%252C%2520reshaping%2520how%2520learning%2520and%2520teaching%2520are%250Aapproached.%2520This%2520study%2520aims%2520to%2520identify%2520and%2520evaluate%2520the%2520key%2520competencies%250Astudents%2520need%2520to%2520effectively%2520engage%2520with%2520GenAI%2520in%2520education%2520and%2520to%2520provide%250Astrategies%2520for%2520lecturers%2520to%2520integrate%2520GenAI%2520into%2520teaching%2520practices.%2520The%2520study%250Aapplied%2520a%2520mixed%2520method%2520approach%2520with%2520a%2520combination%2520of%2520a%2520literature%2520review%2520and%2520a%250Aquantitative%2520survey%2520involving%2520130%2520students%2520from%2520South%2520Asia%2520and%2520Europe%2520to%2520obtain%250Aits%2520findings.%2520The%2520literature%2520review%2520identified%252014%2520essential%2520student%2520skills%2520for%250AGenAI%2520engagement%252C%2520with%2520AI%2520literacy%252C%2520critical%2520thinking%252C%2520and%2520ethical%2520AI%2520practices%250Aemerging%2520as%2520the%2520most%2520critical.%2520The%2520student%2520survey%2520revealed%2520gaps%2520in%2520prompt%250Aengineering%252C%2520bias%2520awareness%252C%2520and%2520AI%2520output%2520management.%2520In%2520our%2520study%2520of%2520lecturer%250Astrategies%252C%2520we%2520identified%2520six%2520key%2520areas%252C%2520with%2520GenAI%2520Integration%2520and%2520Curriculum%250ADesign%2520being%2520the%2520most%2520emphasised.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%250Aincorporating%2520GenAI%2520into%2520education.%2520While%2520literature%2520prioritized%2520ethics%2520and%250Apolicy%2520development%252C%2520students%2520favour%2520hands-on%252C%2520project-based%2520learning%2520and%250Apractical%2520AI%2520applications.%2520To%2520foster%2520inclusive%2520and%2520responsible%2520GenAI%2520adoption%252C%250Ainstitutions%2520should%2520ensure%2520equitable%2520access%2520to%2520GenAI%2520tools%252C%2520establish%2520clear%250Aacademic%2520integrity%2520policies%252C%2520and%2520advocate%2520for%2520global%2520GenAI%2520research%250Ainitiatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Education%3A%20Student%20Skills%20and%20Lecturer%20Roles&entry.906535625=Stefanie%20Krause%20and%20Ashish%20Dalvi%20and%20Syed%20Khubaib%20Zaidi&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20tools%20such%20as%20ChatGPT%20are%20emerging%0Aas%20a%20revolutionary%20tool%20in%20education%20that%20brings%20both%20positive%20aspects%20and%0Achallenges%20for%20educators%20and%20students%2C%20reshaping%20how%20learning%20and%20teaching%20are%0Aapproached.%20This%20study%20aims%20to%20identify%20and%20evaluate%20the%20key%20competencies%0Astudents%20need%20to%20effectively%20engage%20with%20GenAI%20in%20education%20and%20to%20provide%0Astrategies%20for%20lecturers%20to%20integrate%20GenAI%20into%20teaching%20practices.%20The%20study%0Aapplied%20a%20mixed%20method%20approach%20with%20a%20combination%20of%20a%20literature%20review%20and%20a%0Aquantitative%20survey%20involving%20130%20students%20from%20South%20Asia%20and%20Europe%20to%20obtain%0Aits%20findings.%20The%20literature%20review%20identified%2014%20essential%20student%20skills%20for%0AGenAI%20engagement%2C%20with%20AI%20literacy%2C%20critical%20thinking%2C%20and%20ethical%20AI%20practices%0Aemerging%20as%20the%20most%20critical.%20The%20student%20survey%20revealed%20gaps%20in%20prompt%0Aengineering%2C%20bias%20awareness%2C%20and%20AI%20output%20management.%20In%20our%20study%20of%20lecturer%0Astrategies%2C%20we%20identified%20six%20key%20areas%2C%20with%20GenAI%20Integration%20and%20Curriculum%0ADesign%20being%20the%20most%20emphasised.%20Our%20findings%20highlight%20the%20importance%20of%0Aincorporating%20GenAI%20into%20education.%20While%20literature%20prioritized%20ethics%20and%0Apolicy%20development%2C%20students%20favour%20hands-on%2C%20project-based%20learning%20and%0Apractical%20AI%20applications.%20To%20foster%20inclusive%20and%20responsible%20GenAI%20adoption%2C%0Ainstitutions%20should%20ensure%20equitable%20access%20to%20GenAI%20tools%2C%20establish%20clear%0Aacademic%20integrity%20policies%2C%20and%20advocate%20for%20global%20GenAI%20research%0Ainitiatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19673v1&entry.124074799=Read"},
{"title": "Using Fixed and Mobile Eye Tracking to Understand How Visitors View Art\n  in a Museum: A Study at the Bowes Museum, County Durham, UK", "author": "Claire Warwick and Andrew Beresford and Soazig Casteau and Hubert P. H. Shum and Dan Smith and Francis Xiatian Zhang", "abstract": "  The following paper describes a collaborative project involving researchers\nat Durham University, and professionals at the Bowes Museum, Barnard Castle,\nCounty Durham, UK, during which we used fixed and mobile eye tracking to\nunderstand how visitors view art. Our study took place during summer 2024 and\nbuilds on work presented at DH2017 (Bailey-Ross et al., 2017). Our\ninterdisciplinary team included researchers from digital humanities,\npsychology, art history and computer science, working in collaboration with\nprofessionals from the museum. We used fixed and mobile eye tracking to\nunderstand how museum visitors view art in a physical gallery setting. This\nresearch will enable us to make recommendations about how the Museum's\ncollections could be more effectively displayed, encouraging visitors to engage\nwith them more fully.\n", "link": "http://arxiv.org/abs/2504.19881v1", "date": "2025-04-28", "relevancy": 2.4823, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4965}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Fixed%20and%20Mobile%20Eye%20Tracking%20to%20Understand%20How%20Visitors%20View%20Art%0A%20%20in%20a%20Museum%3A%20A%20Study%20at%20the%20Bowes%20Museum%2C%20County%20Durham%2C%20UK&body=Title%3A%20Using%20Fixed%20and%20Mobile%20Eye%20Tracking%20to%20Understand%20How%20Visitors%20View%20Art%0A%20%20in%20a%20Museum%3A%20A%20Study%20at%20the%20Bowes%20Museum%2C%20County%20Durham%2C%20UK%0AAuthor%3A%20Claire%20Warwick%20and%20Andrew%20Beresford%20and%20Soazig%20Casteau%20and%20Hubert%20P.%20H.%20Shum%20and%20Dan%20Smith%20and%20Francis%20Xiatian%20Zhang%0AAbstract%3A%20%20%20The%20following%20paper%20describes%20a%20collaborative%20project%20involving%20researchers%0Aat%20Durham%20University%2C%20and%20professionals%20at%20the%20Bowes%20Museum%2C%20Barnard%20Castle%2C%0ACounty%20Durham%2C%20UK%2C%20during%20which%20we%20used%20fixed%20and%20mobile%20eye%20tracking%20to%0Aunderstand%20how%20visitors%20view%20art.%20Our%20study%20took%20place%20during%20summer%202024%20and%0Abuilds%20on%20work%20presented%20at%20DH2017%20%28Bailey-Ross%20et%20al.%2C%202017%29.%20Our%0Ainterdisciplinary%20team%20included%20researchers%20from%20digital%20humanities%2C%0Apsychology%2C%20art%20history%20and%20computer%20science%2C%20working%20in%20collaboration%20with%0Aprofessionals%20from%20the%20museum.%20We%20used%20fixed%20and%20mobile%20eye%20tracking%20to%0Aunderstand%20how%20museum%20visitors%20view%20art%20in%20a%20physical%20gallery%20setting.%20This%0Aresearch%20will%20enable%20us%20to%20make%20recommendations%20about%20how%20the%20Museum%27s%0Acollections%20could%20be%20more%20effectively%20displayed%2C%20encouraging%20visitors%20to%20engage%0Awith%20them%20more%20fully.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Fixed%2520and%2520Mobile%2520Eye%2520Tracking%2520to%2520Understand%2520How%2520Visitors%2520View%2520Art%250A%2520%2520in%2520a%2520Museum%253A%2520A%2520Study%2520at%2520the%2520Bowes%2520Museum%252C%2520County%2520Durham%252C%2520UK%26entry.906535625%3DClaire%2520Warwick%2520and%2520Andrew%2520Beresford%2520and%2520Soazig%2520Casteau%2520and%2520Hubert%2520P.%2520H.%2520Shum%2520and%2520Dan%2520Smith%2520and%2520Francis%2520Xiatian%2520Zhang%26entry.1292438233%3D%2520%2520The%2520following%2520paper%2520describes%2520a%2520collaborative%2520project%2520involving%2520researchers%250Aat%2520Durham%2520University%252C%2520and%2520professionals%2520at%2520the%2520Bowes%2520Museum%252C%2520Barnard%2520Castle%252C%250ACounty%2520Durham%252C%2520UK%252C%2520during%2520which%2520we%2520used%2520fixed%2520and%2520mobile%2520eye%2520tracking%2520to%250Aunderstand%2520how%2520visitors%2520view%2520art.%2520Our%2520study%2520took%2520place%2520during%2520summer%25202024%2520and%250Abuilds%2520on%2520work%2520presented%2520at%2520DH2017%2520%2528Bailey-Ross%2520et%2520al.%252C%25202017%2529.%2520Our%250Ainterdisciplinary%2520team%2520included%2520researchers%2520from%2520digital%2520humanities%252C%250Apsychology%252C%2520art%2520history%2520and%2520computer%2520science%252C%2520working%2520in%2520collaboration%2520with%250Aprofessionals%2520from%2520the%2520museum.%2520We%2520used%2520fixed%2520and%2520mobile%2520eye%2520tracking%2520to%250Aunderstand%2520how%2520museum%2520visitors%2520view%2520art%2520in%2520a%2520physical%2520gallery%2520setting.%2520This%250Aresearch%2520will%2520enable%2520us%2520to%2520make%2520recommendations%2520about%2520how%2520the%2520Museum%2527s%250Acollections%2520could%2520be%2520more%2520effectively%2520displayed%252C%2520encouraging%2520visitors%2520to%2520engage%250Awith%2520them%2520more%2520fully.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Fixed%20and%20Mobile%20Eye%20Tracking%20to%20Understand%20How%20Visitors%20View%20Art%0A%20%20in%20a%20Museum%3A%20A%20Study%20at%20the%20Bowes%20Museum%2C%20County%20Durham%2C%20UK&entry.906535625=Claire%20Warwick%20and%20Andrew%20Beresford%20and%20Soazig%20Casteau%20and%20Hubert%20P.%20H.%20Shum%20and%20Dan%20Smith%20and%20Francis%20Xiatian%20Zhang&entry.1292438233=%20%20The%20following%20paper%20describes%20a%20collaborative%20project%20involving%20researchers%0Aat%20Durham%20University%2C%20and%20professionals%20at%20the%20Bowes%20Museum%2C%20Barnard%20Castle%2C%0ACounty%20Durham%2C%20UK%2C%20during%20which%20we%20used%20fixed%20and%20mobile%20eye%20tracking%20to%0Aunderstand%20how%20visitors%20view%20art.%20Our%20study%20took%20place%20during%20summer%202024%20and%0Abuilds%20on%20work%20presented%20at%20DH2017%20%28Bailey-Ross%20et%20al.%2C%202017%29.%20Our%0Ainterdisciplinary%20team%20included%20researchers%20from%20digital%20humanities%2C%0Apsychology%2C%20art%20history%20and%20computer%20science%2C%20working%20in%20collaboration%20with%0Aprofessionals%20from%20the%20museum.%20We%20used%20fixed%20and%20mobile%20eye%20tracking%20to%0Aunderstand%20how%20museum%20visitors%20view%20art%20in%20a%20physical%20gallery%20setting.%20This%0Aresearch%20will%20enable%20us%20to%20make%20recommendations%20about%20how%20the%20Museum%27s%0Acollections%20could%20be%20more%20effectively%20displayed%2C%20encouraging%20visitors%20to%20engage%0Awith%20them%20more%20fully.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19881v1&entry.124074799=Read"},
{"title": "Do You Know the Way? Human-in-the-Loop Understanding for Fast\n  Traversability Estimation in Mobile Robotics", "author": "Andre Schreiber and Katherine Driggs-Campbell", "abstract": "  The increasing use of robots in unstructured environments necessitates the\ndevelopment of effective perception and navigation strategies to enable field\nrobots to successfully perform their tasks. In particular, it is key for such\nrobots to understand where in their environment they can and cannot travel -- a\ntask known as traversability estimation. However, existing geometric approaches\nto traversability estimation may fail to capture nuanced representations of\ntraversability, whereas vision-based approaches typically either involve\nmanually annotating a large number of images or require robot experience. In\naddition, existing methods can struggle to address domain shifts as they\ntypically do not learn during deployment. To this end, we propose a\nhuman-in-the-loop (HiL) method for traversability estimation that prompts a\nhuman for annotations as-needed. Our method uses a foundation model to enable\nrapid learning on new annotations and to provide accurate predictions even when\ntrained on a small number of quickly-provided HiL annotations. We extensively\nvalidate our method in simulation and on real-world data, and demonstrate that\nit can provide state-of-the-art traversability prediction performance.\n", "link": "http://arxiv.org/abs/2504.19851v1", "date": "2025-04-28", "relevancy": 2.4688, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6573}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6168}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20You%20Know%20the%20Way%3F%20Human-in-the-Loop%20Understanding%20for%20Fast%0A%20%20Traversability%20Estimation%20in%20Mobile%20Robotics&body=Title%3A%20Do%20You%20Know%20the%20Way%3F%20Human-in-the-Loop%20Understanding%20for%20Fast%0A%20%20Traversability%20Estimation%20in%20Mobile%20Robotics%0AAuthor%3A%20Andre%20Schreiber%20and%20Katherine%20Driggs-Campbell%0AAbstract%3A%20%20%20The%20increasing%20use%20of%20robots%20in%20unstructured%20environments%20necessitates%20the%0Adevelopment%20of%20effective%20perception%20and%20navigation%20strategies%20to%20enable%20field%0Arobots%20to%20successfully%20perform%20their%20tasks.%20In%20particular%2C%20it%20is%20key%20for%20such%0Arobots%20to%20understand%20where%20in%20their%20environment%20they%20can%20and%20cannot%20travel%20--%20a%0Atask%20known%20as%20traversability%20estimation.%20However%2C%20existing%20geometric%20approaches%0Ato%20traversability%20estimation%20may%20fail%20to%20capture%20nuanced%20representations%20of%0Atraversability%2C%20whereas%20vision-based%20approaches%20typically%20either%20involve%0Amanually%20annotating%20a%20large%20number%20of%20images%20or%20require%20robot%20experience.%20In%0Aaddition%2C%20existing%20methods%20can%20struggle%20to%20address%20domain%20shifts%20as%20they%0Atypically%20do%20not%20learn%20during%20deployment.%20To%20this%20end%2C%20we%20propose%20a%0Ahuman-in-the-loop%20%28HiL%29%20method%20for%20traversability%20estimation%20that%20prompts%20a%0Ahuman%20for%20annotations%20as-needed.%20Our%20method%20uses%20a%20foundation%20model%20to%20enable%0Arapid%20learning%20on%20new%20annotations%20and%20to%20provide%20accurate%20predictions%20even%20when%0Atrained%20on%20a%20small%20number%20of%20quickly-provided%20HiL%20annotations.%20We%20extensively%0Avalidate%20our%20method%20in%20simulation%20and%20on%20real-world%20data%2C%20and%20demonstrate%20that%0Ait%20can%20provide%20state-of-the-art%20traversability%20prediction%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520You%2520Know%2520the%2520Way%253F%2520Human-in-the-Loop%2520Understanding%2520for%2520Fast%250A%2520%2520Traversability%2520Estimation%2520in%2520Mobile%2520Robotics%26entry.906535625%3DAndre%2520Schreiber%2520and%2520Katherine%2520Driggs-Campbell%26entry.1292438233%3D%2520%2520The%2520increasing%2520use%2520of%2520robots%2520in%2520unstructured%2520environments%2520necessitates%2520the%250Adevelopment%2520of%2520effective%2520perception%2520and%2520navigation%2520strategies%2520to%2520enable%2520field%250Arobots%2520to%2520successfully%2520perform%2520their%2520tasks.%2520In%2520particular%252C%2520it%2520is%2520key%2520for%2520such%250Arobots%2520to%2520understand%2520where%2520in%2520their%2520environment%2520they%2520can%2520and%2520cannot%2520travel%2520--%2520a%250Atask%2520known%2520as%2520traversability%2520estimation.%2520However%252C%2520existing%2520geometric%2520approaches%250Ato%2520traversability%2520estimation%2520may%2520fail%2520to%2520capture%2520nuanced%2520representations%2520of%250Atraversability%252C%2520whereas%2520vision-based%2520approaches%2520typically%2520either%2520involve%250Amanually%2520annotating%2520a%2520large%2520number%2520of%2520images%2520or%2520require%2520robot%2520experience.%2520In%250Aaddition%252C%2520existing%2520methods%2520can%2520struggle%2520to%2520address%2520domain%2520shifts%2520as%2520they%250Atypically%2520do%2520not%2520learn%2520during%2520deployment.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Ahuman-in-the-loop%2520%2528HiL%2529%2520method%2520for%2520traversability%2520estimation%2520that%2520prompts%2520a%250Ahuman%2520for%2520annotations%2520as-needed.%2520Our%2520method%2520uses%2520a%2520foundation%2520model%2520to%2520enable%250Arapid%2520learning%2520on%2520new%2520annotations%2520and%2520to%2520provide%2520accurate%2520predictions%2520even%2520when%250Atrained%2520on%2520a%2520small%2520number%2520of%2520quickly-provided%2520HiL%2520annotations.%2520We%2520extensively%250Avalidate%2520our%2520method%2520in%2520simulation%2520and%2520on%2520real-world%2520data%252C%2520and%2520demonstrate%2520that%250Ait%2520can%2520provide%2520state-of-the-art%2520traversability%2520prediction%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20You%20Know%20the%20Way%3F%20Human-in-the-Loop%20Understanding%20for%20Fast%0A%20%20Traversability%20Estimation%20in%20Mobile%20Robotics&entry.906535625=Andre%20Schreiber%20and%20Katherine%20Driggs-Campbell&entry.1292438233=%20%20The%20increasing%20use%20of%20robots%20in%20unstructured%20environments%20necessitates%20the%0Adevelopment%20of%20effective%20perception%20and%20navigation%20strategies%20to%20enable%20field%0Arobots%20to%20successfully%20perform%20their%20tasks.%20In%20particular%2C%20it%20is%20key%20for%20such%0Arobots%20to%20understand%20where%20in%20their%20environment%20they%20can%20and%20cannot%20travel%20--%20a%0Atask%20known%20as%20traversability%20estimation.%20However%2C%20existing%20geometric%20approaches%0Ato%20traversability%20estimation%20may%20fail%20to%20capture%20nuanced%20representations%20of%0Atraversability%2C%20whereas%20vision-based%20approaches%20typically%20either%20involve%0Amanually%20annotating%20a%20large%20number%20of%20images%20or%20require%20robot%20experience.%20In%0Aaddition%2C%20existing%20methods%20can%20struggle%20to%20address%20domain%20shifts%20as%20they%0Atypically%20do%20not%20learn%20during%20deployment.%20To%20this%20end%2C%20we%20propose%20a%0Ahuman-in-the-loop%20%28HiL%29%20method%20for%20traversability%20estimation%20that%20prompts%20a%0Ahuman%20for%20annotations%20as-needed.%20Our%20method%20uses%20a%20foundation%20model%20to%20enable%0Arapid%20learning%20on%20new%20annotations%20and%20to%20provide%20accurate%20predictions%20even%20when%0Atrained%20on%20a%20small%20number%20of%20quickly-provided%20HiL%20annotations.%20We%20extensively%0Avalidate%20our%20method%20in%20simulation%20and%20on%20real-world%20data%2C%20and%20demonstrate%20that%0Ait%20can%20provide%20state-of-the-art%20traversability%20prediction%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19851v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Effective Label-free Node\n  Classification in Text-Attributed Graphs", "author": "Taiyan Zhang and Renchi Yang and Yurui Lai and Mingyu Yan and Xiaochun Ye and Dongrui Fan", "abstract": "  Graph neural networks (GNNs) have become the preferred models for node\nclassification in graph data due to their robust capabilities in integrating\ngraph structures and attributes. However, these models heavily depend on a\nsubstantial amount of high-quality labeled data for training, which is often\ncostly to obtain. With the rise of large language models (LLMs), a promising\napproach is to utilize their exceptional zero-shot capabilities and extensive\nknowledge for node labeling. Despite encouraging results, this approach either\nrequires numerous queries to LLMs or suffers from reduced performance due to\nnoisy labels generated by LLMs. To address these challenges, we introduce\nLocle, an active self-training framework that does Label-free node\nClassification with LLMs cost-Effectively. Locle iteratively identifies small\nsets of \"critical\" samples using GNNs and extracts informative pseudo-labels\nfor them with both LLMs and GNNs, serving as additional supervision signals to\nenhance model training. Specifically, Locle comprises three key components: (i)\nan effective active node selection strategy for initial annotations; (ii) a\ncareful sample selection scheme to identify \"critical\" nodes based on label\ndisharmonicity and entropy; and (iii) a label refinement module that combines\nLLMs and GNNs with a rewired topology. Extensive experiments on five benchmark\ntext-attributed graph datasets demonstrate that Locle significantly outperforms\nstate-of-the-art methods under the same query budget to LLMs in terms of\nlabel-free node classification. Notably, on the DBLP dataset with 14.3k nodes,\nLocle achieves an 8.08% improvement in accuracy over the state-of-the-art at a\ncost of less than one cent. Our code is available at\nhttps://github.com/HKBU-LAGAS/Locle.\n", "link": "http://arxiv.org/abs/2412.11983v2", "date": "2025-04-28", "relevancy": 2.4514, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5259}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4793}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Effective%20Label-free%20Node%0A%20%20Classification%20in%20Text-Attributed%20Graphs&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Effective%20Label-free%20Node%0A%20%20Classification%20in%20Text-Attributed%20Graphs%0AAuthor%3A%20Taiyan%20Zhang%20and%20Renchi%20Yang%20and%20Yurui%20Lai%20and%20Mingyu%20Yan%20and%20Xiaochun%20Ye%20and%20Dongrui%20Fan%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20the%20preferred%20models%20for%20node%0Aclassification%20in%20graph%20data%20due%20to%20their%20robust%20capabilities%20in%20integrating%0Agraph%20structures%20and%20attributes.%20However%2C%20these%20models%20heavily%20depend%20on%20a%0Asubstantial%20amount%20of%20high-quality%20labeled%20data%20for%20training%2C%20which%20is%20often%0Acostly%20to%20obtain.%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20promising%0Aapproach%20is%20to%20utilize%20their%20exceptional%20zero-shot%20capabilities%20and%20extensive%0Aknowledge%20for%20node%20labeling.%20Despite%20encouraging%20results%2C%20this%20approach%20either%0Arequires%20numerous%20queries%20to%20LLMs%20or%20suffers%20from%20reduced%20performance%20due%20to%0Anoisy%20labels%20generated%20by%20LLMs.%20To%20address%20these%20challenges%2C%20we%20introduce%0ALocle%2C%20an%20active%20self-training%20framework%20that%20does%20Label-free%20node%0AClassification%20with%20LLMs%20cost-Effectively.%20Locle%20iteratively%20identifies%20small%0Asets%20of%20%22critical%22%20samples%20using%20GNNs%20and%20extracts%20informative%20pseudo-labels%0Afor%20them%20with%20both%20LLMs%20and%20GNNs%2C%20serving%20as%20additional%20supervision%20signals%20to%0Aenhance%20model%20training.%20Specifically%2C%20Locle%20comprises%20three%20key%20components%3A%20%28i%29%0Aan%20effective%20active%20node%20selection%20strategy%20for%20initial%20annotations%3B%20%28ii%29%20a%0Acareful%20sample%20selection%20scheme%20to%20identify%20%22critical%22%20nodes%20based%20on%20label%0Adisharmonicity%20and%20entropy%3B%20and%20%28iii%29%20a%20label%20refinement%20module%20that%20combines%0ALLMs%20and%20GNNs%20with%20a%20rewired%20topology.%20Extensive%20experiments%20on%20five%20benchmark%0Atext-attributed%20graph%20datasets%20demonstrate%20that%20Locle%20significantly%20outperforms%0Astate-of-the-art%20methods%20under%20the%20same%20query%20budget%20to%20LLMs%20in%20terms%20of%0Alabel-free%20node%20classification.%20Notably%2C%20on%20the%20DBLP%20dataset%20with%2014.3k%20nodes%2C%0ALocle%20achieves%20an%208.08%25%20improvement%20in%20accuracy%20over%20the%20state-of-the-art%20at%20a%0Acost%20of%20less%20than%20one%20cent.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/HKBU-LAGAS/Locle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Effective%2520Label-free%2520Node%250A%2520%2520Classification%2520in%2520Text-Attributed%2520Graphs%26entry.906535625%3DTaiyan%2520Zhang%2520and%2520Renchi%2520Yang%2520and%2520Yurui%2520Lai%2520and%2520Mingyu%2520Yan%2520and%2520Xiaochun%2520Ye%2520and%2520Dongrui%2520Fan%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520the%2520preferred%2520models%2520for%2520node%250Aclassification%2520in%2520graph%2520data%2520due%2520to%2520their%2520robust%2520capabilities%2520in%2520integrating%250Agraph%2520structures%2520and%2520attributes.%2520However%252C%2520these%2520models%2520heavily%2520depend%2520on%2520a%250Asubstantial%2520amount%2520of%2520high-quality%2520labeled%2520data%2520for%2520training%252C%2520which%2520is%2520often%250Acostly%2520to%2520obtain.%2520With%2520the%2520rise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520a%2520promising%250Aapproach%2520is%2520to%2520utilize%2520their%2520exceptional%2520zero-shot%2520capabilities%2520and%2520extensive%250Aknowledge%2520for%2520node%2520labeling.%2520Despite%2520encouraging%2520results%252C%2520this%2520approach%2520either%250Arequires%2520numerous%2520queries%2520to%2520LLMs%2520or%2520suffers%2520from%2520reduced%2520performance%2520due%2520to%250Anoisy%2520labels%2520generated%2520by%2520LLMs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ALocle%252C%2520an%2520active%2520self-training%2520framework%2520that%2520does%2520Label-free%2520node%250AClassification%2520with%2520LLMs%2520cost-Effectively.%2520Locle%2520iteratively%2520identifies%2520small%250Asets%2520of%2520%2522critical%2522%2520samples%2520using%2520GNNs%2520and%2520extracts%2520informative%2520pseudo-labels%250Afor%2520them%2520with%2520both%2520LLMs%2520and%2520GNNs%252C%2520serving%2520as%2520additional%2520supervision%2520signals%2520to%250Aenhance%2520model%2520training.%2520Specifically%252C%2520Locle%2520comprises%2520three%2520key%2520components%253A%2520%2528i%2529%250Aan%2520effective%2520active%2520node%2520selection%2520strategy%2520for%2520initial%2520annotations%253B%2520%2528ii%2529%2520a%250Acareful%2520sample%2520selection%2520scheme%2520to%2520identify%2520%2522critical%2522%2520nodes%2520based%2520on%2520label%250Adisharmonicity%2520and%2520entropy%253B%2520and%2520%2528iii%2529%2520a%2520label%2520refinement%2520module%2520that%2520combines%250ALLMs%2520and%2520GNNs%2520with%2520a%2520rewired%2520topology.%2520Extensive%2520experiments%2520on%2520five%2520benchmark%250Atext-attributed%2520graph%2520datasets%2520demonstrate%2520that%2520Locle%2520significantly%2520outperforms%250Astate-of-the-art%2520methods%2520under%2520the%2520same%2520query%2520budget%2520to%2520LLMs%2520in%2520terms%2520of%250Alabel-free%2520node%2520classification.%2520Notably%252C%2520on%2520the%2520DBLP%2520dataset%2520with%252014.3k%2520nodes%252C%250ALocle%2520achieves%2520an%25208.08%2525%2520improvement%2520in%2520accuracy%2520over%2520the%2520state-of-the-art%2520at%2520a%250Acost%2520of%2520less%2520than%2520one%2520cent.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/HKBU-LAGAS/Locle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Effective%20Label-free%20Node%0A%20%20Classification%20in%20Text-Attributed%20Graphs&entry.906535625=Taiyan%20Zhang%20and%20Renchi%20Yang%20and%20Yurui%20Lai%20and%20Mingyu%20Yan%20and%20Xiaochun%20Ye%20and%20Dongrui%20Fan&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20the%20preferred%20models%20for%20node%0Aclassification%20in%20graph%20data%20due%20to%20their%20robust%20capabilities%20in%20integrating%0Agraph%20structures%20and%20attributes.%20However%2C%20these%20models%20heavily%20depend%20on%20a%0Asubstantial%20amount%20of%20high-quality%20labeled%20data%20for%20training%2C%20which%20is%20often%0Acostly%20to%20obtain.%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20promising%0Aapproach%20is%20to%20utilize%20their%20exceptional%20zero-shot%20capabilities%20and%20extensive%0Aknowledge%20for%20node%20labeling.%20Despite%20encouraging%20results%2C%20this%20approach%20either%0Arequires%20numerous%20queries%20to%20LLMs%20or%20suffers%20from%20reduced%20performance%20due%20to%0Anoisy%20labels%20generated%20by%20LLMs.%20To%20address%20these%20challenges%2C%20we%20introduce%0ALocle%2C%20an%20active%20self-training%20framework%20that%20does%20Label-free%20node%0AClassification%20with%20LLMs%20cost-Effectively.%20Locle%20iteratively%20identifies%20small%0Asets%20of%20%22critical%22%20samples%20using%20GNNs%20and%20extracts%20informative%20pseudo-labels%0Afor%20them%20with%20both%20LLMs%20and%20GNNs%2C%20serving%20as%20additional%20supervision%20signals%20to%0Aenhance%20model%20training.%20Specifically%2C%20Locle%20comprises%20three%20key%20components%3A%20%28i%29%0Aan%20effective%20active%20node%20selection%20strategy%20for%20initial%20annotations%3B%20%28ii%29%20a%0Acareful%20sample%20selection%20scheme%20to%20identify%20%22critical%22%20nodes%20based%20on%20label%0Adisharmonicity%20and%20entropy%3B%20and%20%28iii%29%20a%20label%20refinement%20module%20that%20combines%0ALLMs%20and%20GNNs%20with%20a%20rewired%20topology.%20Extensive%20experiments%20on%20five%20benchmark%0Atext-attributed%20graph%20datasets%20demonstrate%20that%20Locle%20significantly%20outperforms%0Astate-of-the-art%20methods%20under%20the%20same%20query%20budget%20to%20LLMs%20in%20terms%20of%0Alabel-free%20node%20classification.%20Notably%2C%20on%20the%20DBLP%20dataset%20with%2014.3k%20nodes%2C%0ALocle%20achieves%20an%208.08%25%20improvement%20in%20accuracy%20over%20the%20state-of-the-art%20at%20a%0Acost%20of%20less%20than%20one%20cent.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/HKBU-LAGAS/Locle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11983v2&entry.124074799=Read"},
{"title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion", "author": "Zador Pataki and Paul-Edouard Sarlin and Johannes L. Sch\u00f6nberger and Marc Pollefeys", "abstract": "  While Structure-from-Motion (SfM) has seen much progress over the years,\nstate-of-the-art systems are prone to failure when facing extreme viewpoint\nchanges in low-overlap, low-parallax or high-symmetry scenarios. Because\ncapturing images that avoid these pitfalls is challenging, this severely limits\nthe wider use of SfM, especially by non-expert users. We overcome these\nlimitations by augmenting the classical SfM paradigm with monocular depth and\nnormal priors inferred by deep neural networks. Thanks to a tight integration\nof monocular and multi-view constraints, our approach significantly outperforms\nexisting ones under extreme viewpoint changes, while maintaining strong\nperformance in standard conditions. We also show that monocular priors can help\nreject faulty associations due to symmetries, which is a long-standing problem\nfor SfM. This makes our approach the first capable of reliably reconstructing\nchallenging indoor environments from few images. Through principled uncertainty\npropagation, it is robust to errors in the priors, can handle priors inferred\nby different models with little tuning, and will thus easily benefit from\nfuture progress in monocular depth and normal estimation. Our code is publicly\navailable at https://github.com/cvg/mpsfm.\n", "link": "http://arxiv.org/abs/2504.20040v1", "date": "2025-04-28", "relevancy": 2.4349, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6197}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6009}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MP-SfM%3A%20Monocular%20Surface%20Priors%20for%20Robust%20Structure-from-Motion&body=Title%3A%20MP-SfM%3A%20Monocular%20Surface%20Priors%20for%20Robust%20Structure-from-Motion%0AAuthor%3A%20Zador%20Pataki%20and%20Paul-Edouard%20Sarlin%20and%20Johannes%20L.%20Sch%C3%B6nberger%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20While%20Structure-from-Motion%20%28SfM%29%20has%20seen%20much%20progress%20over%20the%20years%2C%0Astate-of-the-art%20systems%20are%20prone%20to%20failure%20when%20facing%20extreme%20viewpoint%0Achanges%20in%20low-overlap%2C%20low-parallax%20or%20high-symmetry%20scenarios.%20Because%0Acapturing%20images%20that%20avoid%20these%20pitfalls%20is%20challenging%2C%20this%20severely%20limits%0Athe%20wider%20use%20of%20SfM%2C%20especially%20by%20non-expert%20users.%20We%20overcome%20these%0Alimitations%20by%20augmenting%20the%20classical%20SfM%20paradigm%20with%20monocular%20depth%20and%0Anormal%20priors%20inferred%20by%20deep%20neural%20networks.%20Thanks%20to%20a%20tight%20integration%0Aof%20monocular%20and%20multi-view%20constraints%2C%20our%20approach%20significantly%20outperforms%0Aexisting%20ones%20under%20extreme%20viewpoint%20changes%2C%20while%20maintaining%20strong%0Aperformance%20in%20standard%20conditions.%20We%20also%20show%20that%20monocular%20priors%20can%20help%0Areject%20faulty%20associations%20due%20to%20symmetries%2C%20which%20is%20a%20long-standing%20problem%0Afor%20SfM.%20This%20makes%20our%20approach%20the%20first%20capable%20of%20reliably%20reconstructing%0Achallenging%20indoor%20environments%20from%20few%20images.%20Through%20principled%20uncertainty%0Apropagation%2C%20it%20is%20robust%20to%20errors%20in%20the%20priors%2C%20can%20handle%20priors%20inferred%0Aby%20different%20models%20with%20little%20tuning%2C%20and%20will%20thus%20easily%20benefit%20from%0Afuture%20progress%20in%20monocular%20depth%20and%20normal%20estimation.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/cvg/mpsfm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMP-SfM%253A%2520Monocular%2520Surface%2520Priors%2520for%2520Robust%2520Structure-from-Motion%26entry.906535625%3DZador%2520Pataki%2520and%2520Paul-Edouard%2520Sarlin%2520and%2520Johannes%2520L.%2520Sch%25C3%25B6nberger%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520While%2520Structure-from-Motion%2520%2528SfM%2529%2520has%2520seen%2520much%2520progress%2520over%2520the%2520years%252C%250Astate-of-the-art%2520systems%2520are%2520prone%2520to%2520failure%2520when%2520facing%2520extreme%2520viewpoint%250Achanges%2520in%2520low-overlap%252C%2520low-parallax%2520or%2520high-symmetry%2520scenarios.%2520Because%250Acapturing%2520images%2520that%2520avoid%2520these%2520pitfalls%2520is%2520challenging%252C%2520this%2520severely%2520limits%250Athe%2520wider%2520use%2520of%2520SfM%252C%2520especially%2520by%2520non-expert%2520users.%2520We%2520overcome%2520these%250Alimitations%2520by%2520augmenting%2520the%2520classical%2520SfM%2520paradigm%2520with%2520monocular%2520depth%2520and%250Anormal%2520priors%2520inferred%2520by%2520deep%2520neural%2520networks.%2520Thanks%2520to%2520a%2520tight%2520integration%250Aof%2520monocular%2520and%2520multi-view%2520constraints%252C%2520our%2520approach%2520significantly%2520outperforms%250Aexisting%2520ones%2520under%2520extreme%2520viewpoint%2520changes%252C%2520while%2520maintaining%2520strong%250Aperformance%2520in%2520standard%2520conditions.%2520We%2520also%2520show%2520that%2520monocular%2520priors%2520can%2520help%250Areject%2520faulty%2520associations%2520due%2520to%2520symmetries%252C%2520which%2520is%2520a%2520long-standing%2520problem%250Afor%2520SfM.%2520This%2520makes%2520our%2520approach%2520the%2520first%2520capable%2520of%2520reliably%2520reconstructing%250Achallenging%2520indoor%2520environments%2520from%2520few%2520images.%2520Through%2520principled%2520uncertainty%250Apropagation%252C%2520it%2520is%2520robust%2520to%2520errors%2520in%2520the%2520priors%252C%2520can%2520handle%2520priors%2520inferred%250Aby%2520different%2520models%2520with%2520little%2520tuning%252C%2520and%2520will%2520thus%2520easily%2520benefit%2520from%250Afuture%2520progress%2520in%2520monocular%2520depth%2520and%2520normal%2520estimation.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/cvg/mpsfm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MP-SfM%3A%20Monocular%20Surface%20Priors%20for%20Robust%20Structure-from-Motion&entry.906535625=Zador%20Pataki%20and%20Paul-Edouard%20Sarlin%20and%20Johannes%20L.%20Sch%C3%B6nberger%20and%20Marc%20Pollefeys&entry.1292438233=%20%20While%20Structure-from-Motion%20%28SfM%29%20has%20seen%20much%20progress%20over%20the%20years%2C%0Astate-of-the-art%20systems%20are%20prone%20to%20failure%20when%20facing%20extreme%20viewpoint%0Achanges%20in%20low-overlap%2C%20low-parallax%20or%20high-symmetry%20scenarios.%20Because%0Acapturing%20images%20that%20avoid%20these%20pitfalls%20is%20challenging%2C%20this%20severely%20limits%0Athe%20wider%20use%20of%20SfM%2C%20especially%20by%20non-expert%20users.%20We%20overcome%20these%0Alimitations%20by%20augmenting%20the%20classical%20SfM%20paradigm%20with%20monocular%20depth%20and%0Anormal%20priors%20inferred%20by%20deep%20neural%20networks.%20Thanks%20to%20a%20tight%20integration%0Aof%20monocular%20and%20multi-view%20constraints%2C%20our%20approach%20significantly%20outperforms%0Aexisting%20ones%20under%20extreme%20viewpoint%20changes%2C%20while%20maintaining%20strong%0Aperformance%20in%20standard%20conditions.%20We%20also%20show%20that%20monocular%20priors%20can%20help%0Areject%20faulty%20associations%20due%20to%20symmetries%2C%20which%20is%20a%20long-standing%20problem%0Afor%20SfM.%20This%20makes%20our%20approach%20the%20first%20capable%20of%20reliably%20reconstructing%0Achallenging%20indoor%20environments%20from%20few%20images.%20Through%20principled%20uncertainty%0Apropagation%2C%20it%20is%20robust%20to%20errors%20in%20the%20priors%2C%20can%20handle%20priors%20inferred%0Aby%20different%20models%20with%20little%20tuning%2C%20and%20will%20thus%20easily%20benefit%20from%0Afuture%20progress%20in%20monocular%20depth%20and%20normal%20estimation.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/cvg/mpsfm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20040v1&entry.124074799=Read"},
{"title": "Contrastive Language-Image Learning with Augmented Textual Prompts for\n  3D/4D FER Using Vision-Language Model", "author": "Muzammil Behzad and Guoying Zhao", "abstract": "  In this paper, we introduce AffectVLM, a vision-language model designed to\nintegrate multiviews for a semantically rich and visually comprehensive\nunderstanding of facial emotions from 3D/4D data. To effectively capture visual\nfeatures, we propose a joint representation learning framework paired with a\nnovel gradient-friendly loss function that accelerates model convergence\ntowards optimal feature representation. Additionally, we introduce augmented\ntextual prompts to enhance the model's linguistic capabilities and employ mixed\nview augmentation to expand the visual dataset. We also develop a Streamlit app\nfor a real-time interactive inference and enable the model for distributed\nlearning. Extensive experiments validate the superior performance of AffectVLM\nacross multiple benchmarks.\n", "link": "http://arxiv.org/abs/2504.19739v1", "date": "2025-04-28", "relevancy": 2.4259, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6172}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Language-Image%20Learning%20with%20Augmented%20Textual%20Prompts%20for%0A%20%203D/4D%20FER%20Using%20Vision-Language%20Model&body=Title%3A%20Contrastive%20Language-Image%20Learning%20with%20Augmented%20Textual%20Prompts%20for%0A%20%203D/4D%20FER%20Using%20Vision-Language%20Model%0AAuthor%3A%20Muzammil%20Behzad%20and%20Guoying%20Zhao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20AffectVLM%2C%20a%20vision-language%20model%20designed%20to%0Aintegrate%20multiviews%20for%20a%20semantically%20rich%20and%20visually%20comprehensive%0Aunderstanding%20of%20facial%20emotions%20from%203D/4D%20data.%20To%20effectively%20capture%20visual%0Afeatures%2C%20we%20propose%20a%20joint%20representation%20learning%20framework%20paired%20with%20a%0Anovel%20gradient-friendly%20loss%20function%20that%20accelerates%20model%20convergence%0Atowards%20optimal%20feature%20representation.%20Additionally%2C%20we%20introduce%20augmented%0Atextual%20prompts%20to%20enhance%20the%20model%27s%20linguistic%20capabilities%20and%20employ%20mixed%0Aview%20augmentation%20to%20expand%20the%20visual%20dataset.%20We%20also%20develop%20a%20Streamlit%20app%0Afor%20a%20real-time%20interactive%20inference%20and%20enable%20the%20model%20for%20distributed%0Alearning.%20Extensive%20experiments%20validate%20the%20superior%20performance%20of%20AffectVLM%0Aacross%20multiple%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Language-Image%2520Learning%2520with%2520Augmented%2520Textual%2520Prompts%2520for%250A%2520%25203D/4D%2520FER%2520Using%2520Vision-Language%2520Model%26entry.906535625%3DMuzammil%2520Behzad%2520and%2520Guoying%2520Zhao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520AffectVLM%252C%2520a%2520vision-language%2520model%2520designed%2520to%250Aintegrate%2520multiviews%2520for%2520a%2520semantically%2520rich%2520and%2520visually%2520comprehensive%250Aunderstanding%2520of%2520facial%2520emotions%2520from%25203D/4D%2520data.%2520To%2520effectively%2520capture%2520visual%250Afeatures%252C%2520we%2520propose%2520a%2520joint%2520representation%2520learning%2520framework%2520paired%2520with%2520a%250Anovel%2520gradient-friendly%2520loss%2520function%2520that%2520accelerates%2520model%2520convergence%250Atowards%2520optimal%2520feature%2520representation.%2520Additionally%252C%2520we%2520introduce%2520augmented%250Atextual%2520prompts%2520to%2520enhance%2520the%2520model%2527s%2520linguistic%2520capabilities%2520and%2520employ%2520mixed%250Aview%2520augmentation%2520to%2520expand%2520the%2520visual%2520dataset.%2520We%2520also%2520develop%2520a%2520Streamlit%2520app%250Afor%2520a%2520real-time%2520interactive%2520inference%2520and%2520enable%2520the%2520model%2520for%2520distributed%250Alearning.%2520Extensive%2520experiments%2520validate%2520the%2520superior%2520performance%2520of%2520AffectVLM%250Aacross%2520multiple%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Language-Image%20Learning%20with%20Augmented%20Textual%20Prompts%20for%0A%20%203D/4D%20FER%20Using%20Vision-Language%20Model&entry.906535625=Muzammil%20Behzad%20and%20Guoying%20Zhao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20AffectVLM%2C%20a%20vision-language%20model%20designed%20to%0Aintegrate%20multiviews%20for%20a%20semantically%20rich%20and%20visually%20comprehensive%0Aunderstanding%20of%20facial%20emotions%20from%203D/4D%20data.%20To%20effectively%20capture%20visual%0Afeatures%2C%20we%20propose%20a%20joint%20representation%20learning%20framework%20paired%20with%20a%0Anovel%20gradient-friendly%20loss%20function%20that%20accelerates%20model%20convergence%0Atowards%20optimal%20feature%20representation.%20Additionally%2C%20we%20introduce%20augmented%0Atextual%20prompts%20to%20enhance%20the%20model%27s%20linguistic%20capabilities%20and%20employ%20mixed%0Aview%20augmentation%20to%20expand%20the%20visual%20dataset.%20We%20also%20develop%20a%20Streamlit%20app%0Afor%20a%20real-time%20interactive%20inference%20and%20enable%20the%20model%20for%20distributed%0Alearning.%20Extensive%20experiments%20validate%20the%20superior%20performance%20of%20AffectVLM%0Aacross%20multiple%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19739v1&entry.124074799=Read"},
{"title": "Joint Optimization of Neural Radiance Fields and Continuous Camera\n  Motion from a Monocular Video", "author": "Hoang Chuong Nguyen and Wei Mao and Jose M. Alvarez and Miaomiao Liu", "abstract": "  Neural Radiance Fields (NeRF) has demonstrated its superior capability to\nrepresent 3D geometry but require accurately precomputed camera poses during\ntraining. To mitigate this requirement, existing methods jointly optimize\ncamera poses and NeRF often relying on good pose initialisation or depth\npriors. However, these approaches struggle in challenging scenarios, such as\nlarge rotations, as they map each camera to a world coordinate system. We\npropose a novel method that eliminates prior dependencies by modeling\ncontinuous camera motions as time-dependent angular velocity and velocity.\nRelative motions between cameras are learned first via velocity integration,\nwhile camera poses can be obtained by aggregating such relative motions up to a\nworld coordinate system defined at a single time step within the video.\nSpecifically, accurate continuous camera movements are learned through a\ntime-dependent NeRF, which captures local scene geometry and motion by training\nfrom neighboring frames for each time step. The learned motions enable\nfine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D\nand Scannet show our approach achieves superior camera pose and depth\nestimation and comparable novel-view synthesis performance compared to\nstate-of-the-art methods. Our code is available at\nhttps://github.com/HoangChuongNguyen/cope-nerf.\n", "link": "http://arxiv.org/abs/2504.19819v1", "date": "2025-04-28", "relevancy": 2.3945, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6691}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5882}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Optimization%20of%20Neural%20Radiance%20Fields%20and%20Continuous%20Camera%0A%20%20Motion%20from%20a%20Monocular%20Video&body=Title%3A%20Joint%20Optimization%20of%20Neural%20Radiance%20Fields%20and%20Continuous%20Camera%0A%20%20Motion%20from%20a%20Monocular%20Video%0AAuthor%3A%20Hoang%20Chuong%20Nguyen%20and%20Wei%20Mao%20and%20Jose%20M.%20Alvarez%20and%20Miaomiao%20Liu%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20has%20demonstrated%20its%20superior%20capability%20to%0Arepresent%203D%20geometry%20but%20require%20accurately%20precomputed%20camera%20poses%20during%0Atraining.%20To%20mitigate%20this%20requirement%2C%20existing%20methods%20jointly%20optimize%0Acamera%20poses%20and%20NeRF%20often%20relying%20on%20good%20pose%20initialisation%20or%20depth%0Apriors.%20However%2C%20these%20approaches%20struggle%20in%20challenging%20scenarios%2C%20such%20as%0Alarge%20rotations%2C%20as%20they%20map%20each%20camera%20to%20a%20world%20coordinate%20system.%20We%0Apropose%20a%20novel%20method%20that%20eliminates%20prior%20dependencies%20by%20modeling%0Acontinuous%20camera%20motions%20as%20time-dependent%20angular%20velocity%20and%20velocity.%0ARelative%20motions%20between%20cameras%20are%20learned%20first%20via%20velocity%20integration%2C%0Awhile%20camera%20poses%20can%20be%20obtained%20by%20aggregating%20such%20relative%20motions%20up%20to%20a%0Aworld%20coordinate%20system%20defined%20at%20a%20single%20time%20step%20within%20the%20video.%0ASpecifically%2C%20accurate%20continuous%20camera%20movements%20are%20learned%20through%20a%0Atime-dependent%20NeRF%2C%20which%20captures%20local%20scene%20geometry%20and%20motion%20by%20training%0Afrom%20neighboring%20frames%20for%20each%20time%20step.%20The%20learned%20motions%20enable%0Afine-tuning%20the%20NeRF%20to%20represent%20the%20full%20scene%20geometry.%20Experiments%20on%20Co3D%0Aand%20Scannet%20show%20our%20approach%20achieves%20superior%20camera%20pose%20and%20depth%0Aestimation%20and%20comparable%20novel-view%20synthesis%20performance%20compared%20to%0Astate-of-the-art%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/HoangChuongNguyen/cope-nerf.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Optimization%2520of%2520Neural%2520Radiance%2520Fields%2520and%2520Continuous%2520Camera%250A%2520%2520Motion%2520from%2520a%2520Monocular%2520Video%26entry.906535625%3DHoang%2520Chuong%2520Nguyen%2520and%2520Wei%2520Mao%2520and%2520Jose%2520M.%2520Alvarez%2520and%2520Miaomiao%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520has%2520demonstrated%2520its%2520superior%2520capability%2520to%250Arepresent%25203D%2520geometry%2520but%2520require%2520accurately%2520precomputed%2520camera%2520poses%2520during%250Atraining.%2520To%2520mitigate%2520this%2520requirement%252C%2520existing%2520methods%2520jointly%2520optimize%250Acamera%2520poses%2520and%2520NeRF%2520often%2520relying%2520on%2520good%2520pose%2520initialisation%2520or%2520depth%250Apriors.%2520However%252C%2520these%2520approaches%2520struggle%2520in%2520challenging%2520scenarios%252C%2520such%2520as%250Alarge%2520rotations%252C%2520as%2520they%2520map%2520each%2520camera%2520to%2520a%2520world%2520coordinate%2520system.%2520We%250Apropose%2520a%2520novel%2520method%2520that%2520eliminates%2520prior%2520dependencies%2520by%2520modeling%250Acontinuous%2520camera%2520motions%2520as%2520time-dependent%2520angular%2520velocity%2520and%2520velocity.%250ARelative%2520motions%2520between%2520cameras%2520are%2520learned%2520first%2520via%2520velocity%2520integration%252C%250Awhile%2520camera%2520poses%2520can%2520be%2520obtained%2520by%2520aggregating%2520such%2520relative%2520motions%2520up%2520to%2520a%250Aworld%2520coordinate%2520system%2520defined%2520at%2520a%2520single%2520time%2520step%2520within%2520the%2520video.%250ASpecifically%252C%2520accurate%2520continuous%2520camera%2520movements%2520are%2520learned%2520through%2520a%250Atime-dependent%2520NeRF%252C%2520which%2520captures%2520local%2520scene%2520geometry%2520and%2520motion%2520by%2520training%250Afrom%2520neighboring%2520frames%2520for%2520each%2520time%2520step.%2520The%2520learned%2520motions%2520enable%250Afine-tuning%2520the%2520NeRF%2520to%2520represent%2520the%2520full%2520scene%2520geometry.%2520Experiments%2520on%2520Co3D%250Aand%2520Scannet%2520show%2520our%2520approach%2520achieves%2520superior%2520camera%2520pose%2520and%2520depth%250Aestimation%2520and%2520comparable%2520novel-view%2520synthesis%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/HoangChuongNguyen/cope-nerf.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Optimization%20of%20Neural%20Radiance%20Fields%20and%20Continuous%20Camera%0A%20%20Motion%20from%20a%20Monocular%20Video&entry.906535625=Hoang%20Chuong%20Nguyen%20and%20Wei%20Mao%20and%20Jose%20M.%20Alvarez%20and%20Miaomiao%20Liu&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20has%20demonstrated%20its%20superior%20capability%20to%0Arepresent%203D%20geometry%20but%20require%20accurately%20precomputed%20camera%20poses%20during%0Atraining.%20To%20mitigate%20this%20requirement%2C%20existing%20methods%20jointly%20optimize%0Acamera%20poses%20and%20NeRF%20often%20relying%20on%20good%20pose%20initialisation%20or%20depth%0Apriors.%20However%2C%20these%20approaches%20struggle%20in%20challenging%20scenarios%2C%20such%20as%0Alarge%20rotations%2C%20as%20they%20map%20each%20camera%20to%20a%20world%20coordinate%20system.%20We%0Apropose%20a%20novel%20method%20that%20eliminates%20prior%20dependencies%20by%20modeling%0Acontinuous%20camera%20motions%20as%20time-dependent%20angular%20velocity%20and%20velocity.%0ARelative%20motions%20between%20cameras%20are%20learned%20first%20via%20velocity%20integration%2C%0Awhile%20camera%20poses%20can%20be%20obtained%20by%20aggregating%20such%20relative%20motions%20up%20to%20a%0Aworld%20coordinate%20system%20defined%20at%20a%20single%20time%20step%20within%20the%20video.%0ASpecifically%2C%20accurate%20continuous%20camera%20movements%20are%20learned%20through%20a%0Atime-dependent%20NeRF%2C%20which%20captures%20local%20scene%20geometry%20and%20motion%20by%20training%0Afrom%20neighboring%20frames%20for%20each%20time%20step.%20The%20learned%20motions%20enable%0Afine-tuning%20the%20NeRF%20to%20represent%20the%20full%20scene%20geometry.%20Experiments%20on%20Co3D%0Aand%20Scannet%20show%20our%20approach%20achieves%20superior%20camera%20pose%20and%20depth%0Aestimation%20and%20comparable%20novel-view%20synthesis%20performance%20compared%20to%0Astate-of-the-art%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/HoangChuongNguyen/cope-nerf.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19819v1&entry.124074799=Read"},
{"title": "Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging\n  and Variational Autoencoders", "author": "Mahmoud Abdulsalam and Usman Zahidi and Bradley Hurst and Simon Pearson and Grzegorz Cielniak and James Brown", "abstract": "  Tomato anomalies/damages pose a significant challenge in greenhouse farming.\nWhile this method of cultivation benefits from efficient resource utilization,\nanomalies can significantly degrade the quality of farm produce. A common\nanomaly associated with tomatoes is splitting, characterized by the development\nof cracks on the tomato skin, which degrades its quality. Detecting this type\nof anomaly is challenging due to dynamic variations in appearance and sizes,\ncompounded by dataset scarcity. We address this problem in an unsupervised\nmanner by utilizing a tailored variational autoencoder (VAE) with hyperspectral\ninput. Preliminary analysis of the dataset enabled us to select the optimal\nrange of wavelengths for detecting this anomaly. Our findings indicate that the\n530nm - 550nm range is suitable for identifying tomato dry splits. The proposed\nVAE model achieved a 97% detection accuracy for tomato split anomalies in the\ntest data. The analysis on reconstruction loss allow us to not only detect the\nanomalies but also to some degree estimate the anomalous regions.\n", "link": "http://arxiv.org/abs/2501.02921v2", "date": "2025-04-28", "relevancy": 2.3903, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4823}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4813}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Tomato%20Split%20Anomaly%20Detection%20using%20Hyperspectral%20Imaging%0A%20%20and%20Variational%20Autoencoders&body=Title%3A%20Unsupervised%20Tomato%20Split%20Anomaly%20Detection%20using%20Hyperspectral%20Imaging%0A%20%20and%20Variational%20Autoencoders%0AAuthor%3A%20Mahmoud%20Abdulsalam%20and%20Usman%20Zahidi%20and%20Bradley%20Hurst%20and%20Simon%20Pearson%20and%20Grzegorz%20Cielniak%20and%20James%20Brown%0AAbstract%3A%20%20%20Tomato%20anomalies/damages%20pose%20a%20significant%20challenge%20in%20greenhouse%20farming.%0AWhile%20this%20method%20of%20cultivation%20benefits%20from%20efficient%20resource%20utilization%2C%0Aanomalies%20can%20significantly%20degrade%20the%20quality%20of%20farm%20produce.%20A%20common%0Aanomaly%20associated%20with%20tomatoes%20is%20splitting%2C%20characterized%20by%20the%20development%0Aof%20cracks%20on%20the%20tomato%20skin%2C%20which%20degrades%20its%20quality.%20Detecting%20this%20type%0Aof%20anomaly%20is%20challenging%20due%20to%20dynamic%20variations%20in%20appearance%20and%20sizes%2C%0Acompounded%20by%20dataset%20scarcity.%20We%20address%20this%20problem%20in%20an%20unsupervised%0Amanner%20by%20utilizing%20a%20tailored%20variational%20autoencoder%20%28VAE%29%20with%20hyperspectral%0Ainput.%20Preliminary%20analysis%20of%20the%20dataset%20enabled%20us%20to%20select%20the%20optimal%0Arange%20of%20wavelengths%20for%20detecting%20this%20anomaly.%20Our%20findings%20indicate%20that%20the%0A530nm%20-%20550nm%20range%20is%20suitable%20for%20identifying%20tomato%20dry%20splits.%20The%20proposed%0AVAE%20model%20achieved%20a%2097%25%20detection%20accuracy%20for%20tomato%20split%20anomalies%20in%20the%0Atest%20data.%20The%20analysis%20on%20reconstruction%20loss%20allow%20us%20to%20not%20only%20detect%20the%0Aanomalies%20but%20also%20to%20some%20degree%20estimate%20the%20anomalous%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Tomato%2520Split%2520Anomaly%2520Detection%2520using%2520Hyperspectral%2520Imaging%250A%2520%2520and%2520Variational%2520Autoencoders%26entry.906535625%3DMahmoud%2520Abdulsalam%2520and%2520Usman%2520Zahidi%2520and%2520Bradley%2520Hurst%2520and%2520Simon%2520Pearson%2520and%2520Grzegorz%2520Cielniak%2520and%2520James%2520Brown%26entry.1292438233%3D%2520%2520Tomato%2520anomalies/damages%2520pose%2520a%2520significant%2520challenge%2520in%2520greenhouse%2520farming.%250AWhile%2520this%2520method%2520of%2520cultivation%2520benefits%2520from%2520efficient%2520resource%2520utilization%252C%250Aanomalies%2520can%2520significantly%2520degrade%2520the%2520quality%2520of%2520farm%2520produce.%2520A%2520common%250Aanomaly%2520associated%2520with%2520tomatoes%2520is%2520splitting%252C%2520characterized%2520by%2520the%2520development%250Aof%2520cracks%2520on%2520the%2520tomato%2520skin%252C%2520which%2520degrades%2520its%2520quality.%2520Detecting%2520this%2520type%250Aof%2520anomaly%2520is%2520challenging%2520due%2520to%2520dynamic%2520variations%2520in%2520appearance%2520and%2520sizes%252C%250Acompounded%2520by%2520dataset%2520scarcity.%2520We%2520address%2520this%2520problem%2520in%2520an%2520unsupervised%250Amanner%2520by%2520utilizing%2520a%2520tailored%2520variational%2520autoencoder%2520%2528VAE%2529%2520with%2520hyperspectral%250Ainput.%2520Preliminary%2520analysis%2520of%2520the%2520dataset%2520enabled%2520us%2520to%2520select%2520the%2520optimal%250Arange%2520of%2520wavelengths%2520for%2520detecting%2520this%2520anomaly.%2520Our%2520findings%2520indicate%2520that%2520the%250A530nm%2520-%2520550nm%2520range%2520is%2520suitable%2520for%2520identifying%2520tomato%2520dry%2520splits.%2520The%2520proposed%250AVAE%2520model%2520achieved%2520a%252097%2525%2520detection%2520accuracy%2520for%2520tomato%2520split%2520anomalies%2520in%2520the%250Atest%2520data.%2520The%2520analysis%2520on%2520reconstruction%2520loss%2520allow%2520us%2520to%2520not%2520only%2520detect%2520the%250Aanomalies%2520but%2520also%2520to%2520some%2520degree%2520estimate%2520the%2520anomalous%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Tomato%20Split%20Anomaly%20Detection%20using%20Hyperspectral%20Imaging%0A%20%20and%20Variational%20Autoencoders&entry.906535625=Mahmoud%20Abdulsalam%20and%20Usman%20Zahidi%20and%20Bradley%20Hurst%20and%20Simon%20Pearson%20and%20Grzegorz%20Cielniak%20and%20James%20Brown&entry.1292438233=%20%20Tomato%20anomalies/damages%20pose%20a%20significant%20challenge%20in%20greenhouse%20farming.%0AWhile%20this%20method%20of%20cultivation%20benefits%20from%20efficient%20resource%20utilization%2C%0Aanomalies%20can%20significantly%20degrade%20the%20quality%20of%20farm%20produce.%20A%20common%0Aanomaly%20associated%20with%20tomatoes%20is%20splitting%2C%20characterized%20by%20the%20development%0Aof%20cracks%20on%20the%20tomato%20skin%2C%20which%20degrades%20its%20quality.%20Detecting%20this%20type%0Aof%20anomaly%20is%20challenging%20due%20to%20dynamic%20variations%20in%20appearance%20and%20sizes%2C%0Acompounded%20by%20dataset%20scarcity.%20We%20address%20this%20problem%20in%20an%20unsupervised%0Amanner%20by%20utilizing%20a%20tailored%20variational%20autoencoder%20%28VAE%29%20with%20hyperspectral%0Ainput.%20Preliminary%20analysis%20of%20the%20dataset%20enabled%20us%20to%20select%20the%20optimal%0Arange%20of%20wavelengths%20for%20detecting%20this%20anomaly.%20Our%20findings%20indicate%20that%20the%0A530nm%20-%20550nm%20range%20is%20suitable%20for%20identifying%20tomato%20dry%20splits.%20The%20proposed%0AVAE%20model%20achieved%20a%2097%25%20detection%20accuracy%20for%20tomato%20split%20anomalies%20in%20the%0Atest%20data.%20The%20analysis%20on%20reconstruction%20loss%20allow%20us%20to%20not%20only%20detect%20the%0Aanomalies%20but%20also%20to%20some%20degree%20estimate%20the%20anomalous%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02921v2&entry.124074799=Read"},
{"title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for\n  Embodied Tasks", "author": "Chia-Yu Hung and Qi Sun and Pengfei Hong and Amir Zadeh and Chuan Li and U-Xuan Tan and Navonil Majumder and Soujanya Poria", "abstract": "  Existing Visual-Language-Action (VLA) models have shown promising performance\nin zero-shot scenarios, demonstrating impressive task execution and reasoning\ncapabilities. However, a significant challenge arises from the limitations of\nvisual encoding, which can result in failures during tasks such as object\ngrasping. Moreover, these models typically suffer from high computational\noverhead due to their large sizes, often exceeding 7B parameters. While these\nmodels excel in reasoning and task planning, the substantial computational\noverhead they incur makes them impractical for real-time robotic environments,\nwhere speed and efficiency are paramount. To address the limitations of\nexisting VLA models, we propose NORA, a 3B-parameter model designed to reduce\ncomputational overhead while maintaining strong task performance. NORA adopts\nthe Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior\nvisual-semantic understanding to enhance visual reasoning and action grounding.\nAdditionally, our \\model{} is trained on 970k real-world robot demonstrations\nand equipped with the FAST+ tokenizer for efficient action sequence generation.\nExperimental results demonstrate that NORA outperforms existing large-scale VLA\nmodels, achieving better task performance with significantly reduced\ncomputational overhead, making it a more practical solution for real-time\nrobotic autonomy.\n", "link": "http://arxiv.org/abs/2504.19854v1", "date": "2025-04-28", "relevancy": 2.3281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5858}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NORA%3A%20A%20Small%20Open-Sourced%20Generalist%20Vision%20Language%20Action%20Model%20for%0A%20%20Embodied%20Tasks&body=Title%3A%20NORA%3A%20A%20Small%20Open-Sourced%20Generalist%20Vision%20Language%20Action%20Model%20for%0A%20%20Embodied%20Tasks%0AAuthor%3A%20Chia-Yu%20Hung%20and%20Qi%20Sun%20and%20Pengfei%20Hong%20and%20Amir%20Zadeh%20and%20Chuan%20Li%20and%20U-Xuan%20Tan%20and%20Navonil%20Majumder%20and%20Soujanya%20Poria%0AAbstract%3A%20%20%20Existing%20Visual-Language-Action%20%28VLA%29%20models%20have%20shown%20promising%20performance%0Ain%20zero-shot%20scenarios%2C%20demonstrating%20impressive%20task%20execution%20and%20reasoning%0Acapabilities.%20However%2C%20a%20significant%20challenge%20arises%20from%20the%20limitations%20of%0Avisual%20encoding%2C%20which%20can%20result%20in%20failures%20during%20tasks%20such%20as%20object%0Agrasping.%20Moreover%2C%20these%20models%20typically%20suffer%20from%20high%20computational%0Aoverhead%20due%20to%20their%20large%20sizes%2C%20often%20exceeding%207B%20parameters.%20While%20these%0Amodels%20excel%20in%20reasoning%20and%20task%20planning%2C%20the%20substantial%20computational%0Aoverhead%20they%20incur%20makes%20them%20impractical%20for%20real-time%20robotic%20environments%2C%0Awhere%20speed%20and%20efficiency%20are%20paramount.%20To%20address%20the%20limitations%20of%0Aexisting%20VLA%20models%2C%20we%20propose%20NORA%2C%20a%203B-parameter%20model%20designed%20to%20reduce%0Acomputational%20overhead%20while%20maintaining%20strong%20task%20performance.%20NORA%20adopts%0Athe%20Qwen-2.5-VL-3B%20multimodal%20model%20as%20its%20backbone%2C%20leveraging%20its%20superior%0Avisual-semantic%20understanding%20to%20enhance%20visual%20reasoning%20and%20action%20grounding.%0AAdditionally%2C%20our%20%5Cmodel%7B%7D%20is%20trained%20on%20970k%20real-world%20robot%20demonstrations%0Aand%20equipped%20with%20the%20FAST%2B%20tokenizer%20for%20efficient%20action%20sequence%20generation.%0AExperimental%20results%20demonstrate%20that%20NORA%20outperforms%20existing%20large-scale%20VLA%0Amodels%2C%20achieving%20better%20task%20performance%20with%20significantly%20reduced%0Acomputational%20overhead%2C%20making%20it%20a%20more%20practical%20solution%20for%20real-time%0Arobotic%20autonomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNORA%253A%2520A%2520Small%2520Open-Sourced%2520Generalist%2520Vision%2520Language%2520Action%2520Model%2520for%250A%2520%2520Embodied%2520Tasks%26entry.906535625%3DChia-Yu%2520Hung%2520and%2520Qi%2520Sun%2520and%2520Pengfei%2520Hong%2520and%2520Amir%2520Zadeh%2520and%2520Chuan%2520Li%2520and%2520U-Xuan%2520Tan%2520and%2520Navonil%2520Majumder%2520and%2520Soujanya%2520Poria%26entry.1292438233%3D%2520%2520Existing%2520Visual-Language-Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520promising%2520performance%250Ain%2520zero-shot%2520scenarios%252C%2520demonstrating%2520impressive%2520task%2520execution%2520and%2520reasoning%250Acapabilities.%2520However%252C%2520a%2520significant%2520challenge%2520arises%2520from%2520the%2520limitations%2520of%250Avisual%2520encoding%252C%2520which%2520can%2520result%2520in%2520failures%2520during%2520tasks%2520such%2520as%2520object%250Agrasping.%2520Moreover%252C%2520these%2520models%2520typically%2520suffer%2520from%2520high%2520computational%250Aoverhead%2520due%2520to%2520their%2520large%2520sizes%252C%2520often%2520exceeding%25207B%2520parameters.%2520While%2520these%250Amodels%2520excel%2520in%2520reasoning%2520and%2520task%2520planning%252C%2520the%2520substantial%2520computational%250Aoverhead%2520they%2520incur%2520makes%2520them%2520impractical%2520for%2520real-time%2520robotic%2520environments%252C%250Awhere%2520speed%2520and%2520efficiency%2520are%2520paramount.%2520To%2520address%2520the%2520limitations%2520of%250Aexisting%2520VLA%2520models%252C%2520we%2520propose%2520NORA%252C%2520a%25203B-parameter%2520model%2520designed%2520to%2520reduce%250Acomputational%2520overhead%2520while%2520maintaining%2520strong%2520task%2520performance.%2520NORA%2520adopts%250Athe%2520Qwen-2.5-VL-3B%2520multimodal%2520model%2520as%2520its%2520backbone%252C%2520leveraging%2520its%2520superior%250Avisual-semantic%2520understanding%2520to%2520enhance%2520visual%2520reasoning%2520and%2520action%2520grounding.%250AAdditionally%252C%2520our%2520%255Cmodel%257B%257D%2520is%2520trained%2520on%2520970k%2520real-world%2520robot%2520demonstrations%250Aand%2520equipped%2520with%2520the%2520FAST%252B%2520tokenizer%2520for%2520efficient%2520action%2520sequence%2520generation.%250AExperimental%2520results%2520demonstrate%2520that%2520NORA%2520outperforms%2520existing%2520large-scale%2520VLA%250Amodels%252C%2520achieving%2520better%2520task%2520performance%2520with%2520significantly%2520reduced%250Acomputational%2520overhead%252C%2520making%2520it%2520a%2520more%2520practical%2520solution%2520for%2520real-time%250Arobotic%2520autonomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NORA%3A%20A%20Small%20Open-Sourced%20Generalist%20Vision%20Language%20Action%20Model%20for%0A%20%20Embodied%20Tasks&entry.906535625=Chia-Yu%20Hung%20and%20Qi%20Sun%20and%20Pengfei%20Hong%20and%20Amir%20Zadeh%20and%20Chuan%20Li%20and%20U-Xuan%20Tan%20and%20Navonil%20Majumder%20and%20Soujanya%20Poria&entry.1292438233=%20%20Existing%20Visual-Language-Action%20%28VLA%29%20models%20have%20shown%20promising%20performance%0Ain%20zero-shot%20scenarios%2C%20demonstrating%20impressive%20task%20execution%20and%20reasoning%0Acapabilities.%20However%2C%20a%20significant%20challenge%20arises%20from%20the%20limitations%20of%0Avisual%20encoding%2C%20which%20can%20result%20in%20failures%20during%20tasks%20such%20as%20object%0Agrasping.%20Moreover%2C%20these%20models%20typically%20suffer%20from%20high%20computational%0Aoverhead%20due%20to%20their%20large%20sizes%2C%20often%20exceeding%207B%20parameters.%20While%20these%0Amodels%20excel%20in%20reasoning%20and%20task%20planning%2C%20the%20substantial%20computational%0Aoverhead%20they%20incur%20makes%20them%20impractical%20for%20real-time%20robotic%20environments%2C%0Awhere%20speed%20and%20efficiency%20are%20paramount.%20To%20address%20the%20limitations%20of%0Aexisting%20VLA%20models%2C%20we%20propose%20NORA%2C%20a%203B-parameter%20model%20designed%20to%20reduce%0Acomputational%20overhead%20while%20maintaining%20strong%20task%20performance.%20NORA%20adopts%0Athe%20Qwen-2.5-VL-3B%20multimodal%20model%20as%20its%20backbone%2C%20leveraging%20its%20superior%0Avisual-semantic%20understanding%20to%20enhance%20visual%20reasoning%20and%20action%20grounding.%0AAdditionally%2C%20our%20%5Cmodel%7B%7D%20is%20trained%20on%20970k%20real-world%20robot%20demonstrations%0Aand%20equipped%20with%20the%20FAST%2B%20tokenizer%20for%20efficient%20action%20sequence%20generation.%0AExperimental%20results%20demonstrate%20that%20NORA%20outperforms%20existing%20large-scale%20VLA%0Amodels%2C%20achieving%20better%20task%20performance%20with%20significantly%20reduced%0Acomputational%20overhead%2C%20making%20it%20a%20more%20practical%20solution%20for%20real-time%0Arobotic%20autonomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19854v1&entry.124074799=Read"},
{"title": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition", "author": "Quynh Phung and Long Mai and Fabian David Caba Heilbron and Feng Liu and Jia-Bin Huang and Cusuh Ham", "abstract": "  We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis.\n", "link": "http://arxiv.org/abs/2504.19894v1", "date": "2025-04-28", "relevancy": 2.3043, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CineVerse%3A%20Consistent%20Keyframe%20Synthesis%20for%20Cinematic%20Scene%20Composition&body=Title%3A%20CineVerse%3A%20Consistent%20Keyframe%20Synthesis%20for%20Cinematic%20Scene%20Composition%0AAuthor%3A%20Quynh%20Phung%20and%20Long%20Mai%20and%20Fabian%20David%20Caba%20Heilbron%20and%20Feng%20Liu%20and%20Jia-Bin%20Huang%20and%20Cusuh%20Ham%0AAbstract%3A%20%20%20We%20present%20CineVerse%2C%20a%20novel%20framework%20for%20the%20task%20of%20cinematic%20scene%0Acomposition.%20Similar%20to%20traditional%20multi-shot%20generation%2C%20our%20task%20emphasizes%0Athe%20need%20for%20consistency%20and%20continuity%20across%20frames.%20However%2C%20our%20task%20also%0Afocuses%20on%20addressing%20challenges%20inherent%20to%20filmmaking%2C%20such%20as%20multiple%0Acharacters%2C%20complex%20interactions%2C%20and%20visual%20cinematic%20effects.%20In%20order%20to%0Alearn%20to%20generate%20such%20content%2C%20we%20first%20create%20the%20CineVerse%20dataset.%20We%20use%0Athis%20dataset%20to%20train%20our%20proposed%20two-stage%20approach.%20First%2C%20we%20prompt%20a%20large%0Alanguage%20model%20%28LLM%29%20with%20task-specific%20instructions%20to%20take%20in%20a%20high-level%0Ascene%20description%20and%20generate%20a%20detailed%20plan%20for%20the%20overall%20setting%20and%0Acharacters%2C%20as%20well%20as%20the%20individual%20shots.%20Then%2C%20we%20fine-tune%20a%20text-to-image%0Ageneration%20model%20to%20synthesize%20high-quality%20visual%20keyframes.%20Experimental%0Aresults%20demonstrate%20that%20CineVerse%20yields%20promising%20improvements%20in%20generating%0Avisually%20coherent%20and%20contextually%20rich%20movie%20scenes%2C%20paving%20the%20way%20for%0Afurther%20exploration%20in%20cinematic%20video%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCineVerse%253A%2520Consistent%2520Keyframe%2520Synthesis%2520for%2520Cinematic%2520Scene%2520Composition%26entry.906535625%3DQuynh%2520Phung%2520and%2520Long%2520Mai%2520and%2520Fabian%2520David%2520Caba%2520Heilbron%2520and%2520Feng%2520Liu%2520and%2520Jia-Bin%2520Huang%2520and%2520Cusuh%2520Ham%26entry.1292438233%3D%2520%2520We%2520present%2520CineVerse%252C%2520a%2520novel%2520framework%2520for%2520the%2520task%2520of%2520cinematic%2520scene%250Acomposition.%2520Similar%2520to%2520traditional%2520multi-shot%2520generation%252C%2520our%2520task%2520emphasizes%250Athe%2520need%2520for%2520consistency%2520and%2520continuity%2520across%2520frames.%2520However%252C%2520our%2520task%2520also%250Afocuses%2520on%2520addressing%2520challenges%2520inherent%2520to%2520filmmaking%252C%2520such%2520as%2520multiple%250Acharacters%252C%2520complex%2520interactions%252C%2520and%2520visual%2520cinematic%2520effects.%2520In%2520order%2520to%250Alearn%2520to%2520generate%2520such%2520content%252C%2520we%2520first%2520create%2520the%2520CineVerse%2520dataset.%2520We%2520use%250Athis%2520dataset%2520to%2520train%2520our%2520proposed%2520two-stage%2520approach.%2520First%252C%2520we%2520prompt%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520with%2520task-specific%2520instructions%2520to%2520take%2520in%2520a%2520high-level%250Ascene%2520description%2520and%2520generate%2520a%2520detailed%2520plan%2520for%2520the%2520overall%2520setting%2520and%250Acharacters%252C%2520as%2520well%2520as%2520the%2520individual%2520shots.%2520Then%252C%2520we%2520fine-tune%2520a%2520text-to-image%250Ageneration%2520model%2520to%2520synthesize%2520high-quality%2520visual%2520keyframes.%2520Experimental%250Aresults%2520demonstrate%2520that%2520CineVerse%2520yields%2520promising%2520improvements%2520in%2520generating%250Avisually%2520coherent%2520and%2520contextually%2520rich%2520movie%2520scenes%252C%2520paving%2520the%2520way%2520for%250Afurther%2520exploration%2520in%2520cinematic%2520video%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CineVerse%3A%20Consistent%20Keyframe%20Synthesis%20for%20Cinematic%20Scene%20Composition&entry.906535625=Quynh%20Phung%20and%20Long%20Mai%20and%20Fabian%20David%20Caba%20Heilbron%20and%20Feng%20Liu%20and%20Jia-Bin%20Huang%20and%20Cusuh%20Ham&entry.1292438233=%20%20We%20present%20CineVerse%2C%20a%20novel%20framework%20for%20the%20task%20of%20cinematic%20scene%0Acomposition.%20Similar%20to%20traditional%20multi-shot%20generation%2C%20our%20task%20emphasizes%0Athe%20need%20for%20consistency%20and%20continuity%20across%20frames.%20However%2C%20our%20task%20also%0Afocuses%20on%20addressing%20challenges%20inherent%20to%20filmmaking%2C%20such%20as%20multiple%0Acharacters%2C%20complex%20interactions%2C%20and%20visual%20cinematic%20effects.%20In%20order%20to%0Alearn%20to%20generate%20such%20content%2C%20we%20first%20create%20the%20CineVerse%20dataset.%20We%20use%0Athis%20dataset%20to%20train%20our%20proposed%20two-stage%20approach.%20First%2C%20we%20prompt%20a%20large%0Alanguage%20model%20%28LLM%29%20with%20task-specific%20instructions%20to%20take%20in%20a%20high-level%0Ascene%20description%20and%20generate%20a%20detailed%20plan%20for%20the%20overall%20setting%20and%0Acharacters%2C%20as%20well%20as%20the%20individual%20shots.%20Then%2C%20we%20fine-tune%20a%20text-to-image%0Ageneration%20model%20to%20synthesize%20high-quality%20visual%20keyframes.%20Experimental%0Aresults%20demonstrate%20that%20CineVerse%20yields%20promising%20improvements%20in%20generating%0Avisually%20coherent%20and%20contextually%20rich%20movie%20scenes%2C%20paving%20the%20way%20for%0Afurther%20exploration%20in%20cinematic%20video%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19894v1&entry.124074799=Read"},
{"title": "GPA-RAM: Grasp-Pretraining Augmented Robotic Attention Mamba for Spatial\n  Task Learning", "author": "Juyi Sheng and Yangjun Liu and Sheng Xu and Zhixin Yang and Mengyuan Liu", "abstract": "  Most existing robot manipulation methods prioritize task learning by\nenhancing perception through complex deep network architectures. However, they\nface challenges in real-time collision-free planning. Hence, Robotic Attention\nMamba (RAM) is designed for refined planning. Specifically, by integrating\nMamba and parallel single-view attention, RAM aligns multi-view vision and\ntask-related language features, ensuring efficient fine-grained task planning\nwith linear complexity and robust real-time performance. Nevertheless, it has\nthe potential for further improvement in high-precision grasping and\nmanipulation. Thus, Grasp-Pretraining Augmentation (GPA) is devised, with a\ngrasp pose feature extractor pretrained utilizing object grasp poses directly\ninherited from whole-task demonstrations. Subsequently, the extracted grasp\nfeatures are fused with the spatially aligned planning features from RAM\nthrough attention-based Pre-trained Location Fusion, preserving high-resolution\ngrasping cues overshadowed by an overemphasis on global planning. To summarize,\nwe propose Grasp-Pretraining Augmented Robotic Attention Mamba (GPA-RAM),\ndividing spatial task learning into RAM for planning skill learning and GPA for\ngrasping skill learning. GPA-RAM demonstrates superior performance across three\nrobot systems with distinct camera configurations in simulation and the real\nworld. Compared with previous state-of-the-art methods, it improves the\nabsolute success rate by 8.2% (from 79.3% to 87.5%) on the RLBench multi-task\nbenchmark and 40\\% (from 16% to 56%), 12% (from 86% to 98%) on the ALOHA\nbimanual manipulation tasks, while delivering notably faster inference.\nFurthermore, experimental results demonstrate that both RAM and GPA enhance\ntask learning, with GPA proving robust to different architectures of pretrained\ngrasp pose feature extractors. The website is:\nhttps://logssim.github.io/GPA\\_RAM\\_website/.\n", "link": "http://arxiv.org/abs/2504.19683v1", "date": "2025-04-28", "relevancy": 2.3018, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5805}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5738}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPA-RAM%3A%20Grasp-Pretraining%20Augmented%20Robotic%20Attention%20Mamba%20for%20Spatial%0A%20%20Task%20Learning&body=Title%3A%20GPA-RAM%3A%20Grasp-Pretraining%20Augmented%20Robotic%20Attention%20Mamba%20for%20Spatial%0A%20%20Task%20Learning%0AAuthor%3A%20Juyi%20Sheng%20and%20Yangjun%20Liu%20and%20Sheng%20Xu%20and%20Zhixin%20Yang%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20Most%20existing%20robot%20manipulation%20methods%20prioritize%20task%20learning%20by%0Aenhancing%20perception%20through%20complex%20deep%20network%20architectures.%20However%2C%20they%0Aface%20challenges%20in%20real-time%20collision-free%20planning.%20Hence%2C%20Robotic%20Attention%0AMamba%20%28RAM%29%20is%20designed%20for%20refined%20planning.%20Specifically%2C%20by%20integrating%0AMamba%20and%20parallel%20single-view%20attention%2C%20RAM%20aligns%20multi-view%20vision%20and%0Atask-related%20language%20features%2C%20ensuring%20efficient%20fine-grained%20task%20planning%0Awith%20linear%20complexity%20and%20robust%20real-time%20performance.%20Nevertheless%2C%20it%20has%0Athe%20potential%20for%20further%20improvement%20in%20high-precision%20grasping%20and%0Amanipulation.%20Thus%2C%20Grasp-Pretraining%20Augmentation%20%28GPA%29%20is%20devised%2C%20with%20a%0Agrasp%20pose%20feature%20extractor%20pretrained%20utilizing%20object%20grasp%20poses%20directly%0Ainherited%20from%20whole-task%20demonstrations.%20Subsequently%2C%20the%20extracted%20grasp%0Afeatures%20are%20fused%20with%20the%20spatially%20aligned%20planning%20features%20from%20RAM%0Athrough%20attention-based%20Pre-trained%20Location%20Fusion%2C%20preserving%20high-resolution%0Agrasping%20cues%20overshadowed%20by%20an%20overemphasis%20on%20global%20planning.%20To%20summarize%2C%0Awe%20propose%20Grasp-Pretraining%20Augmented%20Robotic%20Attention%20Mamba%20%28GPA-RAM%29%2C%0Adividing%20spatial%20task%20learning%20into%20RAM%20for%20planning%20skill%20learning%20and%20GPA%20for%0Agrasping%20skill%20learning.%20GPA-RAM%20demonstrates%20superior%20performance%20across%20three%0Arobot%20systems%20with%20distinct%20camera%20configurations%20in%20simulation%20and%20the%20real%0Aworld.%20Compared%20with%20previous%20state-of-the-art%20methods%2C%20it%20improves%20the%0Aabsolute%20success%20rate%20by%208.2%25%20%28from%2079.3%25%20to%2087.5%25%29%20on%20the%20RLBench%20multi-task%0Abenchmark%20and%2040%5C%25%20%28from%2016%25%20to%2056%25%29%2C%2012%25%20%28from%2086%25%20to%2098%25%29%20on%20the%20ALOHA%0Abimanual%20manipulation%20tasks%2C%20while%20delivering%20notably%20faster%20inference.%0AFurthermore%2C%20experimental%20results%20demonstrate%20that%20both%20RAM%20and%20GPA%20enhance%0Atask%20learning%2C%20with%20GPA%20proving%20robust%20to%20different%20architectures%20of%20pretrained%0Agrasp%20pose%20feature%20extractors.%20The%20website%20is%3A%0Ahttps%3A//logssim.github.io/GPA%5C_RAM%5C_website/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPA-RAM%253A%2520Grasp-Pretraining%2520Augmented%2520Robotic%2520Attention%2520Mamba%2520for%2520Spatial%250A%2520%2520Task%2520Learning%26entry.906535625%3DJuyi%2520Sheng%2520and%2520Yangjun%2520Liu%2520and%2520Sheng%2520Xu%2520and%2520Zhixin%2520Yang%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3D%2520%2520Most%2520existing%2520robot%2520manipulation%2520methods%2520prioritize%2520task%2520learning%2520by%250Aenhancing%2520perception%2520through%2520complex%2520deep%2520network%2520architectures.%2520However%252C%2520they%250Aface%2520challenges%2520in%2520real-time%2520collision-free%2520planning.%2520Hence%252C%2520Robotic%2520Attention%250AMamba%2520%2528RAM%2529%2520is%2520designed%2520for%2520refined%2520planning.%2520Specifically%252C%2520by%2520integrating%250AMamba%2520and%2520parallel%2520single-view%2520attention%252C%2520RAM%2520aligns%2520multi-view%2520vision%2520and%250Atask-related%2520language%2520features%252C%2520ensuring%2520efficient%2520fine-grained%2520task%2520planning%250Awith%2520linear%2520complexity%2520and%2520robust%2520real-time%2520performance.%2520Nevertheless%252C%2520it%2520has%250Athe%2520potential%2520for%2520further%2520improvement%2520in%2520high-precision%2520grasping%2520and%250Amanipulation.%2520Thus%252C%2520Grasp-Pretraining%2520Augmentation%2520%2528GPA%2529%2520is%2520devised%252C%2520with%2520a%250Agrasp%2520pose%2520feature%2520extractor%2520pretrained%2520utilizing%2520object%2520grasp%2520poses%2520directly%250Ainherited%2520from%2520whole-task%2520demonstrations.%2520Subsequently%252C%2520the%2520extracted%2520grasp%250Afeatures%2520are%2520fused%2520with%2520the%2520spatially%2520aligned%2520planning%2520features%2520from%2520RAM%250Athrough%2520attention-based%2520Pre-trained%2520Location%2520Fusion%252C%2520preserving%2520high-resolution%250Agrasping%2520cues%2520overshadowed%2520by%2520an%2520overemphasis%2520on%2520global%2520planning.%2520To%2520summarize%252C%250Awe%2520propose%2520Grasp-Pretraining%2520Augmented%2520Robotic%2520Attention%2520Mamba%2520%2528GPA-RAM%2529%252C%250Adividing%2520spatial%2520task%2520learning%2520into%2520RAM%2520for%2520planning%2520skill%2520learning%2520and%2520GPA%2520for%250Agrasping%2520skill%2520learning.%2520GPA-RAM%2520demonstrates%2520superior%2520performance%2520across%2520three%250Arobot%2520systems%2520with%2520distinct%2520camera%2520configurations%2520in%2520simulation%2520and%2520the%2520real%250Aworld.%2520Compared%2520with%2520previous%2520state-of-the-art%2520methods%252C%2520it%2520improves%2520the%250Aabsolute%2520success%2520rate%2520by%25208.2%2525%2520%2528from%252079.3%2525%2520to%252087.5%2525%2529%2520on%2520the%2520RLBench%2520multi-task%250Abenchmark%2520and%252040%255C%2525%2520%2528from%252016%2525%2520to%252056%2525%2529%252C%252012%2525%2520%2528from%252086%2525%2520to%252098%2525%2529%2520on%2520the%2520ALOHA%250Abimanual%2520manipulation%2520tasks%252C%2520while%2520delivering%2520notably%2520faster%2520inference.%250AFurthermore%252C%2520experimental%2520results%2520demonstrate%2520that%2520both%2520RAM%2520and%2520GPA%2520enhance%250Atask%2520learning%252C%2520with%2520GPA%2520proving%2520robust%2520to%2520different%2520architectures%2520of%2520pretrained%250Agrasp%2520pose%2520feature%2520extractors.%2520The%2520website%2520is%253A%250Ahttps%253A//logssim.github.io/GPA%255C_RAM%255C_website/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPA-RAM%3A%20Grasp-Pretraining%20Augmented%20Robotic%20Attention%20Mamba%20for%20Spatial%0A%20%20Task%20Learning&entry.906535625=Juyi%20Sheng%20and%20Yangjun%20Liu%20and%20Sheng%20Xu%20and%20Zhixin%20Yang%20and%20Mengyuan%20Liu&entry.1292438233=%20%20Most%20existing%20robot%20manipulation%20methods%20prioritize%20task%20learning%20by%0Aenhancing%20perception%20through%20complex%20deep%20network%20architectures.%20However%2C%20they%0Aface%20challenges%20in%20real-time%20collision-free%20planning.%20Hence%2C%20Robotic%20Attention%0AMamba%20%28RAM%29%20is%20designed%20for%20refined%20planning.%20Specifically%2C%20by%20integrating%0AMamba%20and%20parallel%20single-view%20attention%2C%20RAM%20aligns%20multi-view%20vision%20and%0Atask-related%20language%20features%2C%20ensuring%20efficient%20fine-grained%20task%20planning%0Awith%20linear%20complexity%20and%20robust%20real-time%20performance.%20Nevertheless%2C%20it%20has%0Athe%20potential%20for%20further%20improvement%20in%20high-precision%20grasping%20and%0Amanipulation.%20Thus%2C%20Grasp-Pretraining%20Augmentation%20%28GPA%29%20is%20devised%2C%20with%20a%0Agrasp%20pose%20feature%20extractor%20pretrained%20utilizing%20object%20grasp%20poses%20directly%0Ainherited%20from%20whole-task%20demonstrations.%20Subsequently%2C%20the%20extracted%20grasp%0Afeatures%20are%20fused%20with%20the%20spatially%20aligned%20planning%20features%20from%20RAM%0Athrough%20attention-based%20Pre-trained%20Location%20Fusion%2C%20preserving%20high-resolution%0Agrasping%20cues%20overshadowed%20by%20an%20overemphasis%20on%20global%20planning.%20To%20summarize%2C%0Awe%20propose%20Grasp-Pretraining%20Augmented%20Robotic%20Attention%20Mamba%20%28GPA-RAM%29%2C%0Adividing%20spatial%20task%20learning%20into%20RAM%20for%20planning%20skill%20learning%20and%20GPA%20for%0Agrasping%20skill%20learning.%20GPA-RAM%20demonstrates%20superior%20performance%20across%20three%0Arobot%20systems%20with%20distinct%20camera%20configurations%20in%20simulation%20and%20the%20real%0Aworld.%20Compared%20with%20previous%20state-of-the-art%20methods%2C%20it%20improves%20the%0Aabsolute%20success%20rate%20by%208.2%25%20%28from%2079.3%25%20to%2087.5%25%29%20on%20the%20RLBench%20multi-task%0Abenchmark%20and%2040%5C%25%20%28from%2016%25%20to%2056%25%29%2C%2012%25%20%28from%2086%25%20to%2098%25%29%20on%20the%20ALOHA%0Abimanual%20manipulation%20tasks%2C%20while%20delivering%20notably%20faster%20inference.%0AFurthermore%2C%20experimental%20results%20demonstrate%20that%20both%20RAM%20and%20GPA%20enhance%0Atask%20learning%2C%20with%20GPA%20proving%20robust%20to%20different%20architectures%20of%20pretrained%0Agrasp%20pose%20feature%20extractors.%20The%20website%20is%3A%0Ahttps%3A//logssim.github.io/GPA%5C_RAM%5C_website/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19683v1&entry.124074799=Read"},
{"title": "If Concept Bottlenecks are the Question, are Foundation Models the\n  Answer?", "author": "Nicola Debole and Pietro Barbiero and Francesco Giannini and Andrea Passeggini and Stefano Teso and Emanuele Marconato", "abstract": "  Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high\nperformance with ante-hoc interpretability. CBMs work by first mapping inputs\n(e.g., images) to high-level concepts (e.g., visible objects and their\nproperties) and then use these to solve a downstream task (e.g., tagging or\nscoring an image) in an interpretable manner. Their performance and\ninterpretability, however, hinge on the quality of the concepts they learn. The\ngo-to strategy for ensuring good quality concepts is to leverage expert\nannotations, which are expensive to collect and seldom available in\napplications. Researchers have recently addressed this issue by introducing\n\"VLM-CBM\" architectures that replace manual annotations with weak supervision\nfrom foundation models. It is however unclear what is the impact of doing so on\nthe quality of the learned concepts. To answer this question, we put\nstate-of-the-art VLM-CBMs to the test, analyzing their learned concepts\nempirically using a selection of significant metrics. Our results show that,\ndepending on the task, VLM supervision can sensibly differ from expert\nannotations, and that concept accuracy and quality are not strongly correlated.\nOur code is available at https://github.com/debryu/CQA.\n", "link": "http://arxiv.org/abs/2504.19774v1", "date": "2025-04-28", "relevancy": 2.2942, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20If%20Concept%20Bottlenecks%20are%20the%20Question%2C%20are%20Foundation%20Models%20the%0A%20%20Answer%3F&body=Title%3A%20If%20Concept%20Bottlenecks%20are%20the%20Question%2C%20are%20Foundation%20Models%20the%0A%20%20Answer%3F%0AAuthor%3A%20Nicola%20Debole%20and%20Pietro%20Barbiero%20and%20Francesco%20Giannini%20and%20Andrea%20Passeggini%20and%20Stefano%20Teso%20and%20Emanuele%20Marconato%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20are%20neural%20networks%20designed%20to%20conjoin%20high%0Aperformance%20with%20ante-hoc%20interpretability.%20CBMs%20work%20by%20first%20mapping%20inputs%0A%28e.g.%2C%20images%29%20to%20high-level%20concepts%20%28e.g.%2C%20visible%20objects%20and%20their%0Aproperties%29%20and%20then%20use%20these%20to%20solve%20a%20downstream%20task%20%28e.g.%2C%20tagging%20or%0Ascoring%20an%20image%29%20in%20an%20interpretable%20manner.%20Their%20performance%20and%0Ainterpretability%2C%20however%2C%20hinge%20on%20the%20quality%20of%20the%20concepts%20they%20learn.%20The%0Ago-to%20strategy%20for%20ensuring%20good%20quality%20concepts%20is%20to%20leverage%20expert%0Aannotations%2C%20which%20are%20expensive%20to%20collect%20and%20seldom%20available%20in%0Aapplications.%20Researchers%20have%20recently%20addressed%20this%20issue%20by%20introducing%0A%22VLM-CBM%22%20architectures%20that%20replace%20manual%20annotations%20with%20weak%20supervision%0Afrom%20foundation%20models.%20It%20is%20however%20unclear%20what%20is%20the%20impact%20of%20doing%20so%20on%0Athe%20quality%20of%20the%20learned%20concepts.%20To%20answer%20this%20question%2C%20we%20put%0Astate-of-the-art%20VLM-CBMs%20to%20the%20test%2C%20analyzing%20their%20learned%20concepts%0Aempirically%20using%20a%20selection%20of%20significant%20metrics.%20Our%20results%20show%20that%2C%0Adepending%20on%20the%20task%2C%20VLM%20supervision%20can%20sensibly%20differ%20from%20expert%0Aannotations%2C%20and%20that%20concept%20accuracy%20and%20quality%20are%20not%20strongly%20correlated.%0AOur%20code%20is%20available%20at%20https%3A//github.com/debryu/CQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIf%2520Concept%2520Bottlenecks%2520are%2520the%2520Question%252C%2520are%2520Foundation%2520Models%2520the%250A%2520%2520Answer%253F%26entry.906535625%3DNicola%2520Debole%2520and%2520Pietro%2520Barbiero%2520and%2520Francesco%2520Giannini%2520and%2520Andrea%2520Passeggini%2520and%2520Stefano%2520Teso%2520and%2520Emanuele%2520Marconato%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520are%2520neural%2520networks%2520designed%2520to%2520conjoin%2520high%250Aperformance%2520with%2520ante-hoc%2520interpretability.%2520CBMs%2520work%2520by%2520first%2520mapping%2520inputs%250A%2528e.g.%252C%2520images%2529%2520to%2520high-level%2520concepts%2520%2528e.g.%252C%2520visible%2520objects%2520and%2520their%250Aproperties%2529%2520and%2520then%2520use%2520these%2520to%2520solve%2520a%2520downstream%2520task%2520%2528e.g.%252C%2520tagging%2520or%250Ascoring%2520an%2520image%2529%2520in%2520an%2520interpretable%2520manner.%2520Their%2520performance%2520and%250Ainterpretability%252C%2520however%252C%2520hinge%2520on%2520the%2520quality%2520of%2520the%2520concepts%2520they%2520learn.%2520The%250Ago-to%2520strategy%2520for%2520ensuring%2520good%2520quality%2520concepts%2520is%2520to%2520leverage%2520expert%250Aannotations%252C%2520which%2520are%2520expensive%2520to%2520collect%2520and%2520seldom%2520available%2520in%250Aapplications.%2520Researchers%2520have%2520recently%2520addressed%2520this%2520issue%2520by%2520introducing%250A%2522VLM-CBM%2522%2520architectures%2520that%2520replace%2520manual%2520annotations%2520with%2520weak%2520supervision%250Afrom%2520foundation%2520models.%2520It%2520is%2520however%2520unclear%2520what%2520is%2520the%2520impact%2520of%2520doing%2520so%2520on%250Athe%2520quality%2520of%2520the%2520learned%2520concepts.%2520To%2520answer%2520this%2520question%252C%2520we%2520put%250Astate-of-the-art%2520VLM-CBMs%2520to%2520the%2520test%252C%2520analyzing%2520their%2520learned%2520concepts%250Aempirically%2520using%2520a%2520selection%2520of%2520significant%2520metrics.%2520Our%2520results%2520show%2520that%252C%250Adepending%2520on%2520the%2520task%252C%2520VLM%2520supervision%2520can%2520sensibly%2520differ%2520from%2520expert%250Aannotations%252C%2520and%2520that%2520concept%2520accuracy%2520and%2520quality%2520are%2520not%2520strongly%2520correlated.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/debryu/CQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=If%20Concept%20Bottlenecks%20are%20the%20Question%2C%20are%20Foundation%20Models%20the%0A%20%20Answer%3F&entry.906535625=Nicola%20Debole%20and%20Pietro%20Barbiero%20and%20Francesco%20Giannini%20and%20Andrea%20Passeggini%20and%20Stefano%20Teso%20and%20Emanuele%20Marconato&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20are%20neural%20networks%20designed%20to%20conjoin%20high%0Aperformance%20with%20ante-hoc%20interpretability.%20CBMs%20work%20by%20first%20mapping%20inputs%0A%28e.g.%2C%20images%29%20to%20high-level%20concepts%20%28e.g.%2C%20visible%20objects%20and%20their%0Aproperties%29%20and%20then%20use%20these%20to%20solve%20a%20downstream%20task%20%28e.g.%2C%20tagging%20or%0Ascoring%20an%20image%29%20in%20an%20interpretable%20manner.%20Their%20performance%20and%0Ainterpretability%2C%20however%2C%20hinge%20on%20the%20quality%20of%20the%20concepts%20they%20learn.%20The%0Ago-to%20strategy%20for%20ensuring%20good%20quality%20concepts%20is%20to%20leverage%20expert%0Aannotations%2C%20which%20are%20expensive%20to%20collect%20and%20seldom%20available%20in%0Aapplications.%20Researchers%20have%20recently%20addressed%20this%20issue%20by%20introducing%0A%22VLM-CBM%22%20architectures%20that%20replace%20manual%20annotations%20with%20weak%20supervision%0Afrom%20foundation%20models.%20It%20is%20however%20unclear%20what%20is%20the%20impact%20of%20doing%20so%20on%0Athe%20quality%20of%20the%20learned%20concepts.%20To%20answer%20this%20question%2C%20we%20put%0Astate-of-the-art%20VLM-CBMs%20to%20the%20test%2C%20analyzing%20their%20learned%20concepts%0Aempirically%20using%20a%20selection%20of%20significant%20metrics.%20Our%20results%20show%20that%2C%0Adepending%20on%20the%20task%2C%20VLM%20supervision%20can%20sensibly%20differ%20from%20expert%0Aannotations%2C%20and%20that%20concept%20accuracy%20and%20quality%20are%20not%20strongly%20correlated.%0AOur%20code%20is%20available%20at%20https%3A//github.com/debryu/CQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19774v1&entry.124074799=Read"},
{"title": "STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and\n  Scene Flow Prediction", "author": "Zhimin Liao and Ping Wei and Shuaijia Chen and Haoxuan Wang and Ziyang Ren", "abstract": "  3D occupancy and scene flow offer a detailed and dynamic representation of 3D\nscene. Recognizing the sparsity and complexity of 3D space, previous\nvision-centric methods have employed implicit learning-based approaches to\nmodel spatial and temporal information. However, these approaches struggle to\ncapture local details and diminish the model's spatial discriminative ability.\nTo address these challenges, we propose a novel explicit state-based modeling\nmethod designed to leverage the occupied state to renovate the 3D features.\nSpecifically, we propose a sparse occlusion-aware attention mechanism,\nintegrated with a cascade refinement strategy, which accurately renovates 3D\nfeatures with the guidance of occupied state information. Additionally, we\nintroduce a novel method for modeling long-term dynamic interactions, which\nreduces computational costs and preserves spatial information. Compared to the\nprevious state-of-the-art methods, our efficient explicit renovation strategy\nnot only delivers superior performance in terms of RayIoU and mAVE for\noccupancy and scene flow prediction but also markedly reduces GPU memory usage\nduring training, bringing it down to 8.7GB. Our code is available on\nhttps://github.com/lzzzzzm/STCOcc\n", "link": "http://arxiv.org/abs/2504.19749v1", "date": "2025-04-28", "relevancy": 2.2752, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6029}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STCOcc%3A%20Sparse%20Spatial-Temporal%20Cascade%20Renovation%20for%203D%20Occupancy%20and%0A%20%20Scene%20Flow%20Prediction&body=Title%3A%20STCOcc%3A%20Sparse%20Spatial-Temporal%20Cascade%20Renovation%20for%203D%20Occupancy%20and%0A%20%20Scene%20Flow%20Prediction%0AAuthor%3A%20Zhimin%20Liao%20and%20Ping%20Wei%20and%20Shuaijia%20Chen%20and%20Haoxuan%20Wang%20and%20Ziyang%20Ren%0AAbstract%3A%20%20%203D%20occupancy%20and%20scene%20flow%20offer%20a%20detailed%20and%20dynamic%20representation%20of%203D%0Ascene.%20Recognizing%20the%20sparsity%20and%20complexity%20of%203D%20space%2C%20previous%0Avision-centric%20methods%20have%20employed%20implicit%20learning-based%20approaches%20to%0Amodel%20spatial%20and%20temporal%20information.%20However%2C%20these%20approaches%20struggle%20to%0Acapture%20local%20details%20and%20diminish%20the%20model%27s%20spatial%20discriminative%20ability.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20explicit%20state-based%20modeling%0Amethod%20designed%20to%20leverage%20the%20occupied%20state%20to%20renovate%20the%203D%20features.%0ASpecifically%2C%20we%20propose%20a%20sparse%20occlusion-aware%20attention%20mechanism%2C%0Aintegrated%20with%20a%20cascade%20refinement%20strategy%2C%20which%20accurately%20renovates%203D%0Afeatures%20with%20the%20guidance%20of%20occupied%20state%20information.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20method%20for%20modeling%20long-term%20dynamic%20interactions%2C%20which%0Areduces%20computational%20costs%20and%20preserves%20spatial%20information.%20Compared%20to%20the%0Aprevious%20state-of-the-art%20methods%2C%20our%20efficient%20explicit%20renovation%20strategy%0Anot%20only%20delivers%20superior%20performance%20in%20terms%20of%20RayIoU%20and%20mAVE%20for%0Aoccupancy%20and%20scene%20flow%20prediction%20but%20also%20markedly%20reduces%20GPU%20memory%20usage%0Aduring%20training%2C%20bringing%20it%20down%20to%208.7GB.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/lzzzzzm/STCOcc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTCOcc%253A%2520Sparse%2520Spatial-Temporal%2520Cascade%2520Renovation%2520for%25203D%2520Occupancy%2520and%250A%2520%2520Scene%2520Flow%2520Prediction%26entry.906535625%3DZhimin%2520Liao%2520and%2520Ping%2520Wei%2520and%2520Shuaijia%2520Chen%2520and%2520Haoxuan%2520Wang%2520and%2520Ziyang%2520Ren%26entry.1292438233%3D%2520%25203D%2520occupancy%2520and%2520scene%2520flow%2520offer%2520a%2520detailed%2520and%2520dynamic%2520representation%2520of%25203D%250Ascene.%2520Recognizing%2520the%2520sparsity%2520and%2520complexity%2520of%25203D%2520space%252C%2520previous%250Avision-centric%2520methods%2520have%2520employed%2520implicit%2520learning-based%2520approaches%2520to%250Amodel%2520spatial%2520and%2520temporal%2520information.%2520However%252C%2520these%2520approaches%2520struggle%2520to%250Acapture%2520local%2520details%2520and%2520diminish%2520the%2520model%2527s%2520spatial%2520discriminative%2520ability.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520explicit%2520state-based%2520modeling%250Amethod%2520designed%2520to%2520leverage%2520the%2520occupied%2520state%2520to%2520renovate%2520the%25203D%2520features.%250ASpecifically%252C%2520we%2520propose%2520a%2520sparse%2520occlusion-aware%2520attention%2520mechanism%252C%250Aintegrated%2520with%2520a%2520cascade%2520refinement%2520strategy%252C%2520which%2520accurately%2520renovates%25203D%250Afeatures%2520with%2520the%2520guidance%2520of%2520occupied%2520state%2520information.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520novel%2520method%2520for%2520modeling%2520long-term%2520dynamic%2520interactions%252C%2520which%250Areduces%2520computational%2520costs%2520and%2520preserves%2520spatial%2520information.%2520Compared%2520to%2520the%250Aprevious%2520state-of-the-art%2520methods%252C%2520our%2520efficient%2520explicit%2520renovation%2520strategy%250Anot%2520only%2520delivers%2520superior%2520performance%2520in%2520terms%2520of%2520RayIoU%2520and%2520mAVE%2520for%250Aoccupancy%2520and%2520scene%2520flow%2520prediction%2520but%2520also%2520markedly%2520reduces%2520GPU%2520memory%2520usage%250Aduring%2520training%252C%2520bringing%2520it%2520down%2520to%25208.7GB.%2520Our%2520code%2520is%2520available%2520on%250Ahttps%253A//github.com/lzzzzzm/STCOcc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STCOcc%3A%20Sparse%20Spatial-Temporal%20Cascade%20Renovation%20for%203D%20Occupancy%20and%0A%20%20Scene%20Flow%20Prediction&entry.906535625=Zhimin%20Liao%20and%20Ping%20Wei%20and%20Shuaijia%20Chen%20and%20Haoxuan%20Wang%20and%20Ziyang%20Ren&entry.1292438233=%20%203D%20occupancy%20and%20scene%20flow%20offer%20a%20detailed%20and%20dynamic%20representation%20of%203D%0Ascene.%20Recognizing%20the%20sparsity%20and%20complexity%20of%203D%20space%2C%20previous%0Avision-centric%20methods%20have%20employed%20implicit%20learning-based%20approaches%20to%0Amodel%20spatial%20and%20temporal%20information.%20However%2C%20these%20approaches%20struggle%20to%0Acapture%20local%20details%20and%20diminish%20the%20model%27s%20spatial%20discriminative%20ability.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20explicit%20state-based%20modeling%0Amethod%20designed%20to%20leverage%20the%20occupied%20state%20to%20renovate%20the%203D%20features.%0ASpecifically%2C%20we%20propose%20a%20sparse%20occlusion-aware%20attention%20mechanism%2C%0Aintegrated%20with%20a%20cascade%20refinement%20strategy%2C%20which%20accurately%20renovates%203D%0Afeatures%20with%20the%20guidance%20of%20occupied%20state%20information.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20method%20for%20modeling%20long-term%20dynamic%20interactions%2C%20which%0Areduces%20computational%20costs%20and%20preserves%20spatial%20information.%20Compared%20to%20the%0Aprevious%20state-of-the-art%20methods%2C%20our%20efficient%20explicit%20renovation%20strategy%0Anot%20only%20delivers%20superior%20performance%20in%20terms%20of%20RayIoU%20and%20mAVE%20for%0Aoccupancy%20and%20scene%20flow%20prediction%20but%20also%20markedly%20reduces%20GPU%20memory%20usage%0Aduring%20training%2C%20bringing%20it%20down%20to%208.7GB.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/lzzzzzm/STCOcc%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19749v1&entry.124074799=Read"},
{"title": "Robot Motion Planning using One-Step Diffusion with Noise-Optimized\n  Approximate Motions", "author": "Tomoharu Aizu and Takeru Oba and Yuki Kondo and Norimichi Ukita", "abstract": "  This paper proposes an image-based robot motion planning method using a\none-step diffusion model. While the diffusion model allows for high-quality\nmotion generation, its computational cost is too expensive to control a robot\nin real time. To achieve high quality and efficiency simultaneously, our\none-step diffusion model takes an approximately generated motion, which is\npredicted directly from input images. This approximate motion is optimized by\nadditive noise provided by our novel noise optimizer. Unlike general isotropic\nnoise, our noise optimizer adjusts noise anisotropically depending on the\nuncertainty of each motion element. Our experimental results demonstrate that\nour method outperforms state-of-the-art methods while maintaining its\nefficiency by one-step diffusion.\n", "link": "http://arxiv.org/abs/2504.19652v1", "date": "2025-04-28", "relevancy": 2.2623, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5702}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5677}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robot%20Motion%20Planning%20using%20One-Step%20Diffusion%20with%20Noise-Optimized%0A%20%20Approximate%20Motions&body=Title%3A%20Robot%20Motion%20Planning%20using%20One-Step%20Diffusion%20with%20Noise-Optimized%0A%20%20Approximate%20Motions%0AAuthor%3A%20Tomoharu%20Aizu%20and%20Takeru%20Oba%20and%20Yuki%20Kondo%20and%20Norimichi%20Ukita%0AAbstract%3A%20%20%20This%20paper%20proposes%20an%20image-based%20robot%20motion%20planning%20method%20using%20a%0Aone-step%20diffusion%20model.%20While%20the%20diffusion%20model%20allows%20for%20high-quality%0Amotion%20generation%2C%20its%20computational%20cost%20is%20too%20expensive%20to%20control%20a%20robot%0Ain%20real%20time.%20To%20achieve%20high%20quality%20and%20efficiency%20simultaneously%2C%20our%0Aone-step%20diffusion%20model%20takes%20an%20approximately%20generated%20motion%2C%20which%20is%0Apredicted%20directly%20from%20input%20images.%20This%20approximate%20motion%20is%20optimized%20by%0Aadditive%20noise%20provided%20by%20our%20novel%20noise%20optimizer.%20Unlike%20general%20isotropic%0Anoise%2C%20our%20noise%20optimizer%20adjusts%20noise%20anisotropically%20depending%20on%20the%0Auncertainty%20of%20each%20motion%20element.%20Our%20experimental%20results%20demonstrate%20that%0Aour%20method%20outperforms%20state-of-the-art%20methods%20while%20maintaining%20its%0Aefficiency%20by%20one-step%20diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobot%2520Motion%2520Planning%2520using%2520One-Step%2520Diffusion%2520with%2520Noise-Optimized%250A%2520%2520Approximate%2520Motions%26entry.906535625%3DTomoharu%2520Aizu%2520and%2520Takeru%2520Oba%2520and%2520Yuki%2520Kondo%2520and%2520Norimichi%2520Ukita%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520an%2520image-based%2520robot%2520motion%2520planning%2520method%2520using%2520a%250Aone-step%2520diffusion%2520model.%2520While%2520the%2520diffusion%2520model%2520allows%2520for%2520high-quality%250Amotion%2520generation%252C%2520its%2520computational%2520cost%2520is%2520too%2520expensive%2520to%2520control%2520a%2520robot%250Ain%2520real%2520time.%2520To%2520achieve%2520high%2520quality%2520and%2520efficiency%2520simultaneously%252C%2520our%250Aone-step%2520diffusion%2520model%2520takes%2520an%2520approximately%2520generated%2520motion%252C%2520which%2520is%250Apredicted%2520directly%2520from%2520input%2520images.%2520This%2520approximate%2520motion%2520is%2520optimized%2520by%250Aadditive%2520noise%2520provided%2520by%2520our%2520novel%2520noise%2520optimizer.%2520Unlike%2520general%2520isotropic%250Anoise%252C%2520our%2520noise%2520optimizer%2520adjusts%2520noise%2520anisotropically%2520depending%2520on%2520the%250Auncertainty%2520of%2520each%2520motion%2520element.%2520Our%2520experimental%2520results%2520demonstrate%2520that%250Aour%2520method%2520outperforms%2520state-of-the-art%2520methods%2520while%2520maintaining%2520its%250Aefficiency%2520by%2520one-step%2520diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot%20Motion%20Planning%20using%20One-Step%20Diffusion%20with%20Noise-Optimized%0A%20%20Approximate%20Motions&entry.906535625=Tomoharu%20Aizu%20and%20Takeru%20Oba%20and%20Yuki%20Kondo%20and%20Norimichi%20Ukita&entry.1292438233=%20%20This%20paper%20proposes%20an%20image-based%20robot%20motion%20planning%20method%20using%20a%0Aone-step%20diffusion%20model.%20While%20the%20diffusion%20model%20allows%20for%20high-quality%0Amotion%20generation%2C%20its%20computational%20cost%20is%20too%20expensive%20to%20control%20a%20robot%0Ain%20real%20time.%20To%20achieve%20high%20quality%20and%20efficiency%20simultaneously%2C%20our%0Aone-step%20diffusion%20model%20takes%20an%20approximately%20generated%20motion%2C%20which%20is%0Apredicted%20directly%20from%20input%20images.%20This%20approximate%20motion%20is%20optimized%20by%0Aadditive%20noise%20provided%20by%20our%20novel%20noise%20optimizer.%20Unlike%20general%20isotropic%0Anoise%2C%20our%20noise%20optimizer%20adjusts%20noise%20anisotropically%20depending%20on%20the%0Auncertainty%20of%20each%20motion%20element.%20Our%20experimental%20results%20demonstrate%20that%0Aour%20method%20outperforms%20state-of-the-art%20methods%20while%20maintaining%20its%0Aefficiency%20by%20one-step%20diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19652v1&entry.124074799=Read"},
{"title": "Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast\n  Videos via Physically Grounded Synthetic-to-Real Transfer", "author": "Daniel Kienzle and Robin Sch\u00f6n and Rainer Lienhart and Shin'Ichi Satoh", "abstract": "  Analyzing a player's technique in table tennis requires knowledge of the\nball's 3D trajectory and spin. While, the spin is not directly observable in\nstandard broadcasting videos, we show that it can be inferred from the ball's\ntrajectory in the video. We present a novel method to infer the initial spin\nand 3D trajectory from the corresponding 2D trajectory in a video. Without\nground truth labels for broadcast videos, we train a neural network solely on\nsynthetic data. Due to the choice of our input data representation, physically\ncorrect synthetic training data, and using targeted augmentations, the network\nnaturally generalizes to real data. Notably, these simple techniques are\nsufficient to achieve generalization. No real data at all is required for\ntraining. To the best of our knowledge, we are the first to present a method\nfor spin and trajectory prediction in simple monocular broadcast videos,\nachieving an accuracy of 92.0% in spin classification and a 2D reprojection\nerror of 0.19% of the image diagonal.\n", "link": "http://arxiv.org/abs/2504.19863v1", "date": "2025-04-28", "relevancy": 2.2263, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5582}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5559}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Ball%20Spin%20and%20Trajectory%20Analysis%20in%20Table%20Tennis%20Broadcast%0A%20%20Videos%20via%20Physically%20Grounded%20Synthetic-to-Real%20Transfer&body=Title%3A%20Towards%20Ball%20Spin%20and%20Trajectory%20Analysis%20in%20Table%20Tennis%20Broadcast%0A%20%20Videos%20via%20Physically%20Grounded%20Synthetic-to-Real%20Transfer%0AAuthor%3A%20Daniel%20Kienzle%20and%20Robin%20Sch%C3%B6n%20and%20Rainer%20Lienhart%20and%20Shin%27Ichi%20Satoh%0AAbstract%3A%20%20%20Analyzing%20a%20player%27s%20technique%20in%20table%20tennis%20requires%20knowledge%20of%20the%0Aball%27s%203D%20trajectory%20and%20spin.%20While%2C%20the%20spin%20is%20not%20directly%20observable%20in%0Astandard%20broadcasting%20videos%2C%20we%20show%20that%20it%20can%20be%20inferred%20from%20the%20ball%27s%0Atrajectory%20in%20the%20video.%20We%20present%20a%20novel%20method%20to%20infer%20the%20initial%20spin%0Aand%203D%20trajectory%20from%20the%20corresponding%202D%20trajectory%20in%20a%20video.%20Without%0Aground%20truth%20labels%20for%20broadcast%20videos%2C%20we%20train%20a%20neural%20network%20solely%20on%0Asynthetic%20data.%20Due%20to%20the%20choice%20of%20our%20input%20data%20representation%2C%20physically%0Acorrect%20synthetic%20training%20data%2C%20and%20using%20targeted%20augmentations%2C%20the%20network%0Anaturally%20generalizes%20to%20real%20data.%20Notably%2C%20these%20simple%20techniques%20are%0Asufficient%20to%20achieve%20generalization.%20No%20real%20data%20at%20all%20is%20required%20for%0Atraining.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20present%20a%20method%0Afor%20spin%20and%20trajectory%20prediction%20in%20simple%20monocular%20broadcast%20videos%2C%0Aachieving%20an%20accuracy%20of%2092.0%25%20in%20spin%20classification%20and%20a%202D%20reprojection%0Aerror%20of%200.19%25%20of%20the%20image%20diagonal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Ball%2520Spin%2520and%2520Trajectory%2520Analysis%2520in%2520Table%2520Tennis%2520Broadcast%250A%2520%2520Videos%2520via%2520Physically%2520Grounded%2520Synthetic-to-Real%2520Transfer%26entry.906535625%3DDaniel%2520Kienzle%2520and%2520Robin%2520Sch%25C3%25B6n%2520and%2520Rainer%2520Lienhart%2520and%2520Shin%2527Ichi%2520Satoh%26entry.1292438233%3D%2520%2520Analyzing%2520a%2520player%2527s%2520technique%2520in%2520table%2520tennis%2520requires%2520knowledge%2520of%2520the%250Aball%2527s%25203D%2520trajectory%2520and%2520spin.%2520While%252C%2520the%2520spin%2520is%2520not%2520directly%2520observable%2520in%250Astandard%2520broadcasting%2520videos%252C%2520we%2520show%2520that%2520it%2520can%2520be%2520inferred%2520from%2520the%2520ball%2527s%250Atrajectory%2520in%2520the%2520video.%2520We%2520present%2520a%2520novel%2520method%2520to%2520infer%2520the%2520initial%2520spin%250Aand%25203D%2520trajectory%2520from%2520the%2520corresponding%25202D%2520trajectory%2520in%2520a%2520video.%2520Without%250Aground%2520truth%2520labels%2520for%2520broadcast%2520videos%252C%2520we%2520train%2520a%2520neural%2520network%2520solely%2520on%250Asynthetic%2520data.%2520Due%2520to%2520the%2520choice%2520of%2520our%2520input%2520data%2520representation%252C%2520physically%250Acorrect%2520synthetic%2520training%2520data%252C%2520and%2520using%2520targeted%2520augmentations%252C%2520the%2520network%250Anaturally%2520generalizes%2520to%2520real%2520data.%2520Notably%252C%2520these%2520simple%2520techniques%2520are%250Asufficient%2520to%2520achieve%2520generalization.%2520No%2520real%2520data%2520at%2520all%2520is%2520required%2520for%250Atraining.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520present%2520a%2520method%250Afor%2520spin%2520and%2520trajectory%2520prediction%2520in%2520simple%2520monocular%2520broadcast%2520videos%252C%250Aachieving%2520an%2520accuracy%2520of%252092.0%2525%2520in%2520spin%2520classification%2520and%2520a%25202D%2520reprojection%250Aerror%2520of%25200.19%2525%2520of%2520the%2520image%2520diagonal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Ball%20Spin%20and%20Trajectory%20Analysis%20in%20Table%20Tennis%20Broadcast%0A%20%20Videos%20via%20Physically%20Grounded%20Synthetic-to-Real%20Transfer&entry.906535625=Daniel%20Kienzle%20and%20Robin%20Sch%C3%B6n%20and%20Rainer%20Lienhart%20and%20Shin%27Ichi%20Satoh&entry.1292438233=%20%20Analyzing%20a%20player%27s%20technique%20in%20table%20tennis%20requires%20knowledge%20of%20the%0Aball%27s%203D%20trajectory%20and%20spin.%20While%2C%20the%20spin%20is%20not%20directly%20observable%20in%0Astandard%20broadcasting%20videos%2C%20we%20show%20that%20it%20can%20be%20inferred%20from%20the%20ball%27s%0Atrajectory%20in%20the%20video.%20We%20present%20a%20novel%20method%20to%20infer%20the%20initial%20spin%0Aand%203D%20trajectory%20from%20the%20corresponding%202D%20trajectory%20in%20a%20video.%20Without%0Aground%20truth%20labels%20for%20broadcast%20videos%2C%20we%20train%20a%20neural%20network%20solely%20on%0Asynthetic%20data.%20Due%20to%20the%20choice%20of%20our%20input%20data%20representation%2C%20physically%0Acorrect%20synthetic%20training%20data%2C%20and%20using%20targeted%20augmentations%2C%20the%20network%0Anaturally%20generalizes%20to%20real%20data.%20Notably%2C%20these%20simple%20techniques%20are%0Asufficient%20to%20achieve%20generalization.%20No%20real%20data%20at%20all%20is%20required%20for%0Atraining.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20present%20a%20method%0Afor%20spin%20and%20trajectory%20prediction%20in%20simple%20monocular%20broadcast%20videos%2C%0Aachieving%20an%20accuracy%20of%2092.0%25%20in%20spin%20classification%20and%20a%202D%20reprojection%0Aerror%20of%200.19%25%20of%20the%20image%20diagonal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19863v1&entry.124074799=Read"},
{"title": "Real-Time Imitation of Human Head Motions, Blinks and Emotions by Nao\n  Robot: A Closed-Loop Approach", "author": "Keyhan Rayati and Amirhossein Feizi and Alireza Beigy and Pourya Shahverdi and Mehdi Tale Masouleh and Ahmad Kalhor", "abstract": "  This paper introduces a novel approach for enabling real-time imitation of\nhuman head motion by a Nao robot, with a primary focus on elevating human-robot\ninteractions. By using the robust capabilities of the MediaPipe as a computer\nvision library and the DeepFace as an emotion recognition library, this\nresearch endeavors to capture the subtleties of human head motion, including\nblink actions and emotional expressions, and seamlessly incorporate these\nindicators into the robot's responses. The result is a comprehensive framework\nwhich facilitates precise head imitation within human-robot interactions,\nutilizing a closed-loop approach that involves gathering real-time feedback\nfrom the robot's imitation performance. This feedback loop ensures a high\ndegree of accuracy in modeling head motion, as evidenced by an impressive R2\nscore of 96.3 for pitch and 98.9 for yaw. Notably, the proposed approach holds\npromise in improving communication for children with autism, offering them a\nvaluable tool for more effective interaction. In essence, proposed work\nexplores the integration of real-time head imitation and real-time emotion\nrecognition to enhance human-robot interactions, with potential benefits for\nindividuals with unique communication needs.\n", "link": "http://arxiv.org/abs/2504.19985v1", "date": "2025-04-28", "relevancy": 2.1983, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.549}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Imitation%20of%20Human%20Head%20Motions%2C%20Blinks%20and%20Emotions%20by%20Nao%0A%20%20Robot%3A%20A%20Closed-Loop%20Approach&body=Title%3A%20Real-Time%20Imitation%20of%20Human%20Head%20Motions%2C%20Blinks%20and%20Emotions%20by%20Nao%0A%20%20Robot%3A%20A%20Closed-Loop%20Approach%0AAuthor%3A%20Keyhan%20Rayati%20and%20Amirhossein%20Feizi%20and%20Alireza%20Beigy%20and%20Pourya%20Shahverdi%20and%20Mehdi%20Tale%20Masouleh%20and%20Ahmad%20Kalhor%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20for%20enabling%20real-time%20imitation%20of%0Ahuman%20head%20motion%20by%20a%20Nao%20robot%2C%20with%20a%20primary%20focus%20on%20elevating%20human-robot%0Ainteractions.%20By%20using%20the%20robust%20capabilities%20of%20the%20MediaPipe%20as%20a%20computer%0Avision%20library%20and%20the%20DeepFace%20as%20an%20emotion%20recognition%20library%2C%20this%0Aresearch%20endeavors%20to%20capture%20the%20subtleties%20of%20human%20head%20motion%2C%20including%0Ablink%20actions%20and%20emotional%20expressions%2C%20and%20seamlessly%20incorporate%20these%0Aindicators%20into%20the%20robot%27s%20responses.%20The%20result%20is%20a%20comprehensive%20framework%0Awhich%20facilitates%20precise%20head%20imitation%20within%20human-robot%20interactions%2C%0Autilizing%20a%20closed-loop%20approach%20that%20involves%20gathering%20real-time%20feedback%0Afrom%20the%20robot%27s%20imitation%20performance.%20This%20feedback%20loop%20ensures%20a%20high%0Adegree%20of%20accuracy%20in%20modeling%20head%20motion%2C%20as%20evidenced%20by%20an%20impressive%20R2%0Ascore%20of%2096.3%20for%20pitch%20and%2098.9%20for%20yaw.%20Notably%2C%20the%20proposed%20approach%20holds%0Apromise%20in%20improving%20communication%20for%20children%20with%20autism%2C%20offering%20them%20a%0Avaluable%20tool%20for%20more%20effective%20interaction.%20In%20essence%2C%20proposed%20work%0Aexplores%20the%20integration%20of%20real-time%20head%20imitation%20and%20real-time%20emotion%0Arecognition%20to%20enhance%20human-robot%20interactions%2C%20with%20potential%20benefits%20for%0Aindividuals%20with%20unique%20communication%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Imitation%2520of%2520Human%2520Head%2520Motions%252C%2520Blinks%2520and%2520Emotions%2520by%2520Nao%250A%2520%2520Robot%253A%2520A%2520Closed-Loop%2520Approach%26entry.906535625%3DKeyhan%2520Rayati%2520and%2520Amirhossein%2520Feizi%2520and%2520Alireza%2520Beigy%2520and%2520Pourya%2520Shahverdi%2520and%2520Mehdi%2520Tale%2520Masouleh%2520and%2520Ahmad%2520Kalhor%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520for%2520enabling%2520real-time%2520imitation%2520of%250Ahuman%2520head%2520motion%2520by%2520a%2520Nao%2520robot%252C%2520with%2520a%2520primary%2520focus%2520on%2520elevating%2520human-robot%250Ainteractions.%2520By%2520using%2520the%2520robust%2520capabilities%2520of%2520the%2520MediaPipe%2520as%2520a%2520computer%250Avision%2520library%2520and%2520the%2520DeepFace%2520as%2520an%2520emotion%2520recognition%2520library%252C%2520this%250Aresearch%2520endeavors%2520to%2520capture%2520the%2520subtleties%2520of%2520human%2520head%2520motion%252C%2520including%250Ablink%2520actions%2520and%2520emotional%2520expressions%252C%2520and%2520seamlessly%2520incorporate%2520these%250Aindicators%2520into%2520the%2520robot%2527s%2520responses.%2520The%2520result%2520is%2520a%2520comprehensive%2520framework%250Awhich%2520facilitates%2520precise%2520head%2520imitation%2520within%2520human-robot%2520interactions%252C%250Autilizing%2520a%2520closed-loop%2520approach%2520that%2520involves%2520gathering%2520real-time%2520feedback%250Afrom%2520the%2520robot%2527s%2520imitation%2520performance.%2520This%2520feedback%2520loop%2520ensures%2520a%2520high%250Adegree%2520of%2520accuracy%2520in%2520modeling%2520head%2520motion%252C%2520as%2520evidenced%2520by%2520an%2520impressive%2520R2%250Ascore%2520of%252096.3%2520for%2520pitch%2520and%252098.9%2520for%2520yaw.%2520Notably%252C%2520the%2520proposed%2520approach%2520holds%250Apromise%2520in%2520improving%2520communication%2520for%2520children%2520with%2520autism%252C%2520offering%2520them%2520a%250Avaluable%2520tool%2520for%2520more%2520effective%2520interaction.%2520In%2520essence%252C%2520proposed%2520work%250Aexplores%2520the%2520integration%2520of%2520real-time%2520head%2520imitation%2520and%2520real-time%2520emotion%250Arecognition%2520to%2520enhance%2520human-robot%2520interactions%252C%2520with%2520potential%2520benefits%2520for%250Aindividuals%2520with%2520unique%2520communication%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Imitation%20of%20Human%20Head%20Motions%2C%20Blinks%20and%20Emotions%20by%20Nao%0A%20%20Robot%3A%20A%20Closed-Loop%20Approach&entry.906535625=Keyhan%20Rayati%20and%20Amirhossein%20Feizi%20and%20Alireza%20Beigy%20and%20Pourya%20Shahverdi%20and%20Mehdi%20Tale%20Masouleh%20and%20Ahmad%20Kalhor&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20for%20enabling%20real-time%20imitation%20of%0Ahuman%20head%20motion%20by%20a%20Nao%20robot%2C%20with%20a%20primary%20focus%20on%20elevating%20human-robot%0Ainteractions.%20By%20using%20the%20robust%20capabilities%20of%20the%20MediaPipe%20as%20a%20computer%0Avision%20library%20and%20the%20DeepFace%20as%20an%20emotion%20recognition%20library%2C%20this%0Aresearch%20endeavors%20to%20capture%20the%20subtleties%20of%20human%20head%20motion%2C%20including%0Ablink%20actions%20and%20emotional%20expressions%2C%20and%20seamlessly%20incorporate%20these%0Aindicators%20into%20the%20robot%27s%20responses.%20The%20result%20is%20a%20comprehensive%20framework%0Awhich%20facilitates%20precise%20head%20imitation%20within%20human-robot%20interactions%2C%0Autilizing%20a%20closed-loop%20approach%20that%20involves%20gathering%20real-time%20feedback%0Afrom%20the%20robot%27s%20imitation%20performance.%20This%20feedback%20loop%20ensures%20a%20high%0Adegree%20of%20accuracy%20in%20modeling%20head%20motion%2C%20as%20evidenced%20by%20an%20impressive%20R2%0Ascore%20of%2096.3%20for%20pitch%20and%2098.9%20for%20yaw.%20Notably%2C%20the%20proposed%20approach%20holds%0Apromise%20in%20improving%20communication%20for%20children%20with%20autism%2C%20offering%20them%20a%0Avaluable%20tool%20for%20more%20effective%20interaction.%20In%20essence%2C%20proposed%20work%0Aexplores%20the%20integration%20of%20real-time%20head%20imitation%20and%20real-time%20emotion%0Arecognition%20to%20enhance%20human-robot%20interactions%2C%20with%20potential%20benefits%20for%0Aindividuals%20with%20unique%20communication%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19985v1&entry.124074799=Read"},
{"title": "Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation", "author": "Hugo Porta and Emanuele Dalsasso and Diego Marcos and Devis Tuia", "abstract": "  Prototypical part learning is emerging as a promising approach for making\nsemantic segmentation interpretable. The model selects real patches seen during\ntraining as prototypes and constructs the dense prediction map based on the\nsimilarity between parts of the test image and the prototypes. This improves\ninterpretability since the user can inspect the link between the predicted\noutput and the patterns learned by the model in terms of prototypical\ninformation. In this paper, we propose a method for interpretable semantic\nsegmentation that leverages multi-scale image representation for prototypical\npart learning. First, we introduce a prototype layer that explicitly learns\ndiverse prototypical parts at several scales, leading to multi-scale\nrepresentations in the prototype activation output. Then, we propose a sparse\ngrouping mechanism that produces multi-scale sparse groups of these\nscale-specific prototypical parts. This provides a deeper understanding of the\ninteractions between multi-scale object representations while enhancing the\ninterpretability of the segmentation model. The experiments conducted on Pascal\nVOC, Cityscapes, and ADE20K demonstrate that the proposed method increases\nmodel sparsity, improves interpretability over existing prototype-based\nmethods, and narrows the performance gap with the non-interpretable counterpart\nmodels. Code is available at github.com/eceo-epfl/ScaleProtoSeg.\n", "link": "http://arxiv.org/abs/2409.09497v2", "date": "2025-04-28", "relevancy": 2.1716, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5664}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Grouped%20Prototypes%20for%20Interpretable%20Semantic%20Segmentation&body=Title%3A%20Multi-Scale%20Grouped%20Prototypes%20for%20Interpretable%20Semantic%20Segmentation%0AAuthor%3A%20Hugo%20Porta%20and%20Emanuele%20Dalsasso%20and%20Diego%20Marcos%20and%20Devis%20Tuia%0AAbstract%3A%20%20%20Prototypical%20part%20learning%20is%20emerging%20as%20a%20promising%20approach%20for%20making%0Asemantic%20segmentation%20interpretable.%20The%20model%20selects%20real%20patches%20seen%20during%0Atraining%20as%20prototypes%20and%20constructs%20the%20dense%20prediction%20map%20based%20on%20the%0Asimilarity%20between%20parts%20of%20the%20test%20image%20and%20the%20prototypes.%20This%20improves%0Ainterpretability%20since%20the%20user%20can%20inspect%20the%20link%20between%20the%20predicted%0Aoutput%20and%20the%20patterns%20learned%20by%20the%20model%20in%20terms%20of%20prototypical%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20interpretable%20semantic%0Asegmentation%20that%20leverages%20multi-scale%20image%20representation%20for%20prototypical%0Apart%20learning.%20First%2C%20we%20introduce%20a%20prototype%20layer%20that%20explicitly%20learns%0Adiverse%20prototypical%20parts%20at%20several%20scales%2C%20leading%20to%20multi-scale%0Arepresentations%20in%20the%20prototype%20activation%20output.%20Then%2C%20we%20propose%20a%20sparse%0Agrouping%20mechanism%20that%20produces%20multi-scale%20sparse%20groups%20of%20these%0Ascale-specific%20prototypical%20parts.%20This%20provides%20a%20deeper%20understanding%20of%20the%0Ainteractions%20between%20multi-scale%20object%20representations%20while%20enhancing%20the%0Ainterpretability%20of%20the%20segmentation%20model.%20The%20experiments%20conducted%20on%20Pascal%0AVOC%2C%20Cityscapes%2C%20and%20ADE20K%20demonstrate%20that%20the%20proposed%20method%20increases%0Amodel%20sparsity%2C%20improves%20interpretability%20over%20existing%20prototype-based%0Amethods%2C%20and%20narrows%20the%20performance%20gap%20with%20the%20non-interpretable%20counterpart%0Amodels.%20Code%20is%20available%20at%20github.com/eceo-epfl/ScaleProtoSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09497v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Grouped%2520Prototypes%2520for%2520Interpretable%2520Semantic%2520Segmentation%26entry.906535625%3DHugo%2520Porta%2520and%2520Emanuele%2520Dalsasso%2520and%2520Diego%2520Marcos%2520and%2520Devis%2520Tuia%26entry.1292438233%3D%2520%2520Prototypical%2520part%2520learning%2520is%2520emerging%2520as%2520a%2520promising%2520approach%2520for%2520making%250Asemantic%2520segmentation%2520interpretable.%2520The%2520model%2520selects%2520real%2520patches%2520seen%2520during%250Atraining%2520as%2520prototypes%2520and%2520constructs%2520the%2520dense%2520prediction%2520map%2520based%2520on%2520the%250Asimilarity%2520between%2520parts%2520of%2520the%2520test%2520image%2520and%2520the%2520prototypes.%2520This%2520improves%250Ainterpretability%2520since%2520the%2520user%2520can%2520inspect%2520the%2520link%2520between%2520the%2520predicted%250Aoutput%2520and%2520the%2520patterns%2520learned%2520by%2520the%2520model%2520in%2520terms%2520of%2520prototypical%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520for%2520interpretable%2520semantic%250Asegmentation%2520that%2520leverages%2520multi-scale%2520image%2520representation%2520for%2520prototypical%250Apart%2520learning.%2520First%252C%2520we%2520introduce%2520a%2520prototype%2520layer%2520that%2520explicitly%2520learns%250Adiverse%2520prototypical%2520parts%2520at%2520several%2520scales%252C%2520leading%2520to%2520multi-scale%250Arepresentations%2520in%2520the%2520prototype%2520activation%2520output.%2520Then%252C%2520we%2520propose%2520a%2520sparse%250Agrouping%2520mechanism%2520that%2520produces%2520multi-scale%2520sparse%2520groups%2520of%2520these%250Ascale-specific%2520prototypical%2520parts.%2520This%2520provides%2520a%2520deeper%2520understanding%2520of%2520the%250Ainteractions%2520between%2520multi-scale%2520object%2520representations%2520while%2520enhancing%2520the%250Ainterpretability%2520of%2520the%2520segmentation%2520model.%2520The%2520experiments%2520conducted%2520on%2520Pascal%250AVOC%252C%2520Cityscapes%252C%2520and%2520ADE20K%2520demonstrate%2520that%2520the%2520proposed%2520method%2520increases%250Amodel%2520sparsity%252C%2520improves%2520interpretability%2520over%2520existing%2520prototype-based%250Amethods%252C%2520and%2520narrows%2520the%2520performance%2520gap%2520with%2520the%2520non-interpretable%2520counterpart%250Amodels.%2520Code%2520is%2520available%2520at%2520github.com/eceo-epfl/ScaleProtoSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09497v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Grouped%20Prototypes%20for%20Interpretable%20Semantic%20Segmentation&entry.906535625=Hugo%20Porta%20and%20Emanuele%20Dalsasso%20and%20Diego%20Marcos%20and%20Devis%20Tuia&entry.1292438233=%20%20Prototypical%20part%20learning%20is%20emerging%20as%20a%20promising%20approach%20for%20making%0Asemantic%20segmentation%20interpretable.%20The%20model%20selects%20real%20patches%20seen%20during%0Atraining%20as%20prototypes%20and%20constructs%20the%20dense%20prediction%20map%20based%20on%20the%0Asimilarity%20between%20parts%20of%20the%20test%20image%20and%20the%20prototypes.%20This%20improves%0Ainterpretability%20since%20the%20user%20can%20inspect%20the%20link%20between%20the%20predicted%0Aoutput%20and%20the%20patterns%20learned%20by%20the%20model%20in%20terms%20of%20prototypical%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20interpretable%20semantic%0Asegmentation%20that%20leverages%20multi-scale%20image%20representation%20for%20prototypical%0Apart%20learning.%20First%2C%20we%20introduce%20a%20prototype%20layer%20that%20explicitly%20learns%0Adiverse%20prototypical%20parts%20at%20several%20scales%2C%20leading%20to%20multi-scale%0Arepresentations%20in%20the%20prototype%20activation%20output.%20Then%2C%20we%20propose%20a%20sparse%0Agrouping%20mechanism%20that%20produces%20multi-scale%20sparse%20groups%20of%20these%0Ascale-specific%20prototypical%20parts.%20This%20provides%20a%20deeper%20understanding%20of%20the%0Ainteractions%20between%20multi-scale%20object%20representations%20while%20enhancing%20the%0Ainterpretability%20of%20the%20segmentation%20model.%20The%20experiments%20conducted%20on%20Pascal%0AVOC%2C%20Cityscapes%2C%20and%20ADE20K%20demonstrate%20that%20the%20proposed%20method%20increases%0Amodel%20sparsity%2C%20improves%20interpretability%20over%20existing%20prototype-based%0Amethods%2C%20and%20narrows%20the%20performance%20gap%20with%20the%20non-interpretable%20counterpart%0Amodels.%20Code%20is%20available%20at%20github.com/eceo-epfl/ScaleProtoSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09497v2&entry.124074799=Read"},
{"title": "CompleteMe: Reference-based Human Image Completion", "author": "Yu-Ju Tsai and Brian Price and Qing Liu and Luis Figueroa and Daniil Pakhomov and Zhihong Ding and Scott Cohen and Ming-Hsuan Yang", "abstract": "  Recent methods for human image completion can reconstruct plausible body\nshapes but often fail to preserve unique details, such as specific clothing\npatterns or distinctive accessories, without explicit reference images. Even\nstate-of-the-art reference-based inpainting approaches struggle to accurately\ncapture and integrate fine-grained details from reference images. To address\nthis limitation, we propose CompleteMe, a novel reference-based human image\ncompletion framework. CompleteMe employs a dual U-Net architecture combined\nwith a Region-focused Attention (RFA) Block, which explicitly guides the\nmodel's attention toward relevant regions in reference images. This approach\neffectively captures fine details and ensures accurate semantic correspondence,\nsignificantly improving the fidelity and consistency of completed images.\nAdditionally, we introduce a challenging benchmark specifically designed for\nevaluating reference-based human image completion tasks. Extensive experiments\ndemonstrate that our proposed method achieves superior visual quality and\nsemantic consistency compared to existing techniques. Project page:\nhttps://liagm.github.io/CompleteMe/\n", "link": "http://arxiv.org/abs/2504.20042v1", "date": "2025-04-28", "relevancy": 2.1672, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5618}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompleteMe%3A%20Reference-based%20Human%20Image%20Completion&body=Title%3A%20CompleteMe%3A%20Reference-based%20Human%20Image%20Completion%0AAuthor%3A%20Yu-Ju%20Tsai%20and%20Brian%20Price%20and%20Qing%20Liu%20and%20Luis%20Figueroa%20and%20Daniil%20Pakhomov%20and%20Zhihong%20Ding%20and%20Scott%20Cohen%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Recent%20methods%20for%20human%20image%20completion%20can%20reconstruct%20plausible%20body%0Ashapes%20but%20often%20fail%20to%20preserve%20unique%20details%2C%20such%20as%20specific%20clothing%0Apatterns%20or%20distinctive%20accessories%2C%20without%20explicit%20reference%20images.%20Even%0Astate-of-the-art%20reference-based%20inpainting%20approaches%20struggle%20to%20accurately%0Acapture%20and%20integrate%20fine-grained%20details%20from%20reference%20images.%20To%20address%0Athis%20limitation%2C%20we%20propose%20CompleteMe%2C%20a%20novel%20reference-based%20human%20image%0Acompletion%20framework.%20CompleteMe%20employs%20a%20dual%20U-Net%20architecture%20combined%0Awith%20a%20Region-focused%20Attention%20%28RFA%29%20Block%2C%20which%20explicitly%20guides%20the%0Amodel%27s%20attention%20toward%20relevant%20regions%20in%20reference%20images.%20This%20approach%0Aeffectively%20captures%20fine%20details%20and%20ensures%20accurate%20semantic%20correspondence%2C%0Asignificantly%20improving%20the%20fidelity%20and%20consistency%20of%20completed%20images.%0AAdditionally%2C%20we%20introduce%20a%20challenging%20benchmark%20specifically%20designed%20for%0Aevaluating%20reference-based%20human%20image%20completion%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20method%20achieves%20superior%20visual%20quality%20and%0Asemantic%20consistency%20compared%20to%20existing%20techniques.%20Project%20page%3A%0Ahttps%3A//liagm.github.io/CompleteMe/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompleteMe%253A%2520Reference-based%2520Human%2520Image%2520Completion%26entry.906535625%3DYu-Ju%2520Tsai%2520and%2520Brian%2520Price%2520and%2520Qing%2520Liu%2520and%2520Luis%2520Figueroa%2520and%2520Daniil%2520Pakhomov%2520and%2520Zhihong%2520Ding%2520and%2520Scott%2520Cohen%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520methods%2520for%2520human%2520image%2520completion%2520can%2520reconstruct%2520plausible%2520body%250Ashapes%2520but%2520often%2520fail%2520to%2520preserve%2520unique%2520details%252C%2520such%2520as%2520specific%2520clothing%250Apatterns%2520or%2520distinctive%2520accessories%252C%2520without%2520explicit%2520reference%2520images.%2520Even%250Astate-of-the-art%2520reference-based%2520inpainting%2520approaches%2520struggle%2520to%2520accurately%250Acapture%2520and%2520integrate%2520fine-grained%2520details%2520from%2520reference%2520images.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520propose%2520CompleteMe%252C%2520a%2520novel%2520reference-based%2520human%2520image%250Acompletion%2520framework.%2520CompleteMe%2520employs%2520a%2520dual%2520U-Net%2520architecture%2520combined%250Awith%2520a%2520Region-focused%2520Attention%2520%2528RFA%2529%2520Block%252C%2520which%2520explicitly%2520guides%2520the%250Amodel%2527s%2520attention%2520toward%2520relevant%2520regions%2520in%2520reference%2520images.%2520This%2520approach%250Aeffectively%2520captures%2520fine%2520details%2520and%2520ensures%2520accurate%2520semantic%2520correspondence%252C%250Asignificantly%2520improving%2520the%2520fidelity%2520and%2520consistency%2520of%2520completed%2520images.%250AAdditionally%252C%2520we%2520introduce%2520a%2520challenging%2520benchmark%2520specifically%2520designed%2520for%250Aevaluating%2520reference-based%2520human%2520image%2520completion%2520tasks.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520proposed%2520method%2520achieves%2520superior%2520visual%2520quality%2520and%250Asemantic%2520consistency%2520compared%2520to%2520existing%2520techniques.%2520Project%2520page%253A%250Ahttps%253A//liagm.github.io/CompleteMe/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompleteMe%3A%20Reference-based%20Human%20Image%20Completion&entry.906535625=Yu-Ju%20Tsai%20and%20Brian%20Price%20and%20Qing%20Liu%20and%20Luis%20Figueroa%20and%20Daniil%20Pakhomov%20and%20Zhihong%20Ding%20and%20Scott%20Cohen%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Recent%20methods%20for%20human%20image%20completion%20can%20reconstruct%20plausible%20body%0Ashapes%20but%20often%20fail%20to%20preserve%20unique%20details%2C%20such%20as%20specific%20clothing%0Apatterns%20or%20distinctive%20accessories%2C%20without%20explicit%20reference%20images.%20Even%0Astate-of-the-art%20reference-based%20inpainting%20approaches%20struggle%20to%20accurately%0Acapture%20and%20integrate%20fine-grained%20details%20from%20reference%20images.%20To%20address%0Athis%20limitation%2C%20we%20propose%20CompleteMe%2C%20a%20novel%20reference-based%20human%20image%0Acompletion%20framework.%20CompleteMe%20employs%20a%20dual%20U-Net%20architecture%20combined%0Awith%20a%20Region-focused%20Attention%20%28RFA%29%20Block%2C%20which%20explicitly%20guides%20the%0Amodel%27s%20attention%20toward%20relevant%20regions%20in%20reference%20images.%20This%20approach%0Aeffectively%20captures%20fine%20details%20and%20ensures%20accurate%20semantic%20correspondence%2C%0Asignificantly%20improving%20the%20fidelity%20and%20consistency%20of%20completed%20images.%0AAdditionally%2C%20we%20introduce%20a%20challenging%20benchmark%20specifically%20designed%20for%0Aevaluating%20reference-based%20human%20image%20completion%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20method%20achieves%20superior%20visual%20quality%20and%0Asemantic%20consistency%20compared%20to%20existing%20techniques.%20Project%20page%3A%0Ahttps%3A//liagm.github.io/CompleteMe/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20042v1&entry.124074799=Read"},
{"title": "Feelbert: A Feedback Linearization-based Embedded Real-Time Quadrupedal\n  Locomotion Framework", "author": "Aristide Emanuele Casucci and Federico Nesti and Mauro Marinoni and Giorgio Buttazzo", "abstract": "  Quadruped robots have become quite popular for their ability to adapt their\nlocomotion to generic uneven terrains. For this reason, over time, several\nframeworks for quadrupedal locomotion have been proposed, but with little\nattention to ensuring a predictable timing behavior of the controller.\n  To address this issue, this work presents \\NAME, a modular control framework\nfor quadrupedal locomotion suitable for execution on an embedded system under\nhard real-time execution constraints. It leverages the feedback linearization\ncontrol technique to obtain a closed-form control law for the body, valid for\nall configurations of the robot. The control law was derived after defining an\nappropriate rigid body model that uses the accelerations of the feet as control\nvariables, instead of the estimated contact forces. This work also provides a\nnovel algorithm to compute footholds and gait temporal parameters using the\nconcept of imaginary wheels, and a heuristic algorithm to select the best gait\nschedule for the current velocity commands.\n  The proposed framework is developed entirely in C++, with no dependencies on\nthird-party libraries and no dynamic memory allocation, to ensure\npredictability and real-time performance. Its implementation allows \\NAME\\ to\nbe both compiled and executed on an embedded system for critical applications,\nas well as integrated into larger systems such as Robot Operating System 2 (ROS\n2). For this reason, \\NAME\\ has been tested in both scenarios, demonstrating\nsatisfactory results both in terms of reference tracking and temporal\npredictability, whether integrated into ROS 2 or compiled as a standalone\napplication on a Raspberry Pi 5.\n", "link": "http://arxiv.org/abs/2504.19965v1", "date": "2025-04-28", "relevancy": 2.1506, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6175}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5455}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feelbert%3A%20A%20Feedback%20Linearization-based%20Embedded%20Real-Time%20Quadrupedal%0A%20%20Locomotion%20Framework&body=Title%3A%20Feelbert%3A%20A%20Feedback%20Linearization-based%20Embedded%20Real-Time%20Quadrupedal%0A%20%20Locomotion%20Framework%0AAuthor%3A%20Aristide%20Emanuele%20Casucci%20and%20Federico%20Nesti%20and%20Mauro%20Marinoni%20and%20Giorgio%20Buttazzo%0AAbstract%3A%20%20%20Quadruped%20robots%20have%20become%20quite%20popular%20for%20their%20ability%20to%20adapt%20their%0Alocomotion%20to%20generic%20uneven%20terrains.%20For%20this%20reason%2C%20over%20time%2C%20several%0Aframeworks%20for%20quadrupedal%20locomotion%20have%20been%20proposed%2C%20but%20with%20little%0Aattention%20to%20ensuring%20a%20predictable%20timing%20behavior%20of%20the%20controller.%0A%20%20To%20address%20this%20issue%2C%20this%20work%20presents%20%5CNAME%2C%20a%20modular%20control%20framework%0Afor%20quadrupedal%20locomotion%20suitable%20for%20execution%20on%20an%20embedded%20system%20under%0Ahard%20real-time%20execution%20constraints.%20It%20leverages%20the%20feedback%20linearization%0Acontrol%20technique%20to%20obtain%20a%20closed-form%20control%20law%20for%20the%20body%2C%20valid%20for%0Aall%20configurations%20of%20the%20robot.%20The%20control%20law%20was%20derived%20after%20defining%20an%0Aappropriate%20rigid%20body%20model%20that%20uses%20the%20accelerations%20of%20the%20feet%20as%20control%0Avariables%2C%20instead%20of%20the%20estimated%20contact%20forces.%20This%20work%20also%20provides%20a%0Anovel%20algorithm%20to%20compute%20footholds%20and%20gait%20temporal%20parameters%20using%20the%0Aconcept%20of%20imaginary%20wheels%2C%20and%20a%20heuristic%20algorithm%20to%20select%20the%20best%20gait%0Aschedule%20for%20the%20current%20velocity%20commands.%0A%20%20The%20proposed%20framework%20is%20developed%20entirely%20in%20C%2B%2B%2C%20with%20no%20dependencies%20on%0Athird-party%20libraries%20and%20no%20dynamic%20memory%20allocation%2C%20to%20ensure%0Apredictability%20and%20real-time%20performance.%20Its%20implementation%20allows%20%5CNAME%5C%20to%0Abe%20both%20compiled%20and%20executed%20on%20an%20embedded%20system%20for%20critical%20applications%2C%0Aas%20well%20as%20integrated%20into%20larger%20systems%20such%20as%20Robot%20Operating%20System%202%20%28ROS%0A2%29.%20For%20this%20reason%2C%20%5CNAME%5C%20has%20been%20tested%20in%20both%20scenarios%2C%20demonstrating%0Asatisfactory%20results%20both%20in%20terms%20of%20reference%20tracking%20and%20temporal%0Apredictability%2C%20whether%20integrated%20into%20ROS%202%20or%20compiled%20as%20a%20standalone%0Aapplication%20on%20a%20Raspberry%20Pi%205.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeelbert%253A%2520A%2520Feedback%2520Linearization-based%2520Embedded%2520Real-Time%2520Quadrupedal%250A%2520%2520Locomotion%2520Framework%26entry.906535625%3DAristide%2520Emanuele%2520Casucci%2520and%2520Federico%2520Nesti%2520and%2520Mauro%2520Marinoni%2520and%2520Giorgio%2520Buttazzo%26entry.1292438233%3D%2520%2520Quadruped%2520robots%2520have%2520become%2520quite%2520popular%2520for%2520their%2520ability%2520to%2520adapt%2520their%250Alocomotion%2520to%2520generic%2520uneven%2520terrains.%2520For%2520this%2520reason%252C%2520over%2520time%252C%2520several%250Aframeworks%2520for%2520quadrupedal%2520locomotion%2520have%2520been%2520proposed%252C%2520but%2520with%2520little%250Aattention%2520to%2520ensuring%2520a%2520predictable%2520timing%2520behavior%2520of%2520the%2520controller.%250A%2520%2520To%2520address%2520this%2520issue%252C%2520this%2520work%2520presents%2520%255CNAME%252C%2520a%2520modular%2520control%2520framework%250Afor%2520quadrupedal%2520locomotion%2520suitable%2520for%2520execution%2520on%2520an%2520embedded%2520system%2520under%250Ahard%2520real-time%2520execution%2520constraints.%2520It%2520leverages%2520the%2520feedback%2520linearization%250Acontrol%2520technique%2520to%2520obtain%2520a%2520closed-form%2520control%2520law%2520for%2520the%2520body%252C%2520valid%2520for%250Aall%2520configurations%2520of%2520the%2520robot.%2520The%2520control%2520law%2520was%2520derived%2520after%2520defining%2520an%250Aappropriate%2520rigid%2520body%2520model%2520that%2520uses%2520the%2520accelerations%2520of%2520the%2520feet%2520as%2520control%250Avariables%252C%2520instead%2520of%2520the%2520estimated%2520contact%2520forces.%2520This%2520work%2520also%2520provides%2520a%250Anovel%2520algorithm%2520to%2520compute%2520footholds%2520and%2520gait%2520temporal%2520parameters%2520using%2520the%250Aconcept%2520of%2520imaginary%2520wheels%252C%2520and%2520a%2520heuristic%2520algorithm%2520to%2520select%2520the%2520best%2520gait%250Aschedule%2520for%2520the%2520current%2520velocity%2520commands.%250A%2520%2520The%2520proposed%2520framework%2520is%2520developed%2520entirely%2520in%2520C%252B%252B%252C%2520with%2520no%2520dependencies%2520on%250Athird-party%2520libraries%2520and%2520no%2520dynamic%2520memory%2520allocation%252C%2520to%2520ensure%250Apredictability%2520and%2520real-time%2520performance.%2520Its%2520implementation%2520allows%2520%255CNAME%255C%2520to%250Abe%2520both%2520compiled%2520and%2520executed%2520on%2520an%2520embedded%2520system%2520for%2520critical%2520applications%252C%250Aas%2520well%2520as%2520integrated%2520into%2520larger%2520systems%2520such%2520as%2520Robot%2520Operating%2520System%25202%2520%2528ROS%250A2%2529.%2520For%2520this%2520reason%252C%2520%255CNAME%255C%2520has%2520been%2520tested%2520in%2520both%2520scenarios%252C%2520demonstrating%250Asatisfactory%2520results%2520both%2520in%2520terms%2520of%2520reference%2520tracking%2520and%2520temporal%250Apredictability%252C%2520whether%2520integrated%2520into%2520ROS%25202%2520or%2520compiled%2520as%2520a%2520standalone%250Aapplication%2520on%2520a%2520Raspberry%2520Pi%25205.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feelbert%3A%20A%20Feedback%20Linearization-based%20Embedded%20Real-Time%20Quadrupedal%0A%20%20Locomotion%20Framework&entry.906535625=Aristide%20Emanuele%20Casucci%20and%20Federico%20Nesti%20and%20Mauro%20Marinoni%20and%20Giorgio%20Buttazzo&entry.1292438233=%20%20Quadruped%20robots%20have%20become%20quite%20popular%20for%20their%20ability%20to%20adapt%20their%0Alocomotion%20to%20generic%20uneven%20terrains.%20For%20this%20reason%2C%20over%20time%2C%20several%0Aframeworks%20for%20quadrupedal%20locomotion%20have%20been%20proposed%2C%20but%20with%20little%0Aattention%20to%20ensuring%20a%20predictable%20timing%20behavior%20of%20the%20controller.%0A%20%20To%20address%20this%20issue%2C%20this%20work%20presents%20%5CNAME%2C%20a%20modular%20control%20framework%0Afor%20quadrupedal%20locomotion%20suitable%20for%20execution%20on%20an%20embedded%20system%20under%0Ahard%20real-time%20execution%20constraints.%20It%20leverages%20the%20feedback%20linearization%0Acontrol%20technique%20to%20obtain%20a%20closed-form%20control%20law%20for%20the%20body%2C%20valid%20for%0Aall%20configurations%20of%20the%20robot.%20The%20control%20law%20was%20derived%20after%20defining%20an%0Aappropriate%20rigid%20body%20model%20that%20uses%20the%20accelerations%20of%20the%20feet%20as%20control%0Avariables%2C%20instead%20of%20the%20estimated%20contact%20forces.%20This%20work%20also%20provides%20a%0Anovel%20algorithm%20to%20compute%20footholds%20and%20gait%20temporal%20parameters%20using%20the%0Aconcept%20of%20imaginary%20wheels%2C%20and%20a%20heuristic%20algorithm%20to%20select%20the%20best%20gait%0Aschedule%20for%20the%20current%20velocity%20commands.%0A%20%20The%20proposed%20framework%20is%20developed%20entirely%20in%20C%2B%2B%2C%20with%20no%20dependencies%20on%0Athird-party%20libraries%20and%20no%20dynamic%20memory%20allocation%2C%20to%20ensure%0Apredictability%20and%20real-time%20performance.%20Its%20implementation%20allows%20%5CNAME%5C%20to%0Abe%20both%20compiled%20and%20executed%20on%20an%20embedded%20system%20for%20critical%20applications%2C%0Aas%20well%20as%20integrated%20into%20larger%20systems%20such%20as%20Robot%20Operating%20System%202%20%28ROS%0A2%29.%20For%20this%20reason%2C%20%5CNAME%5C%20has%20been%20tested%20in%20both%20scenarios%2C%20demonstrating%0Asatisfactory%20results%20both%20in%20terms%20of%20reference%20tracking%20and%20temporal%0Apredictability%2C%20whether%20integrated%20into%20ROS%202%20or%20compiled%20as%20a%20standalone%0Aapplication%20on%20a%20Raspberry%20Pi%205.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19965v1&entry.124074799=Read"},
{"title": "LODAP: On-Device Incremental Learning Via Lightweight Operations and\n  Data Pruning", "author": "Biqing Duan and Qing Wang and Di Liu and Wei Zhou and Zhenli He and Shengfa Miao", "abstract": "  Incremental learning that learns new classes over time after the model's\ndeployment is becoming increasingly crucial, particularly for industrial edge\nsystems, where it is difficult to communicate with a remote server to conduct\ncomputation-intensive learning. As more classes are expected to learn after\ntheir execution for edge devices. In this paper, we propose LODAP, a new\non-device incremental learning framework for edge systems. The key part of\nLODAP is a new module, namely Efficient Incremental Module (EIM). EIM is\ncomposed of normal convolutions and lightweight operations. During incremental\nlearning, EIM exploits some lightweight operations, called adapters, to\neffectively and efficiently learn features for new classes so that it can\nimprove the accuracy of incremental learning while reducing model complexity as\nwell as training overhead. The efficiency of LODAP is further enhanced by a\ndata pruning strategy that significantly reduces the training data, thereby\nlowering the training overhead. We conducted extensive experiments on the\nCIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP\nimproves the accuracy by up to 4.32\\% over existing methods while reducing\naround 50\\% of model complexity. In addition, evaluations on real edge systems\ndemonstrate its applicability for on-device machine learning. The code is\navailable at https://github.com/duanbiqing/LODAP.\n", "link": "http://arxiv.org/abs/2504.19638v1", "date": "2025-04-28", "relevancy": 2.1459, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5367}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5365}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LODAP%3A%20On-Device%20Incremental%20Learning%20Via%20Lightweight%20Operations%20and%0A%20%20Data%20Pruning&body=Title%3A%20LODAP%3A%20On-Device%20Incremental%20Learning%20Via%20Lightweight%20Operations%20and%0A%20%20Data%20Pruning%0AAuthor%3A%20Biqing%20Duan%20and%20Qing%20Wang%20and%20Di%20Liu%20and%20Wei%20Zhou%20and%20Zhenli%20He%20and%20Shengfa%20Miao%0AAbstract%3A%20%20%20Incremental%20learning%20that%20learns%20new%20classes%20over%20time%20after%20the%20model%27s%0Adeployment%20is%20becoming%20increasingly%20crucial%2C%20particularly%20for%20industrial%20edge%0Asystems%2C%20where%20it%20is%20difficult%20to%20communicate%20with%20a%20remote%20server%20to%20conduct%0Acomputation-intensive%20learning.%20As%20more%20classes%20are%20expected%20to%20learn%20after%0Atheir%20execution%20for%20edge%20devices.%20In%20this%20paper%2C%20we%20propose%20LODAP%2C%20a%20new%0Aon-device%20incremental%20learning%20framework%20for%20edge%20systems.%20The%20key%20part%20of%0ALODAP%20is%20a%20new%20module%2C%20namely%20Efficient%20Incremental%20Module%20%28EIM%29.%20EIM%20is%0Acomposed%20of%20normal%20convolutions%20and%20lightweight%20operations.%20During%20incremental%0Alearning%2C%20EIM%20exploits%20some%20lightweight%20operations%2C%20called%20adapters%2C%20to%0Aeffectively%20and%20efficiently%20learn%20features%20for%20new%20classes%20so%20that%20it%20can%0Aimprove%20the%20accuracy%20of%20incremental%20learning%20while%20reducing%20model%20complexity%20as%0Awell%20as%20training%20overhead.%20The%20efficiency%20of%20LODAP%20is%20further%20enhanced%20by%20a%0Adata%20pruning%20strategy%20that%20significantly%20reduces%20the%20training%20data%2C%20thereby%0Alowering%20the%20training%20overhead.%20We%20conducted%20extensive%20experiments%20on%20the%0ACIFAR-100%20and%20Tiny-%20ImageNet%20datasets.%20Experimental%20results%20show%20that%20LODAP%0Aimproves%20the%20accuracy%20by%20up%20to%204.32%5C%25%20over%20existing%20methods%20while%20reducing%0Aaround%2050%5C%25%20of%20model%20complexity.%20In%20addition%2C%20evaluations%20on%20real%20edge%20systems%0Ademonstrate%20its%20applicability%20for%20on-device%20machine%20learning.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/duanbiqing/LODAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLODAP%253A%2520On-Device%2520Incremental%2520Learning%2520Via%2520Lightweight%2520Operations%2520and%250A%2520%2520Data%2520Pruning%26entry.906535625%3DBiqing%2520Duan%2520and%2520Qing%2520Wang%2520and%2520Di%2520Liu%2520and%2520Wei%2520Zhou%2520and%2520Zhenli%2520He%2520and%2520Shengfa%2520Miao%26entry.1292438233%3D%2520%2520Incremental%2520learning%2520that%2520learns%2520new%2520classes%2520over%2520time%2520after%2520the%2520model%2527s%250Adeployment%2520is%2520becoming%2520increasingly%2520crucial%252C%2520particularly%2520for%2520industrial%2520edge%250Asystems%252C%2520where%2520it%2520is%2520difficult%2520to%2520communicate%2520with%2520a%2520remote%2520server%2520to%2520conduct%250Acomputation-intensive%2520learning.%2520As%2520more%2520classes%2520are%2520expected%2520to%2520learn%2520after%250Atheir%2520execution%2520for%2520edge%2520devices.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LODAP%252C%2520a%2520new%250Aon-device%2520incremental%2520learning%2520framework%2520for%2520edge%2520systems.%2520The%2520key%2520part%2520of%250ALODAP%2520is%2520a%2520new%2520module%252C%2520namely%2520Efficient%2520Incremental%2520Module%2520%2528EIM%2529.%2520EIM%2520is%250Acomposed%2520of%2520normal%2520convolutions%2520and%2520lightweight%2520operations.%2520During%2520incremental%250Alearning%252C%2520EIM%2520exploits%2520some%2520lightweight%2520operations%252C%2520called%2520adapters%252C%2520to%250Aeffectively%2520and%2520efficiently%2520learn%2520features%2520for%2520new%2520classes%2520so%2520that%2520it%2520can%250Aimprove%2520the%2520accuracy%2520of%2520incremental%2520learning%2520while%2520reducing%2520model%2520complexity%2520as%250Awell%2520as%2520training%2520overhead.%2520The%2520efficiency%2520of%2520LODAP%2520is%2520further%2520enhanced%2520by%2520a%250Adata%2520pruning%2520strategy%2520that%2520significantly%2520reduces%2520the%2520training%2520data%252C%2520thereby%250Alowering%2520the%2520training%2520overhead.%2520We%2520conducted%2520extensive%2520experiments%2520on%2520the%250ACIFAR-100%2520and%2520Tiny-%2520ImageNet%2520datasets.%2520Experimental%2520results%2520show%2520that%2520LODAP%250Aimproves%2520the%2520accuracy%2520by%2520up%2520to%25204.32%255C%2525%2520over%2520existing%2520methods%2520while%2520reducing%250Aaround%252050%255C%2525%2520of%2520model%2520complexity.%2520In%2520addition%252C%2520evaluations%2520on%2520real%2520edge%2520systems%250Ademonstrate%2520its%2520applicability%2520for%2520on-device%2520machine%2520learning.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/duanbiqing/LODAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LODAP%3A%20On-Device%20Incremental%20Learning%20Via%20Lightweight%20Operations%20and%0A%20%20Data%20Pruning&entry.906535625=Biqing%20Duan%20and%20Qing%20Wang%20and%20Di%20Liu%20and%20Wei%20Zhou%20and%20Zhenli%20He%20and%20Shengfa%20Miao&entry.1292438233=%20%20Incremental%20learning%20that%20learns%20new%20classes%20over%20time%20after%20the%20model%27s%0Adeployment%20is%20becoming%20increasingly%20crucial%2C%20particularly%20for%20industrial%20edge%0Asystems%2C%20where%20it%20is%20difficult%20to%20communicate%20with%20a%20remote%20server%20to%20conduct%0Acomputation-intensive%20learning.%20As%20more%20classes%20are%20expected%20to%20learn%20after%0Atheir%20execution%20for%20edge%20devices.%20In%20this%20paper%2C%20we%20propose%20LODAP%2C%20a%20new%0Aon-device%20incremental%20learning%20framework%20for%20edge%20systems.%20The%20key%20part%20of%0ALODAP%20is%20a%20new%20module%2C%20namely%20Efficient%20Incremental%20Module%20%28EIM%29.%20EIM%20is%0Acomposed%20of%20normal%20convolutions%20and%20lightweight%20operations.%20During%20incremental%0Alearning%2C%20EIM%20exploits%20some%20lightweight%20operations%2C%20called%20adapters%2C%20to%0Aeffectively%20and%20efficiently%20learn%20features%20for%20new%20classes%20so%20that%20it%20can%0Aimprove%20the%20accuracy%20of%20incremental%20learning%20while%20reducing%20model%20complexity%20as%0Awell%20as%20training%20overhead.%20The%20efficiency%20of%20LODAP%20is%20further%20enhanced%20by%20a%0Adata%20pruning%20strategy%20that%20significantly%20reduces%20the%20training%20data%2C%20thereby%0Alowering%20the%20training%20overhead.%20We%20conducted%20extensive%20experiments%20on%20the%0ACIFAR-100%20and%20Tiny-%20ImageNet%20datasets.%20Experimental%20results%20show%20that%20LODAP%0Aimproves%20the%20accuracy%20by%20up%20to%204.32%5C%25%20over%20existing%20methods%20while%20reducing%0Aaround%2050%5C%25%20of%20model%20complexity.%20In%20addition%2C%20evaluations%20on%20real%20edge%20systems%0Ademonstrate%20its%20applicability%20for%20on-device%20machine%20learning.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/duanbiqing/LODAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19638v1&entry.124074799=Read"},
{"title": "MINT: Multi-Vector Search Index Tuning", "author": "Jiongli Zhu and Yue Wang and Bailu Ding and Philip A. Bernstein and Vivek Narasayya and Surajit Chaudhuri", "abstract": "  Vector search plays a crucial role in many real-world applications. In\naddition to single-vector search, multi-vector search becomes important for\nmulti-modal and multi-feature scenarios today. In a multi-vector database, each\nrow is an item, each column represents a feature of items, and each cell is a\nhigh-dimensional vector. In multi-vector databases, the choice of indexes can\nhave a significant impact on performance. Although index tuning for relational\ndatabases has been extensively studied, index tuning for multi-vector search\nremains unclear and challenging. In this paper, we define multi-vector search\nindex tuning and propose a framework to solve it. Specifically, given a\nmulti-vector search workload, we develop algorithms to find indexes that\nminimize latency and meet storage and recall constraints. Compared to the\nbaseline, our latency achieves 2.1X to 8.3X speedup.\n", "link": "http://arxiv.org/abs/2504.20018v1", "date": "2025-04-28", "relevancy": 2.1307, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4343}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4343}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MINT%3A%20Multi-Vector%20Search%20Index%20Tuning&body=Title%3A%20MINT%3A%20Multi-Vector%20Search%20Index%20Tuning%0AAuthor%3A%20Jiongli%20Zhu%20and%20Yue%20Wang%20and%20Bailu%20Ding%20and%20Philip%20A.%20Bernstein%20and%20Vivek%20Narasayya%20and%20Surajit%20Chaudhuri%0AAbstract%3A%20%20%20Vector%20search%20plays%20a%20crucial%20role%20in%20many%20real-world%20applications.%20In%0Aaddition%20to%20single-vector%20search%2C%20multi-vector%20search%20becomes%20important%20for%0Amulti-modal%20and%20multi-feature%20scenarios%20today.%20In%20a%20multi-vector%20database%2C%20each%0Arow%20is%20an%20item%2C%20each%20column%20represents%20a%20feature%20of%20items%2C%20and%20each%20cell%20is%20a%0Ahigh-dimensional%20vector.%20In%20multi-vector%20databases%2C%20the%20choice%20of%20indexes%20can%0Ahave%20a%20significant%20impact%20on%20performance.%20Although%20index%20tuning%20for%20relational%0Adatabases%20has%20been%20extensively%20studied%2C%20index%20tuning%20for%20multi-vector%20search%0Aremains%20unclear%20and%20challenging.%20In%20this%20paper%2C%20we%20define%20multi-vector%20search%0Aindex%20tuning%20and%20propose%20a%20framework%20to%20solve%20it.%20Specifically%2C%20given%20a%0Amulti-vector%20search%20workload%2C%20we%20develop%20algorithms%20to%20find%20indexes%20that%0Aminimize%20latency%20and%20meet%20storage%20and%20recall%20constraints.%20Compared%20to%20the%0Abaseline%2C%20our%20latency%20achieves%202.1X%20to%208.3X%20speedup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMINT%253A%2520Multi-Vector%2520Search%2520Index%2520Tuning%26entry.906535625%3DJiongli%2520Zhu%2520and%2520Yue%2520Wang%2520and%2520Bailu%2520Ding%2520and%2520Philip%2520A.%2520Bernstein%2520and%2520Vivek%2520Narasayya%2520and%2520Surajit%2520Chaudhuri%26entry.1292438233%3D%2520%2520Vector%2520search%2520plays%2520a%2520crucial%2520role%2520in%2520many%2520real-world%2520applications.%2520In%250Aaddition%2520to%2520single-vector%2520search%252C%2520multi-vector%2520search%2520becomes%2520important%2520for%250Amulti-modal%2520and%2520multi-feature%2520scenarios%2520today.%2520In%2520a%2520multi-vector%2520database%252C%2520each%250Arow%2520is%2520an%2520item%252C%2520each%2520column%2520represents%2520a%2520feature%2520of%2520items%252C%2520and%2520each%2520cell%2520is%2520a%250Ahigh-dimensional%2520vector.%2520In%2520multi-vector%2520databases%252C%2520the%2520choice%2520of%2520indexes%2520can%250Ahave%2520a%2520significant%2520impact%2520on%2520performance.%2520Although%2520index%2520tuning%2520for%2520relational%250Adatabases%2520has%2520been%2520extensively%2520studied%252C%2520index%2520tuning%2520for%2520multi-vector%2520search%250Aremains%2520unclear%2520and%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520define%2520multi-vector%2520search%250Aindex%2520tuning%2520and%2520propose%2520a%2520framework%2520to%2520solve%2520it.%2520Specifically%252C%2520given%2520a%250Amulti-vector%2520search%2520workload%252C%2520we%2520develop%2520algorithms%2520to%2520find%2520indexes%2520that%250Aminimize%2520latency%2520and%2520meet%2520storage%2520and%2520recall%2520constraints.%2520Compared%2520to%2520the%250Abaseline%252C%2520our%2520latency%2520achieves%25202.1X%2520to%25208.3X%2520speedup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MINT%3A%20Multi-Vector%20Search%20Index%20Tuning&entry.906535625=Jiongli%20Zhu%20and%20Yue%20Wang%20and%20Bailu%20Ding%20and%20Philip%20A.%20Bernstein%20and%20Vivek%20Narasayya%20and%20Surajit%20Chaudhuri&entry.1292438233=%20%20Vector%20search%20plays%20a%20crucial%20role%20in%20many%20real-world%20applications.%20In%0Aaddition%20to%20single-vector%20search%2C%20multi-vector%20search%20becomes%20important%20for%0Amulti-modal%20and%20multi-feature%20scenarios%20today.%20In%20a%20multi-vector%20database%2C%20each%0Arow%20is%20an%20item%2C%20each%20column%20represents%20a%20feature%20of%20items%2C%20and%20each%20cell%20is%20a%0Ahigh-dimensional%20vector.%20In%20multi-vector%20databases%2C%20the%20choice%20of%20indexes%20can%0Ahave%20a%20significant%20impact%20on%20performance.%20Although%20index%20tuning%20for%20relational%0Adatabases%20has%20been%20extensively%20studied%2C%20index%20tuning%20for%20multi-vector%20search%0Aremains%20unclear%20and%20challenging.%20In%20this%20paper%2C%20we%20define%20multi-vector%20search%0Aindex%20tuning%20and%20propose%20a%20framework%20to%20solve%20it.%20Specifically%2C%20given%20a%0Amulti-vector%20search%20workload%2C%20we%20develop%20algorithms%20to%20find%20indexes%20that%0Aminimize%20latency%20and%20meet%20storage%20and%20recall%20constraints.%20Compared%20to%20the%0Abaseline%2C%20our%20latency%20achieves%202.1X%20to%208.3X%20speedup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20018v1&entry.124074799=Read"},
{"title": "Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network\n  for Low-Dose CT MAR", "author": "Baoshun Shi and Bing Chen and Shaolei Zhang and Huazhu Fu and Zhanli Hu", "abstract": "  Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it\nwill potentially degrade image quality, even yields metal artifacts at the case\nof metallic implants. For simultaneous LDCT reconstruction and metal artifact\nreduction (LDMAR), existing deep learning-based efforts face two main\nlimitations: i) the network design neglects multi-scale and within-scale\ninformation; ii) training a distinct model for each dose necessitates\nsignificant storage space for multiple doses. To fill these gaps, we propose a\nprompt guiding multi-scale adaptive sparse representation-driven network,\nabbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet\ninspired from multi-scale sparsifying frames, and it can simultaneously employ\nwithin-scale characteristics and cross-scale complementarity owing to an\nelaborated prompt guiding scale-adaptive threshold generator (PSATG) and a\nbuilt multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively\ncapture multiple contextual information to generate more faithful thresholds,\nachieved by fusing features from local, regional, and global levels.\nFurthermore, we elaborate a model interpretable dual domain LDMAR framework\ncalled PDuMSRNet, and train single model with a prompt guiding strategy for\nmultiple dose levels. We build a prompt guiding module, whose input contains\ndose level, metal mask and input instance, to provide various guiding\ninformation, allowing a single model to accommodate various CT dose settings.\nExtensive experiments at various dose levels demonstrate that the proposed\nmethods outperform the state-of-the-art LDMAR methods.\n", "link": "http://arxiv.org/abs/2504.19687v1", "date": "2025-04-28", "relevancy": 2.1297, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Guiding%20Multi-Scale%20Adaptive%20Sparse%20Representation-driven%20Network%0A%20%20for%20Low-Dose%20CT%20MAR&body=Title%3A%20Prompt%20Guiding%20Multi-Scale%20Adaptive%20Sparse%20Representation-driven%20Network%0A%20%20for%20Low-Dose%20CT%20MAR%0AAuthor%3A%20Baoshun%20Shi%20and%20Bing%20Chen%20and%20Shaolei%20Zhang%20and%20Huazhu%20Fu%20and%20Zhanli%20Hu%0AAbstract%3A%20%20%20Low-dose%20CT%20%28LDCT%29%20is%20capable%20of%20reducing%20X-ray%20radiation%20exposure%2C%20but%20it%0Awill%20potentially%20degrade%20image%20quality%2C%20even%20yields%20metal%20artifacts%20at%20the%20case%0Aof%20metallic%20implants.%20For%20simultaneous%20LDCT%20reconstruction%20and%20metal%20artifact%0Areduction%20%28LDMAR%29%2C%20existing%20deep%20learning-based%20efforts%20face%20two%20main%0Alimitations%3A%20i%29%20the%20network%20design%20neglects%20multi-scale%20and%20within-scale%0Ainformation%3B%20ii%29%20training%20a%20distinct%20model%20for%20each%20dose%20necessitates%0Asignificant%20storage%20space%20for%20multiple%20doses.%20To%20fill%20these%20gaps%2C%20we%20propose%20a%0Aprompt%20guiding%20multi-scale%20adaptive%20sparse%20representation-driven%20network%2C%0Aabbreviated%20as%20PMSRNet%2C%20for%20LDMAR%20task.%20Specifically%2C%20we%20construct%20PMSRNet%0Ainspired%20from%20multi-scale%20sparsifying%20frames%2C%20and%20it%20can%20simultaneously%20employ%0Awithin-scale%20characteristics%20and%20cross-scale%20complementarity%20owing%20to%20an%0Aelaborated%20prompt%20guiding%20scale-adaptive%20threshold%20generator%20%28PSATG%29%20and%20a%0Abuilt%20multi-scale%20coefficient%20fusion%20module%20%28MSFuM%29.%20The%20PSATG%20can%20adaptively%0Acapture%20multiple%20contextual%20information%20to%20generate%20more%20faithful%20thresholds%2C%0Aachieved%20by%20fusing%20features%20from%20local%2C%20regional%2C%20and%20global%20levels.%0AFurthermore%2C%20we%20elaborate%20a%20model%20interpretable%20dual%20domain%20LDMAR%20framework%0Acalled%20PDuMSRNet%2C%20and%20train%20single%20model%20with%20a%20prompt%20guiding%20strategy%20for%0Amultiple%20dose%20levels.%20We%20build%20a%20prompt%20guiding%20module%2C%20whose%20input%20contains%0Adose%20level%2C%20metal%20mask%20and%20input%20instance%2C%20to%20provide%20various%20guiding%0Ainformation%2C%20allowing%20a%20single%20model%20to%20accommodate%20various%20CT%20dose%20settings.%0AExtensive%20experiments%20at%20various%20dose%20levels%20demonstrate%20that%20the%20proposed%0Amethods%20outperform%20the%20state-of-the-art%20LDMAR%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Guiding%2520Multi-Scale%2520Adaptive%2520Sparse%2520Representation-driven%2520Network%250A%2520%2520for%2520Low-Dose%2520CT%2520MAR%26entry.906535625%3DBaoshun%2520Shi%2520and%2520Bing%2520Chen%2520and%2520Shaolei%2520Zhang%2520and%2520Huazhu%2520Fu%2520and%2520Zhanli%2520Hu%26entry.1292438233%3D%2520%2520Low-dose%2520CT%2520%2528LDCT%2529%2520is%2520capable%2520of%2520reducing%2520X-ray%2520radiation%2520exposure%252C%2520but%2520it%250Awill%2520potentially%2520degrade%2520image%2520quality%252C%2520even%2520yields%2520metal%2520artifacts%2520at%2520the%2520case%250Aof%2520metallic%2520implants.%2520For%2520simultaneous%2520LDCT%2520reconstruction%2520and%2520metal%2520artifact%250Areduction%2520%2528LDMAR%2529%252C%2520existing%2520deep%2520learning-based%2520efforts%2520face%2520two%2520main%250Alimitations%253A%2520i%2529%2520the%2520network%2520design%2520neglects%2520multi-scale%2520and%2520within-scale%250Ainformation%253B%2520ii%2529%2520training%2520a%2520distinct%2520model%2520for%2520each%2520dose%2520necessitates%250Asignificant%2520storage%2520space%2520for%2520multiple%2520doses.%2520To%2520fill%2520these%2520gaps%252C%2520we%2520propose%2520a%250Aprompt%2520guiding%2520multi-scale%2520adaptive%2520sparse%2520representation-driven%2520network%252C%250Aabbreviated%2520as%2520PMSRNet%252C%2520for%2520LDMAR%2520task.%2520Specifically%252C%2520we%2520construct%2520PMSRNet%250Ainspired%2520from%2520multi-scale%2520sparsifying%2520frames%252C%2520and%2520it%2520can%2520simultaneously%2520employ%250Awithin-scale%2520characteristics%2520and%2520cross-scale%2520complementarity%2520owing%2520to%2520an%250Aelaborated%2520prompt%2520guiding%2520scale-adaptive%2520threshold%2520generator%2520%2528PSATG%2529%2520and%2520a%250Abuilt%2520multi-scale%2520coefficient%2520fusion%2520module%2520%2528MSFuM%2529.%2520The%2520PSATG%2520can%2520adaptively%250Acapture%2520multiple%2520contextual%2520information%2520to%2520generate%2520more%2520faithful%2520thresholds%252C%250Aachieved%2520by%2520fusing%2520features%2520from%2520local%252C%2520regional%252C%2520and%2520global%2520levels.%250AFurthermore%252C%2520we%2520elaborate%2520a%2520model%2520interpretable%2520dual%2520domain%2520LDMAR%2520framework%250Acalled%2520PDuMSRNet%252C%2520and%2520train%2520single%2520model%2520with%2520a%2520prompt%2520guiding%2520strategy%2520for%250Amultiple%2520dose%2520levels.%2520We%2520build%2520a%2520prompt%2520guiding%2520module%252C%2520whose%2520input%2520contains%250Adose%2520level%252C%2520metal%2520mask%2520and%2520input%2520instance%252C%2520to%2520provide%2520various%2520guiding%250Ainformation%252C%2520allowing%2520a%2520single%2520model%2520to%2520accommodate%2520various%2520CT%2520dose%2520settings.%250AExtensive%2520experiments%2520at%2520various%2520dose%2520levels%2520demonstrate%2520that%2520the%2520proposed%250Amethods%2520outperform%2520the%2520state-of-the-art%2520LDMAR%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Guiding%20Multi-Scale%20Adaptive%20Sparse%20Representation-driven%20Network%0A%20%20for%20Low-Dose%20CT%20MAR&entry.906535625=Baoshun%20Shi%20and%20Bing%20Chen%20and%20Shaolei%20Zhang%20and%20Huazhu%20Fu%20and%20Zhanli%20Hu&entry.1292438233=%20%20Low-dose%20CT%20%28LDCT%29%20is%20capable%20of%20reducing%20X-ray%20radiation%20exposure%2C%20but%20it%0Awill%20potentially%20degrade%20image%20quality%2C%20even%20yields%20metal%20artifacts%20at%20the%20case%0Aof%20metallic%20implants.%20For%20simultaneous%20LDCT%20reconstruction%20and%20metal%20artifact%0Areduction%20%28LDMAR%29%2C%20existing%20deep%20learning-based%20efforts%20face%20two%20main%0Alimitations%3A%20i%29%20the%20network%20design%20neglects%20multi-scale%20and%20within-scale%0Ainformation%3B%20ii%29%20training%20a%20distinct%20model%20for%20each%20dose%20necessitates%0Asignificant%20storage%20space%20for%20multiple%20doses.%20To%20fill%20these%20gaps%2C%20we%20propose%20a%0Aprompt%20guiding%20multi-scale%20adaptive%20sparse%20representation-driven%20network%2C%0Aabbreviated%20as%20PMSRNet%2C%20for%20LDMAR%20task.%20Specifically%2C%20we%20construct%20PMSRNet%0Ainspired%20from%20multi-scale%20sparsifying%20frames%2C%20and%20it%20can%20simultaneously%20employ%0Awithin-scale%20characteristics%20and%20cross-scale%20complementarity%20owing%20to%20an%0Aelaborated%20prompt%20guiding%20scale-adaptive%20threshold%20generator%20%28PSATG%29%20and%20a%0Abuilt%20multi-scale%20coefficient%20fusion%20module%20%28MSFuM%29.%20The%20PSATG%20can%20adaptively%0Acapture%20multiple%20contextual%20information%20to%20generate%20more%20faithful%20thresholds%2C%0Aachieved%20by%20fusing%20features%20from%20local%2C%20regional%2C%20and%20global%20levels.%0AFurthermore%2C%20we%20elaborate%20a%20model%20interpretable%20dual%20domain%20LDMAR%20framework%0Acalled%20PDuMSRNet%2C%20and%20train%20single%20model%20with%20a%20prompt%20guiding%20strategy%20for%0Amultiple%20dose%20levels.%20We%20build%20a%20prompt%20guiding%20module%2C%20whose%20input%20contains%0Adose%20level%2C%20metal%20mask%20and%20input%20instance%2C%20to%20provide%20various%20guiding%0Ainformation%2C%20allowing%20a%20single%20model%20to%20accommodate%20various%20CT%20dose%20settings.%0AExtensive%20experiments%20at%20various%20dose%20levels%20demonstrate%20that%20the%20proposed%0Amethods%20outperform%20the%20state-of-the-art%20LDMAR%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19687v1&entry.124074799=Read"},
{"title": "BARIS: Boundary-Aware Refinement with Environmental Degradation Priors\n  for Robust Underwater Instance Segmentation", "author": "Pin-Chi Pan and Soo-Chang Pei", "abstract": "  Underwater instance segmentation is challenging due to adverse visual\nconditions such as light attenuation, scattering, and color distortion, which\ndegrade model performance. In this work, we propose BARIS-Decoder\n(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that\nenhances segmentation accuracy through feature refinement. To address\nunderwater degradations, we introduce the Environmental Robust Adapter (ERA),\nwhich efficiently models underwater degradation patterns while reducing\ntrainable parameters by over 90\\% compared to full fine-tuning. The integration\nof BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves\nstate-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B\nbackbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the\neffectiveness of BARIS-ERA in advancing underwater instance segmentation,\nproviding a robust and efficient solution.\n", "link": "http://arxiv.org/abs/2504.19643v1", "date": "2025-04-28", "relevancy": 2.1191, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BARIS%3A%20Boundary-Aware%20Refinement%20with%20Environmental%20Degradation%20Priors%0A%20%20for%20Robust%20Underwater%20Instance%20Segmentation&body=Title%3A%20BARIS%3A%20Boundary-Aware%20Refinement%20with%20Environmental%20Degradation%20Priors%0A%20%20for%20Robust%20Underwater%20Instance%20Segmentation%0AAuthor%3A%20Pin-Chi%20Pan%20and%20Soo-Chang%20Pei%0AAbstract%3A%20%20%20Underwater%20instance%20segmentation%20is%20challenging%20due%20to%20adverse%20visual%0Aconditions%20such%20as%20light%20attenuation%2C%20scattering%2C%20and%20color%20distortion%2C%20which%0Adegrade%20model%20performance.%20In%20this%20work%2C%20we%20propose%20BARIS-Decoder%0A%28Boundary-Aware%20Refinement%20Decoder%20for%20Instance%20Segmentation%29%2C%20a%20framework%20that%0Aenhances%20segmentation%20accuracy%20through%20feature%20refinement.%20To%20address%0Aunderwater%20degradations%2C%20we%20introduce%20the%20Environmental%20Robust%20Adapter%20%28ERA%29%2C%0Awhich%20efficiently%20models%20underwater%20degradation%20patterns%20while%20reducing%0Atrainable%20parameters%20by%20over%2090%5C%25%20compared%20to%20full%20fine-tuning.%20The%20integration%0Aof%20BARIS-Decoder%20with%20ERA-tuning%2C%20referred%20to%20as%20BARIS-ERA%2C%20achieves%0Astate-of-the-art%20performance%2C%20surpassing%20Mask%20R-CNN%20by%203.4%20mAP%20with%20a%20Swin-B%0Abackbone%20and%203.8%20mAP%20with%20ConvNeXt%20V2.%20Our%20findings%20demonstrate%20the%0Aeffectiveness%20of%20BARIS-ERA%20in%20advancing%20underwater%20instance%20segmentation%2C%0Aproviding%20a%20robust%20and%20efficient%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBARIS%253A%2520Boundary-Aware%2520Refinement%2520with%2520Environmental%2520Degradation%2520Priors%250A%2520%2520for%2520Robust%2520Underwater%2520Instance%2520Segmentation%26entry.906535625%3DPin-Chi%2520Pan%2520and%2520Soo-Chang%2520Pei%26entry.1292438233%3D%2520%2520Underwater%2520instance%2520segmentation%2520is%2520challenging%2520due%2520to%2520adverse%2520visual%250Aconditions%2520such%2520as%2520light%2520attenuation%252C%2520scattering%252C%2520and%2520color%2520distortion%252C%2520which%250Adegrade%2520model%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520BARIS-Decoder%250A%2528Boundary-Aware%2520Refinement%2520Decoder%2520for%2520Instance%2520Segmentation%2529%252C%2520a%2520framework%2520that%250Aenhances%2520segmentation%2520accuracy%2520through%2520feature%2520refinement.%2520To%2520address%250Aunderwater%2520degradations%252C%2520we%2520introduce%2520the%2520Environmental%2520Robust%2520Adapter%2520%2528ERA%2529%252C%250Awhich%2520efficiently%2520models%2520underwater%2520degradation%2520patterns%2520while%2520reducing%250Atrainable%2520parameters%2520by%2520over%252090%255C%2525%2520compared%2520to%2520full%2520fine-tuning.%2520The%2520integration%250Aof%2520BARIS-Decoder%2520with%2520ERA-tuning%252C%2520referred%2520to%2520as%2520BARIS-ERA%252C%2520achieves%250Astate-of-the-art%2520performance%252C%2520surpassing%2520Mask%2520R-CNN%2520by%25203.4%2520mAP%2520with%2520a%2520Swin-B%250Abackbone%2520and%25203.8%2520mAP%2520with%2520ConvNeXt%2520V2.%2520Our%2520findings%2520demonstrate%2520the%250Aeffectiveness%2520of%2520BARIS-ERA%2520in%2520advancing%2520underwater%2520instance%2520segmentation%252C%250Aproviding%2520a%2520robust%2520and%2520efficient%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BARIS%3A%20Boundary-Aware%20Refinement%20with%20Environmental%20Degradation%20Priors%0A%20%20for%20Robust%20Underwater%20Instance%20Segmentation&entry.906535625=Pin-Chi%20Pan%20and%20Soo-Chang%20Pei&entry.1292438233=%20%20Underwater%20instance%20segmentation%20is%20challenging%20due%20to%20adverse%20visual%0Aconditions%20such%20as%20light%20attenuation%2C%20scattering%2C%20and%20color%20distortion%2C%20which%0Adegrade%20model%20performance.%20In%20this%20work%2C%20we%20propose%20BARIS-Decoder%0A%28Boundary-Aware%20Refinement%20Decoder%20for%20Instance%20Segmentation%29%2C%20a%20framework%20that%0Aenhances%20segmentation%20accuracy%20through%20feature%20refinement.%20To%20address%0Aunderwater%20degradations%2C%20we%20introduce%20the%20Environmental%20Robust%20Adapter%20%28ERA%29%2C%0Awhich%20efficiently%20models%20underwater%20degradation%20patterns%20while%20reducing%0Atrainable%20parameters%20by%20over%2090%5C%25%20compared%20to%20full%20fine-tuning.%20The%20integration%0Aof%20BARIS-Decoder%20with%20ERA-tuning%2C%20referred%20to%20as%20BARIS-ERA%2C%20achieves%0Astate-of-the-art%20performance%2C%20surpassing%20Mask%20R-CNN%20by%203.4%20mAP%20with%20a%20Swin-B%0Abackbone%20and%203.8%20mAP%20with%20ConvNeXt%20V2.%20Our%20findings%20demonstrate%20the%0Aeffectiveness%20of%20BARIS-ERA%20in%20advancing%20underwater%20instance%20segmentation%2C%0Aproviding%20a%20robust%20and%20efficient%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19643v1&entry.124074799=Read"},
{"title": "Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge\n  using FPGAs", "author": "Gang Mao and Tousif Rahman and Sidharth Maheshwari and Bob Pattison and Zhuang Shao and Rishad Shafik and Alex Yakovlev", "abstract": "  The increased demand for data privacy and security in machine learning (ML)\napplications has put impetus on effective edge training on Internet-of-Things\n(IoT) nodes. Edge training aims to leverage speed, energy efficiency and\nadaptability within the resource constraints of the nodes. Deploying and\ntraining Deep Neural Networks (DNNs)-based models at the edge, although\naccurate, posit significant challenges from the back-propagation algorithm's\ncomplexity, bit precision trade-offs, and heterogeneity of DNN layers. This\npaper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an\nalternative to DNN implementations. DTM utilizes logic-based on-chip inference\nwith finite-state automata-driven learning within the same Field Programmable\nGate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin\nMachine algorithms, the dynamic aspect of the accelerator design allows for a\nrun-time reconfiguration targeting different datasets, model architectures, and\nmodel sizes without resynthesis. This makes the DTM suitable for targeting\nmultivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer\nmultiply-accumulates, devoid of derivative computation. It is a data-centric ML\nalgorithm that learns by aligning Tsetlin automata with input data to form\nlogical propositions enabling efficient Look-up-Table (LUT) mapping and frugal\nBlock RAM usage in FPGA training implementations. The proposed accelerator\noffers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x\nless power than the next-best comparable design.\n", "link": "http://arxiv.org/abs/2504.19797v1", "date": "2025-04-28", "relevancy": 2.116, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.569}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5263}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Tsetlin%20Machine%20Accelerators%20for%20On-Chip%20Training%20at%20the%20Edge%0A%20%20using%20FPGAs&body=Title%3A%20Dynamic%20Tsetlin%20Machine%20Accelerators%20for%20On-Chip%20Training%20at%20the%20Edge%0A%20%20using%20FPGAs%0AAuthor%3A%20Gang%20Mao%20and%20Tousif%20Rahman%20and%20Sidharth%20Maheshwari%20and%20Bob%20Pattison%20and%20Zhuang%20Shao%20and%20Rishad%20Shafik%20and%20Alex%20Yakovlev%0AAbstract%3A%20%20%20The%20increased%20demand%20for%20data%20privacy%20and%20security%20in%20machine%20learning%20%28ML%29%0Aapplications%20has%20put%20impetus%20on%20effective%20edge%20training%20on%20Internet-of-Things%0A%28IoT%29%20nodes.%20Edge%20training%20aims%20to%20leverage%20speed%2C%20energy%20efficiency%20and%0Aadaptability%20within%20the%20resource%20constraints%20of%20the%20nodes.%20Deploying%20and%0Atraining%20Deep%20Neural%20Networks%20%28DNNs%29-based%20models%20at%20the%20edge%2C%20although%0Aaccurate%2C%20posit%20significant%20challenges%20from%20the%20back-propagation%20algorithm%27s%0Acomplexity%2C%20bit%20precision%20trade-offs%2C%20and%20heterogeneity%20of%20DNN%20layers.%20This%0Apaper%20presents%20a%20Dynamic%20Tsetlin%20Machine%20%28DTM%29%20training%20accelerator%20as%20an%0Aalternative%20to%20DNN%20implementations.%20DTM%20utilizes%20logic-based%20on-chip%20inference%0Awith%20finite-state%20automata-driven%20learning%20within%20the%20same%20Field%20Programmable%0AGate%20Array%20%28FPGA%29%20package.%20Underpinned%20on%20the%20Vanilla%20and%20Coalesced%20Tsetlin%0AMachine%20algorithms%2C%20the%20dynamic%20aspect%20of%20the%20accelerator%20design%20allows%20for%20a%0Arun-time%20reconfiguration%20targeting%20different%20datasets%2C%20model%20architectures%2C%20and%0Amodel%20sizes%20without%20resynthesis.%20This%20makes%20the%20DTM%20suitable%20for%20targeting%0Amultivariate%20sensor-based%20edge%20tasks.%20Compared%20to%20DNNs%2C%20DTM%20trains%20with%20fewer%0Amultiply-accumulates%2C%20devoid%20of%20derivative%20computation.%20It%20is%20a%20data-centric%20ML%0Aalgorithm%20that%20learns%20by%20aligning%20Tsetlin%20automata%20with%20input%20data%20to%20form%0Alogical%20propositions%20enabling%20efficient%20Look-up-Table%20%28LUT%29%20mapping%20and%20frugal%0ABlock%20RAM%20usage%20in%20FPGA%20training%20implementations.%20The%20proposed%20accelerator%0Aoffers%202.54x%20more%20Giga%20operations%20per%20second%20per%20Watt%20%28GOP/s%20per%20W%29%20and%20uses%206x%0Aless%20power%20than%20the%20next-best%20comparable%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Tsetlin%2520Machine%2520Accelerators%2520for%2520On-Chip%2520Training%2520at%2520the%2520Edge%250A%2520%2520using%2520FPGAs%26entry.906535625%3DGang%2520Mao%2520and%2520Tousif%2520Rahman%2520and%2520Sidharth%2520Maheshwari%2520and%2520Bob%2520Pattison%2520and%2520Zhuang%2520Shao%2520and%2520Rishad%2520Shafik%2520and%2520Alex%2520Yakovlev%26entry.1292438233%3D%2520%2520The%2520increased%2520demand%2520for%2520data%2520privacy%2520and%2520security%2520in%2520machine%2520learning%2520%2528ML%2529%250Aapplications%2520has%2520put%2520impetus%2520on%2520effective%2520edge%2520training%2520on%2520Internet-of-Things%250A%2528IoT%2529%2520nodes.%2520Edge%2520training%2520aims%2520to%2520leverage%2520speed%252C%2520energy%2520efficiency%2520and%250Aadaptability%2520within%2520the%2520resource%2520constraints%2520of%2520the%2520nodes.%2520Deploying%2520and%250Atraining%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529-based%2520models%2520at%2520the%2520edge%252C%2520although%250Aaccurate%252C%2520posit%2520significant%2520challenges%2520from%2520the%2520back-propagation%2520algorithm%2527s%250Acomplexity%252C%2520bit%2520precision%2520trade-offs%252C%2520and%2520heterogeneity%2520of%2520DNN%2520layers.%2520This%250Apaper%2520presents%2520a%2520Dynamic%2520Tsetlin%2520Machine%2520%2528DTM%2529%2520training%2520accelerator%2520as%2520an%250Aalternative%2520to%2520DNN%2520implementations.%2520DTM%2520utilizes%2520logic-based%2520on-chip%2520inference%250Awith%2520finite-state%2520automata-driven%2520learning%2520within%2520the%2520same%2520Field%2520Programmable%250AGate%2520Array%2520%2528FPGA%2529%2520package.%2520Underpinned%2520on%2520the%2520Vanilla%2520and%2520Coalesced%2520Tsetlin%250AMachine%2520algorithms%252C%2520the%2520dynamic%2520aspect%2520of%2520the%2520accelerator%2520design%2520allows%2520for%2520a%250Arun-time%2520reconfiguration%2520targeting%2520different%2520datasets%252C%2520model%2520architectures%252C%2520and%250Amodel%2520sizes%2520without%2520resynthesis.%2520This%2520makes%2520the%2520DTM%2520suitable%2520for%2520targeting%250Amultivariate%2520sensor-based%2520edge%2520tasks.%2520Compared%2520to%2520DNNs%252C%2520DTM%2520trains%2520with%2520fewer%250Amultiply-accumulates%252C%2520devoid%2520of%2520derivative%2520computation.%2520It%2520is%2520a%2520data-centric%2520ML%250Aalgorithm%2520that%2520learns%2520by%2520aligning%2520Tsetlin%2520automata%2520with%2520input%2520data%2520to%2520form%250Alogical%2520propositions%2520enabling%2520efficient%2520Look-up-Table%2520%2528LUT%2529%2520mapping%2520and%2520frugal%250ABlock%2520RAM%2520usage%2520in%2520FPGA%2520training%2520implementations.%2520The%2520proposed%2520accelerator%250Aoffers%25202.54x%2520more%2520Giga%2520operations%2520per%2520second%2520per%2520Watt%2520%2528GOP/s%2520per%2520W%2529%2520and%2520uses%25206x%250Aless%2520power%2520than%2520the%2520next-best%2520comparable%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Tsetlin%20Machine%20Accelerators%20for%20On-Chip%20Training%20at%20the%20Edge%0A%20%20using%20FPGAs&entry.906535625=Gang%20Mao%20and%20Tousif%20Rahman%20and%20Sidharth%20Maheshwari%20and%20Bob%20Pattison%20and%20Zhuang%20Shao%20and%20Rishad%20Shafik%20and%20Alex%20Yakovlev&entry.1292438233=%20%20The%20increased%20demand%20for%20data%20privacy%20and%20security%20in%20machine%20learning%20%28ML%29%0Aapplications%20has%20put%20impetus%20on%20effective%20edge%20training%20on%20Internet-of-Things%0A%28IoT%29%20nodes.%20Edge%20training%20aims%20to%20leverage%20speed%2C%20energy%20efficiency%20and%0Aadaptability%20within%20the%20resource%20constraints%20of%20the%20nodes.%20Deploying%20and%0Atraining%20Deep%20Neural%20Networks%20%28DNNs%29-based%20models%20at%20the%20edge%2C%20although%0Aaccurate%2C%20posit%20significant%20challenges%20from%20the%20back-propagation%20algorithm%27s%0Acomplexity%2C%20bit%20precision%20trade-offs%2C%20and%20heterogeneity%20of%20DNN%20layers.%20This%0Apaper%20presents%20a%20Dynamic%20Tsetlin%20Machine%20%28DTM%29%20training%20accelerator%20as%20an%0Aalternative%20to%20DNN%20implementations.%20DTM%20utilizes%20logic-based%20on-chip%20inference%0Awith%20finite-state%20automata-driven%20learning%20within%20the%20same%20Field%20Programmable%0AGate%20Array%20%28FPGA%29%20package.%20Underpinned%20on%20the%20Vanilla%20and%20Coalesced%20Tsetlin%0AMachine%20algorithms%2C%20the%20dynamic%20aspect%20of%20the%20accelerator%20design%20allows%20for%20a%0Arun-time%20reconfiguration%20targeting%20different%20datasets%2C%20model%20architectures%2C%20and%0Amodel%20sizes%20without%20resynthesis.%20This%20makes%20the%20DTM%20suitable%20for%20targeting%0Amultivariate%20sensor-based%20edge%20tasks.%20Compared%20to%20DNNs%2C%20DTM%20trains%20with%20fewer%0Amultiply-accumulates%2C%20devoid%20of%20derivative%20computation.%20It%20is%20a%20data-centric%20ML%0Aalgorithm%20that%20learns%20by%20aligning%20Tsetlin%20automata%20with%20input%20data%20to%20form%0Alogical%20propositions%20enabling%20efficient%20Look-up-Table%20%28LUT%29%20mapping%20and%20frugal%0ABlock%20RAM%20usage%20in%20FPGA%20training%20implementations.%20The%20proposed%20accelerator%0Aoffers%202.54x%20more%20Giga%20operations%20per%20second%20per%20Watt%20%28GOP/s%20per%20W%29%20and%20uses%206x%0Aless%20power%20than%20the%20next-best%20comparable%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19797v1&entry.124074799=Read"},
{"title": "Model-based controller assisted domain randomization in deep\n  reinforcement learning: application to nonlinear powertrain control", "author": "Heisei Yonezawa and Ansei Yonezawa and Itsuro Kajiwara", "abstract": "  Complex mechanical systems such as vehicle powertrains are inherently subject\nto multiple nonlinearities and uncertainties arising from parametric\nvariations. Modeling and calibration errors are therefore unavoidable, making\nthe transfer of control systems from simulation to real-world systems a\ncritical challenge. Traditional robust controls have limitations in handling\ncertain types of nonlinearities and uncertainties, requiring a more practical\napproach capable of comprehensively compensating for these various constraints.\nThis study proposes a new robust control approach using the framework of deep\nreinforcement learning (DRL). The key strategy lies in the synergy among domain\nrandomization-based DRL, long short-term memory (LSTM)-based actor and critic\nnetworks, and model-based control (MBC). The problem setup is modeled via the\nlatent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled\nsystem subject to uncertainties and nonlinearities. In LMDP, the dynamics of an\nenvironment simulator is randomized during training to improve the robustness\nof the control system to real testing environments. The randomization increases\ntraining difficulties as well as conservativeness of the resultant control\nsystem; therefore, progress is assisted by concurrent use of a model-based\ncontroller based on a nominal system model. Compared to traditional DRL-based\ncontrols, the proposed controller design is smarter in that we can achieve a\nhigh level of generalization ability with a more compact neural network\narchitecture and a smaller amount of training data. The proposed approach is\nverified via practical application to active damping for a complex powertrain\nsystem with nonlinearities and parametric variations. Comparative tests\ndemonstrate the high robustness of the proposed approach.\n", "link": "http://arxiv.org/abs/2504.19715v1", "date": "2025-04-28", "relevancy": 2.1071, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5694}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5284}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-based%20controller%20assisted%20domain%20randomization%20in%20deep%0A%20%20reinforcement%20learning%3A%20application%20to%20nonlinear%20powertrain%20control&body=Title%3A%20Model-based%20controller%20assisted%20domain%20randomization%20in%20deep%0A%20%20reinforcement%20learning%3A%20application%20to%20nonlinear%20powertrain%20control%0AAuthor%3A%20Heisei%20Yonezawa%20and%20Ansei%20Yonezawa%20and%20Itsuro%20Kajiwara%0AAbstract%3A%20%20%20Complex%20mechanical%20systems%20such%20as%20vehicle%20powertrains%20are%20inherently%20subject%0Ato%20multiple%20nonlinearities%20and%20uncertainties%20arising%20from%20parametric%0Avariations.%20Modeling%20and%20calibration%20errors%20are%20therefore%20unavoidable%2C%20making%0Athe%20transfer%20of%20control%20systems%20from%20simulation%20to%20real-world%20systems%20a%0Acritical%20challenge.%20Traditional%20robust%20controls%20have%20limitations%20in%20handling%0Acertain%20types%20of%20nonlinearities%20and%20uncertainties%2C%20requiring%20a%20more%20practical%0Aapproach%20capable%20of%20comprehensively%20compensating%20for%20these%20various%20constraints.%0AThis%20study%20proposes%20a%20new%20robust%20control%20approach%20using%20the%20framework%20of%20deep%0Areinforcement%20learning%20%28DRL%29.%20The%20key%20strategy%20lies%20in%20the%20synergy%20among%20domain%0Arandomization-based%20DRL%2C%20long%20short-term%20memory%20%28LSTM%29-based%20actor%20and%20critic%0Anetworks%2C%20and%20model-based%20control%20%28MBC%29.%20The%20problem%20setup%20is%20modeled%20via%20the%0Alatent%20Markov%20decision%20process%20%28LMDP%29%2C%20a%20set%20of%20vanilla%20MDPs%2C%20for%20a%20controlled%0Asystem%20subject%20to%20uncertainties%20and%20nonlinearities.%20In%20LMDP%2C%20the%20dynamics%20of%20an%0Aenvironment%20simulator%20is%20randomized%20during%20training%20to%20improve%20the%20robustness%0Aof%20the%20control%20system%20to%20real%20testing%20environments.%20The%20randomization%20increases%0Atraining%20difficulties%20as%20well%20as%20conservativeness%20of%20the%20resultant%20control%0Asystem%3B%20therefore%2C%20progress%20is%20assisted%20by%20concurrent%20use%20of%20a%20model-based%0Acontroller%20based%20on%20a%20nominal%20system%20model.%20Compared%20to%20traditional%20DRL-based%0Acontrols%2C%20the%20proposed%20controller%20design%20is%20smarter%20in%20that%20we%20can%20achieve%20a%0Ahigh%20level%20of%20generalization%20ability%20with%20a%20more%20compact%20neural%20network%0Aarchitecture%20and%20a%20smaller%20amount%20of%20training%20data.%20The%20proposed%20approach%20is%0Averified%20via%20practical%20application%20to%20active%20damping%20for%20a%20complex%20powertrain%0Asystem%20with%20nonlinearities%20and%20parametric%20variations.%20Comparative%20tests%0Ademonstrate%20the%20high%20robustness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-based%2520controller%2520assisted%2520domain%2520randomization%2520in%2520deep%250A%2520%2520reinforcement%2520learning%253A%2520application%2520to%2520nonlinear%2520powertrain%2520control%26entry.906535625%3DHeisei%2520Yonezawa%2520and%2520Ansei%2520Yonezawa%2520and%2520Itsuro%2520Kajiwara%26entry.1292438233%3D%2520%2520Complex%2520mechanical%2520systems%2520such%2520as%2520vehicle%2520powertrains%2520are%2520inherently%2520subject%250Ato%2520multiple%2520nonlinearities%2520and%2520uncertainties%2520arising%2520from%2520parametric%250Avariations.%2520Modeling%2520and%2520calibration%2520errors%2520are%2520therefore%2520unavoidable%252C%2520making%250Athe%2520transfer%2520of%2520control%2520systems%2520from%2520simulation%2520to%2520real-world%2520systems%2520a%250Acritical%2520challenge.%2520Traditional%2520robust%2520controls%2520have%2520limitations%2520in%2520handling%250Acertain%2520types%2520of%2520nonlinearities%2520and%2520uncertainties%252C%2520requiring%2520a%2520more%2520practical%250Aapproach%2520capable%2520of%2520comprehensively%2520compensating%2520for%2520these%2520various%2520constraints.%250AThis%2520study%2520proposes%2520a%2520new%2520robust%2520control%2520approach%2520using%2520the%2520framework%2520of%2520deep%250Areinforcement%2520learning%2520%2528DRL%2529.%2520The%2520key%2520strategy%2520lies%2520in%2520the%2520synergy%2520among%2520domain%250Arandomization-based%2520DRL%252C%2520long%2520short-term%2520memory%2520%2528LSTM%2529-based%2520actor%2520and%2520critic%250Anetworks%252C%2520and%2520model-based%2520control%2520%2528MBC%2529.%2520The%2520problem%2520setup%2520is%2520modeled%2520via%2520the%250Alatent%2520Markov%2520decision%2520process%2520%2528LMDP%2529%252C%2520a%2520set%2520of%2520vanilla%2520MDPs%252C%2520for%2520a%2520controlled%250Asystem%2520subject%2520to%2520uncertainties%2520and%2520nonlinearities.%2520In%2520LMDP%252C%2520the%2520dynamics%2520of%2520an%250Aenvironment%2520simulator%2520is%2520randomized%2520during%2520training%2520to%2520improve%2520the%2520robustness%250Aof%2520the%2520control%2520system%2520to%2520real%2520testing%2520environments.%2520The%2520randomization%2520increases%250Atraining%2520difficulties%2520as%2520well%2520as%2520conservativeness%2520of%2520the%2520resultant%2520control%250Asystem%253B%2520therefore%252C%2520progress%2520is%2520assisted%2520by%2520concurrent%2520use%2520of%2520a%2520model-based%250Acontroller%2520based%2520on%2520a%2520nominal%2520system%2520model.%2520Compared%2520to%2520traditional%2520DRL-based%250Acontrols%252C%2520the%2520proposed%2520controller%2520design%2520is%2520smarter%2520in%2520that%2520we%2520can%2520achieve%2520a%250Ahigh%2520level%2520of%2520generalization%2520ability%2520with%2520a%2520more%2520compact%2520neural%2520network%250Aarchitecture%2520and%2520a%2520smaller%2520amount%2520of%2520training%2520data.%2520The%2520proposed%2520approach%2520is%250Averified%2520via%2520practical%2520application%2520to%2520active%2520damping%2520for%2520a%2520complex%2520powertrain%250Asystem%2520with%2520nonlinearities%2520and%2520parametric%2520variations.%2520Comparative%2520tests%250Ademonstrate%2520the%2520high%2520robustness%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-based%20controller%20assisted%20domain%20randomization%20in%20deep%0A%20%20reinforcement%20learning%3A%20application%20to%20nonlinear%20powertrain%20control&entry.906535625=Heisei%20Yonezawa%20and%20Ansei%20Yonezawa%20and%20Itsuro%20Kajiwara&entry.1292438233=%20%20Complex%20mechanical%20systems%20such%20as%20vehicle%20powertrains%20are%20inherently%20subject%0Ato%20multiple%20nonlinearities%20and%20uncertainties%20arising%20from%20parametric%0Avariations.%20Modeling%20and%20calibration%20errors%20are%20therefore%20unavoidable%2C%20making%0Athe%20transfer%20of%20control%20systems%20from%20simulation%20to%20real-world%20systems%20a%0Acritical%20challenge.%20Traditional%20robust%20controls%20have%20limitations%20in%20handling%0Acertain%20types%20of%20nonlinearities%20and%20uncertainties%2C%20requiring%20a%20more%20practical%0Aapproach%20capable%20of%20comprehensively%20compensating%20for%20these%20various%20constraints.%0AThis%20study%20proposes%20a%20new%20robust%20control%20approach%20using%20the%20framework%20of%20deep%0Areinforcement%20learning%20%28DRL%29.%20The%20key%20strategy%20lies%20in%20the%20synergy%20among%20domain%0Arandomization-based%20DRL%2C%20long%20short-term%20memory%20%28LSTM%29-based%20actor%20and%20critic%0Anetworks%2C%20and%20model-based%20control%20%28MBC%29.%20The%20problem%20setup%20is%20modeled%20via%20the%0Alatent%20Markov%20decision%20process%20%28LMDP%29%2C%20a%20set%20of%20vanilla%20MDPs%2C%20for%20a%20controlled%0Asystem%20subject%20to%20uncertainties%20and%20nonlinearities.%20In%20LMDP%2C%20the%20dynamics%20of%20an%0Aenvironment%20simulator%20is%20randomized%20during%20training%20to%20improve%20the%20robustness%0Aof%20the%20control%20system%20to%20real%20testing%20environments.%20The%20randomization%20increases%0Atraining%20difficulties%20as%20well%20as%20conservativeness%20of%20the%20resultant%20control%0Asystem%3B%20therefore%2C%20progress%20is%20assisted%20by%20concurrent%20use%20of%20a%20model-based%0Acontroller%20based%20on%20a%20nominal%20system%20model.%20Compared%20to%20traditional%20DRL-based%0Acontrols%2C%20the%20proposed%20controller%20design%20is%20smarter%20in%20that%20we%20can%20achieve%20a%0Ahigh%20level%20of%20generalization%20ability%20with%20a%20more%20compact%20neural%20network%0Aarchitecture%20and%20a%20smaller%20amount%20of%20training%20data.%20The%20proposed%20approach%20is%0Averified%20via%20practical%20application%20to%20active%20damping%20for%20a%20complex%20powertrain%0Asystem%20with%20nonlinearities%20and%20parametric%20variations.%20Comparative%20tests%0Ademonstrate%20the%20high%20robustness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19715v1&entry.124074799=Read"},
{"title": "ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather\n  Classification in Traffic Camera Imagery", "author": "Anush Lakshman Sivaraman and Kojo Adu-Gyamfi and Ibne Farabi Shihab and Anuj Sharma", "abstract": "  Accurate weather classification from low-quality traffic camera imagery\nremains a challenging task, particularly under adverse nighttime conditions. In\nthis study, we propose a scalable framework that combines generative domain\nadaptation with efficient contrastive learning to enhance classification\nperformance. Using CycleGAN-based domain translation, we improve the quality of\nnighttime images, enabling better feature extraction by downstream models.\nWhile the baseline EVA-02 model employing CLIP-based contrastive loss achieves\nan overall accuracy of 96.55\\%, it exhibits a significant performance gap\nbetween daytime (97.21\\%) and nighttime conditions (63.40\\%). Replacing CLIP\nwith the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive\noverall accuracy of 94.00\\%, with substantial improvements in nighttime\nperformance (85.90\\% accuracy). The combination of Vision-SigLIP-2,\nText-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime\naccuracy (85.90\\%) among all models tested, while EVA-02 with CycleGAN\nmaintains the highest overall accuracy (97.01\\%) and per-class accuracies.\nThese findings demonstrate the potential of combining domain adaptation and\nefficient contrastive learning to build practical, resource-efficient weather\nclassification systems for intelligent transportation infrastructure.\n", "link": "http://arxiv.org/abs/2504.19684v1", "date": "2025-04-28", "relevancy": 2.0918, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5293}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5207}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClearVision%3A%20Leveraging%20CycleGAN%20and%20SigLIP-2%20for%20Robust%20All-Weather%0A%20%20Classification%20in%20Traffic%20Camera%20Imagery&body=Title%3A%20ClearVision%3A%20Leveraging%20CycleGAN%20and%20SigLIP-2%20for%20Robust%20All-Weather%0A%20%20Classification%20in%20Traffic%20Camera%20Imagery%0AAuthor%3A%20Anush%20Lakshman%20Sivaraman%20and%20Kojo%20Adu-Gyamfi%20and%20Ibne%20Farabi%20Shihab%20and%20Anuj%20Sharma%0AAbstract%3A%20%20%20Accurate%20weather%20classification%20from%20low-quality%20traffic%20camera%20imagery%0Aremains%20a%20challenging%20task%2C%20particularly%20under%20adverse%20nighttime%20conditions.%20In%0Athis%20study%2C%20we%20propose%20a%20scalable%20framework%20that%20combines%20generative%20domain%0Aadaptation%20with%20efficient%20contrastive%20learning%20to%20enhance%20classification%0Aperformance.%20Using%20CycleGAN-based%20domain%20translation%2C%20we%20improve%20the%20quality%20of%0Anighttime%20images%2C%20enabling%20better%20feature%20extraction%20by%20downstream%20models.%0AWhile%20the%20baseline%20EVA-02%20model%20employing%20CLIP-based%20contrastive%20loss%20achieves%0Aan%20overall%20accuracy%20of%2096.55%5C%25%2C%20it%20exhibits%20a%20significant%20performance%20gap%0Abetween%20daytime%20%2897.21%5C%25%29%20and%20nighttime%20conditions%20%2863.40%5C%25%29.%20Replacing%20CLIP%0Awith%20the%20lightweight%20SigLIP-2%20%28Sigmoid%20contrastive%20loss%29%20achieves%20a%20competitive%0Aoverall%20accuracy%20of%2094.00%5C%25%2C%20with%20substantial%20improvements%20in%20nighttime%0Aperformance%20%2885.90%5C%25%20accuracy%29.%20The%20combination%20of%20Vision-SigLIP-2%2C%0AText-SigLIP-2%2C%20CycleGAN%2C%20and%20contrastive%20training%20achieves%20the%20best%20nighttime%0Aaccuracy%20%2885.90%5C%25%29%20among%20all%20models%20tested%2C%20while%20EVA-02%20with%20CycleGAN%0Amaintains%20the%20highest%20overall%20accuracy%20%2897.01%5C%25%29%20and%20per-class%20accuracies.%0AThese%20findings%20demonstrate%20the%20potential%20of%20combining%20domain%20adaptation%20and%0Aefficient%20contrastive%20learning%20to%20build%20practical%2C%20resource-efficient%20weather%0Aclassification%20systems%20for%20intelligent%20transportation%20infrastructure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClearVision%253A%2520Leveraging%2520CycleGAN%2520and%2520SigLIP-2%2520for%2520Robust%2520All-Weather%250A%2520%2520Classification%2520in%2520Traffic%2520Camera%2520Imagery%26entry.906535625%3DAnush%2520Lakshman%2520Sivaraman%2520and%2520Kojo%2520Adu-Gyamfi%2520and%2520Ibne%2520Farabi%2520Shihab%2520and%2520Anuj%2520Sharma%26entry.1292438233%3D%2520%2520Accurate%2520weather%2520classification%2520from%2520low-quality%2520traffic%2520camera%2520imagery%250Aremains%2520a%2520challenging%2520task%252C%2520particularly%2520under%2520adverse%2520nighttime%2520conditions.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520scalable%2520framework%2520that%2520combines%2520generative%2520domain%250Aadaptation%2520with%2520efficient%2520contrastive%2520learning%2520to%2520enhance%2520classification%250Aperformance.%2520Using%2520CycleGAN-based%2520domain%2520translation%252C%2520we%2520improve%2520the%2520quality%2520of%250Anighttime%2520images%252C%2520enabling%2520better%2520feature%2520extraction%2520by%2520downstream%2520models.%250AWhile%2520the%2520baseline%2520EVA-02%2520model%2520employing%2520CLIP-based%2520contrastive%2520loss%2520achieves%250Aan%2520overall%2520accuracy%2520of%252096.55%255C%2525%252C%2520it%2520exhibits%2520a%2520significant%2520performance%2520gap%250Abetween%2520daytime%2520%252897.21%255C%2525%2529%2520and%2520nighttime%2520conditions%2520%252863.40%255C%2525%2529.%2520Replacing%2520CLIP%250Awith%2520the%2520lightweight%2520SigLIP-2%2520%2528Sigmoid%2520contrastive%2520loss%2529%2520achieves%2520a%2520competitive%250Aoverall%2520accuracy%2520of%252094.00%255C%2525%252C%2520with%2520substantial%2520improvements%2520in%2520nighttime%250Aperformance%2520%252885.90%255C%2525%2520accuracy%2529.%2520The%2520combination%2520of%2520Vision-SigLIP-2%252C%250AText-SigLIP-2%252C%2520CycleGAN%252C%2520and%2520contrastive%2520training%2520achieves%2520the%2520best%2520nighttime%250Aaccuracy%2520%252885.90%255C%2525%2529%2520among%2520all%2520models%2520tested%252C%2520while%2520EVA-02%2520with%2520CycleGAN%250Amaintains%2520the%2520highest%2520overall%2520accuracy%2520%252897.01%255C%2525%2529%2520and%2520per-class%2520accuracies.%250AThese%2520findings%2520demonstrate%2520the%2520potential%2520of%2520combining%2520domain%2520adaptation%2520and%250Aefficient%2520contrastive%2520learning%2520to%2520build%2520practical%252C%2520resource-efficient%2520weather%250Aclassification%2520systems%2520for%2520intelligent%2520transportation%2520infrastructure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClearVision%3A%20Leveraging%20CycleGAN%20and%20SigLIP-2%20for%20Robust%20All-Weather%0A%20%20Classification%20in%20Traffic%20Camera%20Imagery&entry.906535625=Anush%20Lakshman%20Sivaraman%20and%20Kojo%20Adu-Gyamfi%20and%20Ibne%20Farabi%20Shihab%20and%20Anuj%20Sharma&entry.1292438233=%20%20Accurate%20weather%20classification%20from%20low-quality%20traffic%20camera%20imagery%0Aremains%20a%20challenging%20task%2C%20particularly%20under%20adverse%20nighttime%20conditions.%20In%0Athis%20study%2C%20we%20propose%20a%20scalable%20framework%20that%20combines%20generative%20domain%0Aadaptation%20with%20efficient%20contrastive%20learning%20to%20enhance%20classification%0Aperformance.%20Using%20CycleGAN-based%20domain%20translation%2C%20we%20improve%20the%20quality%20of%0Anighttime%20images%2C%20enabling%20better%20feature%20extraction%20by%20downstream%20models.%0AWhile%20the%20baseline%20EVA-02%20model%20employing%20CLIP-based%20contrastive%20loss%20achieves%0Aan%20overall%20accuracy%20of%2096.55%5C%25%2C%20it%20exhibits%20a%20significant%20performance%20gap%0Abetween%20daytime%20%2897.21%5C%25%29%20and%20nighttime%20conditions%20%2863.40%5C%25%29.%20Replacing%20CLIP%0Awith%20the%20lightweight%20SigLIP-2%20%28Sigmoid%20contrastive%20loss%29%20achieves%20a%20competitive%0Aoverall%20accuracy%20of%2094.00%5C%25%2C%20with%20substantial%20improvements%20in%20nighttime%0Aperformance%20%2885.90%5C%25%20accuracy%29.%20The%20combination%20of%20Vision-SigLIP-2%2C%0AText-SigLIP-2%2C%20CycleGAN%2C%20and%20contrastive%20training%20achieves%20the%20best%20nighttime%0Aaccuracy%20%2885.90%5C%25%29%20among%20all%20models%20tested%2C%20while%20EVA-02%20with%20CycleGAN%0Amaintains%20the%20highest%20overall%20accuracy%20%2897.01%5C%25%29%20and%20per-class%20accuracies.%0AThese%20findings%20demonstrate%20the%20potential%20of%20combining%20domain%20adaptation%20and%0Aefficient%20contrastive%20learning%20to%20build%20practical%2C%20resource-efficient%20weather%0Aclassification%20systems%20for%20intelligent%20transportation%20infrastructure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19684v1&entry.124074799=Read"},
{"title": "A Tripartite Perspective on GraphRAG", "author": "Michael Banf and Johannes Kuhn", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities across\nvarious domains, yet they struggle with knowledge-intensive tasks in areas that\ndemand factual accuracy, e.g. industrial automation and healthcare. Key\nlimitations include their tendency to hallucinate, lack of source traceability\n(provenance), and challenges in timely knowledge updates. Combining language\nmodels with knowledge graphs (GraphRAG) offers promising avenues for overcoming\nthese deficits. However, a major challenge lies in creating such a knowledge\ngraph in the first place. Here, we propose a novel approach that combines LLMs\nwith a tripartite knowledge graph representation, which is constructed by\nconnecting complex, domain-specific objects via a curated ontology of\ncorresponding, domain-specific concepts to relevant sections within chunks of\ntext through a concept-anchored pre-analysis of source documents starting from\nan initial lexical graph. As a consequence, our Tripartite-GraphRAG approach\nimplements: i) a concept-specific, information-preserving pre-compression of\ntextual chunks; ii) allows for the formation of a concept-specific relevance\nestimation of embedding similarities grounded in statistics; and iii) avoids\ncommon challenges w.r.t. continuous extendability, such as the need for entity\nresolution and deduplication. By applying a transformation to the knowledge\ngraph, we formulate LLM prompt creation as an unsupervised node classification\nproblem, drawing on ideas from Markov Random Fields. We evaluate our approach\non a healthcare use case, involving multi-faceted analyses of patient anamneses\ngiven a set of medical concepts as well as clinical literature. Experiments\nindicate that it can optimize information density, coverage, and arrangement of\nLLM prompts while reducing their lengths, which may lead to reduced costs and\nmore consistent and reliable LLM outputs.\n", "link": "http://arxiv.org/abs/2504.19667v1", "date": "2025-04-28", "relevancy": 2.0883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tripartite%20Perspective%20on%20GraphRAG&body=Title%3A%20A%20Tripartite%20Perspective%20on%20GraphRAG%0AAuthor%3A%20Michael%20Banf%20and%20Johannes%20Kuhn%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20across%0Avarious%20domains%2C%20yet%20they%20struggle%20with%20knowledge-intensive%20tasks%20in%20areas%20that%0Ademand%20factual%20accuracy%2C%20e.g.%20industrial%20automation%20and%20healthcare.%20Key%0Alimitations%20include%20their%20tendency%20to%20hallucinate%2C%20lack%20of%20source%20traceability%0A%28provenance%29%2C%20and%20challenges%20in%20timely%20knowledge%20updates.%20Combining%20language%0Amodels%20with%20knowledge%20graphs%20%28GraphRAG%29%20offers%20promising%20avenues%20for%20overcoming%0Athese%20deficits.%20However%2C%20a%20major%20challenge%20lies%20in%20creating%20such%20a%20knowledge%0Agraph%20in%20the%20first%20place.%20Here%2C%20we%20propose%20a%20novel%20approach%20that%20combines%20LLMs%0Awith%20a%20tripartite%20knowledge%20graph%20representation%2C%20which%20is%20constructed%20by%0Aconnecting%20complex%2C%20domain-specific%20objects%20via%20a%20curated%20ontology%20of%0Acorresponding%2C%20domain-specific%20concepts%20to%20relevant%20sections%20within%20chunks%20of%0Atext%20through%20a%20concept-anchored%20pre-analysis%20of%20source%20documents%20starting%20from%0Aan%20initial%20lexical%20graph.%20As%20a%20consequence%2C%20our%20Tripartite-GraphRAG%20approach%0Aimplements%3A%20i%29%20a%20concept-specific%2C%20information-preserving%20pre-compression%20of%0Atextual%20chunks%3B%20ii%29%20allows%20for%20the%20formation%20of%20a%20concept-specific%20relevance%0Aestimation%20of%20embedding%20similarities%20grounded%20in%20statistics%3B%20and%20iii%29%20avoids%0Acommon%20challenges%20w.r.t.%20continuous%20extendability%2C%20such%20as%20the%20need%20for%20entity%0Aresolution%20and%20deduplication.%20By%20applying%20a%20transformation%20to%20the%20knowledge%0Agraph%2C%20we%20formulate%20LLM%20prompt%20creation%20as%20an%20unsupervised%20node%20classification%0Aproblem%2C%20drawing%20on%20ideas%20from%20Markov%20Random%20Fields.%20We%20evaluate%20our%20approach%0Aon%20a%20healthcare%20use%20case%2C%20involving%20multi-faceted%20analyses%20of%20patient%20anamneses%0Agiven%20a%20set%20of%20medical%20concepts%20as%20well%20as%20clinical%20literature.%20Experiments%0Aindicate%20that%20it%20can%20optimize%20information%20density%2C%20coverage%2C%20and%20arrangement%20of%0ALLM%20prompts%20while%20reducing%20their%20lengths%2C%20which%20may%20lead%20to%20reduced%20costs%20and%0Amore%20consistent%20and%20reliable%20LLM%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tripartite%2520Perspective%2520on%2520GraphRAG%26entry.906535625%3DMichael%2520Banf%2520and%2520Johannes%2520Kuhn%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520across%250Avarious%2520domains%252C%2520yet%2520they%2520struggle%2520with%2520knowledge-intensive%2520tasks%2520in%2520areas%2520that%250Ademand%2520factual%2520accuracy%252C%2520e.g.%2520industrial%2520automation%2520and%2520healthcare.%2520Key%250Alimitations%2520include%2520their%2520tendency%2520to%2520hallucinate%252C%2520lack%2520of%2520source%2520traceability%250A%2528provenance%2529%252C%2520and%2520challenges%2520in%2520timely%2520knowledge%2520updates.%2520Combining%2520language%250Amodels%2520with%2520knowledge%2520graphs%2520%2528GraphRAG%2529%2520offers%2520promising%2520avenues%2520for%2520overcoming%250Athese%2520deficits.%2520However%252C%2520a%2520major%2520challenge%2520lies%2520in%2520creating%2520such%2520a%2520knowledge%250Agraph%2520in%2520the%2520first%2520place.%2520Here%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520combines%2520LLMs%250Awith%2520a%2520tripartite%2520knowledge%2520graph%2520representation%252C%2520which%2520is%2520constructed%2520by%250Aconnecting%2520complex%252C%2520domain-specific%2520objects%2520via%2520a%2520curated%2520ontology%2520of%250Acorresponding%252C%2520domain-specific%2520concepts%2520to%2520relevant%2520sections%2520within%2520chunks%2520of%250Atext%2520through%2520a%2520concept-anchored%2520pre-analysis%2520of%2520source%2520documents%2520starting%2520from%250Aan%2520initial%2520lexical%2520graph.%2520As%2520a%2520consequence%252C%2520our%2520Tripartite-GraphRAG%2520approach%250Aimplements%253A%2520i%2529%2520a%2520concept-specific%252C%2520information-preserving%2520pre-compression%2520of%250Atextual%2520chunks%253B%2520ii%2529%2520allows%2520for%2520the%2520formation%2520of%2520a%2520concept-specific%2520relevance%250Aestimation%2520of%2520embedding%2520similarities%2520grounded%2520in%2520statistics%253B%2520and%2520iii%2529%2520avoids%250Acommon%2520challenges%2520w.r.t.%2520continuous%2520extendability%252C%2520such%2520as%2520the%2520need%2520for%2520entity%250Aresolution%2520and%2520deduplication.%2520By%2520applying%2520a%2520transformation%2520to%2520the%2520knowledge%250Agraph%252C%2520we%2520formulate%2520LLM%2520prompt%2520creation%2520as%2520an%2520unsupervised%2520node%2520classification%250Aproblem%252C%2520drawing%2520on%2520ideas%2520from%2520Markov%2520Random%2520Fields.%2520We%2520evaluate%2520our%2520approach%250Aon%2520a%2520healthcare%2520use%2520case%252C%2520involving%2520multi-faceted%2520analyses%2520of%2520patient%2520anamneses%250Agiven%2520a%2520set%2520of%2520medical%2520concepts%2520as%2520well%2520as%2520clinical%2520literature.%2520Experiments%250Aindicate%2520that%2520it%2520can%2520optimize%2520information%2520density%252C%2520coverage%252C%2520and%2520arrangement%2520of%250ALLM%2520prompts%2520while%2520reducing%2520their%2520lengths%252C%2520which%2520may%2520lead%2520to%2520reduced%2520costs%2520and%250Amore%2520consistent%2520and%2520reliable%2520LLM%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tripartite%20Perspective%20on%20GraphRAG&entry.906535625=Michael%20Banf%20and%20Johannes%20Kuhn&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20across%0Avarious%20domains%2C%20yet%20they%20struggle%20with%20knowledge-intensive%20tasks%20in%20areas%20that%0Ademand%20factual%20accuracy%2C%20e.g.%20industrial%20automation%20and%20healthcare.%20Key%0Alimitations%20include%20their%20tendency%20to%20hallucinate%2C%20lack%20of%20source%20traceability%0A%28provenance%29%2C%20and%20challenges%20in%20timely%20knowledge%20updates.%20Combining%20language%0Amodels%20with%20knowledge%20graphs%20%28GraphRAG%29%20offers%20promising%20avenues%20for%20overcoming%0Athese%20deficits.%20However%2C%20a%20major%20challenge%20lies%20in%20creating%20such%20a%20knowledge%0Agraph%20in%20the%20first%20place.%20Here%2C%20we%20propose%20a%20novel%20approach%20that%20combines%20LLMs%0Awith%20a%20tripartite%20knowledge%20graph%20representation%2C%20which%20is%20constructed%20by%0Aconnecting%20complex%2C%20domain-specific%20objects%20via%20a%20curated%20ontology%20of%0Acorresponding%2C%20domain-specific%20concepts%20to%20relevant%20sections%20within%20chunks%20of%0Atext%20through%20a%20concept-anchored%20pre-analysis%20of%20source%20documents%20starting%20from%0Aan%20initial%20lexical%20graph.%20As%20a%20consequence%2C%20our%20Tripartite-GraphRAG%20approach%0Aimplements%3A%20i%29%20a%20concept-specific%2C%20information-preserving%20pre-compression%20of%0Atextual%20chunks%3B%20ii%29%20allows%20for%20the%20formation%20of%20a%20concept-specific%20relevance%0Aestimation%20of%20embedding%20similarities%20grounded%20in%20statistics%3B%20and%20iii%29%20avoids%0Acommon%20challenges%20w.r.t.%20continuous%20extendability%2C%20such%20as%20the%20need%20for%20entity%0Aresolution%20and%20deduplication.%20By%20applying%20a%20transformation%20to%20the%20knowledge%0Agraph%2C%20we%20formulate%20LLM%20prompt%20creation%20as%20an%20unsupervised%20node%20classification%0Aproblem%2C%20drawing%20on%20ideas%20from%20Markov%20Random%20Fields.%20We%20evaluate%20our%20approach%0Aon%20a%20healthcare%20use%20case%2C%20involving%20multi-faceted%20analyses%20of%20patient%20anamneses%0Agiven%20a%20set%20of%20medical%20concepts%20as%20well%20as%20clinical%20literature.%20Experiments%0Aindicate%20that%20it%20can%20optimize%20information%20density%2C%20coverage%2C%20and%20arrangement%20of%0ALLM%20prompts%20while%20reducing%20their%20lengths%2C%20which%20may%20lead%20to%20reduced%20costs%20and%0Amore%20consistent%20and%20reliable%20LLM%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19667v1&entry.124074799=Read"},
{"title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "author": "Shijie Xia and Yiwei Qin and Xuefeng Li and Yan Ma and Run-Ze Fan and Steffi Chern and Haoyang Zou and Fan Zhou and Xiangkun Hu and Jiahe Jin and Yanheng He and Yixin Ye and Yixiu Liu and Pengfei Liu", "abstract": "  The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations such as\nknowledge latency, shallow reasoning, and constrained cognitive processes.\nDuring this era, prompt engineering emerged as our primary interface with AI,\nenabling dialogue-level communication through natural language. We now witness\nthe emergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering\n", "link": "http://arxiv.org/abs/2504.13828v3", "date": "2025-04-28", "relevancy": 2.084, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.558}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4998}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering&body=Title%3A%20Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering%0AAuthor%3A%20Shijie%20Xia%20and%20Yiwei%20Qin%20and%20Xuefeng%20Li%20and%20Yan%20Ma%20and%20Run-Ze%20Fan%20and%20Steffi%20Chern%20and%20Haoyang%20Zou%20and%20Fan%20Zhou%20and%20Xiangkun%20Hu%20and%20Jiahe%20Jin%20and%20Yanheng%20He%20and%20Yixin%20Ye%20and%20Yixiu%20Liu%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20The%20first%20generation%20of%20Large%20Language%20Models%20-%20what%20might%20be%20called%20%22Act%20I%22%0Aof%20generative%20AI%20%282020-2023%29%20-%20achieved%20remarkable%20success%20through%20massive%0Aparameter%20and%20data%20scaling%2C%20yet%20exhibited%20fundamental%20limitations%20such%20as%0Aknowledge%20latency%2C%20shallow%20reasoning%2C%20and%20constrained%20cognitive%20processes.%0ADuring%20this%20era%2C%20prompt%20engineering%20emerged%20as%20our%20primary%20interface%20with%20AI%2C%0Aenabling%20dialogue-level%20communication%20through%20natural%20language.%20We%20now%20witness%0Athe%20emergence%20of%20%22Act%20II%22%20%282024-present%29%2C%20where%20models%20are%20transitioning%20from%0Aknowledge-retrieval%20systems%20%28in%20latent%20space%29%20to%20thought-construction%20engines%0Athrough%20test-time%20scaling%20techniques.%20This%20new%20paradigm%20establishes%20a%0Amind-level%20connection%20with%20AI%20through%20language-based%20thoughts.%20In%20this%20paper%2C%0Awe%20clarify%20the%20conceptual%20foundations%20of%20cognition%20engineering%20and%20explain%20why%0Athis%20moment%20is%20critical%20for%20its%20development.%20We%20systematically%20break%20down%20these%0Aadvanced%20approaches%20through%20comprehensive%20tutorials%20and%20optimized%0Aimplementations%2C%20democratizing%20access%20to%20cognition%20engineering%20and%20enabling%0Aevery%20practitioner%20to%20participate%20in%20AI%27s%20second%20act.%20We%20provide%20a%20regularly%0Aupdated%20collection%20of%20papers%20on%20test-time%20scaling%20in%20the%20GitHub%20Repository%3A%0Ahttps%3A//github.com/GAIR-NLP/cognition-engineering%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13828v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Act%2520II%253A%2520Test%2520Time%2520Scaling%2520Drives%2520Cognition%2520Engineering%26entry.906535625%3DShijie%2520Xia%2520and%2520Yiwei%2520Qin%2520and%2520Xuefeng%2520Li%2520and%2520Yan%2520Ma%2520and%2520Run-Ze%2520Fan%2520and%2520Steffi%2520Chern%2520and%2520Haoyang%2520Zou%2520and%2520Fan%2520Zhou%2520and%2520Xiangkun%2520Hu%2520and%2520Jiahe%2520Jin%2520and%2520Yanheng%2520He%2520and%2520Yixin%2520Ye%2520and%2520Yixiu%2520Liu%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520The%2520first%2520generation%2520of%2520Large%2520Language%2520Models%2520-%2520what%2520might%2520be%2520called%2520%2522Act%2520I%2522%250Aof%2520generative%2520AI%2520%25282020-2023%2529%2520-%2520achieved%2520remarkable%2520success%2520through%2520massive%250Aparameter%2520and%2520data%2520scaling%252C%2520yet%2520exhibited%2520fundamental%2520limitations%2520such%2520as%250Aknowledge%2520latency%252C%2520shallow%2520reasoning%252C%2520and%2520constrained%2520cognitive%2520processes.%250ADuring%2520this%2520era%252C%2520prompt%2520engineering%2520emerged%2520as%2520our%2520primary%2520interface%2520with%2520AI%252C%250Aenabling%2520dialogue-level%2520communication%2520through%2520natural%2520language.%2520We%2520now%2520witness%250Athe%2520emergence%2520of%2520%2522Act%2520II%2522%2520%25282024-present%2529%252C%2520where%2520models%2520are%2520transitioning%2520from%250Aknowledge-retrieval%2520systems%2520%2528in%2520latent%2520space%2529%2520to%2520thought-construction%2520engines%250Athrough%2520test-time%2520scaling%2520techniques.%2520This%2520new%2520paradigm%2520establishes%2520a%250Amind-level%2520connection%2520with%2520AI%2520through%2520language-based%2520thoughts.%2520In%2520this%2520paper%252C%250Awe%2520clarify%2520the%2520conceptual%2520foundations%2520of%2520cognition%2520engineering%2520and%2520explain%2520why%250Athis%2520moment%2520is%2520critical%2520for%2520its%2520development.%2520We%2520systematically%2520break%2520down%2520these%250Aadvanced%2520approaches%2520through%2520comprehensive%2520tutorials%2520and%2520optimized%250Aimplementations%252C%2520democratizing%2520access%2520to%2520cognition%2520engineering%2520and%2520enabling%250Aevery%2520practitioner%2520to%2520participate%2520in%2520AI%2527s%2520second%2520act.%2520We%2520provide%2520a%2520regularly%250Aupdated%2520collection%2520of%2520papers%2520on%2520test-time%2520scaling%2520in%2520the%2520GitHub%2520Repository%253A%250Ahttps%253A//github.com/GAIR-NLP/cognition-engineering%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13828v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering&entry.906535625=Shijie%20Xia%20and%20Yiwei%20Qin%20and%20Xuefeng%20Li%20and%20Yan%20Ma%20and%20Run-Ze%20Fan%20and%20Steffi%20Chern%20and%20Haoyang%20Zou%20and%20Fan%20Zhou%20and%20Xiangkun%20Hu%20and%20Jiahe%20Jin%20and%20Yanheng%20He%20and%20Yixin%20Ye%20and%20Yixiu%20Liu%20and%20Pengfei%20Liu&entry.1292438233=%20%20The%20first%20generation%20of%20Large%20Language%20Models%20-%20what%20might%20be%20called%20%22Act%20I%22%0Aof%20generative%20AI%20%282020-2023%29%20-%20achieved%20remarkable%20success%20through%20massive%0Aparameter%20and%20data%20scaling%2C%20yet%20exhibited%20fundamental%20limitations%20such%20as%0Aknowledge%20latency%2C%20shallow%20reasoning%2C%20and%20constrained%20cognitive%20processes.%0ADuring%20this%20era%2C%20prompt%20engineering%20emerged%20as%20our%20primary%20interface%20with%20AI%2C%0Aenabling%20dialogue-level%20communication%20through%20natural%20language.%20We%20now%20witness%0Athe%20emergence%20of%20%22Act%20II%22%20%282024-present%29%2C%20where%20models%20are%20transitioning%20from%0Aknowledge-retrieval%20systems%20%28in%20latent%20space%29%20to%20thought-construction%20engines%0Athrough%20test-time%20scaling%20techniques.%20This%20new%20paradigm%20establishes%20a%0Amind-level%20connection%20with%20AI%20through%20language-based%20thoughts.%20In%20this%20paper%2C%0Awe%20clarify%20the%20conceptual%20foundations%20of%20cognition%20engineering%20and%20explain%20why%0Athis%20moment%20is%20critical%20for%20its%20development.%20We%20systematically%20break%20down%20these%0Aadvanced%20approaches%20through%20comprehensive%20tutorials%20and%20optimized%0Aimplementations%2C%20democratizing%20access%20to%20cognition%20engineering%20and%20enabling%0Aevery%20practitioner%20to%20participate%20in%20AI%27s%20second%20act.%20We%20provide%20a%20regularly%0Aupdated%20collection%20of%20papers%20on%20test-time%20scaling%20in%20the%20GitHub%20Repository%3A%0Ahttps%3A//github.com/GAIR-NLP/cognition-engineering%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13828v3&entry.124074799=Read"},
{"title": "Enhancing short-term traffic prediction by integrating trends and\n  fluctuations with attention mechanism", "author": "Adway Das and Agnimitra Sengupta and S. Ilgin Guler", "abstract": "  Traffic flow prediction is a critical component of intelligent transportation\nsystems, yet accurately forecasting traffic remains challenging due to the\ninteraction between long-term trends and short-term fluctuations. Standard deep\nlearning models often struggle with these challenges because their\narchitectures inherently smooth over fine-grained fluctuations while focusing\non general trends. This limitation arises from low-pass filtering effects, gate\nbiases favoring stability, and memory update mechanisms that prioritize\nlong-term information retention. To address these shortcomings, this study\nintroduces a hybrid deep learning framework that integrates both long-term\ntrend and short-term fluctuation information using two input features processed\nin parallel, designed to capture complementary aspects of traffic flow\ndynamics. Further, our approach leverages attention mechanisms, specifically\nBahdanau attention, to selectively focus on critical time steps within traffic\ndata, enhancing the model's ability to predict congestion and other transient\nphenomena. Experimental results demonstrate that features learned from both\nbranches are complementary, significantly improving the goodness-of-fit\nstatistics across multiple prediction horizons compared to a baseline model.\nNotably, the attention mechanism enhances short-term forecast accuracy by\ndirectly targeting immediate fluctuations, though challenges remain in fully\nintegrating long-term trends. This framework can contribute to more effective\ncongestion mitigation and urban mobility planning by advancing the robustness\nand precision of traffic prediction models.\n", "link": "http://arxiv.org/abs/2504.19967v1", "date": "2025-04-28", "relevancy": 2.0806, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5423}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5172}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20short-term%20traffic%20prediction%20by%20integrating%20trends%20and%0A%20%20fluctuations%20with%20attention%20mechanism&body=Title%3A%20Enhancing%20short-term%20traffic%20prediction%20by%20integrating%20trends%20and%0A%20%20fluctuations%20with%20attention%20mechanism%0AAuthor%3A%20Adway%20Das%20and%20Agnimitra%20Sengupta%20and%20S.%20Ilgin%20Guler%0AAbstract%3A%20%20%20Traffic%20flow%20prediction%20is%20a%20critical%20component%20of%20intelligent%20transportation%0Asystems%2C%20yet%20accurately%20forecasting%20traffic%20remains%20challenging%20due%20to%20the%0Ainteraction%20between%20long-term%20trends%20and%20short-term%20fluctuations.%20Standard%20deep%0Alearning%20models%20often%20struggle%20with%20these%20challenges%20because%20their%0Aarchitectures%20inherently%20smooth%20over%20fine-grained%20fluctuations%20while%20focusing%0Aon%20general%20trends.%20This%20limitation%20arises%20from%20low-pass%20filtering%20effects%2C%20gate%0Abiases%20favoring%20stability%2C%20and%20memory%20update%20mechanisms%20that%20prioritize%0Along-term%20information%20retention.%20To%20address%20these%20shortcomings%2C%20this%20study%0Aintroduces%20a%20hybrid%20deep%20learning%20framework%20that%20integrates%20both%20long-term%0Atrend%20and%20short-term%20fluctuation%20information%20using%20two%20input%20features%20processed%0Ain%20parallel%2C%20designed%20to%20capture%20complementary%20aspects%20of%20traffic%20flow%0Adynamics.%20Further%2C%20our%20approach%20leverages%20attention%20mechanisms%2C%20specifically%0ABahdanau%20attention%2C%20to%20selectively%20focus%20on%20critical%20time%20steps%20within%20traffic%0Adata%2C%20enhancing%20the%20model%27s%20ability%20to%20predict%20congestion%20and%20other%20transient%0Aphenomena.%20Experimental%20results%20demonstrate%20that%20features%20learned%20from%20both%0Abranches%20are%20complementary%2C%20significantly%20improving%20the%20goodness-of-fit%0Astatistics%20across%20multiple%20prediction%20horizons%20compared%20to%20a%20baseline%20model.%0ANotably%2C%20the%20attention%20mechanism%20enhances%20short-term%20forecast%20accuracy%20by%0Adirectly%20targeting%20immediate%20fluctuations%2C%20though%20challenges%20remain%20in%20fully%0Aintegrating%20long-term%20trends.%20This%20framework%20can%20contribute%20to%20more%20effective%0Acongestion%20mitigation%20and%20urban%20mobility%20planning%20by%20advancing%20the%20robustness%0Aand%20precision%20of%20traffic%20prediction%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520short-term%2520traffic%2520prediction%2520by%2520integrating%2520trends%2520and%250A%2520%2520fluctuations%2520with%2520attention%2520mechanism%26entry.906535625%3DAdway%2520Das%2520and%2520Agnimitra%2520Sengupta%2520and%2520S.%2520Ilgin%2520Guler%26entry.1292438233%3D%2520%2520Traffic%2520flow%2520prediction%2520is%2520a%2520critical%2520component%2520of%2520intelligent%2520transportation%250Asystems%252C%2520yet%2520accurately%2520forecasting%2520traffic%2520remains%2520challenging%2520due%2520to%2520the%250Ainteraction%2520between%2520long-term%2520trends%2520and%2520short-term%2520fluctuations.%2520Standard%2520deep%250Alearning%2520models%2520often%2520struggle%2520with%2520these%2520challenges%2520because%2520their%250Aarchitectures%2520inherently%2520smooth%2520over%2520fine-grained%2520fluctuations%2520while%2520focusing%250Aon%2520general%2520trends.%2520This%2520limitation%2520arises%2520from%2520low-pass%2520filtering%2520effects%252C%2520gate%250Abiases%2520favoring%2520stability%252C%2520and%2520memory%2520update%2520mechanisms%2520that%2520prioritize%250Along-term%2520information%2520retention.%2520To%2520address%2520these%2520shortcomings%252C%2520this%2520study%250Aintroduces%2520a%2520hybrid%2520deep%2520learning%2520framework%2520that%2520integrates%2520both%2520long-term%250Atrend%2520and%2520short-term%2520fluctuation%2520information%2520using%2520two%2520input%2520features%2520processed%250Ain%2520parallel%252C%2520designed%2520to%2520capture%2520complementary%2520aspects%2520of%2520traffic%2520flow%250Adynamics.%2520Further%252C%2520our%2520approach%2520leverages%2520attention%2520mechanisms%252C%2520specifically%250ABahdanau%2520attention%252C%2520to%2520selectively%2520focus%2520on%2520critical%2520time%2520steps%2520within%2520traffic%250Adata%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520predict%2520congestion%2520and%2520other%2520transient%250Aphenomena.%2520Experimental%2520results%2520demonstrate%2520that%2520features%2520learned%2520from%2520both%250Abranches%2520are%2520complementary%252C%2520significantly%2520improving%2520the%2520goodness-of-fit%250Astatistics%2520across%2520multiple%2520prediction%2520horizons%2520compared%2520to%2520a%2520baseline%2520model.%250ANotably%252C%2520the%2520attention%2520mechanism%2520enhances%2520short-term%2520forecast%2520accuracy%2520by%250Adirectly%2520targeting%2520immediate%2520fluctuations%252C%2520though%2520challenges%2520remain%2520in%2520fully%250Aintegrating%2520long-term%2520trends.%2520This%2520framework%2520can%2520contribute%2520to%2520more%2520effective%250Acongestion%2520mitigation%2520and%2520urban%2520mobility%2520planning%2520by%2520advancing%2520the%2520robustness%250Aand%2520precision%2520of%2520traffic%2520prediction%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20short-term%20traffic%20prediction%20by%20integrating%20trends%20and%0A%20%20fluctuations%20with%20attention%20mechanism&entry.906535625=Adway%20Das%20and%20Agnimitra%20Sengupta%20and%20S.%20Ilgin%20Guler&entry.1292438233=%20%20Traffic%20flow%20prediction%20is%20a%20critical%20component%20of%20intelligent%20transportation%0Asystems%2C%20yet%20accurately%20forecasting%20traffic%20remains%20challenging%20due%20to%20the%0Ainteraction%20between%20long-term%20trends%20and%20short-term%20fluctuations.%20Standard%20deep%0Alearning%20models%20often%20struggle%20with%20these%20challenges%20because%20their%0Aarchitectures%20inherently%20smooth%20over%20fine-grained%20fluctuations%20while%20focusing%0Aon%20general%20trends.%20This%20limitation%20arises%20from%20low-pass%20filtering%20effects%2C%20gate%0Abiases%20favoring%20stability%2C%20and%20memory%20update%20mechanisms%20that%20prioritize%0Along-term%20information%20retention.%20To%20address%20these%20shortcomings%2C%20this%20study%0Aintroduces%20a%20hybrid%20deep%20learning%20framework%20that%20integrates%20both%20long-term%0Atrend%20and%20short-term%20fluctuation%20information%20using%20two%20input%20features%20processed%0Ain%20parallel%2C%20designed%20to%20capture%20complementary%20aspects%20of%20traffic%20flow%0Adynamics.%20Further%2C%20our%20approach%20leverages%20attention%20mechanisms%2C%20specifically%0ABahdanau%20attention%2C%20to%20selectively%20focus%20on%20critical%20time%20steps%20within%20traffic%0Adata%2C%20enhancing%20the%20model%27s%20ability%20to%20predict%20congestion%20and%20other%20transient%0Aphenomena.%20Experimental%20results%20demonstrate%20that%20features%20learned%20from%20both%0Abranches%20are%20complementary%2C%20significantly%20improving%20the%20goodness-of-fit%0Astatistics%20across%20multiple%20prediction%20horizons%20compared%20to%20a%20baseline%20model.%0ANotably%2C%20the%20attention%20mechanism%20enhances%20short-term%20forecast%20accuracy%20by%0Adirectly%20targeting%20immediate%20fluctuations%2C%20though%20challenges%20remain%20in%20fully%0Aintegrating%20long-term%20trends.%20This%20framework%20can%20contribute%20to%20more%20effective%0Acongestion%20mitigation%20and%20urban%20mobility%20planning%20by%20advancing%20the%20robustness%0Aand%20precision%20of%20traffic%20prediction%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19967v1&entry.124074799=Read"},
{"title": "Learning Brenier Potentials with Convex Generative Adversarial Neural\n  Networks", "author": "Claudia Drygala and Hanno Gottschalk and Thomas Kruse and S\u00e9gol\u00e8ne Martin and Annika M\u00fctze", "abstract": "  Brenier proved that under certain conditions on a source and a target\nprobability measure there exists a strictly convex function such that its\ngradient is a transport map from the source to the target distribution. This\nfunction is called the Brenier potential. Furthermore, detailed information on\nthe H\\\"older regularity of the Brenier potential is available. In this work we\ndevelop the statistical learning theory of generative adversarial neural\nnetworks that learn the Brenier potential. As by the transformation of\ndensities formula, the density of the generated measure depends on the second\nderivative of the Brenier potential, we develop the universal approximation\ntheory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$\nthat combines the favorable approximation properties of H\\\"older functions with\na Lipschitz continuous density. In order to assure the convexity of such\ngeneral networks, we introduce an adversarial training procedure for a\npotential function represented by the ReCU networks that combines the classical\ndiscriminator cross entropy loss with a penalty term that enforces (strict)\nconvexity. We give a detailed decomposition of learning errors and show that\nfor a suitable high penalty parameter all networks chosen in the adversarial\nmin-max optimization problem are strictly convex. This is further exploited to\nprove the consistency of the learning procedure for (slowly) expanding network\ncapacity. We also implement the described learning algorithm and apply it to a\nnumber of standard test cases from Gaussian mixture to image data as target\ndistributions. As predicted in theory, we observe that the convexity loss\nbecomes inactive during the training process and the potentials represented by\nthe neural networks have learned convexity.\n", "link": "http://arxiv.org/abs/2504.19779v1", "date": "2025-04-28", "relevancy": 2.0796, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5287}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5186}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Brenier%20Potentials%20with%20Convex%20Generative%20Adversarial%20Neural%0A%20%20Networks&body=Title%3A%20Learning%20Brenier%20Potentials%20with%20Convex%20Generative%20Adversarial%20Neural%0A%20%20Networks%0AAuthor%3A%20Claudia%20Drygala%20and%20Hanno%20Gottschalk%20and%20Thomas%20Kruse%20and%20S%C3%A9gol%C3%A8ne%20Martin%20and%20Annika%20M%C3%BCtze%0AAbstract%3A%20%20%20Brenier%20proved%20that%20under%20certain%20conditions%20on%20a%20source%20and%20a%20target%0Aprobability%20measure%20there%20exists%20a%20strictly%20convex%20function%20such%20that%20its%0Agradient%20is%20a%20transport%20map%20from%20the%20source%20to%20the%20target%20distribution.%20This%0Afunction%20is%20called%20the%20Brenier%20potential.%20Furthermore%2C%20detailed%20information%20on%0Athe%20H%5C%22older%20regularity%20of%20the%20Brenier%20potential%20is%20available.%20In%20this%20work%20we%0Adevelop%20the%20statistical%20learning%20theory%20of%20generative%20adversarial%20neural%0Anetworks%20that%20learn%20the%20Brenier%20potential.%20As%20by%20the%20transformation%20of%0Adensities%20formula%2C%20the%20density%20of%20the%20generated%20measure%20depends%20on%20the%20second%0Aderivative%20of%20the%20Brenier%20potential%2C%20we%20develop%20the%20universal%20approximation%0Atheory%20of%20ReCU%20networks%20with%20cubic%20activation%20%24%5Cmathtt%7BReCU%7D%28x%29%3D%5Cmax%5C%7B0%2Cx%5C%7D%5E3%24%0Athat%20combines%20the%20favorable%20approximation%20properties%20of%20H%5C%22older%20functions%20with%0Aa%20Lipschitz%20continuous%20density.%20In%20order%20to%20assure%20the%20convexity%20of%20such%0Ageneral%20networks%2C%20we%20introduce%20an%20adversarial%20training%20procedure%20for%20a%0Apotential%20function%20represented%20by%20the%20ReCU%20networks%20that%20combines%20the%20classical%0Adiscriminator%20cross%20entropy%20loss%20with%20a%20penalty%20term%20that%20enforces%20%28strict%29%0Aconvexity.%20We%20give%20a%20detailed%20decomposition%20of%20learning%20errors%20and%20show%20that%0Afor%20a%20suitable%20high%20penalty%20parameter%20all%20networks%20chosen%20in%20the%20adversarial%0Amin-max%20optimization%20problem%20are%20strictly%20convex.%20This%20is%20further%20exploited%20to%0Aprove%20the%20consistency%20of%20the%20learning%20procedure%20for%20%28slowly%29%20expanding%20network%0Acapacity.%20We%20also%20implement%20the%20described%20learning%20algorithm%20and%20apply%20it%20to%20a%0Anumber%20of%20standard%20test%20cases%20from%20Gaussian%20mixture%20to%20image%20data%20as%20target%0Adistributions.%20As%20predicted%20in%20theory%2C%20we%20observe%20that%20the%20convexity%20loss%0Abecomes%20inactive%20during%20the%20training%20process%20and%20the%20potentials%20represented%20by%0Athe%20neural%20networks%20have%20learned%20convexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Brenier%2520Potentials%2520with%2520Convex%2520Generative%2520Adversarial%2520Neural%250A%2520%2520Networks%26entry.906535625%3DClaudia%2520Drygala%2520and%2520Hanno%2520Gottschalk%2520and%2520Thomas%2520Kruse%2520and%2520S%25C3%25A9gol%25C3%25A8ne%2520Martin%2520and%2520Annika%2520M%25C3%25BCtze%26entry.1292438233%3D%2520%2520Brenier%2520proved%2520that%2520under%2520certain%2520conditions%2520on%2520a%2520source%2520and%2520a%2520target%250Aprobability%2520measure%2520there%2520exists%2520a%2520strictly%2520convex%2520function%2520such%2520that%2520its%250Agradient%2520is%2520a%2520transport%2520map%2520from%2520the%2520source%2520to%2520the%2520target%2520distribution.%2520This%250Afunction%2520is%2520called%2520the%2520Brenier%2520potential.%2520Furthermore%252C%2520detailed%2520information%2520on%250Athe%2520H%255C%2522older%2520regularity%2520of%2520the%2520Brenier%2520potential%2520is%2520available.%2520In%2520this%2520work%2520we%250Adevelop%2520the%2520statistical%2520learning%2520theory%2520of%2520generative%2520adversarial%2520neural%250Anetworks%2520that%2520learn%2520the%2520Brenier%2520potential.%2520As%2520by%2520the%2520transformation%2520of%250Adensities%2520formula%252C%2520the%2520density%2520of%2520the%2520generated%2520measure%2520depends%2520on%2520the%2520second%250Aderivative%2520of%2520the%2520Brenier%2520potential%252C%2520we%2520develop%2520the%2520universal%2520approximation%250Atheory%2520of%2520ReCU%2520networks%2520with%2520cubic%2520activation%2520%2524%255Cmathtt%257BReCU%257D%2528x%2529%253D%255Cmax%255C%257B0%252Cx%255C%257D%255E3%2524%250Athat%2520combines%2520the%2520favorable%2520approximation%2520properties%2520of%2520H%255C%2522older%2520functions%2520with%250Aa%2520Lipschitz%2520continuous%2520density.%2520In%2520order%2520to%2520assure%2520the%2520convexity%2520of%2520such%250Ageneral%2520networks%252C%2520we%2520introduce%2520an%2520adversarial%2520training%2520procedure%2520for%2520a%250Apotential%2520function%2520represented%2520by%2520the%2520ReCU%2520networks%2520that%2520combines%2520the%2520classical%250Adiscriminator%2520cross%2520entropy%2520loss%2520with%2520a%2520penalty%2520term%2520that%2520enforces%2520%2528strict%2529%250Aconvexity.%2520We%2520give%2520a%2520detailed%2520decomposition%2520of%2520learning%2520errors%2520and%2520show%2520that%250Afor%2520a%2520suitable%2520high%2520penalty%2520parameter%2520all%2520networks%2520chosen%2520in%2520the%2520adversarial%250Amin-max%2520optimization%2520problem%2520are%2520strictly%2520convex.%2520This%2520is%2520further%2520exploited%2520to%250Aprove%2520the%2520consistency%2520of%2520the%2520learning%2520procedure%2520for%2520%2528slowly%2529%2520expanding%2520network%250Acapacity.%2520We%2520also%2520implement%2520the%2520described%2520learning%2520algorithm%2520and%2520apply%2520it%2520to%2520a%250Anumber%2520of%2520standard%2520test%2520cases%2520from%2520Gaussian%2520mixture%2520to%2520image%2520data%2520as%2520target%250Adistributions.%2520As%2520predicted%2520in%2520theory%252C%2520we%2520observe%2520that%2520the%2520convexity%2520loss%250Abecomes%2520inactive%2520during%2520the%2520training%2520process%2520and%2520the%2520potentials%2520represented%2520by%250Athe%2520neural%2520networks%2520have%2520learned%2520convexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Brenier%20Potentials%20with%20Convex%20Generative%20Adversarial%20Neural%0A%20%20Networks&entry.906535625=Claudia%20Drygala%20and%20Hanno%20Gottschalk%20and%20Thomas%20Kruse%20and%20S%C3%A9gol%C3%A8ne%20Martin%20and%20Annika%20M%C3%BCtze&entry.1292438233=%20%20Brenier%20proved%20that%20under%20certain%20conditions%20on%20a%20source%20and%20a%20target%0Aprobability%20measure%20there%20exists%20a%20strictly%20convex%20function%20such%20that%20its%0Agradient%20is%20a%20transport%20map%20from%20the%20source%20to%20the%20target%20distribution.%20This%0Afunction%20is%20called%20the%20Brenier%20potential.%20Furthermore%2C%20detailed%20information%20on%0Athe%20H%5C%22older%20regularity%20of%20the%20Brenier%20potential%20is%20available.%20In%20this%20work%20we%0Adevelop%20the%20statistical%20learning%20theory%20of%20generative%20adversarial%20neural%0Anetworks%20that%20learn%20the%20Brenier%20potential.%20As%20by%20the%20transformation%20of%0Adensities%20formula%2C%20the%20density%20of%20the%20generated%20measure%20depends%20on%20the%20second%0Aderivative%20of%20the%20Brenier%20potential%2C%20we%20develop%20the%20universal%20approximation%0Atheory%20of%20ReCU%20networks%20with%20cubic%20activation%20%24%5Cmathtt%7BReCU%7D%28x%29%3D%5Cmax%5C%7B0%2Cx%5C%7D%5E3%24%0Athat%20combines%20the%20favorable%20approximation%20properties%20of%20H%5C%22older%20functions%20with%0Aa%20Lipschitz%20continuous%20density.%20In%20order%20to%20assure%20the%20convexity%20of%20such%0Ageneral%20networks%2C%20we%20introduce%20an%20adversarial%20training%20procedure%20for%20a%0Apotential%20function%20represented%20by%20the%20ReCU%20networks%20that%20combines%20the%20classical%0Adiscriminator%20cross%20entropy%20loss%20with%20a%20penalty%20term%20that%20enforces%20%28strict%29%0Aconvexity.%20We%20give%20a%20detailed%20decomposition%20of%20learning%20errors%20and%20show%20that%0Afor%20a%20suitable%20high%20penalty%20parameter%20all%20networks%20chosen%20in%20the%20adversarial%0Amin-max%20optimization%20problem%20are%20strictly%20convex.%20This%20is%20further%20exploited%20to%0Aprove%20the%20consistency%20of%20the%20learning%20procedure%20for%20%28slowly%29%20expanding%20network%0Acapacity.%20We%20also%20implement%20the%20described%20learning%20algorithm%20and%20apply%20it%20to%20a%0Anumber%20of%20standard%20test%20cases%20from%20Gaussian%20mixture%20to%20image%20data%20as%20target%0Adistributions.%20As%20predicted%20in%20theory%2C%20we%20observe%20that%20the%20convexity%20loss%0Abecomes%20inactive%20during%20the%20training%20process%20and%20the%20potentials%20represented%20by%0Athe%20neural%20networks%20have%20learned%20convexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19779v1&entry.124074799=Read"},
{"title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "author": "Ranran Zhen and Juntao Li and Yixin Ji and Zhenlin Yang and Tong Liu and Qingrong Xia and Xinyu Duan and Zhefeng Wang and Baoxing Huai and Min Zhang", "abstract": "  Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.\n", "link": "http://arxiv.org/abs/2504.19720v1", "date": "2025-04-28", "relevancy": 2.0788, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20the%20Titans%3A%20A%20Survey%20of%20Efficient%20LLM%20Inference%20Serving&body=Title%3A%20Taming%20the%20Titans%3A%20A%20Survey%20of%20Efficient%20LLM%20Inference%20Serving%0AAuthor%3A%20Ranran%20Zhen%20and%20Juntao%20Li%20and%20Yixin%20Ji%20and%20Zhenlin%20Yang%20and%20Tong%20Liu%20and%20Qingrong%20Xia%20and%20Xinyu%20Duan%20and%20Zhefeng%20Wang%20and%20Baoxing%20Huai%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20for%20Generative%20AI%20have%20achieved%20remarkable%0Aprogress%2C%20evolving%20into%20sophisticated%20and%20versatile%20tools%20widely%20adopted%20across%0Avarious%20domains%20and%20applications.%20However%2C%20the%20substantial%20memory%20overhead%0Acaused%20by%20their%20vast%20number%20of%20parameters%2C%20combined%20with%20the%20high%20computational%0Ademands%20of%20the%20attention%20mechanism%2C%20poses%20significant%20challenges%20in%20achieving%0Alow%20latency%20and%20high%20throughput%20for%20LLM%20inference%20services.%20Recent%0Aadvancements%2C%20driven%20by%20groundbreaking%20research%2C%20have%20significantly%20accelerated%0Aprogress%20in%20this%20field.%20This%20paper%20provides%20a%20comprehensive%20survey%20of%20these%0Amethods%2C%20covering%20fundamental%20instance-level%20approaches%2C%20in-depth%20cluster-level%0Astrategies%2C%20emerging%20scenario%20directions%2C%20and%20other%20miscellaneous%20but%20important%0Aareas.%20At%20the%20instance%20level%2C%20we%20review%20model%20placement%2C%20request%20scheduling%2C%0Adecoding%20length%20prediction%2C%20storage%20management%2C%20and%20the%20disaggregation%0Aparadigm.%20At%20the%20cluster%20level%2C%20we%20explore%20GPU%20cluster%20deployment%2C%0Amulti-instance%20load%20balancing%2C%20and%20cloud%20service%20solutions.%20For%20emerging%0Ascenarios%2C%20we%20organize%20the%20discussion%20around%20specific%20tasks%2C%20modules%2C%20and%0Aauxiliary%20methods.%20To%20ensure%20a%20holistic%20overview%2C%20we%20also%20highlight%20several%0Aniche%20yet%20critical%20areas.%20Finally%2C%20we%20outline%20potential%20research%20directions%20to%0Afurther%20advance%20the%20field%20of%20LLM%20inference%20serving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520the%2520Titans%253A%2520A%2520Survey%2520of%2520Efficient%2520LLM%2520Inference%2520Serving%26entry.906535625%3DRanran%2520Zhen%2520and%2520Juntao%2520Li%2520and%2520Yixin%2520Ji%2520and%2520Zhenlin%2520Yang%2520and%2520Tong%2520Liu%2520and%2520Qingrong%2520Xia%2520and%2520Xinyu%2520Duan%2520and%2520Zhefeng%2520Wang%2520and%2520Baoxing%2520Huai%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520Generative%2520AI%2520have%2520achieved%2520remarkable%250Aprogress%252C%2520evolving%2520into%2520sophisticated%2520and%2520versatile%2520tools%2520widely%2520adopted%2520across%250Avarious%2520domains%2520and%2520applications.%2520However%252C%2520the%2520substantial%2520memory%2520overhead%250Acaused%2520by%2520their%2520vast%2520number%2520of%2520parameters%252C%2520combined%2520with%2520the%2520high%2520computational%250Ademands%2520of%2520the%2520attention%2520mechanism%252C%2520poses%2520significant%2520challenges%2520in%2520achieving%250Alow%2520latency%2520and%2520high%2520throughput%2520for%2520LLM%2520inference%2520services.%2520Recent%250Aadvancements%252C%2520driven%2520by%2520groundbreaking%2520research%252C%2520have%2520significantly%2520accelerated%250Aprogress%2520in%2520this%2520field.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520survey%2520of%2520these%250Amethods%252C%2520covering%2520fundamental%2520instance-level%2520approaches%252C%2520in-depth%2520cluster-level%250Astrategies%252C%2520emerging%2520scenario%2520directions%252C%2520and%2520other%2520miscellaneous%2520but%2520important%250Aareas.%2520At%2520the%2520instance%2520level%252C%2520we%2520review%2520model%2520placement%252C%2520request%2520scheduling%252C%250Adecoding%2520length%2520prediction%252C%2520storage%2520management%252C%2520and%2520the%2520disaggregation%250Aparadigm.%2520At%2520the%2520cluster%2520level%252C%2520we%2520explore%2520GPU%2520cluster%2520deployment%252C%250Amulti-instance%2520load%2520balancing%252C%2520and%2520cloud%2520service%2520solutions.%2520For%2520emerging%250Ascenarios%252C%2520we%2520organize%2520the%2520discussion%2520around%2520specific%2520tasks%252C%2520modules%252C%2520and%250Aauxiliary%2520methods.%2520To%2520ensure%2520a%2520holistic%2520overview%252C%2520we%2520also%2520highlight%2520several%250Aniche%2520yet%2520critical%2520areas.%2520Finally%252C%2520we%2520outline%2520potential%2520research%2520directions%2520to%250Afurther%2520advance%2520the%2520field%2520of%2520LLM%2520inference%2520serving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20the%20Titans%3A%20A%20Survey%20of%20Efficient%20LLM%20Inference%20Serving&entry.906535625=Ranran%20Zhen%20and%20Juntao%20Li%20and%20Yixin%20Ji%20and%20Zhenlin%20Yang%20and%20Tong%20Liu%20and%20Qingrong%20Xia%20and%20Xinyu%20Duan%20and%20Zhefeng%20Wang%20and%20Baoxing%20Huai%20and%20Min%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20for%20Generative%20AI%20have%20achieved%20remarkable%0Aprogress%2C%20evolving%20into%20sophisticated%20and%20versatile%20tools%20widely%20adopted%20across%0Avarious%20domains%20and%20applications.%20However%2C%20the%20substantial%20memory%20overhead%0Acaused%20by%20their%20vast%20number%20of%20parameters%2C%20combined%20with%20the%20high%20computational%0Ademands%20of%20the%20attention%20mechanism%2C%20poses%20significant%20challenges%20in%20achieving%0Alow%20latency%20and%20high%20throughput%20for%20LLM%20inference%20services.%20Recent%0Aadvancements%2C%20driven%20by%20groundbreaking%20research%2C%20have%20significantly%20accelerated%0Aprogress%20in%20this%20field.%20This%20paper%20provides%20a%20comprehensive%20survey%20of%20these%0Amethods%2C%20covering%20fundamental%20instance-level%20approaches%2C%20in-depth%20cluster-level%0Astrategies%2C%20emerging%20scenario%20directions%2C%20and%20other%20miscellaneous%20but%20important%0Aareas.%20At%20the%20instance%20level%2C%20we%20review%20model%20placement%2C%20request%20scheduling%2C%0Adecoding%20length%20prediction%2C%20storage%20management%2C%20and%20the%20disaggregation%0Aparadigm.%20At%20the%20cluster%20level%2C%20we%20explore%20GPU%20cluster%20deployment%2C%0Amulti-instance%20load%20balancing%2C%20and%20cloud%20service%20solutions.%20For%20emerging%0Ascenarios%2C%20we%20organize%20the%20discussion%20around%20specific%20tasks%2C%20modules%2C%20and%0Aauxiliary%20methods.%20To%20ensure%20a%20holistic%20overview%2C%20we%20also%20highlight%20several%0Aniche%20yet%20critical%20areas.%20Finally%2C%20we%20outline%20potential%20research%20directions%20to%0Afurther%20advance%20the%20field%20of%20LLM%20inference%20serving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19720v1&entry.124074799=Read"},
{"title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation", "author": "Carlo Merola and Jaspinder Singh", "abstract": "  Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness.\n", "link": "http://arxiv.org/abs/2504.19754v1", "date": "2025-04-28", "relevancy": 2.0727, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Context%3A%20Evaluating%20Advanced%20Chunking%20Strategies%20for%0A%20%20Retrieval-Augmented%20Generation&body=Title%3A%20Reconstructing%20Context%3A%20Evaluating%20Advanced%20Chunking%20Strategies%20for%0A%20%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Carlo%20Merola%20and%20Jaspinder%20Singh%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20become%20a%20transformative%20approach%20for%0Aenhancing%20large%20language%20models%20%28LLMs%29%20by%20grounding%20their%20outputs%20in%20external%0Aknowledge%20sources.%20Yet%2C%20a%20critical%20question%20persists%3A%20how%20can%20vast%20volumes%20of%0Aexternal%20knowledge%20be%20managed%20effectively%20within%20the%20input%20constraints%20of%20LLMs%3F%0ATraditional%20methods%20address%20this%20by%20chunking%20external%20documents%20into%20smaller%2C%0Afixed-size%20segments.%20While%20this%20approach%20alleviates%20input%20limitations%2C%20it%20often%0Afragments%20context%2C%20resulting%20in%20incomplete%20retrieval%20and%20diminished%20coherence%0Ain%20generation.%20To%20overcome%20these%20shortcomings%2C%20two%20advanced%20techniques%2C%20late%0Achunking%20and%20contextual%20retrieval%2C%20have%20been%20introduced%2C%20both%20aiming%20to%0Apreserve%20global%20context.%20Despite%20their%20potential%2C%20their%20comparative%20strengths%0Aand%20limitations%20remain%20unclear.%20This%20study%20presents%20a%20rigorous%20analysis%20of%20late%0Achunking%20and%20contextual%20retrieval%2C%20evaluating%20their%20effectiveness%20and%0Aefficiency%20in%20optimizing%20RAG%20systems.%20Our%20results%20indicate%20that%20contextual%0Aretrieval%20preserves%20semantic%20coherence%20more%20effectively%20but%20requires%20greater%0Acomputational%20resources.%20In%20contrast%2C%20late%20chunking%20offers%20higher%20efficiency%0Abut%20tends%20to%20sacrifice%20relevance%20and%20completeness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520Context%253A%2520Evaluating%2520Advanced%2520Chunking%2520Strategies%2520for%250A%2520%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DCarlo%2520Merola%2520and%2520Jaspinder%2520Singh%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520become%2520a%2520transformative%2520approach%2520for%250Aenhancing%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520grounding%2520their%2520outputs%2520in%2520external%250Aknowledge%2520sources.%2520Yet%252C%2520a%2520critical%2520question%2520persists%253A%2520how%2520can%2520vast%2520volumes%2520of%250Aexternal%2520knowledge%2520be%2520managed%2520effectively%2520within%2520the%2520input%2520constraints%2520of%2520LLMs%253F%250ATraditional%2520methods%2520address%2520this%2520by%2520chunking%2520external%2520documents%2520into%2520smaller%252C%250Afixed-size%2520segments.%2520While%2520this%2520approach%2520alleviates%2520input%2520limitations%252C%2520it%2520often%250Afragments%2520context%252C%2520resulting%2520in%2520incomplete%2520retrieval%2520and%2520diminished%2520coherence%250Ain%2520generation.%2520To%2520overcome%2520these%2520shortcomings%252C%2520two%2520advanced%2520techniques%252C%2520late%250Achunking%2520and%2520contextual%2520retrieval%252C%2520have%2520been%2520introduced%252C%2520both%2520aiming%2520to%250Apreserve%2520global%2520context.%2520Despite%2520their%2520potential%252C%2520their%2520comparative%2520strengths%250Aand%2520limitations%2520remain%2520unclear.%2520This%2520study%2520presents%2520a%2520rigorous%2520analysis%2520of%2520late%250Achunking%2520and%2520contextual%2520retrieval%252C%2520evaluating%2520their%2520effectiveness%2520and%250Aefficiency%2520in%2520optimizing%2520RAG%2520systems.%2520Our%2520results%2520indicate%2520that%2520contextual%250Aretrieval%2520preserves%2520semantic%2520coherence%2520more%2520effectively%2520but%2520requires%2520greater%250Acomputational%2520resources.%2520In%2520contrast%252C%2520late%2520chunking%2520offers%2520higher%2520efficiency%250Abut%2520tends%2520to%2520sacrifice%2520relevance%2520and%2520completeness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Context%3A%20Evaluating%20Advanced%20Chunking%20Strategies%20for%0A%20%20Retrieval-Augmented%20Generation&entry.906535625=Carlo%20Merola%20and%20Jaspinder%20Singh&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20become%20a%20transformative%20approach%20for%0Aenhancing%20large%20language%20models%20%28LLMs%29%20by%20grounding%20their%20outputs%20in%20external%0Aknowledge%20sources.%20Yet%2C%20a%20critical%20question%20persists%3A%20how%20can%20vast%20volumes%20of%0Aexternal%20knowledge%20be%20managed%20effectively%20within%20the%20input%20constraints%20of%20LLMs%3F%0ATraditional%20methods%20address%20this%20by%20chunking%20external%20documents%20into%20smaller%2C%0Afixed-size%20segments.%20While%20this%20approach%20alleviates%20input%20limitations%2C%20it%20often%0Afragments%20context%2C%20resulting%20in%20incomplete%20retrieval%20and%20diminished%20coherence%0Ain%20generation.%20To%20overcome%20these%20shortcomings%2C%20two%20advanced%20techniques%2C%20late%0Achunking%20and%20contextual%20retrieval%2C%20have%20been%20introduced%2C%20both%20aiming%20to%0Apreserve%20global%20context.%20Despite%20their%20potential%2C%20their%20comparative%20strengths%0Aand%20limitations%20remain%20unclear.%20This%20study%20presents%20a%20rigorous%20analysis%20of%20late%0Achunking%20and%20contextual%20retrieval%2C%20evaluating%20their%20effectiveness%20and%0Aefficiency%20in%20optimizing%20RAG%20systems.%20Our%20results%20indicate%20that%20contextual%0Aretrieval%20preserves%20semantic%20coherence%20more%20effectively%20but%20requires%20greater%0Acomputational%20resources.%20In%20contrast%2C%20late%20chunking%20offers%20higher%20efficiency%0Abut%20tends%20to%20sacrifice%20relevance%20and%20completeness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19754v1&entry.124074799=Read"},
{"title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish\n  Language: A Research Guide for Kurdish Natural Language Processing Tasks", "author": "Shadan Shukr Sabr and Nazira Sabr Mustafa and Talar Sabah Omar and Salah Hwayyiz Rasool and Nawzad Anwer Omer and Darya Sabir Hamad and Hemin Abdulhameed Shams and Omer Mahmood Kareem and Rozhan Noori Abdullah and Khabat Atar Abdullah and Mahabad Azad Mohammad and Haneen Al-Raghefy and Safar M. Asaad and Sara Jamal Mohammed and Twana Saeed Ali and Fazil Shawrow and Halgurd S. Maghdid", "abstract": "  - The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks.\n", "link": "http://arxiv.org/abs/2504.19645v1", "date": "2025-04-28", "relevancy": 2.0546, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Part-of-Speech%20Tagging%20to%20Standardize%20Central-Kurdish%0A%20%20Language%3A%20A%20Research%20Guide%20for%20Kurdish%20Natural%20Language%20Processing%20Tasks&body=Title%3A%20A%20Comprehensive%20Part-of-Speech%20Tagging%20to%20Standardize%20Central-Kurdish%0A%20%20Language%3A%20A%20Research%20Guide%20for%20Kurdish%20Natural%20Language%20Processing%20Tasks%0AAuthor%3A%20Shadan%20Shukr%20Sabr%20and%20Nazira%20Sabr%20Mustafa%20and%20Talar%20Sabah%20Omar%20and%20Salah%20Hwayyiz%20Rasool%20and%20Nawzad%20Anwer%20Omer%20and%20Darya%20Sabir%20Hamad%20and%20Hemin%20Abdulhameed%20Shams%20and%20Omer%20Mahmood%20Kareem%20and%20Rozhan%20Noori%20Abdullah%20and%20Khabat%20Atar%20Abdullah%20and%20Mahabad%20Azad%20Mohammad%20and%20Haneen%20Al-Raghefy%20and%20Safar%20M.%20Asaad%20and%20Sara%20Jamal%20Mohammed%20and%20Twana%20Saeed%20Ali%20and%20Fazil%20Shawrow%20and%20Halgurd%20S.%20Maghdid%0AAbstract%3A%20%20%20-%20The%20field%20of%20natural%20language%20processing%20%28NLP%29%20has%20dramatically%20expanded%0Awithin%20the%20last%20decade.%20Many%20human-being%20applications%20are%20conducted%20daily%20via%0ANLP%20tasks%2C%20starting%20from%20machine%20translation%2C%20speech%20recognition%2C%20text%0Ageneration%20and%20recommendations%2C%20Part-of-Speech%20tagging%20%28POS%29%2C%20and%20Named-Entity%0ARecognition%20%28NER%29.%20However%2C%20low-resourced%20languages%2C%20such%20as%20the%0ACentral-Kurdish%20language%20%28CKL%29%2C%20mainly%20remain%20unexamined%20due%20to%20shortage%20of%0Anecessary%20resources%20to%20support%20their%20development.%20The%20POS%20tagging%20task%20is%20the%0Abase%20of%20other%20NLP%20tasks%3B%20for%20example%2C%20the%20POS%20tag%20set%20has%20been%20used%20to%0Astandardized%20languages%20to%20provide%20the%20relationship%20between%20words%20among%20the%0Asentences%2C%20followed%20by%20machine%20translation%20and%20text%20recommendation.%0ASpecifically%2C%20for%20the%20CKL%2C%20most%20of%20the%20utilized%20or%20provided%20POS%20tagsets%20are%0Aneither%20standardized%20nor%20comprehensive.%20To%20this%20end%2C%20this%20study%20presented%20an%0Aaccurate%20and%20comprehensive%20POS%20tagset%20for%20the%20CKL%20to%20provide%20better%20performance%0Aof%20the%20Kurdish%20NLP%20tasks.%20The%20article%20also%20collected%20most%20of%20the%20POS%20tags%20from%0Adifferent%20studies%20as%20well%20as%20from%20Kurdish%20linguistic%20experts%20to%20standardized%0Apart-of-speech%20tags.%20The%20proposed%20POS%20tagset%20is%20designed%20to%20annotate%20a%20large%0ACKL%20corpus%20and%20support%20Kurdish%20NLP%20tasks.%20The%20initial%20investigations%20of%20this%0Astudy%20via%20comparison%20with%20the%20Universal%20Dependencies%20framework%20for%20standard%0Alanguages%2C%20show%20that%20the%20proposed%20POS%20tagset%20can%20streamline%20or%20correct%0Asentences%20more%20accurately%20for%20Kurdish%20NLP%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Part-of-Speech%2520Tagging%2520to%2520Standardize%2520Central-Kurdish%250A%2520%2520Language%253A%2520A%2520Research%2520Guide%2520for%2520Kurdish%2520Natural%2520Language%2520Processing%2520Tasks%26entry.906535625%3DShadan%2520Shukr%2520Sabr%2520and%2520Nazira%2520Sabr%2520Mustafa%2520and%2520Talar%2520Sabah%2520Omar%2520and%2520Salah%2520Hwayyiz%2520Rasool%2520and%2520Nawzad%2520Anwer%2520Omer%2520and%2520Darya%2520Sabir%2520Hamad%2520and%2520Hemin%2520Abdulhameed%2520Shams%2520and%2520Omer%2520Mahmood%2520Kareem%2520and%2520Rozhan%2520Noori%2520Abdullah%2520and%2520Khabat%2520Atar%2520Abdullah%2520and%2520Mahabad%2520Azad%2520Mohammad%2520and%2520Haneen%2520Al-Raghefy%2520and%2520Safar%2520M.%2520Asaad%2520and%2520Sara%2520Jamal%2520Mohammed%2520and%2520Twana%2520Saeed%2520Ali%2520and%2520Fazil%2520Shawrow%2520and%2520Halgurd%2520S.%2520Maghdid%26entry.1292438233%3D%2520%2520-%2520The%2520field%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520has%2520dramatically%2520expanded%250Awithin%2520the%2520last%2520decade.%2520Many%2520human-being%2520applications%2520are%2520conducted%2520daily%2520via%250ANLP%2520tasks%252C%2520starting%2520from%2520machine%2520translation%252C%2520speech%2520recognition%252C%2520text%250Ageneration%2520and%2520recommendations%252C%2520Part-of-Speech%2520tagging%2520%2528POS%2529%252C%2520and%2520Named-Entity%250ARecognition%2520%2528NER%2529.%2520However%252C%2520low-resourced%2520languages%252C%2520such%2520as%2520the%250ACentral-Kurdish%2520language%2520%2528CKL%2529%252C%2520mainly%2520remain%2520unexamined%2520due%2520to%2520shortage%2520of%250Anecessary%2520resources%2520to%2520support%2520their%2520development.%2520The%2520POS%2520tagging%2520task%2520is%2520the%250Abase%2520of%2520other%2520NLP%2520tasks%253B%2520for%2520example%252C%2520the%2520POS%2520tag%2520set%2520has%2520been%2520used%2520to%250Astandardized%2520languages%2520to%2520provide%2520the%2520relationship%2520between%2520words%2520among%2520the%250Asentences%252C%2520followed%2520by%2520machine%2520translation%2520and%2520text%2520recommendation.%250ASpecifically%252C%2520for%2520the%2520CKL%252C%2520most%2520of%2520the%2520utilized%2520or%2520provided%2520POS%2520tagsets%2520are%250Aneither%2520standardized%2520nor%2520comprehensive.%2520To%2520this%2520end%252C%2520this%2520study%2520presented%2520an%250Aaccurate%2520and%2520comprehensive%2520POS%2520tagset%2520for%2520the%2520CKL%2520to%2520provide%2520better%2520performance%250Aof%2520the%2520Kurdish%2520NLP%2520tasks.%2520The%2520article%2520also%2520collected%2520most%2520of%2520the%2520POS%2520tags%2520from%250Adifferent%2520studies%2520as%2520well%2520as%2520from%2520Kurdish%2520linguistic%2520experts%2520to%2520standardized%250Apart-of-speech%2520tags.%2520The%2520proposed%2520POS%2520tagset%2520is%2520designed%2520to%2520annotate%2520a%2520large%250ACKL%2520corpus%2520and%2520support%2520Kurdish%2520NLP%2520tasks.%2520The%2520initial%2520investigations%2520of%2520this%250Astudy%2520via%2520comparison%2520with%2520the%2520Universal%2520Dependencies%2520framework%2520for%2520standard%250Alanguages%252C%2520show%2520that%2520the%2520proposed%2520POS%2520tagset%2520can%2520streamline%2520or%2520correct%250Asentences%2520more%2520accurately%2520for%2520Kurdish%2520NLP%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Part-of-Speech%20Tagging%20to%20Standardize%20Central-Kurdish%0A%20%20Language%3A%20A%20Research%20Guide%20for%20Kurdish%20Natural%20Language%20Processing%20Tasks&entry.906535625=Shadan%20Shukr%20Sabr%20and%20Nazira%20Sabr%20Mustafa%20and%20Talar%20Sabah%20Omar%20and%20Salah%20Hwayyiz%20Rasool%20and%20Nawzad%20Anwer%20Omer%20and%20Darya%20Sabir%20Hamad%20and%20Hemin%20Abdulhameed%20Shams%20and%20Omer%20Mahmood%20Kareem%20and%20Rozhan%20Noori%20Abdullah%20and%20Khabat%20Atar%20Abdullah%20and%20Mahabad%20Azad%20Mohammad%20and%20Haneen%20Al-Raghefy%20and%20Safar%20M.%20Asaad%20and%20Sara%20Jamal%20Mohammed%20and%20Twana%20Saeed%20Ali%20and%20Fazil%20Shawrow%20and%20Halgurd%20S.%20Maghdid&entry.1292438233=%20%20-%20The%20field%20of%20natural%20language%20processing%20%28NLP%29%20has%20dramatically%20expanded%0Awithin%20the%20last%20decade.%20Many%20human-being%20applications%20are%20conducted%20daily%20via%0ANLP%20tasks%2C%20starting%20from%20machine%20translation%2C%20speech%20recognition%2C%20text%0Ageneration%20and%20recommendations%2C%20Part-of-Speech%20tagging%20%28POS%29%2C%20and%20Named-Entity%0ARecognition%20%28NER%29.%20However%2C%20low-resourced%20languages%2C%20such%20as%20the%0ACentral-Kurdish%20language%20%28CKL%29%2C%20mainly%20remain%20unexamined%20due%20to%20shortage%20of%0Anecessary%20resources%20to%20support%20their%20development.%20The%20POS%20tagging%20task%20is%20the%0Abase%20of%20other%20NLP%20tasks%3B%20for%20example%2C%20the%20POS%20tag%20set%20has%20been%20used%20to%0Astandardized%20languages%20to%20provide%20the%20relationship%20between%20words%20among%20the%0Asentences%2C%20followed%20by%20machine%20translation%20and%20text%20recommendation.%0ASpecifically%2C%20for%20the%20CKL%2C%20most%20of%20the%20utilized%20or%20provided%20POS%20tagsets%20are%0Aneither%20standardized%20nor%20comprehensive.%20To%20this%20end%2C%20this%20study%20presented%20an%0Aaccurate%20and%20comprehensive%20POS%20tagset%20for%20the%20CKL%20to%20provide%20better%20performance%0Aof%20the%20Kurdish%20NLP%20tasks.%20The%20article%20also%20collected%20most%20of%20the%20POS%20tags%20from%0Adifferent%20studies%20as%20well%20as%20from%20Kurdish%20linguistic%20experts%20to%20standardized%0Apart-of-speech%20tags.%20The%20proposed%20POS%20tagset%20is%20designed%20to%20annotate%20a%20large%0ACKL%20corpus%20and%20support%20Kurdish%20NLP%20tasks.%20The%20initial%20investigations%20of%20this%0Astudy%20via%20comparison%20with%20the%20Universal%20Dependencies%20framework%20for%20standard%0Alanguages%2C%20show%20that%20the%20proposed%20POS%20tagset%20can%20streamline%20or%20correct%0Asentences%20more%20accurately%20for%20Kurdish%20NLP%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19645v1&entry.124074799=Read"},
{"title": "LMV-RPA: Large Model Voting-based Robotic Process Automation", "author": "Osama Abdellatif and Ahmed Ayman and Ali Hamdi", "abstract": "  Automating high-volume unstructured data processing is essential for\noperational efficiency. Optical Character Recognition (OCR) is critical but\noften struggles with accuracy and efficiency in complex layouts and ambiguous\ntext. These challenges are especially pronounced in large-scale tasks requiring\nboth speed and precision. This paper introduces LMV-RPA, a Large Model\nVoting-based Robotic Process Automation system to enhance OCR workflows.\nLMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR,\nEasy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and\nGemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs\ninto structured JSON formats, improving accuracy, particularly in complex\nlayouts. The multi-phase pipeline processes text extracted by OCR engines\nthrough LLMs, combining results to ensure the most accurate outputs. LMV-RPA\nachieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94\npercent, while reducing processing time by 80 percent. Benchmark evaluations\nconfirm its scalability and demonstrate that LMV-RPA offers a faster, more\nreliable, and efficient solution for automating large-scale document processing\ntasks.\n", "link": "http://arxiv.org/abs/2412.17965v2", "date": "2025-04-28", "relevancy": 2.0456, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMV-RPA%3A%20Large%20Model%20Voting-based%20Robotic%20Process%20Automation&body=Title%3A%20LMV-RPA%3A%20Large%20Model%20Voting-based%20Robotic%20Process%20Automation%0AAuthor%3A%20Osama%20Abdellatif%20and%20Ahmed%20Ayman%20and%20Ali%20Hamdi%0AAbstract%3A%20%20%20Automating%20high-volume%20unstructured%20data%20processing%20is%20essential%20for%0Aoperational%20efficiency.%20Optical%20Character%20Recognition%20%28OCR%29%20is%20critical%20but%0Aoften%20struggles%20with%20accuracy%20and%20efficiency%20in%20complex%20layouts%20and%20ambiguous%0Atext.%20These%20challenges%20are%20especially%20pronounced%20in%20large-scale%20tasks%20requiring%0Aboth%20speed%20and%20precision.%20This%20paper%20introduces%20LMV-RPA%2C%20a%20Large%20Model%0AVoting-based%20Robotic%20Process%20Automation%20system%20to%20enhance%20OCR%20workflows.%0ALMV-RPA%20integrates%20outputs%20from%20OCR%20engines%20such%20as%20Paddle%20OCR%2C%20Tesseract%20OCR%2C%0AEasy%20OCR%2C%20and%20DocTR%20with%20Large%20Language%20Models%20%28LLMs%29%20like%20LLaMA%203%20and%0AGemini-1.5-pro.%20Using%20a%20majority%20voting%20mechanism%2C%20it%20processes%20OCR%20outputs%0Ainto%20structured%20JSON%20formats%2C%20improving%20accuracy%2C%20particularly%20in%20complex%0Alayouts.%20The%20multi-phase%20pipeline%20processes%20text%20extracted%20by%20OCR%20engines%0Athrough%20LLMs%2C%20combining%20results%20to%20ensure%20the%20most%20accurate%20outputs.%20LMV-RPA%0Aachieves%2099%20percent%20accuracy%20in%20OCR%20tasks%2C%20surpassing%20baseline%20models%20with%2094%0Apercent%2C%20while%20reducing%20processing%20time%20by%2080%20percent.%20Benchmark%20evaluations%0Aconfirm%20its%20scalability%20and%20demonstrate%20that%20LMV-RPA%20offers%20a%20faster%2C%20more%0Areliable%2C%20and%20efficient%20solution%20for%20automating%20large-scale%20document%20processing%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMV-RPA%253A%2520Large%2520Model%2520Voting-based%2520Robotic%2520Process%2520Automation%26entry.906535625%3DOsama%2520Abdellatif%2520and%2520Ahmed%2520Ayman%2520and%2520Ali%2520Hamdi%26entry.1292438233%3D%2520%2520Automating%2520high-volume%2520unstructured%2520data%2520processing%2520is%2520essential%2520for%250Aoperational%2520efficiency.%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520is%2520critical%2520but%250Aoften%2520struggles%2520with%2520accuracy%2520and%2520efficiency%2520in%2520complex%2520layouts%2520and%2520ambiguous%250Atext.%2520These%2520challenges%2520are%2520especially%2520pronounced%2520in%2520large-scale%2520tasks%2520requiring%250Aboth%2520speed%2520and%2520precision.%2520This%2520paper%2520introduces%2520LMV-RPA%252C%2520a%2520Large%2520Model%250AVoting-based%2520Robotic%2520Process%2520Automation%2520system%2520to%2520enhance%2520OCR%2520workflows.%250ALMV-RPA%2520integrates%2520outputs%2520from%2520OCR%2520engines%2520such%2520as%2520Paddle%2520OCR%252C%2520Tesseract%2520OCR%252C%250AEasy%2520OCR%252C%2520and%2520DocTR%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520like%2520LLaMA%25203%2520and%250AGemini-1.5-pro.%2520Using%2520a%2520majority%2520voting%2520mechanism%252C%2520it%2520processes%2520OCR%2520outputs%250Ainto%2520structured%2520JSON%2520formats%252C%2520improving%2520accuracy%252C%2520particularly%2520in%2520complex%250Alayouts.%2520The%2520multi-phase%2520pipeline%2520processes%2520text%2520extracted%2520by%2520OCR%2520engines%250Athrough%2520LLMs%252C%2520combining%2520results%2520to%2520ensure%2520the%2520most%2520accurate%2520outputs.%2520LMV-RPA%250Aachieves%252099%2520percent%2520accuracy%2520in%2520OCR%2520tasks%252C%2520surpassing%2520baseline%2520models%2520with%252094%250Apercent%252C%2520while%2520reducing%2520processing%2520time%2520by%252080%2520percent.%2520Benchmark%2520evaluations%250Aconfirm%2520its%2520scalability%2520and%2520demonstrate%2520that%2520LMV-RPA%2520offers%2520a%2520faster%252C%2520more%250Areliable%252C%2520and%2520efficient%2520solution%2520for%2520automating%2520large-scale%2520document%2520processing%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMV-RPA%3A%20Large%20Model%20Voting-based%20Robotic%20Process%20Automation&entry.906535625=Osama%20Abdellatif%20and%20Ahmed%20Ayman%20and%20Ali%20Hamdi&entry.1292438233=%20%20Automating%20high-volume%20unstructured%20data%20processing%20is%20essential%20for%0Aoperational%20efficiency.%20Optical%20Character%20Recognition%20%28OCR%29%20is%20critical%20but%0Aoften%20struggles%20with%20accuracy%20and%20efficiency%20in%20complex%20layouts%20and%20ambiguous%0Atext.%20These%20challenges%20are%20especially%20pronounced%20in%20large-scale%20tasks%20requiring%0Aboth%20speed%20and%20precision.%20This%20paper%20introduces%20LMV-RPA%2C%20a%20Large%20Model%0AVoting-based%20Robotic%20Process%20Automation%20system%20to%20enhance%20OCR%20workflows.%0ALMV-RPA%20integrates%20outputs%20from%20OCR%20engines%20such%20as%20Paddle%20OCR%2C%20Tesseract%20OCR%2C%0AEasy%20OCR%2C%20and%20DocTR%20with%20Large%20Language%20Models%20%28LLMs%29%20like%20LLaMA%203%20and%0AGemini-1.5-pro.%20Using%20a%20majority%20voting%20mechanism%2C%20it%20processes%20OCR%20outputs%0Ainto%20structured%20JSON%20formats%2C%20improving%20accuracy%2C%20particularly%20in%20complex%0Alayouts.%20The%20multi-phase%20pipeline%20processes%20text%20extracted%20by%20OCR%20engines%0Athrough%20LLMs%2C%20combining%20results%20to%20ensure%20the%20most%20accurate%20outputs.%20LMV-RPA%0Aachieves%2099%20percent%20accuracy%20in%20OCR%20tasks%2C%20surpassing%20baseline%20models%20with%2094%0Apercent%2C%20while%20reducing%20processing%20time%20by%2080%20percent.%20Benchmark%20evaluations%0Aconfirm%20its%20scalability%20and%20demonstrate%20that%20LMV-RPA%20offers%20a%20faster%2C%20more%0Areliable%2C%20and%20efficient%20solution%20for%20automating%20large-scale%20document%20processing%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17965v2&entry.124074799=Read"},
{"title": "An $\\ell^1$-Plug-and-Play Approach for MPI Using a Zero Shot Denoiser\n  with Evaluation on the 3D Open MPI Dataset", "author": "Vladyslav Gapyak and Corinna Rentschler and Thomas M\u00e4rz and Andreas Weinmann", "abstract": "  Objective: Magnetic particle imaging (MPI) is an emerging medical imaging\nmodality which has gained increasing interest in recent years. Among the\nbenefits of MPI are its high temporal resolution, and that the technique does\nnot expose the specimen to any kind of ionizing radiation. It is based on the\nnon-linear response of magnetic nanoparticles to an applied magnetic field.\nFrom the electric signal measured in receive coils, the particle concentration\nhas to be reconstructed. Due to the ill-posedness of the reconstruction\nproblem, various regularization methods have been proposed for reconstruction\nranging from early stopping methods, via classical Tikhonov regularization and\niterative methods to modern machine learning approaches. In this work, we\ncontribute to the latter class: we propose a plug-and-play approach based on a\ngeneric zero-shot denoiser with an $\\ell^1$-prior.\n  Approach: We validate the reconstruction parameters of the method on a hybrid\ndataset and compare it with the baseline Tikhonov, DIP and the previous PP-MPI,\nwhich is a plug-and-play method with denoiser trained on MPI-friendly data.\n  Main results: We offer a quantitative and qualitative evaluation of the\nzero-shot plug-and-play approach on the 3D Open MPI dataset. Moreover, we show\nthe quality of the approach with different levels of preprocessing of the data.\n  Significance: The proposed method employs a zero-shot denoiser which has not\nbeen trained for the MPI task and therefore saves the cost for training.\nMoreover, it offers a method that can be potentially applied in future MPI\ncontexts.\n", "link": "http://arxiv.org/abs/2401.00275v3", "date": "2025-04-28", "relevancy": 2.0412, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5156}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5081}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20%24%5Cell%5E1%24-Plug-and-Play%20Approach%20for%20MPI%20Using%20a%20Zero%20Shot%20Denoiser%0A%20%20with%20Evaluation%20on%20the%203D%20Open%20MPI%20Dataset&body=Title%3A%20An%20%24%5Cell%5E1%24-Plug-and-Play%20Approach%20for%20MPI%20Using%20a%20Zero%20Shot%20Denoiser%0A%20%20with%20Evaluation%20on%20the%203D%20Open%20MPI%20Dataset%0AAuthor%3A%20Vladyslav%20Gapyak%20and%20Corinna%20Rentschler%20and%20Thomas%20M%C3%A4rz%20and%20Andreas%20Weinmann%0AAbstract%3A%20%20%20Objective%3A%20Magnetic%20particle%20imaging%20%28MPI%29%20is%20an%20emerging%20medical%20imaging%0Amodality%20which%20has%20gained%20increasing%20interest%20in%20recent%20years.%20Among%20the%0Abenefits%20of%20MPI%20are%20its%20high%20temporal%20resolution%2C%20and%20that%20the%20technique%20does%0Anot%20expose%20the%20specimen%20to%20any%20kind%20of%20ionizing%20radiation.%20It%20is%20based%20on%20the%0Anon-linear%20response%20of%20magnetic%20nanoparticles%20to%20an%20applied%20magnetic%20field.%0AFrom%20the%20electric%20signal%20measured%20in%20receive%20coils%2C%20the%20particle%20concentration%0Ahas%20to%20be%20reconstructed.%20Due%20to%20the%20ill-posedness%20of%20the%20reconstruction%0Aproblem%2C%20various%20regularization%20methods%20have%20been%20proposed%20for%20reconstruction%0Aranging%20from%20early%20stopping%20methods%2C%20via%20classical%20Tikhonov%20regularization%20and%0Aiterative%20methods%20to%20modern%20machine%20learning%20approaches.%20In%20this%20work%2C%20we%0Acontribute%20to%20the%20latter%20class%3A%20we%20propose%20a%20plug-and-play%20approach%20based%20on%20a%0Ageneric%20zero-shot%20denoiser%20with%20an%20%24%5Cell%5E1%24-prior.%0A%20%20Approach%3A%20We%20validate%20the%20reconstruction%20parameters%20of%20the%20method%20on%20a%20hybrid%0Adataset%20and%20compare%20it%20with%20the%20baseline%20Tikhonov%2C%20DIP%20and%20the%20previous%20PP-MPI%2C%0Awhich%20is%20a%20plug-and-play%20method%20with%20denoiser%20trained%20on%20MPI-friendly%20data.%0A%20%20Main%20results%3A%20We%20offer%20a%20quantitative%20and%20qualitative%20evaluation%20of%20the%0Azero-shot%20plug-and-play%20approach%20on%20the%203D%20Open%20MPI%20dataset.%20Moreover%2C%20we%20show%0Athe%20quality%20of%20the%20approach%20with%20different%20levels%20of%20preprocessing%20of%20the%20data.%0A%20%20Significance%3A%20The%20proposed%20method%20employs%20a%20zero-shot%20denoiser%20which%20has%20not%0Abeen%20trained%20for%20the%20MPI%20task%20and%20therefore%20saves%20the%20cost%20for%20training.%0AMoreover%2C%20it%20offers%20a%20method%20that%20can%20be%20potentially%20applied%20in%20future%20MPI%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00275v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520%2524%255Cell%255E1%2524-Plug-and-Play%2520Approach%2520for%2520MPI%2520Using%2520a%2520Zero%2520Shot%2520Denoiser%250A%2520%2520with%2520Evaluation%2520on%2520the%25203D%2520Open%2520MPI%2520Dataset%26entry.906535625%3DVladyslav%2520Gapyak%2520and%2520Corinna%2520Rentschler%2520and%2520Thomas%2520M%25C3%25A4rz%2520and%2520Andreas%2520Weinmann%26entry.1292438233%3D%2520%2520Objective%253A%2520Magnetic%2520particle%2520imaging%2520%2528MPI%2529%2520is%2520an%2520emerging%2520medical%2520imaging%250Amodality%2520which%2520has%2520gained%2520increasing%2520interest%2520in%2520recent%2520years.%2520Among%2520the%250Abenefits%2520of%2520MPI%2520are%2520its%2520high%2520temporal%2520resolution%252C%2520and%2520that%2520the%2520technique%2520does%250Anot%2520expose%2520the%2520specimen%2520to%2520any%2520kind%2520of%2520ionizing%2520radiation.%2520It%2520is%2520based%2520on%2520the%250Anon-linear%2520response%2520of%2520magnetic%2520nanoparticles%2520to%2520an%2520applied%2520magnetic%2520field.%250AFrom%2520the%2520electric%2520signal%2520measured%2520in%2520receive%2520coils%252C%2520the%2520particle%2520concentration%250Ahas%2520to%2520be%2520reconstructed.%2520Due%2520to%2520the%2520ill-posedness%2520of%2520the%2520reconstruction%250Aproblem%252C%2520various%2520regularization%2520methods%2520have%2520been%2520proposed%2520for%2520reconstruction%250Aranging%2520from%2520early%2520stopping%2520methods%252C%2520via%2520classical%2520Tikhonov%2520regularization%2520and%250Aiterative%2520methods%2520to%2520modern%2520machine%2520learning%2520approaches.%2520In%2520this%2520work%252C%2520we%250Acontribute%2520to%2520the%2520latter%2520class%253A%2520we%2520propose%2520a%2520plug-and-play%2520approach%2520based%2520on%2520a%250Ageneric%2520zero-shot%2520denoiser%2520with%2520an%2520%2524%255Cell%255E1%2524-prior.%250A%2520%2520Approach%253A%2520We%2520validate%2520the%2520reconstruction%2520parameters%2520of%2520the%2520method%2520on%2520a%2520hybrid%250Adataset%2520and%2520compare%2520it%2520with%2520the%2520baseline%2520Tikhonov%252C%2520DIP%2520and%2520the%2520previous%2520PP-MPI%252C%250Awhich%2520is%2520a%2520plug-and-play%2520method%2520with%2520denoiser%2520trained%2520on%2520MPI-friendly%2520data.%250A%2520%2520Main%2520results%253A%2520We%2520offer%2520a%2520quantitative%2520and%2520qualitative%2520evaluation%2520of%2520the%250Azero-shot%2520plug-and-play%2520approach%2520on%2520the%25203D%2520Open%2520MPI%2520dataset.%2520Moreover%252C%2520we%2520show%250Athe%2520quality%2520of%2520the%2520approach%2520with%2520different%2520levels%2520of%2520preprocessing%2520of%2520the%2520data.%250A%2520%2520Significance%253A%2520The%2520proposed%2520method%2520employs%2520a%2520zero-shot%2520denoiser%2520which%2520has%2520not%250Abeen%2520trained%2520for%2520the%2520MPI%2520task%2520and%2520therefore%2520saves%2520the%2520cost%2520for%2520training.%250AMoreover%252C%2520it%2520offers%2520a%2520method%2520that%2520can%2520be%2520potentially%2520applied%2520in%2520future%2520MPI%250Acontexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00275v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20%24%5Cell%5E1%24-Plug-and-Play%20Approach%20for%20MPI%20Using%20a%20Zero%20Shot%20Denoiser%0A%20%20with%20Evaluation%20on%20the%203D%20Open%20MPI%20Dataset&entry.906535625=Vladyslav%20Gapyak%20and%20Corinna%20Rentschler%20and%20Thomas%20M%C3%A4rz%20and%20Andreas%20Weinmann&entry.1292438233=%20%20Objective%3A%20Magnetic%20particle%20imaging%20%28MPI%29%20is%20an%20emerging%20medical%20imaging%0Amodality%20which%20has%20gained%20increasing%20interest%20in%20recent%20years.%20Among%20the%0Abenefits%20of%20MPI%20are%20its%20high%20temporal%20resolution%2C%20and%20that%20the%20technique%20does%0Anot%20expose%20the%20specimen%20to%20any%20kind%20of%20ionizing%20radiation.%20It%20is%20based%20on%20the%0Anon-linear%20response%20of%20magnetic%20nanoparticles%20to%20an%20applied%20magnetic%20field.%0AFrom%20the%20electric%20signal%20measured%20in%20receive%20coils%2C%20the%20particle%20concentration%0Ahas%20to%20be%20reconstructed.%20Due%20to%20the%20ill-posedness%20of%20the%20reconstruction%0Aproblem%2C%20various%20regularization%20methods%20have%20been%20proposed%20for%20reconstruction%0Aranging%20from%20early%20stopping%20methods%2C%20via%20classical%20Tikhonov%20regularization%20and%0Aiterative%20methods%20to%20modern%20machine%20learning%20approaches.%20In%20this%20work%2C%20we%0Acontribute%20to%20the%20latter%20class%3A%20we%20propose%20a%20plug-and-play%20approach%20based%20on%20a%0Ageneric%20zero-shot%20denoiser%20with%20an%20%24%5Cell%5E1%24-prior.%0A%20%20Approach%3A%20We%20validate%20the%20reconstruction%20parameters%20of%20the%20method%20on%20a%20hybrid%0Adataset%20and%20compare%20it%20with%20the%20baseline%20Tikhonov%2C%20DIP%20and%20the%20previous%20PP-MPI%2C%0Awhich%20is%20a%20plug-and-play%20method%20with%20denoiser%20trained%20on%20MPI-friendly%20data.%0A%20%20Main%20results%3A%20We%20offer%20a%20quantitative%20and%20qualitative%20evaluation%20of%20the%0Azero-shot%20plug-and-play%20approach%20on%20the%203D%20Open%20MPI%20dataset.%20Moreover%2C%20we%20show%0Athe%20quality%20of%20the%20approach%20with%20different%20levels%20of%20preprocessing%20of%20the%20data.%0A%20%20Significance%3A%20The%20proposed%20method%20employs%20a%20zero-shot%20denoiser%20which%20has%20not%0Abeen%20trained%20for%20the%20MPI%20task%20and%20therefore%20saves%20the%20cost%20for%20training.%0AMoreover%2C%20it%20offers%20a%20method%20that%20can%20be%20potentially%20applied%20in%20future%20MPI%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00275v3&entry.124074799=Read"},
{"title": "Learning Efficiency Meets Symmetry Breaking", "author": "Yingbin Bai and Sylvie Thiebaux and Felipe Trevizan", "abstract": "  Learning-based planners leveraging Graph Neural Networks can learn search\nguidance applicable to large search spaces, yet their potential to address\nsymmetries remains largely unexplored. In this paper, we introduce a graph\nrepresentation of planning problems allying learning efficiency with the\nability to detect symmetries, along with two pruning methods, action pruning\nand state pruning, designed to manage symmetries during search. The integration\nof these techniques into Fast Downward achieves a first-time success over LAMA\non the latest IPC learning track dataset. Code is released at:\nhttps://github.com/bybeye/Distincter.\n", "link": "http://arxiv.org/abs/2504.19738v1", "date": "2025-04-28", "relevancy": 2.0367, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5452}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4965}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Efficiency%20Meets%20Symmetry%20Breaking&body=Title%3A%20Learning%20Efficiency%20Meets%20Symmetry%20Breaking%0AAuthor%3A%20Yingbin%20Bai%20and%20Sylvie%20Thiebaux%20and%20Felipe%20Trevizan%0AAbstract%3A%20%20%20Learning-based%20planners%20leveraging%20Graph%20Neural%20Networks%20can%20learn%20search%0Aguidance%20applicable%20to%20large%20search%20spaces%2C%20yet%20their%20potential%20to%20address%0Asymmetries%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20a%20graph%0Arepresentation%20of%20planning%20problems%20allying%20learning%20efficiency%20with%20the%0Aability%20to%20detect%20symmetries%2C%20along%20with%20two%20pruning%20methods%2C%20action%20pruning%0Aand%20state%20pruning%2C%20designed%20to%20manage%20symmetries%20during%20search.%20The%20integration%0Aof%20these%20techniques%20into%20Fast%20Downward%20achieves%20a%20first-time%20success%20over%20LAMA%0Aon%20the%20latest%20IPC%20learning%20track%20dataset.%20Code%20is%20released%20at%3A%0Ahttps%3A//github.com/bybeye/Distincter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Efficiency%2520Meets%2520Symmetry%2520Breaking%26entry.906535625%3DYingbin%2520Bai%2520and%2520Sylvie%2520Thiebaux%2520and%2520Felipe%2520Trevizan%26entry.1292438233%3D%2520%2520Learning-based%2520planners%2520leveraging%2520Graph%2520Neural%2520Networks%2520can%2520learn%2520search%250Aguidance%2520applicable%2520to%2520large%2520search%2520spaces%252C%2520yet%2520their%2520potential%2520to%2520address%250Asymmetries%2520remains%2520largely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520graph%250Arepresentation%2520of%2520planning%2520problems%2520allying%2520learning%2520efficiency%2520with%2520the%250Aability%2520to%2520detect%2520symmetries%252C%2520along%2520with%2520two%2520pruning%2520methods%252C%2520action%2520pruning%250Aand%2520state%2520pruning%252C%2520designed%2520to%2520manage%2520symmetries%2520during%2520search.%2520The%2520integration%250Aof%2520these%2520techniques%2520into%2520Fast%2520Downward%2520achieves%2520a%2520first-time%2520success%2520over%2520LAMA%250Aon%2520the%2520latest%2520IPC%2520learning%2520track%2520dataset.%2520Code%2520is%2520released%2520at%253A%250Ahttps%253A//github.com/bybeye/Distincter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Efficiency%20Meets%20Symmetry%20Breaking&entry.906535625=Yingbin%20Bai%20and%20Sylvie%20Thiebaux%20and%20Felipe%20Trevizan&entry.1292438233=%20%20Learning-based%20planners%20leveraging%20Graph%20Neural%20Networks%20can%20learn%20search%0Aguidance%20applicable%20to%20large%20search%20spaces%2C%20yet%20their%20potential%20to%20address%0Asymmetries%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20a%20graph%0Arepresentation%20of%20planning%20problems%20allying%20learning%20efficiency%20with%20the%0Aability%20to%20detect%20symmetries%2C%20along%20with%20two%20pruning%20methods%2C%20action%20pruning%0Aand%20state%20pruning%2C%20designed%20to%20manage%20symmetries%20during%20search.%20The%20integration%0Aof%20these%20techniques%20into%20Fast%20Downward%20achieves%20a%20first-time%20success%20over%20LAMA%0Aon%20the%20latest%20IPC%20learning%20track%20dataset.%20Code%20is%20released%20at%3A%0Ahttps%3A//github.com/bybeye/Distincter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19738v1&entry.124074799=Read"},
{"title": "Centaur: a foundation model of human cognition", "author": "Marcel Binz and Elif Akata and Matthias Bethge and Franziska Br\u00e4ndle and Fred Callaway and Julian Coda-Forno and Peter Dayan and Can Demircan and Maria K. Eckstein and No\u00e9mi \u00c9ltet\u0151 and Thomas L. Griffiths and Susanne Haridi and Akshay K. Jagadish and Li Ji-An and Alexander Kipnis and Sreejan Kumar and Tobias Ludwig and Marvin Mathony and Marcelo Mattar and Alireza Modirshanechi and Surabhi S. Nath and Joshua C. Peterson and Milena Rmus and Evan M. Russek and Tankred Saanum and Johannes A. Schubert and Luca M. Schulze Buschoff and Nishad Singhi and Xin Sui and Mirko Thalmann and Fabian Theis and Vuong Truong and Vishaal Udandarao and Konstantinos Voudouris and Robert Wilson and Kristin Witte and Shuchen Wu and Dirk Wulff and Huadong Xiong and Eric Schulz", "abstract": "  Establishing a unified theory of cognition has been a major goal of\npsychology. While there have been previous attempts to instantiate such\ntheories by building computational models, we currently do not have one model\nthat captures the human mind in its entirety. A first step in this direction is\nto create a model that can predict human behavior in a wide range of settings.\nHere we introduce Centaur, a computational model that can predict and simulate\nhuman behavior in any experiment expressible in natural language. We derived\nCentaur by finetuning a state-of-the-art language model on a novel, large-scale\ndata set called Psych-101. Psych-101 reaches an unprecedented scale, covering\ntrial-by-trial data from over 60,000 participants performing over 10,000,000\nchoices in 160 experiments. Centaur not only captures the behavior of held-out\nparticipants better than existing cognitive models, but also generalizes to new\ncover stories, structural task modifications, and entirely new domains.\nFurthermore, we find that the model's internal representations become more\naligned with human neural activity after finetuning. Taken together, our\nresults demonstrate that it is possible to discover computational models that\ncapture human behavior across a wide range of domains. We believe that such\nmodels provide tremendous potential for guiding the development of cognitive\ntheories and present a case study to demonstrate this.\n", "link": "http://arxiv.org/abs/2410.20268v3", "date": "2025-04-28", "relevancy": 2.0355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Centaur%3A%20a%20foundation%20model%20of%20human%20cognition&body=Title%3A%20Centaur%3A%20a%20foundation%20model%20of%20human%20cognition%0AAuthor%3A%20Marcel%20Binz%20and%20Elif%20Akata%20and%20Matthias%20Bethge%20and%20Franziska%20Br%C3%A4ndle%20and%20Fred%20Callaway%20and%20Julian%20Coda-Forno%20and%20Peter%20Dayan%20and%20Can%20Demircan%20and%20Maria%20K.%20Eckstein%20and%20No%C3%A9mi%20%C3%89ltet%C5%91%20and%20Thomas%20L.%20Griffiths%20and%20Susanne%20Haridi%20and%20Akshay%20K.%20Jagadish%20and%20Li%20Ji-An%20and%20Alexander%20Kipnis%20and%20Sreejan%20Kumar%20and%20Tobias%20Ludwig%20and%20Marvin%20Mathony%20and%20Marcelo%20Mattar%20and%20Alireza%20Modirshanechi%20and%20Surabhi%20S.%20Nath%20and%20Joshua%20C.%20Peterson%20and%20Milena%20Rmus%20and%20Evan%20M.%20Russek%20and%20Tankred%20Saanum%20and%20Johannes%20A.%20Schubert%20and%20Luca%20M.%20Schulze%20Buschoff%20and%20Nishad%20Singhi%20and%20Xin%20Sui%20and%20Mirko%20Thalmann%20and%20Fabian%20Theis%20and%20Vuong%20Truong%20and%20Vishaal%20Udandarao%20and%20Konstantinos%20Voudouris%20and%20Robert%20Wilson%20and%20Kristin%20Witte%20and%20Shuchen%20Wu%20and%20Dirk%20Wulff%20and%20Huadong%20Xiong%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20Establishing%20a%20unified%20theory%20of%20cognition%20has%20been%20a%20major%20goal%20of%0Apsychology.%20While%20there%20have%20been%20previous%20attempts%20to%20instantiate%20such%0Atheories%20by%20building%20computational%20models%2C%20we%20currently%20do%20not%20have%20one%20model%0Athat%20captures%20the%20human%20mind%20in%20its%20entirety.%20A%20first%20step%20in%20this%20direction%20is%0Ato%20create%20a%20model%20that%20can%20predict%20human%20behavior%20in%20a%20wide%20range%20of%20settings.%0AHere%20we%20introduce%20Centaur%2C%20a%20computational%20model%20that%20can%20predict%20and%20simulate%0Ahuman%20behavior%20in%20any%20experiment%20expressible%20in%20natural%20language.%20We%20derived%0ACentaur%20by%20finetuning%20a%20state-of-the-art%20language%20model%20on%20a%20novel%2C%20large-scale%0Adata%20set%20called%20Psych-101.%20Psych-101%20reaches%20an%20unprecedented%20scale%2C%20covering%0Atrial-by-trial%20data%20from%20over%2060%2C000%20participants%20performing%20over%2010%2C000%2C000%0Achoices%20in%20160%20experiments.%20Centaur%20not%20only%20captures%20the%20behavior%20of%20held-out%0Aparticipants%20better%20than%20existing%20cognitive%20models%2C%20but%20also%20generalizes%20to%20new%0Acover%20stories%2C%20structural%20task%20modifications%2C%20and%20entirely%20new%20domains.%0AFurthermore%2C%20we%20find%20that%20the%20model%27s%20internal%20representations%20become%20more%0Aaligned%20with%20human%20neural%20activity%20after%20finetuning.%20Taken%20together%2C%20our%0Aresults%20demonstrate%20that%20it%20is%20possible%20to%20discover%20computational%20models%20that%0Acapture%20human%20behavior%20across%20a%20wide%20range%20of%20domains.%20We%20believe%20that%20such%0Amodels%20provide%20tremendous%20potential%20for%20guiding%20the%20development%20of%20cognitive%0Atheories%20and%20present%20a%20case%20study%20to%20demonstrate%20this.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20268v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentaur%253A%2520a%2520foundation%2520model%2520of%2520human%2520cognition%26entry.906535625%3DMarcel%2520Binz%2520and%2520Elif%2520Akata%2520and%2520Matthias%2520Bethge%2520and%2520Franziska%2520Br%25C3%25A4ndle%2520and%2520Fred%2520Callaway%2520and%2520Julian%2520Coda-Forno%2520and%2520Peter%2520Dayan%2520and%2520Can%2520Demircan%2520and%2520Maria%2520K.%2520Eckstein%2520and%2520No%25C3%25A9mi%2520%25C3%2589ltet%25C5%2591%2520and%2520Thomas%2520L.%2520Griffiths%2520and%2520Susanne%2520Haridi%2520and%2520Akshay%2520K.%2520Jagadish%2520and%2520Li%2520Ji-An%2520and%2520Alexander%2520Kipnis%2520and%2520Sreejan%2520Kumar%2520and%2520Tobias%2520Ludwig%2520and%2520Marvin%2520Mathony%2520and%2520Marcelo%2520Mattar%2520and%2520Alireza%2520Modirshanechi%2520and%2520Surabhi%2520S.%2520Nath%2520and%2520Joshua%2520C.%2520Peterson%2520and%2520Milena%2520Rmus%2520and%2520Evan%2520M.%2520Russek%2520and%2520Tankred%2520Saanum%2520and%2520Johannes%2520A.%2520Schubert%2520and%2520Luca%2520M.%2520Schulze%2520Buschoff%2520and%2520Nishad%2520Singhi%2520and%2520Xin%2520Sui%2520and%2520Mirko%2520Thalmann%2520and%2520Fabian%2520Theis%2520and%2520Vuong%2520Truong%2520and%2520Vishaal%2520Udandarao%2520and%2520Konstantinos%2520Voudouris%2520and%2520Robert%2520Wilson%2520and%2520Kristin%2520Witte%2520and%2520Shuchen%2520Wu%2520and%2520Dirk%2520Wulff%2520and%2520Huadong%2520Xiong%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520Establishing%2520a%2520unified%2520theory%2520of%2520cognition%2520has%2520been%2520a%2520major%2520goal%2520of%250Apsychology.%2520While%2520there%2520have%2520been%2520previous%2520attempts%2520to%2520instantiate%2520such%250Atheories%2520by%2520building%2520computational%2520models%252C%2520we%2520currently%2520do%2520not%2520have%2520one%2520model%250Athat%2520captures%2520the%2520human%2520mind%2520in%2520its%2520entirety.%2520A%2520first%2520step%2520in%2520this%2520direction%2520is%250Ato%2520create%2520a%2520model%2520that%2520can%2520predict%2520human%2520behavior%2520in%2520a%2520wide%2520range%2520of%2520settings.%250AHere%2520we%2520introduce%2520Centaur%252C%2520a%2520computational%2520model%2520that%2520can%2520predict%2520and%2520simulate%250Ahuman%2520behavior%2520in%2520any%2520experiment%2520expressible%2520in%2520natural%2520language.%2520We%2520derived%250ACentaur%2520by%2520finetuning%2520a%2520state-of-the-art%2520language%2520model%2520on%2520a%2520novel%252C%2520large-scale%250Adata%2520set%2520called%2520Psych-101.%2520Psych-101%2520reaches%2520an%2520unprecedented%2520scale%252C%2520covering%250Atrial-by-trial%2520data%2520from%2520over%252060%252C000%2520participants%2520performing%2520over%252010%252C000%252C000%250Achoices%2520in%2520160%2520experiments.%2520Centaur%2520not%2520only%2520captures%2520the%2520behavior%2520of%2520held-out%250Aparticipants%2520better%2520than%2520existing%2520cognitive%2520models%252C%2520but%2520also%2520generalizes%2520to%2520new%250Acover%2520stories%252C%2520structural%2520task%2520modifications%252C%2520and%2520entirely%2520new%2520domains.%250AFurthermore%252C%2520we%2520find%2520that%2520the%2520model%2527s%2520internal%2520representations%2520become%2520more%250Aaligned%2520with%2520human%2520neural%2520activity%2520after%2520finetuning.%2520Taken%2520together%252C%2520our%250Aresults%2520demonstrate%2520that%2520it%2520is%2520possible%2520to%2520discover%2520computational%2520models%2520that%250Acapture%2520human%2520behavior%2520across%2520a%2520wide%2520range%2520of%2520domains.%2520We%2520believe%2520that%2520such%250Amodels%2520provide%2520tremendous%2520potential%2520for%2520guiding%2520the%2520development%2520of%2520cognitive%250Atheories%2520and%2520present%2520a%2520case%2520study%2520to%2520demonstrate%2520this.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20268v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centaur%3A%20a%20foundation%20model%20of%20human%20cognition&entry.906535625=Marcel%20Binz%20and%20Elif%20Akata%20and%20Matthias%20Bethge%20and%20Franziska%20Br%C3%A4ndle%20and%20Fred%20Callaway%20and%20Julian%20Coda-Forno%20and%20Peter%20Dayan%20and%20Can%20Demircan%20and%20Maria%20K.%20Eckstein%20and%20No%C3%A9mi%20%C3%89ltet%C5%91%20and%20Thomas%20L.%20Griffiths%20and%20Susanne%20Haridi%20and%20Akshay%20K.%20Jagadish%20and%20Li%20Ji-An%20and%20Alexander%20Kipnis%20and%20Sreejan%20Kumar%20and%20Tobias%20Ludwig%20and%20Marvin%20Mathony%20and%20Marcelo%20Mattar%20and%20Alireza%20Modirshanechi%20and%20Surabhi%20S.%20Nath%20and%20Joshua%20C.%20Peterson%20and%20Milena%20Rmus%20and%20Evan%20M.%20Russek%20and%20Tankred%20Saanum%20and%20Johannes%20A.%20Schubert%20and%20Luca%20M.%20Schulze%20Buschoff%20and%20Nishad%20Singhi%20and%20Xin%20Sui%20and%20Mirko%20Thalmann%20and%20Fabian%20Theis%20and%20Vuong%20Truong%20and%20Vishaal%20Udandarao%20and%20Konstantinos%20Voudouris%20and%20Robert%20Wilson%20and%20Kristin%20Witte%20and%20Shuchen%20Wu%20and%20Dirk%20Wulff%20and%20Huadong%20Xiong%20and%20Eric%20Schulz&entry.1292438233=%20%20Establishing%20a%20unified%20theory%20of%20cognition%20has%20been%20a%20major%20goal%20of%0Apsychology.%20While%20there%20have%20been%20previous%20attempts%20to%20instantiate%20such%0Atheories%20by%20building%20computational%20models%2C%20we%20currently%20do%20not%20have%20one%20model%0Athat%20captures%20the%20human%20mind%20in%20its%20entirety.%20A%20first%20step%20in%20this%20direction%20is%0Ato%20create%20a%20model%20that%20can%20predict%20human%20behavior%20in%20a%20wide%20range%20of%20settings.%0AHere%20we%20introduce%20Centaur%2C%20a%20computational%20model%20that%20can%20predict%20and%20simulate%0Ahuman%20behavior%20in%20any%20experiment%20expressible%20in%20natural%20language.%20We%20derived%0ACentaur%20by%20finetuning%20a%20state-of-the-art%20language%20model%20on%20a%20novel%2C%20large-scale%0Adata%20set%20called%20Psych-101.%20Psych-101%20reaches%20an%20unprecedented%20scale%2C%20covering%0Atrial-by-trial%20data%20from%20over%2060%2C000%20participants%20performing%20over%2010%2C000%2C000%0Achoices%20in%20160%20experiments.%20Centaur%20not%20only%20captures%20the%20behavior%20of%20held-out%0Aparticipants%20better%20than%20existing%20cognitive%20models%2C%20but%20also%20generalizes%20to%20new%0Acover%20stories%2C%20structural%20task%20modifications%2C%20and%20entirely%20new%20domains.%0AFurthermore%2C%20we%20find%20that%20the%20model%27s%20internal%20representations%20become%20more%0Aaligned%20with%20human%20neural%20activity%20after%20finetuning.%20Taken%20together%2C%20our%0Aresults%20demonstrate%20that%20it%20is%20possible%20to%20discover%20computational%20models%20that%0Acapture%20human%20behavior%20across%20a%20wide%20range%20of%20domains.%20We%20believe%20that%20such%0Amodels%20provide%20tremendous%20potential%20for%20guiding%20the%20development%20of%20cognitive%0Atheories%20and%20present%20a%20case%20study%20to%20demonstrate%20this.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20268v3&entry.124074799=Read"},
{"title": "Modelling of Underwater Vehicles using Physics-Informed Neural Networks\n  with Control", "author": "Abdelhakim Amer and David Felsager and Yury Brodskiy and Andriy Sarabakha", "abstract": "  Physics-informed neural networks (PINNs) integrate physical laws with\ndata-driven models to improve generalization and sample efficiency. This work\nintroduces an open-source implementation of the Physics-Informed Neural Network\nwith Control (PINC) framework, designed to model the dynamics of an underwater\nvehicle. Using initial states, control actions, and time inputs, PINC extends\nPINNs to enable physically consistent transitions beyond the training domain.\nVarious PINC configurations are tested, including differing loss functions,\ngradient-weighting schemes, and hyperparameters. Validation on a simulated\nunderwater vehicle demonstrates more accurate long-horizon predictions compared\nto a non-physics-informed baseline\n", "link": "http://arxiv.org/abs/2504.20019v1", "date": "2025-04-28", "relevancy": 2.0244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5182}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modelling%20of%20Underwater%20Vehicles%20using%20Physics-Informed%20Neural%20Networks%0A%20%20with%20Control&body=Title%3A%20Modelling%20of%20Underwater%20Vehicles%20using%20Physics-Informed%20Neural%20Networks%0A%20%20with%20Control%0AAuthor%3A%20Abdelhakim%20Amer%20and%20David%20Felsager%20and%20Yury%20Brodskiy%20and%20Andriy%20Sarabakha%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20integrate%20physical%20laws%20with%0Adata-driven%20models%20to%20improve%20generalization%20and%20sample%20efficiency.%20This%20work%0Aintroduces%20an%20open-source%20implementation%20of%20the%20Physics-Informed%20Neural%20Network%0Awith%20Control%20%28PINC%29%20framework%2C%20designed%20to%20model%20the%20dynamics%20of%20an%20underwater%0Avehicle.%20Using%20initial%20states%2C%20control%20actions%2C%20and%20time%20inputs%2C%20PINC%20extends%0APINNs%20to%20enable%20physically%20consistent%20transitions%20beyond%20the%20training%20domain.%0AVarious%20PINC%20configurations%20are%20tested%2C%20including%20differing%20loss%20functions%2C%0Agradient-weighting%20schemes%2C%20and%20hyperparameters.%20Validation%20on%20a%20simulated%0Aunderwater%20vehicle%20demonstrates%20more%20accurate%20long-horizon%20predictions%20compared%0Ato%20a%20non-physics-informed%20baseline%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModelling%2520of%2520Underwater%2520Vehicles%2520using%2520Physics-Informed%2520Neural%2520Networks%250A%2520%2520with%2520Control%26entry.906535625%3DAbdelhakim%2520Amer%2520and%2520David%2520Felsager%2520and%2520Yury%2520Brodskiy%2520and%2520Andriy%2520Sarabakha%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520integrate%2520physical%2520laws%2520with%250Adata-driven%2520models%2520to%2520improve%2520generalization%2520and%2520sample%2520efficiency.%2520This%2520work%250Aintroduces%2520an%2520open-source%2520implementation%2520of%2520the%2520Physics-Informed%2520Neural%2520Network%250Awith%2520Control%2520%2528PINC%2529%2520framework%252C%2520designed%2520to%2520model%2520the%2520dynamics%2520of%2520an%2520underwater%250Avehicle.%2520Using%2520initial%2520states%252C%2520control%2520actions%252C%2520and%2520time%2520inputs%252C%2520PINC%2520extends%250APINNs%2520to%2520enable%2520physically%2520consistent%2520transitions%2520beyond%2520the%2520training%2520domain.%250AVarious%2520PINC%2520configurations%2520are%2520tested%252C%2520including%2520differing%2520loss%2520functions%252C%250Agradient-weighting%2520schemes%252C%2520and%2520hyperparameters.%2520Validation%2520on%2520a%2520simulated%250Aunderwater%2520vehicle%2520demonstrates%2520more%2520accurate%2520long-horizon%2520predictions%2520compared%250Ato%2520a%2520non-physics-informed%2520baseline%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modelling%20of%20Underwater%20Vehicles%20using%20Physics-Informed%20Neural%20Networks%0A%20%20with%20Control&entry.906535625=Abdelhakim%20Amer%20and%20David%20Felsager%20and%20Yury%20Brodskiy%20and%20Andriy%20Sarabakha&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20integrate%20physical%20laws%20with%0Adata-driven%20models%20to%20improve%20generalization%20and%20sample%20efficiency.%20This%20work%0Aintroduces%20an%20open-source%20implementation%20of%20the%20Physics-Informed%20Neural%20Network%0Awith%20Control%20%28PINC%29%20framework%2C%20designed%20to%20model%20the%20dynamics%20of%20an%20underwater%0Avehicle.%20Using%20initial%20states%2C%20control%20actions%2C%20and%20time%20inputs%2C%20PINC%20extends%0APINNs%20to%20enable%20physically%20consistent%20transitions%20beyond%20the%20training%20domain.%0AVarious%20PINC%20configurations%20are%20tested%2C%20including%20differing%20loss%20functions%2C%0Agradient-weighting%20schemes%2C%20and%20hyperparameters.%20Validation%20on%20a%20simulated%0Aunderwater%20vehicle%20demonstrates%20more%20accurate%20long-horizon%20predictions%20compared%0Ato%20a%20non-physics-informed%20baseline%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20019v1&entry.124074799=Read"},
{"title": "AutoJudge: Judge Decoding Without Manual Annotation", "author": "Roman Garipov and Fedor Velikonivtsev and Ruslan Svirschevski and Vage Egiazarian and Max Ryabinin", "abstract": "  We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks.\n", "link": "http://arxiv.org/abs/2504.20039v1", "date": "2025-04-28", "relevancy": 2.021, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoJudge%3A%20Judge%20Decoding%20Without%20Manual%20Annotation&body=Title%3A%20AutoJudge%3A%20Judge%20Decoding%20Without%20Manual%20Annotation%0AAuthor%3A%20Roman%20Garipov%20and%20Fedor%20Velikonivtsev%20and%20Ruslan%20Svirschevski%20and%20Vage%20Egiazarian%20and%20Max%20Ryabinin%0AAbstract%3A%20%20%20We%20introduce%20AutoJudge%2C%20a%20framework%20that%20accelerates%20large%20language%20model%0A%28LLM%29%20inference%20with%20task-specific%20lossy%20speculative%20decoding.%20Instead%20of%0Amatching%20the%20original%20model%20output%20distribution%20token-by-token%2C%20we%20identify%0Awhich%20of%20the%20generated%20tokens%20affect%20the%20downstream%20quality%20of%20the%20generated%0Aresponse%2C%20relaxing%20the%20guarantee%20so%20that%20the%20%22unimportant%22%20tokens%20can%20be%0Agenerated%20faster.%20Our%20approach%20relies%20on%20a%20semi-greedy%20search%20algorithm%20to%20test%0Awhich%20of%20the%20mismatches%20between%20target%20and%20draft%20model%20should%20be%20corrected%20to%0Apreserve%20quality%2C%20and%20which%20ones%20may%20be%20skipped.%20We%20then%20train%20a%20lightweight%0Aclassifier%20based%20on%20existing%20LLM%20embeddings%20to%20predict%2C%20at%20inference%20time%2C%0Awhich%20mismatching%20tokens%20can%20be%20safely%20accepted%20without%20compromising%20the%20final%0Aanswer%20quality.%20We%20test%20our%20approach%20with%20Llama%203.2%201B%20%28draft%29%20and%20Llama%203.1%208B%0A%28target%29%20models%20on%20zero-shot%20GSM8K%20reasoning%2C%20where%20it%20achieves%20up%20to%201.5x%20more%0Aaccepted%20tokens%20per%20verification%20cycle%20with%20under%201%25%20degradation%20in%20answer%0Aaccuracy%20compared%20to%20standard%20speculative%20decoding%20and%20over%202x%20with%20small%20loss%0Ain%20accuracy.%20When%20applied%20to%20the%20LiveCodeBench%20benchmark%2C%20our%20approach%0Aautomatically%20detects%20other%2C%20programming-specific%20important%20tokens%20and%20shows%0Asimilar%20speedups%2C%20demonstrating%20its%20ability%20to%20generalize%20across%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoJudge%253A%2520Judge%2520Decoding%2520Without%2520Manual%2520Annotation%26entry.906535625%3DRoman%2520Garipov%2520and%2520Fedor%2520Velikonivtsev%2520and%2520Ruslan%2520Svirschevski%2520and%2520Vage%2520Egiazarian%2520and%2520Max%2520Ryabinin%26entry.1292438233%3D%2520%2520We%2520introduce%2520AutoJudge%252C%2520a%2520framework%2520that%2520accelerates%2520large%2520language%2520model%250A%2528LLM%2529%2520inference%2520with%2520task-specific%2520lossy%2520speculative%2520decoding.%2520Instead%2520of%250Amatching%2520the%2520original%2520model%2520output%2520distribution%2520token-by-token%252C%2520we%2520identify%250Awhich%2520of%2520the%2520generated%2520tokens%2520affect%2520the%2520downstream%2520quality%2520of%2520the%2520generated%250Aresponse%252C%2520relaxing%2520the%2520guarantee%2520so%2520that%2520the%2520%2522unimportant%2522%2520tokens%2520can%2520be%250Agenerated%2520faster.%2520Our%2520approach%2520relies%2520on%2520a%2520semi-greedy%2520search%2520algorithm%2520to%2520test%250Awhich%2520of%2520the%2520mismatches%2520between%2520target%2520and%2520draft%2520model%2520should%2520be%2520corrected%2520to%250Apreserve%2520quality%252C%2520and%2520which%2520ones%2520may%2520be%2520skipped.%2520We%2520then%2520train%2520a%2520lightweight%250Aclassifier%2520based%2520on%2520existing%2520LLM%2520embeddings%2520to%2520predict%252C%2520at%2520inference%2520time%252C%250Awhich%2520mismatching%2520tokens%2520can%2520be%2520safely%2520accepted%2520without%2520compromising%2520the%2520final%250Aanswer%2520quality.%2520We%2520test%2520our%2520approach%2520with%2520Llama%25203.2%25201B%2520%2528draft%2529%2520and%2520Llama%25203.1%25208B%250A%2528target%2529%2520models%2520on%2520zero-shot%2520GSM8K%2520reasoning%252C%2520where%2520it%2520achieves%2520up%2520to%25201.5x%2520more%250Aaccepted%2520tokens%2520per%2520verification%2520cycle%2520with%2520under%25201%2525%2520degradation%2520in%2520answer%250Aaccuracy%2520compared%2520to%2520standard%2520speculative%2520decoding%2520and%2520over%25202x%2520with%2520small%2520loss%250Ain%2520accuracy.%2520When%2520applied%2520to%2520the%2520LiveCodeBench%2520benchmark%252C%2520our%2520approach%250Aautomatically%2520detects%2520other%252C%2520programming-specific%2520important%2520tokens%2520and%2520shows%250Asimilar%2520speedups%252C%2520demonstrating%2520its%2520ability%2520to%2520generalize%2520across%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoJudge%3A%20Judge%20Decoding%20Without%20Manual%20Annotation&entry.906535625=Roman%20Garipov%20and%20Fedor%20Velikonivtsev%20and%20Ruslan%20Svirschevski%20and%20Vage%20Egiazarian%20and%20Max%20Ryabinin&entry.1292438233=%20%20We%20introduce%20AutoJudge%2C%20a%20framework%20that%20accelerates%20large%20language%20model%0A%28LLM%29%20inference%20with%20task-specific%20lossy%20speculative%20decoding.%20Instead%20of%0Amatching%20the%20original%20model%20output%20distribution%20token-by-token%2C%20we%20identify%0Awhich%20of%20the%20generated%20tokens%20affect%20the%20downstream%20quality%20of%20the%20generated%0Aresponse%2C%20relaxing%20the%20guarantee%20so%20that%20the%20%22unimportant%22%20tokens%20can%20be%0Agenerated%20faster.%20Our%20approach%20relies%20on%20a%20semi-greedy%20search%20algorithm%20to%20test%0Awhich%20of%20the%20mismatches%20between%20target%20and%20draft%20model%20should%20be%20corrected%20to%0Apreserve%20quality%2C%20and%20which%20ones%20may%20be%20skipped.%20We%20then%20train%20a%20lightweight%0Aclassifier%20based%20on%20existing%20LLM%20embeddings%20to%20predict%2C%20at%20inference%20time%2C%0Awhich%20mismatching%20tokens%20can%20be%20safely%20accepted%20without%20compromising%20the%20final%0Aanswer%20quality.%20We%20test%20our%20approach%20with%20Llama%203.2%201B%20%28draft%29%20and%20Llama%203.1%208B%0A%28target%29%20models%20on%20zero-shot%20GSM8K%20reasoning%2C%20where%20it%20achieves%20up%20to%201.5x%20more%0Aaccepted%20tokens%20per%20verification%20cycle%20with%20under%201%25%20degradation%20in%20answer%0Aaccuracy%20compared%20to%20standard%20speculative%20decoding%20and%20over%202x%20with%20small%20loss%0Ain%20accuracy.%20When%20applied%20to%20the%20LiveCodeBench%20benchmark%2C%20our%20approach%0Aautomatically%20detects%20other%2C%20programming-specific%20important%20tokens%20and%20shows%0Asimilar%20speedups%2C%20demonstrating%20its%20ability%20to%20generalize%20across%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20039v1&entry.124074799=Read"},
{"title": "Accelerated 3D-3D rigid registration of echocardiographic images\n  obtained from apical window using particle filter", "author": "Thanuja Uruththirakodeeswaran and Harald Becher and Michelle Noga and Lawrence H. Le and Pierre Boulanger and Jonathan Windram and Kumaradevan Punithakumar", "abstract": "  The perfect alignment of 3D echocardiographic images captured from various\nangles has improved image quality and broadened the field of view. This study\nproposes an accelerated sequential Monte Carlo (SMC) algorithm for 3D-3D rigid\nregistration of transthoracic echocardiographic images with significant and\nlimited overlap taken from apical window that is robust to the noise and\nintensity variation in ultrasound images. The algorithm estimates the\ntranslational and rotational components of the rigid transform through an\niterative process and requires an initial approximation of the rotation and\ntranslation limits. We perform registration in two ways: the image-based\nregistration computes the transform to align the end-diastolic frame of the\napical nonstandard image to the apical standard image and applies the same\ntransform to all frames of the cardiac cycle, whereas the mask-based\nregistration approach uses the binary masks of the left ventricle in the same\nway. The SMC and exhaustive search (EX) algorithms were evaluated for 4D\ntemporal sequences recorded from 7 volunteers who participated in a study\nconducted at the Mazankowski Alberta Heart Institute. The evaluations\ndemonstrate that the mask-based approach of the accelerated SMC yielded a Dice\nscore value of 0.819 +/- 0.045 for the left ventricle and gained 16.7x speedup\ncompared to the CPU version of the SMC algorithm.\n", "link": "http://arxiv.org/abs/2504.19930v1", "date": "2025-04-28", "relevancy": 2.0193, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.505}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.505}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%203D-3D%20rigid%20registration%20of%20echocardiographic%20images%0A%20%20obtained%20from%20apical%20window%20using%20particle%20filter&body=Title%3A%20Accelerated%203D-3D%20rigid%20registration%20of%20echocardiographic%20images%0A%20%20obtained%20from%20apical%20window%20using%20particle%20filter%0AAuthor%3A%20Thanuja%20Uruththirakodeeswaran%20and%20Harald%20Becher%20and%20Michelle%20Noga%20and%20Lawrence%20H.%20Le%20and%20Pierre%20Boulanger%20and%20Jonathan%20Windram%20and%20Kumaradevan%20Punithakumar%0AAbstract%3A%20%20%20The%20perfect%20alignment%20of%203D%20echocardiographic%20images%20captured%20from%20various%0Aangles%20has%20improved%20image%20quality%20and%20broadened%20the%20field%20of%20view.%20This%20study%0Aproposes%20an%20accelerated%20sequential%20Monte%20Carlo%20%28SMC%29%20algorithm%20for%203D-3D%20rigid%0Aregistration%20of%20transthoracic%20echocardiographic%20images%20with%20significant%20and%0Alimited%20overlap%20taken%20from%20apical%20window%20that%20is%20robust%20to%20the%20noise%20and%0Aintensity%20variation%20in%20ultrasound%20images.%20The%20algorithm%20estimates%20the%0Atranslational%20and%20rotational%20components%20of%20the%20rigid%20transform%20through%20an%0Aiterative%20process%20and%20requires%20an%20initial%20approximation%20of%20the%20rotation%20and%0Atranslation%20limits.%20We%20perform%20registration%20in%20two%20ways%3A%20the%20image-based%0Aregistration%20computes%20the%20transform%20to%20align%20the%20end-diastolic%20frame%20of%20the%0Aapical%20nonstandard%20image%20to%20the%20apical%20standard%20image%20and%20applies%20the%20same%0Atransform%20to%20all%20frames%20of%20the%20cardiac%20cycle%2C%20whereas%20the%20mask-based%0Aregistration%20approach%20uses%20the%20binary%20masks%20of%20the%20left%20ventricle%20in%20the%20same%0Away.%20The%20SMC%20and%20exhaustive%20search%20%28EX%29%20algorithms%20were%20evaluated%20for%204D%0Atemporal%20sequences%20recorded%20from%207%20volunteers%20who%20participated%20in%20a%20study%0Aconducted%20at%20the%20Mazankowski%20Alberta%20Heart%20Institute.%20The%20evaluations%0Ademonstrate%20that%20the%20mask-based%20approach%20of%20the%20accelerated%20SMC%20yielded%20a%20Dice%0Ascore%20value%20of%200.819%20%2B/-%200.045%20for%20the%20left%20ventricle%20and%20gained%2016.7x%20speedup%0Acompared%20to%20the%20CPU%20version%20of%20the%20SMC%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%25203D-3D%2520rigid%2520registration%2520of%2520echocardiographic%2520images%250A%2520%2520obtained%2520from%2520apical%2520window%2520using%2520particle%2520filter%26entry.906535625%3DThanuja%2520Uruththirakodeeswaran%2520and%2520Harald%2520Becher%2520and%2520Michelle%2520Noga%2520and%2520Lawrence%2520H.%2520Le%2520and%2520Pierre%2520Boulanger%2520and%2520Jonathan%2520Windram%2520and%2520Kumaradevan%2520Punithakumar%26entry.1292438233%3D%2520%2520The%2520perfect%2520alignment%2520of%25203D%2520echocardiographic%2520images%2520captured%2520from%2520various%250Aangles%2520has%2520improved%2520image%2520quality%2520and%2520broadened%2520the%2520field%2520of%2520view.%2520This%2520study%250Aproposes%2520an%2520accelerated%2520sequential%2520Monte%2520Carlo%2520%2528SMC%2529%2520algorithm%2520for%25203D-3D%2520rigid%250Aregistration%2520of%2520transthoracic%2520echocardiographic%2520images%2520with%2520significant%2520and%250Alimited%2520overlap%2520taken%2520from%2520apical%2520window%2520that%2520is%2520robust%2520to%2520the%2520noise%2520and%250Aintensity%2520variation%2520in%2520ultrasound%2520images.%2520The%2520algorithm%2520estimates%2520the%250Atranslational%2520and%2520rotational%2520components%2520of%2520the%2520rigid%2520transform%2520through%2520an%250Aiterative%2520process%2520and%2520requires%2520an%2520initial%2520approximation%2520of%2520the%2520rotation%2520and%250Atranslation%2520limits.%2520We%2520perform%2520registration%2520in%2520two%2520ways%253A%2520the%2520image-based%250Aregistration%2520computes%2520the%2520transform%2520to%2520align%2520the%2520end-diastolic%2520frame%2520of%2520the%250Aapical%2520nonstandard%2520image%2520to%2520the%2520apical%2520standard%2520image%2520and%2520applies%2520the%2520same%250Atransform%2520to%2520all%2520frames%2520of%2520the%2520cardiac%2520cycle%252C%2520whereas%2520the%2520mask-based%250Aregistration%2520approach%2520uses%2520the%2520binary%2520masks%2520of%2520the%2520left%2520ventricle%2520in%2520the%2520same%250Away.%2520The%2520SMC%2520and%2520exhaustive%2520search%2520%2528EX%2529%2520algorithms%2520were%2520evaluated%2520for%25204D%250Atemporal%2520sequences%2520recorded%2520from%25207%2520volunteers%2520who%2520participated%2520in%2520a%2520study%250Aconducted%2520at%2520the%2520Mazankowski%2520Alberta%2520Heart%2520Institute.%2520The%2520evaluations%250Ademonstrate%2520that%2520the%2520mask-based%2520approach%2520of%2520the%2520accelerated%2520SMC%2520yielded%2520a%2520Dice%250Ascore%2520value%2520of%25200.819%2520%252B/-%25200.045%2520for%2520the%2520left%2520ventricle%2520and%2520gained%252016.7x%2520speedup%250Acompared%2520to%2520the%2520CPU%2520version%2520of%2520the%2520SMC%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%203D-3D%20rigid%20registration%20of%20echocardiographic%20images%0A%20%20obtained%20from%20apical%20window%20using%20particle%20filter&entry.906535625=Thanuja%20Uruththirakodeeswaran%20and%20Harald%20Becher%20and%20Michelle%20Noga%20and%20Lawrence%20H.%20Le%20and%20Pierre%20Boulanger%20and%20Jonathan%20Windram%20and%20Kumaradevan%20Punithakumar&entry.1292438233=%20%20The%20perfect%20alignment%20of%203D%20echocardiographic%20images%20captured%20from%20various%0Aangles%20has%20improved%20image%20quality%20and%20broadened%20the%20field%20of%20view.%20This%20study%0Aproposes%20an%20accelerated%20sequential%20Monte%20Carlo%20%28SMC%29%20algorithm%20for%203D-3D%20rigid%0Aregistration%20of%20transthoracic%20echocardiographic%20images%20with%20significant%20and%0Alimited%20overlap%20taken%20from%20apical%20window%20that%20is%20robust%20to%20the%20noise%20and%0Aintensity%20variation%20in%20ultrasound%20images.%20The%20algorithm%20estimates%20the%0Atranslational%20and%20rotational%20components%20of%20the%20rigid%20transform%20through%20an%0Aiterative%20process%20and%20requires%20an%20initial%20approximation%20of%20the%20rotation%20and%0Atranslation%20limits.%20We%20perform%20registration%20in%20two%20ways%3A%20the%20image-based%0Aregistration%20computes%20the%20transform%20to%20align%20the%20end-diastolic%20frame%20of%20the%0Aapical%20nonstandard%20image%20to%20the%20apical%20standard%20image%20and%20applies%20the%20same%0Atransform%20to%20all%20frames%20of%20the%20cardiac%20cycle%2C%20whereas%20the%20mask-based%0Aregistration%20approach%20uses%20the%20binary%20masks%20of%20the%20left%20ventricle%20in%20the%20same%0Away.%20The%20SMC%20and%20exhaustive%20search%20%28EX%29%20algorithms%20were%20evaluated%20for%204D%0Atemporal%20sequences%20recorded%20from%207%20volunteers%20who%20participated%20in%20a%20study%0Aconducted%20at%20the%20Mazankowski%20Alberta%20Heart%20Institute.%20The%20evaluations%0Ademonstrate%20that%20the%20mask-based%20approach%20of%20the%20accelerated%20SMC%20yielded%20a%20Dice%0Ascore%20value%20of%200.819%20%2B/-%200.045%20for%20the%20left%20ventricle%20and%20gained%2016.7x%20speedup%0Acompared%20to%20the%20CPU%20version%20of%20the%20SMC%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19930v1&entry.124074799=Read"},
{"title": "Fitness Landscape of Large Language Model-Assisted Automated Algorithm\n  Search", "author": "Fei Liu and Qingfu Zhang and Xialiang Tong and Mingxuan Yuan and Kun Mao", "abstract": "  Large Language Models (LLMs) have demonstrated significant potential in\nalgorithm design. However, when integrated into search frameworks for iterative\nalgorithm search, the underlying fitness landscape--critical for understanding\nsearch behaviou--remains underexplored. In this paper, we illustrate and\nanalyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a\ngraph-based approach, where nodes represent algorithms and edges denote\ntransitions between them. We conduct extensive evaluations across six algorithm\ndesign tasks and six commonly used LLMs. Our findings reveal that LAS\nlandscapes are highly multimodal and rugged, particularly in combinatorial\noptimization tasks, with distinct structural variations across tasks and LLMs.\nFor instance, heuristic design tasks exhibit dense clusters of high-performing\nalgorithms, while symbolic regression tasks show sparse, scattered\ndistributions. Additionally, we demonstrate how population size influences\nexploration-exploitation trade-offs and the evolving trajectory of elite\nalgorithms. These insights not only advance our understanding of LAS landscapes\nbut also provide practical guidance for designing more effective LAS methods.\n", "link": "http://arxiv.org/abs/2504.19636v1", "date": "2025-04-28", "relevancy": 2.0189, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fitness%20Landscape%20of%20Large%20Language%20Model-Assisted%20Automated%20Algorithm%0A%20%20Search&body=Title%3A%20Fitness%20Landscape%20of%20Large%20Language%20Model-Assisted%20Automated%20Algorithm%0A%20%20Search%0AAuthor%3A%20Fei%20Liu%20and%20Qingfu%20Zhang%20and%20Xialiang%20Tong%20and%20Mingxuan%20Yuan%20and%20Kun%20Mao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%0Aalgorithm%20design.%20However%2C%20when%20integrated%20into%20search%20frameworks%20for%20iterative%0Aalgorithm%20search%2C%20the%20underlying%20fitness%20landscape--critical%20for%20understanding%0Asearch%20behaviou--remains%20underexplored.%20In%20this%20paper%2C%20we%20illustrate%20and%0Aanalyze%20the%20fitness%20landscape%20of%20LLM-assisted%20Algorithm%20Search%20%28LAS%29%20using%20a%0Agraph-based%20approach%2C%20where%20nodes%20represent%20algorithms%20and%20edges%20denote%0Atransitions%20between%20them.%20We%20conduct%20extensive%20evaluations%20across%20six%20algorithm%0Adesign%20tasks%20and%20six%20commonly%20used%20LLMs.%20Our%20findings%20reveal%20that%20LAS%0Alandscapes%20are%20highly%20multimodal%20and%20rugged%2C%20particularly%20in%20combinatorial%0Aoptimization%20tasks%2C%20with%20distinct%20structural%20variations%20across%20tasks%20and%20LLMs.%0AFor%20instance%2C%20heuristic%20design%20tasks%20exhibit%20dense%20clusters%20of%20high-performing%0Aalgorithms%2C%20while%20symbolic%20regression%20tasks%20show%20sparse%2C%20scattered%0Adistributions.%20Additionally%2C%20we%20demonstrate%20how%20population%20size%20influences%0Aexploration-exploitation%20trade-offs%20and%20the%20evolving%20trajectory%20of%20elite%0Aalgorithms.%20These%20insights%20not%20only%20advance%20our%20understanding%20of%20LAS%20landscapes%0Abut%20also%20provide%20practical%20guidance%20for%20designing%20more%20effective%20LAS%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFitness%2520Landscape%2520of%2520Large%2520Language%2520Model-Assisted%2520Automated%2520Algorithm%250A%2520%2520Search%26entry.906535625%3DFei%2520Liu%2520and%2520Qingfu%2520Zhang%2520and%2520Xialiang%2520Tong%2520and%2520Mingxuan%2520Yuan%2520and%2520Kun%2520Mao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520potential%2520in%250Aalgorithm%2520design.%2520However%252C%2520when%2520integrated%2520into%2520search%2520frameworks%2520for%2520iterative%250Aalgorithm%2520search%252C%2520the%2520underlying%2520fitness%2520landscape--critical%2520for%2520understanding%250Asearch%2520behaviou--remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520illustrate%2520and%250Aanalyze%2520the%2520fitness%2520landscape%2520of%2520LLM-assisted%2520Algorithm%2520Search%2520%2528LAS%2529%2520using%2520a%250Agraph-based%2520approach%252C%2520where%2520nodes%2520represent%2520algorithms%2520and%2520edges%2520denote%250Atransitions%2520between%2520them.%2520We%2520conduct%2520extensive%2520evaluations%2520across%2520six%2520algorithm%250Adesign%2520tasks%2520and%2520six%2520commonly%2520used%2520LLMs.%2520Our%2520findings%2520reveal%2520that%2520LAS%250Alandscapes%2520are%2520highly%2520multimodal%2520and%2520rugged%252C%2520particularly%2520in%2520combinatorial%250Aoptimization%2520tasks%252C%2520with%2520distinct%2520structural%2520variations%2520across%2520tasks%2520and%2520LLMs.%250AFor%2520instance%252C%2520heuristic%2520design%2520tasks%2520exhibit%2520dense%2520clusters%2520of%2520high-performing%250Aalgorithms%252C%2520while%2520symbolic%2520regression%2520tasks%2520show%2520sparse%252C%2520scattered%250Adistributions.%2520Additionally%252C%2520we%2520demonstrate%2520how%2520population%2520size%2520influences%250Aexploration-exploitation%2520trade-offs%2520and%2520the%2520evolving%2520trajectory%2520of%2520elite%250Aalgorithms.%2520These%2520insights%2520not%2520only%2520advance%2520our%2520understanding%2520of%2520LAS%2520landscapes%250Abut%2520also%2520provide%2520practical%2520guidance%2520for%2520designing%2520more%2520effective%2520LAS%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fitness%20Landscape%20of%20Large%20Language%20Model-Assisted%20Automated%20Algorithm%0A%20%20Search&entry.906535625=Fei%20Liu%20and%20Qingfu%20Zhang%20and%20Xialiang%20Tong%20and%20Mingxuan%20Yuan%20and%20Kun%20Mao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%0Aalgorithm%20design.%20However%2C%20when%20integrated%20into%20search%20frameworks%20for%20iterative%0Aalgorithm%20search%2C%20the%20underlying%20fitness%20landscape--critical%20for%20understanding%0Asearch%20behaviou--remains%20underexplored.%20In%20this%20paper%2C%20we%20illustrate%20and%0Aanalyze%20the%20fitness%20landscape%20of%20LLM-assisted%20Algorithm%20Search%20%28LAS%29%20using%20a%0Agraph-based%20approach%2C%20where%20nodes%20represent%20algorithms%20and%20edges%20denote%0Atransitions%20between%20them.%20We%20conduct%20extensive%20evaluations%20across%20six%20algorithm%0Adesign%20tasks%20and%20six%20commonly%20used%20LLMs.%20Our%20findings%20reveal%20that%20LAS%0Alandscapes%20are%20highly%20multimodal%20and%20rugged%2C%20particularly%20in%20combinatorial%0Aoptimization%20tasks%2C%20with%20distinct%20structural%20variations%20across%20tasks%20and%20LLMs.%0AFor%20instance%2C%20heuristic%20design%20tasks%20exhibit%20dense%20clusters%20of%20high-performing%0Aalgorithms%2C%20while%20symbolic%20regression%20tasks%20show%20sparse%2C%20scattered%0Adistributions.%20Additionally%2C%20we%20demonstrate%20how%20population%20size%20influences%0Aexploration-exploitation%20trade-offs%20and%20the%20evolving%20trajectory%20of%20elite%0Aalgorithms.%20These%20insights%20not%20only%20advance%20our%20understanding%20of%20LAS%20landscapes%0Abut%20also%20provide%20practical%20guidance%20for%20designing%20more%20effective%20LAS%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19636v1&entry.124074799=Read"},
{"title": "Keep your distance: learning dispersed embeddings on $\\mathbb{S}_d$", "author": "Evgeniia Tokarchuk and Hua Chang Bakker and Vlad Niculae", "abstract": "  Learning well-separated features in high-dimensional spaces, such as text or\nimage embeddings, is crucial for many machine learning applications. Achieving\nsuch separation can be effectively accomplished through the dispersion of\nembeddings, where unrelated vectors are pushed apart as much as possible. By\nconstraining features to be on a hypersphere, we can connect dispersion to\nwell-studied problems in mathematics and physics, where optimal solutions are\nknown for limited low-dimensional cases. However, in representation learning we\ntypically deal with a large number of features in high-dimensional space, and\nmoreover, dispersion is usually traded off with some other task-oriented\ntraining objective, making existing theoretical and numerical solutions\ninapplicable. Therefore, it is common to rely on gradient-based methods to\nencourage dispersion, usually by minimizing some function of the pairwise\ndistances. In this work, we first give an overview of existing methods from\ndisconnected literature, making new connections and highlighting similarities.\nNext, we introduce some new angles. We propose to reinterpret pairwise\ndispersion using a maximum mean discrepancy (MMD) motivation. We then propose\nan online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an\neffective alternative regularizer for dispersion on generic domains. Finally,\nwe derive a novel dispersion method that directly exploits properties of the\nhypersphere. Our experiments show the importance of dispersion in image\nclassification and natural language processing tasks, and how algorithms\nexhibit different trade-offs in different regimes.\n", "link": "http://arxiv.org/abs/2502.08231v2", "date": "2025-04-28", "relevancy": 2.0053, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5129}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keep%20your%20distance%3A%20learning%20dispersed%20embeddings%20on%20%24%5Cmathbb%7BS%7D_d%24&body=Title%3A%20Keep%20your%20distance%3A%20learning%20dispersed%20embeddings%20on%20%24%5Cmathbb%7BS%7D_d%24%0AAuthor%3A%20Evgeniia%20Tokarchuk%20and%20Hua%20Chang%20Bakker%20and%20Vlad%20Niculae%0AAbstract%3A%20%20%20Learning%20well-separated%20features%20in%20high-dimensional%20spaces%2C%20such%20as%20text%20or%0Aimage%20embeddings%2C%20is%20crucial%20for%20many%20machine%20learning%20applications.%20Achieving%0Asuch%20separation%20can%20be%20effectively%20accomplished%20through%20the%20dispersion%20of%0Aembeddings%2C%20where%20unrelated%20vectors%20are%20pushed%20apart%20as%20much%20as%20possible.%20By%0Aconstraining%20features%20to%20be%20on%20a%20hypersphere%2C%20we%20can%20connect%20dispersion%20to%0Awell-studied%20problems%20in%20mathematics%20and%20physics%2C%20where%20optimal%20solutions%20are%0Aknown%20for%20limited%20low-dimensional%20cases.%20However%2C%20in%20representation%20learning%20we%0Atypically%20deal%20with%20a%20large%20number%20of%20features%20in%20high-dimensional%20space%2C%20and%0Amoreover%2C%20dispersion%20is%20usually%20traded%20off%20with%20some%20other%20task-oriented%0Atraining%20objective%2C%20making%20existing%20theoretical%20and%20numerical%20solutions%0Ainapplicable.%20Therefore%2C%20it%20is%20common%20to%20rely%20on%20gradient-based%20methods%20to%0Aencourage%20dispersion%2C%20usually%20by%20minimizing%20some%20function%20of%20the%20pairwise%0Adistances.%20In%20this%20work%2C%20we%20first%20give%20an%20overview%20of%20existing%20methods%20from%0Adisconnected%20literature%2C%20making%20new%20connections%20and%20highlighting%20similarities.%0ANext%2C%20we%20introduce%20some%20new%20angles.%20We%20propose%20to%20reinterpret%20pairwise%0Adispersion%20using%20a%20maximum%20mean%20discrepancy%20%28MMD%29%20motivation.%20We%20then%20propose%0Aan%20online%20variant%20of%20the%20celebrated%20Lloyd%27s%20algorithm%2C%20of%20K-Means%20fame%2C%20as%20an%0Aeffective%20alternative%20regularizer%20for%20dispersion%20on%20generic%20domains.%20Finally%2C%0Awe%20derive%20a%20novel%20dispersion%20method%20that%20directly%20exploits%20properties%20of%20the%0Ahypersphere.%20Our%20experiments%20show%20the%20importance%20of%20dispersion%20in%20image%0Aclassification%20and%20natural%20language%20processing%20tasks%2C%20and%20how%20algorithms%0Aexhibit%20different%20trade-offs%20in%20different%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeep%2520your%2520distance%253A%2520learning%2520dispersed%2520embeddings%2520on%2520%2524%255Cmathbb%257BS%257D_d%2524%26entry.906535625%3DEvgeniia%2520Tokarchuk%2520and%2520Hua%2520Chang%2520Bakker%2520and%2520Vlad%2520Niculae%26entry.1292438233%3D%2520%2520Learning%2520well-separated%2520features%2520in%2520high-dimensional%2520spaces%252C%2520such%2520as%2520text%2520or%250Aimage%2520embeddings%252C%2520is%2520crucial%2520for%2520many%2520machine%2520learning%2520applications.%2520Achieving%250Asuch%2520separation%2520can%2520be%2520effectively%2520accomplished%2520through%2520the%2520dispersion%2520of%250Aembeddings%252C%2520where%2520unrelated%2520vectors%2520are%2520pushed%2520apart%2520as%2520much%2520as%2520possible.%2520By%250Aconstraining%2520features%2520to%2520be%2520on%2520a%2520hypersphere%252C%2520we%2520can%2520connect%2520dispersion%2520to%250Awell-studied%2520problems%2520in%2520mathematics%2520and%2520physics%252C%2520where%2520optimal%2520solutions%2520are%250Aknown%2520for%2520limited%2520low-dimensional%2520cases.%2520However%252C%2520in%2520representation%2520learning%2520we%250Atypically%2520deal%2520with%2520a%2520large%2520number%2520of%2520features%2520in%2520high-dimensional%2520space%252C%2520and%250Amoreover%252C%2520dispersion%2520is%2520usually%2520traded%2520off%2520with%2520some%2520other%2520task-oriented%250Atraining%2520objective%252C%2520making%2520existing%2520theoretical%2520and%2520numerical%2520solutions%250Ainapplicable.%2520Therefore%252C%2520it%2520is%2520common%2520to%2520rely%2520on%2520gradient-based%2520methods%2520to%250Aencourage%2520dispersion%252C%2520usually%2520by%2520minimizing%2520some%2520function%2520of%2520the%2520pairwise%250Adistances.%2520In%2520this%2520work%252C%2520we%2520first%2520give%2520an%2520overview%2520of%2520existing%2520methods%2520from%250Adisconnected%2520literature%252C%2520making%2520new%2520connections%2520and%2520highlighting%2520similarities.%250ANext%252C%2520we%2520introduce%2520some%2520new%2520angles.%2520We%2520propose%2520to%2520reinterpret%2520pairwise%250Adispersion%2520using%2520a%2520maximum%2520mean%2520discrepancy%2520%2528MMD%2529%2520motivation.%2520We%2520then%2520propose%250Aan%2520online%2520variant%2520of%2520the%2520celebrated%2520Lloyd%2527s%2520algorithm%252C%2520of%2520K-Means%2520fame%252C%2520as%2520an%250Aeffective%2520alternative%2520regularizer%2520for%2520dispersion%2520on%2520generic%2520domains.%2520Finally%252C%250Awe%2520derive%2520a%2520novel%2520dispersion%2520method%2520that%2520directly%2520exploits%2520properties%2520of%2520the%250Ahypersphere.%2520Our%2520experiments%2520show%2520the%2520importance%2520of%2520dispersion%2520in%2520image%250Aclassification%2520and%2520natural%2520language%2520processing%2520tasks%252C%2520and%2520how%2520algorithms%250Aexhibit%2520different%2520trade-offs%2520in%2520different%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keep%20your%20distance%3A%20learning%20dispersed%20embeddings%20on%20%24%5Cmathbb%7BS%7D_d%24&entry.906535625=Evgeniia%20Tokarchuk%20and%20Hua%20Chang%20Bakker%20and%20Vlad%20Niculae&entry.1292438233=%20%20Learning%20well-separated%20features%20in%20high-dimensional%20spaces%2C%20such%20as%20text%20or%0Aimage%20embeddings%2C%20is%20crucial%20for%20many%20machine%20learning%20applications.%20Achieving%0Asuch%20separation%20can%20be%20effectively%20accomplished%20through%20the%20dispersion%20of%0Aembeddings%2C%20where%20unrelated%20vectors%20are%20pushed%20apart%20as%20much%20as%20possible.%20By%0Aconstraining%20features%20to%20be%20on%20a%20hypersphere%2C%20we%20can%20connect%20dispersion%20to%0Awell-studied%20problems%20in%20mathematics%20and%20physics%2C%20where%20optimal%20solutions%20are%0Aknown%20for%20limited%20low-dimensional%20cases.%20However%2C%20in%20representation%20learning%20we%0Atypically%20deal%20with%20a%20large%20number%20of%20features%20in%20high-dimensional%20space%2C%20and%0Amoreover%2C%20dispersion%20is%20usually%20traded%20off%20with%20some%20other%20task-oriented%0Atraining%20objective%2C%20making%20existing%20theoretical%20and%20numerical%20solutions%0Ainapplicable.%20Therefore%2C%20it%20is%20common%20to%20rely%20on%20gradient-based%20methods%20to%0Aencourage%20dispersion%2C%20usually%20by%20minimizing%20some%20function%20of%20the%20pairwise%0Adistances.%20In%20this%20work%2C%20we%20first%20give%20an%20overview%20of%20existing%20methods%20from%0Adisconnected%20literature%2C%20making%20new%20connections%20and%20highlighting%20similarities.%0ANext%2C%20we%20introduce%20some%20new%20angles.%20We%20propose%20to%20reinterpret%20pairwise%0Adispersion%20using%20a%20maximum%20mean%20discrepancy%20%28MMD%29%20motivation.%20We%20then%20propose%0Aan%20online%20variant%20of%20the%20celebrated%20Lloyd%27s%20algorithm%2C%20of%20K-Means%20fame%2C%20as%20an%0Aeffective%20alternative%20regularizer%20for%20dispersion%20on%20generic%20domains.%20Finally%2C%0Awe%20derive%20a%20novel%20dispersion%20method%20that%20directly%20exploits%20properties%20of%20the%0Ahypersphere.%20Our%20experiments%20show%20the%20importance%20of%20dispersion%20in%20image%0Aclassification%20and%20natural%20language%20processing%20tasks%2C%20and%20how%20algorithms%0Aexhibit%20different%20trade-offs%20in%20different%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08231v2&entry.124074799=Read"},
{"title": "LightRAG: Simple and Fast Retrieval-Augmented Generation", "author": "Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang", "abstract": "  Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG\n", "link": "http://arxiv.org/abs/2410.05779v3", "date": "2025-04-28", "relevancy": 2.0051, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5176}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5174}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightRAG%3A%20Simple%20and%20Fast%20Retrieval-Augmented%20Generation&body=Title%3A%20LightRAG%3A%20Simple%20and%20Fast%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Zirui%20Guo%20and%20Lianghao%20Xia%20and%20Yanhua%20Yu%20and%20Tu%20Ao%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20enhance%20large%20language%20models%0A%28LLMs%29%20by%20integrating%20external%20knowledge%20sources%2C%20enabling%20more%20accurate%20and%0Acontextually%20relevant%20responses%20tailored%20to%20user%20needs.%20However%2C%20existing%20RAG%0Asystems%20have%20significant%20limitations%2C%20including%20reliance%20on%20flat%20data%0Arepresentations%20and%20inadequate%20contextual%20awareness%2C%20which%20can%20lead%20to%0Afragmented%20answers%20that%20fail%20to%20capture%20complex%20inter-dependencies.%20To%20address%0Athese%20challenges%2C%20we%20propose%20LightRAG%2C%20which%20incorporates%20graph%20structures%20into%0Atext%20indexing%20and%20retrieval%20processes.%20This%20innovative%20framework%20employs%20a%0Adual-level%20retrieval%20system%20that%20enhances%20comprehensive%20information%20retrieval%0Afrom%20both%20low-level%20and%20high-level%20knowledge%20discovery.%20Additionally%2C%20the%0Aintegration%20of%20graph%20structures%20with%20vector%20representations%20facilitates%0Aefficient%20retrieval%20of%20related%20entities%20and%20their%20relationships%2C%20significantly%0Aimproving%20response%20times%20while%20maintaining%20contextual%20relevance.%20This%0Acapability%20is%20further%20enhanced%20by%20an%20incremental%20update%20algorithm%20that%20ensures%0Athe%20timely%20integration%20of%20new%20data%2C%20allowing%20the%20system%20to%20remain%20effective%20and%0Aresponsive%20in%20rapidly%20changing%20data%20environments.%20Extensive%20experimental%0Avalidation%20demonstrates%20considerable%20improvements%20in%20retrieval%20accuracy%20and%0Aefficiency%20compared%20to%20existing%20approaches.%20We%20have%20made%20our%20LightRAG%0Aopen-source%20and%20available%20at%20the%20link%3A%20https%3A//github.com/HKUDS/LightRAG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05779v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightRAG%253A%2520Simple%2520and%2520Fast%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DZirui%2520Guo%2520and%2520Lianghao%2520Xia%2520and%2520Yanhua%2520Yu%2520and%2520Tu%2520Ao%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520enhance%2520large%2520language%2520models%250A%2528LLMs%2529%2520by%2520integrating%2520external%2520knowledge%2520sources%252C%2520enabling%2520more%2520accurate%2520and%250Acontextually%2520relevant%2520responses%2520tailored%2520to%2520user%2520needs.%2520However%252C%2520existing%2520RAG%250Asystems%2520have%2520significant%2520limitations%252C%2520including%2520reliance%2520on%2520flat%2520data%250Arepresentations%2520and%2520inadequate%2520contextual%2520awareness%252C%2520which%2520can%2520lead%2520to%250Afragmented%2520answers%2520that%2520fail%2520to%2520capture%2520complex%2520inter-dependencies.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520LightRAG%252C%2520which%2520incorporates%2520graph%2520structures%2520into%250Atext%2520indexing%2520and%2520retrieval%2520processes.%2520This%2520innovative%2520framework%2520employs%2520a%250Adual-level%2520retrieval%2520system%2520that%2520enhances%2520comprehensive%2520information%2520retrieval%250Afrom%2520both%2520low-level%2520and%2520high-level%2520knowledge%2520discovery.%2520Additionally%252C%2520the%250Aintegration%2520of%2520graph%2520structures%2520with%2520vector%2520representations%2520facilitates%250Aefficient%2520retrieval%2520of%2520related%2520entities%2520and%2520their%2520relationships%252C%2520significantly%250Aimproving%2520response%2520times%2520while%2520maintaining%2520contextual%2520relevance.%2520This%250Acapability%2520is%2520further%2520enhanced%2520by%2520an%2520incremental%2520update%2520algorithm%2520that%2520ensures%250Athe%2520timely%2520integration%2520of%2520new%2520data%252C%2520allowing%2520the%2520system%2520to%2520remain%2520effective%2520and%250Aresponsive%2520in%2520rapidly%2520changing%2520data%2520environments.%2520Extensive%2520experimental%250Avalidation%2520demonstrates%2520considerable%2520improvements%2520in%2520retrieval%2520accuracy%2520and%250Aefficiency%2520compared%2520to%2520existing%2520approaches.%2520We%2520have%2520made%2520our%2520LightRAG%250Aopen-source%2520and%2520available%2520at%2520the%2520link%253A%2520https%253A//github.com/HKUDS/LightRAG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05779v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightRAG%3A%20Simple%20and%20Fast%20Retrieval-Augmented%20Generation&entry.906535625=Zirui%20Guo%20and%20Lianghao%20Xia%20and%20Yanhua%20Yu%20and%20Tu%20Ao%20and%20Chao%20Huang&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20enhance%20large%20language%20models%0A%28LLMs%29%20by%20integrating%20external%20knowledge%20sources%2C%20enabling%20more%20accurate%20and%0Acontextually%20relevant%20responses%20tailored%20to%20user%20needs.%20However%2C%20existing%20RAG%0Asystems%20have%20significant%20limitations%2C%20including%20reliance%20on%20flat%20data%0Arepresentations%20and%20inadequate%20contextual%20awareness%2C%20which%20can%20lead%20to%0Afragmented%20answers%20that%20fail%20to%20capture%20complex%20inter-dependencies.%20To%20address%0Athese%20challenges%2C%20we%20propose%20LightRAG%2C%20which%20incorporates%20graph%20structures%20into%0Atext%20indexing%20and%20retrieval%20processes.%20This%20innovative%20framework%20employs%20a%0Adual-level%20retrieval%20system%20that%20enhances%20comprehensive%20information%20retrieval%0Afrom%20both%20low-level%20and%20high-level%20knowledge%20discovery.%20Additionally%2C%20the%0Aintegration%20of%20graph%20structures%20with%20vector%20representations%20facilitates%0Aefficient%20retrieval%20of%20related%20entities%20and%20their%20relationships%2C%20significantly%0Aimproving%20response%20times%20while%20maintaining%20contextual%20relevance.%20This%0Acapability%20is%20further%20enhanced%20by%20an%20incremental%20update%20algorithm%20that%20ensures%0Athe%20timely%20integration%20of%20new%20data%2C%20allowing%20the%20system%20to%20remain%20effective%20and%0Aresponsive%20in%20rapidly%20changing%20data%20environments.%20Extensive%20experimental%0Avalidation%20demonstrates%20considerable%20improvements%20in%20retrieval%20accuracy%20and%0Aefficiency%20compared%20to%20existing%20approaches.%20We%20have%20made%20our%20LightRAG%0Aopen-source%20and%20available%20at%20the%20link%3A%20https%3A//github.com/HKUDS/LightRAG%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05779v3&entry.124074799=Read"},
{"title": "From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI\n  Synthesis", "author": "Kristofer Grover Roos and Atsushi Fukuda and Quan Huu Cap", "abstract": "  While functional magnetic resonance imaging (fMRI) offers valuable insights\ninto brain activity, it is limited by high operational costs and significant\ninfrastructural demands. In contrast, electroencephalography (EEG) provides\nmillisecond-level precision in capturing electrical activity but lacks the\nspatial fidelity necessary for precise neural localization. To bridge these\ngaps, we propose E2fNet, a simple yet effective deep learning model for\nsynthesizing fMRI images from low-cost EEG data. E2fNet is an encoder-decoder\nnetwork specifically designed to capture and translate meaningful multi-scale\nfeatures from EEG across electrode channels into accurate fMRI representations.\nExtensive evaluations across three public datasets demonstrate that E2fNet\nconsistently outperforms existing CNN- and transformer-based methods, achieving\nstate-of-the-art results in terms of the structural similarity index measure\n(SSIM). These results demonstrate that E2fNet is a promising, cost-effective\nsolution for enhancing neuroimaging capabilities. The code is available at\nhttps://github.com/kgr20/E2fNet.\n", "link": "http://arxiv.org/abs/2502.08025v3", "date": "2025-04-28", "relevancy": 1.9914, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5186}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.494}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Brainwaves%20to%20Brain%20Scans%3A%20A%20Robust%20Neural%20Network%20for%20EEG-to-fMRI%0A%20%20Synthesis&body=Title%3A%20From%20Brainwaves%20to%20Brain%20Scans%3A%20A%20Robust%20Neural%20Network%20for%20EEG-to-fMRI%0A%20%20Synthesis%0AAuthor%3A%20Kristofer%20Grover%20Roos%20and%20Atsushi%20Fukuda%20and%20Quan%20Huu%20Cap%0AAbstract%3A%20%20%20While%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20offers%20valuable%20insights%0Ainto%20brain%20activity%2C%20it%20is%20limited%20by%20high%20operational%20costs%20and%20significant%0Ainfrastructural%20demands.%20In%20contrast%2C%20electroencephalography%20%28EEG%29%20provides%0Amillisecond-level%20precision%20in%20capturing%20electrical%20activity%20but%20lacks%20the%0Aspatial%20fidelity%20necessary%20for%20precise%20neural%20localization.%20To%20bridge%20these%0Agaps%2C%20we%20propose%20E2fNet%2C%20a%20simple%20yet%20effective%20deep%20learning%20model%20for%0Asynthesizing%20fMRI%20images%20from%20low-cost%20EEG%20data.%20E2fNet%20is%20an%20encoder-decoder%0Anetwork%20specifically%20designed%20to%20capture%20and%20translate%20meaningful%20multi-scale%0Afeatures%20from%20EEG%20across%20electrode%20channels%20into%20accurate%20fMRI%20representations.%0AExtensive%20evaluations%20across%20three%20public%20datasets%20demonstrate%20that%20E2fNet%0Aconsistently%20outperforms%20existing%20CNN-%20and%20transformer-based%20methods%2C%20achieving%0Astate-of-the-art%20results%20in%20terms%20of%20the%20structural%20similarity%20index%20measure%0A%28SSIM%29.%20These%20results%20demonstrate%20that%20E2fNet%20is%20a%20promising%2C%20cost-effective%0Asolution%20for%20enhancing%20neuroimaging%20capabilities.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/kgr20/E2fNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08025v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Brainwaves%2520to%2520Brain%2520Scans%253A%2520A%2520Robust%2520Neural%2520Network%2520for%2520EEG-to-fMRI%250A%2520%2520Synthesis%26entry.906535625%3DKristofer%2520Grover%2520Roos%2520and%2520Atsushi%2520Fukuda%2520and%2520Quan%2520Huu%2520Cap%26entry.1292438233%3D%2520%2520While%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520offers%2520valuable%2520insights%250Ainto%2520brain%2520activity%252C%2520it%2520is%2520limited%2520by%2520high%2520operational%2520costs%2520and%2520significant%250Ainfrastructural%2520demands.%2520In%2520contrast%252C%2520electroencephalography%2520%2528EEG%2529%2520provides%250Amillisecond-level%2520precision%2520in%2520capturing%2520electrical%2520activity%2520but%2520lacks%2520the%250Aspatial%2520fidelity%2520necessary%2520for%2520precise%2520neural%2520localization.%2520To%2520bridge%2520these%250Agaps%252C%2520we%2520propose%2520E2fNet%252C%2520a%2520simple%2520yet%2520effective%2520deep%2520learning%2520model%2520for%250Asynthesizing%2520fMRI%2520images%2520from%2520low-cost%2520EEG%2520data.%2520E2fNet%2520is%2520an%2520encoder-decoder%250Anetwork%2520specifically%2520designed%2520to%2520capture%2520and%2520translate%2520meaningful%2520multi-scale%250Afeatures%2520from%2520EEG%2520across%2520electrode%2520channels%2520into%2520accurate%2520fMRI%2520representations.%250AExtensive%2520evaluations%2520across%2520three%2520public%2520datasets%2520demonstrate%2520that%2520E2fNet%250Aconsistently%2520outperforms%2520existing%2520CNN-%2520and%2520transformer-based%2520methods%252C%2520achieving%250Astate-of-the-art%2520results%2520in%2520terms%2520of%2520the%2520structural%2520similarity%2520index%2520measure%250A%2528SSIM%2529.%2520These%2520results%2520demonstrate%2520that%2520E2fNet%2520is%2520a%2520promising%252C%2520cost-effective%250Asolution%2520for%2520enhancing%2520neuroimaging%2520capabilities.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/kgr20/E2fNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08025v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Brainwaves%20to%20Brain%20Scans%3A%20A%20Robust%20Neural%20Network%20for%20EEG-to-fMRI%0A%20%20Synthesis&entry.906535625=Kristofer%20Grover%20Roos%20and%20Atsushi%20Fukuda%20and%20Quan%20Huu%20Cap&entry.1292438233=%20%20While%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20offers%20valuable%20insights%0Ainto%20brain%20activity%2C%20it%20is%20limited%20by%20high%20operational%20costs%20and%20significant%0Ainfrastructural%20demands.%20In%20contrast%2C%20electroencephalography%20%28EEG%29%20provides%0Amillisecond-level%20precision%20in%20capturing%20electrical%20activity%20but%20lacks%20the%0Aspatial%20fidelity%20necessary%20for%20precise%20neural%20localization.%20To%20bridge%20these%0Agaps%2C%20we%20propose%20E2fNet%2C%20a%20simple%20yet%20effective%20deep%20learning%20model%20for%0Asynthesizing%20fMRI%20images%20from%20low-cost%20EEG%20data.%20E2fNet%20is%20an%20encoder-decoder%0Anetwork%20specifically%20designed%20to%20capture%20and%20translate%20meaningful%20multi-scale%0Afeatures%20from%20EEG%20across%20electrode%20channels%20into%20accurate%20fMRI%20representations.%0AExtensive%20evaluations%20across%20three%20public%20datasets%20demonstrate%20that%20E2fNet%0Aconsistently%20outperforms%20existing%20CNN-%20and%20transformer-based%20methods%2C%20achieving%0Astate-of-the-art%20results%20in%20terms%20of%20the%20structural%20similarity%20index%20measure%0A%28SSIM%29.%20These%20results%20demonstrate%20that%20E2fNet%20is%20a%20promising%2C%20cost-effective%0Asolution%20for%20enhancing%20neuroimaging%20capabilities.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/kgr20/E2fNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08025v3&entry.124074799=Read"},
{"title": "Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous\n  Driving", "author": "Daniel Bogdoll and Jan Imhof and Tim Joseph and Svetlana Pavlitska and J. Marius Z\u00f6llner", "abstract": "  In autonomous driving, the most challenging scenarios can only be detected\nwithin their temporal context. Most video anomaly detection approaches focus\neither on surveillance or traffic accidents, which are only a subfield of\nautonomous driving. We present HF$^2$-VAD$_{AD}$, a variation of the HF$^2$-VAD\nsurveillance video anomaly detection method for autonomous driving. We learn a\nrepresentation of normality from a vehicle's ego perspective and evaluate\npixel-wise anomaly detections in rare and critical scenarios.\n", "link": "http://arxiv.org/abs/2406.06423v3", "date": "2025-04-28", "relevancy": 1.9835, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4989}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.496}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Video%20Anomaly%20Detection%20for%20Anomalous%20Scenarios%20in%20Autonomous%0A%20%20Driving&body=Title%3A%20Hybrid%20Video%20Anomaly%20Detection%20for%20Anomalous%20Scenarios%20in%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Daniel%20Bogdoll%20and%20Jan%20Imhof%20and%20Tim%20Joseph%20and%20Svetlana%20Pavlitska%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20the%20most%20challenging%20scenarios%20can%20only%20be%20detected%0Awithin%20their%20temporal%20context.%20Most%20video%20anomaly%20detection%20approaches%20focus%0Aeither%20on%20surveillance%20or%20traffic%20accidents%2C%20which%20are%20only%20a%20subfield%20of%0Aautonomous%20driving.%20We%20present%20HF%24%5E2%24-VAD%24_%7BAD%7D%24%2C%20a%20variation%20of%20the%20HF%24%5E2%24-VAD%0Asurveillance%20video%20anomaly%20detection%20method%20for%20autonomous%20driving.%20We%20learn%20a%0Arepresentation%20of%20normality%20from%20a%20vehicle%27s%20ego%20perspective%20and%20evaluate%0Apixel-wise%20anomaly%20detections%20in%20rare%20and%20critical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06423v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Video%2520Anomaly%2520Detection%2520for%2520Anomalous%2520Scenarios%2520in%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DDaniel%2520Bogdoll%2520and%2520Jan%2520Imhof%2520and%2520Tim%2520Joseph%2520and%2520Svetlana%2520Pavlitska%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520the%2520most%2520challenging%2520scenarios%2520can%2520only%2520be%2520detected%250Awithin%2520their%2520temporal%2520context.%2520Most%2520video%2520anomaly%2520detection%2520approaches%2520focus%250Aeither%2520on%2520surveillance%2520or%2520traffic%2520accidents%252C%2520which%2520are%2520only%2520a%2520subfield%2520of%250Aautonomous%2520driving.%2520We%2520present%2520HF%2524%255E2%2524-VAD%2524_%257BAD%257D%2524%252C%2520a%2520variation%2520of%2520the%2520HF%2524%255E2%2524-VAD%250Asurveillance%2520video%2520anomaly%2520detection%2520method%2520for%2520autonomous%2520driving.%2520We%2520learn%2520a%250Arepresentation%2520of%2520normality%2520from%2520a%2520vehicle%2527s%2520ego%2520perspective%2520and%2520evaluate%250Apixel-wise%2520anomaly%2520detections%2520in%2520rare%2520and%2520critical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06423v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Video%20Anomaly%20Detection%20for%20Anomalous%20Scenarios%20in%20Autonomous%0A%20%20Driving&entry.906535625=Daniel%20Bogdoll%20and%20Jan%20Imhof%20and%20Tim%20Joseph%20and%20Svetlana%20Pavlitska%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20In%20autonomous%20driving%2C%20the%20most%20challenging%20scenarios%20can%20only%20be%20detected%0Awithin%20their%20temporal%20context.%20Most%20video%20anomaly%20detection%20approaches%20focus%0Aeither%20on%20surveillance%20or%20traffic%20accidents%2C%20which%20are%20only%20a%20subfield%20of%0Aautonomous%20driving.%20We%20present%20HF%24%5E2%24-VAD%24_%7BAD%7D%24%2C%20a%20variation%20of%20the%20HF%24%5E2%24-VAD%0Asurveillance%20video%20anomaly%20detection%20method%20for%20autonomous%20driving.%20We%20learn%20a%0Arepresentation%20of%20normality%20from%20a%20vehicle%27s%20ego%20perspective%20and%20evaluate%0Apixel-wise%20anomaly%20detections%20in%20rare%20and%20critical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06423v3&entry.124074799=Read"},
{"title": "Towards Automated Scoping of AI for Social Good Projects", "author": "Jacob Emmerson and Rayid Ghani and Zheyuan Ryan Shi", "abstract": "  Artificial Intelligence for Social Good (AI4SG) is an emerging effort that\naims to address complex societal challenges with the powerful capabilities of\nAI systems. These challenges range from local issues with transit networks to\nglobal wildlife preservation. However, regardless of scale, a critical\nbottleneck for many AI4SG initiatives is the laborious process of problem\nscoping -- a complex and resource-intensive task -- due to a scarcity of\nprofessionals with both technical and domain expertise. Given the remarkable\napplications of large language models (LLM), we propose a Problem Scoping Agent\n(PSA) that uses an LLM to generate comprehensive project proposals grounded in\nscientific literature and real-world knowledge. We demonstrate that our PSA\nframework generates proposals comparable to those written by experts through a\nblind review and AI evaluations. Finally, we document the challenges of\nreal-world problem scoping and note several areas for future work.\n", "link": "http://arxiv.org/abs/2504.20010v1", "date": "2025-04-28", "relevancy": 1.9676, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Automated%20Scoping%20of%20AI%20for%20Social%20Good%20Projects&body=Title%3A%20Towards%20Automated%20Scoping%20of%20AI%20for%20Social%20Good%20Projects%0AAuthor%3A%20Jacob%20Emmerson%20and%20Rayid%20Ghani%20and%20Zheyuan%20Ryan%20Shi%0AAbstract%3A%20%20%20Artificial%20Intelligence%20for%20Social%20Good%20%28AI4SG%29%20is%20an%20emerging%20effort%20that%0Aaims%20to%20address%20complex%20societal%20challenges%20with%20the%20powerful%20capabilities%20of%0AAI%20systems.%20These%20challenges%20range%20from%20local%20issues%20with%20transit%20networks%20to%0Aglobal%20wildlife%20preservation.%20However%2C%20regardless%20of%20scale%2C%20a%20critical%0Abottleneck%20for%20many%20AI4SG%20initiatives%20is%20the%20laborious%20process%20of%20problem%0Ascoping%20--%20a%20complex%20and%20resource-intensive%20task%20--%20due%20to%20a%20scarcity%20of%0Aprofessionals%20with%20both%20technical%20and%20domain%20expertise.%20Given%20the%20remarkable%0Aapplications%20of%20large%20language%20models%20%28LLM%29%2C%20we%20propose%20a%20Problem%20Scoping%20Agent%0A%28PSA%29%20that%20uses%20an%20LLM%20to%20generate%20comprehensive%20project%20proposals%20grounded%20in%0Ascientific%20literature%20and%20real-world%20knowledge.%20We%20demonstrate%20that%20our%20PSA%0Aframework%20generates%20proposals%20comparable%20to%20those%20written%20by%20experts%20through%20a%0Ablind%20review%20and%20AI%20evaluations.%20Finally%2C%20we%20document%20the%20challenges%20of%0Areal-world%20problem%20scoping%20and%20note%20several%20areas%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Automated%2520Scoping%2520of%2520AI%2520for%2520Social%2520Good%2520Projects%26entry.906535625%3DJacob%2520Emmerson%2520and%2520Rayid%2520Ghani%2520and%2520Zheyuan%2520Ryan%2520Shi%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520for%2520Social%2520Good%2520%2528AI4SG%2529%2520is%2520an%2520emerging%2520effort%2520that%250Aaims%2520to%2520address%2520complex%2520societal%2520challenges%2520with%2520the%2520powerful%2520capabilities%2520of%250AAI%2520systems.%2520These%2520challenges%2520range%2520from%2520local%2520issues%2520with%2520transit%2520networks%2520to%250Aglobal%2520wildlife%2520preservation.%2520However%252C%2520regardless%2520of%2520scale%252C%2520a%2520critical%250Abottleneck%2520for%2520many%2520AI4SG%2520initiatives%2520is%2520the%2520laborious%2520process%2520of%2520problem%250Ascoping%2520--%2520a%2520complex%2520and%2520resource-intensive%2520task%2520--%2520due%2520to%2520a%2520scarcity%2520of%250Aprofessionals%2520with%2520both%2520technical%2520and%2520domain%2520expertise.%2520Given%2520the%2520remarkable%250Aapplications%2520of%2520large%2520language%2520models%2520%2528LLM%2529%252C%2520we%2520propose%2520a%2520Problem%2520Scoping%2520Agent%250A%2528PSA%2529%2520that%2520uses%2520an%2520LLM%2520to%2520generate%2520comprehensive%2520project%2520proposals%2520grounded%2520in%250Ascientific%2520literature%2520and%2520real-world%2520knowledge.%2520We%2520demonstrate%2520that%2520our%2520PSA%250Aframework%2520generates%2520proposals%2520comparable%2520to%2520those%2520written%2520by%2520experts%2520through%2520a%250Ablind%2520review%2520and%2520AI%2520evaluations.%2520Finally%252C%2520we%2520document%2520the%2520challenges%2520of%250Areal-world%2520problem%2520scoping%2520and%2520note%2520several%2520areas%2520for%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Automated%20Scoping%20of%20AI%20for%20Social%20Good%20Projects&entry.906535625=Jacob%20Emmerson%20and%20Rayid%20Ghani%20and%20Zheyuan%20Ryan%20Shi&entry.1292438233=%20%20Artificial%20Intelligence%20for%20Social%20Good%20%28AI4SG%29%20is%20an%20emerging%20effort%20that%0Aaims%20to%20address%20complex%20societal%20challenges%20with%20the%20powerful%20capabilities%20of%0AAI%20systems.%20These%20challenges%20range%20from%20local%20issues%20with%20transit%20networks%20to%0Aglobal%20wildlife%20preservation.%20However%2C%20regardless%20of%20scale%2C%20a%20critical%0Abottleneck%20for%20many%20AI4SG%20initiatives%20is%20the%20laborious%20process%20of%20problem%0Ascoping%20--%20a%20complex%20and%20resource-intensive%20task%20--%20due%20to%20a%20scarcity%20of%0Aprofessionals%20with%20both%20technical%20and%20domain%20expertise.%20Given%20the%20remarkable%0Aapplications%20of%20large%20language%20models%20%28LLM%29%2C%20we%20propose%20a%20Problem%20Scoping%20Agent%0A%28PSA%29%20that%20uses%20an%20LLM%20to%20generate%20comprehensive%20project%20proposals%20grounded%20in%0Ascientific%20literature%20and%20real-world%20knowledge.%20We%20demonstrate%20that%20our%20PSA%0Aframework%20generates%20proposals%20comparable%20to%20those%20written%20by%20experts%20through%20a%0Ablind%20review%20and%20AI%20evaluations.%20Finally%2C%20we%20document%20the%20challenges%20of%0Areal-world%20problem%20scoping%20and%20note%20several%20areas%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20010v1&entry.124074799=Read"},
{"title": "Graph Fourier Transformer with Structure-Frequency Information", "author": "Yonghui Zhai and Yang Zhang and Minghao Shang and Lihua Pang and Yaxin Ren", "abstract": "  Graph Transformers (GTs) have shown advantages in numerous graph structure\ntasks but their self-attention mechanism ignores the generalization bias of\ngraphs, with existing methods mainly compensating for this bias from aspects\nlike position encoding, attention bias and relative distance yet still having\nsub-optimal performance and being insufficient by only considering the\nstructural perspective of generalization bias. To address this, this paper\nproposes Grafourierformer, which innovatively combines GT with inductive bias\ncontaining Frequency-Structure information by applying Graph Fourier Transform\nto the Attention Matrix: specifically, eigenvalues from the Graph Laplacian\nmatrix are used to construct an Eigenvalue matrix mask (reflecting node\npositions and structural relationships with neighboring nodes to enable\nconsideration of node range structural characteristics and focus on local graph\ndetails), and inverse Fourier transform is employed to extract node\nhigh-frequency and low-frequency features, calculate low-frequency and\nhigh-frequency energy, and construct a node frequency-energy matrix to filter\nthe eigenvalue matrix mask, allowing attention heads to incorporate both graph\nstructural information and node frequency information optimization, adaptively\ndistinguish global trends from local details, and effectively suppress\nredundant information interference. Extensive experiments on various benchmarks\nshow Grafourierformer consistently outperforms GNN and GT-based models in graph\nclassification and node classification tasks, with ablation experiments further\nvalidating the effectiveness and necessity of the method. Codes are available\nat https://github.com/Arichibald/Grafourierformer.git\n", "link": "http://arxiv.org/abs/2504.19740v1", "date": "2025-04-28", "relevancy": 1.9638, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5547}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4813}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Fourier%20Transformer%20with%20Structure-Frequency%20Information&body=Title%3A%20Graph%20Fourier%20Transformer%20with%20Structure-Frequency%20Information%0AAuthor%3A%20Yonghui%20Zhai%20and%20Yang%20Zhang%20and%20Minghao%20Shang%20and%20Lihua%20Pang%20and%20Yaxin%20Ren%0AAbstract%3A%20%20%20Graph%20Transformers%20%28GTs%29%20have%20shown%20advantages%20in%20numerous%20graph%20structure%0Atasks%20but%20their%20self-attention%20mechanism%20ignores%20the%20generalization%20bias%20of%0Agraphs%2C%20with%20existing%20methods%20mainly%20compensating%20for%20this%20bias%20from%20aspects%0Alike%20position%20encoding%2C%20attention%20bias%20and%20relative%20distance%20yet%20still%20having%0Asub-optimal%20performance%20and%20being%20insufficient%20by%20only%20considering%20the%0Astructural%20perspective%20of%20generalization%20bias.%20To%20address%20this%2C%20this%20paper%0Aproposes%20Grafourierformer%2C%20which%20innovatively%20combines%20GT%20with%20inductive%20bias%0Acontaining%20Frequency-Structure%20information%20by%20applying%20Graph%20Fourier%20Transform%0Ato%20the%20Attention%20Matrix%3A%20specifically%2C%20eigenvalues%20from%20the%20Graph%20Laplacian%0Amatrix%20are%20used%20to%20construct%20an%20Eigenvalue%20matrix%20mask%20%28reflecting%20node%0Apositions%20and%20structural%20relationships%20with%20neighboring%20nodes%20to%20enable%0Aconsideration%20of%20node%20range%20structural%20characteristics%20and%20focus%20on%20local%20graph%0Adetails%29%2C%20and%20inverse%20Fourier%20transform%20is%20employed%20to%20extract%20node%0Ahigh-frequency%20and%20low-frequency%20features%2C%20calculate%20low-frequency%20and%0Ahigh-frequency%20energy%2C%20and%20construct%20a%20node%20frequency-energy%20matrix%20to%20filter%0Athe%20eigenvalue%20matrix%20mask%2C%20allowing%20attention%20heads%20to%20incorporate%20both%20graph%0Astructural%20information%20and%20node%20frequency%20information%20optimization%2C%20adaptively%0Adistinguish%20global%20trends%20from%20local%20details%2C%20and%20effectively%20suppress%0Aredundant%20information%20interference.%20Extensive%20experiments%20on%20various%20benchmarks%0Ashow%20Grafourierformer%20consistently%20outperforms%20GNN%20and%20GT-based%20models%20in%20graph%0Aclassification%20and%20node%20classification%20tasks%2C%20with%20ablation%20experiments%20further%0Avalidating%20the%20effectiveness%20and%20necessity%20of%20the%20method.%20Codes%20are%20available%0Aat%20https%3A//github.com/Arichibald/Grafourierformer.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Fourier%2520Transformer%2520with%2520Structure-Frequency%2520Information%26entry.906535625%3DYonghui%2520Zhai%2520and%2520Yang%2520Zhang%2520and%2520Minghao%2520Shang%2520and%2520Lihua%2520Pang%2520and%2520Yaxin%2520Ren%26entry.1292438233%3D%2520%2520Graph%2520Transformers%2520%2528GTs%2529%2520have%2520shown%2520advantages%2520in%2520numerous%2520graph%2520structure%250Atasks%2520but%2520their%2520self-attention%2520mechanism%2520ignores%2520the%2520generalization%2520bias%2520of%250Agraphs%252C%2520with%2520existing%2520methods%2520mainly%2520compensating%2520for%2520this%2520bias%2520from%2520aspects%250Alike%2520position%2520encoding%252C%2520attention%2520bias%2520and%2520relative%2520distance%2520yet%2520still%2520having%250Asub-optimal%2520performance%2520and%2520being%2520insufficient%2520by%2520only%2520considering%2520the%250Astructural%2520perspective%2520of%2520generalization%2520bias.%2520To%2520address%2520this%252C%2520this%2520paper%250Aproposes%2520Grafourierformer%252C%2520which%2520innovatively%2520combines%2520GT%2520with%2520inductive%2520bias%250Acontaining%2520Frequency-Structure%2520information%2520by%2520applying%2520Graph%2520Fourier%2520Transform%250Ato%2520the%2520Attention%2520Matrix%253A%2520specifically%252C%2520eigenvalues%2520from%2520the%2520Graph%2520Laplacian%250Amatrix%2520are%2520used%2520to%2520construct%2520an%2520Eigenvalue%2520matrix%2520mask%2520%2528reflecting%2520node%250Apositions%2520and%2520structural%2520relationships%2520with%2520neighboring%2520nodes%2520to%2520enable%250Aconsideration%2520of%2520node%2520range%2520structural%2520characteristics%2520and%2520focus%2520on%2520local%2520graph%250Adetails%2529%252C%2520and%2520inverse%2520Fourier%2520transform%2520is%2520employed%2520to%2520extract%2520node%250Ahigh-frequency%2520and%2520low-frequency%2520features%252C%2520calculate%2520low-frequency%2520and%250Ahigh-frequency%2520energy%252C%2520and%2520construct%2520a%2520node%2520frequency-energy%2520matrix%2520to%2520filter%250Athe%2520eigenvalue%2520matrix%2520mask%252C%2520allowing%2520attention%2520heads%2520to%2520incorporate%2520both%2520graph%250Astructural%2520information%2520and%2520node%2520frequency%2520information%2520optimization%252C%2520adaptively%250Adistinguish%2520global%2520trends%2520from%2520local%2520details%252C%2520and%2520effectively%2520suppress%250Aredundant%2520information%2520interference.%2520Extensive%2520experiments%2520on%2520various%2520benchmarks%250Ashow%2520Grafourierformer%2520consistently%2520outperforms%2520GNN%2520and%2520GT-based%2520models%2520in%2520graph%250Aclassification%2520and%2520node%2520classification%2520tasks%252C%2520with%2520ablation%2520experiments%2520further%250Avalidating%2520the%2520effectiveness%2520and%2520necessity%2520of%2520the%2520method.%2520Codes%2520are%2520available%250Aat%2520https%253A//github.com/Arichibald/Grafourierformer.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Fourier%20Transformer%20with%20Structure-Frequency%20Information&entry.906535625=Yonghui%20Zhai%20and%20Yang%20Zhang%20and%20Minghao%20Shang%20and%20Lihua%20Pang%20and%20Yaxin%20Ren&entry.1292438233=%20%20Graph%20Transformers%20%28GTs%29%20have%20shown%20advantages%20in%20numerous%20graph%20structure%0Atasks%20but%20their%20self-attention%20mechanism%20ignores%20the%20generalization%20bias%20of%0Agraphs%2C%20with%20existing%20methods%20mainly%20compensating%20for%20this%20bias%20from%20aspects%0Alike%20position%20encoding%2C%20attention%20bias%20and%20relative%20distance%20yet%20still%20having%0Asub-optimal%20performance%20and%20being%20insufficient%20by%20only%20considering%20the%0Astructural%20perspective%20of%20generalization%20bias.%20To%20address%20this%2C%20this%20paper%0Aproposes%20Grafourierformer%2C%20which%20innovatively%20combines%20GT%20with%20inductive%20bias%0Acontaining%20Frequency-Structure%20information%20by%20applying%20Graph%20Fourier%20Transform%0Ato%20the%20Attention%20Matrix%3A%20specifically%2C%20eigenvalues%20from%20the%20Graph%20Laplacian%0Amatrix%20are%20used%20to%20construct%20an%20Eigenvalue%20matrix%20mask%20%28reflecting%20node%0Apositions%20and%20structural%20relationships%20with%20neighboring%20nodes%20to%20enable%0Aconsideration%20of%20node%20range%20structural%20characteristics%20and%20focus%20on%20local%20graph%0Adetails%29%2C%20and%20inverse%20Fourier%20transform%20is%20employed%20to%20extract%20node%0Ahigh-frequency%20and%20low-frequency%20features%2C%20calculate%20low-frequency%20and%0Ahigh-frequency%20energy%2C%20and%20construct%20a%20node%20frequency-energy%20matrix%20to%20filter%0Athe%20eigenvalue%20matrix%20mask%2C%20allowing%20attention%20heads%20to%20incorporate%20both%20graph%0Astructural%20information%20and%20node%20frequency%20information%20optimization%2C%20adaptively%0Adistinguish%20global%20trends%20from%20local%20details%2C%20and%20effectively%20suppress%0Aredundant%20information%20interference.%20Extensive%20experiments%20on%20various%20benchmarks%0Ashow%20Grafourierformer%20consistently%20outperforms%20GNN%20and%20GT-based%20models%20in%20graph%0Aclassification%20and%20node%20classification%20tasks%2C%20with%20ablation%20experiments%20further%0Avalidating%20the%20effectiveness%20and%20necessity%20of%20the%20method.%20Codes%20are%20available%0Aat%20https%3A//github.com/Arichibald/Grafourierformer.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19740v1&entry.124074799=Read"},
{"title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons", "author": "Emre Can Acikgoz and Carl Guo and Suvodip Dey and Akul Datta and Takyoung Kim and Gokhan Tur and Dilek Hakkani-T\u00fcr", "abstract": "  Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.\n", "link": "http://arxiv.org/abs/2504.19982v1", "date": "2025-04-28", "relevancy": 1.9628, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TD-EVAL%3A%20Revisiting%20Task-Oriented%20Dialogue%20Evaluation%20by%20Combining%0A%20%20Turn-Level%20Precision%20with%20Dialogue-Level%20Comparisons&body=Title%3A%20TD-EVAL%3A%20Revisiting%20Task-Oriented%20Dialogue%20Evaluation%20by%20Combining%0A%20%20Turn-Level%20Precision%20with%20Dialogue-Level%20Comparisons%0AAuthor%3A%20Emre%20Can%20Acikgoz%20and%20Carl%20Guo%20and%20Suvodip%20Dey%20and%20Akul%20Datta%20and%20Takyoung%20Kim%20and%20Gokhan%20Tur%20and%20Dilek%20Hakkani-T%C3%BCr%0AAbstract%3A%20%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20are%20experiencing%20a%20revolution%20driven%20by%0ALarge%20Language%20Models%20%28LLMs%29%2C%20yet%20the%20evaluation%20methodologies%20for%20these%0Asystems%20remain%20insufficient%20for%20their%20growing%20sophistication.%20While%20traditional%0Aautomatic%20metrics%20effectively%20assessed%20earlier%20modular%20systems%2C%20they%20focus%0Asolely%20on%20the%20dialogue%20level%20and%20cannot%20detect%20critical%20intermediate%20errors%0Athat%20can%20arise%20during%20user-agent%20interactions.%20In%20this%20paper%2C%20we%20introduce%0ATD-EVAL%20%28Turn%20and%20Dialogue-level%20Evaluation%29%2C%20a%20two-step%20evaluation%20framework%0Athat%20unifies%20fine-grained%20turn-level%20analysis%20with%20holistic%20dialogue-level%0Acomparisons.%20At%20turn%20level%2C%20we%20evaluate%20each%20response%20along%20three%20TOD-specific%0Adimensions%3A%20conversation%20cohesion%2C%20backend%20knowledge%20consistency%2C%20and%20policy%0Acompliance.%20Meanwhile%2C%20we%20design%20TOD%20Agent%20Arena%20that%20uses%20pairwise%20comparisons%0Ato%20provide%20a%20measure%20of%20dialogue-level%20quality.%20Through%20experiments%20on%20MultiWOZ%0A2.4%20and%20%7B%5Ctau%7D-Bench%2C%20we%20demonstrate%20that%20TD-EVAL%20effectively%20identifies%20the%0Aconversational%20errors%20that%20conventional%20metrics%20miss.%20Furthermore%2C%20TD-EVAL%0Aexhibits%20better%20alignment%20with%20human%20judgments%20than%20traditional%20and%20LLM-based%0Ametrics.%20These%20findings%20demonstrate%20that%20TD-EVAL%20introduces%20a%20new%20paradigm%20for%0ATOD%20system%20evaluation%2C%20efficiently%20assessing%20both%20turn%20and%20system%20levels%20with%20a%0Aplug-and-play%20framework%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTD-EVAL%253A%2520Revisiting%2520Task-Oriented%2520Dialogue%2520Evaluation%2520by%2520Combining%250A%2520%2520Turn-Level%2520Precision%2520with%2520Dialogue-Level%2520Comparisons%26entry.906535625%3DEmre%2520Can%2520Acikgoz%2520and%2520Carl%2520Guo%2520and%2520Suvodip%2520Dey%2520and%2520Akul%2520Datta%2520and%2520Takyoung%2520Kim%2520and%2520Gokhan%2520Tur%2520and%2520Dilek%2520Hakkani-T%25C3%25BCr%26entry.1292438233%3D%2520%2520Task-oriented%2520dialogue%2520%2528TOD%2529%2520systems%2520are%2520experiencing%2520a%2520revolution%2520driven%2520by%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520yet%2520the%2520evaluation%2520methodologies%2520for%2520these%250Asystems%2520remain%2520insufficient%2520for%2520their%2520growing%2520sophistication.%2520While%2520traditional%250Aautomatic%2520metrics%2520effectively%2520assessed%2520earlier%2520modular%2520systems%252C%2520they%2520focus%250Asolely%2520on%2520the%2520dialogue%2520level%2520and%2520cannot%2520detect%2520critical%2520intermediate%2520errors%250Athat%2520can%2520arise%2520during%2520user-agent%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ATD-EVAL%2520%2528Turn%2520and%2520Dialogue-level%2520Evaluation%2529%252C%2520a%2520two-step%2520evaluation%2520framework%250Athat%2520unifies%2520fine-grained%2520turn-level%2520analysis%2520with%2520holistic%2520dialogue-level%250Acomparisons.%2520At%2520turn%2520level%252C%2520we%2520evaluate%2520each%2520response%2520along%2520three%2520TOD-specific%250Adimensions%253A%2520conversation%2520cohesion%252C%2520backend%2520knowledge%2520consistency%252C%2520and%2520policy%250Acompliance.%2520Meanwhile%252C%2520we%2520design%2520TOD%2520Agent%2520Arena%2520that%2520uses%2520pairwise%2520comparisons%250Ato%2520provide%2520a%2520measure%2520of%2520dialogue-level%2520quality.%2520Through%2520experiments%2520on%2520MultiWOZ%250A2.4%2520and%2520%257B%255Ctau%257D-Bench%252C%2520we%2520demonstrate%2520that%2520TD-EVAL%2520effectively%2520identifies%2520the%250Aconversational%2520errors%2520that%2520conventional%2520metrics%2520miss.%2520Furthermore%252C%2520TD-EVAL%250Aexhibits%2520better%2520alignment%2520with%2520human%2520judgments%2520than%2520traditional%2520and%2520LLM-based%250Ametrics.%2520These%2520findings%2520demonstrate%2520that%2520TD-EVAL%2520introduces%2520a%2520new%2520paradigm%2520for%250ATOD%2520system%2520evaluation%252C%2520efficiently%2520assessing%2520both%2520turn%2520and%2520system%2520levels%2520with%2520a%250Aplug-and-play%2520framework%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TD-EVAL%3A%20Revisiting%20Task-Oriented%20Dialogue%20Evaluation%20by%20Combining%0A%20%20Turn-Level%20Precision%20with%20Dialogue-Level%20Comparisons&entry.906535625=Emre%20Can%20Acikgoz%20and%20Carl%20Guo%20and%20Suvodip%20Dey%20and%20Akul%20Datta%20and%20Takyoung%20Kim%20and%20Gokhan%20Tur%20and%20Dilek%20Hakkani-T%C3%BCr&entry.1292438233=%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20are%20experiencing%20a%20revolution%20driven%20by%0ALarge%20Language%20Models%20%28LLMs%29%2C%20yet%20the%20evaluation%20methodologies%20for%20these%0Asystems%20remain%20insufficient%20for%20their%20growing%20sophistication.%20While%20traditional%0Aautomatic%20metrics%20effectively%20assessed%20earlier%20modular%20systems%2C%20they%20focus%0Asolely%20on%20the%20dialogue%20level%20and%20cannot%20detect%20critical%20intermediate%20errors%0Athat%20can%20arise%20during%20user-agent%20interactions.%20In%20this%20paper%2C%20we%20introduce%0ATD-EVAL%20%28Turn%20and%20Dialogue-level%20Evaluation%29%2C%20a%20two-step%20evaluation%20framework%0Athat%20unifies%20fine-grained%20turn-level%20analysis%20with%20holistic%20dialogue-level%0Acomparisons.%20At%20turn%20level%2C%20we%20evaluate%20each%20response%20along%20three%20TOD-specific%0Adimensions%3A%20conversation%20cohesion%2C%20backend%20knowledge%20consistency%2C%20and%20policy%0Acompliance.%20Meanwhile%2C%20we%20design%20TOD%20Agent%20Arena%20that%20uses%20pairwise%20comparisons%0Ato%20provide%20a%20measure%20of%20dialogue-level%20quality.%20Through%20experiments%20on%20MultiWOZ%0A2.4%20and%20%7B%5Ctau%7D-Bench%2C%20we%20demonstrate%20that%20TD-EVAL%20effectively%20identifies%20the%0Aconversational%20errors%20that%20conventional%20metrics%20miss.%20Furthermore%2C%20TD-EVAL%0Aexhibits%20better%20alignment%20with%20human%20judgments%20than%20traditional%20and%20LLM-based%0Ametrics.%20These%20findings%20demonstrate%20that%20TD-EVAL%20introduces%20a%20new%20paradigm%20for%0ATOD%20system%20evaluation%2C%20efficiently%20assessing%20both%20turn%20and%20system%20levels%20with%20a%0Aplug-and-play%20framework%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19982v1&entry.124074799=Read"},
{"title": "HJRNO: Hamilton-Jacobi Reachability with Neural Operators", "author": "Yankai Li and Mo Chen", "abstract": "  Ensuring the safety of autonomous systems under uncertainty is a critical\nchallenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used method\nfor guaranteeing safety under worst-case disturbances. Traditional HJR methods\nprovide safety guarantees but suffer from the curse of dimensionality, limiting\ntheir scalability to high-dimensional systems or varying environmental\nconditions. In this work, we propose HJRNO, a neural operator-based framework\nfor solving backward reachable tubes (BRTs) efficiently and accurately. By\nleveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping between\nvalue functions, enabling fast inference with strong generalization across\ndifferent obstacle shapes, system configurations, and hyperparameters. We\ndemonstrate that HJRNO achieves low error on random obstacle scenarios and\ngeneralizes effectively across varying system dynamics. These results suggest\nthat HJRNO offers a promising foundation model approach for scalable, real-time\nsafety analysis in autonomous systems.\n", "link": "http://arxiv.org/abs/2504.19989v1", "date": "2025-04-28", "relevancy": 1.9613, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5502}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4901}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HJRNO%3A%20Hamilton-Jacobi%20Reachability%20with%20Neural%20Operators&body=Title%3A%20HJRNO%3A%20Hamilton-Jacobi%20Reachability%20with%20Neural%20Operators%0AAuthor%3A%20Yankai%20Li%20and%20Mo%20Chen%0AAbstract%3A%20%20%20Ensuring%20the%20safety%20of%20autonomous%20systems%20under%20uncertainty%20is%20a%20critical%0Achallenge.%20Hamilton-Jacobi%20reachability%20%28HJR%29%20analysis%20is%20a%20widely%20used%20method%0Afor%20guaranteeing%20safety%20under%20worst-case%20disturbances.%20Traditional%20HJR%20methods%0Aprovide%20safety%20guarantees%20but%20suffer%20from%20the%20curse%20of%20dimensionality%2C%20limiting%0Atheir%20scalability%20to%20high-dimensional%20systems%20or%20varying%20environmental%0Aconditions.%20In%20this%20work%2C%20we%20propose%20HJRNO%2C%20a%20neural%20operator-based%20framework%0Afor%20solving%20backward%20reachable%20tubes%20%28BRTs%29%20efficiently%20and%20accurately.%20By%0Aleveraging%20the%20Fourier%20Neural%20Operator%20%28FNO%29%2C%20HJRNO%20learns%20a%20mapping%20between%0Avalue%20functions%2C%20enabling%20fast%20inference%20with%20strong%20generalization%20across%0Adifferent%20obstacle%20shapes%2C%20system%20configurations%2C%20and%20hyperparameters.%20We%0Ademonstrate%20that%20HJRNO%20achieves%20low%20error%20on%20random%20obstacle%20scenarios%20and%0Ageneralizes%20effectively%20across%20varying%20system%20dynamics.%20These%20results%20suggest%0Athat%20HJRNO%20offers%20a%20promising%20foundation%20model%20approach%20for%20scalable%2C%20real-time%0Asafety%20analysis%20in%20autonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHJRNO%253A%2520Hamilton-Jacobi%2520Reachability%2520with%2520Neural%2520Operators%26entry.906535625%3DYankai%2520Li%2520and%2520Mo%2520Chen%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520safety%2520of%2520autonomous%2520systems%2520under%2520uncertainty%2520is%2520a%2520critical%250Achallenge.%2520Hamilton-Jacobi%2520reachability%2520%2528HJR%2529%2520analysis%2520is%2520a%2520widely%2520used%2520method%250Afor%2520guaranteeing%2520safety%2520under%2520worst-case%2520disturbances.%2520Traditional%2520HJR%2520methods%250Aprovide%2520safety%2520guarantees%2520but%2520suffer%2520from%2520the%2520curse%2520of%2520dimensionality%252C%2520limiting%250Atheir%2520scalability%2520to%2520high-dimensional%2520systems%2520or%2520varying%2520environmental%250Aconditions.%2520In%2520this%2520work%252C%2520we%2520propose%2520HJRNO%252C%2520a%2520neural%2520operator-based%2520framework%250Afor%2520solving%2520backward%2520reachable%2520tubes%2520%2528BRTs%2529%2520efficiently%2520and%2520accurately.%2520By%250Aleveraging%2520the%2520Fourier%2520Neural%2520Operator%2520%2528FNO%2529%252C%2520HJRNO%2520learns%2520a%2520mapping%2520between%250Avalue%2520functions%252C%2520enabling%2520fast%2520inference%2520with%2520strong%2520generalization%2520across%250Adifferent%2520obstacle%2520shapes%252C%2520system%2520configurations%252C%2520and%2520hyperparameters.%2520We%250Ademonstrate%2520that%2520HJRNO%2520achieves%2520low%2520error%2520on%2520random%2520obstacle%2520scenarios%2520and%250Ageneralizes%2520effectively%2520across%2520varying%2520system%2520dynamics.%2520These%2520results%2520suggest%250Athat%2520HJRNO%2520offers%2520a%2520promising%2520foundation%2520model%2520approach%2520for%2520scalable%252C%2520real-time%250Asafety%2520analysis%2520in%2520autonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HJRNO%3A%20Hamilton-Jacobi%20Reachability%20with%20Neural%20Operators&entry.906535625=Yankai%20Li%20and%20Mo%20Chen&entry.1292438233=%20%20Ensuring%20the%20safety%20of%20autonomous%20systems%20under%20uncertainty%20is%20a%20critical%0Achallenge.%20Hamilton-Jacobi%20reachability%20%28HJR%29%20analysis%20is%20a%20widely%20used%20method%0Afor%20guaranteeing%20safety%20under%20worst-case%20disturbances.%20Traditional%20HJR%20methods%0Aprovide%20safety%20guarantees%20but%20suffer%20from%20the%20curse%20of%20dimensionality%2C%20limiting%0Atheir%20scalability%20to%20high-dimensional%20systems%20or%20varying%20environmental%0Aconditions.%20In%20this%20work%2C%20we%20propose%20HJRNO%2C%20a%20neural%20operator-based%20framework%0Afor%20solving%20backward%20reachable%20tubes%20%28BRTs%29%20efficiently%20and%20accurately.%20By%0Aleveraging%20the%20Fourier%20Neural%20Operator%20%28FNO%29%2C%20HJRNO%20learns%20a%20mapping%20between%0Avalue%20functions%2C%20enabling%20fast%20inference%20with%20strong%20generalization%20across%0Adifferent%20obstacle%20shapes%2C%20system%20configurations%2C%20and%20hyperparameters.%20We%0Ademonstrate%20that%20HJRNO%20achieves%20low%20error%20on%20random%20obstacle%20scenarios%20and%0Ageneralizes%20effectively%20across%20varying%20system%20dynamics.%20These%20results%20suggest%0Athat%20HJRNO%20offers%20a%20promising%20foundation%20model%20approach%20for%20scalable%2C%20real-time%0Asafety%20analysis%20in%20autonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19989v1&entry.124074799=Read"},
{"title": "xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices", "author": "Anjith George and Sebastien Marcel", "abstract": "  Heterogeneous Face Recognition (HFR) addresses the challenge of matching face\nimages across different sensing modalities, such as thermal to visible or\nnear-infrared to visible, expanding the applicability of face recognition\nsystems in real-world, unconstrained environments. While recent HFR methods\nhave shown promising results, many rely on computation-intensive architectures,\nlimiting their practicality for deployment on resource-constrained edge\ndevices. In this work, we present a lightweight yet effective HFR framework by\nadapting a hybrid CNN-Transformer architecture originally designed for face\nrecognition. Our approach enables efficient end-to-end training with minimal\npaired heterogeneous data while preserving strong performance on standard RGB\nface recognition tasks. This makes it a compelling solution for both\nhomogeneous and heterogeneous scenarios. Extensive experiments across multiple\nchallenging HFR and face recognition benchmarks demonstrate that our method\nconsistently outperforms state-of-the-art approaches while maintaining a low\ncomputational overhead.\n", "link": "http://arxiv.org/abs/2504.19646v1", "date": "2025-04-28", "relevancy": 1.9606, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4935}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4884}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xEdgeFace%3A%20Efficient%20Cross-Spectral%20Face%20Recognition%20for%20Edge%20Devices&body=Title%3A%20xEdgeFace%3A%20Efficient%20Cross-Spectral%20Face%20Recognition%20for%20Edge%20Devices%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20addresses%20the%20challenge%20of%20matching%20face%0Aimages%20across%20different%20sensing%20modalities%2C%20such%20as%20thermal%20to%20visible%20or%0Anear-infrared%20to%20visible%2C%20expanding%20the%20applicability%20of%20face%20recognition%0Asystems%20in%20real-world%2C%20unconstrained%20environments.%20While%20recent%20HFR%20methods%0Ahave%20shown%20promising%20results%2C%20many%20rely%20on%20computation-intensive%20architectures%2C%0Alimiting%20their%20practicality%20for%20deployment%20on%20resource-constrained%20edge%0Adevices.%20In%20this%20work%2C%20we%20present%20a%20lightweight%20yet%20effective%20HFR%20framework%20by%0Aadapting%20a%20hybrid%20CNN-Transformer%20architecture%20originally%20designed%20for%20face%0Arecognition.%20Our%20approach%20enables%20efficient%20end-to-end%20training%20with%20minimal%0Apaired%20heterogeneous%20data%20while%20preserving%20strong%20performance%20on%20standard%20RGB%0Aface%20recognition%20tasks.%20This%20makes%20it%20a%20compelling%20solution%20for%20both%0Ahomogeneous%20and%20heterogeneous%20scenarios.%20Extensive%20experiments%20across%20multiple%0Achallenging%20HFR%20and%20face%20recognition%20benchmarks%20demonstrate%20that%20our%20method%0Aconsistently%20outperforms%20state-of-the-art%20approaches%20while%20maintaining%20a%20low%0Acomputational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxEdgeFace%253A%2520Efficient%2520Cross-Spectral%2520Face%2520Recognition%2520for%2520Edge%2520Devices%26entry.906535625%3DAnjith%2520George%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520Heterogeneous%2520Face%2520Recognition%2520%2528HFR%2529%2520addresses%2520the%2520challenge%2520of%2520matching%2520face%250Aimages%2520across%2520different%2520sensing%2520modalities%252C%2520such%2520as%2520thermal%2520to%2520visible%2520or%250Anear-infrared%2520to%2520visible%252C%2520expanding%2520the%2520applicability%2520of%2520face%2520recognition%250Asystems%2520in%2520real-world%252C%2520unconstrained%2520environments.%2520While%2520recent%2520HFR%2520methods%250Ahave%2520shown%2520promising%2520results%252C%2520many%2520rely%2520on%2520computation-intensive%2520architectures%252C%250Alimiting%2520their%2520practicality%2520for%2520deployment%2520on%2520resource-constrained%2520edge%250Adevices.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520lightweight%2520yet%2520effective%2520HFR%2520framework%2520by%250Aadapting%2520a%2520hybrid%2520CNN-Transformer%2520architecture%2520originally%2520designed%2520for%2520face%250Arecognition.%2520Our%2520approach%2520enables%2520efficient%2520end-to-end%2520training%2520with%2520minimal%250Apaired%2520heterogeneous%2520data%2520while%2520preserving%2520strong%2520performance%2520on%2520standard%2520RGB%250Aface%2520recognition%2520tasks.%2520This%2520makes%2520it%2520a%2520compelling%2520solution%2520for%2520both%250Ahomogeneous%2520and%2520heterogeneous%2520scenarios.%2520Extensive%2520experiments%2520across%2520multiple%250Achallenging%2520HFR%2520and%2520face%2520recognition%2520benchmarks%2520demonstrate%2520that%2520our%2520method%250Aconsistently%2520outperforms%2520state-of-the-art%2520approaches%2520while%2520maintaining%2520a%2520low%250Acomputational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xEdgeFace%3A%20Efficient%20Cross-Spectral%20Face%20Recognition%20for%20Edge%20Devices&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20Heterogeneous%20Face%20Recognition%20%28HFR%29%20addresses%20the%20challenge%20of%20matching%20face%0Aimages%20across%20different%20sensing%20modalities%2C%20such%20as%20thermal%20to%20visible%20or%0Anear-infrared%20to%20visible%2C%20expanding%20the%20applicability%20of%20face%20recognition%0Asystems%20in%20real-world%2C%20unconstrained%20environments.%20While%20recent%20HFR%20methods%0Ahave%20shown%20promising%20results%2C%20many%20rely%20on%20computation-intensive%20architectures%2C%0Alimiting%20their%20practicality%20for%20deployment%20on%20resource-constrained%20edge%0Adevices.%20In%20this%20work%2C%20we%20present%20a%20lightweight%20yet%20effective%20HFR%20framework%20by%0Aadapting%20a%20hybrid%20CNN-Transformer%20architecture%20originally%20designed%20for%20face%0Arecognition.%20Our%20approach%20enables%20efficient%20end-to-end%20training%20with%20minimal%0Apaired%20heterogeneous%20data%20while%20preserving%20strong%20performance%20on%20standard%20RGB%0Aface%20recognition%20tasks.%20This%20makes%20it%20a%20compelling%20solution%20for%20both%0Ahomogeneous%20and%20heterogeneous%20scenarios.%20Extensive%20experiments%20across%20multiple%0Achallenging%20HFR%20and%20face%20recognition%20benchmarks%20demonstrate%20that%20our%20method%0Aconsistently%20outperforms%20state-of-the-art%20approaches%20while%20maintaining%20a%20low%0Acomputational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19646v1&entry.124074799=Read"},
{"title": "Can AI Agents Design and Implement Drug Discovery Pipelines?", "author": "Khachik Smbatyan and Tsolak Ghukasyan and Tigran Aghajanyan and Hovhannes Dabaghyan and Sergey Adamyan and Aram Bughdaryan and Vahagn Altunyan and Gagik Navasardyan and Aram Davtyan and Anush Hakobyan and Aram Gharibyan and Arman Fahradyan and Artur Hakobyan and Hasmik Mnatsakanyan and Narek Ginoyan and Garik Petrosyan", "abstract": "  The rapid advancement of artificial intelligence, particularly autonomous\nagentic systems based on Large Language Models (LLMs), presents new\nopportunities to accelerate drug discovery by improving in-silico modeling and\nreducing dependence on costly experimental trials. Current AI agent-based\nsystems demonstrate proficiency in solving programming challenges and\nconducting research, indicating an emerging potential to develop software\ncapable of addressing complex problems such as pharmaceutical design and drug\ndiscovery. This paper introduces DO Challenge, a benchmark designed to evaluate\nthe decision-making abilities of AI agents in a single, complex problem\nresembling virtual screening scenarios. The benchmark challenges systems to\nindependently develop, implement, and execute efficient strategies for\nidentifying promising molecular structures from extensive datasets, while\nnavigating chemical space, selecting models, and managing limited resources in\na multi-objective context. We also discuss insights from the DO Challenge 2025,\na competition based on the proposed benchmark, which showcased diverse\nstrategies explored by human participants. Furthermore, we present the Deep\nThought multi-agent system, which demonstrated strong performance on the\nbenchmark, outperforming most human teams. Among the language models tested,\nClaude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,\nand GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While\npromising, the system's performance still fell short of expert-designed\nsolutions and showed high instability, highlighting both the potential and\ncurrent limitations of AI-driven methodologies in transforming drug discovery\nand broader scientific research.\n", "link": "http://arxiv.org/abs/2504.19912v1", "date": "2025-04-28", "relevancy": 1.9487, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5132}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20AI%20Agents%20Design%20and%20Implement%20Drug%20Discovery%20Pipelines%3F&body=Title%3A%20Can%20AI%20Agents%20Design%20and%20Implement%20Drug%20Discovery%20Pipelines%3F%0AAuthor%3A%20Khachik%20Smbatyan%20and%20Tsolak%20Ghukasyan%20and%20Tigran%20Aghajanyan%20and%20Hovhannes%20Dabaghyan%20and%20Sergey%20Adamyan%20and%20Aram%20Bughdaryan%20and%20Vahagn%20Altunyan%20and%20Gagik%20Navasardyan%20and%20Aram%20Davtyan%20and%20Anush%20Hakobyan%20and%20Aram%20Gharibyan%20and%20Arman%20Fahradyan%20and%20Artur%20Hakobyan%20and%20Hasmik%20Mnatsakanyan%20and%20Narek%20Ginoyan%20and%20Garik%20Petrosyan%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20artificial%20intelligence%2C%20particularly%20autonomous%0Aagentic%20systems%20based%20on%20Large%20Language%20Models%20%28LLMs%29%2C%20presents%20new%0Aopportunities%20to%20accelerate%20drug%20discovery%20by%20improving%20in-silico%20modeling%20and%0Areducing%20dependence%20on%20costly%20experimental%20trials.%20Current%20AI%20agent-based%0Asystems%20demonstrate%20proficiency%20in%20solving%20programming%20challenges%20and%0Aconducting%20research%2C%20indicating%20an%20emerging%20potential%20to%20develop%20software%0Acapable%20of%20addressing%20complex%20problems%20such%20as%20pharmaceutical%20design%20and%20drug%0Adiscovery.%20This%20paper%20introduces%20DO%20Challenge%2C%20a%20benchmark%20designed%20to%20evaluate%0Athe%20decision-making%20abilities%20of%20AI%20agents%20in%20a%20single%2C%20complex%20problem%0Aresembling%20virtual%20screening%20scenarios.%20The%20benchmark%20challenges%20systems%20to%0Aindependently%20develop%2C%20implement%2C%20and%20execute%20efficient%20strategies%20for%0Aidentifying%20promising%20molecular%20structures%20from%20extensive%20datasets%2C%20while%0Anavigating%20chemical%20space%2C%20selecting%20models%2C%20and%20managing%20limited%20resources%20in%0Aa%20multi-objective%20context.%20We%20also%20discuss%20insights%20from%20the%20DO%20Challenge%202025%2C%0Aa%20competition%20based%20on%20the%20proposed%20benchmark%2C%20which%20showcased%20diverse%0Astrategies%20explored%20by%20human%20participants.%20Furthermore%2C%20we%20present%20the%20Deep%0AThought%20multi-agent%20system%2C%20which%20demonstrated%20strong%20performance%20on%20the%0Abenchmark%2C%20outperforming%20most%20human%20teams.%20Among%20the%20language%20models%20tested%2C%0AClaude%203.7%20Sonnet%2C%20Gemini%202.5%20Pro%20and%20o3%20performed%20best%20in%20primary%20agent%20roles%2C%0Aand%20GPT-4o%2C%20Gemini%202.0%20Flash%20were%20effective%20in%20auxiliary%20roles.%20While%0Apromising%2C%20the%20system%27s%20performance%20still%20fell%20short%20of%20expert-designed%0Asolutions%20and%20showed%20high%20instability%2C%20highlighting%20both%20the%20potential%20and%0Acurrent%20limitations%20of%20AI-driven%20methodologies%20in%20transforming%20drug%20discovery%0Aand%20broader%20scientific%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520AI%2520Agents%2520Design%2520and%2520Implement%2520Drug%2520Discovery%2520Pipelines%253F%26entry.906535625%3DKhachik%2520Smbatyan%2520and%2520Tsolak%2520Ghukasyan%2520and%2520Tigran%2520Aghajanyan%2520and%2520Hovhannes%2520Dabaghyan%2520and%2520Sergey%2520Adamyan%2520and%2520Aram%2520Bughdaryan%2520and%2520Vahagn%2520Altunyan%2520and%2520Gagik%2520Navasardyan%2520and%2520Aram%2520Davtyan%2520and%2520Anush%2520Hakobyan%2520and%2520Aram%2520Gharibyan%2520and%2520Arman%2520Fahradyan%2520and%2520Artur%2520Hakobyan%2520and%2520Hasmik%2520Mnatsakanyan%2520and%2520Narek%2520Ginoyan%2520and%2520Garik%2520Petrosyan%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520artificial%2520intelligence%252C%2520particularly%2520autonomous%250Aagentic%2520systems%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520presents%2520new%250Aopportunities%2520to%2520accelerate%2520drug%2520discovery%2520by%2520improving%2520in-silico%2520modeling%2520and%250Areducing%2520dependence%2520on%2520costly%2520experimental%2520trials.%2520Current%2520AI%2520agent-based%250Asystems%2520demonstrate%2520proficiency%2520in%2520solving%2520programming%2520challenges%2520and%250Aconducting%2520research%252C%2520indicating%2520an%2520emerging%2520potential%2520to%2520develop%2520software%250Acapable%2520of%2520addressing%2520complex%2520problems%2520such%2520as%2520pharmaceutical%2520design%2520and%2520drug%250Adiscovery.%2520This%2520paper%2520introduces%2520DO%2520Challenge%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%250Athe%2520decision-making%2520abilities%2520of%2520AI%2520agents%2520in%2520a%2520single%252C%2520complex%2520problem%250Aresembling%2520virtual%2520screening%2520scenarios.%2520The%2520benchmark%2520challenges%2520systems%2520to%250Aindependently%2520develop%252C%2520implement%252C%2520and%2520execute%2520efficient%2520strategies%2520for%250Aidentifying%2520promising%2520molecular%2520structures%2520from%2520extensive%2520datasets%252C%2520while%250Anavigating%2520chemical%2520space%252C%2520selecting%2520models%252C%2520and%2520managing%2520limited%2520resources%2520in%250Aa%2520multi-objective%2520context.%2520We%2520also%2520discuss%2520insights%2520from%2520the%2520DO%2520Challenge%25202025%252C%250Aa%2520competition%2520based%2520on%2520the%2520proposed%2520benchmark%252C%2520which%2520showcased%2520diverse%250Astrategies%2520explored%2520by%2520human%2520participants.%2520Furthermore%252C%2520we%2520present%2520the%2520Deep%250AThought%2520multi-agent%2520system%252C%2520which%2520demonstrated%2520strong%2520performance%2520on%2520the%250Abenchmark%252C%2520outperforming%2520most%2520human%2520teams.%2520Among%2520the%2520language%2520models%2520tested%252C%250AClaude%25203.7%2520Sonnet%252C%2520Gemini%25202.5%2520Pro%2520and%2520o3%2520performed%2520best%2520in%2520primary%2520agent%2520roles%252C%250Aand%2520GPT-4o%252C%2520Gemini%25202.0%2520Flash%2520were%2520effective%2520in%2520auxiliary%2520roles.%2520While%250Apromising%252C%2520the%2520system%2527s%2520performance%2520still%2520fell%2520short%2520of%2520expert-designed%250Asolutions%2520and%2520showed%2520high%2520instability%252C%2520highlighting%2520both%2520the%2520potential%2520and%250Acurrent%2520limitations%2520of%2520AI-driven%2520methodologies%2520in%2520transforming%2520drug%2520discovery%250Aand%2520broader%2520scientific%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20AI%20Agents%20Design%20and%20Implement%20Drug%20Discovery%20Pipelines%3F&entry.906535625=Khachik%20Smbatyan%20and%20Tsolak%20Ghukasyan%20and%20Tigran%20Aghajanyan%20and%20Hovhannes%20Dabaghyan%20and%20Sergey%20Adamyan%20and%20Aram%20Bughdaryan%20and%20Vahagn%20Altunyan%20and%20Gagik%20Navasardyan%20and%20Aram%20Davtyan%20and%20Anush%20Hakobyan%20and%20Aram%20Gharibyan%20and%20Arman%20Fahradyan%20and%20Artur%20Hakobyan%20and%20Hasmik%20Mnatsakanyan%20and%20Narek%20Ginoyan%20and%20Garik%20Petrosyan&entry.1292438233=%20%20The%20rapid%20advancement%20of%20artificial%20intelligence%2C%20particularly%20autonomous%0Aagentic%20systems%20based%20on%20Large%20Language%20Models%20%28LLMs%29%2C%20presents%20new%0Aopportunities%20to%20accelerate%20drug%20discovery%20by%20improving%20in-silico%20modeling%20and%0Areducing%20dependence%20on%20costly%20experimental%20trials.%20Current%20AI%20agent-based%0Asystems%20demonstrate%20proficiency%20in%20solving%20programming%20challenges%20and%0Aconducting%20research%2C%20indicating%20an%20emerging%20potential%20to%20develop%20software%0Acapable%20of%20addressing%20complex%20problems%20such%20as%20pharmaceutical%20design%20and%20drug%0Adiscovery.%20This%20paper%20introduces%20DO%20Challenge%2C%20a%20benchmark%20designed%20to%20evaluate%0Athe%20decision-making%20abilities%20of%20AI%20agents%20in%20a%20single%2C%20complex%20problem%0Aresembling%20virtual%20screening%20scenarios.%20The%20benchmark%20challenges%20systems%20to%0Aindependently%20develop%2C%20implement%2C%20and%20execute%20efficient%20strategies%20for%0Aidentifying%20promising%20molecular%20structures%20from%20extensive%20datasets%2C%20while%0Anavigating%20chemical%20space%2C%20selecting%20models%2C%20and%20managing%20limited%20resources%20in%0Aa%20multi-objective%20context.%20We%20also%20discuss%20insights%20from%20the%20DO%20Challenge%202025%2C%0Aa%20competition%20based%20on%20the%20proposed%20benchmark%2C%20which%20showcased%20diverse%0Astrategies%20explored%20by%20human%20participants.%20Furthermore%2C%20we%20present%20the%20Deep%0AThought%20multi-agent%20system%2C%20which%20demonstrated%20strong%20performance%20on%20the%0Abenchmark%2C%20outperforming%20most%20human%20teams.%20Among%20the%20language%20models%20tested%2C%0AClaude%203.7%20Sonnet%2C%20Gemini%202.5%20Pro%20and%20o3%20performed%20best%20in%20primary%20agent%20roles%2C%0Aand%20GPT-4o%2C%20Gemini%202.0%20Flash%20were%20effective%20in%20auxiliary%20roles.%20While%0Apromising%2C%20the%20system%27s%20performance%20still%20fell%20short%20of%20expert-designed%0Asolutions%20and%20showed%20high%20instability%2C%20highlighting%20both%20the%20potential%20and%0Acurrent%20limitations%20of%20AI-driven%20methodologies%20in%20transforming%20drug%20discovery%0Aand%20broader%20scientific%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19912v1&entry.124074799=Read"},
{"title": "Measuring Train Driver Performance as Key to Approval of Driverless\n  Trains", "author": "Rustam Tagiew and Prasannavenkatesh Balaji", "abstract": "  Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation\n(EU) No. 402/2013 allow a simplified approach for the safety approval of\ncomputer vision systems for driverless trains, if they have 'similar' functions\nand interfaces as the replaced human driver. The human driver is not replaced\none-to-one by a technical system - only a limited set of cognitive functions\nare replaced. However, performance in the most challenging function, obstacle\ndetection, is difficult to quantify due to the deficiency of published\nmeasurement results. This article summarizes the data published so far. This\narticle also goes a long way to remedy this situation by providing a new public\nand anonymized dataset of 711 train driver performance measurements from\ncontrolled experiments. The measurements are made for different speeds,\nobstacle sizes, train protection systems and obstacle color contrasts\nrespectively. The measured values are reaction time and distance to the\nobstacle. The goal of this paper is an unbiased and exhaustive description of\nthe presented dataset for research, standardization and regulation. Further\nproject related information including the dataset and source code is available\nat https://atosense-02371c.usercontent.opencode.de/\n", "link": "http://arxiv.org/abs/2504.19735v1", "date": "2025-04-28", "relevancy": 1.9484, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4984}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4954}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Train%20Driver%20Performance%20as%20Key%20to%20Approval%20of%20Driverless%0A%20%20Trains&body=Title%3A%20Measuring%20Train%20Driver%20Performance%20as%20Key%20to%20Approval%20of%20Driverless%0A%20%20Trains%0AAuthor%3A%20Rustam%20Tagiew%20and%20Prasannavenkatesh%20Balaji%0AAbstract%3A%20%20%20Points%202.1.4%28b%29%2C%202.4.2%28b%29%20and%202.4.3%28b%29%20in%20Annex%20I%20of%20Implementing%20Regulation%0A%28EU%29%20No.%20402/2013%20allow%20a%20simplified%20approach%20for%20the%20safety%20approval%20of%0Acomputer%20vision%20systems%20for%20driverless%20trains%2C%20if%20they%20have%20%27similar%27%20functions%0Aand%20interfaces%20as%20the%20replaced%20human%20driver.%20The%20human%20driver%20is%20not%20replaced%0Aone-to-one%20by%20a%20technical%20system%20-%20only%20a%20limited%20set%20of%20cognitive%20functions%0Aare%20replaced.%20However%2C%20performance%20in%20the%20most%20challenging%20function%2C%20obstacle%0Adetection%2C%20is%20difficult%20to%20quantify%20due%20to%20the%20deficiency%20of%20published%0Ameasurement%20results.%20This%20article%20summarizes%20the%20data%20published%20so%20far.%20This%0Aarticle%20also%20goes%20a%20long%20way%20to%20remedy%20this%20situation%20by%20providing%20a%20new%20public%0Aand%20anonymized%20dataset%20of%20711%20train%20driver%20performance%20measurements%20from%0Acontrolled%20experiments.%20The%20measurements%20are%20made%20for%20different%20speeds%2C%0Aobstacle%20sizes%2C%20train%20protection%20systems%20and%20obstacle%20color%20contrasts%0Arespectively.%20The%20measured%20values%20are%20reaction%20time%20and%20distance%20to%20the%0Aobstacle.%20The%20goal%20of%20this%20paper%20is%20an%20unbiased%20and%20exhaustive%20description%20of%0Athe%20presented%20dataset%20for%20research%2C%20standardization%20and%20regulation.%20Further%0Aproject%20related%20information%20including%20the%20dataset%20and%20source%20code%20is%20available%0Aat%20https%3A//atosense-02371c.usercontent.opencode.de/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Train%2520Driver%2520Performance%2520as%2520Key%2520to%2520Approval%2520of%2520Driverless%250A%2520%2520Trains%26entry.906535625%3DRustam%2520Tagiew%2520and%2520Prasannavenkatesh%2520Balaji%26entry.1292438233%3D%2520%2520Points%25202.1.4%2528b%2529%252C%25202.4.2%2528b%2529%2520and%25202.4.3%2528b%2529%2520in%2520Annex%2520I%2520of%2520Implementing%2520Regulation%250A%2528EU%2529%2520No.%2520402/2013%2520allow%2520a%2520simplified%2520approach%2520for%2520the%2520safety%2520approval%2520of%250Acomputer%2520vision%2520systems%2520for%2520driverless%2520trains%252C%2520if%2520they%2520have%2520%2527similar%2527%2520functions%250Aand%2520interfaces%2520as%2520the%2520replaced%2520human%2520driver.%2520The%2520human%2520driver%2520is%2520not%2520replaced%250Aone-to-one%2520by%2520a%2520technical%2520system%2520-%2520only%2520a%2520limited%2520set%2520of%2520cognitive%2520functions%250Aare%2520replaced.%2520However%252C%2520performance%2520in%2520the%2520most%2520challenging%2520function%252C%2520obstacle%250Adetection%252C%2520is%2520difficult%2520to%2520quantify%2520due%2520to%2520the%2520deficiency%2520of%2520published%250Ameasurement%2520results.%2520This%2520article%2520summarizes%2520the%2520data%2520published%2520so%2520far.%2520This%250Aarticle%2520also%2520goes%2520a%2520long%2520way%2520to%2520remedy%2520this%2520situation%2520by%2520providing%2520a%2520new%2520public%250Aand%2520anonymized%2520dataset%2520of%2520711%2520train%2520driver%2520performance%2520measurements%2520from%250Acontrolled%2520experiments.%2520The%2520measurements%2520are%2520made%2520for%2520different%2520speeds%252C%250Aobstacle%2520sizes%252C%2520train%2520protection%2520systems%2520and%2520obstacle%2520color%2520contrasts%250Arespectively.%2520The%2520measured%2520values%2520are%2520reaction%2520time%2520and%2520distance%2520to%2520the%250Aobstacle.%2520The%2520goal%2520of%2520this%2520paper%2520is%2520an%2520unbiased%2520and%2520exhaustive%2520description%2520of%250Athe%2520presented%2520dataset%2520for%2520research%252C%2520standardization%2520and%2520regulation.%2520Further%250Aproject%2520related%2520information%2520including%2520the%2520dataset%2520and%2520source%2520code%2520is%2520available%250Aat%2520https%253A//atosense-02371c.usercontent.opencode.de/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Train%20Driver%20Performance%20as%20Key%20to%20Approval%20of%20Driverless%0A%20%20Trains&entry.906535625=Rustam%20Tagiew%20and%20Prasannavenkatesh%20Balaji&entry.1292438233=%20%20Points%202.1.4%28b%29%2C%202.4.2%28b%29%20and%202.4.3%28b%29%20in%20Annex%20I%20of%20Implementing%20Regulation%0A%28EU%29%20No.%20402/2013%20allow%20a%20simplified%20approach%20for%20the%20safety%20approval%20of%0Acomputer%20vision%20systems%20for%20driverless%20trains%2C%20if%20they%20have%20%27similar%27%20functions%0Aand%20interfaces%20as%20the%20replaced%20human%20driver.%20The%20human%20driver%20is%20not%20replaced%0Aone-to-one%20by%20a%20technical%20system%20-%20only%20a%20limited%20set%20of%20cognitive%20functions%0Aare%20replaced.%20However%2C%20performance%20in%20the%20most%20challenging%20function%2C%20obstacle%0Adetection%2C%20is%20difficult%20to%20quantify%20due%20to%20the%20deficiency%20of%20published%0Ameasurement%20results.%20This%20article%20summarizes%20the%20data%20published%20so%20far.%20This%0Aarticle%20also%20goes%20a%20long%20way%20to%20remedy%20this%20situation%20by%20providing%20a%20new%20public%0Aand%20anonymized%20dataset%20of%20711%20train%20driver%20performance%20measurements%20from%0Acontrolled%20experiments.%20The%20measurements%20are%20made%20for%20different%20speeds%2C%0Aobstacle%20sizes%2C%20train%20protection%20systems%20and%20obstacle%20color%20contrasts%0Arespectively.%20The%20measured%20values%20are%20reaction%20time%20and%20distance%20to%20the%0Aobstacle.%20The%20goal%20of%20this%20paper%20is%20an%20unbiased%20and%20exhaustive%20description%20of%0Athe%20presented%20dataset%20for%20research%2C%20standardization%20and%20regulation.%20Further%0Aproject%20related%20information%20including%20the%20dataset%20and%20source%20code%20is%20available%0Aat%20https%3A//atosense-02371c.usercontent.opencode.de/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19735v1&entry.124074799=Read"},
{"title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate", "author": "Amir Zandieh and Majid Daliri and Majid Hadian and Vahab Mirrokni", "abstract": "  Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.\n", "link": "http://arxiv.org/abs/2504.19874v1", "date": "2025-04-28", "relevancy": 1.9461, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5182}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4767}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TurboQuant%3A%20Online%20Vector%20Quantization%20with%20Near-optimal%20Distortion%20Rate&body=Title%3A%20TurboQuant%3A%20Online%20Vector%20Quantization%20with%20Near-optimal%20Distortion%20Rate%0AAuthor%3A%20Amir%20Zandieh%20and%20Majid%20Daliri%20and%20Majid%20Hadian%20and%20Vahab%20Mirrokni%0AAbstract%3A%20%20%20Vector%20quantization%2C%20a%20problem%20rooted%20in%20Shannon%27s%20source%20coding%20theory%2C%20aims%0Ato%20quantize%20high-dimensional%20Euclidean%20vectors%20while%20minimizing%20distortion%20in%0Atheir%20geometric%20structure.%20We%20propose%20TurboQuant%20to%20address%20both%20mean-squared%0Aerror%20%28MSE%29%20and%20inner%20product%20distortion%2C%20overcoming%20limitations%20of%20existing%0Amethods%20that%20fail%20to%20achieve%20optimal%20distortion%20rates.%20Our%20data-oblivious%0Aalgorithms%2C%20suitable%20for%20online%20applications%2C%20achieve%20near-optimal%20distortion%0Arates%20%28within%20a%20small%20constant%20factor%29%20across%20all%20bit-widths%20and%20dimensions.%0ATurboQuant%20achieves%20this%20by%20randomly%20rotating%20input%20vectors%2C%20inducing%20a%0Aconcentrated%20Beta%20distribution%20on%20coordinates%2C%20and%20leveraging%20the%0Anear-independence%20property%20of%20distinct%20coordinates%20in%20high%20dimensions%20to%20simply%0Aapply%20optimal%20scalar%20quantizers%20per%20each%20coordinate.%20Recognizing%20that%0AMSE-optimal%20quantizers%20introduce%20bias%20in%20inner%20product%20estimation%2C%20we%20propose%20a%0Atwo-stage%20approach%3A%20applying%20an%20MSE%20quantizer%20followed%20by%20a%201-bit%20Quantized%20JL%0A%28QJL%29%20transform%20on%20the%20residual%2C%20resulting%20in%20an%20unbiased%20inner%20product%0Aquantizer.%20We%20also%20provide%20a%20formal%20proof%20of%20the%20information-theoretic%20lower%0Abounds%20on%20best%20achievable%20distortion%20rate%20by%20any%20vector%20quantizer%2C%0Ademonstrating%20that%20TurboQuant%20closely%20matches%20these%20bounds%2C%20differing%20only%20by%20a%0Asmall%20constant%20%28%24%5Capprox%202.7%24%29%20factor.%20Experimental%20results%20validate%20our%0Atheoretical%20findings%2C%20showing%20that%20for%20KV%20cache%20quantization%2C%20we%20achieve%0Aabsolute%20quality%20neutrality%20with%203.5%20bits%20per%20channel%20and%20marginal%20quality%0Adegradation%20with%202.5%20bits%20per%20channel.%20Furthermore%2C%20in%20nearest%20neighbor%20search%0Atasks%2C%20our%20method%20outperforms%20existing%20product%20quantization%20techniques%20in%0Arecall%20while%20reducing%20indexing%20time%20to%20virtually%20zero.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurboQuant%253A%2520Online%2520Vector%2520Quantization%2520with%2520Near-optimal%2520Distortion%2520Rate%26entry.906535625%3DAmir%2520Zandieh%2520and%2520Majid%2520Daliri%2520and%2520Majid%2520Hadian%2520and%2520Vahab%2520Mirrokni%26entry.1292438233%3D%2520%2520Vector%2520quantization%252C%2520a%2520problem%2520rooted%2520in%2520Shannon%2527s%2520source%2520coding%2520theory%252C%2520aims%250Ato%2520quantize%2520high-dimensional%2520Euclidean%2520vectors%2520while%2520minimizing%2520distortion%2520in%250Atheir%2520geometric%2520structure.%2520We%2520propose%2520TurboQuant%2520to%2520address%2520both%2520mean-squared%250Aerror%2520%2528MSE%2529%2520and%2520inner%2520product%2520distortion%252C%2520overcoming%2520limitations%2520of%2520existing%250Amethods%2520that%2520fail%2520to%2520achieve%2520optimal%2520distortion%2520rates.%2520Our%2520data-oblivious%250Aalgorithms%252C%2520suitable%2520for%2520online%2520applications%252C%2520achieve%2520near-optimal%2520distortion%250Arates%2520%2528within%2520a%2520small%2520constant%2520factor%2529%2520across%2520all%2520bit-widths%2520and%2520dimensions.%250ATurboQuant%2520achieves%2520this%2520by%2520randomly%2520rotating%2520input%2520vectors%252C%2520inducing%2520a%250Aconcentrated%2520Beta%2520distribution%2520on%2520coordinates%252C%2520and%2520leveraging%2520the%250Anear-independence%2520property%2520of%2520distinct%2520coordinates%2520in%2520high%2520dimensions%2520to%2520simply%250Aapply%2520optimal%2520scalar%2520quantizers%2520per%2520each%2520coordinate.%2520Recognizing%2520that%250AMSE-optimal%2520quantizers%2520introduce%2520bias%2520in%2520inner%2520product%2520estimation%252C%2520we%2520propose%2520a%250Atwo-stage%2520approach%253A%2520applying%2520an%2520MSE%2520quantizer%2520followed%2520by%2520a%25201-bit%2520Quantized%2520JL%250A%2528QJL%2529%2520transform%2520on%2520the%2520residual%252C%2520resulting%2520in%2520an%2520unbiased%2520inner%2520product%250Aquantizer.%2520We%2520also%2520provide%2520a%2520formal%2520proof%2520of%2520the%2520information-theoretic%2520lower%250Abounds%2520on%2520best%2520achievable%2520distortion%2520rate%2520by%2520any%2520vector%2520quantizer%252C%250Ademonstrating%2520that%2520TurboQuant%2520closely%2520matches%2520these%2520bounds%252C%2520differing%2520only%2520by%2520a%250Asmall%2520constant%2520%2528%2524%255Capprox%25202.7%2524%2529%2520factor.%2520Experimental%2520results%2520validate%2520our%250Atheoretical%2520findings%252C%2520showing%2520that%2520for%2520KV%2520cache%2520quantization%252C%2520we%2520achieve%250Aabsolute%2520quality%2520neutrality%2520with%25203.5%2520bits%2520per%2520channel%2520and%2520marginal%2520quality%250Adegradation%2520with%25202.5%2520bits%2520per%2520channel.%2520Furthermore%252C%2520in%2520nearest%2520neighbor%2520search%250Atasks%252C%2520our%2520method%2520outperforms%2520existing%2520product%2520quantization%2520techniques%2520in%250Arecall%2520while%2520reducing%2520indexing%2520time%2520to%2520virtually%2520zero.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TurboQuant%3A%20Online%20Vector%20Quantization%20with%20Near-optimal%20Distortion%20Rate&entry.906535625=Amir%20Zandieh%20and%20Majid%20Daliri%20and%20Majid%20Hadian%20and%20Vahab%20Mirrokni&entry.1292438233=%20%20Vector%20quantization%2C%20a%20problem%20rooted%20in%20Shannon%27s%20source%20coding%20theory%2C%20aims%0Ato%20quantize%20high-dimensional%20Euclidean%20vectors%20while%20minimizing%20distortion%20in%0Atheir%20geometric%20structure.%20We%20propose%20TurboQuant%20to%20address%20both%20mean-squared%0Aerror%20%28MSE%29%20and%20inner%20product%20distortion%2C%20overcoming%20limitations%20of%20existing%0Amethods%20that%20fail%20to%20achieve%20optimal%20distortion%20rates.%20Our%20data-oblivious%0Aalgorithms%2C%20suitable%20for%20online%20applications%2C%20achieve%20near-optimal%20distortion%0Arates%20%28within%20a%20small%20constant%20factor%29%20across%20all%20bit-widths%20and%20dimensions.%0ATurboQuant%20achieves%20this%20by%20randomly%20rotating%20input%20vectors%2C%20inducing%20a%0Aconcentrated%20Beta%20distribution%20on%20coordinates%2C%20and%20leveraging%20the%0Anear-independence%20property%20of%20distinct%20coordinates%20in%20high%20dimensions%20to%20simply%0Aapply%20optimal%20scalar%20quantizers%20per%20each%20coordinate.%20Recognizing%20that%0AMSE-optimal%20quantizers%20introduce%20bias%20in%20inner%20product%20estimation%2C%20we%20propose%20a%0Atwo-stage%20approach%3A%20applying%20an%20MSE%20quantizer%20followed%20by%20a%201-bit%20Quantized%20JL%0A%28QJL%29%20transform%20on%20the%20residual%2C%20resulting%20in%20an%20unbiased%20inner%20product%0Aquantizer.%20We%20also%20provide%20a%20formal%20proof%20of%20the%20information-theoretic%20lower%0Abounds%20on%20best%20achievable%20distortion%20rate%20by%20any%20vector%20quantizer%2C%0Ademonstrating%20that%20TurboQuant%20closely%20matches%20these%20bounds%2C%20differing%20only%20by%20a%0Asmall%20constant%20%28%24%5Capprox%202.7%24%29%20factor.%20Experimental%20results%20validate%20our%0Atheoretical%20findings%2C%20showing%20that%20for%20KV%20cache%20quantization%2C%20we%20achieve%0Aabsolute%20quality%20neutrality%20with%203.5%20bits%20per%20channel%20and%20marginal%20quality%0Adegradation%20with%202.5%20bits%20per%20channel.%20Furthermore%2C%20in%20nearest%20neighbor%20search%0Atasks%2C%20our%20method%20outperforms%20existing%20product%20quantization%20techniques%20in%0Arecall%20while%20reducing%20indexing%20time%20to%20virtually%20zero.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19874v1&entry.124074799=Read"},
{"title": "FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained\n  Mixed-Precision Quantization of LLMs", "author": "Xilong Xie and Liang Wang and Limin Xiao and Meng Han and Lin Sun and Shuai Zheng and Xiangrong Xu", "abstract": "  Large language models (LLMs) have significantly advanced the natural language\nprocessing paradigm but impose substantial demands on memory and computational\nresources. Quantization is one of the most effective ways to reduce memory\nconsumption of LLMs. However, advanced single-precision quantization methods\nexperience significant accuracy degradation when quantizing to ultra-low bits.\nExisting mixed-precision quantization methods are quantized by groups with\ncoarse granularity. Employing high precision for group data leads to\nsubstantial memory overhead, whereas low precision severely impacts model\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\npartitions the weights into finer-grained clusters and considers the\ndistribution of outliers within these clusters, thus achieving a balance\nbetween model accuracy and memory overhead. Then, we propose an outlier\nprotection mechanism within clusters that uses 3 bits to represent outliers and\nintroduce an encoding scheme for index and data concatenation to enable aligned\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\nthat effectively supports the quantization algorithm while simplifying the\nmultipliers in the systolic array. FineQ achieves higher model accuracy\ncompared to the SOTA mixed-precision quantization algorithm at a close average\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\nand reduces the area of the systolic array by 61.2%.\n", "link": "http://arxiv.org/abs/2504.19746v1", "date": "2025-04-28", "relevancy": 1.9413, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4975}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.484}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineQ%3A%20Software-Hardware%20Co-Design%20for%20Low-Bit%20Fine-Grained%0A%20%20Mixed-Precision%20Quantization%20of%20LLMs&body=Title%3A%20FineQ%3A%20Software-Hardware%20Co-Design%20for%20Low-Bit%20Fine-Grained%0A%20%20Mixed-Precision%20Quantization%20of%20LLMs%0AAuthor%3A%20Xilong%20Xie%20and%20Liang%20Wang%20and%20Limin%20Xiao%20and%20Meng%20Han%20and%20Lin%20Sun%20and%20Shuai%20Zheng%20and%20Xiangrong%20Xu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20the%20natural%20language%0Aprocessing%20paradigm%20but%20impose%20substantial%20demands%20on%20memory%20and%20computational%0Aresources.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20reduce%20memory%0Aconsumption%20of%20LLMs.%20However%2C%20advanced%20single-precision%20quantization%20methods%0Aexperience%20significant%20accuracy%20degradation%20when%20quantizing%20to%20ultra-low%20bits.%0AExisting%20mixed-precision%20quantization%20methods%20are%20quantized%20by%20groups%20with%0Acoarse%20granularity.%20Employing%20high%20precision%20for%20group%20data%20leads%20to%0Asubstantial%20memory%20overhead%2C%20whereas%20low%20precision%20severely%20impacts%20model%0Aaccuracy.%20To%20address%20this%20issue%2C%20we%20propose%20FineQ%2C%20software-hardware%20co-design%0Afor%20low-bit%20fine-grained%20mixed-precision%20quantization%20of%20LLMs.%20First%2C%20FineQ%0Apartitions%20the%20weights%20into%20finer-grained%20clusters%20and%20considers%20the%0Adistribution%20of%20outliers%20within%20these%20clusters%2C%20thus%20achieving%20a%20balance%0Abetween%20model%20accuracy%20and%20memory%20overhead.%20Then%2C%20we%20propose%20an%20outlier%0Aprotection%20mechanism%20within%20clusters%20that%20uses%203%20bits%20to%20represent%20outliers%20and%0Aintroduce%20an%20encoding%20scheme%20for%20index%20and%20data%20concatenation%20to%20enable%20aligned%0Amemory%20access.%20Finally%2C%20we%20introduce%20an%20accelerator%20utilizing%20temporal%20coding%0Athat%20effectively%20supports%20the%20quantization%20algorithm%20while%20simplifying%20the%0Amultipliers%20in%20the%20systolic%20array.%20FineQ%20achieves%20higher%20model%20accuracy%0Acompared%20to%20the%20SOTA%20mixed-precision%20quantization%20algorithm%20at%20a%20close%20average%0Abit-width.%20Meanwhile%2C%20the%20accelerator%20achieves%20up%20to%201.79x%20energy%20efficiency%0Aand%20reduces%20the%20area%20of%20the%20systolic%20array%20by%2061.2%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineQ%253A%2520Software-Hardware%2520Co-Design%2520for%2520Low-Bit%2520Fine-Grained%250A%2520%2520Mixed-Precision%2520Quantization%2520of%2520LLMs%26entry.906535625%3DXilong%2520Xie%2520and%2520Liang%2520Wang%2520and%2520Limin%2520Xiao%2520and%2520Meng%2520Han%2520and%2520Lin%2520Sun%2520and%2520Shuai%2520Zheng%2520and%2520Xiangrong%2520Xu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%2520the%2520natural%2520language%250Aprocessing%2520paradigm%2520but%2520impose%2520substantial%2520demands%2520on%2520memory%2520and%2520computational%250Aresources.%2520Quantization%2520is%2520one%2520of%2520the%2520most%2520effective%2520ways%2520to%2520reduce%2520memory%250Aconsumption%2520of%2520LLMs.%2520However%252C%2520advanced%2520single-precision%2520quantization%2520methods%250Aexperience%2520significant%2520accuracy%2520degradation%2520when%2520quantizing%2520to%2520ultra-low%2520bits.%250AExisting%2520mixed-precision%2520quantization%2520methods%2520are%2520quantized%2520by%2520groups%2520with%250Acoarse%2520granularity.%2520Employing%2520high%2520precision%2520for%2520group%2520data%2520leads%2520to%250Asubstantial%2520memory%2520overhead%252C%2520whereas%2520low%2520precision%2520severely%2520impacts%2520model%250Aaccuracy.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520FineQ%252C%2520software-hardware%2520co-design%250Afor%2520low-bit%2520fine-grained%2520mixed-precision%2520quantization%2520of%2520LLMs.%2520First%252C%2520FineQ%250Apartitions%2520the%2520weights%2520into%2520finer-grained%2520clusters%2520and%2520considers%2520the%250Adistribution%2520of%2520outliers%2520within%2520these%2520clusters%252C%2520thus%2520achieving%2520a%2520balance%250Abetween%2520model%2520accuracy%2520and%2520memory%2520overhead.%2520Then%252C%2520we%2520propose%2520an%2520outlier%250Aprotection%2520mechanism%2520within%2520clusters%2520that%2520uses%25203%2520bits%2520to%2520represent%2520outliers%2520and%250Aintroduce%2520an%2520encoding%2520scheme%2520for%2520index%2520and%2520data%2520concatenation%2520to%2520enable%2520aligned%250Amemory%2520access.%2520Finally%252C%2520we%2520introduce%2520an%2520accelerator%2520utilizing%2520temporal%2520coding%250Athat%2520effectively%2520supports%2520the%2520quantization%2520algorithm%2520while%2520simplifying%2520the%250Amultipliers%2520in%2520the%2520systolic%2520array.%2520FineQ%2520achieves%2520higher%2520model%2520accuracy%250Acompared%2520to%2520the%2520SOTA%2520mixed-precision%2520quantization%2520algorithm%2520at%2520a%2520close%2520average%250Abit-width.%2520Meanwhile%252C%2520the%2520accelerator%2520achieves%2520up%2520to%25201.79x%2520energy%2520efficiency%250Aand%2520reduces%2520the%2520area%2520of%2520the%2520systolic%2520array%2520by%252061.2%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineQ%3A%20Software-Hardware%20Co-Design%20for%20Low-Bit%20Fine-Grained%0A%20%20Mixed-Precision%20Quantization%20of%20LLMs&entry.906535625=Xilong%20Xie%20and%20Liang%20Wang%20and%20Limin%20Xiao%20and%20Meng%20Han%20and%20Lin%20Sun%20and%20Shuai%20Zheng%20and%20Xiangrong%20Xu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20the%20natural%20language%0Aprocessing%20paradigm%20but%20impose%20substantial%20demands%20on%20memory%20and%20computational%0Aresources.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20reduce%20memory%0Aconsumption%20of%20LLMs.%20However%2C%20advanced%20single-precision%20quantization%20methods%0Aexperience%20significant%20accuracy%20degradation%20when%20quantizing%20to%20ultra-low%20bits.%0AExisting%20mixed-precision%20quantization%20methods%20are%20quantized%20by%20groups%20with%0Acoarse%20granularity.%20Employing%20high%20precision%20for%20group%20data%20leads%20to%0Asubstantial%20memory%20overhead%2C%20whereas%20low%20precision%20severely%20impacts%20model%0Aaccuracy.%20To%20address%20this%20issue%2C%20we%20propose%20FineQ%2C%20software-hardware%20co-design%0Afor%20low-bit%20fine-grained%20mixed-precision%20quantization%20of%20LLMs.%20First%2C%20FineQ%0Apartitions%20the%20weights%20into%20finer-grained%20clusters%20and%20considers%20the%0Adistribution%20of%20outliers%20within%20these%20clusters%2C%20thus%20achieving%20a%20balance%0Abetween%20model%20accuracy%20and%20memory%20overhead.%20Then%2C%20we%20propose%20an%20outlier%0Aprotection%20mechanism%20within%20clusters%20that%20uses%203%20bits%20to%20represent%20outliers%20and%0Aintroduce%20an%20encoding%20scheme%20for%20index%20and%20data%20concatenation%20to%20enable%20aligned%0Amemory%20access.%20Finally%2C%20we%20introduce%20an%20accelerator%20utilizing%20temporal%20coding%0Athat%20effectively%20supports%20the%20quantization%20algorithm%20while%20simplifying%20the%0Amultipliers%20in%20the%20systolic%20array.%20FineQ%20achieves%20higher%20model%20accuracy%0Acompared%20to%20the%20SOTA%20mixed-precision%20quantization%20algorithm%20at%20a%20close%20average%0Abit-width.%20Meanwhile%2C%20the%20accelerator%20achieves%20up%20to%201.79x%20energy%20efficiency%0Aand%20reduces%20the%20area%20of%20the%20systolic%20array%20by%2061.2%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19746v1&entry.124074799=Read"},
{"title": "PINN-MEP: Continuous Neural Representations for Minimum-Energy Path\n  Discovery in Molecular Systems", "author": "Magnus Petersen and Roberto Covino", "abstract": "  Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms.\n", "link": "http://arxiv.org/abs/2504.16381v2", "date": "2025-04-28", "relevancy": 1.9351, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4794}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PINN-MEP%3A%20Continuous%20Neural%20Representations%20for%20Minimum-Energy%20Path%0A%20%20Discovery%20in%20Molecular%20Systems&body=Title%3A%20PINN-MEP%3A%20Continuous%20Neural%20Representations%20for%20Minimum-Energy%20Path%0A%20%20Discovery%20in%20Molecular%20Systems%0AAuthor%3A%20Magnus%20Petersen%20and%20Roberto%20Covino%0AAbstract%3A%20%20%20Characterizing%20conformational%20transitions%20in%20physical%20systems%20remains%20a%0Afundamental%20challenge%20in%20the%20computational%20sciences.%20Traditional%20sampling%0Amethods%20like%20molecular%20dynamics%20%28MD%29%20or%20MCMC%20often%20struggle%20with%20the%0Ahigh-dimensional%20nature%20of%20molecular%20systems%20and%20the%20high%20energy%20barriers%20of%0Atransitions%20between%20stable%20states.%20While%20these%20transitions%20are%20rare%20events%20in%0Asimulation%20timescales%2C%20they%20often%20represent%20the%20most%20biologically%20significant%0Aprocesses%20-%20for%20example%2C%20the%20conformational%20change%20of%20an%20ion%20channel%20protein%0Afrom%20its%20closed%20to%20open%20state%2C%20which%20controls%20cellular%20ion%20flow%20and%20is%20crucial%0Afor%20neural%20signaling.%20Such%20transitions%20in%20real%20systems%20may%20take%20milliseconds%20to%0Aseconds%20but%20could%20require%20months%20or%20years%20of%20continuous%20simulation%20to%20observe%0Aeven%20once.%20We%20present%20a%20method%20that%20reformulates%20transition%20path%20generation%20as%0Aa%20continuous%20optimization%20problem%20solved%20through%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20inspired%20by%20string%20methods%20for%20minimum-energy%20path%20%28MEP%29%0Ageneration.%20By%20representing%20transition%20paths%20as%20implicit%20neural%20functions%20and%0Aleveraging%20automatic%20differentiation%20with%20differentiable%20molecular%20dynamics%0Aforce%20fields%2C%20our%20method%20enables%20the%20efficient%20discovery%20of%20physically%0Arealistic%20transition%20pathways%20without%20requiring%20expensive%20path%20sampling.%20We%0Ademonstrate%20our%20method%27s%20effectiveness%20on%20two%20proteins%2C%20including%20an%20explicitly%0Ahydrated%20bovine%20pancreatic%20trypsin%20inhibitor%20%28BPTI%29%20system%20with%20over%208%2C300%0Aatoms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16381v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPINN-MEP%253A%2520Continuous%2520Neural%2520Representations%2520for%2520Minimum-Energy%2520Path%250A%2520%2520Discovery%2520in%2520Molecular%2520Systems%26entry.906535625%3DMagnus%2520Petersen%2520and%2520Roberto%2520Covino%26entry.1292438233%3D%2520%2520Characterizing%2520conformational%2520transitions%2520in%2520physical%2520systems%2520remains%2520a%250Afundamental%2520challenge%2520in%2520the%2520computational%2520sciences.%2520Traditional%2520sampling%250Amethods%2520like%2520molecular%2520dynamics%2520%2528MD%2529%2520or%2520MCMC%2520often%2520struggle%2520with%2520the%250Ahigh-dimensional%2520nature%2520of%2520molecular%2520systems%2520and%2520the%2520high%2520energy%2520barriers%2520of%250Atransitions%2520between%2520stable%2520states.%2520While%2520these%2520transitions%2520are%2520rare%2520events%2520in%250Asimulation%2520timescales%252C%2520they%2520often%2520represent%2520the%2520most%2520biologically%2520significant%250Aprocesses%2520-%2520for%2520example%252C%2520the%2520conformational%2520change%2520of%2520an%2520ion%2520channel%2520protein%250Afrom%2520its%2520closed%2520to%2520open%2520state%252C%2520which%2520controls%2520cellular%2520ion%2520flow%2520and%2520is%2520crucial%250Afor%2520neural%2520signaling.%2520Such%2520transitions%2520in%2520real%2520systems%2520may%2520take%2520milliseconds%2520to%250Aseconds%2520but%2520could%2520require%2520months%2520or%2520years%2520of%2520continuous%2520simulation%2520to%2520observe%250Aeven%2520once.%2520We%2520present%2520a%2520method%2520that%2520reformulates%2520transition%2520path%2520generation%2520as%250Aa%2520continuous%2520optimization%2520problem%2520solved%2520through%2520physics-informed%2520neural%250Anetworks%2520%2528PINNs%2529%2520inspired%2520by%2520string%2520methods%2520for%2520minimum-energy%2520path%2520%2528MEP%2529%250Ageneration.%2520By%2520representing%2520transition%2520paths%2520as%2520implicit%2520neural%2520functions%2520and%250Aleveraging%2520automatic%2520differentiation%2520with%2520differentiable%2520molecular%2520dynamics%250Aforce%2520fields%252C%2520our%2520method%2520enables%2520the%2520efficient%2520discovery%2520of%2520physically%250Arealistic%2520transition%2520pathways%2520without%2520requiring%2520expensive%2520path%2520sampling.%2520We%250Ademonstrate%2520our%2520method%2527s%2520effectiveness%2520on%2520two%2520proteins%252C%2520including%2520an%2520explicitly%250Ahydrated%2520bovine%2520pancreatic%2520trypsin%2520inhibitor%2520%2528BPTI%2529%2520system%2520with%2520over%25208%252C300%250Aatoms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16381v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PINN-MEP%3A%20Continuous%20Neural%20Representations%20for%20Minimum-Energy%20Path%0A%20%20Discovery%20in%20Molecular%20Systems&entry.906535625=Magnus%20Petersen%20and%20Roberto%20Covino&entry.1292438233=%20%20Characterizing%20conformational%20transitions%20in%20physical%20systems%20remains%20a%0Afundamental%20challenge%20in%20the%20computational%20sciences.%20Traditional%20sampling%0Amethods%20like%20molecular%20dynamics%20%28MD%29%20or%20MCMC%20often%20struggle%20with%20the%0Ahigh-dimensional%20nature%20of%20molecular%20systems%20and%20the%20high%20energy%20barriers%20of%0Atransitions%20between%20stable%20states.%20While%20these%20transitions%20are%20rare%20events%20in%0Asimulation%20timescales%2C%20they%20often%20represent%20the%20most%20biologically%20significant%0Aprocesses%20-%20for%20example%2C%20the%20conformational%20change%20of%20an%20ion%20channel%20protein%0Afrom%20its%20closed%20to%20open%20state%2C%20which%20controls%20cellular%20ion%20flow%20and%20is%20crucial%0Afor%20neural%20signaling.%20Such%20transitions%20in%20real%20systems%20may%20take%20milliseconds%20to%0Aseconds%20but%20could%20require%20months%20or%20years%20of%20continuous%20simulation%20to%20observe%0Aeven%20once.%20We%20present%20a%20method%20that%20reformulates%20transition%20path%20generation%20as%0Aa%20continuous%20optimization%20problem%20solved%20through%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20inspired%20by%20string%20methods%20for%20minimum-energy%20path%20%28MEP%29%0Ageneration.%20By%20representing%20transition%20paths%20as%20implicit%20neural%20functions%20and%0Aleveraging%20automatic%20differentiation%20with%20differentiable%20molecular%20dynamics%0Aforce%20fields%2C%20our%20method%20enables%20the%20efficient%20discovery%20of%20physically%0Arealistic%20transition%20pathways%20without%20requiring%20expensive%20path%20sampling.%20We%0Ademonstrate%20our%20method%27s%20effectiveness%20on%20two%20proteins%2C%20including%20an%20explicitly%0Ahydrated%20bovine%20pancreatic%20trypsin%20inhibitor%20%28BPTI%29%20system%20with%20over%208%2C300%0Aatoms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16381v2&entry.124074799=Read"},
{"title": "Automated decision-making for dynamic task assignment at scale", "author": "Riccardo Lo Bianco and Willem van Jaarsveld and Jeroen Middelhuis and Luca Begnardi and Remco Dijkman", "abstract": "  The Dynamic Task Assignment Problem (DTAP) concerns matching resources to\ntasks in real time while minimizing some objectives, like resource costs or\ntask cycle time. In this work, we consider a DTAP variant where every task is a\ncase composed of a stochastic sequence of activities. The DTAP, in this case,\ninvolves the decision of which employee to assign to which activity to process\nrequests as quickly as possible. In recent years, Deep Reinforcement Learning\n(DRL) has emerged as a promising tool for tackling this DTAP variant, but most\nresearch is limited to solving small-scale, synthetic problems, neglecting the\nchallenges posed by real-world use cases. To bridge this gap, this work\nproposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS.\nTo this end, we introduce a DRL agent with two novel elements: a graph\nstructure for observations and actions that can effectively represent any DTAP\nand a reward function that is provably equivalent to the objective of\nminimizing the average cycle time of tasks. The combination of these two\nnovelties allows the agent to learn effective and generalizable assignment\npolicies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP\ninstances whose parameters are extracted from real-world logs through process\nmining. The experimental evaluation shows how the proposed DRL agent matches or\noutperforms the best baseline in all DTAP instances and generalizes on\ndifferent time horizons and across instances.\n", "link": "http://arxiv.org/abs/2504.19933v1", "date": "2025-04-28", "relevancy": 1.9303, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5312}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4758}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20decision-making%20for%20dynamic%20task%20assignment%20at%20scale&body=Title%3A%20Automated%20decision-making%20for%20dynamic%20task%20assignment%20at%20scale%0AAuthor%3A%20Riccardo%20Lo%20Bianco%20and%20Willem%20van%20Jaarsveld%20and%20Jeroen%20Middelhuis%20and%20Luca%20Begnardi%20and%20Remco%20Dijkman%0AAbstract%3A%20%20%20The%20Dynamic%20Task%20Assignment%20Problem%20%28DTAP%29%20concerns%20matching%20resources%20to%0Atasks%20in%20real%20time%20while%20minimizing%20some%20objectives%2C%20like%20resource%20costs%20or%0Atask%20cycle%20time.%20In%20this%20work%2C%20we%20consider%20a%20DTAP%20variant%20where%20every%20task%20is%20a%0Acase%20composed%20of%20a%20stochastic%20sequence%20of%20activities.%20The%20DTAP%2C%20in%20this%20case%2C%0Ainvolves%20the%20decision%20of%20which%20employee%20to%20assign%20to%20which%20activity%20to%20process%0Arequests%20as%20quickly%20as%20possible.%20In%20recent%20years%2C%20Deep%20Reinforcement%20Learning%0A%28DRL%29%20has%20emerged%20as%20a%20promising%20tool%20for%20tackling%20this%20DTAP%20variant%2C%20but%20most%0Aresearch%20is%20limited%20to%20solving%20small-scale%2C%20synthetic%20problems%2C%20neglecting%20the%0Achallenges%20posed%20by%20real-world%20use%20cases.%20To%20bridge%20this%20gap%2C%20this%20work%0Aproposes%20a%20DRL-based%20Decision%20Support%20System%20%28DSS%29%20for%20real-world%20scale%20DTAPS.%0ATo%20this%20end%2C%20we%20introduce%20a%20DRL%20agent%20with%20two%20novel%20elements%3A%20a%20graph%0Astructure%20for%20observations%20and%20actions%20that%20can%20effectively%20represent%20any%20DTAP%0Aand%20a%20reward%20function%20that%20is%20provably%20equivalent%20to%20the%20objective%20of%0Aminimizing%20the%20average%20cycle%20time%20of%20tasks.%20The%20combination%20of%20these%20two%0Anovelties%20allows%20the%20agent%20to%20learn%20effective%20and%20generalizable%20assignment%0Apolicies%20for%20real-world%20scale%20DTAPs.%20The%20proposed%20DSS%20is%20evaluated%20on%20five%20DTAP%0Ainstances%20whose%20parameters%20are%20extracted%20from%20real-world%20logs%20through%20process%0Amining.%20The%20experimental%20evaluation%20shows%20how%20the%20proposed%20DRL%20agent%20matches%20or%0Aoutperforms%20the%20best%20baseline%20in%20all%20DTAP%20instances%20and%20generalizes%20on%0Adifferent%20time%20horizons%20and%20across%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520decision-making%2520for%2520dynamic%2520task%2520assignment%2520at%2520scale%26entry.906535625%3DRiccardo%2520Lo%2520Bianco%2520and%2520Willem%2520van%2520Jaarsveld%2520and%2520Jeroen%2520Middelhuis%2520and%2520Luca%2520Begnardi%2520and%2520Remco%2520Dijkman%26entry.1292438233%3D%2520%2520The%2520Dynamic%2520Task%2520Assignment%2520Problem%2520%2528DTAP%2529%2520concerns%2520matching%2520resources%2520to%250Atasks%2520in%2520real%2520time%2520while%2520minimizing%2520some%2520objectives%252C%2520like%2520resource%2520costs%2520or%250Atask%2520cycle%2520time.%2520In%2520this%2520work%252C%2520we%2520consider%2520a%2520DTAP%2520variant%2520where%2520every%2520task%2520is%2520a%250Acase%2520composed%2520of%2520a%2520stochastic%2520sequence%2520of%2520activities.%2520The%2520DTAP%252C%2520in%2520this%2520case%252C%250Ainvolves%2520the%2520decision%2520of%2520which%2520employee%2520to%2520assign%2520to%2520which%2520activity%2520to%2520process%250Arequests%2520as%2520quickly%2520as%2520possible.%2520In%2520recent%2520years%252C%2520Deep%2520Reinforcement%2520Learning%250A%2528DRL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520tool%2520for%2520tackling%2520this%2520DTAP%2520variant%252C%2520but%2520most%250Aresearch%2520is%2520limited%2520to%2520solving%2520small-scale%252C%2520synthetic%2520problems%252C%2520neglecting%2520the%250Achallenges%2520posed%2520by%2520real-world%2520use%2520cases.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520work%250Aproposes%2520a%2520DRL-based%2520Decision%2520Support%2520System%2520%2528DSS%2529%2520for%2520real-world%2520scale%2520DTAPS.%250ATo%2520this%2520end%252C%2520we%2520introduce%2520a%2520DRL%2520agent%2520with%2520two%2520novel%2520elements%253A%2520a%2520graph%250Astructure%2520for%2520observations%2520and%2520actions%2520that%2520can%2520effectively%2520represent%2520any%2520DTAP%250Aand%2520a%2520reward%2520function%2520that%2520is%2520provably%2520equivalent%2520to%2520the%2520objective%2520of%250Aminimizing%2520the%2520average%2520cycle%2520time%2520of%2520tasks.%2520The%2520combination%2520of%2520these%2520two%250Anovelties%2520allows%2520the%2520agent%2520to%2520learn%2520effective%2520and%2520generalizable%2520assignment%250Apolicies%2520for%2520real-world%2520scale%2520DTAPs.%2520The%2520proposed%2520DSS%2520is%2520evaluated%2520on%2520five%2520DTAP%250Ainstances%2520whose%2520parameters%2520are%2520extracted%2520from%2520real-world%2520logs%2520through%2520process%250Amining.%2520The%2520experimental%2520evaluation%2520shows%2520how%2520the%2520proposed%2520DRL%2520agent%2520matches%2520or%250Aoutperforms%2520the%2520best%2520baseline%2520in%2520all%2520DTAP%2520instances%2520and%2520generalizes%2520on%250Adifferent%2520time%2520horizons%2520and%2520across%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20decision-making%20for%20dynamic%20task%20assignment%20at%20scale&entry.906535625=Riccardo%20Lo%20Bianco%20and%20Willem%20van%20Jaarsveld%20and%20Jeroen%20Middelhuis%20and%20Luca%20Begnardi%20and%20Remco%20Dijkman&entry.1292438233=%20%20The%20Dynamic%20Task%20Assignment%20Problem%20%28DTAP%29%20concerns%20matching%20resources%20to%0Atasks%20in%20real%20time%20while%20minimizing%20some%20objectives%2C%20like%20resource%20costs%20or%0Atask%20cycle%20time.%20In%20this%20work%2C%20we%20consider%20a%20DTAP%20variant%20where%20every%20task%20is%20a%0Acase%20composed%20of%20a%20stochastic%20sequence%20of%20activities.%20The%20DTAP%2C%20in%20this%20case%2C%0Ainvolves%20the%20decision%20of%20which%20employee%20to%20assign%20to%20which%20activity%20to%20process%0Arequests%20as%20quickly%20as%20possible.%20In%20recent%20years%2C%20Deep%20Reinforcement%20Learning%0A%28DRL%29%20has%20emerged%20as%20a%20promising%20tool%20for%20tackling%20this%20DTAP%20variant%2C%20but%20most%0Aresearch%20is%20limited%20to%20solving%20small-scale%2C%20synthetic%20problems%2C%20neglecting%20the%0Achallenges%20posed%20by%20real-world%20use%20cases.%20To%20bridge%20this%20gap%2C%20this%20work%0Aproposes%20a%20DRL-based%20Decision%20Support%20System%20%28DSS%29%20for%20real-world%20scale%20DTAPS.%0ATo%20this%20end%2C%20we%20introduce%20a%20DRL%20agent%20with%20two%20novel%20elements%3A%20a%20graph%0Astructure%20for%20observations%20and%20actions%20that%20can%20effectively%20represent%20any%20DTAP%0Aand%20a%20reward%20function%20that%20is%20provably%20equivalent%20to%20the%20objective%20of%0Aminimizing%20the%20average%20cycle%20time%20of%20tasks.%20The%20combination%20of%20these%20two%0Anovelties%20allows%20the%20agent%20to%20learn%20effective%20and%20generalizable%20assignment%0Apolicies%20for%20real-world%20scale%20DTAPs.%20The%20proposed%20DSS%20is%20evaluated%20on%20five%20DTAP%0Ainstances%20whose%20parameters%20are%20extracted%20from%20real-world%20logs%20through%20process%0Amining.%20The%20experimental%20evaluation%20shows%20how%20the%20proposed%20DRL%20agent%20matches%20or%0Aoutperforms%20the%20best%20baseline%20in%20all%20DTAP%20instances%20and%20generalizes%20on%0Adifferent%20time%20horizons%20and%20across%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19933v1&entry.124074799=Read"},
{"title": "Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse\n  DNNs on FPGAs", "author": "Muhammad Sabih and Abrarul Karim and Jakob Wittmann and Frank Hannig and J\u00fcrgen Teich", "abstract": "  The customizability of RISC-V makes it an attractive choice for accelerating\ndeep neural networks (DNNs). It can be achieved through instruction set\nextensions and corresponding custom functional units. Yet, efficiently\nexploiting these opportunities requires a hardware/software co-design approach\nin which the DNN model, software, and hardware are designed together. In this\npaper, we propose novel RISC-V extensions for accelerating DNN models\ncontaining semi-structured and unstructured sparsity. While the idea of\naccelerating structured and unstructured pruning is not new, our novel design\noffers various advantages over other designs. To exploit semi-structured\nsparsity, we take advantage of the fine-grained (bit-level) configurability of\nFPGAs and suggest reserving a few bits in a block of DNN weights to encode the\ninformation about sparsity in the succeeding blocks. The proposed custom\nfunctional unit utilizes this information to skip computations. To exploit\nunstructured sparsity, we propose a variable cycle sequential\nmultiply-and-accumulate unit that performs only as many multiplications as the\nnon-zero weights. Our implementation of unstructured and semi-structured\npruning accelerators can provide speedups of up to a factor of 3 and 4,\nrespectively. We then propose a combined design that can accelerate both types\nof sparsities, providing speedups of up to a factor of 5. Our designs consume a\nsmall amount of additional FPGA resources such that the resulting co-designs\nenable the acceleration of DNNs even on small FPGAs. We benchmark our designs\non standard TinyML applications such as keyword spotting, image classification,\nand person detection.\n", "link": "http://arxiv.org/abs/2504.19659v1", "date": "2025-04-28", "relevancy": 1.9232, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5223}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4913}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hardware/Software%20Co-Design%20of%20RISC-V%20Extensions%20for%20Accelerating%20Sparse%0A%20%20DNNs%20on%20FPGAs&body=Title%3A%20Hardware/Software%20Co-Design%20of%20RISC-V%20Extensions%20for%20Accelerating%20Sparse%0A%20%20DNNs%20on%20FPGAs%0AAuthor%3A%20Muhammad%20Sabih%20and%20Abrarul%20Karim%20and%20Jakob%20Wittmann%20and%20Frank%20Hannig%20and%20J%C3%BCrgen%20Teich%0AAbstract%3A%20%20%20The%20customizability%20of%20RISC-V%20makes%20it%20an%20attractive%20choice%20for%20accelerating%0Adeep%20neural%20networks%20%28DNNs%29.%20It%20can%20be%20achieved%20through%20instruction%20set%0Aextensions%20and%20corresponding%20custom%20functional%20units.%20Yet%2C%20efficiently%0Aexploiting%20these%20opportunities%20requires%20a%20hardware/software%20co-design%20approach%0Ain%20which%20the%20DNN%20model%2C%20software%2C%20and%20hardware%20are%20designed%20together.%20In%20this%0Apaper%2C%20we%20propose%20novel%20RISC-V%20extensions%20for%20accelerating%20DNN%20models%0Acontaining%20semi-structured%20and%20unstructured%20sparsity.%20While%20the%20idea%20of%0Aaccelerating%20structured%20and%20unstructured%20pruning%20is%20not%20new%2C%20our%20novel%20design%0Aoffers%20various%20advantages%20over%20other%20designs.%20To%20exploit%20semi-structured%0Asparsity%2C%20we%20take%20advantage%20of%20the%20fine-grained%20%28bit-level%29%20configurability%20of%0AFPGAs%20and%20suggest%20reserving%20a%20few%20bits%20in%20a%20block%20of%20DNN%20weights%20to%20encode%20the%0Ainformation%20about%20sparsity%20in%20the%20succeeding%20blocks.%20The%20proposed%20custom%0Afunctional%20unit%20utilizes%20this%20information%20to%20skip%20computations.%20To%20exploit%0Aunstructured%20sparsity%2C%20we%20propose%20a%20variable%20cycle%20sequential%0Amultiply-and-accumulate%20unit%20that%20performs%20only%20as%20many%20multiplications%20as%20the%0Anon-zero%20weights.%20Our%20implementation%20of%20unstructured%20and%20semi-structured%0Apruning%20accelerators%20can%20provide%20speedups%20of%20up%20to%20a%20factor%20of%203%20and%204%2C%0Arespectively.%20We%20then%20propose%20a%20combined%20design%20that%20can%20accelerate%20both%20types%0Aof%20sparsities%2C%20providing%20speedups%20of%20up%20to%20a%20factor%20of%205.%20Our%20designs%20consume%20a%0Asmall%20amount%20of%20additional%20FPGA%20resources%20such%20that%20the%20resulting%20co-designs%0Aenable%20the%20acceleration%20of%20DNNs%20even%20on%20small%20FPGAs.%20We%20benchmark%20our%20designs%0Aon%20standard%20TinyML%20applications%20such%20as%20keyword%20spotting%2C%20image%20classification%2C%0Aand%20person%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHardware/Software%2520Co-Design%2520of%2520RISC-V%2520Extensions%2520for%2520Accelerating%2520Sparse%250A%2520%2520DNNs%2520on%2520FPGAs%26entry.906535625%3DMuhammad%2520Sabih%2520and%2520Abrarul%2520Karim%2520and%2520Jakob%2520Wittmann%2520and%2520Frank%2520Hannig%2520and%2520J%25C3%25BCrgen%2520Teich%26entry.1292438233%3D%2520%2520The%2520customizability%2520of%2520RISC-V%2520makes%2520it%2520an%2520attractive%2520choice%2520for%2520accelerating%250Adeep%2520neural%2520networks%2520%2528DNNs%2529.%2520It%2520can%2520be%2520achieved%2520through%2520instruction%2520set%250Aextensions%2520and%2520corresponding%2520custom%2520functional%2520units.%2520Yet%252C%2520efficiently%250Aexploiting%2520these%2520opportunities%2520requires%2520a%2520hardware/software%2520co-design%2520approach%250Ain%2520which%2520the%2520DNN%2520model%252C%2520software%252C%2520and%2520hardware%2520are%2520designed%2520together.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520novel%2520RISC-V%2520extensions%2520for%2520accelerating%2520DNN%2520models%250Acontaining%2520semi-structured%2520and%2520unstructured%2520sparsity.%2520While%2520the%2520idea%2520of%250Aaccelerating%2520structured%2520and%2520unstructured%2520pruning%2520is%2520not%2520new%252C%2520our%2520novel%2520design%250Aoffers%2520various%2520advantages%2520over%2520other%2520designs.%2520To%2520exploit%2520semi-structured%250Asparsity%252C%2520we%2520take%2520advantage%2520of%2520the%2520fine-grained%2520%2528bit-level%2529%2520configurability%2520of%250AFPGAs%2520and%2520suggest%2520reserving%2520a%2520few%2520bits%2520in%2520a%2520block%2520of%2520DNN%2520weights%2520to%2520encode%2520the%250Ainformation%2520about%2520sparsity%2520in%2520the%2520succeeding%2520blocks.%2520The%2520proposed%2520custom%250Afunctional%2520unit%2520utilizes%2520this%2520information%2520to%2520skip%2520computations.%2520To%2520exploit%250Aunstructured%2520sparsity%252C%2520we%2520propose%2520a%2520variable%2520cycle%2520sequential%250Amultiply-and-accumulate%2520unit%2520that%2520performs%2520only%2520as%2520many%2520multiplications%2520as%2520the%250Anon-zero%2520weights.%2520Our%2520implementation%2520of%2520unstructured%2520and%2520semi-structured%250Apruning%2520accelerators%2520can%2520provide%2520speedups%2520of%2520up%2520to%2520a%2520factor%2520of%25203%2520and%25204%252C%250Arespectively.%2520We%2520then%2520propose%2520a%2520combined%2520design%2520that%2520can%2520accelerate%2520both%2520types%250Aof%2520sparsities%252C%2520providing%2520speedups%2520of%2520up%2520to%2520a%2520factor%2520of%25205.%2520Our%2520designs%2520consume%2520a%250Asmall%2520amount%2520of%2520additional%2520FPGA%2520resources%2520such%2520that%2520the%2520resulting%2520co-designs%250Aenable%2520the%2520acceleration%2520of%2520DNNs%2520even%2520on%2520small%2520FPGAs.%2520We%2520benchmark%2520our%2520designs%250Aon%2520standard%2520TinyML%2520applications%2520such%2520as%2520keyword%2520spotting%252C%2520image%2520classification%252C%250Aand%2520person%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hardware/Software%20Co-Design%20of%20RISC-V%20Extensions%20for%20Accelerating%20Sparse%0A%20%20DNNs%20on%20FPGAs&entry.906535625=Muhammad%20Sabih%20and%20Abrarul%20Karim%20and%20Jakob%20Wittmann%20and%20Frank%20Hannig%20and%20J%C3%BCrgen%20Teich&entry.1292438233=%20%20The%20customizability%20of%20RISC-V%20makes%20it%20an%20attractive%20choice%20for%20accelerating%0Adeep%20neural%20networks%20%28DNNs%29.%20It%20can%20be%20achieved%20through%20instruction%20set%0Aextensions%20and%20corresponding%20custom%20functional%20units.%20Yet%2C%20efficiently%0Aexploiting%20these%20opportunities%20requires%20a%20hardware/software%20co-design%20approach%0Ain%20which%20the%20DNN%20model%2C%20software%2C%20and%20hardware%20are%20designed%20together.%20In%20this%0Apaper%2C%20we%20propose%20novel%20RISC-V%20extensions%20for%20accelerating%20DNN%20models%0Acontaining%20semi-structured%20and%20unstructured%20sparsity.%20While%20the%20idea%20of%0Aaccelerating%20structured%20and%20unstructured%20pruning%20is%20not%20new%2C%20our%20novel%20design%0Aoffers%20various%20advantages%20over%20other%20designs.%20To%20exploit%20semi-structured%0Asparsity%2C%20we%20take%20advantage%20of%20the%20fine-grained%20%28bit-level%29%20configurability%20of%0AFPGAs%20and%20suggest%20reserving%20a%20few%20bits%20in%20a%20block%20of%20DNN%20weights%20to%20encode%20the%0Ainformation%20about%20sparsity%20in%20the%20succeeding%20blocks.%20The%20proposed%20custom%0Afunctional%20unit%20utilizes%20this%20information%20to%20skip%20computations.%20To%20exploit%0Aunstructured%20sparsity%2C%20we%20propose%20a%20variable%20cycle%20sequential%0Amultiply-and-accumulate%20unit%20that%20performs%20only%20as%20many%20multiplications%20as%20the%0Anon-zero%20weights.%20Our%20implementation%20of%20unstructured%20and%20semi-structured%0Apruning%20accelerators%20can%20provide%20speedups%20of%20up%20to%20a%20factor%20of%203%20and%204%2C%0Arespectively.%20We%20then%20propose%20a%20combined%20design%20that%20can%20accelerate%20both%20types%0Aof%20sparsities%2C%20providing%20speedups%20of%20up%20to%20a%20factor%20of%205.%20Our%20designs%20consume%20a%0Asmall%20amount%20of%20additional%20FPGA%20resources%20such%20that%20the%20resulting%20co-designs%0Aenable%20the%20acceleration%20of%20DNNs%20even%20on%20small%20FPGAs.%20We%20benchmark%20our%20designs%0Aon%20standard%20TinyML%20applications%20such%20as%20keyword%20spotting%2C%20image%20classification%2C%0Aand%20person%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19659v1&entry.124074799=Read"},
{"title": "Securing Agentic AI: A Comprehensive Threat Model and Mitigation\n  Framework for Generative AI Agents", "author": "Vineeth Sai Narajala and Om Narayan", "abstract": "  As generative AI (GenAI) agents become more common in enterprise settings,\nthey introduce security challenges that differ significantly from those posed\nby traditional systems. These agents are not just LLMs; they reason, remember,\nand act, often with minimal human oversight. This paper introduces a\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\nhow their autonomy, persistent memory access, complex reasoning, and tool\nintegration create novel risks. This research work identifies 9 primary threats\nand organizes them across five key domains: cognitive architecture\nvulnerabilities, temporal persistence threats, operational execution\nvulnerabilities, trust boundary violations, and governance circumvention. These\nthreats are not just theoretical they bring practical challenges such as\ndelayed exploitability, cross-system propagation, cross system lateral\nmovement, and subtle goal misalignments that are hard to detect with existing\nframeworks and standard approaches. To help address this, the research work\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\nframework proposing practical mitigation strategies designed to reduce\nenterprise exposure. While this work builds on existing work in LLM and AI\nsecurity, the focus is squarely on what makes agents different and why those\ndifferences matter. Ultimately, this research argues that GenAI agents require\na new lens for security. If we fail to adapt our threat models and defenses to\naccount for their unique architecture and behavior, we risk turning a powerful\nnew tool into a serious enterprise liability.\n", "link": "http://arxiv.org/abs/2504.19956v1", "date": "2025-04-28", "relevancy": 1.9044, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5098}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4815}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Securing%20Agentic%20AI%3A%20A%20Comprehensive%20Threat%20Model%20and%20Mitigation%0A%20%20Framework%20for%20Generative%20AI%20Agents&body=Title%3A%20Securing%20Agentic%20AI%3A%20A%20Comprehensive%20Threat%20Model%20and%20Mitigation%0A%20%20Framework%20for%20Generative%20AI%20Agents%0AAuthor%3A%20Vineeth%20Sai%20Narajala%20and%20Om%20Narayan%0AAbstract%3A%20%20%20As%20generative%20AI%20%28GenAI%29%20agents%20become%20more%20common%20in%20enterprise%20settings%2C%0Athey%20introduce%20security%20challenges%20that%20differ%20significantly%20from%20those%20posed%0Aby%20traditional%20systems.%20These%20agents%20are%20not%20just%20LLMs%3B%20they%20reason%2C%20remember%2C%0Aand%20act%2C%20often%20with%20minimal%20human%20oversight.%20This%20paper%20introduces%20a%0Acomprehensive%20threat%20model%20tailored%20specifically%20for%20GenAI%20agents%2C%20focusing%20on%0Ahow%20their%20autonomy%2C%20persistent%20memory%20access%2C%20complex%20reasoning%2C%20and%20tool%0Aintegration%20create%20novel%20risks.%20This%20research%20work%20identifies%209%20primary%20threats%0Aand%20organizes%20them%20across%20five%20key%20domains%3A%20cognitive%20architecture%0Avulnerabilities%2C%20temporal%20persistence%20threats%2C%20operational%20execution%0Avulnerabilities%2C%20trust%20boundary%20violations%2C%20and%20governance%20circumvention.%20These%0Athreats%20are%20not%20just%20theoretical%20they%20bring%20practical%20challenges%20such%20as%0Adelayed%20exploitability%2C%20cross-system%20propagation%2C%20cross%20system%20lateral%0Amovement%2C%20and%20subtle%20goal%20misalignments%20that%20are%20hard%20to%20detect%20with%20existing%0Aframeworks%20and%20standard%20approaches.%20To%20help%20address%20this%2C%20the%20research%20work%0Apresent%20two%20complementary%20frameworks%3A%20ATFAA%20-%20Advanced%20Threat%20Framework%20for%0AAutonomous%20AI%20Agents%2C%20which%20organizes%20agent-specific%20risks%2C%20and%20SHIELD%2C%20a%0Aframework%20proposing%20practical%20mitigation%20strategies%20designed%20to%20reduce%0Aenterprise%20exposure.%20While%20this%20work%20builds%20on%20existing%20work%20in%20LLM%20and%20AI%0Asecurity%2C%20the%20focus%20is%20squarely%20on%20what%20makes%20agents%20different%20and%20why%20those%0Adifferences%20matter.%20Ultimately%2C%20this%20research%20argues%20that%20GenAI%20agents%20require%0Aa%20new%20lens%20for%20security.%20If%20we%20fail%20to%20adapt%20our%20threat%20models%20and%20defenses%20to%0Aaccount%20for%20their%20unique%20architecture%20and%20behavior%2C%20we%20risk%20turning%20a%20powerful%0Anew%20tool%20into%20a%20serious%20enterprise%20liability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecuring%2520Agentic%2520AI%253A%2520A%2520Comprehensive%2520Threat%2520Model%2520and%2520Mitigation%250A%2520%2520Framework%2520for%2520Generative%2520AI%2520Agents%26entry.906535625%3DVineeth%2520Sai%2520Narajala%2520and%2520Om%2520Narayan%26entry.1292438233%3D%2520%2520As%2520generative%2520AI%2520%2528GenAI%2529%2520agents%2520become%2520more%2520common%2520in%2520enterprise%2520settings%252C%250Athey%2520introduce%2520security%2520challenges%2520that%2520differ%2520significantly%2520from%2520those%2520posed%250Aby%2520traditional%2520systems.%2520These%2520agents%2520are%2520not%2520just%2520LLMs%253B%2520they%2520reason%252C%2520remember%252C%250Aand%2520act%252C%2520often%2520with%2520minimal%2520human%2520oversight.%2520This%2520paper%2520introduces%2520a%250Acomprehensive%2520threat%2520model%2520tailored%2520specifically%2520for%2520GenAI%2520agents%252C%2520focusing%2520on%250Ahow%2520their%2520autonomy%252C%2520persistent%2520memory%2520access%252C%2520complex%2520reasoning%252C%2520and%2520tool%250Aintegration%2520create%2520novel%2520risks.%2520This%2520research%2520work%2520identifies%25209%2520primary%2520threats%250Aand%2520organizes%2520them%2520across%2520five%2520key%2520domains%253A%2520cognitive%2520architecture%250Avulnerabilities%252C%2520temporal%2520persistence%2520threats%252C%2520operational%2520execution%250Avulnerabilities%252C%2520trust%2520boundary%2520violations%252C%2520and%2520governance%2520circumvention.%2520These%250Athreats%2520are%2520not%2520just%2520theoretical%2520they%2520bring%2520practical%2520challenges%2520such%2520as%250Adelayed%2520exploitability%252C%2520cross-system%2520propagation%252C%2520cross%2520system%2520lateral%250Amovement%252C%2520and%2520subtle%2520goal%2520misalignments%2520that%2520are%2520hard%2520to%2520detect%2520with%2520existing%250Aframeworks%2520and%2520standard%2520approaches.%2520To%2520help%2520address%2520this%252C%2520the%2520research%2520work%250Apresent%2520two%2520complementary%2520frameworks%253A%2520ATFAA%2520-%2520Advanced%2520Threat%2520Framework%2520for%250AAutonomous%2520AI%2520Agents%252C%2520which%2520organizes%2520agent-specific%2520risks%252C%2520and%2520SHIELD%252C%2520a%250Aframework%2520proposing%2520practical%2520mitigation%2520strategies%2520designed%2520to%2520reduce%250Aenterprise%2520exposure.%2520While%2520this%2520work%2520builds%2520on%2520existing%2520work%2520in%2520LLM%2520and%2520AI%250Asecurity%252C%2520the%2520focus%2520is%2520squarely%2520on%2520what%2520makes%2520agents%2520different%2520and%2520why%2520those%250Adifferences%2520matter.%2520Ultimately%252C%2520this%2520research%2520argues%2520that%2520GenAI%2520agents%2520require%250Aa%2520new%2520lens%2520for%2520security.%2520If%2520we%2520fail%2520to%2520adapt%2520our%2520threat%2520models%2520and%2520defenses%2520to%250Aaccount%2520for%2520their%2520unique%2520architecture%2520and%2520behavior%252C%2520we%2520risk%2520turning%2520a%2520powerful%250Anew%2520tool%2520into%2520a%2520serious%2520enterprise%2520liability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Securing%20Agentic%20AI%3A%20A%20Comprehensive%20Threat%20Model%20and%20Mitigation%0A%20%20Framework%20for%20Generative%20AI%20Agents&entry.906535625=Vineeth%20Sai%20Narajala%20and%20Om%20Narayan&entry.1292438233=%20%20As%20generative%20AI%20%28GenAI%29%20agents%20become%20more%20common%20in%20enterprise%20settings%2C%0Athey%20introduce%20security%20challenges%20that%20differ%20significantly%20from%20those%20posed%0Aby%20traditional%20systems.%20These%20agents%20are%20not%20just%20LLMs%3B%20they%20reason%2C%20remember%2C%0Aand%20act%2C%20often%20with%20minimal%20human%20oversight.%20This%20paper%20introduces%20a%0Acomprehensive%20threat%20model%20tailored%20specifically%20for%20GenAI%20agents%2C%20focusing%20on%0Ahow%20their%20autonomy%2C%20persistent%20memory%20access%2C%20complex%20reasoning%2C%20and%20tool%0Aintegration%20create%20novel%20risks.%20This%20research%20work%20identifies%209%20primary%20threats%0Aand%20organizes%20them%20across%20five%20key%20domains%3A%20cognitive%20architecture%0Avulnerabilities%2C%20temporal%20persistence%20threats%2C%20operational%20execution%0Avulnerabilities%2C%20trust%20boundary%20violations%2C%20and%20governance%20circumvention.%20These%0Athreats%20are%20not%20just%20theoretical%20they%20bring%20practical%20challenges%20such%20as%0Adelayed%20exploitability%2C%20cross-system%20propagation%2C%20cross%20system%20lateral%0Amovement%2C%20and%20subtle%20goal%20misalignments%20that%20are%20hard%20to%20detect%20with%20existing%0Aframeworks%20and%20standard%20approaches.%20To%20help%20address%20this%2C%20the%20research%20work%0Apresent%20two%20complementary%20frameworks%3A%20ATFAA%20-%20Advanced%20Threat%20Framework%20for%0AAutonomous%20AI%20Agents%2C%20which%20organizes%20agent-specific%20risks%2C%20and%20SHIELD%2C%20a%0Aframework%20proposing%20practical%20mitigation%20strategies%20designed%20to%20reduce%0Aenterprise%20exposure.%20While%20this%20work%20builds%20on%20existing%20work%20in%20LLM%20and%20AI%0Asecurity%2C%20the%20focus%20is%20squarely%20on%20what%20makes%20agents%20different%20and%20why%20those%0Adifferences%20matter.%20Ultimately%2C%20this%20research%20argues%20that%20GenAI%20agents%20require%0Aa%20new%20lens%20for%20security.%20If%20we%20fail%20to%20adapt%20our%20threat%20models%20and%20defenses%20to%0Aaccount%20for%20their%20unique%20architecture%20and%20behavior%2C%20we%20risk%20turning%20a%20powerful%0Anew%20tool%20into%20a%20serious%20enterprise%20liability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19956v1&entry.124074799=Read"},
{"title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage", "author": "Ke Hong and Lufang Chen and Zhong Wang and Xiuhong Li and Qiuli Mao and Jianping Ma and Chao Xiong and Guanyu Wu and Buhe Han and Guohao Dai and Yun Liang and Yu Wang", "abstract": "  Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.\n", "link": "http://arxiv.org/abs/2504.19867v1", "date": "2025-04-28", "relevancy": 1.8492, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.451}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20semi-PD%3A%20Towards%20Efficient%20LLM%20Serving%20via%20Phase-Wise%20Disaggregated%0A%20%20Computation%20and%20Unified%20Storage&body=Title%3A%20semi-PD%3A%20Towards%20Efficient%20LLM%20Serving%20via%20Phase-Wise%20Disaggregated%0A%20%20Computation%20and%20Unified%20Storage%0AAuthor%3A%20Ke%20Hong%20and%20Lufang%20Chen%20and%20Zhong%20Wang%20and%20Xiuhong%20Li%20and%20Qiuli%20Mao%20and%20Jianping%20Ma%20and%20Chao%20Xiong%20and%20Guanyu%20Wu%20and%20Buhe%20Han%20and%20Guohao%20Dai%20and%20Yun%20Liang%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Existing%20large%20language%20model%20%28LLM%29%20serving%20systems%20fall%20into%20two%20categories%3A%0A1%29%20a%20unified%20system%20where%20prefill%20phase%20and%20decode%20phase%20are%20co-located%20on%20the%0Asame%20GPU%2C%20sharing%20the%20unified%20computational%20resource%20and%20storage%2C%20and%202%29%20a%0Adisaggregated%20system%20where%20the%20two%20phases%20are%20disaggregated%20to%20different%20GPUs.%0AThe%20design%20of%20the%20disaggregated%20system%20addresses%20the%20latency%20interference%20and%0Asophisticated%20scheduling%20issues%20in%20the%20unified%20system%20but%20leads%20to%20storage%0Achallenges%20including%201%29%20replicated%20weights%20for%20both%20phases%20that%20prevent%0Aflexible%20deployment%2C%202%29%20KV%20cache%20transfer%20overhead%20between%20the%20two%20phases%2C%203%29%0Astorage%20imbalance%20that%20causes%20substantial%20wasted%20space%20of%20the%20GPU%20capacity%2C%20and%0A4%29%20suboptimal%20resource%20adjustment%20arising%20from%20the%20difficulties%20in%20migrating%20KV%0Acache.%20Such%20storage%20inefficiency%20delivers%20poor%20serving%20performance%20under%20high%0Arequest%20rates.%0A%20%20In%20this%20paper%2C%20we%20identify%20that%20the%20advantage%20of%20the%20disaggregated%20system%0Alies%20in%20the%20disaggregated%20computation%2C%20i.e.%2C%20partitioning%20the%20computational%0Aresource%20to%20enable%20the%20asynchronous%20computation%20of%20two%20phases.%20Thus%2C%20we%20propose%0Aa%20novel%20LLM%20serving%20system%2C%20semi-PD%2C%20characterized%20by%20disaggregated%20computation%0Aand%20unified%20storage.%20In%20semi-PD%2C%20we%20introduce%20a%20computation%20resource%20controller%0Ato%20achieve%20disaggregated%20computation%20at%20the%20streaming%20multi-processor%20%28SM%29%0Alevel%2C%20and%20a%20unified%20memory%20manager%20to%20manage%20the%20asynchronous%20memory%20access%0Afrom%20both%20phases.%20semi-PD%20has%20a%20low-overhead%20resource%20adjustment%20mechanism%0Abetween%20the%20two%20phases%2C%20and%20a%20service-level%20objective%20%28SLO%29%20aware%20dynamic%0Apartitioning%20algorithm%20to%20optimize%20the%20SLO%20attainment.%20Compared%20to%0Astate-of-the-art%20systems%2C%20semi-PD%20maintains%20lower%20latency%20at%20higher%20request%0Arates%2C%20reducing%20the%20average%20end-to-end%20latency%20per%20request%20by%201.27-2.58x%20on%0ADeepSeek%20series%20models%2C%20and%20serves%201.55-1.72x%20more%20requests%20adhering%20to%20latency%0Aconstraints%20on%20Llama%20series%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dsemi-PD%253A%2520Towards%2520Efficient%2520LLM%2520Serving%2520via%2520Phase-Wise%2520Disaggregated%250A%2520%2520Computation%2520and%2520Unified%2520Storage%26entry.906535625%3DKe%2520Hong%2520and%2520Lufang%2520Chen%2520and%2520Zhong%2520Wang%2520and%2520Xiuhong%2520Li%2520and%2520Qiuli%2520Mao%2520and%2520Jianping%2520Ma%2520and%2520Chao%2520Xiong%2520and%2520Guanyu%2520Wu%2520and%2520Buhe%2520Han%2520and%2520Guohao%2520Dai%2520and%2520Yun%2520Liang%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520large%2520language%2520model%2520%2528LLM%2529%2520serving%2520systems%2520fall%2520into%2520two%2520categories%253A%250A1%2529%2520a%2520unified%2520system%2520where%2520prefill%2520phase%2520and%2520decode%2520phase%2520are%2520co-located%2520on%2520the%250Asame%2520GPU%252C%2520sharing%2520the%2520unified%2520computational%2520resource%2520and%2520storage%252C%2520and%25202%2529%2520a%250Adisaggregated%2520system%2520where%2520the%2520two%2520phases%2520are%2520disaggregated%2520to%2520different%2520GPUs.%250AThe%2520design%2520of%2520the%2520disaggregated%2520system%2520addresses%2520the%2520latency%2520interference%2520and%250Asophisticated%2520scheduling%2520issues%2520in%2520the%2520unified%2520system%2520but%2520leads%2520to%2520storage%250Achallenges%2520including%25201%2529%2520replicated%2520weights%2520for%2520both%2520phases%2520that%2520prevent%250Aflexible%2520deployment%252C%25202%2529%2520KV%2520cache%2520transfer%2520overhead%2520between%2520the%2520two%2520phases%252C%25203%2529%250Astorage%2520imbalance%2520that%2520causes%2520substantial%2520wasted%2520space%2520of%2520the%2520GPU%2520capacity%252C%2520and%250A4%2529%2520suboptimal%2520resource%2520adjustment%2520arising%2520from%2520the%2520difficulties%2520in%2520migrating%2520KV%250Acache.%2520Such%2520storage%2520inefficiency%2520delivers%2520poor%2520serving%2520performance%2520under%2520high%250Arequest%2520rates.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520identify%2520that%2520the%2520advantage%2520of%2520the%2520disaggregated%2520system%250Alies%2520in%2520the%2520disaggregated%2520computation%252C%2520i.e.%252C%2520partitioning%2520the%2520computational%250Aresource%2520to%2520enable%2520the%2520asynchronous%2520computation%2520of%2520two%2520phases.%2520Thus%252C%2520we%2520propose%250Aa%2520novel%2520LLM%2520serving%2520system%252C%2520semi-PD%252C%2520characterized%2520by%2520disaggregated%2520computation%250Aand%2520unified%2520storage.%2520In%2520semi-PD%252C%2520we%2520introduce%2520a%2520computation%2520resource%2520controller%250Ato%2520achieve%2520disaggregated%2520computation%2520at%2520the%2520streaming%2520multi-processor%2520%2528SM%2529%250Alevel%252C%2520and%2520a%2520unified%2520memory%2520manager%2520to%2520manage%2520the%2520asynchronous%2520memory%2520access%250Afrom%2520both%2520phases.%2520semi-PD%2520has%2520a%2520low-overhead%2520resource%2520adjustment%2520mechanism%250Abetween%2520the%2520two%2520phases%252C%2520and%2520a%2520service-level%2520objective%2520%2528SLO%2529%2520aware%2520dynamic%250Apartitioning%2520algorithm%2520to%2520optimize%2520the%2520SLO%2520attainment.%2520Compared%2520to%250Astate-of-the-art%2520systems%252C%2520semi-PD%2520maintains%2520lower%2520latency%2520at%2520higher%2520request%250Arates%252C%2520reducing%2520the%2520average%2520end-to-end%2520latency%2520per%2520request%2520by%25201.27-2.58x%2520on%250ADeepSeek%2520series%2520models%252C%2520and%2520serves%25201.55-1.72x%2520more%2520requests%2520adhering%2520to%2520latency%250Aconstraints%2520on%2520Llama%2520series%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=semi-PD%3A%20Towards%20Efficient%20LLM%20Serving%20via%20Phase-Wise%20Disaggregated%0A%20%20Computation%20and%20Unified%20Storage&entry.906535625=Ke%20Hong%20and%20Lufang%20Chen%20and%20Zhong%20Wang%20and%20Xiuhong%20Li%20and%20Qiuli%20Mao%20and%20Jianping%20Ma%20and%20Chao%20Xiong%20and%20Guanyu%20Wu%20and%20Buhe%20Han%20and%20Guohao%20Dai%20and%20Yun%20Liang%20and%20Yu%20Wang&entry.1292438233=%20%20Existing%20large%20language%20model%20%28LLM%29%20serving%20systems%20fall%20into%20two%20categories%3A%0A1%29%20a%20unified%20system%20where%20prefill%20phase%20and%20decode%20phase%20are%20co-located%20on%20the%0Asame%20GPU%2C%20sharing%20the%20unified%20computational%20resource%20and%20storage%2C%20and%202%29%20a%0Adisaggregated%20system%20where%20the%20two%20phases%20are%20disaggregated%20to%20different%20GPUs.%0AThe%20design%20of%20the%20disaggregated%20system%20addresses%20the%20latency%20interference%20and%0Asophisticated%20scheduling%20issues%20in%20the%20unified%20system%20but%20leads%20to%20storage%0Achallenges%20including%201%29%20replicated%20weights%20for%20both%20phases%20that%20prevent%0Aflexible%20deployment%2C%202%29%20KV%20cache%20transfer%20overhead%20between%20the%20two%20phases%2C%203%29%0Astorage%20imbalance%20that%20causes%20substantial%20wasted%20space%20of%20the%20GPU%20capacity%2C%20and%0A4%29%20suboptimal%20resource%20adjustment%20arising%20from%20the%20difficulties%20in%20migrating%20KV%0Acache.%20Such%20storage%20inefficiency%20delivers%20poor%20serving%20performance%20under%20high%0Arequest%20rates.%0A%20%20In%20this%20paper%2C%20we%20identify%20that%20the%20advantage%20of%20the%20disaggregated%20system%0Alies%20in%20the%20disaggregated%20computation%2C%20i.e.%2C%20partitioning%20the%20computational%0Aresource%20to%20enable%20the%20asynchronous%20computation%20of%20two%20phases.%20Thus%2C%20we%20propose%0Aa%20novel%20LLM%20serving%20system%2C%20semi-PD%2C%20characterized%20by%20disaggregated%20computation%0Aand%20unified%20storage.%20In%20semi-PD%2C%20we%20introduce%20a%20computation%20resource%20controller%0Ato%20achieve%20disaggregated%20computation%20at%20the%20streaming%20multi-processor%20%28SM%29%0Alevel%2C%20and%20a%20unified%20memory%20manager%20to%20manage%20the%20asynchronous%20memory%20access%0Afrom%20both%20phases.%20semi-PD%20has%20a%20low-overhead%20resource%20adjustment%20mechanism%0Abetween%20the%20two%20phases%2C%20and%20a%20service-level%20objective%20%28SLO%29%20aware%20dynamic%0Apartitioning%20algorithm%20to%20optimize%20the%20SLO%20attainment.%20Compared%20to%0Astate-of-the-art%20systems%2C%20semi-PD%20maintains%20lower%20latency%20at%20higher%20request%0Arates%2C%20reducing%20the%20average%20end-to-end%20latency%20per%20request%20by%201.27-2.58x%20on%0ADeepSeek%20series%20models%2C%20and%20serves%201.55-1.72x%20more%20requests%20adhering%20to%20latency%0Aconstraints%20on%20Llama%20series%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19867v1&entry.124074799=Read"},
{"title": "Robust Federated Personalised Mean Estimation for the Gaussian Mixture\n  Model", "author": "Malhar A. Managoli and Vinod M. Prabhakaran and Suhas Diggavi", "abstract": "  Federated learning with heterogeneous data and personalization has received\nsignificant recent attention. Separately, robustness to corrupted data in the\ncontext of federated learning has also been studied. In this paper we explore\ncombining personalization for heterogeneous data with robustness, where a\nconstant fraction of the clients are corrupted. Motivated by this broad\nproblem, we formulate a simple instantiation which captures some of its\ndifficulty. We focus on the specific problem of personalized mean estimation\nwhere the data is drawn from a Gaussian mixture model. We give an algorithm\nwhose error depends almost linearly on the ratio of corrupted to uncorrupted\nsamples, and show a lower bound with the same behavior, albeit with a gap of a\nconstant factor.\n", "link": "http://arxiv.org/abs/2504.19955v1", "date": "2025-04-28", "relevancy": 1.8458, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4662}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Federated%20Personalised%20Mean%20Estimation%20for%20the%20Gaussian%20Mixture%0A%20%20Model&body=Title%3A%20Robust%20Federated%20Personalised%20Mean%20Estimation%20for%20the%20Gaussian%20Mixture%0A%20%20Model%0AAuthor%3A%20Malhar%20A.%20Managoli%20and%20Vinod%20M.%20Prabhakaran%20and%20Suhas%20Diggavi%0AAbstract%3A%20%20%20Federated%20learning%20with%20heterogeneous%20data%20and%20personalization%20has%20received%0Asignificant%20recent%20attention.%20Separately%2C%20robustness%20to%20corrupted%20data%20in%20the%0Acontext%20of%20federated%20learning%20has%20also%20been%20studied.%20In%20this%20paper%20we%20explore%0Acombining%20personalization%20for%20heterogeneous%20data%20with%20robustness%2C%20where%20a%0Aconstant%20fraction%20of%20the%20clients%20are%20corrupted.%20Motivated%20by%20this%20broad%0Aproblem%2C%20we%20formulate%20a%20simple%20instantiation%20which%20captures%20some%20of%20its%0Adifficulty.%20We%20focus%20on%20the%20specific%20problem%20of%20personalized%20mean%20estimation%0Awhere%20the%20data%20is%20drawn%20from%20a%20Gaussian%20mixture%20model.%20We%20give%20an%20algorithm%0Awhose%20error%20depends%20almost%20linearly%20on%20the%20ratio%20of%20corrupted%20to%20uncorrupted%0Asamples%2C%20and%20show%20a%20lower%20bound%20with%20the%20same%20behavior%2C%20albeit%20with%20a%20gap%20of%20a%0Aconstant%20factor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Federated%2520Personalised%2520Mean%2520Estimation%2520for%2520the%2520Gaussian%2520Mixture%250A%2520%2520Model%26entry.906535625%3DMalhar%2520A.%2520Managoli%2520and%2520Vinod%2520M.%2520Prabhakaran%2520and%2520Suhas%2520Diggavi%26entry.1292438233%3D%2520%2520Federated%2520learning%2520with%2520heterogeneous%2520data%2520and%2520personalization%2520has%2520received%250Asignificant%2520recent%2520attention.%2520Separately%252C%2520robustness%2520to%2520corrupted%2520data%2520in%2520the%250Acontext%2520of%2520federated%2520learning%2520has%2520also%2520been%2520studied.%2520In%2520this%2520paper%2520we%2520explore%250Acombining%2520personalization%2520for%2520heterogeneous%2520data%2520with%2520robustness%252C%2520where%2520a%250Aconstant%2520fraction%2520of%2520the%2520clients%2520are%2520corrupted.%2520Motivated%2520by%2520this%2520broad%250Aproblem%252C%2520we%2520formulate%2520a%2520simple%2520instantiation%2520which%2520captures%2520some%2520of%2520its%250Adifficulty.%2520We%2520focus%2520on%2520the%2520specific%2520problem%2520of%2520personalized%2520mean%2520estimation%250Awhere%2520the%2520data%2520is%2520drawn%2520from%2520a%2520Gaussian%2520mixture%2520model.%2520We%2520give%2520an%2520algorithm%250Awhose%2520error%2520depends%2520almost%2520linearly%2520on%2520the%2520ratio%2520of%2520corrupted%2520to%2520uncorrupted%250Asamples%252C%2520and%2520show%2520a%2520lower%2520bound%2520with%2520the%2520same%2520behavior%252C%2520albeit%2520with%2520a%2520gap%2520of%2520a%250Aconstant%2520factor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Federated%20Personalised%20Mean%20Estimation%20for%20the%20Gaussian%20Mixture%0A%20%20Model&entry.906535625=Malhar%20A.%20Managoli%20and%20Vinod%20M.%20Prabhakaran%20and%20Suhas%20Diggavi&entry.1292438233=%20%20Federated%20learning%20with%20heterogeneous%20data%20and%20personalization%20has%20received%0Asignificant%20recent%20attention.%20Separately%2C%20robustness%20to%20corrupted%20data%20in%20the%0Acontext%20of%20federated%20learning%20has%20also%20been%20studied.%20In%20this%20paper%20we%20explore%0Acombining%20personalization%20for%20heterogeneous%20data%20with%20robustness%2C%20where%20a%0Aconstant%20fraction%20of%20the%20clients%20are%20corrupted.%20Motivated%20by%20this%20broad%0Aproblem%2C%20we%20formulate%20a%20simple%20instantiation%20which%20captures%20some%20of%20its%0Adifficulty.%20We%20focus%20on%20the%20specific%20problem%20of%20personalized%20mean%20estimation%0Awhere%20the%20data%20is%20drawn%20from%20a%20Gaussian%20mixture%20model.%20We%20give%20an%20algorithm%0Awhose%20error%20depends%20almost%20linearly%20on%20the%20ratio%20of%20corrupted%20to%20uncorrupted%0Asamples%2C%20and%20show%20a%20lower%20bound%20with%20the%20same%20behavior%2C%20albeit%20with%20a%20gap%20of%20a%0Aconstant%20factor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19955v1&entry.124074799=Read"},
{"title": "Heterophily-informed Message Passing", "author": "Haishan Wang and Arno Solin and Vikas Garg", "abstract": "  Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due\nto their implicit homophily assumption. We mitigate this problem with a novel\nscheme that regulates the aggregation of messages, modulating the type and\nextent of message passing locally thereby preserving both the low and\nhigh-frequency components of information. Our approach relies solely on learnt\nembeddings, obviating the need for auxiliary labels, thus extending the\nbenefits of heterophily-aware embeddings to broader applications, e.g.,\ngenerative modelling. Our experiments, conducted across various data sets and\nGNN architectures, demonstrate performance enhancements and reveal heterophily\npatterns across standard classification benchmarks. Furthermore, application to\nmolecular generation showcases notable performance improvements on\nchemoinformatics benchmarks.\n", "link": "http://arxiv.org/abs/2504.19785v1", "date": "2025-04-28", "relevancy": 1.8097, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4659}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4444}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterophily-informed%20Message%20Passing&body=Title%3A%20Heterophily-informed%20Message%20Passing%0AAuthor%3A%20Haishan%20Wang%20and%20Arno%20Solin%20and%20Vikas%20Garg%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20known%20to%20be%20vulnerable%20to%20oversmoothing%20due%0Ato%20their%20implicit%20homophily%20assumption.%20We%20mitigate%20this%20problem%20with%20a%20novel%0Ascheme%20that%20regulates%20the%20aggregation%20of%20messages%2C%20modulating%20the%20type%20and%0Aextent%20of%20message%20passing%20locally%20thereby%20preserving%20both%20the%20low%20and%0Ahigh-frequency%20components%20of%20information.%20Our%20approach%20relies%20solely%20on%20learnt%0Aembeddings%2C%20obviating%20the%20need%20for%20auxiliary%20labels%2C%20thus%20extending%20the%0Abenefits%20of%20heterophily-aware%20embeddings%20to%20broader%20applications%2C%20e.g.%2C%0Agenerative%20modelling.%20Our%20experiments%2C%20conducted%20across%20various%20data%20sets%20and%0AGNN%20architectures%2C%20demonstrate%20performance%20enhancements%20and%20reveal%20heterophily%0Apatterns%20across%20standard%20classification%20benchmarks.%20Furthermore%2C%20application%20to%0Amolecular%20generation%20showcases%20notable%20performance%20improvements%20on%0Achemoinformatics%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterophily-informed%2520Message%2520Passing%26entry.906535625%3DHaishan%2520Wang%2520and%2520Arno%2520Solin%2520and%2520Vikas%2520Garg%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520known%2520to%2520be%2520vulnerable%2520to%2520oversmoothing%2520due%250Ato%2520their%2520implicit%2520homophily%2520assumption.%2520We%2520mitigate%2520this%2520problem%2520with%2520a%2520novel%250Ascheme%2520that%2520regulates%2520the%2520aggregation%2520of%2520messages%252C%2520modulating%2520the%2520type%2520and%250Aextent%2520of%2520message%2520passing%2520locally%2520thereby%2520preserving%2520both%2520the%2520low%2520and%250Ahigh-frequency%2520components%2520of%2520information.%2520Our%2520approach%2520relies%2520solely%2520on%2520learnt%250Aembeddings%252C%2520obviating%2520the%2520need%2520for%2520auxiliary%2520labels%252C%2520thus%2520extending%2520the%250Abenefits%2520of%2520heterophily-aware%2520embeddings%2520to%2520broader%2520applications%252C%2520e.g.%252C%250Agenerative%2520modelling.%2520Our%2520experiments%252C%2520conducted%2520across%2520various%2520data%2520sets%2520and%250AGNN%2520architectures%252C%2520demonstrate%2520performance%2520enhancements%2520and%2520reveal%2520heterophily%250Apatterns%2520across%2520standard%2520classification%2520benchmarks.%2520Furthermore%252C%2520application%2520to%250Amolecular%2520generation%2520showcases%2520notable%2520performance%2520improvements%2520on%250Achemoinformatics%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterophily-informed%20Message%20Passing&entry.906535625=Haishan%20Wang%20and%20Arno%20Solin%20and%20Vikas%20Garg&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20known%20to%20be%20vulnerable%20to%20oversmoothing%20due%0Ato%20their%20implicit%20homophily%20assumption.%20We%20mitigate%20this%20problem%20with%20a%20novel%0Ascheme%20that%20regulates%20the%20aggregation%20of%20messages%2C%20modulating%20the%20type%20and%0Aextent%20of%20message%20passing%20locally%20thereby%20preserving%20both%20the%20low%20and%0Ahigh-frequency%20components%20of%20information.%20Our%20approach%20relies%20solely%20on%20learnt%0Aembeddings%2C%20obviating%20the%20need%20for%20auxiliary%20labels%2C%20thus%20extending%20the%0Abenefits%20of%20heterophily-aware%20embeddings%20to%20broader%20applications%2C%20e.g.%2C%0Agenerative%20modelling.%20Our%20experiments%2C%20conducted%20across%20various%20data%20sets%20and%0AGNN%20architectures%2C%20demonstrate%20performance%20enhancements%20and%20reveal%20heterophily%0Apatterns%20across%20standard%20classification%20benchmarks.%20Furthermore%2C%20application%20to%0Amolecular%20generation%20showcases%20notable%20performance%20improvements%20on%0Achemoinformatics%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19785v1&entry.124074799=Read"},
{"title": "A Unified Benchmark of Federated Learning with Kolmogorov-Arnold\n  Networks for Medical Imaging", "author": "Youngjoon Lee and Jinu Gong and Joonhyuk Kang", "abstract": "  Federated Learning (FL) enables model training across decentralized devices\nwithout sharing raw data, thereby preserving privacy in sensitive domains like\nhealthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN)\narchitectures against traditional MLP across six state-of-the-art FL algorithms\non a blood cell classification dataset. Notably, our experiments demonstrate\nthat KAN can effectively replace MLP in federated environments, achieving\nsuperior performance with simpler architectures. Furthermore, we analyze the\nimpact of key hyperparameters-grid size and network architecture-on KAN\nperformance under varying degrees of Non-IID data distribution. Additionally,\nour ablation studies reveal that optimizing KAN width while maintaining minimal\ndepth yields the best performance in federated settings. As a result, these\nfindings establish KAN as a promising alternative for privacy-preserving\nmedical imaging applications in distributed healthcare. To the best of our\nknowledge, this is the first comprehensive benchmark of KAN in FL settings for\nmedical imaging task.\n", "link": "http://arxiv.org/abs/2504.19639v1", "date": "2025-04-28", "relevancy": 1.466, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4995}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4893}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Benchmark%20of%20Federated%20Learning%20with%20Kolmogorov-Arnold%0A%20%20Networks%20for%20Medical%20Imaging&body=Title%3A%20A%20Unified%20Benchmark%20of%20Federated%20Learning%20with%20Kolmogorov-Arnold%0A%20%20Networks%20for%20Medical%20Imaging%0AAuthor%3A%20Youngjoon%20Lee%20and%20Jinu%20Gong%20and%20Joonhyuk%20Kang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20model%20training%20across%20decentralized%20devices%0Awithout%20sharing%20raw%20data%2C%20thereby%20preserving%20privacy%20in%20sensitive%20domains%20like%0Ahealthcare.%20In%20this%20paper%2C%20we%20evaluate%20Kolmogorov-Arnold%20Networks%20%28KAN%29%0Aarchitectures%20against%20traditional%20MLP%20across%20six%20state-of-the-art%20FL%20algorithms%0Aon%20a%20blood%20cell%20classification%20dataset.%20Notably%2C%20our%20experiments%20demonstrate%0Athat%20KAN%20can%20effectively%20replace%20MLP%20in%20federated%20environments%2C%20achieving%0Asuperior%20performance%20with%20simpler%20architectures.%20Furthermore%2C%20we%20analyze%20the%0Aimpact%20of%20key%20hyperparameters-grid%20size%20and%20network%20architecture-on%20KAN%0Aperformance%20under%20varying%20degrees%20of%20Non-IID%20data%20distribution.%20Additionally%2C%0Aour%20ablation%20studies%20reveal%20that%20optimizing%20KAN%20width%20while%20maintaining%20minimal%0Adepth%20yields%20the%20best%20performance%20in%20federated%20settings.%20As%20a%20result%2C%20these%0Afindings%20establish%20KAN%20as%20a%20promising%20alternative%20for%20privacy-preserving%0Amedical%20imaging%20applications%20in%20distributed%20healthcare.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20comprehensive%20benchmark%20of%20KAN%20in%20FL%20settings%20for%0Amedical%20imaging%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Benchmark%2520of%2520Federated%2520Learning%2520with%2520Kolmogorov-Arnold%250A%2520%2520Networks%2520for%2520Medical%2520Imaging%26entry.906535625%3DYoungjoon%2520Lee%2520and%2520Jinu%2520Gong%2520and%2520Joonhyuk%2520Kang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520model%2520training%2520across%2520decentralized%2520devices%250Awithout%2520sharing%2520raw%2520data%252C%2520thereby%2520preserving%2520privacy%2520in%2520sensitive%2520domains%2520like%250Ahealthcare.%2520In%2520this%2520paper%252C%2520we%2520evaluate%2520Kolmogorov-Arnold%2520Networks%2520%2528KAN%2529%250Aarchitectures%2520against%2520traditional%2520MLP%2520across%2520six%2520state-of-the-art%2520FL%2520algorithms%250Aon%2520a%2520blood%2520cell%2520classification%2520dataset.%2520Notably%252C%2520our%2520experiments%2520demonstrate%250Athat%2520KAN%2520can%2520effectively%2520replace%2520MLP%2520in%2520federated%2520environments%252C%2520achieving%250Asuperior%2520performance%2520with%2520simpler%2520architectures.%2520Furthermore%252C%2520we%2520analyze%2520the%250Aimpact%2520of%2520key%2520hyperparameters-grid%2520size%2520and%2520network%2520architecture-on%2520KAN%250Aperformance%2520under%2520varying%2520degrees%2520of%2520Non-IID%2520data%2520distribution.%2520Additionally%252C%250Aour%2520ablation%2520studies%2520reveal%2520that%2520optimizing%2520KAN%2520width%2520while%2520maintaining%2520minimal%250Adepth%2520yields%2520the%2520best%2520performance%2520in%2520federated%2520settings.%2520As%2520a%2520result%252C%2520these%250Afindings%2520establish%2520KAN%2520as%2520a%2520promising%2520alternative%2520for%2520privacy-preserving%250Amedical%2520imaging%2520applications%2520in%2520distributed%2520healthcare.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520comprehensive%2520benchmark%2520of%2520KAN%2520in%2520FL%2520settings%2520for%250Amedical%2520imaging%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Benchmark%20of%20Federated%20Learning%20with%20Kolmogorov-Arnold%0A%20%20Networks%20for%20Medical%20Imaging&entry.906535625=Youngjoon%20Lee%20and%20Jinu%20Gong%20and%20Joonhyuk%20Kang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20model%20training%20across%20decentralized%20devices%0Awithout%20sharing%20raw%20data%2C%20thereby%20preserving%20privacy%20in%20sensitive%20domains%20like%0Ahealthcare.%20In%20this%20paper%2C%20we%20evaluate%20Kolmogorov-Arnold%20Networks%20%28KAN%29%0Aarchitectures%20against%20traditional%20MLP%20across%20six%20state-of-the-art%20FL%20algorithms%0Aon%20a%20blood%20cell%20classification%20dataset.%20Notably%2C%20our%20experiments%20demonstrate%0Athat%20KAN%20can%20effectively%20replace%20MLP%20in%20federated%20environments%2C%20achieving%0Asuperior%20performance%20with%20simpler%20architectures.%20Furthermore%2C%20we%20analyze%20the%0Aimpact%20of%20key%20hyperparameters-grid%20size%20and%20network%20architecture-on%20KAN%0Aperformance%20under%20varying%20degrees%20of%20Non-IID%20data%20distribution.%20Additionally%2C%0Aour%20ablation%20studies%20reveal%20that%20optimizing%20KAN%20width%20while%20maintaining%20minimal%0Adepth%20yields%20the%20best%20performance%20in%20federated%20settings.%20As%20a%20result%2C%20these%0Afindings%20establish%20KAN%20as%20a%20promising%20alternative%20for%20privacy-preserving%0Amedical%20imaging%20applications%20in%20distributed%20healthcare.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20comprehensive%20benchmark%20of%20KAN%20in%20FL%20settings%20for%0Amedical%20imaging%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19639v1&entry.124074799=Read"},
{"title": "Distributed Multi-Task Learning for Stochastic Bandits with Context\n  Distribution and Stage-wise Constraints", "author": "Jiabin Lin and Shana Moothedath", "abstract": "  We present conservative distributed multi-task learning in stochastic linear\ncontextual bandits with heterogeneous agents. This extends conservative linear\nbandits to a distributed setting where M agents tackle different but related\ntasks while adhering to stage-wise performance constraints. The exact context\nis unknown, and only a context distribution is available to the agents as in\nmany practical applications that involve a prediction mechanism to infer\ncontext, such as stock market prediction and weather forecast. We propose a\ndistributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm\nconstructs a pruned action set during each round to ensure the constraints are\nmet. Additionally, it includes synchronized sharing of estimates among agents\nvia a central server using well-structured synchronization steps. We prove the\nregret and communication bounds on the algorithm. We extend the problem to a\nsetting where the agents are unaware of the baseline reward. For this setting,\nwe provide a modified algorithm, DiSC-UCB2, and we show that the modified\nalgorithm achieves the same regret and communication bounds. We empirically\nvalidated the performance of our algorithm on synthetic data and real-world\nMovielens-100K data.\n", "link": "http://arxiv.org/abs/2401.11563v3", "date": "2025-04-28", "relevancy": 0.9188, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4668}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Multi-Task%20Learning%20for%20Stochastic%20Bandits%20with%20Context%0A%20%20Distribution%20and%20Stage-wise%20Constraints&body=Title%3A%20Distributed%20Multi-Task%20Learning%20for%20Stochastic%20Bandits%20with%20Context%0A%20%20Distribution%20and%20Stage-wise%20Constraints%0AAuthor%3A%20Jiabin%20Lin%20and%20Shana%20Moothedath%0AAbstract%3A%20%20%20We%20present%20conservative%20distributed%20multi-task%20learning%20in%20stochastic%20linear%0Acontextual%20bandits%20with%20heterogeneous%20agents.%20This%20extends%20conservative%20linear%0Abandits%20to%20a%20distributed%20setting%20where%20M%20agents%20tackle%20different%20but%20related%0Atasks%20while%20adhering%20to%20stage-wise%20performance%20constraints.%20The%20exact%20context%0Ais%20unknown%2C%20and%20only%20a%20context%20distribution%20is%20available%20to%20the%20agents%20as%20in%0Amany%20practical%20applications%20that%20involve%20a%20prediction%20mechanism%20to%20infer%0Acontext%2C%20such%20as%20stock%20market%20prediction%20and%20weather%20forecast.%20We%20propose%20a%0Adistributed%20upper%20confidence%20bound%20%28UCB%29%20algorithm%2C%20DiSC-UCB.%20Our%20algorithm%0Aconstructs%20a%20pruned%20action%20set%20during%20each%20round%20to%20ensure%20the%20constraints%20are%0Amet.%20Additionally%2C%20it%20includes%20synchronized%20sharing%20of%20estimates%20among%20agents%0Avia%20a%20central%20server%20using%20well-structured%20synchronization%20steps.%20We%20prove%20the%0Aregret%20and%20communication%20bounds%20on%20the%20algorithm.%20We%20extend%20the%20problem%20to%20a%0Asetting%20where%20the%20agents%20are%20unaware%20of%20the%20baseline%20reward.%20For%20this%20setting%2C%0Awe%20provide%20a%20modified%20algorithm%2C%20DiSC-UCB2%2C%20and%20we%20show%20that%20the%20modified%0Aalgorithm%20achieves%20the%20same%20regret%20and%20communication%20bounds.%20We%20empirically%0Avalidated%20the%20performance%20of%20our%20algorithm%20on%20synthetic%20data%20and%20real-world%0AMovielens-100K%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Multi-Task%2520Learning%2520for%2520Stochastic%2520Bandits%2520with%2520Context%250A%2520%2520Distribution%2520and%2520Stage-wise%2520Constraints%26entry.906535625%3DJiabin%2520Lin%2520and%2520Shana%2520Moothedath%26entry.1292438233%3D%2520%2520We%2520present%2520conservative%2520distributed%2520multi-task%2520learning%2520in%2520stochastic%2520linear%250Acontextual%2520bandits%2520with%2520heterogeneous%2520agents.%2520This%2520extends%2520conservative%2520linear%250Abandits%2520to%2520a%2520distributed%2520setting%2520where%2520M%2520agents%2520tackle%2520different%2520but%2520related%250Atasks%2520while%2520adhering%2520to%2520stage-wise%2520performance%2520constraints.%2520The%2520exact%2520context%250Ais%2520unknown%252C%2520and%2520only%2520a%2520context%2520distribution%2520is%2520available%2520to%2520the%2520agents%2520as%2520in%250Amany%2520practical%2520applications%2520that%2520involve%2520a%2520prediction%2520mechanism%2520to%2520infer%250Acontext%252C%2520such%2520as%2520stock%2520market%2520prediction%2520and%2520weather%2520forecast.%2520We%2520propose%2520a%250Adistributed%2520upper%2520confidence%2520bound%2520%2528UCB%2529%2520algorithm%252C%2520DiSC-UCB.%2520Our%2520algorithm%250Aconstructs%2520a%2520pruned%2520action%2520set%2520during%2520each%2520round%2520to%2520ensure%2520the%2520constraints%2520are%250Amet.%2520Additionally%252C%2520it%2520includes%2520synchronized%2520sharing%2520of%2520estimates%2520among%2520agents%250Avia%2520a%2520central%2520server%2520using%2520well-structured%2520synchronization%2520steps.%2520We%2520prove%2520the%250Aregret%2520and%2520communication%2520bounds%2520on%2520the%2520algorithm.%2520We%2520extend%2520the%2520problem%2520to%2520a%250Asetting%2520where%2520the%2520agents%2520are%2520unaware%2520of%2520the%2520baseline%2520reward.%2520For%2520this%2520setting%252C%250Awe%2520provide%2520a%2520modified%2520algorithm%252C%2520DiSC-UCB2%252C%2520and%2520we%2520show%2520that%2520the%2520modified%250Aalgorithm%2520achieves%2520the%2520same%2520regret%2520and%2520communication%2520bounds.%2520We%2520empirically%250Avalidated%2520the%2520performance%2520of%2520our%2520algorithm%2520on%2520synthetic%2520data%2520and%2520real-world%250AMovielens-100K%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Multi-Task%20Learning%20for%20Stochastic%20Bandits%20with%20Context%0A%20%20Distribution%20and%20Stage-wise%20Constraints&entry.906535625=Jiabin%20Lin%20and%20Shana%20Moothedath&entry.1292438233=%20%20We%20present%20conservative%20distributed%20multi-task%20learning%20in%20stochastic%20linear%0Acontextual%20bandits%20with%20heterogeneous%20agents.%20This%20extends%20conservative%20linear%0Abandits%20to%20a%20distributed%20setting%20where%20M%20agents%20tackle%20different%20but%20related%0Atasks%20while%20adhering%20to%20stage-wise%20performance%20constraints.%20The%20exact%20context%0Ais%20unknown%2C%20and%20only%20a%20context%20distribution%20is%20available%20to%20the%20agents%20as%20in%0Amany%20practical%20applications%20that%20involve%20a%20prediction%20mechanism%20to%20infer%0Acontext%2C%20such%20as%20stock%20market%20prediction%20and%20weather%20forecast.%20We%20propose%20a%0Adistributed%20upper%20confidence%20bound%20%28UCB%29%20algorithm%2C%20DiSC-UCB.%20Our%20algorithm%0Aconstructs%20a%20pruned%20action%20set%20during%20each%20round%20to%20ensure%20the%20constraints%20are%0Amet.%20Additionally%2C%20it%20includes%20synchronized%20sharing%20of%20estimates%20among%20agents%0Avia%20a%20central%20server%20using%20well-structured%20synchronization%20steps.%20We%20prove%20the%0Aregret%20and%20communication%20bounds%20on%20the%20algorithm.%20We%20extend%20the%20problem%20to%20a%0Asetting%20where%20the%20agents%20are%20unaware%20of%20the%20baseline%20reward.%20For%20this%20setting%2C%0Awe%20provide%20a%20modified%20algorithm%2C%20DiSC-UCB2%2C%20and%20we%20show%20that%20the%20modified%0Aalgorithm%20achieves%20the%20same%20regret%20and%20communication%20bounds.%20We%20empirically%0Avalidated%20the%20performance%20of%20our%20algorithm%20on%20synthetic%20data%20and%20real-world%0AMovielens-100K%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11563v3&entry.124074799=Read"},
{"title": "A computer vision method to estimate ventilation rate of Atlantic salmon\n  in sea fish farms", "author": "Lukas Folkman and Quynh LK Vo and Colin Johnston and Bela Stantic and Kylie A Pitt", "abstract": "  The increasing demand for aquaculture production necessitates the development\nof innovative, intelligent tools to effectively monitor and manage fish health\nand welfare. While non-invasive video monitoring has become a common practice\nin finfish aquaculture, existing intelligent monitoring methods predominantly\nfocus on assessing body condition or fish swimming patterns and are often\ndeveloped and evaluated in controlled tank environments, without demonstrating\ntheir applicability to real-world aquaculture settings in open sea farms. This\nunderscores the necessity for methods that can monitor physiological traits\ndirectly within the production environment of sea fish farms. To this end, we\nhave developed a computer vision method for monitoring ventilation rates of\nAtlantic salmon (Salmo salar), which was specifically designed for videos\nrecorded in the production environment of commercial sea fish farms using the\nexisting infrastructure. Our approach uses a fish head detection model, which\nclassifies the mouth state as either open or closed using a convolutional\nneural network. This is followed with multiple object tracking to create\ntemporal sequences of fish swimming across the field of view of the underwater\nvideo camera to estimate ventilation rates. The method demonstrated high\nefficiency, achieving a Pearson correlation coefficient of 0.82 between ground\ntruth and predicted ventilation rates in a test set of 100 fish collected\nindependently of the training data. By accurately identifying pens where fish\nexhibit signs of respiratory distress, our method offers broad applicability\nand the potential to transform fish health and welfare monitoring in finfish\naquaculture.\n", "link": "http://arxiv.org/abs/2504.19719v1", "date": "2025-04-28", "relevancy": 1.4008, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.469}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20computer%20vision%20method%20to%20estimate%20ventilation%20rate%20of%20Atlantic%20salmon%0A%20%20in%20sea%20fish%20farms&body=Title%3A%20A%20computer%20vision%20method%20to%20estimate%20ventilation%20rate%20of%20Atlantic%20salmon%0A%20%20in%20sea%20fish%20farms%0AAuthor%3A%20Lukas%20Folkman%20and%20Quynh%20LK%20Vo%20and%20Colin%20Johnston%20and%20Bela%20Stantic%20and%20Kylie%20A%20Pitt%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20aquaculture%20production%20necessitates%20the%20development%0Aof%20innovative%2C%20intelligent%20tools%20to%20effectively%20monitor%20and%20manage%20fish%20health%0Aand%20welfare.%20While%20non-invasive%20video%20monitoring%20has%20become%20a%20common%20practice%0Ain%20finfish%20aquaculture%2C%20existing%20intelligent%20monitoring%20methods%20predominantly%0Afocus%20on%20assessing%20body%20condition%20or%20fish%20swimming%20patterns%20and%20are%20often%0Adeveloped%20and%20evaluated%20in%20controlled%20tank%20environments%2C%20without%20demonstrating%0Atheir%20applicability%20to%20real-world%20aquaculture%20settings%20in%20open%20sea%20farms.%20This%0Aunderscores%20the%20necessity%20for%20methods%20that%20can%20monitor%20physiological%20traits%0Adirectly%20within%20the%20production%20environment%20of%20sea%20fish%20farms.%20To%20this%20end%2C%20we%0Ahave%20developed%20a%20computer%20vision%20method%20for%20monitoring%20ventilation%20rates%20of%0AAtlantic%20salmon%20%28Salmo%20salar%29%2C%20which%20was%20specifically%20designed%20for%20videos%0Arecorded%20in%20the%20production%20environment%20of%20commercial%20sea%20fish%20farms%20using%20the%0Aexisting%20infrastructure.%20Our%20approach%20uses%20a%20fish%20head%20detection%20model%2C%20which%0Aclassifies%20the%20mouth%20state%20as%20either%20open%20or%20closed%20using%20a%20convolutional%0Aneural%20network.%20This%20is%20followed%20with%20multiple%20object%20tracking%20to%20create%0Atemporal%20sequences%20of%20fish%20swimming%20across%20the%20field%20of%20view%20of%20the%20underwater%0Avideo%20camera%20to%20estimate%20ventilation%20rates.%20The%20method%20demonstrated%20high%0Aefficiency%2C%20achieving%20a%20Pearson%20correlation%20coefficient%20of%200.82%20between%20ground%0Atruth%20and%20predicted%20ventilation%20rates%20in%20a%20test%20set%20of%20100%20fish%20collected%0Aindependently%20of%20the%20training%20data.%20By%20accurately%20identifying%20pens%20where%20fish%0Aexhibit%20signs%20of%20respiratory%20distress%2C%20our%20method%20offers%20broad%20applicability%0Aand%20the%20potential%20to%20transform%20fish%20health%20and%20welfare%20monitoring%20in%20finfish%0Aaquaculture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520computer%2520vision%2520method%2520to%2520estimate%2520ventilation%2520rate%2520of%2520Atlantic%2520salmon%250A%2520%2520in%2520sea%2520fish%2520farms%26entry.906535625%3DLukas%2520Folkman%2520and%2520Quynh%2520LK%2520Vo%2520and%2520Colin%2520Johnston%2520and%2520Bela%2520Stantic%2520and%2520Kylie%2520A%2520Pitt%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520aquaculture%2520production%2520necessitates%2520the%2520development%250Aof%2520innovative%252C%2520intelligent%2520tools%2520to%2520effectively%2520monitor%2520and%2520manage%2520fish%2520health%250Aand%2520welfare.%2520While%2520non-invasive%2520video%2520monitoring%2520has%2520become%2520a%2520common%2520practice%250Ain%2520finfish%2520aquaculture%252C%2520existing%2520intelligent%2520monitoring%2520methods%2520predominantly%250Afocus%2520on%2520assessing%2520body%2520condition%2520or%2520fish%2520swimming%2520patterns%2520and%2520are%2520often%250Adeveloped%2520and%2520evaluated%2520in%2520controlled%2520tank%2520environments%252C%2520without%2520demonstrating%250Atheir%2520applicability%2520to%2520real-world%2520aquaculture%2520settings%2520in%2520open%2520sea%2520farms.%2520This%250Aunderscores%2520the%2520necessity%2520for%2520methods%2520that%2520can%2520monitor%2520physiological%2520traits%250Adirectly%2520within%2520the%2520production%2520environment%2520of%2520sea%2520fish%2520farms.%2520To%2520this%2520end%252C%2520we%250Ahave%2520developed%2520a%2520computer%2520vision%2520method%2520for%2520monitoring%2520ventilation%2520rates%2520of%250AAtlantic%2520salmon%2520%2528Salmo%2520salar%2529%252C%2520which%2520was%2520specifically%2520designed%2520for%2520videos%250Arecorded%2520in%2520the%2520production%2520environment%2520of%2520commercial%2520sea%2520fish%2520farms%2520using%2520the%250Aexisting%2520infrastructure.%2520Our%2520approach%2520uses%2520a%2520fish%2520head%2520detection%2520model%252C%2520which%250Aclassifies%2520the%2520mouth%2520state%2520as%2520either%2520open%2520or%2520closed%2520using%2520a%2520convolutional%250Aneural%2520network.%2520This%2520is%2520followed%2520with%2520multiple%2520object%2520tracking%2520to%2520create%250Atemporal%2520sequences%2520of%2520fish%2520swimming%2520across%2520the%2520field%2520of%2520view%2520of%2520the%2520underwater%250Avideo%2520camera%2520to%2520estimate%2520ventilation%2520rates.%2520The%2520method%2520demonstrated%2520high%250Aefficiency%252C%2520achieving%2520a%2520Pearson%2520correlation%2520coefficient%2520of%25200.82%2520between%2520ground%250Atruth%2520and%2520predicted%2520ventilation%2520rates%2520in%2520a%2520test%2520set%2520of%2520100%2520fish%2520collected%250Aindependently%2520of%2520the%2520training%2520data.%2520By%2520accurately%2520identifying%2520pens%2520where%2520fish%250Aexhibit%2520signs%2520of%2520respiratory%2520distress%252C%2520our%2520method%2520offers%2520broad%2520applicability%250Aand%2520the%2520potential%2520to%2520transform%2520fish%2520health%2520and%2520welfare%2520monitoring%2520in%2520finfish%250Aaquaculture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20computer%20vision%20method%20to%20estimate%20ventilation%20rate%20of%20Atlantic%20salmon%0A%20%20in%20sea%20fish%20farms&entry.906535625=Lukas%20Folkman%20and%20Quynh%20LK%20Vo%20and%20Colin%20Johnston%20and%20Bela%20Stantic%20and%20Kylie%20A%20Pitt&entry.1292438233=%20%20The%20increasing%20demand%20for%20aquaculture%20production%20necessitates%20the%20development%0Aof%20innovative%2C%20intelligent%20tools%20to%20effectively%20monitor%20and%20manage%20fish%20health%0Aand%20welfare.%20While%20non-invasive%20video%20monitoring%20has%20become%20a%20common%20practice%0Ain%20finfish%20aquaculture%2C%20existing%20intelligent%20monitoring%20methods%20predominantly%0Afocus%20on%20assessing%20body%20condition%20or%20fish%20swimming%20patterns%20and%20are%20often%0Adeveloped%20and%20evaluated%20in%20controlled%20tank%20environments%2C%20without%20demonstrating%0Atheir%20applicability%20to%20real-world%20aquaculture%20settings%20in%20open%20sea%20farms.%20This%0Aunderscores%20the%20necessity%20for%20methods%20that%20can%20monitor%20physiological%20traits%0Adirectly%20within%20the%20production%20environment%20of%20sea%20fish%20farms.%20To%20this%20end%2C%20we%0Ahave%20developed%20a%20computer%20vision%20method%20for%20monitoring%20ventilation%20rates%20of%0AAtlantic%20salmon%20%28Salmo%20salar%29%2C%20which%20was%20specifically%20designed%20for%20videos%0Arecorded%20in%20the%20production%20environment%20of%20commercial%20sea%20fish%20farms%20using%20the%0Aexisting%20infrastructure.%20Our%20approach%20uses%20a%20fish%20head%20detection%20model%2C%20which%0Aclassifies%20the%20mouth%20state%20as%20either%20open%20or%20closed%20using%20a%20convolutional%0Aneural%20network.%20This%20is%20followed%20with%20multiple%20object%20tracking%20to%20create%0Atemporal%20sequences%20of%20fish%20swimming%20across%20the%20field%20of%20view%20of%20the%20underwater%0Avideo%20camera%20to%20estimate%20ventilation%20rates.%20The%20method%20demonstrated%20high%0Aefficiency%2C%20achieving%20a%20Pearson%20correlation%20coefficient%20of%200.82%20between%20ground%0Atruth%20and%20predicted%20ventilation%20rates%20in%20a%20test%20set%20of%20100%20fish%20collected%0Aindependently%20of%20the%20training%20data.%20By%20accurately%20identifying%20pens%20where%20fish%0Aexhibit%20signs%20of%20respiratory%20distress%2C%20our%20method%20offers%20broad%20applicability%0Aand%20the%20potential%20to%20transform%20fish%20health%20and%20welfare%20monitoring%20in%20finfish%0Aaquaculture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19719v1&entry.124074799=Read"},
{"title": "How Group Lives Go Well", "author": "John Beverley and Regina Hurley", "abstract": "  This paper explores the ontological space of group well being, proposing a\nframework for representing collective welfare, group functions, and long term\ncontributions within an ontology engineering context. Traditional well being\ntheories focus on individual states, often relying on hedonistic, desire\nsatisfaction, or objective list models. Such approaches struggle to account for\ncases where individual sacrifices contribute to broader social progress, a\ncritical challenge in modeling group flourishing. To address this, the paper\nrefines and extends the Counterfactual Account (CT) of well being, which\nevaluates goodness of an event by comparing an individual's actual well being\nwith a hypothetical counterpart in a nearby possible world. While useful, this\nframework is insufficient for group level ontologies, where well being depends\non functional persistence, institutional roles, and historical impact rather\nthan immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the\npaper introduces a model in which group flourishing is evaluated in terms of\ngroup functional, where members bear roles and exhibit persistence conditions\nakin to biological systems or designed artifacts. This approach enables\nsemantic interoperability for modeling longitudinal social contributions,\nallowing for structured reasoning about group welfare, social institutions, and\ngroup flourishing over time.\n", "link": "http://arxiv.org/abs/2504.19968v1", "date": "2025-04-28", "relevancy": 0.8031, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4241}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Group%20Lives%20Go%20Well&body=Title%3A%20How%20Group%20Lives%20Go%20Well%0AAuthor%3A%20John%20Beverley%20and%20Regina%20Hurley%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20ontological%20space%20of%20group%20well%20being%2C%20proposing%20a%0Aframework%20for%20representing%20collective%20welfare%2C%20group%20functions%2C%20and%20long%20term%0Acontributions%20within%20an%20ontology%20engineering%20context.%20Traditional%20well%20being%0Atheories%20focus%20on%20individual%20states%2C%20often%20relying%20on%20hedonistic%2C%20desire%0Asatisfaction%2C%20or%20objective%20list%20models.%20Such%20approaches%20struggle%20to%20account%20for%0Acases%20where%20individual%20sacrifices%20contribute%20to%20broader%20social%20progress%2C%20a%0Acritical%20challenge%20in%20modeling%20group%20flourishing.%20To%20address%20this%2C%20the%20paper%0Arefines%20and%20extends%20the%20Counterfactual%20Account%20%28CT%29%20of%20well%20being%2C%20which%0Aevaluates%20goodness%20of%20an%20event%20by%20comparing%20an%20individual%27s%20actual%20well%20being%0Awith%20a%20hypothetical%20counterpart%20in%20a%20nearby%20possible%20world.%20While%20useful%2C%20this%0Aframework%20is%20insufficient%20for%20group%20level%20ontologies%2C%20where%20well%20being%20depends%0Aon%20functional%20persistence%2C%20institutional%20roles%2C%20and%20historical%20impact%20rather%0Athan%20immediate%20individual%20outcomes.%20Drawing%20on%20Basic%20Formal%20Ontology%20%28BFO%29%2C%20the%0Apaper%20introduces%20a%20model%20in%20which%20group%20flourishing%20is%20evaluated%20in%20terms%20of%0Agroup%20functional%2C%20where%20members%20bear%20roles%20and%20exhibit%20persistence%20conditions%0Aakin%20to%20biological%20systems%20or%20designed%20artifacts.%20This%20approach%20enables%0Asemantic%20interoperability%20for%20modeling%20longitudinal%20social%20contributions%2C%0Aallowing%20for%20structured%20reasoning%20about%20group%20welfare%2C%20social%20institutions%2C%20and%0Agroup%20flourishing%20over%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Group%2520Lives%2520Go%2520Well%26entry.906535625%3DJohn%2520Beverley%2520and%2520Regina%2520Hurley%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520ontological%2520space%2520of%2520group%2520well%2520being%252C%2520proposing%2520a%250Aframework%2520for%2520representing%2520collective%2520welfare%252C%2520group%2520functions%252C%2520and%2520long%2520term%250Acontributions%2520within%2520an%2520ontology%2520engineering%2520context.%2520Traditional%2520well%2520being%250Atheories%2520focus%2520on%2520individual%2520states%252C%2520often%2520relying%2520on%2520hedonistic%252C%2520desire%250Asatisfaction%252C%2520or%2520objective%2520list%2520models.%2520Such%2520approaches%2520struggle%2520to%2520account%2520for%250Acases%2520where%2520individual%2520sacrifices%2520contribute%2520to%2520broader%2520social%2520progress%252C%2520a%250Acritical%2520challenge%2520in%2520modeling%2520group%2520flourishing.%2520To%2520address%2520this%252C%2520the%2520paper%250Arefines%2520and%2520extends%2520the%2520Counterfactual%2520Account%2520%2528CT%2529%2520of%2520well%2520being%252C%2520which%250Aevaluates%2520goodness%2520of%2520an%2520event%2520by%2520comparing%2520an%2520individual%2527s%2520actual%2520well%2520being%250Awith%2520a%2520hypothetical%2520counterpart%2520in%2520a%2520nearby%2520possible%2520world.%2520While%2520useful%252C%2520this%250Aframework%2520is%2520insufficient%2520for%2520group%2520level%2520ontologies%252C%2520where%2520well%2520being%2520depends%250Aon%2520functional%2520persistence%252C%2520institutional%2520roles%252C%2520and%2520historical%2520impact%2520rather%250Athan%2520immediate%2520individual%2520outcomes.%2520Drawing%2520on%2520Basic%2520Formal%2520Ontology%2520%2528BFO%2529%252C%2520the%250Apaper%2520introduces%2520a%2520model%2520in%2520which%2520group%2520flourishing%2520is%2520evaluated%2520in%2520terms%2520of%250Agroup%2520functional%252C%2520where%2520members%2520bear%2520roles%2520and%2520exhibit%2520persistence%2520conditions%250Aakin%2520to%2520biological%2520systems%2520or%2520designed%2520artifacts.%2520This%2520approach%2520enables%250Asemantic%2520interoperability%2520for%2520modeling%2520longitudinal%2520social%2520contributions%252C%250Aallowing%2520for%2520structured%2520reasoning%2520about%2520group%2520welfare%252C%2520social%2520institutions%252C%2520and%250Agroup%2520flourishing%2520over%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Group%20Lives%20Go%20Well&entry.906535625=John%20Beverley%20and%20Regina%20Hurley&entry.1292438233=%20%20This%20paper%20explores%20the%20ontological%20space%20of%20group%20well%20being%2C%20proposing%20a%0Aframework%20for%20representing%20collective%20welfare%2C%20group%20functions%2C%20and%20long%20term%0Acontributions%20within%20an%20ontology%20engineering%20context.%20Traditional%20well%20being%0Atheories%20focus%20on%20individual%20states%2C%20often%20relying%20on%20hedonistic%2C%20desire%0Asatisfaction%2C%20or%20objective%20list%20models.%20Such%20approaches%20struggle%20to%20account%20for%0Acases%20where%20individual%20sacrifices%20contribute%20to%20broader%20social%20progress%2C%20a%0Acritical%20challenge%20in%20modeling%20group%20flourishing.%20To%20address%20this%2C%20the%20paper%0Arefines%20and%20extends%20the%20Counterfactual%20Account%20%28CT%29%20of%20well%20being%2C%20which%0Aevaluates%20goodness%20of%20an%20event%20by%20comparing%20an%20individual%27s%20actual%20well%20being%0Awith%20a%20hypothetical%20counterpart%20in%20a%20nearby%20possible%20world.%20While%20useful%2C%20this%0Aframework%20is%20insufficient%20for%20group%20level%20ontologies%2C%20where%20well%20being%20depends%0Aon%20functional%20persistence%2C%20institutional%20roles%2C%20and%20historical%20impact%20rather%0Athan%20immediate%20individual%20outcomes.%20Drawing%20on%20Basic%20Formal%20Ontology%20%28BFO%29%2C%20the%0Apaper%20introduces%20a%20model%20in%20which%20group%20flourishing%20is%20evaluated%20in%20terms%20of%0Agroup%20functional%2C%20where%20members%20bear%20roles%20and%20exhibit%20persistence%20conditions%0Aakin%20to%20biological%20systems%20or%20designed%20artifacts.%20This%20approach%20enables%0Asemantic%20interoperability%20for%20modeling%20longitudinal%20social%20contributions%2C%0Aallowing%20for%20structured%20reasoning%20about%20group%20welfare%2C%20social%20institutions%2C%20and%0Agroup%20flourishing%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19968v1&entry.124074799=Read"},
{"title": "Interpretable machine learning-guided design of Fe-based soft magnetic\n  alloys", "author": "Aditi Nachnani and Kai K. Li-Caldwell and Saptarshi Biswas and Prince Sharma and Gaoyuan Ouyang and Prashant Singh", "abstract": "  We present a machine-learning guided approach to predict saturation\nmagnetization (MS) and coercivity (HC) in Fe-rich soft magnetic alloys,\nparticularly Fe-Si-B systems. ML models trained on experimental data reveals\nthat increasing Si and B content reduces MS from 1.81T (DFT~2.04 T) to ~1.54 T\n(DFT~1.56T) in Fe-Si-B, which is attributed to decreased magnetic density and\nstructural modifications. Experimental validation of ML predicted magnetic\nsaturation on Fe-1Si-1B (2.09T), Fe-5Si-5B (2.01T) and Fe-10Si-10B (1.54T)\nalloy compositions further support our findings. These trends are consistent\nwith density functional theory (DFT) predictions, which link increased\nelectronic disorder and band broadening to lower MS values. Experimental\nvalidation on selected alloys confirms the predictive accuracy of the ML model,\nwith good agreement across compositions. Beyond predictive accuracy, detailed\nuncertainty quantification and model interpretability including through feature\nimportance and partial dependence analysis reveals that MS is governed by a\nnonlinear interplay between Fe content, early transition metal ratios, and\nannealing temperature, while HC is more sensitive to processing conditions such\nas ribbon thickness and thermal treatment windows. The ML framework was further\napplied to Fe-Si-B/Cr/Cu/Zr/Nb alloys in a pseudo-quaternary compositional\nspace, which shows comparable magnetic properties to NANOMET\n(Fe84.8Si0.5B9.4Cu0.8 P3.5C1), FINEMET (Fe73.5Si13.5B9 Cu1Nb3), NANOPERM\n(Fe88Zr7B4Cu1), and HITPERM (Fe44Co44Zr7B4Cu1. Our fundings demonstrate the\npotential of ML framework for accelerated search of high-performance, Co- and\nNi-free, soft magnetic materials.\n", "link": "http://arxiv.org/abs/2504.19787v1", "date": "2025-04-28", "relevancy": 0.8132, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4536}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3869}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20machine%20learning-guided%20design%20of%20Fe-based%20soft%20magnetic%0A%20%20alloys&body=Title%3A%20Interpretable%20machine%20learning-guided%20design%20of%20Fe-based%20soft%20magnetic%0A%20%20alloys%0AAuthor%3A%20Aditi%20Nachnani%20and%20Kai%20K.%20Li-Caldwell%20and%20Saptarshi%20Biswas%20and%20Prince%20Sharma%20and%20Gaoyuan%20Ouyang%20and%20Prashant%20Singh%0AAbstract%3A%20%20%20We%20present%20a%20machine-learning%20guided%20approach%20to%20predict%20saturation%0Amagnetization%20%28MS%29%20and%20coercivity%20%28HC%29%20in%20Fe-rich%20soft%20magnetic%20alloys%2C%0Aparticularly%20Fe-Si-B%20systems.%20ML%20models%20trained%20on%20experimental%20data%20reveals%0Athat%20increasing%20Si%20and%20B%20content%20reduces%20MS%20from%201.81T%20%28DFT~2.04%20T%29%20to%20~1.54%20T%0A%28DFT~1.56T%29%20in%20Fe-Si-B%2C%20which%20is%20attributed%20to%20decreased%20magnetic%20density%20and%0Astructural%20modifications.%20Experimental%20validation%20of%20ML%20predicted%20magnetic%0Asaturation%20on%20Fe-1Si-1B%20%282.09T%29%2C%20Fe-5Si-5B%20%282.01T%29%20and%20Fe-10Si-10B%20%281.54T%29%0Aalloy%20compositions%20further%20support%20our%20findings.%20These%20trends%20are%20consistent%0Awith%20density%20functional%20theory%20%28DFT%29%20predictions%2C%20which%20link%20increased%0Aelectronic%20disorder%20and%20band%20broadening%20to%20lower%20MS%20values.%20Experimental%0Avalidation%20on%20selected%20alloys%20confirms%20the%20predictive%20accuracy%20of%20the%20ML%20model%2C%0Awith%20good%20agreement%20across%20compositions.%20Beyond%20predictive%20accuracy%2C%20detailed%0Auncertainty%20quantification%20and%20model%20interpretability%20including%20through%20feature%0Aimportance%20and%20partial%20dependence%20analysis%20reveals%20that%20MS%20is%20governed%20by%20a%0Anonlinear%20interplay%20between%20Fe%20content%2C%20early%20transition%20metal%20ratios%2C%20and%0Aannealing%20temperature%2C%20while%20HC%20is%20more%20sensitive%20to%20processing%20conditions%20such%0Aas%20ribbon%20thickness%20and%20thermal%20treatment%20windows.%20The%20ML%20framework%20was%20further%0Aapplied%20to%20Fe-Si-B/Cr/Cu/Zr/Nb%20alloys%20in%20a%20pseudo-quaternary%20compositional%0Aspace%2C%20which%20shows%20comparable%20magnetic%20properties%20to%20NANOMET%0A%28Fe84.8Si0.5B9.4Cu0.8%20P3.5C1%29%2C%20FINEMET%20%28Fe73.5Si13.5B9%20Cu1Nb3%29%2C%20NANOPERM%0A%28Fe88Zr7B4Cu1%29%2C%20and%20HITPERM%20%28Fe44Co44Zr7B4Cu1.%20Our%20fundings%20demonstrate%20the%0Apotential%20of%20ML%20framework%20for%20accelerated%20search%20of%20high-performance%2C%20Co-%20and%0ANi-free%2C%20soft%20magnetic%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520machine%2520learning-guided%2520design%2520of%2520Fe-based%2520soft%2520magnetic%250A%2520%2520alloys%26entry.906535625%3DAditi%2520Nachnani%2520and%2520Kai%2520K.%2520Li-Caldwell%2520and%2520Saptarshi%2520Biswas%2520and%2520Prince%2520Sharma%2520and%2520Gaoyuan%2520Ouyang%2520and%2520Prashant%2520Singh%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520machine-learning%2520guided%2520approach%2520to%2520predict%2520saturation%250Amagnetization%2520%2528MS%2529%2520and%2520coercivity%2520%2528HC%2529%2520in%2520Fe-rich%2520soft%2520magnetic%2520alloys%252C%250Aparticularly%2520Fe-Si-B%2520systems.%2520ML%2520models%2520trained%2520on%2520experimental%2520data%2520reveals%250Athat%2520increasing%2520Si%2520and%2520B%2520content%2520reduces%2520MS%2520from%25201.81T%2520%2528DFT~2.04%2520T%2529%2520to%2520~1.54%2520T%250A%2528DFT~1.56T%2529%2520in%2520Fe-Si-B%252C%2520which%2520is%2520attributed%2520to%2520decreased%2520magnetic%2520density%2520and%250Astructural%2520modifications.%2520Experimental%2520validation%2520of%2520ML%2520predicted%2520magnetic%250Asaturation%2520on%2520Fe-1Si-1B%2520%25282.09T%2529%252C%2520Fe-5Si-5B%2520%25282.01T%2529%2520and%2520Fe-10Si-10B%2520%25281.54T%2529%250Aalloy%2520compositions%2520further%2520support%2520our%2520findings.%2520These%2520trends%2520are%2520consistent%250Awith%2520density%2520functional%2520theory%2520%2528DFT%2529%2520predictions%252C%2520which%2520link%2520increased%250Aelectronic%2520disorder%2520and%2520band%2520broadening%2520to%2520lower%2520MS%2520values.%2520Experimental%250Avalidation%2520on%2520selected%2520alloys%2520confirms%2520the%2520predictive%2520accuracy%2520of%2520the%2520ML%2520model%252C%250Awith%2520good%2520agreement%2520across%2520compositions.%2520Beyond%2520predictive%2520accuracy%252C%2520detailed%250Auncertainty%2520quantification%2520and%2520model%2520interpretability%2520including%2520through%2520feature%250Aimportance%2520and%2520partial%2520dependence%2520analysis%2520reveals%2520that%2520MS%2520is%2520governed%2520by%2520a%250Anonlinear%2520interplay%2520between%2520Fe%2520content%252C%2520early%2520transition%2520metal%2520ratios%252C%2520and%250Aannealing%2520temperature%252C%2520while%2520HC%2520is%2520more%2520sensitive%2520to%2520processing%2520conditions%2520such%250Aas%2520ribbon%2520thickness%2520and%2520thermal%2520treatment%2520windows.%2520The%2520ML%2520framework%2520was%2520further%250Aapplied%2520to%2520Fe-Si-B/Cr/Cu/Zr/Nb%2520alloys%2520in%2520a%2520pseudo-quaternary%2520compositional%250Aspace%252C%2520which%2520shows%2520comparable%2520magnetic%2520properties%2520to%2520NANOMET%250A%2528Fe84.8Si0.5B9.4Cu0.8%2520P3.5C1%2529%252C%2520FINEMET%2520%2528Fe73.5Si13.5B9%2520Cu1Nb3%2529%252C%2520NANOPERM%250A%2528Fe88Zr7B4Cu1%2529%252C%2520and%2520HITPERM%2520%2528Fe44Co44Zr7B4Cu1.%2520Our%2520fundings%2520demonstrate%2520the%250Apotential%2520of%2520ML%2520framework%2520for%2520accelerated%2520search%2520of%2520high-performance%252C%2520Co-%2520and%250ANi-free%252C%2520soft%2520magnetic%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20machine%20learning-guided%20design%20of%20Fe-based%20soft%20magnetic%0A%20%20alloys&entry.906535625=Aditi%20Nachnani%20and%20Kai%20K.%20Li-Caldwell%20and%20Saptarshi%20Biswas%20and%20Prince%20Sharma%20and%20Gaoyuan%20Ouyang%20and%20Prashant%20Singh&entry.1292438233=%20%20We%20present%20a%20machine-learning%20guided%20approach%20to%20predict%20saturation%0Amagnetization%20%28MS%29%20and%20coercivity%20%28HC%29%20in%20Fe-rich%20soft%20magnetic%20alloys%2C%0Aparticularly%20Fe-Si-B%20systems.%20ML%20models%20trained%20on%20experimental%20data%20reveals%0Athat%20increasing%20Si%20and%20B%20content%20reduces%20MS%20from%201.81T%20%28DFT~2.04%20T%29%20to%20~1.54%20T%0A%28DFT~1.56T%29%20in%20Fe-Si-B%2C%20which%20is%20attributed%20to%20decreased%20magnetic%20density%20and%0Astructural%20modifications.%20Experimental%20validation%20of%20ML%20predicted%20magnetic%0Asaturation%20on%20Fe-1Si-1B%20%282.09T%29%2C%20Fe-5Si-5B%20%282.01T%29%20and%20Fe-10Si-10B%20%281.54T%29%0Aalloy%20compositions%20further%20support%20our%20findings.%20These%20trends%20are%20consistent%0Awith%20density%20functional%20theory%20%28DFT%29%20predictions%2C%20which%20link%20increased%0Aelectronic%20disorder%20and%20band%20broadening%20to%20lower%20MS%20values.%20Experimental%0Avalidation%20on%20selected%20alloys%20confirms%20the%20predictive%20accuracy%20of%20the%20ML%20model%2C%0Awith%20good%20agreement%20across%20compositions.%20Beyond%20predictive%20accuracy%2C%20detailed%0Auncertainty%20quantification%20and%20model%20interpretability%20including%20through%20feature%0Aimportance%20and%20partial%20dependence%20analysis%20reveals%20that%20MS%20is%20governed%20by%20a%0Anonlinear%20interplay%20between%20Fe%20content%2C%20early%20transition%20metal%20ratios%2C%20and%0Aannealing%20temperature%2C%20while%20HC%20is%20more%20sensitive%20to%20processing%20conditions%20such%0Aas%20ribbon%20thickness%20and%20thermal%20treatment%20windows.%20The%20ML%20framework%20was%20further%0Aapplied%20to%20Fe-Si-B/Cr/Cu/Zr/Nb%20alloys%20in%20a%20pseudo-quaternary%20compositional%0Aspace%2C%20which%20shows%20comparable%20magnetic%20properties%20to%20NANOMET%0A%28Fe84.8Si0.5B9.4Cu0.8%20P3.5C1%29%2C%20FINEMET%20%28Fe73.5Si13.5B9%20Cu1Nb3%29%2C%20NANOPERM%0A%28Fe88Zr7B4Cu1%29%2C%20and%20HITPERM%20%28Fe44Co44Zr7B4Cu1.%20Our%20fundings%20demonstrate%20the%0Apotential%20of%20ML%20framework%20for%20accelerated%20search%20of%20high-performance%2C%20Co-%20and%0ANi-free%2C%20soft%20magnetic%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19787v1&entry.124074799=Read"},
{"title": "Tensegrity-based Robot Leg Design with Variable Stiffness", "author": "Erik Mortensen and Jan Petrs and Alexander Dittrich and Dario Floreano", "abstract": "  Animals can finely modulate their leg stiffness to interact with complex\nterrains and absorb sudden shocks. In feats like leaping and sprinting, animals\ndemonstrate a sophisticated interplay of opposing muscle pairs that actively\nmodulate joint stiffness, while tendons and ligaments act as biological springs\nstoring and releasing energy. Although legged robots have achieved notable\nprogress in robust locomotion, they still lack the refined adaptability\ninherent in animal motor control. Integrating mechanisms that allow active\ncontrol of leg stiffness presents a pathway towards more resilient robotic\nsystems. This paper proposes a novel mechanical design to integrate compliancy\ninto robot legs based on tensegrity - a structural principle that combines\nflexible cables and rigid elements to balance tension and compression.\nTensegrity structures naturally allow for passive compliance, making them\nwell-suited for absorbing impacts and adapting to diverse terrains. Our design\nfeatures a robot leg with tensegrity joints and a mechanism to control the\njoint's rotational stiffness by modulating the tension of the cable actuation\nsystem. We demonstrate that the robot leg can reduce the impact forces of\nsudden shocks by at least 34.7 % and achieve a similar leg flexion under a load\ndifference of 10.26 N by adjusting its stiffness configuration. The results\nindicate that tensegrity-based leg designs harbors potential towards more\nresilient and adaptable legged robots.\n", "link": "http://arxiv.org/abs/2504.19685v1", "date": "2025-04-28", "relevancy": 1.4098, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5243}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4557}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensegrity-based%20Robot%20Leg%20Design%20with%20Variable%20Stiffness&body=Title%3A%20Tensegrity-based%20Robot%20Leg%20Design%20with%20Variable%20Stiffness%0AAuthor%3A%20Erik%20Mortensen%20and%20Jan%20Petrs%20and%20Alexander%20Dittrich%20and%20Dario%20Floreano%0AAbstract%3A%20%20%20Animals%20can%20finely%20modulate%20their%20leg%20stiffness%20to%20interact%20with%20complex%0Aterrains%20and%20absorb%20sudden%20shocks.%20In%20feats%20like%20leaping%20and%20sprinting%2C%20animals%0Ademonstrate%20a%20sophisticated%20interplay%20of%20opposing%20muscle%20pairs%20that%20actively%0Amodulate%20joint%20stiffness%2C%20while%20tendons%20and%20ligaments%20act%20as%20biological%20springs%0Astoring%20and%20releasing%20energy.%20Although%20legged%20robots%20have%20achieved%20notable%0Aprogress%20in%20robust%20locomotion%2C%20they%20still%20lack%20the%20refined%20adaptability%0Ainherent%20in%20animal%20motor%20control.%20Integrating%20mechanisms%20that%20allow%20active%0Acontrol%20of%20leg%20stiffness%20presents%20a%20pathway%20towards%20more%20resilient%20robotic%0Asystems.%20This%20paper%20proposes%20a%20novel%20mechanical%20design%20to%20integrate%20compliancy%0Ainto%20robot%20legs%20based%20on%20tensegrity%20-%20a%20structural%20principle%20that%20combines%0Aflexible%20cables%20and%20rigid%20elements%20to%20balance%20tension%20and%20compression.%0ATensegrity%20structures%20naturally%20allow%20for%20passive%20compliance%2C%20making%20them%0Awell-suited%20for%20absorbing%20impacts%20and%20adapting%20to%20diverse%20terrains.%20Our%20design%0Afeatures%20a%20robot%20leg%20with%20tensegrity%20joints%20and%20a%20mechanism%20to%20control%20the%0Ajoint%27s%20rotational%20stiffness%20by%20modulating%20the%20tension%20of%20the%20cable%20actuation%0Asystem.%20We%20demonstrate%20that%20the%20robot%20leg%20can%20reduce%20the%20impact%20forces%20of%0Asudden%20shocks%20by%20at%20least%2034.7%20%25%20and%20achieve%20a%20similar%20leg%20flexion%20under%20a%20load%0Adifference%20of%2010.26%20N%20by%20adjusting%20its%20stiffness%20configuration.%20The%20results%0Aindicate%20that%20tensegrity-based%20leg%20designs%20harbors%20potential%20towards%20more%0Aresilient%20and%20adaptable%20legged%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensegrity-based%2520Robot%2520Leg%2520Design%2520with%2520Variable%2520Stiffness%26entry.906535625%3DErik%2520Mortensen%2520and%2520Jan%2520Petrs%2520and%2520Alexander%2520Dittrich%2520and%2520Dario%2520Floreano%26entry.1292438233%3D%2520%2520Animals%2520can%2520finely%2520modulate%2520their%2520leg%2520stiffness%2520to%2520interact%2520with%2520complex%250Aterrains%2520and%2520absorb%2520sudden%2520shocks.%2520In%2520feats%2520like%2520leaping%2520and%2520sprinting%252C%2520animals%250Ademonstrate%2520a%2520sophisticated%2520interplay%2520of%2520opposing%2520muscle%2520pairs%2520that%2520actively%250Amodulate%2520joint%2520stiffness%252C%2520while%2520tendons%2520and%2520ligaments%2520act%2520as%2520biological%2520springs%250Astoring%2520and%2520releasing%2520energy.%2520Although%2520legged%2520robots%2520have%2520achieved%2520notable%250Aprogress%2520in%2520robust%2520locomotion%252C%2520they%2520still%2520lack%2520the%2520refined%2520adaptability%250Ainherent%2520in%2520animal%2520motor%2520control.%2520Integrating%2520mechanisms%2520that%2520allow%2520active%250Acontrol%2520of%2520leg%2520stiffness%2520presents%2520a%2520pathway%2520towards%2520more%2520resilient%2520robotic%250Asystems.%2520This%2520paper%2520proposes%2520a%2520novel%2520mechanical%2520design%2520to%2520integrate%2520compliancy%250Ainto%2520robot%2520legs%2520based%2520on%2520tensegrity%2520-%2520a%2520structural%2520principle%2520that%2520combines%250Aflexible%2520cables%2520and%2520rigid%2520elements%2520to%2520balance%2520tension%2520and%2520compression.%250ATensegrity%2520structures%2520naturally%2520allow%2520for%2520passive%2520compliance%252C%2520making%2520them%250Awell-suited%2520for%2520absorbing%2520impacts%2520and%2520adapting%2520to%2520diverse%2520terrains.%2520Our%2520design%250Afeatures%2520a%2520robot%2520leg%2520with%2520tensegrity%2520joints%2520and%2520a%2520mechanism%2520to%2520control%2520the%250Ajoint%2527s%2520rotational%2520stiffness%2520by%2520modulating%2520the%2520tension%2520of%2520the%2520cable%2520actuation%250Asystem.%2520We%2520demonstrate%2520that%2520the%2520robot%2520leg%2520can%2520reduce%2520the%2520impact%2520forces%2520of%250Asudden%2520shocks%2520by%2520at%2520least%252034.7%2520%2525%2520and%2520achieve%2520a%2520similar%2520leg%2520flexion%2520under%2520a%2520load%250Adifference%2520of%252010.26%2520N%2520by%2520adjusting%2520its%2520stiffness%2520configuration.%2520The%2520results%250Aindicate%2520that%2520tensegrity-based%2520leg%2520designs%2520harbors%2520potential%2520towards%2520more%250Aresilient%2520and%2520adaptable%2520legged%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensegrity-based%20Robot%20Leg%20Design%20with%20Variable%20Stiffness&entry.906535625=Erik%20Mortensen%20and%20Jan%20Petrs%20and%20Alexander%20Dittrich%20and%20Dario%20Floreano&entry.1292438233=%20%20Animals%20can%20finely%20modulate%20their%20leg%20stiffness%20to%20interact%20with%20complex%0Aterrains%20and%20absorb%20sudden%20shocks.%20In%20feats%20like%20leaping%20and%20sprinting%2C%20animals%0Ademonstrate%20a%20sophisticated%20interplay%20of%20opposing%20muscle%20pairs%20that%20actively%0Amodulate%20joint%20stiffness%2C%20while%20tendons%20and%20ligaments%20act%20as%20biological%20springs%0Astoring%20and%20releasing%20energy.%20Although%20legged%20robots%20have%20achieved%20notable%0Aprogress%20in%20robust%20locomotion%2C%20they%20still%20lack%20the%20refined%20adaptability%0Ainherent%20in%20animal%20motor%20control.%20Integrating%20mechanisms%20that%20allow%20active%0Acontrol%20of%20leg%20stiffness%20presents%20a%20pathway%20towards%20more%20resilient%20robotic%0Asystems.%20This%20paper%20proposes%20a%20novel%20mechanical%20design%20to%20integrate%20compliancy%0Ainto%20robot%20legs%20based%20on%20tensegrity%20-%20a%20structural%20principle%20that%20combines%0Aflexible%20cables%20and%20rigid%20elements%20to%20balance%20tension%20and%20compression.%0ATensegrity%20structures%20naturally%20allow%20for%20passive%20compliance%2C%20making%20them%0Awell-suited%20for%20absorbing%20impacts%20and%20adapting%20to%20diverse%20terrains.%20Our%20design%0Afeatures%20a%20robot%20leg%20with%20tensegrity%20joints%20and%20a%20mechanism%20to%20control%20the%0Ajoint%27s%20rotational%20stiffness%20by%20modulating%20the%20tension%20of%20the%20cable%20actuation%0Asystem.%20We%20demonstrate%20that%20the%20robot%20leg%20can%20reduce%20the%20impact%20forces%20of%0Asudden%20shocks%20by%20at%20least%2034.7%20%25%20and%20achieve%20a%20similar%20leg%20flexion%20under%20a%20load%0Adifference%20of%2010.26%20N%20by%20adjusting%20its%20stiffness%20configuration.%20The%20results%0Aindicate%20that%20tensegrity-based%20leg%20designs%20harbors%20potential%20towards%20more%0Aresilient%20and%20adaptable%20legged%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19685v1&entry.124074799=Read"},
{"title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation", "author": "Bingye Zhou and Caiyang Yu", "abstract": "  Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.\n", "link": "http://arxiv.org/abs/2504.17827v2", "date": "2025-04-28", "relevancy": 1.662, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5853}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5462}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolution%20Meets%20Diffusion%3A%20Efficient%20Neural%20Architecture%20Generation&body=Title%3A%20Evolution%20Meets%20Diffusion%3A%20Efficient%20Neural%20Architecture%20Generation%0AAuthor%3A%20Bingye%20Zhou%20and%20Caiyang%20Yu%0AAbstract%3A%20%20%20Neural%20Architecture%20Search%20%28NAS%29%20has%20gained%20widespread%20attention%20for%20its%0Atransformative%20potential%20in%20deep%20learning%20model%20design.%20However%2C%20the%20vast%20and%0Acomplex%20search%20space%20of%20NAS%20leads%20to%20significant%20computational%20and%20time%20costs.%0ANeural%20Architecture%20Generation%20%28NAG%29%20addresses%20this%20by%20reframing%20NAS%20as%20a%0Ageneration%20problem%2C%20enabling%20the%20precise%20generation%20of%20optimal%20architectures%0Afor%20specific%20tasks.%20Despite%20its%20promise%2C%20mainstream%20methods%20like%20diffusion%0Amodels%20face%20limitations%20in%20global%20search%20capabilities%20and%20are%20still%20hindered%20by%0Ahigh%20computational%20and%20time%20demands.%20To%20overcome%20these%20challenges%2C%20we%20propose%0AEvolutionary%20Diffusion-based%20Neural%20Architecture%20Generation%20%28EDNAG%29%2C%20a%20novel%0Aapproach%20that%20achieves%20efficient%20and%20training-free%20architecture%20generation.%0AEDNAG%20leverages%20evolutionary%20algorithms%20to%20simulate%20the%20denoising%20process%20in%0Adiffusion%20models%2C%20using%20fitness%20to%20guide%20the%20transition%20from%20random%20Gaussian%0Adistributions%20to%20optimal%20architecture%20distributions.%20This%20approach%20combines%20the%0Astrengths%20of%20evolutionary%20strategies%20and%20diffusion%20models%2C%20enabling%20rapid%20and%0Aeffective%20architecture%20generation.%20Extensive%20experiments%20demonstrate%20that%20EDNAG%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20in%20architecture%20optimization%2C%20with%0Aan%20improvement%20in%20accuracy%20of%20up%20to%2010.45%25.%20Furthermore%2C%20it%20eliminates%20the%20need%0Afor%20time-consuming%20training%20and%20boosts%20inference%20speed%20by%20an%20average%20of%2050%0Atimes%2C%20showcasing%20its%20exceptional%20efficiency%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolution%2520Meets%2520Diffusion%253A%2520Efficient%2520Neural%2520Architecture%2520Generation%26entry.906535625%3DBingye%2520Zhou%2520and%2520Caiyang%2520Yu%26entry.1292438233%3D%2520%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%2520has%2520gained%2520widespread%2520attention%2520for%2520its%250Atransformative%2520potential%2520in%2520deep%2520learning%2520model%2520design.%2520However%252C%2520the%2520vast%2520and%250Acomplex%2520search%2520space%2520of%2520NAS%2520leads%2520to%2520significant%2520computational%2520and%2520time%2520costs.%250ANeural%2520Architecture%2520Generation%2520%2528NAG%2529%2520addresses%2520this%2520by%2520reframing%2520NAS%2520as%2520a%250Ageneration%2520problem%252C%2520enabling%2520the%2520precise%2520generation%2520of%2520optimal%2520architectures%250Afor%2520specific%2520tasks.%2520Despite%2520its%2520promise%252C%2520mainstream%2520methods%2520like%2520diffusion%250Amodels%2520face%2520limitations%2520in%2520global%2520search%2520capabilities%2520and%2520are%2520still%2520hindered%2520by%250Ahigh%2520computational%2520and%2520time%2520demands.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%250AEvolutionary%2520Diffusion-based%2520Neural%2520Architecture%2520Generation%2520%2528EDNAG%2529%252C%2520a%2520novel%250Aapproach%2520that%2520achieves%2520efficient%2520and%2520training-free%2520architecture%2520generation.%250AEDNAG%2520leverages%2520evolutionary%2520algorithms%2520to%2520simulate%2520the%2520denoising%2520process%2520in%250Adiffusion%2520models%252C%2520using%2520fitness%2520to%2520guide%2520the%2520transition%2520from%2520random%2520Gaussian%250Adistributions%2520to%2520optimal%2520architecture%2520distributions.%2520This%2520approach%2520combines%2520the%250Astrengths%2520of%2520evolutionary%2520strategies%2520and%2520diffusion%2520models%252C%2520enabling%2520rapid%2520and%250Aeffective%2520architecture%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520EDNAG%250Aachieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520in%2520architecture%2520optimization%252C%2520with%250Aan%2520improvement%2520in%2520accuracy%2520of%2520up%2520to%252010.45%2525.%2520Furthermore%252C%2520it%2520eliminates%2520the%2520need%250Afor%2520time-consuming%2520training%2520and%2520boosts%2520inference%2520speed%2520by%2520an%2520average%2520of%252050%250Atimes%252C%2520showcasing%2520its%2520exceptional%2520efficiency%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%20Meets%20Diffusion%3A%20Efficient%20Neural%20Architecture%20Generation&entry.906535625=Bingye%20Zhou%20and%20Caiyang%20Yu&entry.1292438233=%20%20Neural%20Architecture%20Search%20%28NAS%29%20has%20gained%20widespread%20attention%20for%20its%0Atransformative%20potential%20in%20deep%20learning%20model%20design.%20However%2C%20the%20vast%20and%0Acomplex%20search%20space%20of%20NAS%20leads%20to%20significant%20computational%20and%20time%20costs.%0ANeural%20Architecture%20Generation%20%28NAG%29%20addresses%20this%20by%20reframing%20NAS%20as%20a%0Ageneration%20problem%2C%20enabling%20the%20precise%20generation%20of%20optimal%20architectures%0Afor%20specific%20tasks.%20Despite%20its%20promise%2C%20mainstream%20methods%20like%20diffusion%0Amodels%20face%20limitations%20in%20global%20search%20capabilities%20and%20are%20still%20hindered%20by%0Ahigh%20computational%20and%20time%20demands.%20To%20overcome%20these%20challenges%2C%20we%20propose%0AEvolutionary%20Diffusion-based%20Neural%20Architecture%20Generation%20%28EDNAG%29%2C%20a%20novel%0Aapproach%20that%20achieves%20efficient%20and%20training-free%20architecture%20generation.%0AEDNAG%20leverages%20evolutionary%20algorithms%20to%20simulate%20the%20denoising%20process%20in%0Adiffusion%20models%2C%20using%20fitness%20to%20guide%20the%20transition%20from%20random%20Gaussian%0Adistributions%20to%20optimal%20architecture%20distributions.%20This%20approach%20combines%20the%0Astrengths%20of%20evolutionary%20strategies%20and%20diffusion%20models%2C%20enabling%20rapid%20and%0Aeffective%20architecture%20generation.%20Extensive%20experiments%20demonstrate%20that%20EDNAG%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20in%20architecture%20optimization%2C%20with%0Aan%20improvement%20in%20accuracy%20of%20up%20to%2010.45%25.%20Furthermore%2C%20it%20eliminates%20the%20need%0Afor%20time-consuming%20training%20and%20boosts%20inference%20speed%20by%20an%20average%20of%2050%0Atimes%2C%20showcasing%20its%20exceptional%20efficiency%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17827v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


