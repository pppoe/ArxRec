<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260210.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Toward Fine-Grained Facial Control in 3D Talking Head Generation", "author": "Shaoyang Xie and Xiaofeng Cong and Baosheng Yu and Zhipeng Gui and Jie Gui and Yuan Yan Tang and James Tin-Yau Kwok", "abstract": "Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.", "link": "http://arxiv.org/abs/2602.09736v1", "date": "2026-02-10", "relevancy": 3.483, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.729}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.729}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Fine-Grained%20Facial%20Control%20in%203D%20Talking%20Head%20Generation&body=Title%3A%20Toward%20Fine-Grained%20Facial%20Control%20in%203D%20Talking%20Head%20Generation%0AAuthor%3A%20Shaoyang%20Xie%20and%20Xiaofeng%20Cong%20and%20Baosheng%20Yu%20and%20Zhipeng%20Gui%20and%20Jie%20Gui%20and%20Yuan%20Yan%20Tang%20and%20James%20Tin-Yau%20Kwok%0AAbstract%3A%20Audio-driven%20talking%20head%20generation%20is%20a%20core%20component%20of%20digital%20avatars%2C%20and%203D%20Gaussian%20Splatting%20has%20shown%20strong%20performance%20in%20real-time%20rendering%20of%20high-fidelity%20talking%20heads.%20However%2C%20achieving%20precise%20control%20over%20fine-grained%20facial%20movements%20remains%20a%20significant%20challenge%2C%20particularly%20due%20to%20lip-synchronization%20inaccuracies%20and%20facial%20jitter%2C%20both%20of%20which%20can%20contribute%20to%20the%20uncanny%20valley%20effect.%20To%20address%20these%20challenges%2C%20we%20propose%20Fine-Grained%203D%20Gaussian%20Splatting%20%28FG-3DGS%29%2C%20a%20novel%20framework%20that%20enables%20temporally%20consistent%20and%20high-fidelity%20talking%20head%20generation.%20Our%20method%20introduces%20a%20frequency-aware%20disentanglement%20strategy%20to%20explicitly%20model%20facial%20regions%20based%20on%20their%20motion%20characteristics.%20Low-frequency%20regions%2C%20such%20as%20the%20cheeks%2C%20nose%2C%20and%20forehead%2C%20are%20jointly%20modeled%20using%20a%20standard%20MLP%2C%20while%20high-frequency%20regions%2C%20including%20the%20eyes%20and%20mouth%2C%20are%20captured%20separately%20using%20a%20dedicated%20network%20guided%20by%20facial%20area%20masks.%20The%20predicted%20motion%20dynamics%2C%20represented%20as%20Gaussian%20deltas%2C%20are%20applied%20to%20the%20static%20Gaussians%20to%20generate%20the%20final%20head%20frames%2C%20which%20are%20rendered%20via%20a%20rasterizer%20using%20frame-specific%20camera%20parameters.%20Additionally%2C%20a%20high-frequency-refined%20post-rendering%20alignment%20mechanism%2C%20learned%20from%20large-scale%20audio-video%20pairs%20by%20a%20pretrained%20model%2C%20is%20incorporated%20to%20enhance%20per-frame%20generation%20and%20achieve%20more%20accurate%20lip%20synchronization.%20Extensive%20experiments%20on%20widely%20used%20datasets%20for%20talking%20head%20generation%20demonstrate%20that%20our%20method%20outperforms%20recent%20state-of-the-art%20approaches%20in%20producing%20high-fidelity%2C%20lip-synced%20talking%20head%20videos.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Fine-Grained%2520Facial%2520Control%2520in%25203D%2520Talking%2520Head%2520Generation%26entry.906535625%3DShaoyang%2520Xie%2520and%2520Xiaofeng%2520Cong%2520and%2520Baosheng%2520Yu%2520and%2520Zhipeng%2520Gui%2520and%2520Jie%2520Gui%2520and%2520Yuan%2520Yan%2520Tang%2520and%2520James%2520Tin-Yau%2520Kwok%26entry.1292438233%3DAudio-driven%2520talking%2520head%2520generation%2520is%2520a%2520core%2520component%2520of%2520digital%2520avatars%252C%2520and%25203D%2520Gaussian%2520Splatting%2520has%2520shown%2520strong%2520performance%2520in%2520real-time%2520rendering%2520of%2520high-fidelity%2520talking%2520heads.%2520However%252C%2520achieving%2520precise%2520control%2520over%2520fine-grained%2520facial%2520movements%2520remains%2520a%2520significant%2520challenge%252C%2520particularly%2520due%2520to%2520lip-synchronization%2520inaccuracies%2520and%2520facial%2520jitter%252C%2520both%2520of%2520which%2520can%2520contribute%2520to%2520the%2520uncanny%2520valley%2520effect.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Fine-Grained%25203D%2520Gaussian%2520Splatting%2520%2528FG-3DGS%2529%252C%2520a%2520novel%2520framework%2520that%2520enables%2520temporally%2520consistent%2520and%2520high-fidelity%2520talking%2520head%2520generation.%2520Our%2520method%2520introduces%2520a%2520frequency-aware%2520disentanglement%2520strategy%2520to%2520explicitly%2520model%2520facial%2520regions%2520based%2520on%2520their%2520motion%2520characteristics.%2520Low-frequency%2520regions%252C%2520such%2520as%2520the%2520cheeks%252C%2520nose%252C%2520and%2520forehead%252C%2520are%2520jointly%2520modeled%2520using%2520a%2520standard%2520MLP%252C%2520while%2520high-frequency%2520regions%252C%2520including%2520the%2520eyes%2520and%2520mouth%252C%2520are%2520captured%2520separately%2520using%2520a%2520dedicated%2520network%2520guided%2520by%2520facial%2520area%2520masks.%2520The%2520predicted%2520motion%2520dynamics%252C%2520represented%2520as%2520Gaussian%2520deltas%252C%2520are%2520applied%2520to%2520the%2520static%2520Gaussians%2520to%2520generate%2520the%2520final%2520head%2520frames%252C%2520which%2520are%2520rendered%2520via%2520a%2520rasterizer%2520using%2520frame-specific%2520camera%2520parameters.%2520Additionally%252C%2520a%2520high-frequency-refined%2520post-rendering%2520alignment%2520mechanism%252C%2520learned%2520from%2520large-scale%2520audio-video%2520pairs%2520by%2520a%2520pretrained%2520model%252C%2520is%2520incorporated%2520to%2520enhance%2520per-frame%2520generation%2520and%2520achieve%2520more%2520accurate%2520lip%2520synchronization.%2520Extensive%2520experiments%2520on%2520widely%2520used%2520datasets%2520for%2520talking%2520head%2520generation%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520recent%2520state-of-the-art%2520approaches%2520in%2520producing%2520high-fidelity%252C%2520lip-synced%2520talking%2520head%2520videos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Fine-Grained%20Facial%20Control%20in%203D%20Talking%20Head%20Generation&entry.906535625=Shaoyang%20Xie%20and%20Xiaofeng%20Cong%20and%20Baosheng%20Yu%20and%20Zhipeng%20Gui%20and%20Jie%20Gui%20and%20Yuan%20Yan%20Tang%20and%20James%20Tin-Yau%20Kwok&entry.1292438233=Audio-driven%20talking%20head%20generation%20is%20a%20core%20component%20of%20digital%20avatars%2C%20and%203D%20Gaussian%20Splatting%20has%20shown%20strong%20performance%20in%20real-time%20rendering%20of%20high-fidelity%20talking%20heads.%20However%2C%20achieving%20precise%20control%20over%20fine-grained%20facial%20movements%20remains%20a%20significant%20challenge%2C%20particularly%20due%20to%20lip-synchronization%20inaccuracies%20and%20facial%20jitter%2C%20both%20of%20which%20can%20contribute%20to%20the%20uncanny%20valley%20effect.%20To%20address%20these%20challenges%2C%20we%20propose%20Fine-Grained%203D%20Gaussian%20Splatting%20%28FG-3DGS%29%2C%20a%20novel%20framework%20that%20enables%20temporally%20consistent%20and%20high-fidelity%20talking%20head%20generation.%20Our%20method%20introduces%20a%20frequency-aware%20disentanglement%20strategy%20to%20explicitly%20model%20facial%20regions%20based%20on%20their%20motion%20characteristics.%20Low-frequency%20regions%2C%20such%20as%20the%20cheeks%2C%20nose%2C%20and%20forehead%2C%20are%20jointly%20modeled%20using%20a%20standard%20MLP%2C%20while%20high-frequency%20regions%2C%20including%20the%20eyes%20and%20mouth%2C%20are%20captured%20separately%20using%20a%20dedicated%20network%20guided%20by%20facial%20area%20masks.%20The%20predicted%20motion%20dynamics%2C%20represented%20as%20Gaussian%20deltas%2C%20are%20applied%20to%20the%20static%20Gaussians%20to%20generate%20the%20final%20head%20frames%2C%20which%20are%20rendered%20via%20a%20rasterizer%20using%20frame-specific%20camera%20parameters.%20Additionally%2C%20a%20high-frequency-refined%20post-rendering%20alignment%20mechanism%2C%20learned%20from%20large-scale%20audio-video%20pairs%20by%20a%20pretrained%20model%2C%20is%20incorporated%20to%20enhance%20per-frame%20generation%20and%20achieve%20more%20accurate%20lip%20synchronization.%20Extensive%20experiments%20on%20widely%20used%20datasets%20for%20talking%20head%20generation%20demonstrate%20that%20our%20method%20outperforms%20recent%20state-of-the-art%20approaches%20in%20producing%20high-fidelity%2C%20lip-synced%20talking%20head%20videos.&entry.1838667208=http%3A//arxiv.org/abs/2602.09736v1&entry.124074799=Read"},
{"title": "Faster-GS: Analyzing and Improving Gaussian Splatting Optimization", "author": "Florian Hahlbohm and Linus Franke and Martin Eisemann and Marcus Magnor", "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.", "link": "http://arxiv.org/abs/2602.09999v1", "date": "2026-02-10", "relevancy": 3.4141, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7293}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6765}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster-GS%3A%20Analyzing%20and%20Improving%20Gaussian%20Splatting%20Optimization&body=Title%3A%20Faster-GS%3A%20Analyzing%20and%20Improving%20Gaussian%20Splatting%20Optimization%0AAuthor%3A%20Florian%20Hahlbohm%20and%20Linus%20Franke%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor%0AAbstract%3A%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20focused%20on%20accelerating%20optimization%20while%20preserving%20reconstruction%20quality.%20However%2C%20many%20proposed%20methods%20entangle%20implementation-level%20improvements%20with%20fundamental%20algorithmic%20modifications%20or%20trade%20performance%20for%20fidelity%2C%20leading%20to%20a%20fragmented%20research%20landscape%20that%20complicates%20fair%20comparison.%20In%20this%20work%2C%20we%20consolidate%20and%20evaluate%20the%20most%20effective%20and%20broadly%20applicable%20strategies%20from%20prior%203DGS%20research%20and%20augment%20them%20with%20several%20novel%20optimizations.%20We%20further%20investigate%20underexplored%20aspects%20of%20the%20framework%2C%20including%20numerical%20stability%2C%20Gaussian%20truncation%2C%20and%20gradient%20approximation.%20The%20resulting%20system%2C%20Faster-GS%2C%20provides%20a%20rigorously%20optimized%20algorithm%20that%20we%20evaluate%20across%20a%20comprehensive%20suite%20of%20benchmarks.%20Our%20experiments%20demonstrate%20that%20Faster-GS%20achieves%20up%20to%205%24%5Ctimes%24%20faster%20training%20while%20maintaining%20visual%20quality%2C%20establishing%20a%20new%20cost-effective%20and%20resource%20efficient%20baseline%20for%203DGS%20optimization.%20Furthermore%2C%20we%20demonstrate%20that%20optimizations%20can%20be%20applied%20to%204D%20Gaussian%20reconstruction%2C%20leading%20to%20efficient%20non-rigid%20scene%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster-GS%253A%2520Analyzing%2520and%2520Improving%2520Gaussian%2520Splatting%2520Optimization%26entry.906535625%3DFlorian%2520Hahlbohm%2520and%2520Linus%2520Franke%2520and%2520Martin%2520Eisemann%2520and%2520Marcus%2520Magnor%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520focused%2520on%2520accelerating%2520optimization%2520while%2520preserving%2520reconstruction%2520quality.%2520However%252C%2520many%2520proposed%2520methods%2520entangle%2520implementation-level%2520improvements%2520with%2520fundamental%2520algorithmic%2520modifications%2520or%2520trade%2520performance%2520for%2520fidelity%252C%2520leading%2520to%2520a%2520fragmented%2520research%2520landscape%2520that%2520complicates%2520fair%2520comparison.%2520In%2520this%2520work%252C%2520we%2520consolidate%2520and%2520evaluate%2520the%2520most%2520effective%2520and%2520broadly%2520applicable%2520strategies%2520from%2520prior%25203DGS%2520research%2520and%2520augment%2520them%2520with%2520several%2520novel%2520optimizations.%2520We%2520further%2520investigate%2520underexplored%2520aspects%2520of%2520the%2520framework%252C%2520including%2520numerical%2520stability%252C%2520Gaussian%2520truncation%252C%2520and%2520gradient%2520approximation.%2520The%2520resulting%2520system%252C%2520Faster-GS%252C%2520provides%2520a%2520rigorously%2520optimized%2520algorithm%2520that%2520we%2520evaluate%2520across%2520a%2520comprehensive%2520suite%2520of%2520benchmarks.%2520Our%2520experiments%2520demonstrate%2520that%2520Faster-GS%2520achieves%2520up%2520to%25205%2524%255Ctimes%2524%2520faster%2520training%2520while%2520maintaining%2520visual%2520quality%252C%2520establishing%2520a%2520new%2520cost-effective%2520and%2520resource%2520efficient%2520baseline%2520for%25203DGS%2520optimization.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520optimizations%2520can%2520be%2520applied%2520to%25204D%2520Gaussian%2520reconstruction%252C%2520leading%2520to%2520efficient%2520non-rigid%2520scene%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster-GS%3A%20Analyzing%20and%20Improving%20Gaussian%20Splatting%20Optimization&entry.906535625=Florian%20Hahlbohm%20and%20Linus%20Franke%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor&entry.1292438233=Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20focused%20on%20accelerating%20optimization%20while%20preserving%20reconstruction%20quality.%20However%2C%20many%20proposed%20methods%20entangle%20implementation-level%20improvements%20with%20fundamental%20algorithmic%20modifications%20or%20trade%20performance%20for%20fidelity%2C%20leading%20to%20a%20fragmented%20research%20landscape%20that%20complicates%20fair%20comparison.%20In%20this%20work%2C%20we%20consolidate%20and%20evaluate%20the%20most%20effective%20and%20broadly%20applicable%20strategies%20from%20prior%203DGS%20research%20and%20augment%20them%20with%20several%20novel%20optimizations.%20We%20further%20investigate%20underexplored%20aspects%20of%20the%20framework%2C%20including%20numerical%20stability%2C%20Gaussian%20truncation%2C%20and%20gradient%20approximation.%20The%20resulting%20system%2C%20Faster-GS%2C%20provides%20a%20rigorously%20optimized%20algorithm%20that%20we%20evaluate%20across%20a%20comprehensive%20suite%20of%20benchmarks.%20Our%20experiments%20demonstrate%20that%20Faster-GS%20achieves%20up%20to%205%24%5Ctimes%24%20faster%20training%20while%20maintaining%20visual%20quality%2C%20establishing%20a%20new%20cost-effective%20and%20resource%20efficient%20baseline%20for%203DGS%20optimization.%20Furthermore%2C%20we%20demonstrate%20that%20optimizations%20can%20be%20applied%20to%204D%20Gaussian%20reconstruction%2C%20leading%20to%20efficient%20non-rigid%20scene%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2602.09999v1&entry.124074799=Read"},
{"title": "ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation", "author": "Mingyang Wu and Ashirbad Mishra and Soumik Dey and Shuo Xing and Naveen Ravipati and Hansi Wu and Binbin Li and Zhengzhong Tu", "abstract": "Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.", "link": "http://arxiv.org/abs/2602.10113v1", "date": "2026-02-10", "relevancy": 3.402, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7459}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6504}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConsID-Gen%3A%20View-Consistent%20and%20Identity-Preserving%20Image-to-Video%20Generation&body=Title%3A%20ConsID-Gen%3A%20View-Consistent%20and%20Identity-Preserving%20Image-to-Video%20Generation%0AAuthor%3A%20Mingyang%20Wu%20and%20Ashirbad%20Mishra%20and%20Soumik%20Dey%20and%20Shuo%20Xing%20and%20Naveen%20Ravipati%20and%20Hansi%20Wu%20and%20Binbin%20Li%20and%20Zhengzhong%20Tu%0AAbstract%3A%20Image-to-Video%20generation%20%28I2V%29%20animates%20a%20static%20image%20into%20a%20temporally%20coherent%20video%20sequence%20following%20textual%20instructions%2C%20yet%20preserving%20fine-grained%20object%20identity%20under%20changing%20viewpoints%20remains%20a%20persistent%20challenge.%20Unlike%20text-to-video%20models%2C%20existing%20I2V%20pipelines%20often%20suffer%20from%20appearance%20drift%20and%20geometric%20distortion%2C%20artifacts%20we%20attribute%20to%20the%20sparsity%20of%20single-view%202D%20observations%20and%20weak%20cross-modal%20alignment.%20Here%20we%20address%20this%20problem%20from%20both%20data%20and%20model%20perspectives.%20First%2C%20we%20curate%20ConsIDVid%2C%20a%20large-scale%20object-centric%20dataset%20built%20with%20a%20scalable%20pipeline%20for%20high-quality%2C%20temporally%20aligned%20videos%2C%20and%20establish%20ConsIDVid-Bench%2C%20where%20we%20present%20a%20novel%20benchmarking%20and%20evaluation%20framework%20for%20multi-view%20consistency%20using%20metrics%20sensitive%20to%20subtle%20geometric%20and%20appearance%20deviations.%20We%20further%20propose%20ConsID-Gen%2C%20a%20view-assisted%20I2V%20generation%20framework%20that%20augments%20the%20first%20frame%20with%20unposed%20auxiliary%20views%20and%20fuses%20semantic%20and%20structural%20cues%20via%20a%20dual-stream%20visual-geometric%20encoder%20as%20well%20as%20a%20text-visual%20connector%2C%20yielding%20unified%20conditioning%20for%20a%20Diffusion%20Transformer%20backbone.%20Experiments%20across%20ConsIDVid-Bench%20demonstrate%20that%20ConsID-Gen%20consistently%20outperforms%20in%20multiple%20metrics%2C%20with%20the%20best%20overall%20performance%20surpassing%20leading%20video%20generation%20models%20like%20Wan2.1%20and%20HunyuanVideo%2C%20delivering%20superior%20identity%20fidelity%20and%20temporal%20coherence%20under%20challenging%20real-world%20scenarios.%20We%20will%20release%20our%20model%20and%20dataset%20at%20https%3A//myangwu.github.io/ConsID-Gen.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsID-Gen%253A%2520View-Consistent%2520and%2520Identity-Preserving%2520Image-to-Video%2520Generation%26entry.906535625%3DMingyang%2520Wu%2520and%2520Ashirbad%2520Mishra%2520and%2520Soumik%2520Dey%2520and%2520Shuo%2520Xing%2520and%2520Naveen%2520Ravipati%2520and%2520Hansi%2520Wu%2520and%2520Binbin%2520Li%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3DImage-to-Video%2520generation%2520%2528I2V%2529%2520animates%2520a%2520static%2520image%2520into%2520a%2520temporally%2520coherent%2520video%2520sequence%2520following%2520textual%2520instructions%252C%2520yet%2520preserving%2520fine-grained%2520object%2520identity%2520under%2520changing%2520viewpoints%2520remains%2520a%2520persistent%2520challenge.%2520Unlike%2520text-to-video%2520models%252C%2520existing%2520I2V%2520pipelines%2520often%2520suffer%2520from%2520appearance%2520drift%2520and%2520geometric%2520distortion%252C%2520artifacts%2520we%2520attribute%2520to%2520the%2520sparsity%2520of%2520single-view%25202D%2520observations%2520and%2520weak%2520cross-modal%2520alignment.%2520Here%2520we%2520address%2520this%2520problem%2520from%2520both%2520data%2520and%2520model%2520perspectives.%2520First%252C%2520we%2520curate%2520ConsIDVid%252C%2520a%2520large-scale%2520object-centric%2520dataset%2520built%2520with%2520a%2520scalable%2520pipeline%2520for%2520high-quality%252C%2520temporally%2520aligned%2520videos%252C%2520and%2520establish%2520ConsIDVid-Bench%252C%2520where%2520we%2520present%2520a%2520novel%2520benchmarking%2520and%2520evaluation%2520framework%2520for%2520multi-view%2520consistency%2520using%2520metrics%2520sensitive%2520to%2520subtle%2520geometric%2520and%2520appearance%2520deviations.%2520We%2520further%2520propose%2520ConsID-Gen%252C%2520a%2520view-assisted%2520I2V%2520generation%2520framework%2520that%2520augments%2520the%2520first%2520frame%2520with%2520unposed%2520auxiliary%2520views%2520and%2520fuses%2520semantic%2520and%2520structural%2520cues%2520via%2520a%2520dual-stream%2520visual-geometric%2520encoder%2520as%2520well%2520as%2520a%2520text-visual%2520connector%252C%2520yielding%2520unified%2520conditioning%2520for%2520a%2520Diffusion%2520Transformer%2520backbone.%2520Experiments%2520across%2520ConsIDVid-Bench%2520demonstrate%2520that%2520ConsID-Gen%2520consistently%2520outperforms%2520in%2520multiple%2520metrics%252C%2520with%2520the%2520best%2520overall%2520performance%2520surpassing%2520leading%2520video%2520generation%2520models%2520like%2520Wan2.1%2520and%2520HunyuanVideo%252C%2520delivering%2520superior%2520identity%2520fidelity%2520and%2520temporal%2520coherence%2520under%2520challenging%2520real-world%2520scenarios.%2520We%2520will%2520release%2520our%2520model%2520and%2520dataset%2520at%2520https%253A//myangwu.github.io/ConsID-Gen.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConsID-Gen%3A%20View-Consistent%20and%20Identity-Preserving%20Image-to-Video%20Generation&entry.906535625=Mingyang%20Wu%20and%20Ashirbad%20Mishra%20and%20Soumik%20Dey%20and%20Shuo%20Xing%20and%20Naveen%20Ravipati%20and%20Hansi%20Wu%20and%20Binbin%20Li%20and%20Zhengzhong%20Tu&entry.1292438233=Image-to-Video%20generation%20%28I2V%29%20animates%20a%20static%20image%20into%20a%20temporally%20coherent%20video%20sequence%20following%20textual%20instructions%2C%20yet%20preserving%20fine-grained%20object%20identity%20under%20changing%20viewpoints%20remains%20a%20persistent%20challenge.%20Unlike%20text-to-video%20models%2C%20existing%20I2V%20pipelines%20often%20suffer%20from%20appearance%20drift%20and%20geometric%20distortion%2C%20artifacts%20we%20attribute%20to%20the%20sparsity%20of%20single-view%202D%20observations%20and%20weak%20cross-modal%20alignment.%20Here%20we%20address%20this%20problem%20from%20both%20data%20and%20model%20perspectives.%20First%2C%20we%20curate%20ConsIDVid%2C%20a%20large-scale%20object-centric%20dataset%20built%20with%20a%20scalable%20pipeline%20for%20high-quality%2C%20temporally%20aligned%20videos%2C%20and%20establish%20ConsIDVid-Bench%2C%20where%20we%20present%20a%20novel%20benchmarking%20and%20evaluation%20framework%20for%20multi-view%20consistency%20using%20metrics%20sensitive%20to%20subtle%20geometric%20and%20appearance%20deviations.%20We%20further%20propose%20ConsID-Gen%2C%20a%20view-assisted%20I2V%20generation%20framework%20that%20augments%20the%20first%20frame%20with%20unposed%20auxiliary%20views%20and%20fuses%20semantic%20and%20structural%20cues%20via%20a%20dual-stream%20visual-geometric%20encoder%20as%20well%20as%20a%20text-visual%20connector%2C%20yielding%20unified%20conditioning%20for%20a%20Diffusion%20Transformer%20backbone.%20Experiments%20across%20ConsIDVid-Bench%20demonstrate%20that%20ConsID-Gen%20consistently%20outperforms%20in%20multiple%20metrics%2C%20with%20the%20best%20overall%20performance%20surpassing%20leading%20video%20generation%20models%20like%20Wan2.1%20and%20HunyuanVideo%2C%20delivering%20superior%20identity%20fidelity%20and%20temporal%20coherence%20under%20challenging%20real-world%20scenarios.%20We%20will%20release%20our%20model%20and%20dataset%20at%20https%3A//myangwu.github.io/ConsID-Gen.&entry.1838667208=http%3A//arxiv.org/abs/2602.10113v1&entry.124074799=Read"},
{"title": "CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video", "author": "Hojun Song and Heejung Choi and Aro Kim and Chae-yeong Song and Gahyeon Kim and Soo Ye Kim and Jaehyup Lee and Sang-hyo Park", "abstract": "High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.", "link": "http://arxiv.org/abs/2602.09816v1", "date": "2026-02-10", "relevancy": 3.2066, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6784}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.635}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CompSplat%3A%20Compression-aware%203D%20Gaussian%20Splatting%20for%20Real-world%20Video&body=Title%3A%20CompSplat%3A%20Compression-aware%203D%20Gaussian%20Splatting%20for%20Real-world%20Video%0AAuthor%3A%20Hojun%20Song%20and%20Heejung%20Choi%20and%20Aro%20Kim%20and%20Chae-yeong%20Song%20and%20Gahyeon%20Kim%20and%20Soo%20Ye%20Kim%20and%20Jaehyup%20Lee%20and%20Sang-hyo%20Park%0AAbstract%3A%20High-quality%20novel%20view%20synthesis%20%28NVS%29%20from%20real-world%20videos%20is%20crucial%20for%20applications%20such%20as%20cultural%20heritage%20preservation%2C%20digital%20twins%2C%20and%20immersive%20media.%20However%2C%20real-world%20videos%20typically%20contain%20long%20sequences%20with%20irregular%20camera%20trajectories%20and%20unknown%20poses%2C%20leading%20to%20pose%20drift%2C%20feature%20misalignment%2C%20and%20geometric%20distortion%20during%20reconstruction.%20Moreover%2C%20lossy%20compression%20amplifies%20these%20issues%20by%20introducing%20inconsistencies%20that%20gradually%20degrade%20geometry%20and%20rendering%20quality.%20While%20recent%20studies%20have%20addressed%20either%20long-sequence%20NVS%20or%20unposed%20reconstruction%2C%20compression-aware%20approaches%20still%20focus%20on%20specific%20artifacts%20or%20limited%20scenarios%2C%20leaving%20diverse%20compression%20patterns%20in%20long%20videos%20insufficiently%20explored.%20In%20this%20paper%2C%20we%20propose%20CompSplat%2C%20a%20compression-aware%20training%20framework%20that%20explicitly%20models%20frame-wise%20compression%20characteristics%20to%20mitigate%20inter-frame%20inconsistency%20and%20accumulated%20geometric%20errors.%20CompSplat%20incorporates%20compression-aware%20frame%20weighting%20and%20an%20adaptive%20pruning%20strategy%20to%20enhance%20robustness%20and%20geometric%20consistency%2C%20particularly%20under%20heavy%20compression.%20Extensive%20experiments%20on%20challenging%20benchmarks%2C%20including%20Tanks%20and%20Temples%2C%20Free%2C%20and%20Hike%2C%20demonstrate%20that%20CompSplat%20achieves%20state-of-the-art%20rendering%20quality%20and%20pose%20accuracy%2C%20significantly%20surpassing%20most%20recent%20state-of-the-art%20NVS%20approaches%20under%20severe%20compression%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompSplat%253A%2520Compression-aware%25203D%2520Gaussian%2520Splatting%2520for%2520Real-world%2520Video%26entry.906535625%3DHojun%2520Song%2520and%2520Heejung%2520Choi%2520and%2520Aro%2520Kim%2520and%2520Chae-yeong%2520Song%2520and%2520Gahyeon%2520Kim%2520and%2520Soo%2520Ye%2520Kim%2520and%2520Jaehyup%2520Lee%2520and%2520Sang-hyo%2520Park%26entry.1292438233%3DHigh-quality%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520from%2520real-world%2520videos%2520is%2520crucial%2520for%2520applications%2520such%2520as%2520cultural%2520heritage%2520preservation%252C%2520digital%2520twins%252C%2520and%2520immersive%2520media.%2520However%252C%2520real-world%2520videos%2520typically%2520contain%2520long%2520sequences%2520with%2520irregular%2520camera%2520trajectories%2520and%2520unknown%2520poses%252C%2520leading%2520to%2520pose%2520drift%252C%2520feature%2520misalignment%252C%2520and%2520geometric%2520distortion%2520during%2520reconstruction.%2520Moreover%252C%2520lossy%2520compression%2520amplifies%2520these%2520issues%2520by%2520introducing%2520inconsistencies%2520that%2520gradually%2520degrade%2520geometry%2520and%2520rendering%2520quality.%2520While%2520recent%2520studies%2520have%2520addressed%2520either%2520long-sequence%2520NVS%2520or%2520unposed%2520reconstruction%252C%2520compression-aware%2520approaches%2520still%2520focus%2520on%2520specific%2520artifacts%2520or%2520limited%2520scenarios%252C%2520leaving%2520diverse%2520compression%2520patterns%2520in%2520long%2520videos%2520insufficiently%2520explored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CompSplat%252C%2520a%2520compression-aware%2520training%2520framework%2520that%2520explicitly%2520models%2520frame-wise%2520compression%2520characteristics%2520to%2520mitigate%2520inter-frame%2520inconsistency%2520and%2520accumulated%2520geometric%2520errors.%2520CompSplat%2520incorporates%2520compression-aware%2520frame%2520weighting%2520and%2520an%2520adaptive%2520pruning%2520strategy%2520to%2520enhance%2520robustness%2520and%2520geometric%2520consistency%252C%2520particularly%2520under%2520heavy%2520compression.%2520Extensive%2520experiments%2520on%2520challenging%2520benchmarks%252C%2520including%2520Tanks%2520and%2520Temples%252C%2520Free%252C%2520and%2520Hike%252C%2520demonstrate%2520that%2520CompSplat%2520achieves%2520state-of-the-art%2520rendering%2520quality%2520and%2520pose%2520accuracy%252C%2520significantly%2520surpassing%2520most%2520recent%2520state-of-the-art%2520NVS%2520approaches%2520under%2520severe%2520compression%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CompSplat%3A%20Compression-aware%203D%20Gaussian%20Splatting%20for%20Real-world%20Video&entry.906535625=Hojun%20Song%20and%20Heejung%20Choi%20and%20Aro%20Kim%20and%20Chae-yeong%20Song%20and%20Gahyeon%20Kim%20and%20Soo%20Ye%20Kim%20and%20Jaehyup%20Lee%20and%20Sang-hyo%20Park&entry.1292438233=High-quality%20novel%20view%20synthesis%20%28NVS%29%20from%20real-world%20videos%20is%20crucial%20for%20applications%20such%20as%20cultural%20heritage%20preservation%2C%20digital%20twins%2C%20and%20immersive%20media.%20However%2C%20real-world%20videos%20typically%20contain%20long%20sequences%20with%20irregular%20camera%20trajectories%20and%20unknown%20poses%2C%20leading%20to%20pose%20drift%2C%20feature%20misalignment%2C%20and%20geometric%20distortion%20during%20reconstruction.%20Moreover%2C%20lossy%20compression%20amplifies%20these%20issues%20by%20introducing%20inconsistencies%20that%20gradually%20degrade%20geometry%20and%20rendering%20quality.%20While%20recent%20studies%20have%20addressed%20either%20long-sequence%20NVS%20or%20unposed%20reconstruction%2C%20compression-aware%20approaches%20still%20focus%20on%20specific%20artifacts%20or%20limited%20scenarios%2C%20leaving%20diverse%20compression%20patterns%20in%20long%20videos%20insufficiently%20explored.%20In%20this%20paper%2C%20we%20propose%20CompSplat%2C%20a%20compression-aware%20training%20framework%20that%20explicitly%20models%20frame-wise%20compression%20characteristics%20to%20mitigate%20inter-frame%20inconsistency%20and%20accumulated%20geometric%20errors.%20CompSplat%20incorporates%20compression-aware%20frame%20weighting%20and%20an%20adaptive%20pruning%20strategy%20to%20enhance%20robustness%20and%20geometric%20consistency%2C%20particularly%20under%20heavy%20compression.%20Extensive%20experiments%20on%20challenging%20benchmarks%2C%20including%20Tanks%20and%20Temples%2C%20Free%2C%20and%20Hike%2C%20demonstrate%20that%20CompSplat%20achieves%20state-of-the-art%20rendering%20quality%20and%20pose%20accuracy%2C%20significantly%20surpassing%20most%20recent%20state-of-the-art%20NVS%20approaches%20under%20severe%20compression%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2602.09816v1&entry.124074799=Read"},
{"title": "OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics", "author": "Jisang Yoo and Gyeongjin Kang and Hyun-kyu Ko and Hyeonwoo Yu and Eunbyung Park", "abstract": "Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.", "link": "http://arxiv.org/abs/2512.08625v2", "date": "2026-02-10", "relevancy": 3.1907, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.699}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6089}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenMonoGS-SLAM%3A%20Monocular%20Gaussian%20Splatting%20SLAM%20with%20Open-set%20Semantics&body=Title%3A%20OpenMonoGS-SLAM%3A%20Monocular%20Gaussian%20Splatting%20SLAM%20with%20Open-set%20Semantics%0AAuthor%3A%20Jisang%20Yoo%20and%20Gyeongjin%20Kang%20and%20Hyun-kyu%20Ko%20and%20Hyeonwoo%20Yu%20and%20Eunbyung%20Park%0AAbstract%3A%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20foundational%20component%20in%20robotics%2C%20AR/VR%2C%20and%20autonomous%20systems.%20With%20the%20rising%20focus%20on%20spatial%20AI%20in%20recent%20years%2C%20combining%20SLAM%20with%20semantic%20understanding%20has%20become%20increasingly%20important%20for%20enabling%20intelligent%20perception%20and%20interaction.%20Recent%20efforts%20have%20explored%20this%20integration%2C%20but%20they%20often%20rely%20on%20depth%20sensors%20or%20closed-set%20semantic%20models%2C%20limiting%20their%20scalability%20and%20adaptability%20in%20open-world%20environments.%20In%20this%20work%2C%20we%20present%20OpenMonoGS-SLAM%2C%20the%20first%20monocular%20SLAM%20framework%20that%20unifies%203D%20Gaussian%20Splatting%20%283DGS%29%20with%20open-set%20semantic%20understanding.%20To%20achieve%20our%20goal%2C%20we%20leverage%20recent%20advances%20in%20Visual%20Foundation%20Models%20%28VFMs%29%2C%20including%20MASt3R%20for%20visual%20geometry%20and%20SAM%20and%20CLIP%20for%20open-vocabulary%20semantics.%20These%20models%20provide%20robust%20generalization%20across%20diverse%20tasks%2C%20enabling%20accurate%20monocular%20camera%20tracking%20and%20mapping%2C%20as%20well%20as%20a%20rich%20understanding%20of%20semantics%20in%20open-world%20environments.%20Our%20method%20operates%20without%20any%20depth%20input%20or%203D%20semantic%20ground%20truth%2C%20relying%20solely%20on%20self-supervised%20learning%20objectives.%20Furthermore%2C%20we%20propose%20a%20memory%20mechanism%20specifically%20designed%20to%20manage%20high-dimensional%20semantic%20features%2C%20which%20effectively%20constructs%20Gaussian%20semantic%20feature%20maps%2C%20leading%20to%20strong%20overall%20performance.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20performance%20comparable%20to%20or%20surpassing%20existing%20baselines%20in%20both%20closed-set%20and%20open-set%20segmentation%20tasks%2C%20all%20without%20relying%20on%20supplementary%20sensors%20such%20as%20depth%20maps%20or%20semantic%20annotations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenMonoGS-SLAM%253A%2520Monocular%2520Gaussian%2520Splatting%2520SLAM%2520with%2520Open-set%2520Semantics%26entry.906535625%3DJisang%2520Yoo%2520and%2520Gyeongjin%2520Kang%2520and%2520Hyun-kyu%2520Ko%2520and%2520Hyeonwoo%2520Yu%2520and%2520Eunbyung%2520Park%26entry.1292438233%3DSimultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520a%2520foundational%2520component%2520in%2520robotics%252C%2520AR/VR%252C%2520and%2520autonomous%2520systems.%2520With%2520the%2520rising%2520focus%2520on%2520spatial%2520AI%2520in%2520recent%2520years%252C%2520combining%2520SLAM%2520with%2520semantic%2520understanding%2520has%2520become%2520increasingly%2520important%2520for%2520enabling%2520intelligent%2520perception%2520and%2520interaction.%2520Recent%2520efforts%2520have%2520explored%2520this%2520integration%252C%2520but%2520they%2520often%2520rely%2520on%2520depth%2520sensors%2520or%2520closed-set%2520semantic%2520models%252C%2520limiting%2520their%2520scalability%2520and%2520adaptability%2520in%2520open-world%2520environments.%2520In%2520this%2520work%252C%2520we%2520present%2520OpenMonoGS-SLAM%252C%2520the%2520first%2520monocular%2520SLAM%2520framework%2520that%2520unifies%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520with%2520open-set%2520semantic%2520understanding.%2520To%2520achieve%2520our%2520goal%252C%2520we%2520leverage%2520recent%2520advances%2520in%2520Visual%2520Foundation%2520Models%2520%2528VFMs%2529%252C%2520including%2520MASt3R%2520for%2520visual%2520geometry%2520and%2520SAM%2520and%2520CLIP%2520for%2520open-vocabulary%2520semantics.%2520These%2520models%2520provide%2520robust%2520generalization%2520across%2520diverse%2520tasks%252C%2520enabling%2520accurate%2520monocular%2520camera%2520tracking%2520and%2520mapping%252C%2520as%2520well%2520as%2520a%2520rich%2520understanding%2520of%2520semantics%2520in%2520open-world%2520environments.%2520Our%2520method%2520operates%2520without%2520any%2520depth%2520input%2520or%25203D%2520semantic%2520ground%2520truth%252C%2520relying%2520solely%2520on%2520self-supervised%2520learning%2520objectives.%2520Furthermore%252C%2520we%2520propose%2520a%2520memory%2520mechanism%2520specifically%2520designed%2520to%2520manage%2520high-dimensional%2520semantic%2520features%252C%2520which%2520effectively%2520constructs%2520Gaussian%2520semantic%2520feature%2520maps%252C%2520leading%2520to%2520strong%2520overall%2520performance.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520performance%2520comparable%2520to%2520or%2520surpassing%2520existing%2520baselines%2520in%2520both%2520closed-set%2520and%2520open-set%2520segmentation%2520tasks%252C%2520all%2520without%2520relying%2520on%2520supplementary%2520sensors%2520such%2520as%2520depth%2520maps%2520or%2520semantic%2520annotations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenMonoGS-SLAM%3A%20Monocular%20Gaussian%20Splatting%20SLAM%20with%20Open-set%20Semantics&entry.906535625=Jisang%20Yoo%20and%20Gyeongjin%20Kang%20and%20Hyun-kyu%20Ko%20and%20Hyeonwoo%20Yu%20and%20Eunbyung%20Park&entry.1292438233=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20foundational%20component%20in%20robotics%2C%20AR/VR%2C%20and%20autonomous%20systems.%20With%20the%20rising%20focus%20on%20spatial%20AI%20in%20recent%20years%2C%20combining%20SLAM%20with%20semantic%20understanding%20has%20become%20increasingly%20important%20for%20enabling%20intelligent%20perception%20and%20interaction.%20Recent%20efforts%20have%20explored%20this%20integration%2C%20but%20they%20often%20rely%20on%20depth%20sensors%20or%20closed-set%20semantic%20models%2C%20limiting%20their%20scalability%20and%20adaptability%20in%20open-world%20environments.%20In%20this%20work%2C%20we%20present%20OpenMonoGS-SLAM%2C%20the%20first%20monocular%20SLAM%20framework%20that%20unifies%203D%20Gaussian%20Splatting%20%283DGS%29%20with%20open-set%20semantic%20understanding.%20To%20achieve%20our%20goal%2C%20we%20leverage%20recent%20advances%20in%20Visual%20Foundation%20Models%20%28VFMs%29%2C%20including%20MASt3R%20for%20visual%20geometry%20and%20SAM%20and%20CLIP%20for%20open-vocabulary%20semantics.%20These%20models%20provide%20robust%20generalization%20across%20diverse%20tasks%2C%20enabling%20accurate%20monocular%20camera%20tracking%20and%20mapping%2C%20as%20well%20as%20a%20rich%20understanding%20of%20semantics%20in%20open-world%20environments.%20Our%20method%20operates%20without%20any%20depth%20input%20or%203D%20semantic%20ground%20truth%2C%20relying%20solely%20on%20self-supervised%20learning%20objectives.%20Furthermore%2C%20we%20propose%20a%20memory%20mechanism%20specifically%20designed%20to%20manage%20high-dimensional%20semantic%20features%2C%20which%20effectively%20constructs%20Gaussian%20semantic%20feature%20maps%2C%20leading%20to%20strong%20overall%20performance.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20performance%20comparable%20to%20or%20surpassing%20existing%20baselines%20in%20both%20closed-set%20and%20open-set%20segmentation%20tasks%2C%20all%20without%20relying%20on%20supplementary%20sensors%20such%20as%20depth%20maps%20or%20semantic%20annotations.&entry.1838667208=http%3A//arxiv.org/abs/2512.08625v2&entry.124074799=Read"},
{"title": "VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization", "author": "Yikun Liu and Yuan Liu and Shangzhe Di and Haicheng Wang and Zhongyin Zhao and Le Tian and Xiao Zhou and Jie Zhou and Jiangchao Yao and Yanfeng Wang and Weidi Xie", "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.", "link": "http://arxiv.org/abs/2602.09934v1", "date": "2026-02-10", "relevancy": 3.0968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.631}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VersaViT%3A%20Enhancing%20MLLM%20Vision%20Backbones%20via%20Task-Guided%20Optimization&body=Title%3A%20VersaViT%3A%20Enhancing%20MLLM%20Vision%20Backbones%20via%20Task-Guided%20Optimization%0AAuthor%3A%20Yikun%20Liu%20and%20Yuan%20Liu%20and%20Shangzhe%20Di%20and%20Haicheng%20Wang%20and%20Zhongyin%20Zhao%20and%20Le%20Tian%20and%20Xiao%20Zhou%20and%20Jie%20Zhou%20and%20Jiangchao%20Yao%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20achieved%20remarkable%20success%20in%20visual-language%20understanding%2C%20demonstrating%20superior%20high-level%20semantic%20alignment%20within%20their%20vision%20encoders.%20An%20important%20question%20thus%20arises%3A%20Can%20these%20encoders%20serve%20as%20versatile%20vision%20backbones%2C%20capable%20of%20reliably%20performing%20classic%20vision-centric%20tasks%20as%20well%3F%20To%20address%20the%20question%2C%20we%20make%20the%20following%20contributions%3A%20%28i%29%20we%20identify%20that%20the%20vision%20encoders%20within%20MLLMs%20exhibit%20deficiencies%20in%20their%20dense%20feature%20representations%2C%20as%20evidenced%20by%20their%20suboptimal%20performance%20on%20dense%20prediction%20tasks%20%28e.g.%2C%20semantic%20segmentation%2C%20depth%20estimation%29%3B%20%28ii%29%20we%20propose%20VersaViT%2C%20a%20well-rounded%20vision%20transformer%20that%20instantiates%20a%20novel%20multi-task%20framework%20for%20collaborative%20post-training.%20This%20framework%20facilitates%20the%20optimization%20of%20the%20vision%20backbone%20via%20lightweight%20task%20heads%20with%20multi-granularity%20supervision%3B%20%28iii%29%20extensive%20experiments%20across%20various%20downstream%20tasks%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20yielding%20a%20versatile%20vision%20backbone%20suited%20for%20both%20language-mediated%20reasoning%20and%20pixel-level%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVersaViT%253A%2520Enhancing%2520MLLM%2520Vision%2520Backbones%2520via%2520Task-Guided%2520Optimization%26entry.906535625%3DYikun%2520Liu%2520and%2520Yuan%2520Liu%2520and%2520Shangzhe%2520Di%2520and%2520Haicheng%2520Wang%2520and%2520Zhongyin%2520Zhao%2520and%2520Le%2520Tian%2520and%2520Xiao%2520Zhou%2520and%2520Jie%2520Zhou%2520and%2520Jiangchao%2520Yao%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520recently%2520achieved%2520remarkable%2520success%2520in%2520visual-language%2520understanding%252C%2520demonstrating%2520superior%2520high-level%2520semantic%2520alignment%2520within%2520their%2520vision%2520encoders.%2520An%2520important%2520question%2520thus%2520arises%253A%2520Can%2520these%2520encoders%2520serve%2520as%2520versatile%2520vision%2520backbones%252C%2520capable%2520of%2520reliably%2520performing%2520classic%2520vision-centric%2520tasks%2520as%2520well%253F%2520To%2520address%2520the%2520question%252C%2520we%2520make%2520the%2520following%2520contributions%253A%2520%2528i%2529%2520we%2520identify%2520that%2520the%2520vision%2520encoders%2520within%2520MLLMs%2520exhibit%2520deficiencies%2520in%2520their%2520dense%2520feature%2520representations%252C%2520as%2520evidenced%2520by%2520their%2520suboptimal%2520performance%2520on%2520dense%2520prediction%2520tasks%2520%2528e.g.%252C%2520semantic%2520segmentation%252C%2520depth%2520estimation%2529%253B%2520%2528ii%2529%2520we%2520propose%2520VersaViT%252C%2520a%2520well-rounded%2520vision%2520transformer%2520that%2520instantiates%2520a%2520novel%2520multi-task%2520framework%2520for%2520collaborative%2520post-training.%2520This%2520framework%2520facilitates%2520the%2520optimization%2520of%2520the%2520vision%2520backbone%2520via%2520lightweight%2520task%2520heads%2520with%2520multi-granularity%2520supervision%253B%2520%2528iii%2529%2520extensive%2520experiments%2520across%2520various%2520downstream%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520yielding%2520a%2520versatile%2520vision%2520backbone%2520suited%2520for%2520both%2520language-mediated%2520reasoning%2520and%2520pixel-level%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VersaViT%3A%20Enhancing%20MLLM%20Vision%20Backbones%20via%20Task-Guided%20Optimization&entry.906535625=Yikun%20Liu%20and%20Yuan%20Liu%20and%20Shangzhe%20Di%20and%20Haicheng%20Wang%20and%20Zhongyin%20Zhao%20and%20Le%20Tian%20and%20Xiao%20Zhou%20and%20Jie%20Zhou%20and%20Jiangchao%20Yao%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20achieved%20remarkable%20success%20in%20visual-language%20understanding%2C%20demonstrating%20superior%20high-level%20semantic%20alignment%20within%20their%20vision%20encoders.%20An%20important%20question%20thus%20arises%3A%20Can%20these%20encoders%20serve%20as%20versatile%20vision%20backbones%2C%20capable%20of%20reliably%20performing%20classic%20vision-centric%20tasks%20as%20well%3F%20To%20address%20the%20question%2C%20we%20make%20the%20following%20contributions%3A%20%28i%29%20we%20identify%20that%20the%20vision%20encoders%20within%20MLLMs%20exhibit%20deficiencies%20in%20their%20dense%20feature%20representations%2C%20as%20evidenced%20by%20their%20suboptimal%20performance%20on%20dense%20prediction%20tasks%20%28e.g.%2C%20semantic%20segmentation%2C%20depth%20estimation%29%3B%20%28ii%29%20we%20propose%20VersaViT%2C%20a%20well-rounded%20vision%20transformer%20that%20instantiates%20a%20novel%20multi-task%20framework%20for%20collaborative%20post-training.%20This%20framework%20facilitates%20the%20optimization%20of%20the%20vision%20backbone%20via%20lightweight%20task%20heads%20with%20multi-granularity%20supervision%3B%20%28iii%29%20extensive%20experiments%20across%20various%20downstream%20tasks%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20yielding%20a%20versatile%20vision%20backbone%20suited%20for%20both%20language-mediated%20reasoning%20and%20pixel-level%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2602.09934v1&entry.124074799=Read"},
{"title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors", "author": "Zhengshen Zhang and Hao Li and Yalun Dai and Zhengbang Zhu and Lei Zhou and Chenchen Liu and Dong Wang and Francis E. H. Tay and Sijin Chen and Ziwei Liu and Yuxiao Liu and Xinghang Li and Pan Zhou", "abstract": "Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.", "link": "http://arxiv.org/abs/2510.17439v2", "date": "2026-02-10", "relevancy": 3.0363, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.638}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Spatial%20to%20Actions%3A%20Grounding%20Vision-Language-Action%20Model%20in%20Spatial%20Foundation%20Priors&body=Title%3A%20From%20Spatial%20to%20Actions%3A%20Grounding%20Vision-Language-Action%20Model%20in%20Spatial%20Foundation%20Priors%0AAuthor%3A%20Zhengshen%20Zhang%20and%20Hao%20Li%20and%20Yalun%20Dai%20and%20Zhengbang%20Zhu%20and%20Lei%20Zhou%20and%20Chenchen%20Liu%20and%20Dong%20Wang%20and%20Francis%20E.%20H.%20Tay%20and%20Sijin%20Chen%20and%20Ziwei%20Liu%20and%20Yuxiao%20Liu%20and%20Xinghang%20Li%20and%20Pan%20Zhou%0AAbstract%3A%20Existing%20vision-language-action%20%28VLA%29%20models%20act%20in%203D%20real-world%20but%20are%20typically%20built%20on%202D%20encoders%2C%20leaving%20a%20spatial%20reasoning%20gap%20that%20limits%20generalization%20and%20adaptability.%20Recent%203D%20integration%20techniques%20for%20VLAs%20either%20require%20specialized%20sensors%20and%20transfer%20poorly%20across%20modalities%2C%20or%20inject%20weak%20cues%20that%20lack%20geometry%20and%20degrade%20vision-language%20alignment.%20In%20this%20work%2C%20we%20introduce%20FALCON%20%28From%20Spatial%20to%20Action%29%2C%20a%20novel%20paradigm%20that%20injects%20rich%203D%20spatial%20tokens%20into%20the%20action%20head.%20FALCON%20leverages%20spatial%20foundation%20models%20to%20deliver%20strong%20geometric%20priors%20from%20RGB%20alone%2C%20and%20includes%20an%20Embodied%20Spatial%20Model%20that%20can%20optionally%20fuse%20depth%2C%20or%20pose%20for%20higher%20fidelity%20when%20available%2C%20without%20retraining%20or%20architectural%20changes.%20To%20preserve%20language%20reasoning%2C%20spatial%20tokens%20are%20consumed%20by%20a%20Spatial-Enhanced%20Action%20Head%20rather%20than%20being%20concatenated%20into%20the%20vision-language%20backbone.%20These%20designs%20enable%20FALCON%20to%20address%20limitations%20in%20spatial%20representation%2C%20modality%20transferability%2C%20and%20alignment.%20In%20comprehensive%20evaluations%20across%20three%20simulation%20benchmarks%20and%20eleven%20real-world%20tasks%2C%20our%20proposed%20FALCON%20achieves%20state-of-the-art%20performance%2C%20consistently%20surpasses%20competitive%20baselines%2C%20and%20remains%20robust%20under%20clutter%2C%20spatial-prompt%20conditioning%2C%20and%20variations%20in%20object%20scale%20and%20height.%0ALink%3A%20http%3A//arxiv.org/abs/2510.17439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Spatial%2520to%2520Actions%253A%2520Grounding%2520Vision-Language-Action%2520Model%2520in%2520Spatial%2520Foundation%2520Priors%26entry.906535625%3DZhengshen%2520Zhang%2520and%2520Hao%2520Li%2520and%2520Yalun%2520Dai%2520and%2520Zhengbang%2520Zhu%2520and%2520Lei%2520Zhou%2520and%2520Chenchen%2520Liu%2520and%2520Dong%2520Wang%2520and%2520Francis%2520E.%2520H.%2520Tay%2520and%2520Sijin%2520Chen%2520and%2520Ziwei%2520Liu%2520and%2520Yuxiao%2520Liu%2520and%2520Xinghang%2520Li%2520and%2520Pan%2520Zhou%26entry.1292438233%3DExisting%2520vision-language-action%2520%2528VLA%2529%2520models%2520act%2520in%25203D%2520real-world%2520but%2520are%2520typically%2520built%2520on%25202D%2520encoders%252C%2520leaving%2520a%2520spatial%2520reasoning%2520gap%2520that%2520limits%2520generalization%2520and%2520adaptability.%2520Recent%25203D%2520integration%2520techniques%2520for%2520VLAs%2520either%2520require%2520specialized%2520sensors%2520and%2520transfer%2520poorly%2520across%2520modalities%252C%2520or%2520inject%2520weak%2520cues%2520that%2520lack%2520geometry%2520and%2520degrade%2520vision-language%2520alignment.%2520In%2520this%2520work%252C%2520we%2520introduce%2520FALCON%2520%2528From%2520Spatial%2520to%2520Action%2529%252C%2520a%2520novel%2520paradigm%2520that%2520injects%2520rich%25203D%2520spatial%2520tokens%2520into%2520the%2520action%2520head.%2520FALCON%2520leverages%2520spatial%2520foundation%2520models%2520to%2520deliver%2520strong%2520geometric%2520priors%2520from%2520RGB%2520alone%252C%2520and%2520includes%2520an%2520Embodied%2520Spatial%2520Model%2520that%2520can%2520optionally%2520fuse%2520depth%252C%2520or%2520pose%2520for%2520higher%2520fidelity%2520when%2520available%252C%2520without%2520retraining%2520or%2520architectural%2520changes.%2520To%2520preserve%2520language%2520reasoning%252C%2520spatial%2520tokens%2520are%2520consumed%2520by%2520a%2520Spatial-Enhanced%2520Action%2520Head%2520rather%2520than%2520being%2520concatenated%2520into%2520the%2520vision-language%2520backbone.%2520These%2520designs%2520enable%2520FALCON%2520to%2520address%2520limitations%2520in%2520spatial%2520representation%252C%2520modality%2520transferability%252C%2520and%2520alignment.%2520In%2520comprehensive%2520evaluations%2520across%2520three%2520simulation%2520benchmarks%2520and%2520eleven%2520real-world%2520tasks%252C%2520our%2520proposed%2520FALCON%2520achieves%2520state-of-the-art%2520performance%252C%2520consistently%2520surpasses%2520competitive%2520baselines%252C%2520and%2520remains%2520robust%2520under%2520clutter%252C%2520spatial-prompt%2520conditioning%252C%2520and%2520variations%2520in%2520object%2520scale%2520and%2520height.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Spatial%20to%20Actions%3A%20Grounding%20Vision-Language-Action%20Model%20in%20Spatial%20Foundation%20Priors&entry.906535625=Zhengshen%20Zhang%20and%20Hao%20Li%20and%20Yalun%20Dai%20and%20Zhengbang%20Zhu%20and%20Lei%20Zhou%20and%20Chenchen%20Liu%20and%20Dong%20Wang%20and%20Francis%20E.%20H.%20Tay%20and%20Sijin%20Chen%20and%20Ziwei%20Liu%20and%20Yuxiao%20Liu%20and%20Xinghang%20Li%20and%20Pan%20Zhou&entry.1292438233=Existing%20vision-language-action%20%28VLA%29%20models%20act%20in%203D%20real-world%20but%20are%20typically%20built%20on%202D%20encoders%2C%20leaving%20a%20spatial%20reasoning%20gap%20that%20limits%20generalization%20and%20adaptability.%20Recent%203D%20integration%20techniques%20for%20VLAs%20either%20require%20specialized%20sensors%20and%20transfer%20poorly%20across%20modalities%2C%20or%20inject%20weak%20cues%20that%20lack%20geometry%20and%20degrade%20vision-language%20alignment.%20In%20this%20work%2C%20we%20introduce%20FALCON%20%28From%20Spatial%20to%20Action%29%2C%20a%20novel%20paradigm%20that%20injects%20rich%203D%20spatial%20tokens%20into%20the%20action%20head.%20FALCON%20leverages%20spatial%20foundation%20models%20to%20deliver%20strong%20geometric%20priors%20from%20RGB%20alone%2C%20and%20includes%20an%20Embodied%20Spatial%20Model%20that%20can%20optionally%20fuse%20depth%2C%20or%20pose%20for%20higher%20fidelity%20when%20available%2C%20without%20retraining%20or%20architectural%20changes.%20To%20preserve%20language%20reasoning%2C%20spatial%20tokens%20are%20consumed%20by%20a%20Spatial-Enhanced%20Action%20Head%20rather%20than%20being%20concatenated%20into%20the%20vision-language%20backbone.%20These%20designs%20enable%20FALCON%20to%20address%20limitations%20in%20spatial%20representation%2C%20modality%20transferability%2C%20and%20alignment.%20In%20comprehensive%20evaluations%20across%20three%20simulation%20benchmarks%20and%20eleven%20real-world%20tasks%2C%20our%20proposed%20FALCON%20achieves%20state-of-the-art%20performance%2C%20consistently%20surpasses%20competitive%20baselines%2C%20and%20remains%20robust%20under%20clutter%2C%20spatial-prompt%20conditioning%2C%20and%20variations%20in%20object%20scale%20and%20height.&entry.1838667208=http%3A//arxiv.org/abs/2510.17439v2&entry.124074799=Read"},
{"title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models", "author": "Ruisi Zhao and Haoren Zheng and Zongxin Yang and Hehe Fan and Yi Yang", "abstract": "Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.", "link": "http://arxiv.org/abs/2602.09713v1", "date": "2026-02-10", "relevancy": 3.0257, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6586}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5945}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stroke3D%3A%20Lifting%202D%20strokes%20into%20rigged%203D%20model%20via%20latent%20diffusion%20models&body=Title%3A%20Stroke3D%3A%20Lifting%202D%20strokes%20into%20rigged%203D%20model%20via%20latent%20diffusion%20models%0AAuthor%3A%20Ruisi%20Zhao%20and%20Haoren%20Zheng%20and%20Zongxin%20Yang%20and%20Hehe%20Fan%20and%20Yi%20Yang%0AAbstract%3A%20Rigged%203D%20assets%20are%20fundamental%20to%203D%20deformation%20and%20animation.%20However%2C%20existing%203D%20generation%20methods%20face%20challenges%20in%20generating%20animatable%20geometry%2C%20while%20rigging%20techniques%20lack%20fine-grained%20structural%20control%20over%20skeleton%20creation.%20To%20address%20these%20limitations%2C%20we%20introduce%20Stroke3D%2C%20a%20novel%20framework%20that%20directly%20generates%20rigged%20meshes%20from%20user%20inputs%3A%202D%20drawn%20strokes%20and%20a%20descriptive%20text%20prompt.%20Our%20approach%20pioneers%20a%20two-stage%20pipeline%20that%20separates%20the%20generation%20into%3A%201%29%20Controllable%20Skeleton%20Generation%2C%20we%20employ%20the%20Skeletal%20Graph%20VAE%20%28Sk-VAE%29%20to%20encode%20the%20skeleton%27s%20graph%20structure%20into%20a%20latent%20space%2C%20where%20the%20Skeletal%20Graph%20DiT%20%28Sk-DiT%29%20generates%20a%20skeletal%20embedding.%20The%20generation%20process%20is%20conditioned%20on%20both%20the%20text%20for%20semantics%20and%20the%202D%20strokes%20for%20explicit%20structural%20control%2C%20with%20the%20VAE%27s%20decoder%20reconstructing%20the%20final%20high-quality%203D%20skeleton%3B%20and%202%29%20Enhanced%20Mesh%20Synthesis%20via%20TextuRig%20and%20SKA-DPO%2C%20where%20we%20then%20synthesize%20a%20textured%20mesh%20conditioned%20on%20the%20generated%20skeleton.%20For%20this%20stage%2C%20we%20first%20enhance%20an%20existing%20skeleton-to-mesh%20model%20by%20augmenting%20its%20training%20data%20with%20TextuRig%3A%20a%20dataset%20of%20textured%20and%20rigged%20meshes%20with%20captions%2C%20curated%20from%20Objaverse-XL.%20Additionally%2C%20we%20employ%20a%20preference%20optimization%20strategy%2C%20SKA-DPO%2C%20guided%20by%20a%20skeleton-mesh%20alignment%20score%2C%20to%20further%20improve%20geometric%20fidelity.%20Together%2C%20our%20framework%20enables%20a%20more%20intuitive%20workflow%20for%20creating%20ready%20to%20animate%203D%20content.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20generate%20rigged%203D%20meshes%20conditioned%20on%20user-drawn%202D%20strokes.%20Extensive%20experiments%20demonstrate%20that%20Stroke3D%20produces%20plausible%20skeletons%20and%20high-quality%20meshes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStroke3D%253A%2520Lifting%25202D%2520strokes%2520into%2520rigged%25203D%2520model%2520via%2520latent%2520diffusion%2520models%26entry.906535625%3DRuisi%2520Zhao%2520and%2520Haoren%2520Zheng%2520and%2520Zongxin%2520Yang%2520and%2520Hehe%2520Fan%2520and%2520Yi%2520Yang%26entry.1292438233%3DRigged%25203D%2520assets%2520are%2520fundamental%2520to%25203D%2520deformation%2520and%2520animation.%2520However%252C%2520existing%25203D%2520generation%2520methods%2520face%2520challenges%2520in%2520generating%2520animatable%2520geometry%252C%2520while%2520rigging%2520techniques%2520lack%2520fine-grained%2520structural%2520control%2520over%2520skeleton%2520creation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Stroke3D%252C%2520a%2520novel%2520framework%2520that%2520directly%2520generates%2520rigged%2520meshes%2520from%2520user%2520inputs%253A%25202D%2520drawn%2520strokes%2520and%2520a%2520descriptive%2520text%2520prompt.%2520Our%2520approach%2520pioneers%2520a%2520two-stage%2520pipeline%2520that%2520separates%2520the%2520generation%2520into%253A%25201%2529%2520Controllable%2520Skeleton%2520Generation%252C%2520we%2520employ%2520the%2520Skeletal%2520Graph%2520VAE%2520%2528Sk-VAE%2529%2520to%2520encode%2520the%2520skeleton%2527s%2520graph%2520structure%2520into%2520a%2520latent%2520space%252C%2520where%2520the%2520Skeletal%2520Graph%2520DiT%2520%2528Sk-DiT%2529%2520generates%2520a%2520skeletal%2520embedding.%2520The%2520generation%2520process%2520is%2520conditioned%2520on%2520both%2520the%2520text%2520for%2520semantics%2520and%2520the%25202D%2520strokes%2520for%2520explicit%2520structural%2520control%252C%2520with%2520the%2520VAE%2527s%2520decoder%2520reconstructing%2520the%2520final%2520high-quality%25203D%2520skeleton%253B%2520and%25202%2529%2520Enhanced%2520Mesh%2520Synthesis%2520via%2520TextuRig%2520and%2520SKA-DPO%252C%2520where%2520we%2520then%2520synthesize%2520a%2520textured%2520mesh%2520conditioned%2520on%2520the%2520generated%2520skeleton.%2520For%2520this%2520stage%252C%2520we%2520first%2520enhance%2520an%2520existing%2520skeleton-to-mesh%2520model%2520by%2520augmenting%2520its%2520training%2520data%2520with%2520TextuRig%253A%2520a%2520dataset%2520of%2520textured%2520and%2520rigged%2520meshes%2520with%2520captions%252C%2520curated%2520from%2520Objaverse-XL.%2520Additionally%252C%2520we%2520employ%2520a%2520preference%2520optimization%2520strategy%252C%2520SKA-DPO%252C%2520guided%2520by%2520a%2520skeleton-mesh%2520alignment%2520score%252C%2520to%2520further%2520improve%2520geometric%2520fidelity.%2520Together%252C%2520our%2520framework%2520enables%2520a%2520more%2520intuitive%2520workflow%2520for%2520creating%2520ready%2520to%2520animate%25203D%2520content.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520generate%2520rigged%25203D%2520meshes%2520conditioned%2520on%2520user-drawn%25202D%2520strokes.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Stroke3D%2520produces%2520plausible%2520skeletons%2520and%2520high-quality%2520meshes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stroke3D%3A%20Lifting%202D%20strokes%20into%20rigged%203D%20model%20via%20latent%20diffusion%20models&entry.906535625=Ruisi%20Zhao%20and%20Haoren%20Zheng%20and%20Zongxin%20Yang%20and%20Hehe%20Fan%20and%20Yi%20Yang&entry.1292438233=Rigged%203D%20assets%20are%20fundamental%20to%203D%20deformation%20and%20animation.%20However%2C%20existing%203D%20generation%20methods%20face%20challenges%20in%20generating%20animatable%20geometry%2C%20while%20rigging%20techniques%20lack%20fine-grained%20structural%20control%20over%20skeleton%20creation.%20To%20address%20these%20limitations%2C%20we%20introduce%20Stroke3D%2C%20a%20novel%20framework%20that%20directly%20generates%20rigged%20meshes%20from%20user%20inputs%3A%202D%20drawn%20strokes%20and%20a%20descriptive%20text%20prompt.%20Our%20approach%20pioneers%20a%20two-stage%20pipeline%20that%20separates%20the%20generation%20into%3A%201%29%20Controllable%20Skeleton%20Generation%2C%20we%20employ%20the%20Skeletal%20Graph%20VAE%20%28Sk-VAE%29%20to%20encode%20the%20skeleton%27s%20graph%20structure%20into%20a%20latent%20space%2C%20where%20the%20Skeletal%20Graph%20DiT%20%28Sk-DiT%29%20generates%20a%20skeletal%20embedding.%20The%20generation%20process%20is%20conditioned%20on%20both%20the%20text%20for%20semantics%20and%20the%202D%20strokes%20for%20explicit%20structural%20control%2C%20with%20the%20VAE%27s%20decoder%20reconstructing%20the%20final%20high-quality%203D%20skeleton%3B%20and%202%29%20Enhanced%20Mesh%20Synthesis%20via%20TextuRig%20and%20SKA-DPO%2C%20where%20we%20then%20synthesize%20a%20textured%20mesh%20conditioned%20on%20the%20generated%20skeleton.%20For%20this%20stage%2C%20we%20first%20enhance%20an%20existing%20skeleton-to-mesh%20model%20by%20augmenting%20its%20training%20data%20with%20TextuRig%3A%20a%20dataset%20of%20textured%20and%20rigged%20meshes%20with%20captions%2C%20curated%20from%20Objaverse-XL.%20Additionally%2C%20we%20employ%20a%20preference%20optimization%20strategy%2C%20SKA-DPO%2C%20guided%20by%20a%20skeleton-mesh%20alignment%20score%2C%20to%20further%20improve%20geometric%20fidelity.%20Together%2C%20our%20framework%20enables%20a%20more%20intuitive%20workflow%20for%20creating%20ready%20to%20animate%203D%20content.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20generate%20rigged%203D%20meshes%20conditioned%20on%20user-drawn%202D%20strokes.%20Extensive%20experiments%20demonstrate%20that%20Stroke3D%20produces%20plausible%20skeletons%20and%20high-quality%20meshes.&entry.1838667208=http%3A//arxiv.org/abs/2602.09713v1&entry.124074799=Read"},
{"title": "Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence", "author": "Xiaoyue Ling and Chuqin Zhou and Chunyi Li and Yunuo Chen and Yuan Tian and Guo Lu and Wenjun Zhang", "abstract": "Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.", "link": "http://arxiv.org/abs/2602.09868v1", "date": "2026-02-10", "relevancy": 2.9611, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5862}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-GVC%3A%20Towards%20Training-Free%20Extreme%20Generative%20Video%20Compression%20with%20Temporal%20Coherence&body=Title%3A%20Free-GVC%3A%20Towards%20Training-Free%20Extreme%20Generative%20Video%20Compression%20with%20Temporal%20Coherence%0AAuthor%3A%20Xiaoyue%20Ling%20and%20Chuqin%20Zhou%20and%20Chunyi%20Li%20and%20Yunuo%20Chen%20and%20Yuan%20Tian%20and%20Guo%20Lu%20and%20Wenjun%20Zhang%0AAbstract%3A%20Building%20on%20recent%20advances%20in%20video%20generation%2C%20generative%20video%20compression%20has%20emerged%20as%20a%20new%20paradigm%20for%20achieving%20visually%20pleasing%20reconstructions.%20However%2C%20existing%20methods%20exhibit%20limited%20exploitation%20of%20temporal%20correlations%2C%20causing%20noticeable%20flicker%20and%20degraded%20temporal%20coherence%20at%20ultra-low%20bitrates.%20In%20this%20paper%2C%20we%20propose%20Free-GVC%2C%20a%20training-free%20generative%20video%20compression%20framework%20that%20reformulates%20video%20coding%20as%20latent%20trajectory%20compression%20guided%20by%20a%20video%20diffusion%20prior.%20Our%20method%20operates%20at%20the%20group-of-pictures%20%28GOP%29%20level%2C%20encoding%20video%20segments%20into%20a%20compact%20latent%20space%20and%20progressively%20compressing%20them%20along%20the%20diffusion%20trajectory.%20To%20ensure%20perceptually%20consistent%20reconstruction%20across%20GOPs%2C%20we%20introduce%20an%20Adaptive%20Quality%20Control%20module%20that%20dynamically%20constructs%20an%20online%20rate-perception%20surrogate%20model%20to%20predict%20the%20optimal%20diffusion%20step%20for%20each%20GOP.%20In%20addition%2C%20an%20Inter-GOP%20Alignment%20module%20establishes%20frame%20overlap%20and%20performs%20latent%20fusion%20between%20adjacent%20groups%2C%20thereby%20mitigating%20flicker%20and%20enhancing%20temporal%20coherence.%20Experiments%20show%20that%20Free-GVC%20achieves%20an%20average%20of%2093.29%25%20BD-Rate%20reduction%20in%20DISTS%20over%20the%20latest%20neural%20codec%20DCVC-RT%2C%20and%20a%20user%20study%20further%20confirms%20its%20superior%20perceptual%20quality%20and%20temporal%20coherence%20at%20ultra-low%20bitrates.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-GVC%253A%2520Towards%2520Training-Free%2520Extreme%2520Generative%2520Video%2520Compression%2520with%2520Temporal%2520Coherence%26entry.906535625%3DXiaoyue%2520Ling%2520and%2520Chuqin%2520Zhou%2520and%2520Chunyi%2520Li%2520and%2520Yunuo%2520Chen%2520and%2520Yuan%2520Tian%2520and%2520Guo%2520Lu%2520and%2520Wenjun%2520Zhang%26entry.1292438233%3DBuilding%2520on%2520recent%2520advances%2520in%2520video%2520generation%252C%2520generative%2520video%2520compression%2520has%2520emerged%2520as%2520a%2520new%2520paradigm%2520for%2520achieving%2520visually%2520pleasing%2520reconstructions.%2520However%252C%2520existing%2520methods%2520exhibit%2520limited%2520exploitation%2520of%2520temporal%2520correlations%252C%2520causing%2520noticeable%2520flicker%2520and%2520degraded%2520temporal%2520coherence%2520at%2520ultra-low%2520bitrates.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Free-GVC%252C%2520a%2520training-free%2520generative%2520video%2520compression%2520framework%2520that%2520reformulates%2520video%2520coding%2520as%2520latent%2520trajectory%2520compression%2520guided%2520by%2520a%2520video%2520diffusion%2520prior.%2520Our%2520method%2520operates%2520at%2520the%2520group-of-pictures%2520%2528GOP%2529%2520level%252C%2520encoding%2520video%2520segments%2520into%2520a%2520compact%2520latent%2520space%2520and%2520progressively%2520compressing%2520them%2520along%2520the%2520diffusion%2520trajectory.%2520To%2520ensure%2520perceptually%2520consistent%2520reconstruction%2520across%2520GOPs%252C%2520we%2520introduce%2520an%2520Adaptive%2520Quality%2520Control%2520module%2520that%2520dynamically%2520constructs%2520an%2520online%2520rate-perception%2520surrogate%2520model%2520to%2520predict%2520the%2520optimal%2520diffusion%2520step%2520for%2520each%2520GOP.%2520In%2520addition%252C%2520an%2520Inter-GOP%2520Alignment%2520module%2520establishes%2520frame%2520overlap%2520and%2520performs%2520latent%2520fusion%2520between%2520adjacent%2520groups%252C%2520thereby%2520mitigating%2520flicker%2520and%2520enhancing%2520temporal%2520coherence.%2520Experiments%2520show%2520that%2520Free-GVC%2520achieves%2520an%2520average%2520of%252093.29%2525%2520BD-Rate%2520reduction%2520in%2520DISTS%2520over%2520the%2520latest%2520neural%2520codec%2520DCVC-RT%252C%2520and%2520a%2520user%2520study%2520further%2520confirms%2520its%2520superior%2520perceptual%2520quality%2520and%2520temporal%2520coherence%2520at%2520ultra-low%2520bitrates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-GVC%3A%20Towards%20Training-Free%20Extreme%20Generative%20Video%20Compression%20with%20Temporal%20Coherence&entry.906535625=Xiaoyue%20Ling%20and%20Chuqin%20Zhou%20and%20Chunyi%20Li%20and%20Yunuo%20Chen%20and%20Yuan%20Tian%20and%20Guo%20Lu%20and%20Wenjun%20Zhang&entry.1292438233=Building%20on%20recent%20advances%20in%20video%20generation%2C%20generative%20video%20compression%20has%20emerged%20as%20a%20new%20paradigm%20for%20achieving%20visually%20pleasing%20reconstructions.%20However%2C%20existing%20methods%20exhibit%20limited%20exploitation%20of%20temporal%20correlations%2C%20causing%20noticeable%20flicker%20and%20degraded%20temporal%20coherence%20at%20ultra-low%20bitrates.%20In%20this%20paper%2C%20we%20propose%20Free-GVC%2C%20a%20training-free%20generative%20video%20compression%20framework%20that%20reformulates%20video%20coding%20as%20latent%20trajectory%20compression%20guided%20by%20a%20video%20diffusion%20prior.%20Our%20method%20operates%20at%20the%20group-of-pictures%20%28GOP%29%20level%2C%20encoding%20video%20segments%20into%20a%20compact%20latent%20space%20and%20progressively%20compressing%20them%20along%20the%20diffusion%20trajectory.%20To%20ensure%20perceptually%20consistent%20reconstruction%20across%20GOPs%2C%20we%20introduce%20an%20Adaptive%20Quality%20Control%20module%20that%20dynamically%20constructs%20an%20online%20rate-perception%20surrogate%20model%20to%20predict%20the%20optimal%20diffusion%20step%20for%20each%20GOP.%20In%20addition%2C%20an%20Inter-GOP%20Alignment%20module%20establishes%20frame%20overlap%20and%20performs%20latent%20fusion%20between%20adjacent%20groups%2C%20thereby%20mitigating%20flicker%20and%20enhancing%20temporal%20coherence.%20Experiments%20show%20that%20Free-GVC%20achieves%20an%20average%20of%2093.29%25%20BD-Rate%20reduction%20in%20DISTS%20over%20the%20latest%20neural%20codec%20DCVC-RT%2C%20and%20a%20user%20study%20further%20confirms%20its%20superior%20perceptual%20quality%20and%20temporal%20coherence%20at%20ultra-low%20bitrates.&entry.1838667208=http%3A//arxiv.org/abs/2602.09868v1&entry.124074799=Read"},
{"title": "SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models", "author": "Gulraiz Khan and Kenneth Y. Wertheim and Kevin Pimbblet and Waqas Ahmed", "abstract": "Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.", "link": "http://arxiv.org/abs/2602.09918v1", "date": "2026-02-10", "relevancy": 2.9253, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6069}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5914}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARS%3A%20A%20Novel%20Face%20and%20Body%20Shape%20and%20Appearance%20Aware%203D%20Reconstruction%20System%20extends%20Morphable%20Models&body=Title%3A%20SARS%3A%20A%20Novel%20Face%20and%20Body%20Shape%20and%20Appearance%20Aware%203D%20Reconstruction%20System%20extends%20Morphable%20Models%0AAuthor%3A%20Gulraiz%20Khan%20and%20Kenneth%20Y.%20Wertheim%20and%20Kevin%20Pimbblet%20and%20Waqas%20Ahmed%0AAbstract%3A%20Morphable%20Models%20%283DMMs%29%20are%20a%20type%20of%20morphable%20model%20that%20takes%202D%20images%20as%20inputs%20and%20recreates%20the%20structure%20and%20physical%20appearance%20of%203D%20objects%2C%20especially%20human%20faces%20and%20bodies.%203DMM%20combines%20identity%20and%20expression%20blendshapes%20with%20a%20basic%20face%20mesh%20to%20create%20a%20detailed%203D%20model.%20The%20variability%20in%20the%203D%20Morphable%20models%20can%20be%20controlled%20by%20tuning%20diverse%20parameters.%20They%20are%20high-level%20image%20descriptors%2C%20such%20as%20shape%2C%20texture%2C%20illumination%2C%20and%20camera%20parameters.%20Previous%20research%20in%203D%20human%20reconstruction%20concentrated%20solely%20on%20global%20face%20structure%20or%20geometry%2C%20ignoring%20face%20semantic%20features%20such%20as%20age%2C%20gender%2C%20and%20facial%20landmarks%20characterizing%20facial%20boundaries%2C%20curves%2C%20dips%2C%20and%20wrinkles.%20In%20order%20to%20accommodate%20changes%20in%20these%20high-level%20facial%20characteristics%2C%20this%20work%20introduces%20a%20shape%20and%20appearance-aware%203D%20reconstruction%20system%20%28named%20SARS%20by%20us%29%2C%20a%20c%20modular%20pipeline%20that%20extracts%20body%20and%20face%20information%20from%20a%20single%20image%20to%20properly%20rebuild%20the%203D%20model%20of%20the%20human%20full%20body.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARS%253A%2520A%2520Novel%2520Face%2520and%2520Body%2520Shape%2520and%2520Appearance%2520Aware%25203D%2520Reconstruction%2520System%2520extends%2520Morphable%2520Models%26entry.906535625%3DGulraiz%2520Khan%2520and%2520Kenneth%2520Y.%2520Wertheim%2520and%2520Kevin%2520Pimbblet%2520and%2520Waqas%2520Ahmed%26entry.1292438233%3DMorphable%2520Models%2520%25283DMMs%2529%2520are%2520a%2520type%2520of%2520morphable%2520model%2520that%2520takes%25202D%2520images%2520as%2520inputs%2520and%2520recreates%2520the%2520structure%2520and%2520physical%2520appearance%2520of%25203D%2520objects%252C%2520especially%2520human%2520faces%2520and%2520bodies.%25203DMM%2520combines%2520identity%2520and%2520expression%2520blendshapes%2520with%2520a%2520basic%2520face%2520mesh%2520to%2520create%2520a%2520detailed%25203D%2520model.%2520The%2520variability%2520in%2520the%25203D%2520Morphable%2520models%2520can%2520be%2520controlled%2520by%2520tuning%2520diverse%2520parameters.%2520They%2520are%2520high-level%2520image%2520descriptors%252C%2520such%2520as%2520shape%252C%2520texture%252C%2520illumination%252C%2520and%2520camera%2520parameters.%2520Previous%2520research%2520in%25203D%2520human%2520reconstruction%2520concentrated%2520solely%2520on%2520global%2520face%2520structure%2520or%2520geometry%252C%2520ignoring%2520face%2520semantic%2520features%2520such%2520as%2520age%252C%2520gender%252C%2520and%2520facial%2520landmarks%2520characterizing%2520facial%2520boundaries%252C%2520curves%252C%2520dips%252C%2520and%2520wrinkles.%2520In%2520order%2520to%2520accommodate%2520changes%2520in%2520these%2520high-level%2520facial%2520characteristics%252C%2520this%2520work%2520introduces%2520a%2520shape%2520and%2520appearance-aware%25203D%2520reconstruction%2520system%2520%2528named%2520SARS%2520by%2520us%2529%252C%2520a%2520c%2520modular%2520pipeline%2520that%2520extracts%2520body%2520and%2520face%2520information%2520from%2520a%2520single%2520image%2520to%2520properly%2520rebuild%2520the%25203D%2520model%2520of%2520the%2520human%2520full%2520body.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARS%3A%20A%20Novel%20Face%20and%20Body%20Shape%20and%20Appearance%20Aware%203D%20Reconstruction%20System%20extends%20Morphable%20Models&entry.906535625=Gulraiz%20Khan%20and%20Kenneth%20Y.%20Wertheim%20and%20Kevin%20Pimbblet%20and%20Waqas%20Ahmed&entry.1292438233=Morphable%20Models%20%283DMMs%29%20are%20a%20type%20of%20morphable%20model%20that%20takes%202D%20images%20as%20inputs%20and%20recreates%20the%20structure%20and%20physical%20appearance%20of%203D%20objects%2C%20especially%20human%20faces%20and%20bodies.%203DMM%20combines%20identity%20and%20expression%20blendshapes%20with%20a%20basic%20face%20mesh%20to%20create%20a%20detailed%203D%20model.%20The%20variability%20in%20the%203D%20Morphable%20models%20can%20be%20controlled%20by%20tuning%20diverse%20parameters.%20They%20are%20high-level%20image%20descriptors%2C%20such%20as%20shape%2C%20texture%2C%20illumination%2C%20and%20camera%20parameters.%20Previous%20research%20in%203D%20human%20reconstruction%20concentrated%20solely%20on%20global%20face%20structure%20or%20geometry%2C%20ignoring%20face%20semantic%20features%20such%20as%20age%2C%20gender%2C%20and%20facial%20landmarks%20characterizing%20facial%20boundaries%2C%20curves%2C%20dips%2C%20and%20wrinkles.%20In%20order%20to%20accommodate%20changes%20in%20these%20high-level%20facial%20characteristics%2C%20this%20work%20introduces%20a%20shape%20and%20appearance-aware%203D%20reconstruction%20system%20%28named%20SARS%20by%20us%29%2C%20a%20c%20modular%20pipeline%20that%20extracts%20body%20and%20face%20information%20from%20a%20single%20image%20to%20properly%20rebuild%20the%203D%20model%20of%20the%20human%20full%20body.&entry.1838667208=http%3A//arxiv.org/abs/2602.09918v1&entry.124074799=Read"},
{"title": "Unified Personalized Reward Model for Vision Generation", "author": "Yibin Wang and Yuhang Zang and Feng Han and Jiazi Bu and Yujie Zhou and Cheng Jin and Jiaqi Wang", "abstract": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.", "link": "http://arxiv.org/abs/2602.02380v2", "date": "2026-02-10", "relevancy": 2.8917, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5821}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5774}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Personalized%20Reward%20Model%20for%20Vision%20Generation&body=Title%3A%20Unified%20Personalized%20Reward%20Model%20for%20Vision%20Generation%0AAuthor%3A%20Yibin%20Wang%20and%20Yuhang%20Zang%20and%20Feng%20Han%20and%20Jiazi%20Bu%20and%20Yujie%20Zhou%20and%20Cheng%20Jin%20and%20Jiaqi%20Wang%0AAbstract%3A%20Recent%20advancements%20in%20multimodal%20reward%20models%20%28RMs%29%20have%20significantly%20propelled%20the%20development%20of%20visual%20generation.%20Existing%20frameworks%20typically%20adopt%20Bradley-Terry-style%20preference%20modeling%20or%20leverage%20generative%20VLMs%20as%20judges%2C%20and%20subsequently%20optimize%20visual%20generation%20models%20via%20reinforcement%20learning.%20However%2C%20current%20RMs%20suffer%20from%20inherent%20limitations%3A%20they%20often%20follow%20a%20one-size-fits-all%20paradigm%20that%20assumes%20a%20monolithic%20preference%20distribution%20or%20relies%20on%20fixed%20evaluation%20rubrics.%20As%20a%20result%2C%20they%20are%20insensitive%20to%20content-specific%20visual%20cues%2C%20leading%20to%20systematic%20misalignment%20with%20subjective%20and%20context-dependent%20human%20preferences.%20To%20this%20end%2C%20inspired%20by%20human%20assessment%2C%20we%20propose%20UnifiedReward-Flex%2C%20a%20unified%20personalized%20reward%20model%20for%20vision%20generation%20that%20couples%20reward%20modeling%20with%20flexible%20and%20context-adaptive%20reasoning.%20Specifically%2C%20given%20a%20prompt%20and%20the%20generated%20visual%20content%2C%20it%20first%20interprets%20the%20semantic%20intent%20and%20grounds%20on%20visual%20evidence%2C%20then%20dynamically%20constructs%20a%20hierarchical%20assessment%20by%20instantiating%20fine-grained%20criteria%20under%20both%20predefined%20and%20self-generated%20high-level%20dimensions.%20Our%20training%20pipeline%20follows%20a%20two-stage%20process%3A%20%281%29%20we%20first%20distill%20structured%2C%20high-quality%20reasoning%20traces%20from%20advanced%20closed-source%20VLMs%20to%20bootstrap%20SFT%2C%20equipping%20the%20model%20with%20flexible%20and%20context-adaptive%20reasoning%20behaviors%3B%20%282%29%20we%20then%20perform%20direct%20preference%20optimization%20%28DPO%29%20on%20carefully%20curated%20preference%20pairs%20to%20further%20strengthen%20reasoning%20fidelity%20and%20discriminative%20alignment.%20To%20validate%20the%20effectiveness%2C%20we%20integrate%20UnifiedReward-Flex%20into%20the%20GRPO%20framework%20for%20image%20and%20video%20synthesis%2C%20and%20extensive%20results%20demonstrate%20its%20superiority.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Personalized%2520Reward%2520Model%2520for%2520Vision%2520Generation%26entry.906535625%3DYibin%2520Wang%2520and%2520Yuhang%2520Zang%2520and%2520Feng%2520Han%2520and%2520Jiazi%2520Bu%2520and%2520Yujie%2520Zhou%2520and%2520Cheng%2520Jin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DRecent%2520advancements%2520in%2520multimodal%2520reward%2520models%2520%2528RMs%2529%2520have%2520significantly%2520propelled%2520the%2520development%2520of%2520visual%2520generation.%2520Existing%2520frameworks%2520typically%2520adopt%2520Bradley-Terry-style%2520preference%2520modeling%2520or%2520leverage%2520generative%2520VLMs%2520as%2520judges%252C%2520and%2520subsequently%2520optimize%2520visual%2520generation%2520models%2520via%2520reinforcement%2520learning.%2520However%252C%2520current%2520RMs%2520suffer%2520from%2520inherent%2520limitations%253A%2520they%2520often%2520follow%2520a%2520one-size-fits-all%2520paradigm%2520that%2520assumes%2520a%2520monolithic%2520preference%2520distribution%2520or%2520relies%2520on%2520fixed%2520evaluation%2520rubrics.%2520As%2520a%2520result%252C%2520they%2520are%2520insensitive%2520to%2520content-specific%2520visual%2520cues%252C%2520leading%2520to%2520systematic%2520misalignment%2520with%2520subjective%2520and%2520context-dependent%2520human%2520preferences.%2520To%2520this%2520end%252C%2520inspired%2520by%2520human%2520assessment%252C%2520we%2520propose%2520UnifiedReward-Flex%252C%2520a%2520unified%2520personalized%2520reward%2520model%2520for%2520vision%2520generation%2520that%2520couples%2520reward%2520modeling%2520with%2520flexible%2520and%2520context-adaptive%2520reasoning.%2520Specifically%252C%2520given%2520a%2520prompt%2520and%2520the%2520generated%2520visual%2520content%252C%2520it%2520first%2520interprets%2520the%2520semantic%2520intent%2520and%2520grounds%2520on%2520visual%2520evidence%252C%2520then%2520dynamically%2520constructs%2520a%2520hierarchical%2520assessment%2520by%2520instantiating%2520fine-grained%2520criteria%2520under%2520both%2520predefined%2520and%2520self-generated%2520high-level%2520dimensions.%2520Our%2520training%2520pipeline%2520follows%2520a%2520two-stage%2520process%253A%2520%25281%2529%2520we%2520first%2520distill%2520structured%252C%2520high-quality%2520reasoning%2520traces%2520from%2520advanced%2520closed-source%2520VLMs%2520to%2520bootstrap%2520SFT%252C%2520equipping%2520the%2520model%2520with%2520flexible%2520and%2520context-adaptive%2520reasoning%2520behaviors%253B%2520%25282%2529%2520we%2520then%2520perform%2520direct%2520preference%2520optimization%2520%2528DPO%2529%2520on%2520carefully%2520curated%2520preference%2520pairs%2520to%2520further%2520strengthen%2520reasoning%2520fidelity%2520and%2520discriminative%2520alignment.%2520To%2520validate%2520the%2520effectiveness%252C%2520we%2520integrate%2520UnifiedReward-Flex%2520into%2520the%2520GRPO%2520framework%2520for%2520image%2520and%2520video%2520synthesis%252C%2520and%2520extensive%2520results%2520demonstrate%2520its%2520superiority.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Personalized%20Reward%20Model%20for%20Vision%20Generation&entry.906535625=Yibin%20Wang%20and%20Yuhang%20Zang%20and%20Feng%20Han%20and%20Jiazi%20Bu%20and%20Yujie%20Zhou%20and%20Cheng%20Jin%20and%20Jiaqi%20Wang&entry.1292438233=Recent%20advancements%20in%20multimodal%20reward%20models%20%28RMs%29%20have%20significantly%20propelled%20the%20development%20of%20visual%20generation.%20Existing%20frameworks%20typically%20adopt%20Bradley-Terry-style%20preference%20modeling%20or%20leverage%20generative%20VLMs%20as%20judges%2C%20and%20subsequently%20optimize%20visual%20generation%20models%20via%20reinforcement%20learning.%20However%2C%20current%20RMs%20suffer%20from%20inherent%20limitations%3A%20they%20often%20follow%20a%20one-size-fits-all%20paradigm%20that%20assumes%20a%20monolithic%20preference%20distribution%20or%20relies%20on%20fixed%20evaluation%20rubrics.%20As%20a%20result%2C%20they%20are%20insensitive%20to%20content-specific%20visual%20cues%2C%20leading%20to%20systematic%20misalignment%20with%20subjective%20and%20context-dependent%20human%20preferences.%20To%20this%20end%2C%20inspired%20by%20human%20assessment%2C%20we%20propose%20UnifiedReward-Flex%2C%20a%20unified%20personalized%20reward%20model%20for%20vision%20generation%20that%20couples%20reward%20modeling%20with%20flexible%20and%20context-adaptive%20reasoning.%20Specifically%2C%20given%20a%20prompt%20and%20the%20generated%20visual%20content%2C%20it%20first%20interprets%20the%20semantic%20intent%20and%20grounds%20on%20visual%20evidence%2C%20then%20dynamically%20constructs%20a%20hierarchical%20assessment%20by%20instantiating%20fine-grained%20criteria%20under%20both%20predefined%20and%20self-generated%20high-level%20dimensions.%20Our%20training%20pipeline%20follows%20a%20two-stage%20process%3A%20%281%29%20we%20first%20distill%20structured%2C%20high-quality%20reasoning%20traces%20from%20advanced%20closed-source%20VLMs%20to%20bootstrap%20SFT%2C%20equipping%20the%20model%20with%20flexible%20and%20context-adaptive%20reasoning%20behaviors%3B%20%282%29%20we%20then%20perform%20direct%20preference%20optimization%20%28DPO%29%20on%20carefully%20curated%20preference%20pairs%20to%20further%20strengthen%20reasoning%20fidelity%20and%20discriminative%20alignment.%20To%20validate%20the%20effectiveness%2C%20we%20integrate%20UnifiedReward-Flex%20into%20the%20GRPO%20framework%20for%20image%20and%20video%20synthesis%2C%20and%20extensive%20results%20demonstrate%20its%20superiority.&entry.1838667208=http%3A//arxiv.org/abs/2602.02380v2&entry.124074799=Read"},
{"title": "Kelix Technique Report", "author": "Boyang Ding and Chenglong Chu and Dunju Zang and Han Li and Jiangxia Cao and Kun Gai and Muhao Wei and Ruiming Tang and Shiyao Wang and Siyang Mao and Xinchen Luo and Yahui Liu and Zhixin Ling and Zhuoran Yang and Ziming Li and Chengru Song and Guorui Zhou and Guowang Zhang and Hao Peng and Hao Wang and Jiaxin Deng and Jin Ouyang and Jinghao Zhang and Lejian Ren and Qianqian Wang and Qigen Hu and Tao Wang and Xingmei Wang and Yiping Yang and Zixing Zhang and Ziqi Wang", "abstract": "Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.", "link": "http://arxiv.org/abs/2602.09843v1", "date": "2026-02-10", "relevancy": 2.8902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kelix%20Technique%20Report&body=Title%3A%20Kelix%20Technique%20Report%0AAuthor%3A%20Boyang%20Ding%20and%20Chenglong%20Chu%20and%20Dunju%20Zang%20and%20Han%20Li%20and%20Jiangxia%20Cao%20and%20Kun%20Gai%20and%20Muhao%20Wei%20and%20Ruiming%20Tang%20and%20Shiyao%20Wang%20and%20Siyang%20Mao%20and%20Xinchen%20Luo%20and%20Yahui%20Liu%20and%20Zhixin%20Ling%20and%20Zhuoran%20Yang%20and%20Ziming%20Li%20and%20Chengru%20Song%20and%20Guorui%20Zhou%20and%20Guowang%20Zhang%20and%20Hao%20Peng%20and%20Hao%20Wang%20and%20Jiaxin%20Deng%20and%20Jin%20Ouyang%20and%20Jinghao%20Zhang%20and%20Lejian%20Ren%20and%20Qianqian%20Wang%20and%20Qigen%20Hu%20and%20Tao%20Wang%20and%20Xingmei%20Wang%20and%20Yiping%20Yang%20and%20Zixing%20Zhang%20and%20Ziqi%20Wang%0AAbstract%3A%20Autoregressive%20large%20language%20models%20%28LLMs%29%20scale%20well%20by%20expressing%20diverse%20tasks%20as%20sequences%20of%20discrete%20natural-language%20tokens%20and%20training%20with%20next-token%20prediction%2C%20which%20unifies%20comprehension%20and%20generation%20under%20self-supervision.%20Extending%20this%20paradigm%20to%20multimodal%20data%20requires%20a%20shared%2C%20discrete%20representation%20across%20modalities.%20However%2C%20most%20vision-language%20models%20%28VLMs%29%20still%20rely%20on%20a%20hybrid%20interface%3A%20discrete%20text%20tokens%20paired%20with%20continuous%20Vision%20Transformer%20%28ViT%29%20features.%20Because%20supervision%20is%20largely%20text-driven%2C%20these%20models%20are%20often%20biased%20toward%20understanding%20and%20cannot%20fully%20leverage%20large-scale%20self-supervised%20learning%20on%20non-text%20data.%20Recent%20work%20has%20explored%20discrete%20visual%20tokenization%20to%20enable%20fully%20autoregressive%20multimodal%20modeling%2C%20showing%20promising%20progress%20toward%20unified%20understanding%20and%20generation.%20Yet%20existing%20discrete%20vision%20tokens%20frequently%20lose%20information%20due%20to%20limited%20code%20capacity%2C%20resulting%20in%20noticeably%20weaker%20understanding%20than%20continuous-feature%20VLMs.%20We%20present%20Kelix%2C%20a%20fully%20discrete%20autoregressive%20unified%20model%20that%20closes%20the%20understanding%20gap%20between%20discrete%20and%20continuous%20visual%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKelix%2520Technique%2520Report%26entry.906535625%3DBoyang%2520Ding%2520and%2520Chenglong%2520Chu%2520and%2520Dunju%2520Zang%2520and%2520Han%2520Li%2520and%2520Jiangxia%2520Cao%2520and%2520Kun%2520Gai%2520and%2520Muhao%2520Wei%2520and%2520Ruiming%2520Tang%2520and%2520Shiyao%2520Wang%2520and%2520Siyang%2520Mao%2520and%2520Xinchen%2520Luo%2520and%2520Yahui%2520Liu%2520and%2520Zhixin%2520Ling%2520and%2520Zhuoran%2520Yang%2520and%2520Ziming%2520Li%2520and%2520Chengru%2520Song%2520and%2520Guorui%2520Zhou%2520and%2520Guowang%2520Zhang%2520and%2520Hao%2520Peng%2520and%2520Hao%2520Wang%2520and%2520Jiaxin%2520Deng%2520and%2520Jin%2520Ouyang%2520and%2520Jinghao%2520Zhang%2520and%2520Lejian%2520Ren%2520and%2520Qianqian%2520Wang%2520and%2520Qigen%2520Hu%2520and%2520Tao%2520Wang%2520and%2520Xingmei%2520Wang%2520and%2520Yiping%2520Yang%2520and%2520Zixing%2520Zhang%2520and%2520Ziqi%2520Wang%26entry.1292438233%3DAutoregressive%2520large%2520language%2520models%2520%2528LLMs%2529%2520scale%2520well%2520by%2520expressing%2520diverse%2520tasks%2520as%2520sequences%2520of%2520discrete%2520natural-language%2520tokens%2520and%2520training%2520with%2520next-token%2520prediction%252C%2520which%2520unifies%2520comprehension%2520and%2520generation%2520under%2520self-supervision.%2520Extending%2520this%2520paradigm%2520to%2520multimodal%2520data%2520requires%2520a%2520shared%252C%2520discrete%2520representation%2520across%2520modalities.%2520However%252C%2520most%2520vision-language%2520models%2520%2528VLMs%2529%2520still%2520rely%2520on%2520a%2520hybrid%2520interface%253A%2520discrete%2520text%2520tokens%2520paired%2520with%2520continuous%2520Vision%2520Transformer%2520%2528ViT%2529%2520features.%2520Because%2520supervision%2520is%2520largely%2520text-driven%252C%2520these%2520models%2520are%2520often%2520biased%2520toward%2520understanding%2520and%2520cannot%2520fully%2520leverage%2520large-scale%2520self-supervised%2520learning%2520on%2520non-text%2520data.%2520Recent%2520work%2520has%2520explored%2520discrete%2520visual%2520tokenization%2520to%2520enable%2520fully%2520autoregressive%2520multimodal%2520modeling%252C%2520showing%2520promising%2520progress%2520toward%2520unified%2520understanding%2520and%2520generation.%2520Yet%2520existing%2520discrete%2520vision%2520tokens%2520frequently%2520lose%2520information%2520due%2520to%2520limited%2520code%2520capacity%252C%2520resulting%2520in%2520noticeably%2520weaker%2520understanding%2520than%2520continuous-feature%2520VLMs.%2520We%2520present%2520Kelix%252C%2520a%2520fully%2520discrete%2520autoregressive%2520unified%2520model%2520that%2520closes%2520the%2520understanding%2520gap%2520between%2520discrete%2520and%2520continuous%2520visual%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kelix%20Technique%20Report&entry.906535625=Boyang%20Ding%20and%20Chenglong%20Chu%20and%20Dunju%20Zang%20and%20Han%20Li%20and%20Jiangxia%20Cao%20and%20Kun%20Gai%20and%20Muhao%20Wei%20and%20Ruiming%20Tang%20and%20Shiyao%20Wang%20and%20Siyang%20Mao%20and%20Xinchen%20Luo%20and%20Yahui%20Liu%20and%20Zhixin%20Ling%20and%20Zhuoran%20Yang%20and%20Ziming%20Li%20and%20Chengru%20Song%20and%20Guorui%20Zhou%20and%20Guowang%20Zhang%20and%20Hao%20Peng%20and%20Hao%20Wang%20and%20Jiaxin%20Deng%20and%20Jin%20Ouyang%20and%20Jinghao%20Zhang%20and%20Lejian%20Ren%20and%20Qianqian%20Wang%20and%20Qigen%20Hu%20and%20Tao%20Wang%20and%20Xingmei%20Wang%20and%20Yiping%20Yang%20and%20Zixing%20Zhang%20and%20Ziqi%20Wang&entry.1292438233=Autoregressive%20large%20language%20models%20%28LLMs%29%20scale%20well%20by%20expressing%20diverse%20tasks%20as%20sequences%20of%20discrete%20natural-language%20tokens%20and%20training%20with%20next-token%20prediction%2C%20which%20unifies%20comprehension%20and%20generation%20under%20self-supervision.%20Extending%20this%20paradigm%20to%20multimodal%20data%20requires%20a%20shared%2C%20discrete%20representation%20across%20modalities.%20However%2C%20most%20vision-language%20models%20%28VLMs%29%20still%20rely%20on%20a%20hybrid%20interface%3A%20discrete%20text%20tokens%20paired%20with%20continuous%20Vision%20Transformer%20%28ViT%29%20features.%20Because%20supervision%20is%20largely%20text-driven%2C%20these%20models%20are%20often%20biased%20toward%20understanding%20and%20cannot%20fully%20leverage%20large-scale%20self-supervised%20learning%20on%20non-text%20data.%20Recent%20work%20has%20explored%20discrete%20visual%20tokenization%20to%20enable%20fully%20autoregressive%20multimodal%20modeling%2C%20showing%20promising%20progress%20toward%20unified%20understanding%20and%20generation.%20Yet%20existing%20discrete%20vision%20tokens%20frequently%20lose%20information%20due%20to%20limited%20code%20capacity%2C%20resulting%20in%20noticeably%20weaker%20understanding%20than%20continuous-feature%20VLMs.%20We%20present%20Kelix%2C%20a%20fully%20discrete%20autoregressive%20unified%20model%20that%20closes%20the%20understanding%20gap%20between%20discrete%20and%20continuous%20visual%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2602.09843v1&entry.124074799=Read"},
{"title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models", "author": "Yu Zeng and Wenxuan Huang and Shiting Huang and Xikun Bao and Yukun Qi and Yiming Zhao and Qiuchen Wang and Lin Chen and Zehui Chen and Huaian Chen and Wanli Ouyang and Feng Zhao", "abstract": "Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .", "link": "http://arxiv.org/abs/2510.01304v2", "date": "2026-02-10", "relevancy": 2.8883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5881}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Jigsaw%20Interaction%20Learning%20for%20Enhancing%20Visual%20Perception%20and%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20Agentic%20Jigsaw%20Interaction%20Learning%20for%20Enhancing%20Visual%20Perception%20and%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Yu%20Zeng%20and%20Wenxuan%20Huang%20and%20Shiting%20Huang%20and%20Xikun%20Bao%20and%20Yukun%20Qi%20and%20Yiming%20Zhao%20and%20Qiuchen%20Wang%20and%20Lin%20Chen%20and%20Zehui%20Chen%20and%20Huaian%20Chen%20and%20Wanli%20Ouyang%20and%20Feng%20Zhao%0AAbstract%3A%20Although%20current%20large%20Vision-Language%20Models%20%28VLMs%29%20have%20advanced%20in%20multimodal%20understanding%20and%20reasoning%2C%20their%20fundamental%20perceptual%20and%20reasoning%20abilities%20remain%20limited.%20Specifically%2C%20even%20on%20simple%20jigsaw%20tasks%2C%20existing%20VLMs%20perform%20near%20randomly%2C%20revealing%20deficiencies%20in%20core%20perception%20and%20reasoning%20capabilities.%20While%20high-quality%20vision-language%20data%20can%20enhance%20these%20capabilities%2C%20its%20scarcity%20and%20limited%20scalability%20impose%20significant%20constraints.%20To%20address%20this%2C%20we%20propose%20AGILE%2C%20an%20Agentic%20jiGsaw%20Interaction%20Learning%20for%20Enhancing%20visual%20perception%20and%20reasoning%20in%20VLMs.%20AGILE%20formulates%20jigsaw%20solving%20as%20an%20interactive%20process%2C%20enabling%20the%20model%20to%20progressively%20engage%20with%20the%20environment.%20At%20each%20step%2C%20the%20model%20generates%20executable%20code%20to%20perform%20an%20action%20based%20on%20the%20current%20state%2C%20while%20the%20environment%20provides%20fine-grained%20visual%20feedback%20to%20guide%20task%20completion.%20Through%20this%20iterative%20cycle%20of%20observation%20and%20interaction%2C%20the%20model%20incrementally%20improves%20its%20perceptual%20and%20reasoning%20capabilities%20via%20exploration%20and%20feedback.%20Experimental%20results%20show%20that%20AGILE%20not%20only%20substantially%20boosts%20performance%20on%20jigsaw%20tasks%20of%20varying%20complexity%20%28e.g.%2C%20increasing%20accuracy%20from%209.5%25%20to%2082.8%25%20under%20the%202%20%24%5Ctimes%24%202%20setting%29%20but%20also%20demonstrates%20strong%20generalization%20across%209%20general%20vision%20tasks%2C%20achieving%20an%20average%20improvement%20of%203.1%25.%20These%20results%20indicate%20notable%20enhancements%20in%20both%20perceptual%20and%20reasoning%20abilities.%20This%20work%20opens%20a%20new%20avenue%20for%20advancing%20reasoning%20and%20generalization%20in%20multimodal%20models%20and%20provides%20an%20efficient%2C%20scalable%20solution%20to%20the%20scarcity%20of%20multimodal%20reinforcement%20learning%20data.%20The%20code%20and%20datasets%20is%20available%20at%20https%3A//github.com/yuzeng0-0/AGILE%20.%0ALink%3A%20http%3A//arxiv.org/abs/2510.01304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Jigsaw%2520Interaction%2520Learning%2520for%2520Enhancing%2520Visual%2520Perception%2520and%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYu%2520Zeng%2520and%2520Wenxuan%2520Huang%2520and%2520Shiting%2520Huang%2520and%2520Xikun%2520Bao%2520and%2520Yukun%2520Qi%2520and%2520Yiming%2520Zhao%2520and%2520Qiuchen%2520Wang%2520and%2520Lin%2520Chen%2520and%2520Zehui%2520Chen%2520and%2520Huaian%2520Chen%2520and%2520Wanli%2520Ouyang%2520and%2520Feng%2520Zhao%26entry.1292438233%3DAlthough%2520current%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520advanced%2520in%2520multimodal%2520understanding%2520and%2520reasoning%252C%2520their%2520fundamental%2520perceptual%2520and%2520reasoning%2520abilities%2520remain%2520limited.%2520Specifically%252C%2520even%2520on%2520simple%2520jigsaw%2520tasks%252C%2520existing%2520VLMs%2520perform%2520near%2520randomly%252C%2520revealing%2520deficiencies%2520in%2520core%2520perception%2520and%2520reasoning%2520capabilities.%2520While%2520high-quality%2520vision-language%2520data%2520can%2520enhance%2520these%2520capabilities%252C%2520its%2520scarcity%2520and%2520limited%2520scalability%2520impose%2520significant%2520constraints.%2520To%2520address%2520this%252C%2520we%2520propose%2520AGILE%252C%2520an%2520Agentic%2520jiGsaw%2520Interaction%2520Learning%2520for%2520Enhancing%2520visual%2520perception%2520and%2520reasoning%2520in%2520VLMs.%2520AGILE%2520formulates%2520jigsaw%2520solving%2520as%2520an%2520interactive%2520process%252C%2520enabling%2520the%2520model%2520to%2520progressively%2520engage%2520with%2520the%2520environment.%2520At%2520each%2520step%252C%2520the%2520model%2520generates%2520executable%2520code%2520to%2520perform%2520an%2520action%2520based%2520on%2520the%2520current%2520state%252C%2520while%2520the%2520environment%2520provides%2520fine-grained%2520visual%2520feedback%2520to%2520guide%2520task%2520completion.%2520Through%2520this%2520iterative%2520cycle%2520of%2520observation%2520and%2520interaction%252C%2520the%2520model%2520incrementally%2520improves%2520its%2520perceptual%2520and%2520reasoning%2520capabilities%2520via%2520exploration%2520and%2520feedback.%2520Experimental%2520results%2520show%2520that%2520AGILE%2520not%2520only%2520substantially%2520boosts%2520performance%2520on%2520jigsaw%2520tasks%2520of%2520varying%2520complexity%2520%2528e.g.%252C%2520increasing%2520accuracy%2520from%25209.5%2525%2520to%252082.8%2525%2520under%2520the%25202%2520%2524%255Ctimes%2524%25202%2520setting%2529%2520but%2520also%2520demonstrates%2520strong%2520generalization%2520across%25209%2520general%2520vision%2520tasks%252C%2520achieving%2520an%2520average%2520improvement%2520of%25203.1%2525.%2520These%2520results%2520indicate%2520notable%2520enhancements%2520in%2520both%2520perceptual%2520and%2520reasoning%2520abilities.%2520This%2520work%2520opens%2520a%2520new%2520avenue%2520for%2520advancing%2520reasoning%2520and%2520generalization%2520in%2520multimodal%2520models%2520and%2520provides%2520an%2520efficient%252C%2520scalable%2520solution%2520to%2520the%2520scarcity%2520of%2520multimodal%2520reinforcement%2520learning%2520data.%2520The%2520code%2520and%2520datasets%2520is%2520available%2520at%2520https%253A//github.com/yuzeng0-0/AGILE%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Jigsaw%20Interaction%20Learning%20for%20Enhancing%20Visual%20Perception%20and%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Yu%20Zeng%20and%20Wenxuan%20Huang%20and%20Shiting%20Huang%20and%20Xikun%20Bao%20and%20Yukun%20Qi%20and%20Yiming%20Zhao%20and%20Qiuchen%20Wang%20and%20Lin%20Chen%20and%20Zehui%20Chen%20and%20Huaian%20Chen%20and%20Wanli%20Ouyang%20and%20Feng%20Zhao&entry.1292438233=Although%20current%20large%20Vision-Language%20Models%20%28VLMs%29%20have%20advanced%20in%20multimodal%20understanding%20and%20reasoning%2C%20their%20fundamental%20perceptual%20and%20reasoning%20abilities%20remain%20limited.%20Specifically%2C%20even%20on%20simple%20jigsaw%20tasks%2C%20existing%20VLMs%20perform%20near%20randomly%2C%20revealing%20deficiencies%20in%20core%20perception%20and%20reasoning%20capabilities.%20While%20high-quality%20vision-language%20data%20can%20enhance%20these%20capabilities%2C%20its%20scarcity%20and%20limited%20scalability%20impose%20significant%20constraints.%20To%20address%20this%2C%20we%20propose%20AGILE%2C%20an%20Agentic%20jiGsaw%20Interaction%20Learning%20for%20Enhancing%20visual%20perception%20and%20reasoning%20in%20VLMs.%20AGILE%20formulates%20jigsaw%20solving%20as%20an%20interactive%20process%2C%20enabling%20the%20model%20to%20progressively%20engage%20with%20the%20environment.%20At%20each%20step%2C%20the%20model%20generates%20executable%20code%20to%20perform%20an%20action%20based%20on%20the%20current%20state%2C%20while%20the%20environment%20provides%20fine-grained%20visual%20feedback%20to%20guide%20task%20completion.%20Through%20this%20iterative%20cycle%20of%20observation%20and%20interaction%2C%20the%20model%20incrementally%20improves%20its%20perceptual%20and%20reasoning%20capabilities%20via%20exploration%20and%20feedback.%20Experimental%20results%20show%20that%20AGILE%20not%20only%20substantially%20boosts%20performance%20on%20jigsaw%20tasks%20of%20varying%20complexity%20%28e.g.%2C%20increasing%20accuracy%20from%209.5%25%20to%2082.8%25%20under%20the%202%20%24%5Ctimes%24%202%20setting%29%20but%20also%20demonstrates%20strong%20generalization%20across%209%20general%20vision%20tasks%2C%20achieving%20an%20average%20improvement%20of%203.1%25.%20These%20results%20indicate%20notable%20enhancements%20in%20both%20perceptual%20and%20reasoning%20abilities.%20This%20work%20opens%20a%20new%20avenue%20for%20advancing%20reasoning%20and%20generalization%20in%20multimodal%20models%20and%20provides%20an%20efficient%2C%20scalable%20solution%20to%20the%20scarcity%20of%20multimodal%20reinforcement%20learning%20data.%20The%20code%20and%20datasets%20is%20available%20at%20https%3A//github.com/yuzeng0-0/AGILE%20.&entry.1838667208=http%3A//arxiv.org/abs/2510.01304v2&entry.124074799=Read"},
{"title": "SNAP: Towards Segmenting Anything in Any Point Cloud", "author": "Aniket Gupta and Hanhui Wang and Charles Saunders and Aruni RoyChowdhury and Hanumant Singh and Huaizu Jiang", "abstract": "Interactive 3D point cloud segmentation enables efficient annotation of complex 3D scenes through user-guided prompts. However, current approaches are typically restricted in scope to a single domain (indoor or outdoor), and to a single form of user interaction (either spatial clicks or textual prompts). Moreover, training on multiple datasets often leads to negative transfer, resulting in domain-specific tools that lack generalizability. To address these limitations, we present SNAP (Segment aNything in Any Point cloud), a unified model for interactive 3D segmentation that supports both point-based and text-based prompts across diverse domains. Our approach achieves cross-domain generalizability by training on 7 datasets spanning indoor, outdoor, and aerial environments, while employing domain-adaptive normalization to prevent negative transfer. For text-prompted segmentation, we automatically generate mask proposals without human intervention and match them against CLIP embeddings of textual queries, enabling both panoptic and open-vocabulary segmentation. Extensive experiments demonstrate that SNAP consistently delivers high-quality segmentation results. We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for spatial-prompted segmentation and demonstrate competitive results on all 5 text-prompted benchmarks. These results show that a unified model can match or exceed specialized domain-specific approaches, providing a practical tool for scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/", "link": "http://arxiv.org/abs/2510.11565v2", "date": "2026-02-10", "relevancy": 2.8671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5772}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5716}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SNAP%3A%20Towards%20Segmenting%20Anything%20in%20Any%20Point%20Cloud&body=Title%3A%20SNAP%3A%20Towards%20Segmenting%20Anything%20in%20Any%20Point%20Cloud%0AAuthor%3A%20Aniket%20Gupta%20and%20Hanhui%20Wang%20and%20Charles%20Saunders%20and%20Aruni%20RoyChowdhury%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang%0AAbstract%3A%20Interactive%203D%20point%20cloud%20segmentation%20enables%20efficient%20annotation%20of%20complex%203D%20scenes%20through%20user-guided%20prompts.%20However%2C%20current%20approaches%20are%20typically%20restricted%20in%20scope%20to%20a%20single%20domain%20%28indoor%20or%20outdoor%29%2C%20and%20to%20a%20single%20form%20of%20user%20interaction%20%28either%20spatial%20clicks%20or%20textual%20prompts%29.%20Moreover%2C%20training%20on%20multiple%20datasets%20often%20leads%20to%20negative%20transfer%2C%20resulting%20in%20domain-specific%20tools%20that%20lack%20generalizability.%20To%20address%20these%20limitations%2C%20we%20present%20SNAP%20%28Segment%20aNything%20in%20Any%20Point%20cloud%29%2C%20a%20unified%20model%20for%20interactive%203D%20segmentation%20that%20supports%20both%20point-based%20and%20text-based%20prompts%20across%20diverse%20domains.%20Our%20approach%20achieves%20cross-domain%20generalizability%20by%20training%20on%207%20datasets%20spanning%20indoor%2C%20outdoor%2C%20and%20aerial%20environments%2C%20while%20employing%20domain-adaptive%20normalization%20to%20prevent%20negative%20transfer.%20For%20text-prompted%20segmentation%2C%20we%20automatically%20generate%20mask%20proposals%20without%20human%20intervention%20and%20match%20them%20against%20CLIP%20embeddings%20of%20textual%20queries%2C%20enabling%20both%20panoptic%20and%20open-vocabulary%20segmentation.%20Extensive%20experiments%20demonstrate%20that%20SNAP%20consistently%20delivers%20high-quality%20segmentation%20results.%20We%20achieve%20state-of-the-art%20performance%20on%208%20out%20of%209%20zero-shot%20benchmarks%20for%20spatial-prompted%20segmentation%20and%20demonstrate%20competitive%20results%20on%20all%205%20text-prompted%20benchmarks.%20These%20results%20show%20that%20a%20unified%20model%20can%20match%20or%20exceed%20specialized%20domain-specific%20approaches%2C%20providing%20a%20practical%20tool%20for%20scalable%203D%20annotation.%20Project%20page%20is%20at%2C%20https%3A//neu-vi.github.io/SNAP/%0ALink%3A%20http%3A//arxiv.org/abs/2510.11565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSNAP%253A%2520Towards%2520Segmenting%2520Anything%2520in%2520Any%2520Point%2520Cloud%26entry.906535625%3DAniket%2520Gupta%2520and%2520Hanhui%2520Wang%2520and%2520Charles%2520Saunders%2520and%2520Aruni%2520RoyChowdhury%2520and%2520Hanumant%2520Singh%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3DInteractive%25203D%2520point%2520cloud%2520segmentation%2520enables%2520efficient%2520annotation%2520of%2520complex%25203D%2520scenes%2520through%2520user-guided%2520prompts.%2520However%252C%2520current%2520approaches%2520are%2520typically%2520restricted%2520in%2520scope%2520to%2520a%2520single%2520domain%2520%2528indoor%2520or%2520outdoor%2529%252C%2520and%2520to%2520a%2520single%2520form%2520of%2520user%2520interaction%2520%2528either%2520spatial%2520clicks%2520or%2520textual%2520prompts%2529.%2520Moreover%252C%2520training%2520on%2520multiple%2520datasets%2520often%2520leads%2520to%2520negative%2520transfer%252C%2520resulting%2520in%2520domain-specific%2520tools%2520that%2520lack%2520generalizability.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520SNAP%2520%2528Segment%2520aNything%2520in%2520Any%2520Point%2520cloud%2529%252C%2520a%2520unified%2520model%2520for%2520interactive%25203D%2520segmentation%2520that%2520supports%2520both%2520point-based%2520and%2520text-based%2520prompts%2520across%2520diverse%2520domains.%2520Our%2520approach%2520achieves%2520cross-domain%2520generalizability%2520by%2520training%2520on%25207%2520datasets%2520spanning%2520indoor%252C%2520outdoor%252C%2520and%2520aerial%2520environments%252C%2520while%2520employing%2520domain-adaptive%2520normalization%2520to%2520prevent%2520negative%2520transfer.%2520For%2520text-prompted%2520segmentation%252C%2520we%2520automatically%2520generate%2520mask%2520proposals%2520without%2520human%2520intervention%2520and%2520match%2520them%2520against%2520CLIP%2520embeddings%2520of%2520textual%2520queries%252C%2520enabling%2520both%2520panoptic%2520and%2520open-vocabulary%2520segmentation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SNAP%2520consistently%2520delivers%2520high-quality%2520segmentation%2520results.%2520We%2520achieve%2520state-of-the-art%2520performance%2520on%25208%2520out%2520of%25209%2520zero-shot%2520benchmarks%2520for%2520spatial-prompted%2520segmentation%2520and%2520demonstrate%2520competitive%2520results%2520on%2520all%25205%2520text-prompted%2520benchmarks.%2520These%2520results%2520show%2520that%2520a%2520unified%2520model%2520can%2520match%2520or%2520exceed%2520specialized%2520domain-specific%2520approaches%252C%2520providing%2520a%2520practical%2520tool%2520for%2520scalable%25203D%2520annotation.%2520Project%2520page%2520is%2520at%252C%2520https%253A//neu-vi.github.io/SNAP/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNAP%3A%20Towards%20Segmenting%20Anything%20in%20Any%20Point%20Cloud&entry.906535625=Aniket%20Gupta%20and%20Hanhui%20Wang%20and%20Charles%20Saunders%20and%20Aruni%20RoyChowdhury%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang&entry.1292438233=Interactive%203D%20point%20cloud%20segmentation%20enables%20efficient%20annotation%20of%20complex%203D%20scenes%20through%20user-guided%20prompts.%20However%2C%20current%20approaches%20are%20typically%20restricted%20in%20scope%20to%20a%20single%20domain%20%28indoor%20or%20outdoor%29%2C%20and%20to%20a%20single%20form%20of%20user%20interaction%20%28either%20spatial%20clicks%20or%20textual%20prompts%29.%20Moreover%2C%20training%20on%20multiple%20datasets%20often%20leads%20to%20negative%20transfer%2C%20resulting%20in%20domain-specific%20tools%20that%20lack%20generalizability.%20To%20address%20these%20limitations%2C%20we%20present%20SNAP%20%28Segment%20aNything%20in%20Any%20Point%20cloud%29%2C%20a%20unified%20model%20for%20interactive%203D%20segmentation%20that%20supports%20both%20point-based%20and%20text-based%20prompts%20across%20diverse%20domains.%20Our%20approach%20achieves%20cross-domain%20generalizability%20by%20training%20on%207%20datasets%20spanning%20indoor%2C%20outdoor%2C%20and%20aerial%20environments%2C%20while%20employing%20domain-adaptive%20normalization%20to%20prevent%20negative%20transfer.%20For%20text-prompted%20segmentation%2C%20we%20automatically%20generate%20mask%20proposals%20without%20human%20intervention%20and%20match%20them%20against%20CLIP%20embeddings%20of%20textual%20queries%2C%20enabling%20both%20panoptic%20and%20open-vocabulary%20segmentation.%20Extensive%20experiments%20demonstrate%20that%20SNAP%20consistently%20delivers%20high-quality%20segmentation%20results.%20We%20achieve%20state-of-the-art%20performance%20on%208%20out%20of%209%20zero-shot%20benchmarks%20for%20spatial-prompted%20segmentation%20and%20demonstrate%20competitive%20results%20on%20all%205%20text-prompted%20benchmarks.%20These%20results%20show%20that%20a%20unified%20model%20can%20match%20or%20exceed%20specialized%20domain-specific%20approaches%2C%20providing%20a%20practical%20tool%20for%20scalable%203D%20annotation.%20Project%20page%20is%20at%2C%20https%3A//neu-vi.github.io/SNAP/&entry.1838667208=http%3A//arxiv.org/abs/2510.11565v2&entry.124074799=Read"},
{"title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "author": "Haodong Li and Jingwei Wu and Quan Sun and Guopeng Li and Juanxi Tian and Huanyu Zhang and Yanlin Lai and Ruichuan An and Hongbo Peng and Yuhong Dai and Chenxi Li and Chunmei Qing and Jia Wang and Ziyang Meng and Zheng Ge and Xiangyu Zhang and Daxin Jiang", "abstract": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "link": "http://arxiv.org/abs/2602.09007v2", "date": "2026-02-10", "relevancy": 2.8669, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.593}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5667}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEBench%3A%20Benchmarking%20Image%20Generation%20Models%20as%20GUI%20Environments&body=Title%3A%20GEBench%3A%20Benchmarking%20Image%20Generation%20Models%20as%20GUI%20Environments%0AAuthor%3A%20Haodong%20Li%20and%20Jingwei%20Wu%20and%20Quan%20Sun%20and%20Guopeng%20Li%20and%20Juanxi%20Tian%20and%20Huanyu%20Zhang%20and%20Yanlin%20Lai%20and%20Ruichuan%20An%20and%20Hongbo%20Peng%20and%20Yuhong%20Dai%20and%20Chenxi%20Li%20and%20Chunmei%20Qing%20and%20Jia%20Wang%20and%20Ziyang%20Meng%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%0AAbstract%3A%20Recent%20advancements%20in%20image%20generation%20models%20have%20enabled%20the%20prediction%20of%20future%20Graphical%20User%20Interface%20%28GUI%29%20states%20based%20on%20user%20instructions.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20general%20domain%20visual%20fidelity%2C%20leaving%20the%20evaluation%20of%20state%20transitions%20and%20temporal%20coherence%20in%20GUI-specific%20contexts%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20GEBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20dynamic%20interaction%20and%20temporal%20coherence%20in%20GUI%20generation.%20GEBench%20comprises%20700%20carefully%20curated%20samples%20spanning%20five%20task%20categories%2C%20covering%20both%20single-step%20interactions%20and%20multi-step%20trajectories%20across%20real-world%20and%20fictional%20scenarios%2C%20as%20well%20as%20grounding%20point%20localization.%20To%20support%20systematic%20evaluation%2C%20we%20propose%20GE-Score%2C%20a%20novel%20five-dimensional%20metric%20that%20assesses%20Goal%20Achievement%2C%20Interaction%20Logic%2C%20Content%20Consistency%2C%20UI%20Plausibility%2C%20and%20Visual%20Quality.%20Extensive%20evaluations%20on%20current%20models%20indicate%20that%20while%20they%20perform%20well%20on%20single-step%20transitions%2C%20they%20struggle%20significantly%20with%20maintaining%20temporal%20coherence%20and%20spatial%20grounding%20over%20longer%20interaction%20sequences.%20Our%20findings%20identify%20icon%20interpretation%2C%20text%20rendering%2C%20and%20localization%20precision%20as%20critical%20bottlenecks.%20This%20work%20provides%20a%20foundation%20for%20systematic%20assessment%20and%20suggests%20promising%20directions%20for%20future%20research%20toward%20building%20high-fidelity%20generative%20GUI%20environments.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/stepfun-ai/GEBench.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09007v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEBench%253A%2520Benchmarking%2520Image%2520Generation%2520Models%2520as%2520GUI%2520Environments%26entry.906535625%3DHaodong%2520Li%2520and%2520Jingwei%2520Wu%2520and%2520Quan%2520Sun%2520and%2520Guopeng%2520Li%2520and%2520Juanxi%2520Tian%2520and%2520Huanyu%2520Zhang%2520and%2520Yanlin%2520Lai%2520and%2520Ruichuan%2520An%2520and%2520Hongbo%2520Peng%2520and%2520Yuhong%2520Dai%2520and%2520Chenxi%2520Li%2520and%2520Chunmei%2520Qing%2520and%2520Jia%2520Wang%2520and%2520Ziyang%2520Meng%2520and%2520Zheng%2520Ge%2520and%2520Xiangyu%2520Zhang%2520and%2520Daxin%2520Jiang%26entry.1292438233%3DRecent%2520advancements%2520in%2520image%2520generation%2520models%2520have%2520enabled%2520the%2520prediction%2520of%2520future%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520states%2520based%2520on%2520user%2520instructions.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520general%2520domain%2520visual%2520fidelity%252C%2520leaving%2520the%2520evaluation%2520of%2520state%2520transitions%2520and%2520temporal%2520coherence%2520in%2520GUI-specific%2520contexts%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520GEBench%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520dynamic%2520interaction%2520and%2520temporal%2520coherence%2520in%2520GUI%2520generation.%2520GEBench%2520comprises%2520700%2520carefully%2520curated%2520samples%2520spanning%2520five%2520task%2520categories%252C%2520covering%2520both%2520single-step%2520interactions%2520and%2520multi-step%2520trajectories%2520across%2520real-world%2520and%2520fictional%2520scenarios%252C%2520as%2520well%2520as%2520grounding%2520point%2520localization.%2520To%2520support%2520systematic%2520evaluation%252C%2520we%2520propose%2520GE-Score%252C%2520a%2520novel%2520five-dimensional%2520metric%2520that%2520assesses%2520Goal%2520Achievement%252C%2520Interaction%2520Logic%252C%2520Content%2520Consistency%252C%2520UI%2520Plausibility%252C%2520and%2520Visual%2520Quality.%2520Extensive%2520evaluations%2520on%2520current%2520models%2520indicate%2520that%2520while%2520they%2520perform%2520well%2520on%2520single-step%2520transitions%252C%2520they%2520struggle%2520significantly%2520with%2520maintaining%2520temporal%2520coherence%2520and%2520spatial%2520grounding%2520over%2520longer%2520interaction%2520sequences.%2520Our%2520findings%2520identify%2520icon%2520interpretation%252C%2520text%2520rendering%252C%2520and%2520localization%2520precision%2520as%2520critical%2520bottlenecks.%2520This%2520work%2520provides%2520a%2520foundation%2520for%2520systematic%2520assessment%2520and%2520suggests%2520promising%2520directions%2520for%2520future%2520research%2520toward%2520building%2520high-fidelity%2520generative%2520GUI%2520environments.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/stepfun-ai/GEBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09007v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEBench%3A%20Benchmarking%20Image%20Generation%20Models%20as%20GUI%20Environments&entry.906535625=Haodong%20Li%20and%20Jingwei%20Wu%20and%20Quan%20Sun%20and%20Guopeng%20Li%20and%20Juanxi%20Tian%20and%20Huanyu%20Zhang%20and%20Yanlin%20Lai%20and%20Ruichuan%20An%20and%20Hongbo%20Peng%20and%20Yuhong%20Dai%20and%20Chenxi%20Li%20and%20Chunmei%20Qing%20and%20Jia%20Wang%20and%20Ziyang%20Meng%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang&entry.1292438233=Recent%20advancements%20in%20image%20generation%20models%20have%20enabled%20the%20prediction%20of%20future%20Graphical%20User%20Interface%20%28GUI%29%20states%20based%20on%20user%20instructions.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20general%20domain%20visual%20fidelity%2C%20leaving%20the%20evaluation%20of%20state%20transitions%20and%20temporal%20coherence%20in%20GUI-specific%20contexts%20underexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20GEBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20dynamic%20interaction%20and%20temporal%20coherence%20in%20GUI%20generation.%20GEBench%20comprises%20700%20carefully%20curated%20samples%20spanning%20five%20task%20categories%2C%20covering%20both%20single-step%20interactions%20and%20multi-step%20trajectories%20across%20real-world%20and%20fictional%20scenarios%2C%20as%20well%20as%20grounding%20point%20localization.%20To%20support%20systematic%20evaluation%2C%20we%20propose%20GE-Score%2C%20a%20novel%20five-dimensional%20metric%20that%20assesses%20Goal%20Achievement%2C%20Interaction%20Logic%2C%20Content%20Consistency%2C%20UI%20Plausibility%2C%20and%20Visual%20Quality.%20Extensive%20evaluations%20on%20current%20models%20indicate%20that%20while%20they%20perform%20well%20on%20single-step%20transitions%2C%20they%20struggle%20significantly%20with%20maintaining%20temporal%20coherence%20and%20spatial%20grounding%20over%20longer%20interaction%20sequences.%20Our%20findings%20identify%20icon%20interpretation%2C%20text%20rendering%2C%20and%20localization%20precision%20as%20critical%20bottlenecks.%20This%20work%20provides%20a%20foundation%20for%20systematic%20assessment%20and%20suggests%20promising%20directions%20for%20future%20research%20toward%20building%20high-fidelity%20generative%20GUI%20environments.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/stepfun-ai/GEBench.&entry.1838667208=http%3A//arxiv.org/abs/2602.09007v2&entry.124074799=Read"},
{"title": "Common Objects Out of Context (COOCo): Investigating Multimodal Context and Semantic Scene Violations in Referential Communication", "author": "Filippo Merlo and Ece Takmaz and Wenkai Chen and Albert Gatt", "abstract": "To what degree and under what conditions do VLMs rely on scene context when generating references to objects? To address this question, we introduce the $\\textit{Common Objects Out-of-Context (COOCo)}$ dataset and conduct experiments on several VLMs under different degrees of scene-object congruency and noise. We find that models leverage scene context adaptively, depending on scene-object semantic relatedness and noise level. Based on these consistent trends across models, we turn to the question of how VLM attention patterns change as a function of target-scene semantic fit, and to what degree these patterns are predictive of categorisation accuracy. We find that successful object categorisation is associated with increased mid-layer attention to the target. We also find a non-monotonic dependency on semantic fit, with attention dropping at moderate fit and increasing for both low and high fit. These results suggest that VLMs dynamically balance local and contextual information for reference generation. Dataset and code are available here: $\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}$.", "link": "http://arxiv.org/abs/2506.22274v2", "date": "2026-02-10", "relevancy": 2.8558, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Common%20Objects%20Out%20of%20Context%20%28COOCo%29%3A%20Investigating%20Multimodal%20Context%20and%20Semantic%20Scene%20Violations%20in%20Referential%20Communication&body=Title%3A%20Common%20Objects%20Out%20of%20Context%20%28COOCo%29%3A%20Investigating%20Multimodal%20Context%20and%20Semantic%20Scene%20Violations%20in%20Referential%20Communication%0AAuthor%3A%20Filippo%20Merlo%20and%20Ece%20Takmaz%20and%20Wenkai%20Chen%20and%20Albert%20Gatt%0AAbstract%3A%20To%20what%20degree%20and%20under%20what%20conditions%20do%20VLMs%20rely%20on%20scene%20context%20when%20generating%20references%20to%20objects%3F%20To%20address%20this%20question%2C%20we%20introduce%20the%20%24%5Ctextit%7BCommon%20Objects%20Out-of-Context%20%28COOCo%29%7D%24%20dataset%20and%20conduct%20experiments%20on%20several%20VLMs%20under%20different%20degrees%20of%20scene-object%20congruency%20and%20noise.%20We%20find%20that%20models%20leverage%20scene%20context%20adaptively%2C%20depending%20on%20scene-object%20semantic%20relatedness%20and%20noise%20level.%20Based%20on%20these%20consistent%20trends%20across%20models%2C%20we%20turn%20to%20the%20question%20of%20how%20VLM%20attention%20patterns%20change%20as%20a%20function%20of%20target-scene%20semantic%20fit%2C%20and%20to%20what%20degree%20these%20patterns%20are%20predictive%20of%20categorisation%20accuracy.%20We%20find%20that%20successful%20object%20categorisation%20is%20associated%20with%20increased%20mid-layer%20attention%20to%20the%20target.%20We%20also%20find%20a%20non-monotonic%20dependency%20on%20semantic%20fit%2C%20with%20attention%20dropping%20at%20moderate%20fit%20and%20increasing%20for%20both%20low%20and%20high%20fit.%20These%20results%20suggest%20that%20VLMs%20dynamically%20balance%20local%20and%20contextual%20information%20for%20reference%20generation.%20Dataset%20and%20code%20are%20available%20here%3A%20%24%5Chref%7Bhttps%3A//github.com/cs-nlp-uu/scenereg%7D%7Bhttps%3A//github.com/cs-nlp-uu/scenereg%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2506.22274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommon%2520Objects%2520Out%2520of%2520Context%2520%2528COOCo%2529%253A%2520Investigating%2520Multimodal%2520Context%2520and%2520Semantic%2520Scene%2520Violations%2520in%2520Referential%2520Communication%26entry.906535625%3DFilippo%2520Merlo%2520and%2520Ece%2520Takmaz%2520and%2520Wenkai%2520Chen%2520and%2520Albert%2520Gatt%26entry.1292438233%3DTo%2520what%2520degree%2520and%2520under%2520what%2520conditions%2520do%2520VLMs%2520rely%2520on%2520scene%2520context%2520when%2520generating%2520references%2520to%2520objects%253F%2520To%2520address%2520this%2520question%252C%2520we%2520introduce%2520the%2520%2524%255Ctextit%257BCommon%2520Objects%2520Out-of-Context%2520%2528COOCo%2529%257D%2524%2520dataset%2520and%2520conduct%2520experiments%2520on%2520several%2520VLMs%2520under%2520different%2520degrees%2520of%2520scene-object%2520congruency%2520and%2520noise.%2520We%2520find%2520that%2520models%2520leverage%2520scene%2520context%2520adaptively%252C%2520depending%2520on%2520scene-object%2520semantic%2520relatedness%2520and%2520noise%2520level.%2520Based%2520on%2520these%2520consistent%2520trends%2520across%2520models%252C%2520we%2520turn%2520to%2520the%2520question%2520of%2520how%2520VLM%2520attention%2520patterns%2520change%2520as%2520a%2520function%2520of%2520target-scene%2520semantic%2520fit%252C%2520and%2520to%2520what%2520degree%2520these%2520patterns%2520are%2520predictive%2520of%2520categorisation%2520accuracy.%2520We%2520find%2520that%2520successful%2520object%2520categorisation%2520is%2520associated%2520with%2520increased%2520mid-layer%2520attention%2520to%2520the%2520target.%2520We%2520also%2520find%2520a%2520non-monotonic%2520dependency%2520on%2520semantic%2520fit%252C%2520with%2520attention%2520dropping%2520at%2520moderate%2520fit%2520and%2520increasing%2520for%2520both%2520low%2520and%2520high%2520fit.%2520These%2520results%2520suggest%2520that%2520VLMs%2520dynamically%2520balance%2520local%2520and%2520contextual%2520information%2520for%2520reference%2520generation.%2520Dataset%2520and%2520code%2520are%2520available%2520here%253A%2520%2524%255Chref%257Bhttps%253A//github.com/cs-nlp-uu/scenereg%257D%257Bhttps%253A//github.com/cs-nlp-uu/scenereg%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Common%20Objects%20Out%20of%20Context%20%28COOCo%29%3A%20Investigating%20Multimodal%20Context%20and%20Semantic%20Scene%20Violations%20in%20Referential%20Communication&entry.906535625=Filippo%20Merlo%20and%20Ece%20Takmaz%20and%20Wenkai%20Chen%20and%20Albert%20Gatt&entry.1292438233=To%20what%20degree%20and%20under%20what%20conditions%20do%20VLMs%20rely%20on%20scene%20context%20when%20generating%20references%20to%20objects%3F%20To%20address%20this%20question%2C%20we%20introduce%20the%20%24%5Ctextit%7BCommon%20Objects%20Out-of-Context%20%28COOCo%29%7D%24%20dataset%20and%20conduct%20experiments%20on%20several%20VLMs%20under%20different%20degrees%20of%20scene-object%20congruency%20and%20noise.%20We%20find%20that%20models%20leverage%20scene%20context%20adaptively%2C%20depending%20on%20scene-object%20semantic%20relatedness%20and%20noise%20level.%20Based%20on%20these%20consistent%20trends%20across%20models%2C%20we%20turn%20to%20the%20question%20of%20how%20VLM%20attention%20patterns%20change%20as%20a%20function%20of%20target-scene%20semantic%20fit%2C%20and%20to%20what%20degree%20these%20patterns%20are%20predictive%20of%20categorisation%20accuracy.%20We%20find%20that%20successful%20object%20categorisation%20is%20associated%20with%20increased%20mid-layer%20attention%20to%20the%20target.%20We%20also%20find%20a%20non-monotonic%20dependency%20on%20semantic%20fit%2C%20with%20attention%20dropping%20at%20moderate%20fit%20and%20increasing%20for%20both%20low%20and%20high%20fit.%20These%20results%20suggest%20that%20VLMs%20dynamically%20balance%20local%20and%20contextual%20information%20for%20reference%20generation.%20Dataset%20and%20code%20are%20available%20here%3A%20%24%5Chref%7Bhttps%3A//github.com/cs-nlp-uu/scenereg%7D%7Bhttps%3A//github.com/cs-nlp-uu/scenereg%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2506.22274v2&entry.124074799=Read"},
{"title": "Thinking with Geometry: Active Geometry Integration for Spatial Reasoning", "author": "Haoyuan Li and Qihang Cao and Tao Tang and Kun Xiang and Zihan Guo and Jianhua Han and Hang Xu and Xiaodan Liang", "abstract": "Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.", "link": "http://arxiv.org/abs/2602.06037v2", "date": "2026-02-10", "relevancy": 2.8314, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Geometry%3A%20Active%20Geometry%20Integration%20for%20Spatial%20Reasoning&body=Title%3A%20Thinking%20with%20Geometry%3A%20Active%20Geometry%20Integration%20for%20Spatial%20Reasoning%0AAuthor%3A%20Haoyuan%20Li%20and%20Qihang%20Cao%20and%20Tao%20Tang%20and%20Kun%20Xiang%20and%20Zihan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang%0AAbstract%3A%20Recent%20progress%20in%20spatial%20reasoning%20with%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20increasingly%20leverages%20geometric%20priors%20from%203D%20encoders.%20However%2C%20most%20existing%20integration%20strategies%20remain%20passive%3A%20geometry%20is%20exposed%20as%20a%20global%20stream%20and%20fused%20in%20an%20indiscriminate%20manner%2C%20which%20often%20induces%20semantic-geometry%20misalignment%20and%20redundant%20signals.%20We%20propose%20GeoThinker%2C%20a%20framework%20that%20shifts%20the%20paradigm%20from%20passive%20fusion%20to%20active%20perception.%20Instead%20of%20feature%20mixing%2C%20GeoThinker%20enables%20the%20model%20to%20selectively%20retrieve%20geometric%20evidence%20conditioned%20on%20its%20internal%20reasoning%20demands.%20GeoThinker%20achieves%20this%20through%20Spatial-Grounded%20Fusion%20applied%20at%20carefully%20selected%20VLM%20layers%2C%20where%20semantic%20visual%20priors%20selectively%20query%20and%20integrate%20task-relevant%20geometry%20via%20frame-strict%20cross-attention%2C%20further%20calibrated%20by%20Importance%20Gating%20that%20biases%20per-frame%20attention%20toward%20task-relevant%20structures.%20Comprehensive%20evaluation%20results%20show%20that%20GeoThinker%20sets%20a%20new%20state-of-the-art%20in%20spatial%20intelligence%2C%20achieving%20a%20peak%20score%20of%2072.6%20on%20the%20VSI-Bench.%20Furthermore%2C%20GeoThinker%20demonstrates%20robust%20generalization%20and%20significantly%20improved%20spatial%20perception%20across%20complex%20downstream%20scenarios%2C%20including%20embodied%20referring%20and%20autonomous%20driving.%20Our%20results%20indicate%20that%20the%20ability%20to%20actively%20integrate%20spatial%20structures%20is%20essential%20for%20next-generation%20spatial%20intelligence.%20Code%20can%20be%20found%20at%20https%3A//github.com/Li-Hao-yuan/GeoThinker.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Geometry%253A%2520Active%2520Geometry%2520Integration%2520for%2520Spatial%2520Reasoning%26entry.906535625%3DHaoyuan%2520Li%2520and%2520Qihang%2520Cao%2520and%2520Tao%2520Tang%2520and%2520Kun%2520Xiang%2520and%2520Zihan%2520Guo%2520and%2520Jianhua%2520Han%2520and%2520Hang%2520Xu%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DRecent%2520progress%2520in%2520spatial%2520reasoning%2520with%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520increasingly%2520leverages%2520geometric%2520priors%2520from%25203D%2520encoders.%2520However%252C%2520most%2520existing%2520integration%2520strategies%2520remain%2520passive%253A%2520geometry%2520is%2520exposed%2520as%2520a%2520global%2520stream%2520and%2520fused%2520in%2520an%2520indiscriminate%2520manner%252C%2520which%2520often%2520induces%2520semantic-geometry%2520misalignment%2520and%2520redundant%2520signals.%2520We%2520propose%2520GeoThinker%252C%2520a%2520framework%2520that%2520shifts%2520the%2520paradigm%2520from%2520passive%2520fusion%2520to%2520active%2520perception.%2520Instead%2520of%2520feature%2520mixing%252C%2520GeoThinker%2520enables%2520the%2520model%2520to%2520selectively%2520retrieve%2520geometric%2520evidence%2520conditioned%2520on%2520its%2520internal%2520reasoning%2520demands.%2520GeoThinker%2520achieves%2520this%2520through%2520Spatial-Grounded%2520Fusion%2520applied%2520at%2520carefully%2520selected%2520VLM%2520layers%252C%2520where%2520semantic%2520visual%2520priors%2520selectively%2520query%2520and%2520integrate%2520task-relevant%2520geometry%2520via%2520frame-strict%2520cross-attention%252C%2520further%2520calibrated%2520by%2520Importance%2520Gating%2520that%2520biases%2520per-frame%2520attention%2520toward%2520task-relevant%2520structures.%2520Comprehensive%2520evaluation%2520results%2520show%2520that%2520GeoThinker%2520sets%2520a%2520new%2520state-of-the-art%2520in%2520spatial%2520intelligence%252C%2520achieving%2520a%2520peak%2520score%2520of%252072.6%2520on%2520the%2520VSI-Bench.%2520Furthermore%252C%2520GeoThinker%2520demonstrates%2520robust%2520generalization%2520and%2520significantly%2520improved%2520spatial%2520perception%2520across%2520complex%2520downstream%2520scenarios%252C%2520including%2520embodied%2520referring%2520and%2520autonomous%2520driving.%2520Our%2520results%2520indicate%2520that%2520the%2520ability%2520to%2520actively%2520integrate%2520spatial%2520structures%2520is%2520essential%2520for%2520next-generation%2520spatial%2520intelligence.%2520Code%2520can%2520be%2520found%2520at%2520https%253A//github.com/Li-Hao-yuan/GeoThinker.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Geometry%3A%20Active%20Geometry%20Integration%20for%20Spatial%20Reasoning&entry.906535625=Haoyuan%20Li%20and%20Qihang%20Cao%20and%20Tao%20Tang%20and%20Kun%20Xiang%20and%20Zihan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang&entry.1292438233=Recent%20progress%20in%20spatial%20reasoning%20with%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20increasingly%20leverages%20geometric%20priors%20from%203D%20encoders.%20However%2C%20most%20existing%20integration%20strategies%20remain%20passive%3A%20geometry%20is%20exposed%20as%20a%20global%20stream%20and%20fused%20in%20an%20indiscriminate%20manner%2C%20which%20often%20induces%20semantic-geometry%20misalignment%20and%20redundant%20signals.%20We%20propose%20GeoThinker%2C%20a%20framework%20that%20shifts%20the%20paradigm%20from%20passive%20fusion%20to%20active%20perception.%20Instead%20of%20feature%20mixing%2C%20GeoThinker%20enables%20the%20model%20to%20selectively%20retrieve%20geometric%20evidence%20conditioned%20on%20its%20internal%20reasoning%20demands.%20GeoThinker%20achieves%20this%20through%20Spatial-Grounded%20Fusion%20applied%20at%20carefully%20selected%20VLM%20layers%2C%20where%20semantic%20visual%20priors%20selectively%20query%20and%20integrate%20task-relevant%20geometry%20via%20frame-strict%20cross-attention%2C%20further%20calibrated%20by%20Importance%20Gating%20that%20biases%20per-frame%20attention%20toward%20task-relevant%20structures.%20Comprehensive%20evaluation%20results%20show%20that%20GeoThinker%20sets%20a%20new%20state-of-the-art%20in%20spatial%20intelligence%2C%20achieving%20a%20peak%20score%20of%2072.6%20on%20the%20VSI-Bench.%20Furthermore%2C%20GeoThinker%20demonstrates%20robust%20generalization%20and%20significantly%20improved%20spatial%20perception%20across%20complex%20downstream%20scenarios%2C%20including%20embodied%20referring%20and%20autonomous%20driving.%20Our%20results%20indicate%20that%20the%20ability%20to%20actively%20integrate%20spatial%20structures%20is%20essential%20for%20next-generation%20spatial%20intelligence.%20Code%20can%20be%20found%20at%20https%3A//github.com/Li-Hao-yuan/GeoThinker.&entry.1838667208=http%3A//arxiv.org/abs/2602.06037v2&entry.124074799=Read"},
{"title": "Chunking Strategies for Multimodal AI Systems", "author": "Shashanka B R and Mohith Charan R and Seema Banu F", "abstract": "Chunking has emerged as a critical technique that enhances generative models by grounding their responses in efficiently segmented knowledge [1]. While initially developed for unimodal (primarily textual) domains, recent advances in multimodal foundation models have extended chunking approaches to incorporate diverse data types, including images, audio, and video [2]. A critical component underpinning the success of these systems is the chunking strategy how large, continuous streams of multimodal data are segmented into semantically meaningful units suitable for processing [3]. Despite its importance, chunking remains an under-explored area, especially in the context of multimodal systems where modality-specific constraints, semantic preservation, and alignment across modalities introduce unique challenges.\n\nOur goal is to consolidating the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.", "link": "http://arxiv.org/abs/2512.00185v2", "date": "2026-02-10", "relevancy": 2.8021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chunking%20Strategies%20for%20Multimodal%20AI%20Systems&body=Title%3A%20Chunking%20Strategies%20for%20Multimodal%20AI%20Systems%0AAuthor%3A%20Shashanka%20B%20R%20and%20Mohith%20Charan%20R%20and%20Seema%20Banu%20F%0AAbstract%3A%20Chunking%20has%20emerged%20as%20a%20critical%20technique%20that%20enhances%20generative%20models%20by%20grounding%20their%20responses%20in%20efficiently%20segmented%20knowledge%20%5B1%5D.%20While%20initially%20developed%20for%20unimodal%20%28primarily%20textual%29%20domains%2C%20recent%20advances%20in%20multimodal%20foundation%20models%20have%20extended%20chunking%20approaches%20to%20incorporate%20diverse%20data%20types%2C%20including%20images%2C%20audio%2C%20and%20video%20%5B2%5D.%20A%20critical%20component%20underpinning%20the%20success%20of%20these%20systems%20is%20the%20chunking%20strategy%20how%20large%2C%20continuous%20streams%20of%20multimodal%20data%20are%20segmented%20into%20semantically%20meaningful%20units%20suitable%20for%20processing%20%5B3%5D.%20Despite%20its%20importance%2C%20chunking%20remains%20an%20under-explored%20area%2C%20especially%20in%20the%20context%20of%20multimodal%20systems%20where%20modality-specific%20constraints%2C%20semantic%20preservation%2C%20and%20alignment%20across%20modalities%20introduce%20unique%20challenges.%0A%0AOur%20goal%20is%20to%20consolidating%20the%20landscape%20of%20multimodal%20chunking%20strategies%2C%20providing%20researchers%20and%20practitioners%20with%20a%20technical%20foundation%20and%20design%20space%20for%20developing%20more%20effective%20and%20efficient%20multimodal%20AI%20systems.%20This%20survey%20paves%20the%20way%20for%20innovations%20in%20robust%20chunking%20pipelines%20that%20scale%20with%20modality%20complexity%2C%20enhance%20processing%20accuracy%2C%20and%20improve%20generative%20coherence%20in%20real-world%20applications.%20This%20survey%20provides%20a%20comprehensive%20taxonomy%20and%20technical%20analysis%20of%20chunking%20strategies%20tailored%20for%20each%20modality%3A%20text%2C%20images%2C%20audio%2C%20video%2C%20and%20cross-modal%20data.%20We%20examine%20classical%20and%20modern%20approaches%20such%20as%20fixed-size%20token%20windowing%2C%20recursive%20text%20splitting%2C%20object-centric%20visual%20chunking%2C%20silence-based%20audio%20segmentation%2C%20and%20scene%20detection%20in%20videos.%20Each%20approach%20is%20analyzed%20in%20terms%20of%20its%20underlying%20methodology%2C%20supporting%20tools%20%28e.g.%2C%20LangChain%2C%20Detectron2%2C%20PySceneDetect%29%2C%20benefits%2C%20and%20challenges%2C%20particularly%20those%20related%20to%20granularity-context%20trade-offs%20and%20multimodal%20alignment.%20Furthermore%2C%20we%20explore%20emerging%20cross-modal%20chunking%20strategies%20that%20aim%20to%20preserve%20alignment%20and%20semantic%20consistency%20across%20disparate%20data%20types%20%5B4%5D.%20We%20also%20include%20comparative%20insights%2C%20highlight%20open%20problems%20such%20as%20asynchronous%20information%20density%20and%20noisy%20alignment%20signals%2C%20and%20identify%20opportunities%20for%20future%20research%20in%20adaptive%2C%20learning-based%2C%20and%20task-specific%20chunking.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00185v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChunking%2520Strategies%2520for%2520Multimodal%2520AI%2520Systems%26entry.906535625%3DShashanka%2520B%2520R%2520and%2520Mohith%2520Charan%2520R%2520and%2520Seema%2520Banu%2520F%26entry.1292438233%3DChunking%2520has%2520emerged%2520as%2520a%2520critical%2520technique%2520that%2520enhances%2520generative%2520models%2520by%2520grounding%2520their%2520responses%2520in%2520efficiently%2520segmented%2520knowledge%2520%255B1%255D.%2520While%2520initially%2520developed%2520for%2520unimodal%2520%2528primarily%2520textual%2529%2520domains%252C%2520recent%2520advances%2520in%2520multimodal%2520foundation%2520models%2520have%2520extended%2520chunking%2520approaches%2520to%2520incorporate%2520diverse%2520data%2520types%252C%2520including%2520images%252C%2520audio%252C%2520and%2520video%2520%255B2%255D.%2520A%2520critical%2520component%2520underpinning%2520the%2520success%2520of%2520these%2520systems%2520is%2520the%2520chunking%2520strategy%2520how%2520large%252C%2520continuous%2520streams%2520of%2520multimodal%2520data%2520are%2520segmented%2520into%2520semantically%2520meaningful%2520units%2520suitable%2520for%2520processing%2520%255B3%255D.%2520Despite%2520its%2520importance%252C%2520chunking%2520remains%2520an%2520under-explored%2520area%252C%2520especially%2520in%2520the%2520context%2520of%2520multimodal%2520systems%2520where%2520modality-specific%2520constraints%252C%2520semantic%2520preservation%252C%2520and%2520alignment%2520across%2520modalities%2520introduce%2520unique%2520challenges.%250A%250AOur%2520goal%2520is%2520to%2520consolidating%2520the%2520landscape%2520of%2520multimodal%2520chunking%2520strategies%252C%2520providing%2520researchers%2520and%2520practitioners%2520with%2520a%2520technical%2520foundation%2520and%2520design%2520space%2520for%2520developing%2520more%2520effective%2520and%2520efficient%2520multimodal%2520AI%2520systems.%2520This%2520survey%2520paves%2520the%2520way%2520for%2520innovations%2520in%2520robust%2520chunking%2520pipelines%2520that%2520scale%2520with%2520modality%2520complexity%252C%2520enhance%2520processing%2520accuracy%252C%2520and%2520improve%2520generative%2520coherence%2520in%2520real-world%2520applications.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520taxonomy%2520and%2520technical%2520analysis%2520of%2520chunking%2520strategies%2520tailored%2520for%2520each%2520modality%253A%2520text%252C%2520images%252C%2520audio%252C%2520video%252C%2520and%2520cross-modal%2520data.%2520We%2520examine%2520classical%2520and%2520modern%2520approaches%2520such%2520as%2520fixed-size%2520token%2520windowing%252C%2520recursive%2520text%2520splitting%252C%2520object-centric%2520visual%2520chunking%252C%2520silence-based%2520audio%2520segmentation%252C%2520and%2520scene%2520detection%2520in%2520videos.%2520Each%2520approach%2520is%2520analyzed%2520in%2520terms%2520of%2520its%2520underlying%2520methodology%252C%2520supporting%2520tools%2520%2528e.g.%252C%2520LangChain%252C%2520Detectron2%252C%2520PySceneDetect%2529%252C%2520benefits%252C%2520and%2520challenges%252C%2520particularly%2520those%2520related%2520to%2520granularity-context%2520trade-offs%2520and%2520multimodal%2520alignment.%2520Furthermore%252C%2520we%2520explore%2520emerging%2520cross-modal%2520chunking%2520strategies%2520that%2520aim%2520to%2520preserve%2520alignment%2520and%2520semantic%2520consistency%2520across%2520disparate%2520data%2520types%2520%255B4%255D.%2520We%2520also%2520include%2520comparative%2520insights%252C%2520highlight%2520open%2520problems%2520such%2520as%2520asynchronous%2520information%2520density%2520and%2520noisy%2520alignment%2520signals%252C%2520and%2520identify%2520opportunities%2520for%2520future%2520research%2520in%2520adaptive%252C%2520learning-based%252C%2520and%2520task-specific%2520chunking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00185v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chunking%20Strategies%20for%20Multimodal%20AI%20Systems&entry.906535625=Shashanka%20B%20R%20and%20Mohith%20Charan%20R%20and%20Seema%20Banu%20F&entry.1292438233=Chunking%20has%20emerged%20as%20a%20critical%20technique%20that%20enhances%20generative%20models%20by%20grounding%20their%20responses%20in%20efficiently%20segmented%20knowledge%20%5B1%5D.%20While%20initially%20developed%20for%20unimodal%20%28primarily%20textual%29%20domains%2C%20recent%20advances%20in%20multimodal%20foundation%20models%20have%20extended%20chunking%20approaches%20to%20incorporate%20diverse%20data%20types%2C%20including%20images%2C%20audio%2C%20and%20video%20%5B2%5D.%20A%20critical%20component%20underpinning%20the%20success%20of%20these%20systems%20is%20the%20chunking%20strategy%20how%20large%2C%20continuous%20streams%20of%20multimodal%20data%20are%20segmented%20into%20semantically%20meaningful%20units%20suitable%20for%20processing%20%5B3%5D.%20Despite%20its%20importance%2C%20chunking%20remains%20an%20under-explored%20area%2C%20especially%20in%20the%20context%20of%20multimodal%20systems%20where%20modality-specific%20constraints%2C%20semantic%20preservation%2C%20and%20alignment%20across%20modalities%20introduce%20unique%20challenges.%0A%0AOur%20goal%20is%20to%20consolidating%20the%20landscape%20of%20multimodal%20chunking%20strategies%2C%20providing%20researchers%20and%20practitioners%20with%20a%20technical%20foundation%20and%20design%20space%20for%20developing%20more%20effective%20and%20efficient%20multimodal%20AI%20systems.%20This%20survey%20paves%20the%20way%20for%20innovations%20in%20robust%20chunking%20pipelines%20that%20scale%20with%20modality%20complexity%2C%20enhance%20processing%20accuracy%2C%20and%20improve%20generative%20coherence%20in%20real-world%20applications.%20This%20survey%20provides%20a%20comprehensive%20taxonomy%20and%20technical%20analysis%20of%20chunking%20strategies%20tailored%20for%20each%20modality%3A%20text%2C%20images%2C%20audio%2C%20video%2C%20and%20cross-modal%20data.%20We%20examine%20classical%20and%20modern%20approaches%20such%20as%20fixed-size%20token%20windowing%2C%20recursive%20text%20splitting%2C%20object-centric%20visual%20chunking%2C%20silence-based%20audio%20segmentation%2C%20and%20scene%20detection%20in%20videos.%20Each%20approach%20is%20analyzed%20in%20terms%20of%20its%20underlying%20methodology%2C%20supporting%20tools%20%28e.g.%2C%20LangChain%2C%20Detectron2%2C%20PySceneDetect%29%2C%20benefits%2C%20and%20challenges%2C%20particularly%20those%20related%20to%20granularity-context%20trade-offs%20and%20multimodal%20alignment.%20Furthermore%2C%20we%20explore%20emerging%20cross-modal%20chunking%20strategies%20that%20aim%20to%20preserve%20alignment%20and%20semantic%20consistency%20across%20disparate%20data%20types%20%5B4%5D.%20We%20also%20include%20comparative%20insights%2C%20highlight%20open%20problems%20such%20as%20asynchronous%20information%20density%20and%20noisy%20alignment%20signals%2C%20and%20identify%20opportunities%20for%20future%20research%20in%20adaptive%2C%20learning-based%2C%20and%20task-specific%20chunking.&entry.1838667208=http%3A//arxiv.org/abs/2512.00185v2&entry.124074799=Read"},
{"title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs", "author": "Niccolo Avogaro and Nayanika Debnath and Li Mi and Thomas Frick and Junling Wang and Zexue He and Hang Hua and Konrad Schindler and Mattia Rigotti", "abstract": "Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200$\\times$ lower token budget.", "link": "http://arxiv.org/abs/2602.06566v2", "date": "2026-02-10", "relevancy": 2.7752, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARC%3A%20Separating%20Perception%20And%20Reasoning%20Circuits%20for%20Test-time%20Scaling%20of%20VLMs&body=Title%3A%20SPARC%3A%20Separating%20Perception%20And%20Reasoning%20Circuits%20for%20Test-time%20Scaling%20of%20VLMs%0AAuthor%3A%20Niccolo%20Avogaro%20and%20Nayanika%20Debnath%20and%20Li%20Mi%20and%20Thomas%20Frick%20and%20Junling%20Wang%20and%20Zexue%20He%20and%20Hang%20Hua%20and%20Konrad%20Schindler%20and%20Mattia%20Rigotti%0AAbstract%3A%20Despite%20recent%20successes%2C%20test-time%20scaling%20-%20i.e.%2C%20dynamically%20expanding%20the%20token%20budget%20during%20inference%20as%20needed%20-%20remains%20brittle%20for%20vision-language%20models%20%28VLMs%29%3A%20unstructured%20chains-of-thought%20about%20images%20entangle%20perception%20and%20reasoning%2C%20leading%20to%20long%2C%20disorganized%20contexts%20where%20small%20perceptual%20mistakes%20may%20cascade%20into%20completely%20wrong%20answers.%20Moreover%2C%20expensive%20reinforcement%20learning%20with%20hand-crafted%20rewards%20is%20required%20to%20achieve%20good%20performance.%20Here%2C%20we%20introduce%20SPARC%20%28Separating%20Perception%20And%20Reasoning%20Circuits%29%2C%20a%20modular%20framework%20that%20explicitly%20decouples%20visual%20perception%20from%20reasoning.%20Inspired%20by%20sequential%20sensory-to-cognitive%20processing%20in%20the%20brain%2C%20SPARC%20implements%20a%20two-stage%20pipeline%20where%20the%20model%20first%20performs%20explicit%20visual%20search%20to%20localize%20question-relevant%20regions%2C%20then%20conditions%20its%20reasoning%20on%20those%20regions%20to%20produce%20the%20final%20answer.%20This%20separation%20enables%20independent%20test-time%20scaling%20with%20asymmetric%20compute%20allocation%20%28e.g.%2C%20prioritizing%20perceptual%20processing%20under%20distribution%20shift%29%2C%20supports%20selective%20optimization%20%28e.g.%2C%20improving%20the%20perceptual%20stage%20alone%20when%20it%20is%20the%20bottleneck%20for%20end-to-end%20performance%29%2C%20and%20accommodates%20compressed%20contexts%20by%20running%20global%20search%20at%20lower%20image%20resolutions%20and%20allocating%20high-resolution%20processing%20only%20to%20selected%20regions%2C%20thereby%20reducing%20total%20visual%20tokens%20count%20and%20compute.%20Across%20challenging%20visual%20reasoning%20benchmarks%2C%20SPARC%20outperforms%20monolithic%20baselines%20and%20strong%20visual-grounding%20approaches.%20For%20instance%2C%20SPARC%20improves%20the%20accuracy%20of%20Qwen3VL-4B%20on%20the%20%24V%5E%2A%24%20VQA%20benchmark%20by%206.7%20percentage%20points%2C%20and%20it%20surpasses%20%22thinking%20with%20images%22%20by%204.6%20points%20on%20a%20challenging%20OOD%20task%20despite%20requiring%20a%20200%24%5Ctimes%24%20lower%20token%20budget.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06566v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARC%253A%2520Separating%2520Perception%2520And%2520Reasoning%2520Circuits%2520for%2520Test-time%2520Scaling%2520of%2520VLMs%26entry.906535625%3DNiccolo%2520Avogaro%2520and%2520Nayanika%2520Debnath%2520and%2520Li%2520Mi%2520and%2520Thomas%2520Frick%2520and%2520Junling%2520Wang%2520and%2520Zexue%2520He%2520and%2520Hang%2520Hua%2520and%2520Konrad%2520Schindler%2520and%2520Mattia%2520Rigotti%26entry.1292438233%3DDespite%2520recent%2520successes%252C%2520test-time%2520scaling%2520-%2520i.e.%252C%2520dynamically%2520expanding%2520the%2520token%2520budget%2520during%2520inference%2520as%2520needed%2520-%2520remains%2520brittle%2520for%2520vision-language%2520models%2520%2528VLMs%2529%253A%2520unstructured%2520chains-of-thought%2520about%2520images%2520entangle%2520perception%2520and%2520reasoning%252C%2520leading%2520to%2520long%252C%2520disorganized%2520contexts%2520where%2520small%2520perceptual%2520mistakes%2520may%2520cascade%2520into%2520completely%2520wrong%2520answers.%2520Moreover%252C%2520expensive%2520reinforcement%2520learning%2520with%2520hand-crafted%2520rewards%2520is%2520required%2520to%2520achieve%2520good%2520performance.%2520Here%252C%2520we%2520introduce%2520SPARC%2520%2528Separating%2520Perception%2520And%2520Reasoning%2520Circuits%2529%252C%2520a%2520modular%2520framework%2520that%2520explicitly%2520decouples%2520visual%2520perception%2520from%2520reasoning.%2520Inspired%2520by%2520sequential%2520sensory-to-cognitive%2520processing%2520in%2520the%2520brain%252C%2520SPARC%2520implements%2520a%2520two-stage%2520pipeline%2520where%2520the%2520model%2520first%2520performs%2520explicit%2520visual%2520search%2520to%2520localize%2520question-relevant%2520regions%252C%2520then%2520conditions%2520its%2520reasoning%2520on%2520those%2520regions%2520to%2520produce%2520the%2520final%2520answer.%2520This%2520separation%2520enables%2520independent%2520test-time%2520scaling%2520with%2520asymmetric%2520compute%2520allocation%2520%2528e.g.%252C%2520prioritizing%2520perceptual%2520processing%2520under%2520distribution%2520shift%2529%252C%2520supports%2520selective%2520optimization%2520%2528e.g.%252C%2520improving%2520the%2520perceptual%2520stage%2520alone%2520when%2520it%2520is%2520the%2520bottleneck%2520for%2520end-to-end%2520performance%2529%252C%2520and%2520accommodates%2520compressed%2520contexts%2520by%2520running%2520global%2520search%2520at%2520lower%2520image%2520resolutions%2520and%2520allocating%2520high-resolution%2520processing%2520only%2520to%2520selected%2520regions%252C%2520thereby%2520reducing%2520total%2520visual%2520tokens%2520count%2520and%2520compute.%2520Across%2520challenging%2520visual%2520reasoning%2520benchmarks%252C%2520SPARC%2520outperforms%2520monolithic%2520baselines%2520and%2520strong%2520visual-grounding%2520approaches.%2520For%2520instance%252C%2520SPARC%2520improves%2520the%2520accuracy%2520of%2520Qwen3VL-4B%2520on%2520the%2520%2524V%255E%252A%2524%2520VQA%2520benchmark%2520by%25206.7%2520percentage%2520points%252C%2520and%2520it%2520surpasses%2520%2522thinking%2520with%2520images%2522%2520by%25204.6%2520points%2520on%2520a%2520challenging%2520OOD%2520task%2520despite%2520requiring%2520a%2520200%2524%255Ctimes%2524%2520lower%2520token%2520budget.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06566v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARC%3A%20Separating%20Perception%20And%20Reasoning%20Circuits%20for%20Test-time%20Scaling%20of%20VLMs&entry.906535625=Niccolo%20Avogaro%20and%20Nayanika%20Debnath%20and%20Li%20Mi%20and%20Thomas%20Frick%20and%20Junling%20Wang%20and%20Zexue%20He%20and%20Hang%20Hua%20and%20Konrad%20Schindler%20and%20Mattia%20Rigotti&entry.1292438233=Despite%20recent%20successes%2C%20test-time%20scaling%20-%20i.e.%2C%20dynamically%20expanding%20the%20token%20budget%20during%20inference%20as%20needed%20-%20remains%20brittle%20for%20vision-language%20models%20%28VLMs%29%3A%20unstructured%20chains-of-thought%20about%20images%20entangle%20perception%20and%20reasoning%2C%20leading%20to%20long%2C%20disorganized%20contexts%20where%20small%20perceptual%20mistakes%20may%20cascade%20into%20completely%20wrong%20answers.%20Moreover%2C%20expensive%20reinforcement%20learning%20with%20hand-crafted%20rewards%20is%20required%20to%20achieve%20good%20performance.%20Here%2C%20we%20introduce%20SPARC%20%28Separating%20Perception%20And%20Reasoning%20Circuits%29%2C%20a%20modular%20framework%20that%20explicitly%20decouples%20visual%20perception%20from%20reasoning.%20Inspired%20by%20sequential%20sensory-to-cognitive%20processing%20in%20the%20brain%2C%20SPARC%20implements%20a%20two-stage%20pipeline%20where%20the%20model%20first%20performs%20explicit%20visual%20search%20to%20localize%20question-relevant%20regions%2C%20then%20conditions%20its%20reasoning%20on%20those%20regions%20to%20produce%20the%20final%20answer.%20This%20separation%20enables%20independent%20test-time%20scaling%20with%20asymmetric%20compute%20allocation%20%28e.g.%2C%20prioritizing%20perceptual%20processing%20under%20distribution%20shift%29%2C%20supports%20selective%20optimization%20%28e.g.%2C%20improving%20the%20perceptual%20stage%20alone%20when%20it%20is%20the%20bottleneck%20for%20end-to-end%20performance%29%2C%20and%20accommodates%20compressed%20contexts%20by%20running%20global%20search%20at%20lower%20image%20resolutions%20and%20allocating%20high-resolution%20processing%20only%20to%20selected%20regions%2C%20thereby%20reducing%20total%20visual%20tokens%20count%20and%20compute.%20Across%20challenging%20visual%20reasoning%20benchmarks%2C%20SPARC%20outperforms%20monolithic%20baselines%20and%20strong%20visual-grounding%20approaches.%20For%20instance%2C%20SPARC%20improves%20the%20accuracy%20of%20Qwen3VL-4B%20on%20the%20%24V%5E%2A%24%20VQA%20benchmark%20by%206.7%20percentage%20points%2C%20and%20it%20surpasses%20%22thinking%20with%20images%22%20by%204.6%20points%20on%20a%20challenging%20OOD%20task%20despite%20requiring%20a%20200%24%5Ctimes%24%20lower%20token%20budget.&entry.1838667208=http%3A//arxiv.org/abs/2602.06566v2&entry.124074799=Read"},
{"title": "Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation", "author": "Siyu Chen and Ting Han and Haoling Huang and Chaolei Wang and Chengzheng Fu and Duxin Zhu and Guorong Cai and Jinhe Su", "abstract": "Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.", "link": "http://arxiv.org/abs/2602.09648v1", "date": "2026-02-10", "relevancy": 2.77, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5655}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5576}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time2General%3A%20Learning%20Spatiotemporal%20Invariant%20Representations%20for%20Domain-Generalization%20Video%20Semantic%20Segmentation&body=Title%3A%20Time2General%3A%20Learning%20Spatiotemporal%20Invariant%20Representations%20for%20Domain-Generalization%20Video%20Semantic%20Segmentation%0AAuthor%3A%20Siyu%20Chen%20and%20Ting%20Han%20and%20Haoling%20Huang%20and%20Chaolei%20Wang%20and%20Chengzheng%20Fu%20and%20Duxin%20Zhu%20and%20Guorong%20Cai%20and%20Jinhe%20Su%0AAbstract%3A%20Domain%20Generalized%20Video%20Semantic%20Segmentation%20%28DGVSS%29%20is%20trained%20on%20a%20single%20labeled%20driving%20domain%20and%20is%20directly%20deployed%20on%20unseen%20domains%20without%20target%20labels%20and%20test-time%20adaptation%20while%20maintaining%20temporally%20consistent%20predictions%20over%20video%20streams.%20In%20practice%2C%20both%20domain%20shift%20and%20temporal-sampling%20shift%20break%20correspondence-based%20propagation%20and%20fixed-stride%20temporal%20aggregation%2C%20causing%20severe%20frame-to-frame%20flicker%20even%20in%20label-stable%20regions.%20We%20propose%20Time2General%2C%20a%20DGVSS%20framework%20built%20on%20Stability%20Queries.%20Time2General%20introduces%20a%20Spatio-Temporal%20Memory%20Decoder%20that%20aggregates%20multi-frame%20context%20into%20a%20clip-level%20spatio-temporal%20memory%20and%20decodes%20temporally%20consistent%20per-frame%20masks%20without%20explicit%20correspondence%20propagation.%20To%20further%20suppress%20flicker%20and%20improve%20robustness%20to%20varying%20sampling%20rates%2C%20the%20Masked%20Temporal%20Consistency%20Loss%20is%20proposed%20to%20regularize%20temporal%20prediction%20discrepancies%20across%20different%20strides%2C%20and%20randomize%20training%20strides%20to%20expose%20the%20model%20to%20diverse%20temporal%20gaps.%20Extensive%20experiments%20on%20multiple%20driving%20benchmarks%20show%20that%20Time2General%20achieves%20a%20substantial%20improvement%20in%20cross-domain%20accuracy%20and%20temporal%20stability%20over%20prior%20DGSS%20and%20VSS%20baselines%20while%20running%20at%20up%20to%2018%20FPS.%20Code%20will%20be%20released%20after%20the%20review%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime2General%253A%2520Learning%2520Spatiotemporal%2520Invariant%2520Representations%2520for%2520Domain-Generalization%2520Video%2520Semantic%2520Segmentation%26entry.906535625%3DSiyu%2520Chen%2520and%2520Ting%2520Han%2520and%2520Haoling%2520Huang%2520and%2520Chaolei%2520Wang%2520and%2520Chengzheng%2520Fu%2520and%2520Duxin%2520Zhu%2520and%2520Guorong%2520Cai%2520and%2520Jinhe%2520Su%26entry.1292438233%3DDomain%2520Generalized%2520Video%2520Semantic%2520Segmentation%2520%2528DGVSS%2529%2520is%2520trained%2520on%2520a%2520single%2520labeled%2520driving%2520domain%2520and%2520is%2520directly%2520deployed%2520on%2520unseen%2520domains%2520without%2520target%2520labels%2520and%2520test-time%2520adaptation%2520while%2520maintaining%2520temporally%2520consistent%2520predictions%2520over%2520video%2520streams.%2520In%2520practice%252C%2520both%2520domain%2520shift%2520and%2520temporal-sampling%2520shift%2520break%2520correspondence-based%2520propagation%2520and%2520fixed-stride%2520temporal%2520aggregation%252C%2520causing%2520severe%2520frame-to-frame%2520flicker%2520even%2520in%2520label-stable%2520regions.%2520We%2520propose%2520Time2General%252C%2520a%2520DGVSS%2520framework%2520built%2520on%2520Stability%2520Queries.%2520Time2General%2520introduces%2520a%2520Spatio-Temporal%2520Memory%2520Decoder%2520that%2520aggregates%2520multi-frame%2520context%2520into%2520a%2520clip-level%2520spatio-temporal%2520memory%2520and%2520decodes%2520temporally%2520consistent%2520per-frame%2520masks%2520without%2520explicit%2520correspondence%2520propagation.%2520To%2520further%2520suppress%2520flicker%2520and%2520improve%2520robustness%2520to%2520varying%2520sampling%2520rates%252C%2520the%2520Masked%2520Temporal%2520Consistency%2520Loss%2520is%2520proposed%2520to%2520regularize%2520temporal%2520prediction%2520discrepancies%2520across%2520different%2520strides%252C%2520and%2520randomize%2520training%2520strides%2520to%2520expose%2520the%2520model%2520to%2520diverse%2520temporal%2520gaps.%2520Extensive%2520experiments%2520on%2520multiple%2520driving%2520benchmarks%2520show%2520that%2520Time2General%2520achieves%2520a%2520substantial%2520improvement%2520in%2520cross-domain%2520accuracy%2520and%2520temporal%2520stability%2520over%2520prior%2520DGSS%2520and%2520VSS%2520baselines%2520while%2520running%2520at%2520up%2520to%252018%2520FPS.%2520Code%2520will%2520be%2520released%2520after%2520the%2520review%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time2General%3A%20Learning%20Spatiotemporal%20Invariant%20Representations%20for%20Domain-Generalization%20Video%20Semantic%20Segmentation&entry.906535625=Siyu%20Chen%20and%20Ting%20Han%20and%20Haoling%20Huang%20and%20Chaolei%20Wang%20and%20Chengzheng%20Fu%20and%20Duxin%20Zhu%20and%20Guorong%20Cai%20and%20Jinhe%20Su&entry.1292438233=Domain%20Generalized%20Video%20Semantic%20Segmentation%20%28DGVSS%29%20is%20trained%20on%20a%20single%20labeled%20driving%20domain%20and%20is%20directly%20deployed%20on%20unseen%20domains%20without%20target%20labels%20and%20test-time%20adaptation%20while%20maintaining%20temporally%20consistent%20predictions%20over%20video%20streams.%20In%20practice%2C%20both%20domain%20shift%20and%20temporal-sampling%20shift%20break%20correspondence-based%20propagation%20and%20fixed-stride%20temporal%20aggregation%2C%20causing%20severe%20frame-to-frame%20flicker%20even%20in%20label-stable%20regions.%20We%20propose%20Time2General%2C%20a%20DGVSS%20framework%20built%20on%20Stability%20Queries.%20Time2General%20introduces%20a%20Spatio-Temporal%20Memory%20Decoder%20that%20aggregates%20multi-frame%20context%20into%20a%20clip-level%20spatio-temporal%20memory%20and%20decodes%20temporally%20consistent%20per-frame%20masks%20without%20explicit%20correspondence%20propagation.%20To%20further%20suppress%20flicker%20and%20improve%20robustness%20to%20varying%20sampling%20rates%2C%20the%20Masked%20Temporal%20Consistency%20Loss%20is%20proposed%20to%20regularize%20temporal%20prediction%20discrepancies%20across%20different%20strides%2C%20and%20randomize%20training%20strides%20to%20expose%20the%20model%20to%20diverse%20temporal%20gaps.%20Extensive%20experiments%20on%20multiple%20driving%20benchmarks%20show%20that%20Time2General%20achieves%20a%20substantial%20improvement%20in%20cross-domain%20accuracy%20and%20temporal%20stability%20over%20prior%20DGSS%20and%20VSS%20baselines%20while%20running%20at%20up%20to%2018%20FPS.%20Code%20will%20be%20released%20after%20the%20review%20process.&entry.1838667208=http%3A//arxiv.org/abs/2602.09648v1&entry.124074799=Read"},
{"title": "GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation", "author": "Sandesh Hegde and Jaison Saji Chacko and Debarshi Banerjee and Uma Mahesh", "abstract": "We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.\n  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.\n  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.", "link": "http://arxiv.org/abs/2602.09701v1", "date": "2026-02-10", "relevancy": 2.761, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenSeg-R1%3A%20RL-Driven%20Vision-Language%20Grounding%20for%20Fine-Grained%20Referring%20Segmentation&body=Title%3A%20GenSeg-R1%3A%20RL-Driven%20Vision-Language%20Grounding%20for%20Fine-Grained%20Referring%20Segmentation%0AAuthor%3A%20Sandesh%20Hegde%20and%20Jaison%20Saji%20Chacko%20and%20Debarshi%20Banerjee%20and%20Uma%20Mahesh%0AAbstract%3A%20We%20study%20fine-grained%20referring%20image%20segmentation%20via%20a%20decoupled%20reason-then-segment%20pipeline.%20A%20vision-language%20model%20%28VLM%29%20receives%20an%20image%20and%20a%20natural-language%20query%2C%20reasons%20about%20the%20scene%2C%20and%20emits%20structured%20spatial%20prompts%3A%20a%20bounding%20box%20plus%20two%20interior%20keypoints%20for%20every%20referred%20instance.%20A%20frozen%20promptable%20segmenter%20%28SAM%202%29%20converts%20these%20prompts%20into%20high-quality%20masks.%0A%20%20Within%20our%20GenSeg-R1%20framework%20we%20finetune%20Qwen3-VL%20models%20%284B%20and%208B%20parameters%29%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20requiring%20no%20supervised%20reasoning-chain%20annotations.%20On%20RefCOCOg%20validation%20our%20best%20model%20%28GenSeg-R1-8B%29%20achieves%200.7127%20cIoU%20and%200.7382%20mIoU%2C%20substantially%20outperforming%20the%20corresponding%20Qwen3-VL%20Instruct%20baselines%20%28%2B15.3%20and%20%2B21.9%20points%2C%20respectively%29%20and%20surpassing%20Seg-Zero-7B%20%5B3%5D%20by%20%2B3.3%20cIoU%20under%20identical%20evaluation.%0A%20%20We%20further%20introduce%20GenSeg-R1-G%2C%20a%20variant%20trained%20on%20GRefCOCO%20%5B9%5D%20with%20a%20SAM%202%20in-the-loop%20reward%20that%20directly%20optimizes%20mask%20quality.%20On%20GRefCOCO%20validation%20GenSeg-R1-G%20achieves%2076.69%25%20target%20mIoU%20with%2082.40%25%20accuracy%20on%20negative%20%28no-target%29%20prompts%2C%20substantially%20outperforming%20Seg-R1-7B%20and%20Seg-Zero-7B%2C%20which%20lack%20no-target%20detection%20capability.%20On%20ReasonSeg%20test%2C%20GenSeg-R1-4B%20reaches%2068.40%25%20mIoU%2C%20surpassing%20Seg-Zero-7B%20by%20%2B7.0%20and%20Seg-R1-7B%20by%20%2B10.7%20points.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenSeg-R1%253A%2520RL-Driven%2520Vision-Language%2520Grounding%2520for%2520Fine-Grained%2520Referring%2520Segmentation%26entry.906535625%3DSandesh%2520Hegde%2520and%2520Jaison%2520Saji%2520Chacko%2520and%2520Debarshi%2520Banerjee%2520and%2520Uma%2520Mahesh%26entry.1292438233%3DWe%2520study%2520fine-grained%2520referring%2520image%2520segmentation%2520via%2520a%2520decoupled%2520reason-then-segment%2520pipeline.%2520A%2520vision-language%2520model%2520%2528VLM%2529%2520receives%2520an%2520image%2520and%2520a%2520natural-language%2520query%252C%2520reasons%2520about%2520the%2520scene%252C%2520and%2520emits%2520structured%2520spatial%2520prompts%253A%2520a%2520bounding%2520box%2520plus%2520two%2520interior%2520keypoints%2520for%2520every%2520referred%2520instance.%2520A%2520frozen%2520promptable%2520segmenter%2520%2528SAM%25202%2529%2520converts%2520these%2520prompts%2520into%2520high-quality%2520masks.%250A%2520%2520Within%2520our%2520GenSeg-R1%2520framework%2520we%2520finetune%2520Qwen3-VL%2520models%2520%25284B%2520and%25208B%2520parameters%2529%2520using%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520requiring%2520no%2520supervised%2520reasoning-chain%2520annotations.%2520On%2520RefCOCOg%2520validation%2520our%2520best%2520model%2520%2528GenSeg-R1-8B%2529%2520achieves%25200.7127%2520cIoU%2520and%25200.7382%2520mIoU%252C%2520substantially%2520outperforming%2520the%2520corresponding%2520Qwen3-VL%2520Instruct%2520baselines%2520%2528%252B15.3%2520and%2520%252B21.9%2520points%252C%2520respectively%2529%2520and%2520surpassing%2520Seg-Zero-7B%2520%255B3%255D%2520by%2520%252B3.3%2520cIoU%2520under%2520identical%2520evaluation.%250A%2520%2520We%2520further%2520introduce%2520GenSeg-R1-G%252C%2520a%2520variant%2520trained%2520on%2520GRefCOCO%2520%255B9%255D%2520with%2520a%2520SAM%25202%2520in-the-loop%2520reward%2520that%2520directly%2520optimizes%2520mask%2520quality.%2520On%2520GRefCOCO%2520validation%2520GenSeg-R1-G%2520achieves%252076.69%2525%2520target%2520mIoU%2520with%252082.40%2525%2520accuracy%2520on%2520negative%2520%2528no-target%2529%2520prompts%252C%2520substantially%2520outperforming%2520Seg-R1-7B%2520and%2520Seg-Zero-7B%252C%2520which%2520lack%2520no-target%2520detection%2520capability.%2520On%2520ReasonSeg%2520test%252C%2520GenSeg-R1-4B%2520reaches%252068.40%2525%2520mIoU%252C%2520surpassing%2520Seg-Zero-7B%2520by%2520%252B7.0%2520and%2520Seg-R1-7B%2520by%2520%252B10.7%2520points.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenSeg-R1%3A%20RL-Driven%20Vision-Language%20Grounding%20for%20Fine-Grained%20Referring%20Segmentation&entry.906535625=Sandesh%20Hegde%20and%20Jaison%20Saji%20Chacko%20and%20Debarshi%20Banerjee%20and%20Uma%20Mahesh&entry.1292438233=We%20study%20fine-grained%20referring%20image%20segmentation%20via%20a%20decoupled%20reason-then-segment%20pipeline.%20A%20vision-language%20model%20%28VLM%29%20receives%20an%20image%20and%20a%20natural-language%20query%2C%20reasons%20about%20the%20scene%2C%20and%20emits%20structured%20spatial%20prompts%3A%20a%20bounding%20box%20plus%20two%20interior%20keypoints%20for%20every%20referred%20instance.%20A%20frozen%20promptable%20segmenter%20%28SAM%202%29%20converts%20these%20prompts%20into%20high-quality%20masks.%0A%20%20Within%20our%20GenSeg-R1%20framework%20we%20finetune%20Qwen3-VL%20models%20%284B%20and%208B%20parameters%29%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20requiring%20no%20supervised%20reasoning-chain%20annotations.%20On%20RefCOCOg%20validation%20our%20best%20model%20%28GenSeg-R1-8B%29%20achieves%200.7127%20cIoU%20and%200.7382%20mIoU%2C%20substantially%20outperforming%20the%20corresponding%20Qwen3-VL%20Instruct%20baselines%20%28%2B15.3%20and%20%2B21.9%20points%2C%20respectively%29%20and%20surpassing%20Seg-Zero-7B%20%5B3%5D%20by%20%2B3.3%20cIoU%20under%20identical%20evaluation.%0A%20%20We%20further%20introduce%20GenSeg-R1-G%2C%20a%20variant%20trained%20on%20GRefCOCO%20%5B9%5D%20with%20a%20SAM%202%20in-the-loop%20reward%20that%20directly%20optimizes%20mask%20quality.%20On%20GRefCOCO%20validation%20GenSeg-R1-G%20achieves%2076.69%25%20target%20mIoU%20with%2082.40%25%20accuracy%20on%20negative%20%28no-target%29%20prompts%2C%20substantially%20outperforming%20Seg-R1-7B%20and%20Seg-Zero-7B%2C%20which%20lack%20no-target%20detection%20capability.%20On%20ReasonSeg%20test%2C%20GenSeg-R1-4B%20reaches%2068.40%25%20mIoU%2C%20surpassing%20Seg-Zero-7B%20by%20%2B7.0%20and%20Seg-R1-7B%20by%20%2B10.7%20points.&entry.1838667208=http%3A//arxiv.org/abs/2602.09701v1&entry.124074799=Read"},
{"title": "Learning to Detect Baked Goods with Limited Supervision", "author": "Thomas H. Schmitt and Maximilian Bundscherer and Tobias Bocklet", "abstract": "Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.", "link": "http://arxiv.org/abs/2602.09979v1", "date": "2026-02-10", "relevancy": 2.7282, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5601}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Detect%20Baked%20Goods%20with%20Limited%20Supervision&body=Title%3A%20Learning%20to%20Detect%20Baked%20Goods%20with%20Limited%20Supervision%0AAuthor%3A%20Thomas%20H.%20Schmitt%20and%20Maximilian%20Bundscherer%20and%20Tobias%20Bocklet%0AAbstract%3A%20Monitoring%20leftover%20products%20provides%20valuable%20insights%20that%20can%20be%20used%20to%20optimize%20future%20production.%20This%20is%20especially%20important%20for%20German%20bakeries%20because%20freshly%20baked%20goods%20have%20a%20very%20short%20shelf%20life.%20Automating%20this%20process%20can%20reduce%20labor%20costs%2C%20improve%20accuracy%2C%20and%20streamline%20operations.%20We%20propose%20automating%20this%20process%20using%20an%20object%20detection%20model%20to%20identify%20baked%20goods%20from%20images.%20However%2C%20the%20large%20diversity%20of%20German%20baked%20goods%20makes%20fully%20supervised%20training%20prohibitively%20expensive%20and%20limits%20scalability.%20Although%20open-vocabulary%20detectors%20%28e.g.%2C%20OWLv2%2C%20Grounding%20DINO%29%20offer%20lexibility%2C%20we%20demonstrate%20that%20they%20are%20insufficient%20for%20our%20task.%20While%20motivated%20by%20bakeries%2C%20our%20work%20addresses%20the%20broader%20challenges%20of%20deploying%20computer%20vision%20in%20industries%2C%20where%20tasks%20are%20specialized%20and%20annotated%20datasets%20are%20scarce.%20We%20compile%20dataset%20splits%20with%20varying%20supervision%20levels%2C%20covering%2019%20classes%20of%20baked%20goods.%20We%20propose%20two%20training%20workflows%20to%20train%20an%20object%20detection%20model%20with%20limited%20supervision.%20First%2C%20we%20combine%20OWLv2%20and%20Grounding%20DINO%20localization%20with%20image-level%20supervision%20to%20train%20the%20model%20in%20a%20weakly%20supervised%20manner.%20Second%2C%20we%20improve%20viewpoint%20robustness%20by%20fine-tuning%20on%20video%20frames%20annotated%20using%20Segment%20Anything%202%20as%20a%20pseudo-label%20propagation%20model.%20Using%20these%20workflows%2C%20we%20train%20YOLOv11%20for%20our%20detection%20task%20due%20to%20its%20favorable%20speed%20accuracy%20tradeoff.%20Relying%20solely%20on%20image-level%20supervision%2C%20the%20model%20achieves%20a%20mean%20Average%20Precision%20%28mAP%29%20of%200.91.%20Finetuning%20with%20pseudo-labels%20raises%20model%20performance%20by%2019.3%25%20under%20non-ideal%20deployment%20conditions.%20Combining%20these%20workflows%20trains%20a%20model%20that%20surpasses%20our%20fully-supervised%20baseline%20model%20under%20non-ideal%20deployment%20conditions%2C%20despite%20relying%20only%20on%20image-level%20supervision.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Detect%2520Baked%2520Goods%2520with%2520Limited%2520Supervision%26entry.906535625%3DThomas%2520H.%2520Schmitt%2520and%2520Maximilian%2520Bundscherer%2520and%2520Tobias%2520Bocklet%26entry.1292438233%3DMonitoring%2520leftover%2520products%2520provides%2520valuable%2520insights%2520that%2520can%2520be%2520used%2520to%2520optimize%2520future%2520production.%2520This%2520is%2520especially%2520important%2520for%2520German%2520bakeries%2520because%2520freshly%2520baked%2520goods%2520have%2520a%2520very%2520short%2520shelf%2520life.%2520Automating%2520this%2520process%2520can%2520reduce%2520labor%2520costs%252C%2520improve%2520accuracy%252C%2520and%2520streamline%2520operations.%2520We%2520propose%2520automating%2520this%2520process%2520using%2520an%2520object%2520detection%2520model%2520to%2520identify%2520baked%2520goods%2520from%2520images.%2520However%252C%2520the%2520large%2520diversity%2520of%2520German%2520baked%2520goods%2520makes%2520fully%2520supervised%2520training%2520prohibitively%2520expensive%2520and%2520limits%2520scalability.%2520Although%2520open-vocabulary%2520detectors%2520%2528e.g.%252C%2520OWLv2%252C%2520Grounding%2520DINO%2529%2520offer%2520lexibility%252C%2520we%2520demonstrate%2520that%2520they%2520are%2520insufficient%2520for%2520our%2520task.%2520While%2520motivated%2520by%2520bakeries%252C%2520our%2520work%2520addresses%2520the%2520broader%2520challenges%2520of%2520deploying%2520computer%2520vision%2520in%2520industries%252C%2520where%2520tasks%2520are%2520specialized%2520and%2520annotated%2520datasets%2520are%2520scarce.%2520We%2520compile%2520dataset%2520splits%2520with%2520varying%2520supervision%2520levels%252C%2520covering%252019%2520classes%2520of%2520baked%2520goods.%2520We%2520propose%2520two%2520training%2520workflows%2520to%2520train%2520an%2520object%2520detection%2520model%2520with%2520limited%2520supervision.%2520First%252C%2520we%2520combine%2520OWLv2%2520and%2520Grounding%2520DINO%2520localization%2520with%2520image-level%2520supervision%2520to%2520train%2520the%2520model%2520in%2520a%2520weakly%2520supervised%2520manner.%2520Second%252C%2520we%2520improve%2520viewpoint%2520robustness%2520by%2520fine-tuning%2520on%2520video%2520frames%2520annotated%2520using%2520Segment%2520Anything%25202%2520as%2520a%2520pseudo-label%2520propagation%2520model.%2520Using%2520these%2520workflows%252C%2520we%2520train%2520YOLOv11%2520for%2520our%2520detection%2520task%2520due%2520to%2520its%2520favorable%2520speed%2520accuracy%2520tradeoff.%2520Relying%2520solely%2520on%2520image-level%2520supervision%252C%2520the%2520model%2520achieves%2520a%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520of%25200.91.%2520Finetuning%2520with%2520pseudo-labels%2520raises%2520model%2520performance%2520by%252019.3%2525%2520under%2520non-ideal%2520deployment%2520conditions.%2520Combining%2520these%2520workflows%2520trains%2520a%2520model%2520that%2520surpasses%2520our%2520fully-supervised%2520baseline%2520model%2520under%2520non-ideal%2520deployment%2520conditions%252C%2520despite%2520relying%2520only%2520on%2520image-level%2520supervision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Detect%20Baked%20Goods%20with%20Limited%20Supervision&entry.906535625=Thomas%20H.%20Schmitt%20and%20Maximilian%20Bundscherer%20and%20Tobias%20Bocklet&entry.1292438233=Monitoring%20leftover%20products%20provides%20valuable%20insights%20that%20can%20be%20used%20to%20optimize%20future%20production.%20This%20is%20especially%20important%20for%20German%20bakeries%20because%20freshly%20baked%20goods%20have%20a%20very%20short%20shelf%20life.%20Automating%20this%20process%20can%20reduce%20labor%20costs%2C%20improve%20accuracy%2C%20and%20streamline%20operations.%20We%20propose%20automating%20this%20process%20using%20an%20object%20detection%20model%20to%20identify%20baked%20goods%20from%20images.%20However%2C%20the%20large%20diversity%20of%20German%20baked%20goods%20makes%20fully%20supervised%20training%20prohibitively%20expensive%20and%20limits%20scalability.%20Although%20open-vocabulary%20detectors%20%28e.g.%2C%20OWLv2%2C%20Grounding%20DINO%29%20offer%20lexibility%2C%20we%20demonstrate%20that%20they%20are%20insufficient%20for%20our%20task.%20While%20motivated%20by%20bakeries%2C%20our%20work%20addresses%20the%20broader%20challenges%20of%20deploying%20computer%20vision%20in%20industries%2C%20where%20tasks%20are%20specialized%20and%20annotated%20datasets%20are%20scarce.%20We%20compile%20dataset%20splits%20with%20varying%20supervision%20levels%2C%20covering%2019%20classes%20of%20baked%20goods.%20We%20propose%20two%20training%20workflows%20to%20train%20an%20object%20detection%20model%20with%20limited%20supervision.%20First%2C%20we%20combine%20OWLv2%20and%20Grounding%20DINO%20localization%20with%20image-level%20supervision%20to%20train%20the%20model%20in%20a%20weakly%20supervised%20manner.%20Second%2C%20we%20improve%20viewpoint%20robustness%20by%20fine-tuning%20on%20video%20frames%20annotated%20using%20Segment%20Anything%202%20as%20a%20pseudo-label%20propagation%20model.%20Using%20these%20workflows%2C%20we%20train%20YOLOv11%20for%20our%20detection%20task%20due%20to%20its%20favorable%20speed%20accuracy%20tradeoff.%20Relying%20solely%20on%20image-level%20supervision%2C%20the%20model%20achieves%20a%20mean%20Average%20Precision%20%28mAP%29%20of%200.91.%20Finetuning%20with%20pseudo-labels%20raises%20model%20performance%20by%2019.3%25%20under%20non-ideal%20deployment%20conditions.%20Combining%20these%20workflows%20trains%20a%20model%20that%20surpasses%20our%20fully-supervised%20baseline%20model%20under%20non-ideal%20deployment%20conditions%2C%20despite%20relying%20only%20on%20image-level%20supervision.&entry.1838667208=http%3A//arxiv.org/abs/2602.09979v1&entry.124074799=Read"},
{"title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation", "author": "Yucheng Hu and Jianke Zhang and Yuanfei Luo and Yanjiang Guo and Xiaoyu Chen and Xinshu Sun and Kun Feng and Qingzhou Lu and Sheng Chen and Yangang Zhang and Wei Li and Jianyu Chen", "abstract": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.", "link": "http://arxiv.org/abs/2602.09849v1", "date": "2026-02-10", "relevancy": 2.7189, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BagelVLA%3A%20Enhancing%20Long-Horizon%20Manipulation%20via%20Interleaved%20Vision-Language-Action%20Generation&body=Title%3A%20BagelVLA%3A%20Enhancing%20Long-Horizon%20Manipulation%20via%20Interleaved%20Vision-Language-Action%20Generation%0AAuthor%3A%20Yucheng%20Hu%20and%20Jianke%20Zhang%20and%20Yuanfei%20Luo%20and%20Yanjiang%20Guo%20and%20Xiaoyu%20Chen%20and%20Xinshu%20Sun%20and%20Kun%20Feng%20and%20Qingzhou%20Lu%20and%20Sheng%20Chen%20and%20Yangang%20Zhang%20and%20Wei%20Li%20and%20Jianyu%20Chen%0AAbstract%3A%20Equipping%20embodied%20agents%20with%20the%20ability%20to%20reason%20about%20tasks%2C%20foresee%20physical%20outcomes%2C%20and%20generate%20precise%20actions%20is%20essential%20for%20general-purpose%20manipulation.%20While%20recent%20Vision-Language-Action%20%28VLA%29%20models%20have%20leveraged%20pre-trained%20foundation%20models%2C%20they%20typically%20focus%20on%20either%20linguistic%20planning%20or%20visual%20forecasting%20in%20isolation.%20These%20methods%20rarely%20integrate%20both%20capabilities%20simultaneously%20to%20guide%20action%20generation%2C%20leading%20to%20suboptimal%20performance%20in%20complex%2C%20long-horizon%20manipulation%20tasks.%20To%20bridge%20this%20gap%2C%20we%20propose%20BagelVLA%2C%20a%20unified%20model%20that%20integrates%20linguistic%20planning%2C%20visual%20forecasting%2C%20and%20action%20generation%20within%20a%20single%20framework.%20Initialized%20from%20a%20pretrained%20unified%20understanding%20and%20generative%20model%2C%20BagelVLA%20is%20trained%20to%20interleave%20textual%20reasoning%20and%20visual%20prediction%20directly%20into%20the%20action%20execution%20loop.%20To%20efficiently%20couple%20these%20modalities%2C%20we%20introduce%20Residual%20Flow%20Guidance%20%28RFG%29%2C%20which%20initializes%20from%20current%20observation%20and%20leverages%20single-step%20denoising%20to%20extract%20predictive%20visual%20features%2C%20guiding%20action%20generation%20with%20minimal%20latency.%20Extensive%20experiments%20demonstrate%20that%20BagelVLA%20outperforms%20existing%20baselines%20by%20a%20significant%20margin%20on%20multiple%20simulated%20and%20real-world%20benchmarks%2C%20particularly%20in%20tasks%20requiring%20multi-stage%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBagelVLA%253A%2520Enhancing%2520Long-Horizon%2520Manipulation%2520via%2520Interleaved%2520Vision-Language-Action%2520Generation%26entry.906535625%3DYucheng%2520Hu%2520and%2520Jianke%2520Zhang%2520and%2520Yuanfei%2520Luo%2520and%2520Yanjiang%2520Guo%2520and%2520Xiaoyu%2520Chen%2520and%2520Xinshu%2520Sun%2520and%2520Kun%2520Feng%2520and%2520Qingzhou%2520Lu%2520and%2520Sheng%2520Chen%2520and%2520Yangang%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Jianyu%2520Chen%26entry.1292438233%3DEquipping%2520embodied%2520agents%2520with%2520the%2520ability%2520to%2520reason%2520about%2520tasks%252C%2520foresee%2520physical%2520outcomes%252C%2520and%2520generate%2520precise%2520actions%2520is%2520essential%2520for%2520general-purpose%2520manipulation.%2520While%2520recent%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520leveraged%2520pre-trained%2520foundation%2520models%252C%2520they%2520typically%2520focus%2520on%2520either%2520linguistic%2520planning%2520or%2520visual%2520forecasting%2520in%2520isolation.%2520These%2520methods%2520rarely%2520integrate%2520both%2520capabilities%2520simultaneously%2520to%2520guide%2520action%2520generation%252C%2520leading%2520to%2520suboptimal%2520performance%2520in%2520complex%252C%2520long-horizon%2520manipulation%2520tasks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520BagelVLA%252C%2520a%2520unified%2520model%2520that%2520integrates%2520linguistic%2520planning%252C%2520visual%2520forecasting%252C%2520and%2520action%2520generation%2520within%2520a%2520single%2520framework.%2520Initialized%2520from%2520a%2520pretrained%2520unified%2520understanding%2520and%2520generative%2520model%252C%2520BagelVLA%2520is%2520trained%2520to%2520interleave%2520textual%2520reasoning%2520and%2520visual%2520prediction%2520directly%2520into%2520the%2520action%2520execution%2520loop.%2520To%2520efficiently%2520couple%2520these%2520modalities%252C%2520we%2520introduce%2520Residual%2520Flow%2520Guidance%2520%2528RFG%2529%252C%2520which%2520initializes%2520from%2520current%2520observation%2520and%2520leverages%2520single-step%2520denoising%2520to%2520extract%2520predictive%2520visual%2520features%252C%2520guiding%2520action%2520generation%2520with%2520minimal%2520latency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520BagelVLA%2520outperforms%2520existing%2520baselines%2520by%2520a%2520significant%2520margin%2520on%2520multiple%2520simulated%2520and%2520real-world%2520benchmarks%252C%2520particularly%2520in%2520tasks%2520requiring%2520multi-stage%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BagelVLA%3A%20Enhancing%20Long-Horizon%20Manipulation%20via%20Interleaved%20Vision-Language-Action%20Generation&entry.906535625=Yucheng%20Hu%20and%20Jianke%20Zhang%20and%20Yuanfei%20Luo%20and%20Yanjiang%20Guo%20and%20Xiaoyu%20Chen%20and%20Xinshu%20Sun%20and%20Kun%20Feng%20and%20Qingzhou%20Lu%20and%20Sheng%20Chen%20and%20Yangang%20Zhang%20and%20Wei%20Li%20and%20Jianyu%20Chen&entry.1292438233=Equipping%20embodied%20agents%20with%20the%20ability%20to%20reason%20about%20tasks%2C%20foresee%20physical%20outcomes%2C%20and%20generate%20precise%20actions%20is%20essential%20for%20general-purpose%20manipulation.%20While%20recent%20Vision-Language-Action%20%28VLA%29%20models%20have%20leveraged%20pre-trained%20foundation%20models%2C%20they%20typically%20focus%20on%20either%20linguistic%20planning%20or%20visual%20forecasting%20in%20isolation.%20These%20methods%20rarely%20integrate%20both%20capabilities%20simultaneously%20to%20guide%20action%20generation%2C%20leading%20to%20suboptimal%20performance%20in%20complex%2C%20long-horizon%20manipulation%20tasks.%20To%20bridge%20this%20gap%2C%20we%20propose%20BagelVLA%2C%20a%20unified%20model%20that%20integrates%20linguistic%20planning%2C%20visual%20forecasting%2C%20and%20action%20generation%20within%20a%20single%20framework.%20Initialized%20from%20a%20pretrained%20unified%20understanding%20and%20generative%20model%2C%20BagelVLA%20is%20trained%20to%20interleave%20textual%20reasoning%20and%20visual%20prediction%20directly%20into%20the%20action%20execution%20loop.%20To%20efficiently%20couple%20these%20modalities%2C%20we%20introduce%20Residual%20Flow%20Guidance%20%28RFG%29%2C%20which%20initializes%20from%20current%20observation%20and%20leverages%20single-step%20denoising%20to%20extract%20predictive%20visual%20features%2C%20guiding%20action%20generation%20with%20minimal%20latency.%20Extensive%20experiments%20demonstrate%20that%20BagelVLA%20outperforms%20existing%20baselines%20by%20a%20significant%20margin%20on%20multiple%20simulated%20and%20real-world%20benchmarks%2C%20particularly%20in%20tasks%20requiring%20multi-stage%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.09849v1&entry.124074799=Read"},
{"title": "Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints", "author": "Andres Saurez and Yousung Lee and Dongsoo Har", "abstract": "Linear probes and sparse autoencoders consistently recover meaningful structure from transformer representations -- yet why should such simple methods succeed in deep, nonlinear systems? We show this is not merely an empirical regularity but a consequence of architectural necessity: transformers communicate information through linear interfaces (attention OV circuits, unembedding matrices), and any semantic feature decoded through such an interface must occupy a context-invariant linear subspace. We formalize this as the \\emph{Invariant Subspace Necessity} theorem and derive the \\emph{Self-Reference Property}: tokens directly provide the geometric direction for their associated features, enabling zero-shot identification of semantic structure without labeled data or learned probes. Empirical validation in eight classification tasks and four model families confirms the alignment between class tokens and semantically related instances. Our framework provides \\textbf{a principled architectural explanation} for why linear interpretability methods work, unifying linear probes and sparse autoencoders.", "link": "http://arxiv.org/abs/2602.09783v1", "date": "2026-02-10", "relevancy": 2.7094, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Linear%20Interpretability%20Works%3A%20Invariant%20Subspaces%20as%20a%20Result%20of%20Architectural%20Constraints&body=Title%3A%20Why%20Linear%20Interpretability%20Works%3A%20Invariant%20Subspaces%20as%20a%20Result%20of%20Architectural%20Constraints%0AAuthor%3A%20Andres%20Saurez%20and%20Yousung%20Lee%20and%20Dongsoo%20Har%0AAbstract%3A%20Linear%20probes%20and%20sparse%20autoencoders%20consistently%20recover%20meaningful%20structure%20from%20transformer%20representations%20--%20yet%20why%20should%20such%20simple%20methods%20succeed%20in%20deep%2C%20nonlinear%20systems%3F%20We%20show%20this%20is%20not%20merely%20an%20empirical%20regularity%20but%20a%20consequence%20of%20architectural%20necessity%3A%20transformers%20communicate%20information%20through%20linear%20interfaces%20%28attention%20OV%20circuits%2C%20unembedding%20matrices%29%2C%20and%20any%20semantic%20feature%20decoded%20through%20such%20an%20interface%20must%20occupy%20a%20context-invariant%20linear%20subspace.%20We%20formalize%20this%20as%20the%20%5Cemph%7BInvariant%20Subspace%20Necessity%7D%20theorem%20and%20derive%20the%20%5Cemph%7BSelf-Reference%20Property%7D%3A%20tokens%20directly%20provide%20the%20geometric%20direction%20for%20their%20associated%20features%2C%20enabling%20zero-shot%20identification%20of%20semantic%20structure%20without%20labeled%20data%20or%20learned%20probes.%20Empirical%20validation%20in%20eight%20classification%20tasks%20and%20four%20model%20families%20confirms%20the%20alignment%20between%20class%20tokens%20and%20semantically%20related%20instances.%20Our%20framework%20provides%20%5Ctextbf%7Ba%20principled%20architectural%20explanation%7D%20for%20why%20linear%20interpretability%20methods%20work%2C%20unifying%20linear%20probes%20and%20sparse%20autoencoders.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Linear%2520Interpretability%2520Works%253A%2520Invariant%2520Subspaces%2520as%2520a%2520Result%2520of%2520Architectural%2520Constraints%26entry.906535625%3DAndres%2520Saurez%2520and%2520Yousung%2520Lee%2520and%2520Dongsoo%2520Har%26entry.1292438233%3DLinear%2520probes%2520and%2520sparse%2520autoencoders%2520consistently%2520recover%2520meaningful%2520structure%2520from%2520transformer%2520representations%2520--%2520yet%2520why%2520should%2520such%2520simple%2520methods%2520succeed%2520in%2520deep%252C%2520nonlinear%2520systems%253F%2520We%2520show%2520this%2520is%2520not%2520merely%2520an%2520empirical%2520regularity%2520but%2520a%2520consequence%2520of%2520architectural%2520necessity%253A%2520transformers%2520communicate%2520information%2520through%2520linear%2520interfaces%2520%2528attention%2520OV%2520circuits%252C%2520unembedding%2520matrices%2529%252C%2520and%2520any%2520semantic%2520feature%2520decoded%2520through%2520such%2520an%2520interface%2520must%2520occupy%2520a%2520context-invariant%2520linear%2520subspace.%2520We%2520formalize%2520this%2520as%2520the%2520%255Cemph%257BInvariant%2520Subspace%2520Necessity%257D%2520theorem%2520and%2520derive%2520the%2520%255Cemph%257BSelf-Reference%2520Property%257D%253A%2520tokens%2520directly%2520provide%2520the%2520geometric%2520direction%2520for%2520their%2520associated%2520features%252C%2520enabling%2520zero-shot%2520identification%2520of%2520semantic%2520structure%2520without%2520labeled%2520data%2520or%2520learned%2520probes.%2520Empirical%2520validation%2520in%2520eight%2520classification%2520tasks%2520and%2520four%2520model%2520families%2520confirms%2520the%2520alignment%2520between%2520class%2520tokens%2520and%2520semantically%2520related%2520instances.%2520Our%2520framework%2520provides%2520%255Ctextbf%257Ba%2520principled%2520architectural%2520explanation%257D%2520for%2520why%2520linear%2520interpretability%2520methods%2520work%252C%2520unifying%2520linear%2520probes%2520and%2520sparse%2520autoencoders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Linear%20Interpretability%20Works%3A%20Invariant%20Subspaces%20as%20a%20Result%20of%20Architectural%20Constraints&entry.906535625=Andres%20Saurez%20and%20Yousung%20Lee%20and%20Dongsoo%20Har&entry.1292438233=Linear%20probes%20and%20sparse%20autoencoders%20consistently%20recover%20meaningful%20structure%20from%20transformer%20representations%20--%20yet%20why%20should%20such%20simple%20methods%20succeed%20in%20deep%2C%20nonlinear%20systems%3F%20We%20show%20this%20is%20not%20merely%20an%20empirical%20regularity%20but%20a%20consequence%20of%20architectural%20necessity%3A%20transformers%20communicate%20information%20through%20linear%20interfaces%20%28attention%20OV%20circuits%2C%20unembedding%20matrices%29%2C%20and%20any%20semantic%20feature%20decoded%20through%20such%20an%20interface%20must%20occupy%20a%20context-invariant%20linear%20subspace.%20We%20formalize%20this%20as%20the%20%5Cemph%7BInvariant%20Subspace%20Necessity%7D%20theorem%20and%20derive%20the%20%5Cemph%7BSelf-Reference%20Property%7D%3A%20tokens%20directly%20provide%20the%20geometric%20direction%20for%20their%20associated%20features%2C%20enabling%20zero-shot%20identification%20of%20semantic%20structure%20without%20labeled%20data%20or%20learned%20probes.%20Empirical%20validation%20in%20eight%20classification%20tasks%20and%20four%20model%20families%20confirms%20the%20alignment%20between%20class%20tokens%20and%20semantically%20related%20instances.%20Our%20framework%20provides%20%5Ctextbf%7Ba%20principled%20architectural%20explanation%7D%20for%20why%20linear%20interpretability%20methods%20work%2C%20unifying%20linear%20probes%20and%20sparse%20autoencoders.&entry.1838667208=http%3A//arxiv.org/abs/2602.09783v1&entry.124074799=Read"},
{"title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model", "author": "Jingwen Sun and Wenyao Zhang and Zekun Qi and Shaojie Ren and Zezhi Liu and Hanxin Zhu and Guangzhong Sun and Xin Jin and Zhibo Chen", "abstract": "Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \\emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.", "link": "http://arxiv.org/abs/2602.10098v1", "date": "2026-02-10", "relevancy": 2.6669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA-JEPA%3A%20Enhancing%20Vision-Language-Action%20Model%20with%20Latent%20World%20Model&body=Title%3A%20VLA-JEPA%3A%20Enhancing%20Vision-Language-Action%20Model%20with%20Latent%20World%20Model%0AAuthor%3A%20Jingwen%20Sun%20and%20Wenyao%20Zhang%20and%20Zekun%20Qi%20and%20Shaojie%20Ren%20and%20Zezhi%20Liu%20and%20Hanxin%20Zhu%20and%20Guangzhong%20Sun%20and%20Xin%20Jin%20and%20Zhibo%20Chen%0AAbstract%3A%20Pretraining%20Vision-Language-Action%20%28VLA%29%20policies%20on%20internet-scale%20video%20is%20appealing%2C%20yet%20current%20latent-action%20objectives%20often%20learn%20the%20wrong%20thing%3A%20they%20remain%20anchored%20to%20pixel%20variation%20rather%20than%20action-relevant%20state%20transitions%2C%20making%20them%20vulnerable%20to%20appearance%20bias%2C%20nuisance%20motion%2C%20and%20information%20leakage.%20We%20introduce%20VLA-JEPA%2C%20a%20JEPA-style%20pretraining%20framework%20that%20sidesteps%20these%20pitfalls%20by%20design.%20The%20key%20idea%20is%20%5Cemph%7Bleakage-free%20state%20prediction%7D%3A%20a%20target%20encoder%20produces%20latent%20representations%20from%20future%20frames%2C%20while%20the%20student%20pathway%20sees%20only%20the%20current%20observation%20--%20future%20information%20is%20used%20solely%20as%20supervision%20targets%2C%20never%20as%20input.%20By%20predicting%20in%20latent%20space%20rather%20than%20pixel%20space%2C%20VLA-JEPA%20learns%20dynamics%20abstractions%20that%20are%20robust%20to%20camera%20motion%20and%20irrelevant%20background%20changes.%20This%20yields%20a%20simple%20two-stage%20recipe%20--%20JEPA%20pretraining%20followed%20by%20action-head%20fine-tuning%20--%20without%20the%20multi-stage%20complexity%20of%20prior%20latent-action%20pipelines.%20Experiments%20on%20LIBERO%2C%20LIBERO-Plus%2C%20SimplerEnv%20and%20real-world%20manipulation%20tasks%20show%20that%20VLA-JEPA%20achieves%20consistent%20gains%20in%20generalization%20and%20robustness%20over%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA-JEPA%253A%2520Enhancing%2520Vision-Language-Action%2520Model%2520with%2520Latent%2520World%2520Model%26entry.906535625%3DJingwen%2520Sun%2520and%2520Wenyao%2520Zhang%2520and%2520Zekun%2520Qi%2520and%2520Shaojie%2520Ren%2520and%2520Zezhi%2520Liu%2520and%2520Hanxin%2520Zhu%2520and%2520Guangzhong%2520Sun%2520and%2520Xin%2520Jin%2520and%2520Zhibo%2520Chen%26entry.1292438233%3DPretraining%2520Vision-Language-Action%2520%2528VLA%2529%2520policies%2520on%2520internet-scale%2520video%2520is%2520appealing%252C%2520yet%2520current%2520latent-action%2520objectives%2520often%2520learn%2520the%2520wrong%2520thing%253A%2520they%2520remain%2520anchored%2520to%2520pixel%2520variation%2520rather%2520than%2520action-relevant%2520state%2520transitions%252C%2520making%2520them%2520vulnerable%2520to%2520appearance%2520bias%252C%2520nuisance%2520motion%252C%2520and%2520information%2520leakage.%2520We%2520introduce%2520VLA-JEPA%252C%2520a%2520JEPA-style%2520pretraining%2520framework%2520that%2520sidesteps%2520these%2520pitfalls%2520by%2520design.%2520The%2520key%2520idea%2520is%2520%255Cemph%257Bleakage-free%2520state%2520prediction%257D%253A%2520a%2520target%2520encoder%2520produces%2520latent%2520representations%2520from%2520future%2520frames%252C%2520while%2520the%2520student%2520pathway%2520sees%2520only%2520the%2520current%2520observation%2520--%2520future%2520information%2520is%2520used%2520solely%2520as%2520supervision%2520targets%252C%2520never%2520as%2520input.%2520By%2520predicting%2520in%2520latent%2520space%2520rather%2520than%2520pixel%2520space%252C%2520VLA-JEPA%2520learns%2520dynamics%2520abstractions%2520that%2520are%2520robust%2520to%2520camera%2520motion%2520and%2520irrelevant%2520background%2520changes.%2520This%2520yields%2520a%2520simple%2520two-stage%2520recipe%2520--%2520JEPA%2520pretraining%2520followed%2520by%2520action-head%2520fine-tuning%2520--%2520without%2520the%2520multi-stage%2520complexity%2520of%2520prior%2520latent-action%2520pipelines.%2520Experiments%2520on%2520LIBERO%252C%2520LIBERO-Plus%252C%2520SimplerEnv%2520and%2520real-world%2520manipulation%2520tasks%2520show%2520that%2520VLA-JEPA%2520achieves%2520consistent%2520gains%2520in%2520generalization%2520and%2520robustness%2520over%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA-JEPA%3A%20Enhancing%20Vision-Language-Action%20Model%20with%20Latent%20World%20Model&entry.906535625=Jingwen%20Sun%20and%20Wenyao%20Zhang%20and%20Zekun%20Qi%20and%20Shaojie%20Ren%20and%20Zezhi%20Liu%20and%20Hanxin%20Zhu%20and%20Guangzhong%20Sun%20and%20Xin%20Jin%20and%20Zhibo%20Chen&entry.1292438233=Pretraining%20Vision-Language-Action%20%28VLA%29%20policies%20on%20internet-scale%20video%20is%20appealing%2C%20yet%20current%20latent-action%20objectives%20often%20learn%20the%20wrong%20thing%3A%20they%20remain%20anchored%20to%20pixel%20variation%20rather%20than%20action-relevant%20state%20transitions%2C%20making%20them%20vulnerable%20to%20appearance%20bias%2C%20nuisance%20motion%2C%20and%20information%20leakage.%20We%20introduce%20VLA-JEPA%2C%20a%20JEPA-style%20pretraining%20framework%20that%20sidesteps%20these%20pitfalls%20by%20design.%20The%20key%20idea%20is%20%5Cemph%7Bleakage-free%20state%20prediction%7D%3A%20a%20target%20encoder%20produces%20latent%20representations%20from%20future%20frames%2C%20while%20the%20student%20pathway%20sees%20only%20the%20current%20observation%20--%20future%20information%20is%20used%20solely%20as%20supervision%20targets%2C%20never%20as%20input.%20By%20predicting%20in%20latent%20space%20rather%20than%20pixel%20space%2C%20VLA-JEPA%20learns%20dynamics%20abstractions%20that%20are%20robust%20to%20camera%20motion%20and%20irrelevant%20background%20changes.%20This%20yields%20a%20simple%20two-stage%20recipe%20--%20JEPA%20pretraining%20followed%20by%20action-head%20fine-tuning%20--%20without%20the%20multi-stage%20complexity%20of%20prior%20latent-action%20pipelines.%20Experiments%20on%20LIBERO%2C%20LIBERO-Plus%2C%20SimplerEnv%20and%20real-world%20manipulation%20tasks%20show%20that%20VLA-JEPA%20achieves%20consistent%20gains%20in%20generalization%20and%20robustness%20over%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.10098v1&entry.124074799=Read"},
{"title": "E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs", "author": "Xianjie Liu and Yiman Hu and Liang Wu and Ping Hu and Yixiong Zou and Jian Xu and Bo Zheng", "abstract": "E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a multi-modal information density assessment framework to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce E-commerce Video Ads Benchmark (E-VAds), which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop E-VAds-R1, an RL-based reasoning model featuring a multi-grained reward design called MG-GRPO. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.", "link": "http://arxiv.org/abs/2602.08355v2", "date": "2026-02-10", "relevancy": 2.6617, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5348}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5348}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-VAds%3A%20An%20E-commerce%20Short%20Videos%20Understanding%20Benchmark%20for%20MLLMs&body=Title%3A%20E-VAds%3A%20An%20E-commerce%20Short%20Videos%20Understanding%20Benchmark%20for%20MLLMs%0AAuthor%3A%20Xianjie%20Liu%20and%20Yiman%20Hu%20and%20Liang%20Wu%20and%20Ping%20Hu%20and%20Yixiong%20Zou%20and%20Jian%20Xu%20and%20Bo%20Zheng%0AAbstract%3A%20E-commerce%20short%20videos%20represent%20a%20high-revenue%20segment%20of%20the%20online%20video%20industry%20characterized%20by%20a%20goal-driven%20format%20and%20dense%20multi-modal%20signals.%20Current%20models%20often%20struggle%20with%20these%20videos%20because%20existing%20benchmarks%20focus%20primarily%20on%20general-purpose%20tasks%20and%20neglect%20the%20reasoning%20of%20commercial%20intent.%20In%20this%20work%2C%20we%20first%20propose%20a%20multi-modal%20information%20density%20assessment%20framework%20to%20quantify%20the%20complexity%20of%20this%20domain.%20Our%20evaluation%20reveals%20that%20e-commerce%20content%20exhibits%20substantially%20higher%20density%20across%20visual%2C%20audio%2C%20and%20textual%20modalities%20compared%20to%20mainstream%20datasets%2C%20establishing%20a%20more%20challenging%20frontier%20for%20video%20understanding.%20To%20address%20this%20gap%2C%20we%20introduce%20E-commerce%20Video%20Ads%20Benchmark%20%28E-VAds%29%2C%20which%20is%20the%20first%20benchmark%20specifically%20designed%20for%20e-commerce%20short%20video%20understanding.%20We%20curated%203%2C961%20high-quality%20videos%20from%20Taobao%20covering%20a%20wide%20range%20of%20product%20categories%20and%20used%20a%20multi-agent%20system%20to%20generate%2019%2C785%20open-ended%20Q%26A%20pairs.%20These%20questions%20are%20organized%20into%20two%20primary%20dimensions%2C%20namely%20Perception%20and%20Cognition%20and%20Reasoning%2C%20which%20consist%20of%20five%20distinct%20tasks.%20Finally%2C%20we%20develop%20E-VAds-R1%2C%20an%20RL-based%20reasoning%20model%20featuring%20a%20multi-grained%20reward%20design%20called%20MG-GRPO.%20This%20strategy%20provides%20smooth%20guidance%20for%20early%20exploration%20while%20creating%20a%20non-linear%20incentive%20for%20expert-level%20precision.%20Experimental%20results%20demonstrate%20that%20E-VAds-R1%20achieves%20a%20109.2%25%20performance%20gain%20in%20commercial%20intent%20reasoning%20with%20only%20a%20few%20hundred%20training%20samples.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-VAds%253A%2520An%2520E-commerce%2520Short%2520Videos%2520Understanding%2520Benchmark%2520for%2520MLLMs%26entry.906535625%3DXianjie%2520Liu%2520and%2520Yiman%2520Hu%2520and%2520Liang%2520Wu%2520and%2520Ping%2520Hu%2520and%2520Yixiong%2520Zou%2520and%2520Jian%2520Xu%2520and%2520Bo%2520Zheng%26entry.1292438233%3DE-commerce%2520short%2520videos%2520represent%2520a%2520high-revenue%2520segment%2520of%2520the%2520online%2520video%2520industry%2520characterized%2520by%2520a%2520goal-driven%2520format%2520and%2520dense%2520multi-modal%2520signals.%2520Current%2520models%2520often%2520struggle%2520with%2520these%2520videos%2520because%2520existing%2520benchmarks%2520focus%2520primarily%2520on%2520general-purpose%2520tasks%2520and%2520neglect%2520the%2520reasoning%2520of%2520commercial%2520intent.%2520In%2520this%2520work%252C%2520we%2520first%2520propose%2520a%2520multi-modal%2520information%2520density%2520assessment%2520framework%2520to%2520quantify%2520the%2520complexity%2520of%2520this%2520domain.%2520Our%2520evaluation%2520reveals%2520that%2520e-commerce%2520content%2520exhibits%2520substantially%2520higher%2520density%2520across%2520visual%252C%2520audio%252C%2520and%2520textual%2520modalities%2520compared%2520to%2520mainstream%2520datasets%252C%2520establishing%2520a%2520more%2520challenging%2520frontier%2520for%2520video%2520understanding.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520E-commerce%2520Video%2520Ads%2520Benchmark%2520%2528E-VAds%2529%252C%2520which%2520is%2520the%2520first%2520benchmark%2520specifically%2520designed%2520for%2520e-commerce%2520short%2520video%2520understanding.%2520We%2520curated%25203%252C961%2520high-quality%2520videos%2520from%2520Taobao%2520covering%2520a%2520wide%2520range%2520of%2520product%2520categories%2520and%2520used%2520a%2520multi-agent%2520system%2520to%2520generate%252019%252C785%2520open-ended%2520Q%2526A%2520pairs.%2520These%2520questions%2520are%2520organized%2520into%2520two%2520primary%2520dimensions%252C%2520namely%2520Perception%2520and%2520Cognition%2520and%2520Reasoning%252C%2520which%2520consist%2520of%2520five%2520distinct%2520tasks.%2520Finally%252C%2520we%2520develop%2520E-VAds-R1%252C%2520an%2520RL-based%2520reasoning%2520model%2520featuring%2520a%2520multi-grained%2520reward%2520design%2520called%2520MG-GRPO.%2520This%2520strategy%2520provides%2520smooth%2520guidance%2520for%2520early%2520exploration%2520while%2520creating%2520a%2520non-linear%2520incentive%2520for%2520expert-level%2520precision.%2520Experimental%2520results%2520demonstrate%2520that%2520E-VAds-R1%2520achieves%2520a%2520109.2%2525%2520performance%2520gain%2520in%2520commercial%2520intent%2520reasoning%2520with%2520only%2520a%2520few%2520hundred%2520training%2520samples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-VAds%3A%20An%20E-commerce%20Short%20Videos%20Understanding%20Benchmark%20for%20MLLMs&entry.906535625=Xianjie%20Liu%20and%20Yiman%20Hu%20and%20Liang%20Wu%20and%20Ping%20Hu%20and%20Yixiong%20Zou%20and%20Jian%20Xu%20and%20Bo%20Zheng&entry.1292438233=E-commerce%20short%20videos%20represent%20a%20high-revenue%20segment%20of%20the%20online%20video%20industry%20characterized%20by%20a%20goal-driven%20format%20and%20dense%20multi-modal%20signals.%20Current%20models%20often%20struggle%20with%20these%20videos%20because%20existing%20benchmarks%20focus%20primarily%20on%20general-purpose%20tasks%20and%20neglect%20the%20reasoning%20of%20commercial%20intent.%20In%20this%20work%2C%20we%20first%20propose%20a%20multi-modal%20information%20density%20assessment%20framework%20to%20quantify%20the%20complexity%20of%20this%20domain.%20Our%20evaluation%20reveals%20that%20e-commerce%20content%20exhibits%20substantially%20higher%20density%20across%20visual%2C%20audio%2C%20and%20textual%20modalities%20compared%20to%20mainstream%20datasets%2C%20establishing%20a%20more%20challenging%20frontier%20for%20video%20understanding.%20To%20address%20this%20gap%2C%20we%20introduce%20E-commerce%20Video%20Ads%20Benchmark%20%28E-VAds%29%2C%20which%20is%20the%20first%20benchmark%20specifically%20designed%20for%20e-commerce%20short%20video%20understanding.%20We%20curated%203%2C961%20high-quality%20videos%20from%20Taobao%20covering%20a%20wide%20range%20of%20product%20categories%20and%20used%20a%20multi-agent%20system%20to%20generate%2019%2C785%20open-ended%20Q%26A%20pairs.%20These%20questions%20are%20organized%20into%20two%20primary%20dimensions%2C%20namely%20Perception%20and%20Cognition%20and%20Reasoning%2C%20which%20consist%20of%20five%20distinct%20tasks.%20Finally%2C%20we%20develop%20E-VAds-R1%2C%20an%20RL-based%20reasoning%20model%20featuring%20a%20multi-grained%20reward%20design%20called%20MG-GRPO.%20This%20strategy%20provides%20smooth%20guidance%20for%20early%20exploration%20while%20creating%20a%20non-linear%20incentive%20for%20expert-level%20precision.%20Experimental%20results%20demonstrate%20that%20E-VAds-R1%20achieves%20a%20109.2%25%20performance%20gain%20in%20commercial%20intent%20reasoning%20with%20only%20a%20few%20hundred%20training%20samples.&entry.1838667208=http%3A//arxiv.org/abs/2602.08355v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning Based on Transformed Image Reconstruction for Equivariance-Coherent Feature Representation", "author": "Qin Wang and Alessio Quercia and Benjamin Bruns and Abigail Morrison and Hanno Scharr and Kai Krajsek", "abstract": "Self-supervised learning (SSL) methods have achieved remarkable success in learning image representations allowing invariances in them - but therefore discarding transformation information that some computer vision tasks actually require. While recent approaches attempt to address this limitation by learning equivariant features using linear operators in feature space, they impose restrictive assumptions that constrain flexibility and generalization. We introduce a weaker definition for the transformation relation between image and feature space denoted as equivariance-coherence. We propose a novel SSL auxiliary task that learns equivariance-coherent representations through intermediate transformation reconstruction, which can be integrated with existing joint embedding SSL methods. Our key idea is to reconstruct images at intermediate points along transformation paths, e.g. when training on 30-degree rotations, we reconstruct the 10-degree and 20-degree rotation states. Reconstructing intermediate states requires the transformation information used in augmentations, rather than suppressing it, and therefore fosters features containing the augmented transformation information. Our method decomposes feature vectors into invariant and equivariant parts, training them with standard SSL losses and reconstruction losses, respectively. We demonstrate substantial improvements on synthetic equivariance benchmarks while maintaining competitive performance on downstream tasks requiring invariant representations. The approach seamlessly integrates with existing SSL methods (iBOT, DINOv2) and consistently enhances performance across diverse tasks, including segmentation, detection, depth estimation, and video dense prediction. Our framework provides a practical way for augmenting SSL methods with equivariant capabilities while preserving invariant performance.", "link": "http://arxiv.org/abs/2503.18753v2", "date": "2026-02-10", "relevancy": 2.6585, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5512}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5235}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20Based%20on%20Transformed%20Image%20Reconstruction%20for%20Equivariance-Coherent%20Feature%20Representation&body=Title%3A%20Self-Supervised%20Learning%20Based%20on%20Transformed%20Image%20Reconstruction%20for%20Equivariance-Coherent%20Feature%20Representation%0AAuthor%3A%20Qin%20Wang%20and%20Alessio%20Quercia%20and%20Benjamin%20Bruns%20and%20Abigail%20Morrison%20and%20Hanno%20Scharr%20and%20Kai%20Krajsek%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20methods%20have%20achieved%20remarkable%20success%20in%20learning%20image%20representations%20allowing%20invariances%20in%20them%20-%20but%20therefore%20discarding%20transformation%20information%20that%20some%20computer%20vision%20tasks%20actually%20require.%20While%20recent%20approaches%20attempt%20to%20address%20this%20limitation%20by%20learning%20equivariant%20features%20using%20linear%20operators%20in%20feature%20space%2C%20they%20impose%20restrictive%20assumptions%20that%20constrain%20flexibility%20and%20generalization.%20We%20introduce%20a%20weaker%20definition%20for%20the%20transformation%20relation%20between%20image%20and%20feature%20space%20denoted%20as%20equivariance-coherence.%20We%20propose%20a%20novel%20SSL%20auxiliary%20task%20that%20learns%20equivariance-coherent%20representations%20through%20intermediate%20transformation%20reconstruction%2C%20which%20can%20be%20integrated%20with%20existing%20joint%20embedding%20SSL%20methods.%20Our%20key%20idea%20is%20to%20reconstruct%20images%20at%20intermediate%20points%20along%20transformation%20paths%2C%20e.g.%20when%20training%20on%2030-degree%20rotations%2C%20we%20reconstruct%20the%2010-degree%20and%2020-degree%20rotation%20states.%20Reconstructing%20intermediate%20states%20requires%20the%20transformation%20information%20used%20in%20augmentations%2C%20rather%20than%20suppressing%20it%2C%20and%20therefore%20fosters%20features%20containing%20the%20augmented%20transformation%20information.%20Our%20method%20decomposes%20feature%20vectors%20into%20invariant%20and%20equivariant%20parts%2C%20training%20them%20with%20standard%20SSL%20losses%20and%20reconstruction%20losses%2C%20respectively.%20We%20demonstrate%20substantial%20improvements%20on%20synthetic%20equivariance%20benchmarks%20while%20maintaining%20competitive%20performance%20on%20downstream%20tasks%20requiring%20invariant%20representations.%20The%20approach%20seamlessly%20integrates%20with%20existing%20SSL%20methods%20%28iBOT%2C%20DINOv2%29%20and%20consistently%20enhances%20performance%20across%20diverse%20tasks%2C%20including%20segmentation%2C%20detection%2C%20depth%20estimation%2C%20and%20video%20dense%20prediction.%20Our%20framework%20provides%20a%20practical%20way%20for%20augmenting%20SSL%20methods%20with%20equivariant%20capabilities%20while%20preserving%20invariant%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2503.18753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520Based%2520on%2520Transformed%2520Image%2520Reconstruction%2520for%2520Equivariance-Coherent%2520Feature%2520Representation%26entry.906535625%3DQin%2520Wang%2520and%2520Alessio%2520Quercia%2520and%2520Benjamin%2520Bruns%2520and%2520Abigail%2520Morrison%2520and%2520Hanno%2520Scharr%2520and%2520Kai%2520Krajsek%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520methods%2520have%2520achieved%2520remarkable%2520success%2520in%2520learning%2520image%2520representations%2520allowing%2520invariances%2520in%2520them%2520-%2520but%2520therefore%2520discarding%2520transformation%2520information%2520that%2520some%2520computer%2520vision%2520tasks%2520actually%2520require.%2520While%2520recent%2520approaches%2520attempt%2520to%2520address%2520this%2520limitation%2520by%2520learning%2520equivariant%2520features%2520using%2520linear%2520operators%2520in%2520feature%2520space%252C%2520they%2520impose%2520restrictive%2520assumptions%2520that%2520constrain%2520flexibility%2520and%2520generalization.%2520We%2520introduce%2520a%2520weaker%2520definition%2520for%2520the%2520transformation%2520relation%2520between%2520image%2520and%2520feature%2520space%2520denoted%2520as%2520equivariance-coherence.%2520We%2520propose%2520a%2520novel%2520SSL%2520auxiliary%2520task%2520that%2520learns%2520equivariance-coherent%2520representations%2520through%2520intermediate%2520transformation%2520reconstruction%252C%2520which%2520can%2520be%2520integrated%2520with%2520existing%2520joint%2520embedding%2520SSL%2520methods.%2520Our%2520key%2520idea%2520is%2520to%2520reconstruct%2520images%2520at%2520intermediate%2520points%2520along%2520transformation%2520paths%252C%2520e.g.%2520when%2520training%2520on%252030-degree%2520rotations%252C%2520we%2520reconstruct%2520the%252010-degree%2520and%252020-degree%2520rotation%2520states.%2520Reconstructing%2520intermediate%2520states%2520requires%2520the%2520transformation%2520information%2520used%2520in%2520augmentations%252C%2520rather%2520than%2520suppressing%2520it%252C%2520and%2520therefore%2520fosters%2520features%2520containing%2520the%2520augmented%2520transformation%2520information.%2520Our%2520method%2520decomposes%2520feature%2520vectors%2520into%2520invariant%2520and%2520equivariant%2520parts%252C%2520training%2520them%2520with%2520standard%2520SSL%2520losses%2520and%2520reconstruction%2520losses%252C%2520respectively.%2520We%2520demonstrate%2520substantial%2520improvements%2520on%2520synthetic%2520equivariance%2520benchmarks%2520while%2520maintaining%2520competitive%2520performance%2520on%2520downstream%2520tasks%2520requiring%2520invariant%2520representations.%2520The%2520approach%2520seamlessly%2520integrates%2520with%2520existing%2520SSL%2520methods%2520%2528iBOT%252C%2520DINOv2%2529%2520and%2520consistently%2520enhances%2520performance%2520across%2520diverse%2520tasks%252C%2520including%2520segmentation%252C%2520detection%252C%2520depth%2520estimation%252C%2520and%2520video%2520dense%2520prediction.%2520Our%2520framework%2520provides%2520a%2520practical%2520way%2520for%2520augmenting%2520SSL%2520methods%2520with%2520equivariant%2520capabilities%2520while%2520preserving%2520invariant%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20Based%20on%20Transformed%20Image%20Reconstruction%20for%20Equivariance-Coherent%20Feature%20Representation&entry.906535625=Qin%20Wang%20and%20Alessio%20Quercia%20and%20Benjamin%20Bruns%20and%20Abigail%20Morrison%20and%20Hanno%20Scharr%20and%20Kai%20Krajsek&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20methods%20have%20achieved%20remarkable%20success%20in%20learning%20image%20representations%20allowing%20invariances%20in%20them%20-%20but%20therefore%20discarding%20transformation%20information%20that%20some%20computer%20vision%20tasks%20actually%20require.%20While%20recent%20approaches%20attempt%20to%20address%20this%20limitation%20by%20learning%20equivariant%20features%20using%20linear%20operators%20in%20feature%20space%2C%20they%20impose%20restrictive%20assumptions%20that%20constrain%20flexibility%20and%20generalization.%20We%20introduce%20a%20weaker%20definition%20for%20the%20transformation%20relation%20between%20image%20and%20feature%20space%20denoted%20as%20equivariance-coherence.%20We%20propose%20a%20novel%20SSL%20auxiliary%20task%20that%20learns%20equivariance-coherent%20representations%20through%20intermediate%20transformation%20reconstruction%2C%20which%20can%20be%20integrated%20with%20existing%20joint%20embedding%20SSL%20methods.%20Our%20key%20idea%20is%20to%20reconstruct%20images%20at%20intermediate%20points%20along%20transformation%20paths%2C%20e.g.%20when%20training%20on%2030-degree%20rotations%2C%20we%20reconstruct%20the%2010-degree%20and%2020-degree%20rotation%20states.%20Reconstructing%20intermediate%20states%20requires%20the%20transformation%20information%20used%20in%20augmentations%2C%20rather%20than%20suppressing%20it%2C%20and%20therefore%20fosters%20features%20containing%20the%20augmented%20transformation%20information.%20Our%20method%20decomposes%20feature%20vectors%20into%20invariant%20and%20equivariant%20parts%2C%20training%20them%20with%20standard%20SSL%20losses%20and%20reconstruction%20losses%2C%20respectively.%20We%20demonstrate%20substantial%20improvements%20on%20synthetic%20equivariance%20benchmarks%20while%20maintaining%20competitive%20performance%20on%20downstream%20tasks%20requiring%20invariant%20representations.%20The%20approach%20seamlessly%20integrates%20with%20existing%20SSL%20methods%20%28iBOT%2C%20DINOv2%29%20and%20consistently%20enhances%20performance%20across%20diverse%20tasks%2C%20including%20segmentation%2C%20detection%2C%20depth%20estimation%2C%20and%20video%20dense%20prediction.%20Our%20framework%20provides%20a%20practical%20way%20for%20augmenting%20SSL%20methods%20with%20equivariant%20capabilities%20while%20preserving%20invariant%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2503.18753v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning as Discrete Communication", "author": "Kawtar Zaher and Ilyass Moummad and Olivier Buisson and Alexis Joly", "abstract": "Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.", "link": "http://arxiv.org/abs/2602.09764v1", "date": "2026-02-10", "relevancy": 2.623, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.567}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5037}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20as%20Discrete%20Communication&body=Title%3A%20Self-Supervised%20Learning%20as%20Discrete%20Communication%0AAuthor%3A%20Kawtar%20Zaher%20and%20Ilyass%20Moummad%20and%20Olivier%20Buisson%20and%20Alexis%20Joly%0AAbstract%3A%20Most%20self-supervised%20learning%20%28SSL%29%20methods%20learn%20continuous%20visual%20representations%20by%20aligning%20different%20views%20of%20the%20same%20input%2C%20offering%20limited%20control%20over%20how%20information%20is%20structured%20across%20representation%20dimensions.%20In%20this%20work%2C%20we%20frame%20visual%20self-supervised%20learning%20as%20a%20discrete%20communication%20process%20between%20a%20teacher%20and%20a%20student%20network%2C%20where%20semantic%20information%20is%20transmitted%20through%20a%20fixed-capacity%20binary%20channel.%20Rather%20than%20aligning%20continuous%20features%2C%20the%20student%20predicts%20multi-label%20binary%20messages%20produced%20by%20the%20teacher.%20Discrete%20agreement%20is%20enforced%20through%20an%20element-wise%20binary%20cross-entropy%20objective%2C%20while%20a%20coding-rate%20regularization%20term%20encourages%20effective%20utilization%20of%20the%20constrained%20channel%2C%20promoting%20structured%20representations.%20We%20further%20show%20that%20periodically%20reinitializing%20the%20projection%20head%20strengthens%20this%20effect%20by%20encouraging%20embeddings%20that%20remain%20predictive%20across%20multiple%20discrete%20encodings.%20Extensive%20experiments%20demonstrate%20consistent%20improvements%20over%20continuous%20agreement%20baselines%20on%20image%20classification%2C%20retrieval%2C%20and%20dense%20visual%20prediction%20tasks%2C%20as%20well%20as%20under%20domain%20shift%20through%20self-supervised%20adaptation.%20Beyond%20backbone%20representations%2C%20we%20analyze%20the%20learned%20binary%20codes%20and%20show%20that%20they%20form%20a%20compact%20and%20informative%20discrete%20language%2C%20capturing%20semantic%20factors%20reusable%20across%20classes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520as%2520Discrete%2520Communication%26entry.906535625%3DKawtar%2520Zaher%2520and%2520Ilyass%2520Moummad%2520and%2520Olivier%2520Buisson%2520and%2520Alexis%2520Joly%26entry.1292438233%3DMost%2520self-supervised%2520learning%2520%2528SSL%2529%2520methods%2520learn%2520continuous%2520visual%2520representations%2520by%2520aligning%2520different%2520views%2520of%2520the%2520same%2520input%252C%2520offering%2520limited%2520control%2520over%2520how%2520information%2520is%2520structured%2520across%2520representation%2520dimensions.%2520In%2520this%2520work%252C%2520we%2520frame%2520visual%2520self-supervised%2520learning%2520as%2520a%2520discrete%2520communication%2520process%2520between%2520a%2520teacher%2520and%2520a%2520student%2520network%252C%2520where%2520semantic%2520information%2520is%2520transmitted%2520through%2520a%2520fixed-capacity%2520binary%2520channel.%2520Rather%2520than%2520aligning%2520continuous%2520features%252C%2520the%2520student%2520predicts%2520multi-label%2520binary%2520messages%2520produced%2520by%2520the%2520teacher.%2520Discrete%2520agreement%2520is%2520enforced%2520through%2520an%2520element-wise%2520binary%2520cross-entropy%2520objective%252C%2520while%2520a%2520coding-rate%2520regularization%2520term%2520encourages%2520effective%2520utilization%2520of%2520the%2520constrained%2520channel%252C%2520promoting%2520structured%2520representations.%2520We%2520further%2520show%2520that%2520periodically%2520reinitializing%2520the%2520projection%2520head%2520strengthens%2520this%2520effect%2520by%2520encouraging%2520embeddings%2520that%2520remain%2520predictive%2520across%2520multiple%2520discrete%2520encodings.%2520Extensive%2520experiments%2520demonstrate%2520consistent%2520improvements%2520over%2520continuous%2520agreement%2520baselines%2520on%2520image%2520classification%252C%2520retrieval%252C%2520and%2520dense%2520visual%2520prediction%2520tasks%252C%2520as%2520well%2520as%2520under%2520domain%2520shift%2520through%2520self-supervised%2520adaptation.%2520Beyond%2520backbone%2520representations%252C%2520we%2520analyze%2520the%2520learned%2520binary%2520codes%2520and%2520show%2520that%2520they%2520form%2520a%2520compact%2520and%2520informative%2520discrete%2520language%252C%2520capturing%2520semantic%2520factors%2520reusable%2520across%2520classes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20as%20Discrete%20Communication&entry.906535625=Kawtar%20Zaher%20and%20Ilyass%20Moummad%20and%20Olivier%20Buisson%20and%20Alexis%20Joly&entry.1292438233=Most%20self-supervised%20learning%20%28SSL%29%20methods%20learn%20continuous%20visual%20representations%20by%20aligning%20different%20views%20of%20the%20same%20input%2C%20offering%20limited%20control%20over%20how%20information%20is%20structured%20across%20representation%20dimensions.%20In%20this%20work%2C%20we%20frame%20visual%20self-supervised%20learning%20as%20a%20discrete%20communication%20process%20between%20a%20teacher%20and%20a%20student%20network%2C%20where%20semantic%20information%20is%20transmitted%20through%20a%20fixed-capacity%20binary%20channel.%20Rather%20than%20aligning%20continuous%20features%2C%20the%20student%20predicts%20multi-label%20binary%20messages%20produced%20by%20the%20teacher.%20Discrete%20agreement%20is%20enforced%20through%20an%20element-wise%20binary%20cross-entropy%20objective%2C%20while%20a%20coding-rate%20regularization%20term%20encourages%20effective%20utilization%20of%20the%20constrained%20channel%2C%20promoting%20structured%20representations.%20We%20further%20show%20that%20periodically%20reinitializing%20the%20projection%20head%20strengthens%20this%20effect%20by%20encouraging%20embeddings%20that%20remain%20predictive%20across%20multiple%20discrete%20encodings.%20Extensive%20experiments%20demonstrate%20consistent%20improvements%20over%20continuous%20agreement%20baselines%20on%20image%20classification%2C%20retrieval%2C%20and%20dense%20visual%20prediction%20tasks%2C%20as%20well%20as%20under%20domain%20shift%20through%20self-supervised%20adaptation.%20Beyond%20backbone%20representations%2C%20we%20analyze%20the%20learned%20binary%20codes%20and%20show%20that%20they%20form%20a%20compact%20and%20informative%20discrete%20language%2C%20capturing%20semantic%20factors%20reusable%20across%20classes.&entry.1838667208=http%3A//arxiv.org/abs/2602.09764v1&entry.124074799=Read"},
{"title": "Resilient Class-Incremental Learning: on the Interplay of Drifting, Unlabelled and Imbalanced Data Streams", "author": "Jin Li and Kleanthis Malialis and Marios Polycarpou", "abstract": "In today's connected world, the generation of massive streaming data across diverse domains has become commonplace. In the presence of concept drift, class imbalance, label scarcity, and new class emergence, they jointly degrade representation stability, bias learning toward outdated distributions, and reduce the resilience and reliability of detection in dynamic environments. This paper proposes SCIL (Streaming Class-Incremental Learning) to address these challenges. The SCIL framework integrates an autoencoder (AE) with a multi-layer perceptron for multi-class prediction, uses a dual-loss strategy (classification and reconstruction) for prediction and new class detection, employs corrected pseudo-labels for online training, manages classes with queues, and applies oversampling to handle imbalance. The rationale behind the method's structure is elucidated through ablation studies and a comprehensive experimental evaluation is performed using both real-world and synthetic datasets that feature class imbalance, incremental classes, and concept drifts. Our results demonstrate that SCIL outperforms strong baselines and state-of-the-art methods. Based on our commitment to Open Science, we make our code and datasets available to the community.", "link": "http://arxiv.org/abs/2602.09681v1", "date": "2026-02-10", "relevancy": 2.6007, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5292}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5271}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resilient%20Class-Incremental%20Learning%3A%20on%20the%20Interplay%20of%20Drifting%2C%20Unlabelled%20and%20Imbalanced%20Data%20Streams&body=Title%3A%20Resilient%20Class-Incremental%20Learning%3A%20on%20the%20Interplay%20of%20Drifting%2C%20Unlabelled%20and%20Imbalanced%20Data%20Streams%0AAuthor%3A%20Jin%20Li%20and%20Kleanthis%20Malialis%20and%20Marios%20Polycarpou%0AAbstract%3A%20In%20today%27s%20connected%20world%2C%20the%20generation%20of%20massive%20streaming%20data%20across%20diverse%20domains%20has%20become%20commonplace.%20In%20the%20presence%20of%20concept%20drift%2C%20class%20imbalance%2C%20label%20scarcity%2C%20and%20new%20class%20emergence%2C%20they%20jointly%20degrade%20representation%20stability%2C%20bias%20learning%20toward%20outdated%20distributions%2C%20and%20reduce%20the%20resilience%20and%20reliability%20of%20detection%20in%20dynamic%20environments.%20This%20paper%20proposes%20SCIL%20%28Streaming%20Class-Incremental%20Learning%29%20to%20address%20these%20challenges.%20The%20SCIL%20framework%20integrates%20an%20autoencoder%20%28AE%29%20with%20a%20multi-layer%20perceptron%20for%20multi-class%20prediction%2C%20uses%20a%20dual-loss%20strategy%20%28classification%20and%20reconstruction%29%20for%20prediction%20and%20new%20class%20detection%2C%20employs%20corrected%20pseudo-labels%20for%20online%20training%2C%20manages%20classes%20with%20queues%2C%20and%20applies%20oversampling%20to%20handle%20imbalance.%20The%20rationale%20behind%20the%20method%27s%20structure%20is%20elucidated%20through%20ablation%20studies%20and%20a%20comprehensive%20experimental%20evaluation%20is%20performed%20using%20both%20real-world%20and%20synthetic%20datasets%20that%20feature%20class%20imbalance%2C%20incremental%20classes%2C%20and%20concept%20drifts.%20Our%20results%20demonstrate%20that%20SCIL%20outperforms%20strong%20baselines%20and%20state-of-the-art%20methods.%20Based%20on%20our%20commitment%20to%20Open%20Science%2C%20we%20make%20our%20code%20and%20datasets%20available%20to%20the%20community.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResilient%2520Class-Incremental%2520Learning%253A%2520on%2520the%2520Interplay%2520of%2520Drifting%252C%2520Unlabelled%2520and%2520Imbalanced%2520Data%2520Streams%26entry.906535625%3DJin%2520Li%2520and%2520Kleanthis%2520Malialis%2520and%2520Marios%2520Polycarpou%26entry.1292438233%3DIn%2520today%2527s%2520connected%2520world%252C%2520the%2520generation%2520of%2520massive%2520streaming%2520data%2520across%2520diverse%2520domains%2520has%2520become%2520commonplace.%2520In%2520the%2520presence%2520of%2520concept%2520drift%252C%2520class%2520imbalance%252C%2520label%2520scarcity%252C%2520and%2520new%2520class%2520emergence%252C%2520they%2520jointly%2520degrade%2520representation%2520stability%252C%2520bias%2520learning%2520toward%2520outdated%2520distributions%252C%2520and%2520reduce%2520the%2520resilience%2520and%2520reliability%2520of%2520detection%2520in%2520dynamic%2520environments.%2520This%2520paper%2520proposes%2520SCIL%2520%2528Streaming%2520Class-Incremental%2520Learning%2529%2520to%2520address%2520these%2520challenges.%2520The%2520SCIL%2520framework%2520integrates%2520an%2520autoencoder%2520%2528AE%2529%2520with%2520a%2520multi-layer%2520perceptron%2520for%2520multi-class%2520prediction%252C%2520uses%2520a%2520dual-loss%2520strategy%2520%2528classification%2520and%2520reconstruction%2529%2520for%2520prediction%2520and%2520new%2520class%2520detection%252C%2520employs%2520corrected%2520pseudo-labels%2520for%2520online%2520training%252C%2520manages%2520classes%2520with%2520queues%252C%2520and%2520applies%2520oversampling%2520to%2520handle%2520imbalance.%2520The%2520rationale%2520behind%2520the%2520method%2527s%2520structure%2520is%2520elucidated%2520through%2520ablation%2520studies%2520and%2520a%2520comprehensive%2520experimental%2520evaluation%2520is%2520performed%2520using%2520both%2520real-world%2520and%2520synthetic%2520datasets%2520that%2520feature%2520class%2520imbalance%252C%2520incremental%2520classes%252C%2520and%2520concept%2520drifts.%2520Our%2520results%2520demonstrate%2520that%2520SCIL%2520outperforms%2520strong%2520baselines%2520and%2520state-of-the-art%2520methods.%2520Based%2520on%2520our%2520commitment%2520to%2520Open%2520Science%252C%2520we%2520make%2520our%2520code%2520and%2520datasets%2520available%2520to%2520the%2520community.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resilient%20Class-Incremental%20Learning%3A%20on%20the%20Interplay%20of%20Drifting%2C%20Unlabelled%20and%20Imbalanced%20Data%20Streams&entry.906535625=Jin%20Li%20and%20Kleanthis%20Malialis%20and%20Marios%20Polycarpou&entry.1292438233=In%20today%27s%20connected%20world%2C%20the%20generation%20of%20massive%20streaming%20data%20across%20diverse%20domains%20has%20become%20commonplace.%20In%20the%20presence%20of%20concept%20drift%2C%20class%20imbalance%2C%20label%20scarcity%2C%20and%20new%20class%20emergence%2C%20they%20jointly%20degrade%20representation%20stability%2C%20bias%20learning%20toward%20outdated%20distributions%2C%20and%20reduce%20the%20resilience%20and%20reliability%20of%20detection%20in%20dynamic%20environments.%20This%20paper%20proposes%20SCIL%20%28Streaming%20Class-Incremental%20Learning%29%20to%20address%20these%20challenges.%20The%20SCIL%20framework%20integrates%20an%20autoencoder%20%28AE%29%20with%20a%20multi-layer%20perceptron%20for%20multi-class%20prediction%2C%20uses%20a%20dual-loss%20strategy%20%28classification%20and%20reconstruction%29%20for%20prediction%20and%20new%20class%20detection%2C%20employs%20corrected%20pseudo-labels%20for%20online%20training%2C%20manages%20classes%20with%20queues%2C%20and%20applies%20oversampling%20to%20handle%20imbalance.%20The%20rationale%20behind%20the%20method%27s%20structure%20is%20elucidated%20through%20ablation%20studies%20and%20a%20comprehensive%20experimental%20evaluation%20is%20performed%20using%20both%20real-world%20and%20synthetic%20datasets%20that%20feature%20class%20imbalance%2C%20incremental%20classes%2C%20and%20concept%20drifts.%20Our%20results%20demonstrate%20that%20SCIL%20outperforms%20strong%20baselines%20and%20state-of-the-art%20methods.%20Based%20on%20our%20commitment%20to%20Open%20Science%2C%20we%20make%20our%20code%20and%20datasets%20available%20to%20the%20community.&entry.1838667208=http%3A//arxiv.org/abs/2602.09681v1&entry.124074799=Read"},
{"title": "MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation", "author": "Jiaxu Wang and Yicheng Jiang and Tianlun He and Jingkai Sun and Qiang Zhang and Junhao He and Jiahang Cao and Zesen Gan and Mingyuan Sun and Qiming Shao and Xiangyu Yue", "abstract": "World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.", "link": "http://arxiv.org/abs/2602.09878v1", "date": "2026-02-10", "relevancy": 2.5462, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6586}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6321}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVISTA-4D%3A%20View-Consistent%204D%20World%20Model%20with%20Test-Time%20Action%20Inference%20for%20Robotic%20Manipulation&body=Title%3A%20MVISTA-4D%3A%20View-Consistent%204D%20World%20Model%20with%20Test-Time%20Action%20Inference%20for%20Robotic%20Manipulation%0AAuthor%3A%20Jiaxu%20Wang%20and%20Yicheng%20Jiang%20and%20Tianlun%20He%20and%20Jingkai%20Sun%20and%20Qiang%20Zhang%20and%20Junhao%20He%20and%20Jiahang%20Cao%20and%20Zesen%20Gan%20and%20Mingyuan%20Sun%20and%20Qiming%20Shao%20and%20Xiangyu%20Yue%0AAbstract%3A%20World-model-based%20imagine-then-act%20becomes%20a%20promising%20paradigm%20for%20robotic%20manipulation%2C%20yet%20existing%20approaches%20typically%20support%20either%20purely%20image-based%20forecasting%20or%20reasoning%20over%20partial%203D%20geometry%2C%20limiting%20their%20ability%20to%20predict%20complete%204D%20scene%20dynamics.%20This%20work%20proposes%20a%20novel%20embodied%204D%20world%20model%20that%20enables%20geometrically%20consistent%2C%20arbitrary-view%20RGBD%20generation%3A%20given%20only%20a%20single-view%20RGBD%20observation%20as%20input%2C%20the%20model%20imagines%20the%20remaining%20viewpoints%2C%20which%20can%20then%20be%20back-projected%20and%20fused%20to%20assemble%20a%20more%20complete%203D%20structure%20across%20time.%20To%20efficiently%20learn%20the%20multi-view%2C%20cross-modality%20generation%2C%20we%20explicitly%20design%20cross-view%20and%20cross-modality%20feature%20fusion%20that%20jointly%20encourage%20consistency%20between%20RGB%20and%20depth%20and%20enforce%20geometric%20alignment%20across%20views.%20Beyond%20prediction%2C%20converting%20generated%20futures%20into%20actions%20is%20often%20handled%20by%20inverse%20dynamics%2C%20which%20is%20ill-posed%20because%20multiple%20actions%20can%20explain%20the%20same%20transition.%20We%20address%20this%20with%20a%20test-time%20action%20optimization%20strategy%20that%20backpropagates%20through%20the%20generative%20model%20to%20infer%20a%20trajectory-level%20latent%20best%20matching%20the%20predicted%20future%2C%20and%20a%20residual%20inverse%20dynamics%20model%20that%20turns%20this%20trajectory%20prior%20into%20accurate%20executable%20actions.%20Experiments%20on%20three%20datasets%20demonstrate%20strong%20performance%20on%20both%204D%20scene%20generation%20and%20downstream%20manipulation%2C%20and%20ablations%20provide%20practical%20insights%20into%20the%20key%20design%20choices.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVISTA-4D%253A%2520View-Consistent%25204D%2520World%2520Model%2520with%2520Test-Time%2520Action%2520Inference%2520for%2520Robotic%2520Manipulation%26entry.906535625%3DJiaxu%2520Wang%2520and%2520Yicheng%2520Jiang%2520and%2520Tianlun%2520He%2520and%2520Jingkai%2520Sun%2520and%2520Qiang%2520Zhang%2520and%2520Junhao%2520He%2520and%2520Jiahang%2520Cao%2520and%2520Zesen%2520Gan%2520and%2520Mingyuan%2520Sun%2520and%2520Qiming%2520Shao%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3DWorld-model-based%2520imagine-then-act%2520becomes%2520a%2520promising%2520paradigm%2520for%2520robotic%2520manipulation%252C%2520yet%2520existing%2520approaches%2520typically%2520support%2520either%2520purely%2520image-based%2520forecasting%2520or%2520reasoning%2520over%2520partial%25203D%2520geometry%252C%2520limiting%2520their%2520ability%2520to%2520predict%2520complete%25204D%2520scene%2520dynamics.%2520This%2520work%2520proposes%2520a%2520novel%2520embodied%25204D%2520world%2520model%2520that%2520enables%2520geometrically%2520consistent%252C%2520arbitrary-view%2520RGBD%2520generation%253A%2520given%2520only%2520a%2520single-view%2520RGBD%2520observation%2520as%2520input%252C%2520the%2520model%2520imagines%2520the%2520remaining%2520viewpoints%252C%2520which%2520can%2520then%2520be%2520back-projected%2520and%2520fused%2520to%2520assemble%2520a%2520more%2520complete%25203D%2520structure%2520across%2520time.%2520To%2520efficiently%2520learn%2520the%2520multi-view%252C%2520cross-modality%2520generation%252C%2520we%2520explicitly%2520design%2520cross-view%2520and%2520cross-modality%2520feature%2520fusion%2520that%2520jointly%2520encourage%2520consistency%2520between%2520RGB%2520and%2520depth%2520and%2520enforce%2520geometric%2520alignment%2520across%2520views.%2520Beyond%2520prediction%252C%2520converting%2520generated%2520futures%2520into%2520actions%2520is%2520often%2520handled%2520by%2520inverse%2520dynamics%252C%2520which%2520is%2520ill-posed%2520because%2520multiple%2520actions%2520can%2520explain%2520the%2520same%2520transition.%2520We%2520address%2520this%2520with%2520a%2520test-time%2520action%2520optimization%2520strategy%2520that%2520backpropagates%2520through%2520the%2520generative%2520model%2520to%2520infer%2520a%2520trajectory-level%2520latent%2520best%2520matching%2520the%2520predicted%2520future%252C%2520and%2520a%2520residual%2520inverse%2520dynamics%2520model%2520that%2520turns%2520this%2520trajectory%2520prior%2520into%2520accurate%2520executable%2520actions.%2520Experiments%2520on%2520three%2520datasets%2520demonstrate%2520strong%2520performance%2520on%2520both%25204D%2520scene%2520generation%2520and%2520downstream%2520manipulation%252C%2520and%2520ablations%2520provide%2520practical%2520insights%2520into%2520the%2520key%2520design%2520choices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVISTA-4D%3A%20View-Consistent%204D%20World%20Model%20with%20Test-Time%20Action%20Inference%20for%20Robotic%20Manipulation&entry.906535625=Jiaxu%20Wang%20and%20Yicheng%20Jiang%20and%20Tianlun%20He%20and%20Jingkai%20Sun%20and%20Qiang%20Zhang%20and%20Junhao%20He%20and%20Jiahang%20Cao%20and%20Zesen%20Gan%20and%20Mingyuan%20Sun%20and%20Qiming%20Shao%20and%20Xiangyu%20Yue&entry.1292438233=World-model-based%20imagine-then-act%20becomes%20a%20promising%20paradigm%20for%20robotic%20manipulation%2C%20yet%20existing%20approaches%20typically%20support%20either%20purely%20image-based%20forecasting%20or%20reasoning%20over%20partial%203D%20geometry%2C%20limiting%20their%20ability%20to%20predict%20complete%204D%20scene%20dynamics.%20This%20work%20proposes%20a%20novel%20embodied%204D%20world%20model%20that%20enables%20geometrically%20consistent%2C%20arbitrary-view%20RGBD%20generation%3A%20given%20only%20a%20single-view%20RGBD%20observation%20as%20input%2C%20the%20model%20imagines%20the%20remaining%20viewpoints%2C%20which%20can%20then%20be%20back-projected%20and%20fused%20to%20assemble%20a%20more%20complete%203D%20structure%20across%20time.%20To%20efficiently%20learn%20the%20multi-view%2C%20cross-modality%20generation%2C%20we%20explicitly%20design%20cross-view%20and%20cross-modality%20feature%20fusion%20that%20jointly%20encourage%20consistency%20between%20RGB%20and%20depth%20and%20enforce%20geometric%20alignment%20across%20views.%20Beyond%20prediction%2C%20converting%20generated%20futures%20into%20actions%20is%20often%20handled%20by%20inverse%20dynamics%2C%20which%20is%20ill-posed%20because%20multiple%20actions%20can%20explain%20the%20same%20transition.%20We%20address%20this%20with%20a%20test-time%20action%20optimization%20strategy%20that%20backpropagates%20through%20the%20generative%20model%20to%20infer%20a%20trajectory-level%20latent%20best%20matching%20the%20predicted%20future%2C%20and%20a%20residual%20inverse%20dynamics%20model%20that%20turns%20this%20trajectory%20prior%20into%20accurate%20executable%20actions.%20Experiments%20on%20three%20datasets%20demonstrate%20strong%20performance%20on%20both%204D%20scene%20generation%20and%20downstream%20manipulation%2C%20and%20ablations%20provide%20practical%20insights%20into%20the%20key%20design%20choices.&entry.1838667208=http%3A//arxiv.org/abs/2602.09878v1&entry.124074799=Read"},
{"title": "Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures", "author": "Yuxi Wang and Wenqi Ouyang and Tianyi Wei and Yi Dong and Zhiqi Shen and Xingang Pan", "abstract": "Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Pl\u00fccker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.", "link": "http://arxiv.org/abs/2602.09600v1", "date": "2026-02-10", "relevancy": 2.5277, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6595}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6321}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hand2World%3A%20Autoregressive%20Egocentric%20Interaction%20Generation%20via%20Free-Space%20Hand%20Gestures&body=Title%3A%20Hand2World%3A%20Autoregressive%20Egocentric%20Interaction%20Generation%20via%20Free-Space%20Hand%20Gestures%0AAuthor%3A%20Yuxi%20Wang%20and%20Wenqi%20Ouyang%20and%20Tianyi%20Wei%20and%20Yi%20Dong%20and%20Zhiqi%20Shen%20and%20Xingang%20Pan%0AAbstract%3A%20Egocentric%20interactive%20world%20models%20are%20essential%20for%20augmented%20reality%20and%20embodied%20AI%2C%20where%20visual%20generation%20must%20respond%20to%20user%20input%20with%20low%20latency%2C%20geometric%20consistency%2C%20and%20long-term%20stability.%20We%20study%20egocentric%20interaction%20generation%20from%20a%20single%20scene%20image%20under%20free-space%20hand%20gestures%2C%20aiming%20to%20synthesize%20photorealistic%20videos%20in%20which%20hands%20enter%20the%20scene%2C%20interact%20with%20objects%2C%20and%20induce%20plausible%20world%20dynamics%20under%20head%20motion.%20This%20setting%20introduces%20fundamental%20challenges%2C%20including%20distribution%20shift%20between%20free-space%20gestures%20and%20contact-heavy%20training%20data%2C%20ambiguity%20between%20hand%20motion%20and%20camera%20motion%20in%20monocular%20views%2C%20and%20the%20need%20for%20arbitrary-length%20video%20generation.%20We%20present%20Hand2World%2C%20a%20unified%20autoregressive%20framework%20that%20addresses%20these%20challenges%20through%20occlusion-invariant%20hand%20conditioning%20based%20on%20projected%203D%20hand%20meshes%2C%20allowing%20visibility%20and%20occlusion%20to%20be%20inferred%20from%20scene%20context%20rather%20than%20encoded%20in%20the%20control%20signal.%20To%20stabilize%20egocentric%20viewpoint%20changes%2C%20we%20inject%20explicit%20camera%20geometry%20via%20per-pixel%20Pl%C3%BCcker-ray%20embeddings%2C%20disentangling%20camera%20motion%20from%20hand%20motion%20and%20preventing%20background%20drift.%20We%20further%20develop%20a%20fully%20automated%20monocular%20annotation%20pipeline%20and%20distill%20a%20bidirectional%20diffusion%20model%20into%20a%20causal%20generator%2C%20enabling%20arbitrary-length%20synthesis.%20Experiments%20on%20three%20egocentric%20interaction%20benchmarks%20show%20substantial%20improvements%20in%20perceptual%20quality%20and%203D%20consistency%20while%20supporting%20camera%20control%20and%20long-horizon%20interactive%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHand2World%253A%2520Autoregressive%2520Egocentric%2520Interaction%2520Generation%2520via%2520Free-Space%2520Hand%2520Gestures%26entry.906535625%3DYuxi%2520Wang%2520and%2520Wenqi%2520Ouyang%2520and%2520Tianyi%2520Wei%2520and%2520Yi%2520Dong%2520and%2520Zhiqi%2520Shen%2520and%2520Xingang%2520Pan%26entry.1292438233%3DEgocentric%2520interactive%2520world%2520models%2520are%2520essential%2520for%2520augmented%2520reality%2520and%2520embodied%2520AI%252C%2520where%2520visual%2520generation%2520must%2520respond%2520to%2520user%2520input%2520with%2520low%2520latency%252C%2520geometric%2520consistency%252C%2520and%2520long-term%2520stability.%2520We%2520study%2520egocentric%2520interaction%2520generation%2520from%2520a%2520single%2520scene%2520image%2520under%2520free-space%2520hand%2520gestures%252C%2520aiming%2520to%2520synthesize%2520photorealistic%2520videos%2520in%2520which%2520hands%2520enter%2520the%2520scene%252C%2520interact%2520with%2520objects%252C%2520and%2520induce%2520plausible%2520world%2520dynamics%2520under%2520head%2520motion.%2520This%2520setting%2520introduces%2520fundamental%2520challenges%252C%2520including%2520distribution%2520shift%2520between%2520free-space%2520gestures%2520and%2520contact-heavy%2520training%2520data%252C%2520ambiguity%2520between%2520hand%2520motion%2520and%2520camera%2520motion%2520in%2520monocular%2520views%252C%2520and%2520the%2520need%2520for%2520arbitrary-length%2520video%2520generation.%2520We%2520present%2520Hand2World%252C%2520a%2520unified%2520autoregressive%2520framework%2520that%2520addresses%2520these%2520challenges%2520through%2520occlusion-invariant%2520hand%2520conditioning%2520based%2520on%2520projected%25203D%2520hand%2520meshes%252C%2520allowing%2520visibility%2520and%2520occlusion%2520to%2520be%2520inferred%2520from%2520scene%2520context%2520rather%2520than%2520encoded%2520in%2520the%2520control%2520signal.%2520To%2520stabilize%2520egocentric%2520viewpoint%2520changes%252C%2520we%2520inject%2520explicit%2520camera%2520geometry%2520via%2520per-pixel%2520Pl%25C3%25BCcker-ray%2520embeddings%252C%2520disentangling%2520camera%2520motion%2520from%2520hand%2520motion%2520and%2520preventing%2520background%2520drift.%2520We%2520further%2520develop%2520a%2520fully%2520automated%2520monocular%2520annotation%2520pipeline%2520and%2520distill%2520a%2520bidirectional%2520diffusion%2520model%2520into%2520a%2520causal%2520generator%252C%2520enabling%2520arbitrary-length%2520synthesis.%2520Experiments%2520on%2520three%2520egocentric%2520interaction%2520benchmarks%2520show%2520substantial%2520improvements%2520in%2520perceptual%2520quality%2520and%25203D%2520consistency%2520while%2520supporting%2520camera%2520control%2520and%2520long-horizon%2520interactive%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hand2World%3A%20Autoregressive%20Egocentric%20Interaction%20Generation%20via%20Free-Space%20Hand%20Gestures&entry.906535625=Yuxi%20Wang%20and%20Wenqi%20Ouyang%20and%20Tianyi%20Wei%20and%20Yi%20Dong%20and%20Zhiqi%20Shen%20and%20Xingang%20Pan&entry.1292438233=Egocentric%20interactive%20world%20models%20are%20essential%20for%20augmented%20reality%20and%20embodied%20AI%2C%20where%20visual%20generation%20must%20respond%20to%20user%20input%20with%20low%20latency%2C%20geometric%20consistency%2C%20and%20long-term%20stability.%20We%20study%20egocentric%20interaction%20generation%20from%20a%20single%20scene%20image%20under%20free-space%20hand%20gestures%2C%20aiming%20to%20synthesize%20photorealistic%20videos%20in%20which%20hands%20enter%20the%20scene%2C%20interact%20with%20objects%2C%20and%20induce%20plausible%20world%20dynamics%20under%20head%20motion.%20This%20setting%20introduces%20fundamental%20challenges%2C%20including%20distribution%20shift%20between%20free-space%20gestures%20and%20contact-heavy%20training%20data%2C%20ambiguity%20between%20hand%20motion%20and%20camera%20motion%20in%20monocular%20views%2C%20and%20the%20need%20for%20arbitrary-length%20video%20generation.%20We%20present%20Hand2World%2C%20a%20unified%20autoregressive%20framework%20that%20addresses%20these%20challenges%20through%20occlusion-invariant%20hand%20conditioning%20based%20on%20projected%203D%20hand%20meshes%2C%20allowing%20visibility%20and%20occlusion%20to%20be%20inferred%20from%20scene%20context%20rather%20than%20encoded%20in%20the%20control%20signal.%20To%20stabilize%20egocentric%20viewpoint%20changes%2C%20we%20inject%20explicit%20camera%20geometry%20via%20per-pixel%20Pl%C3%BCcker-ray%20embeddings%2C%20disentangling%20camera%20motion%20from%20hand%20motion%20and%20preventing%20background%20drift.%20We%20further%20develop%20a%20fully%20automated%20monocular%20annotation%20pipeline%20and%20distill%20a%20bidirectional%20diffusion%20model%20into%20a%20causal%20generator%2C%20enabling%20arbitrary-length%20synthesis.%20Experiments%20on%20three%20egocentric%20interaction%20benchmarks%20show%20substantial%20improvements%20in%20perceptual%20quality%20and%203D%20consistency%20while%20supporting%20camera%20control%20and%20long-horizon%20interactive%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2602.09600v1&entry.124074799=Read"},
{"title": "Decomposing Reasoning Efficiency in Large Language Models", "author": "Daniel Kaiser and Arnoldo Frigessi and Ali Ramezani-Kebrya and Benjamin Ricaud", "abstract": "Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $\u03c1=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.", "link": "http://arxiv.org/abs/2602.09805v1", "date": "2026-02-10", "relevancy": 2.5239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20Reasoning%20Efficiency%20in%20Large%20Language%20Models&body=Title%3A%20Decomposing%20Reasoning%20Efficiency%20in%20Large%20Language%20Models%0AAuthor%3A%20Daniel%20Kaiser%20and%20Arnoldo%20Frigessi%20and%20Ali%20Ramezani-Kebrya%20and%20Benjamin%20Ricaud%0AAbstract%3A%20Large%20language%20models%20trained%20for%20reasoning%20trade%20off%20inference%20tokens%20against%20accuracy%2C%20yet%20standard%20evaluations%20report%20only%20final%20accuracy%2C%20obscuring%20where%20tokens%20are%20spent%20or%20wasted.%20We%20introduce%20a%20trace-optional%20framework%20that%20decomposes%20token%20efficiency%20into%20interpretable%20factors%3A%20completion%20under%20a%20fixed%20token%20budget%20%28avoiding%20truncation%29%2C%20conditional%20correctness%20given%20completion%2C%20and%20verbosity%20%28token%20usage%29.%20When%20benchmark%20metadata%20provides%20per-instance%20workload%20proxies%2C%20we%20further%20factor%20verbosity%20into%20two%20components%3A%20mean%20verbalization%20overhead%20%28tokens%20per%20work%20unit%29%20and%20a%20coupling%20coefficient%20capturing%20how%20overhead%20scales%20with%20task%20workload.%20When%20reasoning%20traces%20are%20available%2C%20we%20add%20deterministic%20trace-quality%20measures%20%28grounding%2C%20repetition%2C%20prompt%20copying%29%20to%20separate%20degenerate%20looping%20from%20verbose-but-engaged%20reasoning%2C%20avoiding%20human%20labeling%20and%20LLM%20judges.%20Evaluating%2025%20models%20on%20CogniLoad%2C%20we%20find%20that%20accuracy%20and%20token-efficiency%20rankings%20diverge%20%28Spearman%20%24%CF%81%3D0.63%24%29%2C%20efficiency%20gaps%20are%20often%20driven%20by%20conditional%20correctness%2C%20and%20verbalization%20overhead%20varies%20by%20about%209%20times%20%28only%20weakly%20related%20to%20model%20scale%29.%20Our%20decomposition%20reveals%20distinct%20bottleneck%20profiles%20that%20suggest%20different%20efficiency%20interventions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520Reasoning%2520Efficiency%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DDaniel%2520Kaiser%2520and%2520Arnoldo%2520Frigessi%2520and%2520Ali%2520Ramezani-Kebrya%2520and%2520Benjamin%2520Ricaud%26entry.1292438233%3DLarge%2520language%2520models%2520trained%2520for%2520reasoning%2520trade%2520off%2520inference%2520tokens%2520against%2520accuracy%252C%2520yet%2520standard%2520evaluations%2520report%2520only%2520final%2520accuracy%252C%2520obscuring%2520where%2520tokens%2520are%2520spent%2520or%2520wasted.%2520We%2520introduce%2520a%2520trace-optional%2520framework%2520that%2520decomposes%2520token%2520efficiency%2520into%2520interpretable%2520factors%253A%2520completion%2520under%2520a%2520fixed%2520token%2520budget%2520%2528avoiding%2520truncation%2529%252C%2520conditional%2520correctness%2520given%2520completion%252C%2520and%2520verbosity%2520%2528token%2520usage%2529.%2520When%2520benchmark%2520metadata%2520provides%2520per-instance%2520workload%2520proxies%252C%2520we%2520further%2520factor%2520verbosity%2520into%2520two%2520components%253A%2520mean%2520verbalization%2520overhead%2520%2528tokens%2520per%2520work%2520unit%2529%2520and%2520a%2520coupling%2520coefficient%2520capturing%2520how%2520overhead%2520scales%2520with%2520task%2520workload.%2520When%2520reasoning%2520traces%2520are%2520available%252C%2520we%2520add%2520deterministic%2520trace-quality%2520measures%2520%2528grounding%252C%2520repetition%252C%2520prompt%2520copying%2529%2520to%2520separate%2520degenerate%2520looping%2520from%2520verbose-but-engaged%2520reasoning%252C%2520avoiding%2520human%2520labeling%2520and%2520LLM%2520judges.%2520Evaluating%252025%2520models%2520on%2520CogniLoad%252C%2520we%2520find%2520that%2520accuracy%2520and%2520token-efficiency%2520rankings%2520diverge%2520%2528Spearman%2520%2524%25CF%2581%253D0.63%2524%2529%252C%2520efficiency%2520gaps%2520are%2520often%2520driven%2520by%2520conditional%2520correctness%252C%2520and%2520verbalization%2520overhead%2520varies%2520by%2520about%25209%2520times%2520%2528only%2520weakly%2520related%2520to%2520model%2520scale%2529.%2520Our%2520decomposition%2520reveals%2520distinct%2520bottleneck%2520profiles%2520that%2520suggest%2520different%2520efficiency%2520interventions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20Reasoning%20Efficiency%20in%20Large%20Language%20Models&entry.906535625=Daniel%20Kaiser%20and%20Arnoldo%20Frigessi%20and%20Ali%20Ramezani-Kebrya%20and%20Benjamin%20Ricaud&entry.1292438233=Large%20language%20models%20trained%20for%20reasoning%20trade%20off%20inference%20tokens%20against%20accuracy%2C%20yet%20standard%20evaluations%20report%20only%20final%20accuracy%2C%20obscuring%20where%20tokens%20are%20spent%20or%20wasted.%20We%20introduce%20a%20trace-optional%20framework%20that%20decomposes%20token%20efficiency%20into%20interpretable%20factors%3A%20completion%20under%20a%20fixed%20token%20budget%20%28avoiding%20truncation%29%2C%20conditional%20correctness%20given%20completion%2C%20and%20verbosity%20%28token%20usage%29.%20When%20benchmark%20metadata%20provides%20per-instance%20workload%20proxies%2C%20we%20further%20factor%20verbosity%20into%20two%20components%3A%20mean%20verbalization%20overhead%20%28tokens%20per%20work%20unit%29%20and%20a%20coupling%20coefficient%20capturing%20how%20overhead%20scales%20with%20task%20workload.%20When%20reasoning%20traces%20are%20available%2C%20we%20add%20deterministic%20trace-quality%20measures%20%28grounding%2C%20repetition%2C%20prompt%20copying%29%20to%20separate%20degenerate%20looping%20from%20verbose-but-engaged%20reasoning%2C%20avoiding%20human%20labeling%20and%20LLM%20judges.%20Evaluating%2025%20models%20on%20CogniLoad%2C%20we%20find%20that%20accuracy%20and%20token-efficiency%20rankings%20diverge%20%28Spearman%20%24%CF%81%3D0.63%24%29%2C%20efficiency%20gaps%20are%20often%20driven%20by%20conditional%20correctness%2C%20and%20verbalization%20overhead%20varies%20by%20about%209%20times%20%28only%20weakly%20related%20to%20model%20scale%29.%20Our%20decomposition%20reveals%20distinct%20bottleneck%20profiles%20that%20suggest%20different%20efficiency%20interventions.&entry.1838667208=http%3A//arxiv.org/abs/2602.09805v1&entry.124074799=Read"},
{"title": "Entropy-Aware Structural Alignment for Zero-Shot Handwritten Chinese Character Recognition", "author": "Qiuming Luo and Tao Zeng and Feng Li and Heming Liu and Rui Mao and Chang Kong", "abstract": "Zero-shot Handwritten Chinese Character Recognition (HCCR) aims to recognize unseen characters by leveraging radical-based semantic compositions. However, existing approaches often treat characters as flat radical sequences, neglecting the hierarchical topology and the uneven information density of different components. To address these limitations, we propose an Entropy-Aware Structural Alignment Network that bridges the visual-semantic gap through information-theoretic modeling. First, we introduce an Information Entropy Prior to dynamically modulate positional embeddings via multiplicative interaction, acting as a saliency detector that prioritizes discriminative roots over ubiquitous components. Second, we construct a Dual-View Radical Tree to extract multi-granularity structural features, which are integrated via an adaptive Sigmoid-based gating network to encode both global layout and local spatial roles. Finally, a Top-K Semantic Feature Fusion mechanism is devised to augment the decoding process by utilizing the centroid of semantic neighbors, effectively rectifying visual ambiguities through feature-level consensus. Extensive experiments demonstrate that our method establishes new state-of-the-art performance, achieving an accuracy of 55.04\\% on the ICDAR 2013 dataset ($m=1500$), significantly outperforming existing CLIP-based baselines in the challenging zero-shot setting. Furthermore, the framework exhibits exceptional data efficiency, demonstrating rapid adaptability with minimal support samples, achieving 92.41\\% accuracy with only one support sample per class.", "link": "http://arxiv.org/abs/2602.03913v2", "date": "2026-02-10", "relevancy": 2.5074, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5137}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5067}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Aware%20Structural%20Alignment%20for%20Zero-Shot%20Handwritten%20Chinese%20Character%20Recognition&body=Title%3A%20Entropy-Aware%20Structural%20Alignment%20for%20Zero-Shot%20Handwritten%20Chinese%20Character%20Recognition%0AAuthor%3A%20Qiuming%20Luo%20and%20Tao%20Zeng%20and%20Feng%20Li%20and%20Heming%20Liu%20and%20Rui%20Mao%20and%20Chang%20Kong%0AAbstract%3A%20Zero-shot%20Handwritten%20Chinese%20Character%20Recognition%20%28HCCR%29%20aims%20to%20recognize%20unseen%20characters%20by%20leveraging%20radical-based%20semantic%20compositions.%20However%2C%20existing%20approaches%20often%20treat%20characters%20as%20flat%20radical%20sequences%2C%20neglecting%20the%20hierarchical%20topology%20and%20the%20uneven%20information%20density%20of%20different%20components.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20Entropy-Aware%20Structural%20Alignment%20Network%20that%20bridges%20the%20visual-semantic%20gap%20through%20information-theoretic%20modeling.%20First%2C%20we%20introduce%20an%20Information%20Entropy%20Prior%20to%20dynamically%20modulate%20positional%20embeddings%20via%20multiplicative%20interaction%2C%20acting%20as%20a%20saliency%20detector%20that%20prioritizes%20discriminative%20roots%20over%20ubiquitous%20components.%20Second%2C%20we%20construct%20a%20Dual-View%20Radical%20Tree%20to%20extract%20multi-granularity%20structural%20features%2C%20which%20are%20integrated%20via%20an%20adaptive%20Sigmoid-based%20gating%20network%20to%20encode%20both%20global%20layout%20and%20local%20spatial%20roles.%20Finally%2C%20a%20Top-K%20Semantic%20Feature%20Fusion%20mechanism%20is%20devised%20to%20augment%20the%20decoding%20process%20by%20utilizing%20the%20centroid%20of%20semantic%20neighbors%2C%20effectively%20rectifying%20visual%20ambiguities%20through%20feature-level%20consensus.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20establishes%20new%20state-of-the-art%20performance%2C%20achieving%20an%20accuracy%20of%2055.04%5C%25%20on%20the%20ICDAR%202013%20dataset%20%28%24m%3D1500%24%29%2C%20significantly%20outperforming%20existing%20CLIP-based%20baselines%20in%20the%20challenging%20zero-shot%20setting.%20Furthermore%2C%20the%20framework%20exhibits%20exceptional%20data%20efficiency%2C%20demonstrating%20rapid%20adaptability%20with%20minimal%20support%20samples%2C%20achieving%2092.41%5C%25%20accuracy%20with%20only%20one%20support%20sample%20per%20class.%0ALink%3A%20http%3A//arxiv.org/abs/2602.03913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Aware%2520Structural%2520Alignment%2520for%2520Zero-Shot%2520Handwritten%2520Chinese%2520Character%2520Recognition%26entry.906535625%3DQiuming%2520Luo%2520and%2520Tao%2520Zeng%2520and%2520Feng%2520Li%2520and%2520Heming%2520Liu%2520and%2520Rui%2520Mao%2520and%2520Chang%2520Kong%26entry.1292438233%3DZero-shot%2520Handwritten%2520Chinese%2520Character%2520Recognition%2520%2528HCCR%2529%2520aims%2520to%2520recognize%2520unseen%2520characters%2520by%2520leveraging%2520radical-based%2520semantic%2520compositions.%2520However%252C%2520existing%2520approaches%2520often%2520treat%2520characters%2520as%2520flat%2520radical%2520sequences%252C%2520neglecting%2520the%2520hierarchical%2520topology%2520and%2520the%2520uneven%2520information%2520density%2520of%2520different%2520components.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520Entropy-Aware%2520Structural%2520Alignment%2520Network%2520that%2520bridges%2520the%2520visual-semantic%2520gap%2520through%2520information-theoretic%2520modeling.%2520First%252C%2520we%2520introduce%2520an%2520Information%2520Entropy%2520Prior%2520to%2520dynamically%2520modulate%2520positional%2520embeddings%2520via%2520multiplicative%2520interaction%252C%2520acting%2520as%2520a%2520saliency%2520detector%2520that%2520prioritizes%2520discriminative%2520roots%2520over%2520ubiquitous%2520components.%2520Second%252C%2520we%2520construct%2520a%2520Dual-View%2520Radical%2520Tree%2520to%2520extract%2520multi-granularity%2520structural%2520features%252C%2520which%2520are%2520integrated%2520via%2520an%2520adaptive%2520Sigmoid-based%2520gating%2520network%2520to%2520encode%2520both%2520global%2520layout%2520and%2520local%2520spatial%2520roles.%2520Finally%252C%2520a%2520Top-K%2520Semantic%2520Feature%2520Fusion%2520mechanism%2520is%2520devised%2520to%2520augment%2520the%2520decoding%2520process%2520by%2520utilizing%2520the%2520centroid%2520of%2520semantic%2520neighbors%252C%2520effectively%2520rectifying%2520visual%2520ambiguities%2520through%2520feature-level%2520consensus.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520establishes%2520new%2520state-of-the-art%2520performance%252C%2520achieving%2520an%2520accuracy%2520of%252055.04%255C%2525%2520on%2520the%2520ICDAR%25202013%2520dataset%2520%2528%2524m%253D1500%2524%2529%252C%2520significantly%2520outperforming%2520existing%2520CLIP-based%2520baselines%2520in%2520the%2520challenging%2520zero-shot%2520setting.%2520Furthermore%252C%2520the%2520framework%2520exhibits%2520exceptional%2520data%2520efficiency%252C%2520demonstrating%2520rapid%2520adaptability%2520with%2520minimal%2520support%2520samples%252C%2520achieving%252092.41%255C%2525%2520accuracy%2520with%2520only%2520one%2520support%2520sample%2520per%2520class.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.03913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Aware%20Structural%20Alignment%20for%20Zero-Shot%20Handwritten%20Chinese%20Character%20Recognition&entry.906535625=Qiuming%20Luo%20and%20Tao%20Zeng%20and%20Feng%20Li%20and%20Heming%20Liu%20and%20Rui%20Mao%20and%20Chang%20Kong&entry.1292438233=Zero-shot%20Handwritten%20Chinese%20Character%20Recognition%20%28HCCR%29%20aims%20to%20recognize%20unseen%20characters%20by%20leveraging%20radical-based%20semantic%20compositions.%20However%2C%20existing%20approaches%20often%20treat%20characters%20as%20flat%20radical%20sequences%2C%20neglecting%20the%20hierarchical%20topology%20and%20the%20uneven%20information%20density%20of%20different%20components.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20Entropy-Aware%20Structural%20Alignment%20Network%20that%20bridges%20the%20visual-semantic%20gap%20through%20information-theoretic%20modeling.%20First%2C%20we%20introduce%20an%20Information%20Entropy%20Prior%20to%20dynamically%20modulate%20positional%20embeddings%20via%20multiplicative%20interaction%2C%20acting%20as%20a%20saliency%20detector%20that%20prioritizes%20discriminative%20roots%20over%20ubiquitous%20components.%20Second%2C%20we%20construct%20a%20Dual-View%20Radical%20Tree%20to%20extract%20multi-granularity%20structural%20features%2C%20which%20are%20integrated%20via%20an%20adaptive%20Sigmoid-based%20gating%20network%20to%20encode%20both%20global%20layout%20and%20local%20spatial%20roles.%20Finally%2C%20a%20Top-K%20Semantic%20Feature%20Fusion%20mechanism%20is%20devised%20to%20augment%20the%20decoding%20process%20by%20utilizing%20the%20centroid%20of%20semantic%20neighbors%2C%20effectively%20rectifying%20visual%20ambiguities%20through%20feature-level%20consensus.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20establishes%20new%20state-of-the-art%20performance%2C%20achieving%20an%20accuracy%20of%2055.04%5C%25%20on%20the%20ICDAR%202013%20dataset%20%28%24m%3D1500%24%29%2C%20significantly%20outperforming%20existing%20CLIP-based%20baselines%20in%20the%20challenging%20zero-shot%20setting.%20Furthermore%2C%20the%20framework%20exhibits%20exceptional%20data%20efficiency%2C%20demonstrating%20rapid%20adaptability%20with%20minimal%20support%20samples%2C%20achieving%2092.41%5C%25%20accuracy%20with%20only%20one%20support%20sample%20per%20class.&entry.1838667208=http%3A//arxiv.org/abs/2602.03913v2&entry.124074799=Read"},
{"title": "A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer", "author": "Azka Nasir and Fatima Dossa and Muhammad Ahmed Atif and Mohammad Ahmed Atif", "abstract": "Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments. Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects. Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior. Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.", "link": "http://arxiv.org/abs/2602.09810v1", "date": "2026-02-10", "relevancy": 2.49, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5353}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Controlled%20Study%20of%20Double%20DQN%20and%20Dueling%20DQN%20Under%20Cross-Environment%20Transfer&body=Title%3A%20A%20Controlled%20Study%20of%20Double%20DQN%20and%20Dueling%20DQN%20Under%20Cross-Environment%20Transfer%0AAuthor%3A%20Azka%20Nasir%20and%20Fatima%20Dossa%20and%20Muhammad%20Ahmed%20Atif%20and%20Mohammad%20Ahmed%20Atif%0AAbstract%3A%20Transfer%20learning%20in%20deep%20reinforcement%20learning%20is%20often%20motivated%20by%20improved%20stability%20and%20reduced%20training%20cost%2C%20but%20it%20can%20also%20fail%20under%20substantial%20domain%20shift.%20This%20paper%20presents%20a%20controlled%20empirical%20study%20examining%20how%20architectural%20differences%20between%20Double%20Deep%20Q-Networks%20%28DDQN%29%20and%20Dueling%20DQN%20influence%20transfer%20behavior%20across%20environments.%20Using%20CartPole%20as%20a%20source%20task%20and%20LunarLander%20as%20a%20structurally%20distinct%20target%20task%2C%20we%20evaluate%20a%20fixed%20layer-wise%20representation%20transfer%20protocol%20under%20identical%20hyperparameters%20and%20training%20conditions%2C%20with%20baseline%20agents%20trained%20from%20scratch%20used%20to%20contextualize%20transfer%20effects.%20Empirical%20results%20show%20that%20DDQN%20consistently%20avoids%20negative%20transfer%20under%20the%20examined%20setup%20and%20maintains%20learning%20dynamics%20comparable%20to%20baseline%20performance%20in%20the%20target%20environment.%20In%20contrast%2C%20Dueling%20DQN%20consistently%20exhibits%20negative%20transfer%20under%20identical%20conditions%2C%20characterized%20by%20degraded%20rewards%20and%20unstable%20optimization%20behavior.%20Statistical%20analysis%20across%20multiple%20random%20seeds%20confirms%20a%20significant%20performance%20gap%20under%20transfer.%20These%20findings%20suggest%20that%20architectural%20inductive%20bias%20is%20strongly%20associated%20with%20robustness%20to%20cross-environment%20transfer%20in%20value-based%20deep%20reinforcement%20learning%20under%20the%20examined%20transfer%20protocol.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Controlled%2520Study%2520of%2520Double%2520DQN%2520and%2520Dueling%2520DQN%2520Under%2520Cross-Environment%2520Transfer%26entry.906535625%3DAzka%2520Nasir%2520and%2520Fatima%2520Dossa%2520and%2520Muhammad%2520Ahmed%2520Atif%2520and%2520Mohammad%2520Ahmed%2520Atif%26entry.1292438233%3DTransfer%2520learning%2520in%2520deep%2520reinforcement%2520learning%2520is%2520often%2520motivated%2520by%2520improved%2520stability%2520and%2520reduced%2520training%2520cost%252C%2520but%2520it%2520can%2520also%2520fail%2520under%2520substantial%2520domain%2520shift.%2520This%2520paper%2520presents%2520a%2520controlled%2520empirical%2520study%2520examining%2520how%2520architectural%2520differences%2520between%2520Double%2520Deep%2520Q-Networks%2520%2528DDQN%2529%2520and%2520Dueling%2520DQN%2520influence%2520transfer%2520behavior%2520across%2520environments.%2520Using%2520CartPole%2520as%2520a%2520source%2520task%2520and%2520LunarLander%2520as%2520a%2520structurally%2520distinct%2520target%2520task%252C%2520we%2520evaluate%2520a%2520fixed%2520layer-wise%2520representation%2520transfer%2520protocol%2520under%2520identical%2520hyperparameters%2520and%2520training%2520conditions%252C%2520with%2520baseline%2520agents%2520trained%2520from%2520scratch%2520used%2520to%2520contextualize%2520transfer%2520effects.%2520Empirical%2520results%2520show%2520that%2520DDQN%2520consistently%2520avoids%2520negative%2520transfer%2520under%2520the%2520examined%2520setup%2520and%2520maintains%2520learning%2520dynamics%2520comparable%2520to%2520baseline%2520performance%2520in%2520the%2520target%2520environment.%2520In%2520contrast%252C%2520Dueling%2520DQN%2520consistently%2520exhibits%2520negative%2520transfer%2520under%2520identical%2520conditions%252C%2520characterized%2520by%2520degraded%2520rewards%2520and%2520unstable%2520optimization%2520behavior.%2520Statistical%2520analysis%2520across%2520multiple%2520random%2520seeds%2520confirms%2520a%2520significant%2520performance%2520gap%2520under%2520transfer.%2520These%2520findings%2520suggest%2520that%2520architectural%2520inductive%2520bias%2520is%2520strongly%2520associated%2520with%2520robustness%2520to%2520cross-environment%2520transfer%2520in%2520value-based%2520deep%2520reinforcement%2520learning%2520under%2520the%2520examined%2520transfer%2520protocol.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Controlled%20Study%20of%20Double%20DQN%20and%20Dueling%20DQN%20Under%20Cross-Environment%20Transfer&entry.906535625=Azka%20Nasir%20and%20Fatima%20Dossa%20and%20Muhammad%20Ahmed%20Atif%20and%20Mohammad%20Ahmed%20Atif&entry.1292438233=Transfer%20learning%20in%20deep%20reinforcement%20learning%20is%20often%20motivated%20by%20improved%20stability%20and%20reduced%20training%20cost%2C%20but%20it%20can%20also%20fail%20under%20substantial%20domain%20shift.%20This%20paper%20presents%20a%20controlled%20empirical%20study%20examining%20how%20architectural%20differences%20between%20Double%20Deep%20Q-Networks%20%28DDQN%29%20and%20Dueling%20DQN%20influence%20transfer%20behavior%20across%20environments.%20Using%20CartPole%20as%20a%20source%20task%20and%20LunarLander%20as%20a%20structurally%20distinct%20target%20task%2C%20we%20evaluate%20a%20fixed%20layer-wise%20representation%20transfer%20protocol%20under%20identical%20hyperparameters%20and%20training%20conditions%2C%20with%20baseline%20agents%20trained%20from%20scratch%20used%20to%20contextualize%20transfer%20effects.%20Empirical%20results%20show%20that%20DDQN%20consistently%20avoids%20negative%20transfer%20under%20the%20examined%20setup%20and%20maintains%20learning%20dynamics%20comparable%20to%20baseline%20performance%20in%20the%20target%20environment.%20In%20contrast%2C%20Dueling%20DQN%20consistently%20exhibits%20negative%20transfer%20under%20identical%20conditions%2C%20characterized%20by%20degraded%20rewards%20and%20unstable%20optimization%20behavior.%20Statistical%20analysis%20across%20multiple%20random%20seeds%20confirms%20a%20significant%20performance%20gap%20under%20transfer.%20These%20findings%20suggest%20that%20architectural%20inductive%20bias%20is%20strongly%20associated%20with%20robustness%20to%20cross-environment%20transfer%20in%20value-based%20deep%20reinforcement%20learning%20under%20the%20examined%20transfer%20protocol.&entry.1838667208=http%3A//arxiv.org/abs/2602.09810v1&entry.124074799=Read"},
{"title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering", "author": "Bo Peng and Jie Lu and Guangquan Zhang and Zhen Fang", "abstract": "This paper investigates the recently emerged problem of Language-assisted Image Clustering (LaIC), where textual semantics are leveraged to improve the discriminability of visual representations to facilitate image clustering. Due to the unavailability of true class names, one of core challenges of LaIC lies in how to filter positive nouns, i.e., those semantically close to the images of interest, from unlabeled wild corpus data. Existing filtering strategies are predominantly based on the off-the-shelf feature space learned by CLIP; however, despite being intuitive, these strategies lack a rigorous theoretical foundation. To fill this gap, we propose a novel gradient-based framework, termed as GradNorm, which is theoretically guaranteed and shows strong empirical performance. In particular, we measure the positiveness of each noun based on the magnitude of gradients back-propagated from the cross-entropy between the predicted target distribution and the softmax output. Theoretically, we provide a rigorous error bound to quantify the separability of positive nouns by GradNorm and prove that GradNorm naturally subsumes existing filtering strategies as extremely special cases of itself. Empirically, extensive experiments show that GradNorm achieves the state-of-the-art clustering performance on various benchmarks.", "link": "http://arxiv.org/abs/2510.16335v2", "date": "2026-02-10", "relevancy": 2.4816, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Provable%20Importance%20of%20Gradients%20for%20Language-Assisted%20Image%20Clustering&body=Title%3A%20On%20the%20Provable%20Importance%20of%20Gradients%20for%20Language-Assisted%20Image%20Clustering%0AAuthor%3A%20Bo%20Peng%20and%20Jie%20Lu%20and%20Guangquan%20Zhang%20and%20Zhen%20Fang%0AAbstract%3A%20This%20paper%20investigates%20the%20recently%20emerged%20problem%20of%20Language-assisted%20Image%20Clustering%20%28LaIC%29%2C%20where%20textual%20semantics%20are%20leveraged%20to%20improve%20the%20discriminability%20of%20visual%20representations%20to%20facilitate%20image%20clustering.%20Due%20to%20the%20unavailability%20of%20true%20class%20names%2C%20one%20of%20core%20challenges%20of%20LaIC%20lies%20in%20how%20to%20filter%20positive%20nouns%2C%20i.e.%2C%20those%20semantically%20close%20to%20the%20images%20of%20interest%2C%20from%20unlabeled%20wild%20corpus%20data.%20Existing%20filtering%20strategies%20are%20predominantly%20based%20on%20the%20off-the-shelf%20feature%20space%20learned%20by%20CLIP%3B%20however%2C%20despite%20being%20intuitive%2C%20these%20strategies%20lack%20a%20rigorous%20theoretical%20foundation.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20novel%20gradient-based%20framework%2C%20termed%20as%20GradNorm%2C%20which%20is%20theoretically%20guaranteed%20and%20shows%20strong%20empirical%20performance.%20In%20particular%2C%20we%20measure%20the%20positiveness%20of%20each%20noun%20based%20on%20the%20magnitude%20of%20gradients%20back-propagated%20from%20the%20cross-entropy%20between%20the%20predicted%20target%20distribution%20and%20the%20softmax%20output.%20Theoretically%2C%20we%20provide%20a%20rigorous%20error%20bound%20to%20quantify%20the%20separability%20of%20positive%20nouns%20by%20GradNorm%20and%20prove%20that%20GradNorm%20naturally%20subsumes%20existing%20filtering%20strategies%20as%20extremely%20special%20cases%20of%20itself.%20Empirically%2C%20extensive%20experiments%20show%20that%20GradNorm%20achieves%20the%20state-of-the-art%20clustering%20performance%20on%20various%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Provable%2520Importance%2520of%2520Gradients%2520for%2520Language-Assisted%2520Image%2520Clustering%26entry.906535625%3DBo%2520Peng%2520and%2520Jie%2520Lu%2520and%2520Guangquan%2520Zhang%2520and%2520Zhen%2520Fang%26entry.1292438233%3DThis%2520paper%2520investigates%2520the%2520recently%2520emerged%2520problem%2520of%2520Language-assisted%2520Image%2520Clustering%2520%2528LaIC%2529%252C%2520where%2520textual%2520semantics%2520are%2520leveraged%2520to%2520improve%2520the%2520discriminability%2520of%2520visual%2520representations%2520to%2520facilitate%2520image%2520clustering.%2520Due%2520to%2520the%2520unavailability%2520of%2520true%2520class%2520names%252C%2520one%2520of%2520core%2520challenges%2520of%2520LaIC%2520lies%2520in%2520how%2520to%2520filter%2520positive%2520nouns%252C%2520i.e.%252C%2520those%2520semantically%2520close%2520to%2520the%2520images%2520of%2520interest%252C%2520from%2520unlabeled%2520wild%2520corpus%2520data.%2520Existing%2520filtering%2520strategies%2520are%2520predominantly%2520based%2520on%2520the%2520off-the-shelf%2520feature%2520space%2520learned%2520by%2520CLIP%253B%2520however%252C%2520despite%2520being%2520intuitive%252C%2520these%2520strategies%2520lack%2520a%2520rigorous%2520theoretical%2520foundation.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520gradient-based%2520framework%252C%2520termed%2520as%2520GradNorm%252C%2520which%2520is%2520theoretically%2520guaranteed%2520and%2520shows%2520strong%2520empirical%2520performance.%2520In%2520particular%252C%2520we%2520measure%2520the%2520positiveness%2520of%2520each%2520noun%2520based%2520on%2520the%2520magnitude%2520of%2520gradients%2520back-propagated%2520from%2520the%2520cross-entropy%2520between%2520the%2520predicted%2520target%2520distribution%2520and%2520the%2520softmax%2520output.%2520Theoretically%252C%2520we%2520provide%2520a%2520rigorous%2520error%2520bound%2520to%2520quantify%2520the%2520separability%2520of%2520positive%2520nouns%2520by%2520GradNorm%2520and%2520prove%2520that%2520GradNorm%2520naturally%2520subsumes%2520existing%2520filtering%2520strategies%2520as%2520extremely%2520special%2520cases%2520of%2520itself.%2520Empirically%252C%2520extensive%2520experiments%2520show%2520that%2520GradNorm%2520achieves%2520the%2520state-of-the-art%2520clustering%2520performance%2520on%2520various%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Provable%20Importance%20of%20Gradients%20for%20Language-Assisted%20Image%20Clustering&entry.906535625=Bo%20Peng%20and%20Jie%20Lu%20and%20Guangquan%20Zhang%20and%20Zhen%20Fang&entry.1292438233=This%20paper%20investigates%20the%20recently%20emerged%20problem%20of%20Language-assisted%20Image%20Clustering%20%28LaIC%29%2C%20where%20textual%20semantics%20are%20leveraged%20to%20improve%20the%20discriminability%20of%20visual%20representations%20to%20facilitate%20image%20clustering.%20Due%20to%20the%20unavailability%20of%20true%20class%20names%2C%20one%20of%20core%20challenges%20of%20LaIC%20lies%20in%20how%20to%20filter%20positive%20nouns%2C%20i.e.%2C%20those%20semantically%20close%20to%20the%20images%20of%20interest%2C%20from%20unlabeled%20wild%20corpus%20data.%20Existing%20filtering%20strategies%20are%20predominantly%20based%20on%20the%20off-the-shelf%20feature%20space%20learned%20by%20CLIP%3B%20however%2C%20despite%20being%20intuitive%2C%20these%20strategies%20lack%20a%20rigorous%20theoretical%20foundation.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20novel%20gradient-based%20framework%2C%20termed%20as%20GradNorm%2C%20which%20is%20theoretically%20guaranteed%20and%20shows%20strong%20empirical%20performance.%20In%20particular%2C%20we%20measure%20the%20positiveness%20of%20each%20noun%20based%20on%20the%20magnitude%20of%20gradients%20back-propagated%20from%20the%20cross-entropy%20between%20the%20predicted%20target%20distribution%20and%20the%20softmax%20output.%20Theoretically%2C%20we%20provide%20a%20rigorous%20error%20bound%20to%20quantify%20the%20separability%20of%20positive%20nouns%20by%20GradNorm%20and%20prove%20that%20GradNorm%20naturally%20subsumes%20existing%20filtering%20strategies%20as%20extremely%20special%20cases%20of%20itself.%20Empirically%2C%20extensive%20experiments%20show%20that%20GradNorm%20achieves%20the%20state-of-the-art%20clustering%20performance%20on%20various%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2510.16335v2&entry.124074799=Read"},
{"title": "Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction", "author": "Sizhe Yang and Linning Xu and Hao Li and Juncheng Mu and Jia Zeng and Dahua Lin and Jiangmiao Pang", "abstract": "3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.", "link": "http://arxiv.org/abs/2602.10101v1", "date": "2026-02-10", "relevancy": 2.4686, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.634}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6263}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robo3R%3A%20Enhancing%20Robotic%20Manipulation%20with%20Accurate%20Feed-Forward%203D%20Reconstruction&body=Title%3A%20Robo3R%3A%20Enhancing%20Robotic%20Manipulation%20with%20Accurate%20Feed-Forward%203D%20Reconstruction%0AAuthor%3A%20Sizhe%20Yang%20and%20Linning%20Xu%20and%20Hao%20Li%20and%20Juncheng%20Mu%20and%20Jia%20Zeng%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%203D%20spatial%20perception%20is%20fundamental%20to%20generalizable%20robotic%20manipulation%2C%20yet%20obtaining%20reliable%2C%20high-quality%203D%20geometry%20remains%20challenging.%20Depth%20sensors%20suffer%20from%20noise%20and%20material%20sensitivity%2C%20while%20existing%20reconstruction%20models%20lack%20the%20precision%20and%20metric%20consistency%20required%20for%20physical%20interaction.%20We%20introduce%20Robo3R%2C%20a%20feed-forward%2C%20manipulation-ready%203D%20reconstruction%20model%20that%20predicts%20accurate%2C%20metric-scale%20scene%20geometry%20directly%20from%20RGB%20images%20and%20robot%20states%20in%20real%20time.%20Robo3R%20jointly%20infers%20scale-invariant%20local%20geometry%20and%20relative%20camera%20poses%2C%20which%20are%20unified%20into%20the%20scene%20representation%20in%20the%20canonical%20robot%20frame%20via%20a%20learned%20global%20similarity%20transformation.%20To%20meet%20the%20precision%20demands%20of%20manipulation%2C%20Robo3R%20employs%20a%20masked%20point%20head%20for%20sharp%2C%20fine-grained%20point%20clouds%2C%20and%20a%20keypoint-based%20Perspective-n-Point%20%28PnP%29%20formulation%20to%20refine%20camera%20extrinsics%20and%20global%20alignment.%20Trained%20on%20Robo3R-4M%2C%20a%20curated%20large-scale%20synthetic%20dataset%20with%20four%20million%20high-fidelity%20annotated%20frames%2C%20Robo3R%20consistently%20outperforms%20state-of-the-art%20reconstruction%20methods%20and%20depth%20sensors.%20Across%20downstream%20tasks%20including%20imitation%20learning%2C%20sim-to-real%20transfer%2C%20grasp%20synthesis%2C%20and%20collision-free%20motion%20planning%2C%20we%20observe%20consistent%20gains%20in%20performance%2C%20suggesting%20the%20promise%20of%20this%20alternative%203D%20sensing%20module%20for%20robotic%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobo3R%253A%2520Enhancing%2520Robotic%2520Manipulation%2520with%2520Accurate%2520Feed-Forward%25203D%2520Reconstruction%26entry.906535625%3DSizhe%2520Yang%2520and%2520Linning%2520Xu%2520and%2520Hao%2520Li%2520and%2520Juncheng%2520Mu%2520and%2520Jia%2520Zeng%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D3D%2520spatial%2520perception%2520is%2520fundamental%2520to%2520generalizable%2520robotic%2520manipulation%252C%2520yet%2520obtaining%2520reliable%252C%2520high-quality%25203D%2520geometry%2520remains%2520challenging.%2520Depth%2520sensors%2520suffer%2520from%2520noise%2520and%2520material%2520sensitivity%252C%2520while%2520existing%2520reconstruction%2520models%2520lack%2520the%2520precision%2520and%2520metric%2520consistency%2520required%2520for%2520physical%2520interaction.%2520We%2520introduce%2520Robo3R%252C%2520a%2520feed-forward%252C%2520manipulation-ready%25203D%2520reconstruction%2520model%2520that%2520predicts%2520accurate%252C%2520metric-scale%2520scene%2520geometry%2520directly%2520from%2520RGB%2520images%2520and%2520robot%2520states%2520in%2520real%2520time.%2520Robo3R%2520jointly%2520infers%2520scale-invariant%2520local%2520geometry%2520and%2520relative%2520camera%2520poses%252C%2520which%2520are%2520unified%2520into%2520the%2520scene%2520representation%2520in%2520the%2520canonical%2520robot%2520frame%2520via%2520a%2520learned%2520global%2520similarity%2520transformation.%2520To%2520meet%2520the%2520precision%2520demands%2520of%2520manipulation%252C%2520Robo3R%2520employs%2520a%2520masked%2520point%2520head%2520for%2520sharp%252C%2520fine-grained%2520point%2520clouds%252C%2520and%2520a%2520keypoint-based%2520Perspective-n-Point%2520%2528PnP%2529%2520formulation%2520to%2520refine%2520camera%2520extrinsics%2520and%2520global%2520alignment.%2520Trained%2520on%2520Robo3R-4M%252C%2520a%2520curated%2520large-scale%2520synthetic%2520dataset%2520with%2520four%2520million%2520high-fidelity%2520annotated%2520frames%252C%2520Robo3R%2520consistently%2520outperforms%2520state-of-the-art%2520reconstruction%2520methods%2520and%2520depth%2520sensors.%2520Across%2520downstream%2520tasks%2520including%2520imitation%2520learning%252C%2520sim-to-real%2520transfer%252C%2520grasp%2520synthesis%252C%2520and%2520collision-free%2520motion%2520planning%252C%2520we%2520observe%2520consistent%2520gains%2520in%2520performance%252C%2520suggesting%2520the%2520promise%2520of%2520this%2520alternative%25203D%2520sensing%2520module%2520for%2520robotic%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robo3R%3A%20Enhancing%20Robotic%20Manipulation%20with%20Accurate%20Feed-Forward%203D%20Reconstruction&entry.906535625=Sizhe%20Yang%20and%20Linning%20Xu%20and%20Hao%20Li%20and%20Juncheng%20Mu%20and%20Jia%20Zeng%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=3D%20spatial%20perception%20is%20fundamental%20to%20generalizable%20robotic%20manipulation%2C%20yet%20obtaining%20reliable%2C%20high-quality%203D%20geometry%20remains%20challenging.%20Depth%20sensors%20suffer%20from%20noise%20and%20material%20sensitivity%2C%20while%20existing%20reconstruction%20models%20lack%20the%20precision%20and%20metric%20consistency%20required%20for%20physical%20interaction.%20We%20introduce%20Robo3R%2C%20a%20feed-forward%2C%20manipulation-ready%203D%20reconstruction%20model%20that%20predicts%20accurate%2C%20metric-scale%20scene%20geometry%20directly%20from%20RGB%20images%20and%20robot%20states%20in%20real%20time.%20Robo3R%20jointly%20infers%20scale-invariant%20local%20geometry%20and%20relative%20camera%20poses%2C%20which%20are%20unified%20into%20the%20scene%20representation%20in%20the%20canonical%20robot%20frame%20via%20a%20learned%20global%20similarity%20transformation.%20To%20meet%20the%20precision%20demands%20of%20manipulation%2C%20Robo3R%20employs%20a%20masked%20point%20head%20for%20sharp%2C%20fine-grained%20point%20clouds%2C%20and%20a%20keypoint-based%20Perspective-n-Point%20%28PnP%29%20formulation%20to%20refine%20camera%20extrinsics%20and%20global%20alignment.%20Trained%20on%20Robo3R-4M%2C%20a%20curated%20large-scale%20synthetic%20dataset%20with%20four%20million%20high-fidelity%20annotated%20frames%2C%20Robo3R%20consistently%20outperforms%20state-of-the-art%20reconstruction%20methods%20and%20depth%20sensors.%20Across%20downstream%20tasks%20including%20imitation%20learning%2C%20sim-to-real%20transfer%2C%20grasp%20synthesis%2C%20and%20collision-free%20motion%20planning%2C%20we%20observe%20consistent%20gains%20in%20performance%2C%20suggesting%20the%20promise%20of%20this%20alternative%203D%20sensing%20module%20for%20robotic%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2602.10101v1&entry.124074799=Read"},
{"title": "DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos", "author": "Juncheng Mu and Sizhe Yang and Yiming Bao and Hojin Bae and Tianming Wei and Linning Xu and Boyi Li and Huazhe Xu and Jiangmiao Pang", "abstract": "Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).", "link": "http://arxiv.org/abs/2602.10105v1", "date": "2026-02-10", "relevancy": 2.4462, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6457}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5891}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexImit%3A%20Learning%20Bimanual%20Dexterous%20Manipulation%20from%20Monocular%20Human%20Videos&body=Title%3A%20DexImit%3A%20Learning%20Bimanual%20Dexterous%20Manipulation%20from%20Monocular%20Human%20Videos%0AAuthor%3A%20Juncheng%20Mu%20and%20Sizhe%20Yang%20and%20Yiming%20Bao%20and%20Hojin%20Bae%20and%20Tianming%20Wei%20and%20Linning%20Xu%20and%20Boyi%20Li%20and%20Huazhe%20Xu%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Data%20scarcity%20fundamentally%20limits%20the%20generalization%20of%20bimanual%20dexterous%20manipulation%2C%20as%20real-world%20data%20collection%20for%20dexterous%20hands%20is%20expensive%20and%20labor-intensive.%20Human%20manipulation%20videos%2C%20as%20a%20direct%20carrier%20of%20manipulation%20knowledge%2C%20offer%20significant%20potential%20for%20scaling%20up%20robot%20learning.%20However%2C%20the%20substantial%20embodiment%20gap%20between%20human%20hands%20and%20robotic%20dexterous%20hands%20makes%20direct%20pretraining%20from%20human%20videos%20extremely%20challenging.%20To%20bridge%20this%20gap%20and%20unleash%20the%20potential%20of%20large-scale%20human%20manipulation%20video%20data%2C%20we%20propose%20DexImit%2C%20an%20automated%20framework%20that%20converts%20monocular%20human%20manipulation%20videos%20into%20physically%20plausible%20robot%20data%2C%20without%20any%20additional%20information.%20DexImit%20employs%20a%20four-stage%20generation%20pipeline%3A%20%281%29%20reconstructing%20hand-object%20interactions%20from%20arbitrary%20viewpoints%20with%20near-metric%20scale%3B%20%282%29%20performing%20subtask%20decomposition%20and%20bimanual%20scheduling%3B%20%283%29%20synthesizing%20robot%20trajectories%20consistent%20with%20the%20demonstrated%20interactions%3B%20%284%29%20comprehensive%20data%20augmentation%20for%20zero-shot%20real-world%20deployment.%20Building%20on%20these%20designs%2C%20DexImit%20can%20generate%20large-scale%20robot%20data%20based%20on%20human%20videos%2C%20either%20from%20the%20Internet%20or%20video%20generation%20models.%20DexImit%20is%20capable%20of%20handling%20diverse%20manipulation%20tasks%2C%20including%20tool%20use%20%28e.g.%2C%20cutting%20an%20apple%29%2C%20long-horizon%20tasks%20%28e.g.%2C%20making%20a%20beverage%29%2C%20and%20fine-grained%20manipulations%20%28e.g.%2C%20stacking%20cups%29.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexImit%253A%2520Learning%2520Bimanual%2520Dexterous%2520Manipulation%2520from%2520Monocular%2520Human%2520Videos%26entry.906535625%3DJuncheng%2520Mu%2520and%2520Sizhe%2520Yang%2520and%2520Yiming%2520Bao%2520and%2520Hojin%2520Bae%2520and%2520Tianming%2520Wei%2520and%2520Linning%2520Xu%2520and%2520Boyi%2520Li%2520and%2520Huazhe%2520Xu%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DData%2520scarcity%2520fundamentally%2520limits%2520the%2520generalization%2520of%2520bimanual%2520dexterous%2520manipulation%252C%2520as%2520real-world%2520data%2520collection%2520for%2520dexterous%2520hands%2520is%2520expensive%2520and%2520labor-intensive.%2520Human%2520manipulation%2520videos%252C%2520as%2520a%2520direct%2520carrier%2520of%2520manipulation%2520knowledge%252C%2520offer%2520significant%2520potential%2520for%2520scaling%2520up%2520robot%2520learning.%2520However%252C%2520the%2520substantial%2520embodiment%2520gap%2520between%2520human%2520hands%2520and%2520robotic%2520dexterous%2520hands%2520makes%2520direct%2520pretraining%2520from%2520human%2520videos%2520extremely%2520challenging.%2520To%2520bridge%2520this%2520gap%2520and%2520unleash%2520the%2520potential%2520of%2520large-scale%2520human%2520manipulation%2520video%2520data%252C%2520we%2520propose%2520DexImit%252C%2520an%2520automated%2520framework%2520that%2520converts%2520monocular%2520human%2520manipulation%2520videos%2520into%2520physically%2520plausible%2520robot%2520data%252C%2520without%2520any%2520additional%2520information.%2520DexImit%2520employs%2520a%2520four-stage%2520generation%2520pipeline%253A%2520%25281%2529%2520reconstructing%2520hand-object%2520interactions%2520from%2520arbitrary%2520viewpoints%2520with%2520near-metric%2520scale%253B%2520%25282%2529%2520performing%2520subtask%2520decomposition%2520and%2520bimanual%2520scheduling%253B%2520%25283%2529%2520synthesizing%2520robot%2520trajectories%2520consistent%2520with%2520the%2520demonstrated%2520interactions%253B%2520%25284%2529%2520comprehensive%2520data%2520augmentation%2520for%2520zero-shot%2520real-world%2520deployment.%2520Building%2520on%2520these%2520designs%252C%2520DexImit%2520can%2520generate%2520large-scale%2520robot%2520data%2520based%2520on%2520human%2520videos%252C%2520either%2520from%2520the%2520Internet%2520or%2520video%2520generation%2520models.%2520DexImit%2520is%2520capable%2520of%2520handling%2520diverse%2520manipulation%2520tasks%252C%2520including%2520tool%2520use%2520%2528e.g.%252C%2520cutting%2520an%2520apple%2529%252C%2520long-horizon%2520tasks%2520%2528e.g.%252C%2520making%2520a%2520beverage%2529%252C%2520and%2520fine-grained%2520manipulations%2520%2528e.g.%252C%2520stacking%2520cups%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexImit%3A%20Learning%20Bimanual%20Dexterous%20Manipulation%20from%20Monocular%20Human%20Videos&entry.906535625=Juncheng%20Mu%20and%20Sizhe%20Yang%20and%20Yiming%20Bao%20and%20Hojin%20Bae%20and%20Tianming%20Wei%20and%20Linning%20Xu%20and%20Boyi%20Li%20and%20Huazhe%20Xu%20and%20Jiangmiao%20Pang&entry.1292438233=Data%20scarcity%20fundamentally%20limits%20the%20generalization%20of%20bimanual%20dexterous%20manipulation%2C%20as%20real-world%20data%20collection%20for%20dexterous%20hands%20is%20expensive%20and%20labor-intensive.%20Human%20manipulation%20videos%2C%20as%20a%20direct%20carrier%20of%20manipulation%20knowledge%2C%20offer%20significant%20potential%20for%20scaling%20up%20robot%20learning.%20However%2C%20the%20substantial%20embodiment%20gap%20between%20human%20hands%20and%20robotic%20dexterous%20hands%20makes%20direct%20pretraining%20from%20human%20videos%20extremely%20challenging.%20To%20bridge%20this%20gap%20and%20unleash%20the%20potential%20of%20large-scale%20human%20manipulation%20video%20data%2C%20we%20propose%20DexImit%2C%20an%20automated%20framework%20that%20converts%20monocular%20human%20manipulation%20videos%20into%20physically%20plausible%20robot%20data%2C%20without%20any%20additional%20information.%20DexImit%20employs%20a%20four-stage%20generation%20pipeline%3A%20%281%29%20reconstructing%20hand-object%20interactions%20from%20arbitrary%20viewpoints%20with%20near-metric%20scale%3B%20%282%29%20performing%20subtask%20decomposition%20and%20bimanual%20scheduling%3B%20%283%29%20synthesizing%20robot%20trajectories%20consistent%20with%20the%20demonstrated%20interactions%3B%20%284%29%20comprehensive%20data%20augmentation%20for%20zero-shot%20real-world%20deployment.%20Building%20on%20these%20designs%2C%20DexImit%20can%20generate%20large-scale%20robot%20data%20based%20on%20human%20videos%2C%20either%20from%20the%20Internet%20or%20video%20generation%20models.%20DexImit%20is%20capable%20of%20handling%20diverse%20manipulation%20tasks%2C%20including%20tool%20use%20%28e.g.%2C%20cutting%20an%20apple%29%2C%20long-horizon%20tasks%20%28e.g.%2C%20making%20a%20beverage%29%2C%20and%20fine-grained%20manipulations%20%28e.g.%2C%20stacking%20cups%29.&entry.1838667208=http%3A//arxiv.org/abs/2602.10105v1&entry.124074799=Read"},
{"title": "HiCL: Hippocampal-Inspired Continual Learning", "author": "Kushal Kapoor and Wyatt Mackey and Yiannis Aloimonos and Xiaomin Lin", "abstract": "We propose HiCL, a novel hippocampal-inspired dual-memory continual learning architecture designed to mitigate catastrophic forgetting by using elements inspired by the hippocampal circuitry. Our system encodes inputs through a grid-cell-like layer, followed by sparse pattern separation using a dentate gyrus-inspired module with top-k sparsity. Episodic memory traces are maintained in a CA3-like autoassociative memory. Task-specific processing is dynamically managed via a DG-gated mixture-of-experts mechanism, wherein inputs are routed to experts based on cosine similarity between their normalized sparse DG representations and learned task-specific DG prototypes computed through online exponential moving averages. This biologically grounded yet mathematically principled gating strategy enables differentiable, scalable task-routing without relying on a separate gating network, and enhances the model's adaptability and efficiency in learning multiple sequential tasks. Cortical outputs are consolidated using Elastic Weight Consolidation weighted by inter-task similarity. Crucially, we incorporate prioritized replay of stored patterns to reinforce essential past experiences. Evaluations on standard continual learning benchmarks demonstrate the effectiveness of our architecture in reducing task interference, achieving near state-of-the-art results in continual learning tasks at lower computational costs. Our code is available here https://github.com/kushalk173-sc/HiCL.", "link": "http://arxiv.org/abs/2508.16651v3", "date": "2026-02-10", "relevancy": 2.446, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4787}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiCL%3A%20Hippocampal-Inspired%20Continual%20Learning&body=Title%3A%20HiCL%3A%20Hippocampal-Inspired%20Continual%20Learning%0AAuthor%3A%20Kushal%20Kapoor%20and%20Wyatt%20Mackey%20and%20Yiannis%20Aloimonos%20and%20Xiaomin%20Lin%0AAbstract%3A%20We%20propose%20HiCL%2C%20a%20novel%20hippocampal-inspired%20dual-memory%20continual%20learning%20architecture%20designed%20to%20mitigate%20catastrophic%20forgetting%20by%20using%20elements%20inspired%20by%20the%20hippocampal%20circuitry.%20Our%20system%20encodes%20inputs%20through%20a%20grid-cell-like%20layer%2C%20followed%20by%20sparse%20pattern%20separation%20using%20a%20dentate%20gyrus-inspired%20module%20with%20top-k%20sparsity.%20Episodic%20memory%20traces%20are%20maintained%20in%20a%20CA3-like%20autoassociative%20memory.%20Task-specific%20processing%20is%20dynamically%20managed%20via%20a%20DG-gated%20mixture-of-experts%20mechanism%2C%20wherein%20inputs%20are%20routed%20to%20experts%20based%20on%20cosine%20similarity%20between%20their%20normalized%20sparse%20DG%20representations%20and%20learned%20task-specific%20DG%20prototypes%20computed%20through%20online%20exponential%20moving%20averages.%20This%20biologically%20grounded%20yet%20mathematically%20principled%20gating%20strategy%20enables%20differentiable%2C%20scalable%20task-routing%20without%20relying%20on%20a%20separate%20gating%20network%2C%20and%20enhances%20the%20model%27s%20adaptability%20and%20efficiency%20in%20learning%20multiple%20sequential%20tasks.%20Cortical%20outputs%20are%20consolidated%20using%20Elastic%20Weight%20Consolidation%20weighted%20by%20inter-task%20similarity.%20Crucially%2C%20we%20incorporate%20prioritized%20replay%20of%20stored%20patterns%20to%20reinforce%20essential%20past%20experiences.%20Evaluations%20on%20standard%20continual%20learning%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20architecture%20in%20reducing%20task%20interference%2C%20achieving%20near%20state-of-the-art%20results%20in%20continual%20learning%20tasks%20at%20lower%20computational%20costs.%20Our%20code%20is%20available%20here%20https%3A//github.com/kushalk173-sc/HiCL.%0ALink%3A%20http%3A//arxiv.org/abs/2508.16651v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiCL%253A%2520Hippocampal-Inspired%2520Continual%2520Learning%26entry.906535625%3DKushal%2520Kapoor%2520and%2520Wyatt%2520Mackey%2520and%2520Yiannis%2520Aloimonos%2520and%2520Xiaomin%2520Lin%26entry.1292438233%3DWe%2520propose%2520HiCL%252C%2520a%2520novel%2520hippocampal-inspired%2520dual-memory%2520continual%2520learning%2520architecture%2520designed%2520to%2520mitigate%2520catastrophic%2520forgetting%2520by%2520using%2520elements%2520inspired%2520by%2520the%2520hippocampal%2520circuitry.%2520Our%2520system%2520encodes%2520inputs%2520through%2520a%2520grid-cell-like%2520layer%252C%2520followed%2520by%2520sparse%2520pattern%2520separation%2520using%2520a%2520dentate%2520gyrus-inspired%2520module%2520with%2520top-k%2520sparsity.%2520Episodic%2520memory%2520traces%2520are%2520maintained%2520in%2520a%2520CA3-like%2520autoassociative%2520memory.%2520Task-specific%2520processing%2520is%2520dynamically%2520managed%2520via%2520a%2520DG-gated%2520mixture-of-experts%2520mechanism%252C%2520wherein%2520inputs%2520are%2520routed%2520to%2520experts%2520based%2520on%2520cosine%2520similarity%2520between%2520their%2520normalized%2520sparse%2520DG%2520representations%2520and%2520learned%2520task-specific%2520DG%2520prototypes%2520computed%2520through%2520online%2520exponential%2520moving%2520averages.%2520This%2520biologically%2520grounded%2520yet%2520mathematically%2520principled%2520gating%2520strategy%2520enables%2520differentiable%252C%2520scalable%2520task-routing%2520without%2520relying%2520on%2520a%2520separate%2520gating%2520network%252C%2520and%2520enhances%2520the%2520model%2527s%2520adaptability%2520and%2520efficiency%2520in%2520learning%2520multiple%2520sequential%2520tasks.%2520Cortical%2520outputs%2520are%2520consolidated%2520using%2520Elastic%2520Weight%2520Consolidation%2520weighted%2520by%2520inter-task%2520similarity.%2520Crucially%252C%2520we%2520incorporate%2520prioritized%2520replay%2520of%2520stored%2520patterns%2520to%2520reinforce%2520essential%2520past%2520experiences.%2520Evaluations%2520on%2520standard%2520continual%2520learning%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520architecture%2520in%2520reducing%2520task%2520interference%252C%2520achieving%2520near%2520state-of-the-art%2520results%2520in%2520continual%2520learning%2520tasks%2520at%2520lower%2520computational%2520costs.%2520Our%2520code%2520is%2520available%2520here%2520https%253A//github.com/kushalk173-sc/HiCL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16651v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiCL%3A%20Hippocampal-Inspired%20Continual%20Learning&entry.906535625=Kushal%20Kapoor%20and%20Wyatt%20Mackey%20and%20Yiannis%20Aloimonos%20and%20Xiaomin%20Lin&entry.1292438233=We%20propose%20HiCL%2C%20a%20novel%20hippocampal-inspired%20dual-memory%20continual%20learning%20architecture%20designed%20to%20mitigate%20catastrophic%20forgetting%20by%20using%20elements%20inspired%20by%20the%20hippocampal%20circuitry.%20Our%20system%20encodes%20inputs%20through%20a%20grid-cell-like%20layer%2C%20followed%20by%20sparse%20pattern%20separation%20using%20a%20dentate%20gyrus-inspired%20module%20with%20top-k%20sparsity.%20Episodic%20memory%20traces%20are%20maintained%20in%20a%20CA3-like%20autoassociative%20memory.%20Task-specific%20processing%20is%20dynamically%20managed%20via%20a%20DG-gated%20mixture-of-experts%20mechanism%2C%20wherein%20inputs%20are%20routed%20to%20experts%20based%20on%20cosine%20similarity%20between%20their%20normalized%20sparse%20DG%20representations%20and%20learned%20task-specific%20DG%20prototypes%20computed%20through%20online%20exponential%20moving%20averages.%20This%20biologically%20grounded%20yet%20mathematically%20principled%20gating%20strategy%20enables%20differentiable%2C%20scalable%20task-routing%20without%20relying%20on%20a%20separate%20gating%20network%2C%20and%20enhances%20the%20model%27s%20adaptability%20and%20efficiency%20in%20learning%20multiple%20sequential%20tasks.%20Cortical%20outputs%20are%20consolidated%20using%20Elastic%20Weight%20Consolidation%20weighted%20by%20inter-task%20similarity.%20Crucially%2C%20we%20incorporate%20prioritized%20replay%20of%20stored%20patterns%20to%20reinforce%20essential%20past%20experiences.%20Evaluations%20on%20standard%20continual%20learning%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20architecture%20in%20reducing%20task%20interference%2C%20achieving%20near%20state-of-the-art%20results%20in%20continual%20learning%20tasks%20at%20lower%20computational%20costs.%20Our%20code%20is%20available%20here%20https%3A//github.com/kushalk173-sc/HiCL.&entry.1838667208=http%3A//arxiv.org/abs/2508.16651v3&entry.124074799=Read"},
{"title": "Generative AI and Firm Productivity: Field Experiments in Online Retail", "author": "Lu Fang and Zhe Yuan and Kaifu Zhang and Dante Donati and Miklos Sarvary", "abstract": "We quantify the impact of Generative Artificial Intelligence (GenAI) on firm productivity through a series of large-scale randomized field experiments involving millions of users and products at a leading cross-border online retail platform. Over six months in 2023-2024, GenAI-based enhancements were integrated into seven consumer-facing business workflows. We find that GenAI adoption significantly increases sales, with treatment effects ranging from $0\\%$ to $16.3\\%$, depending on GenAI's marginal contribution relative to existing firm practices. Because inputs and prices were held constant across experimental arms, these gains map directly into total factor productivity improvements. Across the four GenAI applications with positive sales effects, the implied annual incremental value is approximately $\\$ 5$ per consumer-an economically meaningful impact given the retailer's scale and the early stage of GenAI adoption. The primary mechanism operates through higher conversion rates, consistent with GenAI reducing frictions and improving consumer experience. Importantly, these effects are not associated with worse post-purchase outcomes, as product return rates and customer ratings do not deteriorate. Finally, we document substantial demand-side heterogeneity, with larger gains for less experienced consumers. Our findings provide novel, large-scale causal evidence on the productivity effects of GenAI in online retail, highlighting both its immediate value and broader potential.", "link": "http://arxiv.org/abs/2510.12049v3", "date": "2026-02-10", "relevancy": 2.4364, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5139}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.478}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20and%20Firm%20Productivity%3A%20Field%20Experiments%20in%20Online%20Retail&body=Title%3A%20Generative%20AI%20and%20Firm%20Productivity%3A%20Field%20Experiments%20in%20Online%20Retail%0AAuthor%3A%20Lu%20Fang%20and%20Zhe%20Yuan%20and%20Kaifu%20Zhang%20and%20Dante%20Donati%20and%20Miklos%20Sarvary%0AAbstract%3A%20We%20quantify%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20on%20firm%20productivity%20through%20a%20series%20of%20large-scale%20randomized%20field%20experiments%20involving%20millions%20of%20users%20and%20products%20at%20a%20leading%20cross-border%20online%20retail%20platform.%20Over%20six%20months%20in%202023-2024%2C%20GenAI-based%20enhancements%20were%20integrated%20into%20seven%20consumer-facing%20business%20workflows.%20We%20find%20that%20GenAI%20adoption%20significantly%20increases%20sales%2C%20with%20treatment%20effects%20ranging%20from%20%240%5C%25%24%20to%20%2416.3%5C%25%24%2C%20depending%20on%20GenAI%27s%20marginal%20contribution%20relative%20to%20existing%20firm%20practices.%20Because%20inputs%20and%20prices%20were%20held%20constant%20across%20experimental%20arms%2C%20these%20gains%20map%20directly%20into%20total%20factor%20productivity%20improvements.%20Across%20the%20four%20GenAI%20applications%20with%20positive%20sales%20effects%2C%20the%20implied%20annual%20incremental%20value%20is%20approximately%20%24%5C%24%205%24%20per%20consumer-an%20economically%20meaningful%20impact%20given%20the%20retailer%27s%20scale%20and%20the%20early%20stage%20of%20GenAI%20adoption.%20The%20primary%20mechanism%20operates%20through%20higher%20conversion%20rates%2C%20consistent%20with%20GenAI%20reducing%20frictions%20and%20improving%20consumer%20experience.%20Importantly%2C%20these%20effects%20are%20not%20associated%20with%20worse%20post-purchase%20outcomes%2C%20as%20product%20return%20rates%20and%20customer%20ratings%20do%20not%20deteriorate.%20Finally%2C%20we%20document%20substantial%20demand-side%20heterogeneity%2C%20with%20larger%20gains%20for%20less%20experienced%20consumers.%20Our%20findings%20provide%20novel%2C%20large-scale%20causal%20evidence%20on%20the%20productivity%20effects%20of%20GenAI%20in%20online%20retail%2C%20highlighting%20both%20its%20immediate%20value%20and%20broader%20potential.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12049v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520and%2520Firm%2520Productivity%253A%2520Field%2520Experiments%2520in%2520Online%2520Retail%26entry.906535625%3DLu%2520Fang%2520and%2520Zhe%2520Yuan%2520and%2520Kaifu%2520Zhang%2520and%2520Dante%2520Donati%2520and%2520Miklos%2520Sarvary%26entry.1292438233%3DWe%2520quantify%2520the%2520impact%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520on%2520firm%2520productivity%2520through%2520a%2520series%2520of%2520large-scale%2520randomized%2520field%2520experiments%2520involving%2520millions%2520of%2520users%2520and%2520products%2520at%2520a%2520leading%2520cross-border%2520online%2520retail%2520platform.%2520Over%2520six%2520months%2520in%25202023-2024%252C%2520GenAI-based%2520enhancements%2520were%2520integrated%2520into%2520seven%2520consumer-facing%2520business%2520workflows.%2520We%2520find%2520that%2520GenAI%2520adoption%2520significantly%2520increases%2520sales%252C%2520with%2520treatment%2520effects%2520ranging%2520from%2520%25240%255C%2525%2524%2520to%2520%252416.3%255C%2525%2524%252C%2520depending%2520on%2520GenAI%2527s%2520marginal%2520contribution%2520relative%2520to%2520existing%2520firm%2520practices.%2520Because%2520inputs%2520and%2520prices%2520were%2520held%2520constant%2520across%2520experimental%2520arms%252C%2520these%2520gains%2520map%2520directly%2520into%2520total%2520factor%2520productivity%2520improvements.%2520Across%2520the%2520four%2520GenAI%2520applications%2520with%2520positive%2520sales%2520effects%252C%2520the%2520implied%2520annual%2520incremental%2520value%2520is%2520approximately%2520%2524%255C%2524%25205%2524%2520per%2520consumer-an%2520economically%2520meaningful%2520impact%2520given%2520the%2520retailer%2527s%2520scale%2520and%2520the%2520early%2520stage%2520of%2520GenAI%2520adoption.%2520The%2520primary%2520mechanism%2520operates%2520through%2520higher%2520conversion%2520rates%252C%2520consistent%2520with%2520GenAI%2520reducing%2520frictions%2520and%2520improving%2520consumer%2520experience.%2520Importantly%252C%2520these%2520effects%2520are%2520not%2520associated%2520with%2520worse%2520post-purchase%2520outcomes%252C%2520as%2520product%2520return%2520rates%2520and%2520customer%2520ratings%2520do%2520not%2520deteriorate.%2520Finally%252C%2520we%2520document%2520substantial%2520demand-side%2520heterogeneity%252C%2520with%2520larger%2520gains%2520for%2520less%2520experienced%2520consumers.%2520Our%2520findings%2520provide%2520novel%252C%2520large-scale%2520causal%2520evidence%2520on%2520the%2520productivity%2520effects%2520of%2520GenAI%2520in%2520online%2520retail%252C%2520highlighting%2520both%2520its%2520immediate%2520value%2520and%2520broader%2520potential.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12049v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20and%20Firm%20Productivity%3A%20Field%20Experiments%20in%20Online%20Retail&entry.906535625=Lu%20Fang%20and%20Zhe%20Yuan%20and%20Kaifu%20Zhang%20and%20Dante%20Donati%20and%20Miklos%20Sarvary&entry.1292438233=We%20quantify%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20on%20firm%20productivity%20through%20a%20series%20of%20large-scale%20randomized%20field%20experiments%20involving%20millions%20of%20users%20and%20products%20at%20a%20leading%20cross-border%20online%20retail%20platform.%20Over%20six%20months%20in%202023-2024%2C%20GenAI-based%20enhancements%20were%20integrated%20into%20seven%20consumer-facing%20business%20workflows.%20We%20find%20that%20GenAI%20adoption%20significantly%20increases%20sales%2C%20with%20treatment%20effects%20ranging%20from%20%240%5C%25%24%20to%20%2416.3%5C%25%24%2C%20depending%20on%20GenAI%27s%20marginal%20contribution%20relative%20to%20existing%20firm%20practices.%20Because%20inputs%20and%20prices%20were%20held%20constant%20across%20experimental%20arms%2C%20these%20gains%20map%20directly%20into%20total%20factor%20productivity%20improvements.%20Across%20the%20four%20GenAI%20applications%20with%20positive%20sales%20effects%2C%20the%20implied%20annual%20incremental%20value%20is%20approximately%20%24%5C%24%205%24%20per%20consumer-an%20economically%20meaningful%20impact%20given%20the%20retailer%27s%20scale%20and%20the%20early%20stage%20of%20GenAI%20adoption.%20The%20primary%20mechanism%20operates%20through%20higher%20conversion%20rates%2C%20consistent%20with%20GenAI%20reducing%20frictions%20and%20improving%20consumer%20experience.%20Importantly%2C%20these%20effects%20are%20not%20associated%20with%20worse%20post-purchase%20outcomes%2C%20as%20product%20return%20rates%20and%20customer%20ratings%20do%20not%20deteriorate.%20Finally%2C%20we%20document%20substantial%20demand-side%20heterogeneity%2C%20with%20larger%20gains%20for%20less%20experienced%20consumers.%20Our%20findings%20provide%20novel%2C%20large-scale%20causal%20evidence%20on%20the%20productivity%20effects%20of%20GenAI%20in%20online%20retail%2C%20highlighting%20both%20its%20immediate%20value%20and%20broader%20potential.&entry.1838667208=http%3A//arxiv.org/abs/2510.12049v3&entry.124074799=Read"},
{"title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models", "author": "Elchanan Mossel", "abstract": "Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage.", "link": "http://arxiv.org/abs/2601.02380v3", "date": "2026-02-10", "relevancy": 2.42, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Refutability%20Gap%3A%20Challenges%20in%20Validating%20Reasoning%20by%20Large%20Language%20Models&body=Title%3A%20The%20Refutability%20Gap%3A%20Challenges%20in%20Validating%20Reasoning%20by%20Large%20Language%20Models%0AAuthor%3A%20Elchanan%20Mossel%0AAbstract%3A%20Recent%20reports%20claim%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20the%20ability%20to%20derive%20new%20science%20and%20exhibit%20human-level%20general%20intelligence.%20We%20argue%20that%20such%20claims%20are%20not%20rigorous%20scientific%20claims%2C%20as%20they%20do%20not%20satisfy%20Popper%27s%20refutability%20principle%20%28often%20termed%20falsifiability%29%2C%20which%20requires%20that%20scientific%20statements%20be%20capable%20of%20being%20disproven.%20We%20identify%20several%20methodological%20pitfalls%20in%20current%20AI%20research%20on%20reasoning%2C%20including%20the%20inability%20to%20verify%20the%20novelty%20of%20findings%20due%20to%20opaque%20and%20non-searchable%20training%20data%2C%20the%20lack%20of%20reproducibility%20caused%20by%20continuous%20model%20updates%2C%20and%20the%20omission%20of%20human-interaction%20transcripts%2C%20which%20obscures%20the%20true%20source%20of%20scientific%20discovery.%20Additionally%2C%20the%20absence%20of%20counterfactuals%20and%20data%20on%20failed%20attempts%20creates%20a%20selection%20bias%20that%20may%20exaggerate%20LLM%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%20guidelines%20for%20scientific%20transparency%20and%20reproducibility%20for%20research%20on%20reasoning%20by%20LLMs.%20Establishing%20such%20guidelines%20is%20crucial%20for%20both%20scientific%20integrity%20and%20the%20ongoing%20societal%20debates%20regarding%20fair%20data%20usage.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02380v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Refutability%2520Gap%253A%2520Challenges%2520in%2520Validating%2520Reasoning%2520by%2520Large%2520Language%2520Models%26entry.906535625%3DElchanan%2520Mossel%26entry.1292438233%3DRecent%2520reports%2520claim%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520the%2520ability%2520to%2520derive%2520new%2520science%2520and%2520exhibit%2520human-level%2520general%2520intelligence.%2520We%2520argue%2520that%2520such%2520claims%2520are%2520not%2520rigorous%2520scientific%2520claims%252C%2520as%2520they%2520do%2520not%2520satisfy%2520Popper%2527s%2520refutability%2520principle%2520%2528often%2520termed%2520falsifiability%2529%252C%2520which%2520requires%2520that%2520scientific%2520statements%2520be%2520capable%2520of%2520being%2520disproven.%2520We%2520identify%2520several%2520methodological%2520pitfalls%2520in%2520current%2520AI%2520research%2520on%2520reasoning%252C%2520including%2520the%2520inability%2520to%2520verify%2520the%2520novelty%2520of%2520findings%2520due%2520to%2520opaque%2520and%2520non-searchable%2520training%2520data%252C%2520the%2520lack%2520of%2520reproducibility%2520caused%2520by%2520continuous%2520model%2520updates%252C%2520and%2520the%2520omission%2520of%2520human-interaction%2520transcripts%252C%2520which%2520obscures%2520the%2520true%2520source%2520of%2520scientific%2520discovery.%2520Additionally%252C%2520the%2520absence%2520of%2520counterfactuals%2520and%2520data%2520on%2520failed%2520attempts%2520creates%2520a%2520selection%2520bias%2520that%2520may%2520exaggerate%2520LLM%2520capabilities.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520guidelines%2520for%2520scientific%2520transparency%2520and%2520reproducibility%2520for%2520research%2520on%2520reasoning%2520by%2520LLMs.%2520Establishing%2520such%2520guidelines%2520is%2520crucial%2520for%2520both%2520scientific%2520integrity%2520and%2520the%2520ongoing%2520societal%2520debates%2520regarding%2520fair%2520data%2520usage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02380v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Refutability%20Gap%3A%20Challenges%20in%20Validating%20Reasoning%20by%20Large%20Language%20Models&entry.906535625=Elchanan%20Mossel&entry.1292438233=Recent%20reports%20claim%20that%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20the%20ability%20to%20derive%20new%20science%20and%20exhibit%20human-level%20general%20intelligence.%20We%20argue%20that%20such%20claims%20are%20not%20rigorous%20scientific%20claims%2C%20as%20they%20do%20not%20satisfy%20Popper%27s%20refutability%20principle%20%28often%20termed%20falsifiability%29%2C%20which%20requires%20that%20scientific%20statements%20be%20capable%20of%20being%20disproven.%20We%20identify%20several%20methodological%20pitfalls%20in%20current%20AI%20research%20on%20reasoning%2C%20including%20the%20inability%20to%20verify%20the%20novelty%20of%20findings%20due%20to%20opaque%20and%20non-searchable%20training%20data%2C%20the%20lack%20of%20reproducibility%20caused%20by%20continuous%20model%20updates%2C%20and%20the%20omission%20of%20human-interaction%20transcripts%2C%20which%20obscures%20the%20true%20source%20of%20scientific%20discovery.%20Additionally%2C%20the%20absence%20of%20counterfactuals%20and%20data%20on%20failed%20attempts%20creates%20a%20selection%20bias%20that%20may%20exaggerate%20LLM%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%20guidelines%20for%20scientific%20transparency%20and%20reproducibility%20for%20research%20on%20reasoning%20by%20LLMs.%20Establishing%20such%20guidelines%20is%20crucial%20for%20both%20scientific%20integrity%20and%20the%20ongoing%20societal%20debates%20regarding%20fair%20data%20usage.&entry.1838667208=http%3A//arxiv.org/abs/2601.02380v3&entry.124074799=Read"},
{"title": "Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting", "author": "Yifan Hu and Jie Yang and Tian Zhou and Peiyuan Liu and Yujin Tang and Rong Jin and Liang Sun", "abstract": "Although contrastive and other representation-learning methods have long been explored in vision and NLP, their adoption in modern time series forecasters remains limited. We believe they hold strong promise for this domain. To unlock this potential, we explicitly align past and future representations, thereby bridging the distributional gap between input histories and future targets. To this end, we introduce TimeAlign, a lightweight, plug-and-play framework that establishes a new representation paradigm, distinct from contrastive learning, by aligning auxiliary features via a simple reconstruction task and feeding them back into any base forecaster. Extensive experiments across eight benchmarks verify its superior performance. Further studies indicate that the gains arise primarily from correcting frequency mismatches between historical inputs and future outputs. Additionally, we provide two theoretical justifications for how reconstruction improves forecasting generalization and how alignment increases the mutual information between learned representations and predicted targets. The code is available at https://github.com/TROUBADOUR000/TimeAlign.", "link": "http://arxiv.org/abs/2509.14181v3", "date": "2026-02-10", "relevancy": 2.4179, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4915}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4878}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Past%20and%20Future%3A%20Distribution-Aware%20Alignment%20for%20Time%20Series%20Forecasting&body=Title%3A%20Bridging%20Past%20and%20Future%3A%20Distribution-Aware%20Alignment%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Yifan%20Hu%20and%20Jie%20Yang%20and%20Tian%20Zhou%20and%20Peiyuan%20Liu%20and%20Yujin%20Tang%20and%20Rong%20Jin%20and%20Liang%20Sun%0AAbstract%3A%20Although%20contrastive%20and%20other%20representation-learning%20methods%20have%20long%20been%20explored%20in%20vision%20and%20NLP%2C%20their%20adoption%20in%20modern%20time%20series%20forecasters%20remains%20limited.%20We%20believe%20they%20hold%20strong%20promise%20for%20this%20domain.%20To%20unlock%20this%20potential%2C%20we%20explicitly%20align%20past%20and%20future%20representations%2C%20thereby%20bridging%20the%20distributional%20gap%20between%20input%20histories%20and%20future%20targets.%20To%20this%20end%2C%20we%20introduce%20TimeAlign%2C%20a%20lightweight%2C%20plug-and-play%20framework%20that%20establishes%20a%20new%20representation%20paradigm%2C%20distinct%20from%20contrastive%20learning%2C%20by%20aligning%20auxiliary%20features%20via%20a%20simple%20reconstruction%20task%20and%20feeding%20them%20back%20into%20any%20base%20forecaster.%20Extensive%20experiments%20across%20eight%20benchmarks%20verify%20its%20superior%20performance.%20Further%20studies%20indicate%20that%20the%20gains%20arise%20primarily%20from%20correcting%20frequency%20mismatches%20between%20historical%20inputs%20and%20future%20outputs.%20Additionally%2C%20we%20provide%20two%20theoretical%20justifications%20for%20how%20reconstruction%20improves%20forecasting%20generalization%20and%20how%20alignment%20increases%20the%20mutual%20information%20between%20learned%20representations%20and%20predicted%20targets.%20The%20code%20is%20available%20at%20https%3A//github.com/TROUBADOUR000/TimeAlign.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14181v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Past%2520and%2520Future%253A%2520Distribution-Aware%2520Alignment%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DYifan%2520Hu%2520and%2520Jie%2520Yang%2520and%2520Tian%2520Zhou%2520and%2520Peiyuan%2520Liu%2520and%2520Yujin%2520Tang%2520and%2520Rong%2520Jin%2520and%2520Liang%2520Sun%26entry.1292438233%3DAlthough%2520contrastive%2520and%2520other%2520representation-learning%2520methods%2520have%2520long%2520been%2520explored%2520in%2520vision%2520and%2520NLP%252C%2520their%2520adoption%2520in%2520modern%2520time%2520series%2520forecasters%2520remains%2520limited.%2520We%2520believe%2520they%2520hold%2520strong%2520promise%2520for%2520this%2520domain.%2520To%2520unlock%2520this%2520potential%252C%2520we%2520explicitly%2520align%2520past%2520and%2520future%2520representations%252C%2520thereby%2520bridging%2520the%2520distributional%2520gap%2520between%2520input%2520histories%2520and%2520future%2520targets.%2520To%2520this%2520end%252C%2520we%2520introduce%2520TimeAlign%252C%2520a%2520lightweight%252C%2520plug-and-play%2520framework%2520that%2520establishes%2520a%2520new%2520representation%2520paradigm%252C%2520distinct%2520from%2520contrastive%2520learning%252C%2520by%2520aligning%2520auxiliary%2520features%2520via%2520a%2520simple%2520reconstruction%2520task%2520and%2520feeding%2520them%2520back%2520into%2520any%2520base%2520forecaster.%2520Extensive%2520experiments%2520across%2520eight%2520benchmarks%2520verify%2520its%2520superior%2520performance.%2520Further%2520studies%2520indicate%2520that%2520the%2520gains%2520arise%2520primarily%2520from%2520correcting%2520frequency%2520mismatches%2520between%2520historical%2520inputs%2520and%2520future%2520outputs.%2520Additionally%252C%2520we%2520provide%2520two%2520theoretical%2520justifications%2520for%2520how%2520reconstruction%2520improves%2520forecasting%2520generalization%2520and%2520how%2520alignment%2520increases%2520the%2520mutual%2520information%2520between%2520learned%2520representations%2520and%2520predicted%2520targets.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/TROUBADOUR000/TimeAlign.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14181v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Past%20and%20Future%3A%20Distribution-Aware%20Alignment%20for%20Time%20Series%20Forecasting&entry.906535625=Yifan%20Hu%20and%20Jie%20Yang%20and%20Tian%20Zhou%20and%20Peiyuan%20Liu%20and%20Yujin%20Tang%20and%20Rong%20Jin%20and%20Liang%20Sun&entry.1292438233=Although%20contrastive%20and%20other%20representation-learning%20methods%20have%20long%20been%20explored%20in%20vision%20and%20NLP%2C%20their%20adoption%20in%20modern%20time%20series%20forecasters%20remains%20limited.%20We%20believe%20they%20hold%20strong%20promise%20for%20this%20domain.%20To%20unlock%20this%20potential%2C%20we%20explicitly%20align%20past%20and%20future%20representations%2C%20thereby%20bridging%20the%20distributional%20gap%20between%20input%20histories%20and%20future%20targets.%20To%20this%20end%2C%20we%20introduce%20TimeAlign%2C%20a%20lightweight%2C%20plug-and-play%20framework%20that%20establishes%20a%20new%20representation%20paradigm%2C%20distinct%20from%20contrastive%20learning%2C%20by%20aligning%20auxiliary%20features%20via%20a%20simple%20reconstruction%20task%20and%20feeding%20them%20back%20into%20any%20base%20forecaster.%20Extensive%20experiments%20across%20eight%20benchmarks%20verify%20its%20superior%20performance.%20Further%20studies%20indicate%20that%20the%20gains%20arise%20primarily%20from%20correcting%20frequency%20mismatches%20between%20historical%20inputs%20and%20future%20outputs.%20Additionally%2C%20we%20provide%20two%20theoretical%20justifications%20for%20how%20reconstruction%20improves%20forecasting%20generalization%20and%20how%20alignment%20increases%20the%20mutual%20information%20between%20learned%20representations%20and%20predicted%20targets.%20The%20code%20is%20available%20at%20https%3A//github.com/TROUBADOUR000/TimeAlign.&entry.1838667208=http%3A//arxiv.org/abs/2509.14181v3&entry.124074799=Read"},
{"title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation", "author": "Ying Guo and Qijun Gan and Yifu Zhang and Jinlai Liu and Yifei Hu and Pan Xie and Dongjun Qian and Yu Zhang and Ruiqi Li and Yuqi Zhang and Ruibiao Lu and Xiaofeng Mei and Bo Han and Xiang Yin and Bingyue Peng and Zehuan Yuan", "abstract": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.", "link": "http://arxiv.org/abs/2602.08682v2", "date": "2026-02-10", "relevancy": 2.4127, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6163}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6068}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALIVE%3A%20Animate%20Your%20World%20with%20Lifelike%20Audio-Video%20Generation&body=Title%3A%20ALIVE%3A%20Animate%20Your%20World%20with%20Lifelike%20Audio-Video%20Generation%0AAuthor%3A%20Ying%20Guo%20and%20Qijun%20Gan%20and%20Yifu%20Zhang%20and%20Jinlai%20Liu%20and%20Yifei%20Hu%20and%20Pan%20Xie%20and%20Dongjun%20Qian%20and%20Yu%20Zhang%20and%20Ruiqi%20Li%20and%20Yuqi%20Zhang%20and%20Ruibiao%20Lu%20and%20Xiaofeng%20Mei%20and%20Bo%20Han%20and%20Xiang%20Yin%20and%20Bingyue%20Peng%20and%20Zehuan%20Yuan%0AAbstract%3A%20Video%20generation%20is%20rapidly%20evolving%20towards%20unified%20audio-video%20generation.%20In%20this%20paper%2C%20we%20present%20ALIVE%2C%20a%20generation%20model%20that%20adapts%20a%20pretrained%20Text-to-Video%20%28T2V%29%20model%20to%20Sora-style%20audio-video%20generation%20and%20animation.%20In%20particular%2C%20the%20model%20unlocks%20the%20Text-to-Video%26Audio%20%28T2VA%29%20and%20Reference-to-Video%26Audio%20%28animation%29%20capabilities%20compared%20to%20the%20T2V%20foundation%20models.%20To%20support%20the%20audio-visual%20synchronization%20and%20reference%20animation%2C%20we%20augment%20the%20popular%20MMDiT%20architecture%20with%20a%20joint%20audio-video%20branch%20which%20includes%20TA-CrossAttn%20for%20temporally-aligned%20cross-modal%20fusion%20and%20UniTemp-RoPE%20for%20precise%20audio-visual%20alignment.%20Meanwhile%2C%20a%20comprehensive%20data%20pipeline%20consisting%20of%20audio-video%20captioning%2C%20quality%20control%2C%20etc.%2C%20is%20carefully%20designed%20to%20collect%20high-quality%20finetuning%20data.%20Additionally%2C%20we%20introduce%20a%20new%20benchmark%20to%20perform%20a%20comprehensive%20model%20test%20and%20comparison.%20After%20continue%20pretraining%20and%20finetuning%20on%20million-level%20high-quality%20data%2C%20ALIVE%20demonstrates%20outstanding%20performance%2C%20consistently%20outperforming%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%20commercial%20solutions.%20With%20detailed%20recipes%20and%20benchmarks%2C%20we%20hope%20ALIVE%20helps%20the%20community%20develop%20audio-video%20generation%20models%20more%20efficiently.%20Official%20page%3A%20https%3A//github.com/FoundationVision/Alive.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALIVE%253A%2520Animate%2520Your%2520World%2520with%2520Lifelike%2520Audio-Video%2520Generation%26entry.906535625%3DYing%2520Guo%2520and%2520Qijun%2520Gan%2520and%2520Yifu%2520Zhang%2520and%2520Jinlai%2520Liu%2520and%2520Yifei%2520Hu%2520and%2520Pan%2520Xie%2520and%2520Dongjun%2520Qian%2520and%2520Yu%2520Zhang%2520and%2520Ruiqi%2520Li%2520and%2520Yuqi%2520Zhang%2520and%2520Ruibiao%2520Lu%2520and%2520Xiaofeng%2520Mei%2520and%2520Bo%2520Han%2520and%2520Xiang%2520Yin%2520and%2520Bingyue%2520Peng%2520and%2520Zehuan%2520Yuan%26entry.1292438233%3DVideo%2520generation%2520is%2520rapidly%2520evolving%2520towards%2520unified%2520audio-video%2520generation.%2520In%2520this%2520paper%252C%2520we%2520present%2520ALIVE%252C%2520a%2520generation%2520model%2520that%2520adapts%2520a%2520pretrained%2520Text-to-Video%2520%2528T2V%2529%2520model%2520to%2520Sora-style%2520audio-video%2520generation%2520and%2520animation.%2520In%2520particular%252C%2520the%2520model%2520unlocks%2520the%2520Text-to-Video%2526Audio%2520%2528T2VA%2529%2520and%2520Reference-to-Video%2526Audio%2520%2528animation%2529%2520capabilities%2520compared%2520to%2520the%2520T2V%2520foundation%2520models.%2520To%2520support%2520the%2520audio-visual%2520synchronization%2520and%2520reference%2520animation%252C%2520we%2520augment%2520the%2520popular%2520MMDiT%2520architecture%2520with%2520a%2520joint%2520audio-video%2520branch%2520which%2520includes%2520TA-CrossAttn%2520for%2520temporally-aligned%2520cross-modal%2520fusion%2520and%2520UniTemp-RoPE%2520for%2520precise%2520audio-visual%2520alignment.%2520Meanwhile%252C%2520a%2520comprehensive%2520data%2520pipeline%2520consisting%2520of%2520audio-video%2520captioning%252C%2520quality%2520control%252C%2520etc.%252C%2520is%2520carefully%2520designed%2520to%2520collect%2520high-quality%2520finetuning%2520data.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520to%2520perform%2520a%2520comprehensive%2520model%2520test%2520and%2520comparison.%2520After%2520continue%2520pretraining%2520and%2520finetuning%2520on%2520million-level%2520high-quality%2520data%252C%2520ALIVE%2520demonstrates%2520outstanding%2520performance%252C%2520consistently%2520outperforming%2520open-source%2520models%2520and%2520matching%2520or%2520surpassing%2520state-of-the-art%2520commercial%2520solutions.%2520With%2520detailed%2520recipes%2520and%2520benchmarks%252C%2520we%2520hope%2520ALIVE%2520helps%2520the%2520community%2520develop%2520audio-video%2520generation%2520models%2520more%2520efficiently.%2520Official%2520page%253A%2520https%253A//github.com/FoundationVision/Alive.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALIVE%3A%20Animate%20Your%20World%20with%20Lifelike%20Audio-Video%20Generation&entry.906535625=Ying%20Guo%20and%20Qijun%20Gan%20and%20Yifu%20Zhang%20and%20Jinlai%20Liu%20and%20Yifei%20Hu%20and%20Pan%20Xie%20and%20Dongjun%20Qian%20and%20Yu%20Zhang%20and%20Ruiqi%20Li%20and%20Yuqi%20Zhang%20and%20Ruibiao%20Lu%20and%20Xiaofeng%20Mei%20and%20Bo%20Han%20and%20Xiang%20Yin%20and%20Bingyue%20Peng%20and%20Zehuan%20Yuan&entry.1292438233=Video%20generation%20is%20rapidly%20evolving%20towards%20unified%20audio-video%20generation.%20In%20this%20paper%2C%20we%20present%20ALIVE%2C%20a%20generation%20model%20that%20adapts%20a%20pretrained%20Text-to-Video%20%28T2V%29%20model%20to%20Sora-style%20audio-video%20generation%20and%20animation.%20In%20particular%2C%20the%20model%20unlocks%20the%20Text-to-Video%26Audio%20%28T2VA%29%20and%20Reference-to-Video%26Audio%20%28animation%29%20capabilities%20compared%20to%20the%20T2V%20foundation%20models.%20To%20support%20the%20audio-visual%20synchronization%20and%20reference%20animation%2C%20we%20augment%20the%20popular%20MMDiT%20architecture%20with%20a%20joint%20audio-video%20branch%20which%20includes%20TA-CrossAttn%20for%20temporally-aligned%20cross-modal%20fusion%20and%20UniTemp-RoPE%20for%20precise%20audio-visual%20alignment.%20Meanwhile%2C%20a%20comprehensive%20data%20pipeline%20consisting%20of%20audio-video%20captioning%2C%20quality%20control%2C%20etc.%2C%20is%20carefully%20designed%20to%20collect%20high-quality%20finetuning%20data.%20Additionally%2C%20we%20introduce%20a%20new%20benchmark%20to%20perform%20a%20comprehensive%20model%20test%20and%20comparison.%20After%20continue%20pretraining%20and%20finetuning%20on%20million-level%20high-quality%20data%2C%20ALIVE%20demonstrates%20outstanding%20performance%2C%20consistently%20outperforming%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%20commercial%20solutions.%20With%20detailed%20recipes%20and%20benchmarks%2C%20we%20hope%20ALIVE%20helps%20the%20community%20develop%20audio-video%20generation%20models%20more%20efficiently.%20Official%20page%3A%20https%3A//github.com/FoundationVision/Alive.&entry.1838667208=http%3A//arxiv.org/abs/2602.08682v2&entry.124074799=Read"},
{"title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation", "author": "Senkang Hu and Xudong Han and Jinqi Jiang and Yihang Tao and Zihan Fang and Yong Dai and Sam Tak Wu Kwong and Yuguang Fang", "abstract": "Adapting billion-parameter language models to a downstream task is still costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task adaptation as output-distribution alignment: the objective is to steer the output distribution toward the task distribution directly during decoding rather than indirectly through weight updates. Building on this view, we introduce Steering Vector Decoding (SVDecode), a lightweight, PEFT-compatible, and theoretically grounded method. We start with a short warm-start fine-tune and extract a task-aware steering vector from the Kullback-Leibler (KL) divergence gradient between the output distribution of the warm-started and pre-trained models. This steering vector is then used to guide the decoding process to steer the model's output distribution towards the task distribution. We theoretically prove that SVDecode is first-order equivalent to the gradient step of full fine-tuning and derive a globally optimal solution for the strength of the steering vector. Across three tasks and nine benchmarks, SVDecode paired with four standard PEFT methods improves multiple-choice accuracy by up to 5 percentage points and open-ended truthfulness by 2 percentage points, with similar gains (1-2 percentage points) on commonsense datasets without adding trainable parameters beyond the PEFT adapter. SVDecode thus offers a lightweight, theoretically grounded path to stronger task adaptation for large language models. Code is available at https://github.com/dl-m9/SVDecode.", "link": "http://arxiv.org/abs/2509.15888v4", "date": "2026-02-10", "relevancy": 2.4073, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution-Aligned%20Decoding%20for%20Efficient%20LLM%20Task%20Adaptation&body=Title%3A%20Distribution-Aligned%20Decoding%20for%20Efficient%20LLM%20Task%20Adaptation%0AAuthor%3A%20Senkang%20Hu%20and%20Xudong%20Han%20and%20Jinqi%20Jiang%20and%20Yihang%20Tao%20and%20Zihan%20Fang%20and%20Yong%20Dai%20and%20Sam%20Tak%20Wu%20Kwong%20and%20Yuguang%20Fang%0AAbstract%3A%20Adapting%20billion-parameter%20language%20models%20to%20a%20downstream%20task%20is%20still%20costly%2C%20even%20with%20parameter-efficient%20fine-tuning%20%28PEFT%29.%20We%20re-cast%20task%20adaptation%20as%20output-distribution%20alignment%3A%20the%20objective%20is%20to%20steer%20the%20output%20distribution%20toward%20the%20task%20distribution%20directly%20during%20decoding%20rather%20than%20indirectly%20through%20weight%20updates.%20Building%20on%20this%20view%2C%20we%20introduce%20Steering%20Vector%20Decoding%20%28SVDecode%29%2C%20a%20lightweight%2C%20PEFT-compatible%2C%20and%20theoretically%20grounded%20method.%20We%20start%20with%20a%20short%20warm-start%20fine-tune%20and%20extract%20a%20task-aware%20steering%20vector%20from%20the%20Kullback-Leibler%20%28KL%29%20divergence%20gradient%20between%20the%20output%20distribution%20of%20the%20warm-started%20and%20pre-trained%20models.%20This%20steering%20vector%20is%20then%20used%20to%20guide%20the%20decoding%20process%20to%20steer%20the%20model%27s%20output%20distribution%20towards%20the%20task%20distribution.%20We%20theoretically%20prove%20that%20SVDecode%20is%20first-order%20equivalent%20to%20the%20gradient%20step%20of%20full%20fine-tuning%20and%20derive%20a%20globally%20optimal%20solution%20for%20the%20strength%20of%20the%20steering%20vector.%20Across%20three%20tasks%20and%20nine%20benchmarks%2C%20SVDecode%20paired%20with%20four%20standard%20PEFT%20methods%20improves%20multiple-choice%20accuracy%20by%20up%20to%205%20percentage%20points%20and%20open-ended%20truthfulness%20by%202%20percentage%20points%2C%20with%20similar%20gains%20%281-2%20percentage%20points%29%20on%20commonsense%20datasets%20without%20adding%20trainable%20parameters%20beyond%20the%20PEFT%20adapter.%20SVDecode%20thus%20offers%20a%20lightweight%2C%20theoretically%20grounded%20path%20to%20stronger%20task%20adaptation%20for%20large%20language%20models.%20Code%20is%20available%20at%20https%3A//github.com/dl-m9/SVDecode.%0ALink%3A%20http%3A//arxiv.org/abs/2509.15888v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution-Aligned%2520Decoding%2520for%2520Efficient%2520LLM%2520Task%2520Adaptation%26entry.906535625%3DSenkang%2520Hu%2520and%2520Xudong%2520Han%2520and%2520Jinqi%2520Jiang%2520and%2520Yihang%2520Tao%2520and%2520Zihan%2520Fang%2520and%2520Yong%2520Dai%2520and%2520Sam%2520Tak%2520Wu%2520Kwong%2520and%2520Yuguang%2520Fang%26entry.1292438233%3DAdapting%2520billion-parameter%2520language%2520models%2520to%2520a%2520downstream%2520task%2520is%2520still%2520costly%252C%2520even%2520with%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529.%2520We%2520re-cast%2520task%2520adaptation%2520as%2520output-distribution%2520alignment%253A%2520the%2520objective%2520is%2520to%2520steer%2520the%2520output%2520distribution%2520toward%2520the%2520task%2520distribution%2520directly%2520during%2520decoding%2520rather%2520than%2520indirectly%2520through%2520weight%2520updates.%2520Building%2520on%2520this%2520view%252C%2520we%2520introduce%2520Steering%2520Vector%2520Decoding%2520%2528SVDecode%2529%252C%2520a%2520lightweight%252C%2520PEFT-compatible%252C%2520and%2520theoretically%2520grounded%2520method.%2520We%2520start%2520with%2520a%2520short%2520warm-start%2520fine-tune%2520and%2520extract%2520a%2520task-aware%2520steering%2520vector%2520from%2520the%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%2520gradient%2520between%2520the%2520output%2520distribution%2520of%2520the%2520warm-started%2520and%2520pre-trained%2520models.%2520This%2520steering%2520vector%2520is%2520then%2520used%2520to%2520guide%2520the%2520decoding%2520process%2520to%2520steer%2520the%2520model%2527s%2520output%2520distribution%2520towards%2520the%2520task%2520distribution.%2520We%2520theoretically%2520prove%2520that%2520SVDecode%2520is%2520first-order%2520equivalent%2520to%2520the%2520gradient%2520step%2520of%2520full%2520fine-tuning%2520and%2520derive%2520a%2520globally%2520optimal%2520solution%2520for%2520the%2520strength%2520of%2520the%2520steering%2520vector.%2520Across%2520three%2520tasks%2520and%2520nine%2520benchmarks%252C%2520SVDecode%2520paired%2520with%2520four%2520standard%2520PEFT%2520methods%2520improves%2520multiple-choice%2520accuracy%2520by%2520up%2520to%25205%2520percentage%2520points%2520and%2520open-ended%2520truthfulness%2520by%25202%2520percentage%2520points%252C%2520with%2520similar%2520gains%2520%25281-2%2520percentage%2520points%2529%2520on%2520commonsense%2520datasets%2520without%2520adding%2520trainable%2520parameters%2520beyond%2520the%2520PEFT%2520adapter.%2520SVDecode%2520thus%2520offers%2520a%2520lightweight%252C%2520theoretically%2520grounded%2520path%2520to%2520stronger%2520task%2520adaptation%2520for%2520large%2520language%2520models.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/dl-m9/SVDecode.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15888v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-Aligned%20Decoding%20for%20Efficient%20LLM%20Task%20Adaptation&entry.906535625=Senkang%20Hu%20and%20Xudong%20Han%20and%20Jinqi%20Jiang%20and%20Yihang%20Tao%20and%20Zihan%20Fang%20and%20Yong%20Dai%20and%20Sam%20Tak%20Wu%20Kwong%20and%20Yuguang%20Fang&entry.1292438233=Adapting%20billion-parameter%20language%20models%20to%20a%20downstream%20task%20is%20still%20costly%2C%20even%20with%20parameter-efficient%20fine-tuning%20%28PEFT%29.%20We%20re-cast%20task%20adaptation%20as%20output-distribution%20alignment%3A%20the%20objective%20is%20to%20steer%20the%20output%20distribution%20toward%20the%20task%20distribution%20directly%20during%20decoding%20rather%20than%20indirectly%20through%20weight%20updates.%20Building%20on%20this%20view%2C%20we%20introduce%20Steering%20Vector%20Decoding%20%28SVDecode%29%2C%20a%20lightweight%2C%20PEFT-compatible%2C%20and%20theoretically%20grounded%20method.%20We%20start%20with%20a%20short%20warm-start%20fine-tune%20and%20extract%20a%20task-aware%20steering%20vector%20from%20the%20Kullback-Leibler%20%28KL%29%20divergence%20gradient%20between%20the%20output%20distribution%20of%20the%20warm-started%20and%20pre-trained%20models.%20This%20steering%20vector%20is%20then%20used%20to%20guide%20the%20decoding%20process%20to%20steer%20the%20model%27s%20output%20distribution%20towards%20the%20task%20distribution.%20We%20theoretically%20prove%20that%20SVDecode%20is%20first-order%20equivalent%20to%20the%20gradient%20step%20of%20full%20fine-tuning%20and%20derive%20a%20globally%20optimal%20solution%20for%20the%20strength%20of%20the%20steering%20vector.%20Across%20three%20tasks%20and%20nine%20benchmarks%2C%20SVDecode%20paired%20with%20four%20standard%20PEFT%20methods%20improves%20multiple-choice%20accuracy%20by%20up%20to%205%20percentage%20points%20and%20open-ended%20truthfulness%20by%202%20percentage%20points%2C%20with%20similar%20gains%20%281-2%20percentage%20points%29%20on%20commonsense%20datasets%20without%20adding%20trainable%20parameters%20beyond%20the%20PEFT%20adapter.%20SVDecode%20thus%20offers%20a%20lightweight%2C%20theoretically%20grounded%20path%20to%20stronger%20task%20adaptation%20for%20large%20language%20models.%20Code%20is%20available%20at%20https%3A//github.com/dl-m9/SVDecode.&entry.1838667208=http%3A//arxiv.org/abs/2509.15888v4&entry.124074799=Read"},
{"title": "TabNSA: Native Sparse Attention for Efficient Tabular Data Learning", "author": "Ali Eslamian and Qiang Cheng", "abstract": "Tabular data poses unique challenges for deep learning due to its heterogeneous feature types, lack of spatial structure, and often limited sample sizes. We propose TabNSA, a novel deep learning framework that integrates Native Sparse Attention (NSA) with a TabMixer backbone to efficiently model tabular data. TabNSA tackles computational and representational challenges by dynamically focusing on relevant feature subsets per instance. The NSA module employs a hierarchical sparse attention mechanism, including token compression, selective preservation, and localized sliding windows, to significantly reduce the quadratic complexity of standard attention operations while addressing feature heterogeneity. Complementing this, the TabMixer backbone captures complex, non-linear dependencies through parallel multilayer perceptron (MLP) branches with independent parameters. These modules are synergistically combined via element-wise summation and mean pooling, enabling TabNSA to model both global context and fine-grained interactions. Extensive experiments across supervised and transfer learning settings show that TabNSA consistently outperforms state-of-the-art deep learning models. Furthermore, by augmenting TabNSA with a fine-tuned large language model (LLM), we enable it to effectively address Few-Shot Learning challenges through language-guided generalization on diverse tabular benchmarks.\n  Code available on: https://github.com/aseslamian/TabNSA", "link": "http://arxiv.org/abs/2503.09850v3", "date": "2026-02-10", "relevancy": 2.3987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4926}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4864}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabNSA%3A%20Native%20Sparse%20Attention%20for%20Efficient%20Tabular%20Data%20Learning&body=Title%3A%20TabNSA%3A%20Native%20Sparse%20Attention%20for%20Efficient%20Tabular%20Data%20Learning%0AAuthor%3A%20Ali%20Eslamian%20and%20Qiang%20Cheng%0AAbstract%3A%20Tabular%20data%20poses%20unique%20challenges%20for%20deep%20learning%20due%20to%20its%20heterogeneous%20feature%20types%2C%20lack%20of%20spatial%20structure%2C%20and%20often%20limited%20sample%20sizes.%20We%20propose%20TabNSA%2C%20a%20novel%20deep%20learning%20framework%20that%20integrates%20Native%20Sparse%20Attention%20%28NSA%29%20with%20a%20TabMixer%20backbone%20to%20efficiently%20model%20tabular%20data.%20TabNSA%20tackles%20computational%20and%20representational%20challenges%20by%20dynamically%20focusing%20on%20relevant%20feature%20subsets%20per%20instance.%20The%20NSA%20module%20employs%20a%20hierarchical%20sparse%20attention%20mechanism%2C%20including%20token%20compression%2C%20selective%20preservation%2C%20and%20localized%20sliding%20windows%2C%20to%20significantly%20reduce%20the%20quadratic%20complexity%20of%20standard%20attention%20operations%20while%20addressing%20feature%20heterogeneity.%20Complementing%20this%2C%20the%20TabMixer%20backbone%20captures%20complex%2C%20non-linear%20dependencies%20through%20parallel%20multilayer%20perceptron%20%28MLP%29%20branches%20with%20independent%20parameters.%20These%20modules%20are%20synergistically%20combined%20via%20element-wise%20summation%20and%20mean%20pooling%2C%20enabling%20TabNSA%20to%20model%20both%20global%20context%20and%20fine-grained%20interactions.%20Extensive%20experiments%20across%20supervised%20and%20transfer%20learning%20settings%20show%20that%20TabNSA%20consistently%20outperforms%20state-of-the-art%20deep%20learning%20models.%20Furthermore%2C%20by%20augmenting%20TabNSA%20with%20a%20fine-tuned%20large%20language%20model%20%28LLM%29%2C%20we%20enable%20it%20to%20effectively%20address%20Few-Shot%20Learning%20challenges%20through%20language-guided%20generalization%20on%20diverse%20tabular%20benchmarks.%0A%20%20Code%20available%20on%3A%20https%3A//github.com/aseslamian/TabNSA%0ALink%3A%20http%3A//arxiv.org/abs/2503.09850v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabNSA%253A%2520Native%2520Sparse%2520Attention%2520for%2520Efficient%2520Tabular%2520Data%2520Learning%26entry.906535625%3DAli%2520Eslamian%2520and%2520Qiang%2520Cheng%26entry.1292438233%3DTabular%2520data%2520poses%2520unique%2520challenges%2520for%2520deep%2520learning%2520due%2520to%2520its%2520heterogeneous%2520feature%2520types%252C%2520lack%2520of%2520spatial%2520structure%252C%2520and%2520often%2520limited%2520sample%2520sizes.%2520We%2520propose%2520TabNSA%252C%2520a%2520novel%2520deep%2520learning%2520framework%2520that%2520integrates%2520Native%2520Sparse%2520Attention%2520%2528NSA%2529%2520with%2520a%2520TabMixer%2520backbone%2520to%2520efficiently%2520model%2520tabular%2520data.%2520TabNSA%2520tackles%2520computational%2520and%2520representational%2520challenges%2520by%2520dynamically%2520focusing%2520on%2520relevant%2520feature%2520subsets%2520per%2520instance.%2520The%2520NSA%2520module%2520employs%2520a%2520hierarchical%2520sparse%2520attention%2520mechanism%252C%2520including%2520token%2520compression%252C%2520selective%2520preservation%252C%2520and%2520localized%2520sliding%2520windows%252C%2520to%2520significantly%2520reduce%2520the%2520quadratic%2520complexity%2520of%2520standard%2520attention%2520operations%2520while%2520addressing%2520feature%2520heterogeneity.%2520Complementing%2520this%252C%2520the%2520TabMixer%2520backbone%2520captures%2520complex%252C%2520non-linear%2520dependencies%2520through%2520parallel%2520multilayer%2520perceptron%2520%2528MLP%2529%2520branches%2520with%2520independent%2520parameters.%2520These%2520modules%2520are%2520synergistically%2520combined%2520via%2520element-wise%2520summation%2520and%2520mean%2520pooling%252C%2520enabling%2520TabNSA%2520to%2520model%2520both%2520global%2520context%2520and%2520fine-grained%2520interactions.%2520Extensive%2520experiments%2520across%2520supervised%2520and%2520transfer%2520learning%2520settings%2520show%2520that%2520TabNSA%2520consistently%2520outperforms%2520state-of-the-art%2520deep%2520learning%2520models.%2520Furthermore%252C%2520by%2520augmenting%2520TabNSA%2520with%2520a%2520fine-tuned%2520large%2520language%2520model%2520%2528LLM%2529%252C%2520we%2520enable%2520it%2520to%2520effectively%2520address%2520Few-Shot%2520Learning%2520challenges%2520through%2520language-guided%2520generalization%2520on%2520diverse%2520tabular%2520benchmarks.%250A%2520%2520Code%2520available%2520on%253A%2520https%253A//github.com/aseslamian/TabNSA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09850v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabNSA%3A%20Native%20Sparse%20Attention%20for%20Efficient%20Tabular%20Data%20Learning&entry.906535625=Ali%20Eslamian%20and%20Qiang%20Cheng&entry.1292438233=Tabular%20data%20poses%20unique%20challenges%20for%20deep%20learning%20due%20to%20its%20heterogeneous%20feature%20types%2C%20lack%20of%20spatial%20structure%2C%20and%20often%20limited%20sample%20sizes.%20We%20propose%20TabNSA%2C%20a%20novel%20deep%20learning%20framework%20that%20integrates%20Native%20Sparse%20Attention%20%28NSA%29%20with%20a%20TabMixer%20backbone%20to%20efficiently%20model%20tabular%20data.%20TabNSA%20tackles%20computational%20and%20representational%20challenges%20by%20dynamically%20focusing%20on%20relevant%20feature%20subsets%20per%20instance.%20The%20NSA%20module%20employs%20a%20hierarchical%20sparse%20attention%20mechanism%2C%20including%20token%20compression%2C%20selective%20preservation%2C%20and%20localized%20sliding%20windows%2C%20to%20significantly%20reduce%20the%20quadratic%20complexity%20of%20standard%20attention%20operations%20while%20addressing%20feature%20heterogeneity.%20Complementing%20this%2C%20the%20TabMixer%20backbone%20captures%20complex%2C%20non-linear%20dependencies%20through%20parallel%20multilayer%20perceptron%20%28MLP%29%20branches%20with%20independent%20parameters.%20These%20modules%20are%20synergistically%20combined%20via%20element-wise%20summation%20and%20mean%20pooling%2C%20enabling%20TabNSA%20to%20model%20both%20global%20context%20and%20fine-grained%20interactions.%20Extensive%20experiments%20across%20supervised%20and%20transfer%20learning%20settings%20show%20that%20TabNSA%20consistently%20outperforms%20state-of-the-art%20deep%20learning%20models.%20Furthermore%2C%20by%20augmenting%20TabNSA%20with%20a%20fine-tuned%20large%20language%20model%20%28LLM%29%2C%20we%20enable%20it%20to%20effectively%20address%20Few-Shot%20Learning%20challenges%20through%20language-guided%20generalization%20on%20diverse%20tabular%20benchmarks.%0A%20%20Code%20available%20on%3A%20https%3A//github.com/aseslamian/TabNSA&entry.1838667208=http%3A//arxiv.org/abs/2503.09850v3&entry.124074799=Read"},
{"title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration", "author": "Modi Shi and Shijia Peng and Jin Chen and Haoran Jiang and Yinghui Li and Di Huang and Ping Luo and Hongyang Li and Li Chen", "abstract": "Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.", "link": "http://arxiv.org/abs/2602.10106v1", "date": "2026-02-10", "relevancy": 2.3983, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6239}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6114}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoHumanoid%3A%20Unlocking%20In-the-Wild%20Loco-Manipulation%20with%20Robot-Free%20Egocentric%20Demonstration&body=Title%3A%20EgoHumanoid%3A%20Unlocking%20In-the-Wild%20Loco-Manipulation%20with%20Robot-Free%20Egocentric%20Demonstration%0AAuthor%3A%20Modi%20Shi%20and%20Shijia%20Peng%20and%20Jin%20Chen%20and%20Haoran%20Jiang%20and%20Yinghui%20Li%20and%20Di%20Huang%20and%20Ping%20Luo%20and%20Hongyang%20Li%20and%20Li%20Chen%0AAbstract%3A%20Human%20demonstrations%20offer%20rich%20environmental%20diversity%20and%20scale%20naturally%2C%20making%20them%20an%20appealing%20alternative%20to%20robot%20teleoperation.%20While%20this%20paradigm%20has%20advanced%20robot-arm%20manipulation%2C%20its%20potential%20for%20the%20more%20challenging%2C%20data-hungry%20problem%20of%20humanoid%20loco-manipulation%20remains%20largely%20unexplored.%20We%20present%20EgoHumanoid%2C%20the%20first%20framework%20to%20co-train%20a%20vision-language-action%20policy%20using%20abundant%20egocentric%20human%20demonstrations%20together%20with%20a%20limited%20amount%20of%20robot%20data%2C%20enabling%20humanoids%20to%20perform%20loco-manipulation%20across%20diverse%20real-world%20environments.%20To%20bridge%20the%20embodiment%20gap%20between%20humans%20and%20robots%2C%20including%20discrepancies%20in%20physical%20morphology%20and%20viewpoint%2C%20we%20introduce%20a%20systematic%20alignment%20pipeline%20spanning%20from%20hardware%20design%20to%20data%20processing.%20A%20portable%20system%20for%20scalable%20human%20data%20collection%20is%20developed%2C%20and%20we%20establish%20practical%20collection%20protocols%20to%20improve%20transferability.%20At%20the%20core%20of%20our%20human-to-humanoid%20alignment%20pipeline%20lies%20two%20key%20components.%20The%20view%20alignment%20reduces%20visual%20domain%20discrepancies%20caused%20by%20camera%20height%20and%20perspective%20variation.%20The%20action%20alignment%20maps%20human%20motions%20into%20a%20unified%2C%20kinematically%20feasible%20action%20space%20for%20humanoid%20control.%20Extensive%20real-world%20experiments%20demonstrate%20that%20incorporating%20robot-free%20egocentric%20data%20significantly%20outperforms%20robot-only%20baselines%20by%2051%5C%25%2C%20particularly%20in%20unseen%20environments.%20Our%20analysis%20further%20reveals%20which%20behaviors%20transfer%20effectively%20and%20the%20potential%20for%20scaling%20human%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoHumanoid%253A%2520Unlocking%2520In-the-Wild%2520Loco-Manipulation%2520with%2520Robot-Free%2520Egocentric%2520Demonstration%26entry.906535625%3DModi%2520Shi%2520and%2520Shijia%2520Peng%2520and%2520Jin%2520Chen%2520and%2520Haoran%2520Jiang%2520and%2520Yinghui%2520Li%2520and%2520Di%2520Huang%2520and%2520Ping%2520Luo%2520and%2520Hongyang%2520Li%2520and%2520Li%2520Chen%26entry.1292438233%3DHuman%2520demonstrations%2520offer%2520rich%2520environmental%2520diversity%2520and%2520scale%2520naturally%252C%2520making%2520them%2520an%2520appealing%2520alternative%2520to%2520robot%2520teleoperation.%2520While%2520this%2520paradigm%2520has%2520advanced%2520robot-arm%2520manipulation%252C%2520its%2520potential%2520for%2520the%2520more%2520challenging%252C%2520data-hungry%2520problem%2520of%2520humanoid%2520loco-manipulation%2520remains%2520largely%2520unexplored.%2520We%2520present%2520EgoHumanoid%252C%2520the%2520first%2520framework%2520to%2520co-train%2520a%2520vision-language-action%2520policy%2520using%2520abundant%2520egocentric%2520human%2520demonstrations%2520together%2520with%2520a%2520limited%2520amount%2520of%2520robot%2520data%252C%2520enabling%2520humanoids%2520to%2520perform%2520loco-manipulation%2520across%2520diverse%2520real-world%2520environments.%2520To%2520bridge%2520the%2520embodiment%2520gap%2520between%2520humans%2520and%2520robots%252C%2520including%2520discrepancies%2520in%2520physical%2520morphology%2520and%2520viewpoint%252C%2520we%2520introduce%2520a%2520systematic%2520alignment%2520pipeline%2520spanning%2520from%2520hardware%2520design%2520to%2520data%2520processing.%2520A%2520portable%2520system%2520for%2520scalable%2520human%2520data%2520collection%2520is%2520developed%252C%2520and%2520we%2520establish%2520practical%2520collection%2520protocols%2520to%2520improve%2520transferability.%2520At%2520the%2520core%2520of%2520our%2520human-to-humanoid%2520alignment%2520pipeline%2520lies%2520two%2520key%2520components.%2520The%2520view%2520alignment%2520reduces%2520visual%2520domain%2520discrepancies%2520caused%2520by%2520camera%2520height%2520and%2520perspective%2520variation.%2520The%2520action%2520alignment%2520maps%2520human%2520motions%2520into%2520a%2520unified%252C%2520kinematically%2520feasible%2520action%2520space%2520for%2520humanoid%2520control.%2520Extensive%2520real-world%2520experiments%2520demonstrate%2520that%2520incorporating%2520robot-free%2520egocentric%2520data%2520significantly%2520outperforms%2520robot-only%2520baselines%2520by%252051%255C%2525%252C%2520particularly%2520in%2520unseen%2520environments.%2520Our%2520analysis%2520further%2520reveals%2520which%2520behaviors%2520transfer%2520effectively%2520and%2520the%2520potential%2520for%2520scaling%2520human%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoHumanoid%3A%20Unlocking%20In-the-Wild%20Loco-Manipulation%20with%20Robot-Free%20Egocentric%20Demonstration&entry.906535625=Modi%20Shi%20and%20Shijia%20Peng%20and%20Jin%20Chen%20and%20Haoran%20Jiang%20and%20Yinghui%20Li%20and%20Di%20Huang%20and%20Ping%20Luo%20and%20Hongyang%20Li%20and%20Li%20Chen&entry.1292438233=Human%20demonstrations%20offer%20rich%20environmental%20diversity%20and%20scale%20naturally%2C%20making%20them%20an%20appealing%20alternative%20to%20robot%20teleoperation.%20While%20this%20paradigm%20has%20advanced%20robot-arm%20manipulation%2C%20its%20potential%20for%20the%20more%20challenging%2C%20data-hungry%20problem%20of%20humanoid%20loco-manipulation%20remains%20largely%20unexplored.%20We%20present%20EgoHumanoid%2C%20the%20first%20framework%20to%20co-train%20a%20vision-language-action%20policy%20using%20abundant%20egocentric%20human%20demonstrations%20together%20with%20a%20limited%20amount%20of%20robot%20data%2C%20enabling%20humanoids%20to%20perform%20loco-manipulation%20across%20diverse%20real-world%20environments.%20To%20bridge%20the%20embodiment%20gap%20between%20humans%20and%20robots%2C%20including%20discrepancies%20in%20physical%20morphology%20and%20viewpoint%2C%20we%20introduce%20a%20systematic%20alignment%20pipeline%20spanning%20from%20hardware%20design%20to%20data%20processing.%20A%20portable%20system%20for%20scalable%20human%20data%20collection%20is%20developed%2C%20and%20we%20establish%20practical%20collection%20protocols%20to%20improve%20transferability.%20At%20the%20core%20of%20our%20human-to-humanoid%20alignment%20pipeline%20lies%20two%20key%20components.%20The%20view%20alignment%20reduces%20visual%20domain%20discrepancies%20caused%20by%20camera%20height%20and%20perspective%20variation.%20The%20action%20alignment%20maps%20human%20motions%20into%20a%20unified%2C%20kinematically%20feasible%20action%20space%20for%20humanoid%20control.%20Extensive%20real-world%20experiments%20demonstrate%20that%20incorporating%20robot-free%20egocentric%20data%20significantly%20outperforms%20robot-only%20baselines%20by%2051%5C%25%2C%20particularly%20in%20unseen%20environments.%20Our%20analysis%20further%20reveals%20which%20behaviors%20transfer%20effectively%20and%20the%20potential%20for%20scaling%20human%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.10106v1&entry.124074799=Read"},
{"title": "Short-Context Dominance: How Much Local Context Natural Language Actually Needs?", "author": "Vala Vakilian and Zimeng Wang and Ankit Singh Rawat and Christos Thrampoulidis", "abstract": "We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.", "link": "http://arxiv.org/abs/2512.08082v2", "date": "2026-02-10", "relevancy": 2.3948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Short-Context%20Dominance%3A%20How%20Much%20Local%20Context%20Natural%20Language%20Actually%20Needs%3F&body=Title%3A%20Short-Context%20Dominance%3A%20How%20Much%20Local%20Context%20Natural%20Language%20Actually%20Needs%3F%0AAuthor%3A%20Vala%20Vakilian%20and%20Zimeng%20Wang%20and%20Ankit%20Singh%20Rawat%20and%20Christos%20Thrampoulidis%0AAbstract%3A%20We%20investigate%20the%20short-context%20dominance%20hypothesis%3A%20that%20for%20most%20sequences%2C%20a%20small%20local%20prefix%20suffices%20to%20predict%20their%20next%20tokens.%20Using%20large%20language%20models%20as%20statistical%20oracles%2C%20we%20measure%20the%20minimum%20context%20length%20%28MCL%29%20needed%20to%20reproduce%20accurate%20full-context%20predictions%20across%20datasets%20with%20sequences%20of%20varying%20lengths.%20For%20sequences%20with%201-7k%20tokens%20from%20long-context%20documents%2C%20we%20consistently%20find%20that%2075-80%25%20require%20only%20the%20last%2096%20tokens%20at%20most.%20Given%20the%20dominance%20of%20short-context%20tokens%2C%20we%20then%20ask%20whether%20it%20is%20possible%20to%20detect%20challenging%20long-context%20sequences%20for%20which%20a%20short%20local%20prefix%20does%20not%20suffice%20for%20prediction.%20We%20introduce%20a%20practical%20proxy%20to%20MCL%2C%20called%20Distributionally%20Aware%20MCL%20%28DaMCL%29%2C%20that%20does%20not%20require%20knowledge%20of%20the%20actual%20next-token%20and%20is%20compatible%20with%20sampling%20strategies%20beyond%20greedy%20decoding.%20Our%20experiments%20validate%20that%20simple%20thresholding%20of%20the%20metric%20defining%20DaMCL%20achieves%20high%20performance%20in%20detecting%20long%20vs.%20short%20context%20sequences.%20Finally%2C%20to%20counter%20the%20bias%20that%20short-context%20dominance%20induces%20in%20LLM%20output%20distributions%2C%20we%20develop%20an%20intuitive%20decoding%20algorithm%20that%20leverages%20our%20detector%20to%20identify%20and%20boost%20tokens%20that%20are%20long-range-relevant.%20Across%20Q%26A%20tasks%20and%20model%20architectures%2C%20we%20confirm%20that%20mitigating%20the%20bias%20improves%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShort-Context%2520Dominance%253A%2520How%2520Much%2520Local%2520Context%2520Natural%2520Language%2520Actually%2520Needs%253F%26entry.906535625%3DVala%2520Vakilian%2520and%2520Zimeng%2520Wang%2520and%2520Ankit%2520Singh%2520Rawat%2520and%2520Christos%2520Thrampoulidis%26entry.1292438233%3DWe%2520investigate%2520the%2520short-context%2520dominance%2520hypothesis%253A%2520that%2520for%2520most%2520sequences%252C%2520a%2520small%2520local%2520prefix%2520suffices%2520to%2520predict%2520their%2520next%2520tokens.%2520Using%2520large%2520language%2520models%2520as%2520statistical%2520oracles%252C%2520we%2520measure%2520the%2520minimum%2520context%2520length%2520%2528MCL%2529%2520needed%2520to%2520reproduce%2520accurate%2520full-context%2520predictions%2520across%2520datasets%2520with%2520sequences%2520of%2520varying%2520lengths.%2520For%2520sequences%2520with%25201-7k%2520tokens%2520from%2520long-context%2520documents%252C%2520we%2520consistently%2520find%2520that%252075-80%2525%2520require%2520only%2520the%2520last%252096%2520tokens%2520at%2520most.%2520Given%2520the%2520dominance%2520of%2520short-context%2520tokens%252C%2520we%2520then%2520ask%2520whether%2520it%2520is%2520possible%2520to%2520detect%2520challenging%2520long-context%2520sequences%2520for%2520which%2520a%2520short%2520local%2520prefix%2520does%2520not%2520suffice%2520for%2520prediction.%2520We%2520introduce%2520a%2520practical%2520proxy%2520to%2520MCL%252C%2520called%2520Distributionally%2520Aware%2520MCL%2520%2528DaMCL%2529%252C%2520that%2520does%2520not%2520require%2520knowledge%2520of%2520the%2520actual%2520next-token%2520and%2520is%2520compatible%2520with%2520sampling%2520strategies%2520beyond%2520greedy%2520decoding.%2520Our%2520experiments%2520validate%2520that%2520simple%2520thresholding%2520of%2520the%2520metric%2520defining%2520DaMCL%2520achieves%2520high%2520performance%2520in%2520detecting%2520long%2520vs.%2520short%2520context%2520sequences.%2520Finally%252C%2520to%2520counter%2520the%2520bias%2520that%2520short-context%2520dominance%2520induces%2520in%2520LLM%2520output%2520distributions%252C%2520we%2520develop%2520an%2520intuitive%2520decoding%2520algorithm%2520that%2520leverages%2520our%2520detector%2520to%2520identify%2520and%2520boost%2520tokens%2520that%2520are%2520long-range-relevant.%2520Across%2520Q%2526A%2520tasks%2520and%2520model%2520architectures%252C%2520we%2520confirm%2520that%2520mitigating%2520the%2520bias%2520improves%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Short-Context%20Dominance%3A%20How%20Much%20Local%20Context%20Natural%20Language%20Actually%20Needs%3F&entry.906535625=Vala%20Vakilian%20and%20Zimeng%20Wang%20and%20Ankit%20Singh%20Rawat%20and%20Christos%20Thrampoulidis&entry.1292438233=We%20investigate%20the%20short-context%20dominance%20hypothesis%3A%20that%20for%20most%20sequences%2C%20a%20small%20local%20prefix%20suffices%20to%20predict%20their%20next%20tokens.%20Using%20large%20language%20models%20as%20statistical%20oracles%2C%20we%20measure%20the%20minimum%20context%20length%20%28MCL%29%20needed%20to%20reproduce%20accurate%20full-context%20predictions%20across%20datasets%20with%20sequences%20of%20varying%20lengths.%20For%20sequences%20with%201-7k%20tokens%20from%20long-context%20documents%2C%20we%20consistently%20find%20that%2075-80%25%20require%20only%20the%20last%2096%20tokens%20at%20most.%20Given%20the%20dominance%20of%20short-context%20tokens%2C%20we%20then%20ask%20whether%20it%20is%20possible%20to%20detect%20challenging%20long-context%20sequences%20for%20which%20a%20short%20local%20prefix%20does%20not%20suffice%20for%20prediction.%20We%20introduce%20a%20practical%20proxy%20to%20MCL%2C%20called%20Distributionally%20Aware%20MCL%20%28DaMCL%29%2C%20that%20does%20not%20require%20knowledge%20of%20the%20actual%20next-token%20and%20is%20compatible%20with%20sampling%20strategies%20beyond%20greedy%20decoding.%20Our%20experiments%20validate%20that%20simple%20thresholding%20of%20the%20metric%20defining%20DaMCL%20achieves%20high%20performance%20in%20detecting%20long%20vs.%20short%20context%20sequences.%20Finally%2C%20to%20counter%20the%20bias%20that%20short-context%20dominance%20induces%20in%20LLM%20output%20distributions%2C%20we%20develop%20an%20intuitive%20decoding%20algorithm%20that%20leverages%20our%20detector%20to%20identify%20and%20boost%20tokens%20that%20are%20long-range-relevant.%20Across%20Q%26A%20tasks%20and%20model%20architectures%2C%20we%20confirm%20that%20mitigating%20the%20bias%20improves%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.08082v2&entry.124074799=Read"},
{"title": "4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere", "author": "Yihang Luo and Shangchen Zhou and Yushi Lan and Xingang Pan and Chen Change Loy", "abstract": "We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.", "link": "http://arxiv.org/abs/2602.10094v1", "date": "2026-02-10", "relevancy": 2.3813, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.629}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204RC%3A%204D%20Reconstruction%20via%20Conditional%20Querying%20Anytime%20and%20Anywhere&body=Title%3A%204RC%3A%204D%20Reconstruction%20via%20Conditional%20Querying%20Anytime%20and%20Anywhere%0AAuthor%3A%20Yihang%20Luo%20and%20Shangchen%20Zhou%20and%20Yushi%20Lan%20and%20Xingang%20Pan%20and%20Chen%20Change%20Loy%0AAbstract%3A%20We%20present%204RC%2C%20a%20unified%20feed-forward%20framework%20for%204D%20reconstruction%20from%20monocular%20videos.%20Unlike%20existing%20approaches%20that%20typically%20decouple%20motion%20from%20geometry%20or%20produce%20limited%204D%20attributes%20such%20as%20sparse%20trajectories%20or%20two-view%20scene%20flow%2C%204RC%20learns%20a%20holistic%204D%20representation%20that%20jointly%20captures%20dense%20scene%20geometry%20and%20motion%20dynamics.%20At%20its%20core%2C%204RC%20introduces%20a%20novel%20encode-once%2C%20query-anywhere%20and%20anytime%20paradigm%3A%20a%20transformer%20backbone%20encodes%20the%20entire%20video%20into%20a%20compact%20spatio-temporal%20latent%20space%2C%20from%20which%20a%20conditional%20decoder%20can%20efficiently%20query%203D%20geometry%20and%20motion%20for%20any%20query%20frame%20at%20any%20target%20timestamp.%20To%20facilitate%20learning%2C%20we%20represent%20per-view%204D%20attributes%20in%20a%20minimally%20factorized%20form%20by%20decomposing%20them%20into%20base%20geometry%20and%20time-dependent%20relative%20motion.%20Extensive%20experiments%20demonstrate%20that%204RC%20outperforms%20prior%20and%20concurrent%20methods%20across%20a%20wide%20range%20of%204D%20reconstruction%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4RC%253A%25204D%2520Reconstruction%2520via%2520Conditional%2520Querying%2520Anytime%2520and%2520Anywhere%26entry.906535625%3DYihang%2520Luo%2520and%2520Shangchen%2520Zhou%2520and%2520Yushi%2520Lan%2520and%2520Xingang%2520Pan%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3DWe%2520present%25204RC%252C%2520a%2520unified%2520feed-forward%2520framework%2520for%25204D%2520reconstruction%2520from%2520monocular%2520videos.%2520Unlike%2520existing%2520approaches%2520that%2520typically%2520decouple%2520motion%2520from%2520geometry%2520or%2520produce%2520limited%25204D%2520attributes%2520such%2520as%2520sparse%2520trajectories%2520or%2520two-view%2520scene%2520flow%252C%25204RC%2520learns%2520a%2520holistic%25204D%2520representation%2520that%2520jointly%2520captures%2520dense%2520scene%2520geometry%2520and%2520motion%2520dynamics.%2520At%2520its%2520core%252C%25204RC%2520introduces%2520a%2520novel%2520encode-once%252C%2520query-anywhere%2520and%2520anytime%2520paradigm%253A%2520a%2520transformer%2520backbone%2520encodes%2520the%2520entire%2520video%2520into%2520a%2520compact%2520spatio-temporal%2520latent%2520space%252C%2520from%2520which%2520a%2520conditional%2520decoder%2520can%2520efficiently%2520query%25203D%2520geometry%2520and%2520motion%2520for%2520any%2520query%2520frame%2520at%2520any%2520target%2520timestamp.%2520To%2520facilitate%2520learning%252C%2520we%2520represent%2520per-view%25204D%2520attributes%2520in%2520a%2520minimally%2520factorized%2520form%2520by%2520decomposing%2520them%2520into%2520base%2520geometry%2520and%2520time-dependent%2520relative%2520motion.%2520Extensive%2520experiments%2520demonstrate%2520that%25204RC%2520outperforms%2520prior%2520and%2520concurrent%2520methods%2520across%2520a%2520wide%2520range%2520of%25204D%2520reconstruction%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4RC%3A%204D%20Reconstruction%20via%20Conditional%20Querying%20Anytime%20and%20Anywhere&entry.906535625=Yihang%20Luo%20and%20Shangchen%20Zhou%20and%20Yushi%20Lan%20and%20Xingang%20Pan%20and%20Chen%20Change%20Loy&entry.1292438233=We%20present%204RC%2C%20a%20unified%20feed-forward%20framework%20for%204D%20reconstruction%20from%20monocular%20videos.%20Unlike%20existing%20approaches%20that%20typically%20decouple%20motion%20from%20geometry%20or%20produce%20limited%204D%20attributes%20such%20as%20sparse%20trajectories%20or%20two-view%20scene%20flow%2C%204RC%20learns%20a%20holistic%204D%20representation%20that%20jointly%20captures%20dense%20scene%20geometry%20and%20motion%20dynamics.%20At%20its%20core%2C%204RC%20introduces%20a%20novel%20encode-once%2C%20query-anywhere%20and%20anytime%20paradigm%3A%20a%20transformer%20backbone%20encodes%20the%20entire%20video%20into%20a%20compact%20spatio-temporal%20latent%20space%2C%20from%20which%20a%20conditional%20decoder%20can%20efficiently%20query%203D%20geometry%20and%20motion%20for%20any%20query%20frame%20at%20any%20target%20timestamp.%20To%20facilitate%20learning%2C%20we%20represent%20per-view%204D%20attributes%20in%20a%20minimally%20factorized%20form%20by%20decomposing%20them%20into%20base%20geometry%20and%20time-dependent%20relative%20motion.%20Extensive%20experiments%20demonstrate%20that%204RC%20outperforms%20prior%20and%20concurrent%20methods%20across%20a%20wide%20range%20of%204D%20reconstruction%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.10094v1&entry.124074799=Read"},
{"title": "RAP: KV-Cache Compression via RoPE-Aligned Pruning", "author": "Jihao Xin and Tian Lyu and David Keyes and Hatem Ltaief and Marco Canini", "abstract": "Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \\approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In modern RoPE-based LLMs, this absorption fails: RoPE forces latent KV states to be reconstructed to full dimension, reintroducing substantial memory and compute overhead. We propose RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve RoPE's 2x2 rotation structure, restore B absorption, and eliminate reconstruction. Our evaluation on LLaMA-3-8B and Mistral-7B shows that RAP enables joint reduction of KV-Cache, attention parameters, and FLOPs by 20-30%, all at once, while maintaining strong accuracy. Notably, RAP reduces attention latency to 83% (prefill) and 77% (decode) of baseline.", "link": "http://arxiv.org/abs/2602.02599v3", "date": "2026-02-10", "relevancy": 2.3696, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAP%3A%20KV-Cache%20Compression%20via%20RoPE-Aligned%20Pruning&body=Title%3A%20RAP%3A%20KV-Cache%20Compression%20via%20RoPE-Aligned%20Pruning%0AAuthor%3A%20Jihao%20Xin%20and%20Tian%20Lyu%20and%20David%20Keyes%20and%20Hatem%20Ltaief%20and%20Marco%20Canini%0AAbstract%3A%20Long-context%20inference%20in%20large%20language%20models%20is%20increasingly%20bottlenecked%20by%20the%20memory%20and%20compute%20cost%20of%20the%20KV-Cache.%20Low-rank%20factorization%20compresses%20KV%20projections%20by%20writing%20%24W%20%5Capprox%20A%20%2A%20B%24%2C%20where%20A%20produces%20latent%20KV%20states%20and%20B%20can%20be%20absorbed%20into%20downstream%20weights.%20In%20modern%20RoPE-based%20LLMs%2C%20this%20absorption%20fails%3A%20RoPE%20forces%20latent%20KV%20states%20to%20be%20reconstructed%20to%20full%20dimension%2C%20reintroducing%20substantial%20memory%20and%20compute%20overhead.%20We%20propose%20RoPE-Aligned%20Pruning%20%28RAP%29%2C%20which%20prunes%20entire%20RoPE-aligned%20column%20pairs%20to%20preserve%20RoPE%27s%202x2%20rotation%20structure%2C%20restore%20B%20absorption%2C%20and%20eliminate%20reconstruction.%20Our%20evaluation%20on%20LLaMA-3-8B%20and%20Mistral-7B%20shows%20that%20RAP%20enables%20joint%20reduction%20of%20KV-Cache%2C%20attention%20parameters%2C%20and%20FLOPs%20by%2020-30%25%2C%20all%20at%20once%2C%20while%20maintaining%20strong%20accuracy.%20Notably%2C%20RAP%20reduces%20attention%20latency%20to%2083%25%20%28prefill%29%20and%2077%25%20%28decode%29%20of%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02599v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAP%253A%2520KV-Cache%2520Compression%2520via%2520RoPE-Aligned%2520Pruning%26entry.906535625%3DJihao%2520Xin%2520and%2520Tian%2520Lyu%2520and%2520David%2520Keyes%2520and%2520Hatem%2520Ltaief%2520and%2520Marco%2520Canini%26entry.1292438233%3DLong-context%2520inference%2520in%2520large%2520language%2520models%2520is%2520increasingly%2520bottlenecked%2520by%2520the%2520memory%2520and%2520compute%2520cost%2520of%2520the%2520KV-Cache.%2520Low-rank%2520factorization%2520compresses%2520KV%2520projections%2520by%2520writing%2520%2524W%2520%255Capprox%2520A%2520%252A%2520B%2524%252C%2520where%2520A%2520produces%2520latent%2520KV%2520states%2520and%2520B%2520can%2520be%2520absorbed%2520into%2520downstream%2520weights.%2520In%2520modern%2520RoPE-based%2520LLMs%252C%2520this%2520absorption%2520fails%253A%2520RoPE%2520forces%2520latent%2520KV%2520states%2520to%2520be%2520reconstructed%2520to%2520full%2520dimension%252C%2520reintroducing%2520substantial%2520memory%2520and%2520compute%2520overhead.%2520We%2520propose%2520RoPE-Aligned%2520Pruning%2520%2528RAP%2529%252C%2520which%2520prunes%2520entire%2520RoPE-aligned%2520column%2520pairs%2520to%2520preserve%2520RoPE%2527s%25202x2%2520rotation%2520structure%252C%2520restore%2520B%2520absorption%252C%2520and%2520eliminate%2520reconstruction.%2520Our%2520evaluation%2520on%2520LLaMA-3-8B%2520and%2520Mistral-7B%2520shows%2520that%2520RAP%2520enables%2520joint%2520reduction%2520of%2520KV-Cache%252C%2520attention%2520parameters%252C%2520and%2520FLOPs%2520by%252020-30%2525%252C%2520all%2520at%2520once%252C%2520while%2520maintaining%2520strong%2520accuracy.%2520Notably%252C%2520RAP%2520reduces%2520attention%2520latency%2520to%252083%2525%2520%2528prefill%2529%2520and%252077%2525%2520%2528decode%2529%2520of%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02599v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAP%3A%20KV-Cache%20Compression%20via%20RoPE-Aligned%20Pruning&entry.906535625=Jihao%20Xin%20and%20Tian%20Lyu%20and%20David%20Keyes%20and%20Hatem%20Ltaief%20and%20Marco%20Canini&entry.1292438233=Long-context%20inference%20in%20large%20language%20models%20is%20increasingly%20bottlenecked%20by%20the%20memory%20and%20compute%20cost%20of%20the%20KV-Cache.%20Low-rank%20factorization%20compresses%20KV%20projections%20by%20writing%20%24W%20%5Capprox%20A%20%2A%20B%24%2C%20where%20A%20produces%20latent%20KV%20states%20and%20B%20can%20be%20absorbed%20into%20downstream%20weights.%20In%20modern%20RoPE-based%20LLMs%2C%20this%20absorption%20fails%3A%20RoPE%20forces%20latent%20KV%20states%20to%20be%20reconstructed%20to%20full%20dimension%2C%20reintroducing%20substantial%20memory%20and%20compute%20overhead.%20We%20propose%20RoPE-Aligned%20Pruning%20%28RAP%29%2C%20which%20prunes%20entire%20RoPE-aligned%20column%20pairs%20to%20preserve%20RoPE%27s%202x2%20rotation%20structure%2C%20restore%20B%20absorption%2C%20and%20eliminate%20reconstruction.%20Our%20evaluation%20on%20LLaMA-3-8B%20and%20Mistral-7B%20shows%20that%20RAP%20enables%20joint%20reduction%20of%20KV-Cache%2C%20attention%20parameters%2C%20and%20FLOPs%20by%2020-30%25%2C%20all%20at%20once%2C%20while%20maintaining%20strong%20accuracy.%20Notably%2C%20RAP%20reduces%20attention%20latency%20to%2083%25%20%28prefill%29%20and%2077%25%20%28decode%29%20of%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2602.02599v3&entry.124074799=Read"},
{"title": "Benchmarking 3D Human Pose Estimation Models under Occlusions", "author": "Filipa Lino and Carlos Santiago and Manuel Marques", "abstract": "Human Pose Estimation (HPE) involves detecting and localizing keypoints on the human body from visual data. In 3D HPE, occlusions, where parts of the body are not visible in the image, pose a significant challenge for accurate pose reconstruction. This paper presents a benchmark on the robustness of 3D HPE models under realistic occlusion conditions, involving combinations of occluded keypoints commonly observed in real-world scenarios. We evaluate nine state-of-the-art 2D-to-3D HPE models, spanning convolutional, transformer-based, graph-based, and diffusion-based architectures, using the BlendMimic3D dataset, a synthetic dataset with ground-truth 2D/3D annotations and occlusion labels. All models were originally trained on Human3.6M and tested here without retraining to assess their generalization. We introduce a protocol that simulates occlusion by adding noise into 2D keypoints based on real detector behavior, and conduct both global and per-joint sensitivity analyses. Our findings reveal that all models exhibit notable performance degradation under occlusion, with diffusion-based models underperforming despite their stochastic nature. Additionally, a per-joint occlusion analysis identifies consistent vulnerability in distal joints (e.g., wrists, feet) across models. Overall, this work highlights critical limitations of current 3D HPE models in handling occlusions, and provides insights for improving real-world robustness.", "link": "http://arxiv.org/abs/2504.10350v3", "date": "2026-02-10", "relevancy": 2.3578, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6133}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5805}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%203D%20Human%20Pose%20Estimation%20Models%20under%20Occlusions&body=Title%3A%20Benchmarking%203D%20Human%20Pose%20Estimation%20Models%20under%20Occlusions%0AAuthor%3A%20Filipa%20Lino%20and%20Carlos%20Santiago%20and%20Manuel%20Marques%0AAbstract%3A%20Human%20Pose%20Estimation%20%28HPE%29%20involves%20detecting%20and%20localizing%20keypoints%20on%20the%20human%20body%20from%20visual%20data.%20In%203D%20HPE%2C%20occlusions%2C%20where%20parts%20of%20the%20body%20are%20not%20visible%20in%20the%20image%2C%20pose%20a%20significant%20challenge%20for%20accurate%20pose%20reconstruction.%20This%20paper%20presents%20a%20benchmark%20on%20the%20robustness%20of%203D%20HPE%20models%20under%20realistic%20occlusion%20conditions%2C%20involving%20combinations%20of%20occluded%20keypoints%20commonly%20observed%20in%20real-world%20scenarios.%20We%20evaluate%20nine%20state-of-the-art%202D-to-3D%20HPE%20models%2C%20spanning%20convolutional%2C%20transformer-based%2C%20graph-based%2C%20and%20diffusion-based%20architectures%2C%20using%20the%20BlendMimic3D%20dataset%2C%20a%20synthetic%20dataset%20with%20ground-truth%202D/3D%20annotations%20and%20occlusion%20labels.%20All%20models%20were%20originally%20trained%20on%20Human3.6M%20and%20tested%20here%20without%20retraining%20to%20assess%20their%20generalization.%20We%20introduce%20a%20protocol%20that%20simulates%20occlusion%20by%20adding%20noise%20into%202D%20keypoints%20based%20on%20real%20detector%20behavior%2C%20and%20conduct%20both%20global%20and%20per-joint%20sensitivity%20analyses.%20Our%20findings%20reveal%20that%20all%20models%20exhibit%20notable%20performance%20degradation%20under%20occlusion%2C%20with%20diffusion-based%20models%20underperforming%20despite%20their%20stochastic%20nature.%20Additionally%2C%20a%20per-joint%20occlusion%20analysis%20identifies%20consistent%20vulnerability%20in%20distal%20joints%20%28e.g.%2C%20wrists%2C%20feet%29%20across%20models.%20Overall%2C%20this%20work%20highlights%20critical%20limitations%20of%20current%203D%20HPE%20models%20in%20handling%20occlusions%2C%20and%20provides%20insights%20for%20improving%20real-world%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2504.10350v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%25203D%2520Human%2520Pose%2520Estimation%2520Models%2520under%2520Occlusions%26entry.906535625%3DFilipa%2520Lino%2520and%2520Carlos%2520Santiago%2520and%2520Manuel%2520Marques%26entry.1292438233%3DHuman%2520Pose%2520Estimation%2520%2528HPE%2529%2520involves%2520detecting%2520and%2520localizing%2520keypoints%2520on%2520the%2520human%2520body%2520from%2520visual%2520data.%2520In%25203D%2520HPE%252C%2520occlusions%252C%2520where%2520parts%2520of%2520the%2520body%2520are%2520not%2520visible%2520in%2520the%2520image%252C%2520pose%2520a%2520significant%2520challenge%2520for%2520accurate%2520pose%2520reconstruction.%2520This%2520paper%2520presents%2520a%2520benchmark%2520on%2520the%2520robustness%2520of%25203D%2520HPE%2520models%2520under%2520realistic%2520occlusion%2520conditions%252C%2520involving%2520combinations%2520of%2520occluded%2520keypoints%2520commonly%2520observed%2520in%2520real-world%2520scenarios.%2520We%2520evaluate%2520nine%2520state-of-the-art%25202D-to-3D%2520HPE%2520models%252C%2520spanning%2520convolutional%252C%2520transformer-based%252C%2520graph-based%252C%2520and%2520diffusion-based%2520architectures%252C%2520using%2520the%2520BlendMimic3D%2520dataset%252C%2520a%2520synthetic%2520dataset%2520with%2520ground-truth%25202D/3D%2520annotations%2520and%2520occlusion%2520labels.%2520All%2520models%2520were%2520originally%2520trained%2520on%2520Human3.6M%2520and%2520tested%2520here%2520without%2520retraining%2520to%2520assess%2520their%2520generalization.%2520We%2520introduce%2520a%2520protocol%2520that%2520simulates%2520occlusion%2520by%2520adding%2520noise%2520into%25202D%2520keypoints%2520based%2520on%2520real%2520detector%2520behavior%252C%2520and%2520conduct%2520both%2520global%2520and%2520per-joint%2520sensitivity%2520analyses.%2520Our%2520findings%2520reveal%2520that%2520all%2520models%2520exhibit%2520notable%2520performance%2520degradation%2520under%2520occlusion%252C%2520with%2520diffusion-based%2520models%2520underperforming%2520despite%2520their%2520stochastic%2520nature.%2520Additionally%252C%2520a%2520per-joint%2520occlusion%2520analysis%2520identifies%2520consistent%2520vulnerability%2520in%2520distal%2520joints%2520%2528e.g.%252C%2520wrists%252C%2520feet%2529%2520across%2520models.%2520Overall%252C%2520this%2520work%2520highlights%2520critical%2520limitations%2520of%2520current%25203D%2520HPE%2520models%2520in%2520handling%2520occlusions%252C%2520and%2520provides%2520insights%2520for%2520improving%2520real-world%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10350v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%203D%20Human%20Pose%20Estimation%20Models%20under%20Occlusions&entry.906535625=Filipa%20Lino%20and%20Carlos%20Santiago%20and%20Manuel%20Marques&entry.1292438233=Human%20Pose%20Estimation%20%28HPE%29%20involves%20detecting%20and%20localizing%20keypoints%20on%20the%20human%20body%20from%20visual%20data.%20In%203D%20HPE%2C%20occlusions%2C%20where%20parts%20of%20the%20body%20are%20not%20visible%20in%20the%20image%2C%20pose%20a%20significant%20challenge%20for%20accurate%20pose%20reconstruction.%20This%20paper%20presents%20a%20benchmark%20on%20the%20robustness%20of%203D%20HPE%20models%20under%20realistic%20occlusion%20conditions%2C%20involving%20combinations%20of%20occluded%20keypoints%20commonly%20observed%20in%20real-world%20scenarios.%20We%20evaluate%20nine%20state-of-the-art%202D-to-3D%20HPE%20models%2C%20spanning%20convolutional%2C%20transformer-based%2C%20graph-based%2C%20and%20diffusion-based%20architectures%2C%20using%20the%20BlendMimic3D%20dataset%2C%20a%20synthetic%20dataset%20with%20ground-truth%202D/3D%20annotations%20and%20occlusion%20labels.%20All%20models%20were%20originally%20trained%20on%20Human3.6M%20and%20tested%20here%20without%20retraining%20to%20assess%20their%20generalization.%20We%20introduce%20a%20protocol%20that%20simulates%20occlusion%20by%20adding%20noise%20into%202D%20keypoints%20based%20on%20real%20detector%20behavior%2C%20and%20conduct%20both%20global%20and%20per-joint%20sensitivity%20analyses.%20Our%20findings%20reveal%20that%20all%20models%20exhibit%20notable%20performance%20degradation%20under%20occlusion%2C%20with%20diffusion-based%20models%20underperforming%20despite%20their%20stochastic%20nature.%20Additionally%2C%20a%20per-joint%20occlusion%20analysis%20identifies%20consistent%20vulnerability%20in%20distal%20joints%20%28e.g.%2C%20wrists%2C%20feet%29%20across%20models.%20Overall%2C%20this%20work%20highlights%20critical%20limitations%20of%20current%203D%20HPE%20models%20in%20handling%20occlusions%2C%20and%20provides%20insights%20for%20improving%20real-world%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2504.10350v3&entry.124074799=Read"},
{"title": "Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems", "author": "Tetsuro Abe and Masashi Yamashita and Shu Tanaka", "abstract": "In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutations, the choice of binary encoding strongly affects search efficiency. If the encoding fails to reflect the original neighborhood structure, small Hamming moves may not correspond to meaningful modifications in the original solution space, and constrained problems can yield many infeasible candidates that waste evaluations. Recent work combines FMQA with a binary autoencoder (bAE) that learns a compact binary latent code from feasible solutions, yet the mechanism behind its performance gains is unclear. Using a small traveling salesman problem as an interpretable testbed, we show that the bAE reconstructs feasible tours accurately and, compared with manually designed encodings at similar compression, better aligns tour distances with latent Hamming distances, yields smoother neighborhoods under small bit flips, and produces fewer local optima. These geometric properties explain why bAE+FMQA improves the approximation ratio faster while maintaining feasibility throughout optimization, and they provide guidance for designing latent representations for black-box optimization.", "link": "http://arxiv.org/abs/2602.10037v1", "date": "2026-02-10", "relevancy": 2.3469, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effectiveness%20of%20Binary%20Autoencoders%20for%20QUBO-Based%20Optimization%20Problems&body=Title%3A%20Effectiveness%20of%20Binary%20Autoencoders%20for%20QUBO-Based%20Optimization%20Problems%0AAuthor%3A%20Tetsuro%20Abe%20and%20Masashi%20Yamashita%20and%20Shu%20Tanaka%0AAbstract%3A%20In%20black-box%20combinatorial%20optimization%2C%20objective%20evaluations%20are%20often%20expensive%2C%20so%20high%20quality%20solutions%20must%20be%20found%20under%20a%20limited%20budget.%20Factorization%20machine%20with%20quantum%20annealing%20%28FMQA%29%20builds%20a%20quadratic%20surrogate%20model%20from%20evaluated%20samples%20and%20optimizes%20it%20on%20an%20Ising%20machine.%20However%2C%20FMQA%20requires%20binary%20decision%20variables%2C%20and%20for%20nonbinary%20structures%20such%20as%20integer%20permutations%2C%20the%20choice%20of%20binary%20encoding%20strongly%20affects%20search%20efficiency.%20If%20the%20encoding%20fails%20to%20reflect%20the%20original%20neighborhood%20structure%2C%20small%20Hamming%20moves%20may%20not%20correspond%20to%20meaningful%20modifications%20in%20the%20original%20solution%20space%2C%20and%20constrained%20problems%20can%20yield%20many%20infeasible%20candidates%20that%20waste%20evaluations.%20Recent%20work%20combines%20FMQA%20with%20a%20binary%20autoencoder%20%28bAE%29%20that%20learns%20a%20compact%20binary%20latent%20code%20from%20feasible%20solutions%2C%20yet%20the%20mechanism%20behind%20its%20performance%20gains%20is%20unclear.%20Using%20a%20small%20traveling%20salesman%20problem%20as%20an%20interpretable%20testbed%2C%20we%20show%20that%20the%20bAE%20reconstructs%20feasible%20tours%20accurately%20and%2C%20compared%20with%20manually%20designed%20encodings%20at%20similar%20compression%2C%20better%20aligns%20tour%20distances%20with%20latent%20Hamming%20distances%2C%20yields%20smoother%20neighborhoods%20under%20small%20bit%20flips%2C%20and%20produces%20fewer%20local%20optima.%20These%20geometric%20properties%20explain%20why%20bAE%2BFMQA%20improves%20the%20approximation%20ratio%20faster%20while%20maintaining%20feasibility%20throughout%20optimization%2C%20and%20they%20provide%20guidance%20for%20designing%20latent%20representations%20for%20black-box%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffectiveness%2520of%2520Binary%2520Autoencoders%2520for%2520QUBO-Based%2520Optimization%2520Problems%26entry.906535625%3DTetsuro%2520Abe%2520and%2520Masashi%2520Yamashita%2520and%2520Shu%2520Tanaka%26entry.1292438233%3DIn%2520black-box%2520combinatorial%2520optimization%252C%2520objective%2520evaluations%2520are%2520often%2520expensive%252C%2520so%2520high%2520quality%2520solutions%2520must%2520be%2520found%2520under%2520a%2520limited%2520budget.%2520Factorization%2520machine%2520with%2520quantum%2520annealing%2520%2528FMQA%2529%2520builds%2520a%2520quadratic%2520surrogate%2520model%2520from%2520evaluated%2520samples%2520and%2520optimizes%2520it%2520on%2520an%2520Ising%2520machine.%2520However%252C%2520FMQA%2520requires%2520binary%2520decision%2520variables%252C%2520and%2520for%2520nonbinary%2520structures%2520such%2520as%2520integer%2520permutations%252C%2520the%2520choice%2520of%2520binary%2520encoding%2520strongly%2520affects%2520search%2520efficiency.%2520If%2520the%2520encoding%2520fails%2520to%2520reflect%2520the%2520original%2520neighborhood%2520structure%252C%2520small%2520Hamming%2520moves%2520may%2520not%2520correspond%2520to%2520meaningful%2520modifications%2520in%2520the%2520original%2520solution%2520space%252C%2520and%2520constrained%2520problems%2520can%2520yield%2520many%2520infeasible%2520candidates%2520that%2520waste%2520evaluations.%2520Recent%2520work%2520combines%2520FMQA%2520with%2520a%2520binary%2520autoencoder%2520%2528bAE%2529%2520that%2520learns%2520a%2520compact%2520binary%2520latent%2520code%2520from%2520feasible%2520solutions%252C%2520yet%2520the%2520mechanism%2520behind%2520its%2520performance%2520gains%2520is%2520unclear.%2520Using%2520a%2520small%2520traveling%2520salesman%2520problem%2520as%2520an%2520interpretable%2520testbed%252C%2520we%2520show%2520that%2520the%2520bAE%2520reconstructs%2520feasible%2520tours%2520accurately%2520and%252C%2520compared%2520with%2520manually%2520designed%2520encodings%2520at%2520similar%2520compression%252C%2520better%2520aligns%2520tour%2520distances%2520with%2520latent%2520Hamming%2520distances%252C%2520yields%2520smoother%2520neighborhoods%2520under%2520small%2520bit%2520flips%252C%2520and%2520produces%2520fewer%2520local%2520optima.%2520These%2520geometric%2520properties%2520explain%2520why%2520bAE%252BFMQA%2520improves%2520the%2520approximation%2520ratio%2520faster%2520while%2520maintaining%2520feasibility%2520throughout%2520optimization%252C%2520and%2520they%2520provide%2520guidance%2520for%2520designing%2520latent%2520representations%2520for%2520black-box%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effectiveness%20of%20Binary%20Autoencoders%20for%20QUBO-Based%20Optimization%20Problems&entry.906535625=Tetsuro%20Abe%20and%20Masashi%20Yamashita%20and%20Shu%20Tanaka&entry.1292438233=In%20black-box%20combinatorial%20optimization%2C%20objective%20evaluations%20are%20often%20expensive%2C%20so%20high%20quality%20solutions%20must%20be%20found%20under%20a%20limited%20budget.%20Factorization%20machine%20with%20quantum%20annealing%20%28FMQA%29%20builds%20a%20quadratic%20surrogate%20model%20from%20evaluated%20samples%20and%20optimizes%20it%20on%20an%20Ising%20machine.%20However%2C%20FMQA%20requires%20binary%20decision%20variables%2C%20and%20for%20nonbinary%20structures%20such%20as%20integer%20permutations%2C%20the%20choice%20of%20binary%20encoding%20strongly%20affects%20search%20efficiency.%20If%20the%20encoding%20fails%20to%20reflect%20the%20original%20neighborhood%20structure%2C%20small%20Hamming%20moves%20may%20not%20correspond%20to%20meaningful%20modifications%20in%20the%20original%20solution%20space%2C%20and%20constrained%20problems%20can%20yield%20many%20infeasible%20candidates%20that%20waste%20evaluations.%20Recent%20work%20combines%20FMQA%20with%20a%20binary%20autoencoder%20%28bAE%29%20that%20learns%20a%20compact%20binary%20latent%20code%20from%20feasible%20solutions%2C%20yet%20the%20mechanism%20behind%20its%20performance%20gains%20is%20unclear.%20Using%20a%20small%20traveling%20salesman%20problem%20as%20an%20interpretable%20testbed%2C%20we%20show%20that%20the%20bAE%20reconstructs%20feasible%20tours%20accurately%20and%2C%20compared%20with%20manually%20designed%20encodings%20at%20similar%20compression%2C%20better%20aligns%20tour%20distances%20with%20latent%20Hamming%20distances%2C%20yields%20smoother%20neighborhoods%20under%20small%20bit%20flips%2C%20and%20produces%20fewer%20local%20optima.%20These%20geometric%20properties%20explain%20why%20bAE%2BFMQA%20improves%20the%20approximation%20ratio%20faster%20while%20maintaining%20feasibility%20throughout%20optimization%2C%20and%20they%20provide%20guidance%20for%20designing%20latent%20representations%20for%20black-box%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2602.10037v1&entry.124074799=Read"},
{"title": "Dual-IPO: Dual-Iterative Preference Optimization for Text-to-Video Generation", "author": "Xiaomeng Yang and Mengping Yang and Jia Gong and Luozheng Qin and Zhiyu Tan and Hao Li", "abstract": "Recent advances in video generation have enabled thrilling experiences in producing realistic videos driven by scalable diffusion transformers. However, they usually fail to produce satisfactory outputs that are aligned to users' authentic demands and preferences. In this work, we introduce Dual-Iterative Optimization (Dual-IPO), an iterative paradigm that sequentially optimizes both the reward model and the video generation model for improved synthesis quality and human preference alignment. For the reward model, our framework ensures reliable and robust reward signals via CoT-guided reasoning, voting-based self-consistency, and preference certainty estimation. Given this, we optimize video foundation models with guidance of signals from reward model's feedback, thus improving the synthesis quality in subject consistency, motion smoothness and aesthetic quality, etc. The reward model and video generation model complement each other and are progressively improved in the multi-round iteration, without requiring tediously manual preference annotations. Comprehensive experiments demonstrate that the proposed Dual-IPO can effectively and consistently improve the video generation quality of base model with various architectures and sizes, even help a model with only 2B parameters surpass a 5B one. Moreover, our analysis experiments and ablation studies identify the rational of our systematic design and the efficacy of each component.", "link": "http://arxiv.org/abs/2502.02088v4", "date": "2026-02-10", "relevancy": 2.3417, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6022}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.592}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-IPO%3A%20Dual-Iterative%20Preference%20Optimization%20for%20Text-to-Video%20Generation&body=Title%3A%20Dual-IPO%3A%20Dual-Iterative%20Preference%20Optimization%20for%20Text-to-Video%20Generation%0AAuthor%3A%20Xiaomeng%20Yang%20and%20Mengping%20Yang%20and%20Jia%20Gong%20and%20Luozheng%20Qin%20and%20Zhiyu%20Tan%20and%20Hao%20Li%0AAbstract%3A%20Recent%20advances%20in%20video%20generation%20have%20enabled%20thrilling%20experiences%20in%20producing%20realistic%20videos%20driven%20by%20scalable%20diffusion%20transformers.%20However%2C%20they%20usually%20fail%20to%20produce%20satisfactory%20outputs%20that%20are%20aligned%20to%20users%27%20authentic%20demands%20and%20preferences.%20In%20this%20work%2C%20we%20introduce%20Dual-Iterative%20Optimization%20%28Dual-IPO%29%2C%20an%20iterative%20paradigm%20that%20sequentially%20optimizes%20both%20the%20reward%20model%20and%20the%20video%20generation%20model%20for%20improved%20synthesis%20quality%20and%20human%20preference%20alignment.%20For%20the%20reward%20model%2C%20our%20framework%20ensures%20reliable%20and%20robust%20reward%20signals%20via%20CoT-guided%20reasoning%2C%20voting-based%20self-consistency%2C%20and%20preference%20certainty%20estimation.%20Given%20this%2C%20we%20optimize%20video%20foundation%20models%20with%20guidance%20of%20signals%20from%20reward%20model%27s%20feedback%2C%20thus%20improving%20the%20synthesis%20quality%20in%20subject%20consistency%2C%20motion%20smoothness%20and%20aesthetic%20quality%2C%20etc.%20The%20reward%20model%20and%20video%20generation%20model%20complement%20each%20other%20and%20are%20progressively%20improved%20in%20the%20multi-round%20iteration%2C%20without%20requiring%20tediously%20manual%20preference%20annotations.%20Comprehensive%20experiments%20demonstrate%20that%20the%20proposed%20Dual-IPO%20can%20effectively%20and%20consistently%20improve%20the%20video%20generation%20quality%20of%20base%20model%20with%20various%20architectures%20and%20sizes%2C%20even%20help%20a%20model%20with%20only%202B%20parameters%20surpass%20a%205B%20one.%20Moreover%2C%20our%20analysis%20experiments%20and%20ablation%20studies%20identify%20the%20rational%20of%20our%20systematic%20design%20and%20the%20efficacy%20of%20each%20component.%0ALink%3A%20http%3A//arxiv.org/abs/2502.02088v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-IPO%253A%2520Dual-Iterative%2520Preference%2520Optimization%2520for%2520Text-to-Video%2520Generation%26entry.906535625%3DXiaomeng%2520Yang%2520and%2520Mengping%2520Yang%2520and%2520Jia%2520Gong%2520and%2520Luozheng%2520Qin%2520and%2520Zhiyu%2520Tan%2520and%2520Hao%2520Li%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520generation%2520have%2520enabled%2520thrilling%2520experiences%2520in%2520producing%2520realistic%2520videos%2520driven%2520by%2520scalable%2520diffusion%2520transformers.%2520However%252C%2520they%2520usually%2520fail%2520to%2520produce%2520satisfactory%2520outputs%2520that%2520are%2520aligned%2520to%2520users%2527%2520authentic%2520demands%2520and%2520preferences.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Dual-Iterative%2520Optimization%2520%2528Dual-IPO%2529%252C%2520an%2520iterative%2520paradigm%2520that%2520sequentially%2520optimizes%2520both%2520the%2520reward%2520model%2520and%2520the%2520video%2520generation%2520model%2520for%2520improved%2520synthesis%2520quality%2520and%2520human%2520preference%2520alignment.%2520For%2520the%2520reward%2520model%252C%2520our%2520framework%2520ensures%2520reliable%2520and%2520robust%2520reward%2520signals%2520via%2520CoT-guided%2520reasoning%252C%2520voting-based%2520self-consistency%252C%2520and%2520preference%2520certainty%2520estimation.%2520Given%2520this%252C%2520we%2520optimize%2520video%2520foundation%2520models%2520with%2520guidance%2520of%2520signals%2520from%2520reward%2520model%2527s%2520feedback%252C%2520thus%2520improving%2520the%2520synthesis%2520quality%2520in%2520subject%2520consistency%252C%2520motion%2520smoothness%2520and%2520aesthetic%2520quality%252C%2520etc.%2520The%2520reward%2520model%2520and%2520video%2520generation%2520model%2520complement%2520each%2520other%2520and%2520are%2520progressively%2520improved%2520in%2520the%2520multi-round%2520iteration%252C%2520without%2520requiring%2520tediously%2520manual%2520preference%2520annotations.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520Dual-IPO%2520can%2520effectively%2520and%2520consistently%2520improve%2520the%2520video%2520generation%2520quality%2520of%2520base%2520model%2520with%2520various%2520architectures%2520and%2520sizes%252C%2520even%2520help%2520a%2520model%2520with%2520only%25202B%2520parameters%2520surpass%2520a%25205B%2520one.%2520Moreover%252C%2520our%2520analysis%2520experiments%2520and%2520ablation%2520studies%2520identify%2520the%2520rational%2520of%2520our%2520systematic%2520design%2520and%2520the%2520efficacy%2520of%2520each%2520component.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02088v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-IPO%3A%20Dual-Iterative%20Preference%20Optimization%20for%20Text-to-Video%20Generation&entry.906535625=Xiaomeng%20Yang%20and%20Mengping%20Yang%20and%20Jia%20Gong%20and%20Luozheng%20Qin%20and%20Zhiyu%20Tan%20and%20Hao%20Li&entry.1292438233=Recent%20advances%20in%20video%20generation%20have%20enabled%20thrilling%20experiences%20in%20producing%20realistic%20videos%20driven%20by%20scalable%20diffusion%20transformers.%20However%2C%20they%20usually%20fail%20to%20produce%20satisfactory%20outputs%20that%20are%20aligned%20to%20users%27%20authentic%20demands%20and%20preferences.%20In%20this%20work%2C%20we%20introduce%20Dual-Iterative%20Optimization%20%28Dual-IPO%29%2C%20an%20iterative%20paradigm%20that%20sequentially%20optimizes%20both%20the%20reward%20model%20and%20the%20video%20generation%20model%20for%20improved%20synthesis%20quality%20and%20human%20preference%20alignment.%20For%20the%20reward%20model%2C%20our%20framework%20ensures%20reliable%20and%20robust%20reward%20signals%20via%20CoT-guided%20reasoning%2C%20voting-based%20self-consistency%2C%20and%20preference%20certainty%20estimation.%20Given%20this%2C%20we%20optimize%20video%20foundation%20models%20with%20guidance%20of%20signals%20from%20reward%20model%27s%20feedback%2C%20thus%20improving%20the%20synthesis%20quality%20in%20subject%20consistency%2C%20motion%20smoothness%20and%20aesthetic%20quality%2C%20etc.%20The%20reward%20model%20and%20video%20generation%20model%20complement%20each%20other%20and%20are%20progressively%20improved%20in%20the%20multi-round%20iteration%2C%20without%20requiring%20tediously%20manual%20preference%20annotations.%20Comprehensive%20experiments%20demonstrate%20that%20the%20proposed%20Dual-IPO%20can%20effectively%20and%20consistently%20improve%20the%20video%20generation%20quality%20of%20base%20model%20with%20various%20architectures%20and%20sizes%2C%20even%20help%20a%20model%20with%20only%202B%20parameters%20surpass%20a%205B%20one.%20Moreover%2C%20our%20analysis%20experiments%20and%20ablation%20studies%20identify%20the%20rational%20of%20our%20systematic%20design%20and%20the%20efficacy%20of%20each%20component.&entry.1838667208=http%3A//arxiv.org/abs/2502.02088v4&entry.124074799=Read"},
{"title": "One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning", "author": "Minh Le and Bao-Ngoc Dao and Huy Nguyen and Quyen Tran and Anh Nguyen and Nhat Ho", "abstract": "Prompt-based methods have recently gained prominence in Continual Learning (CL) due to their strong performance and memory efficiency. A prevalent strategy in this paradigm assigns a dedicated subset of prompts to each task, which, while effective, incurs substantial computational overhead and causes memory requirements to scale linearly with the number of tasks. Conversely, approaches employing a single shared prompt across tasks offer greater efficiency but often suffer from degraded performance due to knowledge interference. To reconcile this trade-off, we propose SMoPE, a novel framework that integrates the benefits of both task-specific and shared prompt strategies. Inspired by recent findings on the relationship between Prefix Tuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into multiple \"prompt experts\" within a sparse MoE architecture. For each input, only a select subset of relevant experts is activated, effectively mitigating interference. To facilitate expert selection, we introduce a prompt-attention score aggregation mechanism that computes a unified proxy score for each expert, enabling dynamic and sparse activation. Additionally, we propose an adaptive noise mechanism to encourage balanced expert utilization while preserving knowledge from prior tasks. To further enhance expert specialization, we design a prototype-based loss function that leverages prefix keys as implicit memory representations. Extensive experiments across multiple CL benchmarks demonstrate that SMoPE consistently outperforms task-specific prompt methods and achieves performance competitive with state-of-the-art approaches, all while significantly reducing parameter counts and computational costs.", "link": "http://arxiv.org/abs/2509.24483v2", "date": "2026-02-10", "relevancy": 2.3342, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4665}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Prompt%20Strikes%20Back%3A%20Sparse%20Mixture%20of%20Experts%20for%20Prompt-based%20Continual%20Learning&body=Title%3A%20One-Prompt%20Strikes%20Back%3A%20Sparse%20Mixture%20of%20Experts%20for%20Prompt-based%20Continual%20Learning%0AAuthor%3A%20Minh%20Le%20and%20Bao-Ngoc%20Dao%20and%20Huy%20Nguyen%20and%20Quyen%20Tran%20and%20Anh%20Nguyen%20and%20Nhat%20Ho%0AAbstract%3A%20Prompt-based%20methods%20have%20recently%20gained%20prominence%20in%20Continual%20Learning%20%28CL%29%20due%20to%20their%20strong%20performance%20and%20memory%20efficiency.%20A%20prevalent%20strategy%20in%20this%20paradigm%20assigns%20a%20dedicated%20subset%20of%20prompts%20to%20each%20task%2C%20which%2C%20while%20effective%2C%20incurs%20substantial%20computational%20overhead%20and%20causes%20memory%20requirements%20to%20scale%20linearly%20with%20the%20number%20of%20tasks.%20Conversely%2C%20approaches%20employing%20a%20single%20shared%20prompt%20across%20tasks%20offer%20greater%20efficiency%20but%20often%20suffer%20from%20degraded%20performance%20due%20to%20knowledge%20interference.%20To%20reconcile%20this%20trade-off%2C%20we%20propose%20SMoPE%2C%20a%20novel%20framework%20that%20integrates%20the%20benefits%20of%20both%20task-specific%20and%20shared%20prompt%20strategies.%20Inspired%20by%20recent%20findings%20on%20the%20relationship%20between%20Prefix%20Tuning%20and%20Mixture%20of%20Experts%20%28MoE%29%2C%20SMoPE%20organizes%20a%20shared%20prompt%20into%20multiple%20%22prompt%20experts%22%20within%20a%20sparse%20MoE%20architecture.%20For%20each%20input%2C%20only%20a%20select%20subset%20of%20relevant%20experts%20is%20activated%2C%20effectively%20mitigating%20interference.%20To%20facilitate%20expert%20selection%2C%20we%20introduce%20a%20prompt-attention%20score%20aggregation%20mechanism%20that%20computes%20a%20unified%20proxy%20score%20for%20each%20expert%2C%20enabling%20dynamic%20and%20sparse%20activation.%20Additionally%2C%20we%20propose%20an%20adaptive%20noise%20mechanism%20to%20encourage%20balanced%20expert%20utilization%20while%20preserving%20knowledge%20from%20prior%20tasks.%20To%20further%20enhance%20expert%20specialization%2C%20we%20design%20a%20prototype-based%20loss%20function%20that%20leverages%20prefix%20keys%20as%20implicit%20memory%20representations.%20Extensive%20experiments%20across%20multiple%20CL%20benchmarks%20demonstrate%20that%20SMoPE%20consistently%20outperforms%20task-specific%20prompt%20methods%20and%20achieves%20performance%20competitive%20with%20state-of-the-art%20approaches%2C%20all%20while%20significantly%20reducing%20parameter%20counts%20and%20computational%20costs.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Prompt%2520Strikes%2520Back%253A%2520Sparse%2520Mixture%2520of%2520Experts%2520for%2520Prompt-based%2520Continual%2520Learning%26entry.906535625%3DMinh%2520Le%2520and%2520Bao-Ngoc%2520Dao%2520and%2520Huy%2520Nguyen%2520and%2520Quyen%2520Tran%2520and%2520Anh%2520Nguyen%2520and%2520Nhat%2520Ho%26entry.1292438233%3DPrompt-based%2520methods%2520have%2520recently%2520gained%2520prominence%2520in%2520Continual%2520Learning%2520%2528CL%2529%2520due%2520to%2520their%2520strong%2520performance%2520and%2520memory%2520efficiency.%2520A%2520prevalent%2520strategy%2520in%2520this%2520paradigm%2520assigns%2520a%2520dedicated%2520subset%2520of%2520prompts%2520to%2520each%2520task%252C%2520which%252C%2520while%2520effective%252C%2520incurs%2520substantial%2520computational%2520overhead%2520and%2520causes%2520memory%2520requirements%2520to%2520scale%2520linearly%2520with%2520the%2520number%2520of%2520tasks.%2520Conversely%252C%2520approaches%2520employing%2520a%2520single%2520shared%2520prompt%2520across%2520tasks%2520offer%2520greater%2520efficiency%2520but%2520often%2520suffer%2520from%2520degraded%2520performance%2520due%2520to%2520knowledge%2520interference.%2520To%2520reconcile%2520this%2520trade-off%252C%2520we%2520propose%2520SMoPE%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520the%2520benefits%2520of%2520both%2520task-specific%2520and%2520shared%2520prompt%2520strategies.%2520Inspired%2520by%2520recent%2520findings%2520on%2520the%2520relationship%2520between%2520Prefix%2520Tuning%2520and%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%252C%2520SMoPE%2520organizes%2520a%2520shared%2520prompt%2520into%2520multiple%2520%2522prompt%2520experts%2522%2520within%2520a%2520sparse%2520MoE%2520architecture.%2520For%2520each%2520input%252C%2520only%2520a%2520select%2520subset%2520of%2520relevant%2520experts%2520is%2520activated%252C%2520effectively%2520mitigating%2520interference.%2520To%2520facilitate%2520expert%2520selection%252C%2520we%2520introduce%2520a%2520prompt-attention%2520score%2520aggregation%2520mechanism%2520that%2520computes%2520a%2520unified%2520proxy%2520score%2520for%2520each%2520expert%252C%2520enabling%2520dynamic%2520and%2520sparse%2520activation.%2520Additionally%252C%2520we%2520propose%2520an%2520adaptive%2520noise%2520mechanism%2520to%2520encourage%2520balanced%2520expert%2520utilization%2520while%2520preserving%2520knowledge%2520from%2520prior%2520tasks.%2520To%2520further%2520enhance%2520expert%2520specialization%252C%2520we%2520design%2520a%2520prototype-based%2520loss%2520function%2520that%2520leverages%2520prefix%2520keys%2520as%2520implicit%2520memory%2520representations.%2520Extensive%2520experiments%2520across%2520multiple%2520CL%2520benchmarks%2520demonstrate%2520that%2520SMoPE%2520consistently%2520outperforms%2520task-specific%2520prompt%2520methods%2520and%2520achieves%2520performance%2520competitive%2520with%2520state-of-the-art%2520approaches%252C%2520all%2520while%2520significantly%2520reducing%2520parameter%2520counts%2520and%2520computational%2520costs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Prompt%20Strikes%20Back%3A%20Sparse%20Mixture%20of%20Experts%20for%20Prompt-based%20Continual%20Learning&entry.906535625=Minh%20Le%20and%20Bao-Ngoc%20Dao%20and%20Huy%20Nguyen%20and%20Quyen%20Tran%20and%20Anh%20Nguyen%20and%20Nhat%20Ho&entry.1292438233=Prompt-based%20methods%20have%20recently%20gained%20prominence%20in%20Continual%20Learning%20%28CL%29%20due%20to%20their%20strong%20performance%20and%20memory%20efficiency.%20A%20prevalent%20strategy%20in%20this%20paradigm%20assigns%20a%20dedicated%20subset%20of%20prompts%20to%20each%20task%2C%20which%2C%20while%20effective%2C%20incurs%20substantial%20computational%20overhead%20and%20causes%20memory%20requirements%20to%20scale%20linearly%20with%20the%20number%20of%20tasks.%20Conversely%2C%20approaches%20employing%20a%20single%20shared%20prompt%20across%20tasks%20offer%20greater%20efficiency%20but%20often%20suffer%20from%20degraded%20performance%20due%20to%20knowledge%20interference.%20To%20reconcile%20this%20trade-off%2C%20we%20propose%20SMoPE%2C%20a%20novel%20framework%20that%20integrates%20the%20benefits%20of%20both%20task-specific%20and%20shared%20prompt%20strategies.%20Inspired%20by%20recent%20findings%20on%20the%20relationship%20between%20Prefix%20Tuning%20and%20Mixture%20of%20Experts%20%28MoE%29%2C%20SMoPE%20organizes%20a%20shared%20prompt%20into%20multiple%20%22prompt%20experts%22%20within%20a%20sparse%20MoE%20architecture.%20For%20each%20input%2C%20only%20a%20select%20subset%20of%20relevant%20experts%20is%20activated%2C%20effectively%20mitigating%20interference.%20To%20facilitate%20expert%20selection%2C%20we%20introduce%20a%20prompt-attention%20score%20aggregation%20mechanism%20that%20computes%20a%20unified%20proxy%20score%20for%20each%20expert%2C%20enabling%20dynamic%20and%20sparse%20activation.%20Additionally%2C%20we%20propose%20an%20adaptive%20noise%20mechanism%20to%20encourage%20balanced%20expert%20utilization%20while%20preserving%20knowledge%20from%20prior%20tasks.%20To%20further%20enhance%20expert%20specialization%2C%20we%20design%20a%20prototype-based%20loss%20function%20that%20leverages%20prefix%20keys%20as%20implicit%20memory%20representations.%20Extensive%20experiments%20across%20multiple%20CL%20benchmarks%20demonstrate%20that%20SMoPE%20consistently%20outperforms%20task-specific%20prompt%20methods%20and%20achieves%20performance%20competitive%20with%20state-of-the-art%20approaches%2C%20all%20while%20significantly%20reducing%20parameter%20counts%20and%20computational%20costs.&entry.1838667208=http%3A//arxiv.org/abs/2509.24483v2&entry.124074799=Read"},
{"title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method", "author": "Feiyang Cai and Guijuan He and Yi Hu and Jingjing Wang and Joshua Luo and Tianyu Zhu and Srikanth Pilla and Gang Li and Ling Liu and Feng Luo", "abstract": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.", "link": "http://arxiv.org/abs/2602.02320v2", "date": "2026-02-10", "relevancy": 2.3342, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-Scale%20Dataset%20for%20Molecular%20Structure-Language%20Description%20via%20a%20Rule-Regularized%20Method&body=Title%3A%20A%20Large-Scale%20Dataset%20for%20Molecular%20Structure-Language%20Description%20via%20a%20Rule-Regularized%20Method%0AAuthor%3A%20Feiyang%20Cai%20and%20Guijuan%20He%20and%20Yi%20Hu%20and%20Jingjing%20Wang%20and%20Joshua%20Luo%20and%20Tianyu%20Zhu%20and%20Srikanth%20Pilla%20and%20Gang%20Li%20and%20Ling%20Liu%20and%20Feng%20Luo%0AAbstract%3A%20Molecular%20function%20is%20largely%20determined%20by%20structure.%20Accurately%20aligning%20molecular%20structure%20with%20natural%20language%20is%20therefore%20essential%20for%20enabling%20large%20language%20models%20%28LLMs%29%20to%20reason%20about%20downstream%20chemical%20tasks.%20However%2C%20the%20substantial%20cost%20of%20human%20annotation%20makes%20it%20infeasible%20to%20construct%20large-scale%2C%20high-quality%20datasets%20of%20structure-grounded%20descriptions.%20In%20this%20work%2C%20we%20propose%20a%20fully%20automated%20annotation%20framework%20for%20generating%20precise%20molecular%20structure%20descriptions%20at%20scale.%20Our%20approach%20builds%20upon%20and%20extends%20a%20rule-based%20chemical%20nomenclature%20parser%20to%20interpret%20IUPAC%20names%20and%20construct%20enriched%2C%20structured%20XML%20metadata%20that%20explicitly%20encodes%20molecular%20structure.%20This%20metadata%20is%20then%20used%20to%20guide%20LLMs%20in%20producing%20accurate%20natural-language%20descriptions.%20Using%20this%20framework%2C%20we%20curate%20a%20large-scale%20dataset%20of%20approximately%20%24163%24k%20molecule-description%20pairs.%20A%20rigorous%20validation%20protocol%20combining%20LLM-based%20and%20expert%20human%20evaluation%20on%20a%20subset%20of%20%242%2C000%24%20molecules%20demonstrates%20a%20high%20description%20precision%20of%20%2498.6%5C%25%24.%20The%20resulting%20dataset%20provides%20a%20reliable%20foundation%20for%20future%20molecule-language%20alignment%2C%20and%20the%20proposed%20annotation%20method%20is%20readily%20extensible%20to%20larger%20datasets%20and%20broader%20chemical%20tasks%20that%20rely%20on%20structural%20descriptions.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-Scale%2520Dataset%2520for%2520Molecular%2520Structure-Language%2520Description%2520via%2520a%2520Rule-Regularized%2520Method%26entry.906535625%3DFeiyang%2520Cai%2520and%2520Guijuan%2520He%2520and%2520Yi%2520Hu%2520and%2520Jingjing%2520Wang%2520and%2520Joshua%2520Luo%2520and%2520Tianyu%2520Zhu%2520and%2520Srikanth%2520Pilla%2520and%2520Gang%2520Li%2520and%2520Ling%2520Liu%2520and%2520Feng%2520Luo%26entry.1292438233%3DMolecular%2520function%2520is%2520largely%2520determined%2520by%2520structure.%2520Accurately%2520aligning%2520molecular%2520structure%2520with%2520natural%2520language%2520is%2520therefore%2520essential%2520for%2520enabling%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520reason%2520about%2520downstream%2520chemical%2520tasks.%2520However%252C%2520the%2520substantial%2520cost%2520of%2520human%2520annotation%2520makes%2520it%2520infeasible%2520to%2520construct%2520large-scale%252C%2520high-quality%2520datasets%2520of%2520structure-grounded%2520descriptions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fully%2520automated%2520annotation%2520framework%2520for%2520generating%2520precise%2520molecular%2520structure%2520descriptions%2520at%2520scale.%2520Our%2520approach%2520builds%2520upon%2520and%2520extends%2520a%2520rule-based%2520chemical%2520nomenclature%2520parser%2520to%2520interpret%2520IUPAC%2520names%2520and%2520construct%2520enriched%252C%2520structured%2520XML%2520metadata%2520that%2520explicitly%2520encodes%2520molecular%2520structure.%2520This%2520metadata%2520is%2520then%2520used%2520to%2520guide%2520LLMs%2520in%2520producing%2520accurate%2520natural-language%2520descriptions.%2520Using%2520this%2520framework%252C%2520we%2520curate%2520a%2520large-scale%2520dataset%2520of%2520approximately%2520%2524163%2524k%2520molecule-description%2520pairs.%2520A%2520rigorous%2520validation%2520protocol%2520combining%2520LLM-based%2520and%2520expert%2520human%2520evaluation%2520on%2520a%2520subset%2520of%2520%25242%252C000%2524%2520molecules%2520demonstrates%2520a%2520high%2520description%2520precision%2520of%2520%252498.6%255C%2525%2524.%2520The%2520resulting%2520dataset%2520provides%2520a%2520reliable%2520foundation%2520for%2520future%2520molecule-language%2520alignment%252C%2520and%2520the%2520proposed%2520annotation%2520method%2520is%2520readily%2520extensible%2520to%2520larger%2520datasets%2520and%2520broader%2520chemical%2520tasks%2520that%2520rely%2520on%2520structural%2520descriptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-Scale%20Dataset%20for%20Molecular%20Structure-Language%20Description%20via%20a%20Rule-Regularized%20Method&entry.906535625=Feiyang%20Cai%20and%20Guijuan%20He%20and%20Yi%20Hu%20and%20Jingjing%20Wang%20and%20Joshua%20Luo%20and%20Tianyu%20Zhu%20and%20Srikanth%20Pilla%20and%20Gang%20Li%20and%20Ling%20Liu%20and%20Feng%20Luo&entry.1292438233=Molecular%20function%20is%20largely%20determined%20by%20structure.%20Accurately%20aligning%20molecular%20structure%20with%20natural%20language%20is%20therefore%20essential%20for%20enabling%20large%20language%20models%20%28LLMs%29%20to%20reason%20about%20downstream%20chemical%20tasks.%20However%2C%20the%20substantial%20cost%20of%20human%20annotation%20makes%20it%20infeasible%20to%20construct%20large-scale%2C%20high-quality%20datasets%20of%20structure-grounded%20descriptions.%20In%20this%20work%2C%20we%20propose%20a%20fully%20automated%20annotation%20framework%20for%20generating%20precise%20molecular%20structure%20descriptions%20at%20scale.%20Our%20approach%20builds%20upon%20and%20extends%20a%20rule-based%20chemical%20nomenclature%20parser%20to%20interpret%20IUPAC%20names%20and%20construct%20enriched%2C%20structured%20XML%20metadata%20that%20explicitly%20encodes%20molecular%20structure.%20This%20metadata%20is%20then%20used%20to%20guide%20LLMs%20in%20producing%20accurate%20natural-language%20descriptions.%20Using%20this%20framework%2C%20we%20curate%20a%20large-scale%20dataset%20of%20approximately%20%24163%24k%20molecule-description%20pairs.%20A%20rigorous%20validation%20protocol%20combining%20LLM-based%20and%20expert%20human%20evaluation%20on%20a%20subset%20of%20%242%2C000%24%20molecules%20demonstrates%20a%20high%20description%20precision%20of%20%2498.6%5C%25%24.%20The%20resulting%20dataset%20provides%20a%20reliable%20foundation%20for%20future%20molecule-language%20alignment%2C%20and%20the%20proposed%20annotation%20method%20is%20readily%20extensible%20to%20larger%20datasets%20and%20broader%20chemical%20tasks%20that%20rely%20on%20structural%20descriptions.&entry.1838667208=http%3A//arxiv.org/abs/2602.02320v2&entry.124074799=Read"},
{"title": "Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields", "author": "Weihan Luo and Lily Goli and Sherwin Bahmani and Felix Taubner and Andrea Tagliasacchi and David B. Lindell", "abstract": "Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.", "link": "http://arxiv.org/abs/2602.08958v2", "date": "2026-02-10", "relevancy": 2.3315, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5972}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5818}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grow%20with%20the%20Flow%3A%204D%20Reconstruction%20of%20Growing%20Plants%20with%20Gaussian%20Flow%20Fields&body=Title%3A%20Grow%20with%20the%20Flow%3A%204D%20Reconstruction%20of%20Growing%20Plants%20with%20Gaussian%20Flow%20Fields%0AAuthor%3A%20Weihan%20Luo%20and%20Lily%20Goli%20and%20Sherwin%20Bahmani%20and%20Felix%20Taubner%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%0AAbstract%3A%20Modeling%20the%20time-varying%203D%20appearance%20of%20plants%20during%20their%20growth%20poses%20unique%20challenges%3A%20unlike%20many%20dynamic%20scenes%2C%20plants%20generate%20new%20geometry%20over%20time%20as%20they%20expand%2C%20branch%2C%20and%20differentiate.%20Recent%20motion%20modeling%20techniques%20are%20ill-suited%20to%20this%20problem%20setting.%20For%20example%2C%20deformation%20fields%20cannot%20introduce%20new%20geometry%2C%20and%204D%20Gaussian%20splatting%20constrains%20motion%20to%20a%20linear%20trajectory%20in%20space%20and%20time%20and%20cannot%20track%20the%20same%20set%20of%20Gaussians%20over%20time.%20Here%2C%20we%20introduce%20a%203D%20Gaussian%20flow%20field%20representation%20that%20models%20plant%20growth%20as%20a%20time-varying%20derivative%20over%20Gaussian%20parameters%20--%20position%2C%20scale%2C%20orientation%2C%20color%2C%20and%20opacity%20--%20enabling%20nonlinear%20and%20continuous-time%20growth%20dynamics.%20To%20initialize%20a%20sufficient%20set%20of%20Gaussian%20primitives%2C%20we%20reconstruct%20the%20mature%20plant%20and%20learn%20a%20process%20of%20reverse%20growth%2C%20effectively%20simulating%20the%20plant%27s%20developmental%20history%20in%20reverse.%20Our%20approach%20achieves%20superior%20image%20quality%20and%20geometric%20accuracy%20compared%20to%20prior%20methods%20on%20multi-view%20timelapse%20datasets%20of%20plant%20growth%2C%20providing%20a%20new%20approach%20for%20appearance%20modeling%20of%20growing%203D%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrow%2520with%2520the%2520Flow%253A%25204D%2520Reconstruction%2520of%2520Growing%2520Plants%2520with%2520Gaussian%2520Flow%2520Fields%26entry.906535625%3DWeihan%2520Luo%2520and%2520Lily%2520Goli%2520and%2520Sherwin%2520Bahmani%2520and%2520Felix%2520Taubner%2520and%2520Andrea%2520Tagliasacchi%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3DModeling%2520the%2520time-varying%25203D%2520appearance%2520of%2520plants%2520during%2520their%2520growth%2520poses%2520unique%2520challenges%253A%2520unlike%2520many%2520dynamic%2520scenes%252C%2520plants%2520generate%2520new%2520geometry%2520over%2520time%2520as%2520they%2520expand%252C%2520branch%252C%2520and%2520differentiate.%2520Recent%2520motion%2520modeling%2520techniques%2520are%2520ill-suited%2520to%2520this%2520problem%2520setting.%2520For%2520example%252C%2520deformation%2520fields%2520cannot%2520introduce%2520new%2520geometry%252C%2520and%25204D%2520Gaussian%2520splatting%2520constrains%2520motion%2520to%2520a%2520linear%2520trajectory%2520in%2520space%2520and%2520time%2520and%2520cannot%2520track%2520the%2520same%2520set%2520of%2520Gaussians%2520over%2520time.%2520Here%252C%2520we%2520introduce%2520a%25203D%2520Gaussian%2520flow%2520field%2520representation%2520that%2520models%2520plant%2520growth%2520as%2520a%2520time-varying%2520derivative%2520over%2520Gaussian%2520parameters%2520--%2520position%252C%2520scale%252C%2520orientation%252C%2520color%252C%2520and%2520opacity%2520--%2520enabling%2520nonlinear%2520and%2520continuous-time%2520growth%2520dynamics.%2520To%2520initialize%2520a%2520sufficient%2520set%2520of%2520Gaussian%2520primitives%252C%2520we%2520reconstruct%2520the%2520mature%2520plant%2520and%2520learn%2520a%2520process%2520of%2520reverse%2520growth%252C%2520effectively%2520simulating%2520the%2520plant%2527s%2520developmental%2520history%2520in%2520reverse.%2520Our%2520approach%2520achieves%2520superior%2520image%2520quality%2520and%2520geometric%2520accuracy%2520compared%2520to%2520prior%2520methods%2520on%2520multi-view%2520timelapse%2520datasets%2520of%2520plant%2520growth%252C%2520providing%2520a%2520new%2520approach%2520for%2520appearance%2520modeling%2520of%2520growing%25203D%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grow%20with%20the%20Flow%3A%204D%20Reconstruction%20of%20Growing%20Plants%20with%20Gaussian%20Flow%20Fields&entry.906535625=Weihan%20Luo%20and%20Lily%20Goli%20and%20Sherwin%20Bahmani%20and%20Felix%20Taubner%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell&entry.1292438233=Modeling%20the%20time-varying%203D%20appearance%20of%20plants%20during%20their%20growth%20poses%20unique%20challenges%3A%20unlike%20many%20dynamic%20scenes%2C%20plants%20generate%20new%20geometry%20over%20time%20as%20they%20expand%2C%20branch%2C%20and%20differentiate.%20Recent%20motion%20modeling%20techniques%20are%20ill-suited%20to%20this%20problem%20setting.%20For%20example%2C%20deformation%20fields%20cannot%20introduce%20new%20geometry%2C%20and%204D%20Gaussian%20splatting%20constrains%20motion%20to%20a%20linear%20trajectory%20in%20space%20and%20time%20and%20cannot%20track%20the%20same%20set%20of%20Gaussians%20over%20time.%20Here%2C%20we%20introduce%20a%203D%20Gaussian%20flow%20field%20representation%20that%20models%20plant%20growth%20as%20a%20time-varying%20derivative%20over%20Gaussian%20parameters%20--%20position%2C%20scale%2C%20orientation%2C%20color%2C%20and%20opacity%20--%20enabling%20nonlinear%20and%20continuous-time%20growth%20dynamics.%20To%20initialize%20a%20sufficient%20set%20of%20Gaussian%20primitives%2C%20we%20reconstruct%20the%20mature%20plant%20and%20learn%20a%20process%20of%20reverse%20growth%2C%20effectively%20simulating%20the%20plant%27s%20developmental%20history%20in%20reverse.%20Our%20approach%20achieves%20superior%20image%20quality%20and%20geometric%20accuracy%20compared%20to%20prior%20methods%20on%20multi-view%20timelapse%20datasets%20of%20plant%20growth%2C%20providing%20a%20new%20approach%20for%20appearance%20modeling%20of%20growing%203D%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2602.08958v2&entry.124074799=Read"},
{"title": "Aggregation Models with Optimal Weights for Distributed Gaussian Processes", "author": "Haoyuan Chen and Rui Tuo", "abstract": "Gaussian process (GP) models have received increasing attention in recent years due to their superb prediction accuracy and modeling flexibility. To address the computational burdens of GP models for large-scale datasets, distributed learning for GPs are often adopted. Current aggregation models for distributed GPs is not time-efficient when incorporating correlations between GP experts. In this work, we propose a novel approach for aggregated prediction in distributed GPs. The technique is suitable for both the exact and sparse variational GPs. The proposed method incorporates correlations among experts, leading to better prediction accuracy with manageable computational requirements. As demonstrated by empirical studies, the proposed approach results in more stable predictions in less time than state-of-the-art consistent aggregation models.", "link": "http://arxiv.org/abs/2408.00955v2", "date": "2026-02-10", "relevancy": 2.3274, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4725}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4721}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aggregation%20Models%20with%20Optimal%20Weights%20for%20Distributed%20Gaussian%20Processes&body=Title%3A%20Aggregation%20Models%20with%20Optimal%20Weights%20for%20Distributed%20Gaussian%20Processes%0AAuthor%3A%20Haoyuan%20Chen%20and%20Rui%20Tuo%0AAbstract%3A%20Gaussian%20process%20%28GP%29%20models%20have%20received%20increasing%20attention%20in%20recent%20years%20due%20to%20their%20superb%20prediction%20accuracy%20and%20modeling%20flexibility.%20To%20address%20the%20computational%20burdens%20of%20GP%20models%20for%20large-scale%20datasets%2C%20distributed%20learning%20for%20GPs%20are%20often%20adopted.%20Current%20aggregation%20models%20for%20distributed%20GPs%20is%20not%20time-efficient%20when%20incorporating%20correlations%20between%20GP%20experts.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20for%20aggregated%20prediction%20in%20distributed%20GPs.%20The%20technique%20is%20suitable%20for%20both%20the%20exact%20and%20sparse%20variational%20GPs.%20The%20proposed%20method%20incorporates%20correlations%20among%20experts%2C%20leading%20to%20better%20prediction%20accuracy%20with%20manageable%20computational%20requirements.%20As%20demonstrated%20by%20empirical%20studies%2C%20the%20proposed%20approach%20results%20in%20more%20stable%20predictions%20in%20less%20time%20than%20state-of-the-art%20consistent%20aggregation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2408.00955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAggregation%2520Models%2520with%2520Optimal%2520Weights%2520for%2520Distributed%2520Gaussian%2520Processes%26entry.906535625%3DHaoyuan%2520Chen%2520and%2520Rui%2520Tuo%26entry.1292438233%3DGaussian%2520process%2520%2528GP%2529%2520models%2520have%2520received%2520increasing%2520attention%2520in%2520recent%2520years%2520due%2520to%2520their%2520superb%2520prediction%2520accuracy%2520and%2520modeling%2520flexibility.%2520To%2520address%2520the%2520computational%2520burdens%2520of%2520GP%2520models%2520for%2520large-scale%2520datasets%252C%2520distributed%2520learning%2520for%2520GPs%2520are%2520often%2520adopted.%2520Current%2520aggregation%2520models%2520for%2520distributed%2520GPs%2520is%2520not%2520time-efficient%2520when%2520incorporating%2520correlations%2520between%2520GP%2520experts.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520aggregated%2520prediction%2520in%2520distributed%2520GPs.%2520The%2520technique%2520is%2520suitable%2520for%2520both%2520the%2520exact%2520and%2520sparse%2520variational%2520GPs.%2520The%2520proposed%2520method%2520incorporates%2520correlations%2520among%2520experts%252C%2520leading%2520to%2520better%2520prediction%2520accuracy%2520with%2520manageable%2520computational%2520requirements.%2520As%2520demonstrated%2520by%2520empirical%2520studies%252C%2520the%2520proposed%2520approach%2520results%2520in%2520more%2520stable%2520predictions%2520in%2520less%2520time%2520than%2520state-of-the-art%2520consistent%2520aggregation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aggregation%20Models%20with%20Optimal%20Weights%20for%20Distributed%20Gaussian%20Processes&entry.906535625=Haoyuan%20Chen%20and%20Rui%20Tuo&entry.1292438233=Gaussian%20process%20%28GP%29%20models%20have%20received%20increasing%20attention%20in%20recent%20years%20due%20to%20their%20superb%20prediction%20accuracy%20and%20modeling%20flexibility.%20To%20address%20the%20computational%20burdens%20of%20GP%20models%20for%20large-scale%20datasets%2C%20distributed%20learning%20for%20GPs%20are%20often%20adopted.%20Current%20aggregation%20models%20for%20distributed%20GPs%20is%20not%20time-efficient%20when%20incorporating%20correlations%20between%20GP%20experts.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20for%20aggregated%20prediction%20in%20distributed%20GPs.%20The%20technique%20is%20suitable%20for%20both%20the%20exact%20and%20sparse%20variational%20GPs.%20The%20proposed%20method%20incorporates%20correlations%20among%20experts%2C%20leading%20to%20better%20prediction%20accuracy%20with%20manageable%20computational%20requirements.%20As%20demonstrated%20by%20empirical%20studies%2C%20the%20proposed%20approach%20results%20in%20more%20stable%20predictions%20in%20less%20time%20than%20state-of-the-art%20consistent%20aggregation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2408.00955v2&entry.124074799=Read"},
{"title": "Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning", "author": "Dexun Li and Sidney Tio and Pradeep Varakantham", "abstract": "Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.", "link": "http://arxiv.org/abs/2602.09813v1", "date": "2026-02-10", "relevancy": 2.3203, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6171}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5646}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Unsupervised%20Environment%20Design%20through%20Hierarchical%20Policy%20Representation%20Learning&body=Title%3A%20Efficient%20Unsupervised%20Environment%20Design%20through%20Hierarchical%20Policy%20Representation%20Learning%0AAuthor%3A%20Dexun%20Li%20and%20Sidney%20Tio%20and%20Pradeep%20Varakantham%0AAbstract%3A%20Unsupervised%20Environment%20Design%20%28UED%29%20has%20emerged%20as%20a%20promising%20approach%20to%20developing%20general-purpose%20agents%20through%20automated%20curriculum%20generation.%20Popular%20UED%20methods%20focus%20on%20Open-Endedness%2C%20where%20teacher%20algorithms%20rely%20on%20stochastic%20processes%20for%20infinite%20generation%20of%20useful%20environments.%20This%20assumption%20becomes%20impractical%20in%20resource-constrained%20scenarios%20where%20teacher-student%20interaction%20opportunities%20are%20limited.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20hierarchical%20Markov%20Decision%20Process%20%28MDP%29%20framework%20for%20environment%20design.%20Our%20framework%20features%20a%20teacher%20agent%20that%20leverages%20student%20policy%20representations%20derived%20from%20discovered%20evaluation%20environments%2C%20enabling%20it%20to%20generate%20training%20environments%20based%20on%20the%20student%27s%20capabilities.%20To%20improve%20efficiency%2C%20we%20incorporate%20a%20generative%20model%20that%20augments%20the%20teacher%27s%20training%20dataset%20with%20synthetic%20data%2C%20reducing%20the%20need%20for%20teacher-student%20interactions.%20In%20experiments%20across%20several%20domains%2C%20we%20show%20that%20our%20method%20outperforms%20baseline%20approaches%20while%20requiring%20fewer%20teacher-student%20interactions%20in%20a%20single%20episode.%20The%20results%20suggest%20the%20applicability%20of%20our%20approach%20in%20settings%20where%20training%20opportunities%20are%20limited.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Unsupervised%2520Environment%2520Design%2520through%2520Hierarchical%2520Policy%2520Representation%2520Learning%26entry.906535625%3DDexun%2520Li%2520and%2520Sidney%2520Tio%2520and%2520Pradeep%2520Varakantham%26entry.1292438233%3DUnsupervised%2520Environment%2520Design%2520%2528UED%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520developing%2520general-purpose%2520agents%2520through%2520automated%2520curriculum%2520generation.%2520Popular%2520UED%2520methods%2520focus%2520on%2520Open-Endedness%252C%2520where%2520teacher%2520algorithms%2520rely%2520on%2520stochastic%2520processes%2520for%2520infinite%2520generation%2520of%2520useful%2520environments.%2520This%2520assumption%2520becomes%2520impractical%2520in%2520resource-constrained%2520scenarios%2520where%2520teacher-student%2520interaction%2520opportunities%2520are%2520limited.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520hierarchical%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520framework%2520for%2520environment%2520design.%2520Our%2520framework%2520features%2520a%2520teacher%2520agent%2520that%2520leverages%2520student%2520policy%2520representations%2520derived%2520from%2520discovered%2520evaluation%2520environments%252C%2520enabling%2520it%2520to%2520generate%2520training%2520environments%2520based%2520on%2520the%2520student%2527s%2520capabilities.%2520To%2520improve%2520efficiency%252C%2520we%2520incorporate%2520a%2520generative%2520model%2520that%2520augments%2520the%2520teacher%2527s%2520training%2520dataset%2520with%2520synthetic%2520data%252C%2520reducing%2520the%2520need%2520for%2520teacher-student%2520interactions.%2520In%2520experiments%2520across%2520several%2520domains%252C%2520we%2520show%2520that%2520our%2520method%2520outperforms%2520baseline%2520approaches%2520while%2520requiring%2520fewer%2520teacher-student%2520interactions%2520in%2520a%2520single%2520episode.%2520The%2520results%2520suggest%2520the%2520applicability%2520of%2520our%2520approach%2520in%2520settings%2520where%2520training%2520opportunities%2520are%2520limited.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Unsupervised%20Environment%20Design%20through%20Hierarchical%20Policy%20Representation%20Learning&entry.906535625=Dexun%20Li%20and%20Sidney%20Tio%20and%20Pradeep%20Varakantham&entry.1292438233=Unsupervised%20Environment%20Design%20%28UED%29%20has%20emerged%20as%20a%20promising%20approach%20to%20developing%20general-purpose%20agents%20through%20automated%20curriculum%20generation.%20Popular%20UED%20methods%20focus%20on%20Open-Endedness%2C%20where%20teacher%20algorithms%20rely%20on%20stochastic%20processes%20for%20infinite%20generation%20of%20useful%20environments.%20This%20assumption%20becomes%20impractical%20in%20resource-constrained%20scenarios%20where%20teacher-student%20interaction%20opportunities%20are%20limited.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20hierarchical%20Markov%20Decision%20Process%20%28MDP%29%20framework%20for%20environment%20design.%20Our%20framework%20features%20a%20teacher%20agent%20that%20leverages%20student%20policy%20representations%20derived%20from%20discovered%20evaluation%20environments%2C%20enabling%20it%20to%20generate%20training%20environments%20based%20on%20the%20student%27s%20capabilities.%20To%20improve%20efficiency%2C%20we%20incorporate%20a%20generative%20model%20that%20augments%20the%20teacher%27s%20training%20dataset%20with%20synthetic%20data%2C%20reducing%20the%20need%20for%20teacher-student%20interactions.%20In%20experiments%20across%20several%20domains%2C%20we%20show%20that%20our%20method%20outperforms%20baseline%20approaches%20while%20requiring%20fewer%20teacher-student%20interactions%20in%20a%20single%20episode.%20The%20results%20suggest%20the%20applicability%20of%20our%20approach%20in%20settings%20where%20training%20opportunities%20are%20limited.&entry.1838667208=http%3A//arxiv.org/abs/2602.09813v1&entry.124074799=Read"},
{"title": "AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild", "author": "Xiaolou Sun and Wufei Si and Wenhui Ni and Yuntian Li and Dongming Wu and Fei Xie and Runwei Guan and He-Yang Xu and Henghui Ding and Yuan Wu and Yutao Yue and Yongming Huang and Hui Xiong", "abstract": "Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.", "link": "http://arxiv.org/abs/2602.09657v1", "date": "2026-02-10", "relevancy": 2.3191, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5956}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5795}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoFly%3A%20Vision-Language-Action%20Model%20for%20UAV%20Autonomous%20Navigation%20in%20the%20Wild&body=Title%3A%20AutoFly%3A%20Vision-Language-Action%20Model%20for%20UAV%20Autonomous%20Navigation%20in%20the%20Wild%0AAuthor%3A%20Xiaolou%20Sun%20and%20Wufei%20Si%20and%20Wenhui%20Ni%20and%20Yuntian%20Li%20and%20Dongming%20Wu%20and%20Fei%20Xie%20and%20Runwei%20Guan%20and%20He-Yang%20Xu%20and%20Henghui%20Ding%20and%20Yuan%20Wu%20and%20Yutao%20Yue%20and%20Yongming%20Huang%20and%20Hui%20Xiong%0AAbstract%3A%20Vision-language%20navigation%20%28VLN%29%20requires%20intelligent%20agents%20to%20navigate%20environments%20by%20interpreting%20linguistic%20instructions%20alongside%20visual%20observations%2C%20serving%20as%20a%20cornerstone%20task%20in%20Embodied%20AI.%20Current%20VLN%20research%20for%20unmanned%20aerial%20vehicles%20%28UAVs%29%20relies%20on%20detailed%2C%20pre-specified%20instructions%20to%20guide%20the%20UAV%20along%20predetermined%20routes.%20However%2C%20real-world%20outdoor%20exploration%20typically%20occurs%20in%20unknown%20environments%20where%20detailed%20navigation%20instructions%20are%20unavailable.%20Instead%2C%20only%20coarse-grained%20positional%20or%20directional%20guidance%20can%20be%20provided%2C%20requiring%20UAVs%20to%20autonomously%20navigate%20through%20continuous%20planning%20and%20obstacle%20avoidance.%20To%20bridge%20this%20gap%2C%20we%20propose%20AutoFly%2C%20an%20end-to-end%20Vision-Language-Action%20%28VLA%29%20model%20for%20autonomous%20UAV%20navigation.%20AutoFly%20incorporates%20a%20pseudo-depth%20encoder%20that%20derives%20depth-aware%20features%20from%20RGB%20inputs%20to%20enhance%20spatial%20reasoning%2C%20coupled%20with%20a%20progressive%20two-stage%20training%20strategy%20that%20effectively%20aligns%20visual%2C%20depth%2C%20and%20linguistic%20representations%20with%20action%20policies.%20Moreover%2C%20existing%20VLN%20datasets%20have%20fundamental%20limitations%20for%20real-world%20autonomous%20navigation%2C%20stemming%20from%20their%20heavy%20reliance%20on%20explicit%20instruction-following%20over%20autonomous%20decision-making%20and%20insufficient%20real-world%20data.%20To%20address%20these%20issues%2C%20we%20construct%20a%20novel%20autonomous%20navigation%20dataset%20that%20shifts%20the%20paradigm%20from%20instruction-following%20to%20autonomous%20behavior%20modeling%20through%3A%20%281%29%20trajectory%20collection%20emphasizing%20continuous%20obstacle%20avoidance%2C%20autonomous%20planning%2C%20and%20recognition%20workflows%3B%20%282%29%20comprehensive%20real-world%20data%20integration.%20Experimental%20results%20demonstrate%20that%20AutoFly%20achieves%20a%203.9%25%20higher%20success%20rate%20compared%20to%20state-of-the-art%20VLA%20baselines%2C%20with%20consistent%20performance%20across%20simulated%20and%20real%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoFly%253A%2520Vision-Language-Action%2520Model%2520for%2520UAV%2520Autonomous%2520Navigation%2520in%2520the%2520Wild%26entry.906535625%3DXiaolou%2520Sun%2520and%2520Wufei%2520Si%2520and%2520Wenhui%2520Ni%2520and%2520Yuntian%2520Li%2520and%2520Dongming%2520Wu%2520and%2520Fei%2520Xie%2520and%2520Runwei%2520Guan%2520and%2520He-Yang%2520Xu%2520and%2520Henghui%2520Ding%2520and%2520Yuan%2520Wu%2520and%2520Yutao%2520Yue%2520and%2520Yongming%2520Huang%2520and%2520Hui%2520Xiong%26entry.1292438233%3DVision-language%2520navigation%2520%2528VLN%2529%2520requires%2520intelligent%2520agents%2520to%2520navigate%2520environments%2520by%2520interpreting%2520linguistic%2520instructions%2520alongside%2520visual%2520observations%252C%2520serving%2520as%2520a%2520cornerstone%2520task%2520in%2520Embodied%2520AI.%2520Current%2520VLN%2520research%2520for%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520relies%2520on%2520detailed%252C%2520pre-specified%2520instructions%2520to%2520guide%2520the%2520UAV%2520along%2520predetermined%2520routes.%2520However%252C%2520real-world%2520outdoor%2520exploration%2520typically%2520occurs%2520in%2520unknown%2520environments%2520where%2520detailed%2520navigation%2520instructions%2520are%2520unavailable.%2520Instead%252C%2520only%2520coarse-grained%2520positional%2520or%2520directional%2520guidance%2520can%2520be%2520provided%252C%2520requiring%2520UAVs%2520to%2520autonomously%2520navigate%2520through%2520continuous%2520planning%2520and%2520obstacle%2520avoidance.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520AutoFly%252C%2520an%2520end-to-end%2520Vision-Language-Action%2520%2528VLA%2529%2520model%2520for%2520autonomous%2520UAV%2520navigation.%2520AutoFly%2520incorporates%2520a%2520pseudo-depth%2520encoder%2520that%2520derives%2520depth-aware%2520features%2520from%2520RGB%2520inputs%2520to%2520enhance%2520spatial%2520reasoning%252C%2520coupled%2520with%2520a%2520progressive%2520two-stage%2520training%2520strategy%2520that%2520effectively%2520aligns%2520visual%252C%2520depth%252C%2520and%2520linguistic%2520representations%2520with%2520action%2520policies.%2520Moreover%252C%2520existing%2520VLN%2520datasets%2520have%2520fundamental%2520limitations%2520for%2520real-world%2520autonomous%2520navigation%252C%2520stemming%2520from%2520their%2520heavy%2520reliance%2520on%2520explicit%2520instruction-following%2520over%2520autonomous%2520decision-making%2520and%2520insufficient%2520real-world%2520data.%2520To%2520address%2520these%2520issues%252C%2520we%2520construct%2520a%2520novel%2520autonomous%2520navigation%2520dataset%2520that%2520shifts%2520the%2520paradigm%2520from%2520instruction-following%2520to%2520autonomous%2520behavior%2520modeling%2520through%253A%2520%25281%2529%2520trajectory%2520collection%2520emphasizing%2520continuous%2520obstacle%2520avoidance%252C%2520autonomous%2520planning%252C%2520and%2520recognition%2520workflows%253B%2520%25282%2529%2520comprehensive%2520real-world%2520data%2520integration.%2520Experimental%2520results%2520demonstrate%2520that%2520AutoFly%2520achieves%2520a%25203.9%2525%2520higher%2520success%2520rate%2520compared%2520to%2520state-of-the-art%2520VLA%2520baselines%252C%2520with%2520consistent%2520performance%2520across%2520simulated%2520and%2520real%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoFly%3A%20Vision-Language-Action%20Model%20for%20UAV%20Autonomous%20Navigation%20in%20the%20Wild&entry.906535625=Xiaolou%20Sun%20and%20Wufei%20Si%20and%20Wenhui%20Ni%20and%20Yuntian%20Li%20and%20Dongming%20Wu%20and%20Fei%20Xie%20and%20Runwei%20Guan%20and%20He-Yang%20Xu%20and%20Henghui%20Ding%20and%20Yuan%20Wu%20and%20Yutao%20Yue%20and%20Yongming%20Huang%20and%20Hui%20Xiong&entry.1292438233=Vision-language%20navigation%20%28VLN%29%20requires%20intelligent%20agents%20to%20navigate%20environments%20by%20interpreting%20linguistic%20instructions%20alongside%20visual%20observations%2C%20serving%20as%20a%20cornerstone%20task%20in%20Embodied%20AI.%20Current%20VLN%20research%20for%20unmanned%20aerial%20vehicles%20%28UAVs%29%20relies%20on%20detailed%2C%20pre-specified%20instructions%20to%20guide%20the%20UAV%20along%20predetermined%20routes.%20However%2C%20real-world%20outdoor%20exploration%20typically%20occurs%20in%20unknown%20environments%20where%20detailed%20navigation%20instructions%20are%20unavailable.%20Instead%2C%20only%20coarse-grained%20positional%20or%20directional%20guidance%20can%20be%20provided%2C%20requiring%20UAVs%20to%20autonomously%20navigate%20through%20continuous%20planning%20and%20obstacle%20avoidance.%20To%20bridge%20this%20gap%2C%20we%20propose%20AutoFly%2C%20an%20end-to-end%20Vision-Language-Action%20%28VLA%29%20model%20for%20autonomous%20UAV%20navigation.%20AutoFly%20incorporates%20a%20pseudo-depth%20encoder%20that%20derives%20depth-aware%20features%20from%20RGB%20inputs%20to%20enhance%20spatial%20reasoning%2C%20coupled%20with%20a%20progressive%20two-stage%20training%20strategy%20that%20effectively%20aligns%20visual%2C%20depth%2C%20and%20linguistic%20representations%20with%20action%20policies.%20Moreover%2C%20existing%20VLN%20datasets%20have%20fundamental%20limitations%20for%20real-world%20autonomous%20navigation%2C%20stemming%20from%20their%20heavy%20reliance%20on%20explicit%20instruction-following%20over%20autonomous%20decision-making%20and%20insufficient%20real-world%20data.%20To%20address%20these%20issues%2C%20we%20construct%20a%20novel%20autonomous%20navigation%20dataset%20that%20shifts%20the%20paradigm%20from%20instruction-following%20to%20autonomous%20behavior%20modeling%20through%3A%20%281%29%20trajectory%20collection%20emphasizing%20continuous%20obstacle%20avoidance%2C%20autonomous%20planning%2C%20and%20recognition%20workflows%3B%20%282%29%20comprehensive%20real-world%20data%20integration.%20Experimental%20results%20demonstrate%20that%20AutoFly%20achieves%20a%203.9%25%20higher%20success%20rate%20compared%20to%20state-of-the-art%20VLA%20baselines%2C%20with%20consistent%20performance%20across%20simulated%20and%20real%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.09657v1&entry.124074799=Read"},
{"title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?", "author": "Dyna Soumhane Ouchebara and St\u00e9phane Dupont", "abstract": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented vulnerability types. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance. Code repository is available here : https://github.com/DynaSoumhaneOuchebara/LLM-based-code-augmentation-Generate-or-Refactor-", "link": "http://arxiv.org/abs/2512.08493v2", "date": "2026-02-10", "relevancy": 2.3161, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4881}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.453}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-based%20Vulnerable%20Code%20Augmentation%3A%20Generate%20or%20Refactor%3F&body=Title%3A%20LLM-based%20Vulnerable%20Code%20Augmentation%3A%20Generate%20or%20Refactor%3F%0AAuthor%3A%20Dyna%20Soumhane%20Ouchebara%20and%20St%C3%A9phane%20Dupont%0AAbstract%3A%20Vulnerability%20code-bases%20often%20suffer%20from%20severe%20imbalance%2C%20limiting%20the%20effectiveness%20of%20Deep%20Learning-based%20vulnerability%20classifiers.%20Data%20Augmentation%20could%20help%20solve%20this%20by%20mitigating%20the%20scarcity%20of%20under-represented%20vulnerability%20types.%20In%20this%20context%2C%20we%20investigate%20LLM-based%20augmentation%20for%20vulnerable%20functions%2C%20comparing%20controlled%20generation%20of%20new%20vulnerable%20samples%20with%20semantics-preserving%20refactoring%20of%20existing%20ones.%20Using%20Qwen2.5-Coder%20to%20produce%20augmented%20data%20and%20CodeBERT%20as%20a%20classifier%20on%20the%20SVEN%20dataset%2C%20we%20find%20that%20our%20approaches%20are%20indeed%20effective%20in%20enriching%20vulnerable%20code-bases%20through%20a%20simple%20process%20and%20with%20reasonable%20quality%2C%20and%20that%20a%20hybrid%20strategy%20best%20boosts%20vulnerability%20classifiers%27%20performance.%20Code%20repository%20is%20available%20here%20%3A%20https%3A//github.com/DynaSoumhaneOuchebara/LLM-based-code-augmentation-Generate-or-Refactor-%0ALink%3A%20http%3A//arxiv.org/abs/2512.08493v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-based%2520Vulnerable%2520Code%2520Augmentation%253A%2520Generate%2520or%2520Refactor%253F%26entry.906535625%3DDyna%2520Soumhane%2520Ouchebara%2520and%2520St%25C3%25A9phane%2520Dupont%26entry.1292438233%3DVulnerability%2520code-bases%2520often%2520suffer%2520from%2520severe%2520imbalance%252C%2520limiting%2520the%2520effectiveness%2520of%2520Deep%2520Learning-based%2520vulnerability%2520classifiers.%2520Data%2520Augmentation%2520could%2520help%2520solve%2520this%2520by%2520mitigating%2520the%2520scarcity%2520of%2520under-represented%2520vulnerability%2520types.%2520In%2520this%2520context%252C%2520we%2520investigate%2520LLM-based%2520augmentation%2520for%2520vulnerable%2520functions%252C%2520comparing%2520controlled%2520generation%2520of%2520new%2520vulnerable%2520samples%2520with%2520semantics-preserving%2520refactoring%2520of%2520existing%2520ones.%2520Using%2520Qwen2.5-Coder%2520to%2520produce%2520augmented%2520data%2520and%2520CodeBERT%2520as%2520a%2520classifier%2520on%2520the%2520SVEN%2520dataset%252C%2520we%2520find%2520that%2520our%2520approaches%2520are%2520indeed%2520effective%2520in%2520enriching%2520vulnerable%2520code-bases%2520through%2520a%2520simple%2520process%2520and%2520with%2520reasonable%2520quality%252C%2520and%2520that%2520a%2520hybrid%2520strategy%2520best%2520boosts%2520vulnerability%2520classifiers%2527%2520performance.%2520Code%2520repository%2520is%2520available%2520here%2520%253A%2520https%253A//github.com/DynaSoumhaneOuchebara/LLM-based-code-augmentation-Generate-or-Refactor-%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08493v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-based%20Vulnerable%20Code%20Augmentation%3A%20Generate%20or%20Refactor%3F&entry.906535625=Dyna%20Soumhane%20Ouchebara%20and%20St%C3%A9phane%20Dupont&entry.1292438233=Vulnerability%20code-bases%20often%20suffer%20from%20severe%20imbalance%2C%20limiting%20the%20effectiveness%20of%20Deep%20Learning-based%20vulnerability%20classifiers.%20Data%20Augmentation%20could%20help%20solve%20this%20by%20mitigating%20the%20scarcity%20of%20under-represented%20vulnerability%20types.%20In%20this%20context%2C%20we%20investigate%20LLM-based%20augmentation%20for%20vulnerable%20functions%2C%20comparing%20controlled%20generation%20of%20new%20vulnerable%20samples%20with%20semantics-preserving%20refactoring%20of%20existing%20ones.%20Using%20Qwen2.5-Coder%20to%20produce%20augmented%20data%20and%20CodeBERT%20as%20a%20classifier%20on%20the%20SVEN%20dataset%2C%20we%20find%20that%20our%20approaches%20are%20indeed%20effective%20in%20enriching%20vulnerable%20code-bases%20through%20a%20simple%20process%20and%20with%20reasonable%20quality%2C%20and%20that%20a%20hybrid%20strategy%20best%20boosts%20vulnerability%20classifiers%27%20performance.%20Code%20repository%20is%20available%20here%20%3A%20https%3A//github.com/DynaSoumhaneOuchebara/LLM-based-code-augmentation-Generate-or-Refactor-&entry.1838667208=http%3A//arxiv.org/abs/2512.08493v2&entry.124074799=Read"},
{"title": "VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model", "author": "Hanqing Wang and Mingyu Liu and Xiaoyu Chen and Chengwei MA and Yiming Zhong and Wenti Yin and Yuhao Liu and Zhiqing Cui and Jiahao Yuan and Lu Dai and Zhiyuan Ma and Hui Xiong", "abstract": "3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \\textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \\textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \\textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.", "link": "http://arxiv.org/abs/2602.09638v1", "date": "2026-02-10", "relevancy": 2.3131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5789}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAfford%3A%20Grounding%203D%20Affordance%20from%20Human-Object-Interaction%20Videos%20via%20Multimodal%20Large%20Language%20Model&body=Title%3A%20VideoAfford%3A%20Grounding%203D%20Affordance%20from%20Human-Object-Interaction%20Videos%20via%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Hanqing%20Wang%20and%20Mingyu%20Liu%20and%20Xiaoyu%20Chen%20and%20Chengwei%20MA%20and%20Yiming%20Zhong%20and%20Wenti%20Yin%20and%20Yuhao%20Liu%20and%20Zhiqing%20Cui%20and%20Jiahao%20Yuan%20and%20Lu%20Dai%20and%20Zhiyuan%20Ma%20and%20Hui%20Xiong%0AAbstract%3A%203D%20affordance%20grounding%20aims%20to%20highlight%20the%20actionable%20regions%20on%203D%20objects%2C%20which%20is%20crucial%20for%20robotic%20manipulation.%20Previous%20research%20primarily%20focused%20on%20learning%20affordance%20knowledge%20from%20static%20cues%20such%20as%20language%20and%20images%2C%20which%20struggle%20to%20provide%20sufficient%20dynamic%20interaction%20context%20that%20can%20reveal%20temporal%20and%20causal%20cues.%20To%20alleviate%20this%20predicament%2C%20we%20collect%20a%20comprehensive%20video-based%203D%20affordance%20dataset%2C%20%5Ctextit%7BVIDA%7D%2C%20which%20contains%2038K%20human-object-interaction%20videos%20covering%2016%20affordance%20types%2C%2038%20object%20categories%2C%20and%2022K%20point%20clouds.%20Based%20on%20%5Ctextit%7BVIDA%7D%2C%20we%20propose%20a%20strong%20baseline%3A%20VideoAfford%2C%20which%20activates%20multimodal%20large%20language%20models%20with%20additional%20affordance%20segmentation%20capabilities%2C%20enabling%20both%20world%20knowledge%20reasoning%20and%20fine-grained%20affordance%20grounding%20within%20a%20unified%20framework.%20To%20enhance%20action%20understanding%20capability%2C%20we%20leverage%20a%20latent%20action%20encoder%20to%20extract%20dynamic%20interaction%20priors%20from%20HOI%20videos.%20Moreover%2C%20we%20introduce%20a%20%5Ctextit%7Bspatial-aware%7D%20loss%20function%20to%20enable%20VideoAfford%20to%20obtain%20comprehensive%203D%20spatial%20knowledge.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20our%20model%20significantly%20outperforms%20well-established%20methods%20and%20exhibits%20strong%20open-world%20generalization%20with%20affordance%20reasoning%20abilities.%20All%20datasets%20and%20code%20will%20be%20publicly%20released%20to%20advance%20research%20in%20this%20area.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAfford%253A%2520Grounding%25203D%2520Affordance%2520from%2520Human-Object-Interaction%2520Videos%2520via%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DHanqing%2520Wang%2520and%2520Mingyu%2520Liu%2520and%2520Xiaoyu%2520Chen%2520and%2520Chengwei%2520MA%2520and%2520Yiming%2520Zhong%2520and%2520Wenti%2520Yin%2520and%2520Yuhao%2520Liu%2520and%2520Zhiqing%2520Cui%2520and%2520Jiahao%2520Yuan%2520and%2520Lu%2520Dai%2520and%2520Zhiyuan%2520Ma%2520and%2520Hui%2520Xiong%26entry.1292438233%3D3D%2520affordance%2520grounding%2520aims%2520to%2520highlight%2520the%2520actionable%2520regions%2520on%25203D%2520objects%252C%2520which%2520is%2520crucial%2520for%2520robotic%2520manipulation.%2520Previous%2520research%2520primarily%2520focused%2520on%2520learning%2520affordance%2520knowledge%2520from%2520static%2520cues%2520such%2520as%2520language%2520and%2520images%252C%2520which%2520struggle%2520to%2520provide%2520sufficient%2520dynamic%2520interaction%2520context%2520that%2520can%2520reveal%2520temporal%2520and%2520causal%2520cues.%2520To%2520alleviate%2520this%2520predicament%252C%2520we%2520collect%2520a%2520comprehensive%2520video-based%25203D%2520affordance%2520dataset%252C%2520%255Ctextit%257BVIDA%257D%252C%2520which%2520contains%252038K%2520human-object-interaction%2520videos%2520covering%252016%2520affordance%2520types%252C%252038%2520object%2520categories%252C%2520and%252022K%2520point%2520clouds.%2520Based%2520on%2520%255Ctextit%257BVIDA%257D%252C%2520we%2520propose%2520a%2520strong%2520baseline%253A%2520VideoAfford%252C%2520which%2520activates%2520multimodal%2520large%2520language%2520models%2520with%2520additional%2520affordance%2520segmentation%2520capabilities%252C%2520enabling%2520both%2520world%2520knowledge%2520reasoning%2520and%2520fine-grained%2520affordance%2520grounding%2520within%2520a%2520unified%2520framework.%2520To%2520enhance%2520action%2520understanding%2520capability%252C%2520we%2520leverage%2520a%2520latent%2520action%2520encoder%2520to%2520extract%2520dynamic%2520interaction%2520priors%2520from%2520HOI%2520videos.%2520Moreover%252C%2520we%2520introduce%2520a%2520%255Ctextit%257Bspatial-aware%257D%2520loss%2520function%2520to%2520enable%2520VideoAfford%2520to%2520obtain%2520comprehensive%25203D%2520spatial%2520knowledge.%2520Extensive%2520experimental%2520evaluations%2520demonstrate%2520that%2520our%2520model%2520significantly%2520outperforms%2520well-established%2520methods%2520and%2520exhibits%2520strong%2520open-world%2520generalization%2520with%2520affordance%2520reasoning%2520abilities.%2520All%2520datasets%2520and%2520code%2520will%2520be%2520publicly%2520released%2520to%2520advance%2520research%2520in%2520this%2520area.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAfford%3A%20Grounding%203D%20Affordance%20from%20Human-Object-Interaction%20Videos%20via%20Multimodal%20Large%20Language%20Model&entry.906535625=Hanqing%20Wang%20and%20Mingyu%20Liu%20and%20Xiaoyu%20Chen%20and%20Chengwei%20MA%20and%20Yiming%20Zhong%20and%20Wenti%20Yin%20and%20Yuhao%20Liu%20and%20Zhiqing%20Cui%20and%20Jiahao%20Yuan%20and%20Lu%20Dai%20and%20Zhiyuan%20Ma%20and%20Hui%20Xiong&entry.1292438233=3D%20affordance%20grounding%20aims%20to%20highlight%20the%20actionable%20regions%20on%203D%20objects%2C%20which%20is%20crucial%20for%20robotic%20manipulation.%20Previous%20research%20primarily%20focused%20on%20learning%20affordance%20knowledge%20from%20static%20cues%20such%20as%20language%20and%20images%2C%20which%20struggle%20to%20provide%20sufficient%20dynamic%20interaction%20context%20that%20can%20reveal%20temporal%20and%20causal%20cues.%20To%20alleviate%20this%20predicament%2C%20we%20collect%20a%20comprehensive%20video-based%203D%20affordance%20dataset%2C%20%5Ctextit%7BVIDA%7D%2C%20which%20contains%2038K%20human-object-interaction%20videos%20covering%2016%20affordance%20types%2C%2038%20object%20categories%2C%20and%2022K%20point%20clouds.%20Based%20on%20%5Ctextit%7BVIDA%7D%2C%20we%20propose%20a%20strong%20baseline%3A%20VideoAfford%2C%20which%20activates%20multimodal%20large%20language%20models%20with%20additional%20affordance%20segmentation%20capabilities%2C%20enabling%20both%20world%20knowledge%20reasoning%20and%20fine-grained%20affordance%20grounding%20within%20a%20unified%20framework.%20To%20enhance%20action%20understanding%20capability%2C%20we%20leverage%20a%20latent%20action%20encoder%20to%20extract%20dynamic%20interaction%20priors%20from%20HOI%20videos.%20Moreover%2C%20we%20introduce%20a%20%5Ctextit%7Bspatial-aware%7D%20loss%20function%20to%20enable%20VideoAfford%20to%20obtain%20comprehensive%203D%20spatial%20knowledge.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20our%20model%20significantly%20outperforms%20well-established%20methods%20and%20exhibits%20strong%20open-world%20generalization%20with%20affordance%20reasoning%20abilities.%20All%20datasets%20and%20code%20will%20be%20publicly%20released%20to%20advance%20research%20in%20this%20area.&entry.1838667208=http%3A//arxiv.org/abs/2602.09638v1&entry.124074799=Read"},
{"title": "Text summarization via global structure awareness", "author": "Jiaquan Zhang and Chaoning Zhang and Shuxu Chen and Yibei Liu and Chenghao Li and Qigan Sun and Shuai Yuan and Fachrina Dewi Puspitasari and Dongshen Han and Guoqing Wang and Sung-Ho Bae and Yang Yang", "abstract": "Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often overlooks global structure, leading to disrupted coherence and weakened downstream performance. Some studies employ large language models (LLMs), which achieve higher accuracy but incur substantial resource and time costs. To address these issues, we introduce GloSA-sum, the first summarization approach that achieves global structure awareness via topological data analysis (TDA). GloSA-sum summarizes text efficiently while preserving semantic cores and logical dependencies. Specifically, we construct a semantic-weighted graph from sentence embeddings, where persistent homology identifies core semantics and logical structures, preserved in a ``protection pool'' as the backbone for summarization. We design a topology-guided iterative strategy, where lightweight proxy metrics approximate sentence importance to avoid repeated high-cost computations, thus preserving structural integrity while improving efficiency. To further enhance long-text processing, we propose a hierarchical strategy that integrates segment-level and global summarization. Experiments on multiple datasets demonstrate that GloSA-sum reduces redundancy while preserving semantic and logical integrity, striking a balance between accuracy and efficiency, and further benefits LLM downstream tasks by shortening contexts while retaining essential reasoning chains.", "link": "http://arxiv.org/abs/2602.09821v1", "date": "2026-02-10", "relevancy": 2.3075, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4619}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text%20summarization%20via%20global%20structure%20awareness&body=Title%3A%20Text%20summarization%20via%20global%20structure%20awareness%0AAuthor%3A%20Jiaquan%20Zhang%20and%20Chaoning%20Zhang%20and%20Shuxu%20Chen%20and%20Yibei%20Liu%20and%20Chenghao%20Li%20and%20Qigan%20Sun%20and%20Shuai%20Yuan%20and%20Fachrina%20Dewi%20Puspitasari%20and%20Dongshen%20Han%20and%20Guoqing%20Wang%20and%20Sung-Ho%20Bae%20and%20Yang%20Yang%0AAbstract%3A%20Text%20summarization%20is%20a%20fundamental%20task%20in%20natural%20language%20processing%20%28NLP%29%2C%20and%20the%20information%20explosion%20has%20made%20long-document%20processing%20increasingly%20demanding%2C%20making%20summarization%20essential.%20Existing%20research%20mainly%20focuses%20on%20model%20improvements%20and%20sentence-level%20pruning%2C%20but%20often%20overlooks%20global%20structure%2C%20leading%20to%20disrupted%20coherence%20and%20weakened%20downstream%20performance.%20Some%20studies%20employ%20large%20language%20models%20%28LLMs%29%2C%20which%20achieve%20higher%20accuracy%20but%20incur%20substantial%20resource%20and%20time%20costs.%20To%20address%20these%20issues%2C%20we%20introduce%20GloSA-sum%2C%20the%20first%20summarization%20approach%20that%20achieves%20global%20structure%20awareness%20via%20topological%20data%20analysis%20%28TDA%29.%20GloSA-sum%20summarizes%20text%20efficiently%20while%20preserving%20semantic%20cores%20and%20logical%20dependencies.%20Specifically%2C%20we%20construct%20a%20semantic-weighted%20graph%20from%20sentence%20embeddings%2C%20where%20persistent%20homology%20identifies%20core%20semantics%20and%20logical%20structures%2C%20preserved%20in%20a%20%60%60protection%20pool%27%27%20as%20the%20backbone%20for%20summarization.%20We%20design%20a%20topology-guided%20iterative%20strategy%2C%20where%20lightweight%20proxy%20metrics%20approximate%20sentence%20importance%20to%20avoid%20repeated%20high-cost%20computations%2C%20thus%20preserving%20structural%20integrity%20while%20improving%20efficiency.%20To%20further%20enhance%20long-text%20processing%2C%20we%20propose%20a%20hierarchical%20strategy%20that%20integrates%20segment-level%20and%20global%20summarization.%20Experiments%20on%20multiple%20datasets%20demonstrate%20that%20GloSA-sum%20reduces%20redundancy%20while%20preserving%20semantic%20and%20logical%20integrity%2C%20striking%20a%20balance%20between%20accuracy%20and%20efficiency%2C%20and%20further%20benefits%20LLM%20downstream%20tasks%20by%20shortening%20contexts%20while%20retaining%20essential%20reasoning%20chains.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText%2520summarization%2520via%2520global%2520structure%2520awareness%26entry.906535625%3DJiaquan%2520Zhang%2520and%2520Chaoning%2520Zhang%2520and%2520Shuxu%2520Chen%2520and%2520Yibei%2520Liu%2520and%2520Chenghao%2520Li%2520and%2520Qigan%2520Sun%2520and%2520Shuai%2520Yuan%2520and%2520Fachrina%2520Dewi%2520Puspitasari%2520and%2520Dongshen%2520Han%2520and%2520Guoqing%2520Wang%2520and%2520Sung-Ho%2520Bae%2520and%2520Yang%2520Yang%26entry.1292438233%3DText%2520summarization%2520is%2520a%2520fundamental%2520task%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%252C%2520and%2520the%2520information%2520explosion%2520has%2520made%2520long-document%2520processing%2520increasingly%2520demanding%252C%2520making%2520summarization%2520essential.%2520Existing%2520research%2520mainly%2520focuses%2520on%2520model%2520improvements%2520and%2520sentence-level%2520pruning%252C%2520but%2520often%2520overlooks%2520global%2520structure%252C%2520leading%2520to%2520disrupted%2520coherence%2520and%2520weakened%2520downstream%2520performance.%2520Some%2520studies%2520employ%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520achieve%2520higher%2520accuracy%2520but%2520incur%2520substantial%2520resource%2520and%2520time%2520costs.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520GloSA-sum%252C%2520the%2520first%2520summarization%2520approach%2520that%2520achieves%2520global%2520structure%2520awareness%2520via%2520topological%2520data%2520analysis%2520%2528TDA%2529.%2520GloSA-sum%2520summarizes%2520text%2520efficiently%2520while%2520preserving%2520semantic%2520cores%2520and%2520logical%2520dependencies.%2520Specifically%252C%2520we%2520construct%2520a%2520semantic-weighted%2520graph%2520from%2520sentence%2520embeddings%252C%2520where%2520persistent%2520homology%2520identifies%2520core%2520semantics%2520and%2520logical%2520structures%252C%2520preserved%2520in%2520a%2520%2560%2560protection%2520pool%2527%2527%2520as%2520the%2520backbone%2520for%2520summarization.%2520We%2520design%2520a%2520topology-guided%2520iterative%2520strategy%252C%2520where%2520lightweight%2520proxy%2520metrics%2520approximate%2520sentence%2520importance%2520to%2520avoid%2520repeated%2520high-cost%2520computations%252C%2520thus%2520preserving%2520structural%2520integrity%2520while%2520improving%2520efficiency.%2520To%2520further%2520enhance%2520long-text%2520processing%252C%2520we%2520propose%2520a%2520hierarchical%2520strategy%2520that%2520integrates%2520segment-level%2520and%2520global%2520summarization.%2520Experiments%2520on%2520multiple%2520datasets%2520demonstrate%2520that%2520GloSA-sum%2520reduces%2520redundancy%2520while%2520preserving%2520semantic%2520and%2520logical%2520integrity%252C%2520striking%2520a%2520balance%2520between%2520accuracy%2520and%2520efficiency%252C%2520and%2520further%2520benefits%2520LLM%2520downstream%2520tasks%2520by%2520shortening%2520contexts%2520while%2520retaining%2520essential%2520reasoning%2520chains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20summarization%20via%20global%20structure%20awareness&entry.906535625=Jiaquan%20Zhang%20and%20Chaoning%20Zhang%20and%20Shuxu%20Chen%20and%20Yibei%20Liu%20and%20Chenghao%20Li%20and%20Qigan%20Sun%20and%20Shuai%20Yuan%20and%20Fachrina%20Dewi%20Puspitasari%20and%20Dongshen%20Han%20and%20Guoqing%20Wang%20and%20Sung-Ho%20Bae%20and%20Yang%20Yang&entry.1292438233=Text%20summarization%20is%20a%20fundamental%20task%20in%20natural%20language%20processing%20%28NLP%29%2C%20and%20the%20information%20explosion%20has%20made%20long-document%20processing%20increasingly%20demanding%2C%20making%20summarization%20essential.%20Existing%20research%20mainly%20focuses%20on%20model%20improvements%20and%20sentence-level%20pruning%2C%20but%20often%20overlooks%20global%20structure%2C%20leading%20to%20disrupted%20coherence%20and%20weakened%20downstream%20performance.%20Some%20studies%20employ%20large%20language%20models%20%28LLMs%29%2C%20which%20achieve%20higher%20accuracy%20but%20incur%20substantial%20resource%20and%20time%20costs.%20To%20address%20these%20issues%2C%20we%20introduce%20GloSA-sum%2C%20the%20first%20summarization%20approach%20that%20achieves%20global%20structure%20awareness%20via%20topological%20data%20analysis%20%28TDA%29.%20GloSA-sum%20summarizes%20text%20efficiently%20while%20preserving%20semantic%20cores%20and%20logical%20dependencies.%20Specifically%2C%20we%20construct%20a%20semantic-weighted%20graph%20from%20sentence%20embeddings%2C%20where%20persistent%20homology%20identifies%20core%20semantics%20and%20logical%20structures%2C%20preserved%20in%20a%20%60%60protection%20pool%27%27%20as%20the%20backbone%20for%20summarization.%20We%20design%20a%20topology-guided%20iterative%20strategy%2C%20where%20lightweight%20proxy%20metrics%20approximate%20sentence%20importance%20to%20avoid%20repeated%20high-cost%20computations%2C%20thus%20preserving%20structural%20integrity%20while%20improving%20efficiency.%20To%20further%20enhance%20long-text%20processing%2C%20we%20propose%20a%20hierarchical%20strategy%20that%20integrates%20segment-level%20and%20global%20summarization.%20Experiments%20on%20multiple%20datasets%20demonstrate%20that%20GloSA-sum%20reduces%20redundancy%20while%20preserving%20semantic%20and%20logical%20integrity%2C%20striking%20a%20balance%20between%20accuracy%20and%20efficiency%2C%20and%20further%20benefits%20LLM%20downstream%20tasks%20by%20shortening%20contexts%20while%20retaining%20essential%20reasoning%20chains.&entry.1838667208=http%3A//arxiv.org/abs/2602.09821v1&entry.124074799=Read"},
{"title": "BRAVA-GNN: Betweenness Ranking Approximation Via Degree MAss Inspired Graph Neural Network", "author": "Justin Dachille and Aurora Rossi and Sunil Kumar Maurya and Frederik Mallmann-Trenn and Xin Liu and Fr\u00e9d\u00e9ric Giroire and Tsuyoshi Murata and Emanuele Natale", "abstract": "Computing node importance in networks is a long-standing fundamental problem that has driven extensive study of various centrality measures. A particularly well-known centrality measure is betweenness centrality, which becomes computationally prohibitive on large-scale networks. Graph Neural Network (GNN) models have thus been proposed to predict node rankings according to their relative betweenness centrality. However, state-of-the-art methods fail to generalize to high-diameter graphs such as road networks. We propose BRAVA-GNN, a lightweight GNN architecture that leverages the empirically observed correlation linking betweenness centrality to degree-based quantities, in particular multi-hop degree mass. This correlation motivates the use of degree masses as size-invariant node features and synthetic training graphs that closely match the degree distributions of real networks. Furthermore, while previous work relies on scale-free synthetic graphs, we leverage the hyperbolic random graph model, which reproduces power-law exponents outside the scale-free regime, better capturing the structure of real-world graphs like road networks. This design enables BRAVA-GNN to generalize across diverse graph families while using 54x fewer parameters than the most lightweight existing GNN baseline. Extensive experiments on 19 real-world networks, spanning social, web, email, and road graphs, show that BRAVA-GNN achieves up to 214% improvement in Kendall-Tau correlation and up to 70x speedup in inference time over state-of-the-art GNN-based approaches, particularly on challenging road networks.", "link": "http://arxiv.org/abs/2602.09716v1", "date": "2026-02-10", "relevancy": 2.2969, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4424}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRAVA-GNN%3A%20Betweenness%20Ranking%20Approximation%20Via%20Degree%20MAss%20Inspired%20Graph%20Neural%20Network&body=Title%3A%20BRAVA-GNN%3A%20Betweenness%20Ranking%20Approximation%20Via%20Degree%20MAss%20Inspired%20Graph%20Neural%20Network%0AAuthor%3A%20Justin%20Dachille%20and%20Aurora%20Rossi%20and%20Sunil%20Kumar%20Maurya%20and%20Frederik%20Mallmann-Trenn%20and%20Xin%20Liu%20and%20Fr%C3%A9d%C3%A9ric%20Giroire%20and%20Tsuyoshi%20Murata%20and%20Emanuele%20Natale%0AAbstract%3A%20Computing%20node%20importance%20in%20networks%20is%20a%20long-standing%20fundamental%20problem%20that%20has%20driven%20extensive%20study%20of%20various%20centrality%20measures.%20A%20particularly%20well-known%20centrality%20measure%20is%20betweenness%20centrality%2C%20which%20becomes%20computationally%20prohibitive%20on%20large-scale%20networks.%20Graph%20Neural%20Network%20%28GNN%29%20models%20have%20thus%20been%20proposed%20to%20predict%20node%20rankings%20according%20to%20their%20relative%20betweenness%20centrality.%20However%2C%20state-of-the-art%20methods%20fail%20to%20generalize%20to%20high-diameter%20graphs%20such%20as%20road%20networks.%20We%20propose%20BRAVA-GNN%2C%20a%20lightweight%20GNN%20architecture%20that%20leverages%20the%20empirically%20observed%20correlation%20linking%20betweenness%20centrality%20to%20degree-based%20quantities%2C%20in%20particular%20multi-hop%20degree%20mass.%20This%20correlation%20motivates%20the%20use%20of%20degree%20masses%20as%20size-invariant%20node%20features%20and%20synthetic%20training%20graphs%20that%20closely%20match%20the%20degree%20distributions%20of%20real%20networks.%20Furthermore%2C%20while%20previous%20work%20relies%20on%20scale-free%20synthetic%20graphs%2C%20we%20leverage%20the%20hyperbolic%20random%20graph%20model%2C%20which%20reproduces%20power-law%20exponents%20outside%20the%20scale-free%20regime%2C%20better%20capturing%20the%20structure%20of%20real-world%20graphs%20like%20road%20networks.%20This%20design%20enables%20BRAVA-GNN%20to%20generalize%20across%20diverse%20graph%20families%20while%20using%2054x%20fewer%20parameters%20than%20the%20most%20lightweight%20existing%20GNN%20baseline.%20Extensive%20experiments%20on%2019%20real-world%20networks%2C%20spanning%20social%2C%20web%2C%20email%2C%20and%20road%20graphs%2C%20show%20that%20BRAVA-GNN%20achieves%20up%20to%20214%25%20improvement%20in%20Kendall-Tau%20correlation%20and%20up%20to%2070x%20speedup%20in%20inference%20time%20over%20state-of-the-art%20GNN-based%20approaches%2C%20particularly%20on%20challenging%20road%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRAVA-GNN%253A%2520Betweenness%2520Ranking%2520Approximation%2520Via%2520Degree%2520MAss%2520Inspired%2520Graph%2520Neural%2520Network%26entry.906535625%3DJustin%2520Dachille%2520and%2520Aurora%2520Rossi%2520and%2520Sunil%2520Kumar%2520Maurya%2520and%2520Frederik%2520Mallmann-Trenn%2520and%2520Xin%2520Liu%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Giroire%2520and%2520Tsuyoshi%2520Murata%2520and%2520Emanuele%2520Natale%26entry.1292438233%3DComputing%2520node%2520importance%2520in%2520networks%2520is%2520a%2520long-standing%2520fundamental%2520problem%2520that%2520has%2520driven%2520extensive%2520study%2520of%2520various%2520centrality%2520measures.%2520A%2520particularly%2520well-known%2520centrality%2520measure%2520is%2520betweenness%2520centrality%252C%2520which%2520becomes%2520computationally%2520prohibitive%2520on%2520large-scale%2520networks.%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520models%2520have%2520thus%2520been%2520proposed%2520to%2520predict%2520node%2520rankings%2520according%2520to%2520their%2520relative%2520betweenness%2520centrality.%2520However%252C%2520state-of-the-art%2520methods%2520fail%2520to%2520generalize%2520to%2520high-diameter%2520graphs%2520such%2520as%2520road%2520networks.%2520We%2520propose%2520BRAVA-GNN%252C%2520a%2520lightweight%2520GNN%2520architecture%2520that%2520leverages%2520the%2520empirically%2520observed%2520correlation%2520linking%2520betweenness%2520centrality%2520to%2520degree-based%2520quantities%252C%2520in%2520particular%2520multi-hop%2520degree%2520mass.%2520This%2520correlation%2520motivates%2520the%2520use%2520of%2520degree%2520masses%2520as%2520size-invariant%2520node%2520features%2520and%2520synthetic%2520training%2520graphs%2520that%2520closely%2520match%2520the%2520degree%2520distributions%2520of%2520real%2520networks.%2520Furthermore%252C%2520while%2520previous%2520work%2520relies%2520on%2520scale-free%2520synthetic%2520graphs%252C%2520we%2520leverage%2520the%2520hyperbolic%2520random%2520graph%2520model%252C%2520which%2520reproduces%2520power-law%2520exponents%2520outside%2520the%2520scale-free%2520regime%252C%2520better%2520capturing%2520the%2520structure%2520of%2520real-world%2520graphs%2520like%2520road%2520networks.%2520This%2520design%2520enables%2520BRAVA-GNN%2520to%2520generalize%2520across%2520diverse%2520graph%2520families%2520while%2520using%252054x%2520fewer%2520parameters%2520than%2520the%2520most%2520lightweight%2520existing%2520GNN%2520baseline.%2520Extensive%2520experiments%2520on%252019%2520real-world%2520networks%252C%2520spanning%2520social%252C%2520web%252C%2520email%252C%2520and%2520road%2520graphs%252C%2520show%2520that%2520BRAVA-GNN%2520achieves%2520up%2520to%2520214%2525%2520improvement%2520in%2520Kendall-Tau%2520correlation%2520and%2520up%2520to%252070x%2520speedup%2520in%2520inference%2520time%2520over%2520state-of-the-art%2520GNN-based%2520approaches%252C%2520particularly%2520on%2520challenging%2520road%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRAVA-GNN%3A%20Betweenness%20Ranking%20Approximation%20Via%20Degree%20MAss%20Inspired%20Graph%20Neural%20Network&entry.906535625=Justin%20Dachille%20and%20Aurora%20Rossi%20and%20Sunil%20Kumar%20Maurya%20and%20Frederik%20Mallmann-Trenn%20and%20Xin%20Liu%20and%20Fr%C3%A9d%C3%A9ric%20Giroire%20and%20Tsuyoshi%20Murata%20and%20Emanuele%20Natale&entry.1292438233=Computing%20node%20importance%20in%20networks%20is%20a%20long-standing%20fundamental%20problem%20that%20has%20driven%20extensive%20study%20of%20various%20centrality%20measures.%20A%20particularly%20well-known%20centrality%20measure%20is%20betweenness%20centrality%2C%20which%20becomes%20computationally%20prohibitive%20on%20large-scale%20networks.%20Graph%20Neural%20Network%20%28GNN%29%20models%20have%20thus%20been%20proposed%20to%20predict%20node%20rankings%20according%20to%20their%20relative%20betweenness%20centrality.%20However%2C%20state-of-the-art%20methods%20fail%20to%20generalize%20to%20high-diameter%20graphs%20such%20as%20road%20networks.%20We%20propose%20BRAVA-GNN%2C%20a%20lightweight%20GNN%20architecture%20that%20leverages%20the%20empirically%20observed%20correlation%20linking%20betweenness%20centrality%20to%20degree-based%20quantities%2C%20in%20particular%20multi-hop%20degree%20mass.%20This%20correlation%20motivates%20the%20use%20of%20degree%20masses%20as%20size-invariant%20node%20features%20and%20synthetic%20training%20graphs%20that%20closely%20match%20the%20degree%20distributions%20of%20real%20networks.%20Furthermore%2C%20while%20previous%20work%20relies%20on%20scale-free%20synthetic%20graphs%2C%20we%20leverage%20the%20hyperbolic%20random%20graph%20model%2C%20which%20reproduces%20power-law%20exponents%20outside%20the%20scale-free%20regime%2C%20better%20capturing%20the%20structure%20of%20real-world%20graphs%20like%20road%20networks.%20This%20design%20enables%20BRAVA-GNN%20to%20generalize%20across%20diverse%20graph%20families%20while%20using%2054x%20fewer%20parameters%20than%20the%20most%20lightweight%20existing%20GNN%20baseline.%20Extensive%20experiments%20on%2019%20real-world%20networks%2C%20spanning%20social%2C%20web%2C%20email%2C%20and%20road%20graphs%2C%20show%20that%20BRAVA-GNN%20achieves%20up%20to%20214%25%20improvement%20in%20Kendall-Tau%20correlation%20and%20up%20to%2070x%20speedup%20in%20inference%20time%20over%20state-of-the-art%20GNN-based%20approaches%2C%20particularly%20on%20challenging%20road%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2602.09716v1&entry.124074799=Read"},
{"title": "Code2World: A GUI World Model via Renderable Code Generation", "author": "Yuhao Zheng and Li'an Zhong and Yi Wang and Rui Dai and Kaikui Liu and Xiangxiang Chu and Linyuan Lv and Philip Torr and Kevin Qinghong Lin", "abstract": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "link": "http://arxiv.org/abs/2602.09856v1", "date": "2026-02-10", "relevancy": 2.2901, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6002}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5918}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code2World%3A%20A%20GUI%20World%20Model%20via%20Renderable%20Code%20Generation&body=Title%3A%20Code2World%3A%20A%20GUI%20World%20Model%20via%20Renderable%20Code%20Generation%0AAuthor%3A%20Yuhao%20Zheng%20and%20Li%27an%20Zhong%20and%20Yi%20Wang%20and%20Rui%20Dai%20and%20Kaikui%20Liu%20and%20Xiangxiang%20Chu%20and%20Linyuan%20Lv%20and%20Philip%20Torr%20and%20Kevin%20Qinghong%20Lin%0AAbstract%3A%20Autonomous%20GUI%20agents%20interact%20with%20environments%20by%20perceiving%20interfaces%20and%20executing%20actions.%20As%20a%20virtual%20sandbox%2C%20the%20GUI%20World%20model%20empowers%20agents%20with%20human-like%20foresight%20by%20enabling%20action-conditioned%20prediction.%20However%2C%20existing%20text-%20and%20pixel-based%20approaches%20struggle%20to%20simultaneously%20achieve%20high%20visual%20fidelity%20and%20fine-grained%20structural%20controllability.%20To%20this%20end%2C%20we%20propose%20Code2World%2C%20a%20vision-language%20coder%20that%20simulates%20the%20next%20visual%20state%20via%20renderable%20code%20generation.%20Specifically%2C%20to%20address%20the%20data%20scarcity%20problem%2C%20we%20construct%20AndroidCode%20by%20translating%20GUI%20trajectories%20into%20high-fidelity%20HTML%20and%20refining%20synthesized%20code%20through%20a%20visual-feedback%20revision%20mechanism%2C%20yielding%20a%20corpus%20of%20over%2080K%20high-quality%20screen-action%20pairs.%20To%20adapt%20existing%20VLMs%20into%20code%20prediction%2C%20we%20first%20perform%20SFT%20as%20a%20cold%20start%20for%20format%20layout%20following%2C%20then%20further%20apply%20Render-Aware%20Reinforcement%20Learning%20which%20uses%20rendered%20outcome%20as%20the%20reward%20signal%20by%20enforcing%20visual%20semantic%20fidelity%20and%20action%20consistency.%20Extensive%20experiments%20demonstrate%20that%20Code2World-8B%20achieves%20the%20top-performing%20next%20UI%20prediction%2C%20rivaling%20the%20competitive%20GPT-5%20and%20Gemini-3-Pro-Image.%20Notably%2C%20Code2World%20significantly%20enhances%20downstream%20navigation%20success%20rates%20in%20a%20flexible%20manner%2C%20boosting%20Gemini-2.5-Flash%20by%20%2B9.5%25%20on%20AndroidWorld%20navigation.%20The%20code%20is%20available%20at%20https%3A//github.com/AMAP-ML/Code2World.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode2World%253A%2520A%2520GUI%2520World%2520Model%2520via%2520Renderable%2520Code%2520Generation%26entry.906535625%3DYuhao%2520Zheng%2520and%2520Li%2527an%2520Zhong%2520and%2520Yi%2520Wang%2520and%2520Rui%2520Dai%2520and%2520Kaikui%2520Liu%2520and%2520Xiangxiang%2520Chu%2520and%2520Linyuan%2520Lv%2520and%2520Philip%2520Torr%2520and%2520Kevin%2520Qinghong%2520Lin%26entry.1292438233%3DAutonomous%2520GUI%2520agents%2520interact%2520with%2520environments%2520by%2520perceiving%2520interfaces%2520and%2520executing%2520actions.%2520As%2520a%2520virtual%2520sandbox%252C%2520the%2520GUI%2520World%2520model%2520empowers%2520agents%2520with%2520human-like%2520foresight%2520by%2520enabling%2520action-conditioned%2520prediction.%2520However%252C%2520existing%2520text-%2520and%2520pixel-based%2520approaches%2520struggle%2520to%2520simultaneously%2520achieve%2520high%2520visual%2520fidelity%2520and%2520fine-grained%2520structural%2520controllability.%2520To%2520this%2520end%252C%2520we%2520propose%2520Code2World%252C%2520a%2520vision-language%2520coder%2520that%2520simulates%2520the%2520next%2520visual%2520state%2520via%2520renderable%2520code%2520generation.%2520Specifically%252C%2520to%2520address%2520the%2520data%2520scarcity%2520problem%252C%2520we%2520construct%2520AndroidCode%2520by%2520translating%2520GUI%2520trajectories%2520into%2520high-fidelity%2520HTML%2520and%2520refining%2520synthesized%2520code%2520through%2520a%2520visual-feedback%2520revision%2520mechanism%252C%2520yielding%2520a%2520corpus%2520of%2520over%252080K%2520high-quality%2520screen-action%2520pairs.%2520To%2520adapt%2520existing%2520VLMs%2520into%2520code%2520prediction%252C%2520we%2520first%2520perform%2520SFT%2520as%2520a%2520cold%2520start%2520for%2520format%2520layout%2520following%252C%2520then%2520further%2520apply%2520Render-Aware%2520Reinforcement%2520Learning%2520which%2520uses%2520rendered%2520outcome%2520as%2520the%2520reward%2520signal%2520by%2520enforcing%2520visual%2520semantic%2520fidelity%2520and%2520action%2520consistency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Code2World-8B%2520achieves%2520the%2520top-performing%2520next%2520UI%2520prediction%252C%2520rivaling%2520the%2520competitive%2520GPT-5%2520and%2520Gemini-3-Pro-Image.%2520Notably%252C%2520Code2World%2520significantly%2520enhances%2520downstream%2520navigation%2520success%2520rates%2520in%2520a%2520flexible%2520manner%252C%2520boosting%2520Gemini-2.5-Flash%2520by%2520%252B9.5%2525%2520on%2520AndroidWorld%2520navigation.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/AMAP-ML/Code2World.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code2World%3A%20A%20GUI%20World%20Model%20via%20Renderable%20Code%20Generation&entry.906535625=Yuhao%20Zheng%20and%20Li%27an%20Zhong%20and%20Yi%20Wang%20and%20Rui%20Dai%20and%20Kaikui%20Liu%20and%20Xiangxiang%20Chu%20and%20Linyuan%20Lv%20and%20Philip%20Torr%20and%20Kevin%20Qinghong%20Lin&entry.1292438233=Autonomous%20GUI%20agents%20interact%20with%20environments%20by%20perceiving%20interfaces%20and%20executing%20actions.%20As%20a%20virtual%20sandbox%2C%20the%20GUI%20World%20model%20empowers%20agents%20with%20human-like%20foresight%20by%20enabling%20action-conditioned%20prediction.%20However%2C%20existing%20text-%20and%20pixel-based%20approaches%20struggle%20to%20simultaneously%20achieve%20high%20visual%20fidelity%20and%20fine-grained%20structural%20controllability.%20To%20this%20end%2C%20we%20propose%20Code2World%2C%20a%20vision-language%20coder%20that%20simulates%20the%20next%20visual%20state%20via%20renderable%20code%20generation.%20Specifically%2C%20to%20address%20the%20data%20scarcity%20problem%2C%20we%20construct%20AndroidCode%20by%20translating%20GUI%20trajectories%20into%20high-fidelity%20HTML%20and%20refining%20synthesized%20code%20through%20a%20visual-feedback%20revision%20mechanism%2C%20yielding%20a%20corpus%20of%20over%2080K%20high-quality%20screen-action%20pairs.%20To%20adapt%20existing%20VLMs%20into%20code%20prediction%2C%20we%20first%20perform%20SFT%20as%20a%20cold%20start%20for%20format%20layout%20following%2C%20then%20further%20apply%20Render-Aware%20Reinforcement%20Learning%20which%20uses%20rendered%20outcome%20as%20the%20reward%20signal%20by%20enforcing%20visual%20semantic%20fidelity%20and%20action%20consistency.%20Extensive%20experiments%20demonstrate%20that%20Code2World-8B%20achieves%20the%20top-performing%20next%20UI%20prediction%2C%20rivaling%20the%20competitive%20GPT-5%20and%20Gemini-3-Pro-Image.%20Notably%2C%20Code2World%20significantly%20enhances%20downstream%20navigation%20success%20rates%20in%20a%20flexible%20manner%2C%20boosting%20Gemini-2.5-Flash%20by%20%2B9.5%25%20on%20AndroidWorld%20navigation.%20The%20code%20is%20available%20at%20https%3A//github.com/AMAP-ML/Code2World.&entry.1838667208=http%3A//arxiv.org/abs/2602.09856v1&entry.124074799=Read"},
{"title": "A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula", "author": "Chenruo Liu and Yijun Dong and Yiqiu Shen and Qi Lei", "abstract": "Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.", "link": "http://arxiv.org/abs/2602.10014v1", "date": "2026-02-10", "relevancy": 2.2813, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4656}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4529}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Task-Centric%20Theory%20for%20Iterative%20Self-Improvement%20with%20Easy-to-Hard%20Curricula&body=Title%3A%20A%20Task-Centric%20Theory%20for%20Iterative%20Self-Improvement%20with%20Easy-to-Hard%20Curricula%0AAuthor%3A%20Chenruo%20Liu%20and%20Yijun%20Dong%20and%20Yiqiu%20Shen%20and%20Qi%20Lei%0AAbstract%3A%20Iterative%20self-improvement%20fine-tunes%20an%20autoregressive%20large%20language%20model%20%28LLM%29%20on%20reward-verified%20outputs%20generated%20by%20the%20LLM%20itself.%20In%20contrast%20to%20the%20empirical%20success%20of%20self-improvement%2C%20the%20theoretical%20foundation%20of%20this%20generative%2C%20iterative%20procedure%20in%20a%20practical%2C%20finite-sample%20setting%20remains%20limited.%20We%20make%20progress%20toward%20this%20goal%20by%20modeling%20each%20round%20of%20self-improvement%20as%20maximum-likelihood%20fine-tuning%20on%20a%20reward-filtered%20distribution%20and%20deriving%20finite-sample%20guarantees%20for%20the%20expected%20reward.%20Our%20analysis%20reveals%20an%20explicit%20feedback%20loop%20where%20better%20models%20accept%20more%20data%20per%20iteration%2C%20supporting%20sustained%20self-improvement%20while%20explaining%20eventual%20saturation%20of%20such%20improvement.%20Adopting%20a%20task-centric%20view%20by%20considering%20reasoning%20tasks%20with%20multiple%20difficulty%20levels%2C%20we%20further%20prove%20quantifiable%20conditions%20on%20model%20initialization%2C%20task%20difficulty%2C%20and%20sample%20budget%20where%20easy-to-hard%20curricula%20provably%20achieve%20better%20guarantees%20than%20training%20on%20fixed%20mixtures%20of%20tasks.%20Our%20analyses%20are%20validated%20via%20Monte-Carlo%20simulations%20and%20controlled%20experiments%20on%20graph-based%20reasoning%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Task-Centric%2520Theory%2520for%2520Iterative%2520Self-Improvement%2520with%2520Easy-to-Hard%2520Curricula%26entry.906535625%3DChenruo%2520Liu%2520and%2520Yijun%2520Dong%2520and%2520Yiqiu%2520Shen%2520and%2520Qi%2520Lei%26entry.1292438233%3DIterative%2520self-improvement%2520fine-tunes%2520an%2520autoregressive%2520large%2520language%2520model%2520%2528LLM%2529%2520on%2520reward-verified%2520outputs%2520generated%2520by%2520the%2520LLM%2520itself.%2520In%2520contrast%2520to%2520the%2520empirical%2520success%2520of%2520self-improvement%252C%2520the%2520theoretical%2520foundation%2520of%2520this%2520generative%252C%2520iterative%2520procedure%2520in%2520a%2520practical%252C%2520finite-sample%2520setting%2520remains%2520limited.%2520We%2520make%2520progress%2520toward%2520this%2520goal%2520by%2520modeling%2520each%2520round%2520of%2520self-improvement%2520as%2520maximum-likelihood%2520fine-tuning%2520on%2520a%2520reward-filtered%2520distribution%2520and%2520deriving%2520finite-sample%2520guarantees%2520for%2520the%2520expected%2520reward.%2520Our%2520analysis%2520reveals%2520an%2520explicit%2520feedback%2520loop%2520where%2520better%2520models%2520accept%2520more%2520data%2520per%2520iteration%252C%2520supporting%2520sustained%2520self-improvement%2520while%2520explaining%2520eventual%2520saturation%2520of%2520such%2520improvement.%2520Adopting%2520a%2520task-centric%2520view%2520by%2520considering%2520reasoning%2520tasks%2520with%2520multiple%2520difficulty%2520levels%252C%2520we%2520further%2520prove%2520quantifiable%2520conditions%2520on%2520model%2520initialization%252C%2520task%2520difficulty%252C%2520and%2520sample%2520budget%2520where%2520easy-to-hard%2520curricula%2520provably%2520achieve%2520better%2520guarantees%2520than%2520training%2520on%2520fixed%2520mixtures%2520of%2520tasks.%2520Our%2520analyses%2520are%2520validated%2520via%2520Monte-Carlo%2520simulations%2520and%2520controlled%2520experiments%2520on%2520graph-based%2520reasoning%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Task-Centric%20Theory%20for%20Iterative%20Self-Improvement%20with%20Easy-to-Hard%20Curricula&entry.906535625=Chenruo%20Liu%20and%20Yijun%20Dong%20and%20Yiqiu%20Shen%20and%20Qi%20Lei&entry.1292438233=Iterative%20self-improvement%20fine-tunes%20an%20autoregressive%20large%20language%20model%20%28LLM%29%20on%20reward-verified%20outputs%20generated%20by%20the%20LLM%20itself.%20In%20contrast%20to%20the%20empirical%20success%20of%20self-improvement%2C%20the%20theoretical%20foundation%20of%20this%20generative%2C%20iterative%20procedure%20in%20a%20practical%2C%20finite-sample%20setting%20remains%20limited.%20We%20make%20progress%20toward%20this%20goal%20by%20modeling%20each%20round%20of%20self-improvement%20as%20maximum-likelihood%20fine-tuning%20on%20a%20reward-filtered%20distribution%20and%20deriving%20finite-sample%20guarantees%20for%20the%20expected%20reward.%20Our%20analysis%20reveals%20an%20explicit%20feedback%20loop%20where%20better%20models%20accept%20more%20data%20per%20iteration%2C%20supporting%20sustained%20self-improvement%20while%20explaining%20eventual%20saturation%20of%20such%20improvement.%20Adopting%20a%20task-centric%20view%20by%20considering%20reasoning%20tasks%20with%20multiple%20difficulty%20levels%2C%20we%20further%20prove%20quantifiable%20conditions%20on%20model%20initialization%2C%20task%20difficulty%2C%20and%20sample%20budget%20where%20easy-to-hard%20curricula%20provably%20achieve%20better%20guarantees%20than%20training%20on%20fixed%20mixtures%20of%20tasks.%20Our%20analyses%20are%20validated%20via%20Monte-Carlo%20simulations%20and%20controlled%20experiments%20on%20graph-based%20reasoning%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.10014v1&entry.124074799=Read"},
{"title": "TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior", "author": "Jie Li and Bing Tang and Feng Wu and Rongyun Cao", "abstract": "Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.", "link": "http://arxiv.org/abs/2602.09628v1", "date": "2026-02-10", "relevancy": 2.2725, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5704}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeleGate%3A%20Whole-Body%20Humanoid%20Teleoperation%20via%20Gated%20Expert%20Selection%20with%20Motion%20Prior&body=Title%3A%20TeleGate%3A%20Whole-Body%20Humanoid%20Teleoperation%20via%20Gated%20Expert%20Selection%20with%20Motion%20Prior%0AAuthor%3A%20Jie%20Li%20and%20Bing%20Tang%20and%20Feng%20Wu%20and%20Rongyun%20Cao%0AAbstract%3A%20Real-time%20whole-body%20teleoperation%20is%20a%20critical%20method%20for%20humanoid%20robots%20to%20perform%20complex%20tasks%20in%20unstructured%20environments.%20However%2C%20developing%20a%20unified%20controller%20that%20robustly%20supports%20diverse%20human%20motions%20remains%20a%20significant%20challenge.%20Existing%20methods%20typically%20distill%20multiple%20expert%20policies%20into%20a%20single%20general%20policy%2C%20which%20often%20inevitably%20leads%20to%20performance%20degradation%2C%20particularly%20on%20highly%20dynamic%20motions.%20This%20paper%20presents%20TeleGate%2C%20a%20unified%20whole-body%20teleoperation%20framework%20for%20humanoid%20robots%20that%20achieves%20high-precision%20tracking%20across%20various%20motions%20while%20avoiding%20the%20performance%20loss%20inherent%20in%20knowledge%20distillation.%20Our%20key%20idea%20is%20to%20preserve%20the%20full%20capability%20of%20domain-specific%20expert%20policies%20by%20training%20a%20lightweight%20gating%20network%2C%20which%20dynamically%20activates%20experts%20in%20real-time%20based%20on%20proprioceptive%20states%20and%20reference%20trajectories.%20Furthermore%2C%20to%20compensate%20for%20the%20absence%20of%20future%20reference%20trajectories%20in%20real-time%20teleoperation%2C%20we%20introduce%20a%20VAE-based%20motion%20prior%20module%20that%20extracts%20implicit%20future%20motion%20intent%20from%20historical%20observations%2C%20enabling%20anticipatory%20control%20for%20motions%20requiring%20prediction%20such%20as%20jumping%20and%20standing%20up.%20We%20conducted%20empirical%20evaluations%20in%20simulation%20and%20also%20deployed%20our%20technique%20on%20the%20Unitree%20G1%20humanoid%20robot.%20Using%20only%202.5%20hours%20of%20motion%20capture%20data%20for%20training%2C%20our%20TeleGate%20achieves%20high-precision%20real-time%20teleoperation%20across%20diverse%20dynamic%20motions%20%28e.g.%2C%20running%2C%20fall%20recovery%2C%20and%20jumping%29%2C%20significantly%20outperforming%20the%20baseline%20methods%20in%20both%20tracking%20accuracy%20and%20success%20rate.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeleGate%253A%2520Whole-Body%2520Humanoid%2520Teleoperation%2520via%2520Gated%2520Expert%2520Selection%2520with%2520Motion%2520Prior%26entry.906535625%3DJie%2520Li%2520and%2520Bing%2520Tang%2520and%2520Feng%2520Wu%2520and%2520Rongyun%2520Cao%26entry.1292438233%3DReal-time%2520whole-body%2520teleoperation%2520is%2520a%2520critical%2520method%2520for%2520humanoid%2520robots%2520to%2520perform%2520complex%2520tasks%2520in%2520unstructured%2520environments.%2520However%252C%2520developing%2520a%2520unified%2520controller%2520that%2520robustly%2520supports%2520diverse%2520human%2520motions%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520methods%2520typically%2520distill%2520multiple%2520expert%2520policies%2520into%2520a%2520single%2520general%2520policy%252C%2520which%2520often%2520inevitably%2520leads%2520to%2520performance%2520degradation%252C%2520particularly%2520on%2520highly%2520dynamic%2520motions.%2520This%2520paper%2520presents%2520TeleGate%252C%2520a%2520unified%2520whole-body%2520teleoperation%2520framework%2520for%2520humanoid%2520robots%2520that%2520achieves%2520high-precision%2520tracking%2520across%2520various%2520motions%2520while%2520avoiding%2520the%2520performance%2520loss%2520inherent%2520in%2520knowledge%2520distillation.%2520Our%2520key%2520idea%2520is%2520to%2520preserve%2520the%2520full%2520capability%2520of%2520domain-specific%2520expert%2520policies%2520by%2520training%2520a%2520lightweight%2520gating%2520network%252C%2520which%2520dynamically%2520activates%2520experts%2520in%2520real-time%2520based%2520on%2520proprioceptive%2520states%2520and%2520reference%2520trajectories.%2520Furthermore%252C%2520to%2520compensate%2520for%2520the%2520absence%2520of%2520future%2520reference%2520trajectories%2520in%2520real-time%2520teleoperation%252C%2520we%2520introduce%2520a%2520VAE-based%2520motion%2520prior%2520module%2520that%2520extracts%2520implicit%2520future%2520motion%2520intent%2520from%2520historical%2520observations%252C%2520enabling%2520anticipatory%2520control%2520for%2520motions%2520requiring%2520prediction%2520such%2520as%2520jumping%2520and%2520standing%2520up.%2520We%2520conducted%2520empirical%2520evaluations%2520in%2520simulation%2520and%2520also%2520deployed%2520our%2520technique%2520on%2520the%2520Unitree%2520G1%2520humanoid%2520robot.%2520Using%2520only%25202.5%2520hours%2520of%2520motion%2520capture%2520data%2520for%2520training%252C%2520our%2520TeleGate%2520achieves%2520high-precision%2520real-time%2520teleoperation%2520across%2520diverse%2520dynamic%2520motions%2520%2528e.g.%252C%2520running%252C%2520fall%2520recovery%252C%2520and%2520jumping%2529%252C%2520significantly%2520outperforming%2520the%2520baseline%2520methods%2520in%2520both%2520tracking%2520accuracy%2520and%2520success%2520rate.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeleGate%3A%20Whole-Body%20Humanoid%20Teleoperation%20via%20Gated%20Expert%20Selection%20with%20Motion%20Prior&entry.906535625=Jie%20Li%20and%20Bing%20Tang%20and%20Feng%20Wu%20and%20Rongyun%20Cao&entry.1292438233=Real-time%20whole-body%20teleoperation%20is%20a%20critical%20method%20for%20humanoid%20robots%20to%20perform%20complex%20tasks%20in%20unstructured%20environments.%20However%2C%20developing%20a%20unified%20controller%20that%20robustly%20supports%20diverse%20human%20motions%20remains%20a%20significant%20challenge.%20Existing%20methods%20typically%20distill%20multiple%20expert%20policies%20into%20a%20single%20general%20policy%2C%20which%20often%20inevitably%20leads%20to%20performance%20degradation%2C%20particularly%20on%20highly%20dynamic%20motions.%20This%20paper%20presents%20TeleGate%2C%20a%20unified%20whole-body%20teleoperation%20framework%20for%20humanoid%20robots%20that%20achieves%20high-precision%20tracking%20across%20various%20motions%20while%20avoiding%20the%20performance%20loss%20inherent%20in%20knowledge%20distillation.%20Our%20key%20idea%20is%20to%20preserve%20the%20full%20capability%20of%20domain-specific%20expert%20policies%20by%20training%20a%20lightweight%20gating%20network%2C%20which%20dynamically%20activates%20experts%20in%20real-time%20based%20on%20proprioceptive%20states%20and%20reference%20trajectories.%20Furthermore%2C%20to%20compensate%20for%20the%20absence%20of%20future%20reference%20trajectories%20in%20real-time%20teleoperation%2C%20we%20introduce%20a%20VAE-based%20motion%20prior%20module%20that%20extracts%20implicit%20future%20motion%20intent%20from%20historical%20observations%2C%20enabling%20anticipatory%20control%20for%20motions%20requiring%20prediction%20such%20as%20jumping%20and%20standing%20up.%20We%20conducted%20empirical%20evaluations%20in%20simulation%20and%20also%20deployed%20our%20technique%20on%20the%20Unitree%20G1%20humanoid%20robot.%20Using%20only%202.5%20hours%20of%20motion%20capture%20data%20for%20training%2C%20our%20TeleGate%20achieves%20high-precision%20real-time%20teleoperation%20across%20diverse%20dynamic%20motions%20%28e.g.%2C%20running%2C%20fall%20recovery%2C%20and%20jumping%29%2C%20significantly%20outperforming%20the%20baseline%20methods%20in%20both%20tracking%20accuracy%20and%20success%20rate.&entry.1838667208=http%3A//arxiv.org/abs/2602.09628v1&entry.124074799=Read"},
{"title": "Linear Model Extraction via Factual and Counterfactual Queries", "author": "Daan Otto and Jannis Kurtz and Dick den Hertog and Ilker Birbil", "abstract": "In model extraction attacks, the goal is to reveal the parameters of a black-box machine learning model by querying the model for a selected set of data points. Due to an increasing demand for explanations, this may involve counterfactual queries besides the typically considered factual queries. In this work, we consider linear models and three types of queries: factual, counterfactual, and robust counterfactual. First, for an arbitrary set of queries, we derive novel mathematical formulations for the classification regions for which the decision of the unknown model is known, without recovering any of the model parameters. Second, we derive bounds on the number of queries needed to extract the model's parameters for (robust) counterfactual queries under arbitrary norm-based distances. We show that the full model can be recovered using just a single counterfactual query when differentiable distance measures are employed. In contrast, when using polyhedral distances for instance, the number of required queries grows linearly with the dimension of the data space. For robust counterfactuals, the latter number of queries doubles. Consequently, the applied distance function and robustness of counterfactuals have a significant impact on the model's security.", "link": "http://arxiv.org/abs/2602.09748v1", "date": "2026-02-10", "relevancy": 2.2645, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.461}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Model%20Extraction%20via%20Factual%20and%20Counterfactual%20Queries&body=Title%3A%20Linear%20Model%20Extraction%20via%20Factual%20and%20Counterfactual%20Queries%0AAuthor%3A%20Daan%20Otto%20and%20Jannis%20Kurtz%20and%20Dick%20den%20Hertog%20and%20Ilker%20Birbil%0AAbstract%3A%20In%20model%20extraction%20attacks%2C%20the%20goal%20is%20to%20reveal%20the%20parameters%20of%20a%20black-box%20machine%20learning%20model%20by%20querying%20the%20model%20for%20a%20selected%20set%20of%20data%20points.%20Due%20to%20an%20increasing%20demand%20for%20explanations%2C%20this%20may%20involve%20counterfactual%20queries%20besides%20the%20typically%20considered%20factual%20queries.%20In%20this%20work%2C%20we%20consider%20linear%20models%20and%20three%20types%20of%20queries%3A%20factual%2C%20counterfactual%2C%20and%20robust%20counterfactual.%20First%2C%20for%20an%20arbitrary%20set%20of%20queries%2C%20we%20derive%20novel%20mathematical%20formulations%20for%20the%20classification%20regions%20for%20which%20the%20decision%20of%20the%20unknown%20model%20is%20known%2C%20without%20recovering%20any%20of%20the%20model%20parameters.%20Second%2C%20we%20derive%20bounds%20on%20the%20number%20of%20queries%20needed%20to%20extract%20the%20model%27s%20parameters%20for%20%28robust%29%20counterfactual%20queries%20under%20arbitrary%20norm-based%20distances.%20We%20show%20that%20the%20full%20model%20can%20be%20recovered%20using%20just%20a%20single%20counterfactual%20query%20when%20differentiable%20distance%20measures%20are%20employed.%20In%20contrast%2C%20when%20using%20polyhedral%20distances%20for%20instance%2C%20the%20number%20of%20required%20queries%20grows%20linearly%20with%20the%20dimension%20of%20the%20data%20space.%20For%20robust%20counterfactuals%2C%20the%20latter%20number%20of%20queries%20doubles.%20Consequently%2C%20the%20applied%20distance%20function%20and%20robustness%20of%20counterfactuals%20have%20a%20significant%20impact%20on%20the%20model%27s%20security.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Model%2520Extraction%2520via%2520Factual%2520and%2520Counterfactual%2520Queries%26entry.906535625%3DDaan%2520Otto%2520and%2520Jannis%2520Kurtz%2520and%2520Dick%2520den%2520Hertog%2520and%2520Ilker%2520Birbil%26entry.1292438233%3DIn%2520model%2520extraction%2520attacks%252C%2520the%2520goal%2520is%2520to%2520reveal%2520the%2520parameters%2520of%2520a%2520black-box%2520machine%2520learning%2520model%2520by%2520querying%2520the%2520model%2520for%2520a%2520selected%2520set%2520of%2520data%2520points.%2520Due%2520to%2520an%2520increasing%2520demand%2520for%2520explanations%252C%2520this%2520may%2520involve%2520counterfactual%2520queries%2520besides%2520the%2520typically%2520considered%2520factual%2520queries.%2520In%2520this%2520work%252C%2520we%2520consider%2520linear%2520models%2520and%2520three%2520types%2520of%2520queries%253A%2520factual%252C%2520counterfactual%252C%2520and%2520robust%2520counterfactual.%2520First%252C%2520for%2520an%2520arbitrary%2520set%2520of%2520queries%252C%2520we%2520derive%2520novel%2520mathematical%2520formulations%2520for%2520the%2520classification%2520regions%2520for%2520which%2520the%2520decision%2520of%2520the%2520unknown%2520model%2520is%2520known%252C%2520without%2520recovering%2520any%2520of%2520the%2520model%2520parameters.%2520Second%252C%2520we%2520derive%2520bounds%2520on%2520the%2520number%2520of%2520queries%2520needed%2520to%2520extract%2520the%2520model%2527s%2520parameters%2520for%2520%2528robust%2529%2520counterfactual%2520queries%2520under%2520arbitrary%2520norm-based%2520distances.%2520We%2520show%2520that%2520the%2520full%2520model%2520can%2520be%2520recovered%2520using%2520just%2520a%2520single%2520counterfactual%2520query%2520when%2520differentiable%2520distance%2520measures%2520are%2520employed.%2520In%2520contrast%252C%2520when%2520using%2520polyhedral%2520distances%2520for%2520instance%252C%2520the%2520number%2520of%2520required%2520queries%2520grows%2520linearly%2520with%2520the%2520dimension%2520of%2520the%2520data%2520space.%2520For%2520robust%2520counterfactuals%252C%2520the%2520latter%2520number%2520of%2520queries%2520doubles.%2520Consequently%252C%2520the%2520applied%2520distance%2520function%2520and%2520robustness%2520of%2520counterfactuals%2520have%2520a%2520significant%2520impact%2520on%2520the%2520model%2527s%2520security.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Model%20Extraction%20via%20Factual%20and%20Counterfactual%20Queries&entry.906535625=Daan%20Otto%20and%20Jannis%20Kurtz%20and%20Dick%20den%20Hertog%20and%20Ilker%20Birbil&entry.1292438233=In%20model%20extraction%20attacks%2C%20the%20goal%20is%20to%20reveal%20the%20parameters%20of%20a%20black-box%20machine%20learning%20model%20by%20querying%20the%20model%20for%20a%20selected%20set%20of%20data%20points.%20Due%20to%20an%20increasing%20demand%20for%20explanations%2C%20this%20may%20involve%20counterfactual%20queries%20besides%20the%20typically%20considered%20factual%20queries.%20In%20this%20work%2C%20we%20consider%20linear%20models%20and%20three%20types%20of%20queries%3A%20factual%2C%20counterfactual%2C%20and%20robust%20counterfactual.%20First%2C%20for%20an%20arbitrary%20set%20of%20queries%2C%20we%20derive%20novel%20mathematical%20formulations%20for%20the%20classification%20regions%20for%20which%20the%20decision%20of%20the%20unknown%20model%20is%20known%2C%20without%20recovering%20any%20of%20the%20model%20parameters.%20Second%2C%20we%20derive%20bounds%20on%20the%20number%20of%20queries%20needed%20to%20extract%20the%20model%27s%20parameters%20for%20%28robust%29%20counterfactual%20queries%20under%20arbitrary%20norm-based%20distances.%20We%20show%20that%20the%20full%20model%20can%20be%20recovered%20using%20just%20a%20single%20counterfactual%20query%20when%20differentiable%20distance%20measures%20are%20employed.%20In%20contrast%2C%20when%20using%20polyhedral%20distances%20for%20instance%2C%20the%20number%20of%20required%20queries%20grows%20linearly%20with%20the%20dimension%20of%20the%20data%20space.%20For%20robust%20counterfactuals%2C%20the%20latter%20number%20of%20queries%20doubles.%20Consequently%2C%20the%20applied%20distance%20function%20and%20robustness%20of%20counterfactuals%20have%20a%20significant%20impact%20on%20the%20model%27s%20security.&entry.1838667208=http%3A//arxiv.org/abs/2602.09748v1&entry.124074799=Read"},
{"title": "Vendi Novelty Scores for Out-of-Distribution Detection", "author": "Amey P. Pasarkar and Adji Bousso Dieng", "abstract": "Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems. Existing post-hoc detectors typically rely on model confidence scores or likelihood estimates in feature space, often under restrictive distributional assumptions. In this work, we introduce a third paradigm and formulate OOD detection from a diversity perspective. We propose the Vendi Novelty Score (VNS), an OOD detector based on the Vendi Scores (VS), a family of similarity-based diversity metrics. VNS quantifies how much a test sample increases the VS of the in-distribution feature set, providing a principled notion of novelty that does not require density modeling. VNS is linear-time, non-parametric, and naturally combines class-conditional (local) and dataset-level (global) novelty signals. Across multiple image classification benchmarks and network architectures, VNS achieves state-of-the-art OOD detection performance. Remarkably, VNS retains this performance when computed using only 1% of the training data, enabling deployment in memory- or access-constrained settings.", "link": "http://arxiv.org/abs/2602.10062v1", "date": "2026-02-10", "relevancy": 2.2466, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4547}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4481}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vendi%20Novelty%20Scores%20for%20Out-of-Distribution%20Detection&body=Title%3A%20Vendi%20Novelty%20Scores%20for%20Out-of-Distribution%20Detection%0AAuthor%3A%20Amey%20P.%20Pasarkar%20and%20Adji%20Bousso%20Dieng%0AAbstract%3A%20Out-of-distribution%20%28OOD%29%20detection%20is%20critical%20for%20the%20safe%20deployment%20of%20machine%20learning%20systems.%20Existing%20post-hoc%20detectors%20typically%20rely%20on%20model%20confidence%20scores%20or%20likelihood%20estimates%20in%20feature%20space%2C%20often%20under%20restrictive%20distributional%20assumptions.%20In%20this%20work%2C%20we%20introduce%20a%20third%20paradigm%20and%20formulate%20OOD%20detection%20from%20a%20diversity%20perspective.%20We%20propose%20the%20Vendi%20Novelty%20Score%20%28VNS%29%2C%20an%20OOD%20detector%20based%20on%20the%20Vendi%20Scores%20%28VS%29%2C%20a%20family%20of%20similarity-based%20diversity%20metrics.%20VNS%20quantifies%20how%20much%20a%20test%20sample%20increases%20the%20VS%20of%20the%20in-distribution%20feature%20set%2C%20providing%20a%20principled%20notion%20of%20novelty%20that%20does%20not%20require%20density%20modeling.%20VNS%20is%20linear-time%2C%20non-parametric%2C%20and%20naturally%20combines%20class-conditional%20%28local%29%20and%20dataset-level%20%28global%29%20novelty%20signals.%20Across%20multiple%20image%20classification%20benchmarks%20and%20network%20architectures%2C%20VNS%20achieves%20state-of-the-art%20OOD%20detection%20performance.%20Remarkably%2C%20VNS%20retains%20this%20performance%20when%20computed%20using%20only%201%25%20of%20the%20training%20data%2C%20enabling%20deployment%20in%20memory-%20or%20access-constrained%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVendi%2520Novelty%2520Scores%2520for%2520Out-of-Distribution%2520Detection%26entry.906535625%3DAmey%2520P.%2520Pasarkar%2520and%2520Adji%2520Bousso%2520Dieng%26entry.1292438233%3DOut-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520critical%2520for%2520the%2520safe%2520deployment%2520of%2520machine%2520learning%2520systems.%2520Existing%2520post-hoc%2520detectors%2520typically%2520rely%2520on%2520model%2520confidence%2520scores%2520or%2520likelihood%2520estimates%2520in%2520feature%2520space%252C%2520often%2520under%2520restrictive%2520distributional%2520assumptions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520third%2520paradigm%2520and%2520formulate%2520OOD%2520detection%2520from%2520a%2520diversity%2520perspective.%2520We%2520propose%2520the%2520Vendi%2520Novelty%2520Score%2520%2528VNS%2529%252C%2520an%2520OOD%2520detector%2520based%2520on%2520the%2520Vendi%2520Scores%2520%2528VS%2529%252C%2520a%2520family%2520of%2520similarity-based%2520diversity%2520metrics.%2520VNS%2520quantifies%2520how%2520much%2520a%2520test%2520sample%2520increases%2520the%2520VS%2520of%2520the%2520in-distribution%2520feature%2520set%252C%2520providing%2520a%2520principled%2520notion%2520of%2520novelty%2520that%2520does%2520not%2520require%2520density%2520modeling.%2520VNS%2520is%2520linear-time%252C%2520non-parametric%252C%2520and%2520naturally%2520combines%2520class-conditional%2520%2528local%2529%2520and%2520dataset-level%2520%2528global%2529%2520novelty%2520signals.%2520Across%2520multiple%2520image%2520classification%2520benchmarks%2520and%2520network%2520architectures%252C%2520VNS%2520achieves%2520state-of-the-art%2520OOD%2520detection%2520performance.%2520Remarkably%252C%2520VNS%2520retains%2520this%2520performance%2520when%2520computed%2520using%2520only%25201%2525%2520of%2520the%2520training%2520data%252C%2520enabling%2520deployment%2520in%2520memory-%2520or%2520access-constrained%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vendi%20Novelty%20Scores%20for%20Out-of-Distribution%20Detection&entry.906535625=Amey%20P.%20Pasarkar%20and%20Adji%20Bousso%20Dieng&entry.1292438233=Out-of-distribution%20%28OOD%29%20detection%20is%20critical%20for%20the%20safe%20deployment%20of%20machine%20learning%20systems.%20Existing%20post-hoc%20detectors%20typically%20rely%20on%20model%20confidence%20scores%20or%20likelihood%20estimates%20in%20feature%20space%2C%20often%20under%20restrictive%20distributional%20assumptions.%20In%20this%20work%2C%20we%20introduce%20a%20third%20paradigm%20and%20formulate%20OOD%20detection%20from%20a%20diversity%20perspective.%20We%20propose%20the%20Vendi%20Novelty%20Score%20%28VNS%29%2C%20an%20OOD%20detector%20based%20on%20the%20Vendi%20Scores%20%28VS%29%2C%20a%20family%20of%20similarity-based%20diversity%20metrics.%20VNS%20quantifies%20how%20much%20a%20test%20sample%20increases%20the%20VS%20of%20the%20in-distribution%20feature%20set%2C%20providing%20a%20principled%20notion%20of%20novelty%20that%20does%20not%20require%20density%20modeling.%20VNS%20is%20linear-time%2C%20non-parametric%2C%20and%20naturally%20combines%20class-conditional%20%28local%29%20and%20dataset-level%20%28global%29%20novelty%20signals.%20Across%20multiple%20image%20classification%20benchmarks%20and%20network%20architectures%2C%20VNS%20achieves%20state-of-the-art%20OOD%20detection%20performance.%20Remarkably%2C%20VNS%20retains%20this%20performance%20when%20computed%20using%20only%201%25%20of%20the%20training%20data%2C%20enabling%20deployment%20in%20memory-%20or%20access-constrained%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.10062v1&entry.124074799=Read"},
{"title": "Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning", "author": "Hengwei Zhao and Zhengzhong Tu and Zhuo Zheng and Wei Wang and Junjue Wang and Rusty Feagin and Wenzhe Jiao", "abstract": "Positive-Unlabeled (PU) learning aims to train a binary classifier (positive vs. negative) where only limited positive data and abundant unlabeled data are available. While widely applicable, state-of-the-art PU learning methods substantially underperform their supervised counterparts on complex datasets, especially without auxiliary negatives or pre-estimated parameters (e.g., a 14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the challenge of learning discriminative representations under unreliable supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU learning framework that requires no auxiliary information. NcPU combines a noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns intra-class representations despite unreliable supervision, with a phantom label disambiguation (PLD) scheme that supplies conservative negative supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can iteratively benefit each other from the perspective of the Expectation-Maximization framework. Empirically, extensive experiments demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive performance; and (2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging datasets on post-disaster building damage mapping, highlighting its promise for real-world applications. Code: Code will be open-sourced after review.", "link": "http://arxiv.org/abs/2510.01278v2", "date": "2026-02-10", "relevancy": 2.2423, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5836}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5635}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noisy-Pair%20Robust%20Representation%20Alignment%20for%20Positive-Unlabeled%20Learning&body=Title%3A%20Noisy-Pair%20Robust%20Representation%20Alignment%20for%20Positive-Unlabeled%20Learning%0AAuthor%3A%20Hengwei%20Zhao%20and%20Zhengzhong%20Tu%20and%20Zhuo%20Zheng%20and%20Wei%20Wang%20and%20Junjue%20Wang%20and%20Rusty%20Feagin%20and%20Wenzhe%20Jiao%0AAbstract%3A%20Positive-Unlabeled%20%28PU%29%20learning%20aims%20to%20train%20a%20binary%20classifier%20%28positive%20vs.%20negative%29%20where%20only%20limited%20positive%20data%20and%20abundant%20unlabeled%20data%20are%20available.%20While%20widely%20applicable%2C%20state-of-the-art%20PU%20learning%20methods%20substantially%20underperform%20their%20supervised%20counterparts%20on%20complex%20datasets%2C%20especially%20without%20auxiliary%20negatives%20or%20pre-estimated%20parameters%20%28e.g.%2C%20a%2014.26%25%20gap%20on%20CIFAR-100%20dataset%29.%20We%20identify%20the%20primary%20bottleneck%20as%20the%20challenge%20of%20learning%20discriminative%20representations%20under%20unreliable%20supervision.%20To%20tackle%20this%20challenge%2C%20we%20propose%20NcPU%2C%20a%20non-contrastive%20PU%20learning%20framework%20that%20requires%20no%20auxiliary%20information.%20NcPU%20combines%20a%20noisy-pair%20robust%20supervised%20non-contrastive%20loss%20%28NoiSNCL%29%2C%20which%20aligns%20intra-class%20representations%20despite%20unreliable%20supervision%2C%20with%20a%20phantom%20label%20disambiguation%20%28PLD%29%20scheme%20that%20supplies%20conservative%20negative%20supervision%20via%20regret-based%20label%20updates.%20Theoretically%2C%20NoiSNCL%20and%20PLD%20can%20iteratively%20benefit%20each%20other%20from%20the%20perspective%20of%20the%20Expectation-Maximization%20framework.%20Empirically%2C%20extensive%20experiments%20demonstrate%20that%3A%20%281%29%20NoiSNCL%20enables%20simple%20PU%20methods%20to%20achieve%20competitive%20performance%3B%20and%20%282%29%20NcPU%20achieves%20substantial%20improvements%20over%20state-of-the-art%20PU%20methods%20across%20diverse%20datasets%2C%20including%20challenging%20datasets%20on%20post-disaster%20building%20damage%20mapping%2C%20highlighting%20its%20promise%20for%20real-world%20applications.%20Code%3A%20Code%20will%20be%20open-sourced%20after%20review.%0ALink%3A%20http%3A//arxiv.org/abs/2510.01278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisy-Pair%2520Robust%2520Representation%2520Alignment%2520for%2520Positive-Unlabeled%2520Learning%26entry.906535625%3DHengwei%2520Zhao%2520and%2520Zhengzhong%2520Tu%2520and%2520Zhuo%2520Zheng%2520and%2520Wei%2520Wang%2520and%2520Junjue%2520Wang%2520and%2520Rusty%2520Feagin%2520and%2520Wenzhe%2520Jiao%26entry.1292438233%3DPositive-Unlabeled%2520%2528PU%2529%2520learning%2520aims%2520to%2520train%2520a%2520binary%2520classifier%2520%2528positive%2520vs.%2520negative%2529%2520where%2520only%2520limited%2520positive%2520data%2520and%2520abundant%2520unlabeled%2520data%2520are%2520available.%2520While%2520widely%2520applicable%252C%2520state-of-the-art%2520PU%2520learning%2520methods%2520substantially%2520underperform%2520their%2520supervised%2520counterparts%2520on%2520complex%2520datasets%252C%2520especially%2520without%2520auxiliary%2520negatives%2520or%2520pre-estimated%2520parameters%2520%2528e.g.%252C%2520a%252014.26%2525%2520gap%2520on%2520CIFAR-100%2520dataset%2529.%2520We%2520identify%2520the%2520primary%2520bottleneck%2520as%2520the%2520challenge%2520of%2520learning%2520discriminative%2520representations%2520under%2520unreliable%2520supervision.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520NcPU%252C%2520a%2520non-contrastive%2520PU%2520learning%2520framework%2520that%2520requires%2520no%2520auxiliary%2520information.%2520NcPU%2520combines%2520a%2520noisy-pair%2520robust%2520supervised%2520non-contrastive%2520loss%2520%2528NoiSNCL%2529%252C%2520which%2520aligns%2520intra-class%2520representations%2520despite%2520unreliable%2520supervision%252C%2520with%2520a%2520phantom%2520label%2520disambiguation%2520%2528PLD%2529%2520scheme%2520that%2520supplies%2520conservative%2520negative%2520supervision%2520via%2520regret-based%2520label%2520updates.%2520Theoretically%252C%2520NoiSNCL%2520and%2520PLD%2520can%2520iteratively%2520benefit%2520each%2520other%2520from%2520the%2520perspective%2520of%2520the%2520Expectation-Maximization%2520framework.%2520Empirically%252C%2520extensive%2520experiments%2520demonstrate%2520that%253A%2520%25281%2529%2520NoiSNCL%2520enables%2520simple%2520PU%2520methods%2520to%2520achieve%2520competitive%2520performance%253B%2520and%2520%25282%2529%2520NcPU%2520achieves%2520substantial%2520improvements%2520over%2520state-of-the-art%2520PU%2520methods%2520across%2520diverse%2520datasets%252C%2520including%2520challenging%2520datasets%2520on%2520post-disaster%2520building%2520damage%2520mapping%252C%2520highlighting%2520its%2520promise%2520for%2520real-world%2520applications.%2520Code%253A%2520Code%2520will%2520be%2520open-sourced%2520after%2520review.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noisy-Pair%20Robust%20Representation%20Alignment%20for%20Positive-Unlabeled%20Learning&entry.906535625=Hengwei%20Zhao%20and%20Zhengzhong%20Tu%20and%20Zhuo%20Zheng%20and%20Wei%20Wang%20and%20Junjue%20Wang%20and%20Rusty%20Feagin%20and%20Wenzhe%20Jiao&entry.1292438233=Positive-Unlabeled%20%28PU%29%20learning%20aims%20to%20train%20a%20binary%20classifier%20%28positive%20vs.%20negative%29%20where%20only%20limited%20positive%20data%20and%20abundant%20unlabeled%20data%20are%20available.%20While%20widely%20applicable%2C%20state-of-the-art%20PU%20learning%20methods%20substantially%20underperform%20their%20supervised%20counterparts%20on%20complex%20datasets%2C%20especially%20without%20auxiliary%20negatives%20or%20pre-estimated%20parameters%20%28e.g.%2C%20a%2014.26%25%20gap%20on%20CIFAR-100%20dataset%29.%20We%20identify%20the%20primary%20bottleneck%20as%20the%20challenge%20of%20learning%20discriminative%20representations%20under%20unreliable%20supervision.%20To%20tackle%20this%20challenge%2C%20we%20propose%20NcPU%2C%20a%20non-contrastive%20PU%20learning%20framework%20that%20requires%20no%20auxiliary%20information.%20NcPU%20combines%20a%20noisy-pair%20robust%20supervised%20non-contrastive%20loss%20%28NoiSNCL%29%2C%20which%20aligns%20intra-class%20representations%20despite%20unreliable%20supervision%2C%20with%20a%20phantom%20label%20disambiguation%20%28PLD%29%20scheme%20that%20supplies%20conservative%20negative%20supervision%20via%20regret-based%20label%20updates.%20Theoretically%2C%20NoiSNCL%20and%20PLD%20can%20iteratively%20benefit%20each%20other%20from%20the%20perspective%20of%20the%20Expectation-Maximization%20framework.%20Empirically%2C%20extensive%20experiments%20demonstrate%20that%3A%20%281%29%20NoiSNCL%20enables%20simple%20PU%20methods%20to%20achieve%20competitive%20performance%3B%20and%20%282%29%20NcPU%20achieves%20substantial%20improvements%20over%20state-of-the-art%20PU%20methods%20across%20diverse%20datasets%2C%20including%20challenging%20datasets%20on%20post-disaster%20building%20damage%20mapping%2C%20highlighting%20its%20promise%20for%20real-world%20applications.%20Code%3A%20Code%20will%20be%20open-sourced%20after%20review.&entry.1838667208=http%3A//arxiv.org/abs/2510.01278v2&entry.124074799=Read"},
{"title": "Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization", "author": "Denglin Cheng and Jiarong Kang and Xiaobin Xiong", "abstract": "Accurate state estimation is critical for legged and aerial robots operating in dynamic, uncertain environments. A key challenge lies in specifying process and measurement noise covariances, which are typically unknown or manually tuned. In this work, we introduce a bi-level optimization framework that jointly calibrates covariance matrices and kinematic parameters in an estimator-in-the-loop manner. The upper level treats noise covariances and model parameters as optimization variables, while the lower level executes a full-information estimator. Differentiating through the estimator allows direct optimization of trajectory-level objectives, resulting in accurate and consistent state estimates. We validate our approach on quadrupedal and humanoid robots, demonstrating significantly improved estimation accuracy and uncertainty calibration compared to hand-tuned baselines. Our method unifies state estimation, sensor, and kinematics calibration into a principled, data-driven framework applicable across diverse robotic platforms.", "link": "http://arxiv.org/abs/2510.11539v3", "date": "2026-02-10", "relevancy": 2.2384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6484}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Calibration%20of%20Noise%20Covariance%20and%20Kinematics%20for%20State%20Estimation%20of%20Legged%20Robots%20via%20Bi-level%20Optimization&body=Title%3A%20Simultaneous%20Calibration%20of%20Noise%20Covariance%20and%20Kinematics%20for%20State%20Estimation%20of%20Legged%20Robots%20via%20Bi-level%20Optimization%0AAuthor%3A%20Denglin%20Cheng%20and%20Jiarong%20Kang%20and%20Xiaobin%20Xiong%0AAbstract%3A%20Accurate%20state%20estimation%20is%20critical%20for%20legged%20and%20aerial%20robots%20operating%20in%20dynamic%2C%20uncertain%20environments.%20A%20key%20challenge%20lies%20in%20specifying%20process%20and%20measurement%20noise%20covariances%2C%20which%20are%20typically%20unknown%20or%20manually%20tuned.%20In%20this%20work%2C%20we%20introduce%20a%20bi-level%20optimization%20framework%20that%20jointly%20calibrates%20covariance%20matrices%20and%20kinematic%20parameters%20in%20an%20estimator-in-the-loop%20manner.%20The%20upper%20level%20treats%20noise%20covariances%20and%20model%20parameters%20as%20optimization%20variables%2C%20while%20the%20lower%20level%20executes%20a%20full-information%20estimator.%20Differentiating%20through%20the%20estimator%20allows%20direct%20optimization%20of%20trajectory-level%20objectives%2C%20resulting%20in%20accurate%20and%20consistent%20state%20estimates.%20We%20validate%20our%20approach%20on%20quadrupedal%20and%20humanoid%20robots%2C%20demonstrating%20significantly%20improved%20estimation%20accuracy%20and%20uncertainty%20calibration%20compared%20to%20hand-tuned%20baselines.%20Our%20method%20unifies%20state%20estimation%2C%20sensor%2C%20and%20kinematics%20calibration%20into%20a%20principled%2C%20data-driven%20framework%20applicable%20across%20diverse%20robotic%20platforms.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11539v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Calibration%2520of%2520Noise%2520Covariance%2520and%2520Kinematics%2520for%2520State%2520Estimation%2520of%2520Legged%2520Robots%2520via%2520Bi-level%2520Optimization%26entry.906535625%3DDenglin%2520Cheng%2520and%2520Jiarong%2520Kang%2520and%2520Xiaobin%2520Xiong%26entry.1292438233%3DAccurate%2520state%2520estimation%2520is%2520critical%2520for%2520legged%2520and%2520aerial%2520robots%2520operating%2520in%2520dynamic%252C%2520uncertain%2520environments.%2520A%2520key%2520challenge%2520lies%2520in%2520specifying%2520process%2520and%2520measurement%2520noise%2520covariances%252C%2520which%2520are%2520typically%2520unknown%2520or%2520manually%2520tuned.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520bi-level%2520optimization%2520framework%2520that%2520jointly%2520calibrates%2520covariance%2520matrices%2520and%2520kinematic%2520parameters%2520in%2520an%2520estimator-in-the-loop%2520manner.%2520The%2520upper%2520level%2520treats%2520noise%2520covariances%2520and%2520model%2520parameters%2520as%2520optimization%2520variables%252C%2520while%2520the%2520lower%2520level%2520executes%2520a%2520full-information%2520estimator.%2520Differentiating%2520through%2520the%2520estimator%2520allows%2520direct%2520optimization%2520of%2520trajectory-level%2520objectives%252C%2520resulting%2520in%2520accurate%2520and%2520consistent%2520state%2520estimates.%2520We%2520validate%2520our%2520approach%2520on%2520quadrupedal%2520and%2520humanoid%2520robots%252C%2520demonstrating%2520significantly%2520improved%2520estimation%2520accuracy%2520and%2520uncertainty%2520calibration%2520compared%2520to%2520hand-tuned%2520baselines.%2520Our%2520method%2520unifies%2520state%2520estimation%252C%2520sensor%252C%2520and%2520kinematics%2520calibration%2520into%2520a%2520principled%252C%2520data-driven%2520framework%2520applicable%2520across%2520diverse%2520robotic%2520platforms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11539v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Calibration%20of%20Noise%20Covariance%20and%20Kinematics%20for%20State%20Estimation%20of%20Legged%20Robots%20via%20Bi-level%20Optimization&entry.906535625=Denglin%20Cheng%20and%20Jiarong%20Kang%20and%20Xiaobin%20Xiong&entry.1292438233=Accurate%20state%20estimation%20is%20critical%20for%20legged%20and%20aerial%20robots%20operating%20in%20dynamic%2C%20uncertain%20environments.%20A%20key%20challenge%20lies%20in%20specifying%20process%20and%20measurement%20noise%20covariances%2C%20which%20are%20typically%20unknown%20or%20manually%20tuned.%20In%20this%20work%2C%20we%20introduce%20a%20bi-level%20optimization%20framework%20that%20jointly%20calibrates%20covariance%20matrices%20and%20kinematic%20parameters%20in%20an%20estimator-in-the-loop%20manner.%20The%20upper%20level%20treats%20noise%20covariances%20and%20model%20parameters%20as%20optimization%20variables%2C%20while%20the%20lower%20level%20executes%20a%20full-information%20estimator.%20Differentiating%20through%20the%20estimator%20allows%20direct%20optimization%20of%20trajectory-level%20objectives%2C%20resulting%20in%20accurate%20and%20consistent%20state%20estimates.%20We%20validate%20our%20approach%20on%20quadrupedal%20and%20humanoid%20robots%2C%20demonstrating%20significantly%20improved%20estimation%20accuracy%20and%20uncertainty%20calibration%20compared%20to%20hand-tuned%20baselines.%20Our%20method%20unifies%20state%20estimation%2C%20sensor%2C%20and%20kinematics%20calibration%20into%20a%20principled%2C%20data-driven%20framework%20applicable%20across%20diverse%20robotic%20platforms.&entry.1838667208=http%3A//arxiv.org/abs/2510.11539v3&entry.124074799=Read"},
{"title": "Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation", "author": "Tim Engelbracht and Ren\u00e9 Zurbr\u00fcgg and Matteo Wohlrapp and Martin B\u00fcchner and Abhinav Valada and Marc Pollefeys and Hermann Blum and Zuria Bauer", "abstract": "We present a dataset for force-grounded, cross-view articulated manipulation that couples what is seen with what is done and what is felt during real human interaction. The dataset contains 3048 sequences across 381 articulated objects in 38 environments. Each object is operated under four embodiments - (i) human hand, (ii) human hand with a wrist-mounted camera, (iii) handheld UMI gripper, and (iv) a custom Hoi! gripper - where the tool embodiment provides synchronized end-effector forces and tactile sensing. Our dataset offers a holistic view of interaction understanding from video, enabling researchers to evaluate how well methods transfer between human and robotic viewpoints, but also investigate underexplored modalities such as force sensing and prediction. Further information can be found on the Website.", "link": "http://arxiv.org/abs/2512.04884v2", "date": "2026-02-10", "relevancy": 2.2263, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5704}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5485}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hoi%21%20--%20A%20Multimodal%20Dataset%20for%20Force-Grounded%2C%20Cross-View%20Articulated%20Manipulation&body=Title%3A%20Hoi%21%20--%20A%20Multimodal%20Dataset%20for%20Force-Grounded%2C%20Cross-View%20Articulated%20Manipulation%0AAuthor%3A%20Tim%20Engelbracht%20and%20Ren%C3%A9%20Zurbr%C3%BCgg%20and%20Matteo%20Wohlrapp%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer%0AAbstract%3A%20We%20present%20a%20dataset%20for%20force-grounded%2C%20cross-view%20articulated%20manipulation%20that%20couples%20what%20is%20seen%20with%20what%20is%20done%20and%20what%20is%20felt%20during%20real%20human%20interaction.%20The%20dataset%20contains%203048%20sequences%20across%20381%20articulated%20objects%20in%2038%20environments.%20Each%20object%20is%20operated%20under%20four%20embodiments%20-%20%28i%29%20human%20hand%2C%20%28ii%29%20human%20hand%20with%20a%20wrist-mounted%20camera%2C%20%28iii%29%20handheld%20UMI%20gripper%2C%20and%20%28iv%29%20a%20custom%20Hoi%21%20gripper%20-%20where%20the%20tool%20embodiment%20provides%20synchronized%20end-effector%20forces%20and%20tactile%20sensing.%20Our%20dataset%20offers%20a%20holistic%20view%20of%20interaction%20understanding%20from%20video%2C%20enabling%20researchers%20to%20evaluate%20how%20well%20methods%20transfer%20between%20human%20and%20robotic%20viewpoints%2C%20but%20also%20investigate%20underexplored%20modalities%20such%20as%20force%20sensing%20and%20prediction.%20Further%20information%20can%20be%20found%20on%20the%20Website.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoi%2521%2520--%2520A%2520Multimodal%2520Dataset%2520for%2520Force-Grounded%252C%2520Cross-View%2520Articulated%2520Manipulation%26entry.906535625%3DTim%2520Engelbracht%2520and%2520Ren%25C3%25A9%2520Zurbr%25C3%25BCgg%2520and%2520Matteo%2520Wohlrapp%2520and%2520Martin%2520B%25C3%25BCchner%2520and%2520Abhinav%2520Valada%2520and%2520Marc%2520Pollefeys%2520and%2520Hermann%2520Blum%2520and%2520Zuria%2520Bauer%26entry.1292438233%3DWe%2520present%2520a%2520dataset%2520for%2520force-grounded%252C%2520cross-view%2520articulated%2520manipulation%2520that%2520couples%2520what%2520is%2520seen%2520with%2520what%2520is%2520done%2520and%2520what%2520is%2520felt%2520during%2520real%2520human%2520interaction.%2520The%2520dataset%2520contains%25203048%2520sequences%2520across%2520381%2520articulated%2520objects%2520in%252038%2520environments.%2520Each%2520object%2520is%2520operated%2520under%2520four%2520embodiments%2520-%2520%2528i%2529%2520human%2520hand%252C%2520%2528ii%2529%2520human%2520hand%2520with%2520a%2520wrist-mounted%2520camera%252C%2520%2528iii%2529%2520handheld%2520UMI%2520gripper%252C%2520and%2520%2528iv%2529%2520a%2520custom%2520Hoi%2521%2520gripper%2520-%2520where%2520the%2520tool%2520embodiment%2520provides%2520synchronized%2520end-effector%2520forces%2520and%2520tactile%2520sensing.%2520Our%2520dataset%2520offers%2520a%2520holistic%2520view%2520of%2520interaction%2520understanding%2520from%2520video%252C%2520enabling%2520researchers%2520to%2520evaluate%2520how%2520well%2520methods%2520transfer%2520between%2520human%2520and%2520robotic%2520viewpoints%252C%2520but%2520also%2520investigate%2520underexplored%2520modalities%2520such%2520as%2520force%2520sensing%2520and%2520prediction.%2520Further%2520information%2520can%2520be%2520found%2520on%2520the%2520Website.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hoi%21%20--%20A%20Multimodal%20Dataset%20for%20Force-Grounded%2C%20Cross-View%20Articulated%20Manipulation&entry.906535625=Tim%20Engelbracht%20and%20Ren%C3%A9%20Zurbr%C3%BCgg%20and%20Matteo%20Wohlrapp%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer&entry.1292438233=We%20present%20a%20dataset%20for%20force-grounded%2C%20cross-view%20articulated%20manipulation%20that%20couples%20what%20is%20seen%20with%20what%20is%20done%20and%20what%20is%20felt%20during%20real%20human%20interaction.%20The%20dataset%20contains%203048%20sequences%20across%20381%20articulated%20objects%20in%2038%20environments.%20Each%20object%20is%20operated%20under%20four%20embodiments%20-%20%28i%29%20human%20hand%2C%20%28ii%29%20human%20hand%20with%20a%20wrist-mounted%20camera%2C%20%28iii%29%20handheld%20UMI%20gripper%2C%20and%20%28iv%29%20a%20custom%20Hoi%21%20gripper%20-%20where%20the%20tool%20embodiment%20provides%20synchronized%20end-effector%20forces%20and%20tactile%20sensing.%20Our%20dataset%20offers%20a%20holistic%20view%20of%20interaction%20understanding%20from%20video%2C%20enabling%20researchers%20to%20evaluate%20how%20well%20methods%20transfer%20between%20human%20and%20robotic%20viewpoints%2C%20but%20also%20investigate%20underexplored%20modalities%20such%20as%20force%20sensing%20and%20prediction.%20Further%20information%20can%20be%20found%20on%20the%20Website.&entry.1838667208=http%3A//arxiv.org/abs/2512.04884v2&entry.124074799=Read"},
{"title": "Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization", "author": "Ye Wang and Sipeng Zheng and Hao Luo and Wanpeng Zhang and Haoqi Yuan and Chaoyi Xu and Haiweng Xu and Yicheng Feng and Mingyang Yu and Zhiyu Kang and Zongqing Lu and Qin Jin", "abstract": "While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard \"scale data\" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla", "link": "http://arxiv.org/abs/2602.09722v1", "date": "2026-02-10", "relevancy": 2.2233, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Visual-Language-Action%20Model%20Scaling%3A%20Alignment%2C%20Mixture%2C%20and%20Regularization&body=Title%3A%20Rethinking%20Visual-Language-Action%20Model%20Scaling%3A%20Alignment%2C%20Mixture%2C%20and%20Regularization%0AAuthor%3A%20Ye%20Wang%20and%20Sipeng%20Zheng%20and%20Hao%20Luo%20and%20Wanpeng%20Zhang%20and%20Haoqi%20Yuan%20and%20Chaoyi%20Xu%20and%20Haiweng%20Xu%20and%20Yicheng%20Feng%20and%20Mingyang%20Yu%20and%20Zhiyu%20Kang%20and%20Zongqing%20Lu%20and%20Qin%20Jin%0AAbstract%3A%20While%20Vision-Language-Action%20%28VLA%29%20models%20show%20strong%20promise%20for%20generalist%20robot%20control%2C%20it%20remains%20unclear%20whether%20--%20and%20under%20what%20conditions%20--%20the%20standard%20%22scale%20data%22%20recipe%20translates%20to%20robotics%2C%20where%20training%20data%20is%20inherently%20heterogeneous%20across%20embodiments%2C%20sensors%2C%20and%20action%20spaces.%20We%20present%20a%20systematic%2C%20controlled%20study%20of%20VLA%20scaling%20that%20revisits%20core%20training%20choices%20for%20pretraining%20across%20diverse%20robots.%20Using%20a%20representative%20VLA%20framework%20that%20combines%20a%20vision-language%20backbone%20with%20flow-matching%2C%20we%20ablate%20key%20design%20decisions%20under%20matched%20conditions%20and%20evaluate%20in%20extensive%20simulation%20and%20real-robot%20experiments.%20To%20improve%20the%20reliability%20of%20real-world%20results%2C%20we%20introduce%20a%20Grouped%20Blind%20Ensemble%20protocol%20that%20blinds%20operators%20to%20model%20identity%20and%20separates%20policy%20execution%20from%20outcome%20judgment%2C%20reducing%20experimenter%20bias.%20Our%20analysis%20targets%20three%20dimensions%20of%20VLA%20scaling.%20%281%29%20Physical%20alignment%3A%20we%20show%20that%20a%20unified%20end-effector%20%28EEF%29-relative%20action%20representation%20is%20critical%20for%20robust%20cross-embodiment%20transfer.%20%282%29%20Embodiment%20mixture%3A%20we%20find%20that%20naively%20pooling%20heterogeneous%20robot%20datasets%20often%20induces%20negative%20transfer%20rather%20than%20gains%2C%20underscoring%20the%20fragility%20of%20indiscriminate%20data%20scaling.%20%283%29%20Training%20regularization%3A%20we%20observe%20that%20intuitive%20strategies%2C%20such%20as%20sensory%20dropout%20and%20multi-stage%20fine-tuning%2C%20do%20not%20consistently%20improve%20performance%20at%20scale.%20Together%2C%20this%20study%20challenge%20some%20common%20assumptions%20about%20embodied%20scaling%20and%20provide%20practical%20guidance%20for%20training%20large-scale%20VLA%20policies%20from%20diverse%20robotic%20data.%20Project%20website%3A%20https%3A//research.beingbeyond.com/rethink_vla%0ALink%3A%20http%3A//arxiv.org/abs/2602.09722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Visual-Language-Action%2520Model%2520Scaling%253A%2520Alignment%252C%2520Mixture%252C%2520and%2520Regularization%26entry.906535625%3DYe%2520Wang%2520and%2520Sipeng%2520Zheng%2520and%2520Hao%2520Luo%2520and%2520Wanpeng%2520Zhang%2520and%2520Haoqi%2520Yuan%2520and%2520Chaoyi%2520Xu%2520and%2520Haiweng%2520Xu%2520and%2520Yicheng%2520Feng%2520and%2520Mingyang%2520Yu%2520and%2520Zhiyu%2520Kang%2520and%2520Zongqing%2520Lu%2520and%2520Qin%2520Jin%26entry.1292438233%3DWhile%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520show%2520strong%2520promise%2520for%2520generalist%2520robot%2520control%252C%2520it%2520remains%2520unclear%2520whether%2520--%2520and%2520under%2520what%2520conditions%2520--%2520the%2520standard%2520%2522scale%2520data%2522%2520recipe%2520translates%2520to%2520robotics%252C%2520where%2520training%2520data%2520is%2520inherently%2520heterogeneous%2520across%2520embodiments%252C%2520sensors%252C%2520and%2520action%2520spaces.%2520We%2520present%2520a%2520systematic%252C%2520controlled%2520study%2520of%2520VLA%2520scaling%2520that%2520revisits%2520core%2520training%2520choices%2520for%2520pretraining%2520across%2520diverse%2520robots.%2520Using%2520a%2520representative%2520VLA%2520framework%2520that%2520combines%2520a%2520vision-language%2520backbone%2520with%2520flow-matching%252C%2520we%2520ablate%2520key%2520design%2520decisions%2520under%2520matched%2520conditions%2520and%2520evaluate%2520in%2520extensive%2520simulation%2520and%2520real-robot%2520experiments.%2520To%2520improve%2520the%2520reliability%2520of%2520real-world%2520results%252C%2520we%2520introduce%2520a%2520Grouped%2520Blind%2520Ensemble%2520protocol%2520that%2520blinds%2520operators%2520to%2520model%2520identity%2520and%2520separates%2520policy%2520execution%2520from%2520outcome%2520judgment%252C%2520reducing%2520experimenter%2520bias.%2520Our%2520analysis%2520targets%2520three%2520dimensions%2520of%2520VLA%2520scaling.%2520%25281%2529%2520Physical%2520alignment%253A%2520we%2520show%2520that%2520a%2520unified%2520end-effector%2520%2528EEF%2529-relative%2520action%2520representation%2520is%2520critical%2520for%2520robust%2520cross-embodiment%2520transfer.%2520%25282%2529%2520Embodiment%2520mixture%253A%2520we%2520find%2520that%2520naively%2520pooling%2520heterogeneous%2520robot%2520datasets%2520often%2520induces%2520negative%2520transfer%2520rather%2520than%2520gains%252C%2520underscoring%2520the%2520fragility%2520of%2520indiscriminate%2520data%2520scaling.%2520%25283%2529%2520Training%2520regularization%253A%2520we%2520observe%2520that%2520intuitive%2520strategies%252C%2520such%2520as%2520sensory%2520dropout%2520and%2520multi-stage%2520fine-tuning%252C%2520do%2520not%2520consistently%2520improve%2520performance%2520at%2520scale.%2520Together%252C%2520this%2520study%2520challenge%2520some%2520common%2520assumptions%2520about%2520embodied%2520scaling%2520and%2520provide%2520practical%2520guidance%2520for%2520training%2520large-scale%2520VLA%2520policies%2520from%2520diverse%2520robotic%2520data.%2520Project%2520website%253A%2520https%253A//research.beingbeyond.com/rethink_vla%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Visual-Language-Action%20Model%20Scaling%3A%20Alignment%2C%20Mixture%2C%20and%20Regularization&entry.906535625=Ye%20Wang%20and%20Sipeng%20Zheng%20and%20Hao%20Luo%20and%20Wanpeng%20Zhang%20and%20Haoqi%20Yuan%20and%20Chaoyi%20Xu%20and%20Haiweng%20Xu%20and%20Yicheng%20Feng%20and%20Mingyang%20Yu%20and%20Zhiyu%20Kang%20and%20Zongqing%20Lu%20and%20Qin%20Jin&entry.1292438233=While%20Vision-Language-Action%20%28VLA%29%20models%20show%20strong%20promise%20for%20generalist%20robot%20control%2C%20it%20remains%20unclear%20whether%20--%20and%20under%20what%20conditions%20--%20the%20standard%20%22scale%20data%22%20recipe%20translates%20to%20robotics%2C%20where%20training%20data%20is%20inherently%20heterogeneous%20across%20embodiments%2C%20sensors%2C%20and%20action%20spaces.%20We%20present%20a%20systematic%2C%20controlled%20study%20of%20VLA%20scaling%20that%20revisits%20core%20training%20choices%20for%20pretraining%20across%20diverse%20robots.%20Using%20a%20representative%20VLA%20framework%20that%20combines%20a%20vision-language%20backbone%20with%20flow-matching%2C%20we%20ablate%20key%20design%20decisions%20under%20matched%20conditions%20and%20evaluate%20in%20extensive%20simulation%20and%20real-robot%20experiments.%20To%20improve%20the%20reliability%20of%20real-world%20results%2C%20we%20introduce%20a%20Grouped%20Blind%20Ensemble%20protocol%20that%20blinds%20operators%20to%20model%20identity%20and%20separates%20policy%20execution%20from%20outcome%20judgment%2C%20reducing%20experimenter%20bias.%20Our%20analysis%20targets%20three%20dimensions%20of%20VLA%20scaling.%20%281%29%20Physical%20alignment%3A%20we%20show%20that%20a%20unified%20end-effector%20%28EEF%29-relative%20action%20representation%20is%20critical%20for%20robust%20cross-embodiment%20transfer.%20%282%29%20Embodiment%20mixture%3A%20we%20find%20that%20naively%20pooling%20heterogeneous%20robot%20datasets%20often%20induces%20negative%20transfer%20rather%20than%20gains%2C%20underscoring%20the%20fragility%20of%20indiscriminate%20data%20scaling.%20%283%29%20Training%20regularization%3A%20we%20observe%20that%20intuitive%20strategies%2C%20such%20as%20sensory%20dropout%20and%20multi-stage%20fine-tuning%2C%20do%20not%20consistently%20improve%20performance%20at%20scale.%20Together%2C%20this%20study%20challenge%20some%20common%20assumptions%20about%20embodied%20scaling%20and%20provide%20practical%20guidance%20for%20training%20large-scale%20VLA%20policies%20from%20diverse%20robotic%20data.%20Project%20website%3A%20https%3A//research.beingbeyond.com/rethink_vla&entry.1838667208=http%3A//arxiv.org/abs/2602.09722v1&entry.124074799=Read"},
{"title": "Monocular Normal Estimation via Shading Sequence Estimation", "author": "Zongrui Li and Xinhua Ma and Minghui Hu and Yunqing Zhao and Yingchen Yu and Qian Zheng and Chang Liu and Xudong Jiang and Song Bai", "abstract": "Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.", "link": "http://arxiv.org/abs/2602.09929v1", "date": "2026-02-10", "relevancy": 2.223, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5596}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5577}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Normal%20Estimation%20via%20Shading%20Sequence%20Estimation&body=Title%3A%20Monocular%20Normal%20Estimation%20via%20Shading%20Sequence%20Estimation%0AAuthor%3A%20Zongrui%20Li%20and%20Xinhua%20Ma%20and%20Minghui%20Hu%20and%20Yunqing%20Zhao%20and%20Yingchen%20Yu%20and%20Qian%20Zheng%20and%20Chang%20Liu%20and%20Xudong%20Jiang%20and%20Song%20Bai%0AAbstract%3A%20Monocular%20normal%20estimation%20aims%20to%20estimate%20the%20normal%20map%20from%20a%20single%20RGB%20image%20of%20an%20object%20under%20arbitrary%20lights.%20Existing%20methods%20rely%20on%20deep%20models%20to%20directly%20predict%20normal%20maps.%20However%2C%20they%20often%20suffer%20from%203D%20misalignment%3A%20while%20the%20estimated%20normal%20maps%20may%20appear%20to%20have%20a%20correct%20appearance%2C%20the%20reconstructed%20surfaces%20often%20fail%20to%20align%20with%20the%20geometric%20details.%20We%20argue%20that%20this%20misalignment%20stems%20from%20the%20current%20paradigm%3A%20the%20model%20struggles%20to%20distinguish%20and%20reconstruct%20varying%20geometry%20represented%20in%20normal%20maps%2C%20as%20the%20differences%20in%20underlying%20geometry%20are%20reflected%20only%20through%20relatively%20subtle%20color%20variations.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20paradigm%20that%20reformulates%20normal%20estimation%20as%20shading%20sequence%20estimation%2C%20where%20shading%20sequences%20are%20more%20sensitive%20to%20various%20geometric%20information.%20Building%20on%20this%20paradigm%2C%20we%20present%20RoSE%2C%20a%20method%20that%20leverages%20image-to-video%20generative%20models%20to%20predict%20shading%20sequences.%20The%20predicted%20shading%20sequences%20are%20then%20converted%20into%20normal%20maps%20by%20solving%20a%20simple%20ordinary%20least-squares%20problem.%20To%20enhance%20robustness%20and%20better%20handle%20complex%20objects%2C%20RoSE%20is%20trained%20on%20a%20synthetic%20dataset%2C%20MultiShade%2C%20with%20diverse%20shapes%2C%20materials%2C%20and%20light%20conditions.%20Experiments%20demonstrate%20that%20RoSE%20achieves%20state-of-the-art%20performance%20on%20real-world%20benchmark%20datasets%20for%20object-based%20monocular%20normal%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Normal%2520Estimation%2520via%2520Shading%2520Sequence%2520Estimation%26entry.906535625%3DZongrui%2520Li%2520and%2520Xinhua%2520Ma%2520and%2520Minghui%2520Hu%2520and%2520Yunqing%2520Zhao%2520and%2520Yingchen%2520Yu%2520and%2520Qian%2520Zheng%2520and%2520Chang%2520Liu%2520and%2520Xudong%2520Jiang%2520and%2520Song%2520Bai%26entry.1292438233%3DMonocular%2520normal%2520estimation%2520aims%2520to%2520estimate%2520the%2520normal%2520map%2520from%2520a%2520single%2520RGB%2520image%2520of%2520an%2520object%2520under%2520arbitrary%2520lights.%2520Existing%2520methods%2520rely%2520on%2520deep%2520models%2520to%2520directly%2520predict%2520normal%2520maps.%2520However%252C%2520they%2520often%2520suffer%2520from%25203D%2520misalignment%253A%2520while%2520the%2520estimated%2520normal%2520maps%2520may%2520appear%2520to%2520have%2520a%2520correct%2520appearance%252C%2520the%2520reconstructed%2520surfaces%2520often%2520fail%2520to%2520align%2520with%2520the%2520geometric%2520details.%2520We%2520argue%2520that%2520this%2520misalignment%2520stems%2520from%2520the%2520current%2520paradigm%253A%2520the%2520model%2520struggles%2520to%2520distinguish%2520and%2520reconstruct%2520varying%2520geometry%2520represented%2520in%2520normal%2520maps%252C%2520as%2520the%2520differences%2520in%2520underlying%2520geometry%2520are%2520reflected%2520only%2520through%2520relatively%2520subtle%2520color%2520variations.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520new%2520paradigm%2520that%2520reformulates%2520normal%2520estimation%2520as%2520shading%2520sequence%2520estimation%252C%2520where%2520shading%2520sequences%2520are%2520more%2520sensitive%2520to%2520various%2520geometric%2520information.%2520Building%2520on%2520this%2520paradigm%252C%2520we%2520present%2520RoSE%252C%2520a%2520method%2520that%2520leverages%2520image-to-video%2520generative%2520models%2520to%2520predict%2520shading%2520sequences.%2520The%2520predicted%2520shading%2520sequences%2520are%2520then%2520converted%2520into%2520normal%2520maps%2520by%2520solving%2520a%2520simple%2520ordinary%2520least-squares%2520problem.%2520To%2520enhance%2520robustness%2520and%2520better%2520handle%2520complex%2520objects%252C%2520RoSE%2520is%2520trained%2520on%2520a%2520synthetic%2520dataset%252C%2520MultiShade%252C%2520with%2520diverse%2520shapes%252C%2520materials%252C%2520and%2520light%2520conditions.%2520Experiments%2520demonstrate%2520that%2520RoSE%2520achieves%2520state-of-the-art%2520performance%2520on%2520real-world%2520benchmark%2520datasets%2520for%2520object-based%2520monocular%2520normal%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Normal%20Estimation%20via%20Shading%20Sequence%20Estimation&entry.906535625=Zongrui%20Li%20and%20Xinhua%20Ma%20and%20Minghui%20Hu%20and%20Yunqing%20Zhao%20and%20Yingchen%20Yu%20and%20Qian%20Zheng%20and%20Chang%20Liu%20and%20Xudong%20Jiang%20and%20Song%20Bai&entry.1292438233=Monocular%20normal%20estimation%20aims%20to%20estimate%20the%20normal%20map%20from%20a%20single%20RGB%20image%20of%20an%20object%20under%20arbitrary%20lights.%20Existing%20methods%20rely%20on%20deep%20models%20to%20directly%20predict%20normal%20maps.%20However%2C%20they%20often%20suffer%20from%203D%20misalignment%3A%20while%20the%20estimated%20normal%20maps%20may%20appear%20to%20have%20a%20correct%20appearance%2C%20the%20reconstructed%20surfaces%20often%20fail%20to%20align%20with%20the%20geometric%20details.%20We%20argue%20that%20this%20misalignment%20stems%20from%20the%20current%20paradigm%3A%20the%20model%20struggles%20to%20distinguish%20and%20reconstruct%20varying%20geometry%20represented%20in%20normal%20maps%2C%20as%20the%20differences%20in%20underlying%20geometry%20are%20reflected%20only%20through%20relatively%20subtle%20color%20variations.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20paradigm%20that%20reformulates%20normal%20estimation%20as%20shading%20sequence%20estimation%2C%20where%20shading%20sequences%20are%20more%20sensitive%20to%20various%20geometric%20information.%20Building%20on%20this%20paradigm%2C%20we%20present%20RoSE%2C%20a%20method%20that%20leverages%20image-to-video%20generative%20models%20to%20predict%20shading%20sequences.%20The%20predicted%20shading%20sequences%20are%20then%20converted%20into%20normal%20maps%20by%20solving%20a%20simple%20ordinary%20least-squares%20problem.%20To%20enhance%20robustness%20and%20better%20handle%20complex%20objects%2C%20RoSE%20is%20trained%20on%20a%20synthetic%20dataset%2C%20MultiShade%2C%20with%20diverse%20shapes%2C%20materials%2C%20and%20light%20conditions.%20Experiments%20demonstrate%20that%20RoSE%20achieves%20state-of-the-art%20performance%20on%20real-world%20benchmark%20datasets%20for%20object-based%20monocular%20normal%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2602.09929v1&entry.124074799=Read"},
{"title": "Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving", "author": "Serin Varghese and Kevin Ross and Fabian Hueger and Kira Maag", "abstract": "Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.", "link": "http://arxiv.org/abs/2602.10052v1", "date": "2026-02-10", "relevancy": 2.21, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5711}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5658}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Attention%20for%20Consistent%20Video%20Semantic%20Segmentation%20in%20Automated%20Driving&body=Title%3A%20Spatio-Temporal%20Attention%20for%20Consistent%20Video%20Semantic%20Segmentation%20in%20Automated%20Driving%0AAuthor%3A%20Serin%20Varghese%20and%20Kevin%20Ross%20and%20Fabian%20Hueger%20and%20Kira%20Maag%0AAbstract%3A%20Deep%20neural%20networks%2C%20especially%20transformer-based%20architectures%2C%20have%20achieved%20remarkable%20success%20in%20semantic%20segmentation%20for%20environmental%20perception.%20However%2C%20existing%20models%20process%20video%20frames%20independently%2C%20thus%20failing%20to%20leverage%20temporal%20consistency%2C%20which%20could%20significantly%20improve%20both%20accuracy%20and%20stability%20in%20dynamic%20scenes.%20In%20this%20work%2C%20we%20propose%20a%20Spatio-Temporal%20Attention%20%28STA%29%20mechanism%20that%20extends%20transformer%20attention%20blocks%20to%20incorporate%20multi-frame%20context%2C%20enabling%20robust%20temporal%20feature%20representations%20for%20video%20semantic%20segmentation.%20Our%20approach%20modifies%20standard%20self-attention%20to%20process%20spatio-temporal%20feature%20sequences%20while%20maintaining%20computational%20efficiency%20and%20requiring%20minimal%20changes%20to%20existing%20architectures.%20STA%20demonstrates%20broad%20applicability%20across%20diverse%20transformer%20architectures%20and%20remains%20effective%20across%20both%20lightweight%20and%20larger-scale%20models.%20A%20comprehensive%20evaluation%20on%20the%20Cityscapes%20and%20BDD100k%20datasets%20shows%20substantial%20improvements%20of%209.20%20percentage%20points%20in%20temporal%20consistency%20metrics%20and%20up%20to%201.76%20percentage%20points%20in%20mean%20intersection%20over%20union%20compared%20to%20single-frame%20baselines.%20These%20results%20demonstrate%20STA%20as%20an%20effective%20architectural%20enhancement%20for%20video-based%20semantic%20segmentation%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520Attention%2520for%2520Consistent%2520Video%2520Semantic%2520Segmentation%2520in%2520Automated%2520Driving%26entry.906535625%3DSerin%2520Varghese%2520and%2520Kevin%2520Ross%2520and%2520Fabian%2520Hueger%2520and%2520Kira%2520Maag%26entry.1292438233%3DDeep%2520neural%2520networks%252C%2520especially%2520transformer-based%2520architectures%252C%2520have%2520achieved%2520remarkable%2520success%2520in%2520semantic%2520segmentation%2520for%2520environmental%2520perception.%2520However%252C%2520existing%2520models%2520process%2520video%2520frames%2520independently%252C%2520thus%2520failing%2520to%2520leverage%2520temporal%2520consistency%252C%2520which%2520could%2520significantly%2520improve%2520both%2520accuracy%2520and%2520stability%2520in%2520dynamic%2520scenes.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Spatio-Temporal%2520Attention%2520%2528STA%2529%2520mechanism%2520that%2520extends%2520transformer%2520attention%2520blocks%2520to%2520incorporate%2520multi-frame%2520context%252C%2520enabling%2520robust%2520temporal%2520feature%2520representations%2520for%2520video%2520semantic%2520segmentation.%2520Our%2520approach%2520modifies%2520standard%2520self-attention%2520to%2520process%2520spatio-temporal%2520feature%2520sequences%2520while%2520maintaining%2520computational%2520efficiency%2520and%2520requiring%2520minimal%2520changes%2520to%2520existing%2520architectures.%2520STA%2520demonstrates%2520broad%2520applicability%2520across%2520diverse%2520transformer%2520architectures%2520and%2520remains%2520effective%2520across%2520both%2520lightweight%2520and%2520larger-scale%2520models.%2520A%2520comprehensive%2520evaluation%2520on%2520the%2520Cityscapes%2520and%2520BDD100k%2520datasets%2520shows%2520substantial%2520improvements%2520of%25209.20%2520percentage%2520points%2520in%2520temporal%2520consistency%2520metrics%2520and%2520up%2520to%25201.76%2520percentage%2520points%2520in%2520mean%2520intersection%2520over%2520union%2520compared%2520to%2520single-frame%2520baselines.%2520These%2520results%2520demonstrate%2520STA%2520as%2520an%2520effective%2520architectural%2520enhancement%2520for%2520video-based%2520semantic%2520segmentation%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Attention%20for%20Consistent%20Video%20Semantic%20Segmentation%20in%20Automated%20Driving&entry.906535625=Serin%20Varghese%20and%20Kevin%20Ross%20and%20Fabian%20Hueger%20and%20Kira%20Maag&entry.1292438233=Deep%20neural%20networks%2C%20especially%20transformer-based%20architectures%2C%20have%20achieved%20remarkable%20success%20in%20semantic%20segmentation%20for%20environmental%20perception.%20However%2C%20existing%20models%20process%20video%20frames%20independently%2C%20thus%20failing%20to%20leverage%20temporal%20consistency%2C%20which%20could%20significantly%20improve%20both%20accuracy%20and%20stability%20in%20dynamic%20scenes.%20In%20this%20work%2C%20we%20propose%20a%20Spatio-Temporal%20Attention%20%28STA%29%20mechanism%20that%20extends%20transformer%20attention%20blocks%20to%20incorporate%20multi-frame%20context%2C%20enabling%20robust%20temporal%20feature%20representations%20for%20video%20semantic%20segmentation.%20Our%20approach%20modifies%20standard%20self-attention%20to%20process%20spatio-temporal%20feature%20sequences%20while%20maintaining%20computational%20efficiency%20and%20requiring%20minimal%20changes%20to%20existing%20architectures.%20STA%20demonstrates%20broad%20applicability%20across%20diverse%20transformer%20architectures%20and%20remains%20effective%20across%20both%20lightweight%20and%20larger-scale%20models.%20A%20comprehensive%20evaluation%20on%20the%20Cityscapes%20and%20BDD100k%20datasets%20shows%20substantial%20improvements%20of%209.20%20percentage%20points%20in%20temporal%20consistency%20metrics%20and%20up%20to%201.76%20percentage%20points%20in%20mean%20intersection%20over%20union%20compared%20to%20single-frame%20baselines.%20These%20results%20demonstrate%20STA%20as%20an%20effective%20architectural%20enhancement%20for%20video-based%20semantic%20segmentation%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.10052v1&entry.124074799=Read"},
{"title": "Position: Message-passing and spectral GNNs are two sides of the same coin", "author": "Antonis Vasileiou and Juan Cervino and Pascal Frossard and Charilaos I. Kanatsoulis and Christopher Morris and Michael T. Schaub and Pierre Vandergheynst and Zhiyang Wang and Guy Wolf and Ron Levie", "abstract": "Graph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as different parametrizations of permutation-equivariant operators acting on graph signals. From this perspective, many popular architectures are equivalent in expressive power, while genuine gaps arise only in specific regimes. We further argue that MPNNs and spectral GNNs offer complementary strengths. That is, MPNNs provide a natural language for discrete structure and expressivity analysis using tools from logic and graph isomorphism research, while the spectral perspective provides principled tools for understanding smoothing, bottlenecks, stability, and community structure. Overall, we posit that progress in graph learning will be accelerated by clearly understanding the key similarities and differences between these two types of GNNs, and by working towards unifying these perspectives within a common theoretical and conceptual framework rather than treating them as competing paradigms.", "link": "http://arxiv.org/abs/2602.10031v1", "date": "2026-02-10", "relevancy": 2.2064, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4466}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4454}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Message-passing%20and%20spectral%20GNNs%20are%20two%20sides%20of%20the%20same%20coin&body=Title%3A%20Position%3A%20Message-passing%20and%20spectral%20GNNs%20are%20two%20sides%20of%20the%20same%20coin%0AAuthor%3A%20Antonis%20Vasileiou%20and%20Juan%20Cervino%20and%20Pascal%20Frossard%20and%20Charilaos%20I.%20Kanatsoulis%20and%20Christopher%20Morris%20and%20Michael%20T.%20Schaub%20and%20Pierre%20Vandergheynst%20and%20Zhiyang%20Wang%20and%20Guy%20Wolf%20and%20Ron%20Levie%0AAbstract%3A%20Graph%20neural%20networks%20%28GNNs%29%20are%20commonly%20divided%20into%20message-passing%20neural%20networks%20%28MPNNs%29%20and%20spectral%20graph%20neural%20networks%2C%20reflecting%20two%20largely%20separate%20research%20traditions%20in%20machine%20learning%20and%20signal%20processing.%20This%20paper%20argues%20that%20this%20divide%20is%20mostly%20artificial%2C%20hindering%20progress%20in%20the%20field.%20We%20propose%20a%20viewpoint%20in%20which%20both%20MPNNs%20and%20spectral%20GNNs%20are%20understood%20as%20different%20parametrizations%20of%20permutation-equivariant%20operators%20acting%20on%20graph%20signals.%20From%20this%20perspective%2C%20many%20popular%20architectures%20are%20equivalent%20in%20expressive%20power%2C%20while%20genuine%20gaps%20arise%20only%20in%20specific%20regimes.%20We%20further%20argue%20that%20MPNNs%20and%20spectral%20GNNs%20offer%20complementary%20strengths.%20That%20is%2C%20MPNNs%20provide%20a%20natural%20language%20for%20discrete%20structure%20and%20expressivity%20analysis%20using%20tools%20from%20logic%20and%20graph%20isomorphism%20research%2C%20while%20the%20spectral%20perspective%20provides%20principled%20tools%20for%20understanding%20smoothing%2C%20bottlenecks%2C%20stability%2C%20and%20community%20structure.%20Overall%2C%20we%20posit%20that%20progress%20in%20graph%20learning%20will%20be%20accelerated%20by%20clearly%20understanding%20the%20key%20similarities%20and%20differences%20between%20these%20two%20types%20of%20GNNs%2C%20and%20by%20working%20towards%20unifying%20these%20perspectives%20within%20a%20common%20theoretical%20and%20conceptual%20framework%20rather%20than%20treating%20them%20as%20competing%20paradigms.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Message-passing%2520and%2520spectral%2520GNNs%2520are%2520two%2520sides%2520of%2520the%2520same%2520coin%26entry.906535625%3DAntonis%2520Vasileiou%2520and%2520Juan%2520Cervino%2520and%2520Pascal%2520Frossard%2520and%2520Charilaos%2520I.%2520Kanatsoulis%2520and%2520Christopher%2520Morris%2520and%2520Michael%2520T.%2520Schaub%2520and%2520Pierre%2520Vandergheynst%2520and%2520Zhiyang%2520Wang%2520and%2520Guy%2520Wolf%2520and%2520Ron%2520Levie%26entry.1292438233%3DGraph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520commonly%2520divided%2520into%2520message-passing%2520neural%2520networks%2520%2528MPNNs%2529%2520and%2520spectral%2520graph%2520neural%2520networks%252C%2520reflecting%2520two%2520largely%2520separate%2520research%2520traditions%2520in%2520machine%2520learning%2520and%2520signal%2520processing.%2520This%2520paper%2520argues%2520that%2520this%2520divide%2520is%2520mostly%2520artificial%252C%2520hindering%2520progress%2520in%2520the%2520field.%2520We%2520propose%2520a%2520viewpoint%2520in%2520which%2520both%2520MPNNs%2520and%2520spectral%2520GNNs%2520are%2520understood%2520as%2520different%2520parametrizations%2520of%2520permutation-equivariant%2520operators%2520acting%2520on%2520graph%2520signals.%2520From%2520this%2520perspective%252C%2520many%2520popular%2520architectures%2520are%2520equivalent%2520in%2520expressive%2520power%252C%2520while%2520genuine%2520gaps%2520arise%2520only%2520in%2520specific%2520regimes.%2520We%2520further%2520argue%2520that%2520MPNNs%2520and%2520spectral%2520GNNs%2520offer%2520complementary%2520strengths.%2520That%2520is%252C%2520MPNNs%2520provide%2520a%2520natural%2520language%2520for%2520discrete%2520structure%2520and%2520expressivity%2520analysis%2520using%2520tools%2520from%2520logic%2520and%2520graph%2520isomorphism%2520research%252C%2520while%2520the%2520spectral%2520perspective%2520provides%2520principled%2520tools%2520for%2520understanding%2520smoothing%252C%2520bottlenecks%252C%2520stability%252C%2520and%2520community%2520structure.%2520Overall%252C%2520we%2520posit%2520that%2520progress%2520in%2520graph%2520learning%2520will%2520be%2520accelerated%2520by%2520clearly%2520understanding%2520the%2520key%2520similarities%2520and%2520differences%2520between%2520these%2520two%2520types%2520of%2520GNNs%252C%2520and%2520by%2520working%2520towards%2520unifying%2520these%2520perspectives%2520within%2520a%2520common%2520theoretical%2520and%2520conceptual%2520framework%2520rather%2520than%2520treating%2520them%2520as%2520competing%2520paradigms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Message-passing%20and%20spectral%20GNNs%20are%20two%20sides%20of%20the%20same%20coin&entry.906535625=Antonis%20Vasileiou%20and%20Juan%20Cervino%20and%20Pascal%20Frossard%20and%20Charilaos%20I.%20Kanatsoulis%20and%20Christopher%20Morris%20and%20Michael%20T.%20Schaub%20and%20Pierre%20Vandergheynst%20and%20Zhiyang%20Wang%20and%20Guy%20Wolf%20and%20Ron%20Levie&entry.1292438233=Graph%20neural%20networks%20%28GNNs%29%20are%20commonly%20divided%20into%20message-passing%20neural%20networks%20%28MPNNs%29%20and%20spectral%20graph%20neural%20networks%2C%20reflecting%20two%20largely%20separate%20research%20traditions%20in%20machine%20learning%20and%20signal%20processing.%20This%20paper%20argues%20that%20this%20divide%20is%20mostly%20artificial%2C%20hindering%20progress%20in%20the%20field.%20We%20propose%20a%20viewpoint%20in%20which%20both%20MPNNs%20and%20spectral%20GNNs%20are%20understood%20as%20different%20parametrizations%20of%20permutation-equivariant%20operators%20acting%20on%20graph%20signals.%20From%20this%20perspective%2C%20many%20popular%20architectures%20are%20equivalent%20in%20expressive%20power%2C%20while%20genuine%20gaps%20arise%20only%20in%20specific%20regimes.%20We%20further%20argue%20that%20MPNNs%20and%20spectral%20GNNs%20offer%20complementary%20strengths.%20That%20is%2C%20MPNNs%20provide%20a%20natural%20language%20for%20discrete%20structure%20and%20expressivity%20analysis%20using%20tools%20from%20logic%20and%20graph%20isomorphism%20research%2C%20while%20the%20spectral%20perspective%20provides%20principled%20tools%20for%20understanding%20smoothing%2C%20bottlenecks%2C%20stability%2C%20and%20community%20structure.%20Overall%2C%20we%20posit%20that%20progress%20in%20graph%20learning%20will%20be%20accelerated%20by%20clearly%20understanding%20the%20key%20similarities%20and%20differences%20between%20these%20two%20types%20of%20GNNs%2C%20and%20by%20working%20towards%20unifying%20these%20perspectives%20within%20a%20common%20theoretical%20and%20conceptual%20framework%20rather%20than%20treating%20them%20as%20competing%20paradigms.&entry.1838667208=http%3A//arxiv.org/abs/2602.10031v1&entry.124074799=Read"},
{"title": "Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI", "author": "Gaurang Sharma and Harri Polonen and Juha Pajula and Jutta Suksi and Jussi Tohka", "abstract": "Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.\n  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.", "link": "http://arxiv.org/abs/2602.10043v1", "date": "2026-02-10", "relevancy": 2.2028, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4416}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4401}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Image%20Processing%20and%20Similarity%20Measures%20Can%20Link%20Data%20Samples%20across%20Databases%20through%20Brain%20MRI&body=Title%3A%20Simple%20Image%20Processing%20and%20Similarity%20Measures%20Can%20Link%20Data%20Samples%20across%20Databases%20through%20Brain%20MRI%0AAuthor%3A%20Gaurang%20Sharma%20and%20Harri%20Polonen%20and%20Juha%20Pajula%20and%20Jutta%20Suksi%20and%20Jussi%20Tohka%0AAbstract%3A%20Head%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20routinely%20collected%20and%20shared%20for%20research%20under%20strict%20regulatory%20frameworks.%20These%20frameworks%20require%20removing%20potential%20identifiers%20before%20sharing.%20But%2C%20even%20after%20skull%20stripping%2C%20the%20brain%20parenchyma%20contains%20unique%20signatures%20that%20can%20match%20other%20MRIs%20from%20the%20same%20participants%20across%20databases%2C%20posing%20a%20privacy%20risk%20if%20additional%20data%20features%20are%20available.%20Current%20regulatory%20frameworks%20often%20mandate%20evaluating%20such%20risks%20based%20on%20the%20assessment%20of%20a%20certain%20level%20of%20reasonableness.%20Prior%20studies%20have%20already%20suggested%20that%20a%20brain%20MRI%20could%20enable%20participant%20linkage%2C%20but%20they%20have%20relied%20on%20training-based%20or%20computationally%20intensive%20methods.%0A%20%20Here%2C%20we%20demonstrate%20that%20linking%20an%20individual%27s%20skull-stripped%20T1-weighted%20MRI%2C%20which%20may%20lead%20to%20re-identification%20if%20other%20identifiers%20are%20available%2C%20is%20possible%20using%20standard%20preprocessing%20followed%20by%20image%20similarity%20computation.%20Nearly%20perfect%20linkage%20accuracy%20was%20achieved%20in%20matching%20data%20samples%20across%20various%20time%20intervals%2C%20scanner%20types%2C%20spatial%20resolutions%2C%20and%20acquisition%20protocols%2C%20despite%20potential%20cognitive%20decline%2C%20simulating%20MRI%20matching%20across%20databases.%20These%20results%20aim%20to%20contribute%20meaningfully%20to%20the%20development%20of%20thoughtful%2C%20forward-looking%20policies%20in%20medical%20data%20sharing.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Image%2520Processing%2520and%2520Similarity%2520Measures%2520Can%2520Link%2520Data%2520Samples%2520across%2520Databases%2520through%2520Brain%2520MRI%26entry.906535625%3DGaurang%2520Sharma%2520and%2520Harri%2520Polonen%2520and%2520Juha%2520Pajula%2520and%2520Jutta%2520Suksi%2520and%2520Jussi%2520Tohka%26entry.1292438233%3DHead%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520is%2520routinely%2520collected%2520and%2520shared%2520for%2520research%2520under%2520strict%2520regulatory%2520frameworks.%2520These%2520frameworks%2520require%2520removing%2520potential%2520identifiers%2520before%2520sharing.%2520But%252C%2520even%2520after%2520skull%2520stripping%252C%2520the%2520brain%2520parenchyma%2520contains%2520unique%2520signatures%2520that%2520can%2520match%2520other%2520MRIs%2520from%2520the%2520same%2520participants%2520across%2520databases%252C%2520posing%2520a%2520privacy%2520risk%2520if%2520additional%2520data%2520features%2520are%2520available.%2520Current%2520regulatory%2520frameworks%2520often%2520mandate%2520evaluating%2520such%2520risks%2520based%2520on%2520the%2520assessment%2520of%2520a%2520certain%2520level%2520of%2520reasonableness.%2520Prior%2520studies%2520have%2520already%2520suggested%2520that%2520a%2520brain%2520MRI%2520could%2520enable%2520participant%2520linkage%252C%2520but%2520they%2520have%2520relied%2520on%2520training-based%2520or%2520computationally%2520intensive%2520methods.%250A%2520%2520Here%252C%2520we%2520demonstrate%2520that%2520linking%2520an%2520individual%2527s%2520skull-stripped%2520T1-weighted%2520MRI%252C%2520which%2520may%2520lead%2520to%2520re-identification%2520if%2520other%2520identifiers%2520are%2520available%252C%2520is%2520possible%2520using%2520standard%2520preprocessing%2520followed%2520by%2520image%2520similarity%2520computation.%2520Nearly%2520perfect%2520linkage%2520accuracy%2520was%2520achieved%2520in%2520matching%2520data%2520samples%2520across%2520various%2520time%2520intervals%252C%2520scanner%2520types%252C%2520spatial%2520resolutions%252C%2520and%2520acquisition%2520protocols%252C%2520despite%2520potential%2520cognitive%2520decline%252C%2520simulating%2520MRI%2520matching%2520across%2520databases.%2520These%2520results%2520aim%2520to%2520contribute%2520meaningfully%2520to%2520the%2520development%2520of%2520thoughtful%252C%2520forward-looking%2520policies%2520in%2520medical%2520data%2520sharing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Image%20Processing%20and%20Similarity%20Measures%20Can%20Link%20Data%20Samples%20across%20Databases%20through%20Brain%20MRI&entry.906535625=Gaurang%20Sharma%20and%20Harri%20Polonen%20and%20Juha%20Pajula%20and%20Jutta%20Suksi%20and%20Jussi%20Tohka&entry.1292438233=Head%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20routinely%20collected%20and%20shared%20for%20research%20under%20strict%20regulatory%20frameworks.%20These%20frameworks%20require%20removing%20potential%20identifiers%20before%20sharing.%20But%2C%20even%20after%20skull%20stripping%2C%20the%20brain%20parenchyma%20contains%20unique%20signatures%20that%20can%20match%20other%20MRIs%20from%20the%20same%20participants%20across%20databases%2C%20posing%20a%20privacy%20risk%20if%20additional%20data%20features%20are%20available.%20Current%20regulatory%20frameworks%20often%20mandate%20evaluating%20such%20risks%20based%20on%20the%20assessment%20of%20a%20certain%20level%20of%20reasonableness.%20Prior%20studies%20have%20already%20suggested%20that%20a%20brain%20MRI%20could%20enable%20participant%20linkage%2C%20but%20they%20have%20relied%20on%20training-based%20or%20computationally%20intensive%20methods.%0A%20%20Here%2C%20we%20demonstrate%20that%20linking%20an%20individual%27s%20skull-stripped%20T1-weighted%20MRI%2C%20which%20may%20lead%20to%20re-identification%20if%20other%20identifiers%20are%20available%2C%20is%20possible%20using%20standard%20preprocessing%20followed%20by%20image%20similarity%20computation.%20Nearly%20perfect%20linkage%20accuracy%20was%20achieved%20in%20matching%20data%20samples%20across%20various%20time%20intervals%2C%20scanner%20types%2C%20spatial%20resolutions%2C%20and%20acquisition%20protocols%2C%20despite%20potential%20cognitive%20decline%2C%20simulating%20MRI%20matching%20across%20databases.%20These%20results%20aim%20to%20contribute%20meaningfully%20to%20the%20development%20of%20thoughtful%2C%20forward-looking%20policies%20in%20medical%20data%20sharing.&entry.1838667208=http%3A//arxiv.org/abs/2602.10043v1&entry.124074799=Read"},
{"title": "CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection", "author": "Fatemeh Nejati and Mahdi Rabbani and Morteza Eskandarian and Mansur Mirani and Gunjan Piya and Igor Opushnyev and Ali A. Ghorbani and Sajjad Dadkhah", "abstract": "Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.", "link": "http://arxiv.org/abs/2602.09015v2", "date": "2026-02-10", "relevancy": 2.2012, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.454}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4373}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CIC-Trap4Phish%3A%20A%20Unified%20Multi-Format%20Dataset%20for%20Phishing%20and%20Quishing%20Attachment%20Detection&body=Title%3A%20CIC-Trap4Phish%3A%20A%20Unified%20Multi-Format%20Dataset%20for%20Phishing%20and%20Quishing%20Attachment%20Detection%0AAuthor%3A%20Fatemeh%20Nejati%20and%20Mahdi%20Rabbani%20and%20Morteza%20Eskandarian%20and%20Mansur%20Mirani%20and%20Gunjan%20Piya%20and%20Igor%20Opushnyev%20and%20Ali%20A.%20Ghorbani%20and%20Sajjad%20Dadkhah%0AAbstract%3A%20Phishing%20attacks%20represents%20one%20of%20the%20primary%20attack%20methods%20which%20is%20used%20by%20cyber%20attackers.%20In%20many%20cases%2C%20attackers%20use%20deceptive%20emails%20along%20with%20malicious%20attachments%20to%20trick%20users%20into%20giving%20away%20sensitive%20information%20or%20installing%20malware%20while%20compromising%20entire%20systems.%20The%20flexibility%20of%20malicious%20email%20attachments%20makes%20them%20stand%20out%20as%20a%20preferred%20vector%20for%20attackers%20as%20they%20can%20embed%20harmful%20content%20such%20as%20malware%20or%20malicious%20URLs%20inside%20standard%20document%20formats.%20Although%20phishing%20email%20defenses%20have%20improved%20a%20lot%2C%20attackers%20continue%20to%20abuse%20attachments%2C%20enabling%20malicious%20content%20to%20bypass%20security%20measures.%20Moreover%2C%20another%20challenge%20that%20researches%20face%20in%20training%20advance%20models%2C%20is%20lack%20of%20an%20unified%20and%20comprehensive%20dataset%20that%20covers%20the%20most%20prevalent%20data%20types.%20To%20address%20this%20gap%2C%20we%20generated%20CIC-Trap4Phish%2C%20a%20multi-format%20dataset%20containing%20both%20malicious%20and%20benign%20samples%20across%20five%20categories%20commonly%20used%20in%20phishing%20campaigns%3A%20Microsoft%20Word%20documents%2C%20Excel%20spreadsheets%2C%20PDF%20files%2C%20HTML%20pages%2C%20and%20QR%20code%20images.%20For%20the%20first%20four%20file%20types%2C%20a%20set%20of%20execution-free%20static%20feature%20pipeline%20was%20proposed%2C%20designed%20to%20capture%20structural%2C%20lexical%2C%20and%20metadata-based%20indicators%20without%20the%20need%20to%20open%20or%20execute%20files.%20Feature%20selection%20was%20performed%20using%20a%20combination%20of%20SHAP%20analysis%20and%20feature%20importance%2C%20yielding%20compact%2C%20discriminative%20feature%20subsets%20for%20each%20file%20type.%20The%20selected%20features%20were%20evaluated%20by%20using%20lightweight%20machine%20learning%20models%2C%20including%20Random%20Forest%2C%20XGBoost%2C%20and%20Decision%20Tree.%20All%20models%20demonstrate%20high%20detection%20accuracy%20across%20formats.%20For%20QR%20code-based%20phishing%20%28quishing%29%2C%20two%20complementary%20methods%20were%20implemented%3A%20image-based%20detection%20by%20employing%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20lexical%20analysis%20of%20decoded%20URLs%20using%20recent%20lightweight%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCIC-Trap4Phish%253A%2520A%2520Unified%2520Multi-Format%2520Dataset%2520for%2520Phishing%2520and%2520Quishing%2520Attachment%2520Detection%26entry.906535625%3DFatemeh%2520Nejati%2520and%2520Mahdi%2520Rabbani%2520and%2520Morteza%2520Eskandarian%2520and%2520Mansur%2520Mirani%2520and%2520Gunjan%2520Piya%2520and%2520Igor%2520Opushnyev%2520and%2520Ali%2520A.%2520Ghorbani%2520and%2520Sajjad%2520Dadkhah%26entry.1292438233%3DPhishing%2520attacks%2520represents%2520one%2520of%2520the%2520primary%2520attack%2520methods%2520which%2520is%2520used%2520by%2520cyber%2520attackers.%2520In%2520many%2520cases%252C%2520attackers%2520use%2520deceptive%2520emails%2520along%2520with%2520malicious%2520attachments%2520to%2520trick%2520users%2520into%2520giving%2520away%2520sensitive%2520information%2520or%2520installing%2520malware%2520while%2520compromising%2520entire%2520systems.%2520The%2520flexibility%2520of%2520malicious%2520email%2520attachments%2520makes%2520them%2520stand%2520out%2520as%2520a%2520preferred%2520vector%2520for%2520attackers%2520as%2520they%2520can%2520embed%2520harmful%2520content%2520such%2520as%2520malware%2520or%2520malicious%2520URLs%2520inside%2520standard%2520document%2520formats.%2520Although%2520phishing%2520email%2520defenses%2520have%2520improved%2520a%2520lot%252C%2520attackers%2520continue%2520to%2520abuse%2520attachments%252C%2520enabling%2520malicious%2520content%2520to%2520bypass%2520security%2520measures.%2520Moreover%252C%2520another%2520challenge%2520that%2520researches%2520face%2520in%2520training%2520advance%2520models%252C%2520is%2520lack%2520of%2520an%2520unified%2520and%2520comprehensive%2520dataset%2520that%2520covers%2520the%2520most%2520prevalent%2520data%2520types.%2520To%2520address%2520this%2520gap%252C%2520we%2520generated%2520CIC-Trap4Phish%252C%2520a%2520multi-format%2520dataset%2520containing%2520both%2520malicious%2520and%2520benign%2520samples%2520across%2520five%2520categories%2520commonly%2520used%2520in%2520phishing%2520campaigns%253A%2520Microsoft%2520Word%2520documents%252C%2520Excel%2520spreadsheets%252C%2520PDF%2520files%252C%2520HTML%2520pages%252C%2520and%2520QR%2520code%2520images.%2520For%2520the%2520first%2520four%2520file%2520types%252C%2520a%2520set%2520of%2520execution-free%2520static%2520feature%2520pipeline%2520was%2520proposed%252C%2520designed%2520to%2520capture%2520structural%252C%2520lexical%252C%2520and%2520metadata-based%2520indicators%2520without%2520the%2520need%2520to%2520open%2520or%2520execute%2520files.%2520Feature%2520selection%2520was%2520performed%2520using%2520a%2520combination%2520of%2520SHAP%2520analysis%2520and%2520feature%2520importance%252C%2520yielding%2520compact%252C%2520discriminative%2520feature%2520subsets%2520for%2520each%2520file%2520type.%2520The%2520selected%2520features%2520were%2520evaluated%2520by%2520using%2520lightweight%2520machine%2520learning%2520models%252C%2520including%2520Random%2520Forest%252C%2520XGBoost%252C%2520and%2520Decision%2520Tree.%2520All%2520models%2520demonstrate%2520high%2520detection%2520accuracy%2520across%2520formats.%2520For%2520QR%2520code-based%2520phishing%2520%2528quishing%2529%252C%2520two%2520complementary%2520methods%2520were%2520implemented%253A%2520image-based%2520detection%2520by%2520employing%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520lexical%2520analysis%2520of%2520decoded%2520URLs%2520using%2520recent%2520lightweight%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CIC-Trap4Phish%3A%20A%20Unified%20Multi-Format%20Dataset%20for%20Phishing%20and%20Quishing%20Attachment%20Detection&entry.906535625=Fatemeh%20Nejati%20and%20Mahdi%20Rabbani%20and%20Morteza%20Eskandarian%20and%20Mansur%20Mirani%20and%20Gunjan%20Piya%20and%20Igor%20Opushnyev%20and%20Ali%20A.%20Ghorbani%20and%20Sajjad%20Dadkhah&entry.1292438233=Phishing%20attacks%20represents%20one%20of%20the%20primary%20attack%20methods%20which%20is%20used%20by%20cyber%20attackers.%20In%20many%20cases%2C%20attackers%20use%20deceptive%20emails%20along%20with%20malicious%20attachments%20to%20trick%20users%20into%20giving%20away%20sensitive%20information%20or%20installing%20malware%20while%20compromising%20entire%20systems.%20The%20flexibility%20of%20malicious%20email%20attachments%20makes%20them%20stand%20out%20as%20a%20preferred%20vector%20for%20attackers%20as%20they%20can%20embed%20harmful%20content%20such%20as%20malware%20or%20malicious%20URLs%20inside%20standard%20document%20formats.%20Although%20phishing%20email%20defenses%20have%20improved%20a%20lot%2C%20attackers%20continue%20to%20abuse%20attachments%2C%20enabling%20malicious%20content%20to%20bypass%20security%20measures.%20Moreover%2C%20another%20challenge%20that%20researches%20face%20in%20training%20advance%20models%2C%20is%20lack%20of%20an%20unified%20and%20comprehensive%20dataset%20that%20covers%20the%20most%20prevalent%20data%20types.%20To%20address%20this%20gap%2C%20we%20generated%20CIC-Trap4Phish%2C%20a%20multi-format%20dataset%20containing%20both%20malicious%20and%20benign%20samples%20across%20five%20categories%20commonly%20used%20in%20phishing%20campaigns%3A%20Microsoft%20Word%20documents%2C%20Excel%20spreadsheets%2C%20PDF%20files%2C%20HTML%20pages%2C%20and%20QR%20code%20images.%20For%20the%20first%20four%20file%20types%2C%20a%20set%20of%20execution-free%20static%20feature%20pipeline%20was%20proposed%2C%20designed%20to%20capture%20structural%2C%20lexical%2C%20and%20metadata-based%20indicators%20without%20the%20need%20to%20open%20or%20execute%20files.%20Feature%20selection%20was%20performed%20using%20a%20combination%20of%20SHAP%20analysis%20and%20feature%20importance%2C%20yielding%20compact%2C%20discriminative%20feature%20subsets%20for%20each%20file%20type.%20The%20selected%20features%20were%20evaluated%20by%20using%20lightweight%20machine%20learning%20models%2C%20including%20Random%20Forest%2C%20XGBoost%2C%20and%20Decision%20Tree.%20All%20models%20demonstrate%20high%20detection%20accuracy%20across%20formats.%20For%20QR%20code-based%20phishing%20%28quishing%29%2C%20two%20complementary%20methods%20were%20implemented%3A%20image-based%20detection%20by%20employing%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20lexical%20analysis%20of%20decoded%20URLs%20using%20recent%20lightweight%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.09015v2&entry.124074799=Read"},
{"title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models", "author": "Jinhui Ye and Fangjing Wang and Ning Gao and Junqiu Yu and Yangkun Zhu and Bin Wang and Jinyu Zhang and Weiyang Jin and Yanwei Fu and Feng Zheng and Yilun Chen and Jiangmiao Pang", "abstract": "Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/", "link": "http://arxiv.org/abs/2602.10109v1", "date": "2026-02-10", "relevancy": 2.1946, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ST4VLA%3A%20Spatially%20Guided%20Training%20for%20Vision-Language-Action%20Models&body=Title%3A%20ST4VLA%3A%20Spatially%20Guided%20Training%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Jinhui%20Ye%20and%20Fangjing%20Wang%20and%20Ning%20Gao%20and%20Junqiu%20Yu%20and%20Yangkun%20Zhu%20and%20Bin%20Wang%20and%20Jinyu%20Zhang%20and%20Weiyang%20Jin%20and%20Yanwei%20Fu%20and%20Feng%20Zheng%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20excel%20at%20multimodal%20understanding%20but%20fall%20short%20when%20extended%20to%20embodied%20tasks%2C%20where%20instructions%20must%20be%20transformed%20into%20low-level%20motor%20actions.%20We%20introduce%20ST4VLA%2C%20a%20dual-system%20Vision-Language-Action%20framework%20that%20leverages%20Spatial%20Guided%20Training%20to%20align%20action%20learning%20with%20spatial%20priors%20in%20VLMs.%20ST4VLA%20includes%20two%20stages%3A%20%28i%29%20spatial%20grounding%20pre-training%2C%20which%20equips%20the%20VLM%20with%20transferable%20priors%20via%20scalable%20point%2C%20box%2C%20and%20trajectory%20prediction%20from%20both%20web-scale%20and%20robot-specific%20data%2C%20and%20%28ii%29%20spatially%20guided%20action%20post-training%2C%20which%20encourages%20the%20model%20to%20produce%20richer%20spatial%20priors%20to%20guide%20action%20generation%20via%20spatial%20prompting.%20This%20design%20preserves%20spatial%20grounding%20during%20policy%20learning%20and%20promotes%20consistent%20optimization%20across%20spatial%20and%20action%20objectives.%20Empirically%2C%20ST4VLA%20achieves%20substantial%20improvements%20over%20vanilla%20VLA%2C%20with%20performance%20increasing%20from%2066.1%20-%3E%2084.6%20on%20Google%20Robot%20and%20from%2054.7%20-%3E%2073.2%20on%20WidowX%20Robot%2C%20establishing%20new%20state-of-the-art%20results%20on%20SimplerEnv.%20It%20also%20demonstrates%20stronger%20generalization%20to%20unseen%20objects%20and%20paraphrased%20instructions%2C%20as%20well%20as%20robustness%20to%20long-horizon%20perturbations%20in%20real-world%20settings.%20These%20results%20highlight%20scalable%20spatially%20guided%20training%20as%20a%20promising%20direction%20for%20robust%2C%20generalizable%20robot%20learning.%20Source%20code%2C%20data%20and%20models%20are%20released%20at%20https%3A//internrobotics.github.io/internvla-m1.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2602.10109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DST4VLA%253A%2520Spatially%2520Guided%2520Training%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DJinhui%2520Ye%2520and%2520Fangjing%2520Wang%2520and%2520Ning%2520Gao%2520and%2520Junqiu%2520Yu%2520and%2520Yangkun%2520Zhu%2520and%2520Bin%2520Wang%2520and%2520Jinyu%2520Zhang%2520and%2520Weiyang%2520Jin%2520and%2520Yanwei%2520Fu%2520and%2520Feng%2520Zheng%2520and%2520Yilun%2520Chen%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520excel%2520at%2520multimodal%2520understanding%2520but%2520fall%2520short%2520when%2520extended%2520to%2520embodied%2520tasks%252C%2520where%2520instructions%2520must%2520be%2520transformed%2520into%2520low-level%2520motor%2520actions.%2520We%2520introduce%2520ST4VLA%252C%2520a%2520dual-system%2520Vision-Language-Action%2520framework%2520that%2520leverages%2520Spatial%2520Guided%2520Training%2520to%2520align%2520action%2520learning%2520with%2520spatial%2520priors%2520in%2520VLMs.%2520ST4VLA%2520includes%2520two%2520stages%253A%2520%2528i%2529%2520spatial%2520grounding%2520pre-training%252C%2520which%2520equips%2520the%2520VLM%2520with%2520transferable%2520priors%2520via%2520scalable%2520point%252C%2520box%252C%2520and%2520trajectory%2520prediction%2520from%2520both%2520web-scale%2520and%2520robot-specific%2520data%252C%2520and%2520%2528ii%2529%2520spatially%2520guided%2520action%2520post-training%252C%2520which%2520encourages%2520the%2520model%2520to%2520produce%2520richer%2520spatial%2520priors%2520to%2520guide%2520action%2520generation%2520via%2520spatial%2520prompting.%2520This%2520design%2520preserves%2520spatial%2520grounding%2520during%2520policy%2520learning%2520and%2520promotes%2520consistent%2520optimization%2520across%2520spatial%2520and%2520action%2520objectives.%2520Empirically%252C%2520ST4VLA%2520achieves%2520substantial%2520improvements%2520over%2520vanilla%2520VLA%252C%2520with%2520performance%2520increasing%2520from%252066.1%2520-%253E%252084.6%2520on%2520Google%2520Robot%2520and%2520from%252054.7%2520-%253E%252073.2%2520on%2520WidowX%2520Robot%252C%2520establishing%2520new%2520state-of-the-art%2520results%2520on%2520SimplerEnv.%2520It%2520also%2520demonstrates%2520stronger%2520generalization%2520to%2520unseen%2520objects%2520and%2520paraphrased%2520instructions%252C%2520as%2520well%2520as%2520robustness%2520to%2520long-horizon%2520perturbations%2520in%2520real-world%2520settings.%2520These%2520results%2520highlight%2520scalable%2520spatially%2520guided%2520training%2520as%2520a%2520promising%2520direction%2520for%2520robust%252C%2520generalizable%2520robot%2520learning.%2520Source%2520code%252C%2520data%2520and%2520models%2520are%2520released%2520at%2520https%253A//internrobotics.github.io/internvla-m1.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ST4VLA%3A%20Spatially%20Guided%20Training%20for%20Vision-Language-Action%20Models&entry.906535625=Jinhui%20Ye%20and%20Fangjing%20Wang%20and%20Ning%20Gao%20and%20Junqiu%20Yu%20and%20Yangkun%20Zhu%20and%20Bin%20Wang%20and%20Jinyu%20Zhang%20and%20Weiyang%20Jin%20and%20Yanwei%20Fu%20and%20Feng%20Zheng%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20excel%20at%20multimodal%20understanding%20but%20fall%20short%20when%20extended%20to%20embodied%20tasks%2C%20where%20instructions%20must%20be%20transformed%20into%20low-level%20motor%20actions.%20We%20introduce%20ST4VLA%2C%20a%20dual-system%20Vision-Language-Action%20framework%20that%20leverages%20Spatial%20Guided%20Training%20to%20align%20action%20learning%20with%20spatial%20priors%20in%20VLMs.%20ST4VLA%20includes%20two%20stages%3A%20%28i%29%20spatial%20grounding%20pre-training%2C%20which%20equips%20the%20VLM%20with%20transferable%20priors%20via%20scalable%20point%2C%20box%2C%20and%20trajectory%20prediction%20from%20both%20web-scale%20and%20robot-specific%20data%2C%20and%20%28ii%29%20spatially%20guided%20action%20post-training%2C%20which%20encourages%20the%20model%20to%20produce%20richer%20spatial%20priors%20to%20guide%20action%20generation%20via%20spatial%20prompting.%20This%20design%20preserves%20spatial%20grounding%20during%20policy%20learning%20and%20promotes%20consistent%20optimization%20across%20spatial%20and%20action%20objectives.%20Empirically%2C%20ST4VLA%20achieves%20substantial%20improvements%20over%20vanilla%20VLA%2C%20with%20performance%20increasing%20from%2066.1%20-%3E%2084.6%20on%20Google%20Robot%20and%20from%2054.7%20-%3E%2073.2%20on%20WidowX%20Robot%2C%20establishing%20new%20state-of-the-art%20results%20on%20SimplerEnv.%20It%20also%20demonstrates%20stronger%20generalization%20to%20unseen%20objects%20and%20paraphrased%20instructions%2C%20as%20well%20as%20robustness%20to%20long-horizon%20perturbations%20in%20real-world%20settings.%20These%20results%20highlight%20scalable%20spatially%20guided%20training%20as%20a%20promising%20direction%20for%20robust%2C%20generalizable%20robot%20learning.%20Source%20code%2C%20data%20and%20models%20are%20released%20at%20https%3A//internrobotics.github.io/internvla-m1.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2602.10109v1&entry.124074799=Read"},
{"title": "Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views", "author": "Duc-Anh Nguyen and Nhien-An Le-Khac", "abstract": "Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.", "link": "http://arxiv.org/abs/2602.08755v2", "date": "2026-02-10", "relevancy": 2.1894, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.55}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redundancy-Free%20View%20Alignment%20for%20Multimodal%20Human%20Activity%20Recognition%20with%20Arbitrarily%20Missing%20Views&body=Title%3A%20Redundancy-Free%20View%20Alignment%20for%20Multimodal%20Human%20Activity%20Recognition%20with%20Arbitrarily%20Missing%20Views%0AAuthor%3A%20Duc-Anh%20Nguyen%20and%20Nhien-An%20Le-Khac%0AAbstract%3A%20Multimodal%20multiview%20learning%20seeks%20to%20integrate%20information%20from%20diverse%20sources%20to%20enhance%20task%20performance.%20Existing%20approaches%20often%20struggle%20with%20flexible%20view%20configurations%2C%20including%20arbitrary%20view%20combinations%2C%20numbers%20of%20views%2C%20and%20heterogeneous%20modalities.%20Focusing%20on%20the%20context%20of%20human%20activity%20recognition%2C%20we%20propose%20RALIS%2C%20a%20model%20that%20combines%20multiview%20contrastive%20learning%20with%20a%20mixture-of-experts%20module%20to%20support%20arbitrary%20view%20availability%20during%20both%20training%20and%20inference.%20Instead%20of%20trying%20to%20reconstruct%20missing%20views%2C%20an%20adjusted%20center%20contrastive%20loss%20is%20used%20for%20self-supervised%20representation%20learning%20and%20view%20alignment%2C%20mitigating%20the%20impact%20of%20missing%20views%20on%20multiview%20fusion.%20This%20loss%20formulation%20allows%20for%20the%20integration%20of%20view%20weights%20to%20account%20for%20view%20quality.%20Additionally%2C%20it%20reduces%20computational%20complexity%20from%20%24O%28V%5E2%29%24%20to%20%24O%28V%29%24%2C%20where%20%24V%24%20is%20the%20number%20of%20views.%20To%20address%20residual%20discrepancies%20not%20captured%20by%20contrastive%20learning%2C%20we%20employ%20a%20mixture-of-experts%20module%20with%20a%20specialized%20load%20balancing%20strategy%2C%20tasked%20with%20adapting%20to%20arbitrary%20view%20combinations.%20We%20highlight%20the%20geometric%20relationship%20among%20components%20in%20our%20model%20and%20how%20they%20combine%20well%20in%20the%20latent%20space.%20RALIS%20is%20validated%20on%20four%20datasets%20encompassing%20inertial%20and%20human%20pose%20modalities%2C%20with%20the%20number%20of%20views%20ranging%20from%20three%20to%20nine%2C%20demonstrating%20its%20performance%20and%20flexibility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedundancy-Free%2520View%2520Alignment%2520for%2520Multimodal%2520Human%2520Activity%2520Recognition%2520with%2520Arbitrarily%2520Missing%2520Views%26entry.906535625%3DDuc-Anh%2520Nguyen%2520and%2520Nhien-An%2520Le-Khac%26entry.1292438233%3DMultimodal%2520multiview%2520learning%2520seeks%2520to%2520integrate%2520information%2520from%2520diverse%2520sources%2520to%2520enhance%2520task%2520performance.%2520Existing%2520approaches%2520often%2520struggle%2520with%2520flexible%2520view%2520configurations%252C%2520including%2520arbitrary%2520view%2520combinations%252C%2520numbers%2520of%2520views%252C%2520and%2520heterogeneous%2520modalities.%2520Focusing%2520on%2520the%2520context%2520of%2520human%2520activity%2520recognition%252C%2520we%2520propose%2520RALIS%252C%2520a%2520model%2520that%2520combines%2520multiview%2520contrastive%2520learning%2520with%2520a%2520mixture-of-experts%2520module%2520to%2520support%2520arbitrary%2520view%2520availability%2520during%2520both%2520training%2520and%2520inference.%2520Instead%2520of%2520trying%2520to%2520reconstruct%2520missing%2520views%252C%2520an%2520adjusted%2520center%2520contrastive%2520loss%2520is%2520used%2520for%2520self-supervised%2520representation%2520learning%2520and%2520view%2520alignment%252C%2520mitigating%2520the%2520impact%2520of%2520missing%2520views%2520on%2520multiview%2520fusion.%2520This%2520loss%2520formulation%2520allows%2520for%2520the%2520integration%2520of%2520view%2520weights%2520to%2520account%2520for%2520view%2520quality.%2520Additionally%252C%2520it%2520reduces%2520computational%2520complexity%2520from%2520%2524O%2528V%255E2%2529%2524%2520to%2520%2524O%2528V%2529%2524%252C%2520where%2520%2524V%2524%2520is%2520the%2520number%2520of%2520views.%2520To%2520address%2520residual%2520discrepancies%2520not%2520captured%2520by%2520contrastive%2520learning%252C%2520we%2520employ%2520a%2520mixture-of-experts%2520module%2520with%2520a%2520specialized%2520load%2520balancing%2520strategy%252C%2520tasked%2520with%2520adapting%2520to%2520arbitrary%2520view%2520combinations.%2520We%2520highlight%2520the%2520geometric%2520relationship%2520among%2520components%2520in%2520our%2520model%2520and%2520how%2520they%2520combine%2520well%2520in%2520the%2520latent%2520space.%2520RALIS%2520is%2520validated%2520on%2520four%2520datasets%2520encompassing%2520inertial%2520and%2520human%2520pose%2520modalities%252C%2520with%2520the%2520number%2520of%2520views%2520ranging%2520from%2520three%2520to%2520nine%252C%2520demonstrating%2520its%2520performance%2520and%2520flexibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redundancy-Free%20View%20Alignment%20for%20Multimodal%20Human%20Activity%20Recognition%20with%20Arbitrarily%20Missing%20Views&entry.906535625=Duc-Anh%20Nguyen%20and%20Nhien-An%20Le-Khac&entry.1292438233=Multimodal%20multiview%20learning%20seeks%20to%20integrate%20information%20from%20diverse%20sources%20to%20enhance%20task%20performance.%20Existing%20approaches%20often%20struggle%20with%20flexible%20view%20configurations%2C%20including%20arbitrary%20view%20combinations%2C%20numbers%20of%20views%2C%20and%20heterogeneous%20modalities.%20Focusing%20on%20the%20context%20of%20human%20activity%20recognition%2C%20we%20propose%20RALIS%2C%20a%20model%20that%20combines%20multiview%20contrastive%20learning%20with%20a%20mixture-of-experts%20module%20to%20support%20arbitrary%20view%20availability%20during%20both%20training%20and%20inference.%20Instead%20of%20trying%20to%20reconstruct%20missing%20views%2C%20an%20adjusted%20center%20contrastive%20loss%20is%20used%20for%20self-supervised%20representation%20learning%20and%20view%20alignment%2C%20mitigating%20the%20impact%20of%20missing%20views%20on%20multiview%20fusion.%20This%20loss%20formulation%20allows%20for%20the%20integration%20of%20view%20weights%20to%20account%20for%20view%20quality.%20Additionally%2C%20it%20reduces%20computational%20complexity%20from%20%24O%28V%5E2%29%24%20to%20%24O%28V%29%24%2C%20where%20%24V%24%20is%20the%20number%20of%20views.%20To%20address%20residual%20discrepancies%20not%20captured%20by%20contrastive%20learning%2C%20we%20employ%20a%20mixture-of-experts%20module%20with%20a%20specialized%20load%20balancing%20strategy%2C%20tasked%20with%20adapting%20to%20arbitrary%20view%20combinations.%20We%20highlight%20the%20geometric%20relationship%20among%20components%20in%20our%20model%20and%20how%20they%20combine%20well%20in%20the%20latent%20space.%20RALIS%20is%20validated%20on%20four%20datasets%20encompassing%20inertial%20and%20human%20pose%20modalities%2C%20with%20the%20number%20of%20views%20ranging%20from%20three%20to%20nine%2C%20demonstrating%20its%20performance%20and%20flexibility.&entry.1838667208=http%3A//arxiv.org/abs/2602.08755v2&entry.124074799=Read"},
{"title": "Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for Audio-Visual Learning", "author": "Zhaocheng Liu and Zhiwen Yu and Xiaoqing Liu", "abstract": "Multimodal learning integrates diverse modalities but suffers from modality imbalance, where dominant modalities suppress weaker ones due to inconsistent convergence rates. Existing methods predominantly rely on static modulation or heuristics, overlooking sample-level distributional variations in prediction bias. Specifically, they fail to distinguish outlier samples where the modality gap is exacerbated by low data quality. We propose a framework to quantitatively diagnose and dynamically mitigate this imbalance at the sample level. We introduce the Modality Gap metric to quantify prediction discrepancies. Analysis reveals that this gap follows a bimodal distribution, indicating the coexistence of balanced and imbalanced sample subgroups. We employ a Gaussian Mixture Model (GMM) to explicitly model this distribution, leveraging Bayesian posterior probabilities for soft subgroup separation. Our two-stage framework comprises a Warm-up stage and an Adaptive Training stage. In the latter, a GMM-guided Adaptive Loss dynamically reallocates optimization priorities: it imposes stronger alignment penalties on imbalanced samples to rectify bias, while prioritizing fusion for balanced samples to maximize complementary information. Experiments on CREMA-D, AVE, and KineticSound demonstrate that our method significantly outperforms SOTA baselines. Furthermore, we show that fine-tuning on a GMM-filtered balanced subset serves as an effective data purification strategy, yielding substantial gains by eliminating extreme noisy samples even without the adaptive loss.", "link": "http://arxiv.org/abs/2510.21797v3", "date": "2026-02-10", "relevancy": 2.1879, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Multimodal%20Imbalance%3A%20A%20GMM-Guided%20Adaptive%20Loss%20for%20Audio-Visual%20Learning&body=Title%3A%20Quantifying%20Multimodal%20Imbalance%3A%20A%20GMM-Guided%20Adaptive%20Loss%20for%20Audio-Visual%20Learning%0AAuthor%3A%20Zhaocheng%20Liu%20and%20Zhiwen%20Yu%20and%20Xiaoqing%20Liu%0AAbstract%3A%20Multimodal%20learning%20integrates%20diverse%20modalities%20but%20suffers%20from%20modality%20imbalance%2C%20where%20dominant%20modalities%20suppress%20weaker%20ones%20due%20to%20inconsistent%20convergence%20rates.%20Existing%20methods%20predominantly%20rely%20on%20static%20modulation%20or%20heuristics%2C%20overlooking%20sample-level%20distributional%20variations%20in%20prediction%20bias.%20Specifically%2C%20they%20fail%20to%20distinguish%20outlier%20samples%20where%20the%20modality%20gap%20is%20exacerbated%20by%20low%20data%20quality.%20We%20propose%20a%20framework%20to%20quantitatively%20diagnose%20and%20dynamically%20mitigate%20this%20imbalance%20at%20the%20sample%20level.%20We%20introduce%20the%20Modality%20Gap%20metric%20to%20quantify%20prediction%20discrepancies.%20Analysis%20reveals%20that%20this%20gap%20follows%20a%20bimodal%20distribution%2C%20indicating%20the%20coexistence%20of%20balanced%20and%20imbalanced%20sample%20subgroups.%20We%20employ%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20to%20explicitly%20model%20this%20distribution%2C%20leveraging%20Bayesian%20posterior%20probabilities%20for%20soft%20subgroup%20separation.%20Our%20two-stage%20framework%20comprises%20a%20Warm-up%20stage%20and%20an%20Adaptive%20Training%20stage.%20In%20the%20latter%2C%20a%20GMM-guided%20Adaptive%20Loss%20dynamically%20reallocates%20optimization%20priorities%3A%20it%20imposes%20stronger%20alignment%20penalties%20on%20imbalanced%20samples%20to%20rectify%20bias%2C%20while%20prioritizing%20fusion%20for%20balanced%20samples%20to%20maximize%20complementary%20information.%20Experiments%20on%20CREMA-D%2C%20AVE%2C%20and%20KineticSound%20demonstrate%20that%20our%20method%20significantly%20outperforms%20SOTA%20baselines.%20Furthermore%2C%20we%20show%20that%20fine-tuning%20on%20a%20GMM-filtered%20balanced%20subset%20serves%20as%20an%20effective%20data%20purification%20strategy%2C%20yielding%20substantial%20gains%20by%20eliminating%20extreme%20noisy%20samples%20even%20without%20the%20adaptive%20loss.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21797v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Multimodal%2520Imbalance%253A%2520A%2520GMM-Guided%2520Adaptive%2520Loss%2520for%2520Audio-Visual%2520Learning%26entry.906535625%3DZhaocheng%2520Liu%2520and%2520Zhiwen%2520Yu%2520and%2520Xiaoqing%2520Liu%26entry.1292438233%3DMultimodal%2520learning%2520integrates%2520diverse%2520modalities%2520but%2520suffers%2520from%2520modality%2520imbalance%252C%2520where%2520dominant%2520modalities%2520suppress%2520weaker%2520ones%2520due%2520to%2520inconsistent%2520convergence%2520rates.%2520Existing%2520methods%2520predominantly%2520rely%2520on%2520static%2520modulation%2520or%2520heuristics%252C%2520overlooking%2520sample-level%2520distributional%2520variations%2520in%2520prediction%2520bias.%2520Specifically%252C%2520they%2520fail%2520to%2520distinguish%2520outlier%2520samples%2520where%2520the%2520modality%2520gap%2520is%2520exacerbated%2520by%2520low%2520data%2520quality.%2520We%2520propose%2520a%2520framework%2520to%2520quantitatively%2520diagnose%2520and%2520dynamically%2520mitigate%2520this%2520imbalance%2520at%2520the%2520sample%2520level.%2520We%2520introduce%2520the%2520Modality%2520Gap%2520metric%2520to%2520quantify%2520prediction%2520discrepancies.%2520Analysis%2520reveals%2520that%2520this%2520gap%2520follows%2520a%2520bimodal%2520distribution%252C%2520indicating%2520the%2520coexistence%2520of%2520balanced%2520and%2520imbalanced%2520sample%2520subgroups.%2520We%2520employ%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520to%2520explicitly%2520model%2520this%2520distribution%252C%2520leveraging%2520Bayesian%2520posterior%2520probabilities%2520for%2520soft%2520subgroup%2520separation.%2520Our%2520two-stage%2520framework%2520comprises%2520a%2520Warm-up%2520stage%2520and%2520an%2520Adaptive%2520Training%2520stage.%2520In%2520the%2520latter%252C%2520a%2520GMM-guided%2520Adaptive%2520Loss%2520dynamically%2520reallocates%2520optimization%2520priorities%253A%2520it%2520imposes%2520stronger%2520alignment%2520penalties%2520on%2520imbalanced%2520samples%2520to%2520rectify%2520bias%252C%2520while%2520prioritizing%2520fusion%2520for%2520balanced%2520samples%2520to%2520maximize%2520complementary%2520information.%2520Experiments%2520on%2520CREMA-D%252C%2520AVE%252C%2520and%2520KineticSound%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520SOTA%2520baselines.%2520Furthermore%252C%2520we%2520show%2520that%2520fine-tuning%2520on%2520a%2520GMM-filtered%2520balanced%2520subset%2520serves%2520as%2520an%2520effective%2520data%2520purification%2520strategy%252C%2520yielding%2520substantial%2520gains%2520by%2520eliminating%2520extreme%2520noisy%2520samples%2520even%2520without%2520the%2520adaptive%2520loss.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21797v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Multimodal%20Imbalance%3A%20A%20GMM-Guided%20Adaptive%20Loss%20for%20Audio-Visual%20Learning&entry.906535625=Zhaocheng%20Liu%20and%20Zhiwen%20Yu%20and%20Xiaoqing%20Liu&entry.1292438233=Multimodal%20learning%20integrates%20diverse%20modalities%20but%20suffers%20from%20modality%20imbalance%2C%20where%20dominant%20modalities%20suppress%20weaker%20ones%20due%20to%20inconsistent%20convergence%20rates.%20Existing%20methods%20predominantly%20rely%20on%20static%20modulation%20or%20heuristics%2C%20overlooking%20sample-level%20distributional%20variations%20in%20prediction%20bias.%20Specifically%2C%20they%20fail%20to%20distinguish%20outlier%20samples%20where%20the%20modality%20gap%20is%20exacerbated%20by%20low%20data%20quality.%20We%20propose%20a%20framework%20to%20quantitatively%20diagnose%20and%20dynamically%20mitigate%20this%20imbalance%20at%20the%20sample%20level.%20We%20introduce%20the%20Modality%20Gap%20metric%20to%20quantify%20prediction%20discrepancies.%20Analysis%20reveals%20that%20this%20gap%20follows%20a%20bimodal%20distribution%2C%20indicating%20the%20coexistence%20of%20balanced%20and%20imbalanced%20sample%20subgroups.%20We%20employ%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20to%20explicitly%20model%20this%20distribution%2C%20leveraging%20Bayesian%20posterior%20probabilities%20for%20soft%20subgroup%20separation.%20Our%20two-stage%20framework%20comprises%20a%20Warm-up%20stage%20and%20an%20Adaptive%20Training%20stage.%20In%20the%20latter%2C%20a%20GMM-guided%20Adaptive%20Loss%20dynamically%20reallocates%20optimization%20priorities%3A%20it%20imposes%20stronger%20alignment%20penalties%20on%20imbalanced%20samples%20to%20rectify%20bias%2C%20while%20prioritizing%20fusion%20for%20balanced%20samples%20to%20maximize%20complementary%20information.%20Experiments%20on%20CREMA-D%2C%20AVE%2C%20and%20KineticSound%20demonstrate%20that%20our%20method%20significantly%20outperforms%20SOTA%20baselines.%20Furthermore%2C%20we%20show%20that%20fine-tuning%20on%20a%20GMM-filtered%20balanced%20subset%20serves%20as%20an%20effective%20data%20purification%20strategy%2C%20yielding%20substantial%20gains%20by%20eliminating%20extreme%20noisy%20samples%20even%20without%20the%20adaptive%20loss.&entry.1838667208=http%3A//arxiv.org/abs/2510.21797v3&entry.124074799=Read"},
{"title": "GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery", "author": "Han Jinzhen and JinByeong Lee and JiSung Kim and MinKyung Cho and DaHee Kim and HongSik Yun", "abstract": "Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.", "link": "http://arxiv.org/abs/2602.09932v1", "date": "2026-02-10", "relevancy": 2.1794, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5721}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5295}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoFormer%3A%20A%20Swin%20Transformer-Based%20Framework%20for%20Scene-Level%20Building%20Height%20and%20Footprint%20Estimation%20from%20Sentinel%20Imagery&body=Title%3A%20GeoFormer%3A%20A%20Swin%20Transformer-Based%20Framework%20for%20Scene-Level%20Building%20Height%20and%20Footprint%20Estimation%20from%20Sentinel%20Imagery%0AAuthor%3A%20Han%20Jinzhen%20and%20JinByeong%20Lee%20and%20JiSung%20Kim%20and%20MinKyung%20Cho%20and%20DaHee%20Kim%20and%20HongSik%20Yun%0AAbstract%3A%20Accurate%20three-dimensional%20urban%20data%20are%20critical%20for%20climate%20modelling%2C%20disaster%20risk%20assessment%2C%20and%20urban%20planning%2C%20yet%20remain%20scarce%20due%20to%20reliance%20on%20proprietary%20sensors%20or%20poor%20cross-city%20generalisation.%20We%20propose%20GeoFormer%2C%20an%20open-source%20Swin%20Transformer%20framework%20that%20jointly%20estimates%20building%20height%20%28BH%29%20and%20footprint%20%28BF%29%20on%20a%20100%20m%20grid%20using%20only%20Sentinel-1/2%20imagery%20and%20open%20DEM%20data.%20A%20geo-blocked%20splitting%20strategy%20ensures%20strict%20spatial%20independence%20between%20training%20and%20test%20sets.%20Evaluated%20over%2054%20diverse%20cities%2C%20GeoFormer%20achieves%20a%20BH%20RMSE%20of%203.19%20m%20and%20a%20BF%20RMSE%20of%200.05%2C%20improving%207.5%25%20and%2015.3%25%20over%20the%20strongest%20CNN%20baseline%2C%20while%20maintaining%20under%203.5%20m%20BH%20RMSE%20in%20cross-continent%20transfer.%20Ablation%20studies%20confirm%20that%20DEM%20is%20indispensable%20for%20height%20estimation%20and%20that%20optical%20reflectance%20dominates%20over%20SAR%2C%20though%20multi-source%20fusion%20yields%20the%20best%20overall%20accuracy.%20All%20code%2C%20weights%2C%20and%20global%20products%20are%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoFormer%253A%2520A%2520Swin%2520Transformer-Based%2520Framework%2520for%2520Scene-Level%2520Building%2520Height%2520and%2520Footprint%2520Estimation%2520from%2520Sentinel%2520Imagery%26entry.906535625%3DHan%2520Jinzhen%2520and%2520JinByeong%2520Lee%2520and%2520JiSung%2520Kim%2520and%2520MinKyung%2520Cho%2520and%2520DaHee%2520Kim%2520and%2520HongSik%2520Yun%26entry.1292438233%3DAccurate%2520three-dimensional%2520urban%2520data%2520are%2520critical%2520for%2520climate%2520modelling%252C%2520disaster%2520risk%2520assessment%252C%2520and%2520urban%2520planning%252C%2520yet%2520remain%2520scarce%2520due%2520to%2520reliance%2520on%2520proprietary%2520sensors%2520or%2520poor%2520cross-city%2520generalisation.%2520We%2520propose%2520GeoFormer%252C%2520an%2520open-source%2520Swin%2520Transformer%2520framework%2520that%2520jointly%2520estimates%2520building%2520height%2520%2528BH%2529%2520and%2520footprint%2520%2528BF%2529%2520on%2520a%2520100%2520m%2520grid%2520using%2520only%2520Sentinel-1/2%2520imagery%2520and%2520open%2520DEM%2520data.%2520A%2520geo-blocked%2520splitting%2520strategy%2520ensures%2520strict%2520spatial%2520independence%2520between%2520training%2520and%2520test%2520sets.%2520Evaluated%2520over%252054%2520diverse%2520cities%252C%2520GeoFormer%2520achieves%2520a%2520BH%2520RMSE%2520of%25203.19%2520m%2520and%2520a%2520BF%2520RMSE%2520of%25200.05%252C%2520improving%25207.5%2525%2520and%252015.3%2525%2520over%2520the%2520strongest%2520CNN%2520baseline%252C%2520while%2520maintaining%2520under%25203.5%2520m%2520BH%2520RMSE%2520in%2520cross-continent%2520transfer.%2520Ablation%2520studies%2520confirm%2520that%2520DEM%2520is%2520indispensable%2520for%2520height%2520estimation%2520and%2520that%2520optical%2520reflectance%2520dominates%2520over%2520SAR%252C%2520though%2520multi-source%2520fusion%2520yields%2520the%2520best%2520overall%2520accuracy.%2520All%2520code%252C%2520weights%252C%2520and%2520global%2520products%2520are%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoFormer%3A%20A%20Swin%20Transformer-Based%20Framework%20for%20Scene-Level%20Building%20Height%20and%20Footprint%20Estimation%20from%20Sentinel%20Imagery&entry.906535625=Han%20Jinzhen%20and%20JinByeong%20Lee%20and%20JiSung%20Kim%20and%20MinKyung%20Cho%20and%20DaHee%20Kim%20and%20HongSik%20Yun&entry.1292438233=Accurate%20three-dimensional%20urban%20data%20are%20critical%20for%20climate%20modelling%2C%20disaster%20risk%20assessment%2C%20and%20urban%20planning%2C%20yet%20remain%20scarce%20due%20to%20reliance%20on%20proprietary%20sensors%20or%20poor%20cross-city%20generalisation.%20We%20propose%20GeoFormer%2C%20an%20open-source%20Swin%20Transformer%20framework%20that%20jointly%20estimates%20building%20height%20%28BH%29%20and%20footprint%20%28BF%29%20on%20a%20100%20m%20grid%20using%20only%20Sentinel-1/2%20imagery%20and%20open%20DEM%20data.%20A%20geo-blocked%20splitting%20strategy%20ensures%20strict%20spatial%20independence%20between%20training%20and%20test%20sets.%20Evaluated%20over%2054%20diverse%20cities%2C%20GeoFormer%20achieves%20a%20BH%20RMSE%20of%203.19%20m%20and%20a%20BF%20RMSE%20of%200.05%2C%20improving%207.5%25%20and%2015.3%25%20over%20the%20strongest%20CNN%20baseline%2C%20while%20maintaining%20under%203.5%20m%20BH%20RMSE%20in%20cross-continent%20transfer.%20Ablation%20studies%20confirm%20that%20DEM%20is%20indispensable%20for%20height%20estimation%20and%20that%20optical%20reflectance%20dominates%20over%20SAR%2C%20though%20multi-source%20fusion%20yields%20the%20best%20overall%20accuracy.%20All%20code%2C%20weights%2C%20and%20global%20products%20are%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2602.09932v1&entry.124074799=Read"},
{"title": "Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework", "author": "Dogucan Yaman and Fevziye Irem Eyiokur and Haz\u0131m Kemal Ekenel and Alexander Waibel", "abstract": "Video editing-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leakage, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.", "link": "http://arxiv.org/abs/2511.08613v2", "date": "2026-02-10", "relevancy": 2.1786, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.59}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5164}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Identity%20Leakage%20in%20Talking%20Face%20Generation%3A%20Metrics%20and%20Evaluation%20Framework&body=Title%3A%20Assessing%20Identity%20Leakage%20in%20Talking%20Face%20Generation%3A%20Metrics%20and%20Evaluation%20Framework%0AAuthor%3A%20Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20Video%20editing-based%20talking%20face%20generation%20aims%20to%20preserve%20video%20details%20such%20as%20pose%2C%20lighting%2C%20and%20gestures%20while%20modifying%20only%20lip%20motion%2C%20often%20using%20an%20identity%20reference%20image%20to%20maintain%20speaker%20consistency.%20However%2C%20this%20mechanism%20can%20introduce%20lip%20leakage%2C%20where%20generated%20lips%20are%20influenced%20by%20the%20reference%20image%20rather%20than%20solely%20by%20the%20driving%20audio.%20Such%20leakage%20is%20difficult%20to%20detect%20with%20standard%20metrics%20and%20conventional%20test%20setup.%20To%20address%20this%2C%20we%20propose%20a%20systematic%20evaluation%20methodology%20to%20analyze%20and%20quantify%20lip%20leakage.%20Our%20framework%20employs%20three%20complementary%20test%20setups%3A%20silent-input%20generation%2C%20mismatched%20audio-video%20pairing%2C%20and%20matched%20audio-video%20synthesis.%20We%20also%20introduce%20derived%20metrics%20including%20lip-sync%20discrepancy%20and%20silent-audio-based%20lip-sync%20scores.%20In%20addition%2C%20we%20study%20how%20different%20identity%20reference%20selections%20affect%20leakage%2C%20providing%20insights%20into%20reference%20design.%20The%20proposed%20methodology%20is%20model-agnostic%20and%20establishes%20a%20more%20reliable%20benchmark%20for%20future%20research%20in%20talking%20face%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Identity%2520Leakage%2520in%2520Talking%2520Face%2520Generation%253A%2520Metrics%2520and%2520Evaluation%2520Framework%26entry.906535625%3DDogucan%2520Yaman%2520and%2520Fevziye%2520Irem%2520Eyiokur%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3DVideo%2520editing-based%2520talking%2520face%2520generation%2520aims%2520to%2520preserve%2520video%2520details%2520such%2520as%2520pose%252C%2520lighting%252C%2520and%2520gestures%2520while%2520modifying%2520only%2520lip%2520motion%252C%2520often%2520using%2520an%2520identity%2520reference%2520image%2520to%2520maintain%2520speaker%2520consistency.%2520However%252C%2520this%2520mechanism%2520can%2520introduce%2520lip%2520leakage%252C%2520where%2520generated%2520lips%2520are%2520influenced%2520by%2520the%2520reference%2520image%2520rather%2520than%2520solely%2520by%2520the%2520driving%2520audio.%2520Such%2520leakage%2520is%2520difficult%2520to%2520detect%2520with%2520standard%2520metrics%2520and%2520conventional%2520test%2520setup.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520systematic%2520evaluation%2520methodology%2520to%2520analyze%2520and%2520quantify%2520lip%2520leakage.%2520Our%2520framework%2520employs%2520three%2520complementary%2520test%2520setups%253A%2520silent-input%2520generation%252C%2520mismatched%2520audio-video%2520pairing%252C%2520and%2520matched%2520audio-video%2520synthesis.%2520We%2520also%2520introduce%2520derived%2520metrics%2520including%2520lip-sync%2520discrepancy%2520and%2520silent-audio-based%2520lip-sync%2520scores.%2520In%2520addition%252C%2520we%2520study%2520how%2520different%2520identity%2520reference%2520selections%2520affect%2520leakage%252C%2520providing%2520insights%2520into%2520reference%2520design.%2520The%2520proposed%2520methodology%2520is%2520model-agnostic%2520and%2520establishes%2520a%2520more%2520reliable%2520benchmark%2520for%2520future%2520research%2520in%2520talking%2520face%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Identity%20Leakage%20in%20Talking%20Face%20Generation%3A%20Metrics%20and%20Evaluation%20Framework&entry.906535625=Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=Video%20editing-based%20talking%20face%20generation%20aims%20to%20preserve%20video%20details%20such%20as%20pose%2C%20lighting%2C%20and%20gestures%20while%20modifying%20only%20lip%20motion%2C%20often%20using%20an%20identity%20reference%20image%20to%20maintain%20speaker%20consistency.%20However%2C%20this%20mechanism%20can%20introduce%20lip%20leakage%2C%20where%20generated%20lips%20are%20influenced%20by%20the%20reference%20image%20rather%20than%20solely%20by%20the%20driving%20audio.%20Such%20leakage%20is%20difficult%20to%20detect%20with%20standard%20metrics%20and%20conventional%20test%20setup.%20To%20address%20this%2C%20we%20propose%20a%20systematic%20evaluation%20methodology%20to%20analyze%20and%20quantify%20lip%20leakage.%20Our%20framework%20employs%20three%20complementary%20test%20setups%3A%20silent-input%20generation%2C%20mismatched%20audio-video%20pairing%2C%20and%20matched%20audio-video%20synthesis.%20We%20also%20introduce%20derived%20metrics%20including%20lip-sync%20discrepancy%20and%20silent-audio-based%20lip-sync%20scores.%20In%20addition%2C%20we%20study%20how%20different%20identity%20reference%20selections%20affect%20leakage%2C%20providing%20insights%20into%20reference%20design.%20The%20proposed%20methodology%20is%20model-agnostic%20and%20establishes%20a%20more%20reliable%20benchmark%20for%20future%20research%20in%20talking%20face%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.08613v2&entry.124074799=Read"},
{"title": "GenTrack2: An Improved Hybrid Approach for Multi-Object Tracking", "author": "Toan Van Nguyen and Rasmus G. K. Christiansen and Dirk Kraft and Leon Bodenhagen", "abstract": "This paper proposes a visual multi-object tracking method that jointly employs stochastic and deterministic mechanisms to ensure identifier consistency for unknown and time-varying target numbers under nonlinear dynamics. A stochastic particle filter addresses nonlinear dynamics and non-Gaussian noise, with support from particle swarm optimization (PSO) to guide particles toward state distribution modes and mitigate divergence through proposed fitness measures incorporating motion consistency, appearance similarity, and social-interaction cues with neighboring targets. Deterministic association further enforces identifier consistency via a proposed cost matrix incorporating spatial consistency between particles and current detections, detection confidences, and track penalties. Subsequently, a novel scheme is proposed for the smooth updating of target states while preserving their identities, particularly for weak tracks during interactions with other targets and prolonged occlusions. Moreover, velocity regression over past states provides trend-seed velocities, enhancing particle sampling and state updates. The proposed tracker is designed to operate flexibly for both pre-recorded videos and camera live streams, where future frames are unavailable. Experimental results confirm superior performance compared to state-of-the-art trackers. The source-code reference implementations of both the proposed method and compared-trackers are provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack2", "link": "http://arxiv.org/abs/2510.24410v3", "date": "2026-02-10", "relevancy": 2.1775, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5822}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.54}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenTrack2%3A%20An%20Improved%20Hybrid%20Approach%20for%20Multi-Object%20Tracking&body=Title%3A%20GenTrack2%3A%20An%20Improved%20Hybrid%20Approach%20for%20Multi-Object%20Tracking%0AAuthor%3A%20Toan%20Van%20Nguyen%20and%20Rasmus%20G.%20K.%20Christiansen%20and%20Dirk%20Kraft%20and%20Leon%20Bodenhagen%0AAbstract%3A%20This%20paper%20proposes%20a%20visual%20multi-object%20tracking%20method%20that%20jointly%20employs%20stochastic%20and%20deterministic%20mechanisms%20to%20ensure%20identifier%20consistency%20for%20unknown%20and%20time-varying%20target%20numbers%20under%20nonlinear%20dynamics.%20A%20stochastic%20particle%20filter%20addresses%20nonlinear%20dynamics%20and%20non-Gaussian%20noise%2C%20with%20support%20from%20particle%20swarm%20optimization%20%28PSO%29%20to%20guide%20particles%20toward%20state%20distribution%20modes%20and%20mitigate%20divergence%20through%20proposed%20fitness%20measures%20incorporating%20motion%20consistency%2C%20appearance%20similarity%2C%20and%20social-interaction%20cues%20with%20neighboring%20targets.%20Deterministic%20association%20further%20enforces%20identifier%20consistency%20via%20a%20proposed%20cost%20matrix%20incorporating%20spatial%20consistency%20between%20particles%20and%20current%20detections%2C%20detection%20confidences%2C%20and%20track%20penalties.%20Subsequently%2C%20a%20novel%20scheme%20is%20proposed%20for%20the%20smooth%20updating%20of%20target%20states%20while%20preserving%20their%20identities%2C%20particularly%20for%20weak%20tracks%20during%20interactions%20with%20other%20targets%20and%20prolonged%20occlusions.%20Moreover%2C%20velocity%20regression%20over%20past%20states%20provides%20trend-seed%20velocities%2C%20enhancing%20particle%20sampling%20and%20state%20updates.%20The%20proposed%20tracker%20is%20designed%20to%20operate%20flexibly%20for%20both%20pre-recorded%20videos%20and%20camera%20live%20streams%2C%20where%20future%20frames%20are%20unavailable.%20Experimental%20results%20confirm%20superior%20performance%20compared%20to%20state-of-the-art%20trackers.%20The%20source-code%20reference%20implementations%20of%20both%20the%20proposed%20method%20and%20compared-trackers%20are%20provided%20on%20GitHub%3A%20https%3A//github.com/SDU-VelKoTek/GenTrack2%0ALink%3A%20http%3A//arxiv.org/abs/2510.24410v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenTrack2%253A%2520An%2520Improved%2520Hybrid%2520Approach%2520for%2520Multi-Object%2520Tracking%26entry.906535625%3DToan%2520Van%2520Nguyen%2520and%2520Rasmus%2520G.%2520K.%2520Christiansen%2520and%2520Dirk%2520Kraft%2520and%2520Leon%2520Bodenhagen%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520visual%2520multi-object%2520tracking%2520method%2520that%2520jointly%2520employs%2520stochastic%2520and%2520deterministic%2520mechanisms%2520to%2520ensure%2520identifier%2520consistency%2520for%2520unknown%2520and%2520time-varying%2520target%2520numbers%2520under%2520nonlinear%2520dynamics.%2520A%2520stochastic%2520particle%2520filter%2520addresses%2520nonlinear%2520dynamics%2520and%2520non-Gaussian%2520noise%252C%2520with%2520support%2520from%2520particle%2520swarm%2520optimization%2520%2528PSO%2529%2520to%2520guide%2520particles%2520toward%2520state%2520distribution%2520modes%2520and%2520mitigate%2520divergence%2520through%2520proposed%2520fitness%2520measures%2520incorporating%2520motion%2520consistency%252C%2520appearance%2520similarity%252C%2520and%2520social-interaction%2520cues%2520with%2520neighboring%2520targets.%2520Deterministic%2520association%2520further%2520enforces%2520identifier%2520consistency%2520via%2520a%2520proposed%2520cost%2520matrix%2520incorporating%2520spatial%2520consistency%2520between%2520particles%2520and%2520current%2520detections%252C%2520detection%2520confidences%252C%2520and%2520track%2520penalties.%2520Subsequently%252C%2520a%2520novel%2520scheme%2520is%2520proposed%2520for%2520the%2520smooth%2520updating%2520of%2520target%2520states%2520while%2520preserving%2520their%2520identities%252C%2520particularly%2520for%2520weak%2520tracks%2520during%2520interactions%2520with%2520other%2520targets%2520and%2520prolonged%2520occlusions.%2520Moreover%252C%2520velocity%2520regression%2520over%2520past%2520states%2520provides%2520trend-seed%2520velocities%252C%2520enhancing%2520particle%2520sampling%2520and%2520state%2520updates.%2520The%2520proposed%2520tracker%2520is%2520designed%2520to%2520operate%2520flexibly%2520for%2520both%2520pre-recorded%2520videos%2520and%2520camera%2520live%2520streams%252C%2520where%2520future%2520frames%2520are%2520unavailable.%2520Experimental%2520results%2520confirm%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520trackers.%2520The%2520source-code%2520reference%2520implementations%2520of%2520both%2520the%2520proposed%2520method%2520and%2520compared-trackers%2520are%2520provided%2520on%2520GitHub%253A%2520https%253A//github.com/SDU-VelKoTek/GenTrack2%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24410v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenTrack2%3A%20An%20Improved%20Hybrid%20Approach%20for%20Multi-Object%20Tracking&entry.906535625=Toan%20Van%20Nguyen%20and%20Rasmus%20G.%20K.%20Christiansen%20and%20Dirk%20Kraft%20and%20Leon%20Bodenhagen&entry.1292438233=This%20paper%20proposes%20a%20visual%20multi-object%20tracking%20method%20that%20jointly%20employs%20stochastic%20and%20deterministic%20mechanisms%20to%20ensure%20identifier%20consistency%20for%20unknown%20and%20time-varying%20target%20numbers%20under%20nonlinear%20dynamics.%20A%20stochastic%20particle%20filter%20addresses%20nonlinear%20dynamics%20and%20non-Gaussian%20noise%2C%20with%20support%20from%20particle%20swarm%20optimization%20%28PSO%29%20to%20guide%20particles%20toward%20state%20distribution%20modes%20and%20mitigate%20divergence%20through%20proposed%20fitness%20measures%20incorporating%20motion%20consistency%2C%20appearance%20similarity%2C%20and%20social-interaction%20cues%20with%20neighboring%20targets.%20Deterministic%20association%20further%20enforces%20identifier%20consistency%20via%20a%20proposed%20cost%20matrix%20incorporating%20spatial%20consistency%20between%20particles%20and%20current%20detections%2C%20detection%20confidences%2C%20and%20track%20penalties.%20Subsequently%2C%20a%20novel%20scheme%20is%20proposed%20for%20the%20smooth%20updating%20of%20target%20states%20while%20preserving%20their%20identities%2C%20particularly%20for%20weak%20tracks%20during%20interactions%20with%20other%20targets%20and%20prolonged%20occlusions.%20Moreover%2C%20velocity%20regression%20over%20past%20states%20provides%20trend-seed%20velocities%2C%20enhancing%20particle%20sampling%20and%20state%20updates.%20The%20proposed%20tracker%20is%20designed%20to%20operate%20flexibly%20for%20both%20pre-recorded%20videos%20and%20camera%20live%20streams%2C%20where%20future%20frames%20are%20unavailable.%20Experimental%20results%20confirm%20superior%20performance%20compared%20to%20state-of-the-art%20trackers.%20The%20source-code%20reference%20implementations%20of%20both%20the%20proposed%20method%20and%20compared-trackers%20are%20provided%20on%20GitHub%3A%20https%3A//github.com/SDU-VelKoTek/GenTrack2&entry.1838667208=http%3A//arxiv.org/abs/2510.24410v3&entry.124074799=Read"},
{"title": "Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction", "author": "Abdul Matin and Rupasree Dey and Tanjim Bin Faruk and Shrideep Pallickara and Sangmi Lee Pallickara", "abstract": "Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.", "link": "http://arxiv.org/abs/2512.12445v2", "date": "2026-02-10", "relevancy": 2.1736, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.581}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5205}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Guided%20Masked%20Autoencoder%20with%20Linear%20Spectral%20Mixing%20and%20Spectral-Angle-Aware%20Reconstruction&body=Title%3A%20Knowledge-Guided%20Masked%20Autoencoder%20with%20Linear%20Spectral%20Mixing%20and%20Spectral-Angle-Aware%20Reconstruction%0AAuthor%3A%20Abdul%20Matin%20and%20Rupasree%20Dey%20and%20Tanjim%20Bin%20Faruk%20and%20Shrideep%20Pallickara%20and%20Sangmi%20Lee%20Pallickara%0AAbstract%3A%20Integrating%20domain%20knowledge%20into%20deep%20learning%20has%20emerged%20as%20a%20promising%20direction%20for%20improving%20model%20interpretability%2C%20generalization%2C%20and%20data%20efficiency.%20In%20this%20work%2C%20we%20present%20a%20novel%20knowledge-guided%20ViT-based%20Masked%20Autoencoder%20that%20embeds%20scientific%20domain%20knowledge%20within%20the%20self-supervised%20reconstruction%20process.%20Instead%20of%20relying%20solely%20on%20data-driven%20optimization%2C%20our%20proposed%20approach%20incorporates%20the%20Linear%20Spectral%20Mixing%20Model%20%28LSMM%29%20as%20a%20physical%20constraint%20and%20physically-based%20Spectral%20Angle%20Mapper%20%28SAM%29%2C%20ensuring%20that%20learned%20representations%20adhere%20to%20known%20structural%20relationships%20between%20observed%20signals%20and%20their%20latent%20components.%20The%20framework%20jointly%20optimizes%20LSMM%20and%20SAM%20loss%20with%20a%20conventional%20Huber%20loss%20objective%2C%20promoting%20both%20numerical%20accuracy%20and%20geometric%20consistency%20in%20the%20feature%20space.%20This%20knowledge-guided%20design%20enhances%20reconstruction%20fidelity%2C%20stabilizes%20training%20under%20limited%20supervision%2C%20and%20yields%20interpretable%20latent%20representations%20grounded%20in%20physical%20principles.%20The%20experimental%20findings%20indicate%20that%20the%20proposed%20model%20substantially%20enhances%20reconstruction%20quality%20and%20improves%20downstream%20task%20performance%2C%20highlighting%20the%20promise%20of%20embedding%20physics-informed%20inductive%20biases%20within%20transformer-based%20self-supervised%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Guided%2520Masked%2520Autoencoder%2520with%2520Linear%2520Spectral%2520Mixing%2520and%2520Spectral-Angle-Aware%2520Reconstruction%26entry.906535625%3DAbdul%2520Matin%2520and%2520Rupasree%2520Dey%2520and%2520Tanjim%2520Bin%2520Faruk%2520and%2520Shrideep%2520Pallickara%2520and%2520Sangmi%2520Lee%2520Pallickara%26entry.1292438233%3DIntegrating%2520domain%2520knowledge%2520into%2520deep%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520direction%2520for%2520improving%2520model%2520interpretability%252C%2520generalization%252C%2520and%2520data%2520efficiency.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520knowledge-guided%2520ViT-based%2520Masked%2520Autoencoder%2520that%2520embeds%2520scientific%2520domain%2520knowledge%2520within%2520the%2520self-supervised%2520reconstruction%2520process.%2520Instead%2520of%2520relying%2520solely%2520on%2520data-driven%2520optimization%252C%2520our%2520proposed%2520approach%2520incorporates%2520the%2520Linear%2520Spectral%2520Mixing%2520Model%2520%2528LSMM%2529%2520as%2520a%2520physical%2520constraint%2520and%2520physically-based%2520Spectral%2520Angle%2520Mapper%2520%2528SAM%2529%252C%2520ensuring%2520that%2520learned%2520representations%2520adhere%2520to%2520known%2520structural%2520relationships%2520between%2520observed%2520signals%2520and%2520their%2520latent%2520components.%2520The%2520framework%2520jointly%2520optimizes%2520LSMM%2520and%2520SAM%2520loss%2520with%2520a%2520conventional%2520Huber%2520loss%2520objective%252C%2520promoting%2520both%2520numerical%2520accuracy%2520and%2520geometric%2520consistency%2520in%2520the%2520feature%2520space.%2520This%2520knowledge-guided%2520design%2520enhances%2520reconstruction%2520fidelity%252C%2520stabilizes%2520training%2520under%2520limited%2520supervision%252C%2520and%2520yields%2520interpretable%2520latent%2520representations%2520grounded%2520in%2520physical%2520principles.%2520The%2520experimental%2520findings%2520indicate%2520that%2520the%2520proposed%2520model%2520substantially%2520enhances%2520reconstruction%2520quality%2520and%2520improves%2520downstream%2520task%2520performance%252C%2520highlighting%2520the%2520promise%2520of%2520embedding%2520physics-informed%2520inductive%2520biases%2520within%2520transformer-based%2520self-supervised%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Guided%20Masked%20Autoencoder%20with%20Linear%20Spectral%20Mixing%20and%20Spectral-Angle-Aware%20Reconstruction&entry.906535625=Abdul%20Matin%20and%20Rupasree%20Dey%20and%20Tanjim%20Bin%20Faruk%20and%20Shrideep%20Pallickara%20and%20Sangmi%20Lee%20Pallickara&entry.1292438233=Integrating%20domain%20knowledge%20into%20deep%20learning%20has%20emerged%20as%20a%20promising%20direction%20for%20improving%20model%20interpretability%2C%20generalization%2C%20and%20data%20efficiency.%20In%20this%20work%2C%20we%20present%20a%20novel%20knowledge-guided%20ViT-based%20Masked%20Autoencoder%20that%20embeds%20scientific%20domain%20knowledge%20within%20the%20self-supervised%20reconstruction%20process.%20Instead%20of%20relying%20solely%20on%20data-driven%20optimization%2C%20our%20proposed%20approach%20incorporates%20the%20Linear%20Spectral%20Mixing%20Model%20%28LSMM%29%20as%20a%20physical%20constraint%20and%20physically-based%20Spectral%20Angle%20Mapper%20%28SAM%29%2C%20ensuring%20that%20learned%20representations%20adhere%20to%20known%20structural%20relationships%20between%20observed%20signals%20and%20their%20latent%20components.%20The%20framework%20jointly%20optimizes%20LSMM%20and%20SAM%20loss%20with%20a%20conventional%20Huber%20loss%20objective%2C%20promoting%20both%20numerical%20accuracy%20and%20geometric%20consistency%20in%20the%20feature%20space.%20This%20knowledge-guided%20design%20enhances%20reconstruction%20fidelity%2C%20stabilizes%20training%20under%20limited%20supervision%2C%20and%20yields%20interpretable%20latent%20representations%20grounded%20in%20physical%20principles.%20The%20experimental%20findings%20indicate%20that%20the%20proposed%20model%20substantially%20enhances%20reconstruction%20quality%20and%20improves%20downstream%20task%20performance%2C%20highlighting%20the%20promise%20of%20embedding%20physics-informed%20inductive%20biases%20within%20transformer-based%20self-supervised%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.12445v2&entry.124074799=Read"},
{"title": "Learning Agile Quadrotor Flight in the Real World", "author": "Yunfan Ren and Zhiyuan Zhu and Jiaxu Xing and Davide Scaramuzza", "abstract": "Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.", "link": "http://arxiv.org/abs/2602.10111v1", "date": "2026-02-10", "relevancy": 2.1714, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5751}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5685}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Agile%20Quadrotor%20Flight%20in%20the%20Real%20World&body=Title%3A%20Learning%20Agile%20Quadrotor%20Flight%20in%20the%20Real%20World%0AAuthor%3A%20Yunfan%20Ren%20and%20Zhiyuan%20Zhu%20and%20Jiaxu%20Xing%20and%20Davide%20Scaramuzza%0AAbstract%3A%20Learning-based%20controllers%20have%20achieved%20impressive%20performance%20in%20agile%20quadrotor%20flight%20but%20typically%20rely%20on%20massive%20training%20in%20simulation%2C%20necessitating%20accurate%20system%20identification%20for%20effective%20Sim2Real%20transfer.%20However%2C%20even%20with%20precise%20modeling%2C%20fixed%20policies%20remain%20susceptible%20to%20out-of-distribution%20scenarios%2C%20ranging%20from%20external%20aerodynamic%20disturbances%20to%20internal%20hardware%20degradation.%20To%20ensure%20safety%20under%20these%20evolving%20uncertainties%2C%20such%20controllers%20are%20forced%20to%20operate%20with%20conservative%20safety%20margins%2C%20inherently%20constraining%20their%20agility%20outside%20of%20controlled%20settings.%20While%20online%20adaptation%20offers%20a%20potential%20remedy%2C%20safely%20exploring%20physical%20limits%20remains%20a%20critical%20bottleneck%20due%20to%20data%20scarcity%20and%20safety%20risks.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20self-adaptive%20framework%20that%20eliminates%20the%20need%20for%20precise%20system%20identification%20or%20offline%20Sim2Real%20transfer.%20We%20introduce%20Adaptive%20Temporal%20Scaling%20%28ATS%29%20to%20actively%20explore%20platform%20physical%20limits%2C%20and%20employ%20online%20residual%20learning%20to%20augment%20a%20simple%20nominal%20model.%20%7BBased%20on%20the%20learned%20hybrid%20model%2C%20we%20further%20propose%20Real-world%20Anchored%20Short-horizon%20Backpropagation%20Through%20Time%20%28RASH-BPTT%29%20to%20achieve%20efficient%20and%20robust%20in-flight%20policy%20updates.%20Extensive%20experiments%20demonstrate%20that%20our%20quadrotor%20reliably%20executes%20agile%20maneuvers%20near%20actuator%20saturation%20limits.%20The%20system%20evolves%20a%20conservative%20base%20policy%20with%20a%20peak%20speed%20of%201.9%20m/s%20to%207.3%20m/s%20within%20approximately%20100%20seconds%20of%20flight%20time.%20These%20findings%20underscore%20that%20real-world%20adaptation%20serves%20not%20merely%20to%20compensate%20for%20modeling%20errors%2C%20but%20as%20a%20practical%20mechanism%20for%20sustained%20performance%20improvement%20in%20aggressive%20flight%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Agile%2520Quadrotor%2520Flight%2520in%2520the%2520Real%2520World%26entry.906535625%3DYunfan%2520Ren%2520and%2520Zhiyuan%2520Zhu%2520and%2520Jiaxu%2520Xing%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3DLearning-based%2520controllers%2520have%2520achieved%2520impressive%2520performance%2520in%2520agile%2520quadrotor%2520flight%2520but%2520typically%2520rely%2520on%2520massive%2520training%2520in%2520simulation%252C%2520necessitating%2520accurate%2520system%2520identification%2520for%2520effective%2520Sim2Real%2520transfer.%2520However%252C%2520even%2520with%2520precise%2520modeling%252C%2520fixed%2520policies%2520remain%2520susceptible%2520to%2520out-of-distribution%2520scenarios%252C%2520ranging%2520from%2520external%2520aerodynamic%2520disturbances%2520to%2520internal%2520hardware%2520degradation.%2520To%2520ensure%2520safety%2520under%2520these%2520evolving%2520uncertainties%252C%2520such%2520controllers%2520are%2520forced%2520to%2520operate%2520with%2520conservative%2520safety%2520margins%252C%2520inherently%2520constraining%2520their%2520agility%2520outside%2520of%2520controlled%2520settings.%2520While%2520online%2520adaptation%2520offers%2520a%2520potential%2520remedy%252C%2520safely%2520exploring%2520physical%2520limits%2520remains%2520a%2520critical%2520bottleneck%2520due%2520to%2520data%2520scarcity%2520and%2520safety%2520risks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520self-adaptive%2520framework%2520that%2520eliminates%2520the%2520need%2520for%2520precise%2520system%2520identification%2520or%2520offline%2520Sim2Real%2520transfer.%2520We%2520introduce%2520Adaptive%2520Temporal%2520Scaling%2520%2528ATS%2529%2520to%2520actively%2520explore%2520platform%2520physical%2520limits%252C%2520and%2520employ%2520online%2520residual%2520learning%2520to%2520augment%2520a%2520simple%2520nominal%2520model.%2520%257BBased%2520on%2520the%2520learned%2520hybrid%2520model%252C%2520we%2520further%2520propose%2520Real-world%2520Anchored%2520Short-horizon%2520Backpropagation%2520Through%2520Time%2520%2528RASH-BPTT%2529%2520to%2520achieve%2520efficient%2520and%2520robust%2520in-flight%2520policy%2520updates.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520quadrotor%2520reliably%2520executes%2520agile%2520maneuvers%2520near%2520actuator%2520saturation%2520limits.%2520The%2520system%2520evolves%2520a%2520conservative%2520base%2520policy%2520with%2520a%2520peak%2520speed%2520of%25201.9%2520m/s%2520to%25207.3%2520m/s%2520within%2520approximately%2520100%2520seconds%2520of%2520flight%2520time.%2520These%2520findings%2520underscore%2520that%2520real-world%2520adaptation%2520serves%2520not%2520merely%2520to%2520compensate%2520for%2520modeling%2520errors%252C%2520but%2520as%2520a%2520practical%2520mechanism%2520for%2520sustained%2520performance%2520improvement%2520in%2520aggressive%2520flight%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Agile%20Quadrotor%20Flight%20in%20the%20Real%20World&entry.906535625=Yunfan%20Ren%20and%20Zhiyuan%20Zhu%20and%20Jiaxu%20Xing%20and%20Davide%20Scaramuzza&entry.1292438233=Learning-based%20controllers%20have%20achieved%20impressive%20performance%20in%20agile%20quadrotor%20flight%20but%20typically%20rely%20on%20massive%20training%20in%20simulation%2C%20necessitating%20accurate%20system%20identification%20for%20effective%20Sim2Real%20transfer.%20However%2C%20even%20with%20precise%20modeling%2C%20fixed%20policies%20remain%20susceptible%20to%20out-of-distribution%20scenarios%2C%20ranging%20from%20external%20aerodynamic%20disturbances%20to%20internal%20hardware%20degradation.%20To%20ensure%20safety%20under%20these%20evolving%20uncertainties%2C%20such%20controllers%20are%20forced%20to%20operate%20with%20conservative%20safety%20margins%2C%20inherently%20constraining%20their%20agility%20outside%20of%20controlled%20settings.%20While%20online%20adaptation%20offers%20a%20potential%20remedy%2C%20safely%20exploring%20physical%20limits%20remains%20a%20critical%20bottleneck%20due%20to%20data%20scarcity%20and%20safety%20risks.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20self-adaptive%20framework%20that%20eliminates%20the%20need%20for%20precise%20system%20identification%20or%20offline%20Sim2Real%20transfer.%20We%20introduce%20Adaptive%20Temporal%20Scaling%20%28ATS%29%20to%20actively%20explore%20platform%20physical%20limits%2C%20and%20employ%20online%20residual%20learning%20to%20augment%20a%20simple%20nominal%20model.%20%7BBased%20on%20the%20learned%20hybrid%20model%2C%20we%20further%20propose%20Real-world%20Anchored%20Short-horizon%20Backpropagation%20Through%20Time%20%28RASH-BPTT%29%20to%20achieve%20efficient%20and%20robust%20in-flight%20policy%20updates.%20Extensive%20experiments%20demonstrate%20that%20our%20quadrotor%20reliably%20executes%20agile%20maneuvers%20near%20actuator%20saturation%20limits.%20The%20system%20evolves%20a%20conservative%20base%20policy%20with%20a%20peak%20speed%20of%201.9%20m/s%20to%207.3%20m/s%20within%20approximately%20100%20seconds%20of%20flight%20time.%20These%20findings%20underscore%20that%20real-world%20adaptation%20serves%20not%20merely%20to%20compensate%20for%20modeling%20errors%2C%20but%20as%20a%20practical%20mechanism%20for%20sustained%20performance%20improvement%20in%20aggressive%20flight%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2602.10111v1&entry.124074799=Read"},
{"title": "Block-Recurrent Dynamics in Vision Transformers", "author": "Mozes Jacobs and Thomas Fel and Richard Hakim and Alessandra Brondetta and Demba Ba and T. Andy Keller", "abstract": "As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent runtime. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.", "link": "http://arxiv.org/abs/2512.19941v3", "date": "2026-02-10", "relevancy": 2.1686, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Block-Recurrent%20Dynamics%20in%20Vision%20Transformers&body=Title%3A%20Block-Recurrent%20Dynamics%20in%20Vision%20Transformers%0AAuthor%3A%20Mozes%20Jacobs%20and%20Thomas%20Fel%20and%20Richard%20Hakim%20and%20Alessandra%20Brondetta%20and%20Demba%20Ba%20and%20T.%20Andy%20Keller%0AAbstract%3A%20As%20Vision%20Transformers%20%28ViTs%29%20become%20standard%20vision%20backbones%2C%20a%20mechanistic%20account%20of%20their%20computational%20phenomenology%20is%20essential.%20Despite%20architectural%20cues%20that%20hint%20at%20dynamical%20structure%2C%20there%20is%20no%20settled%20framework%20that%20interprets%20Transformer%20depth%20as%20a%20well-characterized%20flow.%20In%20this%20work%2C%20we%20introduce%20the%20Block-Recurrent%20Hypothesis%20%28BRH%29%2C%20arguing%20that%20trained%20ViTs%20admit%20a%20block-recurrent%20depth%20structure%20such%20that%20the%20computation%20of%20the%20original%20%24L%24%20blocks%20can%20be%20accurately%20rewritten%20using%20only%20%24k%20%5Cll%20L%24%20distinct%20blocks%20applied%20recurrently.%20Across%20diverse%20ViTs%2C%20between-layer%20representational%20similarity%20matrices%20suggest%20few%20contiguous%20phases.%20To%20determine%20whether%20these%20phases%20reflect%20genuinely%20reusable%20computation%2C%20we%20train%20block-recurrent%20surrogates%20of%20pretrained%20ViTs%3A%20Recurrent%20Approximations%20to%20Phase-structured%20TransfORmers%20%28Raptor%29.%20In%20small-scale%2C%20we%20demonstrate%20that%20stochastic%20depth%20and%20training%20promote%20recurrent%20structure%20and%20subsequently%20correlate%20with%20our%20ability%20to%20accurately%20fit%20Raptor.%20We%20then%20provide%20an%20empirical%20existence%20proof%20for%20BRH%20by%20training%20a%20Raptor%20model%20to%20recover%20%2496%5C%25%24%20of%20DINOv2%20ImageNet-1k%20linear%20probe%20accuracy%20in%20only%202%20blocks%20at%20equivalent%20runtime.%20Finally%2C%20we%20leverage%20our%20hypothesis%20to%20develop%20a%20program%20of%20Dynamical%20Interpretability.%20We%20find%20i%29%20directional%20convergence%20into%20class-dependent%20angular%20basins%20with%20self-correcting%20trajectories%20under%20small%20perturbations%2C%20ii%29%20token-specific%20dynamics%2C%20where%20cls%20executes%20sharp%20late%20reorientations%20while%20patch%20tokens%20exhibit%20strong%20late-stage%20coherence%20toward%20their%20mean%20direction%2C%20and%20iii%29%20a%20collapse%20to%20low%20rank%20updates%20in%20late%20depth%2C%20consistent%20with%20convergence%20to%20low-dimensional%20attractors.%20Altogether%2C%20we%20find%20a%20compact%20recurrent%20program%20emerges%20along%20ViT%20depth%2C%20pointing%20to%20a%20low-complexity%20normative%20solution%20that%20enables%20these%20models%20to%20be%20studied%20through%20principled%20dynamical%20systems%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19941v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlock-Recurrent%2520Dynamics%2520in%2520Vision%2520Transformers%26entry.906535625%3DMozes%2520Jacobs%2520and%2520Thomas%2520Fel%2520and%2520Richard%2520Hakim%2520and%2520Alessandra%2520Brondetta%2520and%2520Demba%2520Ba%2520and%2520T.%2520Andy%2520Keller%26entry.1292438233%3DAs%2520Vision%2520Transformers%2520%2528ViTs%2529%2520become%2520standard%2520vision%2520backbones%252C%2520a%2520mechanistic%2520account%2520of%2520their%2520computational%2520phenomenology%2520is%2520essential.%2520Despite%2520architectural%2520cues%2520that%2520hint%2520at%2520dynamical%2520structure%252C%2520there%2520is%2520no%2520settled%2520framework%2520that%2520interprets%2520Transformer%2520depth%2520as%2520a%2520well-characterized%2520flow.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Block-Recurrent%2520Hypothesis%2520%2528BRH%2529%252C%2520arguing%2520that%2520trained%2520ViTs%2520admit%2520a%2520block-recurrent%2520depth%2520structure%2520such%2520that%2520the%2520computation%2520of%2520the%2520original%2520%2524L%2524%2520blocks%2520can%2520be%2520accurately%2520rewritten%2520using%2520only%2520%2524k%2520%255Cll%2520L%2524%2520distinct%2520blocks%2520applied%2520recurrently.%2520Across%2520diverse%2520ViTs%252C%2520between-layer%2520representational%2520similarity%2520matrices%2520suggest%2520few%2520contiguous%2520phases.%2520To%2520determine%2520whether%2520these%2520phases%2520reflect%2520genuinely%2520reusable%2520computation%252C%2520we%2520train%2520block-recurrent%2520surrogates%2520of%2520pretrained%2520ViTs%253A%2520Recurrent%2520Approximations%2520to%2520Phase-structured%2520TransfORmers%2520%2528Raptor%2529.%2520In%2520small-scale%252C%2520we%2520demonstrate%2520that%2520stochastic%2520depth%2520and%2520training%2520promote%2520recurrent%2520structure%2520and%2520subsequently%2520correlate%2520with%2520our%2520ability%2520to%2520accurately%2520fit%2520Raptor.%2520We%2520then%2520provide%2520an%2520empirical%2520existence%2520proof%2520for%2520BRH%2520by%2520training%2520a%2520Raptor%2520model%2520to%2520recover%2520%252496%255C%2525%2524%2520of%2520DINOv2%2520ImageNet-1k%2520linear%2520probe%2520accuracy%2520in%2520only%25202%2520blocks%2520at%2520equivalent%2520runtime.%2520Finally%252C%2520we%2520leverage%2520our%2520hypothesis%2520to%2520develop%2520a%2520program%2520of%2520Dynamical%2520Interpretability.%2520We%2520find%2520i%2529%2520directional%2520convergence%2520into%2520class-dependent%2520angular%2520basins%2520with%2520self-correcting%2520trajectories%2520under%2520small%2520perturbations%252C%2520ii%2529%2520token-specific%2520dynamics%252C%2520where%2520cls%2520executes%2520sharp%2520late%2520reorientations%2520while%2520patch%2520tokens%2520exhibit%2520strong%2520late-stage%2520coherence%2520toward%2520their%2520mean%2520direction%252C%2520and%2520iii%2529%2520a%2520collapse%2520to%2520low%2520rank%2520updates%2520in%2520late%2520depth%252C%2520consistent%2520with%2520convergence%2520to%2520low-dimensional%2520attractors.%2520Altogether%252C%2520we%2520find%2520a%2520compact%2520recurrent%2520program%2520emerges%2520along%2520ViT%2520depth%252C%2520pointing%2520to%2520a%2520low-complexity%2520normative%2520solution%2520that%2520enables%2520these%2520models%2520to%2520be%2520studied%2520through%2520principled%2520dynamical%2520systems%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19941v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Block-Recurrent%20Dynamics%20in%20Vision%20Transformers&entry.906535625=Mozes%20Jacobs%20and%20Thomas%20Fel%20and%20Richard%20Hakim%20and%20Alessandra%20Brondetta%20and%20Demba%20Ba%20and%20T.%20Andy%20Keller&entry.1292438233=As%20Vision%20Transformers%20%28ViTs%29%20become%20standard%20vision%20backbones%2C%20a%20mechanistic%20account%20of%20their%20computational%20phenomenology%20is%20essential.%20Despite%20architectural%20cues%20that%20hint%20at%20dynamical%20structure%2C%20there%20is%20no%20settled%20framework%20that%20interprets%20Transformer%20depth%20as%20a%20well-characterized%20flow.%20In%20this%20work%2C%20we%20introduce%20the%20Block-Recurrent%20Hypothesis%20%28BRH%29%2C%20arguing%20that%20trained%20ViTs%20admit%20a%20block-recurrent%20depth%20structure%20such%20that%20the%20computation%20of%20the%20original%20%24L%24%20blocks%20can%20be%20accurately%20rewritten%20using%20only%20%24k%20%5Cll%20L%24%20distinct%20blocks%20applied%20recurrently.%20Across%20diverse%20ViTs%2C%20between-layer%20representational%20similarity%20matrices%20suggest%20few%20contiguous%20phases.%20To%20determine%20whether%20these%20phases%20reflect%20genuinely%20reusable%20computation%2C%20we%20train%20block-recurrent%20surrogates%20of%20pretrained%20ViTs%3A%20Recurrent%20Approximations%20to%20Phase-structured%20TransfORmers%20%28Raptor%29.%20In%20small-scale%2C%20we%20demonstrate%20that%20stochastic%20depth%20and%20training%20promote%20recurrent%20structure%20and%20subsequently%20correlate%20with%20our%20ability%20to%20accurately%20fit%20Raptor.%20We%20then%20provide%20an%20empirical%20existence%20proof%20for%20BRH%20by%20training%20a%20Raptor%20model%20to%20recover%20%2496%5C%25%24%20of%20DINOv2%20ImageNet-1k%20linear%20probe%20accuracy%20in%20only%202%20blocks%20at%20equivalent%20runtime.%20Finally%2C%20we%20leverage%20our%20hypothesis%20to%20develop%20a%20program%20of%20Dynamical%20Interpretability.%20We%20find%20i%29%20directional%20convergence%20into%20class-dependent%20angular%20basins%20with%20self-correcting%20trajectories%20under%20small%20perturbations%2C%20ii%29%20token-specific%20dynamics%2C%20where%20cls%20executes%20sharp%20late%20reorientations%20while%20patch%20tokens%20exhibit%20strong%20late-stage%20coherence%20toward%20their%20mean%20direction%2C%20and%20iii%29%20a%20collapse%20to%20low%20rank%20updates%20in%20late%20depth%2C%20consistent%20with%20convergence%20to%20low-dimensional%20attractors.%20Altogether%2C%20we%20find%20a%20compact%20recurrent%20program%20emerges%20along%20ViT%20depth%2C%20pointing%20to%20a%20low-complexity%20normative%20solution%20that%20enables%20these%20models%20to%20be%20studied%20through%20principled%20dynamical%20systems%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.19941v3&entry.124074799=Read"},
{"title": "Understanding Image2Video Domain Shift in Food Segmentation: An Instance-level Analysis on Apples", "author": "Keonvin Park and Aditya Pal and Jin Hong Mok", "abstract": "Food segmentation models trained on static images have achieved strong performance on benchmark datasets; however, their reliability in video settings remains poorly understood. In real-world applications such as food monitoring and instance counting, segmentation outputs must be temporally consistent, yet image-trained models often break down when deployed on videos. In this work, we analyze this failure through an instance segmentation and tracking perspective, focusing on apples as a representative food category. Models are trained solely on image-level food segmentation data and evaluated on video sequences using an instance segmentation with tracking-by-matching framework, enabling object-level temporal analysis. Our results reveal that high frame-wise segmentation accuracy does not translate to stable instance identities over time. Temporal appearance variations, particularly illumination changes, specular reflections, and texture ambiguity, lead to mask flickering and identity fragmentation, resulting in significant errors in apple counting. These failures are largely overlooked by conventional image-based metrics, which substantially overestimate real-world video performance. Beyond diagnosing the problem, we examine practical remedies that do not require full video supervision, including post-hoc temporal regularization and self-supervised temporal consistency objectives. Our findings suggest that the root cause of failure lies in image-centric training objectives that ignore temporal coherence, rather than model capacity. This study highlights a critical evaluation gap in food segmentation research and motivates temporally-aware learning and evaluation protocols for video-based food analysis.", "link": "http://arxiv.org/abs/2602.08491v2", "date": "2026-02-10", "relevancy": 2.1614, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Image2Video%20Domain%20Shift%20in%20Food%20Segmentation%3A%20An%20Instance-level%20Analysis%20on%20Apples&body=Title%3A%20Understanding%20Image2Video%20Domain%20Shift%20in%20Food%20Segmentation%3A%20An%20Instance-level%20Analysis%20on%20Apples%0AAuthor%3A%20Keonvin%20Park%20and%20Aditya%20Pal%20and%20Jin%20Hong%20Mok%0AAbstract%3A%20Food%20segmentation%20models%20trained%20on%20static%20images%20have%20achieved%20strong%20performance%20on%20benchmark%20datasets%3B%20however%2C%20their%20reliability%20in%20video%20settings%20remains%20poorly%20understood.%20In%20real-world%20applications%20such%20as%20food%20monitoring%20and%20instance%20counting%2C%20segmentation%20outputs%20must%20be%20temporally%20consistent%2C%20yet%20image-trained%20models%20often%20break%20down%20when%20deployed%20on%20videos.%20In%20this%20work%2C%20we%20analyze%20this%20failure%20through%20an%20instance%20segmentation%20and%20tracking%20perspective%2C%20focusing%20on%20apples%20as%20a%20representative%20food%20category.%20Models%20are%20trained%20solely%20on%20image-level%20food%20segmentation%20data%20and%20evaluated%20on%20video%20sequences%20using%20an%20instance%20segmentation%20with%20tracking-by-matching%20framework%2C%20enabling%20object-level%20temporal%20analysis.%20Our%20results%20reveal%20that%20high%20frame-wise%20segmentation%20accuracy%20does%20not%20translate%20to%20stable%20instance%20identities%20over%20time.%20Temporal%20appearance%20variations%2C%20particularly%20illumination%20changes%2C%20specular%20reflections%2C%20and%20texture%20ambiguity%2C%20lead%20to%20mask%20flickering%20and%20identity%20fragmentation%2C%20resulting%20in%20significant%20errors%20in%20apple%20counting.%20These%20failures%20are%20largely%20overlooked%20by%20conventional%20image-based%20metrics%2C%20which%20substantially%20overestimate%20real-world%20video%20performance.%20Beyond%20diagnosing%20the%20problem%2C%20we%20examine%20practical%20remedies%20that%20do%20not%20require%20full%20video%20supervision%2C%20including%20post-hoc%20temporal%20regularization%20and%20self-supervised%20temporal%20consistency%20objectives.%20Our%20findings%20suggest%20that%20the%20root%20cause%20of%20failure%20lies%20in%20image-centric%20training%20objectives%20that%20ignore%20temporal%20coherence%2C%20rather%20than%20model%20capacity.%20This%20study%20highlights%20a%20critical%20evaluation%20gap%20in%20food%20segmentation%20research%20and%20motivates%20temporally-aware%20learning%20and%20evaluation%20protocols%20for%20video-based%20food%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Image2Video%2520Domain%2520Shift%2520in%2520Food%2520Segmentation%253A%2520An%2520Instance-level%2520Analysis%2520on%2520Apples%26entry.906535625%3DKeonvin%2520Park%2520and%2520Aditya%2520Pal%2520and%2520Jin%2520Hong%2520Mok%26entry.1292438233%3DFood%2520segmentation%2520models%2520trained%2520on%2520static%2520images%2520have%2520achieved%2520strong%2520performance%2520on%2520benchmark%2520datasets%253B%2520however%252C%2520their%2520reliability%2520in%2520video%2520settings%2520remains%2520poorly%2520understood.%2520In%2520real-world%2520applications%2520such%2520as%2520food%2520monitoring%2520and%2520instance%2520counting%252C%2520segmentation%2520outputs%2520must%2520be%2520temporally%2520consistent%252C%2520yet%2520image-trained%2520models%2520often%2520break%2520down%2520when%2520deployed%2520on%2520videos.%2520In%2520this%2520work%252C%2520we%2520analyze%2520this%2520failure%2520through%2520an%2520instance%2520segmentation%2520and%2520tracking%2520perspective%252C%2520focusing%2520on%2520apples%2520as%2520a%2520representative%2520food%2520category.%2520Models%2520are%2520trained%2520solely%2520on%2520image-level%2520food%2520segmentation%2520data%2520and%2520evaluated%2520on%2520video%2520sequences%2520using%2520an%2520instance%2520segmentation%2520with%2520tracking-by-matching%2520framework%252C%2520enabling%2520object-level%2520temporal%2520analysis.%2520Our%2520results%2520reveal%2520that%2520high%2520frame-wise%2520segmentation%2520accuracy%2520does%2520not%2520translate%2520to%2520stable%2520instance%2520identities%2520over%2520time.%2520Temporal%2520appearance%2520variations%252C%2520particularly%2520illumination%2520changes%252C%2520specular%2520reflections%252C%2520and%2520texture%2520ambiguity%252C%2520lead%2520to%2520mask%2520flickering%2520and%2520identity%2520fragmentation%252C%2520resulting%2520in%2520significant%2520errors%2520in%2520apple%2520counting.%2520These%2520failures%2520are%2520largely%2520overlooked%2520by%2520conventional%2520image-based%2520metrics%252C%2520which%2520substantially%2520overestimate%2520real-world%2520video%2520performance.%2520Beyond%2520diagnosing%2520the%2520problem%252C%2520we%2520examine%2520practical%2520remedies%2520that%2520do%2520not%2520require%2520full%2520video%2520supervision%252C%2520including%2520post-hoc%2520temporal%2520regularization%2520and%2520self-supervised%2520temporal%2520consistency%2520objectives.%2520Our%2520findings%2520suggest%2520that%2520the%2520root%2520cause%2520of%2520failure%2520lies%2520in%2520image-centric%2520training%2520objectives%2520that%2520ignore%2520temporal%2520coherence%252C%2520rather%2520than%2520model%2520capacity.%2520This%2520study%2520highlights%2520a%2520critical%2520evaluation%2520gap%2520in%2520food%2520segmentation%2520research%2520and%2520motivates%2520temporally-aware%2520learning%2520and%2520evaluation%2520protocols%2520for%2520video-based%2520food%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Image2Video%20Domain%20Shift%20in%20Food%20Segmentation%3A%20An%20Instance-level%20Analysis%20on%20Apples&entry.906535625=Keonvin%20Park%20and%20Aditya%20Pal%20and%20Jin%20Hong%20Mok&entry.1292438233=Food%20segmentation%20models%20trained%20on%20static%20images%20have%20achieved%20strong%20performance%20on%20benchmark%20datasets%3B%20however%2C%20their%20reliability%20in%20video%20settings%20remains%20poorly%20understood.%20In%20real-world%20applications%20such%20as%20food%20monitoring%20and%20instance%20counting%2C%20segmentation%20outputs%20must%20be%20temporally%20consistent%2C%20yet%20image-trained%20models%20often%20break%20down%20when%20deployed%20on%20videos.%20In%20this%20work%2C%20we%20analyze%20this%20failure%20through%20an%20instance%20segmentation%20and%20tracking%20perspective%2C%20focusing%20on%20apples%20as%20a%20representative%20food%20category.%20Models%20are%20trained%20solely%20on%20image-level%20food%20segmentation%20data%20and%20evaluated%20on%20video%20sequences%20using%20an%20instance%20segmentation%20with%20tracking-by-matching%20framework%2C%20enabling%20object-level%20temporal%20analysis.%20Our%20results%20reveal%20that%20high%20frame-wise%20segmentation%20accuracy%20does%20not%20translate%20to%20stable%20instance%20identities%20over%20time.%20Temporal%20appearance%20variations%2C%20particularly%20illumination%20changes%2C%20specular%20reflections%2C%20and%20texture%20ambiguity%2C%20lead%20to%20mask%20flickering%20and%20identity%20fragmentation%2C%20resulting%20in%20significant%20errors%20in%20apple%20counting.%20These%20failures%20are%20largely%20overlooked%20by%20conventional%20image-based%20metrics%2C%20which%20substantially%20overestimate%20real-world%20video%20performance.%20Beyond%20diagnosing%20the%20problem%2C%20we%20examine%20practical%20remedies%20that%20do%20not%20require%20full%20video%20supervision%2C%20including%20post-hoc%20temporal%20regularization%20and%20self-supervised%20temporal%20consistency%20objectives.%20Our%20findings%20suggest%20that%20the%20root%20cause%20of%20failure%20lies%20in%20image-centric%20training%20objectives%20that%20ignore%20temporal%20coherence%2C%20rather%20than%20model%20capacity.%20This%20study%20highlights%20a%20critical%20evaluation%20gap%20in%20food%20segmentation%20research%20and%20motivates%20temporally-aware%20learning%20and%20evaluation%20protocols%20for%20video-based%20food%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2602.08491v2&entry.124074799=Read"},
{"title": "Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices", "author": "Manon Reusens and Sofie Goethals and Toon Calders and David Martens", "abstract": "As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.", "link": "http://arxiv.org/abs/2602.09802v1", "date": "2026-02-10", "relevancy": 2.1596, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4334}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Would%20a%20Large%20Language%20Model%20Pay%20Extra%20for%20a%20View%3F%20Inferring%20Willingness%20to%20Pay%20from%20Subjective%20Choices&body=Title%3A%20Would%20a%20Large%20Language%20Model%20Pay%20Extra%20for%20a%20View%3F%20Inferring%20Willingness%20to%20Pay%20from%20Subjective%20Choices%0AAuthor%3A%20Manon%20Reusens%20and%20Sofie%20Goethals%20and%20Toon%20Calders%20and%20David%20Martens%0AAbstract%3A%20As%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20applications%20such%20as%20travel%20assistance%20and%20purchasing%20support%2C%20they%20are%20often%20required%20to%20make%20subjective%20choices%20on%20behalf%20of%20users%20in%20settings%20where%20no%20objectively%20correct%20answer%20exists.%20We%20study%20LLM%20decision-making%20in%20a%20travel-assistant%20context%20by%20presenting%20models%20with%20choice%20dilemmas%20and%20analyzing%20their%20responses%20using%20multinomial%20logit%20models%20to%20derive%20implied%20willingness%20to%20pay%20%28WTP%29%20estimates.%20These%20WTP%20values%20are%20subsequently%20compared%20to%20human%20benchmark%20values%20from%20the%20economics%20literature.%20In%20addition%20to%20a%20baseline%20setting%2C%20we%20examine%20how%20model%20behavior%20changes%20under%20more%20realistic%20conditions%2C%20including%20the%20provision%20of%20information%20about%20users%27%20past%20choices%20and%20persona-based%20prompting.%20Our%20results%20show%20that%20while%20meaningful%20WTP%20values%20can%20be%20derived%20for%20larger%20LLMs%2C%20they%20also%20display%20systematic%20deviations%20at%20the%20attribute%20level.%20Additionally%2C%20they%20tend%20to%20overestimate%20human%20WTP%20overall%2C%20particularly%20when%20expensive%20options%20or%20business-oriented%20personas%20are%20introduced.%20Conditioning%20models%20on%20prior%20preferences%20for%20cheaper%20options%20yields%20valuations%20that%20are%20closer%20to%20human%20benchmarks.%20Overall%2C%20our%20findings%20highlight%20both%20the%20potential%20and%20the%20limitations%20of%20using%20LLMs%20for%20subjective%20decision%20support%20and%20underscore%20the%20importance%20of%20careful%20model%20selection%2C%20prompt%20design%2C%20and%20user%20representation%20when%20deploying%20such%20systems%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWould%2520a%2520Large%2520Language%2520Model%2520Pay%2520Extra%2520for%2520a%2520View%253F%2520Inferring%2520Willingness%2520to%2520Pay%2520from%2520Subjective%2520Choices%26entry.906535625%3DManon%2520Reusens%2520and%2520Sofie%2520Goethals%2520and%2520Toon%2520Calders%2520and%2520David%2520Martens%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520applications%2520such%2520as%2520travel%2520assistance%2520and%2520purchasing%2520support%252C%2520they%2520are%2520often%2520required%2520to%2520make%2520subjective%2520choices%2520on%2520behalf%2520of%2520users%2520in%2520settings%2520where%2520no%2520objectively%2520correct%2520answer%2520exists.%2520We%2520study%2520LLM%2520decision-making%2520in%2520a%2520travel-assistant%2520context%2520by%2520presenting%2520models%2520with%2520choice%2520dilemmas%2520and%2520analyzing%2520their%2520responses%2520using%2520multinomial%2520logit%2520models%2520to%2520derive%2520implied%2520willingness%2520to%2520pay%2520%2528WTP%2529%2520estimates.%2520These%2520WTP%2520values%2520are%2520subsequently%2520compared%2520to%2520human%2520benchmark%2520values%2520from%2520the%2520economics%2520literature.%2520In%2520addition%2520to%2520a%2520baseline%2520setting%252C%2520we%2520examine%2520how%2520model%2520behavior%2520changes%2520under%2520more%2520realistic%2520conditions%252C%2520including%2520the%2520provision%2520of%2520information%2520about%2520users%2527%2520past%2520choices%2520and%2520persona-based%2520prompting.%2520Our%2520results%2520show%2520that%2520while%2520meaningful%2520WTP%2520values%2520can%2520be%2520derived%2520for%2520larger%2520LLMs%252C%2520they%2520also%2520display%2520systematic%2520deviations%2520at%2520the%2520attribute%2520level.%2520Additionally%252C%2520they%2520tend%2520to%2520overestimate%2520human%2520WTP%2520overall%252C%2520particularly%2520when%2520expensive%2520options%2520or%2520business-oriented%2520personas%2520are%2520introduced.%2520Conditioning%2520models%2520on%2520prior%2520preferences%2520for%2520cheaper%2520options%2520yields%2520valuations%2520that%2520are%2520closer%2520to%2520human%2520benchmarks.%2520Overall%252C%2520our%2520findings%2520highlight%2520both%2520the%2520potential%2520and%2520the%2520limitations%2520of%2520using%2520LLMs%2520for%2520subjective%2520decision%2520support%2520and%2520underscore%2520the%2520importance%2520of%2520careful%2520model%2520selection%252C%2520prompt%2520design%252C%2520and%2520user%2520representation%2520when%2520deploying%2520such%2520systems%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Would%20a%20Large%20Language%20Model%20Pay%20Extra%20for%20a%20View%3F%20Inferring%20Willingness%20to%20Pay%20from%20Subjective%20Choices&entry.906535625=Manon%20Reusens%20and%20Sofie%20Goethals%20and%20Toon%20Calders%20and%20David%20Martens&entry.1292438233=As%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20applications%20such%20as%20travel%20assistance%20and%20purchasing%20support%2C%20they%20are%20often%20required%20to%20make%20subjective%20choices%20on%20behalf%20of%20users%20in%20settings%20where%20no%20objectively%20correct%20answer%20exists.%20We%20study%20LLM%20decision-making%20in%20a%20travel-assistant%20context%20by%20presenting%20models%20with%20choice%20dilemmas%20and%20analyzing%20their%20responses%20using%20multinomial%20logit%20models%20to%20derive%20implied%20willingness%20to%20pay%20%28WTP%29%20estimates.%20These%20WTP%20values%20are%20subsequently%20compared%20to%20human%20benchmark%20values%20from%20the%20economics%20literature.%20In%20addition%20to%20a%20baseline%20setting%2C%20we%20examine%20how%20model%20behavior%20changes%20under%20more%20realistic%20conditions%2C%20including%20the%20provision%20of%20information%20about%20users%27%20past%20choices%20and%20persona-based%20prompting.%20Our%20results%20show%20that%20while%20meaningful%20WTP%20values%20can%20be%20derived%20for%20larger%20LLMs%2C%20they%20also%20display%20systematic%20deviations%20at%20the%20attribute%20level.%20Additionally%2C%20they%20tend%20to%20overestimate%20human%20WTP%20overall%2C%20particularly%20when%20expensive%20options%20or%20business-oriented%20personas%20are%20introduced.%20Conditioning%20models%20on%20prior%20preferences%20for%20cheaper%20options%20yields%20valuations%20that%20are%20closer%20to%20human%20benchmarks.%20Overall%2C%20our%20findings%20highlight%20both%20the%20potential%20and%20the%20limitations%20of%20using%20LLMs%20for%20subjective%20decision%20support%20and%20underscore%20the%20importance%20of%20careful%20model%20selection%2C%20prompt%20design%2C%20and%20user%20representation%20when%20deploying%20such%20systems%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2602.09802v1&entry.124074799=Read"},
{"title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations", "author": "William Lugoloobi and Thomas Foster and William Bankes and Chris Russell", "abstract": "Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty", "link": "http://arxiv.org/abs/2602.09924v1", "date": "2026-02-10", "relevancy": 2.1528, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Encode%20Their%20Failures%3A%20Predicting%20Success%20from%20Pre-Generation%20Activations&body=Title%3A%20LLMs%20Encode%20Their%20Failures%3A%20Predicting%20Success%20from%20Pre-Generation%20Activations%0AAuthor%3A%20William%20Lugoloobi%20and%20Thomas%20Foster%20and%20William%20Bankes%20and%20Chris%20Russell%0AAbstract%3A%20Running%20LLMs%20with%20extended%20reasoning%20on%20every%20problem%20is%20expensive%2C%20but%20determining%20which%20inputs%20actually%20require%20additional%20compute%20remains%20challenging.%20We%20investigate%20whether%20their%20own%20likelihood%20of%20success%20is%20recoverable%20from%20their%20internal%20representations%20before%20generation%2C%20and%20if%20this%20signal%20can%20guide%20more%20efficient%20inference.%20We%20train%20linear%20probes%20on%20pre-generation%20activations%20to%20predict%20policy-specific%20success%20on%20math%20and%20coding%20tasks%2C%20substantially%20outperforming%20surface%20features%20such%20as%20question%20length%20and%20TF-IDF.%20Using%20E2H-AMC%2C%20which%20provides%20both%20human%20and%20model%20performance%20on%20identical%20problems%2C%20we%20show%20that%20models%20encode%20a%20model-specific%20notion%20of%20difficulty%20that%20is%20distinct%20from%20human%20difficulty%2C%20and%20that%20this%20distinction%20increases%20with%20extended%20reasoning.%20Leveraging%20these%20probes%2C%20we%20demonstrate%20that%20routing%20queries%20across%20a%20pool%20of%20models%20can%20exceed%20the%20best-performing%20model%20whilst%20reducing%20inference%20cost%20by%20up%20to%2070%5C%25%20on%20MATH%2C%20showing%20that%20internal%20representations%20enable%20practical%20efficiency%20gains%20even%20when%20they%20diverge%20from%20human%20intuitions%20about%20difficulty.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/KabakaWilliam/llms_know_difficulty%0ALink%3A%20http%3A//arxiv.org/abs/2602.09924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Encode%2520Their%2520Failures%253A%2520Predicting%2520Success%2520from%2520Pre-Generation%2520Activations%26entry.906535625%3DWilliam%2520Lugoloobi%2520and%2520Thomas%2520Foster%2520and%2520William%2520Bankes%2520and%2520Chris%2520Russell%26entry.1292438233%3DRunning%2520LLMs%2520with%2520extended%2520reasoning%2520on%2520every%2520problem%2520is%2520expensive%252C%2520but%2520determining%2520which%2520inputs%2520actually%2520require%2520additional%2520compute%2520remains%2520challenging.%2520We%2520investigate%2520whether%2520their%2520own%2520likelihood%2520of%2520success%2520is%2520recoverable%2520from%2520their%2520internal%2520representations%2520before%2520generation%252C%2520and%2520if%2520this%2520signal%2520can%2520guide%2520more%2520efficient%2520inference.%2520We%2520train%2520linear%2520probes%2520on%2520pre-generation%2520activations%2520to%2520predict%2520policy-specific%2520success%2520on%2520math%2520and%2520coding%2520tasks%252C%2520substantially%2520outperforming%2520surface%2520features%2520such%2520as%2520question%2520length%2520and%2520TF-IDF.%2520Using%2520E2H-AMC%252C%2520which%2520provides%2520both%2520human%2520and%2520model%2520performance%2520on%2520identical%2520problems%252C%2520we%2520show%2520that%2520models%2520encode%2520a%2520model-specific%2520notion%2520of%2520difficulty%2520that%2520is%2520distinct%2520from%2520human%2520difficulty%252C%2520and%2520that%2520this%2520distinction%2520increases%2520with%2520extended%2520reasoning.%2520Leveraging%2520these%2520probes%252C%2520we%2520demonstrate%2520that%2520routing%2520queries%2520across%2520a%2520pool%2520of%2520models%2520can%2520exceed%2520the%2520best-performing%2520model%2520whilst%2520reducing%2520inference%2520cost%2520by%2520up%2520to%252070%255C%2525%2520on%2520MATH%252C%2520showing%2520that%2520internal%2520representations%2520enable%2520practical%2520efficiency%2520gains%2520even%2520when%2520they%2520diverge%2520from%2520human%2520intuitions%2520about%2520difficulty.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/KabakaWilliam/llms_know_difficulty%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Encode%20Their%20Failures%3A%20Predicting%20Success%20from%20Pre-Generation%20Activations&entry.906535625=William%20Lugoloobi%20and%20Thomas%20Foster%20and%20William%20Bankes%20and%20Chris%20Russell&entry.1292438233=Running%20LLMs%20with%20extended%20reasoning%20on%20every%20problem%20is%20expensive%2C%20but%20determining%20which%20inputs%20actually%20require%20additional%20compute%20remains%20challenging.%20We%20investigate%20whether%20their%20own%20likelihood%20of%20success%20is%20recoverable%20from%20their%20internal%20representations%20before%20generation%2C%20and%20if%20this%20signal%20can%20guide%20more%20efficient%20inference.%20We%20train%20linear%20probes%20on%20pre-generation%20activations%20to%20predict%20policy-specific%20success%20on%20math%20and%20coding%20tasks%2C%20substantially%20outperforming%20surface%20features%20such%20as%20question%20length%20and%20TF-IDF.%20Using%20E2H-AMC%2C%20which%20provides%20both%20human%20and%20model%20performance%20on%20identical%20problems%2C%20we%20show%20that%20models%20encode%20a%20model-specific%20notion%20of%20difficulty%20that%20is%20distinct%20from%20human%20difficulty%2C%20and%20that%20this%20distinction%20increases%20with%20extended%20reasoning.%20Leveraging%20these%20probes%2C%20we%20demonstrate%20that%20routing%20queries%20across%20a%20pool%20of%20models%20can%20exceed%20the%20best-performing%20model%20whilst%20reducing%20inference%20cost%20by%20up%20to%2070%5C%25%20on%20MATH%2C%20showing%20that%20internal%20representations%20enable%20practical%20efficiency%20gains%20even%20when%20they%20diverge%20from%20human%20intuitions%20about%20difficulty.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/KabakaWilliam/llms_know_difficulty&entry.1838667208=http%3A//arxiv.org/abs/2602.09924v1&entry.124074799=Read"},
{"title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance", "author": "Xinrong Chen and Xu Chu and Yingmin Qiu and Hengyuan Zhang and Jing Xiong and Shiyu Tang and Shuai Liu and Shaokang Yang and Cheng Yang and Hayden Kwok-Hay So and Ngai Wong", "abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.", "link": "http://arxiv.org/abs/2602.01047v2", "date": "2026-02-10", "relevancy": 2.1523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Decoding%3A%20Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%20History-Aware%20Residual%20Guidance&body=Title%3A%20Residual%20Decoding%3A%20Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%20History-Aware%20Residual%20Guidance%0AAuthor%3A%20Xinrong%20Chen%20and%20Xu%20Chu%20and%20Yingmin%20Qiu%20and%20Hengyuan%20Zhang%20and%20Jing%20Xiong%20and%20Shiyu%20Tang%20and%20Shuai%20Liu%20and%20Shaokang%20Yang%20and%20Cheng%20Yang%20and%20Hayden%20Kwok-Hay%20So%20and%20Ngai%20Wong%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20can%20reason%20effectively%20from%20image-text%20inputs%20and%20perform%20well%20in%20various%20multimodal%20tasks.%20Despite%20this%20success%2C%20they%20are%20affected%20by%20language%20priors%20and%20often%20produce%20hallucinations.%20Hallucinations%20denote%20generated%20content%20that%20is%20grammatically%20and%20syntactically%20coherent%2C%20yet%20bears%20no%20match%20or%20direct%20relevance%20to%20actual%20visual%20input.%20To%20address%20this%20problem%2C%20we%20propose%20Residual%20Decoding%20%28ResDec%29.%20It%20is%20a%20novel%20training-free%20method%20that%20uses%20historical%20information%20to%20aid%20decoding.%20The%20method%20relies%20on%20the%20internal%20implicit%20reasoning%20mechanism%20and%20token%20logits%20evolution%20mechanism%20of%20LVLMs%20to%20correct%20biases.%20Extensive%20experiments%20demonstrate%20that%20ResDec%20effectively%20suppresses%20hallucinations%20induced%20by%20language%20priors%2C%20significantly%20improves%20visual%20grounding%2C%20and%20reduces%20object%20hallucinations.%20In%20addition%20to%20mitigating%20hallucinations%2C%20ResDec%20also%20performs%20exceptionally%20well%20on%20comprehensive%20LVLM%20benchmarks%2C%20highlighting%20its%20broad%20applicability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Decoding%253A%2520Mitigating%2520Hallucinations%2520in%2520Large%2520Vision-Language%2520Models%2520via%2520History-Aware%2520Residual%2520Guidance%26entry.906535625%3DXinrong%2520Chen%2520and%2520Xu%2520Chu%2520and%2520Yingmin%2520Qiu%2520and%2520Hengyuan%2520Zhang%2520and%2520Jing%2520Xiong%2520and%2520Shiyu%2520Tang%2520and%2520Shuai%2520Liu%2520and%2520Shaokang%2520Yang%2520and%2520Cheng%2520Yang%2520and%2520Hayden%2520Kwok-Hay%2520So%2520and%2520Ngai%2520Wong%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520can%2520reason%2520effectively%2520from%2520image-text%2520inputs%2520and%2520perform%2520well%2520in%2520various%2520multimodal%2520tasks.%2520Despite%2520this%2520success%252C%2520they%2520are%2520affected%2520by%2520language%2520priors%2520and%2520often%2520produce%2520hallucinations.%2520Hallucinations%2520denote%2520generated%2520content%2520that%2520is%2520grammatically%2520and%2520syntactically%2520coherent%252C%2520yet%2520bears%2520no%2520match%2520or%2520direct%2520relevance%2520to%2520actual%2520visual%2520input.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Residual%2520Decoding%2520%2528ResDec%2529.%2520It%2520is%2520a%2520novel%2520training-free%2520method%2520that%2520uses%2520historical%2520information%2520to%2520aid%2520decoding.%2520The%2520method%2520relies%2520on%2520the%2520internal%2520implicit%2520reasoning%2520mechanism%2520and%2520token%2520logits%2520evolution%2520mechanism%2520of%2520LVLMs%2520to%2520correct%2520biases.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ResDec%2520effectively%2520suppresses%2520hallucinations%2520induced%2520by%2520language%2520priors%252C%2520significantly%2520improves%2520visual%2520grounding%252C%2520and%2520reduces%2520object%2520hallucinations.%2520In%2520addition%2520to%2520mitigating%2520hallucinations%252C%2520ResDec%2520also%2520performs%2520exceptionally%2520well%2520on%2520comprehensive%2520LVLM%2520benchmarks%252C%2520highlighting%2520its%2520broad%2520applicability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Decoding%3A%20Mitigating%20Hallucinations%20in%20Large%20Vision-Language%20Models%20via%20History-Aware%20Residual%20Guidance&entry.906535625=Xinrong%20Chen%20and%20Xu%20Chu%20and%20Yingmin%20Qiu%20and%20Hengyuan%20Zhang%20and%20Jing%20Xiong%20and%20Shiyu%20Tang%20and%20Shuai%20Liu%20and%20Shaokang%20Yang%20and%20Cheng%20Yang%20and%20Hayden%20Kwok-Hay%20So%20and%20Ngai%20Wong&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20can%20reason%20effectively%20from%20image-text%20inputs%20and%20perform%20well%20in%20various%20multimodal%20tasks.%20Despite%20this%20success%2C%20they%20are%20affected%20by%20language%20priors%20and%20often%20produce%20hallucinations.%20Hallucinations%20denote%20generated%20content%20that%20is%20grammatically%20and%20syntactically%20coherent%2C%20yet%20bears%20no%20match%20or%20direct%20relevance%20to%20actual%20visual%20input.%20To%20address%20this%20problem%2C%20we%20propose%20Residual%20Decoding%20%28ResDec%29.%20It%20is%20a%20novel%20training-free%20method%20that%20uses%20historical%20information%20to%20aid%20decoding.%20The%20method%20relies%20on%20the%20internal%20implicit%20reasoning%20mechanism%20and%20token%20logits%20evolution%20mechanism%20of%20LVLMs%20to%20correct%20biases.%20Extensive%20experiments%20demonstrate%20that%20ResDec%20effectively%20suppresses%20hallucinations%20induced%20by%20language%20priors%2C%20significantly%20improves%20visual%20grounding%2C%20and%20reduces%20object%20hallucinations.%20In%20addition%20to%20mitigating%20hallucinations%2C%20ResDec%20also%20performs%20exceptionally%20well%20on%20comprehensive%20LVLM%20benchmarks%2C%20highlighting%20its%20broad%20applicability.&entry.1838667208=http%3A//arxiv.org/abs/2602.01047v2&entry.124074799=Read"},
{"title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs", "author": "Richard Bornemann and Pierluigi Vito Amadori and Antoine Cully", "abstract": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\\href{https://sites.google.com/view/code-sharp/homepage}{here}$.", "link": "http://arxiv.org/abs/2602.10085v1", "date": "2026-02-10", "relevancy": 2.152, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5642}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5481}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CODE-SHARP%3A%20Continuous%20Open-ended%20Discovery%20and%20Evolution%20of%20Skills%20as%20Hierarchical%20Reward%20Programs&body=Title%3A%20CODE-SHARP%3A%20Continuous%20Open-ended%20Discovery%20and%20Evolution%20of%20Skills%20as%20Hierarchical%20Reward%20Programs%0AAuthor%3A%20Richard%20Bornemann%20and%20Pierluigi%20Vito%20Amadori%20and%20Antoine%20Cully%0AAbstract%3A%20Developing%20agents%20capable%20of%20open-endedly%20discovering%20and%20learning%20novel%20skills%20is%20a%20grand%20challenge%20in%20Artificial%20Intelligence.%20While%20reinforcement%20learning%20offers%20a%20powerful%20framework%20for%20training%20agents%20to%20master%20complex%20skills%2C%20it%20typically%20relies%20on%20hand-designed%20reward%20functions.%20This%20is%20infeasible%20for%20open-ended%20skill%20discovery%2C%20where%20the%20set%20of%20meaningful%20skills%20is%20not%20known%20a%20priori.%20While%20recent%20methods%20have%20shown%20promising%20results%20towards%20automating%20reward%20function%20design%2C%20they%20remain%20limited%20to%20refining%20rewards%20for%20pre-defined%20tasks.%20To%20address%20this%20limitation%2C%20we%20introduce%20Continuous%20Open-ended%20Discovery%20and%20Evolution%20of%20Skills%20as%20Hierarchical%20Reward%20Programs%20%28CODE-SHARP%29%2C%20a%20novel%20framework%20leveraging%20Foundation%20Models%20%28FM%29%20to%20open-endedly%20expand%20and%20refine%20a%20hierarchical%20skill%20archive%2C%20structured%20as%20a%20directed%20graph%20of%20executable%20reward%20functions%20in%20code.%20We%20show%20that%20a%20goal-conditioned%20agent%20trained%20exclusively%20on%20the%20rewards%20generated%20by%20the%20discovered%20SHARP%20skills%20learns%20to%20solve%20increasingly%20long-horizon%20goals%20in%20the%20Craftax%20environment.%20When%20composed%20by%20a%20high-level%20FM-based%20planner%2C%20the%20discovered%20skills%20enable%20a%20single%20goal-conditioned%20agent%20to%20solve%20complex%2C%20long-horizon%20tasks%2C%20outperforming%20both%20pretrained%20agents%20and%20task-specific%20expert%20policies%20by%20over%20%24134%24%25%20on%20average.%20We%20will%20open-source%20our%20code%20and%20provide%20additional%20videos%20%24%5Chref%7Bhttps%3A//sites.google.com/view/code-sharp/homepage%7D%7Bhere%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCODE-SHARP%253A%2520Continuous%2520Open-ended%2520Discovery%2520and%2520Evolution%2520of%2520Skills%2520as%2520Hierarchical%2520Reward%2520Programs%26entry.906535625%3DRichard%2520Bornemann%2520and%2520Pierluigi%2520Vito%2520Amadori%2520and%2520Antoine%2520Cully%26entry.1292438233%3DDeveloping%2520agents%2520capable%2520of%2520open-endedly%2520discovering%2520and%2520learning%2520novel%2520skills%2520is%2520a%2520grand%2520challenge%2520in%2520Artificial%2520Intelligence.%2520While%2520reinforcement%2520learning%2520offers%2520a%2520powerful%2520framework%2520for%2520training%2520agents%2520to%2520master%2520complex%2520skills%252C%2520it%2520typically%2520relies%2520on%2520hand-designed%2520reward%2520functions.%2520This%2520is%2520infeasible%2520for%2520open-ended%2520skill%2520discovery%252C%2520where%2520the%2520set%2520of%2520meaningful%2520skills%2520is%2520not%2520known%2520a%2520priori.%2520While%2520recent%2520methods%2520have%2520shown%2520promising%2520results%2520towards%2520automating%2520reward%2520function%2520design%252C%2520they%2520remain%2520limited%2520to%2520refining%2520rewards%2520for%2520pre-defined%2520tasks.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Continuous%2520Open-ended%2520Discovery%2520and%2520Evolution%2520of%2520Skills%2520as%2520Hierarchical%2520Reward%2520Programs%2520%2528CODE-SHARP%2529%252C%2520a%2520novel%2520framework%2520leveraging%2520Foundation%2520Models%2520%2528FM%2529%2520to%2520open-endedly%2520expand%2520and%2520refine%2520a%2520hierarchical%2520skill%2520archive%252C%2520structured%2520as%2520a%2520directed%2520graph%2520of%2520executable%2520reward%2520functions%2520in%2520code.%2520We%2520show%2520that%2520a%2520goal-conditioned%2520agent%2520trained%2520exclusively%2520on%2520the%2520rewards%2520generated%2520by%2520the%2520discovered%2520SHARP%2520skills%2520learns%2520to%2520solve%2520increasingly%2520long-horizon%2520goals%2520in%2520the%2520Craftax%2520environment.%2520When%2520composed%2520by%2520a%2520high-level%2520FM-based%2520planner%252C%2520the%2520discovered%2520skills%2520enable%2520a%2520single%2520goal-conditioned%2520agent%2520to%2520solve%2520complex%252C%2520long-horizon%2520tasks%252C%2520outperforming%2520both%2520pretrained%2520agents%2520and%2520task-specific%2520expert%2520policies%2520by%2520over%2520%2524134%2524%2525%2520on%2520average.%2520We%2520will%2520open-source%2520our%2520code%2520and%2520provide%2520additional%2520videos%2520%2524%255Chref%257Bhttps%253A//sites.google.com/view/code-sharp/homepage%257D%257Bhere%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CODE-SHARP%3A%20Continuous%20Open-ended%20Discovery%20and%20Evolution%20of%20Skills%20as%20Hierarchical%20Reward%20Programs&entry.906535625=Richard%20Bornemann%20and%20Pierluigi%20Vito%20Amadori%20and%20Antoine%20Cully&entry.1292438233=Developing%20agents%20capable%20of%20open-endedly%20discovering%20and%20learning%20novel%20skills%20is%20a%20grand%20challenge%20in%20Artificial%20Intelligence.%20While%20reinforcement%20learning%20offers%20a%20powerful%20framework%20for%20training%20agents%20to%20master%20complex%20skills%2C%20it%20typically%20relies%20on%20hand-designed%20reward%20functions.%20This%20is%20infeasible%20for%20open-ended%20skill%20discovery%2C%20where%20the%20set%20of%20meaningful%20skills%20is%20not%20known%20a%20priori.%20While%20recent%20methods%20have%20shown%20promising%20results%20towards%20automating%20reward%20function%20design%2C%20they%20remain%20limited%20to%20refining%20rewards%20for%20pre-defined%20tasks.%20To%20address%20this%20limitation%2C%20we%20introduce%20Continuous%20Open-ended%20Discovery%20and%20Evolution%20of%20Skills%20as%20Hierarchical%20Reward%20Programs%20%28CODE-SHARP%29%2C%20a%20novel%20framework%20leveraging%20Foundation%20Models%20%28FM%29%20to%20open-endedly%20expand%20and%20refine%20a%20hierarchical%20skill%20archive%2C%20structured%20as%20a%20directed%20graph%20of%20executable%20reward%20functions%20in%20code.%20We%20show%20that%20a%20goal-conditioned%20agent%20trained%20exclusively%20on%20the%20rewards%20generated%20by%20the%20discovered%20SHARP%20skills%20learns%20to%20solve%20increasingly%20long-horizon%20goals%20in%20the%20Craftax%20environment.%20When%20composed%20by%20a%20high-level%20FM-based%20planner%2C%20the%20discovered%20skills%20enable%20a%20single%20goal-conditioned%20agent%20to%20solve%20complex%2C%20long-horizon%20tasks%2C%20outperforming%20both%20pretrained%20agents%20and%20task-specific%20expert%20policies%20by%20over%20%24134%24%25%20on%20average.%20We%20will%20open-source%20our%20code%20and%20provide%20additional%20videos%20%24%5Chref%7Bhttps%3A//sites.google.com/view/code-sharp/homepage%7D%7Bhere%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2602.10085v1&entry.124074799=Read"},
{"title": "Conformal Prediction Sets for Instance Segmentation", "author": "Kerri Lu and Dan M. Kluger and Stephen Bates and Sherrie Wang", "abstract": "Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.", "link": "http://arxiv.org/abs/2602.10045v1", "date": "2026-02-10", "relevancy": 2.1515, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5439}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5386}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20Sets%20for%20Instance%20Segmentation&body=Title%3A%20Conformal%20Prediction%20Sets%20for%20Instance%20Segmentation%0AAuthor%3A%20Kerri%20Lu%20and%20Dan%20M.%20Kluger%20and%20Stephen%20Bates%20and%20Sherrie%20Wang%0AAbstract%3A%20Current%20instance%20segmentation%20models%20achieve%20high%20performance%20on%20average%20predictions%2C%20but%20lack%20principled%20uncertainty%20quantification%3A%20their%20outputs%20are%20not%20calibrated%2C%20and%20there%20is%20no%20guarantee%20that%20a%20predicted%20mask%20is%20close%20to%20the%20ground%20truth.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20conformal%20prediction%20algorithm%20to%20generate%20adaptive%20confidence%20sets%20for%20instance%20segmentation.%20Given%20an%20image%20and%20a%20pixel%20coordinate%20query%2C%20our%20algorithm%20generates%20a%20confidence%20set%20of%20instance%20predictions%20for%20that%20pixel%2C%20with%20a%20provable%20guarantee%20for%20the%20probability%20that%20at%20least%20one%20of%20the%20predictions%20has%20high%20Intersection-Over-Union%20%28IoU%29%20with%20the%20true%20object%20instance%20mask.%20We%20apply%20our%20algorithm%20to%20instance%20segmentation%20examples%20in%20agricultural%20field%20delineation%2C%20cell%20segmentation%2C%20and%20vehicle%20detection.%20Empirically%2C%20we%20find%20that%20our%20prediction%20sets%20vary%20in%20size%20based%20on%20query%20difficulty%20and%20attain%20the%20target%20coverage%2C%20outperforming%20existing%20baselines%20such%20as%20Learn%20Then%20Test%2C%20Conformal%20Risk%20Control%2C%20and%20morphological%20dilation-based%20methods.%20We%20provide%20versions%20of%20the%20algorithm%20with%20asymptotic%20and%20finite%20sample%20guarantees.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520Sets%2520for%2520Instance%2520Segmentation%26entry.906535625%3DKerri%2520Lu%2520and%2520Dan%2520M.%2520Kluger%2520and%2520Stephen%2520Bates%2520and%2520Sherrie%2520Wang%26entry.1292438233%3DCurrent%2520instance%2520segmentation%2520models%2520achieve%2520high%2520performance%2520on%2520average%2520predictions%252C%2520but%2520lack%2520principled%2520uncertainty%2520quantification%253A%2520their%2520outputs%2520are%2520not%2520calibrated%252C%2520and%2520there%2520is%2520no%2520guarantee%2520that%2520a%2520predicted%2520mask%2520is%2520close%2520to%2520the%2520ground%2520truth.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520conformal%2520prediction%2520algorithm%2520to%2520generate%2520adaptive%2520confidence%2520sets%2520for%2520instance%2520segmentation.%2520Given%2520an%2520image%2520and%2520a%2520pixel%2520coordinate%2520query%252C%2520our%2520algorithm%2520generates%2520a%2520confidence%2520set%2520of%2520instance%2520predictions%2520for%2520that%2520pixel%252C%2520with%2520a%2520provable%2520guarantee%2520for%2520the%2520probability%2520that%2520at%2520least%2520one%2520of%2520the%2520predictions%2520has%2520high%2520Intersection-Over-Union%2520%2528IoU%2529%2520with%2520the%2520true%2520object%2520instance%2520mask.%2520We%2520apply%2520our%2520algorithm%2520to%2520instance%2520segmentation%2520examples%2520in%2520agricultural%2520field%2520delineation%252C%2520cell%2520segmentation%252C%2520and%2520vehicle%2520detection.%2520Empirically%252C%2520we%2520find%2520that%2520our%2520prediction%2520sets%2520vary%2520in%2520size%2520based%2520on%2520query%2520difficulty%2520and%2520attain%2520the%2520target%2520coverage%252C%2520outperforming%2520existing%2520baselines%2520such%2520as%2520Learn%2520Then%2520Test%252C%2520Conformal%2520Risk%2520Control%252C%2520and%2520morphological%2520dilation-based%2520methods.%2520We%2520provide%2520versions%2520of%2520the%2520algorithm%2520with%2520asymptotic%2520and%2520finite%2520sample%2520guarantees.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20Sets%20for%20Instance%20Segmentation&entry.906535625=Kerri%20Lu%20and%20Dan%20M.%20Kluger%20and%20Stephen%20Bates%20and%20Sherrie%20Wang&entry.1292438233=Current%20instance%20segmentation%20models%20achieve%20high%20performance%20on%20average%20predictions%2C%20but%20lack%20principled%20uncertainty%20quantification%3A%20their%20outputs%20are%20not%20calibrated%2C%20and%20there%20is%20no%20guarantee%20that%20a%20predicted%20mask%20is%20close%20to%20the%20ground%20truth.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20conformal%20prediction%20algorithm%20to%20generate%20adaptive%20confidence%20sets%20for%20instance%20segmentation.%20Given%20an%20image%20and%20a%20pixel%20coordinate%20query%2C%20our%20algorithm%20generates%20a%20confidence%20set%20of%20instance%20predictions%20for%20that%20pixel%2C%20with%20a%20provable%20guarantee%20for%20the%20probability%20that%20at%20least%20one%20of%20the%20predictions%20has%20high%20Intersection-Over-Union%20%28IoU%29%20with%20the%20true%20object%20instance%20mask.%20We%20apply%20our%20algorithm%20to%20instance%20segmentation%20examples%20in%20agricultural%20field%20delineation%2C%20cell%20segmentation%2C%20and%20vehicle%20detection.%20Empirically%2C%20we%20find%20that%20our%20prediction%20sets%20vary%20in%20size%20based%20on%20query%20difficulty%20and%20attain%20the%20target%20coverage%2C%20outperforming%20existing%20baselines%20such%20as%20Learn%20Then%20Test%2C%20Conformal%20Risk%20Control%2C%20and%20morphological%20dilation-based%20methods.%20We%20provide%20versions%20of%20the%20algorithm%20with%20asymptotic%20and%20finite%20sample%20guarantees.&entry.1838667208=http%3A//arxiv.org/abs/2602.10045v1&entry.124074799=Read"},
{"title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception", "author": "Satvik Golechha and Adri\u00e0 Garriga-Alonso", "abstract": "Prior studies on deception in language-based AI agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal. To fix this, we introduce Among Us, a sandbox social deception game where LLM-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, Among Us can be expected to last much longer, because it is a multi-player game far from equilibrium. Using the sandbox, we evaluate 18 proprietary and open-weight LLMs and uncover a general trend: models trained with RL are comparatively much better at producing deception than detecting it. We evaluate the effectiveness of methods to detect lying and deception: logistic regression on the activations and sparse autoencoders (SAEs). We find that probes trained on a dataset of \"pretend you're a dishonest model:..\" generalize extremely well out-of-distribution, consistently obtaining AUROCs over 95% even when evaluated just on the deceptive statement, without the chain of thought. We also find two SAE features that work well at deception detection but are unable to steer the model to lie less. We hope our open-sourced sandbox, game logs, and probes serve to anticipate and mitigate deceptive behavior and capabilities in language-based agents.", "link": "http://arxiv.org/abs/2504.04072v3", "date": "2026-02-10", "relevancy": 0.9548, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4825}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Among%20Us%3A%20A%20Sandbox%20for%20Measuring%20and%20Detecting%20Agentic%20Deception&body=Title%3A%20Among%20Us%3A%20A%20Sandbox%20for%20Measuring%20and%20Detecting%20Agentic%20Deception%0AAuthor%3A%20Satvik%20Golechha%20and%20Adri%C3%A0%20Garriga-Alonso%0AAbstract%3A%20Prior%20studies%20on%20deception%20in%20language-based%20AI%20agents%20typically%20assess%20whether%20the%20agent%20produces%20a%20false%20statement%20about%20a%20topic%2C%20or%20makes%20a%20binary%20choice%20prompted%20by%20a%20goal%2C%20rather%20than%20allowing%20open-ended%20deceptive%20behavior%20to%20emerge%20in%20pursuit%20of%20a%20longer-term%20goal.%20To%20fix%20this%2C%20we%20introduce%20Among%20Us%2C%20a%20sandbox%20social%20deception%20game%20where%20LLM-agents%20exhibit%20long-term%2C%20open-ended%20deception%20as%20a%20consequence%20of%20the%20game%20objectives.%20While%20most%20benchmarks%20saturate%20quickly%2C%20Among%20Us%20can%20be%20expected%20to%20last%20much%20longer%2C%20because%20it%20is%20a%20multi-player%20game%20far%20from%20equilibrium.%20Using%20the%20sandbox%2C%20we%20evaluate%2018%20proprietary%20and%20open-weight%20LLMs%20and%20uncover%20a%20general%20trend%3A%20models%20trained%20with%20RL%20are%20comparatively%20much%20better%20at%20producing%20deception%20than%20detecting%20it.%20We%20evaluate%20the%20effectiveness%20of%20methods%20to%20detect%20lying%20and%20deception%3A%20logistic%20regression%20on%20the%20activations%20and%20sparse%20autoencoders%20%28SAEs%29.%20We%20find%20that%20probes%20trained%20on%20a%20dataset%20of%20%22pretend%20you%27re%20a%20dishonest%20model%3A..%22%20generalize%20extremely%20well%20out-of-distribution%2C%20consistently%20obtaining%20AUROCs%20over%2095%25%20even%20when%20evaluated%20just%20on%20the%20deceptive%20statement%2C%20without%20the%20chain%20of%20thought.%20We%20also%20find%20two%20SAE%20features%20that%20work%20well%20at%20deception%20detection%20but%20are%20unable%20to%20steer%20the%20model%20to%20lie%20less.%20We%20hope%20our%20open-sourced%20sandbox%2C%20game%20logs%2C%20and%20probes%20serve%20to%20anticipate%20and%20mitigate%20deceptive%20behavior%20and%20capabilities%20in%20language-based%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2504.04072v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmong%2520Us%253A%2520A%2520Sandbox%2520for%2520Measuring%2520and%2520Detecting%2520Agentic%2520Deception%26entry.906535625%3DSatvik%2520Golechha%2520and%2520Adri%25C3%25A0%2520Garriga-Alonso%26entry.1292438233%3DPrior%2520studies%2520on%2520deception%2520in%2520language-based%2520AI%2520agents%2520typically%2520assess%2520whether%2520the%2520agent%2520produces%2520a%2520false%2520statement%2520about%2520a%2520topic%252C%2520or%2520makes%2520a%2520binary%2520choice%2520prompted%2520by%2520a%2520goal%252C%2520rather%2520than%2520allowing%2520open-ended%2520deceptive%2520behavior%2520to%2520emerge%2520in%2520pursuit%2520of%2520a%2520longer-term%2520goal.%2520To%2520fix%2520this%252C%2520we%2520introduce%2520Among%2520Us%252C%2520a%2520sandbox%2520social%2520deception%2520game%2520where%2520LLM-agents%2520exhibit%2520long-term%252C%2520open-ended%2520deception%2520as%2520a%2520consequence%2520of%2520the%2520game%2520objectives.%2520While%2520most%2520benchmarks%2520saturate%2520quickly%252C%2520Among%2520Us%2520can%2520be%2520expected%2520to%2520last%2520much%2520longer%252C%2520because%2520it%2520is%2520a%2520multi-player%2520game%2520far%2520from%2520equilibrium.%2520Using%2520the%2520sandbox%252C%2520we%2520evaluate%252018%2520proprietary%2520and%2520open-weight%2520LLMs%2520and%2520uncover%2520a%2520general%2520trend%253A%2520models%2520trained%2520with%2520RL%2520are%2520comparatively%2520much%2520better%2520at%2520producing%2520deception%2520than%2520detecting%2520it.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520methods%2520to%2520detect%2520lying%2520and%2520deception%253A%2520logistic%2520regression%2520on%2520the%2520activations%2520and%2520sparse%2520autoencoders%2520%2528SAEs%2529.%2520We%2520find%2520that%2520probes%2520trained%2520on%2520a%2520dataset%2520of%2520%2522pretend%2520you%2527re%2520a%2520dishonest%2520model%253A..%2522%2520generalize%2520extremely%2520well%2520out-of-distribution%252C%2520consistently%2520obtaining%2520AUROCs%2520over%252095%2525%2520even%2520when%2520evaluated%2520just%2520on%2520the%2520deceptive%2520statement%252C%2520without%2520the%2520chain%2520of%2520thought.%2520We%2520also%2520find%2520two%2520SAE%2520features%2520that%2520work%2520well%2520at%2520deception%2520detection%2520but%2520are%2520unable%2520to%2520steer%2520the%2520model%2520to%2520lie%2520less.%2520We%2520hope%2520our%2520open-sourced%2520sandbox%252C%2520game%2520logs%252C%2520and%2520probes%2520serve%2520to%2520anticipate%2520and%2520mitigate%2520deceptive%2520behavior%2520and%2520capabilities%2520in%2520language-based%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04072v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Among%20Us%3A%20A%20Sandbox%20for%20Measuring%20and%20Detecting%20Agentic%20Deception&entry.906535625=Satvik%20Golechha%20and%20Adri%C3%A0%20Garriga-Alonso&entry.1292438233=Prior%20studies%20on%20deception%20in%20language-based%20AI%20agents%20typically%20assess%20whether%20the%20agent%20produces%20a%20false%20statement%20about%20a%20topic%2C%20or%20makes%20a%20binary%20choice%20prompted%20by%20a%20goal%2C%20rather%20than%20allowing%20open-ended%20deceptive%20behavior%20to%20emerge%20in%20pursuit%20of%20a%20longer-term%20goal.%20To%20fix%20this%2C%20we%20introduce%20Among%20Us%2C%20a%20sandbox%20social%20deception%20game%20where%20LLM-agents%20exhibit%20long-term%2C%20open-ended%20deception%20as%20a%20consequence%20of%20the%20game%20objectives.%20While%20most%20benchmarks%20saturate%20quickly%2C%20Among%20Us%20can%20be%20expected%20to%20last%20much%20longer%2C%20because%20it%20is%20a%20multi-player%20game%20far%20from%20equilibrium.%20Using%20the%20sandbox%2C%20we%20evaluate%2018%20proprietary%20and%20open-weight%20LLMs%20and%20uncover%20a%20general%20trend%3A%20models%20trained%20with%20RL%20are%20comparatively%20much%20better%20at%20producing%20deception%20than%20detecting%20it.%20We%20evaluate%20the%20effectiveness%20of%20methods%20to%20detect%20lying%20and%20deception%3A%20logistic%20regression%20on%20the%20activations%20and%20sparse%20autoencoders%20%28SAEs%29.%20We%20find%20that%20probes%20trained%20on%20a%20dataset%20of%20%22pretend%20you%27re%20a%20dishonest%20model%3A..%22%20generalize%20extremely%20well%20out-of-distribution%2C%20consistently%20obtaining%20AUROCs%20over%2095%25%20even%20when%20evaluated%20just%20on%20the%20deceptive%20statement%2C%20without%20the%20chain%20of%20thought.%20We%20also%20find%20two%20SAE%20features%20that%20work%20well%20at%20deception%20detection%20but%20are%20unable%20to%20steer%20the%20model%20to%20lie%20less.%20We%20hope%20our%20open-sourced%20sandbox%2C%20game%20logs%2C%20and%20probes%20serve%20to%20anticipate%20and%20mitigate%20deceptive%20behavior%20and%20capabilities%20in%20language-based%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2504.04072v3&entry.124074799=Read"},
{"title": "Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability", "author": "Aaditya Vikram Prasad and Connor Watts and Jack Merullo and Dhruvil Gala and Owen Lewis and Thomas McGrath and Ekdeep Singh Lubana", "abstract": "Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.", "link": "http://arxiv.org/abs/2602.10067v1", "date": "2026-02-10", "relevancy": 2.0003, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.502}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Features%20as%20Rewards%3A%20Scalable%20Supervision%20for%20Open-Ended%20Tasks%20via%20Interpretability&body=Title%3A%20Features%20as%20Rewards%3A%20Scalable%20Supervision%20for%20Open-Ended%20Tasks%20via%20Interpretability%0AAuthor%3A%20Aaditya%20Vikram%20Prasad%20and%20Connor%20Watts%20and%20Jack%20Merullo%20and%20Dhruvil%20Gala%20and%20Owen%20Lewis%20and%20Thomas%20McGrath%20and%20Ekdeep%20Singh%20Lubana%0AAbstract%3A%20Language%20models%20trained%20on%20large-scale%20datasets%20have%20been%20shown%20to%20learn%20features%20that%20encode%20abstract%20concepts%20such%20as%20factuality%20or%20intent.%20Such%20features%20are%20traditionally%20used%20for%20test-time%20monitoring%20or%20steering.%20We%20present%20an%20alternative%20affordance%3A%20features%20as%20scalable%20supervision%20for%20open-ended%20tasks.%20We%20consider%20the%20case%20of%20hallucination-reduction%20as%20a%20desirable%2C%20yet%20open-ended%20behavior%20and%20design%20a%20reinforcement%20learning%20%28RL%29%20pipeline%2C%20titled%20RLFR%20%28Reinforcement%20Learning%20from%20Feature%20Rewards%29%2C%20that%20uses%20features%20as%20reward%20functions.%20Grounded%20in%20a%20novel%20probing%20framework%20that%20identifies%20candidate%20hallucinated%20claims%2C%20our%20pipeline%20teaches%20a%20model%20to%20intervene%20and%20correct%20its%20completions%20when%20it%20is%20uncertain%20of%20their%20factuality.%20Furthermore%2C%20the%20pipeline%20enables%20scalable%20test-time%20compute%2C%20guided%20once%20more%20by%20our%20reward%20features.%20This%20end-to-end%20process%20operationalized%20on%20Gemma-3-12B-IT%20results%20in%20a%20policy%20that%20is%2058%25%20less%20likely%20to%20hallucinate%20compared%20to%20the%20original%20model%2C%20while%20preserving%20performance%20on%20standard%20benchmarks.%20Taken%20together%2C%20by%20grounding%20supervision%20in%20the%20language%20of%20features%2C%20this%20paper%20introduces%20a%20novel%20paradigm%20in%20the%20use%20of%20interpretability%20for%20learning%20open-ended%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeatures%2520as%2520Rewards%253A%2520Scalable%2520Supervision%2520for%2520Open-Ended%2520Tasks%2520via%2520Interpretability%26entry.906535625%3DAaditya%2520Vikram%2520Prasad%2520and%2520Connor%2520Watts%2520and%2520Jack%2520Merullo%2520and%2520Dhruvil%2520Gala%2520and%2520Owen%2520Lewis%2520and%2520Thomas%2520McGrath%2520and%2520Ekdeep%2520Singh%2520Lubana%26entry.1292438233%3DLanguage%2520models%2520trained%2520on%2520large-scale%2520datasets%2520have%2520been%2520shown%2520to%2520learn%2520features%2520that%2520encode%2520abstract%2520concepts%2520such%2520as%2520factuality%2520or%2520intent.%2520Such%2520features%2520are%2520traditionally%2520used%2520for%2520test-time%2520monitoring%2520or%2520steering.%2520We%2520present%2520an%2520alternative%2520affordance%253A%2520features%2520as%2520scalable%2520supervision%2520for%2520open-ended%2520tasks.%2520We%2520consider%2520the%2520case%2520of%2520hallucination-reduction%2520as%2520a%2520desirable%252C%2520yet%2520open-ended%2520behavior%2520and%2520design%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520pipeline%252C%2520titled%2520RLFR%2520%2528Reinforcement%2520Learning%2520from%2520Feature%2520Rewards%2529%252C%2520that%2520uses%2520features%2520as%2520reward%2520functions.%2520Grounded%2520in%2520a%2520novel%2520probing%2520framework%2520that%2520identifies%2520candidate%2520hallucinated%2520claims%252C%2520our%2520pipeline%2520teaches%2520a%2520model%2520to%2520intervene%2520and%2520correct%2520its%2520completions%2520when%2520it%2520is%2520uncertain%2520of%2520their%2520factuality.%2520Furthermore%252C%2520the%2520pipeline%2520enables%2520scalable%2520test-time%2520compute%252C%2520guided%2520once%2520more%2520by%2520our%2520reward%2520features.%2520This%2520end-to-end%2520process%2520operationalized%2520on%2520Gemma-3-12B-IT%2520results%2520in%2520a%2520policy%2520that%2520is%252058%2525%2520less%2520likely%2520to%2520hallucinate%2520compared%2520to%2520the%2520original%2520model%252C%2520while%2520preserving%2520performance%2520on%2520standard%2520benchmarks.%2520Taken%2520together%252C%2520by%2520grounding%2520supervision%2520in%2520the%2520language%2520of%2520features%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520paradigm%2520in%2520the%2520use%2520of%2520interpretability%2520for%2520learning%2520open-ended%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Features%20as%20Rewards%3A%20Scalable%20Supervision%20for%20Open-Ended%20Tasks%20via%20Interpretability&entry.906535625=Aaditya%20Vikram%20Prasad%20and%20Connor%20Watts%20and%20Jack%20Merullo%20and%20Dhruvil%20Gala%20and%20Owen%20Lewis%20and%20Thomas%20McGrath%20and%20Ekdeep%20Singh%20Lubana&entry.1292438233=Language%20models%20trained%20on%20large-scale%20datasets%20have%20been%20shown%20to%20learn%20features%20that%20encode%20abstract%20concepts%20such%20as%20factuality%20or%20intent.%20Such%20features%20are%20traditionally%20used%20for%20test-time%20monitoring%20or%20steering.%20We%20present%20an%20alternative%20affordance%3A%20features%20as%20scalable%20supervision%20for%20open-ended%20tasks.%20We%20consider%20the%20case%20of%20hallucination-reduction%20as%20a%20desirable%2C%20yet%20open-ended%20behavior%20and%20design%20a%20reinforcement%20learning%20%28RL%29%20pipeline%2C%20titled%20RLFR%20%28Reinforcement%20Learning%20from%20Feature%20Rewards%29%2C%20that%20uses%20features%20as%20reward%20functions.%20Grounded%20in%20a%20novel%20probing%20framework%20that%20identifies%20candidate%20hallucinated%20claims%2C%20our%20pipeline%20teaches%20a%20model%20to%20intervene%20and%20correct%20its%20completions%20when%20it%20is%20uncertain%20of%20their%20factuality.%20Furthermore%2C%20the%20pipeline%20enables%20scalable%20test-time%20compute%2C%20guided%20once%20more%20by%20our%20reward%20features.%20This%20end-to-end%20process%20operationalized%20on%20Gemma-3-12B-IT%20results%20in%20a%20policy%20that%20is%2058%25%20less%20likely%20to%20hallucinate%20compared%20to%20the%20original%20model%2C%20while%20preserving%20performance%20on%20standard%20benchmarks.%20Taken%20together%2C%20by%20grounding%20supervision%20in%20the%20language%20of%20features%2C%20this%20paper%20introduces%20a%20novel%20paradigm%20in%20the%20use%20of%20interpretability%20for%20learning%20open-ended%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.10067v1&entry.124074799=Read"},
{"title": "MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation", "author": "Yangcheng Yu and Xin Jin and Yu Shang and Xin Zhang and Haisheng Su and Wei Wu and Yong Li", "abstract": "Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach combines motion-aware latent world model features with pixel-space features, enabling MoWM to emphasize action-relevant visual details for action decoding. Extensive evaluations on the CALVIN and real-world manipulation tasks demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.", "link": "http://arxiv.org/abs/2509.21797v3", "date": "2026-02-10", "relevancy": 1.7053, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.596}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.562}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoWM%3A%20Mixture-of-World-Models%20for%20Embodied%20Planning%20via%20Latent-to-Pixel%20Feature%20Modulation&body=Title%3A%20MoWM%3A%20Mixture-of-World-Models%20for%20Embodied%20Planning%20via%20Latent-to-Pixel%20Feature%20Modulation%0AAuthor%3A%20Yangcheng%20Yu%20and%20Xin%20Jin%20and%20Yu%20Shang%20and%20Xin%20Zhang%20and%20Haisheng%20Su%20and%20Wei%20Wu%20and%20Yong%20Li%0AAbstract%3A%20Embodied%20action%20planning%20is%20a%20core%20challenge%20in%20robotics%2C%20requiring%20models%20to%20generate%20precise%20actions%20from%20visual%20observations%20and%20language%20instructions.%20While%20video%20generation%20world%20models%20are%20promising%2C%20their%20reliance%20on%20pixel-level%20reconstruction%20often%20introduces%20visual%20redundancies%20that%20hinder%20action%20decoding%20and%20generalization.%20Latent%20world%20models%20offer%20a%20compact%2C%20motion-aware%20representation%2C%20but%20overlook%20the%20fine-grained%20details%20critical%20for%20precise%20manipulation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20MoWM%2C%20a%20mixture-of-world-model%20framework%20that%20fuses%20representations%20from%20hybrid%20world%20models%20for%20embodied%20action%20planning.%20Our%20approach%20combines%20motion-aware%20latent%20world%20model%20features%20with%20pixel-space%20features%2C%20enabling%20MoWM%20to%20emphasize%20action-relevant%20visual%20details%20for%20action%20decoding.%20Extensive%20evaluations%20on%20the%20CALVIN%20and%20real-world%20manipulation%20tasks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20task%20success%20rates%20and%20superior%20generalization.%20We%20also%20provide%20a%20comprehensive%20analysis%20of%20the%20strengths%20of%20each%20feature%20space%2C%20offering%20valuable%20insights%20for%20future%20research%20in%20embodied%20planning.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/tsinghua-fib-lab/MoWM.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21797v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoWM%253A%2520Mixture-of-World-Models%2520for%2520Embodied%2520Planning%2520via%2520Latent-to-Pixel%2520Feature%2520Modulation%26entry.906535625%3DYangcheng%2520Yu%2520and%2520Xin%2520Jin%2520and%2520Yu%2520Shang%2520and%2520Xin%2520Zhang%2520and%2520Haisheng%2520Su%2520and%2520Wei%2520Wu%2520and%2520Yong%2520Li%26entry.1292438233%3DEmbodied%2520action%2520planning%2520is%2520a%2520core%2520challenge%2520in%2520robotics%252C%2520requiring%2520models%2520to%2520generate%2520precise%2520actions%2520from%2520visual%2520observations%2520and%2520language%2520instructions.%2520While%2520video%2520generation%2520world%2520models%2520are%2520promising%252C%2520their%2520reliance%2520on%2520pixel-level%2520reconstruction%2520often%2520introduces%2520visual%2520redundancies%2520that%2520hinder%2520action%2520decoding%2520and%2520generalization.%2520Latent%2520world%2520models%2520offer%2520a%2520compact%252C%2520motion-aware%2520representation%252C%2520but%2520overlook%2520the%2520fine-grained%2520details%2520critical%2520for%2520precise%2520manipulation.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520MoWM%252C%2520a%2520mixture-of-world-model%2520framework%2520that%2520fuses%2520representations%2520from%2520hybrid%2520world%2520models%2520for%2520embodied%2520action%2520planning.%2520Our%2520approach%2520combines%2520motion-aware%2520latent%2520world%2520model%2520features%2520with%2520pixel-space%2520features%252C%2520enabling%2520MoWM%2520to%2520emphasize%2520action-relevant%2520visual%2520details%2520for%2520action%2520decoding.%2520Extensive%2520evaluations%2520on%2520the%2520CALVIN%2520and%2520real-world%2520manipulation%2520tasks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520task%2520success%2520rates%2520and%2520superior%2520generalization.%2520We%2520also%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520the%2520strengths%2520of%2520each%2520feature%2520space%252C%2520offering%2520valuable%2520insights%2520for%2520future%2520research%2520in%2520embodied%2520planning.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/tsinghua-fib-lab/MoWM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21797v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoWM%3A%20Mixture-of-World-Models%20for%20Embodied%20Planning%20via%20Latent-to-Pixel%20Feature%20Modulation&entry.906535625=Yangcheng%20Yu%20and%20Xin%20Jin%20and%20Yu%20Shang%20and%20Xin%20Zhang%20and%20Haisheng%20Su%20and%20Wei%20Wu%20and%20Yong%20Li&entry.1292438233=Embodied%20action%20planning%20is%20a%20core%20challenge%20in%20robotics%2C%20requiring%20models%20to%20generate%20precise%20actions%20from%20visual%20observations%20and%20language%20instructions.%20While%20video%20generation%20world%20models%20are%20promising%2C%20their%20reliance%20on%20pixel-level%20reconstruction%20often%20introduces%20visual%20redundancies%20that%20hinder%20action%20decoding%20and%20generalization.%20Latent%20world%20models%20offer%20a%20compact%2C%20motion-aware%20representation%2C%20but%20overlook%20the%20fine-grained%20details%20critical%20for%20precise%20manipulation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20MoWM%2C%20a%20mixture-of-world-model%20framework%20that%20fuses%20representations%20from%20hybrid%20world%20models%20for%20embodied%20action%20planning.%20Our%20approach%20combines%20motion-aware%20latent%20world%20model%20features%20with%20pixel-space%20features%2C%20enabling%20MoWM%20to%20emphasize%20action-relevant%20visual%20details%20for%20action%20decoding.%20Extensive%20evaluations%20on%20the%20CALVIN%20and%20real-world%20manipulation%20tasks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20task%20success%20rates%20and%20superior%20generalization.%20We%20also%20provide%20a%20comprehensive%20analysis%20of%20the%20strengths%20of%20each%20feature%20space%2C%20offering%20valuable%20insights%20for%20future%20research%20in%20embodied%20planning.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/tsinghua-fib-lab/MoWM.&entry.1838667208=http%3A//arxiv.org/abs/2509.21797v3&entry.124074799=Read"},
{"title": "Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG", "author": "Gourav Siddhad and Anurag Singh and Rajkumar Saini and Partha Pratim Roy", "abstract": "Driver drowsiness is a leading cause of traffic accidents, necessitating real-time, reliable detection systems to ensure road safety. This study proposes a Modified TSception architecture for robust assessment of driver fatigue and mental workload using Electroencephalography (EEG). The model introduces a five-layer hierarchical temporal refinement strategy to capture multi-scale brain dynamics, surpassing the original TSception's three-layer approach. Key innovations include the use of Adaptive Average Pooling (ADP) for structural flexibility across varying EEG dimensions and a two-stage fusion mechanism to optimize spatiotemporal feature integration for improved stability. Evaluated on the SEED-VIG dataset, the Modified TSception achieves 83.46% accuracy, comparable to the original model (83.15%), but with a significantly reduced confidence interval (0.24 vs. 0.36), indicating better performance stability. The architecture's generalizability was further validated on the STEW mental workload dataset, achieving state-of-the-art accuracies of 95.93% and 95.35% for 2-class and 3-class classification, respectively. These results show that the proposed modifications improve consistency and cross-task generalizability, making the model a reliable framework for EEG-based safety monitoring.", "link": "http://arxiv.org/abs/2512.21747v2", "date": "2026-02-10", "relevancy": 1.857, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4734}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4657}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modified%20TSception%20for%20Analyzing%20Driver%20Drowsiness%20and%20Mental%20Workload%20from%20EEG&body=Title%3A%20Modified%20TSception%20for%20Analyzing%20Driver%20Drowsiness%20and%20Mental%20Workload%20from%20EEG%0AAuthor%3A%20Gourav%20Siddhad%20and%20Anurag%20Singh%20and%20Rajkumar%20Saini%20and%20Partha%20Pratim%20Roy%0AAbstract%3A%20Driver%20drowsiness%20is%20a%20leading%20cause%20of%20traffic%20accidents%2C%20necessitating%20real-time%2C%20reliable%20detection%20systems%20to%20ensure%20road%20safety.%20This%20study%20proposes%20a%20Modified%20TSception%20architecture%20for%20robust%20assessment%20of%20driver%20fatigue%20and%20mental%20workload%20using%20Electroencephalography%20%28EEG%29.%20The%20model%20introduces%20a%20five-layer%20hierarchical%20temporal%20refinement%20strategy%20to%20capture%20multi-scale%20brain%20dynamics%2C%20surpassing%20the%20original%20TSception%27s%20three-layer%20approach.%20Key%20innovations%20include%20the%20use%20of%20Adaptive%20Average%20Pooling%20%28ADP%29%20for%20structural%20flexibility%20across%20varying%20EEG%20dimensions%20and%20a%20two-stage%20fusion%20mechanism%20to%20optimize%20spatiotemporal%20feature%20integration%20for%20improved%20stability.%20Evaluated%20on%20the%20SEED-VIG%20dataset%2C%20the%20Modified%20TSception%20achieves%2083.46%25%20accuracy%2C%20comparable%20to%20the%20original%20model%20%2883.15%25%29%2C%20but%20with%20a%20significantly%20reduced%20confidence%20interval%20%280.24%20vs.%200.36%29%2C%20indicating%20better%20performance%20stability.%20The%20architecture%27s%20generalizability%20was%20further%20validated%20on%20the%20STEW%20mental%20workload%20dataset%2C%20achieving%20state-of-the-art%20accuracies%20of%2095.93%25%20and%2095.35%25%20for%202-class%20and%203-class%20classification%2C%20respectively.%20These%20results%20show%20that%20the%20proposed%20modifications%20improve%20consistency%20and%20cross-task%20generalizability%2C%20making%20the%20model%20a%20reliable%20framework%20for%20EEG-based%20safety%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModified%2520TSception%2520for%2520Analyzing%2520Driver%2520Drowsiness%2520and%2520Mental%2520Workload%2520from%2520EEG%26entry.906535625%3DGourav%2520Siddhad%2520and%2520Anurag%2520Singh%2520and%2520Rajkumar%2520Saini%2520and%2520Partha%2520Pratim%2520Roy%26entry.1292438233%3DDriver%2520drowsiness%2520is%2520a%2520leading%2520cause%2520of%2520traffic%2520accidents%252C%2520necessitating%2520real-time%252C%2520reliable%2520detection%2520systems%2520to%2520ensure%2520road%2520safety.%2520This%2520study%2520proposes%2520a%2520Modified%2520TSception%2520architecture%2520for%2520robust%2520assessment%2520of%2520driver%2520fatigue%2520and%2520mental%2520workload%2520using%2520Electroencephalography%2520%2528EEG%2529.%2520The%2520model%2520introduces%2520a%2520five-layer%2520hierarchical%2520temporal%2520refinement%2520strategy%2520to%2520capture%2520multi-scale%2520brain%2520dynamics%252C%2520surpassing%2520the%2520original%2520TSception%2527s%2520three-layer%2520approach.%2520Key%2520innovations%2520include%2520the%2520use%2520of%2520Adaptive%2520Average%2520Pooling%2520%2528ADP%2529%2520for%2520structural%2520flexibility%2520across%2520varying%2520EEG%2520dimensions%2520and%2520a%2520two-stage%2520fusion%2520mechanism%2520to%2520optimize%2520spatiotemporal%2520feature%2520integration%2520for%2520improved%2520stability.%2520Evaluated%2520on%2520the%2520SEED-VIG%2520dataset%252C%2520the%2520Modified%2520TSception%2520achieves%252083.46%2525%2520accuracy%252C%2520comparable%2520to%2520the%2520original%2520model%2520%252883.15%2525%2529%252C%2520but%2520with%2520a%2520significantly%2520reduced%2520confidence%2520interval%2520%25280.24%2520vs.%25200.36%2529%252C%2520indicating%2520better%2520performance%2520stability.%2520The%2520architecture%2527s%2520generalizability%2520was%2520further%2520validated%2520on%2520the%2520STEW%2520mental%2520workload%2520dataset%252C%2520achieving%2520state-of-the-art%2520accuracies%2520of%252095.93%2525%2520and%252095.35%2525%2520for%25202-class%2520and%25203-class%2520classification%252C%2520respectively.%2520These%2520results%2520show%2520that%2520the%2520proposed%2520modifications%2520improve%2520consistency%2520and%2520cross-task%2520generalizability%252C%2520making%2520the%2520model%2520a%2520reliable%2520framework%2520for%2520EEG-based%2520safety%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modified%20TSception%20for%20Analyzing%20Driver%20Drowsiness%20and%20Mental%20Workload%20from%20EEG&entry.906535625=Gourav%20Siddhad%20and%20Anurag%20Singh%20and%20Rajkumar%20Saini%20and%20Partha%20Pratim%20Roy&entry.1292438233=Driver%20drowsiness%20is%20a%20leading%20cause%20of%20traffic%20accidents%2C%20necessitating%20real-time%2C%20reliable%20detection%20systems%20to%20ensure%20road%20safety.%20This%20study%20proposes%20a%20Modified%20TSception%20architecture%20for%20robust%20assessment%20of%20driver%20fatigue%20and%20mental%20workload%20using%20Electroencephalography%20%28EEG%29.%20The%20model%20introduces%20a%20five-layer%20hierarchical%20temporal%20refinement%20strategy%20to%20capture%20multi-scale%20brain%20dynamics%2C%20surpassing%20the%20original%20TSception%27s%20three-layer%20approach.%20Key%20innovations%20include%20the%20use%20of%20Adaptive%20Average%20Pooling%20%28ADP%29%20for%20structural%20flexibility%20across%20varying%20EEG%20dimensions%20and%20a%20two-stage%20fusion%20mechanism%20to%20optimize%20spatiotemporal%20feature%20integration%20for%20improved%20stability.%20Evaluated%20on%20the%20SEED-VIG%20dataset%2C%20the%20Modified%20TSception%20achieves%2083.46%25%20accuracy%2C%20comparable%20to%20the%20original%20model%20%2883.15%25%29%2C%20but%20with%20a%20significantly%20reduced%20confidence%20interval%20%280.24%20vs.%200.36%29%2C%20indicating%20better%20performance%20stability.%20The%20architecture%27s%20generalizability%20was%20further%20validated%20on%20the%20STEW%20mental%20workload%20dataset%2C%20achieving%20state-of-the-art%20accuracies%20of%2095.93%25%20and%2095.35%25%20for%202-class%20and%203-class%20classification%2C%20respectively.%20These%20results%20show%20that%20the%20proposed%20modifications%20improve%20consistency%20and%20cross-task%20generalizability%2C%20making%20the%20model%20a%20reliable%20framework%20for%20EEG-based%20safety%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2512.21747v2&entry.124074799=Read"},
{"title": "NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models", "author": "Konstantinos Barmpas and Na Lee and Alexandros Koliousis and Yannis Panagakis and Dimitrios A. Adamos and Nikolaos Laskaris and Stefanos Zafeiriou", "abstract": "Electroencephalography (EEG) captures neural activity across multiple temporal and spectral scales, yielding signals that are rich but complex for representation learning. Recently, EEG foundation models trained to predict masked signal-tokens have shown promise for learning generalizable representations. However, their performance is hindered by their signal tokenization modules. Existing neural tokenizers fail to preserve high-frequency dynamics, limiting their ability to reconstruct EEG signals with high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM) centered on a codebook-based tokenizer. Our tokenizer integrates: (i) multi-scale feature extraction modules that capture the full frequency neural spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware loss function for efficient training. This design enables efficient EEG compression while supporting accurate reconstruction across all frequency bands, leading to robust generative masked modeling. Our empirical results demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ tokenizer establishes a strong prior for codebook-based general-purpose brainwave models, enabling advances in neural decoding, generative modeling and multimodal biosignal integration.", "link": "http://arxiv.org/abs/2510.13068v3", "date": "2026-02-10", "relevancy": 2.0514, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.525}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroRVQ%3A%20Multi-Scale%20EEG%20Tokenization%20for%20Generative%20Large%20Brainwave%20Models&body=Title%3A%20NeuroRVQ%3A%20Multi-Scale%20EEG%20Tokenization%20for%20Generative%20Large%20Brainwave%20Models%0AAuthor%3A%20Konstantinos%20Barmpas%20and%20Na%20Lee%20and%20Alexandros%20Koliousis%20and%20Yannis%20Panagakis%20and%20Dimitrios%20A.%20Adamos%20and%20Nikolaos%20Laskaris%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20Electroencephalography%20%28EEG%29%20captures%20neural%20activity%20across%20multiple%20temporal%20and%20spectral%20scales%2C%20yielding%20signals%20that%20are%20rich%20but%20complex%20for%20representation%20learning.%20Recently%2C%20EEG%20foundation%20models%20trained%20to%20predict%20masked%20signal-tokens%20have%20shown%20promise%20for%20learning%20generalizable%20representations.%20However%2C%20their%20performance%20is%20hindered%20by%20their%20signal%20tokenization%20modules.%20Existing%20neural%20tokenizers%20fail%20to%20preserve%20high-frequency%20dynamics%2C%20limiting%20their%20ability%20to%20reconstruct%20EEG%20signals%20with%20high%20fidelity.%20We%20introduce%20NeuroRVQ%2C%20a%20scalable%20Large%20Brainwave%20Model%20%28LBM%29%20centered%20on%20a%20codebook-based%20tokenizer.%20Our%20tokenizer%20integrates%3A%20%28i%29%20multi-scale%20feature%20extraction%20modules%20that%20capture%20the%20full%20frequency%20neural%20spectrum%3B%20%28ii%29%20hierarchical%20residual%20vector%20quantization%20%28RVQ%29%20codebooks%20for%20high-resolution%20encoding%3B%20and%2C%20%28iii%29%20an%20EEG%20signal%20phase-%20and%20amplitude-aware%20loss%20function%20for%20efficient%20training.%20This%20design%20enables%20efficient%20EEG%20compression%20while%20supporting%20accurate%20reconstruction%20across%20all%20frequency%20bands%2C%20leading%20to%20robust%20generative%20masked%20modeling.%20Our%20empirical%20results%20demonstrate%20that%20NeuroRVQ%20achieves%20lower%20reconstruction%20error%20and%20outperforms%20existing%20LBMs%20on%20a%20variety%20of%20downstream%20tasks.%20More%20broadly%2C%20NeuroRVQ%20tokenizer%20establishes%20a%20strong%20prior%20for%20codebook-based%20general-purpose%20brainwave%20models%2C%20enabling%20advances%20in%20neural%20decoding%2C%20generative%20modeling%20and%20multimodal%20biosignal%20integration.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13068v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroRVQ%253A%2520Multi-Scale%2520EEG%2520Tokenization%2520for%2520Generative%2520Large%2520Brainwave%2520Models%26entry.906535625%3DKonstantinos%2520Barmpas%2520and%2520Na%2520Lee%2520and%2520Alexandros%2520Koliousis%2520and%2520Yannis%2520Panagakis%2520and%2520Dimitrios%2520A.%2520Adamos%2520and%2520Nikolaos%2520Laskaris%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3DElectroencephalography%2520%2528EEG%2529%2520captures%2520neural%2520activity%2520across%2520multiple%2520temporal%2520and%2520spectral%2520scales%252C%2520yielding%2520signals%2520that%2520are%2520rich%2520but%2520complex%2520for%2520representation%2520learning.%2520Recently%252C%2520EEG%2520foundation%2520models%2520trained%2520to%2520predict%2520masked%2520signal-tokens%2520have%2520shown%2520promise%2520for%2520learning%2520generalizable%2520representations.%2520However%252C%2520their%2520performance%2520is%2520hindered%2520by%2520their%2520signal%2520tokenization%2520modules.%2520Existing%2520neural%2520tokenizers%2520fail%2520to%2520preserve%2520high-frequency%2520dynamics%252C%2520limiting%2520their%2520ability%2520to%2520reconstruct%2520EEG%2520signals%2520with%2520high%2520fidelity.%2520We%2520introduce%2520NeuroRVQ%252C%2520a%2520scalable%2520Large%2520Brainwave%2520Model%2520%2528LBM%2529%2520centered%2520on%2520a%2520codebook-based%2520tokenizer.%2520Our%2520tokenizer%2520integrates%253A%2520%2528i%2529%2520multi-scale%2520feature%2520extraction%2520modules%2520that%2520capture%2520the%2520full%2520frequency%2520neural%2520spectrum%253B%2520%2528ii%2529%2520hierarchical%2520residual%2520vector%2520quantization%2520%2528RVQ%2529%2520codebooks%2520for%2520high-resolution%2520encoding%253B%2520and%252C%2520%2528iii%2529%2520an%2520EEG%2520signal%2520phase-%2520and%2520amplitude-aware%2520loss%2520function%2520for%2520efficient%2520training.%2520This%2520design%2520enables%2520efficient%2520EEG%2520compression%2520while%2520supporting%2520accurate%2520reconstruction%2520across%2520all%2520frequency%2520bands%252C%2520leading%2520to%2520robust%2520generative%2520masked%2520modeling.%2520Our%2520empirical%2520results%2520demonstrate%2520that%2520NeuroRVQ%2520achieves%2520lower%2520reconstruction%2520error%2520and%2520outperforms%2520existing%2520LBMs%2520on%2520a%2520variety%2520of%2520downstream%2520tasks.%2520More%2520broadly%252C%2520NeuroRVQ%2520tokenizer%2520establishes%2520a%2520strong%2520prior%2520for%2520codebook-based%2520general-purpose%2520brainwave%2520models%252C%2520enabling%2520advances%2520in%2520neural%2520decoding%252C%2520generative%2520modeling%2520and%2520multimodal%2520biosignal%2520integration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13068v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroRVQ%3A%20Multi-Scale%20EEG%20Tokenization%20for%20Generative%20Large%20Brainwave%20Models&entry.906535625=Konstantinos%20Barmpas%20and%20Na%20Lee%20and%20Alexandros%20Koliousis%20and%20Yannis%20Panagakis%20and%20Dimitrios%20A.%20Adamos%20and%20Nikolaos%20Laskaris%20and%20Stefanos%20Zafeiriou&entry.1292438233=Electroencephalography%20%28EEG%29%20captures%20neural%20activity%20across%20multiple%20temporal%20and%20spectral%20scales%2C%20yielding%20signals%20that%20are%20rich%20but%20complex%20for%20representation%20learning.%20Recently%2C%20EEG%20foundation%20models%20trained%20to%20predict%20masked%20signal-tokens%20have%20shown%20promise%20for%20learning%20generalizable%20representations.%20However%2C%20their%20performance%20is%20hindered%20by%20their%20signal%20tokenization%20modules.%20Existing%20neural%20tokenizers%20fail%20to%20preserve%20high-frequency%20dynamics%2C%20limiting%20their%20ability%20to%20reconstruct%20EEG%20signals%20with%20high%20fidelity.%20We%20introduce%20NeuroRVQ%2C%20a%20scalable%20Large%20Brainwave%20Model%20%28LBM%29%20centered%20on%20a%20codebook-based%20tokenizer.%20Our%20tokenizer%20integrates%3A%20%28i%29%20multi-scale%20feature%20extraction%20modules%20that%20capture%20the%20full%20frequency%20neural%20spectrum%3B%20%28ii%29%20hierarchical%20residual%20vector%20quantization%20%28RVQ%29%20codebooks%20for%20high-resolution%20encoding%3B%20and%2C%20%28iii%29%20an%20EEG%20signal%20phase-%20and%20amplitude-aware%20loss%20function%20for%20efficient%20training.%20This%20design%20enables%20efficient%20EEG%20compression%20while%20supporting%20accurate%20reconstruction%20across%20all%20frequency%20bands%2C%20leading%20to%20robust%20generative%20masked%20modeling.%20Our%20empirical%20results%20demonstrate%20that%20NeuroRVQ%20achieves%20lower%20reconstruction%20error%20and%20outperforms%20existing%20LBMs%20on%20a%20variety%20of%20downstream%20tasks.%20More%20broadly%2C%20NeuroRVQ%20tokenizer%20establishes%20a%20strong%20prior%20for%20codebook-based%20general-purpose%20brainwave%20models%2C%20enabling%20advances%20in%20neural%20decoding%2C%20generative%20modeling%20and%20multimodal%20biosignal%20integration.&entry.1838667208=http%3A//arxiv.org/abs/2510.13068v3&entry.124074799=Read"},
{"title": "Life Cycle-Aware Evaluation of Knowledge Distillation for Machine Translation: Environmental Impact and Translation Quality Trade-offs", "author": "Joseph Attieh and Timothee Mickus and Anne-Laure Ligozat and Aur\u00e9lie N\u00e9v\u00e9ol and J\u00f6rg Tiedemann", "abstract": "Knowledge distillation (KD) is a tool to compress a larger system (teacher) into a smaller one (student). In machine translation, studies typically report only the translation quality of the student and omit the computational complexity of performing KD, making it difficult to select among the many available KD choices under compute-induced constraints. In this study, we evaluate representative KD methods by considering both translation quality and computational cost. We express computational cost as a carbon footprint using the machine learning life cycle assessment (MLCA) tool. This assessment accounts for runtime operational emissions and amortized hardware production costs throughout the KD model life cycle (teacher training, distillation, and inference). We find that (i) distillation overhead dominates the total footprint at small deployment volumes, (ii) inference dominates at scale, making KD beneficial only beyond a task-dependent usage threshold, and (iii) word-level distillation typically offers more favorable footprint-quality trade-offs than sequence-level distillation. Our protocol provides reproducible guidance for selecting KD methods under explicit quality and compute-induced constraints.", "link": "http://arxiv.org/abs/2602.09691v1", "date": "2026-02-10", "relevancy": 0.887, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4568}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.44}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Life%20Cycle-Aware%20Evaluation%20of%20Knowledge%20Distillation%20for%20Machine%20Translation%3A%20Environmental%20Impact%20and%20Translation%20Quality%20Trade-offs&body=Title%3A%20Life%20Cycle-Aware%20Evaluation%20of%20Knowledge%20Distillation%20for%20Machine%20Translation%3A%20Environmental%20Impact%20and%20Translation%20Quality%20Trade-offs%0AAuthor%3A%20Joseph%20Attieh%20and%20Timothee%20Mickus%20and%20Anne-Laure%20Ligozat%20and%20Aur%C3%A9lie%20N%C3%A9v%C3%A9ol%20and%20J%C3%B6rg%20Tiedemann%0AAbstract%3A%20Knowledge%20distillation%20%28KD%29%20is%20a%20tool%20to%20compress%20a%20larger%20system%20%28teacher%29%20into%20a%20smaller%20one%20%28student%29.%20In%20machine%20translation%2C%20studies%20typically%20report%20only%20the%20translation%20quality%20of%20the%20student%20and%20omit%20the%20computational%20complexity%20of%20performing%20KD%2C%20making%20it%20difficult%20to%20select%20among%20the%20many%20available%20KD%20choices%20under%20compute-induced%20constraints.%20In%20this%20study%2C%20we%20evaluate%20representative%20KD%20methods%20by%20considering%20both%20translation%20quality%20and%20computational%20cost.%20We%20express%20computational%20cost%20as%20a%20carbon%20footprint%20using%20the%20machine%20learning%20life%20cycle%20assessment%20%28MLCA%29%20tool.%20This%20assessment%20accounts%20for%20runtime%20operational%20emissions%20and%20amortized%20hardware%20production%20costs%20throughout%20the%20KD%20model%20life%20cycle%20%28teacher%20training%2C%20distillation%2C%20and%20inference%29.%20We%20find%20that%20%28i%29%20distillation%20overhead%20dominates%20the%20total%20footprint%20at%20small%20deployment%20volumes%2C%20%28ii%29%20inference%20dominates%20at%20scale%2C%20making%20KD%20beneficial%20only%20beyond%20a%20task-dependent%20usage%20threshold%2C%20and%20%28iii%29%20word-level%20distillation%20typically%20offers%20more%20favorable%20footprint-quality%20trade-offs%20than%20sequence-level%20distillation.%20Our%20protocol%20provides%20reproducible%20guidance%20for%20selecting%20KD%20methods%20under%20explicit%20quality%20and%20compute-induced%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLife%2520Cycle-Aware%2520Evaluation%2520of%2520Knowledge%2520Distillation%2520for%2520Machine%2520Translation%253A%2520Environmental%2520Impact%2520and%2520Translation%2520Quality%2520Trade-offs%26entry.906535625%3DJoseph%2520Attieh%2520and%2520Timothee%2520Mickus%2520and%2520Anne-Laure%2520Ligozat%2520and%2520Aur%25C3%25A9lie%2520N%25C3%25A9v%25C3%25A9ol%2520and%2520J%25C3%25B6rg%2520Tiedemann%26entry.1292438233%3DKnowledge%2520distillation%2520%2528KD%2529%2520is%2520a%2520tool%2520to%2520compress%2520a%2520larger%2520system%2520%2528teacher%2529%2520into%2520a%2520smaller%2520one%2520%2528student%2529.%2520In%2520machine%2520translation%252C%2520studies%2520typically%2520report%2520only%2520the%2520translation%2520quality%2520of%2520the%2520student%2520and%2520omit%2520the%2520computational%2520complexity%2520of%2520performing%2520KD%252C%2520making%2520it%2520difficult%2520to%2520select%2520among%2520the%2520many%2520available%2520KD%2520choices%2520under%2520compute-induced%2520constraints.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520representative%2520KD%2520methods%2520by%2520considering%2520both%2520translation%2520quality%2520and%2520computational%2520cost.%2520We%2520express%2520computational%2520cost%2520as%2520a%2520carbon%2520footprint%2520using%2520the%2520machine%2520learning%2520life%2520cycle%2520assessment%2520%2528MLCA%2529%2520tool.%2520This%2520assessment%2520accounts%2520for%2520runtime%2520operational%2520emissions%2520and%2520amortized%2520hardware%2520production%2520costs%2520throughout%2520the%2520KD%2520model%2520life%2520cycle%2520%2528teacher%2520training%252C%2520distillation%252C%2520and%2520inference%2529.%2520We%2520find%2520that%2520%2528i%2529%2520distillation%2520overhead%2520dominates%2520the%2520total%2520footprint%2520at%2520small%2520deployment%2520volumes%252C%2520%2528ii%2529%2520inference%2520dominates%2520at%2520scale%252C%2520making%2520KD%2520beneficial%2520only%2520beyond%2520a%2520task-dependent%2520usage%2520threshold%252C%2520and%2520%2528iii%2529%2520word-level%2520distillation%2520typically%2520offers%2520more%2520favorable%2520footprint-quality%2520trade-offs%2520than%2520sequence-level%2520distillation.%2520Our%2520protocol%2520provides%2520reproducible%2520guidance%2520for%2520selecting%2520KD%2520methods%2520under%2520explicit%2520quality%2520and%2520compute-induced%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Life%20Cycle-Aware%20Evaluation%20of%20Knowledge%20Distillation%20for%20Machine%20Translation%3A%20Environmental%20Impact%20and%20Translation%20Quality%20Trade-offs&entry.906535625=Joseph%20Attieh%20and%20Timothee%20Mickus%20and%20Anne-Laure%20Ligozat%20and%20Aur%C3%A9lie%20N%C3%A9v%C3%A9ol%20and%20J%C3%B6rg%20Tiedemann&entry.1292438233=Knowledge%20distillation%20%28KD%29%20is%20a%20tool%20to%20compress%20a%20larger%20system%20%28teacher%29%20into%20a%20smaller%20one%20%28student%29.%20In%20machine%20translation%2C%20studies%20typically%20report%20only%20the%20translation%20quality%20of%20the%20student%20and%20omit%20the%20computational%20complexity%20of%20performing%20KD%2C%20making%20it%20difficult%20to%20select%20among%20the%20many%20available%20KD%20choices%20under%20compute-induced%20constraints.%20In%20this%20study%2C%20we%20evaluate%20representative%20KD%20methods%20by%20considering%20both%20translation%20quality%20and%20computational%20cost.%20We%20express%20computational%20cost%20as%20a%20carbon%20footprint%20using%20the%20machine%20learning%20life%20cycle%20assessment%20%28MLCA%29%20tool.%20This%20assessment%20accounts%20for%20runtime%20operational%20emissions%20and%20amortized%20hardware%20production%20costs%20throughout%20the%20KD%20model%20life%20cycle%20%28teacher%20training%2C%20distillation%2C%20and%20inference%29.%20We%20find%20that%20%28i%29%20distillation%20overhead%20dominates%20the%20total%20footprint%20at%20small%20deployment%20volumes%2C%20%28ii%29%20inference%20dominates%20at%20scale%2C%20making%20KD%20beneficial%20only%20beyond%20a%20task-dependent%20usage%20threshold%2C%20and%20%28iii%29%20word-level%20distillation%20typically%20offers%20more%20favorable%20footprint-quality%20trade-offs%20than%20sequence-level%20distillation.%20Our%20protocol%20provides%20reproducible%20guidance%20for%20selecting%20KD%20methods%20under%20explicit%20quality%20and%20compute-induced%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2602.09691v1&entry.124074799=Read"},
{"title": "A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models", "author": "Xiulin Yang and Arianna Bisazza and Nathan Schneider and Ethan Gotlieb Wilcox", "abstract": "How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \\poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \\poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.", "link": "http://arxiv.org/abs/2602.09992v1", "date": "2026-02-10", "relevancy": 1.9683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Assessment%20of%20the%20Poverty%20of%20the%20Stimulus%20Argument%20for%20Neural%20Language%20Models&body=Title%3A%20A%20Unified%20Assessment%20of%20the%20Poverty%20of%20the%20Stimulus%20Argument%20for%20Neural%20Language%20Models%0AAuthor%3A%20Xiulin%20Yang%20and%20Arianna%20Bisazza%20and%20Nathan%20Schneider%20and%20Ethan%20Gotlieb%20Wilcox%0AAbstract%3A%20How%20can%20children%20acquire%20native-level%20syntax%20from%20limited%20input%3F%20According%20to%20the%20Poverty%20of%20the%20Stimulus%20Hypothesis%20%28PoSH%29%2C%20the%20linguistic%20input%20children%20receive%20is%20insufficient%20to%20explain%20certain%20generalizations%20that%20are%20robustly%20learned%3B%20innate%20linguistic%20constraints%2C%20many%20have%20argued%2C%20are%20thus%20necessary%20to%20explain%20language%20learning.%20Neural%20language%20models%2C%20which%20lack%20such%20language-specific%20constraints%20in%20their%20design%2C%20offer%20a%20computational%20test%20of%20this%20longstanding%20%28but%20controversial%29%20claim.%20We%20introduce%20%5Cposhbench%2C%20a%20training-and-evaluation%20suite%20targeting%20question%20formation%2C%20islands%20to%20movement%2C%20and%20other%20English%20phenomena%20at%20the%20center%20of%20the%20PoSH%20arguments.%20Training%20Transformer%20models%20on%2010--50M%20words%20of%20developmentally%20plausible%20text%2C%20we%20find%20indications%20of%20generalization%20on%20all%20phenomena%20even%20without%20direct%20positive%20evidence%20--%20yet%20neural%20models%20remain%20less%20data-efficient%20and%20their%20generalizations%20are%20weaker%20than%20those%20of%20children.%20We%20further%20enhance%20our%20models%20with%20three%20recently%20proposed%20cognitively%20motivated%20inductive%20biases.%20We%20find%20these%20biases%20improve%20general%20syntactic%20competence%20but%20not%20%5Cposhbench%20performance.%20Our%20findings%20challenge%20the%20claim%20that%20innate%20syntax%20is%20the%20only%20possible%20route%20to%20generalization%2C%20while%20suggesting%20that%20human-like%20data%20efficiency%20requires%20inductive%20biases%20beyond%20those%20tested%20here.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Assessment%2520of%2520the%2520Poverty%2520of%2520the%2520Stimulus%2520Argument%2520for%2520Neural%2520Language%2520Models%26entry.906535625%3DXiulin%2520Yang%2520and%2520Arianna%2520Bisazza%2520and%2520Nathan%2520Schneider%2520and%2520Ethan%2520Gotlieb%2520Wilcox%26entry.1292438233%3DHow%2520can%2520children%2520acquire%2520native-level%2520syntax%2520from%2520limited%2520input%253F%2520According%2520to%2520the%2520Poverty%2520of%2520the%2520Stimulus%2520Hypothesis%2520%2528PoSH%2529%252C%2520the%2520linguistic%2520input%2520children%2520receive%2520is%2520insufficient%2520to%2520explain%2520certain%2520generalizations%2520that%2520are%2520robustly%2520learned%253B%2520innate%2520linguistic%2520constraints%252C%2520many%2520have%2520argued%252C%2520are%2520thus%2520necessary%2520to%2520explain%2520language%2520learning.%2520Neural%2520language%2520models%252C%2520which%2520lack%2520such%2520language-specific%2520constraints%2520in%2520their%2520design%252C%2520offer%2520a%2520computational%2520test%2520of%2520this%2520longstanding%2520%2528but%2520controversial%2529%2520claim.%2520We%2520introduce%2520%255Cposhbench%252C%2520a%2520training-and-evaluation%2520suite%2520targeting%2520question%2520formation%252C%2520islands%2520to%2520movement%252C%2520and%2520other%2520English%2520phenomena%2520at%2520the%2520center%2520of%2520the%2520PoSH%2520arguments.%2520Training%2520Transformer%2520models%2520on%252010--50M%2520words%2520of%2520developmentally%2520plausible%2520text%252C%2520we%2520find%2520indications%2520of%2520generalization%2520on%2520all%2520phenomena%2520even%2520without%2520direct%2520positive%2520evidence%2520--%2520yet%2520neural%2520models%2520remain%2520less%2520data-efficient%2520and%2520their%2520generalizations%2520are%2520weaker%2520than%2520those%2520of%2520children.%2520We%2520further%2520enhance%2520our%2520models%2520with%2520three%2520recently%2520proposed%2520cognitively%2520motivated%2520inductive%2520biases.%2520We%2520find%2520these%2520biases%2520improve%2520general%2520syntactic%2520competence%2520but%2520not%2520%255Cposhbench%2520performance.%2520Our%2520findings%2520challenge%2520the%2520claim%2520that%2520innate%2520syntax%2520is%2520the%2520only%2520possible%2520route%2520to%2520generalization%252C%2520while%2520suggesting%2520that%2520human-like%2520data%2520efficiency%2520requires%2520inductive%2520biases%2520beyond%2520those%2520tested%2520here.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Assessment%20of%20the%20Poverty%20of%20the%20Stimulus%20Argument%20for%20Neural%20Language%20Models&entry.906535625=Xiulin%20Yang%20and%20Arianna%20Bisazza%20and%20Nathan%20Schneider%20and%20Ethan%20Gotlieb%20Wilcox&entry.1292438233=How%20can%20children%20acquire%20native-level%20syntax%20from%20limited%20input%3F%20According%20to%20the%20Poverty%20of%20the%20Stimulus%20Hypothesis%20%28PoSH%29%2C%20the%20linguistic%20input%20children%20receive%20is%20insufficient%20to%20explain%20certain%20generalizations%20that%20are%20robustly%20learned%3B%20innate%20linguistic%20constraints%2C%20many%20have%20argued%2C%20are%20thus%20necessary%20to%20explain%20language%20learning.%20Neural%20language%20models%2C%20which%20lack%20such%20language-specific%20constraints%20in%20their%20design%2C%20offer%20a%20computational%20test%20of%20this%20longstanding%20%28but%20controversial%29%20claim.%20We%20introduce%20%5Cposhbench%2C%20a%20training-and-evaluation%20suite%20targeting%20question%20formation%2C%20islands%20to%20movement%2C%20and%20other%20English%20phenomena%20at%20the%20center%20of%20the%20PoSH%20arguments.%20Training%20Transformer%20models%20on%2010--50M%20words%20of%20developmentally%20plausible%20text%2C%20we%20find%20indications%20of%20generalization%20on%20all%20phenomena%20even%20without%20direct%20positive%20evidence%20--%20yet%20neural%20models%20remain%20less%20data-efficient%20and%20their%20generalizations%20are%20weaker%20than%20those%20of%20children.%20We%20further%20enhance%20our%20models%20with%20three%20recently%20proposed%20cognitively%20motivated%20inductive%20biases.%20We%20find%20these%20biases%20improve%20general%20syntactic%20competence%20but%20not%20%5Cposhbench%20performance.%20Our%20findings%20challenge%20the%20claim%20that%20innate%20syntax%20is%20the%20only%20possible%20route%20to%20generalization%2C%20while%20suggesting%20that%20human-like%20data%20efficiency%20requires%20inductive%20biases%20beyond%20those%20tested%20here.&entry.1838667208=http%3A//arxiv.org/abs/2602.09992v1&entry.124074799=Read"},
{"title": "Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning", "author": "Akshay Mete and Shahid Aamir Sheikh and Tzu-Hsiang Lin and Dileep Kalathil and P. R. Kumar", "abstract": "Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.", "link": "http://arxiv.org/abs/2602.10044v1", "date": "2026-02-10", "relevancy": 1.0464, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5326}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20World%20Models%3A%20Efficient%20Exploration%20in%20Model-Based%20Deep%20Reinforcement%20Learning&body=Title%3A%20Optimistic%20World%20Models%3A%20Efficient%20Exploration%20in%20Model-Based%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Akshay%20Mete%20and%20Shahid%20Aamir%20Sheikh%20and%20Tzu-Hsiang%20Lin%20and%20Dileep%20Kalathil%20and%20P.%20R.%20Kumar%0AAbstract%3A%20Efficient%20exploration%20remains%20a%20central%20challenge%20in%20reinforcement%20learning%20%28RL%29%2C%20particularly%20in%20sparse-reward%20environments.%20We%20introduce%20Optimistic%20World%20Models%20%28OWMs%29%2C%20a%20principled%20and%20scalable%20framework%20for%20optimistic%20exploration%20that%20brings%20classical%20reward-biased%20maximum%20likelihood%20estimation%20%28RBMLE%29%20from%20adaptive%20control%20into%20deep%20RL.%20In%20contrast%20to%20upper%20confidence%20bound%20%28UCB%29-style%20exploration%20methods%2C%20OWMs%20incorporate%20optimism%20directly%20into%20model%20learning%20by%20augmentation%20with%20an%20optimistic%20dynamics%20loss%20that%20biases%20imagined%20transitions%20toward%20higher-reward%20outcomes.%20This%20fully%20gradient-based%20loss%20requires%20neither%20uncertainty%20estimates%20nor%20constrained%20optimization.%20Our%20approach%20is%20plug-and-play%20with%20existing%20world%20model%20frameworks%2C%20preserving%20scalability%20while%20requiring%20only%20minimal%20modifications%20to%20standard%20training%20procedures.%20We%20instantiate%20OWMs%20within%20two%20state-of-the-art%20world%20model%20architectures%2C%20leading%20to%20Optimistic%20DreamerV3%20and%20Optimistic%20STORM%2C%20which%20demonstrate%20significant%20improvements%20in%20sample%20efficiency%20and%20cumulative%20return%20compared%20to%20their%20baseline%20counterparts.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520World%2520Models%253A%2520Efficient%2520Exploration%2520in%2520Model-Based%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DAkshay%2520Mete%2520and%2520Shahid%2520Aamir%2520Sheikh%2520and%2520Tzu-Hsiang%2520Lin%2520and%2520Dileep%2520Kalathil%2520and%2520P.%2520R.%2520Kumar%26entry.1292438233%3DEfficient%2520exploration%2520remains%2520a%2520central%2520challenge%2520in%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520particularly%2520in%2520sparse-reward%2520environments.%2520We%2520introduce%2520Optimistic%2520World%2520Models%2520%2528OWMs%2529%252C%2520a%2520principled%2520and%2520scalable%2520framework%2520for%2520optimistic%2520exploration%2520that%2520brings%2520classical%2520reward-biased%2520maximum%2520likelihood%2520estimation%2520%2528RBMLE%2529%2520from%2520adaptive%2520control%2520into%2520deep%2520RL.%2520In%2520contrast%2520to%2520upper%2520confidence%2520bound%2520%2528UCB%2529-style%2520exploration%2520methods%252C%2520OWMs%2520incorporate%2520optimism%2520directly%2520into%2520model%2520learning%2520by%2520augmentation%2520with%2520an%2520optimistic%2520dynamics%2520loss%2520that%2520biases%2520imagined%2520transitions%2520toward%2520higher-reward%2520outcomes.%2520This%2520fully%2520gradient-based%2520loss%2520requires%2520neither%2520uncertainty%2520estimates%2520nor%2520constrained%2520optimization.%2520Our%2520approach%2520is%2520plug-and-play%2520with%2520existing%2520world%2520model%2520frameworks%252C%2520preserving%2520scalability%2520while%2520requiring%2520only%2520minimal%2520modifications%2520to%2520standard%2520training%2520procedures.%2520We%2520instantiate%2520OWMs%2520within%2520two%2520state-of-the-art%2520world%2520model%2520architectures%252C%2520leading%2520to%2520Optimistic%2520DreamerV3%2520and%2520Optimistic%2520STORM%252C%2520which%2520demonstrate%2520significant%2520improvements%2520in%2520sample%2520efficiency%2520and%2520cumulative%2520return%2520compared%2520to%2520their%2520baseline%2520counterparts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20World%20Models%3A%20Efficient%20Exploration%20in%20Model-Based%20Deep%20Reinforcement%20Learning&entry.906535625=Akshay%20Mete%20and%20Shahid%20Aamir%20Sheikh%20and%20Tzu-Hsiang%20Lin%20and%20Dileep%20Kalathil%20and%20P.%20R.%20Kumar&entry.1292438233=Efficient%20exploration%20remains%20a%20central%20challenge%20in%20reinforcement%20learning%20%28RL%29%2C%20particularly%20in%20sparse-reward%20environments.%20We%20introduce%20Optimistic%20World%20Models%20%28OWMs%29%2C%20a%20principled%20and%20scalable%20framework%20for%20optimistic%20exploration%20that%20brings%20classical%20reward-biased%20maximum%20likelihood%20estimation%20%28RBMLE%29%20from%20adaptive%20control%20into%20deep%20RL.%20In%20contrast%20to%20upper%20confidence%20bound%20%28UCB%29-style%20exploration%20methods%2C%20OWMs%20incorporate%20optimism%20directly%20into%20model%20learning%20by%20augmentation%20with%20an%20optimistic%20dynamics%20loss%20that%20biases%20imagined%20transitions%20toward%20higher-reward%20outcomes.%20This%20fully%20gradient-based%20loss%20requires%20neither%20uncertainty%20estimates%20nor%20constrained%20optimization.%20Our%20approach%20is%20plug-and-play%20with%20existing%20world%20model%20frameworks%2C%20preserving%20scalability%20while%20requiring%20only%20minimal%20modifications%20to%20standard%20training%20procedures.%20We%20instantiate%20OWMs%20within%20two%20state-of-the-art%20world%20model%20architectures%2C%20leading%20to%20Optimistic%20DreamerV3%20and%20Optimistic%20STORM%2C%20which%20demonstrate%20significant%20improvements%20in%20sample%20efficiency%20and%20cumulative%20return%20compared%20to%20their%20baseline%20counterparts.&entry.1838667208=http%3A//arxiv.org/abs/2602.10044v1&entry.124074799=Read"},
{"title": "ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference", "author": "Junda Wang and Zhichao Yang and Dongxu Zhang and Sanjit Singh Batra and Robert E. Tillman", "abstract": "Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.", "link": "http://arxiv.org/abs/2602.10004v1", "date": "2026-02-10", "relevancy": 1.8668, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4923}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESTAR%3A%20Early-Stopping%20Token-Aware%20Reasoning%20For%20Efficient%20Inference&body=Title%3A%20ESTAR%3A%20Early-Stopping%20Token-Aware%20Reasoning%20For%20Efficient%20Inference%0AAuthor%3A%20Junda%20Wang%20and%20Zhichao%20Yang%20and%20Dongxu%20Zhang%20and%20Sanjit%20Singh%20Batra%20and%20Robert%20E.%20Tillman%0AAbstract%3A%20Large%20reasoning%20models%20%28LRMs%29%20achieve%20state-of-the-art%20performance%20by%20generating%20long%20chains-of-thought%2C%20but%20often%20waste%20computation%20on%20redundant%20reasoning%20after%20the%20correct%20answer%20has%20already%20been%20reached.%20We%20introduce%20Early-Stopping%20for%20Token-Aware%20Reasoning%20%28ESTAR%29%2C%20which%20detects%20and%20reduces%20such%20reasoning%20redundancy%20to%20improve%20efficiency%20without%20sacrificing%20accuracy.%20Our%20method%20combines%20%28i%29%20a%20trajectory-based%20classifier%20that%20identifies%20when%20reasoning%20can%20be%20safely%20stopped%2C%20%28ii%29%20supervised%20fine-tuning%20to%20teach%20LRMs%20to%20propose%20self-generated%20%3Cstop%3E%20signals%2C%20and%20%28iii%29%20%3Cstop%3E-aware%20reinforcement%20learning%20that%20truncates%20rollouts%20at%20self-generated%20stop%20points%20with%20compute-aware%20rewards.%20Experiments%20on%20four%20reasoning%20datasets%20show%20that%20ESTAR%20reduces%20reasoning%20length%20by%20about%203.7x%20%28from%204%2C799%20to%201%2C290%29%20while%20preserving%20accuracy%20%2874.9%25%20vs.%2074.2%25%29%2C%20with%20strong%20cross-domain%20generalization.%20These%20results%20highlight%20early%20stopping%20as%20a%20simple%20yet%20powerful%20mechanism%20for%20improving%20reasoning%20efficiency%20in%20LRMs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESTAR%253A%2520Early-Stopping%2520Token-Aware%2520Reasoning%2520For%2520Efficient%2520Inference%26entry.906535625%3DJunda%2520Wang%2520and%2520Zhichao%2520Yang%2520and%2520Dongxu%2520Zhang%2520and%2520Sanjit%2520Singh%2520Batra%2520and%2520Robert%2520E.%2520Tillman%26entry.1292438233%3DLarge%2520reasoning%2520models%2520%2528LRMs%2529%2520achieve%2520state-of-the-art%2520performance%2520by%2520generating%2520long%2520chains-of-thought%252C%2520but%2520often%2520waste%2520computation%2520on%2520redundant%2520reasoning%2520after%2520the%2520correct%2520answer%2520has%2520already%2520been%2520reached.%2520We%2520introduce%2520Early-Stopping%2520for%2520Token-Aware%2520Reasoning%2520%2528ESTAR%2529%252C%2520which%2520detects%2520and%2520reduces%2520such%2520reasoning%2520redundancy%2520to%2520improve%2520efficiency%2520without%2520sacrificing%2520accuracy.%2520Our%2520method%2520combines%2520%2528i%2529%2520a%2520trajectory-based%2520classifier%2520that%2520identifies%2520when%2520reasoning%2520can%2520be%2520safely%2520stopped%252C%2520%2528ii%2529%2520supervised%2520fine-tuning%2520to%2520teach%2520LRMs%2520to%2520propose%2520self-generated%2520%253Cstop%253E%2520signals%252C%2520and%2520%2528iii%2529%2520%253Cstop%253E-aware%2520reinforcement%2520learning%2520that%2520truncates%2520rollouts%2520at%2520self-generated%2520stop%2520points%2520with%2520compute-aware%2520rewards.%2520Experiments%2520on%2520four%2520reasoning%2520datasets%2520show%2520that%2520ESTAR%2520reduces%2520reasoning%2520length%2520by%2520about%25203.7x%2520%2528from%25204%252C799%2520to%25201%252C290%2529%2520while%2520preserving%2520accuracy%2520%252874.9%2525%2520vs.%252074.2%2525%2529%252C%2520with%2520strong%2520cross-domain%2520generalization.%2520These%2520results%2520highlight%2520early%2520stopping%2520as%2520a%2520simple%2520yet%2520powerful%2520mechanism%2520for%2520improving%2520reasoning%2520efficiency%2520in%2520LRMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESTAR%3A%20Early-Stopping%20Token-Aware%20Reasoning%20For%20Efficient%20Inference&entry.906535625=Junda%20Wang%20and%20Zhichao%20Yang%20and%20Dongxu%20Zhang%20and%20Sanjit%20Singh%20Batra%20and%20Robert%20E.%20Tillman&entry.1292438233=Large%20reasoning%20models%20%28LRMs%29%20achieve%20state-of-the-art%20performance%20by%20generating%20long%20chains-of-thought%2C%20but%20often%20waste%20computation%20on%20redundant%20reasoning%20after%20the%20correct%20answer%20has%20already%20been%20reached.%20We%20introduce%20Early-Stopping%20for%20Token-Aware%20Reasoning%20%28ESTAR%29%2C%20which%20detects%20and%20reduces%20such%20reasoning%20redundancy%20to%20improve%20efficiency%20without%20sacrificing%20accuracy.%20Our%20method%20combines%20%28i%29%20a%20trajectory-based%20classifier%20that%20identifies%20when%20reasoning%20can%20be%20safely%20stopped%2C%20%28ii%29%20supervised%20fine-tuning%20to%20teach%20LRMs%20to%20propose%20self-generated%20%3Cstop%3E%20signals%2C%20and%20%28iii%29%20%3Cstop%3E-aware%20reinforcement%20learning%20that%20truncates%20rollouts%20at%20self-generated%20stop%20points%20with%20compute-aware%20rewards.%20Experiments%20on%20four%20reasoning%20datasets%20show%20that%20ESTAR%20reduces%20reasoning%20length%20by%20about%203.7x%20%28from%204%2C799%20to%201%2C290%29%20while%20preserving%20accuracy%20%2874.9%25%20vs.%2074.2%25%29%2C%20with%20strong%20cross-domain%20generalization.%20These%20results%20highlight%20early%20stopping%20as%20a%20simple%20yet%20powerful%20mechanism%20for%20improving%20reasoning%20efficiency%20in%20LRMs.&entry.1838667208=http%3A//arxiv.org/abs/2602.10004v1&entry.124074799=Read"},
{"title": "Stabilized Maximum-Likelihood Iterative Quantum Amplitude Estimation for Structural CVaR under Correlated Random Fields", "author": "Alireza Tabarraei", "abstract": "Conditional Value-at-Risk (CVaR) is a central tail-risk measure in stochastic structural mechanics, yet its accurate evaluation under high-dimensional, spatially correlated material uncertainty remains computationally prohibitive for classical Monte Carlo methods. Leveraging bounded-expectation reformulations of CVaR compatible with quantum amplitude estimation, we develop a quantum-enhanced inference framework that casts CVaR evaluation as a statistically consistent, confidence-constrained maximum-likelihood amplitude estimation problem. The proposed method extends iterative quantum amplitude estimation (IQAE) by embedding explicit maximum-likelihood inference within a rigorously controlled interval-tracking architecture. To ensure global correctness under finite-shot noise and the non-injective oscillatory response induced by Grover amplification, we introduce a stabilized inference scheme incorporating multi-hypothesis feasibility tracking, periodic low-depth disambiguation, and a bounded restart mechanism governed by an explicit failure-probability budget. This formulation preserves the quadratic oracle-complexity advantage of amplitude estimation while providing finite-sample confidence guarantees and reduced estimator variance. The framework is demonstrated on benchmark problems with spatially correlated lognormal Young's modulus fields generated using a Nystrom low-rank Gaussian kernel model. Numerical results show that the proposed estimator achieves substantially lower oracle complexity than classical Monte Carlo CVaR estimation at comparable confidence levels, while maintaining rigorous statistical reliability. This work establishes a practically robust and theoretically grounded quantum-enhanced methodology for tail-risk quantification in stochastic continuum mechanics.", "link": "http://arxiv.org/abs/2602.09847v1", "date": "2026-02-10", "relevancy": 1.8375, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5086}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilized%20Maximum-Likelihood%20Iterative%20Quantum%20Amplitude%20Estimation%20for%20Structural%20CVaR%20under%20Correlated%20Random%20Fields&body=Title%3A%20Stabilized%20Maximum-Likelihood%20Iterative%20Quantum%20Amplitude%20Estimation%20for%20Structural%20CVaR%20under%20Correlated%20Random%20Fields%0AAuthor%3A%20Alireza%20Tabarraei%0AAbstract%3A%20Conditional%20Value-at-Risk%20%28CVaR%29%20is%20a%20central%20tail-risk%20measure%20in%20stochastic%20structural%20mechanics%2C%20yet%20its%20accurate%20evaluation%20under%20high-dimensional%2C%20spatially%20correlated%20material%20uncertainty%20remains%20computationally%20prohibitive%20for%20classical%20Monte%20Carlo%20methods.%20Leveraging%20bounded-expectation%20reformulations%20of%20CVaR%20compatible%20with%20quantum%20amplitude%20estimation%2C%20we%20develop%20a%20quantum-enhanced%20inference%20framework%20that%20casts%20CVaR%20evaluation%20as%20a%20statistically%20consistent%2C%20confidence-constrained%20maximum-likelihood%20amplitude%20estimation%20problem.%20The%20proposed%20method%20extends%20iterative%20quantum%20amplitude%20estimation%20%28IQAE%29%20by%20embedding%20explicit%20maximum-likelihood%20inference%20within%20a%20rigorously%20controlled%20interval-tracking%20architecture.%20To%20ensure%20global%20correctness%20under%20finite-shot%20noise%20and%20the%20non-injective%20oscillatory%20response%20induced%20by%20Grover%20amplification%2C%20we%20introduce%20a%20stabilized%20inference%20scheme%20incorporating%20multi-hypothesis%20feasibility%20tracking%2C%20periodic%20low-depth%20disambiguation%2C%20and%20a%20bounded%20restart%20mechanism%20governed%20by%20an%20explicit%20failure-probability%20budget.%20This%20formulation%20preserves%20the%20quadratic%20oracle-complexity%20advantage%20of%20amplitude%20estimation%20while%20providing%20finite-sample%20confidence%20guarantees%20and%20reduced%20estimator%20variance.%20The%20framework%20is%20demonstrated%20on%20benchmark%20problems%20with%20spatially%20correlated%20lognormal%20Young%27s%20modulus%20fields%20generated%20using%20a%20Nystrom%20low-rank%20Gaussian%20kernel%20model.%20Numerical%20results%20show%20that%20the%20proposed%20estimator%20achieves%20substantially%20lower%20oracle%20complexity%20than%20classical%20Monte%20Carlo%20CVaR%20estimation%20at%20comparable%20confidence%20levels%2C%20while%20maintaining%20rigorous%20statistical%20reliability.%20This%20work%20establishes%20a%20practically%20robust%20and%20theoretically%20grounded%20quantum-enhanced%20methodology%20for%20tail-risk%20quantification%20in%20stochastic%20continuum%20mechanics.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilized%2520Maximum-Likelihood%2520Iterative%2520Quantum%2520Amplitude%2520Estimation%2520for%2520Structural%2520CVaR%2520under%2520Correlated%2520Random%2520Fields%26entry.906535625%3DAlireza%2520Tabarraei%26entry.1292438233%3DConditional%2520Value-at-Risk%2520%2528CVaR%2529%2520is%2520a%2520central%2520tail-risk%2520measure%2520in%2520stochastic%2520structural%2520mechanics%252C%2520yet%2520its%2520accurate%2520evaluation%2520under%2520high-dimensional%252C%2520spatially%2520correlated%2520material%2520uncertainty%2520remains%2520computationally%2520prohibitive%2520for%2520classical%2520Monte%2520Carlo%2520methods.%2520Leveraging%2520bounded-expectation%2520reformulations%2520of%2520CVaR%2520compatible%2520with%2520quantum%2520amplitude%2520estimation%252C%2520we%2520develop%2520a%2520quantum-enhanced%2520inference%2520framework%2520that%2520casts%2520CVaR%2520evaluation%2520as%2520a%2520statistically%2520consistent%252C%2520confidence-constrained%2520maximum-likelihood%2520amplitude%2520estimation%2520problem.%2520The%2520proposed%2520method%2520extends%2520iterative%2520quantum%2520amplitude%2520estimation%2520%2528IQAE%2529%2520by%2520embedding%2520explicit%2520maximum-likelihood%2520inference%2520within%2520a%2520rigorously%2520controlled%2520interval-tracking%2520architecture.%2520To%2520ensure%2520global%2520correctness%2520under%2520finite-shot%2520noise%2520and%2520the%2520non-injective%2520oscillatory%2520response%2520induced%2520by%2520Grover%2520amplification%252C%2520we%2520introduce%2520a%2520stabilized%2520inference%2520scheme%2520incorporating%2520multi-hypothesis%2520feasibility%2520tracking%252C%2520periodic%2520low-depth%2520disambiguation%252C%2520and%2520a%2520bounded%2520restart%2520mechanism%2520governed%2520by%2520an%2520explicit%2520failure-probability%2520budget.%2520This%2520formulation%2520preserves%2520the%2520quadratic%2520oracle-complexity%2520advantage%2520of%2520amplitude%2520estimation%2520while%2520providing%2520finite-sample%2520confidence%2520guarantees%2520and%2520reduced%2520estimator%2520variance.%2520The%2520framework%2520is%2520demonstrated%2520on%2520benchmark%2520problems%2520with%2520spatially%2520correlated%2520lognormal%2520Young%2527s%2520modulus%2520fields%2520generated%2520using%2520a%2520Nystrom%2520low-rank%2520Gaussian%2520kernel%2520model.%2520Numerical%2520results%2520show%2520that%2520the%2520proposed%2520estimator%2520achieves%2520substantially%2520lower%2520oracle%2520complexity%2520than%2520classical%2520Monte%2520Carlo%2520CVaR%2520estimation%2520at%2520comparable%2520confidence%2520levels%252C%2520while%2520maintaining%2520rigorous%2520statistical%2520reliability.%2520This%2520work%2520establishes%2520a%2520practically%2520robust%2520and%2520theoretically%2520grounded%2520quantum-enhanced%2520methodology%2520for%2520tail-risk%2520quantification%2520in%2520stochastic%2520continuum%2520mechanics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilized%20Maximum-Likelihood%20Iterative%20Quantum%20Amplitude%20Estimation%20for%20Structural%20CVaR%20under%20Correlated%20Random%20Fields&entry.906535625=Alireza%20Tabarraei&entry.1292438233=Conditional%20Value-at-Risk%20%28CVaR%29%20is%20a%20central%20tail-risk%20measure%20in%20stochastic%20structural%20mechanics%2C%20yet%20its%20accurate%20evaluation%20under%20high-dimensional%2C%20spatially%20correlated%20material%20uncertainty%20remains%20computationally%20prohibitive%20for%20classical%20Monte%20Carlo%20methods.%20Leveraging%20bounded-expectation%20reformulations%20of%20CVaR%20compatible%20with%20quantum%20amplitude%20estimation%2C%20we%20develop%20a%20quantum-enhanced%20inference%20framework%20that%20casts%20CVaR%20evaluation%20as%20a%20statistically%20consistent%2C%20confidence-constrained%20maximum-likelihood%20amplitude%20estimation%20problem.%20The%20proposed%20method%20extends%20iterative%20quantum%20amplitude%20estimation%20%28IQAE%29%20by%20embedding%20explicit%20maximum-likelihood%20inference%20within%20a%20rigorously%20controlled%20interval-tracking%20architecture.%20To%20ensure%20global%20correctness%20under%20finite-shot%20noise%20and%20the%20non-injective%20oscillatory%20response%20induced%20by%20Grover%20amplification%2C%20we%20introduce%20a%20stabilized%20inference%20scheme%20incorporating%20multi-hypothesis%20feasibility%20tracking%2C%20periodic%20low-depth%20disambiguation%2C%20and%20a%20bounded%20restart%20mechanism%20governed%20by%20an%20explicit%20failure-probability%20budget.%20This%20formulation%20preserves%20the%20quadratic%20oracle-complexity%20advantage%20of%20amplitude%20estimation%20while%20providing%20finite-sample%20confidence%20guarantees%20and%20reduced%20estimator%20variance.%20The%20framework%20is%20demonstrated%20on%20benchmark%20problems%20with%20spatially%20correlated%20lognormal%20Young%27s%20modulus%20fields%20generated%20using%20a%20Nystrom%20low-rank%20Gaussian%20kernel%20model.%20Numerical%20results%20show%20that%20the%20proposed%20estimator%20achieves%20substantially%20lower%20oracle%20complexity%20than%20classical%20Monte%20Carlo%20CVaR%20estimation%20at%20comparable%20confidence%20levels%2C%20while%20maintaining%20rigorous%20statistical%20reliability.%20This%20work%20establishes%20a%20practically%20robust%20and%20theoretically%20grounded%20quantum-enhanced%20methodology%20for%20tail-risk%20quantification%20in%20stochastic%20continuum%20mechanics.&entry.1838667208=http%3A//arxiv.org/abs/2602.09847v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


