<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250211.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping", "author": "Aashish Rai and Dilin Wang and Mihir Jain and Nikolaos Sarafianos and Kefan Chen and Srinath Sridhar and Aayush Prakash", "abstract": "  3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D\nobjects and scenes. However, generating 3DGS remains challenging due to their\ndiscrete, unstructured, and permutation-invariant nature. In this work, we\npresent a simple yet effective method to overcome these challenges. We utilize\nspherical mapping to transform 3DGS into a structured 2D representation, termed\nUVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a\nconcatenation of Gaussian attributes such as position, scale, color, opacity,\nand rotation. We further find that these heterogeneous features can be\ncompressed into a lower-dimensional (e.g., 3-channel) shared feature space\nusing a carefully designed multi-branch network. The compressed UVGS can be\ntreated as typical RGB images. Remarkably, we discover that typical VAEs\ntrained with latent diffusion models can directly generalize to this new\nrepresentation without additional training. Our novel representation makes it\neffortless to leverage foundational 2D models, such as diffusion models, to\ndirectly model 3DGS. Additionally, one can simply increase the 2D UV resolution\nto accommodate more Gaussians, making UVGS a scalable solution compared to\ntypical 3D backbones. This approach immediately unlocks various novel\ngeneration applications of 3DGS by inherently utilizing the already developed\nsuperior 2D generation capabilities. In our experiments, we demonstrate various\nunconditional, conditional generation, and inpainting applications of 3DGS\nbased on diffusion models, which were previously non-trivial.\n", "link": "http://arxiv.org/abs/2502.01846v2", "date": "2025-02-11", "relevancy": 3.389, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.703}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6773}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UVGS%3A%20Reimagining%20Unstructured%203D%20Gaussian%20Splatting%20using%20UV%20Mapping&body=Title%3A%20UVGS%3A%20Reimagining%20Unstructured%203D%20Gaussian%20Splatting%20using%20UV%20Mapping%0AAuthor%3A%20Aashish%20Rai%20and%20Dilin%20Wang%20and%20Mihir%20Jain%20and%20Nikolaos%20Sarafianos%20and%20Kefan%20Chen%20and%20Srinath%20Sridhar%20and%20Aayush%20Prakash%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20superior%20quality%20in%20modeling%203D%0Aobjects%20and%20scenes.%20However%2C%20generating%203DGS%20remains%20challenging%20due%20to%20their%0Adiscrete%2C%20unstructured%2C%20and%20permutation-invariant%20nature.%20In%20this%20work%2C%20we%0Apresent%20a%20simple%20yet%20effective%20method%20to%20overcome%20these%20challenges.%20We%20utilize%0Aspherical%20mapping%20to%20transform%203DGS%20into%20a%20structured%202D%20representation%2C%20termed%0AUVGS.%20UVGS%20can%20be%20viewed%20as%20multi-channel%20images%2C%20with%20feature%20dimensions%20as%20a%0Aconcatenation%20of%20Gaussian%20attributes%20such%20as%20position%2C%20scale%2C%20color%2C%20opacity%2C%0Aand%20rotation.%20We%20further%20find%20that%20these%20heterogeneous%20features%20can%20be%0Acompressed%20into%20a%20lower-dimensional%20%28e.g.%2C%203-channel%29%20shared%20feature%20space%0Ausing%20a%20carefully%20designed%20multi-branch%20network.%20The%20compressed%20UVGS%20can%20be%0Atreated%20as%20typical%20RGB%20images.%20Remarkably%2C%20we%20discover%20that%20typical%20VAEs%0Atrained%20with%20latent%20diffusion%20models%20can%20directly%20generalize%20to%20this%20new%0Arepresentation%20without%20additional%20training.%20Our%20novel%20representation%20makes%20it%0Aeffortless%20to%20leverage%20foundational%202D%20models%2C%20such%20as%20diffusion%20models%2C%20to%0Adirectly%20model%203DGS.%20Additionally%2C%20one%20can%20simply%20increase%20the%202D%20UV%20resolution%0Ato%20accommodate%20more%20Gaussians%2C%20making%20UVGS%20a%20scalable%20solution%20compared%20to%0Atypical%203D%20backbones.%20This%20approach%20immediately%20unlocks%20various%20novel%0Ageneration%20applications%20of%203DGS%20by%20inherently%20utilizing%20the%20already%20developed%0Asuperior%202D%20generation%20capabilities.%20In%20our%20experiments%2C%20we%20demonstrate%20various%0Aunconditional%2C%20conditional%20generation%2C%20and%20inpainting%20applications%20of%203DGS%0Abased%20on%20diffusion%20models%2C%20which%20were%20previously%20non-trivial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUVGS%253A%2520Reimagining%2520Unstructured%25203D%2520Gaussian%2520Splatting%2520using%2520UV%2520Mapping%26entry.906535625%3DAashish%2520Rai%2520and%2520Dilin%2520Wang%2520and%2520Mihir%2520Jain%2520and%2520Nikolaos%2520Sarafianos%2520and%2520Kefan%2520Chen%2520and%2520Srinath%2520Sridhar%2520and%2520Aayush%2520Prakash%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520superior%2520quality%2520in%2520modeling%25203D%250Aobjects%2520and%2520scenes.%2520However%252C%2520generating%25203DGS%2520remains%2520challenging%2520due%2520to%2520their%250Adiscrete%252C%2520unstructured%252C%2520and%2520permutation-invariant%2520nature.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520simple%2520yet%2520effective%2520method%2520to%2520overcome%2520these%2520challenges.%2520We%2520utilize%250Aspherical%2520mapping%2520to%2520transform%25203DGS%2520into%2520a%2520structured%25202D%2520representation%252C%2520termed%250AUVGS.%2520UVGS%2520can%2520be%2520viewed%2520as%2520multi-channel%2520images%252C%2520with%2520feature%2520dimensions%2520as%2520a%250Aconcatenation%2520of%2520Gaussian%2520attributes%2520such%2520as%2520position%252C%2520scale%252C%2520color%252C%2520opacity%252C%250Aand%2520rotation.%2520We%2520further%2520find%2520that%2520these%2520heterogeneous%2520features%2520can%2520be%250Acompressed%2520into%2520a%2520lower-dimensional%2520%2528e.g.%252C%25203-channel%2529%2520shared%2520feature%2520space%250Ausing%2520a%2520carefully%2520designed%2520multi-branch%2520network.%2520The%2520compressed%2520UVGS%2520can%2520be%250Atreated%2520as%2520typical%2520RGB%2520images.%2520Remarkably%252C%2520we%2520discover%2520that%2520typical%2520VAEs%250Atrained%2520with%2520latent%2520diffusion%2520models%2520can%2520directly%2520generalize%2520to%2520this%2520new%250Arepresentation%2520without%2520additional%2520training.%2520Our%2520novel%2520representation%2520makes%2520it%250Aeffortless%2520to%2520leverage%2520foundational%25202D%2520models%252C%2520such%2520as%2520diffusion%2520models%252C%2520to%250Adirectly%2520model%25203DGS.%2520Additionally%252C%2520one%2520can%2520simply%2520increase%2520the%25202D%2520UV%2520resolution%250Ato%2520accommodate%2520more%2520Gaussians%252C%2520making%2520UVGS%2520a%2520scalable%2520solution%2520compared%2520to%250Atypical%25203D%2520backbones.%2520This%2520approach%2520immediately%2520unlocks%2520various%2520novel%250Ageneration%2520applications%2520of%25203DGS%2520by%2520inherently%2520utilizing%2520the%2520already%2520developed%250Asuperior%25202D%2520generation%2520capabilities.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520various%250Aunconditional%252C%2520conditional%2520generation%252C%2520and%2520inpainting%2520applications%2520of%25203DGS%250Abased%2520on%2520diffusion%2520models%252C%2520which%2520were%2520previously%2520non-trivial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UVGS%3A%20Reimagining%20Unstructured%203D%20Gaussian%20Splatting%20using%20UV%20Mapping&entry.906535625=Aashish%20Rai%20and%20Dilin%20Wang%20and%20Mihir%20Jain%20and%20Nikolaos%20Sarafianos%20and%20Kefan%20Chen%20and%20Srinath%20Sridhar%20and%20Aayush%20Prakash&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20superior%20quality%20in%20modeling%203D%0Aobjects%20and%20scenes.%20However%2C%20generating%203DGS%20remains%20challenging%20due%20to%20their%0Adiscrete%2C%20unstructured%2C%20and%20permutation-invariant%20nature.%20In%20this%20work%2C%20we%0Apresent%20a%20simple%20yet%20effective%20method%20to%20overcome%20these%20challenges.%20We%20utilize%0Aspherical%20mapping%20to%20transform%203DGS%20into%20a%20structured%202D%20representation%2C%20termed%0AUVGS.%20UVGS%20can%20be%20viewed%20as%20multi-channel%20images%2C%20with%20feature%20dimensions%20as%20a%0Aconcatenation%20of%20Gaussian%20attributes%20such%20as%20position%2C%20scale%2C%20color%2C%20opacity%2C%0Aand%20rotation.%20We%20further%20find%20that%20these%20heterogeneous%20features%20can%20be%0Acompressed%20into%20a%20lower-dimensional%20%28e.g.%2C%203-channel%29%20shared%20feature%20space%0Ausing%20a%20carefully%20designed%20multi-branch%20network.%20The%20compressed%20UVGS%20can%20be%0Atreated%20as%20typical%20RGB%20images.%20Remarkably%2C%20we%20discover%20that%20typical%20VAEs%0Atrained%20with%20latent%20diffusion%20models%20can%20directly%20generalize%20to%20this%20new%0Arepresentation%20without%20additional%20training.%20Our%20novel%20representation%20makes%20it%0Aeffortless%20to%20leverage%20foundational%202D%20models%2C%20such%20as%20diffusion%20models%2C%20to%0Adirectly%20model%203DGS.%20Additionally%2C%20one%20can%20simply%20increase%20the%202D%20UV%20resolution%0Ato%20accommodate%20more%20Gaussians%2C%20making%20UVGS%20a%20scalable%20solution%20compared%20to%0Atypical%203D%20backbones.%20This%20approach%20immediately%20unlocks%20various%20novel%0Ageneration%20applications%20of%203DGS%20by%20inherently%20utilizing%20the%20already%20developed%0Asuperior%202D%20generation%20capabilities.%20In%20our%20experiments%2C%20we%20demonstrate%20various%0Aunconditional%2C%20conditional%20generation%2C%20and%20inpainting%20applications%20of%203DGS%0Abased%20on%20diffusion%20models%2C%20which%20were%20previously%20non-trivial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01846v2&entry.124074799=Read"},
{"title": "MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization", "author": "Rafa\u0142 Tobiasz and Grzegorz Wilczy\u0144ski and Marcin Mazur and S\u0142awomir Tadeja and Przemys\u0142aw Spurek", "abstract": "  Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer\ngraphics. GS-based algorithms almost always bypass classical methods such as\nray tracing, which offers numerous inherent advantages for rendering. For\nexample, ray tracing is able to handle incoherent rays for advanced lighting\neffects, including shadows and reflections. To address this limitation, we\nintroduce MeshSplats, a method which converts GS to a mesh-like format.\nFollowing the completion of training, MeshSplats transforms Gaussian elements\ninto mesh faces, enabling rendering using ray tracing methods with all their\nassociated benefits. Our model can be utilized immediately following\ntransformation, yielding a mesh of slightly reduced quality without additional\ntraining. Furthermore, we can enhance the reconstruction quality through the\napplication of a dedicated optimization algorithm that operates on mesh faces\nrather than Gaussian components. The efficacy of our method is substantiated by\nexperimental results, underscoring its extensive applications in computer\ngraphics and image processing.\n", "link": "http://arxiv.org/abs/2502.07754v1", "date": "2025-02-11", "relevancy": 3.3494, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6728}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6707}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshSplats%3A%20Mesh-Based%20Rendering%20with%20Gaussian%20Splatting%20Initialization&body=Title%3A%20MeshSplats%3A%20Mesh-Based%20Rendering%20with%20Gaussian%20Splatting%20Initialization%0AAuthor%3A%20Rafa%C5%82%20Tobiasz%20and%20Grzegorz%20Wilczy%C5%84ski%20and%20Marcin%20Mazur%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Gaussian%20Splatting%20%28GS%29%20is%20a%20recent%20and%20pivotal%20technique%20in%203D%20computer%0Agraphics.%20GS-based%20algorithms%20almost%20always%20bypass%20classical%20methods%20such%20as%0Aray%20tracing%2C%20which%20offers%20numerous%20inherent%20advantages%20for%20rendering.%20For%0Aexample%2C%20ray%20tracing%20is%20able%20to%20handle%20incoherent%20rays%20for%20advanced%20lighting%0Aeffects%2C%20including%20shadows%20and%20reflections.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20MeshSplats%2C%20a%20method%20which%20converts%20GS%20to%20a%20mesh-like%20format.%0AFollowing%20the%20completion%20of%20training%2C%20MeshSplats%20transforms%20Gaussian%20elements%0Ainto%20mesh%20faces%2C%20enabling%20rendering%20using%20ray%20tracing%20methods%20with%20all%20their%0Aassociated%20benefits.%20Our%20model%20can%20be%20utilized%20immediately%20following%0Atransformation%2C%20yielding%20a%20mesh%20of%20slightly%20reduced%20quality%20without%20additional%0Atraining.%20Furthermore%2C%20we%20can%20enhance%20the%20reconstruction%20quality%20through%20the%0Aapplication%20of%20a%20dedicated%20optimization%20algorithm%20that%20operates%20on%20mesh%20faces%0Arather%20than%20Gaussian%20components.%20The%20efficacy%20of%20our%20method%20is%20substantiated%20by%0Aexperimental%20results%2C%20underscoring%20its%20extensive%20applications%20in%20computer%0Agraphics%20and%20image%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshSplats%253A%2520Mesh-Based%2520Rendering%2520with%2520Gaussian%2520Splatting%2520Initialization%26entry.906535625%3DRafa%25C5%2582%2520Tobiasz%2520and%2520Grzegorz%2520Wilczy%25C5%2584ski%2520and%2520Marcin%2520Mazur%2520and%2520S%25C5%2582awomir%2520Tadeja%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520%2528GS%2529%2520is%2520a%2520recent%2520and%2520pivotal%2520technique%2520in%25203D%2520computer%250Agraphics.%2520GS-based%2520algorithms%2520almost%2520always%2520bypass%2520classical%2520methods%2520such%2520as%250Aray%2520tracing%252C%2520which%2520offers%2520numerous%2520inherent%2520advantages%2520for%2520rendering.%2520For%250Aexample%252C%2520ray%2520tracing%2520is%2520able%2520to%2520handle%2520incoherent%2520rays%2520for%2520advanced%2520lighting%250Aeffects%252C%2520including%2520shadows%2520and%2520reflections.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520MeshSplats%252C%2520a%2520method%2520which%2520converts%2520GS%2520to%2520a%2520mesh-like%2520format.%250AFollowing%2520the%2520completion%2520of%2520training%252C%2520MeshSplats%2520transforms%2520Gaussian%2520elements%250Ainto%2520mesh%2520faces%252C%2520enabling%2520rendering%2520using%2520ray%2520tracing%2520methods%2520with%2520all%2520their%250Aassociated%2520benefits.%2520Our%2520model%2520can%2520be%2520utilized%2520immediately%2520following%250Atransformation%252C%2520yielding%2520a%2520mesh%2520of%2520slightly%2520reduced%2520quality%2520without%2520additional%250Atraining.%2520Furthermore%252C%2520we%2520can%2520enhance%2520the%2520reconstruction%2520quality%2520through%2520the%250Aapplication%2520of%2520a%2520dedicated%2520optimization%2520algorithm%2520that%2520operates%2520on%2520mesh%2520faces%250Arather%2520than%2520Gaussian%2520components.%2520The%2520efficacy%2520of%2520our%2520method%2520is%2520substantiated%2520by%250Aexperimental%2520results%252C%2520underscoring%2520its%2520extensive%2520applications%2520in%2520computer%250Agraphics%2520and%2520image%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshSplats%3A%20Mesh-Based%20Rendering%20with%20Gaussian%20Splatting%20Initialization&entry.906535625=Rafa%C5%82%20Tobiasz%20and%20Grzegorz%20Wilczy%C5%84ski%20and%20Marcin%20Mazur%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Gaussian%20Splatting%20%28GS%29%20is%20a%20recent%20and%20pivotal%20technique%20in%203D%20computer%0Agraphics.%20GS-based%20algorithms%20almost%20always%20bypass%20classical%20methods%20such%20as%0Aray%20tracing%2C%20which%20offers%20numerous%20inherent%20advantages%20for%20rendering.%20For%0Aexample%2C%20ray%20tracing%20is%20able%20to%20handle%20incoherent%20rays%20for%20advanced%20lighting%0Aeffects%2C%20including%20shadows%20and%20reflections.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20MeshSplats%2C%20a%20method%20which%20converts%20GS%20to%20a%20mesh-like%20format.%0AFollowing%20the%20completion%20of%20training%2C%20MeshSplats%20transforms%20Gaussian%20elements%0Ainto%20mesh%20faces%2C%20enabling%20rendering%20using%20ray%20tracing%20methods%20with%20all%20their%0Aassociated%20benefits.%20Our%20model%20can%20be%20utilized%20immediately%20following%0Atransformation%2C%20yielding%20a%20mesh%20of%20slightly%20reduced%20quality%20without%20additional%0Atraining.%20Furthermore%2C%20we%20can%20enhance%20the%20reconstruction%20quality%20through%20the%0Aapplication%20of%20a%20dedicated%20optimization%20algorithm%20that%20operates%20on%20mesh%20faces%0Arather%20than%20Gaussian%20components.%20The%20efficacy%20of%20our%20method%20is%20substantiated%20by%0Aexperimental%20results%2C%20underscoring%20its%20extensive%20applications%20in%20computer%0Agraphics%20and%20image%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07754v1&entry.124074799=Read"},
{"title": "Matrix3D: Large Photogrammetry Model All-in-One", "author": "Yuanxun Lu and Jingyang Zhang and Tian Fang and Jean-Daniel Nahmias and Yanghai Tsin and Long Quan and Xun Cao and Yao Yao and Shiwei Li", "abstract": "  We present Matrix3D, a unified model that performs several photogrammetry\nsubtasks, including pose estimation, depth prediction, and novel view synthesis\nusing just the same model. Matrix3D utilizes a multi-modal diffusion\ntransformer (DiT) to integrate transformations across several modalities, such\nas images, camera parameters, and depth maps. The key to Matrix3D's large-scale\nmulti-modal training lies in the incorporation of a mask learning strategy.\nThis enables full-modality model training even with partially complete data,\nsuch as bi-modality data of image-pose and image-depth pairs, thus\nsignificantly increases the pool of available training data. Matrix3D\ndemonstrates state-of-the-art performance in pose estimation and novel view\nsynthesis tasks. Additionally, it offers fine-grained control through\nmulti-round interactions, making it an innovative tool for 3D content creation.\nProject page: https://nju-3dv.github.io/projects/matrix3d.\n", "link": "http://arxiv.org/abs/2502.07685v1", "date": "2025-02-11", "relevancy": 3.2809, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6763}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6763}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix3D%3A%20Large%20Photogrammetry%20Model%20All-in-One&body=Title%3A%20Matrix3D%3A%20Large%20Photogrammetry%20Model%20All-in-One%0AAuthor%3A%20Yuanxun%20Lu%20and%20Jingyang%20Zhang%20and%20Tian%20Fang%20and%20Jean-Daniel%20Nahmias%20and%20Yanghai%20Tsin%20and%20Long%20Quan%20and%20Xun%20Cao%20and%20Yao%20Yao%20and%20Shiwei%20Li%0AAbstract%3A%20%20%20We%20present%20Matrix3D%2C%20a%20unified%20model%20that%20performs%20several%20photogrammetry%0Asubtasks%2C%20including%20pose%20estimation%2C%20depth%20prediction%2C%20and%20novel%20view%20synthesis%0Ausing%20just%20the%20same%20model.%20Matrix3D%20utilizes%20a%20multi-modal%20diffusion%0Atransformer%20%28DiT%29%20to%20integrate%20transformations%20across%20several%20modalities%2C%20such%0Aas%20images%2C%20camera%20parameters%2C%20and%20depth%20maps.%20The%20key%20to%20Matrix3D%27s%20large-scale%0Amulti-modal%20training%20lies%20in%20the%20incorporation%20of%20a%20mask%20learning%20strategy.%0AThis%20enables%20full-modality%20model%20training%20even%20with%20partially%20complete%20data%2C%0Asuch%20as%20bi-modality%20data%20of%20image-pose%20and%20image-depth%20pairs%2C%20thus%0Asignificantly%20increases%20the%20pool%20of%20available%20training%20data.%20Matrix3D%0Ademonstrates%20state-of-the-art%20performance%20in%20pose%20estimation%20and%20novel%20view%0Asynthesis%20tasks.%20Additionally%2C%20it%20offers%20fine-grained%20control%20through%0Amulti-round%20interactions%2C%20making%20it%20an%20innovative%20tool%20for%203D%20content%20creation.%0AProject%20page%3A%20https%3A//nju-3dv.github.io/projects/matrix3d.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix3D%253A%2520Large%2520Photogrammetry%2520Model%2520All-in-One%26entry.906535625%3DYuanxun%2520Lu%2520and%2520Jingyang%2520Zhang%2520and%2520Tian%2520Fang%2520and%2520Jean-Daniel%2520Nahmias%2520and%2520Yanghai%2520Tsin%2520and%2520Long%2520Quan%2520and%2520Xun%2520Cao%2520and%2520Yao%2520Yao%2520and%2520Shiwei%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520Matrix3D%252C%2520a%2520unified%2520model%2520that%2520performs%2520several%2520photogrammetry%250Asubtasks%252C%2520including%2520pose%2520estimation%252C%2520depth%2520prediction%252C%2520and%2520novel%2520view%2520synthesis%250Ausing%2520just%2520the%2520same%2520model.%2520Matrix3D%2520utilizes%2520a%2520multi-modal%2520diffusion%250Atransformer%2520%2528DiT%2529%2520to%2520integrate%2520transformations%2520across%2520several%2520modalities%252C%2520such%250Aas%2520images%252C%2520camera%2520parameters%252C%2520and%2520depth%2520maps.%2520The%2520key%2520to%2520Matrix3D%2527s%2520large-scale%250Amulti-modal%2520training%2520lies%2520in%2520the%2520incorporation%2520of%2520a%2520mask%2520learning%2520strategy.%250AThis%2520enables%2520full-modality%2520model%2520training%2520even%2520with%2520partially%2520complete%2520data%252C%250Asuch%2520as%2520bi-modality%2520data%2520of%2520image-pose%2520and%2520image-depth%2520pairs%252C%2520thus%250Asignificantly%2520increases%2520the%2520pool%2520of%2520available%2520training%2520data.%2520Matrix3D%250Ademonstrates%2520state-of-the-art%2520performance%2520in%2520pose%2520estimation%2520and%2520novel%2520view%250Asynthesis%2520tasks.%2520Additionally%252C%2520it%2520offers%2520fine-grained%2520control%2520through%250Amulti-round%2520interactions%252C%2520making%2520it%2520an%2520innovative%2520tool%2520for%25203D%2520content%2520creation.%250AProject%2520page%253A%2520https%253A//nju-3dv.github.io/projects/matrix3d.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix3D%3A%20Large%20Photogrammetry%20Model%20All-in-One&entry.906535625=Yuanxun%20Lu%20and%20Jingyang%20Zhang%20and%20Tian%20Fang%20and%20Jean-Daniel%20Nahmias%20and%20Yanghai%20Tsin%20and%20Long%20Quan%20and%20Xun%20Cao%20and%20Yao%20Yao%20and%20Shiwei%20Li&entry.1292438233=%20%20We%20present%20Matrix3D%2C%20a%20unified%20model%20that%20performs%20several%20photogrammetry%0Asubtasks%2C%20including%20pose%20estimation%2C%20depth%20prediction%2C%20and%20novel%20view%20synthesis%0Ausing%20just%20the%20same%20model.%20Matrix3D%20utilizes%20a%20multi-modal%20diffusion%0Atransformer%20%28DiT%29%20to%20integrate%20transformations%20across%20several%20modalities%2C%20such%0Aas%20images%2C%20camera%20parameters%2C%20and%20depth%20maps.%20The%20key%20to%20Matrix3D%27s%20large-scale%0Amulti-modal%20training%20lies%20in%20the%20incorporation%20of%20a%20mask%20learning%20strategy.%0AThis%20enables%20full-modality%20model%20training%20even%20with%20partially%20complete%20data%2C%0Asuch%20as%20bi-modality%20data%20of%20image-pose%20and%20image-depth%20pairs%2C%20thus%0Asignificantly%20increases%20the%20pool%20of%20available%20training%20data.%20Matrix3D%0Ademonstrates%20state-of-the-art%20performance%20in%20pose%20estimation%20and%20novel%20view%0Asynthesis%20tasks.%20Additionally%2C%20it%20offers%20fine-grained%20control%20through%0Amulti-round%20interactions%2C%20making%20it%20an%20innovative%20tool%20for%203D%20content%20creation.%0AProject%20page%3A%20https%3A//nju-3dv.github.io/projects/matrix3d.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07685v1&entry.124074799=Read"},
{"title": "EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting", "author": "Kunal Chelani and Assia Benbihi and Torsten Sattler and Fredrik Kahl", "abstract": "  With their meaningful geometry and their omnipresence in the 3D world, edges\nare extremely useful primitives in computer vision. 3D edges comprise of lines\nand curves, and methods to reconstruct them use either multi-view images or\npoint clouds as input. State-of-the-art image-based methods first learn a 3D\nedge point cloud then fit 3D edges to it. The edge point cloud is obtained by\nlearning a 3D neural implicit edge field from which the 3D edge points are\nsampled on a specific level set (0 or 1). However, such methods present two\nimportant drawbacks: i) it is not realistic to sample points on exact level\nsets due to float imprecision and training inaccuracies. Instead, they are\nsampled within a range of levels so the points do not lie accurately on the 3D\nedges and require further processing. ii) Such implicit representations are\ncomputationally expensive and require long training times. In this paper, we\naddress these two limitations and propose a 3D edge mapping that is simpler,\nmore efficient, and preserves accuracy. Our method learns explicitly the 3D\nedge points and their edge direction hence bypassing the need for point\nsampling. It casts a 3D edge point as the center of a 3D Gaussian and the edge\ndirection as the principal axis of the Gaussian. Such a representation has the\nadvantage of being not only geometrically meaningful but also compatible with\nthe efficient training optimization defined in Gaussian Splatting. Results show\nthat the proposed method produces edges as accurate and complete as the\nstate-of-the-art while being an order of magnitude faster. Code is released at\nhttps://github.com/kunalchelani/EdgeGaussians.\n", "link": "http://arxiv.org/abs/2409.12886v2", "date": "2025-02-11", "relevancy": 3.2678, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6658}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6505}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeGaussians%20--%203D%20Edge%20Mapping%20via%20Gaussian%20Splatting&body=Title%3A%20EdgeGaussians%20--%203D%20Edge%20Mapping%20via%20Gaussian%20Splatting%0AAuthor%3A%20Kunal%20Chelani%20and%20Assia%20Benbihi%20and%20Torsten%20Sattler%20and%20Fredrik%20Kahl%0AAbstract%3A%20%20%20With%20their%20meaningful%20geometry%20and%20their%20omnipresence%20in%20the%203D%20world%2C%20edges%0Aare%20extremely%20useful%20primitives%20in%20computer%20vision.%203D%20edges%20comprise%20of%20lines%0Aand%20curves%2C%20and%20methods%20to%20reconstruct%20them%20use%20either%20multi-view%20images%20or%0Apoint%20clouds%20as%20input.%20State-of-the-art%20image-based%20methods%20first%20learn%20a%203D%0Aedge%20point%20cloud%20then%20fit%203D%20edges%20to%20it.%20The%20edge%20point%20cloud%20is%20obtained%20by%0Alearning%20a%203D%20neural%20implicit%20edge%20field%20from%20which%20the%203D%20edge%20points%20are%0Asampled%20on%20a%20specific%20level%20set%20%280%20or%201%29.%20However%2C%20such%20methods%20present%20two%0Aimportant%20drawbacks%3A%20i%29%20it%20is%20not%20realistic%20to%20sample%20points%20on%20exact%20level%0Asets%20due%20to%20float%20imprecision%20and%20training%20inaccuracies.%20Instead%2C%20they%20are%0Asampled%20within%20a%20range%20of%20levels%20so%20the%20points%20do%20not%20lie%20accurately%20on%20the%203D%0Aedges%20and%20require%20further%20processing.%20ii%29%20Such%20implicit%20representations%20are%0Acomputationally%20expensive%20and%20require%20long%20training%20times.%20In%20this%20paper%2C%20we%0Aaddress%20these%20two%20limitations%20and%20propose%20a%203D%20edge%20mapping%20that%20is%20simpler%2C%0Amore%20efficient%2C%20and%20preserves%20accuracy.%20Our%20method%20learns%20explicitly%20the%203D%0Aedge%20points%20and%20their%20edge%20direction%20hence%20bypassing%20the%20need%20for%20point%0Asampling.%20It%20casts%20a%203D%20edge%20point%20as%20the%20center%20of%20a%203D%20Gaussian%20and%20the%20edge%0Adirection%20as%20the%20principal%20axis%20of%20the%20Gaussian.%20Such%20a%20representation%20has%20the%0Aadvantage%20of%20being%20not%20only%20geometrically%20meaningful%20but%20also%20compatible%20with%0Athe%20efficient%20training%20optimization%20defined%20in%20Gaussian%20Splatting.%20Results%20show%0Athat%20the%20proposed%20method%20produces%20edges%20as%20accurate%20and%20complete%20as%20the%0Astate-of-the-art%20while%20being%20an%20order%20of%20magnitude%20faster.%20Code%20is%20released%20at%0Ahttps%3A//github.com/kunalchelani/EdgeGaussians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeGaussians%2520--%25203D%2520Edge%2520Mapping%2520via%2520Gaussian%2520Splatting%26entry.906535625%3DKunal%2520Chelani%2520and%2520Assia%2520Benbihi%2520and%2520Torsten%2520Sattler%2520and%2520Fredrik%2520Kahl%26entry.1292438233%3D%2520%2520With%2520their%2520meaningful%2520geometry%2520and%2520their%2520omnipresence%2520in%2520the%25203D%2520world%252C%2520edges%250Aare%2520extremely%2520useful%2520primitives%2520in%2520computer%2520vision.%25203D%2520edges%2520comprise%2520of%2520lines%250Aand%2520curves%252C%2520and%2520methods%2520to%2520reconstruct%2520them%2520use%2520either%2520multi-view%2520images%2520or%250Apoint%2520clouds%2520as%2520input.%2520State-of-the-art%2520image-based%2520methods%2520first%2520learn%2520a%25203D%250Aedge%2520point%2520cloud%2520then%2520fit%25203D%2520edges%2520to%2520it.%2520The%2520edge%2520point%2520cloud%2520is%2520obtained%2520by%250Alearning%2520a%25203D%2520neural%2520implicit%2520edge%2520field%2520from%2520which%2520the%25203D%2520edge%2520points%2520are%250Asampled%2520on%2520a%2520specific%2520level%2520set%2520%25280%2520or%25201%2529.%2520However%252C%2520such%2520methods%2520present%2520two%250Aimportant%2520drawbacks%253A%2520i%2529%2520it%2520is%2520not%2520realistic%2520to%2520sample%2520points%2520on%2520exact%2520level%250Asets%2520due%2520to%2520float%2520imprecision%2520and%2520training%2520inaccuracies.%2520Instead%252C%2520they%2520are%250Asampled%2520within%2520a%2520range%2520of%2520levels%2520so%2520the%2520points%2520do%2520not%2520lie%2520accurately%2520on%2520the%25203D%250Aedges%2520and%2520require%2520further%2520processing.%2520ii%2529%2520Such%2520implicit%2520representations%2520are%250Acomputationally%2520expensive%2520and%2520require%2520long%2520training%2520times.%2520In%2520this%2520paper%252C%2520we%250Aaddress%2520these%2520two%2520limitations%2520and%2520propose%2520a%25203D%2520edge%2520mapping%2520that%2520is%2520simpler%252C%250Amore%2520efficient%252C%2520and%2520preserves%2520accuracy.%2520Our%2520method%2520learns%2520explicitly%2520the%25203D%250Aedge%2520points%2520and%2520their%2520edge%2520direction%2520hence%2520bypassing%2520the%2520need%2520for%2520point%250Asampling.%2520It%2520casts%2520a%25203D%2520edge%2520point%2520as%2520the%2520center%2520of%2520a%25203D%2520Gaussian%2520and%2520the%2520edge%250Adirection%2520as%2520the%2520principal%2520axis%2520of%2520the%2520Gaussian.%2520Such%2520a%2520representation%2520has%2520the%250Aadvantage%2520of%2520being%2520not%2520only%2520geometrically%2520meaningful%2520but%2520also%2520compatible%2520with%250Athe%2520efficient%2520training%2520optimization%2520defined%2520in%2520Gaussian%2520Splatting.%2520Results%2520show%250Athat%2520the%2520proposed%2520method%2520produces%2520edges%2520as%2520accurate%2520and%2520complete%2520as%2520the%250Astate-of-the-art%2520while%2520being%2520an%2520order%2520of%2520magnitude%2520faster.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/kunalchelani/EdgeGaussians.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeGaussians%20--%203D%20Edge%20Mapping%20via%20Gaussian%20Splatting&entry.906535625=Kunal%20Chelani%20and%20Assia%20Benbihi%20and%20Torsten%20Sattler%20and%20Fredrik%20Kahl&entry.1292438233=%20%20With%20their%20meaningful%20geometry%20and%20their%20omnipresence%20in%20the%203D%20world%2C%20edges%0Aare%20extremely%20useful%20primitives%20in%20computer%20vision.%203D%20edges%20comprise%20of%20lines%0Aand%20curves%2C%20and%20methods%20to%20reconstruct%20them%20use%20either%20multi-view%20images%20or%0Apoint%20clouds%20as%20input.%20State-of-the-art%20image-based%20methods%20first%20learn%20a%203D%0Aedge%20point%20cloud%20then%20fit%203D%20edges%20to%20it.%20The%20edge%20point%20cloud%20is%20obtained%20by%0Alearning%20a%203D%20neural%20implicit%20edge%20field%20from%20which%20the%203D%20edge%20points%20are%0Asampled%20on%20a%20specific%20level%20set%20%280%20or%201%29.%20However%2C%20such%20methods%20present%20two%0Aimportant%20drawbacks%3A%20i%29%20it%20is%20not%20realistic%20to%20sample%20points%20on%20exact%20level%0Asets%20due%20to%20float%20imprecision%20and%20training%20inaccuracies.%20Instead%2C%20they%20are%0Asampled%20within%20a%20range%20of%20levels%20so%20the%20points%20do%20not%20lie%20accurately%20on%20the%203D%0Aedges%20and%20require%20further%20processing.%20ii%29%20Such%20implicit%20representations%20are%0Acomputationally%20expensive%20and%20require%20long%20training%20times.%20In%20this%20paper%2C%20we%0Aaddress%20these%20two%20limitations%20and%20propose%20a%203D%20edge%20mapping%20that%20is%20simpler%2C%0Amore%20efficient%2C%20and%20preserves%20accuracy.%20Our%20method%20learns%20explicitly%20the%203D%0Aedge%20points%20and%20their%20edge%20direction%20hence%20bypassing%20the%20need%20for%20point%0Asampling.%20It%20casts%20a%203D%20edge%20point%20as%20the%20center%20of%20a%203D%20Gaussian%20and%20the%20edge%0Adirection%20as%20the%20principal%20axis%20of%20the%20Gaussian.%20Such%20a%20representation%20has%20the%0Aadvantage%20of%20being%20not%20only%20geometrically%20meaningful%20but%20also%20compatible%20with%0Athe%20efficient%20training%20optimization%20defined%20in%20Gaussian%20Splatting.%20Results%20show%0Athat%20the%20proposed%20method%20produces%20edges%20as%20accurate%20and%20complete%20as%20the%0Astate-of-the-art%20while%20being%20an%20order%20of%20magnitude%20faster.%20Code%20is%20released%20at%0Ahttps%3A//github.com/kunalchelani/EdgeGaussians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12886v2&entry.124074799=Read"},
{"title": "HAC++: Towards 100X Compression of 3D Gaussian Splatting", "author": "Yihang Chen and Qianyi Wu and Weiyao Lin and Mehrtash Harandi and Jianfei Cai", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To achieve a compact size, we propose HAC++, which leverages the\nrelationships between unorganized anchors and a structured hash grid, utilizing\ntheir mutual information for context modeling. Additionally, HAC++ captures\nintra-anchor contextual relationships to further enhance compression\nperformance. To facilitate entropy coding, we utilize Gaussian distributions to\nprecisely estimate the probability of each quantized attribute, where an\nadaptive quantization module is proposed to enable high-precision quantization\nof these attributes for improved fidelity restoration. Moreover, we incorporate\nan adaptive masking strategy to eliminate invalid Gaussians and anchors.\nOverall, HAC++ achieves a remarkable size reduction of over 100X compared to\nvanilla 3DGS when averaged on all datasets, while simultaneously improving\nfidelity. It also delivers more than 20X size reduction compared to\nScaffold-GS. Our code is available at\nhttps://github.com/YihangChen-ee/HAC-plus.\n", "link": "http://arxiv.org/abs/2501.12255v4", "date": "2025-02-11", "relevancy": 3.2301, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6576}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting&body=Title%3A%20HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20achieve%20a%20compact%20size%2C%20we%20propose%20HAC%2B%2B%2C%20which%20leverages%20the%0Arelationships%20between%20unorganized%20anchors%20and%20a%20structured%20hash%20grid%2C%20utilizing%0Atheir%20mutual%20information%20for%20context%20modeling.%20Additionally%2C%20HAC%2B%2B%20captures%0Aintra-anchor%20contextual%20relationships%20to%20further%20enhance%20compression%0Aperformance.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%20distributions%20to%0Aprecisely%20estimate%20the%20probability%20of%20each%20quantized%20attribute%2C%20where%20an%0Aadaptive%20quantization%20module%20is%20proposed%20to%20enable%20high-precision%20quantization%0Aof%20these%20attributes%20for%20improved%20fidelity%20restoration.%20Moreover%2C%20we%20incorporate%0Aan%20adaptive%20masking%20strategy%20to%20eliminate%20invalid%20Gaussians%20and%20anchors.%0AOverall%2C%20HAC%2B%2B%20achieves%20a%20remarkable%20size%20reduction%20of%20over%20100X%20compared%20to%0Avanilla%203DGS%20when%20averaged%20on%20all%20datasets%2C%20while%20simultaneously%20improving%0Afidelity.%20It%20also%20delivers%20more%20than%2020X%20size%20reduction%20compared%20to%0AScaffold-GS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/YihangChen-ee/HAC-plus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12255v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAC%252B%252B%253A%2520Towards%2520100X%2520Compression%2520of%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYihang%2520Chen%2520and%2520Qianyi%2520Wu%2520and%2520Weiyao%2520Lin%2520and%2520Mehrtash%2520Harandi%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520framework%2520for%2520novel%250Aview%2520synthesis%252C%2520boasting%2520rapid%2520rendering%2520speed%2520with%2520high%2520fidelity.%2520However%252C%2520the%250Asubstantial%2520Gaussians%2520and%2520their%2520associated%2520attributes%2520necessitate%2520effective%250Acompression%2520techniques.%2520Nevertheless%252C%2520the%2520sparse%2520and%2520unorganized%2520nature%2520of%2520the%250Apoint%2520cloud%2520of%2520Gaussians%2520%2528or%2520anchors%2520in%2520our%2520paper%2529%2520presents%2520challenges%2520for%250Acompression.%2520To%2520achieve%2520a%2520compact%2520size%252C%2520we%2520propose%2520HAC%252B%252B%252C%2520which%2520leverages%2520the%250Arelationships%2520between%2520unorganized%2520anchors%2520and%2520a%2520structured%2520hash%2520grid%252C%2520utilizing%250Atheir%2520mutual%2520information%2520for%2520context%2520modeling.%2520Additionally%252C%2520HAC%252B%252B%2520captures%250Aintra-anchor%2520contextual%2520relationships%2520to%2520further%2520enhance%2520compression%250Aperformance.%2520To%2520facilitate%2520entropy%2520coding%252C%2520we%2520utilize%2520Gaussian%2520distributions%2520to%250Aprecisely%2520estimate%2520the%2520probability%2520of%2520each%2520quantized%2520attribute%252C%2520where%2520an%250Aadaptive%2520quantization%2520module%2520is%2520proposed%2520to%2520enable%2520high-precision%2520quantization%250Aof%2520these%2520attributes%2520for%2520improved%2520fidelity%2520restoration.%2520Moreover%252C%2520we%2520incorporate%250Aan%2520adaptive%2520masking%2520strategy%2520to%2520eliminate%2520invalid%2520Gaussians%2520and%2520anchors.%250AOverall%252C%2520HAC%252B%252B%2520achieves%2520a%2520remarkable%2520size%2520reduction%2520of%2520over%2520100X%2520compared%2520to%250Avanilla%25203DGS%2520when%2520averaged%2520on%2520all%2520datasets%252C%2520while%2520simultaneously%2520improving%250Afidelity.%2520It%2520also%2520delivers%2520more%2520than%252020X%2520size%2520reduction%2520compared%2520to%250AScaffold-GS.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/YihangChen-ee/HAC-plus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12255v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAC%2B%2B%3A%20Towards%20100X%20Compression%20of%203D%20Gaussian%20Splatting&entry.906535625=Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Weiyao%20Lin%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20achieve%20a%20compact%20size%2C%20we%20propose%20HAC%2B%2B%2C%20which%20leverages%20the%0Arelationships%20between%20unorganized%20anchors%20and%20a%20structured%20hash%20grid%2C%20utilizing%0Atheir%20mutual%20information%20for%20context%20modeling.%20Additionally%2C%20HAC%2B%2B%20captures%0Aintra-anchor%20contextual%20relationships%20to%20further%20enhance%20compression%0Aperformance.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%20distributions%20to%0Aprecisely%20estimate%20the%20probability%20of%20each%20quantized%20attribute%2C%20where%20an%0Aadaptive%20quantization%20module%20is%20proposed%20to%20enable%20high-precision%20quantization%0Aof%20these%20attributes%20for%20improved%20fidelity%20restoration.%20Moreover%2C%20we%20incorporate%0Aan%20adaptive%20masking%20strategy%20to%20eliminate%20invalid%20Gaussians%20and%20anchors.%0AOverall%2C%20HAC%2B%2B%20achieves%20a%20remarkable%20size%20reduction%20of%20over%20100X%20compared%20to%0Avanilla%203DGS%20when%20averaged%20on%20all%20datasets%2C%20while%20simultaneously%20improving%0Afidelity.%20It%20also%20delivers%20more%20than%2020X%20size%20reduction%20compared%20to%0AScaffold-GS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/YihangChen-ee/HAC-plus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12255v4&entry.124074799=Read"},
{"title": "Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained\n  Matching Priors", "author": "Lin-Zhuo Chen and Kangjie Liu and Youtian Lin and Siyu Zhu and Zhihao Li and Xun Cao and Yao Yao", "abstract": "  3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with\nfast training and rendering speed. However, its optimization process lacks\nexplicit geometric constraints, leading to suboptimal geometric reconstruction\nin regions with sparse or no observational input views. In this work, we try to\nmitigate the issue by incorporating a pre-trained matching prior to the 3DGS\noptimization process. We introduce Flow Distillation Sampling (FDS), a\ntechnique that leverages pre-trained geometric knowledge to bolster the\naccuracy of the Gaussian radiance field. Our method employs a strategic\nsampling technique to target unobserved views adjacent to the input views,\nutilizing the optical flow calculated from the matching model (Prior Flow) to\nguide the flow analytically calculated from the 3DGS geometry (Radiance Flow).\nComprehensive experiments in depth rendering, mesh reconstruction, and novel\nview synthesis showcase the significant advantages of FDS over state-of-the-art\nmethods. Additionally, our interpretive experiments and analysis aim to shed\nlight on the effects of FDS on geometric accuracy and rendering quality,\npotentially providing readers with insights into its performance. Project page:\nhttps://nju-3dv.github.io/projects/fds\n", "link": "http://arxiv.org/abs/2502.07615v1", "date": "2025-02-11", "relevancy": 3.1907, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6497}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6375}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20Distillation%20Sampling%3A%20Regularizing%203D%20Gaussians%20with%20Pre-trained%0A%20%20Matching%20Priors&body=Title%3A%20Flow%20Distillation%20Sampling%3A%20Regularizing%203D%20Gaussians%20with%20Pre-trained%0A%20%20Matching%20Priors%0AAuthor%3A%20Lin-Zhuo%20Chen%20and%20Kangjie%20Liu%20and%20Youtian%20Lin%20and%20Siyu%20Zhu%20and%20Zhihao%20Li%20and%20Xun%20Cao%20and%20Yao%20Yao%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20achieved%20excellent%20rendering%20quality%20with%0Afast%20training%20and%20rendering%20speed.%20However%2C%20its%20optimization%20process%20lacks%0Aexplicit%20geometric%20constraints%2C%20leading%20to%20suboptimal%20geometric%20reconstruction%0Ain%20regions%20with%20sparse%20or%20no%20observational%20input%20views.%20In%20this%20work%2C%20we%20try%20to%0Amitigate%20the%20issue%20by%20incorporating%20a%20pre-trained%20matching%20prior%20to%20the%203DGS%0Aoptimization%20process.%20We%20introduce%20Flow%20Distillation%20Sampling%20%28FDS%29%2C%20a%0Atechnique%20that%20leverages%20pre-trained%20geometric%20knowledge%20to%20bolster%20the%0Aaccuracy%20of%20the%20Gaussian%20radiance%20field.%20Our%20method%20employs%20a%20strategic%0Asampling%20technique%20to%20target%20unobserved%20views%20adjacent%20to%20the%20input%20views%2C%0Autilizing%20the%20optical%20flow%20calculated%20from%20the%20matching%20model%20%28Prior%20Flow%29%20to%0Aguide%20the%20flow%20analytically%20calculated%20from%20the%203DGS%20geometry%20%28Radiance%20Flow%29.%0AComprehensive%20experiments%20in%20depth%20rendering%2C%20mesh%20reconstruction%2C%20and%20novel%0Aview%20synthesis%20showcase%20the%20significant%20advantages%20of%20FDS%20over%20state-of-the-art%0Amethods.%20Additionally%2C%20our%20interpretive%20experiments%20and%20analysis%20aim%20to%20shed%0Alight%20on%20the%20effects%20of%20FDS%20on%20geometric%20accuracy%20and%20rendering%20quality%2C%0Apotentially%20providing%20readers%20with%20insights%20into%20its%20performance.%20Project%20page%3A%0Ahttps%3A//nju-3dv.github.io/projects/fds%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520Distillation%2520Sampling%253A%2520Regularizing%25203D%2520Gaussians%2520with%2520Pre-trained%250A%2520%2520Matching%2520Priors%26entry.906535625%3DLin-Zhuo%2520Chen%2520and%2520Kangjie%2520Liu%2520and%2520Youtian%2520Lin%2520and%2520Siyu%2520Zhu%2520and%2520Zhihao%2520Li%2520and%2520Xun%2520Cao%2520and%2520Yao%2520Yao%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520achieved%2520excellent%2520rendering%2520quality%2520with%250Afast%2520training%2520and%2520rendering%2520speed.%2520However%252C%2520its%2520optimization%2520process%2520lacks%250Aexplicit%2520geometric%2520constraints%252C%2520leading%2520to%2520suboptimal%2520geometric%2520reconstruction%250Ain%2520regions%2520with%2520sparse%2520or%2520no%2520observational%2520input%2520views.%2520In%2520this%2520work%252C%2520we%2520try%2520to%250Amitigate%2520the%2520issue%2520by%2520incorporating%2520a%2520pre-trained%2520matching%2520prior%2520to%2520the%25203DGS%250Aoptimization%2520process.%2520We%2520introduce%2520Flow%2520Distillation%2520Sampling%2520%2528FDS%2529%252C%2520a%250Atechnique%2520that%2520leverages%2520pre-trained%2520geometric%2520knowledge%2520to%2520bolster%2520the%250Aaccuracy%2520of%2520the%2520Gaussian%2520radiance%2520field.%2520Our%2520method%2520employs%2520a%2520strategic%250Asampling%2520technique%2520to%2520target%2520unobserved%2520views%2520adjacent%2520to%2520the%2520input%2520views%252C%250Autilizing%2520the%2520optical%2520flow%2520calculated%2520from%2520the%2520matching%2520model%2520%2528Prior%2520Flow%2529%2520to%250Aguide%2520the%2520flow%2520analytically%2520calculated%2520from%2520the%25203DGS%2520geometry%2520%2528Radiance%2520Flow%2529.%250AComprehensive%2520experiments%2520in%2520depth%2520rendering%252C%2520mesh%2520reconstruction%252C%2520and%2520novel%250Aview%2520synthesis%2520showcase%2520the%2520significant%2520advantages%2520of%2520FDS%2520over%2520state-of-the-art%250Amethods.%2520Additionally%252C%2520our%2520interpretive%2520experiments%2520and%2520analysis%2520aim%2520to%2520shed%250Alight%2520on%2520the%2520effects%2520of%2520FDS%2520on%2520geometric%2520accuracy%2520and%2520rendering%2520quality%252C%250Apotentially%2520providing%2520readers%2520with%2520insights%2520into%2520its%2520performance.%2520Project%2520page%253A%250Ahttps%253A//nju-3dv.github.io/projects/fds%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20Distillation%20Sampling%3A%20Regularizing%203D%20Gaussians%20with%20Pre-trained%0A%20%20Matching%20Priors&entry.906535625=Lin-Zhuo%20Chen%20and%20Kangjie%20Liu%20and%20Youtian%20Lin%20and%20Siyu%20Zhu%20and%20Zhihao%20Li%20and%20Xun%20Cao%20and%20Yao%20Yao&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20achieved%20excellent%20rendering%20quality%20with%0Afast%20training%20and%20rendering%20speed.%20However%2C%20its%20optimization%20process%20lacks%0Aexplicit%20geometric%20constraints%2C%20leading%20to%20suboptimal%20geometric%20reconstruction%0Ain%20regions%20with%20sparse%20or%20no%20observational%20input%20views.%20In%20this%20work%2C%20we%20try%20to%0Amitigate%20the%20issue%20by%20incorporating%20a%20pre-trained%20matching%20prior%20to%20the%203DGS%0Aoptimization%20process.%20We%20introduce%20Flow%20Distillation%20Sampling%20%28FDS%29%2C%20a%0Atechnique%20that%20leverages%20pre-trained%20geometric%20knowledge%20to%20bolster%20the%0Aaccuracy%20of%20the%20Gaussian%20radiance%20field.%20Our%20method%20employs%20a%20strategic%0Asampling%20technique%20to%20target%20unobserved%20views%20adjacent%20to%20the%20input%20views%2C%0Autilizing%20the%20optical%20flow%20calculated%20from%20the%20matching%20model%20%28Prior%20Flow%29%20to%0Aguide%20the%20flow%20analytically%20calculated%20from%20the%203DGS%20geometry%20%28Radiance%20Flow%29.%0AComprehensive%20experiments%20in%20depth%20rendering%2C%20mesh%20reconstruction%2C%20and%20novel%0Aview%20synthesis%20showcase%20the%20significant%20advantages%20of%20FDS%20over%20state-of-the-art%0Amethods.%20Additionally%2C%20our%20interpretive%20experiments%20and%20analysis%20aim%20to%20shed%0Alight%20on%20the%20effects%20of%20FDS%20on%20geometric%20accuracy%20and%20rendering%20quality%2C%0Apotentially%20providing%20readers%20with%20insights%20into%20its%20performance.%20Project%20page%3A%0Ahttps%3A//nju-3dv.github.io/projects/fds%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07615v1&entry.124074799=Read"},
{"title": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis", "author": "David Svitov and Pietro Morerio and Lourdes Agapito and Alessio Del Bue", "abstract": "  We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The\nproposed primitives close the rendering quality gap between 2D and 3D Gaussian\nSplatting (GS), preserving the accurate mesh extraction ability of 2D\nprimitives. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in the\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360.\n", "link": "http://arxiv.org/abs/2411.08508v3", "date": "2025-02-11", "relevancy": 3.0985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6447}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6257}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis&body=Title%3A%20BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis%0AAuthor%3A%20David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20We%20present%20billboard%20Splatting%20%28BBSplat%29%20-%20a%20novel%20approach%20for%203D%20scene%0Arepresentation%20based%20on%20textured%20geometric%20primitives.%20BBSplat%20represents%20the%0Ascene%20as%20a%20set%20of%20optimizable%20textured%20planar%20primitives%20with%20learnable%20RGB%0Atextures%20and%20alpha-maps%20to%20control%20their%20shape.%20BBSplat%20primitives%20can%20be%20used%0Ain%20any%20Gaussian%20Splatting%20pipeline%20as%20drop-in%20replacements%20for%20Gaussians.%20The%0Aproposed%20primitives%20close%20the%20rendering%20quality%20gap%20between%202D%20and%203D%20Gaussian%0ASplatting%20%28GS%29%2C%20preserving%20the%20accurate%20mesh%20extraction%20ability%20of%202D%0Aprimitives.%20Our%20novel%20regularization%20term%20encourages%20textures%20to%20have%20a%20sparser%0Astructure%2C%20unlocking%20an%20efficient%20compression%20that%20leads%20to%20a%20reduction%20in%20the%0Astorage%20space%20of%20the%20model.%20Our%20experiments%20show%20the%20efficiency%20of%20BBSplat%20on%0Astandard%20datasets%20of%20real%20indoor%20and%20outdoor%20scenes%20such%20as%20Tanks%26Temples%2C%20DTU%2C%0Aand%20Mip-NeRF-360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08508v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBillBoard%2520Splatting%2520%2528BBSplat%2529%253A%2520Learnable%2520Textured%2520Primitives%2520for%2520Novel%250A%2520%2520View%2520Synthesis%26entry.906535625%3DDavid%2520Svitov%2520and%2520Pietro%2520Morerio%2520and%2520Lourdes%2520Agapito%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520We%2520present%2520billboard%2520Splatting%2520%2528BBSplat%2529%2520-%2520a%2520novel%2520approach%2520for%25203D%2520scene%250Arepresentation%2520based%2520on%2520textured%2520geometric%2520primitives.%2520BBSplat%2520represents%2520the%250Ascene%2520as%2520a%2520set%2520of%2520optimizable%2520textured%2520planar%2520primitives%2520with%2520learnable%2520RGB%250Atextures%2520and%2520alpha-maps%2520to%2520control%2520their%2520shape.%2520BBSplat%2520primitives%2520can%2520be%2520used%250Ain%2520any%2520Gaussian%2520Splatting%2520pipeline%2520as%2520drop-in%2520replacements%2520for%2520Gaussians.%2520The%250Aproposed%2520primitives%2520close%2520the%2520rendering%2520quality%2520gap%2520between%25202D%2520and%25203D%2520Gaussian%250ASplatting%2520%2528GS%2529%252C%2520preserving%2520the%2520accurate%2520mesh%2520extraction%2520ability%2520of%25202D%250Aprimitives.%2520Our%2520novel%2520regularization%2520term%2520encourages%2520textures%2520to%2520have%2520a%2520sparser%250Astructure%252C%2520unlocking%2520an%2520efficient%2520compression%2520that%2520leads%2520to%2520a%2520reduction%2520in%2520the%250Astorage%2520space%2520of%2520the%2520model.%2520Our%2520experiments%2520show%2520the%2520efficiency%2520of%2520BBSplat%2520on%250Astandard%2520datasets%2520of%2520real%2520indoor%2520and%2520outdoor%2520scenes%2520such%2520as%2520Tanks%2526Temples%252C%2520DTU%252C%250Aand%2520Mip-NeRF-360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08508v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis&entry.906535625=David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20We%20present%20billboard%20Splatting%20%28BBSplat%29%20-%20a%20novel%20approach%20for%203D%20scene%0Arepresentation%20based%20on%20textured%20geometric%20primitives.%20BBSplat%20represents%20the%0Ascene%20as%20a%20set%20of%20optimizable%20textured%20planar%20primitives%20with%20learnable%20RGB%0Atextures%20and%20alpha-maps%20to%20control%20their%20shape.%20BBSplat%20primitives%20can%20be%20used%0Ain%20any%20Gaussian%20Splatting%20pipeline%20as%20drop-in%20replacements%20for%20Gaussians.%20The%0Aproposed%20primitives%20close%20the%20rendering%20quality%20gap%20between%202D%20and%203D%20Gaussian%0ASplatting%20%28GS%29%2C%20preserving%20the%20accurate%20mesh%20extraction%20ability%20of%202D%0Aprimitives.%20Our%20novel%20regularization%20term%20encourages%20textures%20to%20have%20a%20sparser%0Astructure%2C%20unlocking%20an%20efficient%20compression%20that%20leads%20to%20a%20reduction%20in%20the%0Astorage%20space%20of%20the%20model.%20Our%20experiments%20show%20the%20efficiency%20of%20BBSplat%20on%0Astandard%20datasets%20of%20real%20indoor%20and%20outdoor%20scenes%20such%20as%20Tanks%26Temples%2C%20DTU%2C%0Aand%20Mip-NeRF-360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08508v3&entry.124074799=Read"},
{"title": "Near, far: Patch-ordering enhances vision foundation models' scene\n  understanding", "author": "Valentinos Pariza and Mohammadreza Salehi and Gertjan Burghouts and Francesco Locatello and Yuki M. Asano", "abstract": "  We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised\ntraining loss that enforces patch-level nearest neighbor consistency across a\nstudent and teacher model. Compared to contrastive approaches that only yield\nbinary learning signals, i.e., 'attract' and 'repel', this approach benefits\nfrom the more fine-grained learning signal of sorting spatially dense features\nrelative to reference patches. Our method leverages differentiable sorting\napplied on top of pretrained representations, such as DINOv2-registers to\nbootstrap the learning signal and further improve upon them. This dense\npost-pretraining leads to superior performance across various models and\ndatasets, despite requiring only 19 hours on a single GPU. This method\ngenerates high-quality dense feature encoders and establishes several new\nstate-of-the-art results such as +5.5% and +6% for non-parametric in-context\nsemantic segmentation on ADE20k and Pascal VOC, +7.2% and +5.7% for linear\nsegmentation evaluations on COCO-Things and -Stuff and improvements in the 3D\nunderstanding of multi-view consistency on SPair-71k, by more than 1.5%.\n", "link": "http://arxiv.org/abs/2408.11054v2", "date": "2025-02-11", "relevancy": 3.0325, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near%2C%20far%3A%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%0A%20%20understanding&body=Title%3A%20Near%2C%20far%3A%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%0A%20%20understanding%0AAuthor%3A%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Gertjan%20Burghouts%20and%20Francesco%20Locatello%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20We%20introduce%20NeCo%3A%20Patch%20Neighbor%20Consistency%2C%20a%20novel%20self-supervised%0Atraining%20loss%20that%20enforces%20patch-level%20nearest%20neighbor%20consistency%20across%20a%0Astudent%20and%20teacher%20model.%20Compared%20to%20contrastive%20approaches%20that%20only%20yield%0Abinary%20learning%20signals%2C%20i.e.%2C%20%27attract%27%20and%20%27repel%27%2C%20this%20approach%20benefits%0Afrom%20the%20more%20fine-grained%20learning%20signal%20of%20sorting%20spatially%20dense%20features%0Arelative%20to%20reference%20patches.%20Our%20method%20leverages%20differentiable%20sorting%0Aapplied%20on%20top%20of%20pretrained%20representations%2C%20such%20as%20DINOv2-registers%20to%0Abootstrap%20the%20learning%20signal%20and%20further%20improve%20upon%20them.%20This%20dense%0Apost-pretraining%20leads%20to%20superior%20performance%20across%20various%20models%20and%0Adatasets%2C%20despite%20requiring%20only%2019%20hours%20on%20a%20single%20GPU.%20This%20method%0Agenerates%20high-quality%20dense%20feature%20encoders%20and%20establishes%20several%20new%0Astate-of-the-art%20results%20such%20as%20%2B5.5%25%20and%20%2B6%25%20for%20non-parametric%20in-context%0Asemantic%20segmentation%20on%20ADE20k%20and%20Pascal%20VOC%2C%20%2B7.2%25%20and%20%2B5.7%25%20for%20linear%0Asegmentation%20evaluations%20on%20COCO-Things%20and%20-Stuff%20and%20improvements%20in%20the%203D%0Aunderstanding%20of%20multi-view%20consistency%20on%20SPair-71k%2C%20by%20more%20than%201.5%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear%252C%2520far%253A%2520Patch-ordering%2520enhances%2520vision%2520foundation%2520models%2527%2520scene%250A%2520%2520understanding%26entry.906535625%3DValentinos%2520Pariza%2520and%2520Mohammadreza%2520Salehi%2520and%2520Gertjan%2520Burghouts%2520and%2520Francesco%2520Locatello%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520We%2520introduce%2520NeCo%253A%2520Patch%2520Neighbor%2520Consistency%252C%2520a%2520novel%2520self-supervised%250Atraining%2520loss%2520that%2520enforces%2520patch-level%2520nearest%2520neighbor%2520consistency%2520across%2520a%250Astudent%2520and%2520teacher%2520model.%2520Compared%2520to%2520contrastive%2520approaches%2520that%2520only%2520yield%250Abinary%2520learning%2520signals%252C%2520i.e.%252C%2520%2527attract%2527%2520and%2520%2527repel%2527%252C%2520this%2520approach%2520benefits%250Afrom%2520the%2520more%2520fine-grained%2520learning%2520signal%2520of%2520sorting%2520spatially%2520dense%2520features%250Arelative%2520to%2520reference%2520patches.%2520Our%2520method%2520leverages%2520differentiable%2520sorting%250Aapplied%2520on%2520top%2520of%2520pretrained%2520representations%252C%2520such%2520as%2520DINOv2-registers%2520to%250Abootstrap%2520the%2520learning%2520signal%2520and%2520further%2520improve%2520upon%2520them.%2520This%2520dense%250Apost-pretraining%2520leads%2520to%2520superior%2520performance%2520across%2520various%2520models%2520and%250Adatasets%252C%2520despite%2520requiring%2520only%252019%2520hours%2520on%2520a%2520single%2520GPU.%2520This%2520method%250Agenerates%2520high-quality%2520dense%2520feature%2520encoders%2520and%2520establishes%2520several%2520new%250Astate-of-the-art%2520results%2520such%2520as%2520%252B5.5%2525%2520and%2520%252B6%2525%2520for%2520non-parametric%2520in-context%250Asemantic%2520segmentation%2520on%2520ADE20k%2520and%2520Pascal%2520VOC%252C%2520%252B7.2%2525%2520and%2520%252B5.7%2525%2520for%2520linear%250Asegmentation%2520evaluations%2520on%2520COCO-Things%2520and%2520-Stuff%2520and%2520improvements%2520in%2520the%25203D%250Aunderstanding%2520of%2520multi-view%2520consistency%2520on%2520SPair-71k%252C%2520by%2520more%2520than%25201.5%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near%2C%20far%3A%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%0A%20%20understanding&entry.906535625=Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Gertjan%20Burghouts%20and%20Francesco%20Locatello%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20We%20introduce%20NeCo%3A%20Patch%20Neighbor%20Consistency%2C%20a%20novel%20self-supervised%0Atraining%20loss%20that%20enforces%20patch-level%20nearest%20neighbor%20consistency%20across%20a%0Astudent%20and%20teacher%20model.%20Compared%20to%20contrastive%20approaches%20that%20only%20yield%0Abinary%20learning%20signals%2C%20i.e.%2C%20%27attract%27%20and%20%27repel%27%2C%20this%20approach%20benefits%0Afrom%20the%20more%20fine-grained%20learning%20signal%20of%20sorting%20spatially%20dense%20features%0Arelative%20to%20reference%20patches.%20Our%20method%20leverages%20differentiable%20sorting%0Aapplied%20on%20top%20of%20pretrained%20representations%2C%20such%20as%20DINOv2-registers%20to%0Abootstrap%20the%20learning%20signal%20and%20further%20improve%20upon%20them.%20This%20dense%0Apost-pretraining%20leads%20to%20superior%20performance%20across%20various%20models%20and%0Adatasets%2C%20despite%20requiring%20only%2019%20hours%20on%20a%20single%20GPU.%20This%20method%0Agenerates%20high-quality%20dense%20feature%20encoders%20and%20establishes%20several%20new%0Astate-of-the-art%20results%20such%20as%20%2B5.5%25%20and%20%2B6%25%20for%20non-parametric%20in-context%0Asemantic%20segmentation%20on%20ADE20k%20and%20Pascal%20VOC%2C%20%2B7.2%25%20and%20%2B5.7%25%20for%20linear%0Asegmentation%20evaluations%20on%20COCO-Things%20and%20-Stuff%20and%20improvements%20in%20the%203D%0Aunderstanding%20of%20multi-view%20consistency%20on%20SPair-71k%2C%20by%20more%20than%201.5%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11054v2&entry.124074799=Read"},
{"title": "CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person\n  Re-Identification", "author": "Huazhong Zhao and Lei Qi and Xin Geng", "abstract": "  The Visual Language Model, known for its robust cross-modal capabilities, has\nbeen extensively applied in various computer vision tasks. In this paper, we\nexplore the use of CLIP (Contrastive Language-Image Pretraining), a\nvision-language model pretrained on large-scale image-text pairs to align\nvisual and textual features, for acquiring fine-grained and domain-invariant\nrepresentations in generalizable person re-identification. The adaptation of\nCLIP to the task presents two primary challenges: learning more fine-grained\nfeatures to enhance discriminative ability, and learning more domain-invariant\nfeatures to improve the model's generalization capabilities. To mitigate the\nfirst challenge thereby enhance the ability to learn fine-grained features, a\nthree-stage strategy is proposed to boost the accuracy of text descriptions.\nInitially, the image encoder is trained to effectively adapt to person\nre-identification tasks. In the second stage, the features extracted by the\nimage encoder are used to generate textual descriptions (i.e., prompts) for\neach image. Finally, the text encoder with the learned prompts is employed to\nguide the training of the final image encoder. To enhance the model's\ngeneralization capabilities to unseen domains, a bidirectional guiding method\nis introduced to learn domain-invariant image features. Specifically,\ndomain-invariant and domain-relevant prompts are generated, and both positive\n(pulling together image features and domain-invariant prompts) and negative\n(pushing apart image features and domain-relevant prompts) views are used to\ntrain the image encoder. Collectively, these strategies contribute to the\ndevelopment of an innovative CLIP-based framework for learning fine-grained\ngeneralized features in person re-identification.\n", "link": "http://arxiv.org/abs/2501.16065v3", "date": "2025-02-11", "relevancy": 2.9827, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6053}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CILP-FGDI%3A%20Exploiting%20Vision-Language%20Model%20for%20Generalizable%20Person%0A%20%20Re-Identification&body=Title%3A%20CILP-FGDI%3A%20Exploiting%20Vision-Language%20Model%20for%20Generalizable%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Huazhong%20Zhao%20and%20Lei%20Qi%20and%20Xin%20Geng%0AAbstract%3A%20%20%20The%20Visual%20Language%20Model%2C%20known%20for%20its%20robust%20cross-modal%20capabilities%2C%20has%0Abeen%20extensively%20applied%20in%20various%20computer%20vision%20tasks.%20In%20this%20paper%2C%20we%0Aexplore%20the%20use%20of%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%2C%20a%0Avision-language%20model%20pretrained%20on%20large-scale%20image-text%20pairs%20to%20align%0Avisual%20and%20textual%20features%2C%20for%20acquiring%20fine-grained%20and%20domain-invariant%0Arepresentations%20in%20generalizable%20person%20re-identification.%20The%20adaptation%20of%0ACLIP%20to%20the%20task%20presents%20two%20primary%20challenges%3A%20learning%20more%20fine-grained%0Afeatures%20to%20enhance%20discriminative%20ability%2C%20and%20learning%20more%20domain-invariant%0Afeatures%20to%20improve%20the%20model%27s%20generalization%20capabilities.%20To%20mitigate%20the%0Afirst%20challenge%20thereby%20enhance%20the%20ability%20to%20learn%20fine-grained%20features%2C%20a%0Athree-stage%20strategy%20is%20proposed%20to%20boost%20the%20accuracy%20of%20text%20descriptions.%0AInitially%2C%20the%20image%20encoder%20is%20trained%20to%20effectively%20adapt%20to%20person%0Are-identification%20tasks.%20In%20the%20second%20stage%2C%20the%20features%20extracted%20by%20the%0Aimage%20encoder%20are%20used%20to%20generate%20textual%20descriptions%20%28i.e.%2C%20prompts%29%20for%0Aeach%20image.%20Finally%2C%20the%20text%20encoder%20with%20the%20learned%20prompts%20is%20employed%20to%0Aguide%20the%20training%20of%20the%20final%20image%20encoder.%20To%20enhance%20the%20model%27s%0Ageneralization%20capabilities%20to%20unseen%20domains%2C%20a%20bidirectional%20guiding%20method%0Ais%20introduced%20to%20learn%20domain-invariant%20image%20features.%20Specifically%2C%0Adomain-invariant%20and%20domain-relevant%20prompts%20are%20generated%2C%20and%20both%20positive%0A%28pulling%20together%20image%20features%20and%20domain-invariant%20prompts%29%20and%20negative%0A%28pushing%20apart%20image%20features%20and%20domain-relevant%20prompts%29%20views%20are%20used%20to%0Atrain%20the%20image%20encoder.%20Collectively%2C%20these%20strategies%20contribute%20to%20the%0Adevelopment%20of%20an%20innovative%20CLIP-based%20framework%20for%20learning%20fine-grained%0Ageneralized%20features%20in%20person%20re-identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16065v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCILP-FGDI%253A%2520Exploiting%2520Vision-Language%2520Model%2520for%2520Generalizable%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DHuazhong%2520Zhao%2520and%2520Lei%2520Qi%2520and%2520Xin%2520Geng%26entry.1292438233%3D%2520%2520The%2520Visual%2520Language%2520Model%252C%2520known%2520for%2520its%2520robust%2520cross-modal%2520capabilities%252C%2520has%250Abeen%2520extensively%2520applied%2520in%2520various%2520computer%2520vision%2520tasks.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520the%2520use%2520of%2520CLIP%2520%2528Contrastive%2520Language-Image%2520Pretraining%2529%252C%2520a%250Avision-language%2520model%2520pretrained%2520on%2520large-scale%2520image-text%2520pairs%2520to%2520align%250Avisual%2520and%2520textual%2520features%252C%2520for%2520acquiring%2520fine-grained%2520and%2520domain-invariant%250Arepresentations%2520in%2520generalizable%2520person%2520re-identification.%2520The%2520adaptation%2520of%250ACLIP%2520to%2520the%2520task%2520presents%2520two%2520primary%2520challenges%253A%2520learning%2520more%2520fine-grained%250Afeatures%2520to%2520enhance%2520discriminative%2520ability%252C%2520and%2520learning%2520more%2520domain-invariant%250Afeatures%2520to%2520improve%2520the%2520model%2527s%2520generalization%2520capabilities.%2520To%2520mitigate%2520the%250Afirst%2520challenge%2520thereby%2520enhance%2520the%2520ability%2520to%2520learn%2520fine-grained%2520features%252C%2520a%250Athree-stage%2520strategy%2520is%2520proposed%2520to%2520boost%2520the%2520accuracy%2520of%2520text%2520descriptions.%250AInitially%252C%2520the%2520image%2520encoder%2520is%2520trained%2520to%2520effectively%2520adapt%2520to%2520person%250Are-identification%2520tasks.%2520In%2520the%2520second%2520stage%252C%2520the%2520features%2520extracted%2520by%2520the%250Aimage%2520encoder%2520are%2520used%2520to%2520generate%2520textual%2520descriptions%2520%2528i.e.%252C%2520prompts%2529%2520for%250Aeach%2520image.%2520Finally%252C%2520the%2520text%2520encoder%2520with%2520the%2520learned%2520prompts%2520is%2520employed%2520to%250Aguide%2520the%2520training%2520of%2520the%2520final%2520image%2520encoder.%2520To%2520enhance%2520the%2520model%2527s%250Ageneralization%2520capabilities%2520to%2520unseen%2520domains%252C%2520a%2520bidirectional%2520guiding%2520method%250Ais%2520introduced%2520to%2520learn%2520domain-invariant%2520image%2520features.%2520Specifically%252C%250Adomain-invariant%2520and%2520domain-relevant%2520prompts%2520are%2520generated%252C%2520and%2520both%2520positive%250A%2528pulling%2520together%2520image%2520features%2520and%2520domain-invariant%2520prompts%2529%2520and%2520negative%250A%2528pushing%2520apart%2520image%2520features%2520and%2520domain-relevant%2520prompts%2529%2520views%2520are%2520used%2520to%250Atrain%2520the%2520image%2520encoder.%2520Collectively%252C%2520these%2520strategies%2520contribute%2520to%2520the%250Adevelopment%2520of%2520an%2520innovative%2520CLIP-based%2520framework%2520for%2520learning%2520fine-grained%250Ageneralized%2520features%2520in%2520person%2520re-identification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16065v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CILP-FGDI%3A%20Exploiting%20Vision-Language%20Model%20for%20Generalizable%20Person%0A%20%20Re-Identification&entry.906535625=Huazhong%20Zhao%20and%20Lei%20Qi%20and%20Xin%20Geng&entry.1292438233=%20%20The%20Visual%20Language%20Model%2C%20known%20for%20its%20robust%20cross-modal%20capabilities%2C%20has%0Abeen%20extensively%20applied%20in%20various%20computer%20vision%20tasks.%20In%20this%20paper%2C%20we%0Aexplore%20the%20use%20of%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%2C%20a%0Avision-language%20model%20pretrained%20on%20large-scale%20image-text%20pairs%20to%20align%0Avisual%20and%20textual%20features%2C%20for%20acquiring%20fine-grained%20and%20domain-invariant%0Arepresentations%20in%20generalizable%20person%20re-identification.%20The%20adaptation%20of%0ACLIP%20to%20the%20task%20presents%20two%20primary%20challenges%3A%20learning%20more%20fine-grained%0Afeatures%20to%20enhance%20discriminative%20ability%2C%20and%20learning%20more%20domain-invariant%0Afeatures%20to%20improve%20the%20model%27s%20generalization%20capabilities.%20To%20mitigate%20the%0Afirst%20challenge%20thereby%20enhance%20the%20ability%20to%20learn%20fine-grained%20features%2C%20a%0Athree-stage%20strategy%20is%20proposed%20to%20boost%20the%20accuracy%20of%20text%20descriptions.%0AInitially%2C%20the%20image%20encoder%20is%20trained%20to%20effectively%20adapt%20to%20person%0Are-identification%20tasks.%20In%20the%20second%20stage%2C%20the%20features%20extracted%20by%20the%0Aimage%20encoder%20are%20used%20to%20generate%20textual%20descriptions%20%28i.e.%2C%20prompts%29%20for%0Aeach%20image.%20Finally%2C%20the%20text%20encoder%20with%20the%20learned%20prompts%20is%20employed%20to%0Aguide%20the%20training%20of%20the%20final%20image%20encoder.%20To%20enhance%20the%20model%27s%0Ageneralization%20capabilities%20to%20unseen%20domains%2C%20a%20bidirectional%20guiding%20method%0Ais%20introduced%20to%20learn%20domain-invariant%20image%20features.%20Specifically%2C%0Adomain-invariant%20and%20domain-relevant%20prompts%20are%20generated%2C%20and%20both%20positive%0A%28pulling%20together%20image%20features%20and%20domain-invariant%20prompts%29%20and%20negative%0A%28pushing%20apart%20image%20features%20and%20domain-relevant%20prompts%29%20views%20are%20used%20to%0Atrain%20the%20image%20encoder.%20Collectively%2C%20these%20strategies%20contribute%20to%20the%0Adevelopment%20of%20an%20innovative%20CLIP-based%20framework%20for%20learning%20fine-grained%0Ageneralized%20features%20in%20person%20re-identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16065v3&entry.124074799=Read"},
{"title": "Pippo: High-Resolution Multi-View Humans from a Single Image", "author": "Yash Kant and Ethan Weber and Jin Kyu Kim and Rawal Khirodkar and Su Zhaoen and Julieta Martinez and Igor Gilitschenski and Shunsuke Saito and Timur Bagautdinov", "abstract": "  We present Pippo, a generative model capable of producing 1K resolution dense\nturnaround videos of a person from a single casually clicked photo. Pippo is a\nmulti-view diffusion transformer and does not require any additional inputs -\ne.g., a fitted parametric model or camera parameters of the input image. We\npre-train Pippo on 3B human images without captions, and conduct multi-view\nmid-training and post-training on studio captured humans. During mid-training,\nto quickly absorb the studio dataset, we denoise several (up to 48) views at\nlow-resolution, and encode target cameras coarsely using a shallow MLP. During\npost-training, we denoise fewer views at high-resolution and use pixel-aligned\ncontrols (e.g., Spatial anchor and Plucker rays) to enable 3D consistent\ngenerations. At inference, we propose an attention biasing technique that\nallows Pippo to simultaneously generate greater than 5 times as many views as\nseen during training. Finally, we also introduce an improved metric to evaluate\n3D consistency of multi-view generations, and show that Pippo outperforms\nexisting works on multi-view human generation from a single image.\n", "link": "http://arxiv.org/abs/2502.07785v1", "date": "2025-02-11", "relevancy": 2.9796, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5985}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5985}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pippo%3A%20High-Resolution%20Multi-View%20Humans%20from%20a%20Single%20Image&body=Title%3A%20Pippo%3A%20High-Resolution%20Multi-View%20Humans%20from%20a%20Single%20Image%0AAuthor%3A%20Yash%20Kant%20and%20Ethan%20Weber%20and%20Jin%20Kyu%20Kim%20and%20Rawal%20Khirodkar%20and%20Su%20Zhaoen%20and%20Julieta%20Martinez%20and%20Igor%20Gilitschenski%20and%20Shunsuke%20Saito%20and%20Timur%20Bagautdinov%0AAbstract%3A%20%20%20We%20present%20Pippo%2C%20a%20generative%20model%20capable%20of%20producing%201K%20resolution%20dense%0Aturnaround%20videos%20of%20a%20person%20from%20a%20single%20casually%20clicked%20photo.%20Pippo%20is%20a%0Amulti-view%20diffusion%20transformer%20and%20does%20not%20require%20any%20additional%20inputs%20-%0Ae.g.%2C%20a%20fitted%20parametric%20model%20or%20camera%20parameters%20of%20the%20input%20image.%20We%0Apre-train%20Pippo%20on%203B%20human%20images%20without%20captions%2C%20and%20conduct%20multi-view%0Amid-training%20and%20post-training%20on%20studio%20captured%20humans.%20During%20mid-training%2C%0Ato%20quickly%20absorb%20the%20studio%20dataset%2C%20we%20denoise%20several%20%28up%20to%2048%29%20views%20at%0Alow-resolution%2C%20and%20encode%20target%20cameras%20coarsely%20using%20a%20shallow%20MLP.%20During%0Apost-training%2C%20we%20denoise%20fewer%20views%20at%20high-resolution%20and%20use%20pixel-aligned%0Acontrols%20%28e.g.%2C%20Spatial%20anchor%20and%20Plucker%20rays%29%20to%20enable%203D%20consistent%0Agenerations.%20At%20inference%2C%20we%20propose%20an%20attention%20biasing%20technique%20that%0Aallows%20Pippo%20to%20simultaneously%20generate%20greater%20than%205%20times%20as%20many%20views%20as%0Aseen%20during%20training.%20Finally%2C%20we%20also%20introduce%20an%20improved%20metric%20to%20evaluate%0A3D%20consistency%20of%20multi-view%20generations%2C%20and%20show%20that%20Pippo%20outperforms%0Aexisting%20works%20on%20multi-view%20human%20generation%20from%20a%20single%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPippo%253A%2520High-Resolution%2520Multi-View%2520Humans%2520from%2520a%2520Single%2520Image%26entry.906535625%3DYash%2520Kant%2520and%2520Ethan%2520Weber%2520and%2520Jin%2520Kyu%2520Kim%2520and%2520Rawal%2520Khirodkar%2520and%2520Su%2520Zhaoen%2520and%2520Julieta%2520Martinez%2520and%2520Igor%2520Gilitschenski%2520and%2520Shunsuke%2520Saito%2520and%2520Timur%2520Bagautdinov%26entry.1292438233%3D%2520%2520We%2520present%2520Pippo%252C%2520a%2520generative%2520model%2520capable%2520of%2520producing%25201K%2520resolution%2520dense%250Aturnaround%2520videos%2520of%2520a%2520person%2520from%2520a%2520single%2520casually%2520clicked%2520photo.%2520Pippo%2520is%2520a%250Amulti-view%2520diffusion%2520transformer%2520and%2520does%2520not%2520require%2520any%2520additional%2520inputs%2520-%250Ae.g.%252C%2520a%2520fitted%2520parametric%2520model%2520or%2520camera%2520parameters%2520of%2520the%2520input%2520image.%2520We%250Apre-train%2520Pippo%2520on%25203B%2520human%2520images%2520without%2520captions%252C%2520and%2520conduct%2520multi-view%250Amid-training%2520and%2520post-training%2520on%2520studio%2520captured%2520humans.%2520During%2520mid-training%252C%250Ato%2520quickly%2520absorb%2520the%2520studio%2520dataset%252C%2520we%2520denoise%2520several%2520%2528up%2520to%252048%2529%2520views%2520at%250Alow-resolution%252C%2520and%2520encode%2520target%2520cameras%2520coarsely%2520using%2520a%2520shallow%2520MLP.%2520During%250Apost-training%252C%2520we%2520denoise%2520fewer%2520views%2520at%2520high-resolution%2520and%2520use%2520pixel-aligned%250Acontrols%2520%2528e.g.%252C%2520Spatial%2520anchor%2520and%2520Plucker%2520rays%2529%2520to%2520enable%25203D%2520consistent%250Agenerations.%2520At%2520inference%252C%2520we%2520propose%2520an%2520attention%2520biasing%2520technique%2520that%250Aallows%2520Pippo%2520to%2520simultaneously%2520generate%2520greater%2520than%25205%2520times%2520as%2520many%2520views%2520as%250Aseen%2520during%2520training.%2520Finally%252C%2520we%2520also%2520introduce%2520an%2520improved%2520metric%2520to%2520evaluate%250A3D%2520consistency%2520of%2520multi-view%2520generations%252C%2520and%2520show%2520that%2520Pippo%2520outperforms%250Aexisting%2520works%2520on%2520multi-view%2520human%2520generation%2520from%2520a%2520single%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pippo%3A%20High-Resolution%20Multi-View%20Humans%20from%20a%20Single%20Image&entry.906535625=Yash%20Kant%20and%20Ethan%20Weber%20and%20Jin%20Kyu%20Kim%20and%20Rawal%20Khirodkar%20and%20Su%20Zhaoen%20and%20Julieta%20Martinez%20and%20Igor%20Gilitschenski%20and%20Shunsuke%20Saito%20and%20Timur%20Bagautdinov&entry.1292438233=%20%20We%20present%20Pippo%2C%20a%20generative%20model%20capable%20of%20producing%201K%20resolution%20dense%0Aturnaround%20videos%20of%20a%20person%20from%20a%20single%20casually%20clicked%20photo.%20Pippo%20is%20a%0Amulti-view%20diffusion%20transformer%20and%20does%20not%20require%20any%20additional%20inputs%20-%0Ae.g.%2C%20a%20fitted%20parametric%20model%20or%20camera%20parameters%20of%20the%20input%20image.%20We%0Apre-train%20Pippo%20on%203B%20human%20images%20without%20captions%2C%20and%20conduct%20multi-view%0Amid-training%20and%20post-training%20on%20studio%20captured%20humans.%20During%20mid-training%2C%0Ato%20quickly%20absorb%20the%20studio%20dataset%2C%20we%20denoise%20several%20%28up%20to%2048%29%20views%20at%0Alow-resolution%2C%20and%20encode%20target%20cameras%20coarsely%20using%20a%20shallow%20MLP.%20During%0Apost-training%2C%20we%20denoise%20fewer%20views%20at%20high-resolution%20and%20use%20pixel-aligned%0Acontrols%20%28e.g.%2C%20Spatial%20anchor%20and%20Plucker%20rays%29%20to%20enable%203D%20consistent%0Agenerations.%20At%20inference%2C%20we%20propose%20an%20attention%20biasing%20technique%20that%0Aallows%20Pippo%20to%20simultaneously%20generate%20greater%20than%205%20times%20as%20many%20views%20as%0Aseen%20during%20training.%20Finally%2C%20we%20also%20introduce%20an%20improved%20metric%20to%20evaluate%0A3D%20consistency%20of%20multi-view%20generations%2C%20and%20show%20that%20Pippo%20outperforms%0Aexisting%20works%20on%20multi-view%20human%20generation%20from%20a%20single%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07785v1&entry.124074799=Read"},
{"title": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large\n  Language Models", "author": "Jiacong Xu and Shao-Yuan Lo and Bardia Safaei and Vishal M. Patel and Isht Dwivedi", "abstract": "  Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the\ntraditional unsupervised AD setting that requires a large number of normal\nsamples to train a model, ZSAD is more practical for handling data-restricted\nreal-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have\nshown revolutionary reasoning capabilities in various vision tasks. However,\nthe reasoning of image abnormalities remains underexplored due to the lack of\ncorresponding datasets and benchmarks. To facilitate research in AD &\nreasoning, we establish the first visual instruction tuning dataset,\nAnomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through\ninvestigation with our benchmark, we reveal that current MLLMs like GPT-4o\ncannot accurately detect and describe fine-grained anomalous details in images.\nTo address this, we propose Anomaly-OneVision (Anomaly-OV), the first\nspecialist visual assistant for ZSAD and reasoning. Inspired by human behavior\nin visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM)\nmechanism to adaptively select and emphasize abnormal visual tokens. Extensive\nexperiments demonstrate that Anomaly-OV achieves significant improvements over\nadvanced generalist models in both detection and reasoning. Extensions to\nmedical and 3D AD are provided for future study. The link to our project page:\nhttps://xujiacong.github.io/Anomaly-OV/\n", "link": "http://arxiv.org/abs/2502.07601v1", "date": "2025-02-11", "relevancy": 2.9445, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Zero-Shot%20Anomaly%20Detection%20and%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20Towards%20Zero-Shot%20Anomaly%20Detection%20and%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Jiacong%20Xu%20and%20Shao-Yuan%20Lo%20and%20Bardia%20Safaei%20and%20Vishal%20M.%20Patel%20and%20Isht%20Dwivedi%0AAbstract%3A%20%20%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%20is%20an%20emerging%20AD%20paradigm.%20Unlike%20the%0Atraditional%20unsupervised%20AD%20setting%20that%20requires%20a%20large%20number%20of%20normal%0Asamples%20to%20train%20a%20model%2C%20ZSAD%20is%20more%20practical%20for%20handling%20data-restricted%0Areal-world%20scenarios.%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Ashown%20revolutionary%20reasoning%20capabilities%20in%20various%20vision%20tasks.%20However%2C%0Athe%20reasoning%20of%20image%20abnormalities%20remains%20underexplored%20due%20to%20the%20lack%20of%0Acorresponding%20datasets%20and%20benchmarks.%20To%20facilitate%20research%20in%20AD%20%26%0Areasoning%2C%20we%20establish%20the%20first%20visual%20instruction%20tuning%20dataset%2C%0AAnomaly-Instruct-125k%2C%20and%20the%20evaluation%20benchmark%2C%20VisA-D%26R.%20Through%0Ainvestigation%20with%20our%20benchmark%2C%20we%20reveal%20that%20current%20MLLMs%20like%20GPT-4o%0Acannot%20accurately%20detect%20and%20describe%20fine-grained%20anomalous%20details%20in%20images.%0ATo%20address%20this%2C%20we%20propose%20Anomaly-OneVision%20%28Anomaly-OV%29%2C%20the%20first%0Aspecialist%20visual%20assistant%20for%20ZSAD%20and%20reasoning.%20Inspired%20by%20human%20behavior%0Ain%20visual%20inspection%2C%20Anomaly-OV%20leverages%20a%20Look-Twice%20Feature%20Matching%20%28LTFM%29%0Amechanism%20to%20adaptively%20select%20and%20emphasize%20abnormal%20visual%20tokens.%20Extensive%0Aexperiments%20demonstrate%20that%20Anomaly-OV%20achieves%20significant%20improvements%20over%0Aadvanced%20generalist%20models%20in%20both%20detection%20and%20reasoning.%20Extensions%20to%0Amedical%20and%203D%20AD%20are%20provided%20for%20future%20study.%20The%20link%20to%20our%20project%20page%3A%0Ahttps%3A//xujiacong.github.io/Anomaly-OV/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Zero-Shot%2520Anomaly%2520Detection%2520and%2520Reasoning%2520with%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DJiacong%2520Xu%2520and%2520Shao-Yuan%2520Lo%2520and%2520Bardia%2520Safaei%2520and%2520Vishal%2520M.%2520Patel%2520and%2520Isht%2520Dwivedi%26entry.1292438233%3D%2520%2520Zero-Shot%2520Anomaly%2520Detection%2520%2528ZSAD%2529%2520is%2520an%2520emerging%2520AD%2520paradigm.%2520Unlike%2520the%250Atraditional%2520unsupervised%2520AD%2520setting%2520that%2520requires%2520a%2520large%2520number%2520of%2520normal%250Asamples%2520to%2520train%2520a%2520model%252C%2520ZSAD%2520is%2520more%2520practical%2520for%2520handling%2520data-restricted%250Areal-world%2520scenarios.%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%250Ashown%2520revolutionary%2520reasoning%2520capabilities%2520in%2520various%2520vision%2520tasks.%2520However%252C%250Athe%2520reasoning%2520of%2520image%2520abnormalities%2520remains%2520underexplored%2520due%2520to%2520the%2520lack%2520of%250Acorresponding%2520datasets%2520and%2520benchmarks.%2520To%2520facilitate%2520research%2520in%2520AD%2520%2526%250Areasoning%252C%2520we%2520establish%2520the%2520first%2520visual%2520instruction%2520tuning%2520dataset%252C%250AAnomaly-Instruct-125k%252C%2520and%2520the%2520evaluation%2520benchmark%252C%2520VisA-D%2526R.%2520Through%250Ainvestigation%2520with%2520our%2520benchmark%252C%2520we%2520reveal%2520that%2520current%2520MLLMs%2520like%2520GPT-4o%250Acannot%2520accurately%2520detect%2520and%2520describe%2520fine-grained%2520anomalous%2520details%2520in%2520images.%250ATo%2520address%2520this%252C%2520we%2520propose%2520Anomaly-OneVision%2520%2528Anomaly-OV%2529%252C%2520the%2520first%250Aspecialist%2520visual%2520assistant%2520for%2520ZSAD%2520and%2520reasoning.%2520Inspired%2520by%2520human%2520behavior%250Ain%2520visual%2520inspection%252C%2520Anomaly-OV%2520leverages%2520a%2520Look-Twice%2520Feature%2520Matching%2520%2528LTFM%2529%250Amechanism%2520to%2520adaptively%2520select%2520and%2520emphasize%2520abnormal%2520visual%2520tokens.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520Anomaly-OV%2520achieves%2520significant%2520improvements%2520over%250Aadvanced%2520generalist%2520models%2520in%2520both%2520detection%2520and%2520reasoning.%2520Extensions%2520to%250Amedical%2520and%25203D%2520AD%2520are%2520provided%2520for%2520future%2520study.%2520The%2520link%2520to%2520our%2520project%2520page%253A%250Ahttps%253A//xujiacong.github.io/Anomaly-OV/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Zero-Shot%20Anomaly%20Detection%20and%20Reasoning%20with%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Jiacong%20Xu%20and%20Shao-Yuan%20Lo%20and%20Bardia%20Safaei%20and%20Vishal%20M.%20Patel%20and%20Isht%20Dwivedi&entry.1292438233=%20%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%20is%20an%20emerging%20AD%20paradigm.%20Unlike%20the%0Atraditional%20unsupervised%20AD%20setting%20that%20requires%20a%20large%20number%20of%20normal%0Asamples%20to%20train%20a%20model%2C%20ZSAD%20is%20more%20practical%20for%20handling%20data-restricted%0Areal-world%20scenarios.%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Ashown%20revolutionary%20reasoning%20capabilities%20in%20various%20vision%20tasks.%20However%2C%0Athe%20reasoning%20of%20image%20abnormalities%20remains%20underexplored%20due%20to%20the%20lack%20of%0Acorresponding%20datasets%20and%20benchmarks.%20To%20facilitate%20research%20in%20AD%20%26%0Areasoning%2C%20we%20establish%20the%20first%20visual%20instruction%20tuning%20dataset%2C%0AAnomaly-Instruct-125k%2C%20and%20the%20evaluation%20benchmark%2C%20VisA-D%26R.%20Through%0Ainvestigation%20with%20our%20benchmark%2C%20we%20reveal%20that%20current%20MLLMs%20like%20GPT-4o%0Acannot%20accurately%20detect%20and%20describe%20fine-grained%20anomalous%20details%20in%20images.%0ATo%20address%20this%2C%20we%20propose%20Anomaly-OneVision%20%28Anomaly-OV%29%2C%20the%20first%0Aspecialist%20visual%20assistant%20for%20ZSAD%20and%20reasoning.%20Inspired%20by%20human%20behavior%0Ain%20visual%20inspection%2C%20Anomaly-OV%20leverages%20a%20Look-Twice%20Feature%20Matching%20%28LTFM%29%0Amechanism%20to%20adaptively%20select%20and%20emphasize%20abnormal%20visual%20tokens.%20Extensive%0Aexperiments%20demonstrate%20that%20Anomaly-OV%20achieves%20significant%20improvements%20over%0Aadvanced%20generalist%20models%20in%20both%20detection%20and%20reasoning.%20Extensions%20to%0Amedical%20and%203D%20AD%20are%20provided%20for%20future%20study.%20The%20link%20to%20our%20project%20page%3A%0Ahttps%3A//xujiacong.github.io/Anomaly-OV/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07601v1&entry.124074799=Read"},
{"title": "PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric\n  Visual Query Localization", "author": "Bing Fan and Yunhe Feng and Yapeng Tian and Yuewei Lin and Yan Huang and Heng Fan", "abstract": "  Egocentric visual query localization (EgoVQL) focuses on localizing the\ntarget of interest in space and time from first-person videos, given a visual\nquery. Despite recent progressive, existing methods often struggle to handle\nsevere object appearance changes and cluttering background in the video due to\nlacking sufficient target cues, leading to degradation. Addressing this, we\nintroduce PRVQL, a novel Progressive knowledge-guided Refinement framework for\nEgoVQL. The core is to continuously exploit target-relevant knowledge directly\nfrom videos and utilize it as guidance to refine both query and video features\nfor improving target localization. Our PRVQL contains multiple processing\nstages. The target knowledge from one stage, comprising appearance and spatial\nknowledge extracted via two specially designed knowledge learning modules, are\nutilized as guidance to refine the query and videos features for the next\nstage, which are used to generate more accurate knowledge for further feature\nrefinement. With such a progressive process, target knowledge in PRVQL can be\ngradually improved, which, in turn, leads to better refined query and video\nfeatures for localization in the final stage. Compared to previous methods, our\nPRVQL, besides the given object cues, enjoys additional crucial target\ninformation from a video as guidance to refine features, and hence enhances\nEgoVQL in complicated scenes. In our experiments on challenging Ego4D, PRVQL\nachieves state-of-the-art result and largely surpasses other methods, showing\nits efficacy. Our code, model and results will be released at\nhttps://github.com/fb-reps/PRVQL.\n", "link": "http://arxiv.org/abs/2502.07707v1", "date": "2025-02-11", "relevancy": 2.7575, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRVQL%3A%20Progressive%20Knowledge-guided%20Refinement%20for%20Robust%20Egocentric%0A%20%20Visual%20Query%20Localization&body=Title%3A%20PRVQL%3A%20Progressive%20Knowledge-guided%20Refinement%20for%20Robust%20Egocentric%0A%20%20Visual%20Query%20Localization%0AAuthor%3A%20Bing%20Fan%20and%20Yunhe%20Feng%20and%20Yapeng%20Tian%20and%20Yuewei%20Lin%20and%20Yan%20Huang%20and%20Heng%20Fan%0AAbstract%3A%20%20%20Egocentric%20visual%20query%20localization%20%28EgoVQL%29%20focuses%20on%20localizing%20the%0Atarget%20of%20interest%20in%20space%20and%20time%20from%20first-person%20videos%2C%20given%20a%20visual%0Aquery.%20Despite%20recent%20progressive%2C%20existing%20methods%20often%20struggle%20to%20handle%0Asevere%20object%20appearance%20changes%20and%20cluttering%20background%20in%20the%20video%20due%20to%0Alacking%20sufficient%20target%20cues%2C%20leading%20to%20degradation.%20Addressing%20this%2C%20we%0Aintroduce%20PRVQL%2C%20a%20novel%20Progressive%20knowledge-guided%20Refinement%20framework%20for%0AEgoVQL.%20The%20core%20is%20to%20continuously%20exploit%20target-relevant%20knowledge%20directly%0Afrom%20videos%20and%20utilize%20it%20as%20guidance%20to%20refine%20both%20query%20and%20video%20features%0Afor%20improving%20target%20localization.%20Our%20PRVQL%20contains%20multiple%20processing%0Astages.%20The%20target%20knowledge%20from%20one%20stage%2C%20comprising%20appearance%20and%20spatial%0Aknowledge%20extracted%20via%20two%20specially%20designed%20knowledge%20learning%20modules%2C%20are%0Autilized%20as%20guidance%20to%20refine%20the%20query%20and%20videos%20features%20for%20the%20next%0Astage%2C%20which%20are%20used%20to%20generate%20more%20accurate%20knowledge%20for%20further%20feature%0Arefinement.%20With%20such%20a%20progressive%20process%2C%20target%20knowledge%20in%20PRVQL%20can%20be%0Agradually%20improved%2C%20which%2C%20in%20turn%2C%20leads%20to%20better%20refined%20query%20and%20video%0Afeatures%20for%20localization%20in%20the%20final%20stage.%20Compared%20to%20previous%20methods%2C%20our%0APRVQL%2C%20besides%20the%20given%20object%20cues%2C%20enjoys%20additional%20crucial%20target%0Ainformation%20from%20a%20video%20as%20guidance%20to%20refine%20features%2C%20and%20hence%20enhances%0AEgoVQL%20in%20complicated%20scenes.%20In%20our%20experiments%20on%20challenging%20Ego4D%2C%20PRVQL%0Aachieves%20state-of-the-art%20result%20and%20largely%20surpasses%20other%20methods%2C%20showing%0Aits%20efficacy.%20Our%20code%2C%20model%20and%20results%20will%20be%20released%20at%0Ahttps%3A//github.com/fb-reps/PRVQL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRVQL%253A%2520Progressive%2520Knowledge-guided%2520Refinement%2520for%2520Robust%2520Egocentric%250A%2520%2520Visual%2520Query%2520Localization%26entry.906535625%3DBing%2520Fan%2520and%2520Yunhe%2520Feng%2520and%2520Yapeng%2520Tian%2520and%2520Yuewei%2520Lin%2520and%2520Yan%2520Huang%2520and%2520Heng%2520Fan%26entry.1292438233%3D%2520%2520Egocentric%2520visual%2520query%2520localization%2520%2528EgoVQL%2529%2520focuses%2520on%2520localizing%2520the%250Atarget%2520of%2520interest%2520in%2520space%2520and%2520time%2520from%2520first-person%2520videos%252C%2520given%2520a%2520visual%250Aquery.%2520Despite%2520recent%2520progressive%252C%2520existing%2520methods%2520often%2520struggle%2520to%2520handle%250Asevere%2520object%2520appearance%2520changes%2520and%2520cluttering%2520background%2520in%2520the%2520video%2520due%2520to%250Alacking%2520sufficient%2520target%2520cues%252C%2520leading%2520to%2520degradation.%2520Addressing%2520this%252C%2520we%250Aintroduce%2520PRVQL%252C%2520a%2520novel%2520Progressive%2520knowledge-guided%2520Refinement%2520framework%2520for%250AEgoVQL.%2520The%2520core%2520is%2520to%2520continuously%2520exploit%2520target-relevant%2520knowledge%2520directly%250Afrom%2520videos%2520and%2520utilize%2520it%2520as%2520guidance%2520to%2520refine%2520both%2520query%2520and%2520video%2520features%250Afor%2520improving%2520target%2520localization.%2520Our%2520PRVQL%2520contains%2520multiple%2520processing%250Astages.%2520The%2520target%2520knowledge%2520from%2520one%2520stage%252C%2520comprising%2520appearance%2520and%2520spatial%250Aknowledge%2520extracted%2520via%2520two%2520specially%2520designed%2520knowledge%2520learning%2520modules%252C%2520are%250Autilized%2520as%2520guidance%2520to%2520refine%2520the%2520query%2520and%2520videos%2520features%2520for%2520the%2520next%250Astage%252C%2520which%2520are%2520used%2520to%2520generate%2520more%2520accurate%2520knowledge%2520for%2520further%2520feature%250Arefinement.%2520With%2520such%2520a%2520progressive%2520process%252C%2520target%2520knowledge%2520in%2520PRVQL%2520can%2520be%250Agradually%2520improved%252C%2520which%252C%2520in%2520turn%252C%2520leads%2520to%2520better%2520refined%2520query%2520and%2520video%250Afeatures%2520for%2520localization%2520in%2520the%2520final%2520stage.%2520Compared%2520to%2520previous%2520methods%252C%2520our%250APRVQL%252C%2520besides%2520the%2520given%2520object%2520cues%252C%2520enjoys%2520additional%2520crucial%2520target%250Ainformation%2520from%2520a%2520video%2520as%2520guidance%2520to%2520refine%2520features%252C%2520and%2520hence%2520enhances%250AEgoVQL%2520in%2520complicated%2520scenes.%2520In%2520our%2520experiments%2520on%2520challenging%2520Ego4D%252C%2520PRVQL%250Aachieves%2520state-of-the-art%2520result%2520and%2520largely%2520surpasses%2520other%2520methods%252C%2520showing%250Aits%2520efficacy.%2520Our%2520code%252C%2520model%2520and%2520results%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/fb-reps/PRVQL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRVQL%3A%20Progressive%20Knowledge-guided%20Refinement%20for%20Robust%20Egocentric%0A%20%20Visual%20Query%20Localization&entry.906535625=Bing%20Fan%20and%20Yunhe%20Feng%20and%20Yapeng%20Tian%20and%20Yuewei%20Lin%20and%20Yan%20Huang%20and%20Heng%20Fan&entry.1292438233=%20%20Egocentric%20visual%20query%20localization%20%28EgoVQL%29%20focuses%20on%20localizing%20the%0Atarget%20of%20interest%20in%20space%20and%20time%20from%20first-person%20videos%2C%20given%20a%20visual%0Aquery.%20Despite%20recent%20progressive%2C%20existing%20methods%20often%20struggle%20to%20handle%0Asevere%20object%20appearance%20changes%20and%20cluttering%20background%20in%20the%20video%20due%20to%0Alacking%20sufficient%20target%20cues%2C%20leading%20to%20degradation.%20Addressing%20this%2C%20we%0Aintroduce%20PRVQL%2C%20a%20novel%20Progressive%20knowledge-guided%20Refinement%20framework%20for%0AEgoVQL.%20The%20core%20is%20to%20continuously%20exploit%20target-relevant%20knowledge%20directly%0Afrom%20videos%20and%20utilize%20it%20as%20guidance%20to%20refine%20both%20query%20and%20video%20features%0Afor%20improving%20target%20localization.%20Our%20PRVQL%20contains%20multiple%20processing%0Astages.%20The%20target%20knowledge%20from%20one%20stage%2C%20comprising%20appearance%20and%20spatial%0Aknowledge%20extracted%20via%20two%20specially%20designed%20knowledge%20learning%20modules%2C%20are%0Autilized%20as%20guidance%20to%20refine%20the%20query%20and%20videos%20features%20for%20the%20next%0Astage%2C%20which%20are%20used%20to%20generate%20more%20accurate%20knowledge%20for%20further%20feature%0Arefinement.%20With%20such%20a%20progressive%20process%2C%20target%20knowledge%20in%20PRVQL%20can%20be%0Agradually%20improved%2C%20which%2C%20in%20turn%2C%20leads%20to%20better%20refined%20query%20and%20video%0Afeatures%20for%20localization%20in%20the%20final%20stage.%20Compared%20to%20previous%20methods%2C%20our%0APRVQL%2C%20besides%20the%20given%20object%20cues%2C%20enjoys%20additional%20crucial%20target%0Ainformation%20from%20a%20video%20as%20guidance%20to%20refine%20features%2C%20and%20hence%20enhances%0AEgoVQL%20in%20complicated%20scenes.%20In%20our%20experiments%20on%20challenging%20Ego4D%2C%20PRVQL%0Aachieves%20state-of-the-art%20result%20and%20largely%20surpasses%20other%20methods%2C%20showing%0Aits%20efficacy.%20Our%20code%2C%20model%20and%20results%20will%20be%20released%20at%0Ahttps%3A//github.com/fb-reps/PRVQL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07707v1&entry.124074799=Read"},
{"title": "LP-DETR: Layer-wise Progressive Relations for Object Detection", "author": "Zhengjian Kang and Ye Zhang and Xiaoyu Deng and Xintao Li and Yongzhe Zhang", "abstract": "  This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach\nthat enhances DETR-based object detection through multi-scale relation\nmodeling. Our method introduces learnable spatial relationships between object\nqueries through a relation-aware self-attention mechanism, which adaptively\nlearns to balance different scales of relations (local, medium and global)\nacross decoder layers. This progressive design enables the model to effectively\ncapture evolving spatial dependencies throughout the detection pipeline.\nExtensive experiments on COCO 2017 dataset demonstrate that our method improves\nboth convergence speed and detection accuracy compared to standard\nself-attention module. The proposed method achieves competitive results,\nreaching 52.3\\% AP with 12 epochs and 52.5\\% AP with 24 epochs using ResNet-50\nbackbone, and further improving to 58.0\\% AP with Swin-L backbone. Furthermore,\nour analysis reveals an interesting pattern: the model naturally learns to\nprioritize local spatial relations in early decoder layers while gradually\nshifting attention to broader contexts in deeper layers, providing valuable\ninsights for future research in object detection.\n", "link": "http://arxiv.org/abs/2502.05147v2", "date": "2025-02-11", "relevancy": 2.7479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5615}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LP-DETR%3A%20Layer-wise%20Progressive%20Relations%20for%20Object%20Detection&body=Title%3A%20LP-DETR%3A%20Layer-wise%20Progressive%20Relations%20for%20Object%20Detection%0AAuthor%3A%20Zhengjian%20Kang%20and%20Ye%20Zhang%20and%20Xiaoyu%20Deng%20and%20Xintao%20Li%20and%20Yongzhe%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20LP-DETR%20%28Layer-wise%20Progressive%20DETR%29%2C%20a%20novel%20approach%0Athat%20enhances%20DETR-based%20object%20detection%20through%20multi-scale%20relation%0Amodeling.%20Our%20method%20introduces%20learnable%20spatial%20relationships%20between%20object%0Aqueries%20through%20a%20relation-aware%20self-attention%20mechanism%2C%20which%20adaptively%0Alearns%20to%20balance%20different%20scales%20of%20relations%20%28local%2C%20medium%20and%20global%29%0Aacross%20decoder%20layers.%20This%20progressive%20design%20enables%20the%20model%20to%20effectively%0Acapture%20evolving%20spatial%20dependencies%20throughout%20the%20detection%20pipeline.%0AExtensive%20experiments%20on%20COCO%202017%20dataset%20demonstrate%20that%20our%20method%20improves%0Aboth%20convergence%20speed%20and%20detection%20accuracy%20compared%20to%20standard%0Aself-attention%20module.%20The%20proposed%20method%20achieves%20competitive%20results%2C%0Areaching%2052.3%5C%25%20AP%20with%2012%20epochs%20and%2052.5%5C%25%20AP%20with%2024%20epochs%20using%20ResNet-50%0Abackbone%2C%20and%20further%20improving%20to%2058.0%5C%25%20AP%20with%20Swin-L%20backbone.%20Furthermore%2C%0Aour%20analysis%20reveals%20an%20interesting%20pattern%3A%20the%20model%20naturally%20learns%20to%0Aprioritize%20local%20spatial%20relations%20in%20early%20decoder%20layers%20while%20gradually%0Ashifting%20attention%20to%20broader%20contexts%20in%20deeper%20layers%2C%20providing%20valuable%0Ainsights%20for%20future%20research%20in%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLP-DETR%253A%2520Layer-wise%2520Progressive%2520Relations%2520for%2520Object%2520Detection%26entry.906535625%3DZhengjian%2520Kang%2520and%2520Ye%2520Zhang%2520and%2520Xiaoyu%2520Deng%2520and%2520Xintao%2520Li%2520and%2520Yongzhe%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520LP-DETR%2520%2528Layer-wise%2520Progressive%2520DETR%2529%252C%2520a%2520novel%2520approach%250Athat%2520enhances%2520DETR-based%2520object%2520detection%2520through%2520multi-scale%2520relation%250Amodeling.%2520Our%2520method%2520introduces%2520learnable%2520spatial%2520relationships%2520between%2520object%250Aqueries%2520through%2520a%2520relation-aware%2520self-attention%2520mechanism%252C%2520which%2520adaptively%250Alearns%2520to%2520balance%2520different%2520scales%2520of%2520relations%2520%2528local%252C%2520medium%2520and%2520global%2529%250Aacross%2520decoder%2520layers.%2520This%2520progressive%2520design%2520enables%2520the%2520model%2520to%2520effectively%250Acapture%2520evolving%2520spatial%2520dependencies%2520throughout%2520the%2520detection%2520pipeline.%250AExtensive%2520experiments%2520on%2520COCO%25202017%2520dataset%2520demonstrate%2520that%2520our%2520method%2520improves%250Aboth%2520convergence%2520speed%2520and%2520detection%2520accuracy%2520compared%2520to%2520standard%250Aself-attention%2520module.%2520The%2520proposed%2520method%2520achieves%2520competitive%2520results%252C%250Areaching%252052.3%255C%2525%2520AP%2520with%252012%2520epochs%2520and%252052.5%255C%2525%2520AP%2520with%252024%2520epochs%2520using%2520ResNet-50%250Abackbone%252C%2520and%2520further%2520improving%2520to%252058.0%255C%2525%2520AP%2520with%2520Swin-L%2520backbone.%2520Furthermore%252C%250Aour%2520analysis%2520reveals%2520an%2520interesting%2520pattern%253A%2520the%2520model%2520naturally%2520learns%2520to%250Aprioritize%2520local%2520spatial%2520relations%2520in%2520early%2520decoder%2520layers%2520while%2520gradually%250Ashifting%2520attention%2520to%2520broader%2520contexts%2520in%2520deeper%2520layers%252C%2520providing%2520valuable%250Ainsights%2520for%2520future%2520research%2520in%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LP-DETR%3A%20Layer-wise%20Progressive%20Relations%20for%20Object%20Detection&entry.906535625=Zhengjian%20Kang%20and%20Ye%20Zhang%20and%20Xiaoyu%20Deng%20and%20Xintao%20Li%20and%20Yongzhe%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20LP-DETR%20%28Layer-wise%20Progressive%20DETR%29%2C%20a%20novel%20approach%0Athat%20enhances%20DETR-based%20object%20detection%20through%20multi-scale%20relation%0Amodeling.%20Our%20method%20introduces%20learnable%20spatial%20relationships%20between%20object%0Aqueries%20through%20a%20relation-aware%20self-attention%20mechanism%2C%20which%20adaptively%0Alearns%20to%20balance%20different%20scales%20of%20relations%20%28local%2C%20medium%20and%20global%29%0Aacross%20decoder%20layers.%20This%20progressive%20design%20enables%20the%20model%20to%20effectively%0Acapture%20evolving%20spatial%20dependencies%20throughout%20the%20detection%20pipeline.%0AExtensive%20experiments%20on%20COCO%202017%20dataset%20demonstrate%20that%20our%20method%20improves%0Aboth%20convergence%20speed%20and%20detection%20accuracy%20compared%20to%20standard%0Aself-attention%20module.%20The%20proposed%20method%20achieves%20competitive%20results%2C%0Areaching%2052.3%5C%25%20AP%20with%2012%20epochs%20and%2052.5%5C%25%20AP%20with%2024%20epochs%20using%20ResNet-50%0Abackbone%2C%20and%20further%20improving%20to%2058.0%5C%25%20AP%20with%20Swin-L%20backbone.%20Furthermore%2C%0Aour%20analysis%20reveals%20an%20interesting%20pattern%3A%20the%20model%20naturally%20learns%20to%0Aprioritize%20local%20spatial%20relations%20in%20early%20decoder%20layers%20while%20gradually%0Ashifting%20attention%20to%20broader%20contexts%20in%20deeper%20layers%2C%20providing%20valuable%0Ainsights%20for%20future%20research%20in%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05147v2&entry.124074799=Read"},
{"title": "Unified Graph Networks (UGN): A Deep Neural Framework for Solving Graph\n  Problems", "author": "Rudrajit Dawn and Madhusudan Ghosh and Partha Basuchowdhuri and Sudip Kumar Naskar", "abstract": "  Deep neural networks have enabled researchers to create powerful generalized\nframeworks, such as transformers, that can be used to solve well-studied\nproblems in various application domains, such as text and image. However, such\ngeneralized frameworks are not available for solving graph problems. Graph\nstructures are ubiquitous in many applications around us and many graph\nproblems have been widely studied over years. In recent times, there has been a\nsurge in deep neural network based approaches to solve graph problems, with\ngrowing availability of graph structured datasets across diverse domains.\nNevertheless, existing methods are mostly tailored to solve a specific task and\nlack the capability to create a generalized model leading to solutions for\ndifferent downstream tasks. In this work, we propose a novel,\nresource-efficient framework named \\emph{U}nified \\emph{G}raph \\emph{N}etwork\n(UGN) by leveraging the feature extraction capability of graph convolutional\nneural networks (GCN) and 2-dimensional convolutional neural networks (Conv2D).\nUGN unifies various graph learning tasks, such as link prediction, node\nclassification, community detection, graph-to-graph translation, knowledge\ngraph completion, and more, within a cohesive framework, while exercising\nminimal task-specific extensions (e.g., formation of supernodes for coarsening\nmassive networks to increase scalability, use of \\textit{mean target\nconnectivity matrix} (MTCM) representation for achieving scalability in graph\ntranslation task, etc.) to enhance the generalization capability of graph\nlearning and analysis. We test the novel UGN framework for six uncorrelated\ngraph problems, using twelve different datasets. Experimental results show that\nUGN outperforms the state-of-the-art baselines by a significant margin on ten\ndatasets, while producing comparable results on the remaining dataset.\n", "link": "http://arxiv.org/abs/2502.07500v1", "date": "2025-02-11", "relevancy": 2.69, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5904}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.515}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Graph%20Networks%20%28UGN%29%3A%20A%20Deep%20Neural%20Framework%20for%20Solving%20Graph%0A%20%20Problems&body=Title%3A%20Unified%20Graph%20Networks%20%28UGN%29%3A%20A%20Deep%20Neural%20Framework%20for%20Solving%20Graph%0A%20%20Problems%0AAuthor%3A%20Rudrajit%20Dawn%20and%20Madhusudan%20Ghosh%20and%20Partha%20Basuchowdhuri%20and%20Sudip%20Kumar%20Naskar%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20enabled%20researchers%20to%20create%20powerful%20generalized%0Aframeworks%2C%20such%20as%20transformers%2C%20that%20can%20be%20used%20to%20solve%20well-studied%0Aproblems%20in%20various%20application%20domains%2C%20such%20as%20text%20and%20image.%20However%2C%20such%0Ageneralized%20frameworks%20are%20not%20available%20for%20solving%20graph%20problems.%20Graph%0Astructures%20are%20ubiquitous%20in%20many%20applications%20around%20us%20and%20many%20graph%0Aproblems%20have%20been%20widely%20studied%20over%20years.%20In%20recent%20times%2C%20there%20has%20been%20a%0Asurge%20in%20deep%20neural%20network%20based%20approaches%20to%20solve%20graph%20problems%2C%20with%0Agrowing%20availability%20of%20graph%20structured%20datasets%20across%20diverse%20domains.%0ANevertheless%2C%20existing%20methods%20are%20mostly%20tailored%20to%20solve%20a%20specific%20task%20and%0Alack%20the%20capability%20to%20create%20a%20generalized%20model%20leading%20to%20solutions%20for%0Adifferent%20downstream%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20novel%2C%0Aresource-efficient%20framework%20named%20%5Cemph%7BU%7Dnified%20%5Cemph%7BG%7Draph%20%5Cemph%7BN%7Detwork%0A%28UGN%29%20by%20leveraging%20the%20feature%20extraction%20capability%20of%20graph%20convolutional%0Aneural%20networks%20%28GCN%29%20and%202-dimensional%20convolutional%20neural%20networks%20%28Conv2D%29.%0AUGN%20unifies%20various%20graph%20learning%20tasks%2C%20such%20as%20link%20prediction%2C%20node%0Aclassification%2C%20community%20detection%2C%20graph-to-graph%20translation%2C%20knowledge%0Agraph%20completion%2C%20and%20more%2C%20within%20a%20cohesive%20framework%2C%20while%20exercising%0Aminimal%20task-specific%20extensions%20%28e.g.%2C%20formation%20of%20supernodes%20for%20coarsening%0Amassive%20networks%20to%20increase%20scalability%2C%20use%20of%20%5Ctextit%7Bmean%20target%0Aconnectivity%20matrix%7D%20%28MTCM%29%20representation%20for%20achieving%20scalability%20in%20graph%0Atranslation%20task%2C%20etc.%29%20to%20enhance%20the%20generalization%20capability%20of%20graph%0Alearning%20and%20analysis.%20We%20test%20the%20novel%20UGN%20framework%20for%20six%20uncorrelated%0Agraph%20problems%2C%20using%20twelve%20different%20datasets.%20Experimental%20results%20show%20that%0AUGN%20outperforms%20the%20state-of-the-art%20baselines%20by%20a%20significant%20margin%20on%20ten%0Adatasets%2C%20while%20producing%20comparable%20results%20on%20the%20remaining%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Graph%2520Networks%2520%2528UGN%2529%253A%2520A%2520Deep%2520Neural%2520Framework%2520for%2520Solving%2520Graph%250A%2520%2520Problems%26entry.906535625%3DRudrajit%2520Dawn%2520and%2520Madhusudan%2520Ghosh%2520and%2520Partha%2520Basuchowdhuri%2520and%2520Sudip%2520Kumar%2520Naskar%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520enabled%2520researchers%2520to%2520create%2520powerful%2520generalized%250Aframeworks%252C%2520such%2520as%2520transformers%252C%2520that%2520can%2520be%2520used%2520to%2520solve%2520well-studied%250Aproblems%2520in%2520various%2520application%2520domains%252C%2520such%2520as%2520text%2520and%2520image.%2520However%252C%2520such%250Ageneralized%2520frameworks%2520are%2520not%2520available%2520for%2520solving%2520graph%2520problems.%2520Graph%250Astructures%2520are%2520ubiquitous%2520in%2520many%2520applications%2520around%2520us%2520and%2520many%2520graph%250Aproblems%2520have%2520been%2520widely%2520studied%2520over%2520years.%2520In%2520recent%2520times%252C%2520there%2520has%2520been%2520a%250Asurge%2520in%2520deep%2520neural%2520network%2520based%2520approaches%2520to%2520solve%2520graph%2520problems%252C%2520with%250Agrowing%2520availability%2520of%2520graph%2520structured%2520datasets%2520across%2520diverse%2520domains.%250ANevertheless%252C%2520existing%2520methods%2520are%2520mostly%2520tailored%2520to%2520solve%2520a%2520specific%2520task%2520and%250Alack%2520the%2520capability%2520to%2520create%2520a%2520generalized%2520model%2520leading%2520to%2520solutions%2520for%250Adifferent%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%252C%250Aresource-efficient%2520framework%2520named%2520%255Cemph%257BU%257Dnified%2520%255Cemph%257BG%257Draph%2520%255Cemph%257BN%257Detwork%250A%2528UGN%2529%2520by%2520leveraging%2520the%2520feature%2520extraction%2520capability%2520of%2520graph%2520convolutional%250Aneural%2520networks%2520%2528GCN%2529%2520and%25202-dimensional%2520convolutional%2520neural%2520networks%2520%2528Conv2D%2529.%250AUGN%2520unifies%2520various%2520graph%2520learning%2520tasks%252C%2520such%2520as%2520link%2520prediction%252C%2520node%250Aclassification%252C%2520community%2520detection%252C%2520graph-to-graph%2520translation%252C%2520knowledge%250Agraph%2520completion%252C%2520and%2520more%252C%2520within%2520a%2520cohesive%2520framework%252C%2520while%2520exercising%250Aminimal%2520task-specific%2520extensions%2520%2528e.g.%252C%2520formation%2520of%2520supernodes%2520for%2520coarsening%250Amassive%2520networks%2520to%2520increase%2520scalability%252C%2520use%2520of%2520%255Ctextit%257Bmean%2520target%250Aconnectivity%2520matrix%257D%2520%2528MTCM%2529%2520representation%2520for%2520achieving%2520scalability%2520in%2520graph%250Atranslation%2520task%252C%2520etc.%2529%2520to%2520enhance%2520the%2520generalization%2520capability%2520of%2520graph%250Alearning%2520and%2520analysis.%2520We%2520test%2520the%2520novel%2520UGN%2520framework%2520for%2520six%2520uncorrelated%250Agraph%2520problems%252C%2520using%2520twelve%2520different%2520datasets.%2520Experimental%2520results%2520show%2520that%250AUGN%2520outperforms%2520the%2520state-of-the-art%2520baselines%2520by%2520a%2520significant%2520margin%2520on%2520ten%250Adatasets%252C%2520while%2520producing%2520comparable%2520results%2520on%2520the%2520remaining%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Graph%20Networks%20%28UGN%29%3A%20A%20Deep%20Neural%20Framework%20for%20Solving%20Graph%0A%20%20Problems&entry.906535625=Rudrajit%20Dawn%20and%20Madhusudan%20Ghosh%20and%20Partha%20Basuchowdhuri%20and%20Sudip%20Kumar%20Naskar&entry.1292438233=%20%20Deep%20neural%20networks%20have%20enabled%20researchers%20to%20create%20powerful%20generalized%0Aframeworks%2C%20such%20as%20transformers%2C%20that%20can%20be%20used%20to%20solve%20well-studied%0Aproblems%20in%20various%20application%20domains%2C%20such%20as%20text%20and%20image.%20However%2C%20such%0Ageneralized%20frameworks%20are%20not%20available%20for%20solving%20graph%20problems.%20Graph%0Astructures%20are%20ubiquitous%20in%20many%20applications%20around%20us%20and%20many%20graph%0Aproblems%20have%20been%20widely%20studied%20over%20years.%20In%20recent%20times%2C%20there%20has%20been%20a%0Asurge%20in%20deep%20neural%20network%20based%20approaches%20to%20solve%20graph%20problems%2C%20with%0Agrowing%20availability%20of%20graph%20structured%20datasets%20across%20diverse%20domains.%0ANevertheless%2C%20existing%20methods%20are%20mostly%20tailored%20to%20solve%20a%20specific%20task%20and%0Alack%20the%20capability%20to%20create%20a%20generalized%20model%20leading%20to%20solutions%20for%0Adifferent%20downstream%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20novel%2C%0Aresource-efficient%20framework%20named%20%5Cemph%7BU%7Dnified%20%5Cemph%7BG%7Draph%20%5Cemph%7BN%7Detwork%0A%28UGN%29%20by%20leveraging%20the%20feature%20extraction%20capability%20of%20graph%20convolutional%0Aneural%20networks%20%28GCN%29%20and%202-dimensional%20convolutional%20neural%20networks%20%28Conv2D%29.%0AUGN%20unifies%20various%20graph%20learning%20tasks%2C%20such%20as%20link%20prediction%2C%20node%0Aclassification%2C%20community%20detection%2C%20graph-to-graph%20translation%2C%20knowledge%0Agraph%20completion%2C%20and%20more%2C%20within%20a%20cohesive%20framework%2C%20while%20exercising%0Aminimal%20task-specific%20extensions%20%28e.g.%2C%20formation%20of%20supernodes%20for%20coarsening%0Amassive%20networks%20to%20increase%20scalability%2C%20use%20of%20%5Ctextit%7Bmean%20target%0Aconnectivity%20matrix%7D%20%28MTCM%29%20representation%20for%20achieving%20scalability%20in%20graph%0Atranslation%20task%2C%20etc.%29%20to%20enhance%20the%20generalization%20capability%20of%20graph%0Alearning%20and%20analysis.%20We%20test%20the%20novel%20UGN%20framework%20for%20six%20uncorrelated%0Agraph%20problems%2C%20using%20twelve%20different%20datasets.%20Experimental%20results%20show%20that%0AUGN%20outperforms%20the%20state-of-the-art%20baselines%20by%20a%20significant%20margin%20on%20ten%0Adatasets%2C%20while%20producing%20comparable%20results%20on%20the%20remaining%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07500v1&entry.124074799=Read"},
{"title": "RomanLens: Latent Romanization and its role in Multilinguality in LLMs", "author": "Alan Saji and Jaavid Aktar Husain and Thanmay Jayakumar and Raj Dabre and Anoop Kunchukuttan and Mitesh M. Khapra and Ratish Puduppully", "abstract": "  Large Language Models (LLMs) exhibit remarkable multilingual generalization\ndespite being predominantly trained on English-centric corpora. A fundamental\nquestion arises: how do LLMs achieve such robust multilingual capabilities? For\nnon-Latin script languages, we investigate the role of romanization - the\nrepresentation of non-Latin scripts using Latin characters - as a bridge in\nmultilingual processing. Using mechanistic interpretability techniques, we\nanalyze next-token generation and find that intermediate layers frequently\nrepresent target words in romanized form before transitioning to native script,\na phenomenon we term Latent Romanization. Further, through activation patching\nexperiments, we demonstrate that LLMs encode semantic concepts similarly across\nnative and romanized scripts, suggesting a shared underlying representation.\nAdditionally in translation towards non Latin languages, our findings reveal\nthat when the target language is in romanized form, its representations emerge\nearlier in the model's layers compared to native script. These insights\ncontribute to a deeper understanding of multilingual representation in LLMs and\nhighlight the implicit role of romanization in facilitating language transfer.\nOur work provides new directions for potentially improving multilingual\nlanguage modeling and interpretability.\n", "link": "http://arxiv.org/abs/2502.07424v1", "date": "2025-02-11", "relevancy": 2.6695, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RomanLens%3A%20Latent%20Romanization%20and%20its%20role%20in%20Multilinguality%20in%20LLMs&body=Title%3A%20RomanLens%3A%20Latent%20Romanization%20and%20its%20role%20in%20Multilinguality%20in%20LLMs%0AAuthor%3A%20Alan%20Saji%20and%20Jaavid%20Aktar%20Husain%20and%20Thanmay%20Jayakumar%20and%20Raj%20Dabre%20and%20Anoop%20Kunchukuttan%20and%20Mitesh%20M.%20Khapra%20and%20Ratish%20Puduppully%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20multilingual%20generalization%0Adespite%20being%20predominantly%20trained%20on%20English-centric%20corpora.%20A%20fundamental%0Aquestion%20arises%3A%20how%20do%20LLMs%20achieve%20such%20robust%20multilingual%20capabilities%3F%20For%0Anon-Latin%20script%20languages%2C%20we%20investigate%20the%20role%20of%20romanization%20-%20the%0Arepresentation%20of%20non-Latin%20scripts%20using%20Latin%20characters%20-%20as%20a%20bridge%20in%0Amultilingual%20processing.%20Using%20mechanistic%20interpretability%20techniques%2C%20we%0Aanalyze%20next-token%20generation%20and%20find%20that%20intermediate%20layers%20frequently%0Arepresent%20target%20words%20in%20romanized%20form%20before%20transitioning%20to%20native%20script%2C%0Aa%20phenomenon%20we%20term%20Latent%20Romanization.%20Further%2C%20through%20activation%20patching%0Aexperiments%2C%20we%20demonstrate%20that%20LLMs%20encode%20semantic%20concepts%20similarly%20across%0Anative%20and%20romanized%20scripts%2C%20suggesting%20a%20shared%20underlying%20representation.%0AAdditionally%20in%20translation%20towards%20non%20Latin%20languages%2C%20our%20findings%20reveal%0Athat%20when%20the%20target%20language%20is%20in%20romanized%20form%2C%20its%20representations%20emerge%0Aearlier%20in%20the%20model%27s%20layers%20compared%20to%20native%20script.%20These%20insights%0Acontribute%20to%20a%20deeper%20understanding%20of%20multilingual%20representation%20in%20LLMs%20and%0Ahighlight%20the%20implicit%20role%20of%20romanization%20in%20facilitating%20language%20transfer.%0AOur%20work%20provides%20new%20directions%20for%20potentially%20improving%20multilingual%0Alanguage%20modeling%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRomanLens%253A%2520Latent%2520Romanization%2520and%2520its%2520role%2520in%2520Multilinguality%2520in%2520LLMs%26entry.906535625%3DAlan%2520Saji%2520and%2520Jaavid%2520Aktar%2520Husain%2520and%2520Thanmay%2520Jayakumar%2520and%2520Raj%2520Dabre%2520and%2520Anoop%2520Kunchukuttan%2520and%2520Mitesh%2520M.%2520Khapra%2520and%2520Ratish%2520Puduppully%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520multilingual%2520generalization%250Adespite%2520being%2520predominantly%2520trained%2520on%2520English-centric%2520corpora.%2520A%2520fundamental%250Aquestion%2520arises%253A%2520how%2520do%2520LLMs%2520achieve%2520such%2520robust%2520multilingual%2520capabilities%253F%2520For%250Anon-Latin%2520script%2520languages%252C%2520we%2520investigate%2520the%2520role%2520of%2520romanization%2520-%2520the%250Arepresentation%2520of%2520non-Latin%2520scripts%2520using%2520Latin%2520characters%2520-%2520as%2520a%2520bridge%2520in%250Amultilingual%2520processing.%2520Using%2520mechanistic%2520interpretability%2520techniques%252C%2520we%250Aanalyze%2520next-token%2520generation%2520and%2520find%2520that%2520intermediate%2520layers%2520frequently%250Arepresent%2520target%2520words%2520in%2520romanized%2520form%2520before%2520transitioning%2520to%2520native%2520script%252C%250Aa%2520phenomenon%2520we%2520term%2520Latent%2520Romanization.%2520Further%252C%2520through%2520activation%2520patching%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520LLMs%2520encode%2520semantic%2520concepts%2520similarly%2520across%250Anative%2520and%2520romanized%2520scripts%252C%2520suggesting%2520a%2520shared%2520underlying%2520representation.%250AAdditionally%2520in%2520translation%2520towards%2520non%2520Latin%2520languages%252C%2520our%2520findings%2520reveal%250Athat%2520when%2520the%2520target%2520language%2520is%2520in%2520romanized%2520form%252C%2520its%2520representations%2520emerge%250Aearlier%2520in%2520the%2520model%2527s%2520layers%2520compared%2520to%2520native%2520script.%2520These%2520insights%250Acontribute%2520to%2520a%2520deeper%2520understanding%2520of%2520multilingual%2520representation%2520in%2520LLMs%2520and%250Ahighlight%2520the%2520implicit%2520role%2520of%2520romanization%2520in%2520facilitating%2520language%2520transfer.%250AOur%2520work%2520provides%2520new%2520directions%2520for%2520potentially%2520improving%2520multilingual%250Alanguage%2520modeling%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RomanLens%3A%20Latent%20Romanization%20and%20its%20role%20in%20Multilinguality%20in%20LLMs&entry.906535625=Alan%20Saji%20and%20Jaavid%20Aktar%20Husain%20and%20Thanmay%20Jayakumar%20and%20Raj%20Dabre%20and%20Anoop%20Kunchukuttan%20and%20Mitesh%20M.%20Khapra%20and%20Ratish%20Puduppully&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20multilingual%20generalization%0Adespite%20being%20predominantly%20trained%20on%20English-centric%20corpora.%20A%20fundamental%0Aquestion%20arises%3A%20how%20do%20LLMs%20achieve%20such%20robust%20multilingual%20capabilities%3F%20For%0Anon-Latin%20script%20languages%2C%20we%20investigate%20the%20role%20of%20romanization%20-%20the%0Arepresentation%20of%20non-Latin%20scripts%20using%20Latin%20characters%20-%20as%20a%20bridge%20in%0Amultilingual%20processing.%20Using%20mechanistic%20interpretability%20techniques%2C%20we%0Aanalyze%20next-token%20generation%20and%20find%20that%20intermediate%20layers%20frequently%0Arepresent%20target%20words%20in%20romanized%20form%20before%20transitioning%20to%20native%20script%2C%0Aa%20phenomenon%20we%20term%20Latent%20Romanization.%20Further%2C%20through%20activation%20patching%0Aexperiments%2C%20we%20demonstrate%20that%20LLMs%20encode%20semantic%20concepts%20similarly%20across%0Anative%20and%20romanized%20scripts%2C%20suggesting%20a%20shared%20underlying%20representation.%0AAdditionally%20in%20translation%20towards%20non%20Latin%20languages%2C%20our%20findings%20reveal%0Athat%20when%20the%20target%20language%20is%20in%20romanized%20form%2C%20its%20representations%20emerge%0Aearlier%20in%20the%20model%27s%20layers%20compared%20to%20native%20script.%20These%20insights%0Acontribute%20to%20a%20deeper%20understanding%20of%20multilingual%20representation%20in%20LLMs%20and%0Ahighlight%20the%20implicit%20role%20of%20romanization%20in%20facilitating%20language%20transfer.%0AOur%20work%20provides%20new%20directions%20for%20potentially%20improving%20multilingual%0Alanguage%20modeling%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07424v1&entry.124074799=Read"},
{"title": "DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force\n  Feedback Glove", "author": "Han Zhang and Songbo Hu and Zhecheng Yuan and Huazhe Xu", "abstract": "  Dexterous hand teleoperation plays a pivotal role in enabling robots to\nachieve human-level manipulation dexterity. However, current teleoperation\nsystems often rely on expensive equipment and lack multi-modal sensory\nfeedback, restricting human operators' ability to perceive object properties\nand perform complex manipulation tasks. To address these limitations, we\npresent DOGlove, a low-cost, precise, and haptic force feedback glove system\nfor teleoperation and manipulation. DoGlove can be assembled in hours at a cost\nunder 600 USD. It features a customized joint structure for 21-DoF motion\ncapture, a compact cable-driven torque transmission mechanism for 5-DoF\nmultidirectional force feedback, and a linear resonate actuator for 5-DoF\nfingertip haptic feedback. Leveraging action and haptic force retargeting,\nDOGlove enables precise and immersive teleoperation of dexterous robotic hands,\nachieving high success rates in complex, contact-rich tasks. We further\nevaluate DOGlove in scenarios without visual feedback, demonstrating the\ncritical role of haptic force feedback in task performance. In addition, we\nutilize the collected demonstrations to train imitation learning policies,\nhighlighting the potential and effectiveness of DOGlove. DOGlove's hardware and\nsoftware system will be fully open-sourced at https://do-glove.github.io/.\n", "link": "http://arxiv.org/abs/2502.07730v1", "date": "2025-02-11", "relevancy": 2.6638, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5678}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5194}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOGlove%3A%20Dexterous%20Manipulation%20with%20a%20Low-Cost%20Open-Source%20Haptic%20Force%0A%20%20Feedback%20Glove&body=Title%3A%20DOGlove%3A%20Dexterous%20Manipulation%20with%20a%20Low-Cost%20Open-Source%20Haptic%20Force%0A%20%20Feedback%20Glove%0AAuthor%3A%20Han%20Zhang%20and%20Songbo%20Hu%20and%20Zhecheng%20Yuan%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Dexterous%20hand%20teleoperation%20plays%20a%20pivotal%20role%20in%20enabling%20robots%20to%0Aachieve%20human-level%20manipulation%20dexterity.%20However%2C%20current%20teleoperation%0Asystems%20often%20rely%20on%20expensive%20equipment%20and%20lack%20multi-modal%20sensory%0Afeedback%2C%20restricting%20human%20operators%27%20ability%20to%20perceive%20object%20properties%0Aand%20perform%20complex%20manipulation%20tasks.%20To%20address%20these%20limitations%2C%20we%0Apresent%20DOGlove%2C%20a%20low-cost%2C%20precise%2C%20and%20haptic%20force%20feedback%20glove%20system%0Afor%20teleoperation%20and%20manipulation.%20DoGlove%20can%20be%20assembled%20in%20hours%20at%20a%20cost%0Aunder%20600%20USD.%20It%20features%20a%20customized%20joint%20structure%20for%2021-DoF%20motion%0Acapture%2C%20a%20compact%20cable-driven%20torque%20transmission%20mechanism%20for%205-DoF%0Amultidirectional%20force%20feedback%2C%20and%20a%20linear%20resonate%20actuator%20for%205-DoF%0Afingertip%20haptic%20feedback.%20Leveraging%20action%20and%20haptic%20force%20retargeting%2C%0ADOGlove%20enables%20precise%20and%20immersive%20teleoperation%20of%20dexterous%20robotic%20hands%2C%0Aachieving%20high%20success%20rates%20in%20complex%2C%20contact-rich%20tasks.%20We%20further%0Aevaluate%20DOGlove%20in%20scenarios%20without%20visual%20feedback%2C%20demonstrating%20the%0Acritical%20role%20of%20haptic%20force%20feedback%20in%20task%20performance.%20In%20addition%2C%20we%0Autilize%20the%20collected%20demonstrations%20to%20train%20imitation%20learning%20policies%2C%0Ahighlighting%20the%20potential%20and%20effectiveness%20of%20DOGlove.%20DOGlove%27s%20hardware%20and%0Asoftware%20system%20will%20be%20fully%20open-sourced%20at%20https%3A//do-glove.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOGlove%253A%2520Dexterous%2520Manipulation%2520with%2520a%2520Low-Cost%2520Open-Source%2520Haptic%2520Force%250A%2520%2520Feedback%2520Glove%26entry.906535625%3DHan%2520Zhang%2520and%2520Songbo%2520Hu%2520and%2520Zhecheng%2520Yuan%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Dexterous%2520hand%2520teleoperation%2520plays%2520a%2520pivotal%2520role%2520in%2520enabling%2520robots%2520to%250Aachieve%2520human-level%2520manipulation%2520dexterity.%2520However%252C%2520current%2520teleoperation%250Asystems%2520often%2520rely%2520on%2520expensive%2520equipment%2520and%2520lack%2520multi-modal%2520sensory%250Afeedback%252C%2520restricting%2520human%2520operators%2527%2520ability%2520to%2520perceive%2520object%2520properties%250Aand%2520perform%2520complex%2520manipulation%2520tasks.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apresent%2520DOGlove%252C%2520a%2520low-cost%252C%2520precise%252C%2520and%2520haptic%2520force%2520feedback%2520glove%2520system%250Afor%2520teleoperation%2520and%2520manipulation.%2520DoGlove%2520can%2520be%2520assembled%2520in%2520hours%2520at%2520a%2520cost%250Aunder%2520600%2520USD.%2520It%2520features%2520a%2520customized%2520joint%2520structure%2520for%252021-DoF%2520motion%250Acapture%252C%2520a%2520compact%2520cable-driven%2520torque%2520transmission%2520mechanism%2520for%25205-DoF%250Amultidirectional%2520force%2520feedback%252C%2520and%2520a%2520linear%2520resonate%2520actuator%2520for%25205-DoF%250Afingertip%2520haptic%2520feedback.%2520Leveraging%2520action%2520and%2520haptic%2520force%2520retargeting%252C%250ADOGlove%2520enables%2520precise%2520and%2520immersive%2520teleoperation%2520of%2520dexterous%2520robotic%2520hands%252C%250Aachieving%2520high%2520success%2520rates%2520in%2520complex%252C%2520contact-rich%2520tasks.%2520We%2520further%250Aevaluate%2520DOGlove%2520in%2520scenarios%2520without%2520visual%2520feedback%252C%2520demonstrating%2520the%250Acritical%2520role%2520of%2520haptic%2520force%2520feedback%2520in%2520task%2520performance.%2520In%2520addition%252C%2520we%250Autilize%2520the%2520collected%2520demonstrations%2520to%2520train%2520imitation%2520learning%2520policies%252C%250Ahighlighting%2520the%2520potential%2520and%2520effectiveness%2520of%2520DOGlove.%2520DOGlove%2527s%2520hardware%2520and%250Asoftware%2520system%2520will%2520be%2520fully%2520open-sourced%2520at%2520https%253A//do-glove.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOGlove%3A%20Dexterous%20Manipulation%20with%20a%20Low-Cost%20Open-Source%20Haptic%20Force%0A%20%20Feedback%20Glove&entry.906535625=Han%20Zhang%20and%20Songbo%20Hu%20and%20Zhecheng%20Yuan%20and%20Huazhe%20Xu&entry.1292438233=%20%20Dexterous%20hand%20teleoperation%20plays%20a%20pivotal%20role%20in%20enabling%20robots%20to%0Aachieve%20human-level%20manipulation%20dexterity.%20However%2C%20current%20teleoperation%0Asystems%20often%20rely%20on%20expensive%20equipment%20and%20lack%20multi-modal%20sensory%0Afeedback%2C%20restricting%20human%20operators%27%20ability%20to%20perceive%20object%20properties%0Aand%20perform%20complex%20manipulation%20tasks.%20To%20address%20these%20limitations%2C%20we%0Apresent%20DOGlove%2C%20a%20low-cost%2C%20precise%2C%20and%20haptic%20force%20feedback%20glove%20system%0Afor%20teleoperation%20and%20manipulation.%20DoGlove%20can%20be%20assembled%20in%20hours%20at%20a%20cost%0Aunder%20600%20USD.%20It%20features%20a%20customized%20joint%20structure%20for%2021-DoF%20motion%0Acapture%2C%20a%20compact%20cable-driven%20torque%20transmission%20mechanism%20for%205-DoF%0Amultidirectional%20force%20feedback%2C%20and%20a%20linear%20resonate%20actuator%20for%205-DoF%0Afingertip%20haptic%20feedback.%20Leveraging%20action%20and%20haptic%20force%20retargeting%2C%0ADOGlove%20enables%20precise%20and%20immersive%20teleoperation%20of%20dexterous%20robotic%20hands%2C%0Aachieving%20high%20success%20rates%20in%20complex%2C%20contact-rich%20tasks.%20We%20further%0Aevaluate%20DOGlove%20in%20scenarios%20without%20visual%20feedback%2C%20demonstrating%20the%0Acritical%20role%20of%20haptic%20force%20feedback%20in%20task%20performance.%20In%20addition%2C%20we%0Autilize%20the%20collected%20demonstrations%20to%20train%20imitation%20learning%20policies%2C%0Ahighlighting%20the%20potential%20and%20effectiveness%20of%20DOGlove.%20DOGlove%27s%20hardware%20and%0Asoftware%20system%20will%20be%20fully%20open-sourced%20at%20https%3A//do-glove.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07730v1&entry.124074799=Read"},
{"title": "Obfuscation Based Privacy Preserving Representations are Recoverable\n  Using Neighborhood Information", "author": "Kunal Chelani and Assia Benbihi and Fredrik Kahl and Torsten Sattler and Zuzana Kukelova", "abstract": "  Rapid growth in the popularity of AR/VR/MR applications and cloud-based\nvisual localization systems has given rise to an increased focus on the privacy\nof user content in the localization process. This privacy concern has been\nfurther escalated by the ability of deep neural networks to recover detailed\nimages of a scene from a sparse set of 3D or 2D points and their descriptors -\nthe so-called inversion attacks. Research on privacy-preserving localization\nhas therefore focused on preventing these inversion attacks on both the query\nimage keypoints and the 3D points of the scene map. To this end, several\ngeometry obfuscation techniques that lift points to higher-dimensional spaces,\ni.e., lines or planes, or that swap coordinates between points % have been\nproposed. In this paper, we point to a common weakness of these obfuscations\nthat allows to recover approximations of the original point positions under the\nassumption of known neighborhoods. We further show that these neighborhoods can\nbe computed by learning to identify descriptors that co-occur in neighborhoods.\nExtensive experiments show that our approach for point recovery is practically\napplicable to all existing geometric obfuscation schemes. Our results show that\nthese schemes should not be considered privacy-preserving, even though they are\nclaimed to be privacy-preserving. Code will be available at\nhttps://github.com/kunalchelani/RecoverPointsNeighborhood.\n", "link": "http://arxiv.org/abs/2409.11536v2", "date": "2025-02-11", "relevancy": 2.656, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5635}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5208}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Obfuscation%20Based%20Privacy%20Preserving%20Representations%20are%20Recoverable%0A%20%20Using%20Neighborhood%20Information&body=Title%3A%20Obfuscation%20Based%20Privacy%20Preserving%20Representations%20are%20Recoverable%0A%20%20Using%20Neighborhood%20Information%0AAuthor%3A%20Kunal%20Chelani%20and%20Assia%20Benbihi%20and%20Fredrik%20Kahl%20and%20Torsten%20Sattler%20and%20Zuzana%20Kukelova%0AAbstract%3A%20%20%20Rapid%20growth%20in%20the%20popularity%20of%20AR/VR/MR%20applications%20and%20cloud-based%0Avisual%20localization%20systems%20has%20given%20rise%20to%20an%20increased%20focus%20on%20the%20privacy%0Aof%20user%20content%20in%20the%20localization%20process.%20This%20privacy%20concern%20has%20been%0Afurther%20escalated%20by%20the%20ability%20of%20deep%20neural%20networks%20to%20recover%20detailed%0Aimages%20of%20a%20scene%20from%20a%20sparse%20set%20of%203D%20or%202D%20points%20and%20their%20descriptors%20-%0Athe%20so-called%20inversion%20attacks.%20Research%20on%20privacy-preserving%20localization%0Ahas%20therefore%20focused%20on%20preventing%20these%20inversion%20attacks%20on%20both%20the%20query%0Aimage%20keypoints%20and%20the%203D%20points%20of%20the%20scene%20map.%20To%20this%20end%2C%20several%0Ageometry%20obfuscation%20techniques%20that%20lift%20points%20to%20higher-dimensional%20spaces%2C%0Ai.e.%2C%20lines%20or%20planes%2C%20or%20that%20swap%20coordinates%20between%20points%20%25%20have%20been%0Aproposed.%20In%20this%20paper%2C%20we%20point%20to%20a%20common%20weakness%20of%20these%20obfuscations%0Athat%20allows%20to%20recover%20approximations%20of%20the%20original%20point%20positions%20under%20the%0Aassumption%20of%20known%20neighborhoods.%20We%20further%20show%20that%20these%20neighborhoods%20can%0Abe%20computed%20by%20learning%20to%20identify%20descriptors%20that%20co-occur%20in%20neighborhoods.%0AExtensive%20experiments%20show%20that%20our%20approach%20for%20point%20recovery%20is%20practically%0Aapplicable%20to%20all%20existing%20geometric%20obfuscation%20schemes.%20Our%20results%20show%20that%0Athese%20schemes%20should%20not%20be%20considered%20privacy-preserving%2C%20even%20though%20they%20are%0Aclaimed%20to%20be%20privacy-preserving.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/kunalchelani/RecoverPointsNeighborhood.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObfuscation%2520Based%2520Privacy%2520Preserving%2520Representations%2520are%2520Recoverable%250A%2520%2520Using%2520Neighborhood%2520Information%26entry.906535625%3DKunal%2520Chelani%2520and%2520Assia%2520Benbihi%2520and%2520Fredrik%2520Kahl%2520and%2520Torsten%2520Sattler%2520and%2520Zuzana%2520Kukelova%26entry.1292438233%3D%2520%2520Rapid%2520growth%2520in%2520the%2520popularity%2520of%2520AR/VR/MR%2520applications%2520and%2520cloud-based%250Avisual%2520localization%2520systems%2520has%2520given%2520rise%2520to%2520an%2520increased%2520focus%2520on%2520the%2520privacy%250Aof%2520user%2520content%2520in%2520the%2520localization%2520process.%2520This%2520privacy%2520concern%2520has%2520been%250Afurther%2520escalated%2520by%2520the%2520ability%2520of%2520deep%2520neural%2520networks%2520to%2520recover%2520detailed%250Aimages%2520of%2520a%2520scene%2520from%2520a%2520sparse%2520set%2520of%25203D%2520or%25202D%2520points%2520and%2520their%2520descriptors%2520-%250Athe%2520so-called%2520inversion%2520attacks.%2520Research%2520on%2520privacy-preserving%2520localization%250Ahas%2520therefore%2520focused%2520on%2520preventing%2520these%2520inversion%2520attacks%2520on%2520both%2520the%2520query%250Aimage%2520keypoints%2520and%2520the%25203D%2520points%2520of%2520the%2520scene%2520map.%2520To%2520this%2520end%252C%2520several%250Ageometry%2520obfuscation%2520techniques%2520that%2520lift%2520points%2520to%2520higher-dimensional%2520spaces%252C%250Ai.e.%252C%2520lines%2520or%2520planes%252C%2520or%2520that%2520swap%2520coordinates%2520between%2520points%2520%2525%2520have%2520been%250Aproposed.%2520In%2520this%2520paper%252C%2520we%2520point%2520to%2520a%2520common%2520weakness%2520of%2520these%2520obfuscations%250Athat%2520allows%2520to%2520recover%2520approximations%2520of%2520the%2520original%2520point%2520positions%2520under%2520the%250Aassumption%2520of%2520known%2520neighborhoods.%2520We%2520further%2520show%2520that%2520these%2520neighborhoods%2520can%250Abe%2520computed%2520by%2520learning%2520to%2520identify%2520descriptors%2520that%2520co-occur%2520in%2520neighborhoods.%250AExtensive%2520experiments%2520show%2520that%2520our%2520approach%2520for%2520point%2520recovery%2520is%2520practically%250Aapplicable%2520to%2520all%2520existing%2520geometric%2520obfuscation%2520schemes.%2520Our%2520results%2520show%2520that%250Athese%2520schemes%2520should%2520not%2520be%2520considered%2520privacy-preserving%252C%2520even%2520though%2520they%2520are%250Aclaimed%2520to%2520be%2520privacy-preserving.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/kunalchelani/RecoverPointsNeighborhood.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Obfuscation%20Based%20Privacy%20Preserving%20Representations%20are%20Recoverable%0A%20%20Using%20Neighborhood%20Information&entry.906535625=Kunal%20Chelani%20and%20Assia%20Benbihi%20and%20Fredrik%20Kahl%20and%20Torsten%20Sattler%20and%20Zuzana%20Kukelova&entry.1292438233=%20%20Rapid%20growth%20in%20the%20popularity%20of%20AR/VR/MR%20applications%20and%20cloud-based%0Avisual%20localization%20systems%20has%20given%20rise%20to%20an%20increased%20focus%20on%20the%20privacy%0Aof%20user%20content%20in%20the%20localization%20process.%20This%20privacy%20concern%20has%20been%0Afurther%20escalated%20by%20the%20ability%20of%20deep%20neural%20networks%20to%20recover%20detailed%0Aimages%20of%20a%20scene%20from%20a%20sparse%20set%20of%203D%20or%202D%20points%20and%20their%20descriptors%20-%0Athe%20so-called%20inversion%20attacks.%20Research%20on%20privacy-preserving%20localization%0Ahas%20therefore%20focused%20on%20preventing%20these%20inversion%20attacks%20on%20both%20the%20query%0Aimage%20keypoints%20and%20the%203D%20points%20of%20the%20scene%20map.%20To%20this%20end%2C%20several%0Ageometry%20obfuscation%20techniques%20that%20lift%20points%20to%20higher-dimensional%20spaces%2C%0Ai.e.%2C%20lines%20or%20planes%2C%20or%20that%20swap%20coordinates%20between%20points%20%25%20have%20been%0Aproposed.%20In%20this%20paper%2C%20we%20point%20to%20a%20common%20weakness%20of%20these%20obfuscations%0Athat%20allows%20to%20recover%20approximations%20of%20the%20original%20point%20positions%20under%20the%0Aassumption%20of%20known%20neighborhoods.%20We%20further%20show%20that%20these%20neighborhoods%20can%0Abe%20computed%20by%20learning%20to%20identify%20descriptors%20that%20co-occur%20in%20neighborhoods.%0AExtensive%20experiments%20show%20that%20our%20approach%20for%20point%20recovery%20is%20practically%0Aapplicable%20to%20all%20existing%20geometric%20obfuscation%20schemes.%20Our%20results%20show%20that%0Athese%20schemes%20should%20not%20be%20considered%20privacy-preserving%2C%20even%20though%20they%20are%0Aclaimed%20to%20be%20privacy-preserving.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/kunalchelani/RecoverPointsNeighborhood.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11536v2&entry.124074799=Read"},
{"title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery", "author": "Yingce Xia and Peiran Jin and Shufang Xie and Liang He and Chuan Cao and Renqian Luo and Guoqing Liu and Yue Wang and Zequn Liu and Yuan-Jyue Chen and Zekun Guo and Yeqi Bai and Pan Deng and Yaosen Min and Ziheng Lu and Hongxia Hao and Han Yang and Jielan Li and Chang Liu and Jia Zhang and Jianwei Zhu and Kehan Wu and Wei Zhang and Kaiyuan Gao and Qizhi Pei and Qian Wang and Xixian Liu and Yanting Li and Houtian Zhu and Yeqing Lu and Mingqian Ma and Zun Wang and Tian Xie and Krzysztof Maziarz and Marwin Segler and Zhao Yang and Zilong Chen and Yu Shi and Shuxin Zheng and Lijun Wu and Chen Hu and Peggy Dai and Tie-Yan Liu and Haiguang Liu and Tao Qin", "abstract": "  Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, and RNA. However, these\nmodels are typically trained in isolation, lacking the ability to integrate\nacross different scientific domains. Recognizing that entities within these\ndomains can all be represented as sequences, which together form the \"language\nof nature\", we introduce Nature Language Model (briefly, NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) achieving\nstate-of-the-art performance in tasks like SMILES-to-IUPAC translation and\nretrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach\nfor various scientific tasks, including drug discovery (hit\ngeneration/optimization, ADMET optimization, synthesis), novel material design,\nand the development of therapeutic proteins or nucleotides. We have developed\nNatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion\nparameters) and observed a clear improvement in performance as the model size\nincreases.\n", "link": "http://arxiv.org/abs/2502.07527v1", "date": "2025-02-11", "relevancy": 2.6536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NatureLM%3A%20Deciphering%20the%20Language%20of%20Nature%20for%20Scientific%20Discovery&body=Title%3A%20NatureLM%3A%20Deciphering%20the%20Language%20of%20Nature%20for%20Scientific%20Discovery%0AAuthor%3A%20Yingce%20Xia%20and%20Peiran%20Jin%20and%20Shufang%20Xie%20and%20Liang%20He%20and%20Chuan%20Cao%20and%20Renqian%20Luo%20and%20Guoqing%20Liu%20and%20Yue%20Wang%20and%20Zequn%20Liu%20and%20Yuan-Jyue%20Chen%20and%20Zekun%20Guo%20and%20Yeqi%20Bai%20and%20Pan%20Deng%20and%20Yaosen%20Min%20and%20Ziheng%20Lu%20and%20Hongxia%20Hao%20and%20Han%20Yang%20and%20Jielan%20Li%20and%20Chang%20Liu%20and%20Jia%20Zhang%20and%20Jianwei%20Zhu%20and%20Kehan%20Wu%20and%20Wei%20Zhang%20and%20Kaiyuan%20Gao%20and%20Qizhi%20Pei%20and%20Qian%20Wang%20and%20Xixian%20Liu%20and%20Yanting%20Li%20and%20Houtian%20Zhu%20and%20Yeqing%20Lu%20and%20Mingqian%20Ma%20and%20Zun%20Wang%20and%20Tian%20Xie%20and%20Krzysztof%20Maziarz%20and%20Marwin%20Segler%20and%20Zhao%20Yang%20and%20Zilong%20Chen%20and%20Yu%20Shi%20and%20Shuxin%20Zheng%20and%20Lijun%20Wu%20and%20Chen%20Hu%20and%20Peggy%20Dai%20and%20Tie-Yan%20Liu%20and%20Haiguang%20Liu%20and%20Tao%20Qin%0AAbstract%3A%20%20%20Foundation%20models%20have%20revolutionized%20natural%20language%20processing%20and%0Aartificial%20intelligence%2C%20significantly%20enhancing%20how%20machines%20comprehend%20and%0Agenerate%20human%20languages.%20Inspired%20by%20the%20success%20of%20these%20foundation%20models%2C%0Aresearchers%20have%20developed%20foundation%20models%20for%20individual%20scientific%20domains%2C%0Aincluding%20small%20molecules%2C%20materials%2C%20proteins%2C%20DNA%2C%20and%20RNA.%20However%2C%20these%0Amodels%20are%20typically%20trained%20in%20isolation%2C%20lacking%20the%20ability%20to%20integrate%0Aacross%20different%20scientific%20domains.%20Recognizing%20that%20entities%20within%20these%0Adomains%20can%20all%20be%20represented%20as%20sequences%2C%20which%20together%20form%20the%20%22language%0Aof%20nature%22%2C%20we%20introduce%20Nature%20Language%20Model%20%28briefly%2C%20NatureLM%29%2C%20a%0Asequence-based%20science%20foundation%20model%20designed%20for%20scientific%20discovery.%0APre-trained%20with%20data%20from%20multiple%20scientific%20domains%2C%20NatureLM%20offers%20a%0Aunified%2C%20versatile%20model%20that%20enables%20various%20applications%20including%3A%20%28i%29%0Agenerating%20and%20optimizing%20small%20molecules%2C%20proteins%2C%20RNA%2C%20and%20materials%20using%0Atext%20instructions%3B%20%28ii%29%20cross-domain%20generation/design%2C%20such%20as%0Aprotein-to-molecule%20and%20protein-to-RNA%20generation%3B%20and%20%28iii%29%20achieving%0Astate-of-the-art%20performance%20in%20tasks%20like%20SMILES-to-IUPAC%20translation%20and%0Aretrosynthesis%20on%20USPTO-50k.%20NatureLM%20offers%20a%20promising%20generalist%20approach%0Afor%20various%20scientific%20tasks%2C%20including%20drug%20discovery%20%28hit%0Ageneration/optimization%2C%20ADMET%20optimization%2C%20synthesis%29%2C%20novel%20material%20design%2C%0Aand%20the%20development%20of%20therapeutic%20proteins%20or%20nucleotides.%20We%20have%20developed%0ANatureLM%20models%20in%20different%20sizes%20%281%20billion%2C%208%20billion%2C%20and%2046.7%20billion%0Aparameters%29%20and%20observed%20a%20clear%20improvement%20in%20performance%20as%20the%20model%20size%0Aincreases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatureLM%253A%2520Deciphering%2520the%2520Language%2520of%2520Nature%2520for%2520Scientific%2520Discovery%26entry.906535625%3DYingce%2520Xia%2520and%2520Peiran%2520Jin%2520and%2520Shufang%2520Xie%2520and%2520Liang%2520He%2520and%2520Chuan%2520Cao%2520and%2520Renqian%2520Luo%2520and%2520Guoqing%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Zequn%2520Liu%2520and%2520Yuan-Jyue%2520Chen%2520and%2520Zekun%2520Guo%2520and%2520Yeqi%2520Bai%2520and%2520Pan%2520Deng%2520and%2520Yaosen%2520Min%2520and%2520Ziheng%2520Lu%2520and%2520Hongxia%2520Hao%2520and%2520Han%2520Yang%2520and%2520Jielan%2520Li%2520and%2520Chang%2520Liu%2520and%2520Jia%2520Zhang%2520and%2520Jianwei%2520Zhu%2520and%2520Kehan%2520Wu%2520and%2520Wei%2520Zhang%2520and%2520Kaiyuan%2520Gao%2520and%2520Qizhi%2520Pei%2520and%2520Qian%2520Wang%2520and%2520Xixian%2520Liu%2520and%2520Yanting%2520Li%2520and%2520Houtian%2520Zhu%2520and%2520Yeqing%2520Lu%2520and%2520Mingqian%2520Ma%2520and%2520Zun%2520Wang%2520and%2520Tian%2520Xie%2520and%2520Krzysztof%2520Maziarz%2520and%2520Marwin%2520Segler%2520and%2520Zhao%2520Yang%2520and%2520Zilong%2520Chen%2520and%2520Yu%2520Shi%2520and%2520Shuxin%2520Zheng%2520and%2520Lijun%2520Wu%2520and%2520Chen%2520Hu%2520and%2520Peggy%2520Dai%2520and%2520Tie-Yan%2520Liu%2520and%2520Haiguang%2520Liu%2520and%2520Tao%2520Qin%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520revolutionized%2520natural%2520language%2520processing%2520and%250Aartificial%2520intelligence%252C%2520significantly%2520enhancing%2520how%2520machines%2520comprehend%2520and%250Agenerate%2520human%2520languages.%2520Inspired%2520by%2520the%2520success%2520of%2520these%2520foundation%2520models%252C%250Aresearchers%2520have%2520developed%2520foundation%2520models%2520for%2520individual%2520scientific%2520domains%252C%250Aincluding%2520small%2520molecules%252C%2520materials%252C%2520proteins%252C%2520DNA%252C%2520and%2520RNA.%2520However%252C%2520these%250Amodels%2520are%2520typically%2520trained%2520in%2520isolation%252C%2520lacking%2520the%2520ability%2520to%2520integrate%250Aacross%2520different%2520scientific%2520domains.%2520Recognizing%2520that%2520entities%2520within%2520these%250Adomains%2520can%2520all%2520be%2520represented%2520as%2520sequences%252C%2520which%2520together%2520form%2520the%2520%2522language%250Aof%2520nature%2522%252C%2520we%2520introduce%2520Nature%2520Language%2520Model%2520%2528briefly%252C%2520NatureLM%2529%252C%2520a%250Asequence-based%2520science%2520foundation%2520model%2520designed%2520for%2520scientific%2520discovery.%250APre-trained%2520with%2520data%2520from%2520multiple%2520scientific%2520domains%252C%2520NatureLM%2520offers%2520a%250Aunified%252C%2520versatile%2520model%2520that%2520enables%2520various%2520applications%2520including%253A%2520%2528i%2529%250Agenerating%2520and%2520optimizing%2520small%2520molecules%252C%2520proteins%252C%2520RNA%252C%2520and%2520materials%2520using%250Atext%2520instructions%253B%2520%2528ii%2529%2520cross-domain%2520generation/design%252C%2520such%2520as%250Aprotein-to-molecule%2520and%2520protein-to-RNA%2520generation%253B%2520and%2520%2528iii%2529%2520achieving%250Astate-of-the-art%2520performance%2520in%2520tasks%2520like%2520SMILES-to-IUPAC%2520translation%2520and%250Aretrosynthesis%2520on%2520USPTO-50k.%2520NatureLM%2520offers%2520a%2520promising%2520generalist%2520approach%250Afor%2520various%2520scientific%2520tasks%252C%2520including%2520drug%2520discovery%2520%2528hit%250Ageneration/optimization%252C%2520ADMET%2520optimization%252C%2520synthesis%2529%252C%2520novel%2520material%2520design%252C%250Aand%2520the%2520development%2520of%2520therapeutic%2520proteins%2520or%2520nucleotides.%2520We%2520have%2520developed%250ANatureLM%2520models%2520in%2520different%2520sizes%2520%25281%2520billion%252C%25208%2520billion%252C%2520and%252046.7%2520billion%250Aparameters%2529%2520and%2520observed%2520a%2520clear%2520improvement%2520in%2520performance%2520as%2520the%2520model%2520size%250Aincreases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NatureLM%3A%20Deciphering%20the%20Language%20of%20Nature%20for%20Scientific%20Discovery&entry.906535625=Yingce%20Xia%20and%20Peiran%20Jin%20and%20Shufang%20Xie%20and%20Liang%20He%20and%20Chuan%20Cao%20and%20Renqian%20Luo%20and%20Guoqing%20Liu%20and%20Yue%20Wang%20and%20Zequn%20Liu%20and%20Yuan-Jyue%20Chen%20and%20Zekun%20Guo%20and%20Yeqi%20Bai%20and%20Pan%20Deng%20and%20Yaosen%20Min%20and%20Ziheng%20Lu%20and%20Hongxia%20Hao%20and%20Han%20Yang%20and%20Jielan%20Li%20and%20Chang%20Liu%20and%20Jia%20Zhang%20and%20Jianwei%20Zhu%20and%20Kehan%20Wu%20and%20Wei%20Zhang%20and%20Kaiyuan%20Gao%20and%20Qizhi%20Pei%20and%20Qian%20Wang%20and%20Xixian%20Liu%20and%20Yanting%20Li%20and%20Houtian%20Zhu%20and%20Yeqing%20Lu%20and%20Mingqian%20Ma%20and%20Zun%20Wang%20and%20Tian%20Xie%20and%20Krzysztof%20Maziarz%20and%20Marwin%20Segler%20and%20Zhao%20Yang%20and%20Zilong%20Chen%20and%20Yu%20Shi%20and%20Shuxin%20Zheng%20and%20Lijun%20Wu%20and%20Chen%20Hu%20and%20Peggy%20Dai%20and%20Tie-Yan%20Liu%20and%20Haiguang%20Liu%20and%20Tao%20Qin&entry.1292438233=%20%20Foundation%20models%20have%20revolutionized%20natural%20language%20processing%20and%0Aartificial%20intelligence%2C%20significantly%20enhancing%20how%20machines%20comprehend%20and%0Agenerate%20human%20languages.%20Inspired%20by%20the%20success%20of%20these%20foundation%20models%2C%0Aresearchers%20have%20developed%20foundation%20models%20for%20individual%20scientific%20domains%2C%0Aincluding%20small%20molecules%2C%20materials%2C%20proteins%2C%20DNA%2C%20and%20RNA.%20However%2C%20these%0Amodels%20are%20typically%20trained%20in%20isolation%2C%20lacking%20the%20ability%20to%20integrate%0Aacross%20different%20scientific%20domains.%20Recognizing%20that%20entities%20within%20these%0Adomains%20can%20all%20be%20represented%20as%20sequences%2C%20which%20together%20form%20the%20%22language%0Aof%20nature%22%2C%20we%20introduce%20Nature%20Language%20Model%20%28briefly%2C%20NatureLM%29%2C%20a%0Asequence-based%20science%20foundation%20model%20designed%20for%20scientific%20discovery.%0APre-trained%20with%20data%20from%20multiple%20scientific%20domains%2C%20NatureLM%20offers%20a%0Aunified%2C%20versatile%20model%20that%20enables%20various%20applications%20including%3A%20%28i%29%0Agenerating%20and%20optimizing%20small%20molecules%2C%20proteins%2C%20RNA%2C%20and%20materials%20using%0Atext%20instructions%3B%20%28ii%29%20cross-domain%20generation/design%2C%20such%20as%0Aprotein-to-molecule%20and%20protein-to-RNA%20generation%3B%20and%20%28iii%29%20achieving%0Astate-of-the-art%20performance%20in%20tasks%20like%20SMILES-to-IUPAC%20translation%20and%0Aretrosynthesis%20on%20USPTO-50k.%20NatureLM%20offers%20a%20promising%20generalist%20approach%0Afor%20various%20scientific%20tasks%2C%20including%20drug%20discovery%20%28hit%0Ageneration/optimization%2C%20ADMET%20optimization%2C%20synthesis%29%2C%20novel%20material%20design%2C%0Aand%20the%20development%20of%20therapeutic%20proteins%20or%20nucleotides.%20We%20have%20developed%0ANatureLM%20models%20in%20different%20sizes%20%281%20billion%2C%208%20billion%2C%20and%2046.7%20billion%0Aparameters%29%20and%20observed%20a%20clear%20improvement%20in%20performance%20as%20the%20model%20size%0Aincreases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07527v1&entry.124074799=Read"},
{"title": "PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with\n  Pre-trained Language Models", "author": "Tonglong Wei and Yan Lin and Youfang Lin and Shengnan Guo and Jilin Hu and Haitao Yuan and Gao Cong and Huaiyu Wan", "abstract": "  Spatiotemporal trajectory data is crucial for various applications. However,\nissues such as device malfunctions and network instability often cause sparse\ntrajectories, leading to lost detailed movement information. Recovering the\nmissing points in sparse trajectories to restore the detailed information is\nthus essential. Despite recent progress, several challenges remain. First, the\nlack of large-scale dense trajectory data makes it difficult to train a\ntrajectory recovery model from scratch. Second, the varying spatiotemporal\ncorrelations in sparse trajectories make it hard to generalize recovery across\ndifferent sampling intervals. Third, the lack of location information\ncomplicates the extraction of road conditions for missing points.\n  To address these challenges, we propose a novel trajectory recovery model\ncalled PLMTrajRec. It leverages the scalability of a pre-trained language model\n(PLM) and can be fine-tuned with only a limited set of dense trajectories. To\nhandle different sampling intervals in sparse trajectories, we first convert\neach trajectory's sampling interval and movement features into natural language\nrepresentations, allowing the PLM to recognize its interval. We then introduce\na trajectory encoder to unify trajectories of varying intervals into a single\ninterval and capture their spatiotemporal relationships. To obtain road\nconditions for missing points, we propose an area flow-guided implicit\ntrajectory prompt, which models road conditions by collecting traffic flows in\neach region. We also introduce a road condition passing mechanism that uses\nobserved points' road conditions to infer those of the missing points.\nExperiments on two public trajectory datasets with three sampling intervals\neach demonstrate the effectiveness, scalability, and generalization ability of\nPLMTrajRec.\n", "link": "http://arxiv.org/abs/2410.14281v2", "date": "2025-02-11", "relevancy": 2.6501, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5398}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5288}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLMTrajRec%3A%20A%20Scalable%20and%20Generalizable%20Trajectory%20Recovery%20Method%20with%0A%20%20Pre-trained%20Language%20Models&body=Title%3A%20PLMTrajRec%3A%20A%20Scalable%20and%20Generalizable%20Trajectory%20Recovery%20Method%20with%0A%20%20Pre-trained%20Language%20Models%0AAuthor%3A%20Tonglong%20Wei%20and%20Yan%20Lin%20and%20Youfang%20Lin%20and%20Shengnan%20Guo%20and%20Jilin%20Hu%20and%20Haitao%20Yuan%20and%20Gao%20Cong%20and%20Huaiyu%20Wan%0AAbstract%3A%20%20%20Spatiotemporal%20trajectory%20data%20is%20crucial%20for%20various%20applications.%20However%2C%0Aissues%20such%20as%20device%20malfunctions%20and%20network%20instability%20often%20cause%20sparse%0Atrajectories%2C%20leading%20to%20lost%20detailed%20movement%20information.%20Recovering%20the%0Amissing%20points%20in%20sparse%20trajectories%20to%20restore%20the%20detailed%20information%20is%0Athus%20essential.%20Despite%20recent%20progress%2C%20several%20challenges%20remain.%20First%2C%20the%0Alack%20of%20large-scale%20dense%20trajectory%20data%20makes%20it%20difficult%20to%20train%20a%0Atrajectory%20recovery%20model%20from%20scratch.%20Second%2C%20the%20varying%20spatiotemporal%0Acorrelations%20in%20sparse%20trajectories%20make%20it%20hard%20to%20generalize%20recovery%20across%0Adifferent%20sampling%20intervals.%20Third%2C%20the%20lack%20of%20location%20information%0Acomplicates%20the%20extraction%20of%20road%20conditions%20for%20missing%20points.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20trajectory%20recovery%20model%0Acalled%20PLMTrajRec.%20It%20leverages%20the%20scalability%20of%20a%20pre-trained%20language%20model%0A%28PLM%29%20and%20can%20be%20fine-tuned%20with%20only%20a%20limited%20set%20of%20dense%20trajectories.%20To%0Ahandle%20different%20sampling%20intervals%20in%20sparse%20trajectories%2C%20we%20first%20convert%0Aeach%20trajectory%27s%20sampling%20interval%20and%20movement%20features%20into%20natural%20language%0Arepresentations%2C%20allowing%20the%20PLM%20to%20recognize%20its%20interval.%20We%20then%20introduce%0Aa%20trajectory%20encoder%20to%20unify%20trajectories%20of%20varying%20intervals%20into%20a%20single%0Ainterval%20and%20capture%20their%20spatiotemporal%20relationships.%20To%20obtain%20road%0Aconditions%20for%20missing%20points%2C%20we%20propose%20an%20area%20flow-guided%20implicit%0Atrajectory%20prompt%2C%20which%20models%20road%20conditions%20by%20collecting%20traffic%20flows%20in%0Aeach%20region.%20We%20also%20introduce%20a%20road%20condition%20passing%20mechanism%20that%20uses%0Aobserved%20points%27%20road%20conditions%20to%20infer%20those%20of%20the%20missing%20points.%0AExperiments%20on%20two%20public%20trajectory%20datasets%20with%20three%20sampling%20intervals%0Aeach%20demonstrate%20the%20effectiveness%2C%20scalability%2C%20and%20generalization%20ability%20of%0APLMTrajRec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14281v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLMTrajRec%253A%2520A%2520Scalable%2520and%2520Generalizable%2520Trajectory%2520Recovery%2520Method%2520with%250A%2520%2520Pre-trained%2520Language%2520Models%26entry.906535625%3DTonglong%2520Wei%2520and%2520Yan%2520Lin%2520and%2520Youfang%2520Lin%2520and%2520Shengnan%2520Guo%2520and%2520Jilin%2520Hu%2520and%2520Haitao%2520Yuan%2520and%2520Gao%2520Cong%2520and%2520Huaiyu%2520Wan%26entry.1292438233%3D%2520%2520Spatiotemporal%2520trajectory%2520data%2520is%2520crucial%2520for%2520various%2520applications.%2520However%252C%250Aissues%2520such%2520as%2520device%2520malfunctions%2520and%2520network%2520instability%2520often%2520cause%2520sparse%250Atrajectories%252C%2520leading%2520to%2520lost%2520detailed%2520movement%2520information.%2520Recovering%2520the%250Amissing%2520points%2520in%2520sparse%2520trajectories%2520to%2520restore%2520the%2520detailed%2520information%2520is%250Athus%2520essential.%2520Despite%2520recent%2520progress%252C%2520several%2520challenges%2520remain.%2520First%252C%2520the%250Alack%2520of%2520large-scale%2520dense%2520trajectory%2520data%2520makes%2520it%2520difficult%2520to%2520train%2520a%250Atrajectory%2520recovery%2520model%2520from%2520scratch.%2520Second%252C%2520the%2520varying%2520spatiotemporal%250Acorrelations%2520in%2520sparse%2520trajectories%2520make%2520it%2520hard%2520to%2520generalize%2520recovery%2520across%250Adifferent%2520sampling%2520intervals.%2520Third%252C%2520the%2520lack%2520of%2520location%2520information%250Acomplicates%2520the%2520extraction%2520of%2520road%2520conditions%2520for%2520missing%2520points.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520trajectory%2520recovery%2520model%250Acalled%2520PLMTrajRec.%2520It%2520leverages%2520the%2520scalability%2520of%2520a%2520pre-trained%2520language%2520model%250A%2528PLM%2529%2520and%2520can%2520be%2520fine-tuned%2520with%2520only%2520a%2520limited%2520set%2520of%2520dense%2520trajectories.%2520To%250Ahandle%2520different%2520sampling%2520intervals%2520in%2520sparse%2520trajectories%252C%2520we%2520first%2520convert%250Aeach%2520trajectory%2527s%2520sampling%2520interval%2520and%2520movement%2520features%2520into%2520natural%2520language%250Arepresentations%252C%2520allowing%2520the%2520PLM%2520to%2520recognize%2520its%2520interval.%2520We%2520then%2520introduce%250Aa%2520trajectory%2520encoder%2520to%2520unify%2520trajectories%2520of%2520varying%2520intervals%2520into%2520a%2520single%250Ainterval%2520and%2520capture%2520their%2520spatiotemporal%2520relationships.%2520To%2520obtain%2520road%250Aconditions%2520for%2520missing%2520points%252C%2520we%2520propose%2520an%2520area%2520flow-guided%2520implicit%250Atrajectory%2520prompt%252C%2520which%2520models%2520road%2520conditions%2520by%2520collecting%2520traffic%2520flows%2520in%250Aeach%2520region.%2520We%2520also%2520introduce%2520a%2520road%2520condition%2520passing%2520mechanism%2520that%2520uses%250Aobserved%2520points%2527%2520road%2520conditions%2520to%2520infer%2520those%2520of%2520the%2520missing%2520points.%250AExperiments%2520on%2520two%2520public%2520trajectory%2520datasets%2520with%2520three%2520sampling%2520intervals%250Aeach%2520demonstrate%2520the%2520effectiveness%252C%2520scalability%252C%2520and%2520generalization%2520ability%2520of%250APLMTrajRec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14281v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLMTrajRec%3A%20A%20Scalable%20and%20Generalizable%20Trajectory%20Recovery%20Method%20with%0A%20%20Pre-trained%20Language%20Models&entry.906535625=Tonglong%20Wei%20and%20Yan%20Lin%20and%20Youfang%20Lin%20and%20Shengnan%20Guo%20and%20Jilin%20Hu%20and%20Haitao%20Yuan%20and%20Gao%20Cong%20and%20Huaiyu%20Wan&entry.1292438233=%20%20Spatiotemporal%20trajectory%20data%20is%20crucial%20for%20various%20applications.%20However%2C%0Aissues%20such%20as%20device%20malfunctions%20and%20network%20instability%20often%20cause%20sparse%0Atrajectories%2C%20leading%20to%20lost%20detailed%20movement%20information.%20Recovering%20the%0Amissing%20points%20in%20sparse%20trajectories%20to%20restore%20the%20detailed%20information%20is%0Athus%20essential.%20Despite%20recent%20progress%2C%20several%20challenges%20remain.%20First%2C%20the%0Alack%20of%20large-scale%20dense%20trajectory%20data%20makes%20it%20difficult%20to%20train%20a%0Atrajectory%20recovery%20model%20from%20scratch.%20Second%2C%20the%20varying%20spatiotemporal%0Acorrelations%20in%20sparse%20trajectories%20make%20it%20hard%20to%20generalize%20recovery%20across%0Adifferent%20sampling%20intervals.%20Third%2C%20the%20lack%20of%20location%20information%0Acomplicates%20the%20extraction%20of%20road%20conditions%20for%20missing%20points.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20trajectory%20recovery%20model%0Acalled%20PLMTrajRec.%20It%20leverages%20the%20scalability%20of%20a%20pre-trained%20language%20model%0A%28PLM%29%20and%20can%20be%20fine-tuned%20with%20only%20a%20limited%20set%20of%20dense%20trajectories.%20To%0Ahandle%20different%20sampling%20intervals%20in%20sparse%20trajectories%2C%20we%20first%20convert%0Aeach%20trajectory%27s%20sampling%20interval%20and%20movement%20features%20into%20natural%20language%0Arepresentations%2C%20allowing%20the%20PLM%20to%20recognize%20its%20interval.%20We%20then%20introduce%0Aa%20trajectory%20encoder%20to%20unify%20trajectories%20of%20varying%20intervals%20into%20a%20single%0Ainterval%20and%20capture%20their%20spatiotemporal%20relationships.%20To%20obtain%20road%0Aconditions%20for%20missing%20points%2C%20we%20propose%20an%20area%20flow-guided%20implicit%0Atrajectory%20prompt%2C%20which%20models%20road%20conditions%20by%20collecting%20traffic%20flows%20in%0Aeach%20region.%20We%20also%20introduce%20a%20road%20condition%20passing%20mechanism%20that%20uses%0Aobserved%20points%27%20road%20conditions%20to%20infer%20those%20of%20the%20missing%20points.%0AExperiments%20on%20two%20public%20trajectory%20datasets%20with%20three%20sampling%20intervals%0Aeach%20demonstrate%20the%20effectiveness%2C%20scalability%2C%20and%20generalization%20ability%20of%0APLMTrajRec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14281v2&entry.124074799=Read"},
{"title": "HiPoNet: A Topology-Preserving Multi-View Neural Network For High\n  Dimensional Point Cloud and Single-Cell Data", "author": "Siddharth Viswanath and Hiren Madhu and Dhananjay Bhaskar and Jake Kovalic and Dave Johnson and Rex Ying and Christopher Tape and Ian Adelstein and Michael Perlmutter and Smita Krishnaswamy", "abstract": "  In this paper, we propose HiPoNet, an end-to-end differentiable neural\nnetwork for regression, classification, and representation learning on\nhigh-dimensional point clouds. Single-cell data can have high dimensionality\nexceeding the capabilities of existing methods point cloud tailored for 3D\ndata. Moreover, modern single-cell and spatial experiments now yield entire\ncohorts of datasets (i.e. one on every patient), necessitating models that can\nprocess large, high-dimensional point clouds at scale. Most current approaches\nbuild a single nearest-neighbor graph, discarding important geometric\ninformation. In contrast, HiPoNet forms higher-order simplicial complexes\nthrough learnable feature reweighting, generating multiple data views that\ndisentangle distinct biological processes. It then employs simplicial wavelet\ntransforms to extract multi-scale features - capturing both local and global\ntopology. We empirically show that these components preserve topological\ninformation in the learned representations, and that HiPoNet significantly\noutperforms state-of-the-art point-cloud and graph-based models on single cell.\nWe also show an application of HiPoNet on spatial transcriptomics datasets\nusing spatial co-ordinates as one of the views. Overall, HiPoNet offers a\nrobust and scalable solution for high-dimensional data analysis.\n", "link": "http://arxiv.org/abs/2502.07746v1", "date": "2025-02-11", "relevancy": 2.6499, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5441}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.525}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiPoNet%3A%20A%20Topology-Preserving%20Multi-View%20Neural%20Network%20For%20High%0A%20%20Dimensional%20Point%20Cloud%20and%20Single-Cell%20Data&body=Title%3A%20HiPoNet%3A%20A%20Topology-Preserving%20Multi-View%20Neural%20Network%20For%20High%0A%20%20Dimensional%20Point%20Cloud%20and%20Single-Cell%20Data%0AAuthor%3A%20Siddharth%20Viswanath%20and%20Hiren%20Madhu%20and%20Dhananjay%20Bhaskar%20and%20Jake%20Kovalic%20and%20Dave%20Johnson%20and%20Rex%20Ying%20and%20Christopher%20Tape%20and%20Ian%20Adelstein%20and%20Michael%20Perlmutter%20and%20Smita%20Krishnaswamy%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20HiPoNet%2C%20an%20end-to-end%20differentiable%20neural%0Anetwork%20for%20regression%2C%20classification%2C%20and%20representation%20learning%20on%0Ahigh-dimensional%20point%20clouds.%20Single-cell%20data%20can%20have%20high%20dimensionality%0Aexceeding%20the%20capabilities%20of%20existing%20methods%20point%20cloud%20tailored%20for%203D%0Adata.%20Moreover%2C%20modern%20single-cell%20and%20spatial%20experiments%20now%20yield%20entire%0Acohorts%20of%20datasets%20%28i.e.%20one%20on%20every%20patient%29%2C%20necessitating%20models%20that%20can%0Aprocess%20large%2C%20high-dimensional%20point%20clouds%20at%20scale.%20Most%20current%20approaches%0Abuild%20a%20single%20nearest-neighbor%20graph%2C%20discarding%20important%20geometric%0Ainformation.%20In%20contrast%2C%20HiPoNet%20forms%20higher-order%20simplicial%20complexes%0Athrough%20learnable%20feature%20reweighting%2C%20generating%20multiple%20data%20views%20that%0Adisentangle%20distinct%20biological%20processes.%20It%20then%20employs%20simplicial%20wavelet%0Atransforms%20to%20extract%20multi-scale%20features%20-%20capturing%20both%20local%20and%20global%0Atopology.%20We%20empirically%20show%20that%20these%20components%20preserve%20topological%0Ainformation%20in%20the%20learned%20representations%2C%20and%20that%20HiPoNet%20significantly%0Aoutperforms%20state-of-the-art%20point-cloud%20and%20graph-based%20models%20on%20single%20cell.%0AWe%20also%20show%20an%20application%20of%20HiPoNet%20on%20spatial%20transcriptomics%20datasets%0Ausing%20spatial%20co-ordinates%20as%20one%20of%20the%20views.%20Overall%2C%20HiPoNet%20offers%20a%0Arobust%20and%20scalable%20solution%20for%20high-dimensional%20data%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiPoNet%253A%2520A%2520Topology-Preserving%2520Multi-View%2520Neural%2520Network%2520For%2520High%250A%2520%2520Dimensional%2520Point%2520Cloud%2520and%2520Single-Cell%2520Data%26entry.906535625%3DSiddharth%2520Viswanath%2520and%2520Hiren%2520Madhu%2520and%2520Dhananjay%2520Bhaskar%2520and%2520Jake%2520Kovalic%2520and%2520Dave%2520Johnson%2520and%2520Rex%2520Ying%2520and%2520Christopher%2520Tape%2520and%2520Ian%2520Adelstein%2520and%2520Michael%2520Perlmutter%2520and%2520Smita%2520Krishnaswamy%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520HiPoNet%252C%2520an%2520end-to-end%2520differentiable%2520neural%250Anetwork%2520for%2520regression%252C%2520classification%252C%2520and%2520representation%2520learning%2520on%250Ahigh-dimensional%2520point%2520clouds.%2520Single-cell%2520data%2520can%2520have%2520high%2520dimensionality%250Aexceeding%2520the%2520capabilities%2520of%2520existing%2520methods%2520point%2520cloud%2520tailored%2520for%25203D%250Adata.%2520Moreover%252C%2520modern%2520single-cell%2520and%2520spatial%2520experiments%2520now%2520yield%2520entire%250Acohorts%2520of%2520datasets%2520%2528i.e.%2520one%2520on%2520every%2520patient%2529%252C%2520necessitating%2520models%2520that%2520can%250Aprocess%2520large%252C%2520high-dimensional%2520point%2520clouds%2520at%2520scale.%2520Most%2520current%2520approaches%250Abuild%2520a%2520single%2520nearest-neighbor%2520graph%252C%2520discarding%2520important%2520geometric%250Ainformation.%2520In%2520contrast%252C%2520HiPoNet%2520forms%2520higher-order%2520simplicial%2520complexes%250Athrough%2520learnable%2520feature%2520reweighting%252C%2520generating%2520multiple%2520data%2520views%2520that%250Adisentangle%2520distinct%2520biological%2520processes.%2520It%2520then%2520employs%2520simplicial%2520wavelet%250Atransforms%2520to%2520extract%2520multi-scale%2520features%2520-%2520capturing%2520both%2520local%2520and%2520global%250Atopology.%2520We%2520empirically%2520show%2520that%2520these%2520components%2520preserve%2520topological%250Ainformation%2520in%2520the%2520learned%2520representations%252C%2520and%2520that%2520HiPoNet%2520significantly%250Aoutperforms%2520state-of-the-art%2520point-cloud%2520and%2520graph-based%2520models%2520on%2520single%2520cell.%250AWe%2520also%2520show%2520an%2520application%2520of%2520HiPoNet%2520on%2520spatial%2520transcriptomics%2520datasets%250Ausing%2520spatial%2520co-ordinates%2520as%2520one%2520of%2520the%2520views.%2520Overall%252C%2520HiPoNet%2520offers%2520a%250Arobust%2520and%2520scalable%2520solution%2520for%2520high-dimensional%2520data%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiPoNet%3A%20A%20Topology-Preserving%20Multi-View%20Neural%20Network%20For%20High%0A%20%20Dimensional%20Point%20Cloud%20and%20Single-Cell%20Data&entry.906535625=Siddharth%20Viswanath%20and%20Hiren%20Madhu%20and%20Dhananjay%20Bhaskar%20and%20Jake%20Kovalic%20and%20Dave%20Johnson%20and%20Rex%20Ying%20and%20Christopher%20Tape%20and%20Ian%20Adelstein%20and%20Michael%20Perlmutter%20and%20Smita%20Krishnaswamy&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20HiPoNet%2C%20an%20end-to-end%20differentiable%20neural%0Anetwork%20for%20regression%2C%20classification%2C%20and%20representation%20learning%20on%0Ahigh-dimensional%20point%20clouds.%20Single-cell%20data%20can%20have%20high%20dimensionality%0Aexceeding%20the%20capabilities%20of%20existing%20methods%20point%20cloud%20tailored%20for%203D%0Adata.%20Moreover%2C%20modern%20single-cell%20and%20spatial%20experiments%20now%20yield%20entire%0Acohorts%20of%20datasets%20%28i.e.%20one%20on%20every%20patient%29%2C%20necessitating%20models%20that%20can%0Aprocess%20large%2C%20high-dimensional%20point%20clouds%20at%20scale.%20Most%20current%20approaches%0Abuild%20a%20single%20nearest-neighbor%20graph%2C%20discarding%20important%20geometric%0Ainformation.%20In%20contrast%2C%20HiPoNet%20forms%20higher-order%20simplicial%20complexes%0Athrough%20learnable%20feature%20reweighting%2C%20generating%20multiple%20data%20views%20that%0Adisentangle%20distinct%20biological%20processes.%20It%20then%20employs%20simplicial%20wavelet%0Atransforms%20to%20extract%20multi-scale%20features%20-%20capturing%20both%20local%20and%20global%0Atopology.%20We%20empirically%20show%20that%20these%20components%20preserve%20topological%0Ainformation%20in%20the%20learned%20representations%2C%20and%20that%20HiPoNet%20significantly%0Aoutperforms%20state-of-the-art%20point-cloud%20and%20graph-based%20models%20on%20single%20cell.%0AWe%20also%20show%20an%20application%20of%20HiPoNet%20on%20spatial%20transcriptomics%20datasets%0Ausing%20spatial%20co-ordinates%20as%20one%20of%20the%20views.%20Overall%2C%20HiPoNet%20offers%20a%0Arobust%20and%20scalable%20solution%20for%20high-dimensional%20data%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07746v1&entry.124074799=Read"},
{"title": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters", "author": "Kamer Ali Yuksel and Hassan Sawaf", "abstract": "  Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/\n", "link": "http://arxiv.org/abs/2502.04315v3", "date": "2025-02-11", "relevancy": 2.648, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5648}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5134}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChameleonLLM%3A%20Batch-Aware%20Dynamic%20Low-Rank%20Adaptation%20via%20Inference-Time%0A%20%20Clusters&body=Title%3A%20ChameleonLLM%3A%20Batch-Aware%20Dynamic%20Low-Rank%20Adaptation%20via%20Inference-Time%0A%20%20Clusters%0AAuthor%3A%20Kamer%20Ali%20Yuksel%20and%20Hassan%20Sawaf%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%0Aperformance%20across%20diverse%20tasks.%20However%2C%20these%20models%20are%20typically%20deployed%0Awith%20fixed%20weights%2C%20which%20limits%20their%20ability%20to%20adapt%20dynamically%20to%20the%0Avariability%20inherent%20in%20real-world%20data%20during%20inference.%20This%20paper%20introduces%0AChameleonLLM%2C%20a%20novel%20framework%20that%20enables%20inference-time%20adaptation%20of%20LLMs%0Aby%20leveraging%20batch-aware%20clustering%20and%20on-the-fly%20generation%20of%20low-rank%0Aupdates.%20Unlike%20traditional%20fine-tuning%20approaches%20such%20as%20Low-Rank%20Adaptation%0A%28LoRA%29%20or%20methods%20that%20rely%20on%20a%20fixed%20set%20of%20pre-learned%20uniforms%20%28changeable%0Amasks%29%2C%20our%20method%20dynamically%20generates%20adaptive%20modifications%20to%20the%20decoder%0Aweights%20based%20on%20the%20aggregated%20statistics%20of%20clustered%20batches.%20By%0Aintelligently%20grouping%20similar%20inputs%20and%20computing%20context-aware%20low-rank%0Aupdates%20via%20a%20hyper-network%2C%20ChameleonLLM%20achieves%20significant%20performance%0Agains%2C%20outperforming%20conventional%20LoRA%20methods%20while%20eliminating%20the%20overhead%0Aof%20maintaining%20multiple%20expert%20models.%20Our%20experiments%20highlight%20the%20potential%0Aof%20our%20approach%20to%20serve%20as%20a%20versatile%20and%20highly%20adaptive%20solution%20for%0Alanguage%20model%20inference.%20ChameleonLLM%20is%20open-sourced%20to%20ensure%20the%0Areproducibility%20of%20our%20experiments%3A%0Ahttps%3A//anonymous.4open.science/r/ChamaleonLLM/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04315v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChameleonLLM%253A%2520Batch-Aware%2520Dynamic%2520Low-Rank%2520Adaptation%2520via%2520Inference-Time%250A%2520%2520Clusters%26entry.906535625%3DKamer%2520Ali%2520Yuksel%2520and%2520Hassan%2520Sawaf%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%250Aperformance%2520across%2520diverse%2520tasks.%2520However%252C%2520these%2520models%2520are%2520typically%2520deployed%250Awith%2520fixed%2520weights%252C%2520which%2520limits%2520their%2520ability%2520to%2520adapt%2520dynamically%2520to%2520the%250Avariability%2520inherent%2520in%2520real-world%2520data%2520during%2520inference.%2520This%2520paper%2520introduces%250AChameleonLLM%252C%2520a%2520novel%2520framework%2520that%2520enables%2520inference-time%2520adaptation%2520of%2520LLMs%250Aby%2520leveraging%2520batch-aware%2520clustering%2520and%2520on-the-fly%2520generation%2520of%2520low-rank%250Aupdates.%2520Unlike%2520traditional%2520fine-tuning%2520approaches%2520such%2520as%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%2520or%2520methods%2520that%2520rely%2520on%2520a%2520fixed%2520set%2520of%2520pre-learned%2520uniforms%2520%2528changeable%250Amasks%2529%252C%2520our%2520method%2520dynamically%2520generates%2520adaptive%2520modifications%2520to%2520the%2520decoder%250Aweights%2520based%2520on%2520the%2520aggregated%2520statistics%2520of%2520clustered%2520batches.%2520By%250Aintelligently%2520grouping%2520similar%2520inputs%2520and%2520computing%2520context-aware%2520low-rank%250Aupdates%2520via%2520a%2520hyper-network%252C%2520ChameleonLLM%2520achieves%2520significant%2520performance%250Agains%252C%2520outperforming%2520conventional%2520LoRA%2520methods%2520while%2520eliminating%2520the%2520overhead%250Aof%2520maintaining%2520multiple%2520expert%2520models.%2520Our%2520experiments%2520highlight%2520the%2520potential%250Aof%2520our%2520approach%2520to%2520serve%2520as%2520a%2520versatile%2520and%2520highly%2520adaptive%2520solution%2520for%250Alanguage%2520model%2520inference.%2520ChameleonLLM%2520is%2520open-sourced%2520to%2520ensure%2520the%250Areproducibility%2520of%2520our%2520experiments%253A%250Ahttps%253A//anonymous.4open.science/r/ChamaleonLLM/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04315v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChameleonLLM%3A%20Batch-Aware%20Dynamic%20Low-Rank%20Adaptation%20via%20Inference-Time%0A%20%20Clusters&entry.906535625=Kamer%20Ali%20Yuksel%20and%20Hassan%20Sawaf&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%0Aperformance%20across%20diverse%20tasks.%20However%2C%20these%20models%20are%20typically%20deployed%0Awith%20fixed%20weights%2C%20which%20limits%20their%20ability%20to%20adapt%20dynamically%20to%20the%0Avariability%20inherent%20in%20real-world%20data%20during%20inference.%20This%20paper%20introduces%0AChameleonLLM%2C%20a%20novel%20framework%20that%20enables%20inference-time%20adaptation%20of%20LLMs%0Aby%20leveraging%20batch-aware%20clustering%20and%20on-the-fly%20generation%20of%20low-rank%0Aupdates.%20Unlike%20traditional%20fine-tuning%20approaches%20such%20as%20Low-Rank%20Adaptation%0A%28LoRA%29%20or%20methods%20that%20rely%20on%20a%20fixed%20set%20of%20pre-learned%20uniforms%20%28changeable%0Amasks%29%2C%20our%20method%20dynamically%20generates%20adaptive%20modifications%20to%20the%20decoder%0Aweights%20based%20on%20the%20aggregated%20statistics%20of%20clustered%20batches.%20By%0Aintelligently%20grouping%20similar%20inputs%20and%20computing%20context-aware%20low-rank%0Aupdates%20via%20a%20hyper-network%2C%20ChameleonLLM%20achieves%20significant%20performance%0Agains%2C%20outperforming%20conventional%20LoRA%20methods%20while%20eliminating%20the%20overhead%0Aof%20maintaining%20multiple%20expert%20models.%20Our%20experiments%20highlight%20the%20potential%0Aof%20our%20approach%20to%20serve%20as%20a%20versatile%20and%20highly%20adaptive%20solution%20for%0Alanguage%20model%20inference.%20ChameleonLLM%20is%20open-sourced%20to%20ensure%20the%0Areproducibility%20of%20our%20experiments%3A%0Ahttps%3A//anonymous.4open.science/r/ChamaleonLLM/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04315v3&entry.124074799=Read"},
{"title": "Autonomous localization of multiple ionizing radiation sources using\n  miniature single-layer Compton cameras onboard a group of micro aerial\n  vehicles", "author": "Michal Werner and Tom\u00e1\u0161 B\u00e1\u010da and Petr \u0160tibinger and Daniela Doubravov\u00e1 and Jaroslav \u0160olc and Jan Rus\u0148\u00e1k and Martin Saska", "abstract": "  A novel method for autonomous localization of multiple sources of gamma\nradiation using a group of Micro Aerial Vehicles (MAVs) is presented in this\npaper. The method utilizes an extremely lightweight (44 g) Compton camera\nMiniPIX TPX3. The compact size of the detector allows for deployment onboard\nsafe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed\nradiation mapping approach fuses measurements from multiple distributed Compton\ncamera sensors to accurately estimate the positions of multiple radioactive\nsources in real time. Unlike commonly used intensity-based detectors, the\nCompton camera reconstructs the set of possible directions towards a radiation\nsource from just a single ionizing particle. Therefore, the proposed approach\ncan localize radiation sources without having to estimate the gradient of a\nradiation field or contour lines, which require longer measurements. The\ninstant estimation is able to fully exploit the potential of highly mobile\nMAVs. The radiation mapping method is combined with an active search strategy,\nwhich coordinates the future actions of the MAVs in order to improve the\nquality of the estimate of the sources' positions, as well as to explore the\narea of interest faster. The proposed solution is evaluated in simulation and\nreal world experiments with multiple Cesium-137 radiation sources.\n", "link": "http://arxiv.org/abs/2410.06693v2", "date": "2025-02-11", "relevancy": 2.646, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5674}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5182}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20localization%20of%20multiple%20ionizing%20radiation%20sources%20using%0A%20%20miniature%20single-layer%20Compton%20cameras%20onboard%20a%20group%20of%20micro%20aerial%0A%20%20vehicles&body=Title%3A%20Autonomous%20localization%20of%20multiple%20ionizing%20radiation%20sources%20using%0A%20%20miniature%20single-layer%20Compton%20cameras%20onboard%20a%20group%20of%20micro%20aerial%0A%20%20vehicles%0AAuthor%3A%20Michal%20Werner%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Petr%20%C5%A0tibinger%20and%20Daniela%20Doubravov%C3%A1%20and%20Jaroslav%20%C5%A0olc%20and%20Jan%20Rus%C5%88%C3%A1k%20and%20Martin%20Saska%0AAbstract%3A%20%20%20A%20novel%20method%20for%20autonomous%20localization%20of%20multiple%20sources%20of%20gamma%0Aradiation%20using%20a%20group%20of%20Micro%20Aerial%20Vehicles%20%28MAVs%29%20is%20presented%20in%20this%0Apaper.%20The%20method%20utilizes%20an%20extremely%20lightweight%20%2844%20g%29%20Compton%20camera%0AMiniPIX%20TPX3.%20The%20compact%20size%20of%20the%20detector%20allows%20for%20deployment%20onboard%0Asafe%20and%20agile%20small-scale%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%20The%20proposed%0Aradiation%20mapping%20approach%20fuses%20measurements%20from%20multiple%20distributed%20Compton%0Acamera%20sensors%20to%20accurately%20estimate%20the%20positions%20of%20multiple%20radioactive%0Asources%20in%20real%20time.%20Unlike%20commonly%20used%20intensity-based%20detectors%2C%20the%0ACompton%20camera%20reconstructs%20the%20set%20of%20possible%20directions%20towards%20a%20radiation%0Asource%20from%20just%20a%20single%20ionizing%20particle.%20Therefore%2C%20the%20proposed%20approach%0Acan%20localize%20radiation%20sources%20without%20having%20to%20estimate%20the%20gradient%20of%20a%0Aradiation%20field%20or%20contour%20lines%2C%20which%20require%20longer%20measurements.%20The%0Ainstant%20estimation%20is%20able%20to%20fully%20exploit%20the%20potential%20of%20highly%20mobile%0AMAVs.%20The%20radiation%20mapping%20method%20is%20combined%20with%20an%20active%20search%20strategy%2C%0Awhich%20coordinates%20the%20future%20actions%20of%20the%20MAVs%20in%20order%20to%20improve%20the%0Aquality%20of%20the%20estimate%20of%20the%20sources%27%20positions%2C%20as%20well%20as%20to%20explore%20the%0Aarea%20of%20interest%20faster.%20The%20proposed%20solution%20is%20evaluated%20in%20simulation%20and%0Areal%20world%20experiments%20with%20multiple%20Cesium-137%20radiation%20sources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520localization%2520of%2520multiple%2520ionizing%2520radiation%2520sources%2520using%250A%2520%2520miniature%2520single-layer%2520Compton%2520cameras%2520onboard%2520a%2520group%2520of%2520micro%2520aerial%250A%2520%2520vehicles%26entry.906535625%3DMichal%2520Werner%2520and%2520Tom%25C3%25A1%25C5%25A1%2520B%25C3%25A1%25C4%258Da%2520and%2520Petr%2520%25C5%25A0tibinger%2520and%2520Daniela%2520Doubravov%25C3%25A1%2520and%2520Jaroslav%2520%25C5%25A0olc%2520and%2520Jan%2520Rus%25C5%2588%25C3%25A1k%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520A%2520novel%2520method%2520for%2520autonomous%2520localization%2520of%2520multiple%2520sources%2520of%2520gamma%250Aradiation%2520using%2520a%2520group%2520of%2520Micro%2520Aerial%2520Vehicles%2520%2528MAVs%2529%2520is%2520presented%2520in%2520this%250Apaper.%2520The%2520method%2520utilizes%2520an%2520extremely%2520lightweight%2520%252844%2520g%2529%2520Compton%2520camera%250AMiniPIX%2520TPX3.%2520The%2520compact%2520size%2520of%2520the%2520detector%2520allows%2520for%2520deployment%2520onboard%250Asafe%2520and%2520agile%2520small-scale%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529.%2520The%2520proposed%250Aradiation%2520mapping%2520approach%2520fuses%2520measurements%2520from%2520multiple%2520distributed%2520Compton%250Acamera%2520sensors%2520to%2520accurately%2520estimate%2520the%2520positions%2520of%2520multiple%2520radioactive%250Asources%2520in%2520real%2520time.%2520Unlike%2520commonly%2520used%2520intensity-based%2520detectors%252C%2520the%250ACompton%2520camera%2520reconstructs%2520the%2520set%2520of%2520possible%2520directions%2520towards%2520a%2520radiation%250Asource%2520from%2520just%2520a%2520single%2520ionizing%2520particle.%2520Therefore%252C%2520the%2520proposed%2520approach%250Acan%2520localize%2520radiation%2520sources%2520without%2520having%2520to%2520estimate%2520the%2520gradient%2520of%2520a%250Aradiation%2520field%2520or%2520contour%2520lines%252C%2520which%2520require%2520longer%2520measurements.%2520The%250Ainstant%2520estimation%2520is%2520able%2520to%2520fully%2520exploit%2520the%2520potential%2520of%2520highly%2520mobile%250AMAVs.%2520The%2520radiation%2520mapping%2520method%2520is%2520combined%2520with%2520an%2520active%2520search%2520strategy%252C%250Awhich%2520coordinates%2520the%2520future%2520actions%2520of%2520the%2520MAVs%2520in%2520order%2520to%2520improve%2520the%250Aquality%2520of%2520the%2520estimate%2520of%2520the%2520sources%2527%2520positions%252C%2520as%2520well%2520as%2520to%2520explore%2520the%250Aarea%2520of%2520interest%2520faster.%2520The%2520proposed%2520solution%2520is%2520evaluated%2520in%2520simulation%2520and%250Areal%2520world%2520experiments%2520with%2520multiple%2520Cesium-137%2520radiation%2520sources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20localization%20of%20multiple%20ionizing%20radiation%20sources%20using%0A%20%20miniature%20single-layer%20Compton%20cameras%20onboard%20a%20group%20of%20micro%20aerial%0A%20%20vehicles&entry.906535625=Michal%20Werner%20and%20Tom%C3%A1%C5%A1%20B%C3%A1%C4%8Da%20and%20Petr%20%C5%A0tibinger%20and%20Daniela%20Doubravov%C3%A1%20and%20Jaroslav%20%C5%A0olc%20and%20Jan%20Rus%C5%88%C3%A1k%20and%20Martin%20Saska&entry.1292438233=%20%20A%20novel%20method%20for%20autonomous%20localization%20of%20multiple%20sources%20of%20gamma%0Aradiation%20using%20a%20group%20of%20Micro%20Aerial%20Vehicles%20%28MAVs%29%20is%20presented%20in%20this%0Apaper.%20The%20method%20utilizes%20an%20extremely%20lightweight%20%2844%20g%29%20Compton%20camera%0AMiniPIX%20TPX3.%20The%20compact%20size%20of%20the%20detector%20allows%20for%20deployment%20onboard%0Asafe%20and%20agile%20small-scale%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29.%20The%20proposed%0Aradiation%20mapping%20approach%20fuses%20measurements%20from%20multiple%20distributed%20Compton%0Acamera%20sensors%20to%20accurately%20estimate%20the%20positions%20of%20multiple%20radioactive%0Asources%20in%20real%20time.%20Unlike%20commonly%20used%20intensity-based%20detectors%2C%20the%0ACompton%20camera%20reconstructs%20the%20set%20of%20possible%20directions%20towards%20a%20radiation%0Asource%20from%20just%20a%20single%20ionizing%20particle.%20Therefore%2C%20the%20proposed%20approach%0Acan%20localize%20radiation%20sources%20without%20having%20to%20estimate%20the%20gradient%20of%20a%0Aradiation%20field%20or%20contour%20lines%2C%20which%20require%20longer%20measurements.%20The%0Ainstant%20estimation%20is%20able%20to%20fully%20exploit%20the%20potential%20of%20highly%20mobile%0AMAVs.%20The%20radiation%20mapping%20method%20is%20combined%20with%20an%20active%20search%20strategy%2C%0Awhich%20coordinates%20the%20future%20actions%20of%20the%20MAVs%20in%20order%20to%20improve%20the%0Aquality%20of%20the%20estimate%20of%20the%20sources%27%20positions%2C%20as%20well%20as%20to%20explore%20the%0Aarea%20of%20interest%20faster.%20The%20proposed%20solution%20is%20evaluated%20in%20simulation%20and%0Areal%20world%20experiments%20with%20multiple%20Cesium-137%20radiation%20sources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06693v2&entry.124074799=Read"},
{"title": "Automated Road Extraction and Centreline Fitting in LiDAR Point Clouds", "author": "Xinyu Wang and Muhammad Ibrahim and Atif Mansoor and Hasnein Tareque and Ajmal Mian", "abstract": "  Road information extraction from 3D point clouds is useful for urban planning\nand traffic management. Existing methods often rely on local features and the\nrefraction angle of lasers from kerbs, which makes them sensitive to variable\nkerb designs and issues in high-density areas due to data homogeneity. We\npropose an approach for extracting road points and fitting centrelines using a\ntop-down view of LiDAR based ground-collected point clouds. This prospective\nview reduces reliance on specific kerb design and results in better road\nextraction. We first perform statistical outlier removal and density-based\nclustering to reduce noise from 3D point cloud data. Next, we perform ground\npoint filtering using a grid-based segmentation method that adapts to diverse\nroad scenarios and terrain characteristics. The filtered points are then\nprojected onto a 2D plane, and the road is extracted by a skeletonisation\nalgorithm. The skeleton is back-projected onto the 3D point cloud with\ncalculated normals, which guide a region growing algorithm to find nearby road\npoints. The extracted road points are then smoothed with the Savitzky-Golay\nfilter to produce the final centreline. Our initial approach without\npost-processing of road skeleton achieved 67% in IoU by testing on the Perth\nCBD dataset with different road types. Incorporating the post-processing of the\nroad skeleton improved the extraction of road points around the smoothed\nskeleton. The refined approach achieved a higher IoU value of 73% and with 23%\nreduction in the processing time. Our approach offers a generalised and\ncomputationally efficient solution that combines 3D and 2D processing\ntechniques, laying the groundwork for future road reconstruction and 3D-to-2D\npoint cloud alignment.\n", "link": "http://arxiv.org/abs/2502.07486v1", "date": "2025-02-11", "relevancy": 2.6432, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5318}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Road%20Extraction%20and%20Centreline%20Fitting%20in%20LiDAR%20Point%20Clouds&body=Title%3A%20Automated%20Road%20Extraction%20and%20Centreline%20Fitting%20in%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Atif%20Mansoor%20and%20Hasnein%20Tareque%20and%20Ajmal%20Mian%0AAbstract%3A%20%20%20Road%20information%20extraction%20from%203D%20point%20clouds%20is%20useful%20for%20urban%20planning%0Aand%20traffic%20management.%20Existing%20methods%20often%20rely%20on%20local%20features%20and%20the%0Arefraction%20angle%20of%20lasers%20from%20kerbs%2C%20which%20makes%20them%20sensitive%20to%20variable%0Akerb%20designs%20and%20issues%20in%20high-density%20areas%20due%20to%20data%20homogeneity.%20We%0Apropose%20an%20approach%20for%20extracting%20road%20points%20and%20fitting%20centrelines%20using%20a%0Atop-down%20view%20of%20LiDAR%20based%20ground-collected%20point%20clouds.%20This%20prospective%0Aview%20reduces%20reliance%20on%20specific%20kerb%20design%20and%20results%20in%20better%20road%0Aextraction.%20We%20first%20perform%20statistical%20outlier%20removal%20and%20density-based%0Aclustering%20to%20reduce%20noise%20from%203D%20point%20cloud%20data.%20Next%2C%20we%20perform%20ground%0Apoint%20filtering%20using%20a%20grid-based%20segmentation%20method%20that%20adapts%20to%20diverse%0Aroad%20scenarios%20and%20terrain%20characteristics.%20The%20filtered%20points%20are%20then%0Aprojected%20onto%20a%202D%20plane%2C%20and%20the%20road%20is%20extracted%20by%20a%20skeletonisation%0Aalgorithm.%20The%20skeleton%20is%20back-projected%20onto%20the%203D%20point%20cloud%20with%0Acalculated%20normals%2C%20which%20guide%20a%20region%20growing%20algorithm%20to%20find%20nearby%20road%0Apoints.%20The%20extracted%20road%20points%20are%20then%20smoothed%20with%20the%20Savitzky-Golay%0Afilter%20to%20produce%20the%20final%20centreline.%20Our%20initial%20approach%20without%0Apost-processing%20of%20road%20skeleton%20achieved%2067%25%20in%20IoU%20by%20testing%20on%20the%20Perth%0ACBD%20dataset%20with%20different%20road%20types.%20Incorporating%20the%20post-processing%20of%20the%0Aroad%20skeleton%20improved%20the%20extraction%20of%20road%20points%20around%20the%20smoothed%0Askeleton.%20The%20refined%20approach%20achieved%20a%20higher%20IoU%20value%20of%2073%25%20and%20with%2023%25%0Areduction%20in%20the%20processing%20time.%20Our%20approach%20offers%20a%20generalised%20and%0Acomputationally%20efficient%20solution%20that%20combines%203D%20and%202D%20processing%0Atechniques%2C%20laying%20the%20groundwork%20for%20future%20road%20reconstruction%20and%203D-to-2D%0Apoint%20cloud%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Road%2520Extraction%2520and%2520Centreline%2520Fitting%2520in%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DXinyu%2520Wang%2520and%2520Muhammad%2520Ibrahim%2520and%2520Atif%2520Mansoor%2520and%2520Hasnein%2520Tareque%2520and%2520Ajmal%2520Mian%26entry.1292438233%3D%2520%2520Road%2520information%2520extraction%2520from%25203D%2520point%2520clouds%2520is%2520useful%2520for%2520urban%2520planning%250Aand%2520traffic%2520management.%2520Existing%2520methods%2520often%2520rely%2520on%2520local%2520features%2520and%2520the%250Arefraction%2520angle%2520of%2520lasers%2520from%2520kerbs%252C%2520which%2520makes%2520them%2520sensitive%2520to%2520variable%250Akerb%2520designs%2520and%2520issues%2520in%2520high-density%2520areas%2520due%2520to%2520data%2520homogeneity.%2520We%250Apropose%2520an%2520approach%2520for%2520extracting%2520road%2520points%2520and%2520fitting%2520centrelines%2520using%2520a%250Atop-down%2520view%2520of%2520LiDAR%2520based%2520ground-collected%2520point%2520clouds.%2520This%2520prospective%250Aview%2520reduces%2520reliance%2520on%2520specific%2520kerb%2520design%2520and%2520results%2520in%2520better%2520road%250Aextraction.%2520We%2520first%2520perform%2520statistical%2520outlier%2520removal%2520and%2520density-based%250Aclustering%2520to%2520reduce%2520noise%2520from%25203D%2520point%2520cloud%2520data.%2520Next%252C%2520we%2520perform%2520ground%250Apoint%2520filtering%2520using%2520a%2520grid-based%2520segmentation%2520method%2520that%2520adapts%2520to%2520diverse%250Aroad%2520scenarios%2520and%2520terrain%2520characteristics.%2520The%2520filtered%2520points%2520are%2520then%250Aprojected%2520onto%2520a%25202D%2520plane%252C%2520and%2520the%2520road%2520is%2520extracted%2520by%2520a%2520skeletonisation%250Aalgorithm.%2520The%2520skeleton%2520is%2520back-projected%2520onto%2520the%25203D%2520point%2520cloud%2520with%250Acalculated%2520normals%252C%2520which%2520guide%2520a%2520region%2520growing%2520algorithm%2520to%2520find%2520nearby%2520road%250Apoints.%2520The%2520extracted%2520road%2520points%2520are%2520then%2520smoothed%2520with%2520the%2520Savitzky-Golay%250Afilter%2520to%2520produce%2520the%2520final%2520centreline.%2520Our%2520initial%2520approach%2520without%250Apost-processing%2520of%2520road%2520skeleton%2520achieved%252067%2525%2520in%2520IoU%2520by%2520testing%2520on%2520the%2520Perth%250ACBD%2520dataset%2520with%2520different%2520road%2520types.%2520Incorporating%2520the%2520post-processing%2520of%2520the%250Aroad%2520skeleton%2520improved%2520the%2520extraction%2520of%2520road%2520points%2520around%2520the%2520smoothed%250Askeleton.%2520The%2520refined%2520approach%2520achieved%2520a%2520higher%2520IoU%2520value%2520of%252073%2525%2520and%2520with%252023%2525%250Areduction%2520in%2520the%2520processing%2520time.%2520Our%2520approach%2520offers%2520a%2520generalised%2520and%250Acomputationally%2520efficient%2520solution%2520that%2520combines%25203D%2520and%25202D%2520processing%250Atechniques%252C%2520laying%2520the%2520groundwork%2520for%2520future%2520road%2520reconstruction%2520and%25203D-to-2D%250Apoint%2520cloud%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Road%20Extraction%20and%20Centreline%20Fitting%20in%20LiDAR%20Point%20Clouds&entry.906535625=Xinyu%20Wang%20and%20Muhammad%20Ibrahim%20and%20Atif%20Mansoor%20and%20Hasnein%20Tareque%20and%20Ajmal%20Mian&entry.1292438233=%20%20Road%20information%20extraction%20from%203D%20point%20clouds%20is%20useful%20for%20urban%20planning%0Aand%20traffic%20management.%20Existing%20methods%20often%20rely%20on%20local%20features%20and%20the%0Arefraction%20angle%20of%20lasers%20from%20kerbs%2C%20which%20makes%20them%20sensitive%20to%20variable%0Akerb%20designs%20and%20issues%20in%20high-density%20areas%20due%20to%20data%20homogeneity.%20We%0Apropose%20an%20approach%20for%20extracting%20road%20points%20and%20fitting%20centrelines%20using%20a%0Atop-down%20view%20of%20LiDAR%20based%20ground-collected%20point%20clouds.%20This%20prospective%0Aview%20reduces%20reliance%20on%20specific%20kerb%20design%20and%20results%20in%20better%20road%0Aextraction.%20We%20first%20perform%20statistical%20outlier%20removal%20and%20density-based%0Aclustering%20to%20reduce%20noise%20from%203D%20point%20cloud%20data.%20Next%2C%20we%20perform%20ground%0Apoint%20filtering%20using%20a%20grid-based%20segmentation%20method%20that%20adapts%20to%20diverse%0Aroad%20scenarios%20and%20terrain%20characteristics.%20The%20filtered%20points%20are%20then%0Aprojected%20onto%20a%202D%20plane%2C%20and%20the%20road%20is%20extracted%20by%20a%20skeletonisation%0Aalgorithm.%20The%20skeleton%20is%20back-projected%20onto%20the%203D%20point%20cloud%20with%0Acalculated%20normals%2C%20which%20guide%20a%20region%20growing%20algorithm%20to%20find%20nearby%20road%0Apoints.%20The%20extracted%20road%20points%20are%20then%20smoothed%20with%20the%20Savitzky-Golay%0Afilter%20to%20produce%20the%20final%20centreline.%20Our%20initial%20approach%20without%0Apost-processing%20of%20road%20skeleton%20achieved%2067%25%20in%20IoU%20by%20testing%20on%20the%20Perth%0ACBD%20dataset%20with%20different%20road%20types.%20Incorporating%20the%20post-processing%20of%20the%0Aroad%20skeleton%20improved%20the%20extraction%20of%20road%20points%20around%20the%20smoothed%0Askeleton.%20The%20refined%20approach%20achieved%20a%20higher%20IoU%20value%20of%2073%25%20and%20with%2023%25%0Areduction%20in%20the%20processing%20time.%20Our%20approach%20offers%20a%20generalised%20and%0Acomputationally%20efficient%20solution%20that%20combines%203D%20and%202D%20processing%0Atechniques%2C%20laying%20the%20groundwork%20for%20future%20road%20reconstruction%20and%203D-to-2D%0Apoint%20cloud%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07486v1&entry.124074799=Read"},
{"title": "Enhance-A-Video: Better Generated Video for Free", "author": "Yang Luo and Xuanlei Zhao and Mengzhao Chen and Kaipeng Zhang and Wenqi Shao and Kai Wang and Zhangyang Wang and Yang You", "abstract": "  DiT-based video generation has achieved remarkable results, but research into\nenhancing existing models remains relatively unexplored. In this work, we\nintroduce a training-free approach to enhance the coherence and quality of\nDiT-based generated videos, named Enhance-A-Video. The core idea is enhancing\nthe cross-frame correlations based on non-diagonal temporal attention\ndistributions. Thanks to its simple design, our approach can be easily applied\nto most DiT-based video generation frameworks without any retraining or\nfine-tuning. Across various DiT-based video generation models, our approach\ndemonstrates promising improvements in both temporal consistency and visual\nquality. We hope this research can inspire future explorations in video\ngeneration enhancement.\n", "link": "http://arxiv.org/abs/2502.07508v1", "date": "2025-02-11", "relevancy": 2.6363, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6676}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6601}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhance-A-Video%3A%20Better%20Generated%20Video%20for%20Free&body=Title%3A%20Enhance-A-Video%3A%20Better%20Generated%20Video%20for%20Free%0AAuthor%3A%20Yang%20Luo%20and%20Xuanlei%20Zhao%20and%20Mengzhao%20Chen%20and%20Kaipeng%20Zhang%20and%20Wenqi%20Shao%20and%20Kai%20Wang%20and%20Zhangyang%20Wang%20and%20Yang%20You%0AAbstract%3A%20%20%20DiT-based%20video%20generation%20has%20achieved%20remarkable%20results%2C%20but%20research%20into%0Aenhancing%20existing%20models%20remains%20relatively%20unexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20training-free%20approach%20to%20enhance%20the%20coherence%20and%20quality%20of%0ADiT-based%20generated%20videos%2C%20named%20Enhance-A-Video.%20The%20core%20idea%20is%20enhancing%0Athe%20cross-frame%20correlations%20based%20on%20non-diagonal%20temporal%20attention%0Adistributions.%20Thanks%20to%20its%20simple%20design%2C%20our%20approach%20can%20be%20easily%20applied%0Ato%20most%20DiT-based%20video%20generation%20frameworks%20without%20any%20retraining%20or%0Afine-tuning.%20Across%20various%20DiT-based%20video%20generation%20models%2C%20our%20approach%0Ademonstrates%20promising%20improvements%20in%20both%20temporal%20consistency%20and%20visual%0Aquality.%20We%20hope%20this%20research%20can%20inspire%20future%20explorations%20in%20video%0Ageneration%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhance-A-Video%253A%2520Better%2520Generated%2520Video%2520for%2520Free%26entry.906535625%3DYang%2520Luo%2520and%2520Xuanlei%2520Zhao%2520and%2520Mengzhao%2520Chen%2520and%2520Kaipeng%2520Zhang%2520and%2520Wenqi%2520Shao%2520and%2520Kai%2520Wang%2520and%2520Zhangyang%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520DiT-based%2520video%2520generation%2520has%2520achieved%2520remarkable%2520results%252C%2520but%2520research%2520into%250Aenhancing%2520existing%2520models%2520remains%2520relatively%2520unexplored.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520training-free%2520approach%2520to%2520enhance%2520the%2520coherence%2520and%2520quality%2520of%250ADiT-based%2520generated%2520videos%252C%2520named%2520Enhance-A-Video.%2520The%2520core%2520idea%2520is%2520enhancing%250Athe%2520cross-frame%2520correlations%2520based%2520on%2520non-diagonal%2520temporal%2520attention%250Adistributions.%2520Thanks%2520to%2520its%2520simple%2520design%252C%2520our%2520approach%2520can%2520be%2520easily%2520applied%250Ato%2520most%2520DiT-based%2520video%2520generation%2520frameworks%2520without%2520any%2520retraining%2520or%250Afine-tuning.%2520Across%2520various%2520DiT-based%2520video%2520generation%2520models%252C%2520our%2520approach%250Ademonstrates%2520promising%2520improvements%2520in%2520both%2520temporal%2520consistency%2520and%2520visual%250Aquality.%2520We%2520hope%2520this%2520research%2520can%2520inspire%2520future%2520explorations%2520in%2520video%250Ageneration%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhance-A-Video%3A%20Better%20Generated%20Video%20for%20Free&entry.906535625=Yang%20Luo%20and%20Xuanlei%20Zhao%20and%20Mengzhao%20Chen%20and%20Kaipeng%20Zhang%20and%20Wenqi%20Shao%20and%20Kai%20Wang%20and%20Zhangyang%20Wang%20and%20Yang%20You&entry.1292438233=%20%20DiT-based%20video%20generation%20has%20achieved%20remarkable%20results%2C%20but%20research%20into%0Aenhancing%20existing%20models%20remains%20relatively%20unexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20training-free%20approach%20to%20enhance%20the%20coherence%20and%20quality%20of%0ADiT-based%20generated%20videos%2C%20named%20Enhance-A-Video.%20The%20core%20idea%20is%20enhancing%0Athe%20cross-frame%20correlations%20based%20on%20non-diagonal%20temporal%20attention%0Adistributions.%20Thanks%20to%20its%20simple%20design%2C%20our%20approach%20can%20be%20easily%20applied%0Ato%20most%20DiT-based%20video%20generation%20frameworks%20without%20any%20retraining%20or%0Afine-tuning.%20Across%20various%20DiT-based%20video%20generation%20models%2C%20our%20approach%0Ademonstrates%20promising%20improvements%20in%20both%20temporal%20consistency%20and%20visual%0Aquality.%20We%20hope%20this%20research%20can%20inspire%20future%20explorations%20in%20video%0Ageneration%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07508v1&entry.124074799=Read"},
{"title": "Generalized Least Squares Kernelized Tensor Factorization", "author": "Mengying Lei and Lijun Sun", "abstract": "  Completing multidimensional tensor-structured data with missing entries is a\nfundamental task for many real-world applications involving incomplete or\ncorrupted datasets. For data with spatial or temporal side information,\nlow-rank factorization models with smoothness constraints have demonstrated\nstrong performance. Although effective at capturing global and long-range\ncorrelations, these models often struggle to capture short-scale,\nhigh-frequency variations in the data. To address this limitation, we propose\nthe Generalized Least Squares Kernelized Tensor Factorization (GLSKF) framework\nfor tensor completion. GLSKF integrates smoothness-constrained low-rank\nfactorization with a locally correlated residual process; the resulting\nadditive structure enables effective characterization of both global\ndependencies and local variations. Specifically, we define the covariance norm\nto enforce the smoothness of factor matrices in the global low-rank\nfactorization, and use structured covariance/kernel functions to model the\nlocal processes. For model estimation, we develop an alternating least squares\n(ALS) procedure with closed-form solutions for each subproblem. GLSKF utilizes\nzero-padding and slicing operations based on projection matrices which preserve\nthe Kronecker structure of covariances, facilitating efficient computations\nthrough the conjugate gradient (CG) method. The proposed framework is evaluated\non four real-world datasets across diverse tasks. Experimental results\ndemonstrate that GLSKF achieves superior performance and scalability,\nestablishing it as a novel solution for multidimensional tensor completion.\n", "link": "http://arxiv.org/abs/2412.07041v3", "date": "2025-02-11", "relevancy": 2.6068, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.532}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5287}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Least%20Squares%20Kernelized%20Tensor%20Factorization&body=Title%3A%20Generalized%20Least%20Squares%20Kernelized%20Tensor%20Factorization%0AAuthor%3A%20Mengying%20Lei%20and%20Lijun%20Sun%0AAbstract%3A%20%20%20Completing%20multidimensional%20tensor-structured%20data%20with%20missing%20entries%20is%20a%0Afundamental%20task%20for%20many%20real-world%20applications%20involving%20incomplete%20or%0Acorrupted%20datasets.%20For%20data%20with%20spatial%20or%20temporal%20side%20information%2C%0Alow-rank%20factorization%20models%20with%20smoothness%20constraints%20have%20demonstrated%0Astrong%20performance.%20Although%20effective%20at%20capturing%20global%20and%20long-range%0Acorrelations%2C%20these%20models%20often%20struggle%20to%20capture%20short-scale%2C%0Ahigh-frequency%20variations%20in%20the%20data.%20To%20address%20this%20limitation%2C%20we%20propose%0Athe%20Generalized%20Least%20Squares%20Kernelized%20Tensor%20Factorization%20%28GLSKF%29%20framework%0Afor%20tensor%20completion.%20GLSKF%20integrates%20smoothness-constrained%20low-rank%0Afactorization%20with%20a%20locally%20correlated%20residual%20process%3B%20the%20resulting%0Aadditive%20structure%20enables%20effective%20characterization%20of%20both%20global%0Adependencies%20and%20local%20variations.%20Specifically%2C%20we%20define%20the%20covariance%20norm%0Ato%20enforce%20the%20smoothness%20of%20factor%20matrices%20in%20the%20global%20low-rank%0Afactorization%2C%20and%20use%20structured%20covariance/kernel%20functions%20to%20model%20the%0Alocal%20processes.%20For%20model%20estimation%2C%20we%20develop%20an%20alternating%20least%20squares%0A%28ALS%29%20procedure%20with%20closed-form%20solutions%20for%20each%20subproblem.%20GLSKF%20utilizes%0Azero-padding%20and%20slicing%20operations%20based%20on%20projection%20matrices%20which%20preserve%0Athe%20Kronecker%20structure%20of%20covariances%2C%20facilitating%20efficient%20computations%0Athrough%20the%20conjugate%20gradient%20%28CG%29%20method.%20The%20proposed%20framework%20is%20evaluated%0Aon%20four%20real-world%20datasets%20across%20diverse%20tasks.%20Experimental%20results%0Ademonstrate%20that%20GLSKF%20achieves%20superior%20performance%20and%20scalability%2C%0Aestablishing%20it%20as%20a%20novel%20solution%20for%20multidimensional%20tensor%20completion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07041v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Least%2520Squares%2520Kernelized%2520Tensor%2520Factorization%26entry.906535625%3DMengying%2520Lei%2520and%2520Lijun%2520Sun%26entry.1292438233%3D%2520%2520Completing%2520multidimensional%2520tensor-structured%2520data%2520with%2520missing%2520entries%2520is%2520a%250Afundamental%2520task%2520for%2520many%2520real-world%2520applications%2520involving%2520incomplete%2520or%250Acorrupted%2520datasets.%2520For%2520data%2520with%2520spatial%2520or%2520temporal%2520side%2520information%252C%250Alow-rank%2520factorization%2520models%2520with%2520smoothness%2520constraints%2520have%2520demonstrated%250Astrong%2520performance.%2520Although%2520effective%2520at%2520capturing%2520global%2520and%2520long-range%250Acorrelations%252C%2520these%2520models%2520often%2520struggle%2520to%2520capture%2520short-scale%252C%250Ahigh-frequency%2520variations%2520in%2520the%2520data.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250Athe%2520Generalized%2520Least%2520Squares%2520Kernelized%2520Tensor%2520Factorization%2520%2528GLSKF%2529%2520framework%250Afor%2520tensor%2520completion.%2520GLSKF%2520integrates%2520smoothness-constrained%2520low-rank%250Afactorization%2520with%2520a%2520locally%2520correlated%2520residual%2520process%253B%2520the%2520resulting%250Aadditive%2520structure%2520enables%2520effective%2520characterization%2520of%2520both%2520global%250Adependencies%2520and%2520local%2520variations.%2520Specifically%252C%2520we%2520define%2520the%2520covariance%2520norm%250Ato%2520enforce%2520the%2520smoothness%2520of%2520factor%2520matrices%2520in%2520the%2520global%2520low-rank%250Afactorization%252C%2520and%2520use%2520structured%2520covariance/kernel%2520functions%2520to%2520model%2520the%250Alocal%2520processes.%2520For%2520model%2520estimation%252C%2520we%2520develop%2520an%2520alternating%2520least%2520squares%250A%2528ALS%2529%2520procedure%2520with%2520closed-form%2520solutions%2520for%2520each%2520subproblem.%2520GLSKF%2520utilizes%250Azero-padding%2520and%2520slicing%2520operations%2520based%2520on%2520projection%2520matrices%2520which%2520preserve%250Athe%2520Kronecker%2520structure%2520of%2520covariances%252C%2520facilitating%2520efficient%2520computations%250Athrough%2520the%2520conjugate%2520gradient%2520%2528CG%2529%2520method.%2520The%2520proposed%2520framework%2520is%2520evaluated%250Aon%2520four%2520real-world%2520datasets%2520across%2520diverse%2520tasks.%2520Experimental%2520results%250Ademonstrate%2520that%2520GLSKF%2520achieves%2520superior%2520performance%2520and%2520scalability%252C%250Aestablishing%2520it%2520as%2520a%2520novel%2520solution%2520for%2520multidimensional%2520tensor%2520completion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07041v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Least%20Squares%20Kernelized%20Tensor%20Factorization&entry.906535625=Mengying%20Lei%20and%20Lijun%20Sun&entry.1292438233=%20%20Completing%20multidimensional%20tensor-structured%20data%20with%20missing%20entries%20is%20a%0Afundamental%20task%20for%20many%20real-world%20applications%20involving%20incomplete%20or%0Acorrupted%20datasets.%20For%20data%20with%20spatial%20or%20temporal%20side%20information%2C%0Alow-rank%20factorization%20models%20with%20smoothness%20constraints%20have%20demonstrated%0Astrong%20performance.%20Although%20effective%20at%20capturing%20global%20and%20long-range%0Acorrelations%2C%20these%20models%20often%20struggle%20to%20capture%20short-scale%2C%0Ahigh-frequency%20variations%20in%20the%20data.%20To%20address%20this%20limitation%2C%20we%20propose%0Athe%20Generalized%20Least%20Squares%20Kernelized%20Tensor%20Factorization%20%28GLSKF%29%20framework%0Afor%20tensor%20completion.%20GLSKF%20integrates%20smoothness-constrained%20low-rank%0Afactorization%20with%20a%20locally%20correlated%20residual%20process%3B%20the%20resulting%0Aadditive%20structure%20enables%20effective%20characterization%20of%20both%20global%0Adependencies%20and%20local%20variations.%20Specifically%2C%20we%20define%20the%20covariance%20norm%0Ato%20enforce%20the%20smoothness%20of%20factor%20matrices%20in%20the%20global%20low-rank%0Afactorization%2C%20and%20use%20structured%20covariance/kernel%20functions%20to%20model%20the%0Alocal%20processes.%20For%20model%20estimation%2C%20we%20develop%20an%20alternating%20least%20squares%0A%28ALS%29%20procedure%20with%20closed-form%20solutions%20for%20each%20subproblem.%20GLSKF%20utilizes%0Azero-padding%20and%20slicing%20operations%20based%20on%20projection%20matrices%20which%20preserve%0Athe%20Kronecker%20structure%20of%20covariances%2C%20facilitating%20efficient%20computations%0Athrough%20the%20conjugate%20gradient%20%28CG%29%20method.%20The%20proposed%20framework%20is%20evaluated%0Aon%20four%20real-world%20datasets%20across%20diverse%20tasks.%20Experimental%20results%0Ademonstrate%20that%20GLSKF%20achieves%20superior%20performance%20and%20scalability%2C%0Aestablishing%20it%20as%20a%20novel%20solution%20for%20multidimensional%20tensor%20completion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07041v3&entry.124074799=Read"},
{"title": "ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural\n  Projection", "author": "Giacomo Lastrucci and Artur M. Schweidtmann", "abstract": "  Ensuring neural networks adhere to domain-specific constraints is crucial for\naddressing safety and ethical concerns while also enhancing prediction\naccuracy. Despite the nonlinear nature of most real-world tasks, existing\nmethods are predominantly limited to affine or convex constraints. We introduce\nENFORCE, a neural network architecture that guarantees predictions to satisfy\nnonlinear constraints exactly. ENFORCE is trained with standard unconstrained\ngradient-based optimizers (e.g., Adam) and leverages autodifferentiation and\nlocal neural projections to enforce any $\\mathcal{C}^1$ constraint to arbitrary\ntolerance $\\epsilon$. We build an adaptive-depth neural projection (AdaNP)\nmodule that dynamically adjusts its complexity to suit the specific problem and\nthe required tolerance levels. ENFORCE guarantees satisfaction of equality\nconstraints that are nonlinear in both inputs and outputs of the neural network\nwith minimal (and adjustable) computational cost.\n", "link": "http://arxiv.org/abs/2502.06774v2", "date": "2025-02-11", "relevancy": 2.5883, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4987}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENFORCE%3A%20Exact%20Nonlinear%20Constrained%20Learning%20with%20Adaptive-depth%20Neural%0A%20%20Projection&body=Title%3A%20ENFORCE%3A%20Exact%20Nonlinear%20Constrained%20Learning%20with%20Adaptive-depth%20Neural%0A%20%20Projection%0AAuthor%3A%20Giacomo%20Lastrucci%20and%20Artur%20M.%20Schweidtmann%0AAbstract%3A%20%20%20Ensuring%20neural%20networks%20adhere%20to%20domain-specific%20constraints%20is%20crucial%20for%0Aaddressing%20safety%20and%20ethical%20concerns%20while%20also%20enhancing%20prediction%0Aaccuracy.%20Despite%20the%20nonlinear%20nature%20of%20most%20real-world%20tasks%2C%20existing%0Amethods%20are%20predominantly%20limited%20to%20affine%20or%20convex%20constraints.%20We%20introduce%0AENFORCE%2C%20a%20neural%20network%20architecture%20that%20guarantees%20predictions%20to%20satisfy%0Anonlinear%20constraints%20exactly.%20ENFORCE%20is%20trained%20with%20standard%20unconstrained%0Agradient-based%20optimizers%20%28e.g.%2C%20Adam%29%20and%20leverages%20autodifferentiation%20and%0Alocal%20neural%20projections%20to%20enforce%20any%20%24%5Cmathcal%7BC%7D%5E1%24%20constraint%20to%20arbitrary%0Atolerance%20%24%5Cepsilon%24.%20We%20build%20an%20adaptive-depth%20neural%20projection%20%28AdaNP%29%0Amodule%20that%20dynamically%20adjusts%20its%20complexity%20to%20suit%20the%20specific%20problem%20and%0Athe%20required%20tolerance%20levels.%20ENFORCE%20guarantees%20satisfaction%20of%20equality%0Aconstraints%20that%20are%20nonlinear%20in%20both%20inputs%20and%20outputs%20of%20the%20neural%20network%0Awith%20minimal%20%28and%20adjustable%29%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENFORCE%253A%2520Exact%2520Nonlinear%2520Constrained%2520Learning%2520with%2520Adaptive-depth%2520Neural%250A%2520%2520Projection%26entry.906535625%3DGiacomo%2520Lastrucci%2520and%2520Artur%2520M.%2520Schweidtmann%26entry.1292438233%3D%2520%2520Ensuring%2520neural%2520networks%2520adhere%2520to%2520domain-specific%2520constraints%2520is%2520crucial%2520for%250Aaddressing%2520safety%2520and%2520ethical%2520concerns%2520while%2520also%2520enhancing%2520prediction%250Aaccuracy.%2520Despite%2520the%2520nonlinear%2520nature%2520of%2520most%2520real-world%2520tasks%252C%2520existing%250Amethods%2520are%2520predominantly%2520limited%2520to%2520affine%2520or%2520convex%2520constraints.%2520We%2520introduce%250AENFORCE%252C%2520a%2520neural%2520network%2520architecture%2520that%2520guarantees%2520predictions%2520to%2520satisfy%250Anonlinear%2520constraints%2520exactly.%2520ENFORCE%2520is%2520trained%2520with%2520standard%2520unconstrained%250Agradient-based%2520optimizers%2520%2528e.g.%252C%2520Adam%2529%2520and%2520leverages%2520autodifferentiation%2520and%250Alocal%2520neural%2520projections%2520to%2520enforce%2520any%2520%2524%255Cmathcal%257BC%257D%255E1%2524%2520constraint%2520to%2520arbitrary%250Atolerance%2520%2524%255Cepsilon%2524.%2520We%2520build%2520an%2520adaptive-depth%2520neural%2520projection%2520%2528AdaNP%2529%250Amodule%2520that%2520dynamically%2520adjusts%2520its%2520complexity%2520to%2520suit%2520the%2520specific%2520problem%2520and%250Athe%2520required%2520tolerance%2520levels.%2520ENFORCE%2520guarantees%2520satisfaction%2520of%2520equality%250Aconstraints%2520that%2520are%2520nonlinear%2520in%2520both%2520inputs%2520and%2520outputs%2520of%2520the%2520neural%2520network%250Awith%2520minimal%2520%2528and%2520adjustable%2529%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENFORCE%3A%20Exact%20Nonlinear%20Constrained%20Learning%20with%20Adaptive-depth%20Neural%0A%20%20Projection&entry.906535625=Giacomo%20Lastrucci%20and%20Artur%20M.%20Schweidtmann&entry.1292438233=%20%20Ensuring%20neural%20networks%20adhere%20to%20domain-specific%20constraints%20is%20crucial%20for%0Aaddressing%20safety%20and%20ethical%20concerns%20while%20also%20enhancing%20prediction%0Aaccuracy.%20Despite%20the%20nonlinear%20nature%20of%20most%20real-world%20tasks%2C%20existing%0Amethods%20are%20predominantly%20limited%20to%20affine%20or%20convex%20constraints.%20We%20introduce%0AENFORCE%2C%20a%20neural%20network%20architecture%20that%20guarantees%20predictions%20to%20satisfy%0Anonlinear%20constraints%20exactly.%20ENFORCE%20is%20trained%20with%20standard%20unconstrained%0Agradient-based%20optimizers%20%28e.g.%2C%20Adam%29%20and%20leverages%20autodifferentiation%20and%0Alocal%20neural%20projections%20to%20enforce%20any%20%24%5Cmathcal%7BC%7D%5E1%24%20constraint%20to%20arbitrary%0Atolerance%20%24%5Cepsilon%24.%20We%20build%20an%20adaptive-depth%20neural%20projection%20%28AdaNP%29%0Amodule%20that%20dynamically%20adjusts%20its%20complexity%20to%20suit%20the%20specific%20problem%20and%0Athe%20required%20tolerance%20levels.%20ENFORCE%20guarantees%20satisfaction%20of%20equality%0Aconstraints%20that%20are%20nonlinear%20in%20both%20inputs%20and%20outputs%20of%20the%20neural%20network%0Awith%20minimal%20%28and%20adjustable%29%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06774v2&entry.124074799=Read"},
{"title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation", "author": "Sixiao Zheng and Zimian Peng and Yanpeng Zhou and Yi Zhu and Hang Xu and Xiangru Huang and Yanwei Fu", "abstract": "  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.\n", "link": "http://arxiv.org/abs/2502.07531v1", "date": "2025-02-11", "relevancy": 2.5819, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.688}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6473}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidCRAFT3%3A%20Camera%2C%20Object%2C%20and%20Lighting%20Control%20for%20Image-to-Video%0A%20%20Generation&body=Title%3A%20VidCRAFT3%3A%20Camera%2C%20Object%2C%20and%20Lighting%20Control%20for%20Image-to-Video%0A%20%20Generation%0AAuthor%3A%20Sixiao%20Zheng%20and%20Zimian%20Peng%20and%20Yanpeng%20Zhou%20and%20Yi%20Zhu%20and%20Hang%20Xu%20and%20Xiangru%20Huang%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Recent%20image-to-video%20generation%20methods%20have%20demonstrated%20success%20in%0Aenabling%20control%20over%20one%20or%20two%20visual%20elements%2C%20such%20as%20camera%20trajectory%20or%0Aobject%20motion.%20However%2C%20these%20methods%20are%20unable%20to%20offer%20control%20over%20multiple%0Avisual%20elements%20due%20to%20limitations%20in%20data%20and%20network%20efficacy.%20In%20this%20paper%2C%0Awe%20introduce%20VidCRAFT3%2C%20a%20novel%20framework%20for%20precise%20image-to-video%20generation%0Athat%20enables%20control%20over%20camera%20motion%2C%20object%20motion%2C%20and%20lighting%20direction%0Asimultaneously.%20To%20better%20decouple%20control%20over%20each%20visual%20element%2C%20we%20propose%0Athe%20Spatial%20Triple-Attention%20Transformer%2C%20which%20integrates%20lighting%20direction%2C%0Atext%2C%20and%20image%20in%20a%20symmetric%20way.%20Since%20most%20real-world%20video%20datasets%20lack%0Alighting%20annotations%2C%20we%20construct%20a%20high-quality%20synthetic%20video%20dataset%2C%20the%0AVideoLightingDirection%20%28VLD%29%20dataset.%20This%20dataset%20includes%20lighting%20direction%0Aannotations%20and%20objects%20of%20diverse%20appearance%2C%20enabling%20VidCRAFT3%20to%0Aeffectively%20handle%20strong%20light%20transmission%20and%20reflection%20effects.%0AAdditionally%2C%20we%20propose%20a%20three-stage%20training%20strategy%20that%20eliminates%20the%0Aneed%20for%20training%20data%20annotated%20with%20multiple%20visual%20elements%20%28camera%20motion%2C%0Aobject%20motion%2C%20and%20lighting%20direction%29%20simultaneously.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20the%20efficacy%20of%20VidCRAFT3%20in%20producing%0Ahigh-quality%20video%20content%2C%20surpassing%20existing%20state-of-the-art%20methods%20in%0Aterms%20of%20control%20granularity%20and%20visual%20coherence.%20All%20code%20and%20data%20will%20be%0Apublicly%20available.%20Project%20page%3A%20https%3A//sixiaozheng.github.io/VidCRAFT3/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidCRAFT3%253A%2520Camera%252C%2520Object%252C%2520and%2520Lighting%2520Control%2520for%2520Image-to-Video%250A%2520%2520Generation%26entry.906535625%3DSixiao%2520Zheng%2520and%2520Zimian%2520Peng%2520and%2520Yanpeng%2520Zhou%2520and%2520Yi%2520Zhu%2520and%2520Hang%2520Xu%2520and%2520Xiangru%2520Huang%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Recent%2520image-to-video%2520generation%2520methods%2520have%2520demonstrated%2520success%2520in%250Aenabling%2520control%2520over%2520one%2520or%2520two%2520visual%2520elements%252C%2520such%2520as%2520camera%2520trajectory%2520or%250Aobject%2520motion.%2520However%252C%2520these%2520methods%2520are%2520unable%2520to%2520offer%2520control%2520over%2520multiple%250Avisual%2520elements%2520due%2520to%2520limitations%2520in%2520data%2520and%2520network%2520efficacy.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520VidCRAFT3%252C%2520a%2520novel%2520framework%2520for%2520precise%2520image-to-video%2520generation%250Athat%2520enables%2520control%2520over%2520camera%2520motion%252C%2520object%2520motion%252C%2520and%2520lighting%2520direction%250Asimultaneously.%2520To%2520better%2520decouple%2520control%2520over%2520each%2520visual%2520element%252C%2520we%2520propose%250Athe%2520Spatial%2520Triple-Attention%2520Transformer%252C%2520which%2520integrates%2520lighting%2520direction%252C%250Atext%252C%2520and%2520image%2520in%2520a%2520symmetric%2520way.%2520Since%2520most%2520real-world%2520video%2520datasets%2520lack%250Alighting%2520annotations%252C%2520we%2520construct%2520a%2520high-quality%2520synthetic%2520video%2520dataset%252C%2520the%250AVideoLightingDirection%2520%2528VLD%2529%2520dataset.%2520This%2520dataset%2520includes%2520lighting%2520direction%250Aannotations%2520and%2520objects%2520of%2520diverse%2520appearance%252C%2520enabling%2520VidCRAFT3%2520to%250Aeffectively%2520handle%2520strong%2520light%2520transmission%2520and%2520reflection%2520effects.%250AAdditionally%252C%2520we%2520propose%2520a%2520three-stage%2520training%2520strategy%2520that%2520eliminates%2520the%250Aneed%2520for%2520training%2520data%2520annotated%2520with%2520multiple%2520visual%2520elements%2520%2528camera%2520motion%252C%250Aobject%2520motion%252C%2520and%2520lighting%2520direction%2529%2520simultaneously.%2520Extensive%2520experiments%2520on%250Abenchmark%2520datasets%2520demonstrate%2520the%2520efficacy%2520of%2520VidCRAFT3%2520in%2520producing%250Ahigh-quality%2520video%2520content%252C%2520surpassing%2520existing%2520state-of-the-art%2520methods%2520in%250Aterms%2520of%2520control%2520granularity%2520and%2520visual%2520coherence.%2520All%2520code%2520and%2520data%2520will%2520be%250Apublicly%2520available.%2520Project%2520page%253A%2520https%253A//sixiaozheng.github.io/VidCRAFT3/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidCRAFT3%3A%20Camera%2C%20Object%2C%20and%20Lighting%20Control%20for%20Image-to-Video%0A%20%20Generation&entry.906535625=Sixiao%20Zheng%20and%20Zimian%20Peng%20and%20Yanpeng%20Zhou%20and%20Yi%20Zhu%20and%20Hang%20Xu%20and%20Xiangru%20Huang%20and%20Yanwei%20Fu&entry.1292438233=%20%20Recent%20image-to-video%20generation%20methods%20have%20demonstrated%20success%20in%0Aenabling%20control%20over%20one%20or%20two%20visual%20elements%2C%20such%20as%20camera%20trajectory%20or%0Aobject%20motion.%20However%2C%20these%20methods%20are%20unable%20to%20offer%20control%20over%20multiple%0Avisual%20elements%20due%20to%20limitations%20in%20data%20and%20network%20efficacy.%20In%20this%20paper%2C%0Awe%20introduce%20VidCRAFT3%2C%20a%20novel%20framework%20for%20precise%20image-to-video%20generation%0Athat%20enables%20control%20over%20camera%20motion%2C%20object%20motion%2C%20and%20lighting%20direction%0Asimultaneously.%20To%20better%20decouple%20control%20over%20each%20visual%20element%2C%20we%20propose%0Athe%20Spatial%20Triple-Attention%20Transformer%2C%20which%20integrates%20lighting%20direction%2C%0Atext%2C%20and%20image%20in%20a%20symmetric%20way.%20Since%20most%20real-world%20video%20datasets%20lack%0Alighting%20annotations%2C%20we%20construct%20a%20high-quality%20synthetic%20video%20dataset%2C%20the%0AVideoLightingDirection%20%28VLD%29%20dataset.%20This%20dataset%20includes%20lighting%20direction%0Aannotations%20and%20objects%20of%20diverse%20appearance%2C%20enabling%20VidCRAFT3%20to%0Aeffectively%20handle%20strong%20light%20transmission%20and%20reflection%20effects.%0AAdditionally%2C%20we%20propose%20a%20three-stage%20training%20strategy%20that%20eliminates%20the%0Aneed%20for%20training%20data%20annotated%20with%20multiple%20visual%20elements%20%28camera%20motion%2C%0Aobject%20motion%2C%20and%20lighting%20direction%29%20simultaneously.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20the%20efficacy%20of%20VidCRAFT3%20in%20producing%0Ahigh-quality%20video%20content%2C%20surpassing%20existing%20state-of-the-art%20methods%20in%0Aterms%20of%20control%20granularity%20and%20visual%20coherence.%20All%20code%20and%20data%20will%20be%0Apublicly%20available.%20Project%20page%3A%20https%3A//sixiaozheng.github.io/VidCRAFT3/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07531v1&entry.124074799=Read"},
{"title": "Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art", "author": "Neeloy Chakraborty and Melkior Ornik and Katherine Driggs-Campbell", "abstract": "  Autonomous systems are soon to be ubiquitous, spanning manufacturing,\nagriculture, healthcare, entertainment, and other industries. Most of these\nsystems are developed with modular sub-components for decision-making,\nplanning, and control that may be hand-engineered or learning-based. While\nthese approaches perform well under the situations they were specifically\ndesigned for, they can perform especially poorly in out-of-distribution\nscenarios that will undoubtedly arise at test-time. The rise of foundation\nmodels trained on multiple tasks with impressively large datasets has led\nresearchers to believe that these models may provide \"common sense\" reasoning\nthat existing planners are missing, bridging the gap between algorithm\ndevelopment and deployment. While researchers have shown promising results in\ndeploying foundation models to decision-making tasks, these models are known to\nhallucinate and generate decisions that may sound reasonable, but are in fact\npoor. We argue there is a need to step back and simultaneously design systems\nthat can quantify the certainty of a model's decision, and detect when it may\nbe hallucinating. In this work, we discuss the current use cases of foundation\nmodels for decision-making tasks, provide a general definition for\nhallucinations with examples, discuss existing approaches to hallucination\ndetection and mitigation with a focus on decision problems, present guidelines,\nand explore areas for further research in this exciting field.\n", "link": "http://arxiv.org/abs/2403.16527v2", "date": "2025-02-11", "relevancy": 2.5506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallucination%20Detection%20in%20Foundation%20Models%20for%20Decision-Making%3A%20A%0A%20%20Flexible%20Definition%20and%20Review%20of%20the%20State%20of%20the%20Art&body=Title%3A%20Hallucination%20Detection%20in%20Foundation%20Models%20for%20Decision-Making%3A%20A%0A%20%20Flexible%20Definition%20and%20Review%20of%20the%20State%20of%20the%20Art%0AAuthor%3A%20Neeloy%20Chakraborty%20and%20Melkior%20Ornik%20and%20Katherine%20Driggs-Campbell%0AAbstract%3A%20%20%20Autonomous%20systems%20are%20soon%20to%20be%20ubiquitous%2C%20spanning%20manufacturing%2C%0Aagriculture%2C%20healthcare%2C%20entertainment%2C%20and%20other%20industries.%20Most%20of%20these%0Asystems%20are%20developed%20with%20modular%20sub-components%20for%20decision-making%2C%0Aplanning%2C%20and%20control%20that%20may%20be%20hand-engineered%20or%20learning-based.%20While%0Athese%20approaches%20perform%20well%20under%20the%20situations%20they%20were%20specifically%0Adesigned%20for%2C%20they%20can%20perform%20especially%20poorly%20in%20out-of-distribution%0Ascenarios%20that%20will%20undoubtedly%20arise%20at%20test-time.%20The%20rise%20of%20foundation%0Amodels%20trained%20on%20multiple%20tasks%20with%20impressively%20large%20datasets%20has%20led%0Aresearchers%20to%20believe%20that%20these%20models%20may%20provide%20%22common%20sense%22%20reasoning%0Athat%20existing%20planners%20are%20missing%2C%20bridging%20the%20gap%20between%20algorithm%0Adevelopment%20and%20deployment.%20While%20researchers%20have%20shown%20promising%20results%20in%0Adeploying%20foundation%20models%20to%20decision-making%20tasks%2C%20these%20models%20are%20known%20to%0Ahallucinate%20and%20generate%20decisions%20that%20may%20sound%20reasonable%2C%20but%20are%20in%20fact%0Apoor.%20We%20argue%20there%20is%20a%20need%20to%20step%20back%20and%20simultaneously%20design%20systems%0Athat%20can%20quantify%20the%20certainty%20of%20a%20model%27s%20decision%2C%20and%20detect%20when%20it%20may%0Abe%20hallucinating.%20In%20this%20work%2C%20we%20discuss%20the%20current%20use%20cases%20of%20foundation%0Amodels%20for%20decision-making%20tasks%2C%20provide%20a%20general%20definition%20for%0Ahallucinations%20with%20examples%2C%20discuss%20existing%20approaches%20to%20hallucination%0Adetection%20and%20mitigation%20with%20a%20focus%20on%20decision%20problems%2C%20present%20guidelines%2C%0Aand%20explore%20areas%20for%20further%20research%20in%20this%20exciting%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallucination%2520Detection%2520in%2520Foundation%2520Models%2520for%2520Decision-Making%253A%2520A%250A%2520%2520Flexible%2520Definition%2520and%2520Review%2520of%2520the%2520State%2520of%2520the%2520Art%26entry.906535625%3DNeeloy%2520Chakraborty%2520and%2520Melkior%2520Ornik%2520and%2520Katherine%2520Driggs-Campbell%26entry.1292438233%3D%2520%2520Autonomous%2520systems%2520are%2520soon%2520to%2520be%2520ubiquitous%252C%2520spanning%2520manufacturing%252C%250Aagriculture%252C%2520healthcare%252C%2520entertainment%252C%2520and%2520other%2520industries.%2520Most%2520of%2520these%250Asystems%2520are%2520developed%2520with%2520modular%2520sub-components%2520for%2520decision-making%252C%250Aplanning%252C%2520and%2520control%2520that%2520may%2520be%2520hand-engineered%2520or%2520learning-based.%2520While%250Athese%2520approaches%2520perform%2520well%2520under%2520the%2520situations%2520they%2520were%2520specifically%250Adesigned%2520for%252C%2520they%2520can%2520perform%2520especially%2520poorly%2520in%2520out-of-distribution%250Ascenarios%2520that%2520will%2520undoubtedly%2520arise%2520at%2520test-time.%2520The%2520rise%2520of%2520foundation%250Amodels%2520trained%2520on%2520multiple%2520tasks%2520with%2520impressively%2520large%2520datasets%2520has%2520led%250Aresearchers%2520to%2520believe%2520that%2520these%2520models%2520may%2520provide%2520%2522common%2520sense%2522%2520reasoning%250Athat%2520existing%2520planners%2520are%2520missing%252C%2520bridging%2520the%2520gap%2520between%2520algorithm%250Adevelopment%2520and%2520deployment.%2520While%2520researchers%2520have%2520shown%2520promising%2520results%2520in%250Adeploying%2520foundation%2520models%2520to%2520decision-making%2520tasks%252C%2520these%2520models%2520are%2520known%2520to%250Ahallucinate%2520and%2520generate%2520decisions%2520that%2520may%2520sound%2520reasonable%252C%2520but%2520are%2520in%2520fact%250Apoor.%2520We%2520argue%2520there%2520is%2520a%2520need%2520to%2520step%2520back%2520and%2520simultaneously%2520design%2520systems%250Athat%2520can%2520quantify%2520the%2520certainty%2520of%2520a%2520model%2527s%2520decision%252C%2520and%2520detect%2520when%2520it%2520may%250Abe%2520hallucinating.%2520In%2520this%2520work%252C%2520we%2520discuss%2520the%2520current%2520use%2520cases%2520of%2520foundation%250Amodels%2520for%2520decision-making%2520tasks%252C%2520provide%2520a%2520general%2520definition%2520for%250Ahallucinations%2520with%2520examples%252C%2520discuss%2520existing%2520approaches%2520to%2520hallucination%250Adetection%2520and%2520mitigation%2520with%2520a%2520focus%2520on%2520decision%2520problems%252C%2520present%2520guidelines%252C%250Aand%2520explore%2520areas%2520for%2520further%2520research%2520in%2520this%2520exciting%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucination%20Detection%20in%20Foundation%20Models%20for%20Decision-Making%3A%20A%0A%20%20Flexible%20Definition%20and%20Review%20of%20the%20State%20of%20the%20Art&entry.906535625=Neeloy%20Chakraborty%20and%20Melkior%20Ornik%20and%20Katherine%20Driggs-Campbell&entry.1292438233=%20%20Autonomous%20systems%20are%20soon%20to%20be%20ubiquitous%2C%20spanning%20manufacturing%2C%0Aagriculture%2C%20healthcare%2C%20entertainment%2C%20and%20other%20industries.%20Most%20of%20these%0Asystems%20are%20developed%20with%20modular%20sub-components%20for%20decision-making%2C%0Aplanning%2C%20and%20control%20that%20may%20be%20hand-engineered%20or%20learning-based.%20While%0Athese%20approaches%20perform%20well%20under%20the%20situations%20they%20were%20specifically%0Adesigned%20for%2C%20they%20can%20perform%20especially%20poorly%20in%20out-of-distribution%0Ascenarios%20that%20will%20undoubtedly%20arise%20at%20test-time.%20The%20rise%20of%20foundation%0Amodels%20trained%20on%20multiple%20tasks%20with%20impressively%20large%20datasets%20has%20led%0Aresearchers%20to%20believe%20that%20these%20models%20may%20provide%20%22common%20sense%22%20reasoning%0Athat%20existing%20planners%20are%20missing%2C%20bridging%20the%20gap%20between%20algorithm%0Adevelopment%20and%20deployment.%20While%20researchers%20have%20shown%20promising%20results%20in%0Adeploying%20foundation%20models%20to%20decision-making%20tasks%2C%20these%20models%20are%20known%20to%0Ahallucinate%20and%20generate%20decisions%20that%20may%20sound%20reasonable%2C%20but%20are%20in%20fact%0Apoor.%20We%20argue%20there%20is%20a%20need%20to%20step%20back%20and%20simultaneously%20design%20systems%0Athat%20can%20quantify%20the%20certainty%20of%20a%20model%27s%20decision%2C%20and%20detect%20when%20it%20may%0Abe%20hallucinating.%20In%20this%20work%2C%20we%20discuss%20the%20current%20use%20cases%20of%20foundation%0Amodels%20for%20decision-making%20tasks%2C%20provide%20a%20general%20definition%20for%0Ahallucinations%20with%20examples%2C%20discuss%20existing%20approaches%20to%20hallucination%0Adetection%20and%20mitigation%20with%20a%20focus%20on%20decision%20problems%2C%20present%20guidelines%2C%0Aand%20explore%20areas%20for%20further%20research%20in%20this%20exciting%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16527v2&entry.124074799=Read"},
{"title": "Amuro and Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models", "author": "Kaiser Sun and Mark Dredze", "abstract": "  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n", "link": "http://arxiv.org/abs/2408.06663v4", "date": "2025-02-11", "relevancy": 2.5322, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Amuro%20and%20Char%3A%20Analyzing%20the%20Relationship%20between%20Pre-Training%20and%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models&body=Title%3A%20Amuro%20and%20Char%3A%20Analyzing%20the%20Relationship%20between%20Pre-Training%20and%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models%0AAuthor%3A%20Kaiser%20Sun%20and%20Mark%20Dredze%0AAbstract%3A%20%20%20The%20development%20of%20large%20language%20models%20leads%20to%20the%20formation%20of%20a%0Apre-train-then-align%20paradigm%2C%20in%20which%20the%20model%20is%20typically%20pre-trained%20on%20a%0Alarge%20text%20corpus%20and%20undergoes%20a%20tuning%20stage%20to%20align%20the%20model%20with%20human%0Apreference%20or%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%20the%20relationship%0Abetween%20pre-training%20and%20fine-tuning%20by%20fine-tuning%20multiple%20intermediate%0Apre-trained%20model%20checkpoints.%20Our%20results%20on%2018%20datasets%20suggest%20that%20i%29%0Acontinual%20pre-training%20improves%20the%20model%20in%20a%20latent%20way%20that%20unveils%20after%0Afine-tuning%3B%20ii%29%20with%20extra%20fine-tuning%2C%20the%20datasets%20that%20the%20model%20does%20not%0Ademonstrate%20capability%20gain%20much%20more%20than%20those%20that%20the%20model%20performs%20well%0Aduring%20the%20pre-training%20stage%3B%20iii%29%20although%20model%20benefits%20significantly%0Athrough%20supervised%20fine-tuning%2C%20it%20may%20forget%20previously%20known%20domain%20knowledge%0Aand%20the%20tasks%20that%20are%20not%20seen%20during%20fine-tuning%3B%20iv%29%20the%20model%20resembles%0Ahigh%20sensitivity%20to%20evaluation%20prompts%20after%20supervised%20fine-tuning%2C%20but%20this%0Asensitivity%20can%20be%20alleviated%20by%20more%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06663v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmuro%2520and%2520Char%253A%2520Analyzing%2520the%2520Relationship%2520between%2520Pre-Training%2520and%250A%2520%2520Fine-Tuning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DKaiser%2520Sun%2520and%2520Mark%2520Dredze%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520large%2520language%2520models%2520leads%2520to%2520the%2520formation%2520of%2520a%250Apre-train-then-align%2520paradigm%252C%2520in%2520which%2520the%2520model%2520is%2520typically%2520pre-trained%2520on%2520a%250Alarge%2520text%2520corpus%2520and%2520undergoes%2520a%2520tuning%2520stage%2520to%2520align%2520the%2520model%2520with%2520human%250Apreference%2520or%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520relationship%250Abetween%2520pre-training%2520and%2520fine-tuning%2520by%2520fine-tuning%2520multiple%2520intermediate%250Apre-trained%2520model%2520checkpoints.%2520Our%2520results%2520on%252018%2520datasets%2520suggest%2520that%2520i%2529%250Acontinual%2520pre-training%2520improves%2520the%2520model%2520in%2520a%2520latent%2520way%2520that%2520unveils%2520after%250Afine-tuning%253B%2520ii%2529%2520with%2520extra%2520fine-tuning%252C%2520the%2520datasets%2520that%2520the%2520model%2520does%2520not%250Ademonstrate%2520capability%2520gain%2520much%2520more%2520than%2520those%2520that%2520the%2520model%2520performs%2520well%250Aduring%2520the%2520pre-training%2520stage%253B%2520iii%2529%2520although%2520model%2520benefits%2520significantly%250Athrough%2520supervised%2520fine-tuning%252C%2520it%2520may%2520forget%2520previously%2520known%2520domain%2520knowledge%250Aand%2520the%2520tasks%2520that%2520are%2520not%2520seen%2520during%2520fine-tuning%253B%2520iv%2529%2520the%2520model%2520resembles%250Ahigh%2520sensitivity%2520to%2520evaluation%2520prompts%2520after%2520supervised%2520fine-tuning%252C%2520but%2520this%250Asensitivity%2520can%2520be%2520alleviated%2520by%2520more%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06663v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amuro%20and%20Char%3A%20Analyzing%20the%20Relationship%20between%20Pre-Training%20and%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models&entry.906535625=Kaiser%20Sun%20and%20Mark%20Dredze&entry.1292438233=%20%20The%20development%20of%20large%20language%20models%20leads%20to%20the%20formation%20of%20a%0Apre-train-then-align%20paradigm%2C%20in%20which%20the%20model%20is%20typically%20pre-trained%20on%20a%0Alarge%20text%20corpus%20and%20undergoes%20a%20tuning%20stage%20to%20align%20the%20model%20with%20human%0Apreference%20or%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%20the%20relationship%0Abetween%20pre-training%20and%20fine-tuning%20by%20fine-tuning%20multiple%20intermediate%0Apre-trained%20model%20checkpoints.%20Our%20results%20on%2018%20datasets%20suggest%20that%20i%29%0Acontinual%20pre-training%20improves%20the%20model%20in%20a%20latent%20way%20that%20unveils%20after%0Afine-tuning%3B%20ii%29%20with%20extra%20fine-tuning%2C%20the%20datasets%20that%20the%20model%20does%20not%0Ademonstrate%20capability%20gain%20much%20more%20than%20those%20that%20the%20model%20performs%20well%0Aduring%20the%20pre-training%20stage%3B%20iii%29%20although%20model%20benefits%20significantly%0Athrough%20supervised%20fine-tuning%2C%20it%20may%20forget%20previously%20known%20domain%20knowledge%0Aand%20the%20tasks%20that%20are%20not%20seen%20during%20fine-tuning%3B%20iv%29%20the%20model%20resembles%0Ahigh%20sensitivity%20to%20evaluation%20prompts%20after%20supervised%20fine-tuning%2C%20but%20this%0Asensitivity%20can%20be%20alleviated%20by%20more%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06663v4&entry.124074799=Read"},
{"title": "RoMA: Robust Malware Attribution via Byte-level Adversarial Training\n  with Global Perturbations and Adversarial Consistency Regularization", "author": "Yuxia Sun and Huihong Chen and Jingcai Guo and Aoxiang Sun and Zhetao Li and Haolin Liu", "abstract": "  Attributing APT (Advanced Persistent Threat) malware to their respective\ngroups is crucial for threat intelligence and cybersecurity. However, APT\nadversaries often conceal their identities, rendering attribution inherently\nadversarial. Existing machine learning-based attribution models, while\neffective, remain highly vulnerable to adversarial attacks. For example, the\nstate-of-the-art byte-level model MalConv sees its accuracy drop from over 90%\nto below 2% under PGD (projected gradient descent) attacks. Existing\ngradient-based adversarial training techniques for malware detection or image\nprocessing were applied to malware attribution in this study, revealing that\nboth robustness and training efficiency require significant improvement. To\naddress this, we propose RoMA, a novel single-step adversarial training\napproach that integrates global perturbations to generate enhanced adversarial\nsamples and employs adversarial consistency regularization to improve\nrepresentation quality and resilience. A novel APT malware dataset named AMG18,\nwith diverse samples and realistic class imbalances, is introduced for\nevaluation. Extensive experiments show that RoMA significantly outperforms\nseven competing methods in both adversarial robustness (e.g., achieving over\n80% robust accuracy-more than twice that of the next-best method under PGD\nattacks) and training efficiency (e.g., more than twice as fast as the\nsecond-best method in terms of accuracy), while maintaining superior standard\naccuracy in non-adversarial scenarios.\n", "link": "http://arxiv.org/abs/2502.07492v1", "date": "2025-02-11", "relevancy": 2.522, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5291}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5062}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoMA%3A%20Robust%20Malware%20Attribution%20via%20Byte-level%20Adversarial%20Training%0A%20%20with%20Global%20Perturbations%20and%20Adversarial%20Consistency%20Regularization&body=Title%3A%20RoMA%3A%20Robust%20Malware%20Attribution%20via%20Byte-level%20Adversarial%20Training%0A%20%20with%20Global%20Perturbations%20and%20Adversarial%20Consistency%20Regularization%0AAuthor%3A%20Yuxia%20Sun%20and%20Huihong%20Chen%20and%20Jingcai%20Guo%20and%20Aoxiang%20Sun%20and%20Zhetao%20Li%20and%20Haolin%20Liu%0AAbstract%3A%20%20%20Attributing%20APT%20%28Advanced%20Persistent%20Threat%29%20malware%20to%20their%20respective%0Agroups%20is%20crucial%20for%20threat%20intelligence%20and%20cybersecurity.%20However%2C%20APT%0Aadversaries%20often%20conceal%20their%20identities%2C%20rendering%20attribution%20inherently%0Aadversarial.%20Existing%20machine%20learning-based%20attribution%20models%2C%20while%0Aeffective%2C%20remain%20highly%20vulnerable%20to%20adversarial%20attacks.%20For%20example%2C%20the%0Astate-of-the-art%20byte-level%20model%20MalConv%20sees%20its%20accuracy%20drop%20from%20over%2090%25%0Ato%20below%202%25%20under%20PGD%20%28projected%20gradient%20descent%29%20attacks.%20Existing%0Agradient-based%20adversarial%20training%20techniques%20for%20malware%20detection%20or%20image%0Aprocessing%20were%20applied%20to%20malware%20attribution%20in%20this%20study%2C%20revealing%20that%0Aboth%20robustness%20and%20training%20efficiency%20require%20significant%20improvement.%20To%0Aaddress%20this%2C%20we%20propose%20RoMA%2C%20a%20novel%20single-step%20adversarial%20training%0Aapproach%20that%20integrates%20global%20perturbations%20to%20generate%20enhanced%20adversarial%0Asamples%20and%20employs%20adversarial%20consistency%20regularization%20to%20improve%0Arepresentation%20quality%20and%20resilience.%20A%20novel%20APT%20malware%20dataset%20named%20AMG18%2C%0Awith%20diverse%20samples%20and%20realistic%20class%20imbalances%2C%20is%20introduced%20for%0Aevaluation.%20Extensive%20experiments%20show%20that%20RoMA%20significantly%20outperforms%0Aseven%20competing%20methods%20in%20both%20adversarial%20robustness%20%28e.g.%2C%20achieving%20over%0A80%25%20robust%20accuracy-more%20than%20twice%20that%20of%20the%20next-best%20method%20under%20PGD%0Aattacks%29%20and%20training%20efficiency%20%28e.g.%2C%20more%20than%20twice%20as%20fast%20as%20the%0Asecond-best%20method%20in%20terms%20of%20accuracy%29%2C%20while%20maintaining%20superior%20standard%0Aaccuracy%20in%20non-adversarial%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoMA%253A%2520Robust%2520Malware%2520Attribution%2520via%2520Byte-level%2520Adversarial%2520Training%250A%2520%2520with%2520Global%2520Perturbations%2520and%2520Adversarial%2520Consistency%2520Regularization%26entry.906535625%3DYuxia%2520Sun%2520and%2520Huihong%2520Chen%2520and%2520Jingcai%2520Guo%2520and%2520Aoxiang%2520Sun%2520and%2520Zhetao%2520Li%2520and%2520Haolin%2520Liu%26entry.1292438233%3D%2520%2520Attributing%2520APT%2520%2528Advanced%2520Persistent%2520Threat%2529%2520malware%2520to%2520their%2520respective%250Agroups%2520is%2520crucial%2520for%2520threat%2520intelligence%2520and%2520cybersecurity.%2520However%252C%2520APT%250Aadversaries%2520often%2520conceal%2520their%2520identities%252C%2520rendering%2520attribution%2520inherently%250Aadversarial.%2520Existing%2520machine%2520learning-based%2520attribution%2520models%252C%2520while%250Aeffective%252C%2520remain%2520highly%2520vulnerable%2520to%2520adversarial%2520attacks.%2520For%2520example%252C%2520the%250Astate-of-the-art%2520byte-level%2520model%2520MalConv%2520sees%2520its%2520accuracy%2520drop%2520from%2520over%252090%2525%250Ato%2520below%25202%2525%2520under%2520PGD%2520%2528projected%2520gradient%2520descent%2529%2520attacks.%2520Existing%250Agradient-based%2520adversarial%2520training%2520techniques%2520for%2520malware%2520detection%2520or%2520image%250Aprocessing%2520were%2520applied%2520to%2520malware%2520attribution%2520in%2520this%2520study%252C%2520revealing%2520that%250Aboth%2520robustness%2520and%2520training%2520efficiency%2520require%2520significant%2520improvement.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520RoMA%252C%2520a%2520novel%2520single-step%2520adversarial%2520training%250Aapproach%2520that%2520integrates%2520global%2520perturbations%2520to%2520generate%2520enhanced%2520adversarial%250Asamples%2520and%2520employs%2520adversarial%2520consistency%2520regularization%2520to%2520improve%250Arepresentation%2520quality%2520and%2520resilience.%2520A%2520novel%2520APT%2520malware%2520dataset%2520named%2520AMG18%252C%250Awith%2520diverse%2520samples%2520and%2520realistic%2520class%2520imbalances%252C%2520is%2520introduced%2520for%250Aevaluation.%2520Extensive%2520experiments%2520show%2520that%2520RoMA%2520significantly%2520outperforms%250Aseven%2520competing%2520methods%2520in%2520both%2520adversarial%2520robustness%2520%2528e.g.%252C%2520achieving%2520over%250A80%2525%2520robust%2520accuracy-more%2520than%2520twice%2520that%2520of%2520the%2520next-best%2520method%2520under%2520PGD%250Aattacks%2529%2520and%2520training%2520efficiency%2520%2528e.g.%252C%2520more%2520than%2520twice%2520as%2520fast%2520as%2520the%250Asecond-best%2520method%2520in%2520terms%2520of%2520accuracy%2529%252C%2520while%2520maintaining%2520superior%2520standard%250Aaccuracy%2520in%2520non-adversarial%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoMA%3A%20Robust%20Malware%20Attribution%20via%20Byte-level%20Adversarial%20Training%0A%20%20with%20Global%20Perturbations%20and%20Adversarial%20Consistency%20Regularization&entry.906535625=Yuxia%20Sun%20and%20Huihong%20Chen%20and%20Jingcai%20Guo%20and%20Aoxiang%20Sun%20and%20Zhetao%20Li%20and%20Haolin%20Liu&entry.1292438233=%20%20Attributing%20APT%20%28Advanced%20Persistent%20Threat%29%20malware%20to%20their%20respective%0Agroups%20is%20crucial%20for%20threat%20intelligence%20and%20cybersecurity.%20However%2C%20APT%0Aadversaries%20often%20conceal%20their%20identities%2C%20rendering%20attribution%20inherently%0Aadversarial.%20Existing%20machine%20learning-based%20attribution%20models%2C%20while%0Aeffective%2C%20remain%20highly%20vulnerable%20to%20adversarial%20attacks.%20For%20example%2C%20the%0Astate-of-the-art%20byte-level%20model%20MalConv%20sees%20its%20accuracy%20drop%20from%20over%2090%25%0Ato%20below%202%25%20under%20PGD%20%28projected%20gradient%20descent%29%20attacks.%20Existing%0Agradient-based%20adversarial%20training%20techniques%20for%20malware%20detection%20or%20image%0Aprocessing%20were%20applied%20to%20malware%20attribution%20in%20this%20study%2C%20revealing%20that%0Aboth%20robustness%20and%20training%20efficiency%20require%20significant%20improvement.%20To%0Aaddress%20this%2C%20we%20propose%20RoMA%2C%20a%20novel%20single-step%20adversarial%20training%0Aapproach%20that%20integrates%20global%20perturbations%20to%20generate%20enhanced%20adversarial%0Asamples%20and%20employs%20adversarial%20consistency%20regularization%20to%20improve%0Arepresentation%20quality%20and%20resilience.%20A%20novel%20APT%20malware%20dataset%20named%20AMG18%2C%0Awith%20diverse%20samples%20and%20realistic%20class%20imbalances%2C%20is%20introduced%20for%0Aevaluation.%20Extensive%20experiments%20show%20that%20RoMA%20significantly%20outperforms%0Aseven%20competing%20methods%20in%20both%20adversarial%20robustness%20%28e.g.%2C%20achieving%20over%0A80%25%20robust%20accuracy-more%20than%20twice%20that%20of%20the%20next-best%20method%20under%20PGD%0Aattacks%29%20and%20training%20efficiency%20%28e.g.%2C%20more%20than%20twice%20as%20fast%20as%20the%0Asecond-best%20method%20in%20terms%20of%20accuracy%29%2C%20while%20maintaining%20superior%20standard%0Aaccuracy%20in%20non-adversarial%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07492v1&entry.124074799=Read"},
{"title": "NeuPAN: Direct Point Robot Navigation with End-to-End Model-based\n  Learning", "author": "Ruihua Han and Shuai Wang and Shuaijun Wang and Zeqing Zhang and Jianjun Chen and Shijie Lin and Chengyang Li and Chengzhong Xu and Yonina C. Eldar and Qi Hao and Jia Pan", "abstract": "  Navigating a nonholonomic robot in a cluttered, unknown environment requires\naccurate perception and precise motion control for real-time collision\navoidance. This paper presents NeuPAN: a real-time, highly accurate, map-free,\neasy-to-deploy, and environment-invariant robot motion planner. Leveraging a\ntightly coupled perception-to-control framework, NeuPAN has two key innovations\ncompared to existing approaches: 1) it directly maps raw point cloud data to a\nlatent distance feature space for collision-free motion generation, avoiding\nerror propagation from the perception to control pipeline; 2) it is\ninterpretable from an end-to-end model-based learning perspective. The crux of\nNeuPAN is solving an end-to-end mathematical model with numerous point-level\nconstraints using a plug-and-play (PnP) proximal alternating-minimization\nnetwork (PAN), incorporating neurons in the loop. This allows NeuPAN to\ngenerate real-time, physically interpretable motions. It seamlessly integrates\ndata and knowledge engines, and its network parameters can be fine-tuned via\nbackpropagation. We evaluate NeuPAN on a ground mobile robot, a wheel-legged\nrobot, and an autonomous vehicle, in extensive simulated and real-world\nenvironments. Results demonstrate that NeuPAN outperforms existing baselines in\nterms of accuracy, efficiency, robustness, and generalization capabilities\nacross various environments, including the cluttered sandbox, office, corridor,\nand parking lot. We show that NeuPAN works well in unknown and unstructured\nenvironments with arbitrarily shaped objects, transforming impassable paths\ninto passable ones.\n", "link": "http://arxiv.org/abs/2403.06828v3", "date": "2025-02-11", "relevancy": 2.5186, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6804}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6074}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuPAN%3A%20Direct%20Point%20Robot%20Navigation%20with%20End-to-End%20Model-based%0A%20%20Learning&body=Title%3A%20NeuPAN%3A%20Direct%20Point%20Robot%20Navigation%20with%20End-to-End%20Model-based%0A%20%20Learning%0AAuthor%3A%20Ruihua%20Han%20and%20Shuai%20Wang%20and%20Shuaijun%20Wang%20and%20Zeqing%20Zhang%20and%20Jianjun%20Chen%20and%20Shijie%20Lin%20and%20Chengyang%20Li%20and%20Chengzhong%20Xu%20and%20Yonina%20C.%20Eldar%20and%20Qi%20Hao%20and%20Jia%20Pan%0AAbstract%3A%20%20%20Navigating%20a%20nonholonomic%20robot%20in%20a%20cluttered%2C%20unknown%20environment%20requires%0Aaccurate%20perception%20and%20precise%20motion%20control%20for%20real-time%20collision%0Aavoidance.%20This%20paper%20presents%20NeuPAN%3A%20a%20real-time%2C%20highly%20accurate%2C%20map-free%2C%0Aeasy-to-deploy%2C%20and%20environment-invariant%20robot%20motion%20planner.%20Leveraging%20a%0Atightly%20coupled%20perception-to-control%20framework%2C%20NeuPAN%20has%20two%20key%20innovations%0Acompared%20to%20existing%20approaches%3A%201%29%20it%20directly%20maps%20raw%20point%20cloud%20data%20to%20a%0Alatent%20distance%20feature%20space%20for%20collision-free%20motion%20generation%2C%20avoiding%0Aerror%20propagation%20from%20the%20perception%20to%20control%20pipeline%3B%202%29%20it%20is%0Ainterpretable%20from%20an%20end-to-end%20model-based%20learning%20perspective.%20The%20crux%20of%0ANeuPAN%20is%20solving%20an%20end-to-end%20mathematical%20model%20with%20numerous%20point-level%0Aconstraints%20using%20a%20plug-and-play%20%28PnP%29%20proximal%20alternating-minimization%0Anetwork%20%28PAN%29%2C%20incorporating%20neurons%20in%20the%20loop.%20This%20allows%20NeuPAN%20to%0Agenerate%20real-time%2C%20physically%20interpretable%20motions.%20It%20seamlessly%20integrates%0Adata%20and%20knowledge%20engines%2C%20and%20its%20network%20parameters%20can%20be%20fine-tuned%20via%0Abackpropagation.%20We%20evaluate%20NeuPAN%20on%20a%20ground%20mobile%20robot%2C%20a%20wheel-legged%0Arobot%2C%20and%20an%20autonomous%20vehicle%2C%20in%20extensive%20simulated%20and%20real-world%0Aenvironments.%20Results%20demonstrate%20that%20NeuPAN%20outperforms%20existing%20baselines%20in%0Aterms%20of%20accuracy%2C%20efficiency%2C%20robustness%2C%20and%20generalization%20capabilities%0Aacross%20various%20environments%2C%20including%20the%20cluttered%20sandbox%2C%20office%2C%20corridor%2C%0Aand%20parking%20lot.%20We%20show%20that%20NeuPAN%20works%20well%20in%20unknown%20and%20unstructured%0Aenvironments%20with%20arbitrarily%20shaped%20objects%2C%20transforming%20impassable%20paths%0Ainto%20passable%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06828v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuPAN%253A%2520Direct%2520Point%2520Robot%2520Navigation%2520with%2520End-to-End%2520Model-based%250A%2520%2520Learning%26entry.906535625%3DRuihua%2520Han%2520and%2520Shuai%2520Wang%2520and%2520Shuaijun%2520Wang%2520and%2520Zeqing%2520Zhang%2520and%2520Jianjun%2520Chen%2520and%2520Shijie%2520Lin%2520and%2520Chengyang%2520Li%2520and%2520Chengzhong%2520Xu%2520and%2520Yonina%2520C.%2520Eldar%2520and%2520Qi%2520Hao%2520and%2520Jia%2520Pan%26entry.1292438233%3D%2520%2520Navigating%2520a%2520nonholonomic%2520robot%2520in%2520a%2520cluttered%252C%2520unknown%2520environment%2520requires%250Aaccurate%2520perception%2520and%2520precise%2520motion%2520control%2520for%2520real-time%2520collision%250Aavoidance.%2520This%2520paper%2520presents%2520NeuPAN%253A%2520a%2520real-time%252C%2520highly%2520accurate%252C%2520map-free%252C%250Aeasy-to-deploy%252C%2520and%2520environment-invariant%2520robot%2520motion%2520planner.%2520Leveraging%2520a%250Atightly%2520coupled%2520perception-to-control%2520framework%252C%2520NeuPAN%2520has%2520two%2520key%2520innovations%250Acompared%2520to%2520existing%2520approaches%253A%25201%2529%2520it%2520directly%2520maps%2520raw%2520point%2520cloud%2520data%2520to%2520a%250Alatent%2520distance%2520feature%2520space%2520for%2520collision-free%2520motion%2520generation%252C%2520avoiding%250Aerror%2520propagation%2520from%2520the%2520perception%2520to%2520control%2520pipeline%253B%25202%2529%2520it%2520is%250Ainterpretable%2520from%2520an%2520end-to-end%2520model-based%2520learning%2520perspective.%2520The%2520crux%2520of%250ANeuPAN%2520is%2520solving%2520an%2520end-to-end%2520mathematical%2520model%2520with%2520numerous%2520point-level%250Aconstraints%2520using%2520a%2520plug-and-play%2520%2528PnP%2529%2520proximal%2520alternating-minimization%250Anetwork%2520%2528PAN%2529%252C%2520incorporating%2520neurons%2520in%2520the%2520loop.%2520This%2520allows%2520NeuPAN%2520to%250Agenerate%2520real-time%252C%2520physically%2520interpretable%2520motions.%2520It%2520seamlessly%2520integrates%250Adata%2520and%2520knowledge%2520engines%252C%2520and%2520its%2520network%2520parameters%2520can%2520be%2520fine-tuned%2520via%250Abackpropagation.%2520We%2520evaluate%2520NeuPAN%2520on%2520a%2520ground%2520mobile%2520robot%252C%2520a%2520wheel-legged%250Arobot%252C%2520and%2520an%2520autonomous%2520vehicle%252C%2520in%2520extensive%2520simulated%2520and%2520real-world%250Aenvironments.%2520Results%2520demonstrate%2520that%2520NeuPAN%2520outperforms%2520existing%2520baselines%2520in%250Aterms%2520of%2520accuracy%252C%2520efficiency%252C%2520robustness%252C%2520and%2520generalization%2520capabilities%250Aacross%2520various%2520environments%252C%2520including%2520the%2520cluttered%2520sandbox%252C%2520office%252C%2520corridor%252C%250Aand%2520parking%2520lot.%2520We%2520show%2520that%2520NeuPAN%2520works%2520well%2520in%2520unknown%2520and%2520unstructured%250Aenvironments%2520with%2520arbitrarily%2520shaped%2520objects%252C%2520transforming%2520impassable%2520paths%250Ainto%2520passable%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06828v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuPAN%3A%20Direct%20Point%20Robot%20Navigation%20with%20End-to-End%20Model-based%0A%20%20Learning&entry.906535625=Ruihua%20Han%20and%20Shuai%20Wang%20and%20Shuaijun%20Wang%20and%20Zeqing%20Zhang%20and%20Jianjun%20Chen%20and%20Shijie%20Lin%20and%20Chengyang%20Li%20and%20Chengzhong%20Xu%20and%20Yonina%20C.%20Eldar%20and%20Qi%20Hao%20and%20Jia%20Pan&entry.1292438233=%20%20Navigating%20a%20nonholonomic%20robot%20in%20a%20cluttered%2C%20unknown%20environment%20requires%0Aaccurate%20perception%20and%20precise%20motion%20control%20for%20real-time%20collision%0Aavoidance.%20This%20paper%20presents%20NeuPAN%3A%20a%20real-time%2C%20highly%20accurate%2C%20map-free%2C%0Aeasy-to-deploy%2C%20and%20environment-invariant%20robot%20motion%20planner.%20Leveraging%20a%0Atightly%20coupled%20perception-to-control%20framework%2C%20NeuPAN%20has%20two%20key%20innovations%0Acompared%20to%20existing%20approaches%3A%201%29%20it%20directly%20maps%20raw%20point%20cloud%20data%20to%20a%0Alatent%20distance%20feature%20space%20for%20collision-free%20motion%20generation%2C%20avoiding%0Aerror%20propagation%20from%20the%20perception%20to%20control%20pipeline%3B%202%29%20it%20is%0Ainterpretable%20from%20an%20end-to-end%20model-based%20learning%20perspective.%20The%20crux%20of%0ANeuPAN%20is%20solving%20an%20end-to-end%20mathematical%20model%20with%20numerous%20point-level%0Aconstraints%20using%20a%20plug-and-play%20%28PnP%29%20proximal%20alternating-minimization%0Anetwork%20%28PAN%29%2C%20incorporating%20neurons%20in%20the%20loop.%20This%20allows%20NeuPAN%20to%0Agenerate%20real-time%2C%20physically%20interpretable%20motions.%20It%20seamlessly%20integrates%0Adata%20and%20knowledge%20engines%2C%20and%20its%20network%20parameters%20can%20be%20fine-tuned%20via%0Abackpropagation.%20We%20evaluate%20NeuPAN%20on%20a%20ground%20mobile%20robot%2C%20a%20wheel-legged%0Arobot%2C%20and%20an%20autonomous%20vehicle%2C%20in%20extensive%20simulated%20and%20real-world%0Aenvironments.%20Results%20demonstrate%20that%20NeuPAN%20outperforms%20existing%20baselines%20in%0Aterms%20of%20accuracy%2C%20efficiency%2C%20robustness%2C%20and%20generalization%20capabilities%0Aacross%20various%20environments%2C%20including%20the%20cluttered%20sandbox%2C%20office%2C%20corridor%2C%0Aand%20parking%20lot.%20We%20show%20that%20NeuPAN%20works%20well%20in%20unknown%20and%20unstructured%0Aenvironments%20with%20arbitrarily%20shaped%20objects%2C%20transforming%20impassable%20paths%0Ainto%20passable%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06828v3&entry.124074799=Read"},
{"title": "Large Continual Instruction Assistant", "author": "Jingyang Qiao and Zhizhong Zhang and Xin Tan and Yanyun Qu and Shouhong Ding and Yuan Xie", "abstract": "  Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.\n", "link": "http://arxiv.org/abs/2410.10868v2", "date": "2025-02-11", "relevancy": 2.5158, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5046}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Continual%20Instruction%20Assistant&body=Title%3A%20Large%20Continual%20Instruction%20Assistant%0AAuthor%3A%20Jingyang%20Qiao%20and%20Zhizhong%20Zhang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Shouhong%20Ding%20and%20Yuan%20Xie%0AAbstract%3A%20%20%20Continual%20Instruction%20Tuning%20%28CIT%29%20is%20adopted%20to%20continually%20instruct%20Large%0AModels%20to%20follow%20human%20intent%20data%20by%20data.%20It%20is%20observed%20that%20existing%0Agradient%20update%20would%20heavily%20destroy%20the%20performance%20on%20previous%20datasets%0Aduring%20CIT%20process.%20Instead%2C%20Exponential%20Moving%20Average%20%28EMA%29%2C%20owns%20the%20ability%0Ato%20trace%20previous%20parameters%2C%20which%20can%20aid%20in%20decreasing%20forgetting.%0ANonetheless%2C%20its%20stable%20balance%20weight%20fails%20to%20deal%20with%20the%20ever-changing%0Adatasets%2C%20leading%20to%20the%20out-of-balance%20between%20plasticity%20and%20stability.%20In%0Athis%20paper%2C%20we%20propose%20a%20general%20continual%20instruction%20tuning%20framework%20to%0Aaddress%20the%20challenge.%20Starting%20from%20the%20trade-off%20prerequisite%20and%20EMA%20update%2C%0Awe%20propose%20the%20plasticity%20and%20stability%20ideal%20condition.%20Based%20on%20Taylor%0Aexpansion%20in%20the%20loss%20function%2C%20we%20find%20the%20optimal%20balance%20weight%20can%20be%0Aautomatically%20determined%20by%20the%20gradients%20and%20learned%20parameters.%20Therefore%2C%20we%0Apropose%20a%20stable-plasticity%20balanced%20coefficient%20to%20avoid%20knowledge%20confusion.%0ABased%20on%20the%20semantic%20similarity%20of%20the%20instructions%2C%20we%20can%20determine%20whether%0Ato%20retrain%20or%20expand%20the%20training%20parameters%20and%20allocate%20the%20most%20suitable%0Aparameters%20for%20the%20testing%20instances.%20Extensive%20experiments%20across%20multiple%0Acontinual%20instruction%20tuning%20benchmarks%20demonstrate%20that%20our%20approach%20not%20only%0Aenhances%20anti-forgetting%20capabilities%20but%20also%20significantly%20improves%20overall%0Acontinual%20tuning%20performance.%20For%20example%2C%20based%20on%20LLaVA-7B%2C%20the%20forgetting%20is%0Areduced%20from%205.42%20to%201.93.%20Our%20code%20will%20be%20made%20publicly%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Continual%2520Instruction%2520Assistant%26entry.906535625%3DJingyang%2520Qiao%2520and%2520Zhizhong%2520Zhang%2520and%2520Xin%2520Tan%2520and%2520Yanyun%2520Qu%2520and%2520Shouhong%2520Ding%2520and%2520Yuan%2520Xie%26entry.1292438233%3D%2520%2520Continual%2520Instruction%2520Tuning%2520%2528CIT%2529%2520is%2520adopted%2520to%2520continually%2520instruct%2520Large%250AModels%2520to%2520follow%2520human%2520intent%2520data%2520by%2520data.%2520It%2520is%2520observed%2520that%2520existing%250Agradient%2520update%2520would%2520heavily%2520destroy%2520the%2520performance%2520on%2520previous%2520datasets%250Aduring%2520CIT%2520process.%2520Instead%252C%2520Exponential%2520Moving%2520Average%2520%2528EMA%2529%252C%2520owns%2520the%2520ability%250Ato%2520trace%2520previous%2520parameters%252C%2520which%2520can%2520aid%2520in%2520decreasing%2520forgetting.%250ANonetheless%252C%2520its%2520stable%2520balance%2520weight%2520fails%2520to%2520deal%2520with%2520the%2520ever-changing%250Adatasets%252C%2520leading%2520to%2520the%2520out-of-balance%2520between%2520plasticity%2520and%2520stability.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520general%2520continual%2520instruction%2520tuning%2520framework%2520to%250Aaddress%2520the%2520challenge.%2520Starting%2520from%2520the%2520trade-off%2520prerequisite%2520and%2520EMA%2520update%252C%250Awe%2520propose%2520the%2520plasticity%2520and%2520stability%2520ideal%2520condition.%2520Based%2520on%2520Taylor%250Aexpansion%2520in%2520the%2520loss%2520function%252C%2520we%2520find%2520the%2520optimal%2520balance%2520weight%2520can%2520be%250Aautomatically%2520determined%2520by%2520the%2520gradients%2520and%2520learned%2520parameters.%2520Therefore%252C%2520we%250Apropose%2520a%2520stable-plasticity%2520balanced%2520coefficient%2520to%2520avoid%2520knowledge%2520confusion.%250ABased%2520on%2520the%2520semantic%2520similarity%2520of%2520the%2520instructions%252C%2520we%2520can%2520determine%2520whether%250Ato%2520retrain%2520or%2520expand%2520the%2520training%2520parameters%2520and%2520allocate%2520the%2520most%2520suitable%250Aparameters%2520for%2520the%2520testing%2520instances.%2520Extensive%2520experiments%2520across%2520multiple%250Acontinual%2520instruction%2520tuning%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%250Aenhances%2520anti-forgetting%2520capabilities%2520but%2520also%2520significantly%2520improves%2520overall%250Acontinual%2520tuning%2520performance.%2520For%2520example%252C%2520based%2520on%2520LLaVA-7B%252C%2520the%2520forgetting%2520is%250Areduced%2520from%25205.42%2520to%25201.93.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Continual%20Instruction%20Assistant&entry.906535625=Jingyang%20Qiao%20and%20Zhizhong%20Zhang%20and%20Xin%20Tan%20and%20Yanyun%20Qu%20and%20Shouhong%20Ding%20and%20Yuan%20Xie&entry.1292438233=%20%20Continual%20Instruction%20Tuning%20%28CIT%29%20is%20adopted%20to%20continually%20instruct%20Large%0AModels%20to%20follow%20human%20intent%20data%20by%20data.%20It%20is%20observed%20that%20existing%0Agradient%20update%20would%20heavily%20destroy%20the%20performance%20on%20previous%20datasets%0Aduring%20CIT%20process.%20Instead%2C%20Exponential%20Moving%20Average%20%28EMA%29%2C%20owns%20the%20ability%0Ato%20trace%20previous%20parameters%2C%20which%20can%20aid%20in%20decreasing%20forgetting.%0ANonetheless%2C%20its%20stable%20balance%20weight%20fails%20to%20deal%20with%20the%20ever-changing%0Adatasets%2C%20leading%20to%20the%20out-of-balance%20between%20plasticity%20and%20stability.%20In%0Athis%20paper%2C%20we%20propose%20a%20general%20continual%20instruction%20tuning%20framework%20to%0Aaddress%20the%20challenge.%20Starting%20from%20the%20trade-off%20prerequisite%20and%20EMA%20update%2C%0Awe%20propose%20the%20plasticity%20and%20stability%20ideal%20condition.%20Based%20on%20Taylor%0Aexpansion%20in%20the%20loss%20function%2C%20we%20find%20the%20optimal%20balance%20weight%20can%20be%0Aautomatically%20determined%20by%20the%20gradients%20and%20learned%20parameters.%20Therefore%2C%20we%0Apropose%20a%20stable-plasticity%20balanced%20coefficient%20to%20avoid%20knowledge%20confusion.%0ABased%20on%20the%20semantic%20similarity%20of%20the%20instructions%2C%20we%20can%20determine%20whether%0Ato%20retrain%20or%20expand%20the%20training%20parameters%20and%20allocate%20the%20most%20suitable%0Aparameters%20for%20the%20testing%20instances.%20Extensive%20experiments%20across%20multiple%0Acontinual%20instruction%20tuning%20benchmarks%20demonstrate%20that%20our%20approach%20not%20only%0Aenhances%20anti-forgetting%20capabilities%20but%20also%20significantly%20improves%20overall%0Acontinual%20tuning%20performance.%20For%20example%2C%20based%20on%20LLaVA-7B%2C%20the%20forgetting%20is%0Areduced%20from%205.42%20to%201.93.%20Our%20code%20will%20be%20made%20publicly%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10868v2&entry.124074799=Read"},
{"title": "DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation", "author": "Yunbei Zhang and Akshay Mehra and Shuaicheng Niu and Jihun Hamm", "abstract": "  Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained\nmodels to continually changing, unseen target domains. While existing CTTA\nmethods assume structured domain changes with uniform durations, real-world\nenvironments often exhibit dynamic patterns where domains recur with varying\nfrequencies and durations. Current approaches, which adapt the same parameters\nacross different domains, struggle in such dynamic conditions-they face\nconvergence issues with brief domain exposures, risk forgetting previously\nlearned knowledge, or misapplying it to irrelevant domains. To remedy this, we\npropose DPCore, a method designed for robust performance across diverse domain\nchange patterns while ensuring computational efficiency. DPCore integrates\nthree key components: Visual Prompt Adaptation for efficient domain alignment,\na Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism\nthat intelligently adjusts existing prompts for similar domains while creating\nnew ones for substantially different domains. Extensive experiments on four\nbenchmarks demonstrate that DPCore consistently outperforms various CTTA\nmethods, achieving state-of-the-art performance in both structured and dynamic\nsettings while reducing trainable parameters by 99% and computation time by 64%\ncompared to previous approaches.\n", "link": "http://arxiv.org/abs/2406.10737v3", "date": "2025-02-11", "relevancy": 2.5092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPCore%3A%20Dynamic%20Prompt%20Coreset%20for%20Continual%20Test-Time%20Adaptation&body=Title%3A%20DPCore%3A%20Dynamic%20Prompt%20Coreset%20for%20Continual%20Test-Time%20Adaptation%0AAuthor%3A%20Yunbei%20Zhang%20and%20Akshay%20Mehra%20and%20Shuaicheng%20Niu%20and%20Jihun%20Hamm%0AAbstract%3A%20%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20seeks%20to%20adapt%20source%20pre-trained%0Amodels%20to%20continually%20changing%2C%20unseen%20target%20domains.%20While%20existing%20CTTA%0Amethods%20assume%20structured%20domain%20changes%20with%20uniform%20durations%2C%20real-world%0Aenvironments%20often%20exhibit%20dynamic%20patterns%20where%20domains%20recur%20with%20varying%0Afrequencies%20and%20durations.%20Current%20approaches%2C%20which%20adapt%20the%20same%20parameters%0Aacross%20different%20domains%2C%20struggle%20in%20such%20dynamic%20conditions-they%20face%0Aconvergence%20issues%20with%20brief%20domain%20exposures%2C%20risk%20forgetting%20previously%0Alearned%20knowledge%2C%20or%20misapplying%20it%20to%20irrelevant%20domains.%20To%20remedy%20this%2C%20we%0Apropose%20DPCore%2C%20a%20method%20designed%20for%20robust%20performance%20across%20diverse%20domain%0Achange%20patterns%20while%20ensuring%20computational%20efficiency.%20DPCore%20integrates%0Athree%20key%20components%3A%20Visual%20Prompt%20Adaptation%20for%20efficient%20domain%20alignment%2C%0Aa%20Prompt%20Coreset%20for%20knowledge%20preservation%2C%20and%20a%20Dynamic%20Update%20mechanism%0Athat%20intelligently%20adjusts%20existing%20prompts%20for%20similar%20domains%20while%20creating%0Anew%20ones%20for%20substantially%20different%20domains.%20Extensive%20experiments%20on%20four%0Abenchmarks%20demonstrate%20that%20DPCore%20consistently%20outperforms%20various%20CTTA%0Amethods%2C%20achieving%20state-of-the-art%20performance%20in%20both%20structured%20and%20dynamic%0Asettings%20while%20reducing%20trainable%20parameters%20by%2099%25%20and%20computation%20time%20by%2064%25%0Acompared%20to%20previous%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10737v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPCore%253A%2520Dynamic%2520Prompt%2520Coreset%2520for%2520Continual%2520Test-Time%2520Adaptation%26entry.906535625%3DYunbei%2520Zhang%2520and%2520Akshay%2520Mehra%2520and%2520Shuaicheng%2520Niu%2520and%2520Jihun%2520Hamm%26entry.1292438233%3D%2520%2520Continual%2520Test-Time%2520Adaptation%2520%2528CTTA%2529%2520seeks%2520to%2520adapt%2520source%2520pre-trained%250Amodels%2520to%2520continually%2520changing%252C%2520unseen%2520target%2520domains.%2520While%2520existing%2520CTTA%250Amethods%2520assume%2520structured%2520domain%2520changes%2520with%2520uniform%2520durations%252C%2520real-world%250Aenvironments%2520often%2520exhibit%2520dynamic%2520patterns%2520where%2520domains%2520recur%2520with%2520varying%250Afrequencies%2520and%2520durations.%2520Current%2520approaches%252C%2520which%2520adapt%2520the%2520same%2520parameters%250Aacross%2520different%2520domains%252C%2520struggle%2520in%2520such%2520dynamic%2520conditions-they%2520face%250Aconvergence%2520issues%2520with%2520brief%2520domain%2520exposures%252C%2520risk%2520forgetting%2520previously%250Alearned%2520knowledge%252C%2520or%2520misapplying%2520it%2520to%2520irrelevant%2520domains.%2520To%2520remedy%2520this%252C%2520we%250Apropose%2520DPCore%252C%2520a%2520method%2520designed%2520for%2520robust%2520performance%2520across%2520diverse%2520domain%250Achange%2520patterns%2520while%2520ensuring%2520computational%2520efficiency.%2520DPCore%2520integrates%250Athree%2520key%2520components%253A%2520Visual%2520Prompt%2520Adaptation%2520for%2520efficient%2520domain%2520alignment%252C%250Aa%2520Prompt%2520Coreset%2520for%2520knowledge%2520preservation%252C%2520and%2520a%2520Dynamic%2520Update%2520mechanism%250Athat%2520intelligently%2520adjusts%2520existing%2520prompts%2520for%2520similar%2520domains%2520while%2520creating%250Anew%2520ones%2520for%2520substantially%2520different%2520domains.%2520Extensive%2520experiments%2520on%2520four%250Abenchmarks%2520demonstrate%2520that%2520DPCore%2520consistently%2520outperforms%2520various%2520CTTA%250Amethods%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520both%2520structured%2520and%2520dynamic%250Asettings%2520while%2520reducing%2520trainable%2520parameters%2520by%252099%2525%2520and%2520computation%2520time%2520by%252064%2525%250Acompared%2520to%2520previous%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10737v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPCore%3A%20Dynamic%20Prompt%20Coreset%20for%20Continual%20Test-Time%20Adaptation&entry.906535625=Yunbei%20Zhang%20and%20Akshay%20Mehra%20and%20Shuaicheng%20Niu%20and%20Jihun%20Hamm&entry.1292438233=%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20seeks%20to%20adapt%20source%20pre-trained%0Amodels%20to%20continually%20changing%2C%20unseen%20target%20domains.%20While%20existing%20CTTA%0Amethods%20assume%20structured%20domain%20changes%20with%20uniform%20durations%2C%20real-world%0Aenvironments%20often%20exhibit%20dynamic%20patterns%20where%20domains%20recur%20with%20varying%0Afrequencies%20and%20durations.%20Current%20approaches%2C%20which%20adapt%20the%20same%20parameters%0Aacross%20different%20domains%2C%20struggle%20in%20such%20dynamic%20conditions-they%20face%0Aconvergence%20issues%20with%20brief%20domain%20exposures%2C%20risk%20forgetting%20previously%0Alearned%20knowledge%2C%20or%20misapplying%20it%20to%20irrelevant%20domains.%20To%20remedy%20this%2C%20we%0Apropose%20DPCore%2C%20a%20method%20designed%20for%20robust%20performance%20across%20diverse%20domain%0Achange%20patterns%20while%20ensuring%20computational%20efficiency.%20DPCore%20integrates%0Athree%20key%20components%3A%20Visual%20Prompt%20Adaptation%20for%20efficient%20domain%20alignment%2C%0Aa%20Prompt%20Coreset%20for%20knowledge%20preservation%2C%20and%20a%20Dynamic%20Update%20mechanism%0Athat%20intelligently%20adjusts%20existing%20prompts%20for%20similar%20domains%20while%20creating%0Anew%20ones%20for%20substantially%20different%20domains.%20Extensive%20experiments%20on%20four%0Abenchmarks%20demonstrate%20that%20DPCore%20consistently%20outperforms%20various%20CTTA%0Amethods%2C%20achieving%20state-of-the-art%20performance%20in%20both%20structured%20and%20dynamic%0Asettings%20while%20reducing%20trainable%20parameters%20by%2099%25%20and%20computation%20time%20by%2064%25%0Acompared%20to%20previous%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10737v3&entry.124074799=Read"},
{"title": "DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT\n  Training", "author": "Xin Tan and Yuetao Chen and Yimin Jiang and Xing Chen and Kun Yan and Nan Duan and Yibo Zhu and Daxin Jiang and Hong Xu", "abstract": "  Diffusion Transformers (DiTs) have shown remarkable performance in modeling\nand generating high-quality videos. However, the quadratic computational\ncomplexity of 3D full attention mechanism presents significant challenges in\nscaling video DiT training, especially for high-definition and lengthy videos,\nwhere attention can dominate up to 95% of the end-to-end time and necessitate\nspecialized communication paradigms to handle large input sizes.\n  This paper introduces DSV, a novel framework designed to accelerate and scale\nthe training of video DiTs by leveraging the inherent dynamic attention\nsparsity throughout the training process. DSV employs a two-stage training\nalgorithm that exploits sparsity patterns, focusing on critical elements\nsupported by efficient, tailored kernels. To accommodate the new sparsity\ndimension, we develop a hybrid sparsity-aware context parallelism that\neffectively scales to large inputs by addressing the heterogeneity of sparsity\nacross attention heads and blocks, resulting in optimized sparse computation\nand communication. Extensive evaluations demonstrate that DSV achieves up to\n3.02x gain in training throughput with nearly no quality degradation.\n", "link": "http://arxiv.org/abs/2502.07590v1", "date": "2025-02-11", "relevancy": 2.506, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6472}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.647}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSV%3A%20Exploiting%20Dynamic%20Sparsity%20to%20Accelerate%20Large-Scale%20Video%20DiT%0A%20%20Training&body=Title%3A%20DSV%3A%20Exploiting%20Dynamic%20Sparsity%20to%20Accelerate%20Large-Scale%20Video%20DiT%0A%20%20Training%0AAuthor%3A%20Xin%20Tan%20and%20Yuetao%20Chen%20and%20Yimin%20Jiang%20and%20Xing%20Chen%20and%20Kun%20Yan%20and%20Nan%20Duan%20and%20Yibo%20Zhu%20and%20Daxin%20Jiang%20and%20Hong%20Xu%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20have%20shown%20remarkable%20performance%20in%20modeling%0Aand%20generating%20high-quality%20videos.%20However%2C%20the%20quadratic%20computational%0Acomplexity%20of%203D%20full%20attention%20mechanism%20presents%20significant%20challenges%20in%0Ascaling%20video%20DiT%20training%2C%20especially%20for%20high-definition%20and%20lengthy%20videos%2C%0Awhere%20attention%20can%20dominate%20up%20to%2095%25%20of%20the%20end-to-end%20time%20and%20necessitate%0Aspecialized%20communication%20paradigms%20to%20handle%20large%20input%20sizes.%0A%20%20This%20paper%20introduces%20DSV%2C%20a%20novel%20framework%20designed%20to%20accelerate%20and%20scale%0Athe%20training%20of%20video%20DiTs%20by%20leveraging%20the%20inherent%20dynamic%20attention%0Asparsity%20throughout%20the%20training%20process.%20DSV%20employs%20a%20two-stage%20training%0Aalgorithm%20that%20exploits%20sparsity%20patterns%2C%20focusing%20on%20critical%20elements%0Asupported%20by%20efficient%2C%20tailored%20kernels.%20To%20accommodate%20the%20new%20sparsity%0Adimension%2C%20we%20develop%20a%20hybrid%20sparsity-aware%20context%20parallelism%20that%0Aeffectively%20scales%20to%20large%20inputs%20by%20addressing%20the%20heterogeneity%20of%20sparsity%0Aacross%20attention%20heads%20and%20blocks%2C%20resulting%20in%20optimized%20sparse%20computation%0Aand%20communication.%20Extensive%20evaluations%20demonstrate%20that%20DSV%20achieves%20up%20to%0A3.02x%20gain%20in%20training%20throughput%20with%20nearly%20no%20quality%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSV%253A%2520Exploiting%2520Dynamic%2520Sparsity%2520to%2520Accelerate%2520Large-Scale%2520Video%2520DiT%250A%2520%2520Training%26entry.906535625%3DXin%2520Tan%2520and%2520Yuetao%2520Chen%2520and%2520Yimin%2520Jiang%2520and%2520Xing%2520Chen%2520and%2520Kun%2520Yan%2520and%2520Nan%2520Duan%2520and%2520Yibo%2520Zhu%2520and%2520Daxin%2520Jiang%2520and%2520Hong%2520Xu%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520modeling%250Aand%2520generating%2520high-quality%2520videos.%2520However%252C%2520the%2520quadratic%2520computational%250Acomplexity%2520of%25203D%2520full%2520attention%2520mechanism%2520presents%2520significant%2520challenges%2520in%250Ascaling%2520video%2520DiT%2520training%252C%2520especially%2520for%2520high-definition%2520and%2520lengthy%2520videos%252C%250Awhere%2520attention%2520can%2520dominate%2520up%2520to%252095%2525%2520of%2520the%2520end-to-end%2520time%2520and%2520necessitate%250Aspecialized%2520communication%2520paradigms%2520to%2520handle%2520large%2520input%2520sizes.%250A%2520%2520This%2520paper%2520introduces%2520DSV%252C%2520a%2520novel%2520framework%2520designed%2520to%2520accelerate%2520and%2520scale%250Athe%2520training%2520of%2520video%2520DiTs%2520by%2520leveraging%2520the%2520inherent%2520dynamic%2520attention%250Asparsity%2520throughout%2520the%2520training%2520process.%2520DSV%2520employs%2520a%2520two-stage%2520training%250Aalgorithm%2520that%2520exploits%2520sparsity%2520patterns%252C%2520focusing%2520on%2520critical%2520elements%250Asupported%2520by%2520efficient%252C%2520tailored%2520kernels.%2520To%2520accommodate%2520the%2520new%2520sparsity%250Adimension%252C%2520we%2520develop%2520a%2520hybrid%2520sparsity-aware%2520context%2520parallelism%2520that%250Aeffectively%2520scales%2520to%2520large%2520inputs%2520by%2520addressing%2520the%2520heterogeneity%2520of%2520sparsity%250Aacross%2520attention%2520heads%2520and%2520blocks%252C%2520resulting%2520in%2520optimized%2520sparse%2520computation%250Aand%2520communication.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520DSV%2520achieves%2520up%2520to%250A3.02x%2520gain%2520in%2520training%2520throughput%2520with%2520nearly%2520no%2520quality%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSV%3A%20Exploiting%20Dynamic%20Sparsity%20to%20Accelerate%20Large-Scale%20Video%20DiT%0A%20%20Training&entry.906535625=Xin%20Tan%20and%20Yuetao%20Chen%20and%20Yimin%20Jiang%20and%20Xing%20Chen%20and%20Kun%20Yan%20and%20Nan%20Duan%20and%20Yibo%20Zhu%20and%20Daxin%20Jiang%20and%20Hong%20Xu&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20have%20shown%20remarkable%20performance%20in%20modeling%0Aand%20generating%20high-quality%20videos.%20However%2C%20the%20quadratic%20computational%0Acomplexity%20of%203D%20full%20attention%20mechanism%20presents%20significant%20challenges%20in%0Ascaling%20video%20DiT%20training%2C%20especially%20for%20high-definition%20and%20lengthy%20videos%2C%0Awhere%20attention%20can%20dominate%20up%20to%2095%25%20of%20the%20end-to-end%20time%20and%20necessitate%0Aspecialized%20communication%20paradigms%20to%20handle%20large%20input%20sizes.%0A%20%20This%20paper%20introduces%20DSV%2C%20a%20novel%20framework%20designed%20to%20accelerate%20and%20scale%0Athe%20training%20of%20video%20DiTs%20by%20leveraging%20the%20inherent%20dynamic%20attention%0Asparsity%20throughout%20the%20training%20process.%20DSV%20employs%20a%20two-stage%20training%0Aalgorithm%20that%20exploits%20sparsity%20patterns%2C%20focusing%20on%20critical%20elements%0Asupported%20by%20efficient%2C%20tailored%20kernels.%20To%20accommodate%20the%20new%20sparsity%0Adimension%2C%20we%20develop%20a%20hybrid%20sparsity-aware%20context%20parallelism%20that%0Aeffectively%20scales%20to%20large%20inputs%20by%20addressing%20the%20heterogeneity%20of%20sparsity%0Aacross%20attention%20heads%20and%20blocks%2C%20resulting%20in%20optimized%20sparse%20computation%0Aand%20communication.%20Extensive%20evaluations%20demonstrate%20that%20DSV%20achieves%20up%20to%0A3.02x%20gain%20in%20training%20throughput%20with%20nearly%20no%20quality%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07590v1&entry.124074799=Read"},
{"title": "Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance", "author": "Borui Xu and Yao Chen and Zeyi Wen and Weiguo Liu and Bingsheng He", "abstract": "  The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.\n", "link": "http://arxiv.org/abs/2502.00641v2", "date": "2025-02-11", "relevancy": 2.4985, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Small%20Language%20Models%20for%20News%20Summarization%3A%20Implications%0A%20%20and%20Factors%20Influencing%20Performance&body=Title%3A%20Evaluating%20Small%20Language%20Models%20for%20News%20Summarization%3A%20Implications%0A%20%20and%20Factors%20Influencing%20Performance%0AAuthor%3A%20Borui%20Xu%20and%20Yao%20Chen%20and%20Zeyi%20Wen%20and%20Weiguo%20Liu%20and%20Bingsheng%20He%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20efficient%20summarization%20tools%20in%0Aresource-constrained%20environments%20highlights%20the%20need%20for%20effective%20solutions.%0AWhile%20large%20language%20models%20%28LLMs%29%20deliver%20superior%20summarization%20quality%2C%0Atheir%20high%20computational%20resource%20requirements%20limit%20practical%20use%0Aapplications.%20In%20contrast%2C%20small%20language%20models%20%28SLMs%29%20present%20a%20more%0Aaccessible%20alternative%2C%20capable%20of%20real-time%20summarization%20on%20edge%20devices.%0AHowever%2C%20their%20summarization%20capabilities%20and%20comparative%20performance%20against%0ALLMs%20remain%20underexplored.%20This%20paper%20addresses%20this%20gap%20by%20presenting%20a%0Acomprehensive%20evaluation%20of%2019%20SLMs%20for%20news%20summarization%20across%202%2C000%20news%0Asamples%2C%20focusing%20on%20relevance%2C%20coherence%2C%20factual%20consistency%2C%20and%20summary%0Alength.%20Our%20findings%20reveal%20significant%20variations%20in%20SLM%20performance%2C%20with%0Atop-performing%20models%20such%20as%20Phi3-Mini%20and%20Llama3.2-3B-Ins%20achieving%20results%0Acomparable%20to%20those%20of%2070B%20LLMs%20while%20generating%20more%20concise%20summaries.%0ANotably%2C%20SLMs%20are%20better%20suited%20for%20simple%20prompts%2C%20as%20overly%20complex%20prompts%0Amay%20lead%20to%20a%20decline%20in%20summary%20quality.%20Additionally%2C%20our%20analysis%20indicates%0Athat%20instruction%20tuning%20does%20not%20consistently%20enhance%20the%20news%20summarization%0Acapabilities%20of%20SLMs.%20This%20research%20not%20only%20contributes%20to%20the%20understanding%0Aof%20SLMs%20but%20also%20provides%20practical%20insights%20for%20researchers%20seeking%20efficient%0Asummarization%20solutions%20that%20balance%20performance%20and%20resource%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Small%2520Language%2520Models%2520for%2520News%2520Summarization%253A%2520Implications%250A%2520%2520and%2520Factors%2520Influencing%2520Performance%26entry.906535625%3DBorui%2520Xu%2520and%2520Yao%2520Chen%2520and%2520Zeyi%2520Wen%2520and%2520Weiguo%2520Liu%2520and%2520Bingsheng%2520He%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520efficient%2520summarization%2520tools%2520in%250Aresource-constrained%2520environments%2520highlights%2520the%2520need%2520for%2520effective%2520solutions.%250AWhile%2520large%2520language%2520models%2520%2528LLMs%2529%2520deliver%2520superior%2520summarization%2520quality%252C%250Atheir%2520high%2520computational%2520resource%2520requirements%2520limit%2520practical%2520use%250Aapplications.%2520In%2520contrast%252C%2520small%2520language%2520models%2520%2528SLMs%2529%2520present%2520a%2520more%250Aaccessible%2520alternative%252C%2520capable%2520of%2520real-time%2520summarization%2520on%2520edge%2520devices.%250AHowever%252C%2520their%2520summarization%2520capabilities%2520and%2520comparative%2520performance%2520against%250ALLMs%2520remain%2520underexplored.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520presenting%2520a%250Acomprehensive%2520evaluation%2520of%252019%2520SLMs%2520for%2520news%2520summarization%2520across%25202%252C000%2520news%250Asamples%252C%2520focusing%2520on%2520relevance%252C%2520coherence%252C%2520factual%2520consistency%252C%2520and%2520summary%250Alength.%2520Our%2520findings%2520reveal%2520significant%2520variations%2520in%2520SLM%2520performance%252C%2520with%250Atop-performing%2520models%2520such%2520as%2520Phi3-Mini%2520and%2520Llama3.2-3B-Ins%2520achieving%2520results%250Acomparable%2520to%2520those%2520of%252070B%2520LLMs%2520while%2520generating%2520more%2520concise%2520summaries.%250ANotably%252C%2520SLMs%2520are%2520better%2520suited%2520for%2520simple%2520prompts%252C%2520as%2520overly%2520complex%2520prompts%250Amay%2520lead%2520to%2520a%2520decline%2520in%2520summary%2520quality.%2520Additionally%252C%2520our%2520analysis%2520indicates%250Athat%2520instruction%2520tuning%2520does%2520not%2520consistently%2520enhance%2520the%2520news%2520summarization%250Acapabilities%2520of%2520SLMs.%2520This%2520research%2520not%2520only%2520contributes%2520to%2520the%2520understanding%250Aof%2520SLMs%2520but%2520also%2520provides%2520practical%2520insights%2520for%2520researchers%2520seeking%2520efficient%250Asummarization%2520solutions%2520that%2520balance%2520performance%2520and%2520resource%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Small%20Language%20Models%20for%20News%20Summarization%3A%20Implications%0A%20%20and%20Factors%20Influencing%20Performance&entry.906535625=Borui%20Xu%20and%20Yao%20Chen%20and%20Zeyi%20Wen%20and%20Weiguo%20Liu%20and%20Bingsheng%20He&entry.1292438233=%20%20The%20increasing%20demand%20for%20efficient%20summarization%20tools%20in%0Aresource-constrained%20environments%20highlights%20the%20need%20for%20effective%20solutions.%0AWhile%20large%20language%20models%20%28LLMs%29%20deliver%20superior%20summarization%20quality%2C%0Atheir%20high%20computational%20resource%20requirements%20limit%20practical%20use%0Aapplications.%20In%20contrast%2C%20small%20language%20models%20%28SLMs%29%20present%20a%20more%0Aaccessible%20alternative%2C%20capable%20of%20real-time%20summarization%20on%20edge%20devices.%0AHowever%2C%20their%20summarization%20capabilities%20and%20comparative%20performance%20against%0ALLMs%20remain%20underexplored.%20This%20paper%20addresses%20this%20gap%20by%20presenting%20a%0Acomprehensive%20evaluation%20of%2019%20SLMs%20for%20news%20summarization%20across%202%2C000%20news%0Asamples%2C%20focusing%20on%20relevance%2C%20coherence%2C%20factual%20consistency%2C%20and%20summary%0Alength.%20Our%20findings%20reveal%20significant%20variations%20in%20SLM%20performance%2C%20with%0Atop-performing%20models%20such%20as%20Phi3-Mini%20and%20Llama3.2-3B-Ins%20achieving%20results%0Acomparable%20to%20those%20of%2070B%20LLMs%20while%20generating%20more%20concise%20summaries.%0ANotably%2C%20SLMs%20are%20better%20suited%20for%20simple%20prompts%2C%20as%20overly%20complex%20prompts%0Amay%20lead%20to%20a%20decline%20in%20summary%20quality.%20Additionally%2C%20our%20analysis%20indicates%0Athat%20instruction%20tuning%20does%20not%20consistently%20enhance%20the%20news%20summarization%0Acapabilities%20of%20SLMs.%20This%20research%20not%20only%20contributes%20to%20the%20understanding%0Aof%20SLMs%20but%20also%20provides%20practical%20insights%20for%20researchers%20seeking%20efficient%0Asummarization%20solutions%20that%20balance%20performance%20and%20resource%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00641v2&entry.124074799=Read"},
{"title": "HGTUL: A Hypergraph-based Model For Trajectory User Linking", "author": "Fengjie Chang and Xinning Zhu and Zheng Hu and Yang Qin", "abstract": "  Trajectory User Linking (TUL), which links anonymous trajectories with users\nwho generate them, plays a crucial role in modeling human mobility. Despite\nsignificant advancements in this field, existing studies primarily neglect the\nhigh-order inter-trajectory relationships, which represent complex associations\namong multiple trajectories, manifested through multi-location co-occurrence\npatterns emerging when trajectories intersect at various Points of Interest\n(POIs). Furthermore, they also overlook the variable influence of POIs on\ndifferent trajectories, as well as the user class imbalance problem caused by\ndisparities in user activity levels and check-in frequencies. To address these\nlimitations, we propose a novel HyperGraph-based multi-perspective Trajectory\nUser Linking model (HGTUL). Our model learns trajectory representations from\nboth relational and spatio-temporal perspectives: (1) it captures high-order\nassociations among trajectories by constructing a trajectory hypergraph and\nleverages a hypergraph attention network to learn the variable impact of POIs\non trajectories; (2) it models the spatio-temporal characteristics of\ntrajectories by incorporating their temporal and spatial information into a\nsequential encoder. Moreover, we design a data balancing method to effectively\naddress the user class imbalance problem and experimentally validate its\nsignificance in TUL. Extensive experiments on three real-world datasets\ndemonstrate that HGTUL outperforms state-of-the-art baselines, achieving\nimprovements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics,\nrespectively.\n", "link": "http://arxiv.org/abs/2502.07549v1", "date": "2025-02-11", "relevancy": 2.4937, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGTUL%3A%20A%20Hypergraph-based%20Model%20For%20Trajectory%20User%20Linking&body=Title%3A%20HGTUL%3A%20A%20Hypergraph-based%20Model%20For%20Trajectory%20User%20Linking%0AAuthor%3A%20Fengjie%20Chang%20and%20Xinning%20Zhu%20and%20Zheng%20Hu%20and%20Yang%20Qin%0AAbstract%3A%20%20%20Trajectory%20User%20Linking%20%28TUL%29%2C%20which%20links%20anonymous%20trajectories%20with%20users%0Awho%20generate%20them%2C%20plays%20a%20crucial%20role%20in%20modeling%20human%20mobility.%20Despite%0Asignificant%20advancements%20in%20this%20field%2C%20existing%20studies%20primarily%20neglect%20the%0Ahigh-order%20inter-trajectory%20relationships%2C%20which%20represent%20complex%20associations%0Aamong%20multiple%20trajectories%2C%20manifested%20through%20multi-location%20co-occurrence%0Apatterns%20emerging%20when%20trajectories%20intersect%20at%20various%20Points%20of%20Interest%0A%28POIs%29.%20Furthermore%2C%20they%20also%20overlook%20the%20variable%20influence%20of%20POIs%20on%0Adifferent%20trajectories%2C%20as%20well%20as%20the%20user%20class%20imbalance%20problem%20caused%20by%0Adisparities%20in%20user%20activity%20levels%20and%20check-in%20frequencies.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20novel%20HyperGraph-based%20multi-perspective%20Trajectory%0AUser%20Linking%20model%20%28HGTUL%29.%20Our%20model%20learns%20trajectory%20representations%20from%0Aboth%20relational%20and%20spatio-temporal%20perspectives%3A%20%281%29%20it%20captures%20high-order%0Aassociations%20among%20trajectories%20by%20constructing%20a%20trajectory%20hypergraph%20and%0Aleverages%20a%20hypergraph%20attention%20network%20to%20learn%20the%20variable%20impact%20of%20POIs%0Aon%20trajectories%3B%20%282%29%20it%20models%20the%20spatio-temporal%20characteristics%20of%0Atrajectories%20by%20incorporating%20their%20temporal%20and%20spatial%20information%20into%20a%0Asequential%20encoder.%20Moreover%2C%20we%20design%20a%20data%20balancing%20method%20to%20effectively%0Aaddress%20the%20user%20class%20imbalance%20problem%20and%20experimentally%20validate%20its%0Asignificance%20in%20TUL.%20Extensive%20experiments%20on%20three%20real-world%20datasets%0Ademonstrate%20that%20HGTUL%20outperforms%20state-of-the-art%20baselines%2C%20achieving%0Aimprovements%20of%202.57%25~20.09%25%20and%205.68%25~26.00%25%20in%20ACC%401%20and%20Macro-F1%20metrics%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGTUL%253A%2520A%2520Hypergraph-based%2520Model%2520For%2520Trajectory%2520User%2520Linking%26entry.906535625%3DFengjie%2520Chang%2520and%2520Xinning%2520Zhu%2520and%2520Zheng%2520Hu%2520and%2520Yang%2520Qin%26entry.1292438233%3D%2520%2520Trajectory%2520User%2520Linking%2520%2528TUL%2529%252C%2520which%2520links%2520anonymous%2520trajectories%2520with%2520users%250Awho%2520generate%2520them%252C%2520plays%2520a%2520crucial%2520role%2520in%2520modeling%2520human%2520mobility.%2520Despite%250Asignificant%2520advancements%2520in%2520this%2520field%252C%2520existing%2520studies%2520primarily%2520neglect%2520the%250Ahigh-order%2520inter-trajectory%2520relationships%252C%2520which%2520represent%2520complex%2520associations%250Aamong%2520multiple%2520trajectories%252C%2520manifested%2520through%2520multi-location%2520co-occurrence%250Apatterns%2520emerging%2520when%2520trajectories%2520intersect%2520at%2520various%2520Points%2520of%2520Interest%250A%2528POIs%2529.%2520Furthermore%252C%2520they%2520also%2520overlook%2520the%2520variable%2520influence%2520of%2520POIs%2520on%250Adifferent%2520trajectories%252C%2520as%2520well%2520as%2520the%2520user%2520class%2520imbalance%2520problem%2520caused%2520by%250Adisparities%2520in%2520user%2520activity%2520levels%2520and%2520check-in%2520frequencies.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520novel%2520HyperGraph-based%2520multi-perspective%2520Trajectory%250AUser%2520Linking%2520model%2520%2528HGTUL%2529.%2520Our%2520model%2520learns%2520trajectory%2520representations%2520from%250Aboth%2520relational%2520and%2520spatio-temporal%2520perspectives%253A%2520%25281%2529%2520it%2520captures%2520high-order%250Aassociations%2520among%2520trajectories%2520by%2520constructing%2520a%2520trajectory%2520hypergraph%2520and%250Aleverages%2520a%2520hypergraph%2520attention%2520network%2520to%2520learn%2520the%2520variable%2520impact%2520of%2520POIs%250Aon%2520trajectories%253B%2520%25282%2529%2520it%2520models%2520the%2520spatio-temporal%2520characteristics%2520of%250Atrajectories%2520by%2520incorporating%2520their%2520temporal%2520and%2520spatial%2520information%2520into%2520a%250Asequential%2520encoder.%2520Moreover%252C%2520we%2520design%2520a%2520data%2520balancing%2520method%2520to%2520effectively%250Aaddress%2520the%2520user%2520class%2520imbalance%2520problem%2520and%2520experimentally%2520validate%2520its%250Asignificance%2520in%2520TUL.%2520Extensive%2520experiments%2520on%2520three%2520real-world%2520datasets%250Ademonstrate%2520that%2520HGTUL%2520outperforms%2520state-of-the-art%2520baselines%252C%2520achieving%250Aimprovements%2520of%25202.57%2525~20.09%2525%2520and%25205.68%2525~26.00%2525%2520in%2520ACC%25401%2520and%2520Macro-F1%2520metrics%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGTUL%3A%20A%20Hypergraph-based%20Model%20For%20Trajectory%20User%20Linking&entry.906535625=Fengjie%20Chang%20and%20Xinning%20Zhu%20and%20Zheng%20Hu%20and%20Yang%20Qin&entry.1292438233=%20%20Trajectory%20User%20Linking%20%28TUL%29%2C%20which%20links%20anonymous%20trajectories%20with%20users%0Awho%20generate%20them%2C%20plays%20a%20crucial%20role%20in%20modeling%20human%20mobility.%20Despite%0Asignificant%20advancements%20in%20this%20field%2C%20existing%20studies%20primarily%20neglect%20the%0Ahigh-order%20inter-trajectory%20relationships%2C%20which%20represent%20complex%20associations%0Aamong%20multiple%20trajectories%2C%20manifested%20through%20multi-location%20co-occurrence%0Apatterns%20emerging%20when%20trajectories%20intersect%20at%20various%20Points%20of%20Interest%0A%28POIs%29.%20Furthermore%2C%20they%20also%20overlook%20the%20variable%20influence%20of%20POIs%20on%0Adifferent%20trajectories%2C%20as%20well%20as%20the%20user%20class%20imbalance%20problem%20caused%20by%0Adisparities%20in%20user%20activity%20levels%20and%20check-in%20frequencies.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20novel%20HyperGraph-based%20multi-perspective%20Trajectory%0AUser%20Linking%20model%20%28HGTUL%29.%20Our%20model%20learns%20trajectory%20representations%20from%0Aboth%20relational%20and%20spatio-temporal%20perspectives%3A%20%281%29%20it%20captures%20high-order%0Aassociations%20among%20trajectories%20by%20constructing%20a%20trajectory%20hypergraph%20and%0Aleverages%20a%20hypergraph%20attention%20network%20to%20learn%20the%20variable%20impact%20of%20POIs%0Aon%20trajectories%3B%20%282%29%20it%20models%20the%20spatio-temporal%20characteristics%20of%0Atrajectories%20by%20incorporating%20their%20temporal%20and%20spatial%20information%20into%20a%0Asequential%20encoder.%20Moreover%2C%20we%20design%20a%20data%20balancing%20method%20to%20effectively%0Aaddress%20the%20user%20class%20imbalance%20problem%20and%20experimentally%20validate%20its%0Asignificance%20in%20TUL.%20Extensive%20experiments%20on%20three%20real-world%20datasets%0Ademonstrate%20that%20HGTUL%20outperforms%20state-of-the-art%20baselines%2C%20achieving%0Aimprovements%20of%202.57%25~20.09%25%20and%205.68%25~26.00%25%20in%20ACC%401%20and%20Macro-F1%20metrics%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07549v1&entry.124074799=Read"},
{"title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models", "author": "Shengkun Tang and Oliver Sieberling and Eldar Kurtic and Zhiqiang Shen and Dan Alistarh", "abstract": "  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.\n", "link": "http://arxiv.org/abs/2502.07780v1", "date": "2025-02-11", "relevancy": 2.4905, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DarwinLM%3A%20Evolutionary%20Structured%20Pruning%20of%20Large%20Language%20Models&body=Title%3A%20DarwinLM%3A%20Evolutionary%20Structured%20Pruning%20of%20Large%20Language%20Models%0AAuthor%3A%20Shengkun%20Tang%20and%20Oliver%20Sieberling%20and%20Eldar%20Kurtic%20and%20Zhiqiang%20Shen%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20success%20across%20various%0ANLP%20tasks.%20However%2C%20their%20massive%20computational%20costs%20limit%20their%20widespread%0Ause%2C%20particularly%20in%20real-time%20applications.%20Structured%20pruning%20offers%20an%0Aeffective%20solution%20by%20compressing%20models%20and%20directly%20providing%20end-to-end%0Aspeed%20improvements%2C%20regardless%20of%20the%20hardware%20environment.%20Meanwhile%2C%0Adifferent%20components%20of%20the%20model%20exhibit%20varying%20sensitivities%20towards%0Apruning%2C%20calling%20for%20%5Cemph%7Bnon-uniform%7D%20model%20compression.%20However%2C%20a%20pruning%0Amethod%20should%20not%20only%20identify%20a%20capable%20substructure%2C%20but%20also%20account%20for%0Apost-compression%20training.%20To%20this%20end%2C%20we%20propose%20%5Csysname%2C%20a%20method%20for%0A%5Cemph%7Btraining-aware%7D%20structured%20pruning.%20%5Csysname%20builds%20upon%20an%20evolutionary%0Asearch%20process%2C%20generating%20multiple%20offspring%20models%20in%20each%20generation%20through%0Amutation%2C%20and%20selecting%20the%20fittest%20for%20survival.%20To%20assess%20the%20effect%20of%0Apost-training%2C%20we%20incorporate%20a%20lightweight%2C%20multistep%20training%20process%20within%0Athe%20offspring%20population%2C%20progressively%20increasing%20the%20number%20of%20tokens%20and%0Aeliminating%20poorly%20performing%20models%20in%20each%20selection%20stage.%20We%20validate%20our%0Amethod%20through%20extensive%20experiments%20on%20Llama-2-7B%2C%20Llama-3.1-8B%20and%0AQwen-2.5-14B-Instruct%2C%20achieving%20state-of-the-art%20performance%20for%20structured%0Apruning.%20For%20instance%2C%20%5Csysname%20surpasses%20ShearedLlama%20while%20requiring%0A%245%5Ctimes%24%20less%20training%20data%20during%20post-compression%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDarwinLM%253A%2520Evolutionary%2520Structured%2520Pruning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DShengkun%2520Tang%2520and%2520Oliver%2520Sieberling%2520and%2520Eldar%2520Kurtic%2520and%2520Zhiqiang%2520Shen%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520significant%2520success%2520across%2520various%250ANLP%2520tasks.%2520However%252C%2520their%2520massive%2520computational%2520costs%2520limit%2520their%2520widespread%250Ause%252C%2520particularly%2520in%2520real-time%2520applications.%2520Structured%2520pruning%2520offers%2520an%250Aeffective%2520solution%2520by%2520compressing%2520models%2520and%2520directly%2520providing%2520end-to-end%250Aspeed%2520improvements%252C%2520regardless%2520of%2520the%2520hardware%2520environment.%2520Meanwhile%252C%250Adifferent%2520components%2520of%2520the%2520model%2520exhibit%2520varying%2520sensitivities%2520towards%250Apruning%252C%2520calling%2520for%2520%255Cemph%257Bnon-uniform%257D%2520model%2520compression.%2520However%252C%2520a%2520pruning%250Amethod%2520should%2520not%2520only%2520identify%2520a%2520capable%2520substructure%252C%2520but%2520also%2520account%2520for%250Apost-compression%2520training.%2520To%2520this%2520end%252C%2520we%2520propose%2520%255Csysname%252C%2520a%2520method%2520for%250A%255Cemph%257Btraining-aware%257D%2520structured%2520pruning.%2520%255Csysname%2520builds%2520upon%2520an%2520evolutionary%250Asearch%2520process%252C%2520generating%2520multiple%2520offspring%2520models%2520in%2520each%2520generation%2520through%250Amutation%252C%2520and%2520selecting%2520the%2520fittest%2520for%2520survival.%2520To%2520assess%2520the%2520effect%2520of%250Apost-training%252C%2520we%2520incorporate%2520a%2520lightweight%252C%2520multistep%2520training%2520process%2520within%250Athe%2520offspring%2520population%252C%2520progressively%2520increasing%2520the%2520number%2520of%2520tokens%2520and%250Aeliminating%2520poorly%2520performing%2520models%2520in%2520each%2520selection%2520stage.%2520We%2520validate%2520our%250Amethod%2520through%2520extensive%2520experiments%2520on%2520Llama-2-7B%252C%2520Llama-3.1-8B%2520and%250AQwen-2.5-14B-Instruct%252C%2520achieving%2520state-of-the-art%2520performance%2520for%2520structured%250Apruning.%2520For%2520instance%252C%2520%255Csysname%2520surpasses%2520ShearedLlama%2520while%2520requiring%250A%25245%255Ctimes%2524%2520less%2520training%2520data%2520during%2520post-compression%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DarwinLM%3A%20Evolutionary%20Structured%20Pruning%20of%20Large%20Language%20Models&entry.906535625=Shengkun%20Tang%20and%20Oliver%20Sieberling%20and%20Eldar%20Kurtic%20and%20Zhiqiang%20Shen%20and%20Dan%20Alistarh&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20success%20across%20various%0ANLP%20tasks.%20However%2C%20their%20massive%20computational%20costs%20limit%20their%20widespread%0Ause%2C%20particularly%20in%20real-time%20applications.%20Structured%20pruning%20offers%20an%0Aeffective%20solution%20by%20compressing%20models%20and%20directly%20providing%20end-to-end%0Aspeed%20improvements%2C%20regardless%20of%20the%20hardware%20environment.%20Meanwhile%2C%0Adifferent%20components%20of%20the%20model%20exhibit%20varying%20sensitivities%20towards%0Apruning%2C%20calling%20for%20%5Cemph%7Bnon-uniform%7D%20model%20compression.%20However%2C%20a%20pruning%0Amethod%20should%20not%20only%20identify%20a%20capable%20substructure%2C%20but%20also%20account%20for%0Apost-compression%20training.%20To%20this%20end%2C%20we%20propose%20%5Csysname%2C%20a%20method%20for%0A%5Cemph%7Btraining-aware%7D%20structured%20pruning.%20%5Csysname%20builds%20upon%20an%20evolutionary%0Asearch%20process%2C%20generating%20multiple%20offspring%20models%20in%20each%20generation%20through%0Amutation%2C%20and%20selecting%20the%20fittest%20for%20survival.%20To%20assess%20the%20effect%20of%0Apost-training%2C%20we%20incorporate%20a%20lightweight%2C%20multistep%20training%20process%20within%0Athe%20offspring%20population%2C%20progressively%20increasing%20the%20number%20of%20tokens%20and%0Aeliminating%20poorly%20performing%20models%20in%20each%20selection%20stage.%20We%20validate%20our%0Amethod%20through%20extensive%20experiments%20on%20Llama-2-7B%2C%20Llama-3.1-8B%20and%0AQwen-2.5-14B-Instruct%2C%20achieving%20state-of-the-art%20performance%20for%20structured%0Apruning.%20For%20instance%2C%20%5Csysname%20surpasses%20ShearedLlama%20while%20requiring%0A%245%5Ctimes%24%20less%20training%20data%20during%20post-compression%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07780v1&entry.124074799=Read"},
{"title": "An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation\n  during Multi-stage Fine-tuning", "author": "Andrew Bai and Chih-Kuan Yeh and Cho-Jui Hsieh and Ankur Taly", "abstract": "  Incrementally fine-tuning foundational models on new tasks or domains is now\nthe de facto approach in NLP. A known pitfall of this approach is the\n\\emph{catastrophic forgetting} of prior knowledge that happens during\nfine-tuning. A common approach to alleviate such forgetting is to rehearse\nsamples from prior tasks during fine-tuning. Several existing works assume a\nfixed memory buffer to store prior task examples, while relying on inferences\n(forward passes) with the model at hand for choosing examples for rehearsal\nfrom the buffer. However, given the increasing computational cost of model\ninference, and decreasing cost of data storage, we focus on the setting to\nrehearse samples with a fixed computational budget instead of a fixed memory\nbudget. We propose a sampling scheme, \\texttt{\\bf mix-cd}, that prioritizes\nrehearsal of ``collateral damage'' samples, which are samples predicted\ncorrectly by the prior model but forgotten by the incrementally tuned one. The\ncrux of our scheme is a procedure to efficiently estimate the density of\ncollateral damage samples without incurring additional model inferences. Our\napproach is computationally efficient, easy to implement, and outperforms\nseveral leading continual learning methods in compute-constrained settings. All\nthe code will be publicly available at\nhttps://github.com/jybai/mix-cd-rehearsal.\n", "link": "http://arxiv.org/abs/2402.08096v3", "date": "2025-02-11", "relevancy": 2.4867, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5036}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4958}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Rehearsal%20Scheme%20for%20Catastrophic%20Forgetting%20Mitigation%0A%20%20during%20Multi-stage%20Fine-tuning&body=Title%3A%20An%20Efficient%20Rehearsal%20Scheme%20for%20Catastrophic%20Forgetting%20Mitigation%0A%20%20during%20Multi-stage%20Fine-tuning%0AAuthor%3A%20Andrew%20Bai%20and%20Chih-Kuan%20Yeh%20and%20Cho-Jui%20Hsieh%20and%20Ankur%20Taly%0AAbstract%3A%20%20%20Incrementally%20fine-tuning%20foundational%20models%20on%20new%20tasks%20or%20domains%20is%20now%0Athe%20de%20facto%20approach%20in%20NLP.%20A%20known%20pitfall%20of%20this%20approach%20is%20the%0A%5Cemph%7Bcatastrophic%20forgetting%7D%20of%20prior%20knowledge%20that%20happens%20during%0Afine-tuning.%20A%20common%20approach%20to%20alleviate%20such%20forgetting%20is%20to%20rehearse%0Asamples%20from%20prior%20tasks%20during%20fine-tuning.%20Several%20existing%20works%20assume%20a%0Afixed%20memory%20buffer%20to%20store%20prior%20task%20examples%2C%20while%20relying%20on%20inferences%0A%28forward%20passes%29%20with%20the%20model%20at%20hand%20for%20choosing%20examples%20for%20rehearsal%0Afrom%20the%20buffer.%20However%2C%20given%20the%20increasing%20computational%20cost%20of%20model%0Ainference%2C%20and%20decreasing%20cost%20of%20data%20storage%2C%20we%20focus%20on%20the%20setting%20to%0Arehearse%20samples%20with%20a%20fixed%20computational%20budget%20instead%20of%20a%20fixed%20memory%0Abudget.%20We%20propose%20a%20sampling%20scheme%2C%20%5Ctexttt%7B%5Cbf%20mix-cd%7D%2C%20that%20prioritizes%0Arehearsal%20of%20%60%60collateral%20damage%27%27%20samples%2C%20which%20are%20samples%20predicted%0Acorrectly%20by%20the%20prior%20model%20but%20forgotten%20by%20the%20incrementally%20tuned%20one.%20The%0Acrux%20of%20our%20scheme%20is%20a%20procedure%20to%20efficiently%20estimate%20the%20density%20of%0Acollateral%20damage%20samples%20without%20incurring%20additional%20model%20inferences.%20Our%0Aapproach%20is%20computationally%20efficient%2C%20easy%20to%20implement%2C%20and%20outperforms%0Aseveral%20leading%20continual%20learning%20methods%20in%20compute-constrained%20settings.%20All%0Athe%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/jybai/mix-cd-rehearsal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08096v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Rehearsal%2520Scheme%2520for%2520Catastrophic%2520Forgetting%2520Mitigation%250A%2520%2520during%2520Multi-stage%2520Fine-tuning%26entry.906535625%3DAndrew%2520Bai%2520and%2520Chih-Kuan%2520Yeh%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Ankur%2520Taly%26entry.1292438233%3D%2520%2520Incrementally%2520fine-tuning%2520foundational%2520models%2520on%2520new%2520tasks%2520or%2520domains%2520is%2520now%250Athe%2520de%2520facto%2520approach%2520in%2520NLP.%2520A%2520known%2520pitfall%2520of%2520this%2520approach%2520is%2520the%250A%255Cemph%257Bcatastrophic%2520forgetting%257D%2520of%2520prior%2520knowledge%2520that%2520happens%2520during%250Afine-tuning.%2520A%2520common%2520approach%2520to%2520alleviate%2520such%2520forgetting%2520is%2520to%2520rehearse%250Asamples%2520from%2520prior%2520tasks%2520during%2520fine-tuning.%2520Several%2520existing%2520works%2520assume%2520a%250Afixed%2520memory%2520buffer%2520to%2520store%2520prior%2520task%2520examples%252C%2520while%2520relying%2520on%2520inferences%250A%2528forward%2520passes%2529%2520with%2520the%2520model%2520at%2520hand%2520for%2520choosing%2520examples%2520for%2520rehearsal%250Afrom%2520the%2520buffer.%2520However%252C%2520given%2520the%2520increasing%2520computational%2520cost%2520of%2520model%250Ainference%252C%2520and%2520decreasing%2520cost%2520of%2520data%2520storage%252C%2520we%2520focus%2520on%2520the%2520setting%2520to%250Arehearse%2520samples%2520with%2520a%2520fixed%2520computational%2520budget%2520instead%2520of%2520a%2520fixed%2520memory%250Abudget.%2520We%2520propose%2520a%2520sampling%2520scheme%252C%2520%255Ctexttt%257B%255Cbf%2520mix-cd%257D%252C%2520that%2520prioritizes%250Arehearsal%2520of%2520%2560%2560collateral%2520damage%2527%2527%2520samples%252C%2520which%2520are%2520samples%2520predicted%250Acorrectly%2520by%2520the%2520prior%2520model%2520but%2520forgotten%2520by%2520the%2520incrementally%2520tuned%2520one.%2520The%250Acrux%2520of%2520our%2520scheme%2520is%2520a%2520procedure%2520to%2520efficiently%2520estimate%2520the%2520density%2520of%250Acollateral%2520damage%2520samples%2520without%2520incurring%2520additional%2520model%2520inferences.%2520Our%250Aapproach%2520is%2520computationally%2520efficient%252C%2520easy%2520to%2520implement%252C%2520and%2520outperforms%250Aseveral%2520leading%2520continual%2520learning%2520methods%2520in%2520compute-constrained%2520settings.%2520All%250Athe%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/jybai/mix-cd-rehearsal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08096v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Rehearsal%20Scheme%20for%20Catastrophic%20Forgetting%20Mitigation%0A%20%20during%20Multi-stage%20Fine-tuning&entry.906535625=Andrew%20Bai%20and%20Chih-Kuan%20Yeh%20and%20Cho-Jui%20Hsieh%20and%20Ankur%20Taly&entry.1292438233=%20%20Incrementally%20fine-tuning%20foundational%20models%20on%20new%20tasks%20or%20domains%20is%20now%0Athe%20de%20facto%20approach%20in%20NLP.%20A%20known%20pitfall%20of%20this%20approach%20is%20the%0A%5Cemph%7Bcatastrophic%20forgetting%7D%20of%20prior%20knowledge%20that%20happens%20during%0Afine-tuning.%20A%20common%20approach%20to%20alleviate%20such%20forgetting%20is%20to%20rehearse%0Asamples%20from%20prior%20tasks%20during%20fine-tuning.%20Several%20existing%20works%20assume%20a%0Afixed%20memory%20buffer%20to%20store%20prior%20task%20examples%2C%20while%20relying%20on%20inferences%0A%28forward%20passes%29%20with%20the%20model%20at%20hand%20for%20choosing%20examples%20for%20rehearsal%0Afrom%20the%20buffer.%20However%2C%20given%20the%20increasing%20computational%20cost%20of%20model%0Ainference%2C%20and%20decreasing%20cost%20of%20data%20storage%2C%20we%20focus%20on%20the%20setting%20to%0Arehearse%20samples%20with%20a%20fixed%20computational%20budget%20instead%20of%20a%20fixed%20memory%0Abudget.%20We%20propose%20a%20sampling%20scheme%2C%20%5Ctexttt%7B%5Cbf%20mix-cd%7D%2C%20that%20prioritizes%0Arehearsal%20of%20%60%60collateral%20damage%27%27%20samples%2C%20which%20are%20samples%20predicted%0Acorrectly%20by%20the%20prior%20model%20but%20forgotten%20by%20the%20incrementally%20tuned%20one.%20The%0Acrux%20of%20our%20scheme%20is%20a%20procedure%20to%20efficiently%20estimate%20the%20density%20of%0Acollateral%20damage%20samples%20without%20incurring%20additional%20model%20inferences.%20Our%0Aapproach%20is%20computationally%20efficient%2C%20easy%20to%20implement%2C%20and%20outperforms%0Aseveral%20leading%20continual%20learning%20methods%20in%20compute-constrained%20settings.%20All%0Athe%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/jybai/mix-cd-rehearsal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08096v3&entry.124074799=Read"},
{"title": "NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized\n  Recommendation Systems", "author": "Shuli Wang and Xue Wei and Senjie Kou and Chi Wang and Wenshuai Chen and Qi Tang and Yinhua Zhu and Xiong Xiao and Xingxing Wang", "abstract": "  Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list. Due to the inherent challenges of\ncombinatorial search spaces, some current research adopts an\nevaluator-generator paradigm, with a generator generating feasible sequences\nand an evaluator selecting the best sequence based on the estimated list\nutility. However, these methods still face two issues. Firstly, due to the goal\ninconsistency problem between the evaluator and generator, the generator tends\nto fit the local optimal solution of exposure distribution rather than\ncombinatorial space optimization. Secondly, the strategy of generating target\nitems one by one is difficult to achieve optimality because it ignores the\ninformation of subsequent items.\n  To address these issues, we propose a utilizing Neighbor Lists model for\nGenerative Reranking (NLGR), which aims to improve the performance of the\ngenerator in the combinatorial space. NLGR follows the evaluator-generator\nparadigm and improves the generator's training and generating methods.\nSpecifically, we use neighbor lists in combination space to enhance the\ntraining process, making the generator perceive the relative scores and find\nthe optimization direction. Furthermore, we propose a novel sampling-based\nnon-autoregressive generation method, which allows the generator to jump\nflexibly from the current list to any neighbor list. Extensive experiments on\npublic and industrial datasets validate NLGR's effectiveness and we have\nsuccessfully deployed NLGR on the Meituan food delivery platform.\n", "link": "http://arxiv.org/abs/2502.06097v2", "date": "2025-02-11", "relevancy": 2.4016, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5052}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.468}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NLGR%3A%20Utilizing%20Neighbor%20Lists%20for%20Generative%20Rerank%20in%20Personalized%0A%20%20Recommendation%20Systems&body=Title%3A%20NLGR%3A%20Utilizing%20Neighbor%20Lists%20for%20Generative%20Rerank%20in%20Personalized%0A%20%20Recommendation%20Systems%0AAuthor%3A%20Shuli%20Wang%20and%20Xue%20Wei%20and%20Senjie%20Kou%20and%20Chi%20Wang%20and%20Wenshuai%20Chen%20and%20Qi%20Tang%20and%20Yinhua%20Zhu%20and%20Xiong%20Xiao%20and%20Xingxing%20Wang%0AAbstract%3A%20%20%20Reranking%20plays%20a%20crucial%20role%20in%20modern%20multi-stage%20recommender%20systems%20by%0Arearranging%20the%20initial%20ranking%20list.%20Due%20to%20the%20inherent%20challenges%20of%0Acombinatorial%20search%20spaces%2C%20some%20current%20research%20adopts%20an%0Aevaluator-generator%20paradigm%2C%20with%20a%20generator%20generating%20feasible%20sequences%0Aand%20an%20evaluator%20selecting%20the%20best%20sequence%20based%20on%20the%20estimated%20list%0Autility.%20However%2C%20these%20methods%20still%20face%20two%20issues.%20Firstly%2C%20due%20to%20the%20goal%0Ainconsistency%20problem%20between%20the%20evaluator%20and%20generator%2C%20the%20generator%20tends%0Ato%20fit%20the%20local%20optimal%20solution%20of%20exposure%20distribution%20rather%20than%0Acombinatorial%20space%20optimization.%20Secondly%2C%20the%20strategy%20of%20generating%20target%0Aitems%20one%20by%20one%20is%20difficult%20to%20achieve%20optimality%20because%20it%20ignores%20the%0Ainformation%20of%20subsequent%20items.%0A%20%20To%20address%20these%20issues%2C%20we%20propose%20a%20utilizing%20Neighbor%20Lists%20model%20for%0AGenerative%20Reranking%20%28NLGR%29%2C%20which%20aims%20to%20improve%20the%20performance%20of%20the%0Agenerator%20in%20the%20combinatorial%20space.%20NLGR%20follows%20the%20evaluator-generator%0Aparadigm%20and%20improves%20the%20generator%27s%20training%20and%20generating%20methods.%0ASpecifically%2C%20we%20use%20neighbor%20lists%20in%20combination%20space%20to%20enhance%20the%0Atraining%20process%2C%20making%20the%20generator%20perceive%20the%20relative%20scores%20and%20find%0Athe%20optimization%20direction.%20Furthermore%2C%20we%20propose%20a%20novel%20sampling-based%0Anon-autoregressive%20generation%20method%2C%20which%20allows%20the%20generator%20to%20jump%0Aflexibly%20from%20the%20current%20list%20to%20any%20neighbor%20list.%20Extensive%20experiments%20on%0Apublic%20and%20industrial%20datasets%20validate%20NLGR%27s%20effectiveness%20and%20we%20have%0Asuccessfully%20deployed%20NLGR%20on%20the%20Meituan%20food%20delivery%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNLGR%253A%2520Utilizing%2520Neighbor%2520Lists%2520for%2520Generative%2520Rerank%2520in%2520Personalized%250A%2520%2520Recommendation%2520Systems%26entry.906535625%3DShuli%2520Wang%2520and%2520Xue%2520Wei%2520and%2520Senjie%2520Kou%2520and%2520Chi%2520Wang%2520and%2520Wenshuai%2520Chen%2520and%2520Qi%2520Tang%2520and%2520Yinhua%2520Zhu%2520and%2520Xiong%2520Xiao%2520and%2520Xingxing%2520Wang%26entry.1292438233%3D%2520%2520Reranking%2520plays%2520a%2520crucial%2520role%2520in%2520modern%2520multi-stage%2520recommender%2520systems%2520by%250Arearranging%2520the%2520initial%2520ranking%2520list.%2520Due%2520to%2520the%2520inherent%2520challenges%2520of%250Acombinatorial%2520search%2520spaces%252C%2520some%2520current%2520research%2520adopts%2520an%250Aevaluator-generator%2520paradigm%252C%2520with%2520a%2520generator%2520generating%2520feasible%2520sequences%250Aand%2520an%2520evaluator%2520selecting%2520the%2520best%2520sequence%2520based%2520on%2520the%2520estimated%2520list%250Autility.%2520However%252C%2520these%2520methods%2520still%2520face%2520two%2520issues.%2520Firstly%252C%2520due%2520to%2520the%2520goal%250Ainconsistency%2520problem%2520between%2520the%2520evaluator%2520and%2520generator%252C%2520the%2520generator%2520tends%250Ato%2520fit%2520the%2520local%2520optimal%2520solution%2520of%2520exposure%2520distribution%2520rather%2520than%250Acombinatorial%2520space%2520optimization.%2520Secondly%252C%2520the%2520strategy%2520of%2520generating%2520target%250Aitems%2520one%2520by%2520one%2520is%2520difficult%2520to%2520achieve%2520optimality%2520because%2520it%2520ignores%2520the%250Ainformation%2520of%2520subsequent%2520items.%250A%2520%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520utilizing%2520Neighbor%2520Lists%2520model%2520for%250AGenerative%2520Reranking%2520%2528NLGR%2529%252C%2520which%2520aims%2520to%2520improve%2520the%2520performance%2520of%2520the%250Agenerator%2520in%2520the%2520combinatorial%2520space.%2520NLGR%2520follows%2520the%2520evaluator-generator%250Aparadigm%2520and%2520improves%2520the%2520generator%2527s%2520training%2520and%2520generating%2520methods.%250ASpecifically%252C%2520we%2520use%2520neighbor%2520lists%2520in%2520combination%2520space%2520to%2520enhance%2520the%250Atraining%2520process%252C%2520making%2520the%2520generator%2520perceive%2520the%2520relative%2520scores%2520and%2520find%250Athe%2520optimization%2520direction.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520sampling-based%250Anon-autoregressive%2520generation%2520method%252C%2520which%2520allows%2520the%2520generator%2520to%2520jump%250Aflexibly%2520from%2520the%2520current%2520list%2520to%2520any%2520neighbor%2520list.%2520Extensive%2520experiments%2520on%250Apublic%2520and%2520industrial%2520datasets%2520validate%2520NLGR%2527s%2520effectiveness%2520and%2520we%2520have%250Asuccessfully%2520deployed%2520NLGR%2520on%2520the%2520Meituan%2520food%2520delivery%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NLGR%3A%20Utilizing%20Neighbor%20Lists%20for%20Generative%20Rerank%20in%20Personalized%0A%20%20Recommendation%20Systems&entry.906535625=Shuli%20Wang%20and%20Xue%20Wei%20and%20Senjie%20Kou%20and%20Chi%20Wang%20and%20Wenshuai%20Chen%20and%20Qi%20Tang%20and%20Yinhua%20Zhu%20and%20Xiong%20Xiao%20and%20Xingxing%20Wang&entry.1292438233=%20%20Reranking%20plays%20a%20crucial%20role%20in%20modern%20multi-stage%20recommender%20systems%20by%0Arearranging%20the%20initial%20ranking%20list.%20Due%20to%20the%20inherent%20challenges%20of%0Acombinatorial%20search%20spaces%2C%20some%20current%20research%20adopts%20an%0Aevaluator-generator%20paradigm%2C%20with%20a%20generator%20generating%20feasible%20sequences%0Aand%20an%20evaluator%20selecting%20the%20best%20sequence%20based%20on%20the%20estimated%20list%0Autility.%20However%2C%20these%20methods%20still%20face%20two%20issues.%20Firstly%2C%20due%20to%20the%20goal%0Ainconsistency%20problem%20between%20the%20evaluator%20and%20generator%2C%20the%20generator%20tends%0Ato%20fit%20the%20local%20optimal%20solution%20of%20exposure%20distribution%20rather%20than%0Acombinatorial%20space%20optimization.%20Secondly%2C%20the%20strategy%20of%20generating%20target%0Aitems%20one%20by%20one%20is%20difficult%20to%20achieve%20optimality%20because%20it%20ignores%20the%0Ainformation%20of%20subsequent%20items.%0A%20%20To%20address%20these%20issues%2C%20we%20propose%20a%20utilizing%20Neighbor%20Lists%20model%20for%0AGenerative%20Reranking%20%28NLGR%29%2C%20which%20aims%20to%20improve%20the%20performance%20of%20the%0Agenerator%20in%20the%20combinatorial%20space.%20NLGR%20follows%20the%20evaluator-generator%0Aparadigm%20and%20improves%20the%20generator%27s%20training%20and%20generating%20methods.%0ASpecifically%2C%20we%20use%20neighbor%20lists%20in%20combination%20space%20to%20enhance%20the%0Atraining%20process%2C%20making%20the%20generator%20perceive%20the%20relative%20scores%20and%20find%0Athe%20optimization%20direction.%20Furthermore%2C%20we%20propose%20a%20novel%20sampling-based%0Anon-autoregressive%20generation%20method%2C%20which%20allows%20the%20generator%20to%20jump%0Aflexibly%20from%20the%20current%20list%20to%20any%20neighbor%20list.%20Extensive%20experiments%20on%0Apublic%20and%20industrial%20datasets%20validate%20NLGR%27s%20effectiveness%20and%20we%20have%0Asuccessfully%20deployed%20NLGR%20on%20the%20Meituan%20food%20delivery%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06097v2&entry.124074799=Read"},
{"title": "Towards bandit-based prompt-tuning for in-the-wild foundation agents", "author": "Finn Rietz and Oleg Smirnov and Sara Karimi and Lele Cao", "abstract": "  Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.\n", "link": "http://arxiv.org/abs/2502.06358v2", "date": "2025-02-11", "relevancy": 2.3972, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4861}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.477}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20bandit-based%20prompt-tuning%20for%20in-the-wild%20foundation%20agents&body=Title%3A%20Towards%20bandit-based%20prompt-tuning%20for%20in-the-wild%20foundation%20agents%0AAuthor%3A%20Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao%0AAbstract%3A%20%20%20Prompting%20has%20emerged%20as%20the%20dominant%20paradigm%20for%20adapting%20large%2C%0Apre-trained%20transformer-based%20models%20to%20downstream%20tasks.%20The%20Prompting%0ADecision%20Transformer%20%28PDT%29%20enables%20large-scale%2C%20multi-task%20offline%0Areinforcement%20learning%20pre-training%20by%20leveraging%20stochastic%20trajectory%20prompts%0Ato%20identify%20the%20target%20task.%20However%2C%20these%20prompts%20are%20sampled%20uniformly%20from%0Aexpert%20demonstrations%2C%20overlooking%20a%20critical%20limitation%3A%20Not%20all%20prompts%20are%0Aequally%20informative%20for%20differentiating%20between%20tasks.%20To%20address%20this%2C%20we%0Apropose%20an%20inference%20time%20bandit-based%20prompt-tuning%20framework%20that%20explores%0Aand%20optimizes%20trajectory%20prompt%20selection%20to%20enhance%20task%20performance.%20Our%0Aexperiments%20indicate%20not%20only%20clear%20performance%20gains%20due%20to%20bandit-based%0Aprompt-tuning%2C%20but%20also%20better%20sample%20complexity%2C%20scalability%2C%20and%20prompt%20space%0Aexploration%20compared%20to%20prompt-tuning%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520bandit-based%2520prompt-tuning%2520for%2520in-the-wild%2520foundation%2520agents%26entry.906535625%3DFinn%2520Rietz%2520and%2520Oleg%2520Smirnov%2520and%2520Sara%2520Karimi%2520and%2520Lele%2520Cao%26entry.1292438233%3D%2520%2520Prompting%2520has%2520emerged%2520as%2520the%2520dominant%2520paradigm%2520for%2520adapting%2520large%252C%250Apre-trained%2520transformer-based%2520models%2520to%2520downstream%2520tasks.%2520The%2520Prompting%250ADecision%2520Transformer%2520%2528PDT%2529%2520enables%2520large-scale%252C%2520multi-task%2520offline%250Areinforcement%2520learning%2520pre-training%2520by%2520leveraging%2520stochastic%2520trajectory%2520prompts%250Ato%2520identify%2520the%2520target%2520task.%2520However%252C%2520these%2520prompts%2520are%2520sampled%2520uniformly%2520from%250Aexpert%2520demonstrations%252C%2520overlooking%2520a%2520critical%2520limitation%253A%2520Not%2520all%2520prompts%2520are%250Aequally%2520informative%2520for%2520differentiating%2520between%2520tasks.%2520To%2520address%2520this%252C%2520we%250Apropose%2520an%2520inference%2520time%2520bandit-based%2520prompt-tuning%2520framework%2520that%2520explores%250Aand%2520optimizes%2520trajectory%2520prompt%2520selection%2520to%2520enhance%2520task%2520performance.%2520Our%250Aexperiments%2520indicate%2520not%2520only%2520clear%2520performance%2520gains%2520due%2520to%2520bandit-based%250Aprompt-tuning%252C%2520but%2520also%2520better%2520sample%2520complexity%252C%2520scalability%252C%2520and%2520prompt%2520space%250Aexploration%2520compared%2520to%2520prompt-tuning%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20bandit-based%20prompt-tuning%20for%20in-the-wild%20foundation%20agents&entry.906535625=Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao&entry.1292438233=%20%20Prompting%20has%20emerged%20as%20the%20dominant%20paradigm%20for%20adapting%20large%2C%0Apre-trained%20transformer-based%20models%20to%20downstream%20tasks.%20The%20Prompting%0ADecision%20Transformer%20%28PDT%29%20enables%20large-scale%2C%20multi-task%20offline%0Areinforcement%20learning%20pre-training%20by%20leveraging%20stochastic%20trajectory%20prompts%0Ato%20identify%20the%20target%20task.%20However%2C%20these%20prompts%20are%20sampled%20uniformly%20from%0Aexpert%20demonstrations%2C%20overlooking%20a%20critical%20limitation%3A%20Not%20all%20prompts%20are%0Aequally%20informative%20for%20differentiating%20between%20tasks.%20To%20address%20this%2C%20we%0Apropose%20an%20inference%20time%20bandit-based%20prompt-tuning%20framework%20that%20explores%0Aand%20optimizes%20trajectory%20prompt%20selection%20to%20enhance%20task%20performance.%20Our%0Aexperiments%20indicate%20not%20only%20clear%20performance%20gains%20due%20to%20bandit-based%0Aprompt-tuning%2C%20but%20also%20better%20sample%20complexity%2C%20scalability%2C%20and%20prompt%20space%0Aexploration%20compared%20to%20prompt-tuning%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06358v2&entry.124074799=Read"},
{"title": "Efficient Image-to-Image Diffusion Classifier for Adversarial Robustness", "author": "Hefei Mei and Minjing Dong and Chang Xu", "abstract": "  Diffusion models (DMs) have demonstrated great potential in the field of\nadversarial robustness, where DM-based defense methods can achieve superior\ndefense capability without adversarial training. However, they all require huge\ncomputational costs due to the usage of large-scale pre-trained DMs, making it\ndifficult to conduct full evaluation under strong attacks and compare with\ntraditional CNN-based methods. Simply reducing the network size and timesteps\nin DMs could significantly harm the image generation quality, which invalidates\nprevious frameworks. To alleviate this issue, we redesign the diffusion\nframework from generating high-quality images to predicting distinguishable\nimage labels. Specifically, we employ an image translation framework to learn\nmany-to-one mapping from input samples to designed orthogonal image labels.\nBased on this framework, we introduce an efficient Image-to-Image diffusion\nclassifier with a pruned U-Net structure and reduced diffusion timesteps.\nBesides the framework, we redesign the optimization objective of DMs to fit the\ntarget of image classification, where a new classification loss is incorporated\nin the DM-based image translation framework to distinguish the generated label\nfrom those of other classes. We conduct sufficient evaluations of the proposed\nclassifier under various attacks on popular benchmarks. Extensive experiments\nshow that our method achieves better adversarial robustness with fewer\ncomputational costs than DM-based and CNN-based methods. The code is available\nat https://github.com/hfmei/IDC\n", "link": "http://arxiv.org/abs/2408.08502v2", "date": "2025-02-11", "relevancy": 2.3925, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6742}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5909}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Image-to-Image%20Diffusion%20Classifier%20for%20Adversarial%20Robustness&body=Title%3A%20Efficient%20Image-to-Image%20Diffusion%20Classifier%20for%20Adversarial%20Robustness%0AAuthor%3A%20Hefei%20Mei%20and%20Minjing%20Dong%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20great%20potential%20in%20the%20field%20of%0Aadversarial%20robustness%2C%20where%20DM-based%20defense%20methods%20can%20achieve%20superior%0Adefense%20capability%20without%20adversarial%20training.%20However%2C%20they%20all%20require%20huge%0Acomputational%20costs%20due%20to%20the%20usage%20of%20large-scale%20pre-trained%20DMs%2C%20making%20it%0Adifficult%20to%20conduct%20full%20evaluation%20under%20strong%20attacks%20and%20compare%20with%0Atraditional%20CNN-based%20methods.%20Simply%20reducing%20the%20network%20size%20and%20timesteps%0Ain%20DMs%20could%20significantly%20harm%20the%20image%20generation%20quality%2C%20which%20invalidates%0Aprevious%20frameworks.%20To%20alleviate%20this%20issue%2C%20we%20redesign%20the%20diffusion%0Aframework%20from%20generating%20high-quality%20images%20to%20predicting%20distinguishable%0Aimage%20labels.%20Specifically%2C%20we%20employ%20an%20image%20translation%20framework%20to%20learn%0Amany-to-one%20mapping%20from%20input%20samples%20to%20designed%20orthogonal%20image%20labels.%0ABased%20on%20this%20framework%2C%20we%20introduce%20an%20efficient%20Image-to-Image%20diffusion%0Aclassifier%20with%20a%20pruned%20U-Net%20structure%20and%20reduced%20diffusion%20timesteps.%0ABesides%20the%20framework%2C%20we%20redesign%20the%20optimization%20objective%20of%20DMs%20to%20fit%20the%0Atarget%20of%20image%20classification%2C%20where%20a%20new%20classification%20loss%20is%20incorporated%0Ain%20the%20DM-based%20image%20translation%20framework%20to%20distinguish%20the%20generated%20label%0Afrom%20those%20of%20other%20classes.%20We%20conduct%20sufficient%20evaluations%20of%20the%20proposed%0Aclassifier%20under%20various%20attacks%20on%20popular%20benchmarks.%20Extensive%20experiments%0Ashow%20that%20our%20method%20achieves%20better%20adversarial%20robustness%20with%20fewer%0Acomputational%20costs%20than%20DM-based%20and%20CNN-based%20methods.%20The%20code%20is%20available%0Aat%20https%3A//github.com/hfmei/IDC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Image-to-Image%2520Diffusion%2520Classifier%2520for%2520Adversarial%2520Robustness%26entry.906535625%3DHefei%2520Mei%2520and%2520Minjing%2520Dong%2520and%2520Chang%2520Xu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520demonstrated%2520great%2520potential%2520in%2520the%2520field%2520of%250Aadversarial%2520robustness%252C%2520where%2520DM-based%2520defense%2520methods%2520can%2520achieve%2520superior%250Adefense%2520capability%2520without%2520adversarial%2520training.%2520However%252C%2520they%2520all%2520require%2520huge%250Acomputational%2520costs%2520due%2520to%2520the%2520usage%2520of%2520large-scale%2520pre-trained%2520DMs%252C%2520making%2520it%250Adifficult%2520to%2520conduct%2520full%2520evaluation%2520under%2520strong%2520attacks%2520and%2520compare%2520with%250Atraditional%2520CNN-based%2520methods.%2520Simply%2520reducing%2520the%2520network%2520size%2520and%2520timesteps%250Ain%2520DMs%2520could%2520significantly%2520harm%2520the%2520image%2520generation%2520quality%252C%2520which%2520invalidates%250Aprevious%2520frameworks.%2520To%2520alleviate%2520this%2520issue%252C%2520we%2520redesign%2520the%2520diffusion%250Aframework%2520from%2520generating%2520high-quality%2520images%2520to%2520predicting%2520distinguishable%250Aimage%2520labels.%2520Specifically%252C%2520we%2520employ%2520an%2520image%2520translation%2520framework%2520to%2520learn%250Amany-to-one%2520mapping%2520from%2520input%2520samples%2520to%2520designed%2520orthogonal%2520image%2520labels.%250ABased%2520on%2520this%2520framework%252C%2520we%2520introduce%2520an%2520efficient%2520Image-to-Image%2520diffusion%250Aclassifier%2520with%2520a%2520pruned%2520U-Net%2520structure%2520and%2520reduced%2520diffusion%2520timesteps.%250ABesides%2520the%2520framework%252C%2520we%2520redesign%2520the%2520optimization%2520objective%2520of%2520DMs%2520to%2520fit%2520the%250Atarget%2520of%2520image%2520classification%252C%2520where%2520a%2520new%2520classification%2520loss%2520is%2520incorporated%250Ain%2520the%2520DM-based%2520image%2520translation%2520framework%2520to%2520distinguish%2520the%2520generated%2520label%250Afrom%2520those%2520of%2520other%2520classes.%2520We%2520conduct%2520sufficient%2520evaluations%2520of%2520the%2520proposed%250Aclassifier%2520under%2520various%2520attacks%2520on%2520popular%2520benchmarks.%2520Extensive%2520experiments%250Ashow%2520that%2520our%2520method%2520achieves%2520better%2520adversarial%2520robustness%2520with%2520fewer%250Acomputational%2520costs%2520than%2520DM-based%2520and%2520CNN-based%2520methods.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/hfmei/IDC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Image-to-Image%20Diffusion%20Classifier%20for%20Adversarial%20Robustness&entry.906535625=Hefei%20Mei%20and%20Minjing%20Dong%20and%20Chang%20Xu&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20great%20potential%20in%20the%20field%20of%0Aadversarial%20robustness%2C%20where%20DM-based%20defense%20methods%20can%20achieve%20superior%0Adefense%20capability%20without%20adversarial%20training.%20However%2C%20they%20all%20require%20huge%0Acomputational%20costs%20due%20to%20the%20usage%20of%20large-scale%20pre-trained%20DMs%2C%20making%20it%0Adifficult%20to%20conduct%20full%20evaluation%20under%20strong%20attacks%20and%20compare%20with%0Atraditional%20CNN-based%20methods.%20Simply%20reducing%20the%20network%20size%20and%20timesteps%0Ain%20DMs%20could%20significantly%20harm%20the%20image%20generation%20quality%2C%20which%20invalidates%0Aprevious%20frameworks.%20To%20alleviate%20this%20issue%2C%20we%20redesign%20the%20diffusion%0Aframework%20from%20generating%20high-quality%20images%20to%20predicting%20distinguishable%0Aimage%20labels.%20Specifically%2C%20we%20employ%20an%20image%20translation%20framework%20to%20learn%0Amany-to-one%20mapping%20from%20input%20samples%20to%20designed%20orthogonal%20image%20labels.%0ABased%20on%20this%20framework%2C%20we%20introduce%20an%20efficient%20Image-to-Image%20diffusion%0Aclassifier%20with%20a%20pruned%20U-Net%20structure%20and%20reduced%20diffusion%20timesteps.%0ABesides%20the%20framework%2C%20we%20redesign%20the%20optimization%20objective%20of%20DMs%20to%20fit%20the%0Atarget%20of%20image%20classification%2C%20where%20a%20new%20classification%20loss%20is%20incorporated%0Ain%20the%20DM-based%20image%20translation%20framework%20to%20distinguish%20the%20generated%20label%0Afrom%20those%20of%20other%20classes.%20We%20conduct%20sufficient%20evaluations%20of%20the%20proposed%0Aclassifier%20under%20various%20attacks%20on%20popular%20benchmarks.%20Extensive%20experiments%0Ashow%20that%20our%20method%20achieves%20better%20adversarial%20robustness%20with%20fewer%0Acomputational%20costs%20than%20DM-based%20and%20CNN-based%20methods.%20The%20code%20is%20available%0Aat%20https%3A//github.com/hfmei/IDC%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08502v2&entry.124074799=Read"},
{"title": "Distributed Value Decomposition Networks with Networked Agents", "author": "Guilherme S. Varela and Alberto Sardinha and Francisco S. Melo", "abstract": "  We investigate the problem of distributed training under partial\nobservability, whereby cooperative multi-agent reinforcement learning agents\n(MARL) maximize the expected cumulative joint reward. We propose distributed\nvalue decomposition networks (DVDN) that generate a joint Q-function that\nfactorizes into agent-wise Q-functions. Whereas the original value\ndecomposition networks rely on centralized training, our approach is suitable\nfor domains where centralized training is not possible and agents must learn by\ninteracting with the physical environment in a decentralized manner while\ncommunicating with their peers. DVDN overcomes the need for centralized\ntraining by locally estimating the shared objective. We contribute with two\ninnovative algorithms, DVDN and DVDN (GT), for the heterogeneous and\nhomogeneous agents settings respectively. Empirically, both algorithms\napproximate the performance of value decomposition networks, in spite of the\ninformation loss during communication, as demonstrated in ten MARL tasks in\nthree standard environments.\n", "link": "http://arxiv.org/abs/2502.07635v1", "date": "2025-02-11", "relevancy": 2.3919, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4797}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4785}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Value%20Decomposition%20Networks%20with%20Networked%20Agents&body=Title%3A%20Distributed%20Value%20Decomposition%20Networks%20with%20Networked%20Agents%0AAuthor%3A%20Guilherme%20S.%20Varela%20and%20Alberto%20Sardinha%20and%20Francisco%20S.%20Melo%0AAbstract%3A%20%20%20We%20investigate%20the%20problem%20of%20distributed%20training%20under%20partial%0Aobservability%2C%20whereby%20cooperative%20multi-agent%20reinforcement%20learning%20agents%0A%28MARL%29%20maximize%20the%20expected%20cumulative%20joint%20reward.%20We%20propose%20distributed%0Avalue%20decomposition%20networks%20%28DVDN%29%20that%20generate%20a%20joint%20Q-function%20that%0Afactorizes%20into%20agent-wise%20Q-functions.%20Whereas%20the%20original%20value%0Adecomposition%20networks%20rely%20on%20centralized%20training%2C%20our%20approach%20is%20suitable%0Afor%20domains%20where%20centralized%20training%20is%20not%20possible%20and%20agents%20must%20learn%20by%0Ainteracting%20with%20the%20physical%20environment%20in%20a%20decentralized%20manner%20while%0Acommunicating%20with%20their%20peers.%20DVDN%20overcomes%20the%20need%20for%20centralized%0Atraining%20by%20locally%20estimating%20the%20shared%20objective.%20We%20contribute%20with%20two%0Ainnovative%20algorithms%2C%20DVDN%20and%20DVDN%20%28GT%29%2C%20for%20the%20heterogeneous%20and%0Ahomogeneous%20agents%20settings%20respectively.%20Empirically%2C%20both%20algorithms%0Aapproximate%20the%20performance%20of%20value%20decomposition%20networks%2C%20in%20spite%20of%20the%0Ainformation%20loss%20during%20communication%2C%20as%20demonstrated%20in%20ten%20MARL%20tasks%20in%0Athree%20standard%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Value%2520Decomposition%2520Networks%2520with%2520Networked%2520Agents%26entry.906535625%3DGuilherme%2520S.%2520Varela%2520and%2520Alberto%2520Sardinha%2520and%2520Francisco%2520S.%2520Melo%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520problem%2520of%2520distributed%2520training%2520under%2520partial%250Aobservability%252C%2520whereby%2520cooperative%2520multi-agent%2520reinforcement%2520learning%2520agents%250A%2528MARL%2529%2520maximize%2520the%2520expected%2520cumulative%2520joint%2520reward.%2520We%2520propose%2520distributed%250Avalue%2520decomposition%2520networks%2520%2528DVDN%2529%2520that%2520generate%2520a%2520joint%2520Q-function%2520that%250Afactorizes%2520into%2520agent-wise%2520Q-functions.%2520Whereas%2520the%2520original%2520value%250Adecomposition%2520networks%2520rely%2520on%2520centralized%2520training%252C%2520our%2520approach%2520is%2520suitable%250Afor%2520domains%2520where%2520centralized%2520training%2520is%2520not%2520possible%2520and%2520agents%2520must%2520learn%2520by%250Ainteracting%2520with%2520the%2520physical%2520environment%2520in%2520a%2520decentralized%2520manner%2520while%250Acommunicating%2520with%2520their%2520peers.%2520DVDN%2520overcomes%2520the%2520need%2520for%2520centralized%250Atraining%2520by%2520locally%2520estimating%2520the%2520shared%2520objective.%2520We%2520contribute%2520with%2520two%250Ainnovative%2520algorithms%252C%2520DVDN%2520and%2520DVDN%2520%2528GT%2529%252C%2520for%2520the%2520heterogeneous%2520and%250Ahomogeneous%2520agents%2520settings%2520respectively.%2520Empirically%252C%2520both%2520algorithms%250Aapproximate%2520the%2520performance%2520of%2520value%2520decomposition%2520networks%252C%2520in%2520spite%2520of%2520the%250Ainformation%2520loss%2520during%2520communication%252C%2520as%2520demonstrated%2520in%2520ten%2520MARL%2520tasks%2520in%250Athree%2520standard%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Value%20Decomposition%20Networks%20with%20Networked%20Agents&entry.906535625=Guilherme%20S.%20Varela%20and%20Alberto%20Sardinha%20and%20Francisco%20S.%20Melo&entry.1292438233=%20%20We%20investigate%20the%20problem%20of%20distributed%20training%20under%20partial%0Aobservability%2C%20whereby%20cooperative%20multi-agent%20reinforcement%20learning%20agents%0A%28MARL%29%20maximize%20the%20expected%20cumulative%20joint%20reward.%20We%20propose%20distributed%0Avalue%20decomposition%20networks%20%28DVDN%29%20that%20generate%20a%20joint%20Q-function%20that%0Afactorizes%20into%20agent-wise%20Q-functions.%20Whereas%20the%20original%20value%0Adecomposition%20networks%20rely%20on%20centralized%20training%2C%20our%20approach%20is%20suitable%0Afor%20domains%20where%20centralized%20training%20is%20not%20possible%20and%20agents%20must%20learn%20by%0Ainteracting%20with%20the%20physical%20environment%20in%20a%20decentralized%20manner%20while%0Acommunicating%20with%20their%20peers.%20DVDN%20overcomes%20the%20need%20for%20centralized%0Atraining%20by%20locally%20estimating%20the%20shared%20objective.%20We%20contribute%20with%20two%0Ainnovative%20algorithms%2C%20DVDN%20and%20DVDN%20%28GT%29%2C%20for%20the%20heterogeneous%20and%0Ahomogeneous%20agents%20settings%20respectively.%20Empirically%2C%20both%20algorithms%0Aapproximate%20the%20performance%20of%20value%20decomposition%20networks%2C%20in%20spite%20of%20the%0Ainformation%20loss%20during%20communication%2C%20as%20demonstrated%20in%20ten%20MARL%20tasks%20in%0Athree%20standard%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07635v1&entry.124074799=Read"},
{"title": "GATEAU: Selecting Influential Samples for Long Context Alignment", "author": "Shuzheng Si and Haozhe Zhao and Gang Chen and Yunshui Li and Kangyang Luo and Chuancheng Lv and Kaikai An and Fanchao Qi and Baobao Chang and Maosong Sun", "abstract": "  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n", "link": "http://arxiv.org/abs/2410.15633v3", "date": "2025-02-11", "relevancy": 2.3743, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment&body=Title%3A%20GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment%0AAuthor%3A%20Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20attempt%20to%20scale%20up%0Athe%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%20samples%2C%0Aas%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%20However%2C%0Aa%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%20introduce%0Alow-quality%20samples%20and%20restrict%20the%20model%20performance.%20Thus%2C%20we%20propose%0AGATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%20context%0Aalignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%20long-range%0Adependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%20dependencies%0Afrom%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%20responses%20due%0Ato%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%20understanding%20long%20inputs%0Adue%20to%20such%20dependencies.%20Comprehensive%20experiments%20indicate%20that%20GATEAU%0Aeffectively%20identifies%20influential%20samples%20and%20the%20model%20trained%20on%20these%0Aselected%20samples%20exhibits%20better%20instruction-following%20and%20long-context%0Aunderstanding%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15633v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGATEAU%253A%2520Selecting%2520Influential%2520Samples%2520for%2520Long%2520Context%2520Alignment%26entry.906535625%3DShuzheng%2520Si%2520and%2520Haozhe%2520Zhao%2520and%2520Gang%2520Chen%2520and%2520Yunshui%2520Li%2520and%2520Kangyang%2520Luo%2520and%2520Chuancheng%2520Lv%2520and%2520Kaikai%2520An%2520and%2520Fanchao%2520Qi%2520and%2520Baobao%2520Chang%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Aligning%2520large%2520language%2520models%2520to%2520handle%2520instructions%2520with%2520extremely%2520long%250Acontexts%2520has%2520yet%2520to%2520be%2520fully%2520investigated.%2520Previous%2520studies%2520attempt%2520to%2520scale%2520up%250Athe%2520available%2520data%2520volume%2520by%2520synthesizing%2520long%2520instruction-following%2520samples%252C%250Aas%2520constructing%2520such%2520a%2520dataset%2520tends%2520to%2520be%2520challenging%2520for%2520annotators.%2520However%252C%250Aa%2520lack%2520of%2520a%2520well-defined%2520strategy%2520for%2520ensuring%2520data%2520quality%2520may%2520introduce%250Alow-quality%2520samples%2520and%2520restrict%2520the%2520model%2520performance.%2520Thus%252C%2520we%2520propose%250AGATEAU%252C%2520a%2520novel%2520framework%2520to%2520address%2520the%2520unique%2520challenge%2520of%2520long%2520context%250Aalignment%2520by%2520identifying%2520the%2520influential%2520samples%2520enriched%2520with%2520long-range%250Adependency%2520relations.%2520Specifically%252C%2520GATEAU%2520measures%2520the%2520long-range%2520dependencies%250Afrom%2520two%2520essential%2520aspects%253A%2520the%2520difficulty%2520of%2520generating%2520target%2520responses%2520due%250Ato%2520the%2520long-range%2520dependencies%252C%2520and%2520the%2520difficulty%2520of%2520understanding%2520long%2520inputs%250Adue%2520to%2520such%2520dependencies.%2520Comprehensive%2520experiments%2520indicate%2520that%2520GATEAU%250Aeffectively%2520identifies%2520influential%2520samples%2520and%2520the%2520model%2520trained%2520on%2520these%250Aselected%2520samples%2520exhibits%2520better%2520instruction-following%2520and%2520long-context%250Aunderstanding%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15633v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment&entry.906535625=Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun&entry.1292438233=%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20attempt%20to%20scale%20up%0Athe%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%20samples%2C%0Aas%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%20However%2C%0Aa%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%20introduce%0Alow-quality%20samples%20and%20restrict%20the%20model%20performance.%20Thus%2C%20we%20propose%0AGATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%20context%0Aalignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%20long-range%0Adependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%20dependencies%0Afrom%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%20responses%20due%0Ato%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%20understanding%20long%20inputs%0Adue%20to%20such%20dependencies.%20Comprehensive%20experiments%20indicate%20that%20GATEAU%0Aeffectively%20identifies%20influential%20samples%20and%20the%20model%20trained%20on%20these%0Aselected%20samples%20exhibits%20better%20instruction-following%20and%20long-context%0Aunderstanding%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15633v3&entry.124074799=Read"},
{"title": "LLM-Sketch: Enhancing Network Sketches with LLM", "author": "Yuanpeng Li and Zhen Xu and Zongwei Lv and Yannan Hu and Yong Cui and Tong Yang", "abstract": "  Network stream mining is fundamental to many network operations. Sketches, as\ncompact data structures that offer low memory overhead with bounded accuracy,\nhave emerged as a promising solution for network stream mining. Recent studies\nattempt to optimize sketches using machine learning; however, these approaches\nface the challenges of lacking adaptivity to dynamic networks and incurring\nhigh training costs. In this paper, we propose LLM-Sketch, based on the insight\nthat fields beyond the flow IDs in packet headers can also help infer flow\nsizes. By using a two-tier data structure and separately recording large and\nsmall flows, LLM-Sketch improves accuracy while minimizing memory usage.\nFurthermore, it leverages fine-tuned large language models (LLMs) to reliably\nestimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and\nthe results demonstrate that LLM-Sketch outperforms state-of-the-art methods by\nachieving a $7.5\\times$ accuracy improvement.\n", "link": "http://arxiv.org/abs/2502.07495v1", "date": "2025-02-11", "relevancy": 2.3713, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5037}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4634}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Sketch%3A%20Enhancing%20Network%20Sketches%20with%20LLM&body=Title%3A%20LLM-Sketch%3A%20Enhancing%20Network%20Sketches%20with%20LLM%0AAuthor%3A%20Yuanpeng%20Li%20and%20Zhen%20Xu%20and%20Zongwei%20Lv%20and%20Yannan%20Hu%20and%20Yong%20Cui%20and%20Tong%20Yang%0AAbstract%3A%20%20%20Network%20stream%20mining%20is%20fundamental%20to%20many%20network%20operations.%20Sketches%2C%20as%0Acompact%20data%20structures%20that%20offer%20low%20memory%20overhead%20with%20bounded%20accuracy%2C%0Ahave%20emerged%20as%20a%20promising%20solution%20for%20network%20stream%20mining.%20Recent%20studies%0Aattempt%20to%20optimize%20sketches%20using%20machine%20learning%3B%20however%2C%20these%20approaches%0Aface%20the%20challenges%20of%20lacking%20adaptivity%20to%20dynamic%20networks%20and%20incurring%0Ahigh%20training%20costs.%20In%20this%20paper%2C%20we%20propose%20LLM-Sketch%2C%20based%20on%20the%20insight%0Athat%20fields%20beyond%20the%20flow%20IDs%20in%20packet%20headers%20can%20also%20help%20infer%20flow%0Asizes.%20By%20using%20a%20two-tier%20data%20structure%20and%20separately%20recording%20large%20and%0Asmall%20flows%2C%20LLM-Sketch%20improves%20accuracy%20while%20minimizing%20memory%20usage.%0AFurthermore%2C%20it%20leverages%20fine-tuned%20large%20language%20models%20%28LLMs%29%20to%20reliably%0Aestimate%20flow%20sizes.%20We%20evaluate%20LLM-Sketch%20on%20three%20representative%20tasks%2C%20and%0Athe%20results%20demonstrate%20that%20LLM-Sketch%20outperforms%20state-of-the-art%20methods%20by%0Aachieving%20a%20%247.5%5Ctimes%24%20accuracy%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Sketch%253A%2520Enhancing%2520Network%2520Sketches%2520with%2520LLM%26entry.906535625%3DYuanpeng%2520Li%2520and%2520Zhen%2520Xu%2520and%2520Zongwei%2520Lv%2520and%2520Yannan%2520Hu%2520and%2520Yong%2520Cui%2520and%2520Tong%2520Yang%26entry.1292438233%3D%2520%2520Network%2520stream%2520mining%2520is%2520fundamental%2520to%2520many%2520network%2520operations.%2520Sketches%252C%2520as%250Acompact%2520data%2520structures%2520that%2520offer%2520low%2520memory%2520overhead%2520with%2520bounded%2520accuracy%252C%250Ahave%2520emerged%2520as%2520a%2520promising%2520solution%2520for%2520network%2520stream%2520mining.%2520Recent%2520studies%250Aattempt%2520to%2520optimize%2520sketches%2520using%2520machine%2520learning%253B%2520however%252C%2520these%2520approaches%250Aface%2520the%2520challenges%2520of%2520lacking%2520adaptivity%2520to%2520dynamic%2520networks%2520and%2520incurring%250Ahigh%2520training%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LLM-Sketch%252C%2520based%2520on%2520the%2520insight%250Athat%2520fields%2520beyond%2520the%2520flow%2520IDs%2520in%2520packet%2520headers%2520can%2520also%2520help%2520infer%2520flow%250Asizes.%2520By%2520using%2520a%2520two-tier%2520data%2520structure%2520and%2520separately%2520recording%2520large%2520and%250Asmall%2520flows%252C%2520LLM-Sketch%2520improves%2520accuracy%2520while%2520minimizing%2520memory%2520usage.%250AFurthermore%252C%2520it%2520leverages%2520fine-tuned%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520reliably%250Aestimate%2520flow%2520sizes.%2520We%2520evaluate%2520LLM-Sketch%2520on%2520three%2520representative%2520tasks%252C%2520and%250Athe%2520results%2520demonstrate%2520that%2520LLM-Sketch%2520outperforms%2520state-of-the-art%2520methods%2520by%250Aachieving%2520a%2520%25247.5%255Ctimes%2524%2520accuracy%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Sketch%3A%20Enhancing%20Network%20Sketches%20with%20LLM&entry.906535625=Yuanpeng%20Li%20and%20Zhen%20Xu%20and%20Zongwei%20Lv%20and%20Yannan%20Hu%20and%20Yong%20Cui%20and%20Tong%20Yang&entry.1292438233=%20%20Network%20stream%20mining%20is%20fundamental%20to%20many%20network%20operations.%20Sketches%2C%20as%0Acompact%20data%20structures%20that%20offer%20low%20memory%20overhead%20with%20bounded%20accuracy%2C%0Ahave%20emerged%20as%20a%20promising%20solution%20for%20network%20stream%20mining.%20Recent%20studies%0Aattempt%20to%20optimize%20sketches%20using%20machine%20learning%3B%20however%2C%20these%20approaches%0Aface%20the%20challenges%20of%20lacking%20adaptivity%20to%20dynamic%20networks%20and%20incurring%0Ahigh%20training%20costs.%20In%20this%20paper%2C%20we%20propose%20LLM-Sketch%2C%20based%20on%20the%20insight%0Athat%20fields%20beyond%20the%20flow%20IDs%20in%20packet%20headers%20can%20also%20help%20infer%20flow%0Asizes.%20By%20using%20a%20two-tier%20data%20structure%20and%20separately%20recording%20large%20and%0Asmall%20flows%2C%20LLM-Sketch%20improves%20accuracy%20while%20minimizing%20memory%20usage.%0AFurthermore%2C%20it%20leverages%20fine-tuned%20large%20language%20models%20%28LLMs%29%20to%20reliably%0Aestimate%20flow%20sizes.%20We%20evaluate%20LLM-Sketch%20on%20three%20representative%20tasks%2C%20and%0Athe%20results%20demonstrate%20that%20LLM-Sketch%20outperforms%20state-of-the-art%20methods%20by%0Aachieving%20a%20%247.5%5Ctimes%24%20accuracy%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07495v1&entry.124074799=Read"},
{"title": "Accessing Vision Foundation Models via ImageNet-1K", "author": "Yitian Zhang and Xu Ma and Yue Bai and Huan Wang and Yun Fu", "abstract": "  Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.\n", "link": "http://arxiv.org/abs/2407.10366v2", "date": "2025-02-11", "relevancy": 2.3593, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accessing%20Vision%20Foundation%20Models%20via%20ImageNet-1K&body=Title%3A%20Accessing%20Vision%20Foundation%20Models%20via%20ImageNet-1K%0AAuthor%3A%20Yitian%20Zhang%20and%20Xu%20Ma%20and%20Yue%20Bai%20and%20Huan%20Wang%20and%20Yun%20Fu%0AAbstract%3A%20%20%20Vision%20foundation%20models%20are%20renowned%20for%20the%20generalization%20ability%20due%20to%0Amassive%20training%20data.%20Nevertheless%2C%20they%20demand%20tremendous%20training%20resources%2C%0Aand%20the%20training%20data%20is%20often%20inaccessible%2C%20e.g.%2C%20CLIP%2C%20DINOv2%2C%20posing%20great%0Achallenges%20to%20developing%20derivatives%20that%20could%20facilitate%20the%20research.%20In%0Athis%20work%2C%20we%20offer%20a%20very%20simple%20and%20general%20solution%2C%20named%20%5Ctextit%7BProteus%7D%2C%0Ato%20distill%20foundation%20models%20into%20smaller%20equivalents%20on%20ImageNet-1K%20without%0Aaccess%20to%20the%20original%20training%20data.%20Specifically%2C%20we%20remove%20the%20designs%20from%0Aconventional%20knowledge%20distillation%20settings%20that%20result%20in%20dataset%20bias%20and%0Apresent%20three%20levels%20of%20training%20objectives%2C%20i.e.%2C%20token%2C%20patch%2C%20and%20feature%2C%0Ato%20maximize%20the%20efficacy%20of%20knowledge%20transfer.%20In%20this%20manner%2C%20Proteus%20is%0Atrained%20at%20ImageNet-level%20costs%20with%20surprising%20ability%2C%20facilitating%20the%0Aaccessibility%20of%20training%20foundation%20models%20for%20the%20broader%20research%20community.%0AWhen%20leveraging%20DINOv2-g/14%20as%20the%20teacher%2C%20Proteus-L/14%20matches%20the%0Aperformance%20of%20the%20Oracle%20method%20DINOv2-L/14%20%28142M%20training%20data%29%20across%2019%0Abenchmarks%20and%20outperforms%20other%20vision%20foundation%20models%20including%20CLIP-L/14%0A%28400M%29%2C%20OpenCLIP-L/14%20%28400M/2B%29%20and%20SynCLR-L/14%20%28600M%29%20with%20a%20significantly%0Asmaller%20training%20set%20of%201.2M%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10366v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccessing%2520Vision%2520Foundation%2520Models%2520via%2520ImageNet-1K%26entry.906535625%3DYitian%2520Zhang%2520and%2520Xu%2520Ma%2520and%2520Yue%2520Bai%2520and%2520Huan%2520Wang%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520Vision%2520foundation%2520models%2520are%2520renowned%2520for%2520the%2520generalization%2520ability%2520due%2520to%250Amassive%2520training%2520data.%2520Nevertheless%252C%2520they%2520demand%2520tremendous%2520training%2520resources%252C%250Aand%2520the%2520training%2520data%2520is%2520often%2520inaccessible%252C%2520e.g.%252C%2520CLIP%252C%2520DINOv2%252C%2520posing%2520great%250Achallenges%2520to%2520developing%2520derivatives%2520that%2520could%2520facilitate%2520the%2520research.%2520In%250Athis%2520work%252C%2520we%2520offer%2520a%2520very%2520simple%2520and%2520general%2520solution%252C%2520named%2520%255Ctextit%257BProteus%257D%252C%250Ato%2520distill%2520foundation%2520models%2520into%2520smaller%2520equivalents%2520on%2520ImageNet-1K%2520without%250Aaccess%2520to%2520the%2520original%2520training%2520data.%2520Specifically%252C%2520we%2520remove%2520the%2520designs%2520from%250Aconventional%2520knowledge%2520distillation%2520settings%2520that%2520result%2520in%2520dataset%2520bias%2520and%250Apresent%2520three%2520levels%2520of%2520training%2520objectives%252C%2520i.e.%252C%2520token%252C%2520patch%252C%2520and%2520feature%252C%250Ato%2520maximize%2520the%2520efficacy%2520of%2520knowledge%2520transfer.%2520In%2520this%2520manner%252C%2520Proteus%2520is%250Atrained%2520at%2520ImageNet-level%2520costs%2520with%2520surprising%2520ability%252C%2520facilitating%2520the%250Aaccessibility%2520of%2520training%2520foundation%2520models%2520for%2520the%2520broader%2520research%2520community.%250AWhen%2520leveraging%2520DINOv2-g/14%2520as%2520the%2520teacher%252C%2520Proteus-L/14%2520matches%2520the%250Aperformance%2520of%2520the%2520Oracle%2520method%2520DINOv2-L/14%2520%2528142M%2520training%2520data%2529%2520across%252019%250Abenchmarks%2520and%2520outperforms%2520other%2520vision%2520foundation%2520models%2520including%2520CLIP-L/14%250A%2528400M%2529%252C%2520OpenCLIP-L/14%2520%2528400M/2B%2529%2520and%2520SynCLR-L/14%2520%2528600M%2529%2520with%2520a%2520significantly%250Asmaller%2520training%2520set%2520of%25201.2M%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10366v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accessing%20Vision%20Foundation%20Models%20via%20ImageNet-1K&entry.906535625=Yitian%20Zhang%20and%20Xu%20Ma%20and%20Yue%20Bai%20and%20Huan%20Wang%20and%20Yun%20Fu&entry.1292438233=%20%20Vision%20foundation%20models%20are%20renowned%20for%20the%20generalization%20ability%20due%20to%0Amassive%20training%20data.%20Nevertheless%2C%20they%20demand%20tremendous%20training%20resources%2C%0Aand%20the%20training%20data%20is%20often%20inaccessible%2C%20e.g.%2C%20CLIP%2C%20DINOv2%2C%20posing%20great%0Achallenges%20to%20developing%20derivatives%20that%20could%20facilitate%20the%20research.%20In%0Athis%20work%2C%20we%20offer%20a%20very%20simple%20and%20general%20solution%2C%20named%20%5Ctextit%7BProteus%7D%2C%0Ato%20distill%20foundation%20models%20into%20smaller%20equivalents%20on%20ImageNet-1K%20without%0Aaccess%20to%20the%20original%20training%20data.%20Specifically%2C%20we%20remove%20the%20designs%20from%0Aconventional%20knowledge%20distillation%20settings%20that%20result%20in%20dataset%20bias%20and%0Apresent%20three%20levels%20of%20training%20objectives%2C%20i.e.%2C%20token%2C%20patch%2C%20and%20feature%2C%0Ato%20maximize%20the%20efficacy%20of%20knowledge%20transfer.%20In%20this%20manner%2C%20Proteus%20is%0Atrained%20at%20ImageNet-level%20costs%20with%20surprising%20ability%2C%20facilitating%20the%0Aaccessibility%20of%20training%20foundation%20models%20for%20the%20broader%20research%20community.%0AWhen%20leveraging%20DINOv2-g/14%20as%20the%20teacher%2C%20Proteus-L/14%20matches%20the%0Aperformance%20of%20the%20Oracle%20method%20DINOv2-L/14%20%28142M%20training%20data%29%20across%2019%0Abenchmarks%20and%20outperforms%20other%20vision%20foundation%20models%20including%20CLIP-L/14%0A%28400M%29%2C%20OpenCLIP-L/14%20%28400M/2B%29%20and%20SynCLR-L/14%20%28600M%29%20with%20a%20significantly%0Asmaller%20training%20set%20of%201.2M%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10366v2&entry.124074799=Read"},
{"title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid", "author": "Weigao Sun and Disen Lan and Yiran Zhong and Xiaoye Qu and Yu Cheng", "abstract": "  Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n", "link": "http://arxiv.org/abs/2502.07563v1", "date": "2025-02-11", "relevancy": 2.3528, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4873}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4638}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASP-2%3A%20Rethinking%20Sequence%20Parallelism%20for%20Linear%20Attention%20and%20Its%0A%20%20Hybrid&body=Title%3A%20LASP-2%3A%20Rethinking%20Sequence%20Parallelism%20for%20Linear%20Attention%20and%20Its%0A%20%20Hybrid%0AAuthor%3A%20Weigao%20Sun%20and%20Disen%20Lan%20and%20Yiran%20Zhong%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Linear%20sequence%20modeling%20approaches%2C%20such%20as%20linear%20attention%2C%20provide%0Aadvantages%20like%20linear-time%20training%20and%20constant-memory%20inference%20over%0Asequence%20lengths.%20However%2C%20existing%20sequence%20parallelism%20%28SP%29%20methods%20are%0Aeither%20not%20optimized%20for%20the%20right-product-first%20feature%20of%20linear%20attention%20or%0Ause%20a%20ring-style%20communication%20strategy%2C%20which%20results%20in%20lower%20computation%0Aparallelism%2C%20limits%20their%20scalability%20for%20longer%20sequences%20in%20distributed%0Asystems.%20In%20this%20paper%2C%20we%20introduce%20LASP-2%2C%20a%20new%20SP%20method%20to%20enhance%20both%0Acommunication%20and%20computation%20parallelism%20when%20training%20linear%20attention%0Atransformer%20models%20with%20very-long%20input%20sequences.%20Compared%20to%20previous%20work%0ALASP%2C%20LASP-2%20rethinks%20the%20minimal%20communication%20requirement%20for%20SP%20on%20linear%0Aattention%20layers%2C%20reorganizes%20the%20whole%20communication-computation%20workflow%20of%0ALASP.%20In%20this%20way%2C%20only%20one%20single%20AllGather%20collective%20communication%20is%20needed%0Aon%20intermediate%20memory%20states%2C%20whose%20sizes%20are%20independent%20of%20the%20sequence%0Alength%2C%20leading%20to%20significant%20improvements%20of%20both%20communication%20and%0Acomputation%20parallelism%2C%20as%20well%20as%20their%20overlap.%20Additionally%2C%20we%20extend%0ALASP-2%20to%20LASP-2H%20by%20applying%20similar%20communication%20redesign%20to%20standard%0Aattention%20modules%2C%20offering%20an%20efficient%20SP%20solution%20for%20hybrid%20models%20that%0Ablend%20linear%20and%20standard%20attention%20layers.%20Our%20evaluation%20on%20a%20Linear-Llama3%0Amodel%2C%20a%20variant%20of%20Llama3%20with%20linear%20attention%20replacing%20standard%20attention%2C%0Ademonstrates%20the%20effectiveness%20of%20LASP-2%20and%20LASP-2H.%20Specifically%2C%20LASP-2%0Aachieves%20training%20speed%20improvements%20of%2015.2%25%20over%20LASP%20and%2036.6%25%20over%20Ring%0AAttention%2C%20with%20a%20sequence%20length%20of%202048K%20across%2064%20GPUs.%20The%20Code%20is%20released%0Aas%20a%20part%20of%3A%20https%3A//github.com/OpenSparseLLMs/Linear-MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASP-2%253A%2520Rethinking%2520Sequence%2520Parallelism%2520for%2520Linear%2520Attention%2520and%2520Its%250A%2520%2520Hybrid%26entry.906535625%3DWeigao%2520Sun%2520and%2520Disen%2520Lan%2520and%2520Yiran%2520Zhong%2520and%2520Xiaoye%2520Qu%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Linear%2520sequence%2520modeling%2520approaches%252C%2520such%2520as%2520linear%2520attention%252C%2520provide%250Aadvantages%2520like%2520linear-time%2520training%2520and%2520constant-memory%2520inference%2520over%250Asequence%2520lengths.%2520However%252C%2520existing%2520sequence%2520parallelism%2520%2528SP%2529%2520methods%2520are%250Aeither%2520not%2520optimized%2520for%2520the%2520right-product-first%2520feature%2520of%2520linear%2520attention%2520or%250Ause%2520a%2520ring-style%2520communication%2520strategy%252C%2520which%2520results%2520in%2520lower%2520computation%250Aparallelism%252C%2520limits%2520their%2520scalability%2520for%2520longer%2520sequences%2520in%2520distributed%250Asystems.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LASP-2%252C%2520a%2520new%2520SP%2520method%2520to%2520enhance%2520both%250Acommunication%2520and%2520computation%2520parallelism%2520when%2520training%2520linear%2520attention%250Atransformer%2520models%2520with%2520very-long%2520input%2520sequences.%2520Compared%2520to%2520previous%2520work%250ALASP%252C%2520LASP-2%2520rethinks%2520the%2520minimal%2520communication%2520requirement%2520for%2520SP%2520on%2520linear%250Aattention%2520layers%252C%2520reorganizes%2520the%2520whole%2520communication-computation%2520workflow%2520of%250ALASP.%2520In%2520this%2520way%252C%2520only%2520one%2520single%2520AllGather%2520collective%2520communication%2520is%2520needed%250Aon%2520intermediate%2520memory%2520states%252C%2520whose%2520sizes%2520are%2520independent%2520of%2520the%2520sequence%250Alength%252C%2520leading%2520to%2520significant%2520improvements%2520of%2520both%2520communication%2520and%250Acomputation%2520parallelism%252C%2520as%2520well%2520as%2520their%2520overlap.%2520Additionally%252C%2520we%2520extend%250ALASP-2%2520to%2520LASP-2H%2520by%2520applying%2520similar%2520communication%2520redesign%2520to%2520standard%250Aattention%2520modules%252C%2520offering%2520an%2520efficient%2520SP%2520solution%2520for%2520hybrid%2520models%2520that%250Ablend%2520linear%2520and%2520standard%2520attention%2520layers.%2520Our%2520evaluation%2520on%2520a%2520Linear-Llama3%250Amodel%252C%2520a%2520variant%2520of%2520Llama3%2520with%2520linear%2520attention%2520replacing%2520standard%2520attention%252C%250Ademonstrates%2520the%2520effectiveness%2520of%2520LASP-2%2520and%2520LASP-2H.%2520Specifically%252C%2520LASP-2%250Aachieves%2520training%2520speed%2520improvements%2520of%252015.2%2525%2520over%2520LASP%2520and%252036.6%2525%2520over%2520Ring%250AAttention%252C%2520with%2520a%2520sequence%2520length%2520of%25202048K%2520across%252064%2520GPUs.%2520The%2520Code%2520is%2520released%250Aas%2520a%2520part%2520of%253A%2520https%253A//github.com/OpenSparseLLMs/Linear-MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASP-2%3A%20Rethinking%20Sequence%20Parallelism%20for%20Linear%20Attention%20and%20Its%0A%20%20Hybrid&entry.906535625=Weigao%20Sun%20and%20Disen%20Lan%20and%20Yiran%20Zhong%20and%20Xiaoye%20Qu%20and%20Yu%20Cheng&entry.1292438233=%20%20Linear%20sequence%20modeling%20approaches%2C%20such%20as%20linear%20attention%2C%20provide%0Aadvantages%20like%20linear-time%20training%20and%20constant-memory%20inference%20over%0Asequence%20lengths.%20However%2C%20existing%20sequence%20parallelism%20%28SP%29%20methods%20are%0Aeither%20not%20optimized%20for%20the%20right-product-first%20feature%20of%20linear%20attention%20or%0Ause%20a%20ring-style%20communication%20strategy%2C%20which%20results%20in%20lower%20computation%0Aparallelism%2C%20limits%20their%20scalability%20for%20longer%20sequences%20in%20distributed%0Asystems.%20In%20this%20paper%2C%20we%20introduce%20LASP-2%2C%20a%20new%20SP%20method%20to%20enhance%20both%0Acommunication%20and%20computation%20parallelism%20when%20training%20linear%20attention%0Atransformer%20models%20with%20very-long%20input%20sequences.%20Compared%20to%20previous%20work%0ALASP%2C%20LASP-2%20rethinks%20the%20minimal%20communication%20requirement%20for%20SP%20on%20linear%0Aattention%20layers%2C%20reorganizes%20the%20whole%20communication-computation%20workflow%20of%0ALASP.%20In%20this%20way%2C%20only%20one%20single%20AllGather%20collective%20communication%20is%20needed%0Aon%20intermediate%20memory%20states%2C%20whose%20sizes%20are%20independent%20of%20the%20sequence%0Alength%2C%20leading%20to%20significant%20improvements%20of%20both%20communication%20and%0Acomputation%20parallelism%2C%20as%20well%20as%20their%20overlap.%20Additionally%2C%20we%20extend%0ALASP-2%20to%20LASP-2H%20by%20applying%20similar%20communication%20redesign%20to%20standard%0Aattention%20modules%2C%20offering%20an%20efficient%20SP%20solution%20for%20hybrid%20models%20that%0Ablend%20linear%20and%20standard%20attention%20layers.%20Our%20evaluation%20on%20a%20Linear-Llama3%0Amodel%2C%20a%20variant%20of%20Llama3%20with%20linear%20attention%20replacing%20standard%20attention%2C%0Ademonstrates%20the%20effectiveness%20of%20LASP-2%20and%20LASP-2H.%20Specifically%2C%20LASP-2%0Aachieves%20training%20speed%20improvements%20of%2015.2%25%20over%20LASP%20and%2036.6%25%20over%20Ring%0AAttention%2C%20with%20a%20sequence%20length%20of%202048K%20across%2064%20GPUs.%20The%20Code%20is%20released%0Aas%20a%20part%20of%3A%20https%3A//github.com/OpenSparseLLMs/Linear-MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07563v1&entry.124074799=Read"},
{"title": "Private Low-Rank Approximation for Covariance Matrices, Dyson Brownian\n  Motion, and Eigenvalue-Gap Bounds for Gaussian Perturbations", "author": "Oren Mangoubi and Nisheeth K. Vishnoi", "abstract": "  We consider the problem of approximating a $d \\times d$ covariance matrix $M$\nwith a rank-$k$ matrix under $(\\varepsilon,\\delta)$-differential privacy. We\npresent and analyze a complex variant of the Gaussian mechanism and obtain\nupper bounds on the Frobenius norm of the difference between the matrix output\nby this mechanism and the best rank-$k$ approximation to $M$. Our analysis\nprovides improvements over previous bounds, particularly when the spectrum of\n$M$ satisfies natural structural assumptions. The novel insight is to view the\naddition of Gaussian noise to a matrix as a continuous-time matrix Brownian\nmotion. This viewpoint allows us to track the evolution of eigenvalues and\neigenvectors of the matrix, which are governed by stochastic differential\nequations discovered by Dyson. These equations enable us to upper bound the\nFrobenius distance between the best rank-$k$ approximation of $M$ and that of a\nGaussian perturbation of $M$ as an integral that involves inverse eigenvalue\ngaps of the stochastically evolving matrix, as opposed to a sum of perturbation\nbounds obtained via Davis-Kahan-type theorems. Subsequently, again using the\nDyson Brownian motion viewpoint, we show that the eigenvalues of the matrix $M$\nperturbed by Gaussian noise have large gaps with high probability. These\nresults also contribute to the analysis of low-rank approximations under\naverage-case perturbations, and to an understanding of eigenvalue gaps for\nrandom matrices, both of which may be of independent interest.\n", "link": "http://arxiv.org/abs/2502.07657v1", "date": "2025-02-11", "relevancy": 2.3505, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5019}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4841}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Private%20Low-Rank%20Approximation%20for%20Covariance%20Matrices%2C%20Dyson%20Brownian%0A%20%20Motion%2C%20and%20Eigenvalue-Gap%20Bounds%20for%20Gaussian%20Perturbations&body=Title%3A%20Private%20Low-Rank%20Approximation%20for%20Covariance%20Matrices%2C%20Dyson%20Brownian%0A%20%20Motion%2C%20and%20Eigenvalue-Gap%20Bounds%20for%20Gaussian%20Perturbations%0AAuthor%3A%20Oren%20Mangoubi%20and%20Nisheeth%20K.%20Vishnoi%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20approximating%20a%20%24d%20%5Ctimes%20d%24%20covariance%20matrix%20%24M%24%0Awith%20a%20rank-%24k%24%20matrix%20under%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-differential%20privacy.%20We%0Apresent%20and%20analyze%20a%20complex%20variant%20of%20the%20Gaussian%20mechanism%20and%20obtain%0Aupper%20bounds%20on%20the%20Frobenius%20norm%20of%20the%20difference%20between%20the%20matrix%20output%0Aby%20this%20mechanism%20and%20the%20best%20rank-%24k%24%20approximation%20to%20%24M%24.%20Our%20analysis%0Aprovides%20improvements%20over%20previous%20bounds%2C%20particularly%20when%20the%20spectrum%20of%0A%24M%24%20satisfies%20natural%20structural%20assumptions.%20The%20novel%20insight%20is%20to%20view%20the%0Aaddition%20of%20Gaussian%20noise%20to%20a%20matrix%20as%20a%20continuous-time%20matrix%20Brownian%0Amotion.%20This%20viewpoint%20allows%20us%20to%20track%20the%20evolution%20of%20eigenvalues%20and%0Aeigenvectors%20of%20the%20matrix%2C%20which%20are%20governed%20by%20stochastic%20differential%0Aequations%20discovered%20by%20Dyson.%20These%20equations%20enable%20us%20to%20upper%20bound%20the%0AFrobenius%20distance%20between%20the%20best%20rank-%24k%24%20approximation%20of%20%24M%24%20and%20that%20of%20a%0AGaussian%20perturbation%20of%20%24M%24%20as%20an%20integral%20that%20involves%20inverse%20eigenvalue%0Agaps%20of%20the%20stochastically%20evolving%20matrix%2C%20as%20opposed%20to%20a%20sum%20of%20perturbation%0Abounds%20obtained%20via%20Davis-Kahan-type%20theorems.%20Subsequently%2C%20again%20using%20the%0ADyson%20Brownian%20motion%20viewpoint%2C%20we%20show%20that%20the%20eigenvalues%20of%20the%20matrix%20%24M%24%0Aperturbed%20by%20Gaussian%20noise%20have%20large%20gaps%20with%20high%20probability.%20These%0Aresults%20also%20contribute%20to%20the%20analysis%20of%20low-rank%20approximations%20under%0Aaverage-case%20perturbations%2C%20and%20to%20an%20understanding%20of%20eigenvalue%20gaps%20for%0Arandom%20matrices%2C%20both%20of%20which%20may%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivate%2520Low-Rank%2520Approximation%2520for%2520Covariance%2520Matrices%252C%2520Dyson%2520Brownian%250A%2520%2520Motion%252C%2520and%2520Eigenvalue-Gap%2520Bounds%2520for%2520Gaussian%2520Perturbations%26entry.906535625%3DOren%2520Mangoubi%2520and%2520Nisheeth%2520K.%2520Vishnoi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520approximating%2520a%2520%2524d%2520%255Ctimes%2520d%2524%2520covariance%2520matrix%2520%2524M%2524%250Awith%2520a%2520rank-%2524k%2524%2520matrix%2520under%2520%2524%2528%255Cvarepsilon%252C%255Cdelta%2529%2524-differential%2520privacy.%2520We%250Apresent%2520and%2520analyze%2520a%2520complex%2520variant%2520of%2520the%2520Gaussian%2520mechanism%2520and%2520obtain%250Aupper%2520bounds%2520on%2520the%2520Frobenius%2520norm%2520of%2520the%2520difference%2520between%2520the%2520matrix%2520output%250Aby%2520this%2520mechanism%2520and%2520the%2520best%2520rank-%2524k%2524%2520approximation%2520to%2520%2524M%2524.%2520Our%2520analysis%250Aprovides%2520improvements%2520over%2520previous%2520bounds%252C%2520particularly%2520when%2520the%2520spectrum%2520of%250A%2524M%2524%2520satisfies%2520natural%2520structural%2520assumptions.%2520The%2520novel%2520insight%2520is%2520to%2520view%2520the%250Aaddition%2520of%2520Gaussian%2520noise%2520to%2520a%2520matrix%2520as%2520a%2520continuous-time%2520matrix%2520Brownian%250Amotion.%2520This%2520viewpoint%2520allows%2520us%2520to%2520track%2520the%2520evolution%2520of%2520eigenvalues%2520and%250Aeigenvectors%2520of%2520the%2520matrix%252C%2520which%2520are%2520governed%2520by%2520stochastic%2520differential%250Aequations%2520discovered%2520by%2520Dyson.%2520These%2520equations%2520enable%2520us%2520to%2520upper%2520bound%2520the%250AFrobenius%2520distance%2520between%2520the%2520best%2520rank-%2524k%2524%2520approximation%2520of%2520%2524M%2524%2520and%2520that%2520of%2520a%250AGaussian%2520perturbation%2520of%2520%2524M%2524%2520as%2520an%2520integral%2520that%2520involves%2520inverse%2520eigenvalue%250Agaps%2520of%2520the%2520stochastically%2520evolving%2520matrix%252C%2520as%2520opposed%2520to%2520a%2520sum%2520of%2520perturbation%250Abounds%2520obtained%2520via%2520Davis-Kahan-type%2520theorems.%2520Subsequently%252C%2520again%2520using%2520the%250ADyson%2520Brownian%2520motion%2520viewpoint%252C%2520we%2520show%2520that%2520the%2520eigenvalues%2520of%2520the%2520matrix%2520%2524M%2524%250Aperturbed%2520by%2520Gaussian%2520noise%2520have%2520large%2520gaps%2520with%2520high%2520probability.%2520These%250Aresults%2520also%2520contribute%2520to%2520the%2520analysis%2520of%2520low-rank%2520approximations%2520under%250Aaverage-case%2520perturbations%252C%2520and%2520to%2520an%2520understanding%2520of%2520eigenvalue%2520gaps%2520for%250Arandom%2520matrices%252C%2520both%2520of%2520which%2520may%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Private%20Low-Rank%20Approximation%20for%20Covariance%20Matrices%2C%20Dyson%20Brownian%0A%20%20Motion%2C%20and%20Eigenvalue-Gap%20Bounds%20for%20Gaussian%20Perturbations&entry.906535625=Oren%20Mangoubi%20and%20Nisheeth%20K.%20Vishnoi&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20approximating%20a%20%24d%20%5Ctimes%20d%24%20covariance%20matrix%20%24M%24%0Awith%20a%20rank-%24k%24%20matrix%20under%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-differential%20privacy.%20We%0Apresent%20and%20analyze%20a%20complex%20variant%20of%20the%20Gaussian%20mechanism%20and%20obtain%0Aupper%20bounds%20on%20the%20Frobenius%20norm%20of%20the%20difference%20between%20the%20matrix%20output%0Aby%20this%20mechanism%20and%20the%20best%20rank-%24k%24%20approximation%20to%20%24M%24.%20Our%20analysis%0Aprovides%20improvements%20over%20previous%20bounds%2C%20particularly%20when%20the%20spectrum%20of%0A%24M%24%20satisfies%20natural%20structural%20assumptions.%20The%20novel%20insight%20is%20to%20view%20the%0Aaddition%20of%20Gaussian%20noise%20to%20a%20matrix%20as%20a%20continuous-time%20matrix%20Brownian%0Amotion.%20This%20viewpoint%20allows%20us%20to%20track%20the%20evolution%20of%20eigenvalues%20and%0Aeigenvectors%20of%20the%20matrix%2C%20which%20are%20governed%20by%20stochastic%20differential%0Aequations%20discovered%20by%20Dyson.%20These%20equations%20enable%20us%20to%20upper%20bound%20the%0AFrobenius%20distance%20between%20the%20best%20rank-%24k%24%20approximation%20of%20%24M%24%20and%20that%20of%20a%0AGaussian%20perturbation%20of%20%24M%24%20as%20an%20integral%20that%20involves%20inverse%20eigenvalue%0Agaps%20of%20the%20stochastically%20evolving%20matrix%2C%20as%20opposed%20to%20a%20sum%20of%20perturbation%0Abounds%20obtained%20via%20Davis-Kahan-type%20theorems.%20Subsequently%2C%20again%20using%20the%0ADyson%20Brownian%20motion%20viewpoint%2C%20we%20show%20that%20the%20eigenvalues%20of%20the%20matrix%20%24M%24%0Aperturbed%20by%20Gaussian%20noise%20have%20large%20gaps%20with%20high%20probability.%20These%0Aresults%20also%20contribute%20to%20the%20analysis%20of%20low-rank%20approximations%20under%0Aaverage-case%20perturbations%2C%20and%20to%20an%20understanding%20of%20eigenvalue%20gaps%20for%0Arandom%20matrices%2C%20both%20of%20which%20may%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07657v1&entry.124074799=Read"},
{"title": "Higher-Order Message Passing for Glycan Representation Learning", "author": "Roman Joeres and Daniel Bojar", "abstract": "  Glycans are the most complex biological sequence, with monosaccharides\nforming extended, non-linear sequences. As post-translational modifications,\nthey modulate protein structure, function, and interactions. Due to their\ndiversity and complexity, predictive models of glycan properties and functions\nare still insufficient.\n  Graph Neural Networks (GNNs) are deep learning models designed to process and\nanalyze graph-structured data. These architectures leverage the connectivity\nand relational information in graphs to learn effective representations of\nnodes, edges, and entire graphs. Iteratively aggregating information from\nneighboring nodes, GNNs capture complex patterns within graph data, making them\nparticularly well-suited for tasks such as link prediction or graph\nclassification across domains.\n  This work presents a new model architecture based on combinatorial complexes\nand higher-order message passing to extract features from glycan structures\ninto a latent space representation. The architecture is evaluated on an\nimproved GlycanML benchmark suite, establishing a new state-of-the-art\nperformance. We envision that these improvements will spur further advances in\ncomputational glycosciences and reveal the roles of glycans in biology.\n", "link": "http://arxiv.org/abs/2409.13467v3", "date": "2025-02-11", "relevancy": 2.3466, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4734}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4701}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Message%20Passing%20for%20Glycan%20Representation%20Learning&body=Title%3A%20Higher-Order%20Message%20Passing%20for%20Glycan%20Representation%20Learning%0AAuthor%3A%20Roman%20Joeres%20and%20Daniel%20Bojar%0AAbstract%3A%20%20%20Glycans%20are%20the%20most%20complex%20biological%20sequence%2C%20with%20monosaccharides%0Aforming%20extended%2C%20non-linear%20sequences.%20As%20post-translational%20modifications%2C%0Athey%20modulate%20protein%20structure%2C%20function%2C%20and%20interactions.%20Due%20to%20their%0Adiversity%20and%20complexity%2C%20predictive%20models%20of%20glycan%20properties%20and%20functions%0Aare%20still%20insufficient.%0A%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20deep%20learning%20models%20designed%20to%20process%20and%0Aanalyze%20graph-structured%20data.%20These%20architectures%20leverage%20the%20connectivity%0Aand%20relational%20information%20in%20graphs%20to%20learn%20effective%20representations%20of%0Anodes%2C%20edges%2C%20and%20entire%20graphs.%20Iteratively%20aggregating%20information%20from%0Aneighboring%20nodes%2C%20GNNs%20capture%20complex%20patterns%20within%20graph%20data%2C%20making%20them%0Aparticularly%20well-suited%20for%20tasks%20such%20as%20link%20prediction%20or%20graph%0Aclassification%20across%20domains.%0A%20%20This%20work%20presents%20a%20new%20model%20architecture%20based%20on%20combinatorial%20complexes%0Aand%20higher-order%20message%20passing%20to%20extract%20features%20from%20glycan%20structures%0Ainto%20a%20latent%20space%20representation.%20The%20architecture%20is%20evaluated%20on%20an%0Aimproved%20GlycanML%20benchmark%20suite%2C%20establishing%20a%20new%20state-of-the-art%0Aperformance.%20We%20envision%20that%20these%20improvements%20will%20spur%20further%20advances%20in%0Acomputational%20glycosciences%20and%20reveal%20the%20roles%20of%20glycans%20in%20biology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13467v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Message%2520Passing%2520for%2520Glycan%2520Representation%2520Learning%26entry.906535625%3DRoman%2520Joeres%2520and%2520Daniel%2520Bojar%26entry.1292438233%3D%2520%2520Glycans%2520are%2520the%2520most%2520complex%2520biological%2520sequence%252C%2520with%2520monosaccharides%250Aforming%2520extended%252C%2520non-linear%2520sequences.%2520As%2520post-translational%2520modifications%252C%250Athey%2520modulate%2520protein%2520structure%252C%2520function%252C%2520and%2520interactions.%2520Due%2520to%2520their%250Adiversity%2520and%2520complexity%252C%2520predictive%2520models%2520of%2520glycan%2520properties%2520and%2520functions%250Aare%2520still%2520insufficient.%250A%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520deep%2520learning%2520models%2520designed%2520to%2520process%2520and%250Aanalyze%2520graph-structured%2520data.%2520These%2520architectures%2520leverage%2520the%2520connectivity%250Aand%2520relational%2520information%2520in%2520graphs%2520to%2520learn%2520effective%2520representations%2520of%250Anodes%252C%2520edges%252C%2520and%2520entire%2520graphs.%2520Iteratively%2520aggregating%2520information%2520from%250Aneighboring%2520nodes%252C%2520GNNs%2520capture%2520complex%2520patterns%2520within%2520graph%2520data%252C%2520making%2520them%250Aparticularly%2520well-suited%2520for%2520tasks%2520such%2520as%2520link%2520prediction%2520or%2520graph%250Aclassification%2520across%2520domains.%250A%2520%2520This%2520work%2520presents%2520a%2520new%2520model%2520architecture%2520based%2520on%2520combinatorial%2520complexes%250Aand%2520higher-order%2520message%2520passing%2520to%2520extract%2520features%2520from%2520glycan%2520structures%250Ainto%2520a%2520latent%2520space%2520representation.%2520The%2520architecture%2520is%2520evaluated%2520on%2520an%250Aimproved%2520GlycanML%2520benchmark%2520suite%252C%2520establishing%2520a%2520new%2520state-of-the-art%250Aperformance.%2520We%2520envision%2520that%2520these%2520improvements%2520will%2520spur%2520further%2520advances%2520in%250Acomputational%2520glycosciences%2520and%2520reveal%2520the%2520roles%2520of%2520glycans%2520in%2520biology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13467v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Message%20Passing%20for%20Glycan%20Representation%20Learning&entry.906535625=Roman%20Joeres%20and%20Daniel%20Bojar&entry.1292438233=%20%20Glycans%20are%20the%20most%20complex%20biological%20sequence%2C%20with%20monosaccharides%0Aforming%20extended%2C%20non-linear%20sequences.%20As%20post-translational%20modifications%2C%0Athey%20modulate%20protein%20structure%2C%20function%2C%20and%20interactions.%20Due%20to%20their%0Adiversity%20and%20complexity%2C%20predictive%20models%20of%20glycan%20properties%20and%20functions%0Aare%20still%20insufficient.%0A%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20deep%20learning%20models%20designed%20to%20process%20and%0Aanalyze%20graph-structured%20data.%20These%20architectures%20leverage%20the%20connectivity%0Aand%20relational%20information%20in%20graphs%20to%20learn%20effective%20representations%20of%0Anodes%2C%20edges%2C%20and%20entire%20graphs.%20Iteratively%20aggregating%20information%20from%0Aneighboring%20nodes%2C%20GNNs%20capture%20complex%20patterns%20within%20graph%20data%2C%20making%20them%0Aparticularly%20well-suited%20for%20tasks%20such%20as%20link%20prediction%20or%20graph%0Aclassification%20across%20domains.%0A%20%20This%20work%20presents%20a%20new%20model%20architecture%20based%20on%20combinatorial%20complexes%0Aand%20higher-order%20message%20passing%20to%20extract%20features%20from%20glycan%20structures%0Ainto%20a%20latent%20space%20representation.%20The%20architecture%20is%20evaluated%20on%20an%0Aimproved%20GlycanML%20benchmark%20suite%2C%20establishing%20a%20new%20state-of-the-art%0Aperformance.%20We%20envision%20that%20these%20improvements%20will%20spur%20further%20advances%20in%0Acomputational%20glycosciences%20and%20reveal%20the%20roles%20of%20glycans%20in%20biology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13467v3&entry.124074799=Read"},
{"title": "Quantitative evaluation of unsupervised clustering algorithms for\n  dynamic total-body PET image analysis", "author": "Oona Rainio and Maria K. Jaakkola and Riku Kl\u00e9n", "abstract": "  Background. Recently, dynamic total-body positron emission tomography (PET)\nimaging has become possible due to new scanner devices. While clustering\nalgorithms have been proposed for PET analysis already earlier, there is still\nlittle research systematically evaluating these algorithms for processing of\ndynamic total-body PET images. Materials and methods. Here, we compare the\nperformance of 15 unsupervised clustering methods, including K-means either by\nitself or after principal component analysis (PCA) or independent component\nanalysis (ICA), Gaussian mixture model (GMM), fuzzy c-means (FCM),\nagglomerative clustering, spectral clustering, and several newer clustering\nalgorithms, for classifying time activity curves (TACs) in dynamic PET images.\nWe use dynamic total-body $^{15}$O-water PET images collected from 30 patients\nwith suspected or confirmed coronary artery disease. To evaluate the clustering\nalgorithms in a quantitative way, we use them to classify 5000 TACs from each\nimage based on whether the curve is taken from brain, right heart ventricle,\nright kidney, lower right lung lobe, or urinary bladder. Results. According to\nour results, the best methods are GMM, FCM, and ICA combined with mini batch\nK-means, which classified the TACs with a median accuracies of 89\\%, 83\\%, and\n81\\%, respectively, in a processing time of half a second or less on average\nfor each image. Conclusion. GMM, FCM, and ICA with mini batch K-means show\npromise for dynamic total-body PET analysis.\n", "link": "http://arxiv.org/abs/2502.07511v1", "date": "2025-02-11", "relevancy": 2.3441, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4617}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantitative%20evaluation%20of%20unsupervised%20clustering%20algorithms%20for%0A%20%20dynamic%20total-body%20PET%20image%20analysis&body=Title%3A%20Quantitative%20evaluation%20of%20unsupervised%20clustering%20algorithms%20for%0A%20%20dynamic%20total-body%20PET%20image%20analysis%0AAuthor%3A%20Oona%20Rainio%20and%20Maria%20K.%20Jaakkola%20and%20Riku%20Kl%C3%A9n%0AAbstract%3A%20%20%20Background.%20Recently%2C%20dynamic%20total-body%20positron%20emission%20tomography%20%28PET%29%0Aimaging%20has%20become%20possible%20due%20to%20new%20scanner%20devices.%20While%20clustering%0Aalgorithms%20have%20been%20proposed%20for%20PET%20analysis%20already%20earlier%2C%20there%20is%20still%0Alittle%20research%20systematically%20evaluating%20these%20algorithms%20for%20processing%20of%0Adynamic%20total-body%20PET%20images.%20Materials%20and%20methods.%20Here%2C%20we%20compare%20the%0Aperformance%20of%2015%20unsupervised%20clustering%20methods%2C%20including%20K-means%20either%20by%0Aitself%20or%20after%20principal%20component%20analysis%20%28PCA%29%20or%20independent%20component%0Aanalysis%20%28ICA%29%2C%20Gaussian%20mixture%20model%20%28GMM%29%2C%20fuzzy%20c-means%20%28FCM%29%2C%0Aagglomerative%20clustering%2C%20spectral%20clustering%2C%20and%20several%20newer%20clustering%0Aalgorithms%2C%20for%20classifying%20time%20activity%20curves%20%28TACs%29%20in%20dynamic%20PET%20images.%0AWe%20use%20dynamic%20total-body%20%24%5E%7B15%7D%24O-water%20PET%20images%20collected%20from%2030%20patients%0Awith%20suspected%20or%20confirmed%20coronary%20artery%20disease.%20To%20evaluate%20the%20clustering%0Aalgorithms%20in%20a%20quantitative%20way%2C%20we%20use%20them%20to%20classify%205000%20TACs%20from%20each%0Aimage%20based%20on%20whether%20the%20curve%20is%20taken%20from%20brain%2C%20right%20heart%20ventricle%2C%0Aright%20kidney%2C%20lower%20right%20lung%20lobe%2C%20or%20urinary%20bladder.%20Results.%20According%20to%0Aour%20results%2C%20the%20best%20methods%20are%20GMM%2C%20FCM%2C%20and%20ICA%20combined%20with%20mini%20batch%0AK-means%2C%20which%20classified%20the%20TACs%20with%20a%20median%20accuracies%20of%2089%5C%25%2C%2083%5C%25%2C%20and%0A81%5C%25%2C%20respectively%2C%20in%20a%20processing%20time%20of%20half%20a%20second%20or%20less%20on%20average%0Afor%20each%20image.%20Conclusion.%20GMM%2C%20FCM%2C%20and%20ICA%20with%20mini%20batch%20K-means%20show%0Apromise%20for%20dynamic%20total-body%20PET%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantitative%2520evaluation%2520of%2520unsupervised%2520clustering%2520algorithms%2520for%250A%2520%2520dynamic%2520total-body%2520PET%2520image%2520analysis%26entry.906535625%3DOona%2520Rainio%2520and%2520Maria%2520K.%2520Jaakkola%2520and%2520Riku%2520Kl%25C3%25A9n%26entry.1292438233%3D%2520%2520Background.%2520Recently%252C%2520dynamic%2520total-body%2520positron%2520emission%2520tomography%2520%2528PET%2529%250Aimaging%2520has%2520become%2520possible%2520due%2520to%2520new%2520scanner%2520devices.%2520While%2520clustering%250Aalgorithms%2520have%2520been%2520proposed%2520for%2520PET%2520analysis%2520already%2520earlier%252C%2520there%2520is%2520still%250Alittle%2520research%2520systematically%2520evaluating%2520these%2520algorithms%2520for%2520processing%2520of%250Adynamic%2520total-body%2520PET%2520images.%2520Materials%2520and%2520methods.%2520Here%252C%2520we%2520compare%2520the%250Aperformance%2520of%252015%2520unsupervised%2520clustering%2520methods%252C%2520including%2520K-means%2520either%2520by%250Aitself%2520or%2520after%2520principal%2520component%2520analysis%2520%2528PCA%2529%2520or%2520independent%2520component%250Aanalysis%2520%2528ICA%2529%252C%2520Gaussian%2520mixture%2520model%2520%2528GMM%2529%252C%2520fuzzy%2520c-means%2520%2528FCM%2529%252C%250Aagglomerative%2520clustering%252C%2520spectral%2520clustering%252C%2520and%2520several%2520newer%2520clustering%250Aalgorithms%252C%2520for%2520classifying%2520time%2520activity%2520curves%2520%2528TACs%2529%2520in%2520dynamic%2520PET%2520images.%250AWe%2520use%2520dynamic%2520total-body%2520%2524%255E%257B15%257D%2524O-water%2520PET%2520images%2520collected%2520from%252030%2520patients%250Awith%2520suspected%2520or%2520confirmed%2520coronary%2520artery%2520disease.%2520To%2520evaluate%2520the%2520clustering%250Aalgorithms%2520in%2520a%2520quantitative%2520way%252C%2520we%2520use%2520them%2520to%2520classify%25205000%2520TACs%2520from%2520each%250Aimage%2520based%2520on%2520whether%2520the%2520curve%2520is%2520taken%2520from%2520brain%252C%2520right%2520heart%2520ventricle%252C%250Aright%2520kidney%252C%2520lower%2520right%2520lung%2520lobe%252C%2520or%2520urinary%2520bladder.%2520Results.%2520According%2520to%250Aour%2520results%252C%2520the%2520best%2520methods%2520are%2520GMM%252C%2520FCM%252C%2520and%2520ICA%2520combined%2520with%2520mini%2520batch%250AK-means%252C%2520which%2520classified%2520the%2520TACs%2520with%2520a%2520median%2520accuracies%2520of%252089%255C%2525%252C%252083%255C%2525%252C%2520and%250A81%255C%2525%252C%2520respectively%252C%2520in%2520a%2520processing%2520time%2520of%2520half%2520a%2520second%2520or%2520less%2520on%2520average%250Afor%2520each%2520image.%2520Conclusion.%2520GMM%252C%2520FCM%252C%2520and%2520ICA%2520with%2520mini%2520batch%2520K-means%2520show%250Apromise%2520for%2520dynamic%2520total-body%2520PET%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantitative%20evaluation%20of%20unsupervised%20clustering%20algorithms%20for%0A%20%20dynamic%20total-body%20PET%20image%20analysis&entry.906535625=Oona%20Rainio%20and%20Maria%20K.%20Jaakkola%20and%20Riku%20Kl%C3%A9n&entry.1292438233=%20%20Background.%20Recently%2C%20dynamic%20total-body%20positron%20emission%20tomography%20%28PET%29%0Aimaging%20has%20become%20possible%20due%20to%20new%20scanner%20devices.%20While%20clustering%0Aalgorithms%20have%20been%20proposed%20for%20PET%20analysis%20already%20earlier%2C%20there%20is%20still%0Alittle%20research%20systematically%20evaluating%20these%20algorithms%20for%20processing%20of%0Adynamic%20total-body%20PET%20images.%20Materials%20and%20methods.%20Here%2C%20we%20compare%20the%0Aperformance%20of%2015%20unsupervised%20clustering%20methods%2C%20including%20K-means%20either%20by%0Aitself%20or%20after%20principal%20component%20analysis%20%28PCA%29%20or%20independent%20component%0Aanalysis%20%28ICA%29%2C%20Gaussian%20mixture%20model%20%28GMM%29%2C%20fuzzy%20c-means%20%28FCM%29%2C%0Aagglomerative%20clustering%2C%20spectral%20clustering%2C%20and%20several%20newer%20clustering%0Aalgorithms%2C%20for%20classifying%20time%20activity%20curves%20%28TACs%29%20in%20dynamic%20PET%20images.%0AWe%20use%20dynamic%20total-body%20%24%5E%7B15%7D%24O-water%20PET%20images%20collected%20from%2030%20patients%0Awith%20suspected%20or%20confirmed%20coronary%20artery%20disease.%20To%20evaluate%20the%20clustering%0Aalgorithms%20in%20a%20quantitative%20way%2C%20we%20use%20them%20to%20classify%205000%20TACs%20from%20each%0Aimage%20based%20on%20whether%20the%20curve%20is%20taken%20from%20brain%2C%20right%20heart%20ventricle%2C%0Aright%20kidney%2C%20lower%20right%20lung%20lobe%2C%20or%20urinary%20bladder.%20Results.%20According%20to%0Aour%20results%2C%20the%20best%20methods%20are%20GMM%2C%20FCM%2C%20and%20ICA%20combined%20with%20mini%20batch%0AK-means%2C%20which%20classified%20the%20TACs%20with%20a%20median%20accuracies%20of%2089%5C%25%2C%2083%5C%25%2C%20and%0A81%5C%25%2C%20respectively%2C%20in%20a%20processing%20time%20of%20half%20a%20second%20or%20less%20on%20average%0Afor%20each%20image.%20Conclusion.%20GMM%2C%20FCM%2C%20and%20ICA%20with%20mini%20batch%20K-means%20show%0Apromise%20for%20dynamic%20total-body%20PET%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07511v1&entry.124074799=Read"},
{"title": "Multiview Point Cloud Registration Based on Minimum Potential Energy for\n  Free-Form Blade Measurement", "author": "Zijie Wu and Yaonan Wang and Yang Mo and Qing Zhu and He Xie and Haotian Wu and Mingtao Feng and Ajmal Mian", "abstract": "  Point cloud registration is an essential step for free-form blade\nreconstruction in industrial measurement. Nonetheless, measuring defects of the\n3D acquisition system unavoidably result in noisy and incomplete point cloud\ndata, which renders efficient and accurate registration challenging. In this\npaper, we propose a novel global registration method that is based on the\nminimum potential energy (MPE) method to address these problems. The basic\nstrategy is that the objective function is defined as the minimum potential\nenergy optimization function of the physical registration system. The function\ndistributes more weight to the majority of inlier points and less weight to the\nnoise and outliers, which essentially reduces the influence of perturbations in\nthe mathematical formulation. We decompose the solution into a globally optimal\napproximation procedure and a fine registration process with the trimmed\niterative closest point algorithm to boost convergence. The approximation\nprocedure consists of two main steps. First, according to the construction of\nthe force traction operator, we can simply compute the position of the\npotential energy minimum. Second, to find the MPE point, we propose a new\ntheory that employs two flags to observe the status of the registration\nprocedure. We demonstrate the performance of the proposed algorithm on four\ntypes of blades. The proposed method outperforms the other global methods in\nterms of both accuracy and noise resistance.\n", "link": "http://arxiv.org/abs/2502.07680v1", "date": "2025-02-11", "relevancy": 2.3294, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4767}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4666}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiview%20Point%20Cloud%20Registration%20Based%20on%20Minimum%20Potential%20Energy%20for%0A%20%20Free-Form%20Blade%20Measurement&body=Title%3A%20Multiview%20Point%20Cloud%20Registration%20Based%20on%20Minimum%20Potential%20Energy%20for%0A%20%20Free-Form%20Blade%20Measurement%0AAuthor%3A%20Zijie%20Wu%20and%20Yaonan%20Wang%20and%20Yang%20Mo%20and%20Qing%20Zhu%20and%20He%20Xie%20and%20Haotian%20Wu%20and%20Mingtao%20Feng%20and%20Ajmal%20Mian%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20an%20essential%20step%20for%20free-form%20blade%0Areconstruction%20in%20industrial%20measurement.%20Nonetheless%2C%20measuring%20defects%20of%20the%0A3D%20acquisition%20system%20unavoidably%20result%20in%20noisy%20and%20incomplete%20point%20cloud%0Adata%2C%20which%20renders%20efficient%20and%20accurate%20registration%20challenging.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20global%20registration%20method%20that%20is%20based%20on%20the%0Aminimum%20potential%20energy%20%28MPE%29%20method%20to%20address%20these%20problems.%20The%20basic%0Astrategy%20is%20that%20the%20objective%20function%20is%20defined%20as%20the%20minimum%20potential%0Aenergy%20optimization%20function%20of%20the%20physical%20registration%20system.%20The%20function%0Adistributes%20more%20weight%20to%20the%20majority%20of%20inlier%20points%20and%20less%20weight%20to%20the%0Anoise%20and%20outliers%2C%20which%20essentially%20reduces%20the%20influence%20of%20perturbations%20in%0Athe%20mathematical%20formulation.%20We%20decompose%20the%20solution%20into%20a%20globally%20optimal%0Aapproximation%20procedure%20and%20a%20fine%20registration%20process%20with%20the%20trimmed%0Aiterative%20closest%20point%20algorithm%20to%20boost%20convergence.%20The%20approximation%0Aprocedure%20consists%20of%20two%20main%20steps.%20First%2C%20according%20to%20the%20construction%20of%0Athe%20force%20traction%20operator%2C%20we%20can%20simply%20compute%20the%20position%20of%20the%0Apotential%20energy%20minimum.%20Second%2C%20to%20find%20the%20MPE%20point%2C%20we%20propose%20a%20new%0Atheory%20that%20employs%20two%20flags%20to%20observe%20the%20status%20of%20the%20registration%0Aprocedure.%20We%20demonstrate%20the%20performance%20of%20the%20proposed%20algorithm%20on%20four%0Atypes%20of%20blades.%20The%20proposed%20method%20outperforms%20the%20other%20global%20methods%20in%0Aterms%20of%20both%20accuracy%20and%20noise%20resistance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiview%2520Point%2520Cloud%2520Registration%2520Based%2520on%2520Minimum%2520Potential%2520Energy%2520for%250A%2520%2520Free-Form%2520Blade%2520Measurement%26entry.906535625%3DZijie%2520Wu%2520and%2520Yaonan%2520Wang%2520and%2520Yang%2520Mo%2520and%2520Qing%2520Zhu%2520and%2520He%2520Xie%2520and%2520Haotian%2520Wu%2520and%2520Mingtao%2520Feng%2520and%2520Ajmal%2520Mian%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520an%2520essential%2520step%2520for%2520free-form%2520blade%250Areconstruction%2520in%2520industrial%2520measurement.%2520Nonetheless%252C%2520measuring%2520defects%2520of%2520the%250A3D%2520acquisition%2520system%2520unavoidably%2520result%2520in%2520noisy%2520and%2520incomplete%2520point%2520cloud%250Adata%252C%2520which%2520renders%2520efficient%2520and%2520accurate%2520registration%2520challenging.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520global%2520registration%2520method%2520that%2520is%2520based%2520on%2520the%250Aminimum%2520potential%2520energy%2520%2528MPE%2529%2520method%2520to%2520address%2520these%2520problems.%2520The%2520basic%250Astrategy%2520is%2520that%2520the%2520objective%2520function%2520is%2520defined%2520as%2520the%2520minimum%2520potential%250Aenergy%2520optimization%2520function%2520of%2520the%2520physical%2520registration%2520system.%2520The%2520function%250Adistributes%2520more%2520weight%2520to%2520the%2520majority%2520of%2520inlier%2520points%2520and%2520less%2520weight%2520to%2520the%250Anoise%2520and%2520outliers%252C%2520which%2520essentially%2520reduces%2520the%2520influence%2520of%2520perturbations%2520in%250Athe%2520mathematical%2520formulation.%2520We%2520decompose%2520the%2520solution%2520into%2520a%2520globally%2520optimal%250Aapproximation%2520procedure%2520and%2520a%2520fine%2520registration%2520process%2520with%2520the%2520trimmed%250Aiterative%2520closest%2520point%2520algorithm%2520to%2520boost%2520convergence.%2520The%2520approximation%250Aprocedure%2520consists%2520of%2520two%2520main%2520steps.%2520First%252C%2520according%2520to%2520the%2520construction%2520of%250Athe%2520force%2520traction%2520operator%252C%2520we%2520can%2520simply%2520compute%2520the%2520position%2520of%2520the%250Apotential%2520energy%2520minimum.%2520Second%252C%2520to%2520find%2520the%2520MPE%2520point%252C%2520we%2520propose%2520a%2520new%250Atheory%2520that%2520employs%2520two%2520flags%2520to%2520observe%2520the%2520status%2520of%2520the%2520registration%250Aprocedure.%2520We%2520demonstrate%2520the%2520performance%2520of%2520the%2520proposed%2520algorithm%2520on%2520four%250Atypes%2520of%2520blades.%2520The%2520proposed%2520method%2520outperforms%2520the%2520other%2520global%2520methods%2520in%250Aterms%2520of%2520both%2520accuracy%2520and%2520noise%2520resistance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiview%20Point%20Cloud%20Registration%20Based%20on%20Minimum%20Potential%20Energy%20for%0A%20%20Free-Form%20Blade%20Measurement&entry.906535625=Zijie%20Wu%20and%20Yaonan%20Wang%20and%20Yang%20Mo%20and%20Qing%20Zhu%20and%20He%20Xie%20and%20Haotian%20Wu%20and%20Mingtao%20Feng%20and%20Ajmal%20Mian&entry.1292438233=%20%20Point%20cloud%20registration%20is%20an%20essential%20step%20for%20free-form%20blade%0Areconstruction%20in%20industrial%20measurement.%20Nonetheless%2C%20measuring%20defects%20of%20the%0A3D%20acquisition%20system%20unavoidably%20result%20in%20noisy%20and%20incomplete%20point%20cloud%0Adata%2C%20which%20renders%20efficient%20and%20accurate%20registration%20challenging.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20global%20registration%20method%20that%20is%20based%20on%20the%0Aminimum%20potential%20energy%20%28MPE%29%20method%20to%20address%20these%20problems.%20The%20basic%0Astrategy%20is%20that%20the%20objective%20function%20is%20defined%20as%20the%20minimum%20potential%0Aenergy%20optimization%20function%20of%20the%20physical%20registration%20system.%20The%20function%0Adistributes%20more%20weight%20to%20the%20majority%20of%20inlier%20points%20and%20less%20weight%20to%20the%0Anoise%20and%20outliers%2C%20which%20essentially%20reduces%20the%20influence%20of%20perturbations%20in%0Athe%20mathematical%20formulation.%20We%20decompose%20the%20solution%20into%20a%20globally%20optimal%0Aapproximation%20procedure%20and%20a%20fine%20registration%20process%20with%20the%20trimmed%0Aiterative%20closest%20point%20algorithm%20to%20boost%20convergence.%20The%20approximation%0Aprocedure%20consists%20of%20two%20main%20steps.%20First%2C%20according%20to%20the%20construction%20of%0Athe%20force%20traction%20operator%2C%20we%20can%20simply%20compute%20the%20position%20of%20the%0Apotential%20energy%20minimum.%20Second%2C%20to%20find%20the%20MPE%20point%2C%20we%20propose%20a%20new%0Atheory%20that%20employs%20two%20flags%20to%20observe%20the%20status%20of%20the%20registration%0Aprocedure.%20We%20demonstrate%20the%20performance%20of%20the%20proposed%20algorithm%20on%20four%0Atypes%20of%20blades.%20The%20proposed%20method%20outperforms%20the%20other%20global%20methods%20in%0Aterms%20of%20both%20accuracy%20and%20noise%20resistance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07680v1&entry.124074799=Read"},
{"title": "A Flag Decomposition for Hierarchical Datasets", "author": "Nathan Mankovich and Ignacio Santamaria and Gustau Camps-Valls and Tolga Birdal", "abstract": "  Flag manifolds encode hierarchical nested sequences of subspaces and serve as\npowerful structures for various computer vision and machine learning\napplications. Despite their utility in tasks such as dimensionality reduction,\nmotion averaging, and subspace clustering, current applications are often\nrestricted to extracting flags using common matrix decomposition methods like\nthe singular value decomposition. Here, we address the need for a general\nalgorithm to factorize and work with hierarchical datasets. In particular, we\npropose a novel, flag-based method that decomposes arbitrary hierarchical\nreal-valued data into a hierarchy-preserving flag representation in Stiefel\ncoordinates. Our work harnesses the potential of flag manifolds in applications\nincluding denoising, clustering, and few-shot learning.\n", "link": "http://arxiv.org/abs/2502.07782v1", "date": "2025-02-11", "relevancy": 2.3269, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.453}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Flag%20Decomposition%20for%20Hierarchical%20Datasets&body=Title%3A%20A%20Flag%20Decomposition%20for%20Hierarchical%20Datasets%0AAuthor%3A%20Nathan%20Mankovich%20and%20Ignacio%20Santamaria%20and%20Gustau%20Camps-Valls%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Flag%20manifolds%20encode%20hierarchical%20nested%20sequences%20of%20subspaces%20and%20serve%20as%0Apowerful%20structures%20for%20various%20computer%20vision%20and%20machine%20learning%0Aapplications.%20Despite%20their%20utility%20in%20tasks%20such%20as%20dimensionality%20reduction%2C%0Amotion%20averaging%2C%20and%20subspace%20clustering%2C%20current%20applications%20are%20often%0Arestricted%20to%20extracting%20flags%20using%20common%20matrix%20decomposition%20methods%20like%0Athe%20singular%20value%20decomposition.%20Here%2C%20we%20address%20the%20need%20for%20a%20general%0Aalgorithm%20to%20factorize%20and%20work%20with%20hierarchical%20datasets.%20In%20particular%2C%20we%0Apropose%20a%20novel%2C%20flag-based%20method%20that%20decomposes%20arbitrary%20hierarchical%0Areal-valued%20data%20into%20a%20hierarchy-preserving%20flag%20representation%20in%20Stiefel%0Acoordinates.%20Our%20work%20harnesses%20the%20potential%20of%20flag%20manifolds%20in%20applications%0Aincluding%20denoising%2C%20clustering%2C%20and%20few-shot%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Flag%2520Decomposition%2520for%2520Hierarchical%2520Datasets%26entry.906535625%3DNathan%2520Mankovich%2520and%2520Ignacio%2520Santamaria%2520and%2520Gustau%2520Camps-Valls%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Flag%2520manifolds%2520encode%2520hierarchical%2520nested%2520sequences%2520of%2520subspaces%2520and%2520serve%2520as%250Apowerful%2520structures%2520for%2520various%2520computer%2520vision%2520and%2520machine%2520learning%250Aapplications.%2520Despite%2520their%2520utility%2520in%2520tasks%2520such%2520as%2520dimensionality%2520reduction%252C%250Amotion%2520averaging%252C%2520and%2520subspace%2520clustering%252C%2520current%2520applications%2520are%2520often%250Arestricted%2520to%2520extracting%2520flags%2520using%2520common%2520matrix%2520decomposition%2520methods%2520like%250Athe%2520singular%2520value%2520decomposition.%2520Here%252C%2520we%2520address%2520the%2520need%2520for%2520a%2520general%250Aalgorithm%2520to%2520factorize%2520and%2520work%2520with%2520hierarchical%2520datasets.%2520In%2520particular%252C%2520we%250Apropose%2520a%2520novel%252C%2520flag-based%2520method%2520that%2520decomposes%2520arbitrary%2520hierarchical%250Areal-valued%2520data%2520into%2520a%2520hierarchy-preserving%2520flag%2520representation%2520in%2520Stiefel%250Acoordinates.%2520Our%2520work%2520harnesses%2520the%2520potential%2520of%2520flag%2520manifolds%2520in%2520applications%250Aincluding%2520denoising%252C%2520clustering%252C%2520and%2520few-shot%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Flag%20Decomposition%20for%20Hierarchical%20Datasets&entry.906535625=Nathan%20Mankovich%20and%20Ignacio%20Santamaria%20and%20Gustau%20Camps-Valls%20and%20Tolga%20Birdal&entry.1292438233=%20%20Flag%20manifolds%20encode%20hierarchical%20nested%20sequences%20of%20subspaces%20and%20serve%20as%0Apowerful%20structures%20for%20various%20computer%20vision%20and%20machine%20learning%0Aapplications.%20Despite%20their%20utility%20in%20tasks%20such%20as%20dimensionality%20reduction%2C%0Amotion%20averaging%2C%20and%20subspace%20clustering%2C%20current%20applications%20are%20often%0Arestricted%20to%20extracting%20flags%20using%20common%20matrix%20decomposition%20methods%20like%0Athe%20singular%20value%20decomposition.%20Here%2C%20we%20address%20the%20need%20for%20a%20general%0Aalgorithm%20to%20factorize%20and%20work%20with%20hierarchical%20datasets.%20In%20particular%2C%20we%0Apropose%20a%20novel%2C%20flag-based%20method%20that%20decomposes%20arbitrary%20hierarchical%0Areal-valued%20data%20into%20a%20hierarchy-preserving%20flag%20representation%20in%20Stiefel%0Acoordinates.%20Our%20work%20harnesses%20the%20potential%20of%20flag%20manifolds%20in%20applications%0Aincluding%20denoising%2C%20clustering%2C%20and%20few-shot%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07782v1&entry.124074799=Read"},
{"title": "FedAPA: Server-side Gradient-Based Adaptive Personalized Aggregation for\n  Federated Learning on Heterogeneous Data", "author": "Yuxia Sun and Aoxiang Sun and Siyi Pan and Zhixiao Fu and Jingcai Guo", "abstract": "  Personalized federated learning (PFL) tailors models to clients' unique data\ndistributions while preserving privacy. However, existing\naggregation-weight-based PFL methods often struggle with heterogeneous data,\nfacing challenges in accuracy, computational efficiency, and communication\noverhead. We propose FedAPA, a novel PFL method featuring a server-side,\ngradient-based adaptive aggregation strategy to generate personalized models,\nby updating aggregation weights based on gradients of client-parameter changes\nwith respect to the aggregation weights in a centralized manner. FedAPA\nguarantees theoretical convergence and achieves superior accuracy and\ncomputational efficiency compared to 10 PFL competitors across three datasets,\nwith competitive communication overhead.\n", "link": "http://arxiv.org/abs/2502.07456v1", "date": "2025-02-11", "relevancy": 2.3174, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4685}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4638}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedAPA%3A%20Server-side%20Gradient-Based%20Adaptive%20Personalized%20Aggregation%20for%0A%20%20Federated%20Learning%20on%20Heterogeneous%20Data&body=Title%3A%20FedAPA%3A%20Server-side%20Gradient-Based%20Adaptive%20Personalized%20Aggregation%20for%0A%20%20Federated%20Learning%20on%20Heterogeneous%20Data%0AAuthor%3A%20Yuxia%20Sun%20and%20Aoxiang%20Sun%20and%20Siyi%20Pan%20and%20Zhixiao%20Fu%20and%20Jingcai%20Guo%0AAbstract%3A%20%20%20Personalized%20federated%20learning%20%28PFL%29%20tailors%20models%20to%20clients%27%20unique%20data%0Adistributions%20while%20preserving%20privacy.%20However%2C%20existing%0Aaggregation-weight-based%20PFL%20methods%20often%20struggle%20with%20heterogeneous%20data%2C%0Afacing%20challenges%20in%20accuracy%2C%20computational%20efficiency%2C%20and%20communication%0Aoverhead.%20We%20propose%20FedAPA%2C%20a%20novel%20PFL%20method%20featuring%20a%20server-side%2C%0Agradient-based%20adaptive%20aggregation%20strategy%20to%20generate%20personalized%20models%2C%0Aby%20updating%20aggregation%20weights%20based%20on%20gradients%20of%20client-parameter%20changes%0Awith%20respect%20to%20the%20aggregation%20weights%20in%20a%20centralized%20manner.%20FedAPA%0Aguarantees%20theoretical%20convergence%20and%20achieves%20superior%20accuracy%20and%0Acomputational%20efficiency%20compared%20to%2010%20PFL%20competitors%20across%20three%20datasets%2C%0Awith%20competitive%20communication%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedAPA%253A%2520Server-side%2520Gradient-Based%2520Adaptive%2520Personalized%2520Aggregation%2520for%250A%2520%2520Federated%2520Learning%2520on%2520Heterogeneous%2520Data%26entry.906535625%3DYuxia%2520Sun%2520and%2520Aoxiang%2520Sun%2520and%2520Siyi%2520Pan%2520and%2520Zhixiao%2520Fu%2520and%2520Jingcai%2520Guo%26entry.1292438233%3D%2520%2520Personalized%2520federated%2520learning%2520%2528PFL%2529%2520tailors%2520models%2520to%2520clients%2527%2520unique%2520data%250Adistributions%2520while%2520preserving%2520privacy.%2520However%252C%2520existing%250Aaggregation-weight-based%2520PFL%2520methods%2520often%2520struggle%2520with%2520heterogeneous%2520data%252C%250Afacing%2520challenges%2520in%2520accuracy%252C%2520computational%2520efficiency%252C%2520and%2520communication%250Aoverhead.%2520We%2520propose%2520FedAPA%252C%2520a%2520novel%2520PFL%2520method%2520featuring%2520a%2520server-side%252C%250Agradient-based%2520adaptive%2520aggregation%2520strategy%2520to%2520generate%2520personalized%2520models%252C%250Aby%2520updating%2520aggregation%2520weights%2520based%2520on%2520gradients%2520of%2520client-parameter%2520changes%250Awith%2520respect%2520to%2520the%2520aggregation%2520weights%2520in%2520a%2520centralized%2520manner.%2520FedAPA%250Aguarantees%2520theoretical%2520convergence%2520and%2520achieves%2520superior%2520accuracy%2520and%250Acomputational%2520efficiency%2520compared%2520to%252010%2520PFL%2520competitors%2520across%2520three%2520datasets%252C%250Awith%2520competitive%2520communication%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedAPA%3A%20Server-side%20Gradient-Based%20Adaptive%20Personalized%20Aggregation%20for%0A%20%20Federated%20Learning%20on%20Heterogeneous%20Data&entry.906535625=Yuxia%20Sun%20and%20Aoxiang%20Sun%20and%20Siyi%20Pan%20and%20Zhixiao%20Fu%20and%20Jingcai%20Guo&entry.1292438233=%20%20Personalized%20federated%20learning%20%28PFL%29%20tailors%20models%20to%20clients%27%20unique%20data%0Adistributions%20while%20preserving%20privacy.%20However%2C%20existing%0Aaggregation-weight-based%20PFL%20methods%20often%20struggle%20with%20heterogeneous%20data%2C%0Afacing%20challenges%20in%20accuracy%2C%20computational%20efficiency%2C%20and%20communication%0Aoverhead.%20We%20propose%20FedAPA%2C%20a%20novel%20PFL%20method%20featuring%20a%20server-side%2C%0Agradient-based%20adaptive%20aggregation%20strategy%20to%20generate%20personalized%20models%2C%0Aby%20updating%20aggregation%20weights%20based%20on%20gradients%20of%20client-parameter%20changes%0Awith%20respect%20to%20the%20aggregation%20weights%20in%20a%20centralized%20manner.%20FedAPA%0Aguarantees%20theoretical%20convergence%20and%20achieves%20superior%20accuracy%20and%0Acomputational%20efficiency%20compared%20to%2010%20PFL%20competitors%20across%20three%20datasets%2C%0Awith%20competitive%20communication%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07456v1&entry.124074799=Read"},
{"title": "Revisiting the Initial Steps in Adaptive Gradient Descent Optimization", "author": "Abulikemu Abuduweili and Changliu Liu", "abstract": "  Adaptive gradient optimization methods, such as Adam, are prevalent in\ntraining deep neural networks across diverse machine learning tasks due to\ntheir ability to achieve faster convergence. However, these methods often\nsuffer from suboptimal generalization compared to stochastic gradient descent\n(SGD) and exhibit instability, particularly when training Transformer models.\nIn this work, we show the standard initialization of the second-order moment\nestimation ($v_0 =0$) as a significant factor contributing to these\nlimitations. We introduce simple yet effective solutions: initializing the\nsecond-order moment estimation with non-zero values, using either data-driven\nor random initialization strategies. Empirical evaluations demonstrate that our\napproach not only stabilizes convergence but also enhances the final\nperformance of adaptive gradient optimizers. Furthermore, by adopting the\nproposed initialization strategies, Adam achieves performance comparable to\nmany recently proposed variants of adaptive gradient optimization methods. Our\ncode is available at https://github.com/Walleclipse/Adam_Initialization.\n", "link": "http://arxiv.org/abs/2412.02153v2", "date": "2025-02-11", "relevancy": 2.312, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.476}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4588}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20the%20Initial%20Steps%20in%20Adaptive%20Gradient%20Descent%20Optimization&body=Title%3A%20Revisiting%20the%20Initial%20Steps%20in%20Adaptive%20Gradient%20Descent%20Optimization%0AAuthor%3A%20Abulikemu%20Abuduweili%20and%20Changliu%20Liu%0AAbstract%3A%20%20%20Adaptive%20gradient%20optimization%20methods%2C%20such%20as%20Adam%2C%20are%20prevalent%20in%0Atraining%20deep%20neural%20networks%20across%20diverse%20machine%20learning%20tasks%20due%20to%0Atheir%20ability%20to%20achieve%20faster%20convergence.%20However%2C%20these%20methods%20often%0Asuffer%20from%20suboptimal%20generalization%20compared%20to%20stochastic%20gradient%20descent%0A%28SGD%29%20and%20exhibit%20instability%2C%20particularly%20when%20training%20Transformer%20models.%0AIn%20this%20work%2C%20we%20show%20the%20standard%20initialization%20of%20the%20second-order%20moment%0Aestimation%20%28%24v_0%20%3D0%24%29%20as%20a%20significant%20factor%20contributing%20to%20these%0Alimitations.%20We%20introduce%20simple%20yet%20effective%20solutions%3A%20initializing%20the%0Asecond-order%20moment%20estimation%20with%20non-zero%20values%2C%20using%20either%20data-driven%0Aor%20random%20initialization%20strategies.%20Empirical%20evaluations%20demonstrate%20that%20our%0Aapproach%20not%20only%20stabilizes%20convergence%20but%20also%20enhances%20the%20final%0Aperformance%20of%20adaptive%20gradient%20optimizers.%20Furthermore%2C%20by%20adopting%20the%0Aproposed%20initialization%20strategies%2C%20Adam%20achieves%20performance%20comparable%20to%0Amany%20recently%20proposed%20variants%20of%20adaptive%20gradient%20optimization%20methods.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/Walleclipse/Adam_Initialization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520the%2520Initial%2520Steps%2520in%2520Adaptive%2520Gradient%2520Descent%2520Optimization%26entry.906535625%3DAbulikemu%2520Abuduweili%2520and%2520Changliu%2520Liu%26entry.1292438233%3D%2520%2520Adaptive%2520gradient%2520optimization%2520methods%252C%2520such%2520as%2520Adam%252C%2520are%2520prevalent%2520in%250Atraining%2520deep%2520neural%2520networks%2520across%2520diverse%2520machine%2520learning%2520tasks%2520due%2520to%250Atheir%2520ability%2520to%2520achieve%2520faster%2520convergence.%2520However%252C%2520these%2520methods%2520often%250Asuffer%2520from%2520suboptimal%2520generalization%2520compared%2520to%2520stochastic%2520gradient%2520descent%250A%2528SGD%2529%2520and%2520exhibit%2520instability%252C%2520particularly%2520when%2520training%2520Transformer%2520models.%250AIn%2520this%2520work%252C%2520we%2520show%2520the%2520standard%2520initialization%2520of%2520the%2520second-order%2520moment%250Aestimation%2520%2528%2524v_0%2520%253D0%2524%2529%2520as%2520a%2520significant%2520factor%2520contributing%2520to%2520these%250Alimitations.%2520We%2520introduce%2520simple%2520yet%2520effective%2520solutions%253A%2520initializing%2520the%250Asecond-order%2520moment%2520estimation%2520with%2520non-zero%2520values%252C%2520using%2520either%2520data-driven%250Aor%2520random%2520initialization%2520strategies.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520our%250Aapproach%2520not%2520only%2520stabilizes%2520convergence%2520but%2520also%2520enhances%2520the%2520final%250Aperformance%2520of%2520adaptive%2520gradient%2520optimizers.%2520Furthermore%252C%2520by%2520adopting%2520the%250Aproposed%2520initialization%2520strategies%252C%2520Adam%2520achieves%2520performance%2520comparable%2520to%250Amany%2520recently%2520proposed%2520variants%2520of%2520adaptive%2520gradient%2520optimization%2520methods.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/Walleclipse/Adam_Initialization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20the%20Initial%20Steps%20in%20Adaptive%20Gradient%20Descent%20Optimization&entry.906535625=Abulikemu%20Abuduweili%20and%20Changliu%20Liu&entry.1292438233=%20%20Adaptive%20gradient%20optimization%20methods%2C%20such%20as%20Adam%2C%20are%20prevalent%20in%0Atraining%20deep%20neural%20networks%20across%20diverse%20machine%20learning%20tasks%20due%20to%0Atheir%20ability%20to%20achieve%20faster%20convergence.%20However%2C%20these%20methods%20often%0Asuffer%20from%20suboptimal%20generalization%20compared%20to%20stochastic%20gradient%20descent%0A%28SGD%29%20and%20exhibit%20instability%2C%20particularly%20when%20training%20Transformer%20models.%0AIn%20this%20work%2C%20we%20show%20the%20standard%20initialization%20of%20the%20second-order%20moment%0Aestimation%20%28%24v_0%20%3D0%24%29%20as%20a%20significant%20factor%20contributing%20to%20these%0Alimitations.%20We%20introduce%20simple%20yet%20effective%20solutions%3A%20initializing%20the%0Asecond-order%20moment%20estimation%20with%20non-zero%20values%2C%20using%20either%20data-driven%0Aor%20random%20initialization%20strategies.%20Empirical%20evaluations%20demonstrate%20that%20our%0Aapproach%20not%20only%20stabilizes%20convergence%20but%20also%20enhances%20the%20final%0Aperformance%20of%20adaptive%20gradient%20optimizers.%20Furthermore%2C%20by%20adopting%20the%0Aproposed%20initialization%20strategies%2C%20Adam%20achieves%20performance%20comparable%20to%0Amany%20recently%20proposed%20variants%20of%20adaptive%20gradient%20optimization%20methods.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/Walleclipse/Adam_Initialization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02153v2&entry.124074799=Read"},
{"title": "Robotic In-Hand Manipulation for Large-Range Precise Object Movement:\n  The RGMC Champion Solution", "author": "Mingrui Yu and Yongpeng Jiang and Chen Chen and Yongyi Jia and Xiang Li", "abstract": "  In-hand manipulation using multiple dexterous fingers is a critical robotic\nskill that can reduce the reliance on large arm motions, thereby saving space\nand energy. This letter focuses on in-grasp object movement, which refers to\nmanipulating an object to a desired pose through only finger motions within a\nstable grasp. The key challenge lies in simultaneously achieving high precision\nand large-range movements while maintaining a constant stable grasp. To address\nthis problem, we propose a simple and practical approach based on kinematic\ntrajectory optimization with no need for pretraining or object geometries,\nwhich can be easily applied to novel objects in real-world scenarios. Adopting\nthis approach, we won the championship for the in-hand manipulation track at\nthe 9th Robotic Grasping and Manipulation Competition (RGMC) held at ICRA 2024.\nImplementation details, discussion, and further quantitative experimental\nresults are presented in this letter, which aims to comprehensively evaluate\nour approach and share our key takeaways from the competition. Supplementary\nmaterials including video and code are available at\nhttps://rgmc-xl-team.github.io/ingrasp_manipulation .\n", "link": "http://arxiv.org/abs/2502.07472v1", "date": "2025-02-11", "relevancy": 2.311, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6008}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5986}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20In-Hand%20Manipulation%20for%20Large-Range%20Precise%20Object%20Movement%3A%0A%20%20The%20RGMC%20Champion%20Solution&body=Title%3A%20Robotic%20In-Hand%20Manipulation%20for%20Large-Range%20Precise%20Object%20Movement%3A%0A%20%20The%20RGMC%20Champion%20Solution%0AAuthor%3A%20Mingrui%20Yu%20and%20Yongpeng%20Jiang%20and%20Chen%20Chen%20and%20Yongyi%20Jia%20and%20Xiang%20Li%0AAbstract%3A%20%20%20In-hand%20manipulation%20using%20multiple%20dexterous%20fingers%20is%20a%20critical%20robotic%0Askill%20that%20can%20reduce%20the%20reliance%20on%20large%20arm%20motions%2C%20thereby%20saving%20space%0Aand%20energy.%20This%20letter%20focuses%20on%20in-grasp%20object%20movement%2C%20which%20refers%20to%0Amanipulating%20an%20object%20to%20a%20desired%20pose%20through%20only%20finger%20motions%20within%20a%0Astable%20grasp.%20The%20key%20challenge%20lies%20in%20simultaneously%20achieving%20high%20precision%0Aand%20large-range%20movements%20while%20maintaining%20a%20constant%20stable%20grasp.%20To%20address%0Athis%20problem%2C%20we%20propose%20a%20simple%20and%20practical%20approach%20based%20on%20kinematic%0Atrajectory%20optimization%20with%20no%20need%20for%20pretraining%20or%20object%20geometries%2C%0Awhich%20can%20be%20easily%20applied%20to%20novel%20objects%20in%20real-world%20scenarios.%20Adopting%0Athis%20approach%2C%20we%20won%20the%20championship%20for%20the%20in-hand%20manipulation%20track%20at%0Athe%209th%20Robotic%20Grasping%20and%20Manipulation%20Competition%20%28RGMC%29%20held%20at%20ICRA%202024.%0AImplementation%20details%2C%20discussion%2C%20and%20further%20quantitative%20experimental%0Aresults%20are%20presented%20in%20this%20letter%2C%20which%20aims%20to%20comprehensively%20evaluate%0Aour%20approach%20and%20share%20our%20key%20takeaways%20from%20the%20competition.%20Supplementary%0Amaterials%20including%20video%20and%20code%20are%20available%20at%0Ahttps%3A//rgmc-xl-team.github.io/ingrasp_manipulation%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520In-Hand%2520Manipulation%2520for%2520Large-Range%2520Precise%2520Object%2520Movement%253A%250A%2520%2520The%2520RGMC%2520Champion%2520Solution%26entry.906535625%3DMingrui%2520Yu%2520and%2520Yongpeng%2520Jiang%2520and%2520Chen%2520Chen%2520and%2520Yongyi%2520Jia%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520In-hand%2520manipulation%2520using%2520multiple%2520dexterous%2520fingers%2520is%2520a%2520critical%2520robotic%250Askill%2520that%2520can%2520reduce%2520the%2520reliance%2520on%2520large%2520arm%2520motions%252C%2520thereby%2520saving%2520space%250Aand%2520energy.%2520This%2520letter%2520focuses%2520on%2520in-grasp%2520object%2520movement%252C%2520which%2520refers%2520to%250Amanipulating%2520an%2520object%2520to%2520a%2520desired%2520pose%2520through%2520only%2520finger%2520motions%2520within%2520a%250Astable%2520grasp.%2520The%2520key%2520challenge%2520lies%2520in%2520simultaneously%2520achieving%2520high%2520precision%250Aand%2520large-range%2520movements%2520while%2520maintaining%2520a%2520constant%2520stable%2520grasp.%2520To%2520address%250Athis%2520problem%252C%2520we%2520propose%2520a%2520simple%2520and%2520practical%2520approach%2520based%2520on%2520kinematic%250Atrajectory%2520optimization%2520with%2520no%2520need%2520for%2520pretraining%2520or%2520object%2520geometries%252C%250Awhich%2520can%2520be%2520easily%2520applied%2520to%2520novel%2520objects%2520in%2520real-world%2520scenarios.%2520Adopting%250Athis%2520approach%252C%2520we%2520won%2520the%2520championship%2520for%2520the%2520in-hand%2520manipulation%2520track%2520at%250Athe%25209th%2520Robotic%2520Grasping%2520and%2520Manipulation%2520Competition%2520%2528RGMC%2529%2520held%2520at%2520ICRA%25202024.%250AImplementation%2520details%252C%2520discussion%252C%2520and%2520further%2520quantitative%2520experimental%250Aresults%2520are%2520presented%2520in%2520this%2520letter%252C%2520which%2520aims%2520to%2520comprehensively%2520evaluate%250Aour%2520approach%2520and%2520share%2520our%2520key%2520takeaways%2520from%2520the%2520competition.%2520Supplementary%250Amaterials%2520including%2520video%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//rgmc-xl-team.github.io/ingrasp_manipulation%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20In-Hand%20Manipulation%20for%20Large-Range%20Precise%20Object%20Movement%3A%0A%20%20The%20RGMC%20Champion%20Solution&entry.906535625=Mingrui%20Yu%20and%20Yongpeng%20Jiang%20and%20Chen%20Chen%20and%20Yongyi%20Jia%20and%20Xiang%20Li&entry.1292438233=%20%20In-hand%20manipulation%20using%20multiple%20dexterous%20fingers%20is%20a%20critical%20robotic%0Askill%20that%20can%20reduce%20the%20reliance%20on%20large%20arm%20motions%2C%20thereby%20saving%20space%0Aand%20energy.%20This%20letter%20focuses%20on%20in-grasp%20object%20movement%2C%20which%20refers%20to%0Amanipulating%20an%20object%20to%20a%20desired%20pose%20through%20only%20finger%20motions%20within%20a%0Astable%20grasp.%20The%20key%20challenge%20lies%20in%20simultaneously%20achieving%20high%20precision%0Aand%20large-range%20movements%20while%20maintaining%20a%20constant%20stable%20grasp.%20To%20address%0Athis%20problem%2C%20we%20propose%20a%20simple%20and%20practical%20approach%20based%20on%20kinematic%0Atrajectory%20optimization%20with%20no%20need%20for%20pretraining%20or%20object%20geometries%2C%0Awhich%20can%20be%20easily%20applied%20to%20novel%20objects%20in%20real-world%20scenarios.%20Adopting%0Athis%20approach%2C%20we%20won%20the%20championship%20for%20the%20in-hand%20manipulation%20track%20at%0Athe%209th%20Robotic%20Grasping%20and%20Manipulation%20Competition%20%28RGMC%29%20held%20at%20ICRA%202024.%0AImplementation%20details%2C%20discussion%2C%20and%20further%20quantitative%20experimental%0Aresults%20are%20presented%20in%20this%20letter%2C%20which%20aims%20to%20comprehensively%20evaluate%0Aour%20approach%20and%20share%20our%20key%20takeaways%20from%20the%20competition.%20Supplementary%0Amaterials%20including%20video%20and%20code%20are%20available%20at%0Ahttps%3A//rgmc-xl-team.github.io/ingrasp_manipulation%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07472v1&entry.124074799=Read"},
{"title": "Enhancing Ground-to-Aerial Image Matching for Visual Misinformation\n  Detection Using Semantic Segmentation", "author": "Emanuele Mule and Matteo Pannacci and Ali Ghasemi Goudarzi and Francesco Pro and Lorenzo Papa and Luca Maiano and Irene Amerini", "abstract": "  The recent advancements in generative AI techniques, which have significantly\nincreased the online dissemination of altered images and videos, have raised\nserious concerns about the credibility of digital media available on the\nInternet and distributed through information channels and social networks. This\nissue particularly affects domains that rely heavily on trustworthy data, such\nas journalism, forensic analysis, and Earth observation. To address these\nconcerns, the ability to geolocate a non-geo-tagged ground-view image without\nexternal information, such as GPS coordinates, has become increasingly\ncritical. This study tackles the challenge of linking a ground-view image,\npotentially exhibiting varying fields of view (FoV), to its corresponding\nsatellite image without the aid of GPS data. To achieve this, we propose a\nnovel four-stream Siamese-like architecture, the Quadruple Semantic Align Net\n(SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by\nleveraging semantic segmentation applied to both ground and satellite imagery.\nExperimental results on a subset of the CVUSA dataset demonstrate significant\nimprovements of up to 9.8\\% over prior methods across various FoV settings.\n", "link": "http://arxiv.org/abs/2502.06288v2", "date": "2025-02-11", "relevancy": 2.3058, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.601}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Ground-to-Aerial%20Image%20Matching%20for%20Visual%20Misinformation%0A%20%20Detection%20Using%20Semantic%20Segmentation&body=Title%3A%20Enhancing%20Ground-to-Aerial%20Image%20Matching%20for%20Visual%20Misinformation%0A%20%20Detection%20Using%20Semantic%20Segmentation%0AAuthor%3A%20Emanuele%20Mule%20and%20Matteo%20Pannacci%20and%20Ali%20Ghasemi%20Goudarzi%20and%20Francesco%20Pro%20and%20Lorenzo%20Papa%20and%20Luca%20Maiano%20and%20Irene%20Amerini%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20generative%20AI%20techniques%2C%20which%20have%20significantly%0Aincreased%20the%20online%20dissemination%20of%20altered%20images%20and%20videos%2C%20have%20raised%0Aserious%20concerns%20about%20the%20credibility%20of%20digital%20media%20available%20on%20the%0AInternet%20and%20distributed%20through%20information%20channels%20and%20social%20networks.%20This%0Aissue%20particularly%20affects%20domains%20that%20rely%20heavily%20on%20trustworthy%20data%2C%20such%0Aas%20journalism%2C%20forensic%20analysis%2C%20and%20Earth%20observation.%20To%20address%20these%0Aconcerns%2C%20the%20ability%20to%20geolocate%20a%20non-geo-tagged%20ground-view%20image%20without%0Aexternal%20information%2C%20such%20as%20GPS%20coordinates%2C%20has%20become%20increasingly%0Acritical.%20This%20study%20tackles%20the%20challenge%20of%20linking%20a%20ground-view%20image%2C%0Apotentially%20exhibiting%20varying%20fields%20of%20view%20%28FoV%29%2C%20to%20its%20corresponding%0Asatellite%20image%20without%20the%20aid%20of%20GPS%20data.%20To%20achieve%20this%2C%20we%20propose%20a%0Anovel%20four-stream%20Siamese-like%20architecture%2C%20the%20Quadruple%20Semantic%20Align%20Net%0A%28SAN-QUAD%29%2C%20which%20extends%20previous%20state-of-the-art%20%28SOTA%29%20approaches%20by%0Aleveraging%20semantic%20segmentation%20applied%20to%20both%20ground%20and%20satellite%20imagery.%0AExperimental%20results%20on%20a%20subset%20of%20the%20CVUSA%20dataset%20demonstrate%20significant%0Aimprovements%20of%20up%20to%209.8%5C%25%20over%20prior%20methods%20across%20various%20FoV%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Ground-to-Aerial%2520Image%2520Matching%2520for%2520Visual%2520Misinformation%250A%2520%2520Detection%2520Using%2520Semantic%2520Segmentation%26entry.906535625%3DEmanuele%2520Mule%2520and%2520Matteo%2520Pannacci%2520and%2520Ali%2520Ghasemi%2520Goudarzi%2520and%2520Francesco%2520Pro%2520and%2520Lorenzo%2520Papa%2520and%2520Luca%2520Maiano%2520and%2520Irene%2520Amerini%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520generative%2520AI%2520techniques%252C%2520which%2520have%2520significantly%250Aincreased%2520the%2520online%2520dissemination%2520of%2520altered%2520images%2520and%2520videos%252C%2520have%2520raised%250Aserious%2520concerns%2520about%2520the%2520credibility%2520of%2520digital%2520media%2520available%2520on%2520the%250AInternet%2520and%2520distributed%2520through%2520information%2520channels%2520and%2520social%2520networks.%2520This%250Aissue%2520particularly%2520affects%2520domains%2520that%2520rely%2520heavily%2520on%2520trustworthy%2520data%252C%2520such%250Aas%2520journalism%252C%2520forensic%2520analysis%252C%2520and%2520Earth%2520observation.%2520To%2520address%2520these%250Aconcerns%252C%2520the%2520ability%2520to%2520geolocate%2520a%2520non-geo-tagged%2520ground-view%2520image%2520without%250Aexternal%2520information%252C%2520such%2520as%2520GPS%2520coordinates%252C%2520has%2520become%2520increasingly%250Acritical.%2520This%2520study%2520tackles%2520the%2520challenge%2520of%2520linking%2520a%2520ground-view%2520image%252C%250Apotentially%2520exhibiting%2520varying%2520fields%2520of%2520view%2520%2528FoV%2529%252C%2520to%2520its%2520corresponding%250Asatellite%2520image%2520without%2520the%2520aid%2520of%2520GPS%2520data.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%250Anovel%2520four-stream%2520Siamese-like%2520architecture%252C%2520the%2520Quadruple%2520Semantic%2520Align%2520Net%250A%2528SAN-QUAD%2529%252C%2520which%2520extends%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520approaches%2520by%250Aleveraging%2520semantic%2520segmentation%2520applied%2520to%2520both%2520ground%2520and%2520satellite%2520imagery.%250AExperimental%2520results%2520on%2520a%2520subset%2520of%2520the%2520CVUSA%2520dataset%2520demonstrate%2520significant%250Aimprovements%2520of%2520up%2520to%25209.8%255C%2525%2520over%2520prior%2520methods%2520across%2520various%2520FoV%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Ground-to-Aerial%20Image%20Matching%20for%20Visual%20Misinformation%0A%20%20Detection%20Using%20Semantic%20Segmentation&entry.906535625=Emanuele%20Mule%20and%20Matteo%20Pannacci%20and%20Ali%20Ghasemi%20Goudarzi%20and%20Francesco%20Pro%20and%20Lorenzo%20Papa%20and%20Luca%20Maiano%20and%20Irene%20Amerini&entry.1292438233=%20%20The%20recent%20advancements%20in%20generative%20AI%20techniques%2C%20which%20have%20significantly%0Aincreased%20the%20online%20dissemination%20of%20altered%20images%20and%20videos%2C%20have%20raised%0Aserious%20concerns%20about%20the%20credibility%20of%20digital%20media%20available%20on%20the%0AInternet%20and%20distributed%20through%20information%20channels%20and%20social%20networks.%20This%0Aissue%20particularly%20affects%20domains%20that%20rely%20heavily%20on%20trustworthy%20data%2C%20such%0Aas%20journalism%2C%20forensic%20analysis%2C%20and%20Earth%20observation.%20To%20address%20these%0Aconcerns%2C%20the%20ability%20to%20geolocate%20a%20non-geo-tagged%20ground-view%20image%20without%0Aexternal%20information%2C%20such%20as%20GPS%20coordinates%2C%20has%20become%20increasingly%0Acritical.%20This%20study%20tackles%20the%20challenge%20of%20linking%20a%20ground-view%20image%2C%0Apotentially%20exhibiting%20varying%20fields%20of%20view%20%28FoV%29%2C%20to%20its%20corresponding%0Asatellite%20image%20without%20the%20aid%20of%20GPS%20data.%20To%20achieve%20this%2C%20we%20propose%20a%0Anovel%20four-stream%20Siamese-like%20architecture%2C%20the%20Quadruple%20Semantic%20Align%20Net%0A%28SAN-QUAD%29%2C%20which%20extends%20previous%20state-of-the-art%20%28SOTA%29%20approaches%20by%0Aleveraging%20semantic%20segmentation%20applied%20to%20both%20ground%20and%20satellite%20imagery.%0AExperimental%20results%20on%20a%20subset%20of%20the%20CVUSA%20dataset%20demonstrate%20significant%0Aimprovements%20of%20up%20to%209.8%5C%25%20over%20prior%20methods%20across%20various%20FoV%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06288v2&entry.124074799=Read"},
{"title": "Neural Networks and (Virtual) Extended Formulations", "author": "Christoph Hertrich and Georg Loho", "abstract": "  Neural networks with piecewise linear activation functions, such as rectified\nlinear units (ReLU) or maxout, are among the most fundamental models in modern\nmachine learning. We make a step towards proving lower bounds on the size of\nsuch neural networks by linking their representative capabilities to the notion\nof the extension complexity $\\mathrm{xc}(P)$ of a polytope $P$. This is a\nwell-studied quantity in combinatorial optimization and polyhedral geometry\ndescribing the number of inequalities needed to model $P$ as a linear program.\nWe show that $\\mathrm{xc}(P)$ is a lower bound on the size of any monotone or\ninput-convex neural network that solves the linear optimization problem over\n$P$. This implies exponential lower bounds on such neural networks for a\nvariety of problems, including the polynomially solvable maximum weight\nmatching problem.\n  In an attempt to prove similar bounds also for general neural networks, we\nintroduce the notion of virtual extension complexity $\\mathrm{vxc}(P)$, which\ngeneralizes $\\mathrm{xc}(P)$ and describes the number of inequalities needed to\nrepresent the linear optimization problem over $P$ as a difference of two\nlinear programs. We prove that $\\mathrm{vxc}(P)$ is a lower bound on the size\nof any neural network that optimizes over $P$. While it remains an open\nquestion to derive useful lower bounds on $\\mathrm{vxc}(P)$, we argue that this\nquantity deserves to be studied independently from neural networks by proving\nthat one can efficiently optimize over a polytope $P$ using a small virtual\nextended formulation.\n", "link": "http://arxiv.org/abs/2411.03006v2", "date": "2025-02-11", "relevancy": 2.2962, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.45}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Networks%20and%20%28Virtual%29%20Extended%20Formulations&body=Title%3A%20Neural%20Networks%20and%20%28Virtual%29%20Extended%20Formulations%0AAuthor%3A%20Christoph%20Hertrich%20and%20Georg%20Loho%0AAbstract%3A%20%20%20Neural%20networks%20with%20piecewise%20linear%20activation%20functions%2C%20such%20as%20rectified%0Alinear%20units%20%28ReLU%29%20or%20maxout%2C%20are%20among%20the%20most%20fundamental%20models%20in%20modern%0Amachine%20learning.%20We%20make%20a%20step%20towards%20proving%20lower%20bounds%20on%20the%20size%20of%0Asuch%20neural%20networks%20by%20linking%20their%20representative%20capabilities%20to%20the%20notion%0Aof%20the%20extension%20complexity%20%24%5Cmathrm%7Bxc%7D%28P%29%24%20of%20a%20polytope%20%24P%24.%20This%20is%20a%0Awell-studied%20quantity%20in%20combinatorial%20optimization%20and%20polyhedral%20geometry%0Adescribing%20the%20number%20of%20inequalities%20needed%20to%20model%20%24P%24%20as%20a%20linear%20program.%0AWe%20show%20that%20%24%5Cmathrm%7Bxc%7D%28P%29%24%20is%20a%20lower%20bound%20on%20the%20size%20of%20any%20monotone%20or%0Ainput-convex%20neural%20network%20that%20solves%20the%20linear%20optimization%20problem%20over%0A%24P%24.%20This%20implies%20exponential%20lower%20bounds%20on%20such%20neural%20networks%20for%20a%0Avariety%20of%20problems%2C%20including%20the%20polynomially%20solvable%20maximum%20weight%0Amatching%20problem.%0A%20%20In%20an%20attempt%20to%20prove%20similar%20bounds%20also%20for%20general%20neural%20networks%2C%20we%0Aintroduce%20the%20notion%20of%20virtual%20extension%20complexity%20%24%5Cmathrm%7Bvxc%7D%28P%29%24%2C%20which%0Ageneralizes%20%24%5Cmathrm%7Bxc%7D%28P%29%24%20and%20describes%20the%20number%20of%20inequalities%20needed%20to%0Arepresent%20the%20linear%20optimization%20problem%20over%20%24P%24%20as%20a%20difference%20of%20two%0Alinear%20programs.%20We%20prove%20that%20%24%5Cmathrm%7Bvxc%7D%28P%29%24%20is%20a%20lower%20bound%20on%20the%20size%0Aof%20any%20neural%20network%20that%20optimizes%20over%20%24P%24.%20While%20it%20remains%20an%20open%0Aquestion%20to%20derive%20useful%20lower%20bounds%20on%20%24%5Cmathrm%7Bvxc%7D%28P%29%24%2C%20we%20argue%20that%20this%0Aquantity%20deserves%20to%20be%20studied%20independently%20from%20neural%20networks%20by%20proving%0Athat%20one%20can%20efficiently%20optimize%20over%20a%20polytope%20%24P%24%20using%20a%20small%20virtual%0Aextended%20formulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Networks%2520and%2520%2528Virtual%2529%2520Extended%2520Formulations%26entry.906535625%3DChristoph%2520Hertrich%2520and%2520Georg%2520Loho%26entry.1292438233%3D%2520%2520Neural%2520networks%2520with%2520piecewise%2520linear%2520activation%2520functions%252C%2520such%2520as%2520rectified%250Alinear%2520units%2520%2528ReLU%2529%2520or%2520maxout%252C%2520are%2520among%2520the%2520most%2520fundamental%2520models%2520in%2520modern%250Amachine%2520learning.%2520We%2520make%2520a%2520step%2520towards%2520proving%2520lower%2520bounds%2520on%2520the%2520size%2520of%250Asuch%2520neural%2520networks%2520by%2520linking%2520their%2520representative%2520capabilities%2520to%2520the%2520notion%250Aof%2520the%2520extension%2520complexity%2520%2524%255Cmathrm%257Bxc%257D%2528P%2529%2524%2520of%2520a%2520polytope%2520%2524P%2524.%2520This%2520is%2520a%250Awell-studied%2520quantity%2520in%2520combinatorial%2520optimization%2520and%2520polyhedral%2520geometry%250Adescribing%2520the%2520number%2520of%2520inequalities%2520needed%2520to%2520model%2520%2524P%2524%2520as%2520a%2520linear%2520program.%250AWe%2520show%2520that%2520%2524%255Cmathrm%257Bxc%257D%2528P%2529%2524%2520is%2520a%2520lower%2520bound%2520on%2520the%2520size%2520of%2520any%2520monotone%2520or%250Ainput-convex%2520neural%2520network%2520that%2520solves%2520the%2520linear%2520optimization%2520problem%2520over%250A%2524P%2524.%2520This%2520implies%2520exponential%2520lower%2520bounds%2520on%2520such%2520neural%2520networks%2520for%2520a%250Avariety%2520of%2520problems%252C%2520including%2520the%2520polynomially%2520solvable%2520maximum%2520weight%250Amatching%2520problem.%250A%2520%2520In%2520an%2520attempt%2520to%2520prove%2520similar%2520bounds%2520also%2520for%2520general%2520neural%2520networks%252C%2520we%250Aintroduce%2520the%2520notion%2520of%2520virtual%2520extension%2520complexity%2520%2524%255Cmathrm%257Bvxc%257D%2528P%2529%2524%252C%2520which%250Ageneralizes%2520%2524%255Cmathrm%257Bxc%257D%2528P%2529%2524%2520and%2520describes%2520the%2520number%2520of%2520inequalities%2520needed%2520to%250Arepresent%2520the%2520linear%2520optimization%2520problem%2520over%2520%2524P%2524%2520as%2520a%2520difference%2520of%2520two%250Alinear%2520programs.%2520We%2520prove%2520that%2520%2524%255Cmathrm%257Bvxc%257D%2528P%2529%2524%2520is%2520a%2520lower%2520bound%2520on%2520the%2520size%250Aof%2520any%2520neural%2520network%2520that%2520optimizes%2520over%2520%2524P%2524.%2520While%2520it%2520remains%2520an%2520open%250Aquestion%2520to%2520derive%2520useful%2520lower%2520bounds%2520on%2520%2524%255Cmathrm%257Bvxc%257D%2528P%2529%2524%252C%2520we%2520argue%2520that%2520this%250Aquantity%2520deserves%2520to%2520be%2520studied%2520independently%2520from%2520neural%2520networks%2520by%2520proving%250Athat%2520one%2520can%2520efficiently%2520optimize%2520over%2520a%2520polytope%2520%2524P%2524%2520using%2520a%2520small%2520virtual%250Aextended%2520formulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20and%20%28Virtual%29%20Extended%20Formulations&entry.906535625=Christoph%20Hertrich%20and%20Georg%20Loho&entry.1292438233=%20%20Neural%20networks%20with%20piecewise%20linear%20activation%20functions%2C%20such%20as%20rectified%0Alinear%20units%20%28ReLU%29%20or%20maxout%2C%20are%20among%20the%20most%20fundamental%20models%20in%20modern%0Amachine%20learning.%20We%20make%20a%20step%20towards%20proving%20lower%20bounds%20on%20the%20size%20of%0Asuch%20neural%20networks%20by%20linking%20their%20representative%20capabilities%20to%20the%20notion%0Aof%20the%20extension%20complexity%20%24%5Cmathrm%7Bxc%7D%28P%29%24%20of%20a%20polytope%20%24P%24.%20This%20is%20a%0Awell-studied%20quantity%20in%20combinatorial%20optimization%20and%20polyhedral%20geometry%0Adescribing%20the%20number%20of%20inequalities%20needed%20to%20model%20%24P%24%20as%20a%20linear%20program.%0AWe%20show%20that%20%24%5Cmathrm%7Bxc%7D%28P%29%24%20is%20a%20lower%20bound%20on%20the%20size%20of%20any%20monotone%20or%0Ainput-convex%20neural%20network%20that%20solves%20the%20linear%20optimization%20problem%20over%0A%24P%24.%20This%20implies%20exponential%20lower%20bounds%20on%20such%20neural%20networks%20for%20a%0Avariety%20of%20problems%2C%20including%20the%20polynomially%20solvable%20maximum%20weight%0Amatching%20problem.%0A%20%20In%20an%20attempt%20to%20prove%20similar%20bounds%20also%20for%20general%20neural%20networks%2C%20we%0Aintroduce%20the%20notion%20of%20virtual%20extension%20complexity%20%24%5Cmathrm%7Bvxc%7D%28P%29%24%2C%20which%0Ageneralizes%20%24%5Cmathrm%7Bxc%7D%28P%29%24%20and%20describes%20the%20number%20of%20inequalities%20needed%20to%0Arepresent%20the%20linear%20optimization%20problem%20over%20%24P%24%20as%20a%20difference%20of%20two%0Alinear%20programs.%20We%20prove%20that%20%24%5Cmathrm%7Bvxc%7D%28P%29%24%20is%20a%20lower%20bound%20on%20the%20size%0Aof%20any%20neural%20network%20that%20optimizes%20over%20%24P%24.%20While%20it%20remains%20an%20open%0Aquestion%20to%20derive%20useful%20lower%20bounds%20on%20%24%5Cmathrm%7Bvxc%7D%28P%29%24%2C%20we%20argue%20that%20this%0Aquantity%20deserves%20to%20be%20studied%20independently%20from%20neural%20networks%20by%20proving%0Athat%20one%20can%20efficiently%20optimize%20over%20a%20polytope%20%24P%24%20using%20a%20small%20virtual%0Aextended%20formulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03006v2&entry.124074799=Read"},
{"title": "An Elliptic Curve Based Solution to the Perspective-Three-Point Problem", "author": "Michael Q. Rieck", "abstract": "  The Perspective-Three-Point Problem (P3P) is solved by first focusing on\ndetermining the directions of the lines through pairs of control points,\nrelative to the camera, rather than the distances from the camera to the\ncontrol points. The analysis of this produces an efficient, accurate and\nreasonably simple P3P solver, which is compared with a state-of-the-art P3P\nsolver, \"Lambda Twist.\" Both methods depend on the accurate computation of a\nsingle root of a cubic polynomial. They have been implemented and tested for a\nwide range of control-point triangles, and under certain reasonable\nrestrictions, the new method is noticably more accurate than Lambda Twist,\nthough it is slower. However, the principal value of the present work is not in\nintroducing yet another P3P solver, but lies rather in the discovery of an\nintimate connection between the P3P problem and a special family of elliptic\ncurves that includes curves utilized in cryptography. This holds the potential\nfor further advances in a number of directions. To make this connection, an\ninteresting spherical analogue of an ancient \"sliding\" problem is stated and\nsolved.\n", "link": "http://arxiv.org/abs/2502.07564v1", "date": "2025-02-11", "relevancy": 2.2906, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4617}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4563}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Elliptic%20Curve%20Based%20Solution%20to%20the%20Perspective-Three-Point%20Problem&body=Title%3A%20An%20Elliptic%20Curve%20Based%20Solution%20to%20the%20Perspective-Three-Point%20Problem%0AAuthor%3A%20Michael%20Q.%20Rieck%0AAbstract%3A%20%20%20The%20Perspective-Three-Point%20Problem%20%28P3P%29%20is%20solved%20by%20first%20focusing%20on%0Adetermining%20the%20directions%20of%20the%20lines%20through%20pairs%20of%20control%20points%2C%0Arelative%20to%20the%20camera%2C%20rather%20than%20the%20distances%20from%20the%20camera%20to%20the%0Acontrol%20points.%20The%20analysis%20of%20this%20produces%20an%20efficient%2C%20accurate%20and%0Areasonably%20simple%20P3P%20solver%2C%20which%20is%20compared%20with%20a%20state-of-the-art%20P3P%0Asolver%2C%20%22Lambda%20Twist.%22%20Both%20methods%20depend%20on%20the%20accurate%20computation%20of%20a%0Asingle%20root%20of%20a%20cubic%20polynomial.%20They%20have%20been%20implemented%20and%20tested%20for%20a%0Awide%20range%20of%20control-point%20triangles%2C%20and%20under%20certain%20reasonable%0Arestrictions%2C%20the%20new%20method%20is%20noticably%20more%20accurate%20than%20Lambda%20Twist%2C%0Athough%20it%20is%20slower.%20However%2C%20the%20principal%20value%20of%20the%20present%20work%20is%20not%20in%0Aintroducing%20yet%20another%20P3P%20solver%2C%20but%20lies%20rather%20in%20the%20discovery%20of%20an%0Aintimate%20connection%20between%20the%20P3P%20problem%20and%20a%20special%20family%20of%20elliptic%0Acurves%20that%20includes%20curves%20utilized%20in%20cryptography.%20This%20holds%20the%20potential%0Afor%20further%20advances%20in%20a%20number%20of%20directions.%20To%20make%20this%20connection%2C%20an%0Ainteresting%20spherical%20analogue%20of%20an%20ancient%20%22sliding%22%20problem%20is%20stated%20and%0Asolved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Elliptic%2520Curve%2520Based%2520Solution%2520to%2520the%2520Perspective-Three-Point%2520Problem%26entry.906535625%3DMichael%2520Q.%2520Rieck%26entry.1292438233%3D%2520%2520The%2520Perspective-Three-Point%2520Problem%2520%2528P3P%2529%2520is%2520solved%2520by%2520first%2520focusing%2520on%250Adetermining%2520the%2520directions%2520of%2520the%2520lines%2520through%2520pairs%2520of%2520control%2520points%252C%250Arelative%2520to%2520the%2520camera%252C%2520rather%2520than%2520the%2520distances%2520from%2520the%2520camera%2520to%2520the%250Acontrol%2520points.%2520The%2520analysis%2520of%2520this%2520produces%2520an%2520efficient%252C%2520accurate%2520and%250Areasonably%2520simple%2520P3P%2520solver%252C%2520which%2520is%2520compared%2520with%2520a%2520state-of-the-art%2520P3P%250Asolver%252C%2520%2522Lambda%2520Twist.%2522%2520Both%2520methods%2520depend%2520on%2520the%2520accurate%2520computation%2520of%2520a%250Asingle%2520root%2520of%2520a%2520cubic%2520polynomial.%2520They%2520have%2520been%2520implemented%2520and%2520tested%2520for%2520a%250Awide%2520range%2520of%2520control-point%2520triangles%252C%2520and%2520under%2520certain%2520reasonable%250Arestrictions%252C%2520the%2520new%2520method%2520is%2520noticably%2520more%2520accurate%2520than%2520Lambda%2520Twist%252C%250Athough%2520it%2520is%2520slower.%2520However%252C%2520the%2520principal%2520value%2520of%2520the%2520present%2520work%2520is%2520not%2520in%250Aintroducing%2520yet%2520another%2520P3P%2520solver%252C%2520but%2520lies%2520rather%2520in%2520the%2520discovery%2520of%2520an%250Aintimate%2520connection%2520between%2520the%2520P3P%2520problem%2520and%2520a%2520special%2520family%2520of%2520elliptic%250Acurves%2520that%2520includes%2520curves%2520utilized%2520in%2520cryptography.%2520This%2520holds%2520the%2520potential%250Afor%2520further%2520advances%2520in%2520a%2520number%2520of%2520directions.%2520To%2520make%2520this%2520connection%252C%2520an%250Ainteresting%2520spherical%2520analogue%2520of%2520an%2520ancient%2520%2522sliding%2522%2520problem%2520is%2520stated%2520and%250Asolved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Elliptic%20Curve%20Based%20Solution%20to%20the%20Perspective-Three-Point%20Problem&entry.906535625=Michael%20Q.%20Rieck&entry.1292438233=%20%20The%20Perspective-Three-Point%20Problem%20%28P3P%29%20is%20solved%20by%20first%20focusing%20on%0Adetermining%20the%20directions%20of%20the%20lines%20through%20pairs%20of%20control%20points%2C%0Arelative%20to%20the%20camera%2C%20rather%20than%20the%20distances%20from%20the%20camera%20to%20the%0Acontrol%20points.%20The%20analysis%20of%20this%20produces%20an%20efficient%2C%20accurate%20and%0Areasonably%20simple%20P3P%20solver%2C%20which%20is%20compared%20with%20a%20state-of-the-art%20P3P%0Asolver%2C%20%22Lambda%20Twist.%22%20Both%20methods%20depend%20on%20the%20accurate%20computation%20of%20a%0Asingle%20root%20of%20a%20cubic%20polynomial.%20They%20have%20been%20implemented%20and%20tested%20for%20a%0Awide%20range%20of%20control-point%20triangles%2C%20and%20under%20certain%20reasonable%0Arestrictions%2C%20the%20new%20method%20is%20noticably%20more%20accurate%20than%20Lambda%20Twist%2C%0Athough%20it%20is%20slower.%20However%2C%20the%20principal%20value%20of%20the%20present%20work%20is%20not%20in%0Aintroducing%20yet%20another%20P3P%20solver%2C%20but%20lies%20rather%20in%20the%20discovery%20of%20an%0Aintimate%20connection%20between%20the%20P3P%20problem%20and%20a%20special%20family%20of%20elliptic%0Acurves%20that%20includes%20curves%20utilized%20in%20cryptography.%20This%20holds%20the%20potential%0Afor%20further%20advances%20in%20a%20number%20of%20directions.%20To%20make%20this%20connection%2C%20an%0Ainteresting%20spherical%20analogue%20of%20an%20ancient%20%22sliding%22%20problem%20is%20stated%20and%0Asolved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07564v1&entry.124074799=Read"},
{"title": "Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head\n  Attention without Alignment Barriers", "author": "Zhaodong Bing and Linze Li and Jiajun Liang", "abstract": "  Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance.\n", "link": "http://arxiv.org/abs/2502.07436v1", "date": "2025-02-11", "relevancy": 2.2879, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5672}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Knowledge%20Distillation%20in%20Transformers%3A%20Enabling%20Multi-Head%0A%20%20Attention%20without%20Alignment%20Barriers&body=Title%3A%20Optimizing%20Knowledge%20Distillation%20in%20Transformers%3A%20Enabling%20Multi-Head%0A%20%20Attention%20without%20Alignment%20Barriers%0AAuthor%3A%20Zhaodong%20Bing%20and%20Linze%20Li%20and%20Jiajun%20Liang%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20in%20transformers%20often%20faces%20challenges%20due%20to%0Amisalignment%20in%20the%20number%20of%20attention%20heads%20between%20teacher%20and%20student%0Amodels.%20Existing%20methods%20either%20require%20identical%20head%20counts%20or%20introduce%0Aprojectors%20to%20bridge%20dimensional%20gaps%2C%20limiting%20flexibility%20and%20efficiency.%20We%0Apropose%20Squeezing-Heads%20Distillation%20%28SHD%29%2C%20a%20novel%20approach%20that%20enables%0Aseamless%20knowledge%20transfer%20between%20models%20with%20varying%20head%20counts%20by%0Acompressing%20multi-head%20attention%20maps%20via%20efficient%20linear%20approximation.%0AUnlike%20prior%20work%2C%20SHD%20eliminates%20alignment%20barriers%20without%20additional%0Aparameters%20or%20architectural%20modifications.%20Our%20method%20dynamically%20approximates%0Athe%20combined%20effect%20of%20multiple%20teacher%20heads%20into%20fewer%20student%20heads%2C%0Apreserving%20fine-grained%20attention%20patterns%20while%20reducing%20redundancy.%0AExperiments%20across%20language%20%28LLaMA%2C%20GPT%29%20and%20vision%20%28DiT%2C%20MDT%29%20generative%20and%0Avision%20%28DeiT%29%20discriminative%20tasks%20demonstrate%20SHD%27s%20effectiveness%3A%20it%0Aoutperforms%20logit-based%20and%20feature-alignment%20KD%20baselines%2C%20achieving%0Astate-of-the-art%20results%20in%20image%20classification%2C%20image%20generation%20language%0Afine-tuning%2C%20and%20language%20pre-training.%20The%20key%20innovations%20of%20flexible%20head%0Acompression%2C%20projector-free%20design%2C%20and%20linear-time%20complexity%20make%20SHD%20a%0Aversatile%20and%20scalable%20solution%20for%20distilling%20modern%20transformers.%20This%20work%0Abridges%20a%20critical%20gap%20in%20KD%2C%20enabling%20efficient%20deployment%20of%20compact%20models%0Awithout%20compromising%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Knowledge%2520Distillation%2520in%2520Transformers%253A%2520Enabling%2520Multi-Head%250A%2520%2520Attention%2520without%2520Alignment%2520Barriers%26entry.906535625%3DZhaodong%2520Bing%2520and%2520Linze%2520Li%2520and%2520Jiajun%2520Liang%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520in%2520transformers%2520often%2520faces%2520challenges%2520due%2520to%250Amisalignment%2520in%2520the%2520number%2520of%2520attention%2520heads%2520between%2520teacher%2520and%2520student%250Amodels.%2520Existing%2520methods%2520either%2520require%2520identical%2520head%2520counts%2520or%2520introduce%250Aprojectors%2520to%2520bridge%2520dimensional%2520gaps%252C%2520limiting%2520flexibility%2520and%2520efficiency.%2520We%250Apropose%2520Squeezing-Heads%2520Distillation%2520%2528SHD%2529%252C%2520a%2520novel%2520approach%2520that%2520enables%250Aseamless%2520knowledge%2520transfer%2520between%2520models%2520with%2520varying%2520head%2520counts%2520by%250Acompressing%2520multi-head%2520attention%2520maps%2520via%2520efficient%2520linear%2520approximation.%250AUnlike%2520prior%2520work%252C%2520SHD%2520eliminates%2520alignment%2520barriers%2520without%2520additional%250Aparameters%2520or%2520architectural%2520modifications.%2520Our%2520method%2520dynamically%2520approximates%250Athe%2520combined%2520effect%2520of%2520multiple%2520teacher%2520heads%2520into%2520fewer%2520student%2520heads%252C%250Apreserving%2520fine-grained%2520attention%2520patterns%2520while%2520reducing%2520redundancy.%250AExperiments%2520across%2520language%2520%2528LLaMA%252C%2520GPT%2529%2520and%2520vision%2520%2528DiT%252C%2520MDT%2529%2520generative%2520and%250Avision%2520%2528DeiT%2529%2520discriminative%2520tasks%2520demonstrate%2520SHD%2527s%2520effectiveness%253A%2520it%250Aoutperforms%2520logit-based%2520and%2520feature-alignment%2520KD%2520baselines%252C%2520achieving%250Astate-of-the-art%2520results%2520in%2520image%2520classification%252C%2520image%2520generation%2520language%250Afine-tuning%252C%2520and%2520language%2520pre-training.%2520The%2520key%2520innovations%2520of%2520flexible%2520head%250Acompression%252C%2520projector-free%2520design%252C%2520and%2520linear-time%2520complexity%2520make%2520SHD%2520a%250Aversatile%2520and%2520scalable%2520solution%2520for%2520distilling%2520modern%2520transformers.%2520This%2520work%250Abridges%2520a%2520critical%2520gap%2520in%2520KD%252C%2520enabling%2520efficient%2520deployment%2520of%2520compact%2520models%250Awithout%2520compromising%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Knowledge%20Distillation%20in%20Transformers%3A%20Enabling%20Multi-Head%0A%20%20Attention%20without%20Alignment%20Barriers&entry.906535625=Zhaodong%20Bing%20and%20Linze%20Li%20and%20Jiajun%20Liang&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20in%20transformers%20often%20faces%20challenges%20due%20to%0Amisalignment%20in%20the%20number%20of%20attention%20heads%20between%20teacher%20and%20student%0Amodels.%20Existing%20methods%20either%20require%20identical%20head%20counts%20or%20introduce%0Aprojectors%20to%20bridge%20dimensional%20gaps%2C%20limiting%20flexibility%20and%20efficiency.%20We%0Apropose%20Squeezing-Heads%20Distillation%20%28SHD%29%2C%20a%20novel%20approach%20that%20enables%0Aseamless%20knowledge%20transfer%20between%20models%20with%20varying%20head%20counts%20by%0Acompressing%20multi-head%20attention%20maps%20via%20efficient%20linear%20approximation.%0AUnlike%20prior%20work%2C%20SHD%20eliminates%20alignment%20barriers%20without%20additional%0Aparameters%20or%20architectural%20modifications.%20Our%20method%20dynamically%20approximates%0Athe%20combined%20effect%20of%20multiple%20teacher%20heads%20into%20fewer%20student%20heads%2C%0Apreserving%20fine-grained%20attention%20patterns%20while%20reducing%20redundancy.%0AExperiments%20across%20language%20%28LLaMA%2C%20GPT%29%20and%20vision%20%28DiT%2C%20MDT%29%20generative%20and%0Avision%20%28DeiT%29%20discriminative%20tasks%20demonstrate%20SHD%27s%20effectiveness%3A%20it%0Aoutperforms%20logit-based%20and%20feature-alignment%20KD%20baselines%2C%20achieving%0Astate-of-the-art%20results%20in%20image%20classification%2C%20image%20generation%20language%0Afine-tuning%2C%20and%20language%20pre-training.%20The%20key%20innovations%20of%20flexible%20head%0Acompression%2C%20projector-free%20design%2C%20and%20linear-time%20complexity%20make%20SHD%20a%0Aversatile%20and%20scalable%20solution%20for%20distilling%20modern%20transformers.%20This%20work%0Abridges%20a%20critical%20gap%20in%20KD%2C%20enabling%20efficient%20deployment%20of%20compact%20models%0Awithout%20compromising%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07436v1&entry.124074799=Read"},
{"title": "MatSwap: Light-aware material transfers in images", "author": "Ivan Lopes and Valentin Deschaintre and Yannick Hold-Geoffroy and Raoul de Charette", "abstract": "  We present MatSwap, a method to transfer materials to designated surfaces in\nan image photorealistically. Such a task is non-trivial due to the large\nentanglement of material appearance, geometry, and lighting in a photograph. In\nthe literature, material editing methods typically rely on either cumbersome\ntext engineering or extensive manual annotations requiring artist knowledge and\n3D scene properties that are impractical to obtain. In contrast, we propose to\ndirectly learn the relationship between the input material -- as observed on a\nflat surface -- and its appearance within the scene, without the need for\nexplicit UV mapping. To achieve this, we rely on a custom light- and\ngeometry-aware diffusion model. We fine-tune a large-scale pre-trained\ntext-to-image model for material transfer using our synthetic dataset,\npreserving its strong priors to ensure effective generalization to real images.\nAs a result, our method seamlessly integrates a desired material into the\ntarget location in the photograph while retaining the identity of the scene. We\nevaluate our method on synthetic and real images and show that it compares\nfavorably to recent work both qualitatively and quantitatively. We will release\nour code and data upon publication.\n", "link": "http://arxiv.org/abs/2502.07784v1", "date": "2025-02-11", "relevancy": 2.2845, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6319}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5339}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatSwap%3A%20Light-aware%20material%20transfers%20in%20images&body=Title%3A%20MatSwap%3A%20Light-aware%20material%20transfers%20in%20images%0AAuthor%3A%20Ivan%20Lopes%20and%20Valentin%20Deschaintre%20and%20Yannick%20Hold-Geoffroy%20and%20Raoul%20de%20Charette%0AAbstract%3A%20%20%20We%20present%20MatSwap%2C%20a%20method%20to%20transfer%20materials%20to%20designated%20surfaces%20in%0Aan%20image%20photorealistically.%20Such%20a%20task%20is%20non-trivial%20due%20to%20the%20large%0Aentanglement%20of%20material%20appearance%2C%20geometry%2C%20and%20lighting%20in%20a%20photograph.%20In%0Athe%20literature%2C%20material%20editing%20methods%20typically%20rely%20on%20either%20cumbersome%0Atext%20engineering%20or%20extensive%20manual%20annotations%20requiring%20artist%20knowledge%20and%0A3D%20scene%20properties%20that%20are%20impractical%20to%20obtain.%20In%20contrast%2C%20we%20propose%20to%0Adirectly%20learn%20the%20relationship%20between%20the%20input%20material%20--%20as%20observed%20on%20a%0Aflat%20surface%20--%20and%20its%20appearance%20within%20the%20scene%2C%20without%20the%20need%20for%0Aexplicit%20UV%20mapping.%20To%20achieve%20this%2C%20we%20rely%20on%20a%20custom%20light-%20and%0Ageometry-aware%20diffusion%20model.%20We%20fine-tune%20a%20large-scale%20pre-trained%0Atext-to-image%20model%20for%20material%20transfer%20using%20our%20synthetic%20dataset%2C%0Apreserving%20its%20strong%20priors%20to%20ensure%20effective%20generalization%20to%20real%20images.%0AAs%20a%20result%2C%20our%20method%20seamlessly%20integrates%20a%20desired%20material%20into%20the%0Atarget%20location%20in%20the%20photograph%20while%20retaining%20the%20identity%20of%20the%20scene.%20We%0Aevaluate%20our%20method%20on%20synthetic%20and%20real%20images%20and%20show%20that%20it%20compares%0Afavorably%20to%20recent%20work%20both%20qualitatively%20and%20quantitatively.%20We%20will%20release%0Aour%20code%20and%20data%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatSwap%253A%2520Light-aware%2520material%2520transfers%2520in%2520images%26entry.906535625%3DIvan%2520Lopes%2520and%2520Valentin%2520Deschaintre%2520and%2520Yannick%2520Hold-Geoffroy%2520and%2520Raoul%2520de%2520Charette%26entry.1292438233%3D%2520%2520We%2520present%2520MatSwap%252C%2520a%2520method%2520to%2520transfer%2520materials%2520to%2520designated%2520surfaces%2520in%250Aan%2520image%2520photorealistically.%2520Such%2520a%2520task%2520is%2520non-trivial%2520due%2520to%2520the%2520large%250Aentanglement%2520of%2520material%2520appearance%252C%2520geometry%252C%2520and%2520lighting%2520in%2520a%2520photograph.%2520In%250Athe%2520literature%252C%2520material%2520editing%2520methods%2520typically%2520rely%2520on%2520either%2520cumbersome%250Atext%2520engineering%2520or%2520extensive%2520manual%2520annotations%2520requiring%2520artist%2520knowledge%2520and%250A3D%2520scene%2520properties%2520that%2520are%2520impractical%2520to%2520obtain.%2520In%2520contrast%252C%2520we%2520propose%2520to%250Adirectly%2520learn%2520the%2520relationship%2520between%2520the%2520input%2520material%2520--%2520as%2520observed%2520on%2520a%250Aflat%2520surface%2520--%2520and%2520its%2520appearance%2520within%2520the%2520scene%252C%2520without%2520the%2520need%2520for%250Aexplicit%2520UV%2520mapping.%2520To%2520achieve%2520this%252C%2520we%2520rely%2520on%2520a%2520custom%2520light-%2520and%250Ageometry-aware%2520diffusion%2520model.%2520We%2520fine-tune%2520a%2520large-scale%2520pre-trained%250Atext-to-image%2520model%2520for%2520material%2520transfer%2520using%2520our%2520synthetic%2520dataset%252C%250Apreserving%2520its%2520strong%2520priors%2520to%2520ensure%2520effective%2520generalization%2520to%2520real%2520images.%250AAs%2520a%2520result%252C%2520our%2520method%2520seamlessly%2520integrates%2520a%2520desired%2520material%2520into%2520the%250Atarget%2520location%2520in%2520the%2520photograph%2520while%2520retaining%2520the%2520identity%2520of%2520the%2520scene.%2520We%250Aevaluate%2520our%2520method%2520on%2520synthetic%2520and%2520real%2520images%2520and%2520show%2520that%2520it%2520compares%250Afavorably%2520to%2520recent%2520work%2520both%2520qualitatively%2520and%2520quantitatively.%2520We%2520will%2520release%250Aour%2520code%2520and%2520data%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatSwap%3A%20Light-aware%20material%20transfers%20in%20images&entry.906535625=Ivan%20Lopes%20and%20Valentin%20Deschaintre%20and%20Yannick%20Hold-Geoffroy%20and%20Raoul%20de%20Charette&entry.1292438233=%20%20We%20present%20MatSwap%2C%20a%20method%20to%20transfer%20materials%20to%20designated%20surfaces%20in%0Aan%20image%20photorealistically.%20Such%20a%20task%20is%20non-trivial%20due%20to%20the%20large%0Aentanglement%20of%20material%20appearance%2C%20geometry%2C%20and%20lighting%20in%20a%20photograph.%20In%0Athe%20literature%2C%20material%20editing%20methods%20typically%20rely%20on%20either%20cumbersome%0Atext%20engineering%20or%20extensive%20manual%20annotations%20requiring%20artist%20knowledge%20and%0A3D%20scene%20properties%20that%20are%20impractical%20to%20obtain.%20In%20contrast%2C%20we%20propose%20to%0Adirectly%20learn%20the%20relationship%20between%20the%20input%20material%20--%20as%20observed%20on%20a%0Aflat%20surface%20--%20and%20its%20appearance%20within%20the%20scene%2C%20without%20the%20need%20for%0Aexplicit%20UV%20mapping.%20To%20achieve%20this%2C%20we%20rely%20on%20a%20custom%20light-%20and%0Ageometry-aware%20diffusion%20model.%20We%20fine-tune%20a%20large-scale%20pre-trained%0Atext-to-image%20model%20for%20material%20transfer%20using%20our%20synthetic%20dataset%2C%0Apreserving%20its%20strong%20priors%20to%20ensure%20effective%20generalization%20to%20real%20images.%0AAs%20a%20result%2C%20our%20method%20seamlessly%20integrates%20a%20desired%20material%20into%20the%0Atarget%20location%20in%20the%20photograph%20while%20retaining%20the%20identity%20of%20the%20scene.%20We%0Aevaluate%20our%20method%20on%20synthetic%20and%20real%20images%20and%20show%20that%20it%20compares%0Afavorably%20to%20recent%20work%20both%20qualitatively%20and%20quantitatively.%20We%20will%20release%0Aour%20code%20and%20data%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07784v1&entry.124074799=Read"},
{"title": "Scaling Pre-training to One Hundred Billion Data for Vision Language\n  Models", "author": "Xiao Wang and Ibrahim Alabdulmohsin and Daniel Salz and Zhe Li and Keran Rong and Xiaohua Zhai", "abstract": "  We provide an empirical investigation of the potential of pre-training\nvision-language models on an unprecedented scale: 100 billion examples. We find\nthat model performance tends to saturate at this scale on many common\nWestern-centric classification and retrieval benchmarks, such as COCO Captions.\nNevertheless, tasks of cultural diversity achieve more substantial gains from\nthe 100-billion scale web data, thanks to its coverage of long-tail concepts.\nFurthermore, we analyze the model's multilinguality and show gains in\nlow-resource languages as well. In addition, we observe that reducing the size\nof the pretraining dataset via quality filters like using CLIP, typically used\nto enhance performance, may inadvertently reduce the cultural diversity\nrepresented even in large-scale datasets. Our results highlight that while\ntraditional benchmarks may not benefit significantly from scaling noisy, raw\nweb data to 100 billion examples, this data scale is vital for building truly\ninclusive multimodal systems.\n", "link": "http://arxiv.org/abs/2502.07617v1", "date": "2025-02-11", "relevancy": 2.2806, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Pre-training%20to%20One%20Hundred%20Billion%20Data%20for%20Vision%20Language%0A%20%20Models&body=Title%3A%20Scaling%20Pre-training%20to%20One%20Hundred%20Billion%20Data%20for%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Xiao%20Wang%20and%20Ibrahim%20Alabdulmohsin%20and%20Daniel%20Salz%20and%20Zhe%20Li%20and%20Keran%20Rong%20and%20Xiaohua%20Zhai%0AAbstract%3A%20%20%20We%20provide%20an%20empirical%20investigation%20of%20the%20potential%20of%20pre-training%0Avision-language%20models%20on%20an%20unprecedented%20scale%3A%20100%20billion%20examples.%20We%20find%0Athat%20model%20performance%20tends%20to%20saturate%20at%20this%20scale%20on%20many%20common%0AWestern-centric%20classification%20and%20retrieval%20benchmarks%2C%20such%20as%20COCO%20Captions.%0ANevertheless%2C%20tasks%20of%20cultural%20diversity%20achieve%20more%20substantial%20gains%20from%0Athe%20100-billion%20scale%20web%20data%2C%20thanks%20to%20its%20coverage%20of%20long-tail%20concepts.%0AFurthermore%2C%20we%20analyze%20the%20model%27s%20multilinguality%20and%20show%20gains%20in%0Alow-resource%20languages%20as%20well.%20In%20addition%2C%20we%20observe%20that%20reducing%20the%20size%0Aof%20the%20pretraining%20dataset%20via%20quality%20filters%20like%20using%20CLIP%2C%20typically%20used%0Ato%20enhance%20performance%2C%20may%20inadvertently%20reduce%20the%20cultural%20diversity%0Arepresented%20even%20in%20large-scale%20datasets.%20Our%20results%20highlight%20that%20while%0Atraditional%20benchmarks%20may%20not%20benefit%20significantly%20from%20scaling%20noisy%2C%20raw%0Aweb%20data%20to%20100%20billion%20examples%2C%20this%20data%20scale%20is%20vital%20for%20building%20truly%0Ainclusive%20multimodal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Pre-training%2520to%2520One%2520Hundred%2520Billion%2520Data%2520for%2520Vision%2520Language%250A%2520%2520Models%26entry.906535625%3DXiao%2520Wang%2520and%2520Ibrahim%2520Alabdulmohsin%2520and%2520Daniel%2520Salz%2520and%2520Zhe%2520Li%2520and%2520Keran%2520Rong%2520and%2520Xiaohua%2520Zhai%26entry.1292438233%3D%2520%2520We%2520provide%2520an%2520empirical%2520investigation%2520of%2520the%2520potential%2520of%2520pre-training%250Avision-language%2520models%2520on%2520an%2520unprecedented%2520scale%253A%2520100%2520billion%2520examples.%2520We%2520find%250Athat%2520model%2520performance%2520tends%2520to%2520saturate%2520at%2520this%2520scale%2520on%2520many%2520common%250AWestern-centric%2520classification%2520and%2520retrieval%2520benchmarks%252C%2520such%2520as%2520COCO%2520Captions.%250ANevertheless%252C%2520tasks%2520of%2520cultural%2520diversity%2520achieve%2520more%2520substantial%2520gains%2520from%250Athe%2520100-billion%2520scale%2520web%2520data%252C%2520thanks%2520to%2520its%2520coverage%2520of%2520long-tail%2520concepts.%250AFurthermore%252C%2520we%2520analyze%2520the%2520model%2527s%2520multilinguality%2520and%2520show%2520gains%2520in%250Alow-resource%2520languages%2520as%2520well.%2520In%2520addition%252C%2520we%2520observe%2520that%2520reducing%2520the%2520size%250Aof%2520the%2520pretraining%2520dataset%2520via%2520quality%2520filters%2520like%2520using%2520CLIP%252C%2520typically%2520used%250Ato%2520enhance%2520performance%252C%2520may%2520inadvertently%2520reduce%2520the%2520cultural%2520diversity%250Arepresented%2520even%2520in%2520large-scale%2520datasets.%2520Our%2520results%2520highlight%2520that%2520while%250Atraditional%2520benchmarks%2520may%2520not%2520benefit%2520significantly%2520from%2520scaling%2520noisy%252C%2520raw%250Aweb%2520data%2520to%2520100%2520billion%2520examples%252C%2520this%2520data%2520scale%2520is%2520vital%2520for%2520building%2520truly%250Ainclusive%2520multimodal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Pre-training%20to%20One%20Hundred%20Billion%20Data%20for%20Vision%20Language%0A%20%20Models&entry.906535625=Xiao%20Wang%20and%20Ibrahim%20Alabdulmohsin%20and%20Daniel%20Salz%20and%20Zhe%20Li%20and%20Keran%20Rong%20and%20Xiaohua%20Zhai&entry.1292438233=%20%20We%20provide%20an%20empirical%20investigation%20of%20the%20potential%20of%20pre-training%0Avision-language%20models%20on%20an%20unprecedented%20scale%3A%20100%20billion%20examples.%20We%20find%0Athat%20model%20performance%20tends%20to%20saturate%20at%20this%20scale%20on%20many%20common%0AWestern-centric%20classification%20and%20retrieval%20benchmarks%2C%20such%20as%20COCO%20Captions.%0ANevertheless%2C%20tasks%20of%20cultural%20diversity%20achieve%20more%20substantial%20gains%20from%0Athe%20100-billion%20scale%20web%20data%2C%20thanks%20to%20its%20coverage%20of%20long-tail%20concepts.%0AFurthermore%2C%20we%20analyze%20the%20model%27s%20multilinguality%20and%20show%20gains%20in%0Alow-resource%20languages%20as%20well.%20In%20addition%2C%20we%20observe%20that%20reducing%20the%20size%0Aof%20the%20pretraining%20dataset%20via%20quality%20filters%20like%20using%20CLIP%2C%20typically%20used%0Ato%20enhance%20performance%2C%20may%20inadvertently%20reduce%20the%20cultural%20diversity%0Arepresented%20even%20in%20large-scale%20datasets.%20Our%20results%20highlight%20that%20while%0Atraditional%20benchmarks%20may%20not%20benefit%20significantly%20from%20scaling%20noisy%2C%20raw%0Aweb%20data%20to%20100%20billion%20examples%2C%20this%20data%20scale%20is%20vital%20for%20building%20truly%0Ainclusive%20multimodal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07617v1&entry.124074799=Read"},
{"title": "Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models", "author": "Konstantin Donhauser and Kristina Ulicna and Gemma Elyse Moran and Aditya Ravuri and Kian Kenyon-Dean and Cian Eastwood and Jason Hartford", "abstract": "  Dictionary learning (DL) has emerged as a powerful interpretability tool for\nlarge language models. By extracting known concepts (e.g., Golden-Gate Bridge)\nfrom human-interpretable data (e.g., text), sparse DL can elucidate a model's\ninner workings. In this work, we ask if DL can also be used to discover unknown\nconcepts from less human-interpretable scientific data (e.g., cell images),\nultimately enabling modern approaches to scientific discovery. As a first step,\nwe use DL algorithms to study microscopy foundation models trained on\nmulti-cell image data, where little prior knowledge exists regarding which\nhigh-level concepts should arise. We show that sparse dictionaries indeed\nextract biologically-meaningful concepts such as cell type and genetic\nperturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)\nand combine it with a pre-processing step which uses PCA whitening from a\ncontrol dataset. In our experiments, we demonstrate that both ICFL and PCA\nimprove the selectivity of extracted features compared to TopK sparse\nautoencoders.\n", "link": "http://arxiv.org/abs/2412.16247v2", "date": "2025-02-11", "relevancy": 2.2765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20scientific%20discovery%20with%20dictionary%20learning%3A%20Extracting%0A%20%20biological%20concepts%20from%20microscopy%20foundation%20models&body=Title%3A%20Towards%20scientific%20discovery%20with%20dictionary%20learning%3A%20Extracting%0A%20%20biological%20concepts%20from%20microscopy%20foundation%20models%0AAuthor%3A%20Konstantin%20Donhauser%20and%20Kristina%20Ulicna%20and%20Gemma%20Elyse%20Moran%20and%20Aditya%20Ravuri%20and%20Kian%20Kenyon-Dean%20and%20Cian%20Eastwood%20and%20Jason%20Hartford%0AAbstract%3A%20%20%20Dictionary%20learning%20%28DL%29%20has%20emerged%20as%20a%20powerful%20interpretability%20tool%20for%0Alarge%20language%20models.%20By%20extracting%20known%20concepts%20%28e.g.%2C%20Golden-Gate%20Bridge%29%0Afrom%20human-interpretable%20data%20%28e.g.%2C%20text%29%2C%20sparse%20DL%20can%20elucidate%20a%20model%27s%0Ainner%20workings.%20In%20this%20work%2C%20we%20ask%20if%20DL%20can%20also%20be%20used%20to%20discover%20unknown%0Aconcepts%20from%20less%20human-interpretable%20scientific%20data%20%28e.g.%2C%20cell%20images%29%2C%0Aultimately%20enabling%20modern%20approaches%20to%20scientific%20discovery.%20As%20a%20first%20step%2C%0Awe%20use%20DL%20algorithms%20to%20study%20microscopy%20foundation%20models%20trained%20on%0Amulti-cell%20image%20data%2C%20where%20little%20prior%20knowledge%20exists%20regarding%20which%0Ahigh-level%20concepts%20should%20arise.%20We%20show%20that%20sparse%20dictionaries%20indeed%0Aextract%20biologically-meaningful%20concepts%20such%20as%20cell%20type%20and%20genetic%0Aperturbation%20type.%20We%20also%20propose%20Iterative%20Codebook%20Feature%20Learning~%28ICFL%29%0Aand%20combine%20it%20with%20a%20pre-processing%20step%20which%20uses%20PCA%20whitening%20from%20a%0Acontrol%20dataset.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20both%20ICFL%20and%20PCA%0Aimprove%20the%20selectivity%20of%20extracted%20features%20compared%20to%20TopK%20sparse%0Aautoencoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520scientific%2520discovery%2520with%2520dictionary%2520learning%253A%2520Extracting%250A%2520%2520biological%2520concepts%2520from%2520microscopy%2520foundation%2520models%26entry.906535625%3DKonstantin%2520Donhauser%2520and%2520Kristina%2520Ulicna%2520and%2520Gemma%2520Elyse%2520Moran%2520and%2520Aditya%2520Ravuri%2520and%2520Kian%2520Kenyon-Dean%2520and%2520Cian%2520Eastwood%2520and%2520Jason%2520Hartford%26entry.1292438233%3D%2520%2520Dictionary%2520learning%2520%2528DL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520interpretability%2520tool%2520for%250Alarge%2520language%2520models.%2520By%2520extracting%2520known%2520concepts%2520%2528e.g.%252C%2520Golden-Gate%2520Bridge%2529%250Afrom%2520human-interpretable%2520data%2520%2528e.g.%252C%2520text%2529%252C%2520sparse%2520DL%2520can%2520elucidate%2520a%2520model%2527s%250Ainner%2520workings.%2520In%2520this%2520work%252C%2520we%2520ask%2520if%2520DL%2520can%2520also%2520be%2520used%2520to%2520discover%2520unknown%250Aconcepts%2520from%2520less%2520human-interpretable%2520scientific%2520data%2520%2528e.g.%252C%2520cell%2520images%2529%252C%250Aultimately%2520enabling%2520modern%2520approaches%2520to%2520scientific%2520discovery.%2520As%2520a%2520first%2520step%252C%250Awe%2520use%2520DL%2520algorithms%2520to%2520study%2520microscopy%2520foundation%2520models%2520trained%2520on%250Amulti-cell%2520image%2520data%252C%2520where%2520little%2520prior%2520knowledge%2520exists%2520regarding%2520which%250Ahigh-level%2520concepts%2520should%2520arise.%2520We%2520show%2520that%2520sparse%2520dictionaries%2520indeed%250Aextract%2520biologically-meaningful%2520concepts%2520such%2520as%2520cell%2520type%2520and%2520genetic%250Aperturbation%2520type.%2520We%2520also%2520propose%2520Iterative%2520Codebook%2520Feature%2520Learning~%2528ICFL%2529%250Aand%2520combine%2520it%2520with%2520a%2520pre-processing%2520step%2520which%2520uses%2520PCA%2520whitening%2520from%2520a%250Acontrol%2520dataset.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520that%2520both%2520ICFL%2520and%2520PCA%250Aimprove%2520the%2520selectivity%2520of%2520extracted%2520features%2520compared%2520to%2520TopK%2520sparse%250Aautoencoders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20scientific%20discovery%20with%20dictionary%20learning%3A%20Extracting%0A%20%20biological%20concepts%20from%20microscopy%20foundation%20models&entry.906535625=Konstantin%20Donhauser%20and%20Kristina%20Ulicna%20and%20Gemma%20Elyse%20Moran%20and%20Aditya%20Ravuri%20and%20Kian%20Kenyon-Dean%20and%20Cian%20Eastwood%20and%20Jason%20Hartford&entry.1292438233=%20%20Dictionary%20learning%20%28DL%29%20has%20emerged%20as%20a%20powerful%20interpretability%20tool%20for%0Alarge%20language%20models.%20By%20extracting%20known%20concepts%20%28e.g.%2C%20Golden-Gate%20Bridge%29%0Afrom%20human-interpretable%20data%20%28e.g.%2C%20text%29%2C%20sparse%20DL%20can%20elucidate%20a%20model%27s%0Ainner%20workings.%20In%20this%20work%2C%20we%20ask%20if%20DL%20can%20also%20be%20used%20to%20discover%20unknown%0Aconcepts%20from%20less%20human-interpretable%20scientific%20data%20%28e.g.%2C%20cell%20images%29%2C%0Aultimately%20enabling%20modern%20approaches%20to%20scientific%20discovery.%20As%20a%20first%20step%2C%0Awe%20use%20DL%20algorithms%20to%20study%20microscopy%20foundation%20models%20trained%20on%0Amulti-cell%20image%20data%2C%20where%20little%20prior%20knowledge%20exists%20regarding%20which%0Ahigh-level%20concepts%20should%20arise.%20We%20show%20that%20sparse%20dictionaries%20indeed%0Aextract%20biologically-meaningful%20concepts%20such%20as%20cell%20type%20and%20genetic%0Aperturbation%20type.%20We%20also%20propose%20Iterative%20Codebook%20Feature%20Learning~%28ICFL%29%0Aand%20combine%20it%20with%20a%20pre-processing%20step%20which%20uses%20PCA%20whitening%20from%20a%0Acontrol%20dataset.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20both%20ICFL%20and%20PCA%0Aimprove%20the%20selectivity%20of%20extracted%20features%20compared%20to%20TopK%20sparse%0Aautoencoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16247v2&entry.124074799=Read"},
{"title": "Divide and Merge: Motion and Semantic Learning in End-to-End Autonomous\n  Driving", "author": "Yinzhe Shen and \u00d6mer \u015eahin Ta\u015f and Kaiwen Wang and Royden Wagner and Christoph Stiller", "abstract": "  Perceiving the environment and its changes over time corresponds to two\nfundamental yet heterogeneous types of information: semantics and motion.\nPrevious end-to-end autonomous driving works represent both types of\ninformation in a single feature vector. However, including motion tasks, such\nas prediction and planning, always impairs detection and tracking performance,\na phenomenon known as negative transfer in multi-task learning. To address this\nissue, we propose Neural-Bayes motion decoding, a novel parallel detection,\ntracking, and prediction method separating semantic and motion learning,\nsimilar to the Bayes filter. Specifically, we employ a set of learned motion\nqueries that operate in parallel with the detection and tracking queries,\nsharing a unified set of recursively updated reference points. Moreover, we\nemploy interactive semantic decoding to enhance information exchange in\nsemantic tasks, promoting positive transfer. Experiments on the nuScenes\ndataset show improvements of 5% in detection and 11% in tracking. Our method\nachieves state-of-the-art collision rates in open-loop planning evaluation\nwithout any modifications to the planning module.\n", "link": "http://arxiv.org/abs/2502.07631v1", "date": "2025-02-11", "relevancy": 2.2762, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6333}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide%20and%20Merge%3A%20Motion%20and%20Semantic%20Learning%20in%20End-to-End%20Autonomous%0A%20%20Driving&body=Title%3A%20Divide%20and%20Merge%3A%20Motion%20and%20Semantic%20Learning%20in%20End-to-End%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Yinzhe%20Shen%20and%20%C3%96mer%20%C5%9Eahin%20Ta%C5%9F%20and%20Kaiwen%20Wang%20and%20Royden%20Wagner%20and%20Christoph%20Stiller%0AAbstract%3A%20%20%20Perceiving%20the%20environment%20and%20its%20changes%20over%20time%20corresponds%20to%20two%0Afundamental%20yet%20heterogeneous%20types%20of%20information%3A%20semantics%20and%20motion.%0APrevious%20end-to-end%20autonomous%20driving%20works%20represent%20both%20types%20of%0Ainformation%20in%20a%20single%20feature%20vector.%20However%2C%20including%20motion%20tasks%2C%20such%0Aas%20prediction%20and%20planning%2C%20always%20impairs%20detection%20and%20tracking%20performance%2C%0Aa%20phenomenon%20known%20as%20negative%20transfer%20in%20multi-task%20learning.%20To%20address%20this%0Aissue%2C%20we%20propose%20Neural-Bayes%20motion%20decoding%2C%20a%20novel%20parallel%20detection%2C%0Atracking%2C%20and%20prediction%20method%20separating%20semantic%20and%20motion%20learning%2C%0Asimilar%20to%20the%20Bayes%20filter.%20Specifically%2C%20we%20employ%20a%20set%20of%20learned%20motion%0Aqueries%20that%20operate%20in%20parallel%20with%20the%20detection%20and%20tracking%20queries%2C%0Asharing%20a%20unified%20set%20of%20recursively%20updated%20reference%20points.%20Moreover%2C%20we%0Aemploy%20interactive%20semantic%20decoding%20to%20enhance%20information%20exchange%20in%0Asemantic%20tasks%2C%20promoting%20positive%20transfer.%20Experiments%20on%20the%20nuScenes%0Adataset%20show%20improvements%20of%205%25%20in%20detection%20and%2011%25%20in%20tracking.%20Our%20method%0Aachieves%20state-of-the-art%20collision%20rates%20in%20open-loop%20planning%20evaluation%0Awithout%20any%20modifications%20to%20the%20planning%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide%2520and%2520Merge%253A%2520Motion%2520and%2520Semantic%2520Learning%2520in%2520End-to-End%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DYinzhe%2520Shen%2520and%2520%25C3%2596mer%2520%25C5%259Eahin%2520Ta%25C5%259F%2520and%2520Kaiwen%2520Wang%2520and%2520Royden%2520Wagner%2520and%2520Christoph%2520Stiller%26entry.1292438233%3D%2520%2520Perceiving%2520the%2520environment%2520and%2520its%2520changes%2520over%2520time%2520corresponds%2520to%2520two%250Afundamental%2520yet%2520heterogeneous%2520types%2520of%2520information%253A%2520semantics%2520and%2520motion.%250APrevious%2520end-to-end%2520autonomous%2520driving%2520works%2520represent%2520both%2520types%2520of%250Ainformation%2520in%2520a%2520single%2520feature%2520vector.%2520However%252C%2520including%2520motion%2520tasks%252C%2520such%250Aas%2520prediction%2520and%2520planning%252C%2520always%2520impairs%2520detection%2520and%2520tracking%2520performance%252C%250Aa%2520phenomenon%2520known%2520as%2520negative%2520transfer%2520in%2520multi-task%2520learning.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520Neural-Bayes%2520motion%2520decoding%252C%2520a%2520novel%2520parallel%2520detection%252C%250Atracking%252C%2520and%2520prediction%2520method%2520separating%2520semantic%2520and%2520motion%2520learning%252C%250Asimilar%2520to%2520the%2520Bayes%2520filter.%2520Specifically%252C%2520we%2520employ%2520a%2520set%2520of%2520learned%2520motion%250Aqueries%2520that%2520operate%2520in%2520parallel%2520with%2520the%2520detection%2520and%2520tracking%2520queries%252C%250Asharing%2520a%2520unified%2520set%2520of%2520recursively%2520updated%2520reference%2520points.%2520Moreover%252C%2520we%250Aemploy%2520interactive%2520semantic%2520decoding%2520to%2520enhance%2520information%2520exchange%2520in%250Asemantic%2520tasks%252C%2520promoting%2520positive%2520transfer.%2520Experiments%2520on%2520the%2520nuScenes%250Adataset%2520show%2520improvements%2520of%25205%2525%2520in%2520detection%2520and%252011%2525%2520in%2520tracking.%2520Our%2520method%250Aachieves%2520state-of-the-art%2520collision%2520rates%2520in%2520open-loop%2520planning%2520evaluation%250Awithout%2520any%2520modifications%2520to%2520the%2520planning%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide%20and%20Merge%3A%20Motion%20and%20Semantic%20Learning%20in%20End-to-End%20Autonomous%0A%20%20Driving&entry.906535625=Yinzhe%20Shen%20and%20%C3%96mer%20%C5%9Eahin%20Ta%C5%9F%20and%20Kaiwen%20Wang%20and%20Royden%20Wagner%20and%20Christoph%20Stiller&entry.1292438233=%20%20Perceiving%20the%20environment%20and%20its%20changes%20over%20time%20corresponds%20to%20two%0Afundamental%20yet%20heterogeneous%20types%20of%20information%3A%20semantics%20and%20motion.%0APrevious%20end-to-end%20autonomous%20driving%20works%20represent%20both%20types%20of%0Ainformation%20in%20a%20single%20feature%20vector.%20However%2C%20including%20motion%20tasks%2C%20such%0Aas%20prediction%20and%20planning%2C%20always%20impairs%20detection%20and%20tracking%20performance%2C%0Aa%20phenomenon%20known%20as%20negative%20transfer%20in%20multi-task%20learning.%20To%20address%20this%0Aissue%2C%20we%20propose%20Neural-Bayes%20motion%20decoding%2C%20a%20novel%20parallel%20detection%2C%0Atracking%2C%20and%20prediction%20method%20separating%20semantic%20and%20motion%20learning%2C%0Asimilar%20to%20the%20Bayes%20filter.%20Specifically%2C%20we%20employ%20a%20set%20of%20learned%20motion%0Aqueries%20that%20operate%20in%20parallel%20with%20the%20detection%20and%20tracking%20queries%2C%0Asharing%20a%20unified%20set%20of%20recursively%20updated%20reference%20points.%20Moreover%2C%20we%0Aemploy%20interactive%20semantic%20decoding%20to%20enhance%20information%20exchange%20in%0Asemantic%20tasks%2C%20promoting%20positive%20transfer.%20Experiments%20on%20the%20nuScenes%0Adataset%20show%20improvements%20of%205%25%20in%20detection%20and%2011%25%20in%20tracking.%20Our%20method%0Aachieves%20state-of-the-art%20collision%20rates%20in%20open-loop%20planning%20evaluation%0Awithout%20any%20modifications%20to%20the%20planning%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07631v1&entry.124074799=Read"},
{"title": "SpaceMesh: A Continuous Representation for Learning Manifold Surface\n  Meshes", "author": "Tianchang Shen and Zhaoshuo Li and Marc Law and Matan Atzmon and Sanja Fidler and James Lucas and Jun Gao and Nicholas Sharp", "abstract": "  Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.\n", "link": "http://arxiv.org/abs/2409.20562v2", "date": "2025-02-11", "relevancy": 2.2679, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6348}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5233}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceMesh%3A%20A%20Continuous%20Representation%20for%20Learning%20Manifold%20Surface%0A%20%20Meshes&body=Title%3A%20SpaceMesh%3A%20A%20Continuous%20Representation%20for%20Learning%20Manifold%20Surface%0A%20%20Meshes%0AAuthor%3A%20Tianchang%20Shen%20and%20Zhaoshuo%20Li%20and%20Marc%20Law%20and%20Matan%20Atzmon%20and%20Sanja%20Fidler%20and%20James%20Lucas%20and%20Jun%20Gao%20and%20Nicholas%20Sharp%0AAbstract%3A%20%20%20Meshes%20are%20ubiquitous%20in%20visual%20computing%20and%20simulation%2C%20yet%20most%20existing%0Amachine%20learning%20techniques%20represent%20meshes%20only%20indirectly%2C%20e.g.%20as%20the%20level%0Aset%20of%20a%20scalar%20field%20or%20deformation%20of%20a%20template%2C%20or%20as%20a%20disordered%20triangle%0Asoup%20lacking%20local%20structure.%20This%20work%20presents%20a%20scheme%20to%20directly%20generate%0Amanifold%2C%20polygonal%20meshes%20of%20complex%20connectivity%20as%20the%20output%20of%20a%20neural%0Anetwork.%20Our%20key%20innovation%20is%20to%20define%20a%20continuous%20latent%20connectivity%20space%0Aat%20each%20mesh%20vertex%2C%20which%20implies%20the%20discrete%20mesh.%20In%20particular%2C%20our%20vertex%0Aembeddings%20generate%20cyclic%20neighbor%20relationships%20in%20a%20halfedge%20mesh%0Arepresentation%2C%20which%20gives%20a%20guarantee%20of%20edge-manifoldness%20and%20the%20ability%20to%0Arepresent%20general%20polygonal%20meshes.%20This%20representation%20is%20well-suited%20to%0Amachine%20learning%20and%20stochastic%20optimization%2C%20without%20restriction%20on%0Aconnectivity%20or%20topology.%20We%20first%20explore%20the%20basic%20properties%20of%20this%0Arepresentation%2C%20then%20use%20it%20to%20fit%20distributions%20of%20meshes%20from%20large%20datasets.%0AThe%20resulting%20models%20generate%20diverse%20meshes%20with%20tessellation%20structure%0Alearned%20from%20the%20dataset%20population%2C%20with%20concise%20details%20and%20high-quality%20mesh%0Aelements.%20In%20applications%2C%20this%20approach%20not%20only%20yields%20high-quality%20outputs%0Afrom%20generative%20models%2C%20but%20also%20enables%20directly%20learning%20challenging%20geometry%0Aprocessing%20tasks%20such%20as%20mesh%20repair.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceMesh%253A%2520A%2520Continuous%2520Representation%2520for%2520Learning%2520Manifold%2520Surface%250A%2520%2520Meshes%26entry.906535625%3DTianchang%2520Shen%2520and%2520Zhaoshuo%2520Li%2520and%2520Marc%2520Law%2520and%2520Matan%2520Atzmon%2520and%2520Sanja%2520Fidler%2520and%2520James%2520Lucas%2520and%2520Jun%2520Gao%2520and%2520Nicholas%2520Sharp%26entry.1292438233%3D%2520%2520Meshes%2520are%2520ubiquitous%2520in%2520visual%2520computing%2520and%2520simulation%252C%2520yet%2520most%2520existing%250Amachine%2520learning%2520techniques%2520represent%2520meshes%2520only%2520indirectly%252C%2520e.g.%2520as%2520the%2520level%250Aset%2520of%2520a%2520scalar%2520field%2520or%2520deformation%2520of%2520a%2520template%252C%2520or%2520as%2520a%2520disordered%2520triangle%250Asoup%2520lacking%2520local%2520structure.%2520This%2520work%2520presents%2520a%2520scheme%2520to%2520directly%2520generate%250Amanifold%252C%2520polygonal%2520meshes%2520of%2520complex%2520connectivity%2520as%2520the%2520output%2520of%2520a%2520neural%250Anetwork.%2520Our%2520key%2520innovation%2520is%2520to%2520define%2520a%2520continuous%2520latent%2520connectivity%2520space%250Aat%2520each%2520mesh%2520vertex%252C%2520which%2520implies%2520the%2520discrete%2520mesh.%2520In%2520particular%252C%2520our%2520vertex%250Aembeddings%2520generate%2520cyclic%2520neighbor%2520relationships%2520in%2520a%2520halfedge%2520mesh%250Arepresentation%252C%2520which%2520gives%2520a%2520guarantee%2520of%2520edge-manifoldness%2520and%2520the%2520ability%2520to%250Arepresent%2520general%2520polygonal%2520meshes.%2520This%2520representation%2520is%2520well-suited%2520to%250Amachine%2520learning%2520and%2520stochastic%2520optimization%252C%2520without%2520restriction%2520on%250Aconnectivity%2520or%2520topology.%2520We%2520first%2520explore%2520the%2520basic%2520properties%2520of%2520this%250Arepresentation%252C%2520then%2520use%2520it%2520to%2520fit%2520distributions%2520of%2520meshes%2520from%2520large%2520datasets.%250AThe%2520resulting%2520models%2520generate%2520diverse%2520meshes%2520with%2520tessellation%2520structure%250Alearned%2520from%2520the%2520dataset%2520population%252C%2520with%2520concise%2520details%2520and%2520high-quality%2520mesh%250Aelements.%2520In%2520applications%252C%2520this%2520approach%2520not%2520only%2520yields%2520high-quality%2520outputs%250Afrom%2520generative%2520models%252C%2520but%2520also%2520enables%2520directly%2520learning%2520challenging%2520geometry%250Aprocessing%2520tasks%2520such%2520as%2520mesh%2520repair.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceMesh%3A%20A%20Continuous%20Representation%20for%20Learning%20Manifold%20Surface%0A%20%20Meshes&entry.906535625=Tianchang%20Shen%20and%20Zhaoshuo%20Li%20and%20Marc%20Law%20and%20Matan%20Atzmon%20and%20Sanja%20Fidler%20and%20James%20Lucas%20and%20Jun%20Gao%20and%20Nicholas%20Sharp&entry.1292438233=%20%20Meshes%20are%20ubiquitous%20in%20visual%20computing%20and%20simulation%2C%20yet%20most%20existing%0Amachine%20learning%20techniques%20represent%20meshes%20only%20indirectly%2C%20e.g.%20as%20the%20level%0Aset%20of%20a%20scalar%20field%20or%20deformation%20of%20a%20template%2C%20or%20as%20a%20disordered%20triangle%0Asoup%20lacking%20local%20structure.%20This%20work%20presents%20a%20scheme%20to%20directly%20generate%0Amanifold%2C%20polygonal%20meshes%20of%20complex%20connectivity%20as%20the%20output%20of%20a%20neural%0Anetwork.%20Our%20key%20innovation%20is%20to%20define%20a%20continuous%20latent%20connectivity%20space%0Aat%20each%20mesh%20vertex%2C%20which%20implies%20the%20discrete%20mesh.%20In%20particular%2C%20our%20vertex%0Aembeddings%20generate%20cyclic%20neighbor%20relationships%20in%20a%20halfedge%20mesh%0Arepresentation%2C%20which%20gives%20a%20guarantee%20of%20edge-manifoldness%20and%20the%20ability%20to%0Arepresent%20general%20polygonal%20meshes.%20This%20representation%20is%20well-suited%20to%0Amachine%20learning%20and%20stochastic%20optimization%2C%20without%20restriction%20on%0Aconnectivity%20or%20topology.%20We%20first%20explore%20the%20basic%20properties%20of%20this%0Arepresentation%2C%20then%20use%20it%20to%20fit%20distributions%20of%20meshes%20from%20large%20datasets.%0AThe%20resulting%20models%20generate%20diverse%20meshes%20with%20tessellation%20structure%0Alearned%20from%20the%20dataset%20population%2C%20with%20concise%20details%20and%20high-quality%20mesh%0Aelements.%20In%20applications%2C%20this%20approach%20not%20only%20yields%20high-quality%20outputs%0Afrom%20generative%20models%2C%20but%20also%20enables%20directly%20learning%20challenging%20geometry%0Aprocessing%20tasks%20such%20as%20mesh%20repair.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20562v2&entry.124074799=Read"},
{"title": "Locally Private Estimation with Public Features", "author": "Yuheng Ma and Ke Jia and Hanfang Yang", "abstract": "  We initiate the study of locally differentially private (LDP) learning with\npublic features. We define semi-feature LDP, where some features are publicly\navailable while the remaining ones, along with the label, require protection\nunder local differential privacy. Under semi-feature LDP, we demonstrate that\nthe mini-max convergence rate for non-parametric regression is significantly\nreduced compared to that of classical LDP. Then we propose HistOfTree, an\nestimator that fully leverages the information contained in both public and\nprivate features. Theoretically, HistOfTree reaches the mini-max optimal\nconvergence rate. Empirically, HistOfTree achieves superior performance on both\nsynthetic and real data. We also explore scenarios where users have the\nflexibility to select features for protection manually. In such cases, we\npropose an estimator and a data-driven parameter tuning strategy, leading to\nanalogous theoretical and empirical results.\n", "link": "http://arxiv.org/abs/2405.13481v2", "date": "2025-02-11", "relevancy": 2.2635, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4682}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Private%20Estimation%20with%20Public%20Features&body=Title%3A%20Locally%20Private%20Estimation%20with%20Public%20Features%0AAuthor%3A%20Yuheng%20Ma%20and%20Ke%20Jia%20and%20Hanfang%20Yang%0AAbstract%3A%20%20%20We%20initiate%20the%20study%20of%20locally%20differentially%20private%20%28LDP%29%20learning%20with%0Apublic%20features.%20We%20define%20semi-feature%20LDP%2C%20where%20some%20features%20are%20publicly%0Aavailable%20while%20the%20remaining%20ones%2C%20along%20with%20the%20label%2C%20require%20protection%0Aunder%20local%20differential%20privacy.%20Under%20semi-feature%20LDP%2C%20we%20demonstrate%20that%0Athe%20mini-max%20convergence%20rate%20for%20non-parametric%20regression%20is%20significantly%0Areduced%20compared%20to%20that%20of%20classical%20LDP.%20Then%20we%20propose%20HistOfTree%2C%20an%0Aestimator%20that%20fully%20leverages%20the%20information%20contained%20in%20both%20public%20and%0Aprivate%20features.%20Theoretically%2C%20HistOfTree%20reaches%20the%20mini-max%20optimal%0Aconvergence%20rate.%20Empirically%2C%20HistOfTree%20achieves%20superior%20performance%20on%20both%0Asynthetic%20and%20real%20data.%20We%20also%20explore%20scenarios%20where%20users%20have%20the%0Aflexibility%20to%20select%20features%20for%20protection%20manually.%20In%20such%20cases%2C%20we%0Apropose%20an%20estimator%20and%20a%20data-driven%20parameter%20tuning%20strategy%2C%20leading%20to%0Aanalogous%20theoretical%20and%20empirical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13481v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Private%2520Estimation%2520with%2520Public%2520Features%26entry.906535625%3DYuheng%2520Ma%2520and%2520Ke%2520Jia%2520and%2520Hanfang%2520Yang%26entry.1292438233%3D%2520%2520We%2520initiate%2520the%2520study%2520of%2520locally%2520differentially%2520private%2520%2528LDP%2529%2520learning%2520with%250Apublic%2520features.%2520We%2520define%2520semi-feature%2520LDP%252C%2520where%2520some%2520features%2520are%2520publicly%250Aavailable%2520while%2520the%2520remaining%2520ones%252C%2520along%2520with%2520the%2520label%252C%2520require%2520protection%250Aunder%2520local%2520differential%2520privacy.%2520Under%2520semi-feature%2520LDP%252C%2520we%2520demonstrate%2520that%250Athe%2520mini-max%2520convergence%2520rate%2520for%2520non-parametric%2520regression%2520is%2520significantly%250Areduced%2520compared%2520to%2520that%2520of%2520classical%2520LDP.%2520Then%2520we%2520propose%2520HistOfTree%252C%2520an%250Aestimator%2520that%2520fully%2520leverages%2520the%2520information%2520contained%2520in%2520both%2520public%2520and%250Aprivate%2520features.%2520Theoretically%252C%2520HistOfTree%2520reaches%2520the%2520mini-max%2520optimal%250Aconvergence%2520rate.%2520Empirically%252C%2520HistOfTree%2520achieves%2520superior%2520performance%2520on%2520both%250Asynthetic%2520and%2520real%2520data.%2520We%2520also%2520explore%2520scenarios%2520where%2520users%2520have%2520the%250Aflexibility%2520to%2520select%2520features%2520for%2520protection%2520manually.%2520In%2520such%2520cases%252C%2520we%250Apropose%2520an%2520estimator%2520and%2520a%2520data-driven%2520parameter%2520tuning%2520strategy%252C%2520leading%2520to%250Aanalogous%2520theoretical%2520and%2520empirical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13481v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Private%20Estimation%20with%20Public%20Features&entry.906535625=Yuheng%20Ma%20and%20Ke%20Jia%20and%20Hanfang%20Yang&entry.1292438233=%20%20We%20initiate%20the%20study%20of%20locally%20differentially%20private%20%28LDP%29%20learning%20with%0Apublic%20features.%20We%20define%20semi-feature%20LDP%2C%20where%20some%20features%20are%20publicly%0Aavailable%20while%20the%20remaining%20ones%2C%20along%20with%20the%20label%2C%20require%20protection%0Aunder%20local%20differential%20privacy.%20Under%20semi-feature%20LDP%2C%20we%20demonstrate%20that%0Athe%20mini-max%20convergence%20rate%20for%20non-parametric%20regression%20is%20significantly%0Areduced%20compared%20to%20that%20of%20classical%20LDP.%20Then%20we%20propose%20HistOfTree%2C%20an%0Aestimator%20that%20fully%20leverages%20the%20information%20contained%20in%20both%20public%20and%0Aprivate%20features.%20Theoretically%2C%20HistOfTree%20reaches%20the%20mini-max%20optimal%0Aconvergence%20rate.%20Empirically%2C%20HistOfTree%20achieves%20superior%20performance%20on%20both%0Asynthetic%20and%20real%20data.%20We%20also%20explore%20scenarios%20where%20users%20have%20the%0Aflexibility%20to%20select%20features%20for%20protection%20manually.%20In%20such%20cases%2C%20we%0Apropose%20an%20estimator%20and%20a%20data-driven%20parameter%20tuning%20strategy%2C%20leading%20to%0Aanalogous%20theoretical%20and%20empirical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13481v2&entry.124074799=Read"},
{"title": "CoS: Chain-of-Shot Prompting for Long Video Understanding", "author": "Jian Hu and Zixu Cheng and Chenyang Si and Wei Li and Shaogang Gong", "abstract": "  Multi-modal Large Language Models (MLLMs) struggle with long videos due to\nthe need for excessive visual tokens. These tokens exceed massively the context\nlength of MLLMs, resulting in filled by redundant task-irrelevant shots. How to\nselect shots is an unsolved critical problem: sparse sampling risks missing key\ndetails, while exhaustive sampling overwhelms the model with irrelevant\ncontent, leading to video misunderstanding. To solve this problem, we propose\nChain-of-Shot prompting (CoS). The key idea is to frame shot selection as\ntest-time visual prompt optimisation, choosing shots adaptive to video\nunderstanding semantic task by optimising shots-task alignment. CoS has two key\nparts: (1) a binary video summary mechanism that performs pseudo temporal\ngrounding, discovering a binary coding to identify task-relevant shots, and (2)\na video co-reasoning module that deploys the binary coding to pair (learning to\nalign) task-relevant positive shots with irrelevant negative shots. It embeds\nthe optimised shot selections into the original video, facilitating a focus on\nrelevant context to optimize long video understanding. Experiments across three\nbaselines and five datasets demonstrate the effectiveness and adaptability of\nCoS. Code given in https://lwpyh.github.io/CoS.\n", "link": "http://arxiv.org/abs/2502.06428v2", "date": "2025-02-11", "relevancy": 2.2509, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoS%3A%20Chain-of-Shot%20Prompting%20for%20Long%20Video%20Understanding&body=Title%3A%20CoS%3A%20Chain-of-Shot%20Prompting%20for%20Long%20Video%20Understanding%0AAuthor%3A%20Jian%20Hu%20and%20Zixu%20Cheng%20and%20Chenyang%20Si%20and%20Wei%20Li%20and%20Shaogang%20Gong%0AAbstract%3A%20%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20long%20videos%20due%20to%0Athe%20need%20for%20excessive%20visual%20tokens.%20These%20tokens%20exceed%20massively%20the%20context%0Alength%20of%20MLLMs%2C%20resulting%20in%20filled%20by%20redundant%20task-irrelevant%20shots.%20How%20to%0Aselect%20shots%20is%20an%20unsolved%20critical%20problem%3A%20sparse%20sampling%20risks%20missing%20key%0Adetails%2C%20while%20exhaustive%20sampling%20overwhelms%20the%20model%20with%20irrelevant%0Acontent%2C%20leading%20to%20video%20misunderstanding.%20To%20solve%20this%20problem%2C%20we%20propose%0AChain-of-Shot%20prompting%20%28CoS%29.%20The%20key%20idea%20is%20to%20frame%20shot%20selection%20as%0Atest-time%20visual%20prompt%20optimisation%2C%20choosing%20shots%20adaptive%20to%20video%0Aunderstanding%20semantic%20task%20by%20optimising%20shots-task%20alignment.%20CoS%20has%20two%20key%0Aparts%3A%20%281%29%20a%20binary%20video%20summary%20mechanism%20that%20performs%20pseudo%20temporal%0Agrounding%2C%20discovering%20a%20binary%20coding%20to%20identify%20task-relevant%20shots%2C%20and%20%282%29%0Aa%20video%20co-reasoning%20module%20that%20deploys%20the%20binary%20coding%20to%20pair%20%28learning%20to%0Aalign%29%20task-relevant%20positive%20shots%20with%20irrelevant%20negative%20shots.%20It%20embeds%0Athe%20optimised%20shot%20selections%20into%20the%20original%20video%2C%20facilitating%20a%20focus%20on%0Arelevant%20context%20to%20optimize%20long%20video%20understanding.%20Experiments%20across%20three%0Abaselines%20and%20five%20datasets%20demonstrate%20the%20effectiveness%20and%20adaptability%20of%0ACoS.%20Code%20given%20in%20https%3A//lwpyh.github.io/CoS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06428v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoS%253A%2520Chain-of-Shot%2520Prompting%2520for%2520Long%2520Video%2520Understanding%26entry.906535625%3DJian%2520Hu%2520and%2520Zixu%2520Cheng%2520and%2520Chenyang%2520Si%2520and%2520Wei%2520Li%2520and%2520Shaogang%2520Gong%26entry.1292438233%3D%2520%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520struggle%2520with%2520long%2520videos%2520due%2520to%250Athe%2520need%2520for%2520excessive%2520visual%2520tokens.%2520These%2520tokens%2520exceed%2520massively%2520the%2520context%250Alength%2520of%2520MLLMs%252C%2520resulting%2520in%2520filled%2520by%2520redundant%2520task-irrelevant%2520shots.%2520How%2520to%250Aselect%2520shots%2520is%2520an%2520unsolved%2520critical%2520problem%253A%2520sparse%2520sampling%2520risks%2520missing%2520key%250Adetails%252C%2520while%2520exhaustive%2520sampling%2520overwhelms%2520the%2520model%2520with%2520irrelevant%250Acontent%252C%2520leading%2520to%2520video%2520misunderstanding.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%250AChain-of-Shot%2520prompting%2520%2528CoS%2529.%2520The%2520key%2520idea%2520is%2520to%2520frame%2520shot%2520selection%2520as%250Atest-time%2520visual%2520prompt%2520optimisation%252C%2520choosing%2520shots%2520adaptive%2520to%2520video%250Aunderstanding%2520semantic%2520task%2520by%2520optimising%2520shots-task%2520alignment.%2520CoS%2520has%2520two%2520key%250Aparts%253A%2520%25281%2529%2520a%2520binary%2520video%2520summary%2520mechanism%2520that%2520performs%2520pseudo%2520temporal%250Agrounding%252C%2520discovering%2520a%2520binary%2520coding%2520to%2520identify%2520task-relevant%2520shots%252C%2520and%2520%25282%2529%250Aa%2520video%2520co-reasoning%2520module%2520that%2520deploys%2520the%2520binary%2520coding%2520to%2520pair%2520%2528learning%2520to%250Aalign%2529%2520task-relevant%2520positive%2520shots%2520with%2520irrelevant%2520negative%2520shots.%2520It%2520embeds%250Athe%2520optimised%2520shot%2520selections%2520into%2520the%2520original%2520video%252C%2520facilitating%2520a%2520focus%2520on%250Arelevant%2520context%2520to%2520optimize%2520long%2520video%2520understanding.%2520Experiments%2520across%2520three%250Abaselines%2520and%2520five%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520adaptability%2520of%250ACoS.%2520Code%2520given%2520in%2520https%253A//lwpyh.github.io/CoS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06428v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoS%3A%20Chain-of-Shot%20Prompting%20for%20Long%20Video%20Understanding&entry.906535625=Jian%20Hu%20and%20Zixu%20Cheng%20and%20Chenyang%20Si%20and%20Wei%20Li%20and%20Shaogang%20Gong&entry.1292438233=%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20struggle%20with%20long%20videos%20due%20to%0Athe%20need%20for%20excessive%20visual%20tokens.%20These%20tokens%20exceed%20massively%20the%20context%0Alength%20of%20MLLMs%2C%20resulting%20in%20filled%20by%20redundant%20task-irrelevant%20shots.%20How%20to%0Aselect%20shots%20is%20an%20unsolved%20critical%20problem%3A%20sparse%20sampling%20risks%20missing%20key%0Adetails%2C%20while%20exhaustive%20sampling%20overwhelms%20the%20model%20with%20irrelevant%0Acontent%2C%20leading%20to%20video%20misunderstanding.%20To%20solve%20this%20problem%2C%20we%20propose%0AChain-of-Shot%20prompting%20%28CoS%29.%20The%20key%20idea%20is%20to%20frame%20shot%20selection%20as%0Atest-time%20visual%20prompt%20optimisation%2C%20choosing%20shots%20adaptive%20to%20video%0Aunderstanding%20semantic%20task%20by%20optimising%20shots-task%20alignment.%20CoS%20has%20two%20key%0Aparts%3A%20%281%29%20a%20binary%20video%20summary%20mechanism%20that%20performs%20pseudo%20temporal%0Agrounding%2C%20discovering%20a%20binary%20coding%20to%20identify%20task-relevant%20shots%2C%20and%20%282%29%0Aa%20video%20co-reasoning%20module%20that%20deploys%20the%20binary%20coding%20to%20pair%20%28learning%20to%0Aalign%29%20task-relevant%20positive%20shots%20with%20irrelevant%20negative%20shots.%20It%20embeds%0Athe%20optimised%20shot%20selections%20into%20the%20original%20video%2C%20facilitating%20a%20focus%20on%0Arelevant%20context%20to%20optimize%20long%20video%20understanding.%20Experiments%20across%20three%0Abaselines%20and%20five%20datasets%20demonstrate%20the%20effectiveness%20and%20adaptability%20of%0ACoS.%20Code%20given%20in%20https%3A//lwpyh.github.io/CoS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06428v2&entry.124074799=Read"},
{"title": "Efficient Continuous Group Convolutions for Local SE(3) Equivariance in\n  3D Point Clouds", "author": "Lisa Weijler and Pedro Hermosilla", "abstract": "  Extending the translation equivariance property of convolutional neural\nnetworks to larger symmetry groups has been shown to reduce sample complexity\nand enable more discriminative feature learning. Further, exploiting additional\nsymmetries facilitates greater weight sharing than standard convolutions,\nleading to an enhanced network expressivity without an increase in parameter\ncount. However, extending the equivariant properties of a convolution layer\ncomes at a computational cost. In particular, for 3D data, expanding\nequivariance to the SE(3) group (rotation and translation) results in a 6D\nconvolution operation, which is not tractable for larger data samples such as\n3D scene scans. While efforts have been made to develop efficient SE(3)\nequivariant networks, existing approaches rely on discretization or only\nintroduce global rotation equivariance. This limits their applicability to\npoint clouds representing a scene composed of multiple objects. This work\npresents an efficient, continuous, and local SE(3) equivariant convolution\nlayer for point cloud processing based on general group convolution and local\nreference frames. Our experiments show that our approach achieves competitive\nor superior performance across a range of datasets and tasks, including object\nclassification and semantic segmentation, with negligible computational\noverhead.\n", "link": "http://arxiv.org/abs/2502.07505v1", "date": "2025-02-11", "relevancy": 2.2492, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5744}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5632}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Continuous%20Group%20Convolutions%20for%20Local%20SE%283%29%20Equivariance%20in%0A%20%203D%20Point%20Clouds&body=Title%3A%20Efficient%20Continuous%20Group%20Convolutions%20for%20Local%20SE%283%29%20Equivariance%20in%0A%20%203D%20Point%20Clouds%0AAuthor%3A%20Lisa%20Weijler%20and%20Pedro%20Hermosilla%0AAbstract%3A%20%20%20Extending%20the%20translation%20equivariance%20property%20of%20convolutional%20neural%0Anetworks%20to%20larger%20symmetry%20groups%20has%20been%20shown%20to%20reduce%20sample%20complexity%0Aand%20enable%20more%20discriminative%20feature%20learning.%20Further%2C%20exploiting%20additional%0Asymmetries%20facilitates%20greater%20weight%20sharing%20than%20standard%20convolutions%2C%0Aleading%20to%20an%20enhanced%20network%20expressivity%20without%20an%20increase%20in%20parameter%0Acount.%20However%2C%20extending%20the%20equivariant%20properties%20of%20a%20convolution%20layer%0Acomes%20at%20a%20computational%20cost.%20In%20particular%2C%20for%203D%20data%2C%20expanding%0Aequivariance%20to%20the%20SE%283%29%20group%20%28rotation%20and%20translation%29%20results%20in%20a%206D%0Aconvolution%20operation%2C%20which%20is%20not%20tractable%20for%20larger%20data%20samples%20such%20as%0A3D%20scene%20scans.%20While%20efforts%20have%20been%20made%20to%20develop%20efficient%20SE%283%29%0Aequivariant%20networks%2C%20existing%20approaches%20rely%20on%20discretization%20or%20only%0Aintroduce%20global%20rotation%20equivariance.%20This%20limits%20their%20applicability%20to%0Apoint%20clouds%20representing%20a%20scene%20composed%20of%20multiple%20objects.%20This%20work%0Apresents%20an%20efficient%2C%20continuous%2C%20and%20local%20SE%283%29%20equivariant%20convolution%0Alayer%20for%20point%20cloud%20processing%20based%20on%20general%20group%20convolution%20and%20local%0Areference%20frames.%20Our%20experiments%20show%20that%20our%20approach%20achieves%20competitive%0Aor%20superior%20performance%20across%20a%20range%20of%20datasets%20and%20tasks%2C%20including%20object%0Aclassification%20and%20semantic%20segmentation%2C%20with%20negligible%20computational%0Aoverhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Continuous%2520Group%2520Convolutions%2520for%2520Local%2520SE%25283%2529%2520Equivariance%2520in%250A%2520%25203D%2520Point%2520Clouds%26entry.906535625%3DLisa%2520Weijler%2520and%2520Pedro%2520Hermosilla%26entry.1292438233%3D%2520%2520Extending%2520the%2520translation%2520equivariance%2520property%2520of%2520convolutional%2520neural%250Anetworks%2520to%2520larger%2520symmetry%2520groups%2520has%2520been%2520shown%2520to%2520reduce%2520sample%2520complexity%250Aand%2520enable%2520more%2520discriminative%2520feature%2520learning.%2520Further%252C%2520exploiting%2520additional%250Asymmetries%2520facilitates%2520greater%2520weight%2520sharing%2520than%2520standard%2520convolutions%252C%250Aleading%2520to%2520an%2520enhanced%2520network%2520expressivity%2520without%2520an%2520increase%2520in%2520parameter%250Acount.%2520However%252C%2520extending%2520the%2520equivariant%2520properties%2520of%2520a%2520convolution%2520layer%250Acomes%2520at%2520a%2520computational%2520cost.%2520In%2520particular%252C%2520for%25203D%2520data%252C%2520expanding%250Aequivariance%2520to%2520the%2520SE%25283%2529%2520group%2520%2528rotation%2520and%2520translation%2529%2520results%2520in%2520a%25206D%250Aconvolution%2520operation%252C%2520which%2520is%2520not%2520tractable%2520for%2520larger%2520data%2520samples%2520such%2520as%250A3D%2520scene%2520scans.%2520While%2520efforts%2520have%2520been%2520made%2520to%2520develop%2520efficient%2520SE%25283%2529%250Aequivariant%2520networks%252C%2520existing%2520approaches%2520rely%2520on%2520discretization%2520or%2520only%250Aintroduce%2520global%2520rotation%2520equivariance.%2520This%2520limits%2520their%2520applicability%2520to%250Apoint%2520clouds%2520representing%2520a%2520scene%2520composed%2520of%2520multiple%2520objects.%2520This%2520work%250Apresents%2520an%2520efficient%252C%2520continuous%252C%2520and%2520local%2520SE%25283%2529%2520equivariant%2520convolution%250Alayer%2520for%2520point%2520cloud%2520processing%2520based%2520on%2520general%2520group%2520convolution%2520and%2520local%250Areference%2520frames.%2520Our%2520experiments%2520show%2520that%2520our%2520approach%2520achieves%2520competitive%250Aor%2520superior%2520performance%2520across%2520a%2520range%2520of%2520datasets%2520and%2520tasks%252C%2520including%2520object%250Aclassification%2520and%2520semantic%2520segmentation%252C%2520with%2520negligible%2520computational%250Aoverhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Continuous%20Group%20Convolutions%20for%20Local%20SE%283%29%20Equivariance%20in%0A%20%203D%20Point%20Clouds&entry.906535625=Lisa%20Weijler%20and%20Pedro%20Hermosilla&entry.1292438233=%20%20Extending%20the%20translation%20equivariance%20property%20of%20convolutional%20neural%0Anetworks%20to%20larger%20symmetry%20groups%20has%20been%20shown%20to%20reduce%20sample%20complexity%0Aand%20enable%20more%20discriminative%20feature%20learning.%20Further%2C%20exploiting%20additional%0Asymmetries%20facilitates%20greater%20weight%20sharing%20than%20standard%20convolutions%2C%0Aleading%20to%20an%20enhanced%20network%20expressivity%20without%20an%20increase%20in%20parameter%0Acount.%20However%2C%20extending%20the%20equivariant%20properties%20of%20a%20convolution%20layer%0Acomes%20at%20a%20computational%20cost.%20In%20particular%2C%20for%203D%20data%2C%20expanding%0Aequivariance%20to%20the%20SE%283%29%20group%20%28rotation%20and%20translation%29%20results%20in%20a%206D%0Aconvolution%20operation%2C%20which%20is%20not%20tractable%20for%20larger%20data%20samples%20such%20as%0A3D%20scene%20scans.%20While%20efforts%20have%20been%20made%20to%20develop%20efficient%20SE%283%29%0Aequivariant%20networks%2C%20existing%20approaches%20rely%20on%20discretization%20or%20only%0Aintroduce%20global%20rotation%20equivariance.%20This%20limits%20their%20applicability%20to%0Apoint%20clouds%20representing%20a%20scene%20composed%20of%20multiple%20objects.%20This%20work%0Apresents%20an%20efficient%2C%20continuous%2C%20and%20local%20SE%283%29%20equivariant%20convolution%0Alayer%20for%20point%20cloud%20processing%20based%20on%20general%20group%20convolution%20and%20local%0Areference%20frames.%20Our%20experiments%20show%20that%20our%20approach%20achieves%20competitive%0Aor%20superior%20performance%20across%20a%20range%20of%20datasets%20and%20tasks%2C%20including%20object%0Aclassification%20and%20semantic%20segmentation%2C%20with%20negligible%20computational%0Aoverhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07505v1&entry.124074799=Read"},
{"title": "LOGCAN++: Adaptive Local-global class-aware network for semantic\n  segmentation of remote sensing imagery", "author": "Xiaowen Ma and Rongrong Lian and Zhenkai Wu and Hongbo Guo and Mengting Ma and Sensen Wu and Zhenhong Du and Siyang Song and Wei Zhang", "abstract": "  Remote sensing images usually characterized by complex backgrounds, scale and\norientation variations, and large intra-class variance. General semantic\nsegmentation methods usually fail to fully investigate the above issues, and\nthus their performances on remote sensing image segmentation are limited. In\nthis paper, we propose our LOGCAN++, a semantic segmentation model customized\nfor remote sensing images, which is made up of a Global Class Awareness (GCA)\nmodule and several Local Class Awareness (LCA) modules. The GCA module captures\nglobal representations for class-level context modeling to reduce the\ninterference of background noise. The LCA module generates local class\nrepresentations as intermediate perceptual elements to indirectly associate\npixels with the global class representations, targeting at dealing with the\nlarge intra-class variance problem. In particular, we introduce affine\ntransformations in the LCA module for adaptive extraction of local class\nrepresentations to effectively tolerate scale and orientation variations in\nremotely sensed images. Extensive experiments on three benchmark datasets show\nthat our LOGCAN++ outperforms current mainstream general and remote sensing\nsemantic segmentation methods and achieves a better trade-off between speed and\naccuracy. Code is available at https://github.com/xwmaxwma/rssegmentation.\n", "link": "http://arxiv.org/abs/2406.16502v3", "date": "2025-02-11", "relevancy": 2.2381, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5827}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5473}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOGCAN%2B%2B%3A%20Adaptive%20Local-global%20class-aware%20network%20for%20semantic%0A%20%20segmentation%20of%20remote%20sensing%20imagery&body=Title%3A%20LOGCAN%2B%2B%3A%20Adaptive%20Local-global%20class-aware%20network%20for%20semantic%0A%20%20segmentation%20of%20remote%20sensing%20imagery%0AAuthor%3A%20Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhenkai%20Wu%20and%20Hongbo%20Guo%20and%20Mengting%20Ma%20and%20Sensen%20Wu%20and%20Zhenhong%20Du%20and%20Siyang%20Song%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Remote%20sensing%20images%20usually%20characterized%20by%20complex%20backgrounds%2C%20scale%20and%0Aorientation%20variations%2C%20and%20large%20intra-class%20variance.%20General%20semantic%0Asegmentation%20methods%20usually%20fail%20to%20fully%20investigate%20the%20above%20issues%2C%20and%0Athus%20their%20performances%20on%20remote%20sensing%20image%20segmentation%20are%20limited.%20In%0Athis%20paper%2C%20we%20propose%20our%20LOGCAN%2B%2B%2C%20a%20semantic%20segmentation%20model%20customized%0Afor%20remote%20sensing%20images%2C%20which%20is%20made%20up%20of%20a%20Global%20Class%20Awareness%20%28GCA%29%0Amodule%20and%20several%20Local%20Class%20Awareness%20%28LCA%29%20modules.%20The%20GCA%20module%20captures%0Aglobal%20representations%20for%20class-level%20context%20modeling%20to%20reduce%20the%0Ainterference%20of%20background%20noise.%20The%20LCA%20module%20generates%20local%20class%0Arepresentations%20as%20intermediate%20perceptual%20elements%20to%20indirectly%20associate%0Apixels%20with%20the%20global%20class%20representations%2C%20targeting%20at%20dealing%20with%20the%0Alarge%20intra-class%20variance%20problem.%20In%20particular%2C%20we%20introduce%20affine%0Atransformations%20in%20the%20LCA%20module%20for%20adaptive%20extraction%20of%20local%20class%0Arepresentations%20to%20effectively%20tolerate%20scale%20and%20orientation%20variations%20in%0Aremotely%20sensed%20images.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20show%0Athat%20our%20LOGCAN%2B%2B%20outperforms%20current%20mainstream%20general%20and%20remote%20sensing%0Asemantic%20segmentation%20methods%20and%20achieves%20a%20better%20trade-off%20between%20speed%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rssegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16502v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOGCAN%252B%252B%253A%2520Adaptive%2520Local-global%2520class-aware%2520network%2520for%2520semantic%250A%2520%2520segmentation%2520of%2520remote%2520sensing%2520imagery%26entry.906535625%3DXiaowen%2520Ma%2520and%2520Rongrong%2520Lian%2520and%2520Zhenkai%2520Wu%2520and%2520Hongbo%2520Guo%2520and%2520Mengting%2520Ma%2520and%2520Sensen%2520Wu%2520and%2520Zhenhong%2520Du%2520and%2520Siyang%2520Song%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520images%2520usually%2520characterized%2520by%2520complex%2520backgrounds%252C%2520scale%2520and%250Aorientation%2520variations%252C%2520and%2520large%2520intra-class%2520variance.%2520General%2520semantic%250Asegmentation%2520methods%2520usually%2520fail%2520to%2520fully%2520investigate%2520the%2520above%2520issues%252C%2520and%250Athus%2520their%2520performances%2520on%2520remote%2520sensing%2520image%2520segmentation%2520are%2520limited.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520our%2520LOGCAN%252B%252B%252C%2520a%2520semantic%2520segmentation%2520model%2520customized%250Afor%2520remote%2520sensing%2520images%252C%2520which%2520is%2520made%2520up%2520of%2520a%2520Global%2520Class%2520Awareness%2520%2528GCA%2529%250Amodule%2520and%2520several%2520Local%2520Class%2520Awareness%2520%2528LCA%2529%2520modules.%2520The%2520GCA%2520module%2520captures%250Aglobal%2520representations%2520for%2520class-level%2520context%2520modeling%2520to%2520reduce%2520the%250Ainterference%2520of%2520background%2520noise.%2520The%2520LCA%2520module%2520generates%2520local%2520class%250Arepresentations%2520as%2520intermediate%2520perceptual%2520elements%2520to%2520indirectly%2520associate%250Apixels%2520with%2520the%2520global%2520class%2520representations%252C%2520targeting%2520at%2520dealing%2520with%2520the%250Alarge%2520intra-class%2520variance%2520problem.%2520In%2520particular%252C%2520we%2520introduce%2520affine%250Atransformations%2520in%2520the%2520LCA%2520module%2520for%2520adaptive%2520extraction%2520of%2520local%2520class%250Arepresentations%2520to%2520effectively%2520tolerate%2520scale%2520and%2520orientation%2520variations%2520in%250Aremotely%2520sensed%2520images.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520show%250Athat%2520our%2520LOGCAN%252B%252B%2520outperforms%2520current%2520mainstream%2520general%2520and%2520remote%2520sensing%250Asemantic%2520segmentation%2520methods%2520and%2520achieves%2520a%2520better%2520trade-off%2520between%2520speed%2520and%250Aaccuracy.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/xwmaxwma/rssegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16502v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOGCAN%2B%2B%3A%20Adaptive%20Local-global%20class-aware%20network%20for%20semantic%0A%20%20segmentation%20of%20remote%20sensing%20imagery&entry.906535625=Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhenkai%20Wu%20and%20Hongbo%20Guo%20and%20Mengting%20Ma%20and%20Sensen%20Wu%20and%20Zhenhong%20Du%20and%20Siyang%20Song%20and%20Wei%20Zhang&entry.1292438233=%20%20Remote%20sensing%20images%20usually%20characterized%20by%20complex%20backgrounds%2C%20scale%20and%0Aorientation%20variations%2C%20and%20large%20intra-class%20variance.%20General%20semantic%0Asegmentation%20methods%20usually%20fail%20to%20fully%20investigate%20the%20above%20issues%2C%20and%0Athus%20their%20performances%20on%20remote%20sensing%20image%20segmentation%20are%20limited.%20In%0Athis%20paper%2C%20we%20propose%20our%20LOGCAN%2B%2B%2C%20a%20semantic%20segmentation%20model%20customized%0Afor%20remote%20sensing%20images%2C%20which%20is%20made%20up%20of%20a%20Global%20Class%20Awareness%20%28GCA%29%0Amodule%20and%20several%20Local%20Class%20Awareness%20%28LCA%29%20modules.%20The%20GCA%20module%20captures%0Aglobal%20representations%20for%20class-level%20context%20modeling%20to%20reduce%20the%0Ainterference%20of%20background%20noise.%20The%20LCA%20module%20generates%20local%20class%0Arepresentations%20as%20intermediate%20perceptual%20elements%20to%20indirectly%20associate%0Apixels%20with%20the%20global%20class%20representations%2C%20targeting%20at%20dealing%20with%20the%0Alarge%20intra-class%20variance%20problem.%20In%20particular%2C%20we%20introduce%20affine%0Atransformations%20in%20the%20LCA%20module%20for%20adaptive%20extraction%20of%20local%20class%0Arepresentations%20to%20effectively%20tolerate%20scale%20and%20orientation%20variations%20in%0Aremotely%20sensed%20images.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20show%0Athat%20our%20LOGCAN%2B%2B%20outperforms%20current%20mainstream%20general%20and%20remote%20sensing%0Asemantic%20segmentation%20methods%20and%20achieves%20a%20better%20trade-off%20between%20speed%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rssegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16502v3&entry.124074799=Read"},
{"title": "TransRef: Multi-Scale Reference Embedding Transformer for\n  Reference-Guided Image Inpainting", "author": "Taorong Liu and Liang Liao and Delin Chen and Jing Xiao and Zheng Wang and Chia-Wen Lin and Shin'ichi Satoh", "abstract": "  Image inpainting for completing complicated semantic environments and diverse\nhole patterns of corrupted images is challenging even for state-of-the-art\nlearning-based inpainting methods trained on large-scale data. A reference\nimage capturing the same scene of a corrupted image offers informative guidance\nfor completing the corrupted image as it shares similar texture and structure\npriors to that of the holes of the corrupted image. In this work, we propose a\ntransformer-based encoder-decoder network, named TransRef, for reference-guided\nimage inpainting. Specifically, the guidance is conducted progressively through\na reference embedding procedure, in which the referencing features are\nsubsequently aligned and fused with the features of the corrupted image. For\nprecise utilization of the reference features for guidance, a reference-patch\nalignment (Ref-PA) module is proposed to align the patch features of the\nreference and corrupted images and harmonize their style differences, while a\nreference-patch transformer (Ref-PT) module is proposed to refine the embedded\nreference feature. Moreover, to facilitate the research of reference-guided\nimage restoration tasks, we construct a publicly accessible benchmark dataset\ncontaining 50K pairs of input and reference images. Both quantitative and\nqualitative evaluations demonstrate the efficacy of the reference information\nand the proposed method over the state-of-the-art methods in completing complex\nholes. Code and dataset can be accessed at https://github.com/Cameltr/TransRef.\n", "link": "http://arxiv.org/abs/2306.11528v4", "date": "2025-02-11", "relevancy": 2.2236, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransRef%3A%20Multi-Scale%20Reference%20Embedding%20Transformer%20for%0A%20%20Reference-Guided%20Image%20Inpainting&body=Title%3A%20TransRef%3A%20Multi-Scale%20Reference%20Embedding%20Transformer%20for%0A%20%20Reference-Guided%20Image%20Inpainting%0AAuthor%3A%20Taorong%20Liu%20and%20Liang%20Liao%20and%20Delin%20Chen%20and%20Jing%20Xiao%20and%20Zheng%20Wang%20and%20Chia-Wen%20Lin%20and%20Shin%27ichi%20Satoh%0AAbstract%3A%20%20%20Image%20inpainting%20for%20completing%20complicated%20semantic%20environments%20and%20diverse%0Ahole%20patterns%20of%20corrupted%20images%20is%20challenging%20even%20for%20state-of-the-art%0Alearning-based%20inpainting%20methods%20trained%20on%20large-scale%20data.%20A%20reference%0Aimage%20capturing%20the%20same%20scene%20of%20a%20corrupted%20image%20offers%20informative%20guidance%0Afor%20completing%20the%20corrupted%20image%20as%20it%20shares%20similar%20texture%20and%20structure%0Apriors%20to%20that%20of%20the%20holes%20of%20the%20corrupted%20image.%20In%20this%20work%2C%20we%20propose%20a%0Atransformer-based%20encoder-decoder%20network%2C%20named%20TransRef%2C%20for%20reference-guided%0Aimage%20inpainting.%20Specifically%2C%20the%20guidance%20is%20conducted%20progressively%20through%0Aa%20reference%20embedding%20procedure%2C%20in%20which%20the%20referencing%20features%20are%0Asubsequently%20aligned%20and%20fused%20with%20the%20features%20of%20the%20corrupted%20image.%20For%0Aprecise%20utilization%20of%20the%20reference%20features%20for%20guidance%2C%20a%20reference-patch%0Aalignment%20%28Ref-PA%29%20module%20is%20proposed%20to%20align%20the%20patch%20features%20of%20the%0Areference%20and%20corrupted%20images%20and%20harmonize%20their%20style%20differences%2C%20while%20a%0Areference-patch%20transformer%20%28Ref-PT%29%20module%20is%20proposed%20to%20refine%20the%20embedded%0Areference%20feature.%20Moreover%2C%20to%20facilitate%20the%20research%20of%20reference-guided%0Aimage%20restoration%20tasks%2C%20we%20construct%20a%20publicly%20accessible%20benchmark%20dataset%0Acontaining%2050K%20pairs%20of%20input%20and%20reference%20images.%20Both%20quantitative%20and%0Aqualitative%20evaluations%20demonstrate%20the%20efficacy%20of%20the%20reference%20information%0Aand%20the%20proposed%20method%20over%20the%20state-of-the-art%20methods%20in%20completing%20complex%0Aholes.%20Code%20and%20dataset%20can%20be%20accessed%20at%20https%3A//github.com/Cameltr/TransRef.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11528v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransRef%253A%2520Multi-Scale%2520Reference%2520Embedding%2520Transformer%2520for%250A%2520%2520Reference-Guided%2520Image%2520Inpainting%26entry.906535625%3DTaorong%2520Liu%2520and%2520Liang%2520Liao%2520and%2520Delin%2520Chen%2520and%2520Jing%2520Xiao%2520and%2520Zheng%2520Wang%2520and%2520Chia-Wen%2520Lin%2520and%2520Shin%2527ichi%2520Satoh%26entry.1292438233%3D%2520%2520Image%2520inpainting%2520for%2520completing%2520complicated%2520semantic%2520environments%2520and%2520diverse%250Ahole%2520patterns%2520of%2520corrupted%2520images%2520is%2520challenging%2520even%2520for%2520state-of-the-art%250Alearning-based%2520inpainting%2520methods%2520trained%2520on%2520large-scale%2520data.%2520A%2520reference%250Aimage%2520capturing%2520the%2520same%2520scene%2520of%2520a%2520corrupted%2520image%2520offers%2520informative%2520guidance%250Afor%2520completing%2520the%2520corrupted%2520image%2520as%2520it%2520shares%2520similar%2520texture%2520and%2520structure%250Apriors%2520to%2520that%2520of%2520the%2520holes%2520of%2520the%2520corrupted%2520image.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Atransformer-based%2520encoder-decoder%2520network%252C%2520named%2520TransRef%252C%2520for%2520reference-guided%250Aimage%2520inpainting.%2520Specifically%252C%2520the%2520guidance%2520is%2520conducted%2520progressively%2520through%250Aa%2520reference%2520embedding%2520procedure%252C%2520in%2520which%2520the%2520referencing%2520features%2520are%250Asubsequently%2520aligned%2520and%2520fused%2520with%2520the%2520features%2520of%2520the%2520corrupted%2520image.%2520For%250Aprecise%2520utilization%2520of%2520the%2520reference%2520features%2520for%2520guidance%252C%2520a%2520reference-patch%250Aalignment%2520%2528Ref-PA%2529%2520module%2520is%2520proposed%2520to%2520align%2520the%2520patch%2520features%2520of%2520the%250Areference%2520and%2520corrupted%2520images%2520and%2520harmonize%2520their%2520style%2520differences%252C%2520while%2520a%250Areference-patch%2520transformer%2520%2528Ref-PT%2529%2520module%2520is%2520proposed%2520to%2520refine%2520the%2520embedded%250Areference%2520feature.%2520Moreover%252C%2520to%2520facilitate%2520the%2520research%2520of%2520reference-guided%250Aimage%2520restoration%2520tasks%252C%2520we%2520construct%2520a%2520publicly%2520accessible%2520benchmark%2520dataset%250Acontaining%252050K%2520pairs%2520of%2520input%2520and%2520reference%2520images.%2520Both%2520quantitative%2520and%250Aqualitative%2520evaluations%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520reference%2520information%250Aand%2520the%2520proposed%2520method%2520over%2520the%2520state-of-the-art%2520methods%2520in%2520completing%2520complex%250Aholes.%2520Code%2520and%2520dataset%2520can%2520be%2520accessed%2520at%2520https%253A//github.com/Cameltr/TransRef.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11528v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransRef%3A%20Multi-Scale%20Reference%20Embedding%20Transformer%20for%0A%20%20Reference-Guided%20Image%20Inpainting&entry.906535625=Taorong%20Liu%20and%20Liang%20Liao%20and%20Delin%20Chen%20and%20Jing%20Xiao%20and%20Zheng%20Wang%20and%20Chia-Wen%20Lin%20and%20Shin%27ichi%20Satoh&entry.1292438233=%20%20Image%20inpainting%20for%20completing%20complicated%20semantic%20environments%20and%20diverse%0Ahole%20patterns%20of%20corrupted%20images%20is%20challenging%20even%20for%20state-of-the-art%0Alearning-based%20inpainting%20methods%20trained%20on%20large-scale%20data.%20A%20reference%0Aimage%20capturing%20the%20same%20scene%20of%20a%20corrupted%20image%20offers%20informative%20guidance%0Afor%20completing%20the%20corrupted%20image%20as%20it%20shares%20similar%20texture%20and%20structure%0Apriors%20to%20that%20of%20the%20holes%20of%20the%20corrupted%20image.%20In%20this%20work%2C%20we%20propose%20a%0Atransformer-based%20encoder-decoder%20network%2C%20named%20TransRef%2C%20for%20reference-guided%0Aimage%20inpainting.%20Specifically%2C%20the%20guidance%20is%20conducted%20progressively%20through%0Aa%20reference%20embedding%20procedure%2C%20in%20which%20the%20referencing%20features%20are%0Asubsequently%20aligned%20and%20fused%20with%20the%20features%20of%20the%20corrupted%20image.%20For%0Aprecise%20utilization%20of%20the%20reference%20features%20for%20guidance%2C%20a%20reference-patch%0Aalignment%20%28Ref-PA%29%20module%20is%20proposed%20to%20align%20the%20patch%20features%20of%20the%0Areference%20and%20corrupted%20images%20and%20harmonize%20their%20style%20differences%2C%20while%20a%0Areference-patch%20transformer%20%28Ref-PT%29%20module%20is%20proposed%20to%20refine%20the%20embedded%0Areference%20feature.%20Moreover%2C%20to%20facilitate%20the%20research%20of%20reference-guided%0Aimage%20restoration%20tasks%2C%20we%20construct%20a%20publicly%20accessible%20benchmark%20dataset%0Acontaining%2050K%20pairs%20of%20input%20and%20reference%20images.%20Both%20quantitative%20and%0Aqualitative%20evaluations%20demonstrate%20the%20efficacy%20of%20the%20reference%20information%0Aand%20the%20proposed%20method%20over%20the%20state-of-the-art%20methods%20in%20completing%20complex%0Aholes.%20Code%20and%20dataset%20can%20be%20accessed%20at%20https%3A//github.com/Cameltr/TransRef.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11528v4&entry.124074799=Read"},
{"title": "Bidirectional Uncertainty-Aware Region Learning for Semi-Supervised\n  Medical Image Segmentation", "author": "Shiwei Zhou and Haifeng Zhao and Dengdi Sun", "abstract": "  In semi-supervised medical image segmentation, the poor quality of unlabeled\ndata and the uncertainty in the model's predictions lead to models that\ninevitably produce erroneous pseudo-labels. These errors accumulate throughout\nmodel training, thereby weakening the model's performance. We found that these\nerroneous pseudo-labels are typically concentrated in high-uncertainty regions.\nTraditional methods improve performance by directly discarding pseudo-labels in\nthese regions, but this can also result in neglecting potentially valuable\ntraining data. To alleviate this problem, we propose a bidirectional\nuncertainty-aware region learning strategy. In training labeled data, we focus\non high-uncertainty regions, using precise label information to guide the\nmodel's learning in potentially uncontrollable areas. Meanwhile, in the\ntraining of unlabeled data, we concentrate on low-uncertainty regions to reduce\nthe interference of erroneous pseudo-labels on the model. Through this\nbidirectional learning strategy, the model's overall performance has\nsignificantly improved. Extensive experiments show that our proposed method\nachieves significant performance improvement on different medical image\nsegmentation tasks.\n", "link": "http://arxiv.org/abs/2502.07457v1", "date": "2025-02-11", "relevancy": 2.22, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5661}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Uncertainty-Aware%20Region%20Learning%20for%20Semi-Supervised%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20Bidirectional%20Uncertainty-Aware%20Region%20Learning%20for%20Semi-Supervised%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Shiwei%20Zhou%20and%20Haifeng%20Zhao%20and%20Dengdi%20Sun%0AAbstract%3A%20%20%20In%20semi-supervised%20medical%20image%20segmentation%2C%20the%20poor%20quality%20of%20unlabeled%0Adata%20and%20the%20uncertainty%20in%20the%20model%27s%20predictions%20lead%20to%20models%20that%0Ainevitably%20produce%20erroneous%20pseudo-labels.%20These%20errors%20accumulate%20throughout%0Amodel%20training%2C%20thereby%20weakening%20the%20model%27s%20performance.%20We%20found%20that%20these%0Aerroneous%20pseudo-labels%20are%20typically%20concentrated%20in%20high-uncertainty%20regions.%0ATraditional%20methods%20improve%20performance%20by%20directly%20discarding%20pseudo-labels%20in%0Athese%20regions%2C%20but%20this%20can%20also%20result%20in%20neglecting%20potentially%20valuable%0Atraining%20data.%20To%20alleviate%20this%20problem%2C%20we%20propose%20a%20bidirectional%0Auncertainty-aware%20region%20learning%20strategy.%20In%20training%20labeled%20data%2C%20we%20focus%0Aon%20high-uncertainty%20regions%2C%20using%20precise%20label%20information%20to%20guide%20the%0Amodel%27s%20learning%20in%20potentially%20uncontrollable%20areas.%20Meanwhile%2C%20in%20the%0Atraining%20of%20unlabeled%20data%2C%20we%20concentrate%20on%20low-uncertainty%20regions%20to%20reduce%0Athe%20interference%20of%20erroneous%20pseudo-labels%20on%20the%20model.%20Through%20this%0Abidirectional%20learning%20strategy%2C%20the%20model%27s%20overall%20performance%20has%0Asignificantly%20improved.%20Extensive%20experiments%20show%20that%20our%20proposed%20method%0Aachieves%20significant%20performance%20improvement%20on%20different%20medical%20image%0Asegmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBidirectional%2520Uncertainty-Aware%2520Region%2520Learning%2520for%2520Semi-Supervised%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DShiwei%2520Zhou%2520and%2520Haifeng%2520Zhao%2520and%2520Dengdi%2520Sun%26entry.1292438233%3D%2520%2520In%2520semi-supervised%2520medical%2520image%2520segmentation%252C%2520the%2520poor%2520quality%2520of%2520unlabeled%250Adata%2520and%2520the%2520uncertainty%2520in%2520the%2520model%2527s%2520predictions%2520lead%2520to%2520models%2520that%250Ainevitably%2520produce%2520erroneous%2520pseudo-labels.%2520These%2520errors%2520accumulate%2520throughout%250Amodel%2520training%252C%2520thereby%2520weakening%2520the%2520model%2527s%2520performance.%2520We%2520found%2520that%2520these%250Aerroneous%2520pseudo-labels%2520are%2520typically%2520concentrated%2520in%2520high-uncertainty%2520regions.%250ATraditional%2520methods%2520improve%2520performance%2520by%2520directly%2520discarding%2520pseudo-labels%2520in%250Athese%2520regions%252C%2520but%2520this%2520can%2520also%2520result%2520in%2520neglecting%2520potentially%2520valuable%250Atraining%2520data.%2520To%2520alleviate%2520this%2520problem%252C%2520we%2520propose%2520a%2520bidirectional%250Auncertainty-aware%2520region%2520learning%2520strategy.%2520In%2520training%2520labeled%2520data%252C%2520we%2520focus%250Aon%2520high-uncertainty%2520regions%252C%2520using%2520precise%2520label%2520information%2520to%2520guide%2520the%250Amodel%2527s%2520learning%2520in%2520potentially%2520uncontrollable%2520areas.%2520Meanwhile%252C%2520in%2520the%250Atraining%2520of%2520unlabeled%2520data%252C%2520we%2520concentrate%2520on%2520low-uncertainty%2520regions%2520to%2520reduce%250Athe%2520interference%2520of%2520erroneous%2520pseudo-labels%2520on%2520the%2520model.%2520Through%2520this%250Abidirectional%2520learning%2520strategy%252C%2520the%2520model%2527s%2520overall%2520performance%2520has%250Asignificantly%2520improved.%2520Extensive%2520experiments%2520show%2520that%2520our%2520proposed%2520method%250Aachieves%2520significant%2520performance%2520improvement%2520on%2520different%2520medical%2520image%250Asegmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Uncertainty-Aware%20Region%20Learning%20for%20Semi-Supervised%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Shiwei%20Zhou%20and%20Haifeng%20Zhao%20and%20Dengdi%20Sun&entry.1292438233=%20%20In%20semi-supervised%20medical%20image%20segmentation%2C%20the%20poor%20quality%20of%20unlabeled%0Adata%20and%20the%20uncertainty%20in%20the%20model%27s%20predictions%20lead%20to%20models%20that%0Ainevitably%20produce%20erroneous%20pseudo-labels.%20These%20errors%20accumulate%20throughout%0Amodel%20training%2C%20thereby%20weakening%20the%20model%27s%20performance.%20We%20found%20that%20these%0Aerroneous%20pseudo-labels%20are%20typically%20concentrated%20in%20high-uncertainty%20regions.%0ATraditional%20methods%20improve%20performance%20by%20directly%20discarding%20pseudo-labels%20in%0Athese%20regions%2C%20but%20this%20can%20also%20result%20in%20neglecting%20potentially%20valuable%0Atraining%20data.%20To%20alleviate%20this%20problem%2C%20we%20propose%20a%20bidirectional%0Auncertainty-aware%20region%20learning%20strategy.%20In%20training%20labeled%20data%2C%20we%20focus%0Aon%20high-uncertainty%20regions%2C%20using%20precise%20label%20information%20to%20guide%20the%0Amodel%27s%20learning%20in%20potentially%20uncontrollable%20areas.%20Meanwhile%2C%20in%20the%0Atraining%20of%20unlabeled%20data%2C%20we%20concentrate%20on%20low-uncertainty%20regions%20to%20reduce%0Athe%20interference%20of%20erroneous%20pseudo-labels%20on%20the%20model.%20Through%20this%0Abidirectional%20learning%20strategy%2C%20the%20model%27s%20overall%20performance%20has%0Asignificantly%20improved.%20Extensive%20experiments%20show%20that%20our%20proposed%20method%0Aachieves%20significant%20performance%20improvement%20on%20different%20medical%20image%0Asegmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07457v1&entry.124074799=Read"},
{"title": "Next Block Prediction: Video Generation via Semi-Auto-Regressive\n  Modeling", "author": "Shuhuai Ren and Shuming Ma and Xu Sun and Furu Wei", "abstract": "  Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.\n", "link": "http://arxiv.org/abs/2502.07737v1", "date": "2025-02-11", "relevancy": 2.2115, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5569}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next%20Block%20Prediction%3A%20Video%20Generation%20via%20Semi-Auto-Regressive%0A%20%20Modeling&body=Title%3A%20Next%20Block%20Prediction%3A%20Video%20Generation%20via%20Semi-Auto-Regressive%0A%20%20Modeling%0AAuthor%3A%20Shuhuai%20Ren%20and%20Shuming%20Ma%20and%20Xu%20Sun%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Next-Token%20Prediction%20%28NTP%29%20is%20a%20de%20facto%20approach%20for%20autoregressive%20%28AR%29%0Avideo%20generation%2C%20but%20it%20suffers%20from%20suboptimal%20unidirectional%20dependencies%0Aand%20slow%20inference%20speed.%20In%20this%20work%2C%20we%20propose%20a%20semi-autoregressive%0A%28semi-AR%29%20framework%2C%20called%20Next-Block%20Prediction%20%28NBP%29%2C%20for%20video%20generation.%0ABy%20uniformly%20decomposing%20video%20content%20into%20equal-sized%20blocks%20%28e.g.%2C%20rows%20or%0Aframes%29%2C%20we%20shift%20the%20generation%20unit%20from%20individual%20tokens%20to%20blocks%2C%0Aallowing%20each%20token%20in%20the%20current%20block%20to%20simultaneously%20predict%20the%0Acorresponding%20token%20in%20the%20next%20block.%20Unlike%20traditional%20AR%20modeling%2C%20our%0Aframework%20employs%20bidirectional%20attention%20within%20each%20block%2C%20enabling%20tokens%20to%0Acapture%20more%20robust%20spatial%20dependencies.%20By%20predicting%20multiple%20tokens%20in%0Aparallel%2C%20NBP%20models%20significantly%20reduce%20the%20number%20of%20generation%20steps%2C%0Aleading%20to%20faster%20and%20more%20efficient%20inference.%20Our%20model%20achieves%20FVD%20scores%0Aof%20103.3%20on%20UCF101%20and%2025.5%20on%20K600%2C%20outperforming%20the%20vanilla%20NTP%20model%20by%20an%0Aaverage%20of%204.4.%20Furthermore%2C%20thanks%20to%20the%20reduced%20number%20of%20inference%20steps%2C%0Athe%20NBP%20model%20generates%208.89%20frames%20%28128x128%20resolution%29%20per%20second%2C%20achieving%0Aan%2011x%20speedup.%20We%20also%20explored%20model%20scales%20ranging%20from%20700M%20to%203B%0Aparameters%2C%20observing%20significant%20improvements%20in%20generation%20quality%2C%20with%20FVD%0Ascores%20dropping%20from%20103.3%20to%2055.3%20on%20UCF101%20and%20from%2025.5%20to%2019.5%20on%20K600%2C%0Ademonstrating%20the%20scalability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext%2520Block%2520Prediction%253A%2520Video%2520Generation%2520via%2520Semi-Auto-Regressive%250A%2520%2520Modeling%26entry.906535625%3DShuhuai%2520Ren%2520and%2520Shuming%2520Ma%2520and%2520Xu%2520Sun%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Next-Token%2520Prediction%2520%2528NTP%2529%2520is%2520a%2520de%2520facto%2520approach%2520for%2520autoregressive%2520%2528AR%2529%250Avideo%2520generation%252C%2520but%2520it%2520suffers%2520from%2520suboptimal%2520unidirectional%2520dependencies%250Aand%2520slow%2520inference%2520speed.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520semi-autoregressive%250A%2528semi-AR%2529%2520framework%252C%2520called%2520Next-Block%2520Prediction%2520%2528NBP%2529%252C%2520for%2520video%2520generation.%250ABy%2520uniformly%2520decomposing%2520video%2520content%2520into%2520equal-sized%2520blocks%2520%2528e.g.%252C%2520rows%2520or%250Aframes%2529%252C%2520we%2520shift%2520the%2520generation%2520unit%2520from%2520individual%2520tokens%2520to%2520blocks%252C%250Aallowing%2520each%2520token%2520in%2520the%2520current%2520block%2520to%2520simultaneously%2520predict%2520the%250Acorresponding%2520token%2520in%2520the%2520next%2520block.%2520Unlike%2520traditional%2520AR%2520modeling%252C%2520our%250Aframework%2520employs%2520bidirectional%2520attention%2520within%2520each%2520block%252C%2520enabling%2520tokens%2520to%250Acapture%2520more%2520robust%2520spatial%2520dependencies.%2520By%2520predicting%2520multiple%2520tokens%2520in%250Aparallel%252C%2520NBP%2520models%2520significantly%2520reduce%2520the%2520number%2520of%2520generation%2520steps%252C%250Aleading%2520to%2520faster%2520and%2520more%2520efficient%2520inference.%2520Our%2520model%2520achieves%2520FVD%2520scores%250Aof%2520103.3%2520on%2520UCF101%2520and%252025.5%2520on%2520K600%252C%2520outperforming%2520the%2520vanilla%2520NTP%2520model%2520by%2520an%250Aaverage%2520of%25204.4.%2520Furthermore%252C%2520thanks%2520to%2520the%2520reduced%2520number%2520of%2520inference%2520steps%252C%250Athe%2520NBP%2520model%2520generates%25208.89%2520frames%2520%2528128x128%2520resolution%2529%2520per%2520second%252C%2520achieving%250Aan%252011x%2520speedup.%2520We%2520also%2520explored%2520model%2520scales%2520ranging%2520from%2520700M%2520to%25203B%250Aparameters%252C%2520observing%2520significant%2520improvements%2520in%2520generation%2520quality%252C%2520with%2520FVD%250Ascores%2520dropping%2520from%2520103.3%2520to%252055.3%2520on%2520UCF101%2520and%2520from%252025.5%2520to%252019.5%2520on%2520K600%252C%250Ademonstrating%2520the%2520scalability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next%20Block%20Prediction%3A%20Video%20Generation%20via%20Semi-Auto-Regressive%0A%20%20Modeling&entry.906535625=Shuhuai%20Ren%20and%20Shuming%20Ma%20and%20Xu%20Sun%20and%20Furu%20Wei&entry.1292438233=%20%20Next-Token%20Prediction%20%28NTP%29%20is%20a%20de%20facto%20approach%20for%20autoregressive%20%28AR%29%0Avideo%20generation%2C%20but%20it%20suffers%20from%20suboptimal%20unidirectional%20dependencies%0Aand%20slow%20inference%20speed.%20In%20this%20work%2C%20we%20propose%20a%20semi-autoregressive%0A%28semi-AR%29%20framework%2C%20called%20Next-Block%20Prediction%20%28NBP%29%2C%20for%20video%20generation.%0ABy%20uniformly%20decomposing%20video%20content%20into%20equal-sized%20blocks%20%28e.g.%2C%20rows%20or%0Aframes%29%2C%20we%20shift%20the%20generation%20unit%20from%20individual%20tokens%20to%20blocks%2C%0Aallowing%20each%20token%20in%20the%20current%20block%20to%20simultaneously%20predict%20the%0Acorresponding%20token%20in%20the%20next%20block.%20Unlike%20traditional%20AR%20modeling%2C%20our%0Aframework%20employs%20bidirectional%20attention%20within%20each%20block%2C%20enabling%20tokens%20to%0Acapture%20more%20robust%20spatial%20dependencies.%20By%20predicting%20multiple%20tokens%20in%0Aparallel%2C%20NBP%20models%20significantly%20reduce%20the%20number%20of%20generation%20steps%2C%0Aleading%20to%20faster%20and%20more%20efficient%20inference.%20Our%20model%20achieves%20FVD%20scores%0Aof%20103.3%20on%20UCF101%20and%2025.5%20on%20K600%2C%20outperforming%20the%20vanilla%20NTP%20model%20by%20an%0Aaverage%20of%204.4.%20Furthermore%2C%20thanks%20to%20the%20reduced%20number%20of%20inference%20steps%2C%0Athe%20NBP%20model%20generates%208.89%20frames%20%28128x128%20resolution%29%20per%20second%2C%20achieving%0Aan%2011x%20speedup.%20We%20also%20explored%20model%20scales%20ranging%20from%20700M%20to%203B%0Aparameters%2C%20observing%20significant%20improvements%20in%20generation%20quality%2C%20with%20FVD%0Ascores%20dropping%20from%20103.3%20to%2055.3%20on%20UCF101%20and%20from%2025.5%20to%2019.5%20on%20K600%2C%0Ademonstrating%20the%20scalability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07737v1&entry.124074799=Read"},
{"title": "Exploring Few-Shot Defect Segmentation in General Industrial Scenarios\n  with Metric Learning and Vision Foundation Models", "author": "Tongkun Liu and Bing Li and Xiao Jin and Yupeng Shi and Qiuying Li and Xiang Wei", "abstract": "  Industrial defect segmentation is critical for manufacturing quality control.\nDue to the scarcity of training defect samples, few-shot semantic segmentation\n(FSS) holds significant value in this field. However, existing studies mostly\napply FSS to tackle defects on simple textures, without considering more\ndiverse scenarios. This paper aims to address this gap by exploring FSS in\nbroader industrial products with various defect types. To this end, we\ncontribute a new real-world dataset and reorganize some existing datasets to\nbuild a more comprehensive few-shot defect segmentation (FDS) benchmark. On\nthis benchmark, we thoroughly investigate metric learning-based FSS methods,\nincluding those based on meta-learning and those based on Vision Foundation\nModels (VFMs). We observe that existing meta-learning-based methods are\ngenerally not well-suited for this task, while VFMs hold great potential. We\nfurther systematically study the applicability of various VFMs in this task,\ninvolving two paradigms: feature matching and the use of Segment Anything (SAM)\nmodels. We propose a novel efficient FDS method based on feature matching.\nMeanwhile, we find that SAM2 is particularly effective for addressing FDS\nthrough its video track mode. The contributed dataset and code will be\navailable at: https://github.com/liutongkun/GFDS.\n", "link": "http://arxiv.org/abs/2502.01216v2", "date": "2025-02-11", "relevancy": 2.2106, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Few-Shot%20Defect%20Segmentation%20in%20General%20Industrial%20Scenarios%0A%20%20with%20Metric%20Learning%20and%20Vision%20Foundation%20Models&body=Title%3A%20Exploring%20Few-Shot%20Defect%20Segmentation%20in%20General%20Industrial%20Scenarios%0A%20%20with%20Metric%20Learning%20and%20Vision%20Foundation%20Models%0AAuthor%3A%20Tongkun%20Liu%20and%20Bing%20Li%20and%20Xiao%20Jin%20and%20Yupeng%20Shi%20and%20Qiuying%20Li%20and%20Xiang%20Wei%0AAbstract%3A%20%20%20Industrial%20defect%20segmentation%20is%20critical%20for%20manufacturing%20quality%20control.%0ADue%20to%20the%20scarcity%20of%20training%20defect%20samples%2C%20few-shot%20semantic%20segmentation%0A%28FSS%29%20holds%20significant%20value%20in%20this%20field.%20However%2C%20existing%20studies%20mostly%0Aapply%20FSS%20to%20tackle%20defects%20on%20simple%20textures%2C%20without%20considering%20more%0Adiverse%20scenarios.%20This%20paper%20aims%20to%20address%20this%20gap%20by%20exploring%20FSS%20in%0Abroader%20industrial%20products%20with%20various%20defect%20types.%20To%20this%20end%2C%20we%0Acontribute%20a%20new%20real-world%20dataset%20and%20reorganize%20some%20existing%20datasets%20to%0Abuild%20a%20more%20comprehensive%20few-shot%20defect%20segmentation%20%28FDS%29%20benchmark.%20On%0Athis%20benchmark%2C%20we%20thoroughly%20investigate%20metric%20learning-based%20FSS%20methods%2C%0Aincluding%20those%20based%20on%20meta-learning%20and%20those%20based%20on%20Vision%20Foundation%0AModels%20%28VFMs%29.%20We%20observe%20that%20existing%20meta-learning-based%20methods%20are%0Agenerally%20not%20well-suited%20for%20this%20task%2C%20while%20VFMs%20hold%20great%20potential.%20We%0Afurther%20systematically%20study%20the%20applicability%20of%20various%20VFMs%20in%20this%20task%2C%0Ainvolving%20two%20paradigms%3A%20feature%20matching%20and%20the%20use%20of%20Segment%20Anything%20%28SAM%29%0Amodels.%20We%20propose%20a%20novel%20efficient%20FDS%20method%20based%20on%20feature%20matching.%0AMeanwhile%2C%20we%20find%20that%20SAM2%20is%20particularly%20effective%20for%20addressing%20FDS%0Athrough%20its%20video%20track%20mode.%20The%20contributed%20dataset%20and%20code%20will%20be%0Aavailable%20at%3A%20https%3A//github.com/liutongkun/GFDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Few-Shot%2520Defect%2520Segmentation%2520in%2520General%2520Industrial%2520Scenarios%250A%2520%2520with%2520Metric%2520Learning%2520and%2520Vision%2520Foundation%2520Models%26entry.906535625%3DTongkun%2520Liu%2520and%2520Bing%2520Li%2520and%2520Xiao%2520Jin%2520and%2520Yupeng%2520Shi%2520and%2520Qiuying%2520Li%2520and%2520Xiang%2520Wei%26entry.1292438233%3D%2520%2520Industrial%2520defect%2520segmentation%2520is%2520critical%2520for%2520manufacturing%2520quality%2520control.%250ADue%2520to%2520the%2520scarcity%2520of%2520training%2520defect%2520samples%252C%2520few-shot%2520semantic%2520segmentation%250A%2528FSS%2529%2520holds%2520significant%2520value%2520in%2520this%2520field.%2520However%252C%2520existing%2520studies%2520mostly%250Aapply%2520FSS%2520to%2520tackle%2520defects%2520on%2520simple%2520textures%252C%2520without%2520considering%2520more%250Adiverse%2520scenarios.%2520This%2520paper%2520aims%2520to%2520address%2520this%2520gap%2520by%2520exploring%2520FSS%2520in%250Abroader%2520industrial%2520products%2520with%2520various%2520defect%2520types.%2520To%2520this%2520end%252C%2520we%250Acontribute%2520a%2520new%2520real-world%2520dataset%2520and%2520reorganize%2520some%2520existing%2520datasets%2520to%250Abuild%2520a%2520more%2520comprehensive%2520few-shot%2520defect%2520segmentation%2520%2528FDS%2529%2520benchmark.%2520On%250Athis%2520benchmark%252C%2520we%2520thoroughly%2520investigate%2520metric%2520learning-based%2520FSS%2520methods%252C%250Aincluding%2520those%2520based%2520on%2520meta-learning%2520and%2520those%2520based%2520on%2520Vision%2520Foundation%250AModels%2520%2528VFMs%2529.%2520We%2520observe%2520that%2520existing%2520meta-learning-based%2520methods%2520are%250Agenerally%2520not%2520well-suited%2520for%2520this%2520task%252C%2520while%2520VFMs%2520hold%2520great%2520potential.%2520We%250Afurther%2520systematically%2520study%2520the%2520applicability%2520of%2520various%2520VFMs%2520in%2520this%2520task%252C%250Ainvolving%2520two%2520paradigms%253A%2520feature%2520matching%2520and%2520the%2520use%2520of%2520Segment%2520Anything%2520%2528SAM%2529%250Amodels.%2520We%2520propose%2520a%2520novel%2520efficient%2520FDS%2520method%2520based%2520on%2520feature%2520matching.%250AMeanwhile%252C%2520we%2520find%2520that%2520SAM2%2520is%2520particularly%2520effective%2520for%2520addressing%2520FDS%250Athrough%2520its%2520video%2520track%2520mode.%2520The%2520contributed%2520dataset%2520and%2520code%2520will%2520be%250Aavailable%2520at%253A%2520https%253A//github.com/liutongkun/GFDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Few-Shot%20Defect%20Segmentation%20in%20General%20Industrial%20Scenarios%0A%20%20with%20Metric%20Learning%20and%20Vision%20Foundation%20Models&entry.906535625=Tongkun%20Liu%20and%20Bing%20Li%20and%20Xiao%20Jin%20and%20Yupeng%20Shi%20and%20Qiuying%20Li%20and%20Xiang%20Wei&entry.1292438233=%20%20Industrial%20defect%20segmentation%20is%20critical%20for%20manufacturing%20quality%20control.%0ADue%20to%20the%20scarcity%20of%20training%20defect%20samples%2C%20few-shot%20semantic%20segmentation%0A%28FSS%29%20holds%20significant%20value%20in%20this%20field.%20However%2C%20existing%20studies%20mostly%0Aapply%20FSS%20to%20tackle%20defects%20on%20simple%20textures%2C%20without%20considering%20more%0Adiverse%20scenarios.%20This%20paper%20aims%20to%20address%20this%20gap%20by%20exploring%20FSS%20in%0Abroader%20industrial%20products%20with%20various%20defect%20types.%20To%20this%20end%2C%20we%0Acontribute%20a%20new%20real-world%20dataset%20and%20reorganize%20some%20existing%20datasets%20to%0Abuild%20a%20more%20comprehensive%20few-shot%20defect%20segmentation%20%28FDS%29%20benchmark.%20On%0Athis%20benchmark%2C%20we%20thoroughly%20investigate%20metric%20learning-based%20FSS%20methods%2C%0Aincluding%20those%20based%20on%20meta-learning%20and%20those%20based%20on%20Vision%20Foundation%0AModels%20%28VFMs%29.%20We%20observe%20that%20existing%20meta-learning-based%20methods%20are%0Agenerally%20not%20well-suited%20for%20this%20task%2C%20while%20VFMs%20hold%20great%20potential.%20We%0Afurther%20systematically%20study%20the%20applicability%20of%20various%20VFMs%20in%20this%20task%2C%0Ainvolving%20two%20paradigms%3A%20feature%20matching%20and%20the%20use%20of%20Segment%20Anything%20%28SAM%29%0Amodels.%20We%20propose%20a%20novel%20efficient%20FDS%20method%20based%20on%20feature%20matching.%0AMeanwhile%2C%20we%20find%20that%20SAM2%20is%20particularly%20effective%20for%20addressing%20FDS%0Athrough%20its%20video%20track%20mode.%20The%20contributed%20dataset%20and%20code%20will%20be%0Aavailable%20at%3A%20https%3A//github.com/liutongkun/GFDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01216v2&entry.124074799=Read"},
{"title": "Automated Capability Discovery via Model Self-Exploration", "author": "Cong Lu and Shengran Hu and Jeff Clune", "abstract": "  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n", "link": "http://arxiv.org/abs/2502.07577v1", "date": "2025-02-11", "relevancy": 2.2094, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Capability%20Discovery%20via%20Model%20Self-Exploration&body=Title%3A%20Automated%20Capability%20Discovery%20via%20Model%20Self-Exploration%0AAuthor%3A%20Cong%20Lu%20and%20Shengran%20Hu%20and%20Jeff%20Clune%0AAbstract%3A%20%20%20Foundation%20models%20have%20become%20general-purpose%20assistants%2C%20exhibiting%20diverse%0Acapabilities%20across%20numerous%20domains%20through%20training%20on%20web-scale%20data.%20It%0Aremains%20challenging%20to%20precisely%20characterize%20even%20a%20fraction%20of%20the%20full%0Aspectrum%20of%20capabilities%20and%20potential%20risks%20in%20any%20new%20model.%20Existing%0Aevaluation%20approaches%20often%20require%20significant%20human%20effort%2C%20and%20it%20is%20taking%0Aincreasing%20effort%20to%20design%20ever%20harder%20challenges%20for%20more%20capable%20models.%20We%0Aintroduce%20Automated%20Capability%20Discovery%20%28ACD%29%2C%20a%20framework%20that%20designates%20one%0Afoundation%20model%20as%20a%20scientist%20to%20systematically%20propose%20open-ended%20tasks%0Aprobing%20the%20abilities%20of%20a%20subject%20model%20%28potentially%20itself%29.%20By%20combining%0Afrontier%20models%20with%20ideas%20from%20the%20field%20of%20open-endedness%2C%20ACD%20automatically%0Aand%20systematically%20uncovers%20both%20surprising%20capabilities%20and%20failures%20in%20the%0Asubject%20model.%20We%20demonstrate%20ACD%20across%20a%20range%20of%20foundation%20models%0A%28including%20the%20GPT%2C%20Claude%2C%20and%20Llama%20series%29%2C%20showing%20that%20it%20automatically%0Areveals%20thousands%20of%20capabilities%20that%20would%20be%20challenging%20for%20any%20single%20team%0Ato%20uncover.%20We%20further%20validate%20our%20method%27s%20automated%20scoring%20with%20extensive%0Ahuman%20surveys%2C%20observing%20high%20agreement%20between%20model-generated%20and%20human%0Aevaluations.%20By%20leveraging%20foundation%20models%27%20ability%20to%20both%20create%20tasks%20and%0Aself-evaluate%2C%20ACD%20is%20a%20significant%20step%20toward%20scalable%2C%20automated%20evaluation%0Aof%20novel%20AI%20systems.%20All%20code%20and%20evaluation%20logs%20are%20open-sourced%20at%0Ahttps%3A//github.com/conglu1997/ACD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Capability%2520Discovery%2520via%2520Model%2520Self-Exploration%26entry.906535625%3DCong%2520Lu%2520and%2520Shengran%2520Hu%2520and%2520Jeff%2520Clune%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520become%2520general-purpose%2520assistants%252C%2520exhibiting%2520diverse%250Acapabilities%2520across%2520numerous%2520domains%2520through%2520training%2520on%2520web-scale%2520data.%2520It%250Aremains%2520challenging%2520to%2520precisely%2520characterize%2520even%2520a%2520fraction%2520of%2520the%2520full%250Aspectrum%2520of%2520capabilities%2520and%2520potential%2520risks%2520in%2520any%2520new%2520model.%2520Existing%250Aevaluation%2520approaches%2520often%2520require%2520significant%2520human%2520effort%252C%2520and%2520it%2520is%2520taking%250Aincreasing%2520effort%2520to%2520design%2520ever%2520harder%2520challenges%2520for%2520more%2520capable%2520models.%2520We%250Aintroduce%2520Automated%2520Capability%2520Discovery%2520%2528ACD%2529%252C%2520a%2520framework%2520that%2520designates%2520one%250Afoundation%2520model%2520as%2520a%2520scientist%2520to%2520systematically%2520propose%2520open-ended%2520tasks%250Aprobing%2520the%2520abilities%2520of%2520a%2520subject%2520model%2520%2528potentially%2520itself%2529.%2520By%2520combining%250Afrontier%2520models%2520with%2520ideas%2520from%2520the%2520field%2520of%2520open-endedness%252C%2520ACD%2520automatically%250Aand%2520systematically%2520uncovers%2520both%2520surprising%2520capabilities%2520and%2520failures%2520in%2520the%250Asubject%2520model.%2520We%2520demonstrate%2520ACD%2520across%2520a%2520range%2520of%2520foundation%2520models%250A%2528including%2520the%2520GPT%252C%2520Claude%252C%2520and%2520Llama%2520series%2529%252C%2520showing%2520that%2520it%2520automatically%250Areveals%2520thousands%2520of%2520capabilities%2520that%2520would%2520be%2520challenging%2520for%2520any%2520single%2520team%250Ato%2520uncover.%2520We%2520further%2520validate%2520our%2520method%2527s%2520automated%2520scoring%2520with%2520extensive%250Ahuman%2520surveys%252C%2520observing%2520high%2520agreement%2520between%2520model-generated%2520and%2520human%250Aevaluations.%2520By%2520leveraging%2520foundation%2520models%2527%2520ability%2520to%2520both%2520create%2520tasks%2520and%250Aself-evaluate%252C%2520ACD%2520is%2520a%2520significant%2520step%2520toward%2520scalable%252C%2520automated%2520evaluation%250Aof%2520novel%2520AI%2520systems.%2520All%2520code%2520and%2520evaluation%2520logs%2520are%2520open-sourced%2520at%250Ahttps%253A//github.com/conglu1997/ACD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Capability%20Discovery%20via%20Model%20Self-Exploration&entry.906535625=Cong%20Lu%20and%20Shengran%20Hu%20and%20Jeff%20Clune&entry.1292438233=%20%20Foundation%20models%20have%20become%20general-purpose%20assistants%2C%20exhibiting%20diverse%0Acapabilities%20across%20numerous%20domains%20through%20training%20on%20web-scale%20data.%20It%0Aremains%20challenging%20to%20precisely%20characterize%20even%20a%20fraction%20of%20the%20full%0Aspectrum%20of%20capabilities%20and%20potential%20risks%20in%20any%20new%20model.%20Existing%0Aevaluation%20approaches%20often%20require%20significant%20human%20effort%2C%20and%20it%20is%20taking%0Aincreasing%20effort%20to%20design%20ever%20harder%20challenges%20for%20more%20capable%20models.%20We%0Aintroduce%20Automated%20Capability%20Discovery%20%28ACD%29%2C%20a%20framework%20that%20designates%20one%0Afoundation%20model%20as%20a%20scientist%20to%20systematically%20propose%20open-ended%20tasks%0Aprobing%20the%20abilities%20of%20a%20subject%20model%20%28potentially%20itself%29.%20By%20combining%0Afrontier%20models%20with%20ideas%20from%20the%20field%20of%20open-endedness%2C%20ACD%20automatically%0Aand%20systematically%20uncovers%20both%20surprising%20capabilities%20and%20failures%20in%20the%0Asubject%20model.%20We%20demonstrate%20ACD%20across%20a%20range%20of%20foundation%20models%0A%28including%20the%20GPT%2C%20Claude%2C%20and%20Llama%20series%29%2C%20showing%20that%20it%20automatically%0Areveals%20thousands%20of%20capabilities%20that%20would%20be%20challenging%20for%20any%20single%20team%0Ato%20uncover.%20We%20further%20validate%20our%20method%27s%20automated%20scoring%20with%20extensive%0Ahuman%20surveys%2C%20observing%20high%20agreement%20between%20model-generated%20and%20human%0Aevaluations.%20By%20leveraging%20foundation%20models%27%20ability%20to%20both%20create%20tasks%20and%0Aself-evaluate%2C%20ACD%20is%20a%20significant%20step%20toward%20scalable%2C%20automated%20evaluation%0Aof%20novel%20AI%20systems.%20All%20code%20and%20evaluation%20logs%20are%20open-sourced%20at%0Ahttps%3A//github.com/conglu1997/ACD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07577v1&entry.124074799=Read"},
{"title": "The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation", "author": "Raman Dutt", "abstract": "  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset.\n", "link": "http://arxiv.org/abs/2502.07516v1", "date": "2025-02-11", "relevancy": 2.1952, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5625}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5449}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20the%20Prompts%3A%20De-Identification%20Traces%20Enhance%0A%20%20Memorization%20Risks%20in%20Synthetic%20Chest%20X-Ray%20Generation&body=Title%3A%20The%20Devil%20is%20in%20the%20Prompts%3A%20De-Identification%20Traces%20Enhance%0A%20%20Memorization%20Risks%20in%20Synthetic%20Chest%20X-Ray%20Generation%0AAuthor%3A%20Raman%20Dutt%0AAbstract%3A%20%20%20Generative%20models%2C%20particularly%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20play%20a%0Acrucial%20role%20in%20medical%20image%20analysis.%20However%2C%20these%20models%20are%20prone%20to%0Atraining%20data%20memorization%2C%20posing%20significant%20risks%20to%20patient%20privacy.%0ASynthetic%20chest%20X-ray%20generation%20is%20one%20of%20the%20most%20common%20applications%20in%0Amedical%20image%20analysis%20with%20the%20MIMIC-CXR%20dataset%20serving%20as%20the%20primary%20data%0Arepository%20for%20this%20task.%20This%20study%20adopts%20a%20data-driven%20approach%20and%20presents%0Athe%20first%20systematic%20attempt%20to%20identify%20prompts%20and%20text%20tokens%20in%20MIMIC-CXR%0Athat%20contribute%20the%20most%20to%20training%20data%20memorization.%20Our%20analysis%20reveals%20an%0Aunexpected%20finding%3A%20prompts%20containing%20traces%20of%20de-identification%20procedures%0Aare%20among%20the%20most%20memorized%2C%20with%20de-identification%20markers%20contributing%20the%0Amost.%20Furthermore%2C%20we%20also%20find%20existing%20inference-time%20memorization%20mitigation%0Astrategies%20are%20ineffective%20and%20fail%20to%20sufficiently%20reduce%20the%20model%27s%20reliance%0Aon%20memorized%20text%20tokens%20highlighting%20a%20broader%20issue%20in%20T2I%20synthesis%20with%0AMIMIC-CXR.%20On%20this%20front%2C%20we%20propose%20actionable%20strategies%20to%20enhance%20privacy%0Aand%20improve%20the%20reliability%20of%20generative%20models%20in%20medical%20imaging.%20Finally%2C%0Aour%20results%20provide%20a%20foundation%20for%20future%20work%20on%20developing%20and%20benchmarking%0Amemorization%20mitigation%20techniques%20for%20synthetic%20chest%20X-ray%20generation%20using%0Athe%20MIMIC-CXR%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Devil%2520is%2520in%2520the%2520Prompts%253A%2520De-Identification%2520Traces%2520Enhance%250A%2520%2520Memorization%2520Risks%2520in%2520Synthetic%2520Chest%2520X-Ray%2520Generation%26entry.906535625%3DRaman%2520Dutt%26entry.1292438233%3D%2520%2520Generative%2520models%252C%2520particularly%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%252C%2520play%2520a%250Acrucial%2520role%2520in%2520medical%2520image%2520analysis.%2520However%252C%2520these%2520models%2520are%2520prone%2520to%250Atraining%2520data%2520memorization%252C%2520posing%2520significant%2520risks%2520to%2520patient%2520privacy.%250ASynthetic%2520chest%2520X-ray%2520generation%2520is%2520one%2520of%2520the%2520most%2520common%2520applications%2520in%250Amedical%2520image%2520analysis%2520with%2520the%2520MIMIC-CXR%2520dataset%2520serving%2520as%2520the%2520primary%2520data%250Arepository%2520for%2520this%2520task.%2520This%2520study%2520adopts%2520a%2520data-driven%2520approach%2520and%2520presents%250Athe%2520first%2520systematic%2520attempt%2520to%2520identify%2520prompts%2520and%2520text%2520tokens%2520in%2520MIMIC-CXR%250Athat%2520contribute%2520the%2520most%2520to%2520training%2520data%2520memorization.%2520Our%2520analysis%2520reveals%2520an%250Aunexpected%2520finding%253A%2520prompts%2520containing%2520traces%2520of%2520de-identification%2520procedures%250Aare%2520among%2520the%2520most%2520memorized%252C%2520with%2520de-identification%2520markers%2520contributing%2520the%250Amost.%2520Furthermore%252C%2520we%2520also%2520find%2520existing%2520inference-time%2520memorization%2520mitigation%250Astrategies%2520are%2520ineffective%2520and%2520fail%2520to%2520sufficiently%2520reduce%2520the%2520model%2527s%2520reliance%250Aon%2520memorized%2520text%2520tokens%2520highlighting%2520a%2520broader%2520issue%2520in%2520T2I%2520synthesis%2520with%250AMIMIC-CXR.%2520On%2520this%2520front%252C%2520we%2520propose%2520actionable%2520strategies%2520to%2520enhance%2520privacy%250Aand%2520improve%2520the%2520reliability%2520of%2520generative%2520models%2520in%2520medical%2520imaging.%2520Finally%252C%250Aour%2520results%2520provide%2520a%2520foundation%2520for%2520future%2520work%2520on%2520developing%2520and%2520benchmarking%250Amemorization%2520mitigation%2520techniques%2520for%2520synthetic%2520chest%2520X-ray%2520generation%2520using%250Athe%2520MIMIC-CXR%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20the%20Prompts%3A%20De-Identification%20Traces%20Enhance%0A%20%20Memorization%20Risks%20in%20Synthetic%20Chest%20X-Ray%20Generation&entry.906535625=Raman%20Dutt&entry.1292438233=%20%20Generative%20models%2C%20particularly%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20play%20a%0Acrucial%20role%20in%20medical%20image%20analysis.%20However%2C%20these%20models%20are%20prone%20to%0Atraining%20data%20memorization%2C%20posing%20significant%20risks%20to%20patient%20privacy.%0ASynthetic%20chest%20X-ray%20generation%20is%20one%20of%20the%20most%20common%20applications%20in%0Amedical%20image%20analysis%20with%20the%20MIMIC-CXR%20dataset%20serving%20as%20the%20primary%20data%0Arepository%20for%20this%20task.%20This%20study%20adopts%20a%20data-driven%20approach%20and%20presents%0Athe%20first%20systematic%20attempt%20to%20identify%20prompts%20and%20text%20tokens%20in%20MIMIC-CXR%0Athat%20contribute%20the%20most%20to%20training%20data%20memorization.%20Our%20analysis%20reveals%20an%0Aunexpected%20finding%3A%20prompts%20containing%20traces%20of%20de-identification%20procedures%0Aare%20among%20the%20most%20memorized%2C%20with%20de-identification%20markers%20contributing%20the%0Amost.%20Furthermore%2C%20we%20also%20find%20existing%20inference-time%20memorization%20mitigation%0Astrategies%20are%20ineffective%20and%20fail%20to%20sufficiently%20reduce%20the%20model%27s%20reliance%0Aon%20memorized%20text%20tokens%20highlighting%20a%20broader%20issue%20in%20T2I%20synthesis%20with%0AMIMIC-CXR.%20On%20this%20front%2C%20we%20propose%20actionable%20strategies%20to%20enhance%20privacy%0Aand%20improve%20the%20reliability%20of%20generative%20models%20in%20medical%20imaging.%20Finally%2C%0Aour%20results%20provide%20a%20foundation%20for%20future%20work%20on%20developing%20and%20benchmarking%0Amemorization%20mitigation%20techniques%20for%20synthetic%20chest%20X-ray%20generation%20using%0Athe%20MIMIC-CXR%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07516v1&entry.124074799=Read"},
{"title": "Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of\n  Out-of-Distribution Objects Using Prototypes", "author": "Poulami Sinhamahapatra and Franziska Schwaiger and Shirsha Bose and Huiyu Wang and Karsten Roscher and Stephan Guennemann", "abstract": "  Detecting and localising unknown or out-of-distribution (OOD) objects in any\nscene can be a challenging task in vision, particularly in safety-critical\ncases involving autonomous systems like automated vehicles or trains.\nSupervised anomaly segmentation or open-world object detection models depend on\ntraining on exhaustively annotated datasets for every domain and still struggle\nin distinguishing between background and OOD objects. In this work, we present\na plug-and-play framework - PRototype-based OOD detection Without Labels\n(PROWL). It is an inference-based method that does not require training on the\ndomain dataset and relies on extracting relevant features from self-supervised\npre-trained models. PROWL can be easily adapted to detect in-domain objects in\nany operational design domain (ODD) in a zero-shot manner by specifying a list\nof known classes from this domain. PROWL, as a first zero-shot unsupervised\nmethod, achieves state-of-the-art results on the RoadAnomaly and RoadObstacle\ndatasets provided in road driving benchmarks - SegmentMeIfYouCan (SMIYC) and\nFishyscapes, as well as comparable performance against existing supervised\nmethods trained without auxiliary OOD data. We also demonstrate its\ngeneralisability to other domains such as rail and maritime.\n", "link": "http://arxiv.org/abs/2404.07664v2", "date": "2025-02-11", "relevancy": 2.192, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20Dino%3A%20A%20Plug-and-Play%20Framework%20for%20Zero-Shot%20Detection%20of%0A%20%20Out-of-Distribution%20Objects%20Using%20Prototypes&body=Title%3A%20Finding%20Dino%3A%20A%20Plug-and-Play%20Framework%20for%20Zero-Shot%20Detection%20of%0A%20%20Out-of-Distribution%20Objects%20Using%20Prototypes%0AAuthor%3A%20Poulami%20Sinhamahapatra%20and%20Franziska%20Schwaiger%20and%20Shirsha%20Bose%20and%20Huiyu%20Wang%20and%20Karsten%20Roscher%20and%20Stephan%20Guennemann%0AAbstract%3A%20%20%20Detecting%20and%20localising%20unknown%20or%20out-of-distribution%20%28OOD%29%20objects%20in%20any%0Ascene%20can%20be%20a%20challenging%20task%20in%20vision%2C%20particularly%20in%20safety-critical%0Acases%20involving%20autonomous%20systems%20like%20automated%20vehicles%20or%20trains.%0ASupervised%20anomaly%20segmentation%20or%20open-world%20object%20detection%20models%20depend%20on%0Atraining%20on%20exhaustively%20annotated%20datasets%20for%20every%20domain%20and%20still%20struggle%0Ain%20distinguishing%20between%20background%20and%20OOD%20objects.%20In%20this%20work%2C%20we%20present%0Aa%20plug-and-play%20framework%20-%20PRototype-based%20OOD%20detection%20Without%20Labels%0A%28PROWL%29.%20It%20is%20an%20inference-based%20method%20that%20does%20not%20require%20training%20on%20the%0Adomain%20dataset%20and%20relies%20on%20extracting%20relevant%20features%20from%20self-supervised%0Apre-trained%20models.%20PROWL%20can%20be%20easily%20adapted%20to%20detect%20in-domain%20objects%20in%0Aany%20operational%20design%20domain%20%28ODD%29%20in%20a%20zero-shot%20manner%20by%20specifying%20a%20list%0Aof%20known%20classes%20from%20this%20domain.%20PROWL%2C%20as%20a%20first%20zero-shot%20unsupervised%0Amethod%2C%20achieves%20state-of-the-art%20results%20on%20the%20RoadAnomaly%20and%20RoadObstacle%0Adatasets%20provided%20in%20road%20driving%20benchmarks%20-%20SegmentMeIfYouCan%20%28SMIYC%29%20and%0AFishyscapes%2C%20as%20well%20as%20comparable%20performance%20against%20existing%20supervised%0Amethods%20trained%20without%20auxiliary%20OOD%20data.%20We%20also%20demonstrate%20its%0Ageneralisability%20to%20other%20domains%20such%20as%20rail%20and%20maritime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520Dino%253A%2520A%2520Plug-and-Play%2520Framework%2520for%2520Zero-Shot%2520Detection%2520of%250A%2520%2520Out-of-Distribution%2520Objects%2520Using%2520Prototypes%26entry.906535625%3DPoulami%2520Sinhamahapatra%2520and%2520Franziska%2520Schwaiger%2520and%2520Shirsha%2520Bose%2520and%2520Huiyu%2520Wang%2520and%2520Karsten%2520Roscher%2520and%2520Stephan%2520Guennemann%26entry.1292438233%3D%2520%2520Detecting%2520and%2520localising%2520unknown%2520or%2520out-of-distribution%2520%2528OOD%2529%2520objects%2520in%2520any%250Ascene%2520can%2520be%2520a%2520challenging%2520task%2520in%2520vision%252C%2520particularly%2520in%2520safety-critical%250Acases%2520involving%2520autonomous%2520systems%2520like%2520automated%2520vehicles%2520or%2520trains.%250ASupervised%2520anomaly%2520segmentation%2520or%2520open-world%2520object%2520detection%2520models%2520depend%2520on%250Atraining%2520on%2520exhaustively%2520annotated%2520datasets%2520for%2520every%2520domain%2520and%2520still%2520struggle%250Ain%2520distinguishing%2520between%2520background%2520and%2520OOD%2520objects.%2520In%2520this%2520work%252C%2520we%2520present%250Aa%2520plug-and-play%2520framework%2520-%2520PRototype-based%2520OOD%2520detection%2520Without%2520Labels%250A%2528PROWL%2529.%2520It%2520is%2520an%2520inference-based%2520method%2520that%2520does%2520not%2520require%2520training%2520on%2520the%250Adomain%2520dataset%2520and%2520relies%2520on%2520extracting%2520relevant%2520features%2520from%2520self-supervised%250Apre-trained%2520models.%2520PROWL%2520can%2520be%2520easily%2520adapted%2520to%2520detect%2520in-domain%2520objects%2520in%250Aany%2520operational%2520design%2520domain%2520%2528ODD%2529%2520in%2520a%2520zero-shot%2520manner%2520by%2520specifying%2520a%2520list%250Aof%2520known%2520classes%2520from%2520this%2520domain.%2520PROWL%252C%2520as%2520a%2520first%2520zero-shot%2520unsupervised%250Amethod%252C%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520RoadAnomaly%2520and%2520RoadObstacle%250Adatasets%2520provided%2520in%2520road%2520driving%2520benchmarks%2520-%2520SegmentMeIfYouCan%2520%2528SMIYC%2529%2520and%250AFishyscapes%252C%2520as%2520well%2520as%2520comparable%2520performance%2520against%2520existing%2520supervised%250Amethods%2520trained%2520without%2520auxiliary%2520OOD%2520data.%2520We%2520also%2520demonstrate%2520its%250Ageneralisability%2520to%2520other%2520domains%2520such%2520as%2520rail%2520and%2520maritime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Dino%3A%20A%20Plug-and-Play%20Framework%20for%20Zero-Shot%20Detection%20of%0A%20%20Out-of-Distribution%20Objects%20Using%20Prototypes&entry.906535625=Poulami%20Sinhamahapatra%20and%20Franziska%20Schwaiger%20and%20Shirsha%20Bose%20and%20Huiyu%20Wang%20and%20Karsten%20Roscher%20and%20Stephan%20Guennemann&entry.1292438233=%20%20Detecting%20and%20localising%20unknown%20or%20out-of-distribution%20%28OOD%29%20objects%20in%20any%0Ascene%20can%20be%20a%20challenging%20task%20in%20vision%2C%20particularly%20in%20safety-critical%0Acases%20involving%20autonomous%20systems%20like%20automated%20vehicles%20or%20trains.%0ASupervised%20anomaly%20segmentation%20or%20open-world%20object%20detection%20models%20depend%20on%0Atraining%20on%20exhaustively%20annotated%20datasets%20for%20every%20domain%20and%20still%20struggle%0Ain%20distinguishing%20between%20background%20and%20OOD%20objects.%20In%20this%20work%2C%20we%20present%0Aa%20plug-and-play%20framework%20-%20PRototype-based%20OOD%20detection%20Without%20Labels%0A%28PROWL%29.%20It%20is%20an%20inference-based%20method%20that%20does%20not%20require%20training%20on%20the%0Adomain%20dataset%20and%20relies%20on%20extracting%20relevant%20features%20from%20self-supervised%0Apre-trained%20models.%20PROWL%20can%20be%20easily%20adapted%20to%20detect%20in-domain%20objects%20in%0Aany%20operational%20design%20domain%20%28ODD%29%20in%20a%20zero-shot%20manner%20by%20specifying%20a%20list%0Aof%20known%20classes%20from%20this%20domain.%20PROWL%2C%20as%20a%20first%20zero-shot%20unsupervised%0Amethod%2C%20achieves%20state-of-the-art%20results%20on%20the%20RoadAnomaly%20and%20RoadObstacle%0Adatasets%20provided%20in%20road%20driving%20benchmarks%20-%20SegmentMeIfYouCan%20%28SMIYC%29%20and%0AFishyscapes%2C%20as%20well%20as%20comparable%20performance%20against%20existing%20supervised%0Amethods%20trained%20without%20auxiliary%20OOD%20data.%20We%20also%20demonstrate%20its%0Ageneralisability%20to%20other%20domains%20such%20as%20rail%20and%20maritime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07664v2&entry.124074799=Read"},
{"title": "Tractable Transformers for Flexible Conditional Generation", "author": "Anji Liu and Xuejie Liu and Dayuan Zhao and Mathias Niepert and Yitao Liang and Guy Van den Broeck", "abstract": "  Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries unseen during training. As a result, strong\nunconditional generation performance does not guarantee high-quality\nconditional generation. This paper proposes Tractable Transformers\n(Tracformer), a Transformer-based generative model that is more robust to\ndifferent conditional generation tasks. Unlike existing models that rely solely\non global contextual features derived from full inputs, Tracformers incorporate\na sparse Transformer encoder to capture both local and global contextual\ninformation. This information is routed through a decoder for conditional\ngeneration. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.\n", "link": "http://arxiv.org/abs/2502.07616v1", "date": "2025-02-11", "relevancy": 2.1804, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5932}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5368}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tractable%20Transformers%20for%20Flexible%20Conditional%20Generation&body=Title%3A%20Tractable%20Transformers%20for%20Flexible%20Conditional%20Generation%0AAuthor%3A%20Anji%20Liu%20and%20Xuejie%20Liu%20and%20Dayuan%20Zhao%20and%20Mathias%20Niepert%20and%20Yitao%20Liang%20and%20Guy%20Van%20den%20Broeck%0AAbstract%3A%20%20%20Non-autoregressive%20%28NAR%29%20generative%20models%20are%20valuable%20because%20they%20can%0Ahandle%20diverse%20conditional%20generation%20tasks%20in%20a%20more%20principled%20way%20than%20their%0Aautoregressive%20%28AR%29%20counterparts%2C%20which%20are%20constrained%20by%20sequential%0Adependency%20requirements.%20Recent%20advancements%20in%20NAR%20models%2C%20such%20as%20diffusion%0Alanguage%20models%2C%20have%20demonstrated%20superior%20performance%20in%20unconditional%0Ageneration%20compared%20to%20AR%20models%20%28e.g.%2C%20GPTs%29%20of%20similar%20sizes.%20However%2C%20such%0Aimprovements%20do%20not%20always%20lead%20to%20improved%20conditional%20generation%20performance.%0AWe%20show%20that%20a%20key%20reason%20for%20this%20gap%20is%20the%20difficulty%20in%20generalizing%20to%0Aconditional%20probability%20queries%20unseen%20during%20training.%20As%20a%20result%2C%20strong%0Aunconditional%20generation%20performance%20does%20not%20guarantee%20high-quality%0Aconditional%20generation.%20This%20paper%20proposes%20Tractable%20Transformers%0A%28Tracformer%29%2C%20a%20Transformer-based%20generative%20model%20that%20is%20more%20robust%20to%0Adifferent%20conditional%20generation%20tasks.%20Unlike%20existing%20models%20that%20rely%20solely%0Aon%20global%20contextual%20features%20derived%20from%20full%20inputs%2C%20Tracformers%20incorporate%0Aa%20sparse%20Transformer%20encoder%20to%20capture%20both%20local%20and%20global%20contextual%0Ainformation.%20This%20information%20is%20routed%20through%20a%20decoder%20for%20conditional%0Ageneration.%20Empirical%20results%20demonstrate%20that%20Tracformers%20achieve%0Astate-of-the-art%20conditional%20generation%20performance%20on%20text%20modeling%20compared%0Ato%20recent%20diffusion%20and%20AR%20model%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTractable%2520Transformers%2520for%2520Flexible%2520Conditional%2520Generation%26entry.906535625%3DAnji%2520Liu%2520and%2520Xuejie%2520Liu%2520and%2520Dayuan%2520Zhao%2520and%2520Mathias%2520Niepert%2520and%2520Yitao%2520Liang%2520and%2520Guy%2520Van%2520den%2520Broeck%26entry.1292438233%3D%2520%2520Non-autoregressive%2520%2528NAR%2529%2520generative%2520models%2520are%2520valuable%2520because%2520they%2520can%250Ahandle%2520diverse%2520conditional%2520generation%2520tasks%2520in%2520a%2520more%2520principled%2520way%2520than%2520their%250Aautoregressive%2520%2528AR%2529%2520counterparts%252C%2520which%2520are%2520constrained%2520by%2520sequential%250Adependency%2520requirements.%2520Recent%2520advancements%2520in%2520NAR%2520models%252C%2520such%2520as%2520diffusion%250Alanguage%2520models%252C%2520have%2520demonstrated%2520superior%2520performance%2520in%2520unconditional%250Ageneration%2520compared%2520to%2520AR%2520models%2520%2528e.g.%252C%2520GPTs%2529%2520of%2520similar%2520sizes.%2520However%252C%2520such%250Aimprovements%2520do%2520not%2520always%2520lead%2520to%2520improved%2520conditional%2520generation%2520performance.%250AWe%2520show%2520that%2520a%2520key%2520reason%2520for%2520this%2520gap%2520is%2520the%2520difficulty%2520in%2520generalizing%2520to%250Aconditional%2520probability%2520queries%2520unseen%2520during%2520training.%2520As%2520a%2520result%252C%2520strong%250Aunconditional%2520generation%2520performance%2520does%2520not%2520guarantee%2520high-quality%250Aconditional%2520generation.%2520This%2520paper%2520proposes%2520Tractable%2520Transformers%250A%2528Tracformer%2529%252C%2520a%2520Transformer-based%2520generative%2520model%2520that%2520is%2520more%2520robust%2520to%250Adifferent%2520conditional%2520generation%2520tasks.%2520Unlike%2520existing%2520models%2520that%2520rely%2520solely%250Aon%2520global%2520contextual%2520features%2520derived%2520from%2520full%2520inputs%252C%2520Tracformers%2520incorporate%250Aa%2520sparse%2520Transformer%2520encoder%2520to%2520capture%2520both%2520local%2520and%2520global%2520contextual%250Ainformation.%2520This%2520information%2520is%2520routed%2520through%2520a%2520decoder%2520for%2520conditional%250Ageneration.%2520Empirical%2520results%2520demonstrate%2520that%2520Tracformers%2520achieve%250Astate-of-the-art%2520conditional%2520generation%2520performance%2520on%2520text%2520modeling%2520compared%250Ato%2520recent%2520diffusion%2520and%2520AR%2520model%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tractable%20Transformers%20for%20Flexible%20Conditional%20Generation&entry.906535625=Anji%20Liu%20and%20Xuejie%20Liu%20and%20Dayuan%20Zhao%20and%20Mathias%20Niepert%20and%20Yitao%20Liang%20and%20Guy%20Van%20den%20Broeck&entry.1292438233=%20%20Non-autoregressive%20%28NAR%29%20generative%20models%20are%20valuable%20because%20they%20can%0Ahandle%20diverse%20conditional%20generation%20tasks%20in%20a%20more%20principled%20way%20than%20their%0Aautoregressive%20%28AR%29%20counterparts%2C%20which%20are%20constrained%20by%20sequential%0Adependency%20requirements.%20Recent%20advancements%20in%20NAR%20models%2C%20such%20as%20diffusion%0Alanguage%20models%2C%20have%20demonstrated%20superior%20performance%20in%20unconditional%0Ageneration%20compared%20to%20AR%20models%20%28e.g.%2C%20GPTs%29%20of%20similar%20sizes.%20However%2C%20such%0Aimprovements%20do%20not%20always%20lead%20to%20improved%20conditional%20generation%20performance.%0AWe%20show%20that%20a%20key%20reason%20for%20this%20gap%20is%20the%20difficulty%20in%20generalizing%20to%0Aconditional%20probability%20queries%20unseen%20during%20training.%20As%20a%20result%2C%20strong%0Aunconditional%20generation%20performance%20does%20not%20guarantee%20high-quality%0Aconditional%20generation.%20This%20paper%20proposes%20Tractable%20Transformers%0A%28Tracformer%29%2C%20a%20Transformer-based%20generative%20model%20that%20is%20more%20robust%20to%0Adifferent%20conditional%20generation%20tasks.%20Unlike%20existing%20models%20that%20rely%20solely%0Aon%20global%20contextual%20features%20derived%20from%20full%20inputs%2C%20Tracformers%20incorporate%0Aa%20sparse%20Transformer%20encoder%20to%20capture%20both%20local%20and%20global%20contextual%0Ainformation.%20This%20information%20is%20routed%20through%20a%20decoder%20for%20conditional%0Ageneration.%20Empirical%20results%20demonstrate%20that%20Tracformers%20achieve%0Astate-of-the-art%20conditional%20generation%20performance%20on%20text%20modeling%20compared%0Ato%20recent%20diffusion%20and%20AR%20model%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07616v1&entry.124074799=Read"},
{"title": "CodePhys: Robust Video-based Remote Physiological Measurement through\n  Latent Codebook Querying", "author": "Shuyang Chu and Menghan Xia and Mengyao Yuan and Xin Liu and Tapio Seppanen and Guoying Zhao and Jingang Shi", "abstract": "  Remote photoplethysmography (rPPG) aims to measure non-contact physiological\nsignals from facial videos, which has shown great potential in many\napplications. Most existing methods directly extract video-based rPPG features\nby designing neural networks for heart rate estimation. Although they can\nachieve acceptable results, the recovery of rPPG signal faces intractable\nchallenges when interference from real-world scenarios takes place on facial\nvideo. Specifically, facial videos are inevitably affected by non-physiological\nfactors (e.g., camera device noise, defocus, and motion blur), leading to the\ndistortion of extracted rPPG signals. Recent rPPG extraction methods are easily\naffected by interference and degradation, resulting in noisy rPPG signals. In\nthis paper, we propose a novel method named CodePhys, which innovatively treats\nrPPG measurement as a code query task in a noise-free proxy space (i.e.,\ncodebook) constructed by ground-truth PPG signals. We consider noisy rPPG\nfeatures as queries and generate high-fidelity rPPG features by matching them\nwith noise-free PPG features from the codebook. Our approach also incorporates\na spatial-aware encoder network with a spatial attention mechanism to highlight\nphysiologically active areas and uses a distillation loss to reduce the\ninfluence of non-periodic visual interference. Experimental results on four\nbenchmark datasets demonstrate that CodePhys outperforms state-of-the-art\nmethods in both intra-dataset and cross-dataset settings.\n", "link": "http://arxiv.org/abs/2502.07526v1", "date": "2025-02-11", "relevancy": 2.1783, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.587}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodePhys%3A%20Robust%20Video-based%20Remote%20Physiological%20Measurement%20through%0A%20%20Latent%20Codebook%20Querying&body=Title%3A%20CodePhys%3A%20Robust%20Video-based%20Remote%20Physiological%20Measurement%20through%0A%20%20Latent%20Codebook%20Querying%0AAuthor%3A%20Shuyang%20Chu%20and%20Menghan%20Xia%20and%20Mengyao%20Yuan%20and%20Xin%20Liu%20and%20Tapio%20Seppanen%20and%20Guoying%20Zhao%20and%20Jingang%20Shi%0AAbstract%3A%20%20%20Remote%20photoplethysmography%20%28rPPG%29%20aims%20to%20measure%20non-contact%20physiological%0Asignals%20from%20facial%20videos%2C%20which%20has%20shown%20great%20potential%20in%20many%0Aapplications.%20Most%20existing%20methods%20directly%20extract%20video-based%20rPPG%20features%0Aby%20designing%20neural%20networks%20for%20heart%20rate%20estimation.%20Although%20they%20can%0Aachieve%20acceptable%20results%2C%20the%20recovery%20of%20rPPG%20signal%20faces%20intractable%0Achallenges%20when%20interference%20from%20real-world%20scenarios%20takes%20place%20on%20facial%0Avideo.%20Specifically%2C%20facial%20videos%20are%20inevitably%20affected%20by%20non-physiological%0Afactors%20%28e.g.%2C%20camera%20device%20noise%2C%20defocus%2C%20and%20motion%20blur%29%2C%20leading%20to%20the%0Adistortion%20of%20extracted%20rPPG%20signals.%20Recent%20rPPG%20extraction%20methods%20are%20easily%0Aaffected%20by%20interference%20and%20degradation%2C%20resulting%20in%20noisy%20rPPG%20signals.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20method%20named%20CodePhys%2C%20which%20innovatively%20treats%0ArPPG%20measurement%20as%20a%20code%20query%20task%20in%20a%20noise-free%20proxy%20space%20%28i.e.%2C%0Acodebook%29%20constructed%20by%20ground-truth%20PPG%20signals.%20We%20consider%20noisy%20rPPG%0Afeatures%20as%20queries%20and%20generate%20high-fidelity%20rPPG%20features%20by%20matching%20them%0Awith%20noise-free%20PPG%20features%20from%20the%20codebook.%20Our%20approach%20also%20incorporates%0Aa%20spatial-aware%20encoder%20network%20with%20a%20spatial%20attention%20mechanism%20to%20highlight%0Aphysiologically%20active%20areas%20and%20uses%20a%20distillation%20loss%20to%20reduce%20the%0Ainfluence%20of%20non-periodic%20visual%20interference.%20Experimental%20results%20on%20four%0Abenchmark%20datasets%20demonstrate%20that%20CodePhys%20outperforms%20state-of-the-art%0Amethods%20in%20both%20intra-dataset%20and%20cross-dataset%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodePhys%253A%2520Robust%2520Video-based%2520Remote%2520Physiological%2520Measurement%2520through%250A%2520%2520Latent%2520Codebook%2520Querying%26entry.906535625%3DShuyang%2520Chu%2520and%2520Menghan%2520Xia%2520and%2520Mengyao%2520Yuan%2520and%2520Xin%2520Liu%2520and%2520Tapio%2520Seppanen%2520and%2520Guoying%2520Zhao%2520and%2520Jingang%2520Shi%26entry.1292438233%3D%2520%2520Remote%2520photoplethysmography%2520%2528rPPG%2529%2520aims%2520to%2520measure%2520non-contact%2520physiological%250Asignals%2520from%2520facial%2520videos%252C%2520which%2520has%2520shown%2520great%2520potential%2520in%2520many%250Aapplications.%2520Most%2520existing%2520methods%2520directly%2520extract%2520video-based%2520rPPG%2520features%250Aby%2520designing%2520neural%2520networks%2520for%2520heart%2520rate%2520estimation.%2520Although%2520they%2520can%250Aachieve%2520acceptable%2520results%252C%2520the%2520recovery%2520of%2520rPPG%2520signal%2520faces%2520intractable%250Achallenges%2520when%2520interference%2520from%2520real-world%2520scenarios%2520takes%2520place%2520on%2520facial%250Avideo.%2520Specifically%252C%2520facial%2520videos%2520are%2520inevitably%2520affected%2520by%2520non-physiological%250Afactors%2520%2528e.g.%252C%2520camera%2520device%2520noise%252C%2520defocus%252C%2520and%2520motion%2520blur%2529%252C%2520leading%2520to%2520the%250Adistortion%2520of%2520extracted%2520rPPG%2520signals.%2520Recent%2520rPPG%2520extraction%2520methods%2520are%2520easily%250Aaffected%2520by%2520interference%2520and%2520degradation%252C%2520resulting%2520in%2520noisy%2520rPPG%2520signals.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520named%2520CodePhys%252C%2520which%2520innovatively%2520treats%250ArPPG%2520measurement%2520as%2520a%2520code%2520query%2520task%2520in%2520a%2520noise-free%2520proxy%2520space%2520%2528i.e.%252C%250Acodebook%2529%2520constructed%2520by%2520ground-truth%2520PPG%2520signals.%2520We%2520consider%2520noisy%2520rPPG%250Afeatures%2520as%2520queries%2520and%2520generate%2520high-fidelity%2520rPPG%2520features%2520by%2520matching%2520them%250Awith%2520noise-free%2520PPG%2520features%2520from%2520the%2520codebook.%2520Our%2520approach%2520also%2520incorporates%250Aa%2520spatial-aware%2520encoder%2520network%2520with%2520a%2520spatial%2520attention%2520mechanism%2520to%2520highlight%250Aphysiologically%2520active%2520areas%2520and%2520uses%2520a%2520distillation%2520loss%2520to%2520reduce%2520the%250Ainfluence%2520of%2520non-periodic%2520visual%2520interference.%2520Experimental%2520results%2520on%2520four%250Abenchmark%2520datasets%2520demonstrate%2520that%2520CodePhys%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520both%2520intra-dataset%2520and%2520cross-dataset%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodePhys%3A%20Robust%20Video-based%20Remote%20Physiological%20Measurement%20through%0A%20%20Latent%20Codebook%20Querying&entry.906535625=Shuyang%20Chu%20and%20Menghan%20Xia%20and%20Mengyao%20Yuan%20and%20Xin%20Liu%20and%20Tapio%20Seppanen%20and%20Guoying%20Zhao%20and%20Jingang%20Shi&entry.1292438233=%20%20Remote%20photoplethysmography%20%28rPPG%29%20aims%20to%20measure%20non-contact%20physiological%0Asignals%20from%20facial%20videos%2C%20which%20has%20shown%20great%20potential%20in%20many%0Aapplications.%20Most%20existing%20methods%20directly%20extract%20video-based%20rPPG%20features%0Aby%20designing%20neural%20networks%20for%20heart%20rate%20estimation.%20Although%20they%20can%0Aachieve%20acceptable%20results%2C%20the%20recovery%20of%20rPPG%20signal%20faces%20intractable%0Achallenges%20when%20interference%20from%20real-world%20scenarios%20takes%20place%20on%20facial%0Avideo.%20Specifically%2C%20facial%20videos%20are%20inevitably%20affected%20by%20non-physiological%0Afactors%20%28e.g.%2C%20camera%20device%20noise%2C%20defocus%2C%20and%20motion%20blur%29%2C%20leading%20to%20the%0Adistortion%20of%20extracted%20rPPG%20signals.%20Recent%20rPPG%20extraction%20methods%20are%20easily%0Aaffected%20by%20interference%20and%20degradation%2C%20resulting%20in%20noisy%20rPPG%20signals.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20method%20named%20CodePhys%2C%20which%20innovatively%20treats%0ArPPG%20measurement%20as%20a%20code%20query%20task%20in%20a%20noise-free%20proxy%20space%20%28i.e.%2C%0Acodebook%29%20constructed%20by%20ground-truth%20PPG%20signals.%20We%20consider%20noisy%20rPPG%0Afeatures%20as%20queries%20and%20generate%20high-fidelity%20rPPG%20features%20by%20matching%20them%0Awith%20noise-free%20PPG%20features%20from%20the%20codebook.%20Our%20approach%20also%20incorporates%0Aa%20spatial-aware%20encoder%20network%20with%20a%20spatial%20attention%20mechanism%20to%20highlight%0Aphysiologically%20active%20areas%20and%20uses%20a%20distillation%20loss%20to%20reduce%20the%0Ainfluence%20of%20non-periodic%20visual%20interference.%20Experimental%20results%20on%20four%0Abenchmark%20datasets%20demonstrate%20that%20CodePhys%20outperforms%20state-of-the-art%0Amethods%20in%20both%20intra-dataset%20and%20cross-dataset%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07526v1&entry.124074799=Read"},
{"title": "DeepVL: Dynamics and Inertial Measurements-based Deep Velocity Learning\n  for Underwater Odometry", "author": "Mohit Singh and Kostas Alexis", "abstract": "  This paper presents a learned model to predict the robot-centric velocity of\nan underwater robot through dynamics-aware proprioception. The method exploits\na recurrent neural network using as inputs inertial cues, motor commands, and\nbattery voltage readings alongside the hidden state of the previous time-step\nto output robust velocity estimates and their associated uncertainty. An\nensemble of networks is utilized to enhance the velocity and uncertainty\npredictions. Fusing the network's outputs into an Extended Kalman Filter,\nalongside inertial predictions and barometer updates, the method enables\nlong-term underwater odometry without further exteroception. Furthermore, when\nintegrated into visual-inertial odometry, the method assists in enhanced\nestimation resilience when dealing with an order of magnitude fewer total\nfeatures tracked (as few as 1) as compared to conventional visual-inertial\nsystems. Tested onboard an underwater robot deployed both in a laboratory pool\nand the Trondheim Fjord, the method takes less than 5ms for inference either on\nthe CPU or the GPU of an NVIDIA Orin AGX and demonstrates less than 4% relative\nposition error in novel trajectories during complete visual blackout, and\napproximately 2% relative error when a maximum of 2 visual features from a\nmonocular camera are available.\n", "link": "http://arxiv.org/abs/2502.07726v1", "date": "2025-02-11", "relevancy": 2.1743, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6035}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.533}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepVL%3A%20Dynamics%20and%20Inertial%20Measurements-based%20Deep%20Velocity%20Learning%0A%20%20for%20Underwater%20Odometry&body=Title%3A%20DeepVL%3A%20Dynamics%20and%20Inertial%20Measurements-based%20Deep%20Velocity%20Learning%0A%20%20for%20Underwater%20Odometry%0AAuthor%3A%20Mohit%20Singh%20and%20Kostas%20Alexis%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20learned%20model%20to%20predict%20the%20robot-centric%20velocity%20of%0Aan%20underwater%20robot%20through%20dynamics-aware%20proprioception.%20The%20method%20exploits%0Aa%20recurrent%20neural%20network%20using%20as%20inputs%20inertial%20cues%2C%20motor%20commands%2C%20and%0Abattery%20voltage%20readings%20alongside%20the%20hidden%20state%20of%20the%20previous%20time-step%0Ato%20output%20robust%20velocity%20estimates%20and%20their%20associated%20uncertainty.%20An%0Aensemble%20of%20networks%20is%20utilized%20to%20enhance%20the%20velocity%20and%20uncertainty%0Apredictions.%20Fusing%20the%20network%27s%20outputs%20into%20an%20Extended%20Kalman%20Filter%2C%0Aalongside%20inertial%20predictions%20and%20barometer%20updates%2C%20the%20method%20enables%0Along-term%20underwater%20odometry%20without%20further%20exteroception.%20Furthermore%2C%20when%0Aintegrated%20into%20visual-inertial%20odometry%2C%20the%20method%20assists%20in%20enhanced%0Aestimation%20resilience%20when%20dealing%20with%20an%20order%20of%20magnitude%20fewer%20total%0Afeatures%20tracked%20%28as%20few%20as%201%29%20as%20compared%20to%20conventional%20visual-inertial%0Asystems.%20Tested%20onboard%20an%20underwater%20robot%20deployed%20both%20in%20a%20laboratory%20pool%0Aand%20the%20Trondheim%20Fjord%2C%20the%20method%20takes%20less%20than%205ms%20for%20inference%20either%20on%0Athe%20CPU%20or%20the%20GPU%20of%20an%20NVIDIA%20Orin%20AGX%20and%20demonstrates%20less%20than%204%25%20relative%0Aposition%20error%20in%20novel%20trajectories%20during%20complete%20visual%20blackout%2C%20and%0Aapproximately%202%25%20relative%20error%20when%20a%20maximum%20of%202%20visual%20features%20from%20a%0Amonocular%20camera%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepVL%253A%2520Dynamics%2520and%2520Inertial%2520Measurements-based%2520Deep%2520Velocity%2520Learning%250A%2520%2520for%2520Underwater%2520Odometry%26entry.906535625%3DMohit%2520Singh%2520and%2520Kostas%2520Alexis%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520learned%2520model%2520to%2520predict%2520the%2520robot-centric%2520velocity%2520of%250Aan%2520underwater%2520robot%2520through%2520dynamics-aware%2520proprioception.%2520The%2520method%2520exploits%250Aa%2520recurrent%2520neural%2520network%2520using%2520as%2520inputs%2520inertial%2520cues%252C%2520motor%2520commands%252C%2520and%250Abattery%2520voltage%2520readings%2520alongside%2520the%2520hidden%2520state%2520of%2520the%2520previous%2520time-step%250Ato%2520output%2520robust%2520velocity%2520estimates%2520and%2520their%2520associated%2520uncertainty.%2520An%250Aensemble%2520of%2520networks%2520is%2520utilized%2520to%2520enhance%2520the%2520velocity%2520and%2520uncertainty%250Apredictions.%2520Fusing%2520the%2520network%2527s%2520outputs%2520into%2520an%2520Extended%2520Kalman%2520Filter%252C%250Aalongside%2520inertial%2520predictions%2520and%2520barometer%2520updates%252C%2520the%2520method%2520enables%250Along-term%2520underwater%2520odometry%2520without%2520further%2520exteroception.%2520Furthermore%252C%2520when%250Aintegrated%2520into%2520visual-inertial%2520odometry%252C%2520the%2520method%2520assists%2520in%2520enhanced%250Aestimation%2520resilience%2520when%2520dealing%2520with%2520an%2520order%2520of%2520magnitude%2520fewer%2520total%250Afeatures%2520tracked%2520%2528as%2520few%2520as%25201%2529%2520as%2520compared%2520to%2520conventional%2520visual-inertial%250Asystems.%2520Tested%2520onboard%2520an%2520underwater%2520robot%2520deployed%2520both%2520in%2520a%2520laboratory%2520pool%250Aand%2520the%2520Trondheim%2520Fjord%252C%2520the%2520method%2520takes%2520less%2520than%25205ms%2520for%2520inference%2520either%2520on%250Athe%2520CPU%2520or%2520the%2520GPU%2520of%2520an%2520NVIDIA%2520Orin%2520AGX%2520and%2520demonstrates%2520less%2520than%25204%2525%2520relative%250Aposition%2520error%2520in%2520novel%2520trajectories%2520during%2520complete%2520visual%2520blackout%252C%2520and%250Aapproximately%25202%2525%2520relative%2520error%2520when%2520a%2520maximum%2520of%25202%2520visual%2520features%2520from%2520a%250Amonocular%2520camera%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepVL%3A%20Dynamics%20and%20Inertial%20Measurements-based%20Deep%20Velocity%20Learning%0A%20%20for%20Underwater%20Odometry&entry.906535625=Mohit%20Singh%20and%20Kostas%20Alexis&entry.1292438233=%20%20This%20paper%20presents%20a%20learned%20model%20to%20predict%20the%20robot-centric%20velocity%20of%0Aan%20underwater%20robot%20through%20dynamics-aware%20proprioception.%20The%20method%20exploits%0Aa%20recurrent%20neural%20network%20using%20as%20inputs%20inertial%20cues%2C%20motor%20commands%2C%20and%0Abattery%20voltage%20readings%20alongside%20the%20hidden%20state%20of%20the%20previous%20time-step%0Ato%20output%20robust%20velocity%20estimates%20and%20their%20associated%20uncertainty.%20An%0Aensemble%20of%20networks%20is%20utilized%20to%20enhance%20the%20velocity%20and%20uncertainty%0Apredictions.%20Fusing%20the%20network%27s%20outputs%20into%20an%20Extended%20Kalman%20Filter%2C%0Aalongside%20inertial%20predictions%20and%20barometer%20updates%2C%20the%20method%20enables%0Along-term%20underwater%20odometry%20without%20further%20exteroception.%20Furthermore%2C%20when%0Aintegrated%20into%20visual-inertial%20odometry%2C%20the%20method%20assists%20in%20enhanced%0Aestimation%20resilience%20when%20dealing%20with%20an%20order%20of%20magnitude%20fewer%20total%0Afeatures%20tracked%20%28as%20few%20as%201%29%20as%20compared%20to%20conventional%20visual-inertial%0Asystems.%20Tested%20onboard%20an%20underwater%20robot%20deployed%20both%20in%20a%20laboratory%20pool%0Aand%20the%20Trondheim%20Fjord%2C%20the%20method%20takes%20less%20than%205ms%20for%20inference%20either%20on%0Athe%20CPU%20or%20the%20GPU%20of%20an%20NVIDIA%20Orin%20AGX%20and%20demonstrates%20less%20than%204%25%20relative%0Aposition%20error%20in%20novel%20trajectories%20during%20complete%20visual%20blackout%2C%20and%0Aapproximately%202%25%20relative%20error%20when%20a%20maximum%20of%202%20visual%20features%20from%20a%0Amonocular%20camera%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07726v1&entry.124074799=Read"},
{"title": "The Benefits of Balance: From Information Projections to Variance\n  Reduction", "author": "Lang Liu and Ronak Mehta and Soumik Pal and Zaid Harchaoui", "abstract": "  Data balancing across multiple modalities and sources appears in various\nforms in foundation models in machine learning and AI, e.g. in CLIP and DINO.\nWe show that data balancing across modalities and sources actually offers an\nunsuspected benefit: variance reduction. We present a non-asymptotic\nstatistical bound that quantifies this variance reduction effect and relates it\nto the eigenvalue decay of Markov operators. Furthermore, we describe how\nvarious forms of data balancing in contrastive multimodal learning and\nself-supervised clustering can be better understood, and even improved upon,\nowing to our variance reduction viewpoint.\n", "link": "http://arxiv.org/abs/2408.15065v2", "date": "2025-02-11", "relevancy": 2.1738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Benefits%20of%20Balance%3A%20From%20Information%20Projections%20to%20Variance%0A%20%20Reduction&body=Title%3A%20The%20Benefits%20of%20Balance%3A%20From%20Information%20Projections%20to%20Variance%0A%20%20Reduction%0AAuthor%3A%20Lang%20Liu%20and%20Ronak%20Mehta%20and%20Soumik%20Pal%20and%20Zaid%20Harchaoui%0AAbstract%3A%20%20%20Data%20balancing%20across%20multiple%20modalities%20and%20sources%20appears%20in%20various%0Aforms%20in%20foundation%20models%20in%20machine%20learning%20and%20AI%2C%20e.g.%20in%20CLIP%20and%20DINO.%0AWe%20show%20that%20data%20balancing%20across%20modalities%20and%20sources%20actually%20offers%20an%0Aunsuspected%20benefit%3A%20variance%20reduction.%20We%20present%20a%20non-asymptotic%0Astatistical%20bound%20that%20quantifies%20this%20variance%20reduction%20effect%20and%20relates%20it%0Ato%20the%20eigenvalue%20decay%20of%20Markov%20operators.%20Furthermore%2C%20we%20describe%20how%0Avarious%20forms%20of%20data%20balancing%20in%20contrastive%20multimodal%20learning%20and%0Aself-supervised%20clustering%20can%20be%20better%20understood%2C%20and%20even%20improved%20upon%2C%0Aowing%20to%20our%20variance%20reduction%20viewpoint.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15065v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Benefits%2520of%2520Balance%253A%2520From%2520Information%2520Projections%2520to%2520Variance%250A%2520%2520Reduction%26entry.906535625%3DLang%2520Liu%2520and%2520Ronak%2520Mehta%2520and%2520Soumik%2520Pal%2520and%2520Zaid%2520Harchaoui%26entry.1292438233%3D%2520%2520Data%2520balancing%2520across%2520multiple%2520modalities%2520and%2520sources%2520appears%2520in%2520various%250Aforms%2520in%2520foundation%2520models%2520in%2520machine%2520learning%2520and%2520AI%252C%2520e.g.%2520in%2520CLIP%2520and%2520DINO.%250AWe%2520show%2520that%2520data%2520balancing%2520across%2520modalities%2520and%2520sources%2520actually%2520offers%2520an%250Aunsuspected%2520benefit%253A%2520variance%2520reduction.%2520We%2520present%2520a%2520non-asymptotic%250Astatistical%2520bound%2520that%2520quantifies%2520this%2520variance%2520reduction%2520effect%2520and%2520relates%2520it%250Ato%2520the%2520eigenvalue%2520decay%2520of%2520Markov%2520operators.%2520Furthermore%252C%2520we%2520describe%2520how%250Avarious%2520forms%2520of%2520data%2520balancing%2520in%2520contrastive%2520multimodal%2520learning%2520and%250Aself-supervised%2520clustering%2520can%2520be%2520better%2520understood%252C%2520and%2520even%2520improved%2520upon%252C%250Aowing%2520to%2520our%2520variance%2520reduction%2520viewpoint.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15065v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Benefits%20of%20Balance%3A%20From%20Information%20Projections%20to%20Variance%0A%20%20Reduction&entry.906535625=Lang%20Liu%20and%20Ronak%20Mehta%20and%20Soumik%20Pal%20and%20Zaid%20Harchaoui&entry.1292438233=%20%20Data%20balancing%20across%20multiple%20modalities%20and%20sources%20appears%20in%20various%0Aforms%20in%20foundation%20models%20in%20machine%20learning%20and%20AI%2C%20e.g.%20in%20CLIP%20and%20DINO.%0AWe%20show%20that%20data%20balancing%20across%20modalities%20and%20sources%20actually%20offers%20an%0Aunsuspected%20benefit%3A%20variance%20reduction.%20We%20present%20a%20non-asymptotic%0Astatistical%20bound%20that%20quantifies%20this%20variance%20reduction%20effect%20and%20relates%20it%0Ato%20the%20eigenvalue%20decay%20of%20Markov%20operators.%20Furthermore%2C%20we%20describe%20how%0Avarious%20forms%20of%20data%20balancing%20in%20contrastive%20multimodal%20learning%20and%0Aself-supervised%20clustering%20can%20be%20better%20understood%2C%20and%20even%20improved%20upon%2C%0Aowing%20to%20our%20variance%20reduction%20viewpoint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15065v2&entry.124074799=Read"},
{"title": "MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior\n  Modeling", "author": "Bencheng Yan and Si Chen and Shichang Jia and Jianyu Liu and Yueran Liu and Chenghan Fu and Wanxian Guan and Hui Zhao and Xiang Zhang and Kai Zhang and Wenbo Su and Pengjie Wang and Jian Xu and Bo Zheng and Baolin Liu", "abstract": "  Click-Through Rate (CTR) prediction is a crucial task in recommendation\nsystems, online searches, and advertising platforms, where accurately capturing\nusers' real interests in content is essential for performance. However,\nexisting methods heavily rely on ID embeddings, which fail to reflect users'\ntrue preferences for content such as images and titles. This limitation becomes\nparticularly evident in cold-start and long-tail scenarios, where traditional\napproaches struggle to deliver effective results. To address these challenges,\nwe propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which\nconsists of three key stages: Pre-training, Content-Interest-Aware Supervised\nFine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training\nstage adapts foundational models to domain-specific data, enabling the\nextraction of high-quality multi-modal embeddings. The C-SFT stage bridges the\nsemantic gap between content and user interests by leveraging user behavior\nsignals to guide the alignment of embeddings with user preferences. Finally,\nthe CiUBM stage integrates multi-modal embeddings and ID-based collaborative\nfiltering signals into a unified framework. Comprehensive offline experiments\nand online A/B tests conducted on the Taobao, one of the world's largest\ne-commerce platforms, demonstrated the effectiveness and efficiency of MIM\nmethod. The method has been successfully deployed online, achieving a\nsignificant increase of +14.14% in CTR and +4.12% in RPM, showcasing its\nindustrial applicability and substantial impact on platform performance. To\npromote further research, we have publicly released the code and dataset at\nhttps://pan.quark.cn/s/8fc8ec3e74f3.\n", "link": "http://arxiv.org/abs/2502.00321v3", "date": "2025-02-11", "relevancy": 2.168, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5576}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5355}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIM%3A%20Multi-modal%20Content%20Interest%20Modeling%20Paradigm%20for%20User%20Behavior%0A%20%20Modeling&body=Title%3A%20MIM%3A%20Multi-modal%20Content%20Interest%20Modeling%20Paradigm%20for%20User%20Behavior%0A%20%20Modeling%0AAuthor%3A%20Bencheng%20Yan%20and%20Si%20Chen%20and%20Shichang%20Jia%20and%20Jianyu%20Liu%20and%20Yueran%20Liu%20and%20Chenghan%20Fu%20and%20Wanxian%20Guan%20and%20Hui%20Zhao%20and%20Xiang%20Zhang%20and%20Kai%20Zhang%20and%20Wenbo%20Su%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng%20and%20Baolin%20Liu%0AAbstract%3A%20%20%20Click-Through%20Rate%20%28CTR%29%20prediction%20is%20a%20crucial%20task%20in%20recommendation%0Asystems%2C%20online%20searches%2C%20and%20advertising%20platforms%2C%20where%20accurately%20capturing%0Ausers%27%20real%20interests%20in%20content%20is%20essential%20for%20performance.%20However%2C%0Aexisting%20methods%20heavily%20rely%20on%20ID%20embeddings%2C%20which%20fail%20to%20reflect%20users%27%0Atrue%20preferences%20for%20content%20such%20as%20images%20and%20titles.%20This%20limitation%20becomes%0Aparticularly%20evident%20in%20cold-start%20and%20long-tail%20scenarios%2C%20where%20traditional%0Aapproaches%20struggle%20to%20deliver%20effective%20results.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20Multi-modal%20Content%20Interest%20Modeling%20paradigm%20%28MIM%29%2C%20which%0Aconsists%20of%20three%20key%20stages%3A%20Pre-training%2C%20Content-Interest-Aware%20Supervised%0AFine-Tuning%20%28C-SFT%29%2C%20and%20Content-Interest-Aware%20UBM%20%28CiUBM%29.%20The%20pre-training%0Astage%20adapts%20foundational%20models%20to%20domain-specific%20data%2C%20enabling%20the%0Aextraction%20of%20high-quality%20multi-modal%20embeddings.%20The%20C-SFT%20stage%20bridges%20the%0Asemantic%20gap%20between%20content%20and%20user%20interests%20by%20leveraging%20user%20behavior%0Asignals%20to%20guide%20the%20alignment%20of%20embeddings%20with%20user%20preferences.%20Finally%2C%0Athe%20CiUBM%20stage%20integrates%20multi-modal%20embeddings%20and%20ID-based%20collaborative%0Afiltering%20signals%20into%20a%20unified%20framework.%20Comprehensive%20offline%20experiments%0Aand%20online%20A/B%20tests%20conducted%20on%20the%20Taobao%2C%20one%20of%20the%20world%27s%20largest%0Ae-commerce%20platforms%2C%20demonstrated%20the%20effectiveness%20and%20efficiency%20of%20MIM%0Amethod.%20The%20method%20has%20been%20successfully%20deployed%20online%2C%20achieving%20a%0Asignificant%20increase%20of%20%2B14.14%25%20in%20CTR%20and%20%2B4.12%25%20in%20RPM%2C%20showcasing%20its%0Aindustrial%20applicability%20and%20substantial%20impact%20on%20platform%20performance.%20To%0Apromote%20further%20research%2C%20we%20have%20publicly%20released%20the%20code%20and%20dataset%20at%0Ahttps%3A//pan.quark.cn/s/8fc8ec3e74f3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00321v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIM%253A%2520Multi-modal%2520Content%2520Interest%2520Modeling%2520Paradigm%2520for%2520User%2520Behavior%250A%2520%2520Modeling%26entry.906535625%3DBencheng%2520Yan%2520and%2520Si%2520Chen%2520and%2520Shichang%2520Jia%2520and%2520Jianyu%2520Liu%2520and%2520Yueran%2520Liu%2520and%2520Chenghan%2520Fu%2520and%2520Wanxian%2520Guan%2520and%2520Hui%2520Zhao%2520and%2520Xiang%2520Zhang%2520and%2520Kai%2520Zhang%2520and%2520Wenbo%2520Su%2520and%2520Pengjie%2520Wang%2520and%2520Jian%2520Xu%2520and%2520Bo%2520Zheng%2520and%2520Baolin%2520Liu%26entry.1292438233%3D%2520%2520Click-Through%2520Rate%2520%2528CTR%2529%2520prediction%2520is%2520a%2520crucial%2520task%2520in%2520recommendation%250Asystems%252C%2520online%2520searches%252C%2520and%2520advertising%2520platforms%252C%2520where%2520accurately%2520capturing%250Ausers%2527%2520real%2520interests%2520in%2520content%2520is%2520essential%2520for%2520performance.%2520However%252C%250Aexisting%2520methods%2520heavily%2520rely%2520on%2520ID%2520embeddings%252C%2520which%2520fail%2520to%2520reflect%2520users%2527%250Atrue%2520preferences%2520for%2520content%2520such%2520as%2520images%2520and%2520titles.%2520This%2520limitation%2520becomes%250Aparticularly%2520evident%2520in%2520cold-start%2520and%2520long-tail%2520scenarios%252C%2520where%2520traditional%250Aapproaches%2520struggle%2520to%2520deliver%2520effective%2520results.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520novel%2520Multi-modal%2520Content%2520Interest%2520Modeling%2520paradigm%2520%2528MIM%2529%252C%2520which%250Aconsists%2520of%2520three%2520key%2520stages%253A%2520Pre-training%252C%2520Content-Interest-Aware%2520Supervised%250AFine-Tuning%2520%2528C-SFT%2529%252C%2520and%2520Content-Interest-Aware%2520UBM%2520%2528CiUBM%2529.%2520The%2520pre-training%250Astage%2520adapts%2520foundational%2520models%2520to%2520domain-specific%2520data%252C%2520enabling%2520the%250Aextraction%2520of%2520high-quality%2520multi-modal%2520embeddings.%2520The%2520C-SFT%2520stage%2520bridges%2520the%250Asemantic%2520gap%2520between%2520content%2520and%2520user%2520interests%2520by%2520leveraging%2520user%2520behavior%250Asignals%2520to%2520guide%2520the%2520alignment%2520of%2520embeddings%2520with%2520user%2520preferences.%2520Finally%252C%250Athe%2520CiUBM%2520stage%2520integrates%2520multi-modal%2520embeddings%2520and%2520ID-based%2520collaborative%250Afiltering%2520signals%2520into%2520a%2520unified%2520framework.%2520Comprehensive%2520offline%2520experiments%250Aand%2520online%2520A/B%2520tests%2520conducted%2520on%2520the%2520Taobao%252C%2520one%2520of%2520the%2520world%2527s%2520largest%250Ae-commerce%2520platforms%252C%2520demonstrated%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520MIM%250Amethod.%2520The%2520method%2520has%2520been%2520successfully%2520deployed%2520online%252C%2520achieving%2520a%250Asignificant%2520increase%2520of%2520%252B14.14%2525%2520in%2520CTR%2520and%2520%252B4.12%2525%2520in%2520RPM%252C%2520showcasing%2520its%250Aindustrial%2520applicability%2520and%2520substantial%2520impact%2520on%2520platform%2520performance.%2520To%250Apromote%2520further%2520research%252C%2520we%2520have%2520publicly%2520released%2520the%2520code%2520and%2520dataset%2520at%250Ahttps%253A//pan.quark.cn/s/8fc8ec3e74f3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00321v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIM%3A%20Multi-modal%20Content%20Interest%20Modeling%20Paradigm%20for%20User%20Behavior%0A%20%20Modeling&entry.906535625=Bencheng%20Yan%20and%20Si%20Chen%20and%20Shichang%20Jia%20and%20Jianyu%20Liu%20and%20Yueran%20Liu%20and%20Chenghan%20Fu%20and%20Wanxian%20Guan%20and%20Hui%20Zhao%20and%20Xiang%20Zhang%20and%20Kai%20Zhang%20and%20Wenbo%20Su%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng%20and%20Baolin%20Liu&entry.1292438233=%20%20Click-Through%20Rate%20%28CTR%29%20prediction%20is%20a%20crucial%20task%20in%20recommendation%0Asystems%2C%20online%20searches%2C%20and%20advertising%20platforms%2C%20where%20accurately%20capturing%0Ausers%27%20real%20interests%20in%20content%20is%20essential%20for%20performance.%20However%2C%0Aexisting%20methods%20heavily%20rely%20on%20ID%20embeddings%2C%20which%20fail%20to%20reflect%20users%27%0Atrue%20preferences%20for%20content%20such%20as%20images%20and%20titles.%20This%20limitation%20becomes%0Aparticularly%20evident%20in%20cold-start%20and%20long-tail%20scenarios%2C%20where%20traditional%0Aapproaches%20struggle%20to%20deliver%20effective%20results.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20Multi-modal%20Content%20Interest%20Modeling%20paradigm%20%28MIM%29%2C%20which%0Aconsists%20of%20three%20key%20stages%3A%20Pre-training%2C%20Content-Interest-Aware%20Supervised%0AFine-Tuning%20%28C-SFT%29%2C%20and%20Content-Interest-Aware%20UBM%20%28CiUBM%29.%20The%20pre-training%0Astage%20adapts%20foundational%20models%20to%20domain-specific%20data%2C%20enabling%20the%0Aextraction%20of%20high-quality%20multi-modal%20embeddings.%20The%20C-SFT%20stage%20bridges%20the%0Asemantic%20gap%20between%20content%20and%20user%20interests%20by%20leveraging%20user%20behavior%0Asignals%20to%20guide%20the%20alignment%20of%20embeddings%20with%20user%20preferences.%20Finally%2C%0Athe%20CiUBM%20stage%20integrates%20multi-modal%20embeddings%20and%20ID-based%20collaborative%0Afiltering%20signals%20into%20a%20unified%20framework.%20Comprehensive%20offline%20experiments%0Aand%20online%20A/B%20tests%20conducted%20on%20the%20Taobao%2C%20one%20of%20the%20world%27s%20largest%0Ae-commerce%20platforms%2C%20demonstrated%20the%20effectiveness%20and%20efficiency%20of%20MIM%0Amethod.%20The%20method%20has%20been%20successfully%20deployed%20online%2C%20achieving%20a%0Asignificant%20increase%20of%20%2B14.14%25%20in%20CTR%20and%20%2B4.12%25%20in%20RPM%2C%20showcasing%20its%0Aindustrial%20applicability%20and%20substantial%20impact%20on%20platform%20performance.%20To%0Apromote%20further%20research%2C%20we%20have%20publicly%20released%20the%20code%20and%20dataset%20at%0Ahttps%3A//pan.quark.cn/s/8fc8ec3e74f3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00321v3&entry.124074799=Read"},
{"title": "Instance-dependent Early Stopping", "author": "Suqin Yuan and Runqi Lin and Lei Feng and Bo Han and Tongliang Liu", "abstract": "  In machine learning practice, early stopping has been widely used to\nregularize models and can save computational costs by halting the training\nprocess when the model's performance on a validation set stops improving.\nHowever, conventional early stopping applies the same stopping criterion to all\ninstances without considering their individual learning statuses, which leads\nto redundant computations on instances that are already well-learned. To\nfurther improve the efficiency, we propose an Instance-dependent Early Stopping\n(IES) method that adapts the early stopping mechanism from the entire training\nset to the instance level, based on the core principle that once the model has\nmastered an instance, the training on it should stop. IES considers an instance\nas mastered if the second-order differences of its loss value remain within a\nsmall range around zero. This offers a more consistent measure of an instance's\nlearning status compared with directly using the loss value, and thus allows\nfor a unified threshold to determine when an instance can be excluded from\nfurther backpropagation. We show that excluding mastered instances from\nbackpropagation can increase the gradient norms, thereby accelerating the\ndecrease of the training loss and speeding up the training process. Extensive\nexperiments on benchmarks demonstrate that IES method can reduce\nbackpropagation instances by 10%-50% while maintaining or even slightly\nimproving the test accuracy and transfer learning performance of a model.\n", "link": "http://arxiv.org/abs/2502.07547v1", "date": "2025-02-11", "relevancy": 2.1654, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4548}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4246}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-dependent%20Early%20Stopping&body=Title%3A%20Instance-dependent%20Early%20Stopping%0AAuthor%3A%20Suqin%20Yuan%20and%20Runqi%20Lin%20and%20Lei%20Feng%20and%20Bo%20Han%20and%20Tongliang%20Liu%0AAbstract%3A%20%20%20In%20machine%20learning%20practice%2C%20early%20stopping%20has%20been%20widely%20used%20to%0Aregularize%20models%20and%20can%20save%20computational%20costs%20by%20halting%20the%20training%0Aprocess%20when%20the%20model%27s%20performance%20on%20a%20validation%20set%20stops%20improving.%0AHowever%2C%20conventional%20early%20stopping%20applies%20the%20same%20stopping%20criterion%20to%20all%0Ainstances%20without%20considering%20their%20individual%20learning%20statuses%2C%20which%20leads%0Ato%20redundant%20computations%20on%20instances%20that%20are%20already%20well-learned.%20To%0Afurther%20improve%20the%20efficiency%2C%20we%20propose%20an%20Instance-dependent%20Early%20Stopping%0A%28IES%29%20method%20that%20adapts%20the%20early%20stopping%20mechanism%20from%20the%20entire%20training%0Aset%20to%20the%20instance%20level%2C%20based%20on%20the%20core%20principle%20that%20once%20the%20model%20has%0Amastered%20an%20instance%2C%20the%20training%20on%20it%20should%20stop.%20IES%20considers%20an%20instance%0Aas%20mastered%20if%20the%20second-order%20differences%20of%20its%20loss%20value%20remain%20within%20a%0Asmall%20range%20around%20zero.%20This%20offers%20a%20more%20consistent%20measure%20of%20an%20instance%27s%0Alearning%20status%20compared%20with%20directly%20using%20the%20loss%20value%2C%20and%20thus%20allows%0Afor%20a%20unified%20threshold%20to%20determine%20when%20an%20instance%20can%20be%20excluded%20from%0Afurther%20backpropagation.%20We%20show%20that%20excluding%20mastered%20instances%20from%0Abackpropagation%20can%20increase%20the%20gradient%20norms%2C%20thereby%20accelerating%20the%0Adecrease%20of%20the%20training%20loss%20and%20speeding%20up%20the%20training%20process.%20Extensive%0Aexperiments%20on%20benchmarks%20demonstrate%20that%20IES%20method%20can%20reduce%0Abackpropagation%20instances%20by%2010%25-50%25%20while%20maintaining%20or%20even%20slightly%0Aimproving%20the%20test%20accuracy%20and%20transfer%20learning%20performance%20of%20a%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-dependent%2520Early%2520Stopping%26entry.906535625%3DSuqin%2520Yuan%2520and%2520Runqi%2520Lin%2520and%2520Lei%2520Feng%2520and%2520Bo%2520Han%2520and%2520Tongliang%2520Liu%26entry.1292438233%3D%2520%2520In%2520machine%2520learning%2520practice%252C%2520early%2520stopping%2520has%2520been%2520widely%2520used%2520to%250Aregularize%2520models%2520and%2520can%2520save%2520computational%2520costs%2520by%2520halting%2520the%2520training%250Aprocess%2520when%2520the%2520model%2527s%2520performance%2520on%2520a%2520validation%2520set%2520stops%2520improving.%250AHowever%252C%2520conventional%2520early%2520stopping%2520applies%2520the%2520same%2520stopping%2520criterion%2520to%2520all%250Ainstances%2520without%2520considering%2520their%2520individual%2520learning%2520statuses%252C%2520which%2520leads%250Ato%2520redundant%2520computations%2520on%2520instances%2520that%2520are%2520already%2520well-learned.%2520To%250Afurther%2520improve%2520the%2520efficiency%252C%2520we%2520propose%2520an%2520Instance-dependent%2520Early%2520Stopping%250A%2528IES%2529%2520method%2520that%2520adapts%2520the%2520early%2520stopping%2520mechanism%2520from%2520the%2520entire%2520training%250Aset%2520to%2520the%2520instance%2520level%252C%2520based%2520on%2520the%2520core%2520principle%2520that%2520once%2520the%2520model%2520has%250Amastered%2520an%2520instance%252C%2520the%2520training%2520on%2520it%2520should%2520stop.%2520IES%2520considers%2520an%2520instance%250Aas%2520mastered%2520if%2520the%2520second-order%2520differences%2520of%2520its%2520loss%2520value%2520remain%2520within%2520a%250Asmall%2520range%2520around%2520zero.%2520This%2520offers%2520a%2520more%2520consistent%2520measure%2520of%2520an%2520instance%2527s%250Alearning%2520status%2520compared%2520with%2520directly%2520using%2520the%2520loss%2520value%252C%2520and%2520thus%2520allows%250Afor%2520a%2520unified%2520threshold%2520to%2520determine%2520when%2520an%2520instance%2520can%2520be%2520excluded%2520from%250Afurther%2520backpropagation.%2520We%2520show%2520that%2520excluding%2520mastered%2520instances%2520from%250Abackpropagation%2520can%2520increase%2520the%2520gradient%2520norms%252C%2520thereby%2520accelerating%2520the%250Adecrease%2520of%2520the%2520training%2520loss%2520and%2520speeding%2520up%2520the%2520training%2520process.%2520Extensive%250Aexperiments%2520on%2520benchmarks%2520demonstrate%2520that%2520IES%2520method%2520can%2520reduce%250Abackpropagation%2520instances%2520by%252010%2525-50%2525%2520while%2520maintaining%2520or%2520even%2520slightly%250Aimproving%2520the%2520test%2520accuracy%2520and%2520transfer%2520learning%2520performance%2520of%2520a%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-dependent%20Early%20Stopping&entry.906535625=Suqin%20Yuan%20and%20Runqi%20Lin%20and%20Lei%20Feng%20and%20Bo%20Han%20and%20Tongliang%20Liu&entry.1292438233=%20%20In%20machine%20learning%20practice%2C%20early%20stopping%20has%20been%20widely%20used%20to%0Aregularize%20models%20and%20can%20save%20computational%20costs%20by%20halting%20the%20training%0Aprocess%20when%20the%20model%27s%20performance%20on%20a%20validation%20set%20stops%20improving.%0AHowever%2C%20conventional%20early%20stopping%20applies%20the%20same%20stopping%20criterion%20to%20all%0Ainstances%20without%20considering%20their%20individual%20learning%20statuses%2C%20which%20leads%0Ato%20redundant%20computations%20on%20instances%20that%20are%20already%20well-learned.%20To%0Afurther%20improve%20the%20efficiency%2C%20we%20propose%20an%20Instance-dependent%20Early%20Stopping%0A%28IES%29%20method%20that%20adapts%20the%20early%20stopping%20mechanism%20from%20the%20entire%20training%0Aset%20to%20the%20instance%20level%2C%20based%20on%20the%20core%20principle%20that%20once%20the%20model%20has%0Amastered%20an%20instance%2C%20the%20training%20on%20it%20should%20stop.%20IES%20considers%20an%20instance%0Aas%20mastered%20if%20the%20second-order%20differences%20of%20its%20loss%20value%20remain%20within%20a%0Asmall%20range%20around%20zero.%20This%20offers%20a%20more%20consistent%20measure%20of%20an%20instance%27s%0Alearning%20status%20compared%20with%20directly%20using%20the%20loss%20value%2C%20and%20thus%20allows%0Afor%20a%20unified%20threshold%20to%20determine%20when%20an%20instance%20can%20be%20excluded%20from%0Afurther%20backpropagation.%20We%20show%20that%20excluding%20mastered%20instances%20from%0Abackpropagation%20can%20increase%20the%20gradient%20norms%2C%20thereby%20accelerating%20the%0Adecrease%20of%20the%20training%20loss%20and%20speeding%20up%20the%20training%20process.%20Extensive%0Aexperiments%20on%20benchmarks%20demonstrate%20that%20IES%20method%20can%20reduce%0Abackpropagation%20instances%20by%2010%25-50%25%20while%20maintaining%20or%20even%20slightly%0Aimproving%20the%20test%20accuracy%20and%20transfer%20learning%20performance%20of%20a%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07547v1&entry.124074799=Read"},
{"title": "OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?", "author": "Zijian Chen and Tingzhu Chen and Wenjun Zhang and Guangtao Zhai", "abstract": "  We introduce OBI-Bench, a holistic benchmark crafted to systematically\nevaluate large multi-modal models (LMMs) on whole-process oracle bone\ninscriptions (OBI) processing tasks demanding expert-level domain knowledge and\ndeliberate cognition. OBI-Bench includes 5,523 meticulously collected\ndiverse-sourced images, covering five key domain problems: recognition,\nrejoining, classification, retrieval, and deciphering. These images span\ncenturies of archaeological findings and years of research by front-line\nscholars, comprising multi-stage font appearances from excavation to synthesis,\nsuch as original oracle bone, inked rubbings, oracle bone fragments, cropped\nsingle characters, and handprinted characters. Unlike existing benchmarks,\nOBI-Bench focuses on advanced visual perception and reasoning with OBI-specific\nknowledge, challenging LMMs to perform tasks akin to those faced by experts.\nThe evaluation of 6 proprietary LMMs as well as 17 open-source LMMs highlights\nthe substantial challenges and demands posed by OBI-Bench. Even the latest\nversions of GPT-4o, Gemini 1.5 Pro, and Qwen-VL-Max are still far from\npublic-level humans in some fine-grained perception tasks. However, they\nperform at a level comparable to untrained humans in deciphering tasks,\nindicating remarkable capabilities in offering new interpretative perspectives\nand generating creative guesses. We hope OBI-Bench can facilitate the community\nto develop domain-specific multi-modal foundation models towards ancient\nlanguage research and delve deeper to discover and enhance these untapped\npotentials of LMMs.\n", "link": "http://arxiv.org/abs/2412.01175v2", "date": "2025-02-11", "relevancy": 2.1603, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5531}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OBI-Bench%3A%20Can%20LMMs%20Aid%20in%20Study%20of%20Ancient%20Script%20on%20Oracle%20Bones%3F&body=Title%3A%20OBI-Bench%3A%20Can%20LMMs%20Aid%20in%20Study%20of%20Ancient%20Script%20on%20Oracle%20Bones%3F%0AAuthor%3A%20Zijian%20Chen%20and%20Tingzhu%20Chen%20and%20Wenjun%20Zhang%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20We%20introduce%20OBI-Bench%2C%20a%20holistic%20benchmark%20crafted%20to%20systematically%0Aevaluate%20large%20multi-modal%20models%20%28LMMs%29%20on%20whole-process%20oracle%20bone%0Ainscriptions%20%28OBI%29%20processing%20tasks%20demanding%20expert-level%20domain%20knowledge%20and%0Adeliberate%20cognition.%20OBI-Bench%20includes%205%2C523%20meticulously%20collected%0Adiverse-sourced%20images%2C%20covering%20five%20key%20domain%20problems%3A%20recognition%2C%0Arejoining%2C%20classification%2C%20retrieval%2C%20and%20deciphering.%20These%20images%20span%0Acenturies%20of%20archaeological%20findings%20and%20years%20of%20research%20by%20front-line%0Ascholars%2C%20comprising%20multi-stage%20font%20appearances%20from%20excavation%20to%20synthesis%2C%0Asuch%20as%20original%20oracle%20bone%2C%20inked%20rubbings%2C%20oracle%20bone%20fragments%2C%20cropped%0Asingle%20characters%2C%20and%20handprinted%20characters.%20Unlike%20existing%20benchmarks%2C%0AOBI-Bench%20focuses%20on%20advanced%20visual%20perception%20and%20reasoning%20with%20OBI-specific%0Aknowledge%2C%20challenging%20LMMs%20to%20perform%20tasks%20akin%20to%20those%20faced%20by%20experts.%0AThe%20evaluation%20of%206%20proprietary%20LMMs%20as%20well%20as%2017%20open-source%20LMMs%20highlights%0Athe%20substantial%20challenges%20and%20demands%20posed%20by%20OBI-Bench.%20Even%20the%20latest%0Aversions%20of%20GPT-4o%2C%20Gemini%201.5%20Pro%2C%20and%20Qwen-VL-Max%20are%20still%20far%20from%0Apublic-level%20humans%20in%20some%20fine-grained%20perception%20tasks.%20However%2C%20they%0Aperform%20at%20a%20level%20comparable%20to%20untrained%20humans%20in%20deciphering%20tasks%2C%0Aindicating%20remarkable%20capabilities%20in%20offering%20new%20interpretative%20perspectives%0Aand%20generating%20creative%20guesses.%20We%20hope%20OBI-Bench%20can%20facilitate%20the%20community%0Ato%20develop%20domain-specific%20multi-modal%20foundation%20models%20towards%20ancient%0Alanguage%20research%20and%20delve%20deeper%20to%20discover%20and%20enhance%20these%20untapped%0Apotentials%20of%20LMMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOBI-Bench%253A%2520Can%2520LMMs%2520Aid%2520in%2520Study%2520of%2520Ancient%2520Script%2520on%2520Oracle%2520Bones%253F%26entry.906535625%3DZijian%2520Chen%2520and%2520Tingzhu%2520Chen%2520and%2520Wenjun%2520Zhang%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520We%2520introduce%2520OBI-Bench%252C%2520a%2520holistic%2520benchmark%2520crafted%2520to%2520systematically%250Aevaluate%2520large%2520multi-modal%2520models%2520%2528LMMs%2529%2520on%2520whole-process%2520oracle%2520bone%250Ainscriptions%2520%2528OBI%2529%2520processing%2520tasks%2520demanding%2520expert-level%2520domain%2520knowledge%2520and%250Adeliberate%2520cognition.%2520OBI-Bench%2520includes%25205%252C523%2520meticulously%2520collected%250Adiverse-sourced%2520images%252C%2520covering%2520five%2520key%2520domain%2520problems%253A%2520recognition%252C%250Arejoining%252C%2520classification%252C%2520retrieval%252C%2520and%2520deciphering.%2520These%2520images%2520span%250Acenturies%2520of%2520archaeological%2520findings%2520and%2520years%2520of%2520research%2520by%2520front-line%250Ascholars%252C%2520comprising%2520multi-stage%2520font%2520appearances%2520from%2520excavation%2520to%2520synthesis%252C%250Asuch%2520as%2520original%2520oracle%2520bone%252C%2520inked%2520rubbings%252C%2520oracle%2520bone%2520fragments%252C%2520cropped%250Asingle%2520characters%252C%2520and%2520handprinted%2520characters.%2520Unlike%2520existing%2520benchmarks%252C%250AOBI-Bench%2520focuses%2520on%2520advanced%2520visual%2520perception%2520and%2520reasoning%2520with%2520OBI-specific%250Aknowledge%252C%2520challenging%2520LMMs%2520to%2520perform%2520tasks%2520akin%2520to%2520those%2520faced%2520by%2520experts.%250AThe%2520evaluation%2520of%25206%2520proprietary%2520LMMs%2520as%2520well%2520as%252017%2520open-source%2520LMMs%2520highlights%250Athe%2520substantial%2520challenges%2520and%2520demands%2520posed%2520by%2520OBI-Bench.%2520Even%2520the%2520latest%250Aversions%2520of%2520GPT-4o%252C%2520Gemini%25201.5%2520Pro%252C%2520and%2520Qwen-VL-Max%2520are%2520still%2520far%2520from%250Apublic-level%2520humans%2520in%2520some%2520fine-grained%2520perception%2520tasks.%2520However%252C%2520they%250Aperform%2520at%2520a%2520level%2520comparable%2520to%2520untrained%2520humans%2520in%2520deciphering%2520tasks%252C%250Aindicating%2520remarkable%2520capabilities%2520in%2520offering%2520new%2520interpretative%2520perspectives%250Aand%2520generating%2520creative%2520guesses.%2520We%2520hope%2520OBI-Bench%2520can%2520facilitate%2520the%2520community%250Ato%2520develop%2520domain-specific%2520multi-modal%2520foundation%2520models%2520towards%2520ancient%250Alanguage%2520research%2520and%2520delve%2520deeper%2520to%2520discover%2520and%2520enhance%2520these%2520untapped%250Apotentials%2520of%2520LMMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OBI-Bench%3A%20Can%20LMMs%20Aid%20in%20Study%20of%20Ancient%20Script%20on%20Oracle%20Bones%3F&entry.906535625=Zijian%20Chen%20and%20Tingzhu%20Chen%20and%20Wenjun%20Zhang%20and%20Guangtao%20Zhai&entry.1292438233=%20%20We%20introduce%20OBI-Bench%2C%20a%20holistic%20benchmark%20crafted%20to%20systematically%0Aevaluate%20large%20multi-modal%20models%20%28LMMs%29%20on%20whole-process%20oracle%20bone%0Ainscriptions%20%28OBI%29%20processing%20tasks%20demanding%20expert-level%20domain%20knowledge%20and%0Adeliberate%20cognition.%20OBI-Bench%20includes%205%2C523%20meticulously%20collected%0Adiverse-sourced%20images%2C%20covering%20five%20key%20domain%20problems%3A%20recognition%2C%0Arejoining%2C%20classification%2C%20retrieval%2C%20and%20deciphering.%20These%20images%20span%0Acenturies%20of%20archaeological%20findings%20and%20years%20of%20research%20by%20front-line%0Ascholars%2C%20comprising%20multi-stage%20font%20appearances%20from%20excavation%20to%20synthesis%2C%0Asuch%20as%20original%20oracle%20bone%2C%20inked%20rubbings%2C%20oracle%20bone%20fragments%2C%20cropped%0Asingle%20characters%2C%20and%20handprinted%20characters.%20Unlike%20existing%20benchmarks%2C%0AOBI-Bench%20focuses%20on%20advanced%20visual%20perception%20and%20reasoning%20with%20OBI-specific%0Aknowledge%2C%20challenging%20LMMs%20to%20perform%20tasks%20akin%20to%20those%20faced%20by%20experts.%0AThe%20evaluation%20of%206%20proprietary%20LMMs%20as%20well%20as%2017%20open-source%20LMMs%20highlights%0Athe%20substantial%20challenges%20and%20demands%20posed%20by%20OBI-Bench.%20Even%20the%20latest%0Aversions%20of%20GPT-4o%2C%20Gemini%201.5%20Pro%2C%20and%20Qwen-VL-Max%20are%20still%20far%20from%0Apublic-level%20humans%20in%20some%20fine-grained%20perception%20tasks.%20However%2C%20they%0Aperform%20at%20a%20level%20comparable%20to%20untrained%20humans%20in%20deciphering%20tasks%2C%0Aindicating%20remarkable%20capabilities%20in%20offering%20new%20interpretative%20perspectives%0Aand%20generating%20creative%20guesses.%20We%20hope%20OBI-Bench%20can%20facilitate%20the%20community%0Ato%20develop%20domain-specific%20multi-modal%20foundation%20models%20towards%20ancient%0Alanguage%20research%20and%20delve%20deeper%20to%20discover%20and%20enhance%20these%20untapped%0Apotentials%20of%20LMMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01175v2&entry.124074799=Read"},
{"title": "GaRLIO: Gravity enhanced Radar-LiDAR-Inertial Odometry", "author": "Chiyun Noh and Wooseong Yang and Minwoo Jung and Sangwoo Jung and Ayoung Kim", "abstract": "  Recently, gravity has been highlighted as a crucial constraint for state\nestimation to alleviate potential vertical drift. Existing online gravity\nestimation methods rely on pose estimation combined with IMU measurements,\nwhich is considered best practice when direct velocity measurements are\nunavailable. However, with radar sensors providing direct velocity data-a\nmeasurement not yet utilized for gravity estimation-we found a significant\nopportunity to improve gravity estimation accuracy substantially. GaRLIO, the\nproposed gravity-enhanced Radar-LiDAR-Inertial Odometry, can robustly predict\ngravity to reduce vertical drift while simultaneously enhancing state\nestimation performance using pointwise velocity measurements. Furthermore,\nGaRLIO ensures robustness in dynamic environments by utilizing radar to remove\ndynamic objects from LiDAR point clouds. Our method is validated through\nexperiments in various environments prone to vertical drift, demonstrating\nsuperior performance compared to traditional LiDAR-Inertial Odometry methods.\nWe make our source code publicly available to encourage further research and\ndevelopment. https://github.com/ChiyunNoh/GaRLIO\n", "link": "http://arxiv.org/abs/2502.07703v1", "date": "2025-02-11", "relevancy": 2.1582, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5512}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaRLIO%3A%20Gravity%20enhanced%20Radar-LiDAR-Inertial%20Odometry&body=Title%3A%20GaRLIO%3A%20Gravity%20enhanced%20Radar-LiDAR-Inertial%20Odometry%0AAuthor%3A%20Chiyun%20Noh%20and%20Wooseong%20Yang%20and%20Minwoo%20Jung%20and%20Sangwoo%20Jung%20and%20Ayoung%20Kim%0AAbstract%3A%20%20%20Recently%2C%20gravity%20has%20been%20highlighted%20as%20a%20crucial%20constraint%20for%20state%0Aestimation%20to%20alleviate%20potential%20vertical%20drift.%20Existing%20online%20gravity%0Aestimation%20methods%20rely%20on%20pose%20estimation%20combined%20with%20IMU%20measurements%2C%0Awhich%20is%20considered%20best%20practice%20when%20direct%20velocity%20measurements%20are%0Aunavailable.%20However%2C%20with%20radar%20sensors%20providing%20direct%20velocity%20data-a%0Ameasurement%20not%20yet%20utilized%20for%20gravity%20estimation-we%20found%20a%20significant%0Aopportunity%20to%20improve%20gravity%20estimation%20accuracy%20substantially.%20GaRLIO%2C%20the%0Aproposed%20gravity-enhanced%20Radar-LiDAR-Inertial%20Odometry%2C%20can%20robustly%20predict%0Agravity%20to%20reduce%20vertical%20drift%20while%20simultaneously%20enhancing%20state%0Aestimation%20performance%20using%20pointwise%20velocity%20measurements.%20Furthermore%2C%0AGaRLIO%20ensures%20robustness%20in%20dynamic%20environments%20by%20utilizing%20radar%20to%20remove%0Adynamic%20objects%20from%20LiDAR%20point%20clouds.%20Our%20method%20is%20validated%20through%0Aexperiments%20in%20various%20environments%20prone%20to%20vertical%20drift%2C%20demonstrating%0Asuperior%20performance%20compared%20to%20traditional%20LiDAR-Inertial%20Odometry%20methods.%0AWe%20make%20our%20source%20code%20publicly%20available%20to%20encourage%20further%20research%20and%0Adevelopment.%20https%3A//github.com/ChiyunNoh/GaRLIO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaRLIO%253A%2520Gravity%2520enhanced%2520Radar-LiDAR-Inertial%2520Odometry%26entry.906535625%3DChiyun%2520Noh%2520and%2520Wooseong%2520Yang%2520and%2520Minwoo%2520Jung%2520and%2520Sangwoo%2520Jung%2520and%2520Ayoung%2520Kim%26entry.1292438233%3D%2520%2520Recently%252C%2520gravity%2520has%2520been%2520highlighted%2520as%2520a%2520crucial%2520constraint%2520for%2520state%250Aestimation%2520to%2520alleviate%2520potential%2520vertical%2520drift.%2520Existing%2520online%2520gravity%250Aestimation%2520methods%2520rely%2520on%2520pose%2520estimation%2520combined%2520with%2520IMU%2520measurements%252C%250Awhich%2520is%2520considered%2520best%2520practice%2520when%2520direct%2520velocity%2520measurements%2520are%250Aunavailable.%2520However%252C%2520with%2520radar%2520sensors%2520providing%2520direct%2520velocity%2520data-a%250Ameasurement%2520not%2520yet%2520utilized%2520for%2520gravity%2520estimation-we%2520found%2520a%2520significant%250Aopportunity%2520to%2520improve%2520gravity%2520estimation%2520accuracy%2520substantially.%2520GaRLIO%252C%2520the%250Aproposed%2520gravity-enhanced%2520Radar-LiDAR-Inertial%2520Odometry%252C%2520can%2520robustly%2520predict%250Agravity%2520to%2520reduce%2520vertical%2520drift%2520while%2520simultaneously%2520enhancing%2520state%250Aestimation%2520performance%2520using%2520pointwise%2520velocity%2520measurements.%2520Furthermore%252C%250AGaRLIO%2520ensures%2520robustness%2520in%2520dynamic%2520environments%2520by%2520utilizing%2520radar%2520to%2520remove%250Adynamic%2520objects%2520from%2520LiDAR%2520point%2520clouds.%2520Our%2520method%2520is%2520validated%2520through%250Aexperiments%2520in%2520various%2520environments%2520prone%2520to%2520vertical%2520drift%252C%2520demonstrating%250Asuperior%2520performance%2520compared%2520to%2520traditional%2520LiDAR-Inertial%2520Odometry%2520methods.%250AWe%2520make%2520our%2520source%2520code%2520publicly%2520available%2520to%2520encourage%2520further%2520research%2520and%250Adevelopment.%2520https%253A//github.com/ChiyunNoh/GaRLIO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaRLIO%3A%20Gravity%20enhanced%20Radar-LiDAR-Inertial%20Odometry&entry.906535625=Chiyun%20Noh%20and%20Wooseong%20Yang%20and%20Minwoo%20Jung%20and%20Sangwoo%20Jung%20and%20Ayoung%20Kim&entry.1292438233=%20%20Recently%2C%20gravity%20has%20been%20highlighted%20as%20a%20crucial%20constraint%20for%20state%0Aestimation%20to%20alleviate%20potential%20vertical%20drift.%20Existing%20online%20gravity%0Aestimation%20methods%20rely%20on%20pose%20estimation%20combined%20with%20IMU%20measurements%2C%0Awhich%20is%20considered%20best%20practice%20when%20direct%20velocity%20measurements%20are%0Aunavailable.%20However%2C%20with%20radar%20sensors%20providing%20direct%20velocity%20data-a%0Ameasurement%20not%20yet%20utilized%20for%20gravity%20estimation-we%20found%20a%20significant%0Aopportunity%20to%20improve%20gravity%20estimation%20accuracy%20substantially.%20GaRLIO%2C%20the%0Aproposed%20gravity-enhanced%20Radar-LiDAR-Inertial%20Odometry%2C%20can%20robustly%20predict%0Agravity%20to%20reduce%20vertical%20drift%20while%20simultaneously%20enhancing%20state%0Aestimation%20performance%20using%20pointwise%20velocity%20measurements.%20Furthermore%2C%0AGaRLIO%20ensures%20robustness%20in%20dynamic%20environments%20by%20utilizing%20radar%20to%20remove%0Adynamic%20objects%20from%20LiDAR%20point%20clouds.%20Our%20method%20is%20validated%20through%0Aexperiments%20in%20various%20environments%20prone%20to%20vertical%20drift%2C%20demonstrating%0Asuperior%20performance%20compared%20to%20traditional%20LiDAR-Inertial%20Odometry%20methods.%0AWe%20make%20our%20source%20code%20publicly%20available%20to%20encourage%20further%20research%20and%0Adevelopment.%20https%3A//github.com/ChiyunNoh/GaRLIO%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07703v1&entry.124074799=Read"},
{"title": "Single-Step Consistent Diffusion Samplers", "author": "Pascal Jutras-Dub\u00e9 and Patrick Pynadath and Ruqi Zhang", "abstract": "  Sampling from unnormalized target distributions is a fundamental yet\nchallenging task in machine learning and statistics. Existing sampling\nalgorithms typically require many iterative steps to produce high-quality\nsamples, leading to high computational costs that limit their practicality in\ntime-sensitive or resource-constrained settings. In this work, we introduce\nconsistent diffusion samplers, a new class of samplers designed to generate\nhigh-fidelity samples in a single step. We first develop a distillation\nalgorithm to train a consistent diffusion sampler from a pretrained diffusion\nmodel without pre-collecting large datasets of samples. Our algorithm leverages\nincomplete sampling trajectories and noisy intermediate states directly from\nthe diffusion process. We further propose a method to train a consistent\ndiffusion sampler from scratch, fully amortizing exploration by training a\nsingle model that both performs diffusion sampling and skips intermediate steps\nusing a self-consistency loss. Through extensive experiments on a variety of\nunnormalized distributions, we show that our approach yields high-fidelity\nsamples using less than 1% of the network evaluations required by traditional\ndiffusion samplers.\n", "link": "http://arxiv.org/abs/2502.07579v1", "date": "2025-02-11", "relevancy": 2.146, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5893}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5644}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Step%20Consistent%20Diffusion%20Samplers&body=Title%3A%20Single-Step%20Consistent%20Diffusion%20Samplers%0AAuthor%3A%20Pascal%20Jutras-Dub%C3%A9%20and%20Patrick%20Pynadath%20and%20Ruqi%20Zhang%0AAbstract%3A%20%20%20Sampling%20from%20unnormalized%20target%20distributions%20is%20a%20fundamental%20yet%0Achallenging%20task%20in%20machine%20learning%20and%20statistics.%20Existing%20sampling%0Aalgorithms%20typically%20require%20many%20iterative%20steps%20to%20produce%20high-quality%0Asamples%2C%20leading%20to%20high%20computational%20costs%20that%20limit%20their%20practicality%20in%0Atime-sensitive%20or%20resource-constrained%20settings.%20In%20this%20work%2C%20we%20introduce%0Aconsistent%20diffusion%20samplers%2C%20a%20new%20class%20of%20samplers%20designed%20to%20generate%0Ahigh-fidelity%20samples%20in%20a%20single%20step.%20We%20first%20develop%20a%20distillation%0Aalgorithm%20to%20train%20a%20consistent%20diffusion%20sampler%20from%20a%20pretrained%20diffusion%0Amodel%20without%20pre-collecting%20large%20datasets%20of%20samples.%20Our%20algorithm%20leverages%0Aincomplete%20sampling%20trajectories%20and%20noisy%20intermediate%20states%20directly%20from%0Athe%20diffusion%20process.%20We%20further%20propose%20a%20method%20to%20train%20a%20consistent%0Adiffusion%20sampler%20from%20scratch%2C%20fully%20amortizing%20exploration%20by%20training%20a%0Asingle%20model%20that%20both%20performs%20diffusion%20sampling%20and%20skips%20intermediate%20steps%0Ausing%20a%20self-consistency%20loss.%20Through%20extensive%20experiments%20on%20a%20variety%20of%0Aunnormalized%20distributions%2C%20we%20show%20that%20our%20approach%20yields%20high-fidelity%0Asamples%20using%20less%20than%201%25%20of%20the%20network%20evaluations%20required%20by%20traditional%0Adiffusion%20samplers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Step%2520Consistent%2520Diffusion%2520Samplers%26entry.906535625%3DPascal%2520Jutras-Dub%25C3%25A9%2520and%2520Patrick%2520Pynadath%2520and%2520Ruqi%2520Zhang%26entry.1292438233%3D%2520%2520Sampling%2520from%2520unnormalized%2520target%2520distributions%2520is%2520a%2520fundamental%2520yet%250Achallenging%2520task%2520in%2520machine%2520learning%2520and%2520statistics.%2520Existing%2520sampling%250Aalgorithms%2520typically%2520require%2520many%2520iterative%2520steps%2520to%2520produce%2520high-quality%250Asamples%252C%2520leading%2520to%2520high%2520computational%2520costs%2520that%2520limit%2520their%2520practicality%2520in%250Atime-sensitive%2520or%2520resource-constrained%2520settings.%2520In%2520this%2520work%252C%2520we%2520introduce%250Aconsistent%2520diffusion%2520samplers%252C%2520a%2520new%2520class%2520of%2520samplers%2520designed%2520to%2520generate%250Ahigh-fidelity%2520samples%2520in%2520a%2520single%2520step.%2520We%2520first%2520develop%2520a%2520distillation%250Aalgorithm%2520to%2520train%2520a%2520consistent%2520diffusion%2520sampler%2520from%2520a%2520pretrained%2520diffusion%250Amodel%2520without%2520pre-collecting%2520large%2520datasets%2520of%2520samples.%2520Our%2520algorithm%2520leverages%250Aincomplete%2520sampling%2520trajectories%2520and%2520noisy%2520intermediate%2520states%2520directly%2520from%250Athe%2520diffusion%2520process.%2520We%2520further%2520propose%2520a%2520method%2520to%2520train%2520a%2520consistent%250Adiffusion%2520sampler%2520from%2520scratch%252C%2520fully%2520amortizing%2520exploration%2520by%2520training%2520a%250Asingle%2520model%2520that%2520both%2520performs%2520diffusion%2520sampling%2520and%2520skips%2520intermediate%2520steps%250Ausing%2520a%2520self-consistency%2520loss.%2520Through%2520extensive%2520experiments%2520on%2520a%2520variety%2520of%250Aunnormalized%2520distributions%252C%2520we%2520show%2520that%2520our%2520approach%2520yields%2520high-fidelity%250Asamples%2520using%2520less%2520than%25201%2525%2520of%2520the%2520network%2520evaluations%2520required%2520by%2520traditional%250Adiffusion%2520samplers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Step%20Consistent%20Diffusion%20Samplers&entry.906535625=Pascal%20Jutras-Dub%C3%A9%20and%20Patrick%20Pynadath%20and%20Ruqi%20Zhang&entry.1292438233=%20%20Sampling%20from%20unnormalized%20target%20distributions%20is%20a%20fundamental%20yet%0Achallenging%20task%20in%20machine%20learning%20and%20statistics.%20Existing%20sampling%0Aalgorithms%20typically%20require%20many%20iterative%20steps%20to%20produce%20high-quality%0Asamples%2C%20leading%20to%20high%20computational%20costs%20that%20limit%20their%20practicality%20in%0Atime-sensitive%20or%20resource-constrained%20settings.%20In%20this%20work%2C%20we%20introduce%0Aconsistent%20diffusion%20samplers%2C%20a%20new%20class%20of%20samplers%20designed%20to%20generate%0Ahigh-fidelity%20samples%20in%20a%20single%20step.%20We%20first%20develop%20a%20distillation%0Aalgorithm%20to%20train%20a%20consistent%20diffusion%20sampler%20from%20a%20pretrained%20diffusion%0Amodel%20without%20pre-collecting%20large%20datasets%20of%20samples.%20Our%20algorithm%20leverages%0Aincomplete%20sampling%20trajectories%20and%20noisy%20intermediate%20states%20directly%20from%0Athe%20diffusion%20process.%20We%20further%20propose%20a%20method%20to%20train%20a%20consistent%0Adiffusion%20sampler%20from%20scratch%2C%20fully%20amortizing%20exploration%20by%20training%20a%0Asingle%20model%20that%20both%20performs%20diffusion%20sampling%20and%20skips%20intermediate%20steps%0Ausing%20a%20self-consistency%20loss.%20Through%20extensive%20experiments%20on%20a%20variety%20of%0Aunnormalized%20distributions%2C%20we%20show%20that%20our%20approach%20yields%20high-fidelity%0Asamples%20using%20less%20than%201%25%20of%20the%20network%20evaluations%20required%20by%20traditional%0Adiffusion%20samplers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07579v1&entry.124074799=Read"},
{"title": "Distributed Coverage Control for Time-Varying Spatial Processes", "author": "Federico Pratissoli and Mattia Mantovani and Amanda Prorok and Lorenzo Sabattini", "abstract": "  Multi-robot systems are essential for environmental monitoring, particularly\nfor tracking spatial phenomena like pollution, soil minerals, and water\nsalinity, and more. This study addresses the challenge of deploying a\nmulti-robot team for optimal coverage in environments where the density\ndistribution, describing areas of interest, is unknown and changes over time.\nWe propose a fully distributed control strategy that uses Gaussian Processes\n(GPs) to model the spatial field and balance the trade-off between learning the\nfield and optimally covering it. Unlike existing approaches, we address a more\nrealistic scenario by handling time-varying spatial fields, where the\nexploration-exploitation trade-off is dynamically adjusted over time. Each\nrobot operates locally, using only its own collected data and the information\nshared by the neighboring robots. To address the computational limits of GPs,\nthe algorithm efficiently manages the volume of data by selecting only the most\nrelevant samples for the process estimation. The performance of the proposed\nalgorithm is evaluated through several simulations and experiments,\nincorporating real-world data phenomena to validate its effectiveness.\n", "link": "http://arxiv.org/abs/2502.07595v1", "date": "2025-02-11", "relevancy": 2.1437, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5391}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Coverage%20Control%20for%20Time-Varying%20Spatial%20Processes&body=Title%3A%20Distributed%20Coverage%20Control%20for%20Time-Varying%20Spatial%20Processes%0AAuthor%3A%20Federico%20Pratissoli%20and%20Mattia%20Mantovani%20and%20Amanda%20Prorok%20and%20Lorenzo%20Sabattini%0AAbstract%3A%20%20%20Multi-robot%20systems%20are%20essential%20for%20environmental%20monitoring%2C%20particularly%0Afor%20tracking%20spatial%20phenomena%20like%20pollution%2C%20soil%20minerals%2C%20and%20water%0Asalinity%2C%20and%20more.%20This%20study%20addresses%20the%20challenge%20of%20deploying%20a%0Amulti-robot%20team%20for%20optimal%20coverage%20in%20environments%20where%20the%20density%0Adistribution%2C%20describing%20areas%20of%20interest%2C%20is%20unknown%20and%20changes%20over%20time.%0AWe%20propose%20a%20fully%20distributed%20control%20strategy%20that%20uses%20Gaussian%20Processes%0A%28GPs%29%20to%20model%20the%20spatial%20field%20and%20balance%20the%20trade-off%20between%20learning%20the%0Afield%20and%20optimally%20covering%20it.%20Unlike%20existing%20approaches%2C%20we%20address%20a%20more%0Arealistic%20scenario%20by%20handling%20time-varying%20spatial%20fields%2C%20where%20the%0Aexploration-exploitation%20trade-off%20is%20dynamically%20adjusted%20over%20time.%20Each%0Arobot%20operates%20locally%2C%20using%20only%20its%20own%20collected%20data%20and%20the%20information%0Ashared%20by%20the%20neighboring%20robots.%20To%20address%20the%20computational%20limits%20of%20GPs%2C%0Athe%20algorithm%20efficiently%20manages%20the%20volume%20of%20data%20by%20selecting%20only%20the%20most%0Arelevant%20samples%20for%20the%20process%20estimation.%20The%20performance%20of%20the%20proposed%0Aalgorithm%20is%20evaluated%20through%20several%20simulations%20and%20experiments%2C%0Aincorporating%20real-world%20data%20phenomena%20to%20validate%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Coverage%2520Control%2520for%2520Time-Varying%2520Spatial%2520Processes%26entry.906535625%3DFederico%2520Pratissoli%2520and%2520Mattia%2520Mantovani%2520and%2520Amanda%2520Prorok%2520and%2520Lorenzo%2520Sabattini%26entry.1292438233%3D%2520%2520Multi-robot%2520systems%2520are%2520essential%2520for%2520environmental%2520monitoring%252C%2520particularly%250Afor%2520tracking%2520spatial%2520phenomena%2520like%2520pollution%252C%2520soil%2520minerals%252C%2520and%2520water%250Asalinity%252C%2520and%2520more.%2520This%2520study%2520addresses%2520the%2520challenge%2520of%2520deploying%2520a%250Amulti-robot%2520team%2520for%2520optimal%2520coverage%2520in%2520environments%2520where%2520the%2520density%250Adistribution%252C%2520describing%2520areas%2520of%2520interest%252C%2520is%2520unknown%2520and%2520changes%2520over%2520time.%250AWe%2520propose%2520a%2520fully%2520distributed%2520control%2520strategy%2520that%2520uses%2520Gaussian%2520Processes%250A%2528GPs%2529%2520to%2520model%2520the%2520spatial%2520field%2520and%2520balance%2520the%2520trade-off%2520between%2520learning%2520the%250Afield%2520and%2520optimally%2520covering%2520it.%2520Unlike%2520existing%2520approaches%252C%2520we%2520address%2520a%2520more%250Arealistic%2520scenario%2520by%2520handling%2520time-varying%2520spatial%2520fields%252C%2520where%2520the%250Aexploration-exploitation%2520trade-off%2520is%2520dynamically%2520adjusted%2520over%2520time.%2520Each%250Arobot%2520operates%2520locally%252C%2520using%2520only%2520its%2520own%2520collected%2520data%2520and%2520the%2520information%250Ashared%2520by%2520the%2520neighboring%2520robots.%2520To%2520address%2520the%2520computational%2520limits%2520of%2520GPs%252C%250Athe%2520algorithm%2520efficiently%2520manages%2520the%2520volume%2520of%2520data%2520by%2520selecting%2520only%2520the%2520most%250Arelevant%2520samples%2520for%2520the%2520process%2520estimation.%2520The%2520performance%2520of%2520the%2520proposed%250Aalgorithm%2520is%2520evaluated%2520through%2520several%2520simulations%2520and%2520experiments%252C%250Aincorporating%2520real-world%2520data%2520phenomena%2520to%2520validate%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Coverage%20Control%20for%20Time-Varying%20Spatial%20Processes&entry.906535625=Federico%20Pratissoli%20and%20Mattia%20Mantovani%20and%20Amanda%20Prorok%20and%20Lorenzo%20Sabattini&entry.1292438233=%20%20Multi-robot%20systems%20are%20essential%20for%20environmental%20monitoring%2C%20particularly%0Afor%20tracking%20spatial%20phenomena%20like%20pollution%2C%20soil%20minerals%2C%20and%20water%0Asalinity%2C%20and%20more.%20This%20study%20addresses%20the%20challenge%20of%20deploying%20a%0Amulti-robot%20team%20for%20optimal%20coverage%20in%20environments%20where%20the%20density%0Adistribution%2C%20describing%20areas%20of%20interest%2C%20is%20unknown%20and%20changes%20over%20time.%0AWe%20propose%20a%20fully%20distributed%20control%20strategy%20that%20uses%20Gaussian%20Processes%0A%28GPs%29%20to%20model%20the%20spatial%20field%20and%20balance%20the%20trade-off%20between%20learning%20the%0Afield%20and%20optimally%20covering%20it.%20Unlike%20existing%20approaches%2C%20we%20address%20a%20more%0Arealistic%20scenario%20by%20handling%20time-varying%20spatial%20fields%2C%20where%20the%0Aexploration-exploitation%20trade-off%20is%20dynamically%20adjusted%20over%20time.%20Each%0Arobot%20operates%20locally%2C%20using%20only%20its%20own%20collected%20data%20and%20the%20information%0Ashared%20by%20the%20neighboring%20robots.%20To%20address%20the%20computational%20limits%20of%20GPs%2C%0Athe%20algorithm%20efficiently%20manages%20the%20volume%20of%20data%20by%20selecting%20only%20the%20most%0Arelevant%20samples%20for%20the%20process%20estimation.%20The%20performance%20of%20the%20proposed%0Aalgorithm%20is%20evaluated%20through%20several%20simulations%20and%20experiments%2C%0Aincorporating%20real-world%20data%20phenomena%20to%20validate%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07595v1&entry.124074799=Read"},
{"title": "Learning from Demonstration with Implicit Nonlinear Dynamics Models", "author": "Peter David Fagan and Subramanian Ramamoorthy", "abstract": "  Learning from Demonstration (LfD) is a useful paradigm for training policies\nthat solve tasks involving complex motions, such as those encountered in\nrobotic manipulation. In practice, the successful application of LfD requires\novercoming error accumulation during policy execution, i.e. the problem of\ndrift due to errors compounding over time and the consequent\nout-of-distribution behaviours. Existing works seek to address this problem\nthrough scaling data collection, correcting policy errors with a\nhuman-in-the-loop, temporally ensembling policy predictions or through learning\na dynamical system model with convergence guarantees. In this work, we propose\nand validate an alternative approach to overcoming this issue. Inspired by\nreservoir computing, we develop a recurrent neural network layer that includes\na fixed nonlinear dynamical system with tunable dynamical properties for\nmodelling temporal dynamics. We validate the efficacy of our neural network\nlayer on the task of reproducing human handwriting motions using the LASA Human\nHandwriting Dataset. Through empirical experiments we demonstrate that\nincorporating our layer into existing neural network architectures addresses\nthe issue of compounding errors in LfD. Furthermore, we perform a comparative\nevaluation against existing approaches including a temporal ensemble of policy\npredictions and an Echo State Network (ESN) implementation. We find that our\napproach yields greater policy precision and robustness on the handwriting task\nwhile also generalising to multiple dynamics regimes and maintaining\ncompetitive latency scores.\n", "link": "http://arxiv.org/abs/2409.18768v3", "date": "2025-02-11", "relevancy": 2.1152, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5332}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5283}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Demonstration%20with%20Implicit%20Nonlinear%20Dynamics%20Models&body=Title%3A%20Learning%20from%20Demonstration%20with%20Implicit%20Nonlinear%20Dynamics%20Models%0AAuthor%3A%20Peter%20David%20Fagan%20and%20Subramanian%20Ramamoorthy%0AAbstract%3A%20%20%20Learning%20from%20Demonstration%20%28LfD%29%20is%20a%20useful%20paradigm%20for%20training%20policies%0Athat%20solve%20tasks%20involving%20complex%20motions%2C%20such%20as%20those%20encountered%20in%0Arobotic%20manipulation.%20In%20practice%2C%20the%20successful%20application%20of%20LfD%20requires%0Aovercoming%20error%20accumulation%20during%20policy%20execution%2C%20i.e.%20the%20problem%20of%0Adrift%20due%20to%20errors%20compounding%20over%20time%20and%20the%20consequent%0Aout-of-distribution%20behaviours.%20Existing%20works%20seek%20to%20address%20this%20problem%0Athrough%20scaling%20data%20collection%2C%20correcting%20policy%20errors%20with%20a%0Ahuman-in-the-loop%2C%20temporally%20ensembling%20policy%20predictions%20or%20through%20learning%0Aa%20dynamical%20system%20model%20with%20convergence%20guarantees.%20In%20this%20work%2C%20we%20propose%0Aand%20validate%20an%20alternative%20approach%20to%20overcoming%20this%20issue.%20Inspired%20by%0Areservoir%20computing%2C%20we%20develop%20a%20recurrent%20neural%20network%20layer%20that%20includes%0Aa%20fixed%20nonlinear%20dynamical%20system%20with%20tunable%20dynamical%20properties%20for%0Amodelling%20temporal%20dynamics.%20We%20validate%20the%20efficacy%20of%20our%20neural%20network%0Alayer%20on%20the%20task%20of%20reproducing%20human%20handwriting%20motions%20using%20the%20LASA%20Human%0AHandwriting%20Dataset.%20Through%20empirical%20experiments%20we%20demonstrate%20that%0Aincorporating%20our%20layer%20into%20existing%20neural%20network%20architectures%20addresses%0Athe%20issue%20of%20compounding%20errors%20in%20LfD.%20Furthermore%2C%20we%20perform%20a%20comparative%0Aevaluation%20against%20existing%20approaches%20including%20a%20temporal%20ensemble%20of%20policy%0Apredictions%20and%20an%20Echo%20State%20Network%20%28ESN%29%20implementation.%20We%20find%20that%20our%0Aapproach%20yields%20greater%20policy%20precision%20and%20robustness%20on%20the%20handwriting%20task%0Awhile%20also%20generalising%20to%20multiple%20dynamics%20regimes%20and%20maintaining%0Acompetitive%20latency%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18768v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Demonstration%2520with%2520Implicit%2520Nonlinear%2520Dynamics%2520Models%26entry.906535625%3DPeter%2520David%2520Fagan%2520and%2520Subramanian%2520Ramamoorthy%26entry.1292438233%3D%2520%2520Learning%2520from%2520Demonstration%2520%2528LfD%2529%2520is%2520a%2520useful%2520paradigm%2520for%2520training%2520policies%250Athat%2520solve%2520tasks%2520involving%2520complex%2520motions%252C%2520such%2520as%2520those%2520encountered%2520in%250Arobotic%2520manipulation.%2520In%2520practice%252C%2520the%2520successful%2520application%2520of%2520LfD%2520requires%250Aovercoming%2520error%2520accumulation%2520during%2520policy%2520execution%252C%2520i.e.%2520the%2520problem%2520of%250Adrift%2520due%2520to%2520errors%2520compounding%2520over%2520time%2520and%2520the%2520consequent%250Aout-of-distribution%2520behaviours.%2520Existing%2520works%2520seek%2520to%2520address%2520this%2520problem%250Athrough%2520scaling%2520data%2520collection%252C%2520correcting%2520policy%2520errors%2520with%2520a%250Ahuman-in-the-loop%252C%2520temporally%2520ensembling%2520policy%2520predictions%2520or%2520through%2520learning%250Aa%2520dynamical%2520system%2520model%2520with%2520convergence%2520guarantees.%2520In%2520this%2520work%252C%2520we%2520propose%250Aand%2520validate%2520an%2520alternative%2520approach%2520to%2520overcoming%2520this%2520issue.%2520Inspired%2520by%250Areservoir%2520computing%252C%2520we%2520develop%2520a%2520recurrent%2520neural%2520network%2520layer%2520that%2520includes%250Aa%2520fixed%2520nonlinear%2520dynamical%2520system%2520with%2520tunable%2520dynamical%2520properties%2520for%250Amodelling%2520temporal%2520dynamics.%2520We%2520validate%2520the%2520efficacy%2520of%2520our%2520neural%2520network%250Alayer%2520on%2520the%2520task%2520of%2520reproducing%2520human%2520handwriting%2520motions%2520using%2520the%2520LASA%2520Human%250AHandwriting%2520Dataset.%2520Through%2520empirical%2520experiments%2520we%2520demonstrate%2520that%250Aincorporating%2520our%2520layer%2520into%2520existing%2520neural%2520network%2520architectures%2520addresses%250Athe%2520issue%2520of%2520compounding%2520errors%2520in%2520LfD.%2520Furthermore%252C%2520we%2520perform%2520a%2520comparative%250Aevaluation%2520against%2520existing%2520approaches%2520including%2520a%2520temporal%2520ensemble%2520of%2520policy%250Apredictions%2520and%2520an%2520Echo%2520State%2520Network%2520%2528ESN%2529%2520implementation.%2520We%2520find%2520that%2520our%250Aapproach%2520yields%2520greater%2520policy%2520precision%2520and%2520robustness%2520on%2520the%2520handwriting%2520task%250Awhile%2520also%2520generalising%2520to%2520multiple%2520dynamics%2520regimes%2520and%2520maintaining%250Acompetitive%2520latency%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18768v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Demonstration%20with%20Implicit%20Nonlinear%20Dynamics%20Models&entry.906535625=Peter%20David%20Fagan%20and%20Subramanian%20Ramamoorthy&entry.1292438233=%20%20Learning%20from%20Demonstration%20%28LfD%29%20is%20a%20useful%20paradigm%20for%20training%20policies%0Athat%20solve%20tasks%20involving%20complex%20motions%2C%20such%20as%20those%20encountered%20in%0Arobotic%20manipulation.%20In%20practice%2C%20the%20successful%20application%20of%20LfD%20requires%0Aovercoming%20error%20accumulation%20during%20policy%20execution%2C%20i.e.%20the%20problem%20of%0Adrift%20due%20to%20errors%20compounding%20over%20time%20and%20the%20consequent%0Aout-of-distribution%20behaviours.%20Existing%20works%20seek%20to%20address%20this%20problem%0Athrough%20scaling%20data%20collection%2C%20correcting%20policy%20errors%20with%20a%0Ahuman-in-the-loop%2C%20temporally%20ensembling%20policy%20predictions%20or%20through%20learning%0Aa%20dynamical%20system%20model%20with%20convergence%20guarantees.%20In%20this%20work%2C%20we%20propose%0Aand%20validate%20an%20alternative%20approach%20to%20overcoming%20this%20issue.%20Inspired%20by%0Areservoir%20computing%2C%20we%20develop%20a%20recurrent%20neural%20network%20layer%20that%20includes%0Aa%20fixed%20nonlinear%20dynamical%20system%20with%20tunable%20dynamical%20properties%20for%0Amodelling%20temporal%20dynamics.%20We%20validate%20the%20efficacy%20of%20our%20neural%20network%0Alayer%20on%20the%20task%20of%20reproducing%20human%20handwriting%20motions%20using%20the%20LASA%20Human%0AHandwriting%20Dataset.%20Through%20empirical%20experiments%20we%20demonstrate%20that%0Aincorporating%20our%20layer%20into%20existing%20neural%20network%20architectures%20addresses%0Athe%20issue%20of%20compounding%20errors%20in%20LfD.%20Furthermore%2C%20we%20perform%20a%20comparative%0Aevaluation%20against%20existing%20approaches%20including%20a%20temporal%20ensemble%20of%20policy%0Apredictions%20and%20an%20Echo%20State%20Network%20%28ESN%29%20implementation.%20We%20find%20that%20our%0Aapproach%20yields%20greater%20policy%20precision%20and%20robustness%20on%20the%20handwriting%20task%0Awhile%20also%20generalising%20to%20multiple%20dynamics%20regimes%20and%20maintaining%0Acompetitive%20latency%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18768v3&entry.124074799=Read"},
{"title": "ViSIR: Vision Transformer Single Image Reconstruction Method for Earth\n  System Models", "author": "Ehsan Zeraatkar and Salah Faroughi and Jelena Te\u0161i\u0107", "abstract": "  Purpose: Earth system models (ESMs) integrate the interactions of the\natmosphere, ocean, land, ice, and biosphere to estimate the state of regional\nand global climate under a wide variety of conditions. The ESMs are highly\ncomplex, and thus, deep neural network architectures are used to model the\ncomplexity and store the down-sampled data. In this paper, we propose the\nVision Transformer Sinusoidal Representation Networks (ViSIR) to improve the\nsingle image SR (SR) reconstruction task for the ESM data.\n  Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with\nthe high-frequency detail preservation of the Sinusoidal Representation Network\n(SIREN) to address the spectral bias observed in SR tasks.\n  Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and\nSR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three\ndifferent measurements.\n  Conclusion: The proposed ViSIR is evaluated and compared with\nstate-of-the-art methods. The results show that the proposed algorithm is\noutperforming other methods in terms of Mean Square Error(MSE),\nPeak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index\nMeasure(SSIM).\n", "link": "http://arxiv.org/abs/2502.06741v2", "date": "2025-02-11", "relevancy": 2.113, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5301}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSIR%3A%20Vision%20Transformer%20Single%20Image%20Reconstruction%20Method%20for%20Earth%0A%20%20System%20Models&body=Title%3A%20ViSIR%3A%20Vision%20Transformer%20Single%20Image%20Reconstruction%20Method%20for%20Earth%0A%20%20System%20Models%0AAuthor%3A%20Ehsan%20Zeraatkar%20and%20Salah%20Faroughi%20and%20Jelena%20Te%C5%A1i%C4%87%0AAbstract%3A%20%20%20Purpose%3A%20Earth%20system%20models%20%28ESMs%29%20integrate%20the%20interactions%20of%20the%0Aatmosphere%2C%20ocean%2C%20land%2C%20ice%2C%20and%20biosphere%20to%20estimate%20the%20state%20of%20regional%0Aand%20global%20climate%20under%20a%20wide%20variety%20of%20conditions.%20The%20ESMs%20are%20highly%0Acomplex%2C%20and%20thus%2C%20deep%20neural%20network%20architectures%20are%20used%20to%20model%20the%0Acomplexity%20and%20store%20the%20down-sampled%20data.%20In%20this%20paper%2C%20we%20propose%20the%0AVision%20Transformer%20Sinusoidal%20Representation%20Networks%20%28ViSIR%29%20to%20improve%20the%0Asingle%20image%20SR%20%28SR%29%20reconstruction%20task%20for%20the%20ESM%20data.%0A%20%20Methods%3A%20ViSIR%20combines%20the%20SR%20capability%20of%20Vision%20Transformers%20%28ViT%29%20with%0Athe%20high-frequency%20detail%20preservation%20of%20the%20Sinusoidal%20Representation%20Network%0A%28SIREN%29%20to%20address%20the%20spectral%20bias%20observed%20in%20SR%20tasks.%0A%20%20Results%3A%20The%20ViSIR%20outperforms%20ViT%20by%204.1%20dB%2C%20SIREN%20by%207.5%20dB%2C%20and%0ASR-Generative%20Adversarial%20%28SR-GANs%29%20by%207.1dB%20PSNR%20on%20average%20for%20three%0Adifferent%20measurements.%0A%20%20Conclusion%3A%20The%20proposed%20ViSIR%20is%20evaluated%20and%20compared%20with%0Astate-of-the-art%20methods.%20The%20results%20show%20that%20the%20proposed%20algorithm%20is%0Aoutperforming%20other%20methods%20in%20terms%20of%20Mean%20Square%20Error%28MSE%29%2C%0APeak-Signal-to-Noise-Ratio%28PSNR%29%2C%20and%20Structural%20Similarity%20Index%0AMeasure%28SSIM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSIR%253A%2520Vision%2520Transformer%2520Single%2520Image%2520Reconstruction%2520Method%2520for%2520Earth%250A%2520%2520System%2520Models%26entry.906535625%3DEhsan%2520Zeraatkar%2520and%2520Salah%2520Faroughi%2520and%2520Jelena%2520Te%25C5%25A1i%25C4%2587%26entry.1292438233%3D%2520%2520Purpose%253A%2520Earth%2520system%2520models%2520%2528ESMs%2529%2520integrate%2520the%2520interactions%2520of%2520the%250Aatmosphere%252C%2520ocean%252C%2520land%252C%2520ice%252C%2520and%2520biosphere%2520to%2520estimate%2520the%2520state%2520of%2520regional%250Aand%2520global%2520climate%2520under%2520a%2520wide%2520variety%2520of%2520conditions.%2520The%2520ESMs%2520are%2520highly%250Acomplex%252C%2520and%2520thus%252C%2520deep%2520neural%2520network%2520architectures%2520are%2520used%2520to%2520model%2520the%250Acomplexity%2520and%2520store%2520the%2520down-sampled%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250AVision%2520Transformer%2520Sinusoidal%2520Representation%2520Networks%2520%2528ViSIR%2529%2520to%2520improve%2520the%250Asingle%2520image%2520SR%2520%2528SR%2529%2520reconstruction%2520task%2520for%2520the%2520ESM%2520data.%250A%2520%2520Methods%253A%2520ViSIR%2520combines%2520the%2520SR%2520capability%2520of%2520Vision%2520Transformers%2520%2528ViT%2529%2520with%250Athe%2520high-frequency%2520detail%2520preservation%2520of%2520the%2520Sinusoidal%2520Representation%2520Network%250A%2528SIREN%2529%2520to%2520address%2520the%2520spectral%2520bias%2520observed%2520in%2520SR%2520tasks.%250A%2520%2520Results%253A%2520The%2520ViSIR%2520outperforms%2520ViT%2520by%25204.1%2520dB%252C%2520SIREN%2520by%25207.5%2520dB%252C%2520and%250ASR-Generative%2520Adversarial%2520%2528SR-GANs%2529%2520by%25207.1dB%2520PSNR%2520on%2520average%2520for%2520three%250Adifferent%2520measurements.%250A%2520%2520Conclusion%253A%2520The%2520proposed%2520ViSIR%2520is%2520evaluated%2520and%2520compared%2520with%250Astate-of-the-art%2520methods.%2520The%2520results%2520show%2520that%2520the%2520proposed%2520algorithm%2520is%250Aoutperforming%2520other%2520methods%2520in%2520terms%2520of%2520Mean%2520Square%2520Error%2528MSE%2529%252C%250APeak-Signal-to-Noise-Ratio%2528PSNR%2529%252C%2520and%2520Structural%2520Similarity%2520Index%250AMeasure%2528SSIM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSIR%3A%20Vision%20Transformer%20Single%20Image%20Reconstruction%20Method%20for%20Earth%0A%20%20System%20Models&entry.906535625=Ehsan%20Zeraatkar%20and%20Salah%20Faroughi%20and%20Jelena%20Te%C5%A1i%C4%87&entry.1292438233=%20%20Purpose%3A%20Earth%20system%20models%20%28ESMs%29%20integrate%20the%20interactions%20of%20the%0Aatmosphere%2C%20ocean%2C%20land%2C%20ice%2C%20and%20biosphere%20to%20estimate%20the%20state%20of%20regional%0Aand%20global%20climate%20under%20a%20wide%20variety%20of%20conditions.%20The%20ESMs%20are%20highly%0Acomplex%2C%20and%20thus%2C%20deep%20neural%20network%20architectures%20are%20used%20to%20model%20the%0Acomplexity%20and%20store%20the%20down-sampled%20data.%20In%20this%20paper%2C%20we%20propose%20the%0AVision%20Transformer%20Sinusoidal%20Representation%20Networks%20%28ViSIR%29%20to%20improve%20the%0Asingle%20image%20SR%20%28SR%29%20reconstruction%20task%20for%20the%20ESM%20data.%0A%20%20Methods%3A%20ViSIR%20combines%20the%20SR%20capability%20of%20Vision%20Transformers%20%28ViT%29%20with%0Athe%20high-frequency%20detail%20preservation%20of%20the%20Sinusoidal%20Representation%20Network%0A%28SIREN%29%20to%20address%20the%20spectral%20bias%20observed%20in%20SR%20tasks.%0A%20%20Results%3A%20The%20ViSIR%20outperforms%20ViT%20by%204.1%20dB%2C%20SIREN%20by%207.5%20dB%2C%20and%0ASR-Generative%20Adversarial%20%28SR-GANs%29%20by%207.1dB%20PSNR%20on%20average%20for%20three%0Adifferent%20measurements.%0A%20%20Conclusion%3A%20The%20proposed%20ViSIR%20is%20evaluated%20and%20compared%20with%0Astate-of-the-art%20methods.%20The%20results%20show%20that%20the%20proposed%20algorithm%20is%0Aoutperforming%20other%20methods%20in%20terms%20of%20Mean%20Square%20Error%28MSE%29%2C%0APeak-Signal-to-Noise-Ratio%28PSNR%29%2C%20and%20Structural%20Similarity%20Index%0AMeasure%28SSIM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06741v2&entry.124074799=Read"},
{"title": "Interpretable Vision-Language Survival Analysis with Ordinal Inductive\n  Bias for Computational Pathology", "author": "Pei Liu and Luping Ji and Jiaxiang Gou and Bo Fu and Mao Ye", "abstract": "  Histopathology Whole-Slide Images (WSIs) provide an important tool to assess\ncancer prognosis in computational pathology (CPATH). While existing survival\nanalysis (SA) approaches have made exciting progress, they are generally\nlimited to adopting highly-expressive network architectures and only\ncoarse-grained patient-level labels to learn visual prognostic representations\nfrom gigapixel WSIs. Such learning paradigm suffers from critical performance\nbottlenecks, when facing present scarce training data and standard\nmulti-instance learning (MIL) framework in CPATH. To overcome it, this paper,\nfor the first time, proposes a new Vision-Language-based SA (VLSA) paradigm.\nConcretely, (1) VLSA is driven by pathology VL foundation models. It no longer\nrelies on high-capability networks and shows the advantage of data efficiency.\n(2) In vision-end, VLSA encodes textual prognostic prior and then employs it as\nauxiliary signals to guide the aggregating of visual prognostic features at\ninstance level, thereby compensating for the weak supervision in MIL. Moreover,\ngiven the characteristics of SA, we propose i) ordinal survival prompt learning\nto transform continuous survival labels into textual prompts; and ii) ordinal\nincidence function as prediction target to make SA compatible with VL-based\nprediction. Notably, VLSA's predictions can be interpreted intuitively by our\nShapley values-based method. The extensive experiments on five datasets confirm\nthe effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH\nby offering weakly-supervised MIL an effective means to learn valuable\nprognostic clues from gigapixel WSIs. Our source code is available at\nhttps://github.com/liupei101/VLSA.\n", "link": "http://arxiv.org/abs/2409.09369v4", "date": "2025-02-11", "relevancy": 2.1019, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology&body=Title%3A%20Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology%0AAuthor%3A%20Pei%20Liu%20and%20Luping%20Ji%20and%20Jiaxiang%20Gou%20and%20Bo%20Fu%20and%20Mao%20Ye%0AAbstract%3A%20%20%20Histopathology%20Whole-Slide%20Images%20%28WSIs%29%20provide%20an%20important%20tool%20to%20assess%0Acancer%20prognosis%20in%20computational%20pathology%20%28CPATH%29.%20While%20existing%20survival%0Aanalysis%20%28SA%29%20approaches%20have%20made%20exciting%20progress%2C%20they%20are%20generally%0Alimited%20to%20adopting%20highly-expressive%20network%20architectures%20and%20only%0Acoarse-grained%20patient-level%20labels%20to%20learn%20visual%20prognostic%20representations%0Afrom%20gigapixel%20WSIs.%20Such%20learning%20paradigm%20suffers%20from%20critical%20performance%0Abottlenecks%2C%20when%20facing%20present%20scarce%20training%20data%20and%20standard%0Amulti-instance%20learning%20%28MIL%29%20framework%20in%20CPATH.%20To%20overcome%20it%2C%20this%20paper%2C%0Afor%20the%20first%20time%2C%20proposes%20a%20new%20Vision-Language-based%20SA%20%28VLSA%29%20paradigm.%0AConcretely%2C%20%281%29%20VLSA%20is%20driven%20by%20pathology%20VL%20foundation%20models.%20It%20no%20longer%0Arelies%20on%20high-capability%20networks%20and%20shows%20the%20advantage%20of%20data%20efficiency.%0A%282%29%20In%20vision-end%2C%20VLSA%20encodes%20textual%20prognostic%20prior%20and%20then%20employs%20it%20as%0Aauxiliary%20signals%20to%20guide%20the%20aggregating%20of%20visual%20prognostic%20features%20at%0Ainstance%20level%2C%20thereby%20compensating%20for%20the%20weak%20supervision%20in%20MIL.%20Moreover%2C%0Agiven%20the%20characteristics%20of%20SA%2C%20we%20propose%20i%29%20ordinal%20survival%20prompt%20learning%0Ato%20transform%20continuous%20survival%20labels%20into%20textual%20prompts%3B%20and%20ii%29%20ordinal%0Aincidence%20function%20as%20prediction%20target%20to%20make%20SA%20compatible%20with%20VL-based%0Aprediction.%20Notably%2C%20VLSA%27s%20predictions%20can%20be%20interpreted%20intuitively%20by%20our%0AShapley%20values-based%20method.%20The%20extensive%20experiments%20on%20five%20datasets%20confirm%0Athe%20effectiveness%20of%20our%20scheme.%20Our%20VLSA%20could%20pave%20a%20new%20way%20for%20SA%20in%20CPATH%0Aby%20offering%20weakly-supervised%20MIL%20an%20effective%20means%20to%20learn%20valuable%0Aprognostic%20clues%20from%20gigapixel%20WSIs.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/liupei101/VLSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09369v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Vision-Language%2520Survival%2520Analysis%2520with%2520Ordinal%2520Inductive%250A%2520%2520Bias%2520for%2520Computational%2520Pathology%26entry.906535625%3DPei%2520Liu%2520and%2520Luping%2520Ji%2520and%2520Jiaxiang%2520Gou%2520and%2520Bo%2520Fu%2520and%2520Mao%2520Ye%26entry.1292438233%3D%2520%2520Histopathology%2520Whole-Slide%2520Images%2520%2528WSIs%2529%2520provide%2520an%2520important%2520tool%2520to%2520assess%250Acancer%2520prognosis%2520in%2520computational%2520pathology%2520%2528CPATH%2529.%2520While%2520existing%2520survival%250Aanalysis%2520%2528SA%2529%2520approaches%2520have%2520made%2520exciting%2520progress%252C%2520they%2520are%2520generally%250Alimited%2520to%2520adopting%2520highly-expressive%2520network%2520architectures%2520and%2520only%250Acoarse-grained%2520patient-level%2520labels%2520to%2520learn%2520visual%2520prognostic%2520representations%250Afrom%2520gigapixel%2520WSIs.%2520Such%2520learning%2520paradigm%2520suffers%2520from%2520critical%2520performance%250Abottlenecks%252C%2520when%2520facing%2520present%2520scarce%2520training%2520data%2520and%2520standard%250Amulti-instance%2520learning%2520%2528MIL%2529%2520framework%2520in%2520CPATH.%2520To%2520overcome%2520it%252C%2520this%2520paper%252C%250Afor%2520the%2520first%2520time%252C%2520proposes%2520a%2520new%2520Vision-Language-based%2520SA%2520%2528VLSA%2529%2520paradigm.%250AConcretely%252C%2520%25281%2529%2520VLSA%2520is%2520driven%2520by%2520pathology%2520VL%2520foundation%2520models.%2520It%2520no%2520longer%250Arelies%2520on%2520high-capability%2520networks%2520and%2520shows%2520the%2520advantage%2520of%2520data%2520efficiency.%250A%25282%2529%2520In%2520vision-end%252C%2520VLSA%2520encodes%2520textual%2520prognostic%2520prior%2520and%2520then%2520employs%2520it%2520as%250Aauxiliary%2520signals%2520to%2520guide%2520the%2520aggregating%2520of%2520visual%2520prognostic%2520features%2520at%250Ainstance%2520level%252C%2520thereby%2520compensating%2520for%2520the%2520weak%2520supervision%2520in%2520MIL.%2520Moreover%252C%250Agiven%2520the%2520characteristics%2520of%2520SA%252C%2520we%2520propose%2520i%2529%2520ordinal%2520survival%2520prompt%2520learning%250Ato%2520transform%2520continuous%2520survival%2520labels%2520into%2520textual%2520prompts%253B%2520and%2520ii%2529%2520ordinal%250Aincidence%2520function%2520as%2520prediction%2520target%2520to%2520make%2520SA%2520compatible%2520with%2520VL-based%250Aprediction.%2520Notably%252C%2520VLSA%2527s%2520predictions%2520can%2520be%2520interpreted%2520intuitively%2520by%2520our%250AShapley%2520values-based%2520method.%2520The%2520extensive%2520experiments%2520on%2520five%2520datasets%2520confirm%250Athe%2520effectiveness%2520of%2520our%2520scheme.%2520Our%2520VLSA%2520could%2520pave%2520a%2520new%2520way%2520for%2520SA%2520in%2520CPATH%250Aby%2520offering%2520weakly-supervised%2520MIL%2520an%2520effective%2520means%2520to%2520learn%2520valuable%250Aprognostic%2520clues%2520from%2520gigapixel%2520WSIs.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/liupei101/VLSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09369v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology&entry.906535625=Pei%20Liu%20and%20Luping%20Ji%20and%20Jiaxiang%20Gou%20and%20Bo%20Fu%20and%20Mao%20Ye&entry.1292438233=%20%20Histopathology%20Whole-Slide%20Images%20%28WSIs%29%20provide%20an%20important%20tool%20to%20assess%0Acancer%20prognosis%20in%20computational%20pathology%20%28CPATH%29.%20While%20existing%20survival%0Aanalysis%20%28SA%29%20approaches%20have%20made%20exciting%20progress%2C%20they%20are%20generally%0Alimited%20to%20adopting%20highly-expressive%20network%20architectures%20and%20only%0Acoarse-grained%20patient-level%20labels%20to%20learn%20visual%20prognostic%20representations%0Afrom%20gigapixel%20WSIs.%20Such%20learning%20paradigm%20suffers%20from%20critical%20performance%0Abottlenecks%2C%20when%20facing%20present%20scarce%20training%20data%20and%20standard%0Amulti-instance%20learning%20%28MIL%29%20framework%20in%20CPATH.%20To%20overcome%20it%2C%20this%20paper%2C%0Afor%20the%20first%20time%2C%20proposes%20a%20new%20Vision-Language-based%20SA%20%28VLSA%29%20paradigm.%0AConcretely%2C%20%281%29%20VLSA%20is%20driven%20by%20pathology%20VL%20foundation%20models.%20It%20no%20longer%0Arelies%20on%20high-capability%20networks%20and%20shows%20the%20advantage%20of%20data%20efficiency.%0A%282%29%20In%20vision-end%2C%20VLSA%20encodes%20textual%20prognostic%20prior%20and%20then%20employs%20it%20as%0Aauxiliary%20signals%20to%20guide%20the%20aggregating%20of%20visual%20prognostic%20features%20at%0Ainstance%20level%2C%20thereby%20compensating%20for%20the%20weak%20supervision%20in%20MIL.%20Moreover%2C%0Agiven%20the%20characteristics%20of%20SA%2C%20we%20propose%20i%29%20ordinal%20survival%20prompt%20learning%0Ato%20transform%20continuous%20survival%20labels%20into%20textual%20prompts%3B%20and%20ii%29%20ordinal%0Aincidence%20function%20as%20prediction%20target%20to%20make%20SA%20compatible%20with%20VL-based%0Aprediction.%20Notably%2C%20VLSA%27s%20predictions%20can%20be%20interpreted%20intuitively%20by%20our%0AShapley%20values-based%20method.%20The%20extensive%20experiments%20on%20five%20datasets%20confirm%0Athe%20effectiveness%20of%20our%20scheme.%20Our%20VLSA%20could%20pave%20a%20new%20way%20for%20SA%20in%20CPATH%0Aby%20offering%20weakly-supervised%20MIL%20an%20effective%20means%20to%20learn%20valuable%0Aprognostic%20clues%20from%20gigapixel%20WSIs.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/liupei101/VLSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09369v4&entry.124074799=Read"},
{"title": "Graph Neural Networks in EEG-based Emotion Recognition: A Survey", "author": "Chenyu Liu and Xinliang Zhou and Yihao Wu and Ruizhi Yang and Zhongruo Wang and Liming Zhai and Ziyu Jia and Yang Liu", "abstract": "  Compared to other modalities, EEG-based emotion recognition can intuitively\nrespond to the emotional patterns in the human brain and, therefore, has become\none of the most concerning tasks in the brain-computer interfaces field. Since\ndependencies within brain regions are closely related to emotion, a significant\ntrend is to develop Graph Neural Networks (GNNs) for EEG-based emotion\nrecognition. However, brain region dependencies in emotional EEG have\nphysiological bases that distinguish GNNs in this field from those in other\ntime series fields. Besides, there is neither a comprehensive review nor\nguidance for constructing GNNs in EEG-based emotion recognition. In the survey,\nour categorization reveals the commonalities and differences of existing\napproaches under a unified framework of graph construction. We analyze and\ncategorize methods from three stages in the framework to provide clear guidance\non constructing GNNs in EEG-based emotion recognition. In addition, we discuss\nseveral open challenges and future directions, such as Temporal full-connected\ngraph and Graph condensation.\n", "link": "http://arxiv.org/abs/2402.01138v4", "date": "2025-02-11", "relevancy": 2.1004, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4302}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4241}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20in%20EEG-based%20Emotion%20Recognition%3A%20A%20Survey&body=Title%3A%20Graph%20Neural%20Networks%20in%20EEG-based%20Emotion%20Recognition%3A%20A%20Survey%0AAuthor%3A%20Chenyu%20Liu%20and%20Xinliang%20Zhou%20and%20Yihao%20Wu%20and%20Ruizhi%20Yang%20and%20Zhongruo%20Wang%20and%20Liming%20Zhai%20and%20Ziyu%20Jia%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Compared%20to%20other%20modalities%2C%20EEG-based%20emotion%20recognition%20can%20intuitively%0Arespond%20to%20the%20emotional%20patterns%20in%20the%20human%20brain%20and%2C%20therefore%2C%20has%20become%0Aone%20of%20the%20most%20concerning%20tasks%20in%20the%20brain-computer%20interfaces%20field.%20Since%0Adependencies%20within%20brain%20regions%20are%20closely%20related%20to%20emotion%2C%20a%20significant%0Atrend%20is%20to%20develop%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20EEG-based%20emotion%0Arecognition.%20However%2C%20brain%20region%20dependencies%20in%20emotional%20EEG%20have%0Aphysiological%20bases%20that%20distinguish%20GNNs%20in%20this%20field%20from%20those%20in%20other%0Atime%20series%20fields.%20Besides%2C%20there%20is%20neither%20a%20comprehensive%20review%20nor%0Aguidance%20for%20constructing%20GNNs%20in%20EEG-based%20emotion%20recognition.%20In%20the%20survey%2C%0Aour%20categorization%20reveals%20the%20commonalities%20and%20differences%20of%20existing%0Aapproaches%20under%20a%20unified%20framework%20of%20graph%20construction.%20We%20analyze%20and%0Acategorize%20methods%20from%20three%20stages%20in%20the%20framework%20to%20provide%20clear%20guidance%0Aon%20constructing%20GNNs%20in%20EEG-based%20emotion%20recognition.%20In%20addition%2C%20we%20discuss%0Aseveral%20open%20challenges%20and%20future%20directions%2C%20such%20as%20Temporal%20full-connected%0Agraph%20and%20Graph%20condensation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01138v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520in%2520EEG-based%2520Emotion%2520Recognition%253A%2520A%2520Survey%26entry.906535625%3DChenyu%2520Liu%2520and%2520Xinliang%2520Zhou%2520and%2520Yihao%2520Wu%2520and%2520Ruizhi%2520Yang%2520and%2520Zhongruo%2520Wang%2520and%2520Liming%2520Zhai%2520and%2520Ziyu%2520Jia%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Compared%2520to%2520other%2520modalities%252C%2520EEG-based%2520emotion%2520recognition%2520can%2520intuitively%250Arespond%2520to%2520the%2520emotional%2520patterns%2520in%2520the%2520human%2520brain%2520and%252C%2520therefore%252C%2520has%2520become%250Aone%2520of%2520the%2520most%2520concerning%2520tasks%2520in%2520the%2520brain-computer%2520interfaces%2520field.%2520Since%250Adependencies%2520within%2520brain%2520regions%2520are%2520closely%2520related%2520to%2520emotion%252C%2520a%2520significant%250Atrend%2520is%2520to%2520develop%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520for%2520EEG-based%2520emotion%250Arecognition.%2520However%252C%2520brain%2520region%2520dependencies%2520in%2520emotional%2520EEG%2520have%250Aphysiological%2520bases%2520that%2520distinguish%2520GNNs%2520in%2520this%2520field%2520from%2520those%2520in%2520other%250Atime%2520series%2520fields.%2520Besides%252C%2520there%2520is%2520neither%2520a%2520comprehensive%2520review%2520nor%250Aguidance%2520for%2520constructing%2520GNNs%2520in%2520EEG-based%2520emotion%2520recognition.%2520In%2520the%2520survey%252C%250Aour%2520categorization%2520reveals%2520the%2520commonalities%2520and%2520differences%2520of%2520existing%250Aapproaches%2520under%2520a%2520unified%2520framework%2520of%2520graph%2520construction.%2520We%2520analyze%2520and%250Acategorize%2520methods%2520from%2520three%2520stages%2520in%2520the%2520framework%2520to%2520provide%2520clear%2520guidance%250Aon%2520constructing%2520GNNs%2520in%2520EEG-based%2520emotion%2520recognition.%2520In%2520addition%252C%2520we%2520discuss%250Aseveral%2520open%2520challenges%2520and%2520future%2520directions%252C%2520such%2520as%2520Temporal%2520full-connected%250Agraph%2520and%2520Graph%2520condensation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01138v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20in%20EEG-based%20Emotion%20Recognition%3A%20A%20Survey&entry.906535625=Chenyu%20Liu%20and%20Xinliang%20Zhou%20and%20Yihao%20Wu%20and%20Ruizhi%20Yang%20and%20Zhongruo%20Wang%20and%20Liming%20Zhai%20and%20Ziyu%20Jia%20and%20Yang%20Liu&entry.1292438233=%20%20Compared%20to%20other%20modalities%2C%20EEG-based%20emotion%20recognition%20can%20intuitively%0Arespond%20to%20the%20emotional%20patterns%20in%20the%20human%20brain%20and%2C%20therefore%2C%20has%20become%0Aone%20of%20the%20most%20concerning%20tasks%20in%20the%20brain-computer%20interfaces%20field.%20Since%0Adependencies%20within%20brain%20regions%20are%20closely%20related%20to%20emotion%2C%20a%20significant%0Atrend%20is%20to%20develop%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20EEG-based%20emotion%0Arecognition.%20However%2C%20brain%20region%20dependencies%20in%20emotional%20EEG%20have%0Aphysiological%20bases%20that%20distinguish%20GNNs%20in%20this%20field%20from%20those%20in%20other%0Atime%20series%20fields.%20Besides%2C%20there%20is%20neither%20a%20comprehensive%20review%20nor%0Aguidance%20for%20constructing%20GNNs%20in%20EEG-based%20emotion%20recognition.%20In%20the%20survey%2C%0Aour%20categorization%20reveals%20the%20commonalities%20and%20differences%20of%20existing%0Aapproaches%20under%20a%20unified%20framework%20of%20graph%20construction.%20We%20analyze%20and%0Acategorize%20methods%20from%20three%20stages%20in%20the%20framework%20to%20provide%20clear%20guidance%0Aon%20constructing%20GNNs%20in%20EEG-based%20emotion%20recognition.%20In%20addition%2C%20we%20discuss%0Aseveral%20open%20challenges%20and%20future%20directions%2C%20such%20as%20Temporal%20full-connected%0Agraph%20and%20Graph%20condensation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01138v4&entry.124074799=Read"},
{"title": "Stay-Positive: A Case for Ignoring Real Image Features in Fake Image\n  Detection", "author": "Anirudh Sundara Rajan and Yong Jae Lee", "abstract": "  Detecting AI generated images is a challenging yet essential task. A primary\ndifficulty arises from the detectors tendency to rely on spurious patterns,\nsuch as compression artifacts, which can influence its decisions. These issues\noften stem from specific patterns that the detector associates with the real\ndata distribution, making it difficult to isolate the actual generative traces.\nWe argue that an image should be classified as fake if and only if it contains\nartifacts introduced by the generative model. Based on this premise, we propose\nStay Positive, an algorithm designed to constrain the detectors focus to\ngenerative artifacts while disregarding those associated with real data.\nExperimental results demonstrate that detectors trained with Stay Positive\nexhibit reduced susceptibility to spurious correlations, leading to improved\ngeneralization and robustness to post processing. Additionally, unlike\ndetectors that associate artifacts with real images, those that focus purely on\nfake artifacts are better at detecting inpainted real images.\n", "link": "http://arxiv.org/abs/2502.07778v1", "date": "2025-02-11", "relevancy": 2.095, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5316}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.523}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stay-Positive%3A%20A%20Case%20for%20Ignoring%20Real%20Image%20Features%20in%20Fake%20Image%0A%20%20Detection&body=Title%3A%20Stay-Positive%3A%20A%20Case%20for%20Ignoring%20Real%20Image%20Features%20in%20Fake%20Image%0A%20%20Detection%0AAuthor%3A%20Anirudh%20Sundara%20Rajan%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20Detecting%20AI%20generated%20images%20is%20a%20challenging%20yet%20essential%20task.%20A%20primary%0Adifficulty%20arises%20from%20the%20detectors%20tendency%20to%20rely%20on%20spurious%20patterns%2C%0Asuch%20as%20compression%20artifacts%2C%20which%20can%20influence%20its%20decisions.%20These%20issues%0Aoften%20stem%20from%20specific%20patterns%20that%20the%20detector%20associates%20with%20the%20real%0Adata%20distribution%2C%20making%20it%20difficult%20to%20isolate%20the%20actual%20generative%20traces.%0AWe%20argue%20that%20an%20image%20should%20be%20classified%20as%20fake%20if%20and%20only%20if%20it%20contains%0Aartifacts%20introduced%20by%20the%20generative%20model.%20Based%20on%20this%20premise%2C%20we%20propose%0AStay%20Positive%2C%20an%20algorithm%20designed%20to%20constrain%20the%20detectors%20focus%20to%0Agenerative%20artifacts%20while%20disregarding%20those%20associated%20with%20real%20data.%0AExperimental%20results%20demonstrate%20that%20detectors%20trained%20with%20Stay%20Positive%0Aexhibit%20reduced%20susceptibility%20to%20spurious%20correlations%2C%20leading%20to%20improved%0Ageneralization%20and%20robustness%20to%20post%20processing.%20Additionally%2C%20unlike%0Adetectors%20that%20associate%20artifacts%20with%20real%20images%2C%20those%20that%20focus%20purely%20on%0Afake%20artifacts%20are%20better%20at%20detecting%20inpainted%20real%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStay-Positive%253A%2520A%2520Case%2520for%2520Ignoring%2520Real%2520Image%2520Features%2520in%2520Fake%2520Image%250A%2520%2520Detection%26entry.906535625%3DAnirudh%2520Sundara%2520Rajan%2520and%2520Yong%2520Jae%2520Lee%26entry.1292438233%3D%2520%2520Detecting%2520AI%2520generated%2520images%2520is%2520a%2520challenging%2520yet%2520essential%2520task.%2520A%2520primary%250Adifficulty%2520arises%2520from%2520the%2520detectors%2520tendency%2520to%2520rely%2520on%2520spurious%2520patterns%252C%250Asuch%2520as%2520compression%2520artifacts%252C%2520which%2520can%2520influence%2520its%2520decisions.%2520These%2520issues%250Aoften%2520stem%2520from%2520specific%2520patterns%2520that%2520the%2520detector%2520associates%2520with%2520the%2520real%250Adata%2520distribution%252C%2520making%2520it%2520difficult%2520to%2520isolate%2520the%2520actual%2520generative%2520traces.%250AWe%2520argue%2520that%2520an%2520image%2520should%2520be%2520classified%2520as%2520fake%2520if%2520and%2520only%2520if%2520it%2520contains%250Aartifacts%2520introduced%2520by%2520the%2520generative%2520model.%2520Based%2520on%2520this%2520premise%252C%2520we%2520propose%250AStay%2520Positive%252C%2520an%2520algorithm%2520designed%2520to%2520constrain%2520the%2520detectors%2520focus%2520to%250Agenerative%2520artifacts%2520while%2520disregarding%2520those%2520associated%2520with%2520real%2520data.%250AExperimental%2520results%2520demonstrate%2520that%2520detectors%2520trained%2520with%2520Stay%2520Positive%250Aexhibit%2520reduced%2520susceptibility%2520to%2520spurious%2520correlations%252C%2520leading%2520to%2520improved%250Ageneralization%2520and%2520robustness%2520to%2520post%2520processing.%2520Additionally%252C%2520unlike%250Adetectors%2520that%2520associate%2520artifacts%2520with%2520real%2520images%252C%2520those%2520that%2520focus%2520purely%2520on%250Afake%2520artifacts%2520are%2520better%2520at%2520detecting%2520inpainted%2520real%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stay-Positive%3A%20A%20Case%20for%20Ignoring%20Real%20Image%20Features%20in%20Fake%20Image%0A%20%20Detection&entry.906535625=Anirudh%20Sundara%20Rajan%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20Detecting%20AI%20generated%20images%20is%20a%20challenging%20yet%20essential%20task.%20A%20primary%0Adifficulty%20arises%20from%20the%20detectors%20tendency%20to%20rely%20on%20spurious%20patterns%2C%0Asuch%20as%20compression%20artifacts%2C%20which%20can%20influence%20its%20decisions.%20These%20issues%0Aoften%20stem%20from%20specific%20patterns%20that%20the%20detector%20associates%20with%20the%20real%0Adata%20distribution%2C%20making%20it%20difficult%20to%20isolate%20the%20actual%20generative%20traces.%0AWe%20argue%20that%20an%20image%20should%20be%20classified%20as%20fake%20if%20and%20only%20if%20it%20contains%0Aartifacts%20introduced%20by%20the%20generative%20model.%20Based%20on%20this%20premise%2C%20we%20propose%0AStay%20Positive%2C%20an%20algorithm%20designed%20to%20constrain%20the%20detectors%20focus%20to%0Agenerative%20artifacts%20while%20disregarding%20those%20associated%20with%20real%20data.%0AExperimental%20results%20demonstrate%20that%20detectors%20trained%20with%20Stay%20Positive%0Aexhibit%20reduced%20susceptibility%20to%20spurious%20correlations%2C%20leading%20to%20improved%0Ageneralization%20and%20robustness%20to%20post%20processing.%20Additionally%2C%20unlike%0Adetectors%20that%20associate%20artifacts%20with%20real%20images%2C%20those%20that%20focus%20purely%20on%0Afake%20artifacts%20are%20better%20at%20detecting%20inpainted%20real%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07778v1&entry.124074799=Read"},
{"title": "OLMES: A Standard for Language Model Evaluations", "author": "Yuling Gu and Oyvind Tafjord and Bailey Kuehl and Dany Haddad and Jesse Dodge and Hannaneh Hajishirzi", "abstract": "  Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions.\n", "link": "http://arxiv.org/abs/2406.08446v2", "date": "2025-02-11", "relevancy": 2.0935, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLMES%3A%20A%20Standard%20for%20Language%20Model%20Evaluations&body=Title%3A%20OLMES%3A%20A%20Standard%20for%20Language%20Model%20Evaluations%0AAuthor%3A%20Yuling%20Gu%20and%20Oyvind%20Tafjord%20and%20Bailey%20Kuehl%20and%20Dany%20Haddad%20and%20Jesse%20Dodge%20and%20Hannaneh%20Hajishirzi%0AAbstract%3A%20%20%20Progress%20in%20AI%20is%20often%20demonstrated%20by%20new%20models%20claiming%20improved%0Aperformance%20on%20tasks%20measuring%20model%20capabilities.%20Evaluating%20language%20models%0Acan%20be%20particularly%20challenging%2C%20as%20choices%20of%20how%20a%20model%20is%20evaluated%20on%20a%0Atask%20can%20lead%20to%20large%20changes%20in%20measured%20performance.%20There%20is%20no%20common%0Astandard%20setup%2C%20so%20different%20models%20are%20evaluated%20on%20the%20same%20tasks%20in%0Adifferent%20ways%2C%20leading%20to%20claims%20about%20which%20models%20perform%20best%20not%20being%0Areproducible.%20We%20propose%20OLMES%2C%20a%20completely%20documented%2C%20practical%2C%20open%0Astandard%20for%20reproducible%20LLM%20evaluations.%20In%20developing%20this%20standard%2C%20we%0Aidentify%20and%20review%20the%20varying%20factors%20in%20evaluation%20practices%20adopted%20by%20the%0Acommunity%20-%20such%20as%20details%20of%20prompt%20formatting%2C%20choice%20of%20in-context%0Aexamples%2C%20probability%20normalizations%2C%20and%20task%20formulation.%20In%20particular%2C%0AOLMES%20supports%20meaningful%20comparisons%20between%20smaller%20base%20models%20that%20require%0Athe%20unnatural%20%22cloze%22%20formulation%20of%20multiple-choice%20questions%20against%20larger%0Amodels%20that%20can%20utilize%20the%20original%20formulation.%20OLMES%20includes%0Awell-considered%2C%20documented%20recommendations%20guided%20by%20results%20from%20existing%0Aliterature%20as%20well%20as%20new%20experiments%20resolving%20open%20questions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08446v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLMES%253A%2520A%2520Standard%2520for%2520Language%2520Model%2520Evaluations%26entry.906535625%3DYuling%2520Gu%2520and%2520Oyvind%2520Tafjord%2520and%2520Bailey%2520Kuehl%2520and%2520Dany%2520Haddad%2520and%2520Jesse%2520Dodge%2520and%2520Hannaneh%2520Hajishirzi%26entry.1292438233%3D%2520%2520Progress%2520in%2520AI%2520is%2520often%2520demonstrated%2520by%2520new%2520models%2520claiming%2520improved%250Aperformance%2520on%2520tasks%2520measuring%2520model%2520capabilities.%2520Evaluating%2520language%2520models%250Acan%2520be%2520particularly%2520challenging%252C%2520as%2520choices%2520of%2520how%2520a%2520model%2520is%2520evaluated%2520on%2520a%250Atask%2520can%2520lead%2520to%2520large%2520changes%2520in%2520measured%2520performance.%2520There%2520is%2520no%2520common%250Astandard%2520setup%252C%2520so%2520different%2520models%2520are%2520evaluated%2520on%2520the%2520same%2520tasks%2520in%250Adifferent%2520ways%252C%2520leading%2520to%2520claims%2520about%2520which%2520models%2520perform%2520best%2520not%2520being%250Areproducible.%2520We%2520propose%2520OLMES%252C%2520a%2520completely%2520documented%252C%2520practical%252C%2520open%250Astandard%2520for%2520reproducible%2520LLM%2520evaluations.%2520In%2520developing%2520this%2520standard%252C%2520we%250Aidentify%2520and%2520review%2520the%2520varying%2520factors%2520in%2520evaluation%2520practices%2520adopted%2520by%2520the%250Acommunity%2520-%2520such%2520as%2520details%2520of%2520prompt%2520formatting%252C%2520choice%2520of%2520in-context%250Aexamples%252C%2520probability%2520normalizations%252C%2520and%2520task%2520formulation.%2520In%2520particular%252C%250AOLMES%2520supports%2520meaningful%2520comparisons%2520between%2520smaller%2520base%2520models%2520that%2520require%250Athe%2520unnatural%2520%2522cloze%2522%2520formulation%2520of%2520multiple-choice%2520questions%2520against%2520larger%250Amodels%2520that%2520can%2520utilize%2520the%2520original%2520formulation.%2520OLMES%2520includes%250Awell-considered%252C%2520documented%2520recommendations%2520guided%2520by%2520results%2520from%2520existing%250Aliterature%2520as%2520well%2520as%2520new%2520experiments%2520resolving%2520open%2520questions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08446v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLMES%3A%20A%20Standard%20for%20Language%20Model%20Evaluations&entry.906535625=Yuling%20Gu%20and%20Oyvind%20Tafjord%20and%20Bailey%20Kuehl%20and%20Dany%20Haddad%20and%20Jesse%20Dodge%20and%20Hannaneh%20Hajishirzi&entry.1292438233=%20%20Progress%20in%20AI%20is%20often%20demonstrated%20by%20new%20models%20claiming%20improved%0Aperformance%20on%20tasks%20measuring%20model%20capabilities.%20Evaluating%20language%20models%0Acan%20be%20particularly%20challenging%2C%20as%20choices%20of%20how%20a%20model%20is%20evaluated%20on%20a%0Atask%20can%20lead%20to%20large%20changes%20in%20measured%20performance.%20There%20is%20no%20common%0Astandard%20setup%2C%20so%20different%20models%20are%20evaluated%20on%20the%20same%20tasks%20in%0Adifferent%20ways%2C%20leading%20to%20claims%20about%20which%20models%20perform%20best%20not%20being%0Areproducible.%20We%20propose%20OLMES%2C%20a%20completely%20documented%2C%20practical%2C%20open%0Astandard%20for%20reproducible%20LLM%20evaluations.%20In%20developing%20this%20standard%2C%20we%0Aidentify%20and%20review%20the%20varying%20factors%20in%20evaluation%20practices%20adopted%20by%20the%0Acommunity%20-%20such%20as%20details%20of%20prompt%20formatting%2C%20choice%20of%20in-context%0Aexamples%2C%20probability%20normalizations%2C%20and%20task%20formulation.%20In%20particular%2C%0AOLMES%20supports%20meaningful%20comparisons%20between%20smaller%20base%20models%20that%20require%0Athe%20unnatural%20%22cloze%22%20formulation%20of%20multiple-choice%20questions%20against%20larger%0Amodels%20that%20can%20utilize%20the%20original%20formulation.%20OLMES%20includes%0Awell-considered%2C%20documented%20recommendations%20guided%20by%20results%20from%20existing%0Aliterature%20as%20well%20as%20new%20experiments%20resolving%20open%20questions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08446v2&entry.124074799=Read"},
{"title": "CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression\n  Generation", "author": "Rabeya Tus Sadia and Md Atik Ahamed and Qiang Cheng", "abstract": "  The integration of single-cell RNA sequencing (scRNA-seq) and spatial\ntranscriptomics (ST) data is crucial for understanding gene expression in\nspatial context. Existing methods for such integration have limited\nperformance, with structural similarity often below 60\\%, We attribute this\nlimitation to the failure to consider causal relationships between genes. We\npresent CausalGeD, which combines diffusion and autoregressive processes to\nleverage these relationships. By generalizing the Causal Attention Transformer\nfrom image generation to gene expression data, our model captures regulatory\nmechanisms without predefined relationships. Across 10 tissue datasets,\nCausalGeD outperformed state-of-the-art baselines by 5- 32\\% in key metrics,\nincluding Pearson's correlation and structural similarity, advancing both\ntechnical and biological insights.\n", "link": "http://arxiv.org/abs/2502.07751v1", "date": "2025-02-11", "relevancy": 1.5495, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5225}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5152}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalGeD%3A%20Blending%20Causality%20and%20Diffusion%20for%20Spatial%20Gene%20Expression%0A%20%20Generation&body=Title%3A%20CausalGeD%3A%20Blending%20Causality%20and%20Diffusion%20for%20Spatial%20Gene%20Expression%0A%20%20Generation%0AAuthor%3A%20Rabeya%20Tus%20Sadia%20and%20Md%20Atik%20Ahamed%20and%20Qiang%20Cheng%0AAbstract%3A%20%20%20The%20integration%20of%20single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20and%20spatial%0Atranscriptomics%20%28ST%29%20data%20is%20crucial%20for%20understanding%20gene%20expression%20in%0Aspatial%20context.%20Existing%20methods%20for%20such%20integration%20have%20limited%0Aperformance%2C%20with%20structural%20similarity%20often%20below%2060%5C%25%2C%20We%20attribute%20this%0Alimitation%20to%20the%20failure%20to%20consider%20causal%20relationships%20between%20genes.%20We%0Apresent%20CausalGeD%2C%20which%20combines%20diffusion%20and%20autoregressive%20processes%20to%0Aleverage%20these%20relationships.%20By%20generalizing%20the%20Causal%20Attention%20Transformer%0Afrom%20image%20generation%20to%20gene%20expression%20data%2C%20our%20model%20captures%20regulatory%0Amechanisms%20without%20predefined%20relationships.%20Across%2010%20tissue%20datasets%2C%0ACausalGeD%20outperformed%20state-of-the-art%20baselines%20by%205-%2032%5C%25%20in%20key%20metrics%2C%0Aincluding%20Pearson%27s%20correlation%20and%20structural%20similarity%2C%20advancing%20both%0Atechnical%20and%20biological%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalGeD%253A%2520Blending%2520Causality%2520and%2520Diffusion%2520for%2520Spatial%2520Gene%2520Expression%250A%2520%2520Generation%26entry.906535625%3DRabeya%2520Tus%2520Sadia%2520and%2520Md%2520Atik%2520Ahamed%2520and%2520Qiang%2520Cheng%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520single-cell%2520RNA%2520sequencing%2520%2528scRNA-seq%2529%2520and%2520spatial%250Atranscriptomics%2520%2528ST%2529%2520data%2520is%2520crucial%2520for%2520understanding%2520gene%2520expression%2520in%250Aspatial%2520context.%2520Existing%2520methods%2520for%2520such%2520integration%2520have%2520limited%250Aperformance%252C%2520with%2520structural%2520similarity%2520often%2520below%252060%255C%2525%252C%2520We%2520attribute%2520this%250Alimitation%2520to%2520the%2520failure%2520to%2520consider%2520causal%2520relationships%2520between%2520genes.%2520We%250Apresent%2520CausalGeD%252C%2520which%2520combines%2520diffusion%2520and%2520autoregressive%2520processes%2520to%250Aleverage%2520these%2520relationships.%2520By%2520generalizing%2520the%2520Causal%2520Attention%2520Transformer%250Afrom%2520image%2520generation%2520to%2520gene%2520expression%2520data%252C%2520our%2520model%2520captures%2520regulatory%250Amechanisms%2520without%2520predefined%2520relationships.%2520Across%252010%2520tissue%2520datasets%252C%250ACausalGeD%2520outperformed%2520state-of-the-art%2520baselines%2520by%25205-%252032%255C%2525%2520in%2520key%2520metrics%252C%250Aincluding%2520Pearson%2527s%2520correlation%2520and%2520structural%2520similarity%252C%2520advancing%2520both%250Atechnical%2520and%2520biological%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalGeD%3A%20Blending%20Causality%20and%20Diffusion%20for%20Spatial%20Gene%20Expression%0A%20%20Generation&entry.906535625=Rabeya%20Tus%20Sadia%20and%20Md%20Atik%20Ahamed%20and%20Qiang%20Cheng&entry.1292438233=%20%20The%20integration%20of%20single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20and%20spatial%0Atranscriptomics%20%28ST%29%20data%20is%20crucial%20for%20understanding%20gene%20expression%20in%0Aspatial%20context.%20Existing%20methods%20for%20such%20integration%20have%20limited%0Aperformance%2C%20with%20structural%20similarity%20often%20below%2060%5C%25%2C%20We%20attribute%20this%0Alimitation%20to%20the%20failure%20to%20consider%20causal%20relationships%20between%20genes.%20We%0Apresent%20CausalGeD%2C%20which%20combines%20diffusion%20and%20autoregressive%20processes%20to%0Aleverage%20these%20relationships.%20By%20generalizing%20the%20Causal%20Attention%20Transformer%0Afrom%20image%20generation%20to%20gene%20expression%20data%2C%20our%20model%20captures%20regulatory%0Amechanisms%20without%20predefined%20relationships.%20Across%2010%20tissue%20datasets%2C%0ACausalGeD%20outperformed%20state-of-the-art%20baselines%20by%205-%2032%5C%25%20in%20key%20metrics%2C%0Aincluding%20Pearson%27s%20correlation%20and%20structural%20similarity%2C%20advancing%20both%0Atechnical%20and%20biological%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07751v1&entry.124074799=Read"},
{"title": "Scaling Off-Policy Reinforcement Learning with Batch and Weight\n  Normalization", "author": "Daniel Palenicek and Florian Vogt and Jan Peters", "abstract": "  Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics, which are\nemphasized by higher UTD ratios. To address these, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nhas been shown to prevent potential loss of plasticity and keeps the effective\nlearning rate constant. Our proposed approach reliably scales with increasing\nUTD ratios, achieving competitive performance across 25 challenging continuous\ncontrol tasks on the DeepMind Control Suite and Myosuite benchmarks, notably\nthe complex dog and humanoid environments. This work eliminates the need for\ndrastic interventions, such as network resets, and offers a simple yet robust\npathway for improving sample efficiency and scalability in model-free\nreinforcement learning.\n", "link": "http://arxiv.org/abs/2502.07523v1", "date": "2025-02-11", "relevancy": 2.0541, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5329}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5329}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Off-Policy%20Reinforcement%20Learning%20with%20Batch%20and%20Weight%0A%20%20Normalization&body=Title%3A%20Scaling%20Off-Policy%20Reinforcement%20Learning%20with%20Batch%20and%20Weight%0A%20%20Normalization%0AAuthor%3A%20Daniel%20Palenicek%20and%20Florian%20Vogt%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Reinforcement%20learning%20has%20achieved%20significant%20milestones%2C%20but%20sample%0Aefficiency%20remains%20a%20bottleneck%20for%20real-world%20applications.%20Recently%2C%20CrossQ%0Ahas%20demonstrated%20state-of-the-art%20sample%20efficiency%20with%20a%20low%20update-to-data%0A%28UTD%29%20ratio%20of%201.%20In%20this%20work%2C%20we%20explore%20CrossQ%27s%20scaling%20behavior%20with%0Ahigher%20UTD%20ratios.%20We%20identify%20challenges%20in%20the%20training%20dynamics%2C%20which%20are%0Aemphasized%20by%20higher%20UTD%20ratios.%20To%20address%20these%2C%20we%20integrate%20weight%0Anormalization%20into%20the%20CrossQ%20framework%2C%20a%20solution%20that%20stabilizes%20training%2C%0Ahas%20been%20shown%20to%20prevent%20potential%20loss%20of%20plasticity%20and%20keeps%20the%20effective%0Alearning%20rate%20constant.%20Our%20proposed%20approach%20reliably%20scales%20with%20increasing%0AUTD%20ratios%2C%20achieving%20competitive%20performance%20across%2025%20challenging%20continuous%0Acontrol%20tasks%20on%20the%20DeepMind%20Control%20Suite%20and%20Myosuite%20benchmarks%2C%20notably%0Athe%20complex%20dog%20and%20humanoid%20environments.%20This%20work%20eliminates%20the%20need%20for%0Adrastic%20interventions%2C%20such%20as%20network%20resets%2C%20and%20offers%20a%20simple%20yet%20robust%0Apathway%20for%20improving%20sample%20efficiency%20and%20scalability%20in%20model-free%0Areinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Off-Policy%2520Reinforcement%2520Learning%2520with%2520Batch%2520and%2520Weight%250A%2520%2520Normalization%26entry.906535625%3DDaniel%2520Palenicek%2520and%2520Florian%2520Vogt%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520has%2520achieved%2520significant%2520milestones%252C%2520but%2520sample%250Aefficiency%2520remains%2520a%2520bottleneck%2520for%2520real-world%2520applications.%2520Recently%252C%2520CrossQ%250Ahas%2520demonstrated%2520state-of-the-art%2520sample%2520efficiency%2520with%2520a%2520low%2520update-to-data%250A%2528UTD%2529%2520ratio%2520of%25201.%2520In%2520this%2520work%252C%2520we%2520explore%2520CrossQ%2527s%2520scaling%2520behavior%2520with%250Ahigher%2520UTD%2520ratios.%2520We%2520identify%2520challenges%2520in%2520the%2520training%2520dynamics%252C%2520which%2520are%250Aemphasized%2520by%2520higher%2520UTD%2520ratios.%2520To%2520address%2520these%252C%2520we%2520integrate%2520weight%250Anormalization%2520into%2520the%2520CrossQ%2520framework%252C%2520a%2520solution%2520that%2520stabilizes%2520training%252C%250Ahas%2520been%2520shown%2520to%2520prevent%2520potential%2520loss%2520of%2520plasticity%2520and%2520keeps%2520the%2520effective%250Alearning%2520rate%2520constant.%2520Our%2520proposed%2520approach%2520reliably%2520scales%2520with%2520increasing%250AUTD%2520ratios%252C%2520achieving%2520competitive%2520performance%2520across%252025%2520challenging%2520continuous%250Acontrol%2520tasks%2520on%2520the%2520DeepMind%2520Control%2520Suite%2520and%2520Myosuite%2520benchmarks%252C%2520notably%250Athe%2520complex%2520dog%2520and%2520humanoid%2520environments.%2520This%2520work%2520eliminates%2520the%2520need%2520for%250Adrastic%2520interventions%252C%2520such%2520as%2520network%2520resets%252C%2520and%2520offers%2520a%2520simple%2520yet%2520robust%250Apathway%2520for%2520improving%2520sample%2520efficiency%2520and%2520scalability%2520in%2520model-free%250Areinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Off-Policy%20Reinforcement%20Learning%20with%20Batch%20and%20Weight%0A%20%20Normalization&entry.906535625=Daniel%20Palenicek%20and%20Florian%20Vogt%20and%20Jan%20Peters&entry.1292438233=%20%20Reinforcement%20learning%20has%20achieved%20significant%20milestones%2C%20but%20sample%0Aefficiency%20remains%20a%20bottleneck%20for%20real-world%20applications.%20Recently%2C%20CrossQ%0Ahas%20demonstrated%20state-of-the-art%20sample%20efficiency%20with%20a%20low%20update-to-data%0A%28UTD%29%20ratio%20of%201.%20In%20this%20work%2C%20we%20explore%20CrossQ%27s%20scaling%20behavior%20with%0Ahigher%20UTD%20ratios.%20We%20identify%20challenges%20in%20the%20training%20dynamics%2C%20which%20are%0Aemphasized%20by%20higher%20UTD%20ratios.%20To%20address%20these%2C%20we%20integrate%20weight%0Anormalization%20into%20the%20CrossQ%20framework%2C%20a%20solution%20that%20stabilizes%20training%2C%0Ahas%20been%20shown%20to%20prevent%20potential%20loss%20of%20plasticity%20and%20keeps%20the%20effective%0Alearning%20rate%20constant.%20Our%20proposed%20approach%20reliably%20scales%20with%20increasing%0AUTD%20ratios%2C%20achieving%20competitive%20performance%20across%2025%20challenging%20continuous%0Acontrol%20tasks%20on%20the%20DeepMind%20Control%20Suite%20and%20Myosuite%20benchmarks%2C%20notably%0Athe%20complex%20dog%20and%20humanoid%20environments.%20This%20work%20eliminates%20the%20need%20for%0Adrastic%20interventions%2C%20such%20as%20network%20resets%2C%20and%20offers%20a%20simple%20yet%20robust%0Apathway%20for%20improving%20sample%20efficiency%20and%20scalability%20in%20model-free%0Areinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07523v1&entry.124074799=Read"},
{"title": "SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image\n  Generation with Region-Based Sketches", "author": "Haichuan Lin and Yilin Ye and Jiazhi Xia and Wei Zeng", "abstract": "  Text-to-image models can generate visually appealing images from text\ndescriptions. Efforts have been devoted to improving model controls with prompt\ntuning and spatial conditioning. However, our formative study highlights the\nchallenges for non-expert users in crafting appropriate prompts and specifying\nfine-grained spatial conditions (e.g., depth or canny references) to generate\nsemantically cohesive images, especially when multiple objects are involved. In\nresponse, we introduce SketchFlex, an interactive system designed to improve\nthe flexibility of spatially conditioned image generation using rough region\nsketches. The system automatically infers user prompts with rational\ndescriptions within a semantic space enriched by crowd-sourced object\nattributes and relationships. Additionally, SketchFlex refines users' rough\nsketches into canny-based shape anchors, ensuring the generation quality and\nalignment of user intentions. Experimental results demonstrate that SketchFlex\nachieves more cohesive image generations than end-to-end models, meanwhile\nsignificantly reducing cognitive load and better matching user intentions\ncompared to region-based generation baseline.\n", "link": "http://arxiv.org/abs/2502.07556v1", "date": "2025-02-11", "relevancy": 1.6498, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5769}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.56}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SketchFlex%3A%20Facilitating%20Spatial-Semantic%20Coherence%20in%20Text-to-Image%0A%20%20Generation%20with%20Region-Based%20Sketches&body=Title%3A%20SketchFlex%3A%20Facilitating%20Spatial-Semantic%20Coherence%20in%20Text-to-Image%0A%20%20Generation%20with%20Region-Based%20Sketches%0AAuthor%3A%20Haichuan%20Lin%20and%20Yilin%20Ye%20and%20Jiazhi%20Xia%20and%20Wei%20Zeng%0AAbstract%3A%20%20%20Text-to-image%20models%20can%20generate%20visually%20appealing%20images%20from%20text%0Adescriptions.%20Efforts%20have%20been%20devoted%20to%20improving%20model%20controls%20with%20prompt%0Atuning%20and%20spatial%20conditioning.%20However%2C%20our%20formative%20study%20highlights%20the%0Achallenges%20for%20non-expert%20users%20in%20crafting%20appropriate%20prompts%20and%20specifying%0Afine-grained%20spatial%20conditions%20%28e.g.%2C%20depth%20or%20canny%20references%29%20to%20generate%0Asemantically%20cohesive%20images%2C%20especially%20when%20multiple%20objects%20are%20involved.%20In%0Aresponse%2C%20we%20introduce%20SketchFlex%2C%20an%20interactive%20system%20designed%20to%20improve%0Athe%20flexibility%20of%20spatially%20conditioned%20image%20generation%20using%20rough%20region%0Asketches.%20The%20system%20automatically%20infers%20user%20prompts%20with%20rational%0Adescriptions%20within%20a%20semantic%20space%20enriched%20by%20crowd-sourced%20object%0Aattributes%20and%20relationships.%20Additionally%2C%20SketchFlex%20refines%20users%27%20rough%0Asketches%20into%20canny-based%20shape%20anchors%2C%20ensuring%20the%20generation%20quality%20and%0Aalignment%20of%20user%20intentions.%20Experimental%20results%20demonstrate%20that%20SketchFlex%0Aachieves%20more%20cohesive%20image%20generations%20than%20end-to-end%20models%2C%20meanwhile%0Asignificantly%20reducing%20cognitive%20load%20and%20better%20matching%20user%20intentions%0Acompared%20to%20region-based%20generation%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketchFlex%253A%2520Facilitating%2520Spatial-Semantic%2520Coherence%2520in%2520Text-to-Image%250A%2520%2520Generation%2520with%2520Region-Based%2520Sketches%26entry.906535625%3DHaichuan%2520Lin%2520and%2520Yilin%2520Ye%2520and%2520Jiazhi%2520Xia%2520and%2520Wei%2520Zeng%26entry.1292438233%3D%2520%2520Text-to-image%2520models%2520can%2520generate%2520visually%2520appealing%2520images%2520from%2520text%250Adescriptions.%2520Efforts%2520have%2520been%2520devoted%2520to%2520improving%2520model%2520controls%2520with%2520prompt%250Atuning%2520and%2520spatial%2520conditioning.%2520However%252C%2520our%2520formative%2520study%2520highlights%2520the%250Achallenges%2520for%2520non-expert%2520users%2520in%2520crafting%2520appropriate%2520prompts%2520and%2520specifying%250Afine-grained%2520spatial%2520conditions%2520%2528e.g.%252C%2520depth%2520or%2520canny%2520references%2529%2520to%2520generate%250Asemantically%2520cohesive%2520images%252C%2520especially%2520when%2520multiple%2520objects%2520are%2520involved.%2520In%250Aresponse%252C%2520we%2520introduce%2520SketchFlex%252C%2520an%2520interactive%2520system%2520designed%2520to%2520improve%250Athe%2520flexibility%2520of%2520spatially%2520conditioned%2520image%2520generation%2520using%2520rough%2520region%250Asketches.%2520The%2520system%2520automatically%2520infers%2520user%2520prompts%2520with%2520rational%250Adescriptions%2520within%2520a%2520semantic%2520space%2520enriched%2520by%2520crowd-sourced%2520object%250Aattributes%2520and%2520relationships.%2520Additionally%252C%2520SketchFlex%2520refines%2520users%2527%2520rough%250Asketches%2520into%2520canny-based%2520shape%2520anchors%252C%2520ensuring%2520the%2520generation%2520quality%2520and%250Aalignment%2520of%2520user%2520intentions.%2520Experimental%2520results%2520demonstrate%2520that%2520SketchFlex%250Aachieves%2520more%2520cohesive%2520image%2520generations%2520than%2520end-to-end%2520models%252C%2520meanwhile%250Asignificantly%2520reducing%2520cognitive%2520load%2520and%2520better%2520matching%2520user%2520intentions%250Acompared%2520to%2520region-based%2520generation%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SketchFlex%3A%20Facilitating%20Spatial-Semantic%20Coherence%20in%20Text-to-Image%0A%20%20Generation%20with%20Region-Based%20Sketches&entry.906535625=Haichuan%20Lin%20and%20Yilin%20Ye%20and%20Jiazhi%20Xia%20and%20Wei%20Zeng&entry.1292438233=%20%20Text-to-image%20models%20can%20generate%20visually%20appealing%20images%20from%20text%0Adescriptions.%20Efforts%20have%20been%20devoted%20to%20improving%20model%20controls%20with%20prompt%0Atuning%20and%20spatial%20conditioning.%20However%2C%20our%20formative%20study%20highlights%20the%0Achallenges%20for%20non-expert%20users%20in%20crafting%20appropriate%20prompts%20and%20specifying%0Afine-grained%20spatial%20conditions%20%28e.g.%2C%20depth%20or%20canny%20references%29%20to%20generate%0Asemantically%20cohesive%20images%2C%20especially%20when%20multiple%20objects%20are%20involved.%20In%0Aresponse%2C%20we%20introduce%20SketchFlex%2C%20an%20interactive%20system%20designed%20to%20improve%0Athe%20flexibility%20of%20spatially%20conditioned%20image%20generation%20using%20rough%20region%0Asketches.%20The%20system%20automatically%20infers%20user%20prompts%20with%20rational%0Adescriptions%20within%20a%20semantic%20space%20enriched%20by%20crowd-sourced%20object%0Aattributes%20and%20relationships.%20Additionally%2C%20SketchFlex%20refines%20users%27%20rough%0Asketches%20into%20canny-based%20shape%20anchors%2C%20ensuring%20the%20generation%20quality%20and%0Aalignment%20of%20user%20intentions.%20Experimental%20results%20demonstrate%20that%20SketchFlex%0Aachieves%20more%20cohesive%20image%20generations%20than%20end-to-end%20models%2C%20meanwhile%0Asignificantly%20reducing%20cognitive%20load%20and%20better%20matching%20user%20intentions%0Acompared%20to%20region-based%20generation%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07556v1&entry.124074799=Read"},
{"title": "Novelty Detection in Reinforcement Learning with World Models", "author": "Geigh Zollicoffer and Kenneth Eaton and Jonathan Balloch and Julia Kim and Wei Zhou and Robert Wright and Mark O. Riedl", "abstract": "  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n", "link": "http://arxiv.org/abs/2310.08731v3", "date": "2025-02-11", "relevancy": 1.49, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4925}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novelty%20Detection%20in%20Reinforcement%20Learning%20with%20World%20Models&body=Title%3A%20Novelty%20Detection%20in%20Reinforcement%20Learning%20with%20World%20Models%0AAuthor%3A%20Geigh%20Zollicoffer%20and%20Kenneth%20Eaton%20and%20Jonathan%20Balloch%20and%20Julia%20Kim%20and%20Wei%20Zhou%20and%20Robert%20Wright%20and%20Mark%20O.%20Riedl%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20using%20world%20models%20has%20found%20significant%20recent%0Asuccesses.%20However%2C%20when%20a%20sudden%20change%20to%20world%20mechanics%20or%20properties%0Aoccurs%20then%20agent%20performance%20and%20reliability%20can%20dramatically%20decline.%20We%0Arefer%20to%20the%20sudden%20change%20in%20visual%20properties%20or%20state%20transitions%20as%0Anovelties.%20Implementing%20novelty%20detection%20within%20generated%20world%20model%0Aframeworks%20is%20a%20crucial%20task%20for%20protecting%20the%20agent%20when%20deployed.%20In%20this%0Apaper%2C%20we%20propose%20straightforward%20bounding%20approaches%20to%20incorporate%20novelty%0Adetection%20into%20world%20model%20RL%20agents%2C%20by%20utilizing%20the%20misalignment%20of%20the%0Aworld%20model%27s%20hallucinated%20states%20and%20the%20true%20observed%20states%20as%20an%20anomaly%0Ascore.%20We%20provide%20effective%20approaches%20to%20detecting%20novelties%20in%20a%20distribution%0Aof%20transitions%20learned%20by%20an%20agent%20in%20a%20world%20model.%20Finally%2C%20we%20show%20the%0Aadvantage%20of%20our%20work%20in%20a%20novel%20environment%20compared%20to%20traditional%20machine%0Alearning%20novelty%20detection%20methods%20as%20well%20as%20currently%20accepted%20RL%20focused%0Anovelty%20detection%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08731v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovelty%2520Detection%2520in%2520Reinforcement%2520Learning%2520with%2520World%2520Models%26entry.906535625%3DGeigh%2520Zollicoffer%2520and%2520Kenneth%2520Eaton%2520and%2520Jonathan%2520Balloch%2520and%2520Julia%2520Kim%2520and%2520Wei%2520Zhou%2520and%2520Robert%2520Wright%2520and%2520Mark%2520O.%2520Riedl%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520using%2520world%2520models%2520has%2520found%2520significant%2520recent%250Asuccesses.%2520However%252C%2520when%2520a%2520sudden%2520change%2520to%2520world%2520mechanics%2520or%2520properties%250Aoccurs%2520then%2520agent%2520performance%2520and%2520reliability%2520can%2520dramatically%2520decline.%2520We%250Arefer%2520to%2520the%2520sudden%2520change%2520in%2520visual%2520properties%2520or%2520state%2520transitions%2520as%250Anovelties.%2520Implementing%2520novelty%2520detection%2520within%2520generated%2520world%2520model%250Aframeworks%2520is%2520a%2520crucial%2520task%2520for%2520protecting%2520the%2520agent%2520when%2520deployed.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520straightforward%2520bounding%2520approaches%2520to%2520incorporate%2520novelty%250Adetection%2520into%2520world%2520model%2520RL%2520agents%252C%2520by%2520utilizing%2520the%2520misalignment%2520of%2520the%250Aworld%2520model%2527s%2520hallucinated%2520states%2520and%2520the%2520true%2520observed%2520states%2520as%2520an%2520anomaly%250Ascore.%2520We%2520provide%2520effective%2520approaches%2520to%2520detecting%2520novelties%2520in%2520a%2520distribution%250Aof%2520transitions%2520learned%2520by%2520an%2520agent%2520in%2520a%2520world%2520model.%2520Finally%252C%2520we%2520show%2520the%250Aadvantage%2520of%2520our%2520work%2520in%2520a%2520novel%2520environment%2520compared%2520to%2520traditional%2520machine%250Alearning%2520novelty%2520detection%2520methods%2520as%2520well%2520as%2520currently%2520accepted%2520RL%2520focused%250Anovelty%2520detection%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08731v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novelty%20Detection%20in%20Reinforcement%20Learning%20with%20World%20Models&entry.906535625=Geigh%20Zollicoffer%20and%20Kenneth%20Eaton%20and%20Jonathan%20Balloch%20and%20Julia%20Kim%20and%20Wei%20Zhou%20and%20Robert%20Wright%20and%20Mark%20O.%20Riedl&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20using%20world%20models%20has%20found%20significant%20recent%0Asuccesses.%20However%2C%20when%20a%20sudden%20change%20to%20world%20mechanics%20or%20properties%0Aoccurs%20then%20agent%20performance%20and%20reliability%20can%20dramatically%20decline.%20We%0Arefer%20to%20the%20sudden%20change%20in%20visual%20properties%20or%20state%20transitions%20as%0Anovelties.%20Implementing%20novelty%20detection%20within%20generated%20world%20model%0Aframeworks%20is%20a%20crucial%20task%20for%20protecting%20the%20agent%20when%20deployed.%20In%20this%0Apaper%2C%20we%20propose%20straightforward%20bounding%20approaches%20to%20incorporate%20novelty%0Adetection%20into%20world%20model%20RL%20agents%2C%20by%20utilizing%20the%20misalignment%20of%20the%0Aworld%20model%27s%20hallucinated%20states%20and%20the%20true%20observed%20states%20as%20an%20anomaly%0Ascore.%20We%20provide%20effective%20approaches%20to%20detecting%20novelties%20in%20a%20distribution%0Aof%20transitions%20learned%20by%20an%20agent%20in%20a%20world%20model.%20Finally%2C%20we%20show%20the%0Aadvantage%20of%20our%20work%20in%20a%20novel%20environment%20compared%20to%20traditional%20machine%0Alearning%20novelty%20detection%20methods%20as%20well%20as%20currently%20accepted%20RL%20focused%0Anovelty%20detection%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08731v3&entry.124074799=Read"},
{"title": "Causal-Informed Contrastive Learning: Towards Bias-Resilient\n  Pre-training under Concept Drift", "author": "Xiaoyu Yang and Jie Lu and En Yu", "abstract": "  The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/.\n", "link": "http://arxiv.org/abs/2502.07620v1", "date": "2025-02-11", "relevancy": 1.9473, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4837}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal-Informed%20Contrastive%20Learning%3A%20Towards%20Bias-Resilient%0A%20%20Pre-training%20under%20Concept%20Drift&body=Title%3A%20Causal-Informed%20Contrastive%20Learning%3A%20Towards%20Bias-Resilient%0A%20%20Pre-training%20under%20Concept%20Drift%0AAuthor%3A%20Xiaoyu%20Yang%20and%20Jie%20Lu%20and%20En%20Yu%0AAbstract%3A%20%20%20The%20evolution%20of%20large-scale%20contrastive%20pre-training%20propelled%20by%20top-tier%0Adatasets%20has%20reached%20a%20transition%20point%20in%20the%20scaling%20law.%20Consequently%2C%0Asustaining%20and%20enhancing%20a%20model%27s%20pre-training%20capabilities%20in%20drift%0Aenvironments%20have%20surfaced%20as%20a%20notable%20challenge.%20In%20this%20paper%2C%20we%20initially%0Auncover%20that%20contrastive%20pre-training%20methods%20are%20significantly%20impacted%20by%0Aconcept%20drift%20wherein%20distributions%20change%20unpredictably%2C%20resulting%20in%20notable%0Abiases%20in%20the%20feature%20space%20of%20the%20pre-trained%20model.%20Empowered%20by%20causal%0Ainference%2C%20we%20construct%20a%20structural%20causal%20graph%20to%20analyze%20the%20impact%20of%0Aconcept%20drift%20to%20contrastive%20pre-training%20systemically%2C%20and%20propose%20the%20causal%0Ainterventional%20contrastive%20objective.%20Upon%20achieving%20this%2C%20we%20devise%20a%0Aresilient%20contrastive%20pre-training%20approach%20to%20accommodate%20the%20data%20stream%20of%0Aconcept%20drift%2C%20with%20simple%20and%20scalable%20implementation.%20Extensive%20experiments%0Aon%20various%20downstream%20tasks%20demonstrate%20our%20resilient%20contrastive%20pre-training%0Aeffectively%20mitigates%20the%20bias%20stemming%20from%20the%20concept%20drift%20data%20stream.%0ACodes%20are%20available%20at%20https%3A//anonymous.4open.science/r/ResilientCL/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal-Informed%2520Contrastive%2520Learning%253A%2520Towards%2520Bias-Resilient%250A%2520%2520Pre-training%2520under%2520Concept%2520Drift%26entry.906535625%3DXiaoyu%2520Yang%2520and%2520Jie%2520Lu%2520and%2520En%2520Yu%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520large-scale%2520contrastive%2520pre-training%2520propelled%2520by%2520top-tier%250Adatasets%2520has%2520reached%2520a%2520transition%2520point%2520in%2520the%2520scaling%2520law.%2520Consequently%252C%250Asustaining%2520and%2520enhancing%2520a%2520model%2527s%2520pre-training%2520capabilities%2520in%2520drift%250Aenvironments%2520have%2520surfaced%2520as%2520a%2520notable%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520initially%250Auncover%2520that%2520contrastive%2520pre-training%2520methods%2520are%2520significantly%2520impacted%2520by%250Aconcept%2520drift%2520wherein%2520distributions%2520change%2520unpredictably%252C%2520resulting%2520in%2520notable%250Abiases%2520in%2520the%2520feature%2520space%2520of%2520the%2520pre-trained%2520model.%2520Empowered%2520by%2520causal%250Ainference%252C%2520we%2520construct%2520a%2520structural%2520causal%2520graph%2520to%2520analyze%2520the%2520impact%2520of%250Aconcept%2520drift%2520to%2520contrastive%2520pre-training%2520systemically%252C%2520and%2520propose%2520the%2520causal%250Ainterventional%2520contrastive%2520objective.%2520Upon%2520achieving%2520this%252C%2520we%2520devise%2520a%250Aresilient%2520contrastive%2520pre-training%2520approach%2520to%2520accommodate%2520the%2520data%2520stream%2520of%250Aconcept%2520drift%252C%2520with%2520simple%2520and%2520scalable%2520implementation.%2520Extensive%2520experiments%250Aon%2520various%2520downstream%2520tasks%2520demonstrate%2520our%2520resilient%2520contrastive%2520pre-training%250Aeffectively%2520mitigates%2520the%2520bias%2520stemming%2520from%2520the%2520concept%2520drift%2520data%2520stream.%250ACodes%2520are%2520available%2520at%2520https%253A//anonymous.4open.science/r/ResilientCL/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal-Informed%20Contrastive%20Learning%3A%20Towards%20Bias-Resilient%0A%20%20Pre-training%20under%20Concept%20Drift&entry.906535625=Xiaoyu%20Yang%20and%20Jie%20Lu%20and%20En%20Yu&entry.1292438233=%20%20The%20evolution%20of%20large-scale%20contrastive%20pre-training%20propelled%20by%20top-tier%0Adatasets%20has%20reached%20a%20transition%20point%20in%20the%20scaling%20law.%20Consequently%2C%0Asustaining%20and%20enhancing%20a%20model%27s%20pre-training%20capabilities%20in%20drift%0Aenvironments%20have%20surfaced%20as%20a%20notable%20challenge.%20In%20this%20paper%2C%20we%20initially%0Auncover%20that%20contrastive%20pre-training%20methods%20are%20significantly%20impacted%20by%0Aconcept%20drift%20wherein%20distributions%20change%20unpredictably%2C%20resulting%20in%20notable%0Abiases%20in%20the%20feature%20space%20of%20the%20pre-trained%20model.%20Empowered%20by%20causal%0Ainference%2C%20we%20construct%20a%20structural%20causal%20graph%20to%20analyze%20the%20impact%20of%0Aconcept%20drift%20to%20contrastive%20pre-training%20systemically%2C%20and%20propose%20the%20causal%0Ainterventional%20contrastive%20objective.%20Upon%20achieving%20this%2C%20we%20devise%20a%0Aresilient%20contrastive%20pre-training%20approach%20to%20accommodate%20the%20data%20stream%20of%0Aconcept%20drift%2C%20with%20simple%20and%20scalable%20implementation.%20Extensive%20experiments%0Aon%20various%20downstream%20tasks%20demonstrate%20our%20resilient%20contrastive%20pre-training%0Aeffectively%20mitigates%20the%20bias%20stemming%20from%20the%20concept%20drift%20data%20stream.%0ACodes%20are%20available%20at%20https%3A//anonymous.4open.science/r/ResilientCL/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07620v1&entry.124074799=Read"},
{"title": "Advancing climate model interpretability: Feature attribution for Arctic\n  melt anomalies", "author": "Tolulope Ale and Nicole-Jeanne Schlegel and Vandana P. Janeja", "abstract": "  The focus of our work is improving the interpretability of anomalies in\nclimate models and advancing our understanding of Arctic melt dynamics. The\nArctic and Antarctic ice sheets are experiencing rapid surface melting and\nincreased freshwater runoff, contributing significantly to global sea level\nrise. Understanding the mechanisms driving snowmelt in these regions is\ncrucial. ERA5, a widely used reanalysis dataset in polar climate studies,\noffers extensive climate variables and global data assimilation. However, its\nsnowmelt model employs an energy imbalance approach that may oversimplify the\ncomplexity of surface melt. In contrast, the Glacier Energy and Mass Balance\n(GEMB) model incorporates additional physical processes, such as snow\naccumulation, firn densification, and meltwater percolation/refreezing,\nproviding a more detailed representation of surface melt dynamics. In this\nresearch, we focus on analyzing surface snowmelt dynamics of the Greenland Ice\nSheet using feature attribution for anomalous melt events in ERA5 and GEMB\nmodels. We present a novel unsupervised attribution method leveraging\ncounterfactual explanation method to analyze detected anomalies in ERA5 and\nGEMB. Our anomaly detection results are validated using MEaSUREs ground-truth\ndata, and the attributions are evaluated against established feature ranking\nmethods, including XGBoost, Shapley values, and Random Forest. Our attribution\nframework identifies the physics behind each model and the climate features\ndriving melt anomalies. These findings demonstrate the utility of our\nattribution method in enhancing the interpretability of anomalies in climate\nmodels and advancing our understanding of Arctic melt dynamics.\n", "link": "http://arxiv.org/abs/2502.07741v1", "date": "2025-02-11", "relevancy": 1.2229, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4124}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20climate%20model%20interpretability%3A%20Feature%20attribution%20for%20Arctic%0A%20%20melt%20anomalies&body=Title%3A%20Advancing%20climate%20model%20interpretability%3A%20Feature%20attribution%20for%20Arctic%0A%20%20melt%20anomalies%0AAuthor%3A%20Tolulope%20Ale%20and%20Nicole-Jeanne%20Schlegel%20and%20Vandana%20P.%20Janeja%0AAbstract%3A%20%20%20The%20focus%20of%20our%20work%20is%20improving%20the%20interpretability%20of%20anomalies%20in%0Aclimate%20models%20and%20advancing%20our%20understanding%20of%20Arctic%20melt%20dynamics.%20The%0AArctic%20and%20Antarctic%20ice%20sheets%20are%20experiencing%20rapid%20surface%20melting%20and%0Aincreased%20freshwater%20runoff%2C%20contributing%20significantly%20to%20global%20sea%20level%0Arise.%20Understanding%20the%20mechanisms%20driving%20snowmelt%20in%20these%20regions%20is%0Acrucial.%20ERA5%2C%20a%20widely%20used%20reanalysis%20dataset%20in%20polar%20climate%20studies%2C%0Aoffers%20extensive%20climate%20variables%20and%20global%20data%20assimilation.%20However%2C%20its%0Asnowmelt%20model%20employs%20an%20energy%20imbalance%20approach%20that%20may%20oversimplify%20the%0Acomplexity%20of%20surface%20melt.%20In%20contrast%2C%20the%20Glacier%20Energy%20and%20Mass%20Balance%0A%28GEMB%29%20model%20incorporates%20additional%20physical%20processes%2C%20such%20as%20snow%0Aaccumulation%2C%20firn%20densification%2C%20and%20meltwater%20percolation/refreezing%2C%0Aproviding%20a%20more%20detailed%20representation%20of%20surface%20melt%20dynamics.%20In%20this%0Aresearch%2C%20we%20focus%20on%20analyzing%20surface%20snowmelt%20dynamics%20of%20the%20Greenland%20Ice%0ASheet%20using%20feature%20attribution%20for%20anomalous%20melt%20events%20in%20ERA5%20and%20GEMB%0Amodels.%20We%20present%20a%20novel%20unsupervised%20attribution%20method%20leveraging%0Acounterfactual%20explanation%20method%20to%20analyze%20detected%20anomalies%20in%20ERA5%20and%0AGEMB.%20Our%20anomaly%20detection%20results%20are%20validated%20using%20MEaSUREs%20ground-truth%0Adata%2C%20and%20the%20attributions%20are%20evaluated%20against%20established%20feature%20ranking%0Amethods%2C%20including%20XGBoost%2C%20Shapley%20values%2C%20and%20Random%20Forest.%20Our%20attribution%0Aframework%20identifies%20the%20physics%20behind%20each%20model%20and%20the%20climate%20features%0Adriving%20melt%20anomalies.%20These%20findings%20demonstrate%20the%20utility%20of%20our%0Aattribution%20method%20in%20enhancing%20the%20interpretability%20of%20anomalies%20in%20climate%0Amodels%20and%20advancing%20our%20understanding%20of%20Arctic%20melt%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520climate%2520model%2520interpretability%253A%2520Feature%2520attribution%2520for%2520Arctic%250A%2520%2520melt%2520anomalies%26entry.906535625%3DTolulope%2520Ale%2520and%2520Nicole-Jeanne%2520Schlegel%2520and%2520Vandana%2520P.%2520Janeja%26entry.1292438233%3D%2520%2520The%2520focus%2520of%2520our%2520work%2520is%2520improving%2520the%2520interpretability%2520of%2520anomalies%2520in%250Aclimate%2520models%2520and%2520advancing%2520our%2520understanding%2520of%2520Arctic%2520melt%2520dynamics.%2520The%250AArctic%2520and%2520Antarctic%2520ice%2520sheets%2520are%2520experiencing%2520rapid%2520surface%2520melting%2520and%250Aincreased%2520freshwater%2520runoff%252C%2520contributing%2520significantly%2520to%2520global%2520sea%2520level%250Arise.%2520Understanding%2520the%2520mechanisms%2520driving%2520snowmelt%2520in%2520these%2520regions%2520is%250Acrucial.%2520ERA5%252C%2520a%2520widely%2520used%2520reanalysis%2520dataset%2520in%2520polar%2520climate%2520studies%252C%250Aoffers%2520extensive%2520climate%2520variables%2520and%2520global%2520data%2520assimilation.%2520However%252C%2520its%250Asnowmelt%2520model%2520employs%2520an%2520energy%2520imbalance%2520approach%2520that%2520may%2520oversimplify%2520the%250Acomplexity%2520of%2520surface%2520melt.%2520In%2520contrast%252C%2520the%2520Glacier%2520Energy%2520and%2520Mass%2520Balance%250A%2528GEMB%2529%2520model%2520incorporates%2520additional%2520physical%2520processes%252C%2520such%2520as%2520snow%250Aaccumulation%252C%2520firn%2520densification%252C%2520and%2520meltwater%2520percolation/refreezing%252C%250Aproviding%2520a%2520more%2520detailed%2520representation%2520of%2520surface%2520melt%2520dynamics.%2520In%2520this%250Aresearch%252C%2520we%2520focus%2520on%2520analyzing%2520surface%2520snowmelt%2520dynamics%2520of%2520the%2520Greenland%2520Ice%250ASheet%2520using%2520feature%2520attribution%2520for%2520anomalous%2520melt%2520events%2520in%2520ERA5%2520and%2520GEMB%250Amodels.%2520We%2520present%2520a%2520novel%2520unsupervised%2520attribution%2520method%2520leveraging%250Acounterfactual%2520explanation%2520method%2520to%2520analyze%2520detected%2520anomalies%2520in%2520ERA5%2520and%250AGEMB.%2520Our%2520anomaly%2520detection%2520results%2520are%2520validated%2520using%2520MEaSUREs%2520ground-truth%250Adata%252C%2520and%2520the%2520attributions%2520are%2520evaluated%2520against%2520established%2520feature%2520ranking%250Amethods%252C%2520including%2520XGBoost%252C%2520Shapley%2520values%252C%2520and%2520Random%2520Forest.%2520Our%2520attribution%250Aframework%2520identifies%2520the%2520physics%2520behind%2520each%2520model%2520and%2520the%2520climate%2520features%250Adriving%2520melt%2520anomalies.%2520These%2520findings%2520demonstrate%2520the%2520utility%2520of%2520our%250Aattribution%2520method%2520in%2520enhancing%2520the%2520interpretability%2520of%2520anomalies%2520in%2520climate%250Amodels%2520and%2520advancing%2520our%2520understanding%2520of%2520Arctic%2520melt%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20climate%20model%20interpretability%3A%20Feature%20attribution%20for%20Arctic%0A%20%20melt%20anomalies&entry.906535625=Tolulope%20Ale%20and%20Nicole-Jeanne%20Schlegel%20and%20Vandana%20P.%20Janeja&entry.1292438233=%20%20The%20focus%20of%20our%20work%20is%20improving%20the%20interpretability%20of%20anomalies%20in%0Aclimate%20models%20and%20advancing%20our%20understanding%20of%20Arctic%20melt%20dynamics.%20The%0AArctic%20and%20Antarctic%20ice%20sheets%20are%20experiencing%20rapid%20surface%20melting%20and%0Aincreased%20freshwater%20runoff%2C%20contributing%20significantly%20to%20global%20sea%20level%0Arise.%20Understanding%20the%20mechanisms%20driving%20snowmelt%20in%20these%20regions%20is%0Acrucial.%20ERA5%2C%20a%20widely%20used%20reanalysis%20dataset%20in%20polar%20climate%20studies%2C%0Aoffers%20extensive%20climate%20variables%20and%20global%20data%20assimilation.%20However%2C%20its%0Asnowmelt%20model%20employs%20an%20energy%20imbalance%20approach%20that%20may%20oversimplify%20the%0Acomplexity%20of%20surface%20melt.%20In%20contrast%2C%20the%20Glacier%20Energy%20and%20Mass%20Balance%0A%28GEMB%29%20model%20incorporates%20additional%20physical%20processes%2C%20such%20as%20snow%0Aaccumulation%2C%20firn%20densification%2C%20and%20meltwater%20percolation/refreezing%2C%0Aproviding%20a%20more%20detailed%20representation%20of%20surface%20melt%20dynamics.%20In%20this%0Aresearch%2C%20we%20focus%20on%20analyzing%20surface%20snowmelt%20dynamics%20of%20the%20Greenland%20Ice%0ASheet%20using%20feature%20attribution%20for%20anomalous%20melt%20events%20in%20ERA5%20and%20GEMB%0Amodels.%20We%20present%20a%20novel%20unsupervised%20attribution%20method%20leveraging%0Acounterfactual%20explanation%20method%20to%20analyze%20detected%20anomalies%20in%20ERA5%20and%0AGEMB.%20Our%20anomaly%20detection%20results%20are%20validated%20using%20MEaSUREs%20ground-truth%0Adata%2C%20and%20the%20attributions%20are%20evaluated%20against%20established%20feature%20ranking%0Amethods%2C%20including%20XGBoost%2C%20Shapley%20values%2C%20and%20Random%20Forest.%20Our%20attribution%0Aframework%20identifies%20the%20physics%20behind%20each%20model%20and%20the%20climate%20features%0Adriving%20melt%20anomalies.%20These%20findings%20demonstrate%20the%20utility%20of%20our%0Aattribution%20method%20in%20enhancing%20the%20interpretability%20of%20anomalies%20in%20climate%0Amodels%20and%20advancing%20our%20understanding%20of%20Arctic%20melt%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07741v1&entry.124074799=Read"},
{"title": "Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras", "author": "Nektarios A. Valous and Eckhard Hitzer and Drago\u015f Du\u015fe and Rodrigo Rojas Moraleda and Ferdinand Popp and Meggy Suarez-Carmona and Anna Berthel and Ismini Papageorgiou and Carlo Fremd and Alexander R\u00f6lle and Christina C. Westhoff and B\u00e9n\u00e9dicte Lenoir and Niels Halama and Inka Z\u00f6rnig and Dirk J\u00e4ger", "abstract": "  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n", "link": "http://arxiv.org/abs/2502.07758v1", "date": "2025-02-11", "relevancy": 1.5521, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.531}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5176}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras&body=Title%3A%20Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras%0AAuthor%3A%20Nektarios%20A.%20Valous%20and%20Eckhard%20Hitzer%20and%20Drago%C5%9F%20Du%C5%9Fe%20and%20Rodrigo%20Rojas%20Moraleda%20and%20Ferdinand%20Popp%20and%20Meggy%20Suarez-Carmona%20and%20Anna%20Berthel%20and%20Ismini%20Papageorgiou%20and%20Carlo%20Fremd%20and%20Alexander%20R%C3%B6lle%20and%20Christina%20C.%20Westhoff%20and%20B%C3%A9n%C3%A9dicte%20Lenoir%20and%20Niels%20Halama%20and%20Inka%20Z%C3%B6rnig%20and%20Dirk%20J%C3%A4ger%0AAbstract%3A%20%20%20Hypercomplex%20image%20processing%20extends%20conventional%20techniques%20in%20a%20unified%0Aparadigm%20encompassing%20algebraic%20and%20geometric%20principles.%20This%20work%20leverages%0Aquaternions%20and%20the%20two-dimensional%20orthogonal%20planes%20split%20framework%0A%28splitting%20of%20a%20quaternion%20-%20representing%20a%20pixel%20-%20into%20pairs%20of%20orthogonal%202D%0Aplanes%29%20for%20natural/biomedical%20image%20analysis%20through%20the%20following%0Acomputational%20workflows%20and%20outcomes%3A%20natural/biomedical%20image%20re-colorization%2C%0Anatural%20image%20de-colorization%2C%20natural/biomedical%20image%20contrast%20enhancement%2C%0Acomputational%20re-staining%20and%20stain%20separation%20in%20histological%20images%2C%20and%0Aperformance%20gains%20in%20machine/deep%20learning%20pipelines%20for%20histological%20images.%0AThe%20workflows%20are%20analyzed%20separately%20for%20natural%20and%20biomedical%20images%20to%0Ashowcase%20the%20effectiveness%20of%20the%20proposed%20approaches.%20The%20proposed%20workflows%0Acan%20regulate%20color%20appearance%20%28e.g.%20with%20alternative%20renditions%20and%20grayscale%0Aconversion%29%20and%20image%20contrast%2C%20be%20part%20of%20automated%20image%20processing%20pipelines%0A%28e.g.%20isolating%20stain%20components%2C%20boosting%20learning%20models%29%2C%20and%20assist%20in%0Adigital%20pathology%20applications%20%28e.g.%20enhancing%20biomarker%20visibility%2C%20enabling%0Acolorblind-friendly%20renditions%29.%20Employing%20only%20basic%20arithmetic%20and%20matrix%0Aoperations%2C%20this%20work%20offers%20a%20computationally%20accessible%20methodology%20-%20in%20the%0Ahypercomplex%20domain%20-%20that%20showcases%20versatility%20and%20consistency%20across%20image%0Aprocessing%20tasks%20and%20a%20range%20of%20computer%20vision%20and%20biomedical%20applications.%0AThe%20proposed%20non-data-driven%20methods%20achieve%20comparable%20or%20better%20results%0A%28particularly%20in%20cases%20involving%20well-known%20methods%29%20to%20those%20reported%20in%20the%0Aliterature%2C%20showcasing%20the%20potential%20of%20robust%20theoretical%20frameworks%20with%0Apractical%20effectiveness.%20Results%2C%20methods%2C%20and%20limitations%20are%20detailed%0Aalongside%20discussion%20of%20promising%20extensions%2C%20emphasizing%20the%20potential%20of%0Afeature-rich%20mathematical/computational%20frameworks%20for%20natural%20and%20biomedical%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520computational%2520workflows%2520for%2520natural%2520and%2520biomedical%2520image%250A%2520%2520processing%2520based%2520on%2520hypercomplex%2520algebras%26entry.906535625%3DNektarios%2520A.%2520Valous%2520and%2520Eckhard%2520Hitzer%2520and%2520Drago%25C5%259F%2520Du%25C5%259Fe%2520and%2520Rodrigo%2520Rojas%2520Moraleda%2520and%2520Ferdinand%2520Popp%2520and%2520Meggy%2520Suarez-Carmona%2520and%2520Anna%2520Berthel%2520and%2520Ismini%2520Papageorgiou%2520and%2520Carlo%2520Fremd%2520and%2520Alexander%2520R%25C3%25B6lle%2520and%2520Christina%2520C.%2520Westhoff%2520and%2520B%25C3%25A9n%25C3%25A9dicte%2520Lenoir%2520and%2520Niels%2520Halama%2520and%2520Inka%2520Z%25C3%25B6rnig%2520and%2520Dirk%2520J%25C3%25A4ger%26entry.1292438233%3D%2520%2520Hypercomplex%2520image%2520processing%2520extends%2520conventional%2520techniques%2520in%2520a%2520unified%250Aparadigm%2520encompassing%2520algebraic%2520and%2520geometric%2520principles.%2520This%2520work%2520leverages%250Aquaternions%2520and%2520the%2520two-dimensional%2520orthogonal%2520planes%2520split%2520framework%250A%2528splitting%2520of%2520a%2520quaternion%2520-%2520representing%2520a%2520pixel%2520-%2520into%2520pairs%2520of%2520orthogonal%25202D%250Aplanes%2529%2520for%2520natural/biomedical%2520image%2520analysis%2520through%2520the%2520following%250Acomputational%2520workflows%2520and%2520outcomes%253A%2520natural/biomedical%2520image%2520re-colorization%252C%250Anatural%2520image%2520de-colorization%252C%2520natural/biomedical%2520image%2520contrast%2520enhancement%252C%250Acomputational%2520re-staining%2520and%2520stain%2520separation%2520in%2520histological%2520images%252C%2520and%250Aperformance%2520gains%2520in%2520machine/deep%2520learning%2520pipelines%2520for%2520histological%2520images.%250AThe%2520workflows%2520are%2520analyzed%2520separately%2520for%2520natural%2520and%2520biomedical%2520images%2520to%250Ashowcase%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approaches.%2520The%2520proposed%2520workflows%250Acan%2520regulate%2520color%2520appearance%2520%2528e.g.%2520with%2520alternative%2520renditions%2520and%2520grayscale%250Aconversion%2529%2520and%2520image%2520contrast%252C%2520be%2520part%2520of%2520automated%2520image%2520processing%2520pipelines%250A%2528e.g.%2520isolating%2520stain%2520components%252C%2520boosting%2520learning%2520models%2529%252C%2520and%2520assist%2520in%250Adigital%2520pathology%2520applications%2520%2528e.g.%2520enhancing%2520biomarker%2520visibility%252C%2520enabling%250Acolorblind-friendly%2520renditions%2529.%2520Employing%2520only%2520basic%2520arithmetic%2520and%2520matrix%250Aoperations%252C%2520this%2520work%2520offers%2520a%2520computationally%2520accessible%2520methodology%2520-%2520in%2520the%250Ahypercomplex%2520domain%2520-%2520that%2520showcases%2520versatility%2520and%2520consistency%2520across%2520image%250Aprocessing%2520tasks%2520and%2520a%2520range%2520of%2520computer%2520vision%2520and%2520biomedical%2520applications.%250AThe%2520proposed%2520non-data-driven%2520methods%2520achieve%2520comparable%2520or%2520better%2520results%250A%2528particularly%2520in%2520cases%2520involving%2520well-known%2520methods%2529%2520to%2520those%2520reported%2520in%2520the%250Aliterature%252C%2520showcasing%2520the%2520potential%2520of%2520robust%2520theoretical%2520frameworks%2520with%250Apractical%2520effectiveness.%2520Results%252C%2520methods%252C%2520and%2520limitations%2520are%2520detailed%250Aalongside%2520discussion%2520of%2520promising%2520extensions%252C%2520emphasizing%2520the%2520potential%2520of%250Afeature-rich%2520mathematical/computational%2520frameworks%2520for%2520natural%2520and%2520biomedical%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras&entry.906535625=Nektarios%20A.%20Valous%20and%20Eckhard%20Hitzer%20and%20Drago%C5%9F%20Du%C5%9Fe%20and%20Rodrigo%20Rojas%20Moraleda%20and%20Ferdinand%20Popp%20and%20Meggy%20Suarez-Carmona%20and%20Anna%20Berthel%20and%20Ismini%20Papageorgiou%20and%20Carlo%20Fremd%20and%20Alexander%20R%C3%B6lle%20and%20Christina%20C.%20Westhoff%20and%20B%C3%A9n%C3%A9dicte%20Lenoir%20and%20Niels%20Halama%20and%20Inka%20Z%C3%B6rnig%20and%20Dirk%20J%C3%A4ger&entry.1292438233=%20%20Hypercomplex%20image%20processing%20extends%20conventional%20techniques%20in%20a%20unified%0Aparadigm%20encompassing%20algebraic%20and%20geometric%20principles.%20This%20work%20leverages%0Aquaternions%20and%20the%20two-dimensional%20orthogonal%20planes%20split%20framework%0A%28splitting%20of%20a%20quaternion%20-%20representing%20a%20pixel%20-%20into%20pairs%20of%20orthogonal%202D%0Aplanes%29%20for%20natural/biomedical%20image%20analysis%20through%20the%20following%0Acomputational%20workflows%20and%20outcomes%3A%20natural/biomedical%20image%20re-colorization%2C%0Anatural%20image%20de-colorization%2C%20natural/biomedical%20image%20contrast%20enhancement%2C%0Acomputational%20re-staining%20and%20stain%20separation%20in%20histological%20images%2C%20and%0Aperformance%20gains%20in%20machine/deep%20learning%20pipelines%20for%20histological%20images.%0AThe%20workflows%20are%20analyzed%20separately%20for%20natural%20and%20biomedical%20images%20to%0Ashowcase%20the%20effectiveness%20of%20the%20proposed%20approaches.%20The%20proposed%20workflows%0Acan%20regulate%20color%20appearance%20%28e.g.%20with%20alternative%20renditions%20and%20grayscale%0Aconversion%29%20and%20image%20contrast%2C%20be%20part%20of%20automated%20image%20processing%20pipelines%0A%28e.g.%20isolating%20stain%20components%2C%20boosting%20learning%20models%29%2C%20and%20assist%20in%0Adigital%20pathology%20applications%20%28e.g.%20enhancing%20biomarker%20visibility%2C%20enabling%0Acolorblind-friendly%20renditions%29.%20Employing%20only%20basic%20arithmetic%20and%20matrix%0Aoperations%2C%20this%20work%20offers%20a%20computationally%20accessible%20methodology%20-%20in%20the%0Ahypercomplex%20domain%20-%20that%20showcases%20versatility%20and%20consistency%20across%20image%0Aprocessing%20tasks%20and%20a%20range%20of%20computer%20vision%20and%20biomedical%20applications.%0AThe%20proposed%20non-data-driven%20methods%20achieve%20comparable%20or%20better%20results%0A%28particularly%20in%20cases%20involving%20well-known%20methods%29%20to%20those%20reported%20in%20the%0Aliterature%2C%20showcasing%20the%20potential%20of%20robust%20theoretical%20frameworks%20with%0Apractical%20effectiveness.%20Results%2C%20methods%2C%20and%20limitations%20are%20detailed%0Aalongside%20discussion%20of%20promising%20extensions%2C%20emphasizing%20the%20potential%20of%0Afeature-rich%20mathematical/computational%20frameworks%20for%20natural%20and%20biomedical%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07758v1&entry.124074799=Read"},
{"title": "Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time\n  Series Forecasting Based on Biological ODEs", "author": "Christian Kl\u00f6tergens and Vijaya Krishna Yalavarthi and Randolf Scholz and Maximilian Stubbemann and Stefan Born and Lars Schmidt-Thieme", "abstract": "  State-of-the-art methods for forecasting irregularly sampled time series with\nmissing values predominantly rely on just four datasets and a few small toy\nexamples for evaluation. While ordinary differential equations (ODE) are the\nprevalent models in science and engineering, a baseline model that forecasts a\nconstant value outperforms ODE-based models from the last five years on three\nof these existing datasets. This unintuitive finding hampers further research\non ODE-based models, a more plausible model family. In this paper, we develop a\nmethodology to generate irregularly sampled multivariate time series (IMTS)\ndatasets from ordinary differential equations and to select challenging\ninstances via rejection sampling. Using this methodology, we create\nPhysiome-ODE, a large and sophisticated benchmark of IMTS datasets consisting\nof 50 individual datasets, derived from real-world ordinary differential\nequations from research in biology. Physiome-ODE is the first benchmark for\nIMTS forecasting that we are aware of and an order of magnitude larger than the\ncurrent evaluation setting of four datasets. Using our benchmark Physiome-ODE,\nwe show qualitatively completely different results than those derived from the\ncurrent four datasets: on Physiome-ODE ODE-based models can play to their\nstrength and our benchmark can differentiate in a meaningful way between\ndifferent IMTS forecasting models. This way, we expect to give a new impulse to\nresearch on ODE-based time series modeling.\n", "link": "http://arxiv.org/abs/2502.07489v1", "date": "2025-02-11", "relevancy": 1.8805, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4771}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4737}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physiome-ODE%3A%20A%20Benchmark%20for%20Irregularly%20Sampled%20Multivariate%20Time%0A%20%20Series%20Forecasting%20Based%20on%20Biological%20ODEs&body=Title%3A%20Physiome-ODE%3A%20A%20Benchmark%20for%20Irregularly%20Sampled%20Multivariate%20Time%0A%20%20Series%20Forecasting%20Based%20on%20Biological%20ODEs%0AAuthor%3A%20Christian%20Kl%C3%B6tergens%20and%20Vijaya%20Krishna%20Yalavarthi%20and%20Randolf%20Scholz%20and%20Maximilian%20Stubbemann%20and%20Stefan%20Born%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20%20%20State-of-the-art%20methods%20for%20forecasting%20irregularly%20sampled%20time%20series%20with%0Amissing%20values%20predominantly%20rely%20on%20just%20four%20datasets%20and%20a%20few%20small%20toy%0Aexamples%20for%20evaluation.%20While%20ordinary%20differential%20equations%20%28ODE%29%20are%20the%0Aprevalent%20models%20in%20science%20and%20engineering%2C%20a%20baseline%20model%20that%20forecasts%20a%0Aconstant%20value%20outperforms%20ODE-based%20models%20from%20the%20last%20five%20years%20on%20three%0Aof%20these%20existing%20datasets.%20This%20unintuitive%20finding%20hampers%20further%20research%0Aon%20ODE-based%20models%2C%20a%20more%20plausible%20model%20family.%20In%20this%20paper%2C%20we%20develop%20a%0Amethodology%20to%20generate%20irregularly%20sampled%20multivariate%20time%20series%20%28IMTS%29%0Adatasets%20from%20ordinary%20differential%20equations%20and%20to%20select%20challenging%0Ainstances%20via%20rejection%20sampling.%20Using%20this%20methodology%2C%20we%20create%0APhysiome-ODE%2C%20a%20large%20and%20sophisticated%20benchmark%20of%20IMTS%20datasets%20consisting%0Aof%2050%20individual%20datasets%2C%20derived%20from%20real-world%20ordinary%20differential%0Aequations%20from%20research%20in%20biology.%20Physiome-ODE%20is%20the%20first%20benchmark%20for%0AIMTS%20forecasting%20that%20we%20are%20aware%20of%20and%20an%20order%20of%20magnitude%20larger%20than%20the%0Acurrent%20evaluation%20setting%20of%20four%20datasets.%20Using%20our%20benchmark%20Physiome-ODE%2C%0Awe%20show%20qualitatively%20completely%20different%20results%20than%20those%20derived%20from%20the%0Acurrent%20four%20datasets%3A%20on%20Physiome-ODE%20ODE-based%20models%20can%20play%20to%20their%0Astrength%20and%20our%20benchmark%20can%20differentiate%20in%20a%20meaningful%20way%20between%0Adifferent%20IMTS%20forecasting%20models.%20This%20way%2C%20we%20expect%20to%20give%20a%20new%20impulse%20to%0Aresearch%20on%20ODE-based%20time%20series%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysiome-ODE%253A%2520A%2520Benchmark%2520for%2520Irregularly%2520Sampled%2520Multivariate%2520Time%250A%2520%2520Series%2520Forecasting%2520Based%2520on%2520Biological%2520ODEs%26entry.906535625%3DChristian%2520Kl%25C3%25B6tergens%2520and%2520Vijaya%2520Krishna%2520Yalavarthi%2520and%2520Randolf%2520Scholz%2520and%2520Maximilian%2520Stubbemann%2520and%2520Stefan%2520Born%2520and%2520Lars%2520Schmidt-Thieme%26entry.1292438233%3D%2520%2520State-of-the-art%2520methods%2520for%2520forecasting%2520irregularly%2520sampled%2520time%2520series%2520with%250Amissing%2520values%2520predominantly%2520rely%2520on%2520just%2520four%2520datasets%2520and%2520a%2520few%2520small%2520toy%250Aexamples%2520for%2520evaluation.%2520While%2520ordinary%2520differential%2520equations%2520%2528ODE%2529%2520are%2520the%250Aprevalent%2520models%2520in%2520science%2520and%2520engineering%252C%2520a%2520baseline%2520model%2520that%2520forecasts%2520a%250Aconstant%2520value%2520outperforms%2520ODE-based%2520models%2520from%2520the%2520last%2520five%2520years%2520on%2520three%250Aof%2520these%2520existing%2520datasets.%2520This%2520unintuitive%2520finding%2520hampers%2520further%2520research%250Aon%2520ODE-based%2520models%252C%2520a%2520more%2520plausible%2520model%2520family.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%250Amethodology%2520to%2520generate%2520irregularly%2520sampled%2520multivariate%2520time%2520series%2520%2528IMTS%2529%250Adatasets%2520from%2520ordinary%2520differential%2520equations%2520and%2520to%2520select%2520challenging%250Ainstances%2520via%2520rejection%2520sampling.%2520Using%2520this%2520methodology%252C%2520we%2520create%250APhysiome-ODE%252C%2520a%2520large%2520and%2520sophisticated%2520benchmark%2520of%2520IMTS%2520datasets%2520consisting%250Aof%252050%2520individual%2520datasets%252C%2520derived%2520from%2520real-world%2520ordinary%2520differential%250Aequations%2520from%2520research%2520in%2520biology.%2520Physiome-ODE%2520is%2520the%2520first%2520benchmark%2520for%250AIMTS%2520forecasting%2520that%2520we%2520are%2520aware%2520of%2520and%2520an%2520order%2520of%2520magnitude%2520larger%2520than%2520the%250Acurrent%2520evaluation%2520setting%2520of%2520four%2520datasets.%2520Using%2520our%2520benchmark%2520Physiome-ODE%252C%250Awe%2520show%2520qualitatively%2520completely%2520different%2520results%2520than%2520those%2520derived%2520from%2520the%250Acurrent%2520four%2520datasets%253A%2520on%2520Physiome-ODE%2520ODE-based%2520models%2520can%2520play%2520to%2520their%250Astrength%2520and%2520our%2520benchmark%2520can%2520differentiate%2520in%2520a%2520meaningful%2520way%2520between%250Adifferent%2520IMTS%2520forecasting%2520models.%2520This%2520way%252C%2520we%2520expect%2520to%2520give%2520a%2520new%2520impulse%2520to%250Aresearch%2520on%2520ODE-based%2520time%2520series%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physiome-ODE%3A%20A%20Benchmark%20for%20Irregularly%20Sampled%20Multivariate%20Time%0A%20%20Series%20Forecasting%20Based%20on%20Biological%20ODEs&entry.906535625=Christian%20Kl%C3%B6tergens%20and%20Vijaya%20Krishna%20Yalavarthi%20and%20Randolf%20Scholz%20and%20Maximilian%20Stubbemann%20and%20Stefan%20Born%20and%20Lars%20Schmidt-Thieme&entry.1292438233=%20%20State-of-the-art%20methods%20for%20forecasting%20irregularly%20sampled%20time%20series%20with%0Amissing%20values%20predominantly%20rely%20on%20just%20four%20datasets%20and%20a%20few%20small%20toy%0Aexamples%20for%20evaluation.%20While%20ordinary%20differential%20equations%20%28ODE%29%20are%20the%0Aprevalent%20models%20in%20science%20and%20engineering%2C%20a%20baseline%20model%20that%20forecasts%20a%0Aconstant%20value%20outperforms%20ODE-based%20models%20from%20the%20last%20five%20years%20on%20three%0Aof%20these%20existing%20datasets.%20This%20unintuitive%20finding%20hampers%20further%20research%0Aon%20ODE-based%20models%2C%20a%20more%20plausible%20model%20family.%20In%20this%20paper%2C%20we%20develop%20a%0Amethodology%20to%20generate%20irregularly%20sampled%20multivariate%20time%20series%20%28IMTS%29%0Adatasets%20from%20ordinary%20differential%20equations%20and%20to%20select%20challenging%0Ainstances%20via%20rejection%20sampling.%20Using%20this%20methodology%2C%20we%20create%0APhysiome-ODE%2C%20a%20large%20and%20sophisticated%20benchmark%20of%20IMTS%20datasets%20consisting%0Aof%2050%20individual%20datasets%2C%20derived%20from%20real-world%20ordinary%20differential%0Aequations%20from%20research%20in%20biology.%20Physiome-ODE%20is%20the%20first%20benchmark%20for%0AIMTS%20forecasting%20that%20we%20are%20aware%20of%20and%20an%20order%20of%20magnitude%20larger%20than%20the%0Acurrent%20evaluation%20setting%20of%20four%20datasets.%20Using%20our%20benchmark%20Physiome-ODE%2C%0Awe%20show%20qualitatively%20completely%20different%20results%20than%20those%20derived%20from%20the%0Acurrent%20four%20datasets%3A%20on%20Physiome-ODE%20ODE-based%20models%20can%20play%20to%20their%0Astrength%20and%20our%20benchmark%20can%20differentiate%20in%20a%20meaningful%20way%20between%0Adifferent%20IMTS%20forecasting%20models.%20This%20way%2C%20we%20expect%20to%20give%20a%20new%20impulse%20to%0Aresearch%20on%20ODE-based%20time%20series%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07489v1&entry.124074799=Read"},
{"title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning", "author": "Heyang Zhao and Chenlu Ye and Wei Xiong and Quanquan Gu and Tong Zhang", "abstract": "  Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.\n", "link": "http://arxiv.org/abs/2502.07460v1", "date": "2025-02-11", "relevancy": 1.2987, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4426}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4352}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logarithmic%20Regret%20for%20Online%20KL-Regularized%20Reinforcement%20Learning&body=Title%3A%20Logarithmic%20Regret%20for%20Online%20KL-Regularized%20Reinforcement%20Learning%0AAuthor%3A%20Heyang%20Zhao%20and%20Chenlu%20Ye%20and%20Wei%20Xiong%20and%20Quanquan%20Gu%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20have%0Ashown%20that%20KL-regularization%20plays%20a%20pivotal%20role%20in%20improving%20the%20efficiency%0Aof%20RL%20fine-tuning%20for%20large%20language%20models%20%28LLMs%29.%20Despite%20its%20empirical%0Aadvantage%2C%20the%20theoretical%20difference%20between%20KL-regularized%20RL%20and%20standard%20RL%0Aremains%20largely%20under-explored.%20While%20there%20is%20a%20recent%20line%20of%20work%20on%20the%0Atheoretical%20analysis%20of%20KL-regularized%20objective%20in%20decision%20making%0A%5Ccitep%7Bxiong2024iterative%2C%20xie2024exploratory%2Czhao2024sharp%7D%2C%20these%20analyses%0Aeither%20reduce%20to%20the%20traditional%20RL%20setting%20or%20rely%20on%20strong%20coverage%0Aassumptions.%20In%20this%20paper%2C%20we%20propose%20an%20optimism-based%20KL-regularized%20online%0Acontextual%20bandit%20algorithm%2C%20and%20provide%20a%20novel%20analysis%20of%20its%20regret.%20By%0Acarefully%20leveraging%20the%20benign%20optimization%20landscape%20induced%20by%20the%0AKL-regularization%20and%20the%20optimistic%20reward%20estimation%2C%20our%20algorithm%20achieves%0Aan%20%24%5Cmathcal%7BO%7D%5Cbig%28%5Ceta%5Clog%20%28N_%7B%5Cmathcal%20R%7D%20T%29%5Ccdot%20d_%7B%5Cmathcal%20R%7D%5Cbig%29%24%0Alogarithmic%20regret%20bound%2C%20where%20%24%5Ceta%2C%20N_%7B%5Cmathcal%20R%7D%2CT%2Cd_%7B%5Cmathcal%20R%7D%24%20denote%0Athe%20KL-regularization%20parameter%2C%20the%20cardinality%20of%20the%20reward%20function%20class%2C%0Anumber%20of%20rounds%2C%20and%20the%20complexity%20of%20the%20reward%20function%20class.%20Furthermore%2C%0Awe%20extend%20our%20algorithm%20and%20analysis%20to%20reinforcement%20learning%20by%20developing%20a%0Anovel%20decomposition%20over%20transition%20steps%20and%20also%20obtain%20a%20similar%20logarithmic%0Aregret%20bound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogarithmic%2520Regret%2520for%2520Online%2520KL-Regularized%2520Reinforcement%2520Learning%26entry.906535625%3DHeyang%2520Zhao%2520and%2520Chenlu%2520Ye%2520and%2520Wei%2520Xiong%2520and%2520Quanquan%2520Gu%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520have%250Ashown%2520that%2520KL-regularization%2520plays%2520a%2520pivotal%2520role%2520in%2520improving%2520the%2520efficiency%250Aof%2520RL%2520fine-tuning%2520for%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Despite%2520its%2520empirical%250Aadvantage%252C%2520the%2520theoretical%2520difference%2520between%2520KL-regularized%2520RL%2520and%2520standard%2520RL%250Aremains%2520largely%2520under-explored.%2520While%2520there%2520is%2520a%2520recent%2520line%2520of%2520work%2520on%2520the%250Atheoretical%2520analysis%2520of%2520KL-regularized%2520objective%2520in%2520decision%2520making%250A%255Ccitep%257Bxiong2024iterative%252C%2520xie2024exploratory%252Czhao2024sharp%257D%252C%2520these%2520analyses%250Aeither%2520reduce%2520to%2520the%2520traditional%2520RL%2520setting%2520or%2520rely%2520on%2520strong%2520coverage%250Aassumptions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520optimism-based%2520KL-regularized%2520online%250Acontextual%2520bandit%2520algorithm%252C%2520and%2520provide%2520a%2520novel%2520analysis%2520of%2520its%2520regret.%2520By%250Acarefully%2520leveraging%2520the%2520benign%2520optimization%2520landscape%2520induced%2520by%2520the%250AKL-regularization%2520and%2520the%2520optimistic%2520reward%2520estimation%252C%2520our%2520algorithm%2520achieves%250Aan%2520%2524%255Cmathcal%257BO%257D%255Cbig%2528%255Ceta%255Clog%2520%2528N_%257B%255Cmathcal%2520R%257D%2520T%2529%255Ccdot%2520d_%257B%255Cmathcal%2520R%257D%255Cbig%2529%2524%250Alogarithmic%2520regret%2520bound%252C%2520where%2520%2524%255Ceta%252C%2520N_%257B%255Cmathcal%2520R%257D%252CT%252Cd_%257B%255Cmathcal%2520R%257D%2524%2520denote%250Athe%2520KL-regularization%2520parameter%252C%2520the%2520cardinality%2520of%2520the%2520reward%2520function%2520class%252C%250Anumber%2520of%2520rounds%252C%2520and%2520the%2520complexity%2520of%2520the%2520reward%2520function%2520class.%2520Furthermore%252C%250Awe%2520extend%2520our%2520algorithm%2520and%2520analysis%2520to%2520reinforcement%2520learning%2520by%2520developing%2520a%250Anovel%2520decomposition%2520over%2520transition%2520steps%2520and%2520also%2520obtain%2520a%2520similar%2520logarithmic%250Aregret%2520bound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logarithmic%20Regret%20for%20Online%20KL-Regularized%20Reinforcement%20Learning&entry.906535625=Heyang%20Zhao%20and%20Chenlu%20Ye%20and%20Wei%20Xiong%20and%20Quanquan%20Gu%20and%20Tong%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20have%0Ashown%20that%20KL-regularization%20plays%20a%20pivotal%20role%20in%20improving%20the%20efficiency%0Aof%20RL%20fine-tuning%20for%20large%20language%20models%20%28LLMs%29.%20Despite%20its%20empirical%0Aadvantage%2C%20the%20theoretical%20difference%20between%20KL-regularized%20RL%20and%20standard%20RL%0Aremains%20largely%20under-explored.%20While%20there%20is%20a%20recent%20line%20of%20work%20on%20the%0Atheoretical%20analysis%20of%20KL-regularized%20objective%20in%20decision%20making%0A%5Ccitep%7Bxiong2024iterative%2C%20xie2024exploratory%2Czhao2024sharp%7D%2C%20these%20analyses%0Aeither%20reduce%20to%20the%20traditional%20RL%20setting%20or%20rely%20on%20strong%20coverage%0Aassumptions.%20In%20this%20paper%2C%20we%20propose%20an%20optimism-based%20KL-regularized%20online%0Acontextual%20bandit%20algorithm%2C%20and%20provide%20a%20novel%20analysis%20of%20its%20regret.%20By%0Acarefully%20leveraging%20the%20benign%20optimization%20landscape%20induced%20by%20the%0AKL-regularization%20and%20the%20optimistic%20reward%20estimation%2C%20our%20algorithm%20achieves%0Aan%20%24%5Cmathcal%7BO%7D%5Cbig%28%5Ceta%5Clog%20%28N_%7B%5Cmathcal%20R%7D%20T%29%5Ccdot%20d_%7B%5Cmathcal%20R%7D%5Cbig%29%24%0Alogarithmic%20regret%20bound%2C%20where%20%24%5Ceta%2C%20N_%7B%5Cmathcal%20R%7D%2CT%2Cd_%7B%5Cmathcal%20R%7D%24%20denote%0Athe%20KL-regularization%20parameter%2C%20the%20cardinality%20of%20the%20reward%20function%20class%2C%0Anumber%20of%20rounds%2C%20and%20the%20complexity%20of%20the%20reward%20function%20class.%20Furthermore%2C%0Awe%20extend%20our%20algorithm%20and%20analysis%20to%20reinforcement%20learning%20by%20developing%20a%0Anovel%20decomposition%20over%20transition%20steps%20and%20also%20obtain%20a%20similar%20logarithmic%0Aregret%20bound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07460v1&entry.124074799=Read"},
{"title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces", "author": "Loris Gaven and Thomas Carta and Cl\u00e9ment Romac and C\u00e9dric Colas and Sylvain Lamprier and Olivier Sigaud and Pierre-Yves Oudeyer", "abstract": "  Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.\n", "link": "http://arxiv.org/abs/2502.07709v1", "date": "2025-02-11", "relevancy": 1.5343, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5341}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGELLAN%3A%20Metacognitive%20predictions%20of%20learning%20progress%20guide%20autotelic%0A%20%20LLM%20agents%20in%20large%20goal%20spaces&body=Title%3A%20MAGELLAN%3A%20Metacognitive%20predictions%20of%20learning%20progress%20guide%20autotelic%0A%20%20LLM%20agents%20in%20large%20goal%20spaces%0AAuthor%3A%20Loris%20Gaven%20and%20Thomas%20Carta%20and%20Cl%C3%A9ment%20Romac%20and%20C%C3%A9dric%20Colas%20and%20Sylvain%20Lamprier%20and%20Olivier%20Sigaud%20and%20Pierre-Yves%20Oudeyer%0AAbstract%3A%20%20%20Open-ended%20learning%20agents%20must%20efficiently%20prioritize%20goals%20in%20vast%0Apossibility%20spaces%2C%20focusing%20on%20those%20that%20maximize%20learning%20progress%20%28LP%29.%0AWhen%20such%20autotelic%20exploration%20is%20achieved%20by%20LLM%20agents%20trained%20with%20online%0ARL%20in%20high-dimensional%20and%20evolving%20goal%20spaces%2C%20a%20key%20challenge%20for%20LP%0Aprediction%20is%20modeling%20one%27s%20own%20competence%2C%20a%20form%20of%20metacognitive%0Amonitoring.%20Traditional%20approaches%20either%20require%20extensive%20sampling%20or%20rely%20on%0Abrittle%20expert-defined%20goal%20groupings.%20We%20introduce%20MAGELLAN%2C%20a%20metacognitive%0Aframework%20that%20lets%20LLM%20agents%20learn%20to%20predict%20their%20competence%20and%20LP%20online.%0ABy%20capturing%20semantic%20relationships%20between%20goals%2C%20MAGELLAN%20enables%0Asample-efficient%20LP%20estimation%20and%20dynamic%20adaptation%20to%20evolving%20goal%20spaces%0Athrough%20generalization.%20In%20an%20interactive%20learning%20environment%2C%20we%20show%20that%0AMAGELLAN%20improves%20LP%20prediction%20efficiency%20and%20goal%20prioritization%2C%20being%20the%0Aonly%20method%20allowing%20the%20agent%20to%20fully%20master%20a%20large%20and%20evolving%20goal%20space.%0AThese%20results%20demonstrate%20how%20augmenting%20LLM%20agents%20with%20a%20metacognitive%0Aability%20for%20LP%20predictions%20can%20effectively%20scale%20curriculum%20learning%20to%0Aopen-ended%20goal%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGELLAN%253A%2520Metacognitive%2520predictions%2520of%2520learning%2520progress%2520guide%2520autotelic%250A%2520%2520LLM%2520agents%2520in%2520large%2520goal%2520spaces%26entry.906535625%3DLoris%2520Gaven%2520and%2520Thomas%2520Carta%2520and%2520Cl%25C3%25A9ment%2520Romac%2520and%2520C%25C3%25A9dric%2520Colas%2520and%2520Sylvain%2520Lamprier%2520and%2520Olivier%2520Sigaud%2520and%2520Pierre-Yves%2520Oudeyer%26entry.1292438233%3D%2520%2520Open-ended%2520learning%2520agents%2520must%2520efficiently%2520prioritize%2520goals%2520in%2520vast%250Apossibility%2520spaces%252C%2520focusing%2520on%2520those%2520that%2520maximize%2520learning%2520progress%2520%2528LP%2529.%250AWhen%2520such%2520autotelic%2520exploration%2520is%2520achieved%2520by%2520LLM%2520agents%2520trained%2520with%2520online%250ARL%2520in%2520high-dimensional%2520and%2520evolving%2520goal%2520spaces%252C%2520a%2520key%2520challenge%2520for%2520LP%250Aprediction%2520is%2520modeling%2520one%2527s%2520own%2520competence%252C%2520a%2520form%2520of%2520metacognitive%250Amonitoring.%2520Traditional%2520approaches%2520either%2520require%2520extensive%2520sampling%2520or%2520rely%2520on%250Abrittle%2520expert-defined%2520goal%2520groupings.%2520We%2520introduce%2520MAGELLAN%252C%2520a%2520metacognitive%250Aframework%2520that%2520lets%2520LLM%2520agents%2520learn%2520to%2520predict%2520their%2520competence%2520and%2520LP%2520online.%250ABy%2520capturing%2520semantic%2520relationships%2520between%2520goals%252C%2520MAGELLAN%2520enables%250Asample-efficient%2520LP%2520estimation%2520and%2520dynamic%2520adaptation%2520to%2520evolving%2520goal%2520spaces%250Athrough%2520generalization.%2520In%2520an%2520interactive%2520learning%2520environment%252C%2520we%2520show%2520that%250AMAGELLAN%2520improves%2520LP%2520prediction%2520efficiency%2520and%2520goal%2520prioritization%252C%2520being%2520the%250Aonly%2520method%2520allowing%2520the%2520agent%2520to%2520fully%2520master%2520a%2520large%2520and%2520evolving%2520goal%2520space.%250AThese%2520results%2520demonstrate%2520how%2520augmenting%2520LLM%2520agents%2520with%2520a%2520metacognitive%250Aability%2520for%2520LP%2520predictions%2520can%2520effectively%2520scale%2520curriculum%2520learning%2520to%250Aopen-ended%2520goal%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGELLAN%3A%20Metacognitive%20predictions%20of%20learning%20progress%20guide%20autotelic%0A%20%20LLM%20agents%20in%20large%20goal%20spaces&entry.906535625=Loris%20Gaven%20and%20Thomas%20Carta%20and%20Cl%C3%A9ment%20Romac%20and%20C%C3%A9dric%20Colas%20and%20Sylvain%20Lamprier%20and%20Olivier%20Sigaud%20and%20Pierre-Yves%20Oudeyer&entry.1292438233=%20%20Open-ended%20learning%20agents%20must%20efficiently%20prioritize%20goals%20in%20vast%0Apossibility%20spaces%2C%20focusing%20on%20those%20that%20maximize%20learning%20progress%20%28LP%29.%0AWhen%20such%20autotelic%20exploration%20is%20achieved%20by%20LLM%20agents%20trained%20with%20online%0ARL%20in%20high-dimensional%20and%20evolving%20goal%20spaces%2C%20a%20key%20challenge%20for%20LP%0Aprediction%20is%20modeling%20one%27s%20own%20competence%2C%20a%20form%20of%20metacognitive%0Amonitoring.%20Traditional%20approaches%20either%20require%20extensive%20sampling%20or%20rely%20on%0Abrittle%20expert-defined%20goal%20groupings.%20We%20introduce%20MAGELLAN%2C%20a%20metacognitive%0Aframework%20that%20lets%20LLM%20agents%20learn%20to%20predict%20their%20competence%20and%20LP%20online.%0ABy%20capturing%20semantic%20relationships%20between%20goals%2C%20MAGELLAN%20enables%0Asample-efficient%20LP%20estimation%20and%20dynamic%20adaptation%20to%20evolving%20goal%20spaces%0Athrough%20generalization.%20In%20an%20interactive%20learning%20environment%2C%20we%20show%20that%0AMAGELLAN%20improves%20LP%20prediction%20efficiency%20and%20goal%20prioritization%2C%20being%20the%0Aonly%20method%20allowing%20the%20agent%20to%20fully%20master%20a%20large%20and%20evolving%20goal%20space.%0AThese%20results%20demonstrate%20how%20augmenting%20LLM%20agents%20with%20a%20metacognitive%0Aability%20for%20LP%20predictions%20can%20effectively%20scale%20curriculum%20learning%20to%0Aopen-ended%20goal%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07709v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


