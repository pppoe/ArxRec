<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250903.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring", "author": "Zhijing Wu and Longguang Wang", "abstract": "  Reconstructing dynamic 3D scenes from monocular video has broad applications\nin AR/VR, robotics, and autonomous navigation, but often fails due to severe\nmotion blur caused by camera and object motion. Existing methods commonly\nfollow a two-step pipeline, where camera poses are first estimated and then 3D\nGaussians are optimized. Since blurring artifacts usually undermine pose\nestimation, pose errors could be accumulated to produce inferior reconstruction\nresults. To address this issue, we introduce a unified optimization framework\nby incorporating camera poses as learnable parameters complementary to 3DGS\nattributes for end-to-end optimization. Specifically, we recast camera and\nobject motion as per-primitive SE(3) affine transformations on 3D Gaussians and\nformulate a unified optimization objective. For stable optimization, we\nintroduce a three-stage training schedule that optimizes camera poses and\nGaussians alternatively. Particularly, 3D Gaussians are first trained with\nposes being fixed, and then poses are optimized with 3D Gaussians being\nuntouched. Finally, all learnable parameters are optimized together. Extensive\nexperiments on the Stereo Blur dataset and challenging real-world sequences\ndemonstrate that our method achieves significant gains in reconstruction\nquality and pose estimation accuracy over prior dynamic deblurring methods.\n", "link": "http://arxiv.org/abs/2509.00831v2", "date": "2025-09-03", "relevancy": 3.4694, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6998}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6961}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UPGS%3A%20Unified%20Pose-aware%20Gaussian%20Splatting%20for%20Dynamic%20Scene%20Deblurring&body=Title%3A%20UPGS%3A%20Unified%20Pose-aware%20Gaussian%20Splatting%20for%20Dynamic%20Scene%20Deblurring%0AAuthor%3A%20Zhijing%20Wu%20and%20Longguang%20Wang%0AAbstract%3A%20%20%20Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20video%20has%20broad%20applications%0Ain%20AR/VR%2C%20robotics%2C%20and%20autonomous%20navigation%2C%20but%20often%20fails%20due%20to%20severe%0Amotion%20blur%20caused%20by%20camera%20and%20object%20motion.%20Existing%20methods%20commonly%0Afollow%20a%20two-step%20pipeline%2C%20where%20camera%20poses%20are%20first%20estimated%20and%20then%203D%0AGaussians%20are%20optimized.%20Since%20blurring%20artifacts%20usually%20undermine%20pose%0Aestimation%2C%20pose%20errors%20could%20be%20accumulated%20to%20produce%20inferior%20reconstruction%0Aresults.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20unified%20optimization%20framework%0Aby%20incorporating%20camera%20poses%20as%20learnable%20parameters%20complementary%20to%203DGS%0Aattributes%20for%20end-to-end%20optimization.%20Specifically%2C%20we%20recast%20camera%20and%0Aobject%20motion%20as%20per-primitive%20SE%283%29%20affine%20transformations%20on%203D%20Gaussians%20and%0Aformulate%20a%20unified%20optimization%20objective.%20For%20stable%20optimization%2C%20we%0Aintroduce%20a%20three-stage%20training%20schedule%20that%20optimizes%20camera%20poses%20and%0AGaussians%20alternatively.%20Particularly%2C%203D%20Gaussians%20are%20first%20trained%20with%0Aposes%20being%20fixed%2C%20and%20then%20poses%20are%20optimized%20with%203D%20Gaussians%20being%0Auntouched.%20Finally%2C%20all%20learnable%20parameters%20are%20optimized%20together.%20Extensive%0Aexperiments%20on%20the%20Stereo%20Blur%20dataset%20and%20challenging%20real-world%20sequences%0Ademonstrate%20that%20our%20method%20achieves%20significant%20gains%20in%20reconstruction%0Aquality%20and%20pose%20estimation%20accuracy%20over%20prior%20dynamic%20deblurring%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUPGS%253A%2520Unified%2520Pose-aware%2520Gaussian%2520Splatting%2520for%2520Dynamic%2520Scene%2520Deblurring%26entry.906535625%3DZhijing%2520Wu%2520and%2520Longguang%2520Wang%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%25203D%2520scenes%2520from%2520monocular%2520video%2520has%2520broad%2520applications%250Ain%2520AR/VR%252C%2520robotics%252C%2520and%2520autonomous%2520navigation%252C%2520but%2520often%2520fails%2520due%2520to%2520severe%250Amotion%2520blur%2520caused%2520by%2520camera%2520and%2520object%2520motion.%2520Existing%2520methods%2520commonly%250Afollow%2520a%2520two-step%2520pipeline%252C%2520where%2520camera%2520poses%2520are%2520first%2520estimated%2520and%2520then%25203D%250AGaussians%2520are%2520optimized.%2520Since%2520blurring%2520artifacts%2520usually%2520undermine%2520pose%250Aestimation%252C%2520pose%2520errors%2520could%2520be%2520accumulated%2520to%2520produce%2520inferior%2520reconstruction%250Aresults.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520unified%2520optimization%2520framework%250Aby%2520incorporating%2520camera%2520poses%2520as%2520learnable%2520parameters%2520complementary%2520to%25203DGS%250Aattributes%2520for%2520end-to-end%2520optimization.%2520Specifically%252C%2520we%2520recast%2520camera%2520and%250Aobject%2520motion%2520as%2520per-primitive%2520SE%25283%2529%2520affine%2520transformations%2520on%25203D%2520Gaussians%2520and%250Aformulate%2520a%2520unified%2520optimization%2520objective.%2520For%2520stable%2520optimization%252C%2520we%250Aintroduce%2520a%2520three-stage%2520training%2520schedule%2520that%2520optimizes%2520camera%2520poses%2520and%250AGaussians%2520alternatively.%2520Particularly%252C%25203D%2520Gaussians%2520are%2520first%2520trained%2520with%250Aposes%2520being%2520fixed%252C%2520and%2520then%2520poses%2520are%2520optimized%2520with%25203D%2520Gaussians%2520being%250Auntouched.%2520Finally%252C%2520all%2520learnable%2520parameters%2520are%2520optimized%2520together.%2520Extensive%250Aexperiments%2520on%2520the%2520Stereo%2520Blur%2520dataset%2520and%2520challenging%2520real-world%2520sequences%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520significant%2520gains%2520in%2520reconstruction%250Aquality%2520and%2520pose%2520estimation%2520accuracy%2520over%2520prior%2520dynamic%2520deblurring%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UPGS%3A%20Unified%20Pose-aware%20Gaussian%20Splatting%20for%20Dynamic%20Scene%20Deblurring&entry.906535625=Zhijing%20Wu%20and%20Longguang%20Wang&entry.1292438233=%20%20Reconstructing%20dynamic%203D%20scenes%20from%20monocular%20video%20has%20broad%20applications%0Ain%20AR/VR%2C%20robotics%2C%20and%20autonomous%20navigation%2C%20but%20often%20fails%20due%20to%20severe%0Amotion%20blur%20caused%20by%20camera%20and%20object%20motion.%20Existing%20methods%20commonly%0Afollow%20a%20two-step%20pipeline%2C%20where%20camera%20poses%20are%20first%20estimated%20and%20then%203D%0AGaussians%20are%20optimized.%20Since%20blurring%20artifacts%20usually%20undermine%20pose%0Aestimation%2C%20pose%20errors%20could%20be%20accumulated%20to%20produce%20inferior%20reconstruction%0Aresults.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20unified%20optimization%20framework%0Aby%20incorporating%20camera%20poses%20as%20learnable%20parameters%20complementary%20to%203DGS%0Aattributes%20for%20end-to-end%20optimization.%20Specifically%2C%20we%20recast%20camera%20and%0Aobject%20motion%20as%20per-primitive%20SE%283%29%20affine%20transformations%20on%203D%20Gaussians%20and%0Aformulate%20a%20unified%20optimization%20objective.%20For%20stable%20optimization%2C%20we%0Aintroduce%20a%20three-stage%20training%20schedule%20that%20optimizes%20camera%20poses%20and%0AGaussians%20alternatively.%20Particularly%2C%203D%20Gaussians%20are%20first%20trained%20with%0Aposes%20being%20fixed%2C%20and%20then%20poses%20are%20optimized%20with%203D%20Gaussians%20being%0Auntouched.%20Finally%2C%20all%20learnable%20parameters%20are%20optimized%20together.%20Extensive%0Aexperiments%20on%20the%20Stereo%20Blur%20dataset%20and%20challenging%20real-world%20sequences%0Ademonstrate%20that%20our%20method%20achieves%20significant%20gains%20in%20reconstruction%0Aquality%20and%20pose%20estimation%20accuracy%20over%20prior%20dynamic%20deblurring%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00831v2&entry.124074799=Read"},
{"title": "Refinement of Monocular Depth Maps via Multi-View Differentiable\n  Rendering", "author": "Laura Fink and Linus Franke and Bernhard Egger and Joachim Keinert and Marc Stamminger", "abstract": "  Accurate depth estimation is at the core of many applications in computer\ngraphics, vision, and robotics. Current state-of-the-art monocular depth\nestimators, trained on extensive datasets, generalize well but lack 3D\nconsistency needed for many applications. In this paper, we combine the\nstrength of those generalizing monocular depth estimation techniques with\nmulti-view data by framing this as an analysis-by-synthesis optimization\nproblem to lift and refine such relative depth maps to accurate error-free\ndepth maps. After an initial global scale estimation through\nstructure-from-motion point clouds, we further refine the depth map through\noptimization enforcing multi-view consistency via photometric and geometric\nlosses with differentiable rendering of the meshed depth map. In a two-stage\noptimization, scaling is further refined first, and afterwards artifacts and\nerrors in the depth map are corrected via nearby-view photometric supervision.\nOur evaluation shows that our method is able to generate detailed,\nhigh-quality, view consistent, accurate depth maps, also in challenging indoor\nscenarios, and outperforms state-of-the-art multi-view depth reconstruction\napproaches on such datasets.\n  Project page and source code can be found at\nhttps://lorafib.github.io/ref_depth/.\n", "link": "http://arxiv.org/abs/2410.03861v2", "date": "2025-09-03", "relevancy": 3.0596, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6148}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6148}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refinement%20of%20Monocular%20Depth%20Maps%20via%20Multi-View%20Differentiable%0A%20%20Rendering&body=Title%3A%20Refinement%20of%20Monocular%20Depth%20Maps%20via%20Multi-View%20Differentiable%0A%20%20Rendering%0AAuthor%3A%20Laura%20Fink%20and%20Linus%20Franke%20and%20Bernhard%20Egger%20and%20Joachim%20Keinert%20and%20Marc%20Stamminger%0AAbstract%3A%20%20%20Accurate%20depth%20estimation%20is%20at%20the%20core%20of%20many%20applications%20in%20computer%0Agraphics%2C%20vision%2C%20and%20robotics.%20Current%20state-of-the-art%20monocular%20depth%0Aestimators%2C%20trained%20on%20extensive%20datasets%2C%20generalize%20well%20but%20lack%203D%0Aconsistency%20needed%20for%20many%20applications.%20In%20this%20paper%2C%20we%20combine%20the%0Astrength%20of%20those%20generalizing%20monocular%20depth%20estimation%20techniques%20with%0Amulti-view%20data%20by%20framing%20this%20as%20an%20analysis-by-synthesis%20optimization%0Aproblem%20to%20lift%20and%20refine%20such%20relative%20depth%20maps%20to%20accurate%20error-free%0Adepth%20maps.%20After%20an%20initial%20global%20scale%20estimation%20through%0Astructure-from-motion%20point%20clouds%2C%20we%20further%20refine%20the%20depth%20map%20through%0Aoptimization%20enforcing%20multi-view%20consistency%20via%20photometric%20and%20geometric%0Alosses%20with%20differentiable%20rendering%20of%20the%20meshed%20depth%20map.%20In%20a%20two-stage%0Aoptimization%2C%20scaling%20is%20further%20refined%20first%2C%20and%20afterwards%20artifacts%20and%0Aerrors%20in%20the%20depth%20map%20are%20corrected%20via%20nearby-view%20photometric%20supervision.%0AOur%20evaluation%20shows%20that%20our%20method%20is%20able%20to%20generate%20detailed%2C%0Ahigh-quality%2C%20view%20consistent%2C%20accurate%20depth%20maps%2C%20also%20in%20challenging%20indoor%0Ascenarios%2C%20and%20outperforms%20state-of-the-art%20multi-view%20depth%20reconstruction%0Aapproaches%20on%20such%20datasets.%0A%20%20Project%20page%20and%20source%20code%20can%20be%20found%20at%0Ahttps%3A//lorafib.github.io/ref_depth/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefinement%2520of%2520Monocular%2520Depth%2520Maps%2520via%2520Multi-View%2520Differentiable%250A%2520%2520Rendering%26entry.906535625%3DLaura%2520Fink%2520and%2520Linus%2520Franke%2520and%2520Bernhard%2520Egger%2520and%2520Joachim%2520Keinert%2520and%2520Marc%2520Stamminger%26entry.1292438233%3D%2520%2520Accurate%2520depth%2520estimation%2520is%2520at%2520the%2520core%2520of%2520many%2520applications%2520in%2520computer%250Agraphics%252C%2520vision%252C%2520and%2520robotics.%2520Current%2520state-of-the-art%2520monocular%2520depth%250Aestimators%252C%2520trained%2520on%2520extensive%2520datasets%252C%2520generalize%2520well%2520but%2520lack%25203D%250Aconsistency%2520needed%2520for%2520many%2520applications.%2520In%2520this%2520paper%252C%2520we%2520combine%2520the%250Astrength%2520of%2520those%2520generalizing%2520monocular%2520depth%2520estimation%2520techniques%2520with%250Amulti-view%2520data%2520by%2520framing%2520this%2520as%2520an%2520analysis-by-synthesis%2520optimization%250Aproblem%2520to%2520lift%2520and%2520refine%2520such%2520relative%2520depth%2520maps%2520to%2520accurate%2520error-free%250Adepth%2520maps.%2520After%2520an%2520initial%2520global%2520scale%2520estimation%2520through%250Astructure-from-motion%2520point%2520clouds%252C%2520we%2520further%2520refine%2520the%2520depth%2520map%2520through%250Aoptimization%2520enforcing%2520multi-view%2520consistency%2520via%2520photometric%2520and%2520geometric%250Alosses%2520with%2520differentiable%2520rendering%2520of%2520the%2520meshed%2520depth%2520map.%2520In%2520a%2520two-stage%250Aoptimization%252C%2520scaling%2520is%2520further%2520refined%2520first%252C%2520and%2520afterwards%2520artifacts%2520and%250Aerrors%2520in%2520the%2520depth%2520map%2520are%2520corrected%2520via%2520nearby-view%2520photometric%2520supervision.%250AOur%2520evaluation%2520shows%2520that%2520our%2520method%2520is%2520able%2520to%2520generate%2520detailed%252C%250Ahigh-quality%252C%2520view%2520consistent%252C%2520accurate%2520depth%2520maps%252C%2520also%2520in%2520challenging%2520indoor%250Ascenarios%252C%2520and%2520outperforms%2520state-of-the-art%2520multi-view%2520depth%2520reconstruction%250Aapproaches%2520on%2520such%2520datasets.%250A%2520%2520Project%2520page%2520and%2520source%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//lorafib.github.io/ref_depth/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refinement%20of%20Monocular%20Depth%20Maps%20via%20Multi-View%20Differentiable%0A%20%20Rendering&entry.906535625=Laura%20Fink%20and%20Linus%20Franke%20and%20Bernhard%20Egger%20and%20Joachim%20Keinert%20and%20Marc%20Stamminger&entry.1292438233=%20%20Accurate%20depth%20estimation%20is%20at%20the%20core%20of%20many%20applications%20in%20computer%0Agraphics%2C%20vision%2C%20and%20robotics.%20Current%20state-of-the-art%20monocular%20depth%0Aestimators%2C%20trained%20on%20extensive%20datasets%2C%20generalize%20well%20but%20lack%203D%0Aconsistency%20needed%20for%20many%20applications.%20In%20this%20paper%2C%20we%20combine%20the%0Astrength%20of%20those%20generalizing%20monocular%20depth%20estimation%20techniques%20with%0Amulti-view%20data%20by%20framing%20this%20as%20an%20analysis-by-synthesis%20optimization%0Aproblem%20to%20lift%20and%20refine%20such%20relative%20depth%20maps%20to%20accurate%20error-free%0Adepth%20maps.%20After%20an%20initial%20global%20scale%20estimation%20through%0Astructure-from-motion%20point%20clouds%2C%20we%20further%20refine%20the%20depth%20map%20through%0Aoptimization%20enforcing%20multi-view%20consistency%20via%20photometric%20and%20geometric%0Alosses%20with%20differentiable%20rendering%20of%20the%20meshed%20depth%20map.%20In%20a%20two-stage%0Aoptimization%2C%20scaling%20is%20further%20refined%20first%2C%20and%20afterwards%20artifacts%20and%0Aerrors%20in%20the%20depth%20map%20are%20corrected%20via%20nearby-view%20photometric%20supervision.%0AOur%20evaluation%20shows%20that%20our%20method%20is%20able%20to%20generate%20detailed%2C%0Ahigh-quality%2C%20view%20consistent%2C%20accurate%20depth%20maps%2C%20also%20in%20challenging%20indoor%0Ascenarios%2C%20and%20outperforms%20state-of-the-art%20multi-view%20depth%20reconstruction%0Aapproaches%20on%20such%20datasets.%0A%20%20Project%20page%20and%20source%20code%20can%20be%20found%20at%0Ahttps%3A//lorafib.github.io/ref_depth/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03861v2&entry.124074799=Read"},
{"title": "Point Cloud Recombination: Systematic Real Data Augmentation Using\n  Robotic Targets for LiDAR Perception Validation", "author": "Hubert Padusinski and Christian Steinhauser and Christian Scherl and Julian Gaal and Jacob Langner", "abstract": "  The validation of LiDAR-based perception of intelligent mobile systems\noperating in open-world applications remains a challenge due to the variability\nof real environmental conditions. Virtual simulations allow the generation of\narbitrary scenes under controlled conditions but lack physical sensor\ncharacteristics, such as intensity responses or material-dependent effects. In\ncontrast, real-world data offers true sensor realism but provides less control\nover influencing factors, hindering sufficient validation. Existing approaches\naddress this problem with augmentation of real-world point cloud data by\ntransferring objects between scenes. However, these methods do not consider\nvalidation and remain limited in controllability because they rely on empirical\ndata. We solve these limitations by proposing Point Cloud Recombination, which\nsystematically augments captured point cloud scenes by integrating point clouds\nacquired from physical target objects measured in controlled laboratory\nenvironments. Thus enabling the creation of vast amounts and varieties of\nrepeatable, physically accurate test scenes with respect to phenomena-aware\nocclusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we\ndemonstrate the augmentation of real-world urban and rural scenes with humanoid\ntargets featuring varied clothing and poses, for repeatable positioning. We\nshow that the recombined scenes closely match real sensor outputs, enabling\ntargeted testing, scalable failure analysis, and improved system safety. By\nproviding controlled yet sensor-realistic data, our method enables trustworthy\nconclusions about the limitations of specific sensors in compound with their\nalgorithms, e.g., object detection.\n", "link": "http://arxiv.org/abs/2505.02476v2", "date": "2025-09-03", "relevancy": 2.9647, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6079}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5943}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Cloud%20Recombination%3A%20Systematic%20Real%20Data%20Augmentation%20Using%0A%20%20Robotic%20Targets%20for%20LiDAR%20Perception%20Validation&body=Title%3A%20Point%20Cloud%20Recombination%3A%20Systematic%20Real%20Data%20Augmentation%20Using%0A%20%20Robotic%20Targets%20for%20LiDAR%20Perception%20Validation%0AAuthor%3A%20Hubert%20Padusinski%20and%20Christian%20Steinhauser%20and%20Christian%20Scherl%20and%20Julian%20Gaal%20and%20Jacob%20Langner%0AAbstract%3A%20%20%20The%20validation%20of%20LiDAR-based%20perception%20of%20intelligent%20mobile%20systems%0Aoperating%20in%20open-world%20applications%20remains%20a%20challenge%20due%20to%20the%20variability%0Aof%20real%20environmental%20conditions.%20Virtual%20simulations%20allow%20the%20generation%20of%0Aarbitrary%20scenes%20under%20controlled%20conditions%20but%20lack%20physical%20sensor%0Acharacteristics%2C%20such%20as%20intensity%20responses%20or%20material-dependent%20effects.%20In%0Acontrast%2C%20real-world%20data%20offers%20true%20sensor%20realism%20but%20provides%20less%20control%0Aover%20influencing%20factors%2C%20hindering%20sufficient%20validation.%20Existing%20approaches%0Aaddress%20this%20problem%20with%20augmentation%20of%20real-world%20point%20cloud%20data%20by%0Atransferring%20objects%20between%20scenes.%20However%2C%20these%20methods%20do%20not%20consider%0Avalidation%20and%20remain%20limited%20in%20controllability%20because%20they%20rely%20on%20empirical%0Adata.%20We%20solve%20these%20limitations%20by%20proposing%20Point%20Cloud%20Recombination%2C%20which%0Asystematically%20augments%20captured%20point%20cloud%20scenes%20by%20integrating%20point%20clouds%0Aacquired%20from%20physical%20target%20objects%20measured%20in%20controlled%20laboratory%0Aenvironments.%20Thus%20enabling%20the%20creation%20of%20vast%20amounts%20and%20varieties%20of%0Arepeatable%2C%20physically%20accurate%20test%20scenes%20with%20respect%20to%20phenomena-aware%0Aocclusions%20with%20registered%203D%20meshes.%20Using%20the%20Ouster%20OS1-128%20Rev7%20sensor%2C%20we%0Ademonstrate%20the%20augmentation%20of%20real-world%20urban%20and%20rural%20scenes%20with%20humanoid%0Atargets%20featuring%20varied%20clothing%20and%20poses%2C%20for%20repeatable%20positioning.%20We%0Ashow%20that%20the%20recombined%20scenes%20closely%20match%20real%20sensor%20outputs%2C%20enabling%0Atargeted%20testing%2C%20scalable%20failure%20analysis%2C%20and%20improved%20system%20safety.%20By%0Aproviding%20controlled%20yet%20sensor-realistic%20data%2C%20our%20method%20enables%20trustworthy%0Aconclusions%20about%20the%20limitations%20of%20specific%20sensors%20in%20compound%20with%20their%0Aalgorithms%2C%20e.g.%2C%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Cloud%2520Recombination%253A%2520Systematic%2520Real%2520Data%2520Augmentation%2520Using%250A%2520%2520Robotic%2520Targets%2520for%2520LiDAR%2520Perception%2520Validation%26entry.906535625%3DHubert%2520Padusinski%2520and%2520Christian%2520Steinhauser%2520and%2520Christian%2520Scherl%2520and%2520Julian%2520Gaal%2520and%2520Jacob%2520Langner%26entry.1292438233%3D%2520%2520The%2520validation%2520of%2520LiDAR-based%2520perception%2520of%2520intelligent%2520mobile%2520systems%250Aoperating%2520in%2520open-world%2520applications%2520remains%2520a%2520challenge%2520due%2520to%2520the%2520variability%250Aof%2520real%2520environmental%2520conditions.%2520Virtual%2520simulations%2520allow%2520the%2520generation%2520of%250Aarbitrary%2520scenes%2520under%2520controlled%2520conditions%2520but%2520lack%2520physical%2520sensor%250Acharacteristics%252C%2520such%2520as%2520intensity%2520responses%2520or%2520material-dependent%2520effects.%2520In%250Acontrast%252C%2520real-world%2520data%2520offers%2520true%2520sensor%2520realism%2520but%2520provides%2520less%2520control%250Aover%2520influencing%2520factors%252C%2520hindering%2520sufficient%2520validation.%2520Existing%2520approaches%250Aaddress%2520this%2520problem%2520with%2520augmentation%2520of%2520real-world%2520point%2520cloud%2520data%2520by%250Atransferring%2520objects%2520between%2520scenes.%2520However%252C%2520these%2520methods%2520do%2520not%2520consider%250Avalidation%2520and%2520remain%2520limited%2520in%2520controllability%2520because%2520they%2520rely%2520on%2520empirical%250Adata.%2520We%2520solve%2520these%2520limitations%2520by%2520proposing%2520Point%2520Cloud%2520Recombination%252C%2520which%250Asystematically%2520augments%2520captured%2520point%2520cloud%2520scenes%2520by%2520integrating%2520point%2520clouds%250Aacquired%2520from%2520physical%2520target%2520objects%2520measured%2520in%2520controlled%2520laboratory%250Aenvironments.%2520Thus%2520enabling%2520the%2520creation%2520of%2520vast%2520amounts%2520and%2520varieties%2520of%250Arepeatable%252C%2520physically%2520accurate%2520test%2520scenes%2520with%2520respect%2520to%2520phenomena-aware%250Aocclusions%2520with%2520registered%25203D%2520meshes.%2520Using%2520the%2520Ouster%2520OS1-128%2520Rev7%2520sensor%252C%2520we%250Ademonstrate%2520the%2520augmentation%2520of%2520real-world%2520urban%2520and%2520rural%2520scenes%2520with%2520humanoid%250Atargets%2520featuring%2520varied%2520clothing%2520and%2520poses%252C%2520for%2520repeatable%2520positioning.%2520We%250Ashow%2520that%2520the%2520recombined%2520scenes%2520closely%2520match%2520real%2520sensor%2520outputs%252C%2520enabling%250Atargeted%2520testing%252C%2520scalable%2520failure%2520analysis%252C%2520and%2520improved%2520system%2520safety.%2520By%250Aproviding%2520controlled%2520yet%2520sensor-realistic%2520data%252C%2520our%2520method%2520enables%2520trustworthy%250Aconclusions%2520about%2520the%2520limitations%2520of%2520specific%2520sensors%2520in%2520compound%2520with%2520their%250Aalgorithms%252C%2520e.g.%252C%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Cloud%20Recombination%3A%20Systematic%20Real%20Data%20Augmentation%20Using%0A%20%20Robotic%20Targets%20for%20LiDAR%20Perception%20Validation&entry.906535625=Hubert%20Padusinski%20and%20Christian%20Steinhauser%20and%20Christian%20Scherl%20and%20Julian%20Gaal%20and%20Jacob%20Langner&entry.1292438233=%20%20The%20validation%20of%20LiDAR-based%20perception%20of%20intelligent%20mobile%20systems%0Aoperating%20in%20open-world%20applications%20remains%20a%20challenge%20due%20to%20the%20variability%0Aof%20real%20environmental%20conditions.%20Virtual%20simulations%20allow%20the%20generation%20of%0Aarbitrary%20scenes%20under%20controlled%20conditions%20but%20lack%20physical%20sensor%0Acharacteristics%2C%20such%20as%20intensity%20responses%20or%20material-dependent%20effects.%20In%0Acontrast%2C%20real-world%20data%20offers%20true%20sensor%20realism%20but%20provides%20less%20control%0Aover%20influencing%20factors%2C%20hindering%20sufficient%20validation.%20Existing%20approaches%0Aaddress%20this%20problem%20with%20augmentation%20of%20real-world%20point%20cloud%20data%20by%0Atransferring%20objects%20between%20scenes.%20However%2C%20these%20methods%20do%20not%20consider%0Avalidation%20and%20remain%20limited%20in%20controllability%20because%20they%20rely%20on%20empirical%0Adata.%20We%20solve%20these%20limitations%20by%20proposing%20Point%20Cloud%20Recombination%2C%20which%0Asystematically%20augments%20captured%20point%20cloud%20scenes%20by%20integrating%20point%20clouds%0Aacquired%20from%20physical%20target%20objects%20measured%20in%20controlled%20laboratory%0Aenvironments.%20Thus%20enabling%20the%20creation%20of%20vast%20amounts%20and%20varieties%20of%0Arepeatable%2C%20physically%20accurate%20test%20scenes%20with%20respect%20to%20phenomena-aware%0Aocclusions%20with%20registered%203D%20meshes.%20Using%20the%20Ouster%20OS1-128%20Rev7%20sensor%2C%20we%0Ademonstrate%20the%20augmentation%20of%20real-world%20urban%20and%20rural%20scenes%20with%20humanoid%0Atargets%20featuring%20varied%20clothing%20and%20poses%2C%20for%20repeatable%20positioning.%20We%0Ashow%20that%20the%20recombined%20scenes%20closely%20match%20real%20sensor%20outputs%2C%20enabling%0Atargeted%20testing%2C%20scalable%20failure%20analysis%2C%20and%20improved%20system%20safety.%20By%0Aproviding%20controlled%20yet%20sensor-realistic%20data%2C%20our%20method%20enables%20trustworthy%0Aconclusions%20about%20the%20limitations%20of%20specific%20sensors%20in%20compound%20with%20their%0Aalgorithms%2C%20e.g.%2C%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02476v2&entry.124074799=Read"},
{"title": "SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D\n  Diffusion Model", "author": "Hongxu Yang and Edina Timko and Levente Lippenszky and Vanda Czipczer and Lehel Ferenczi", "abstract": "  Synthetic tumors in medical images offer controllable characteristics that\nfacilitate the training of machine learning models, leading to an improved\nsegmentation performance. However, the existing methods of tumor synthesis\nyield suboptimal performances when tumor occupies a large spatial volume, such\nas breast tumor segmentation in MRI with a large field-of-view (FOV), while\ncommonly used tumor generation methods are based on small patches. In this\npaper, we propose a 3D medical diffusion model, called SynBT, to generate\nhigh-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed\nmodel consists of a patch-to-volume autoencoder, which is able to compress the\nhigh-resolution MRIs into compact latent space, while preserving the resolution\nof volumes with large FOV. Using the obtained latent space feature vector, a\nmask-conditioned diffusion model is used to synthesize breast tumors within\nselected regions of breast tissue, resulting in realistic tumor appearances. We\nevaluated the proposed method for a tumor segmentation task, which demonstrated\nthe proposed high-quality tumor synthesis method can facilitate the common\nsegmentation models with performance improvement of 2-3% Dice Score on a large\npublic dataset, and therefore provides benefits for tumor segmentation in MRI\nimages.\n", "link": "http://arxiv.org/abs/2509.03267v1", "date": "2025-09-03", "relevancy": 2.8348, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5707}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5707}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynBT%3A%20High-quality%20Tumor%20Synthesis%20for%20Breast%20Tumor%20Segmentation%20by%203D%0A%20%20Diffusion%20Model&body=Title%3A%20SynBT%3A%20High-quality%20Tumor%20Synthesis%20for%20Breast%20Tumor%20Segmentation%20by%203D%0A%20%20Diffusion%20Model%0AAuthor%3A%20Hongxu%20Yang%20and%20Edina%20Timko%20and%20Levente%20Lippenszky%20and%20Vanda%20Czipczer%20and%20Lehel%20Ferenczi%0AAbstract%3A%20%20%20Synthetic%20tumors%20in%20medical%20images%20offer%20controllable%20characteristics%20that%0Afacilitate%20the%20training%20of%20machine%20learning%20models%2C%20leading%20to%20an%20improved%0Asegmentation%20performance.%20However%2C%20the%20existing%20methods%20of%20tumor%20synthesis%0Ayield%20suboptimal%20performances%20when%20tumor%20occupies%20a%20large%20spatial%20volume%2C%20such%0Aas%20breast%20tumor%20segmentation%20in%20MRI%20with%20a%20large%20field-of-view%20%28FOV%29%2C%20while%0Acommonly%20used%20tumor%20generation%20methods%20are%20based%20on%20small%20patches.%20In%20this%0Apaper%2C%20we%20propose%20a%203D%20medical%20diffusion%20model%2C%20called%20SynBT%2C%20to%20generate%0Ahigh-quality%20breast%20tumor%20%28BT%29%20in%20contrast-enhanced%20MRI%20images.%20The%20proposed%0Amodel%20consists%20of%20a%20patch-to-volume%20autoencoder%2C%20which%20is%20able%20to%20compress%20the%0Ahigh-resolution%20MRIs%20into%20compact%20latent%20space%2C%20while%20preserving%20the%20resolution%0Aof%20volumes%20with%20large%20FOV.%20Using%20the%20obtained%20latent%20space%20feature%20vector%2C%20a%0Amask-conditioned%20diffusion%20model%20is%20used%20to%20synthesize%20breast%20tumors%20within%0Aselected%20regions%20of%20breast%20tissue%2C%20resulting%20in%20realistic%20tumor%20appearances.%20We%0Aevaluated%20the%20proposed%20method%20for%20a%20tumor%20segmentation%20task%2C%20which%20demonstrated%0Athe%20proposed%20high-quality%20tumor%20synthesis%20method%20can%20facilitate%20the%20common%0Asegmentation%20models%20with%20performance%20improvement%20of%202-3%25%20Dice%20Score%20on%20a%20large%0Apublic%20dataset%2C%20and%20therefore%20provides%20benefits%20for%20tumor%20segmentation%20in%20MRI%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynBT%253A%2520High-quality%2520Tumor%2520Synthesis%2520for%2520Breast%2520Tumor%2520Segmentation%2520by%25203D%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DHongxu%2520Yang%2520and%2520Edina%2520Timko%2520and%2520Levente%2520Lippenszky%2520and%2520Vanda%2520Czipczer%2520and%2520Lehel%2520Ferenczi%26entry.1292438233%3D%2520%2520Synthetic%2520tumors%2520in%2520medical%2520images%2520offer%2520controllable%2520characteristics%2520that%250Afacilitate%2520the%2520training%2520of%2520machine%2520learning%2520models%252C%2520leading%2520to%2520an%2520improved%250Asegmentation%2520performance.%2520However%252C%2520the%2520existing%2520methods%2520of%2520tumor%2520synthesis%250Ayield%2520suboptimal%2520performances%2520when%2520tumor%2520occupies%2520a%2520large%2520spatial%2520volume%252C%2520such%250Aas%2520breast%2520tumor%2520segmentation%2520in%2520MRI%2520with%2520a%2520large%2520field-of-view%2520%2528FOV%2529%252C%2520while%250Acommonly%2520used%2520tumor%2520generation%2520methods%2520are%2520based%2520on%2520small%2520patches.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%25203D%2520medical%2520diffusion%2520model%252C%2520called%2520SynBT%252C%2520to%2520generate%250Ahigh-quality%2520breast%2520tumor%2520%2528BT%2529%2520in%2520contrast-enhanced%2520MRI%2520images.%2520The%2520proposed%250Amodel%2520consists%2520of%2520a%2520patch-to-volume%2520autoencoder%252C%2520which%2520is%2520able%2520to%2520compress%2520the%250Ahigh-resolution%2520MRIs%2520into%2520compact%2520latent%2520space%252C%2520while%2520preserving%2520the%2520resolution%250Aof%2520volumes%2520with%2520large%2520FOV.%2520Using%2520the%2520obtained%2520latent%2520space%2520feature%2520vector%252C%2520a%250Amask-conditioned%2520diffusion%2520model%2520is%2520used%2520to%2520synthesize%2520breast%2520tumors%2520within%250Aselected%2520regions%2520of%2520breast%2520tissue%252C%2520resulting%2520in%2520realistic%2520tumor%2520appearances.%2520We%250Aevaluated%2520the%2520proposed%2520method%2520for%2520a%2520tumor%2520segmentation%2520task%252C%2520which%2520demonstrated%250Athe%2520proposed%2520high-quality%2520tumor%2520synthesis%2520method%2520can%2520facilitate%2520the%2520common%250Asegmentation%2520models%2520with%2520performance%2520improvement%2520of%25202-3%2525%2520Dice%2520Score%2520on%2520a%2520large%250Apublic%2520dataset%252C%2520and%2520therefore%2520provides%2520benefits%2520for%2520tumor%2520segmentation%2520in%2520MRI%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynBT%3A%20High-quality%20Tumor%20Synthesis%20for%20Breast%20Tumor%20Segmentation%20by%203D%0A%20%20Diffusion%20Model&entry.906535625=Hongxu%20Yang%20and%20Edina%20Timko%20and%20Levente%20Lippenszky%20and%20Vanda%20Czipczer%20and%20Lehel%20Ferenczi&entry.1292438233=%20%20Synthetic%20tumors%20in%20medical%20images%20offer%20controllable%20characteristics%20that%0Afacilitate%20the%20training%20of%20machine%20learning%20models%2C%20leading%20to%20an%20improved%0Asegmentation%20performance.%20However%2C%20the%20existing%20methods%20of%20tumor%20synthesis%0Ayield%20suboptimal%20performances%20when%20tumor%20occupies%20a%20large%20spatial%20volume%2C%20such%0Aas%20breast%20tumor%20segmentation%20in%20MRI%20with%20a%20large%20field-of-view%20%28FOV%29%2C%20while%0Acommonly%20used%20tumor%20generation%20methods%20are%20based%20on%20small%20patches.%20In%20this%0Apaper%2C%20we%20propose%20a%203D%20medical%20diffusion%20model%2C%20called%20SynBT%2C%20to%20generate%0Ahigh-quality%20breast%20tumor%20%28BT%29%20in%20contrast-enhanced%20MRI%20images.%20The%20proposed%0Amodel%20consists%20of%20a%20patch-to-volume%20autoencoder%2C%20which%20is%20able%20to%20compress%20the%0Ahigh-resolution%20MRIs%20into%20compact%20latent%20space%2C%20while%20preserving%20the%20resolution%0Aof%20volumes%20with%20large%20FOV.%20Using%20the%20obtained%20latent%20space%20feature%20vector%2C%20a%0Amask-conditioned%20diffusion%20model%20is%20used%20to%20synthesize%20breast%20tumors%20within%0Aselected%20regions%20of%20breast%20tissue%2C%20resulting%20in%20realistic%20tumor%20appearances.%20We%0Aevaluated%20the%20proposed%20method%20for%20a%20tumor%20segmentation%20task%2C%20which%20demonstrated%0Athe%20proposed%20high-quality%20tumor%20synthesis%20method%20can%20facilitate%20the%20common%0Asegmentation%20models%20with%20performance%20improvement%20of%202-3%25%20Dice%20Score%20on%20a%20large%0Apublic%20dataset%2C%20and%20therefore%20provides%20benefits%20for%20tumor%20segmentation%20in%20MRI%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03267v1&entry.124074799=Read"},
{"title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation", "author": "Han Li and Xinyu Peng and Yaoming Wang and Zelin Peng and Xin Chen and Rongxiang Weng and Jingang Wang and Xunliang Cai and Wenrui Dai and Hongkai Xiong", "abstract": "  We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.\n", "link": "http://arxiv.org/abs/2509.03498v1", "date": "2025-09-03", "relevancy": 2.8213, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.565}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneCAT%3A%20Decoder-Only%20Auto-Regressive%20Model%20for%20Unified%20Understanding%20and%0A%20%20Generation&body=Title%3A%20OneCAT%3A%20Decoder-Only%20Auto-Regressive%20Model%20for%20Unified%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Han%20Li%20and%20Xinyu%20Peng%20and%20Yaoming%20Wang%20and%20Zelin%20Peng%20and%20Xin%20Chen%20and%20Rongxiang%20Weng%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Wenrui%20Dai%20and%20Hongkai%20Xiong%0AAbstract%3A%20%20%20We%20introduce%20OneCAT%2C%20a%20unified%20multimodal%20model%20that%20seamlessly%20integrates%0Aunderstanding%2C%20generation%2C%20and%20editing%20within%20a%20novel%2C%20pure%20decoder-only%0Atransformer%20architecture.%20Our%20framework%20uniquely%20eliminates%20the%20need%20for%0Aexternal%20components%20such%20as%20Vision%20Transformers%20%28ViT%29%20or%20vision%20tokenizer%0Aduring%20inference%2C%20leading%20to%20significant%20efficiency%20gains%2C%20especially%20for%0Ahigh-resolution%20inputs.%20This%20is%20achieved%20through%20a%20modality-specific%0AMixture-of-Experts%20%28MoE%29%20structure%20trained%20with%20a%20single%20autoregressive%20%28AR%29%0Aobjective%2C%20which%20also%20natively%20supports%20dynamic%20resolutions.%20Furthermore%2C%20we%0Apioneer%20a%20multi-scale%20visual%20autoregressive%20mechanism%20within%20the%20Large%20Language%0AModel%20%28LLM%29%20that%20drastically%20reduces%20decoding%20steps%20compared%20to%20diffusion-based%0Amethods%20while%20maintaining%20state-of-the-art%20performance.%20Our%20findings%0Ademonstrate%20the%20powerful%20potential%20of%20pure%20autoregressive%20modeling%20as%20a%0Asufficient%20and%20elegant%20foundation%20for%20unified%20multimodal%20intelligence.%20As%20a%0Aresult%2C%20OneCAT%20sets%20a%20new%20performance%20standard%2C%20outperforming%20existing%0Aopen-source%20unified%20multimodal%20models%20across%20benchmarks%20for%20multimodal%0Ageneration%2C%20editing%2C%20and%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneCAT%253A%2520Decoder-Only%2520Auto-Regressive%2520Model%2520for%2520Unified%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DHan%2520Li%2520and%2520Xinyu%2520Peng%2520and%2520Yaoming%2520Wang%2520and%2520Zelin%2520Peng%2520and%2520Xin%2520Chen%2520and%2520Rongxiang%2520Weng%2520and%2520Jingang%2520Wang%2520and%2520Xunliang%2520Cai%2520and%2520Wenrui%2520Dai%2520and%2520Hongkai%2520Xiong%26entry.1292438233%3D%2520%2520We%2520introduce%2520OneCAT%252C%2520a%2520unified%2520multimodal%2520model%2520that%2520seamlessly%2520integrates%250Aunderstanding%252C%2520generation%252C%2520and%2520editing%2520within%2520a%2520novel%252C%2520pure%2520decoder-only%250Atransformer%2520architecture.%2520Our%2520framework%2520uniquely%2520eliminates%2520the%2520need%2520for%250Aexternal%2520components%2520such%2520as%2520Vision%2520Transformers%2520%2528ViT%2529%2520or%2520vision%2520tokenizer%250Aduring%2520inference%252C%2520leading%2520to%2520significant%2520efficiency%2520gains%252C%2520especially%2520for%250Ahigh-resolution%2520inputs.%2520This%2520is%2520achieved%2520through%2520a%2520modality-specific%250AMixture-of-Experts%2520%2528MoE%2529%2520structure%2520trained%2520with%2520a%2520single%2520autoregressive%2520%2528AR%2529%250Aobjective%252C%2520which%2520also%2520natively%2520supports%2520dynamic%2520resolutions.%2520Furthermore%252C%2520we%250Apioneer%2520a%2520multi-scale%2520visual%2520autoregressive%2520mechanism%2520within%2520the%2520Large%2520Language%250AModel%2520%2528LLM%2529%2520that%2520drastically%2520reduces%2520decoding%2520steps%2520compared%2520to%2520diffusion-based%250Amethods%2520while%2520maintaining%2520state-of-the-art%2520performance.%2520Our%2520findings%250Ademonstrate%2520the%2520powerful%2520potential%2520of%2520pure%2520autoregressive%2520modeling%2520as%2520a%250Asufficient%2520and%2520elegant%2520foundation%2520for%2520unified%2520multimodal%2520intelligence.%2520As%2520a%250Aresult%252C%2520OneCAT%2520sets%2520a%2520new%2520performance%2520standard%252C%2520outperforming%2520existing%250Aopen-source%2520unified%2520multimodal%2520models%2520across%2520benchmarks%2520for%2520multimodal%250Ageneration%252C%2520editing%252C%2520and%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneCAT%3A%20Decoder-Only%20Auto-Regressive%20Model%20for%20Unified%20Understanding%20and%0A%20%20Generation&entry.906535625=Han%20Li%20and%20Xinyu%20Peng%20and%20Yaoming%20Wang%20and%20Zelin%20Peng%20and%20Xin%20Chen%20and%20Rongxiang%20Weng%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Wenrui%20Dai%20and%20Hongkai%20Xiong&entry.1292438233=%20%20We%20introduce%20OneCAT%2C%20a%20unified%20multimodal%20model%20that%20seamlessly%20integrates%0Aunderstanding%2C%20generation%2C%20and%20editing%20within%20a%20novel%2C%20pure%20decoder-only%0Atransformer%20architecture.%20Our%20framework%20uniquely%20eliminates%20the%20need%20for%0Aexternal%20components%20such%20as%20Vision%20Transformers%20%28ViT%29%20or%20vision%20tokenizer%0Aduring%20inference%2C%20leading%20to%20significant%20efficiency%20gains%2C%20especially%20for%0Ahigh-resolution%20inputs.%20This%20is%20achieved%20through%20a%20modality-specific%0AMixture-of-Experts%20%28MoE%29%20structure%20trained%20with%20a%20single%20autoregressive%20%28AR%29%0Aobjective%2C%20which%20also%20natively%20supports%20dynamic%20resolutions.%20Furthermore%2C%20we%0Apioneer%20a%20multi-scale%20visual%20autoregressive%20mechanism%20within%20the%20Large%20Language%0AModel%20%28LLM%29%20that%20drastically%20reduces%20decoding%20steps%20compared%20to%20diffusion-based%0Amethods%20while%20maintaining%20state-of-the-art%20performance.%20Our%20findings%0Ademonstrate%20the%20powerful%20potential%20of%20pure%20autoregressive%20modeling%20as%20a%0Asufficient%20and%20elegant%20foundation%20for%20unified%20multimodal%20intelligence.%20As%20a%0Aresult%2C%20OneCAT%20sets%20a%20new%20performance%20standard%2C%20outperforming%20existing%0Aopen-source%20unified%20multimodal%20models%20across%20benchmarks%20for%20multimodal%0Ageneration%2C%20editing%2C%20and%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03498v1&entry.124074799=Read"},
{"title": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist\n  Intelligence", "author": "Xingxuan Zhang and Gang Ren and Han Yu and Hao Yuan and Hui Wang and Jiansheng Li and Jiayun Wu and Lang Mo and Li Mao and Mingchao Hao and Ningbo Dai and Renzhe Xu and Shuyang Li and Tianyang Zhang and Yue He and Yuanrui Wang and Yunjia Zhang and Zijing Xu and Dongzhe Li and Fang Gao and Hao Zou and Jiandong Liu and Jiashuo Liu and Jiawei Xu and Kaijie Cheng and Kehan Li and Linjun Zhou and Qing Li and Shaohua Fan and Xiaoyu Lin and Xinyan Han and Xuanyue Li and Yan Lu and Yuan Xue and Yuanyuan Jiang and Zimu Wang and Zhenlei Wang and Peng Cui", "abstract": "  We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX, the first installment of our large\nstructured-data models (LDMs). LimiX treats structured data as a joint\ndistribution over variables and missingness, thus capable of addressing a wide\nrange of tabular tasks through query-based conditional prediction via a single\nmodel. LimiX is pretrained using masked joint-distribution modeling with an\nepisodic, context-conditional objective, where the model predicts for query\nsubsets conditioned on dataset-specific contexts, supporting rapid,\ntraining-free adaptation at inference. We evaluate LimiX across 10 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. With a single model and a unified\ninterface, LimiX consistently surpasses strong baselines including\ngradient-boosting trees, deep tabular networks, recent tabular foundation\nmodels, and automated ensembles, as shown in Figure 1 and Figure 2. The\nsuperiority holds across a wide range of tasks, such as classification,\nregression, missing value imputation, and data generation, often by substantial\nmargins, while avoiding task-specific architectures or bespoke training per\ntask. All LimiX models are publicly accessible under Apache 2.0.\n", "link": "http://arxiv.org/abs/2509.03505v1", "date": "2025-09-03", "relevancy": 2.816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LimiX%3A%20Unleashing%20Structured-Data%20Modeling%20Capability%20for%20Generalist%0A%20%20Intelligence&body=Title%3A%20LimiX%3A%20Unleashing%20Structured-Data%20Modeling%20Capability%20for%20Generalist%0A%20%20Intelligence%0AAuthor%3A%20Xingxuan%20Zhang%20and%20Gang%20Ren%20and%20Han%20Yu%20and%20Hao%20Yuan%20and%20Hui%20Wang%20and%20Jiansheng%20Li%20and%20Jiayun%20Wu%20and%20Lang%20Mo%20and%20Li%20Mao%20and%20Mingchao%20Hao%20and%20Ningbo%20Dai%20and%20Renzhe%20Xu%20and%20Shuyang%20Li%20and%20Tianyang%20Zhang%20and%20Yue%20He%20and%20Yuanrui%20Wang%20and%20Yunjia%20Zhang%20and%20Zijing%20Xu%20and%20Dongzhe%20Li%20and%20Fang%20Gao%20and%20Hao%20Zou%20and%20Jiandong%20Liu%20and%20Jiashuo%20Liu%20and%20Jiawei%20Xu%20and%20Kaijie%20Cheng%20and%20Kehan%20Li%20and%20Linjun%20Zhou%20and%20Qing%20Li%20and%20Shaohua%20Fan%20and%20Xiaoyu%20Lin%20and%20Xinyan%20Han%20and%20Xuanyue%20Li%20and%20Yan%20Lu%20and%20Yuan%20Xue%20and%20Yuanyuan%20Jiang%20and%20Zimu%20Wang%20and%20Zhenlei%20Wang%20and%20Peng%20Cui%0AAbstract%3A%20%20%20We%20argue%20that%20progress%20toward%20general%20intelligence%20requires%20complementary%0Afoundation%20models%20grounded%20in%20language%2C%20the%20physical%20world%2C%20and%20structured%0Adata.%20This%20report%20presents%20LimiX%2C%20the%20first%20installment%20of%20our%20large%0Astructured-data%20models%20%28LDMs%29.%20LimiX%20treats%20structured%20data%20as%20a%20joint%0Adistribution%20over%20variables%20and%20missingness%2C%20thus%20capable%20of%20addressing%20a%20wide%0Arange%20of%20tabular%20tasks%20through%20query-based%20conditional%20prediction%20via%20a%20single%0Amodel.%20LimiX%20is%20pretrained%20using%20masked%20joint-distribution%20modeling%20with%20an%0Aepisodic%2C%20context-conditional%20objective%2C%20where%20the%20model%20predicts%20for%20query%0Asubsets%20conditioned%20on%20dataset-specific%20contexts%2C%20supporting%20rapid%2C%0Atraining-free%20adaptation%20at%20inference.%20We%20evaluate%20LimiX%20across%2010%20large%0Astructured-data%20benchmarks%20with%20broad%20regimes%20of%20sample%20size%2C%20feature%0Adimensionality%2C%20class%20number%2C%20categorical-to-numerical%20feature%20ratio%2C%0Amissingness%2C%20and%20sample-to-feature%20ratios.%20With%20a%20single%20model%20and%20a%20unified%0Ainterface%2C%20LimiX%20consistently%20surpasses%20strong%20baselines%20including%0Agradient-boosting%20trees%2C%20deep%20tabular%20networks%2C%20recent%20tabular%20foundation%0Amodels%2C%20and%20automated%20ensembles%2C%20as%20shown%20in%20Figure%201%20and%20Figure%202.%20The%0Asuperiority%20holds%20across%20a%20wide%20range%20of%20tasks%2C%20such%20as%20classification%2C%0Aregression%2C%20missing%20value%20imputation%2C%20and%20data%20generation%2C%20often%20by%20substantial%0Amargins%2C%20while%20avoiding%20task-specific%20architectures%20or%20bespoke%20training%20per%0Atask.%20All%20LimiX%20models%20are%20publicly%20accessible%20under%20Apache%202.0.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimiX%253A%2520Unleashing%2520Structured-Data%2520Modeling%2520Capability%2520for%2520Generalist%250A%2520%2520Intelligence%26entry.906535625%3DXingxuan%2520Zhang%2520and%2520Gang%2520Ren%2520and%2520Han%2520Yu%2520and%2520Hao%2520Yuan%2520and%2520Hui%2520Wang%2520and%2520Jiansheng%2520Li%2520and%2520Jiayun%2520Wu%2520and%2520Lang%2520Mo%2520and%2520Li%2520Mao%2520and%2520Mingchao%2520Hao%2520and%2520Ningbo%2520Dai%2520and%2520Renzhe%2520Xu%2520and%2520Shuyang%2520Li%2520and%2520Tianyang%2520Zhang%2520and%2520Yue%2520He%2520and%2520Yuanrui%2520Wang%2520and%2520Yunjia%2520Zhang%2520and%2520Zijing%2520Xu%2520and%2520Dongzhe%2520Li%2520and%2520Fang%2520Gao%2520and%2520Hao%2520Zou%2520and%2520Jiandong%2520Liu%2520and%2520Jiashuo%2520Liu%2520and%2520Jiawei%2520Xu%2520and%2520Kaijie%2520Cheng%2520and%2520Kehan%2520Li%2520and%2520Linjun%2520Zhou%2520and%2520Qing%2520Li%2520and%2520Shaohua%2520Fan%2520and%2520Xiaoyu%2520Lin%2520and%2520Xinyan%2520Han%2520and%2520Xuanyue%2520Li%2520and%2520Yan%2520Lu%2520and%2520Yuan%2520Xue%2520and%2520Yuanyuan%2520Jiang%2520and%2520Zimu%2520Wang%2520and%2520Zhenlei%2520Wang%2520and%2520Peng%2520Cui%26entry.1292438233%3D%2520%2520We%2520argue%2520that%2520progress%2520toward%2520general%2520intelligence%2520requires%2520complementary%250Afoundation%2520models%2520grounded%2520in%2520language%252C%2520the%2520physical%2520world%252C%2520and%2520structured%250Adata.%2520This%2520report%2520presents%2520LimiX%252C%2520the%2520first%2520installment%2520of%2520our%2520large%250Astructured-data%2520models%2520%2528LDMs%2529.%2520LimiX%2520treats%2520structured%2520data%2520as%2520a%2520joint%250Adistribution%2520over%2520variables%2520and%2520missingness%252C%2520thus%2520capable%2520of%2520addressing%2520a%2520wide%250Arange%2520of%2520tabular%2520tasks%2520through%2520query-based%2520conditional%2520prediction%2520via%2520a%2520single%250Amodel.%2520LimiX%2520is%2520pretrained%2520using%2520masked%2520joint-distribution%2520modeling%2520with%2520an%250Aepisodic%252C%2520context-conditional%2520objective%252C%2520where%2520the%2520model%2520predicts%2520for%2520query%250Asubsets%2520conditioned%2520on%2520dataset-specific%2520contexts%252C%2520supporting%2520rapid%252C%250Atraining-free%2520adaptation%2520at%2520inference.%2520We%2520evaluate%2520LimiX%2520across%252010%2520large%250Astructured-data%2520benchmarks%2520with%2520broad%2520regimes%2520of%2520sample%2520size%252C%2520feature%250Adimensionality%252C%2520class%2520number%252C%2520categorical-to-numerical%2520feature%2520ratio%252C%250Amissingness%252C%2520and%2520sample-to-feature%2520ratios.%2520With%2520a%2520single%2520model%2520and%2520a%2520unified%250Ainterface%252C%2520LimiX%2520consistently%2520surpasses%2520strong%2520baselines%2520including%250Agradient-boosting%2520trees%252C%2520deep%2520tabular%2520networks%252C%2520recent%2520tabular%2520foundation%250Amodels%252C%2520and%2520automated%2520ensembles%252C%2520as%2520shown%2520in%2520Figure%25201%2520and%2520Figure%25202.%2520The%250Asuperiority%2520holds%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520such%2520as%2520classification%252C%250Aregression%252C%2520missing%2520value%2520imputation%252C%2520and%2520data%2520generation%252C%2520often%2520by%2520substantial%250Amargins%252C%2520while%2520avoiding%2520task-specific%2520architectures%2520or%2520bespoke%2520training%2520per%250Atask.%2520All%2520LimiX%2520models%2520are%2520publicly%2520accessible%2520under%2520Apache%25202.0.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LimiX%3A%20Unleashing%20Structured-Data%20Modeling%20Capability%20for%20Generalist%0A%20%20Intelligence&entry.906535625=Xingxuan%20Zhang%20and%20Gang%20Ren%20and%20Han%20Yu%20and%20Hao%20Yuan%20and%20Hui%20Wang%20and%20Jiansheng%20Li%20and%20Jiayun%20Wu%20and%20Lang%20Mo%20and%20Li%20Mao%20and%20Mingchao%20Hao%20and%20Ningbo%20Dai%20and%20Renzhe%20Xu%20and%20Shuyang%20Li%20and%20Tianyang%20Zhang%20and%20Yue%20He%20and%20Yuanrui%20Wang%20and%20Yunjia%20Zhang%20and%20Zijing%20Xu%20and%20Dongzhe%20Li%20and%20Fang%20Gao%20and%20Hao%20Zou%20and%20Jiandong%20Liu%20and%20Jiashuo%20Liu%20and%20Jiawei%20Xu%20and%20Kaijie%20Cheng%20and%20Kehan%20Li%20and%20Linjun%20Zhou%20and%20Qing%20Li%20and%20Shaohua%20Fan%20and%20Xiaoyu%20Lin%20and%20Xinyan%20Han%20and%20Xuanyue%20Li%20and%20Yan%20Lu%20and%20Yuan%20Xue%20and%20Yuanyuan%20Jiang%20and%20Zimu%20Wang%20and%20Zhenlei%20Wang%20and%20Peng%20Cui&entry.1292438233=%20%20We%20argue%20that%20progress%20toward%20general%20intelligence%20requires%20complementary%0Afoundation%20models%20grounded%20in%20language%2C%20the%20physical%20world%2C%20and%20structured%0Adata.%20This%20report%20presents%20LimiX%2C%20the%20first%20installment%20of%20our%20large%0Astructured-data%20models%20%28LDMs%29.%20LimiX%20treats%20structured%20data%20as%20a%20joint%0Adistribution%20over%20variables%20and%20missingness%2C%20thus%20capable%20of%20addressing%20a%20wide%0Arange%20of%20tabular%20tasks%20through%20query-based%20conditional%20prediction%20via%20a%20single%0Amodel.%20LimiX%20is%20pretrained%20using%20masked%20joint-distribution%20modeling%20with%20an%0Aepisodic%2C%20context-conditional%20objective%2C%20where%20the%20model%20predicts%20for%20query%0Asubsets%20conditioned%20on%20dataset-specific%20contexts%2C%20supporting%20rapid%2C%0Atraining-free%20adaptation%20at%20inference.%20We%20evaluate%20LimiX%20across%2010%20large%0Astructured-data%20benchmarks%20with%20broad%20regimes%20of%20sample%20size%2C%20feature%0Adimensionality%2C%20class%20number%2C%20categorical-to-numerical%20feature%20ratio%2C%0Amissingness%2C%20and%20sample-to-feature%20ratios.%20With%20a%20single%20model%20and%20a%20unified%0Ainterface%2C%20LimiX%20consistently%20surpasses%20strong%20baselines%20including%0Agradient-boosting%20trees%2C%20deep%20tabular%20networks%2C%20recent%20tabular%20foundation%0Amodels%2C%20and%20automated%20ensembles%2C%20as%20shown%20in%20Figure%201%20and%20Figure%202.%20The%0Asuperiority%20holds%20across%20a%20wide%20range%20of%20tasks%2C%20such%20as%20classification%2C%0Aregression%2C%20missing%20value%20imputation%2C%20and%20data%20generation%2C%20often%20by%20substantial%0Amargins%2C%20while%20avoiding%20task-specific%20architectures%20or%20bespoke%20training%20per%0Atask.%20All%20LimiX%20models%20are%20publicly%20accessible%20under%20Apache%202.0.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03505v1&entry.124074799=Read"},
{"title": "HodgeFormer: Transformers for Learnable Operators on Triangular Meshes\n  through Data-Driven Hodge Matrices", "author": "Akis Nousias and Stavros Nousias", "abstract": "  Currently, prominent Transformer architectures applied on graphs and meshes\nfor shape analysis tasks employ traditional attention layers that heavily\nutilize spectral features requiring costly eigenvalue decomposition-based\nmethods. To encode the mesh structure, these methods derive positional\nembeddings, that heavily rely on eigenvalue decomposition based operations,\ne.g. on the Laplacian matrix, or on heat-kernel signatures, which are then\nconcatenated to the input features. This paper proposes a novel approach\ninspired by the explicit construction of the Hodge Laplacian operator in\nDiscrete Exterior Calculus as a product of discrete Hodge operators and\nexterior derivatives, i.e. $(L := \\star_0^{-1} d_0^T \\star_1 d_0)$. We adjust\nthe Transformer architecture in a novel deep learning layer that utilizes the\nmulti-head attention mechanism to approximate Hodge matrices $\\star_0$,\n$\\star_1$ and $\\star_2$ and learn families of discrete operators $L$ that act\non mesh vertices, edges and faces. Our approach results in a\ncomputationally-efficient architecture that achieves comparable performance in\nmesh segmentation and classification tasks, through a direct learning\nframework, while eliminating the need for costly eigenvalue decomposition\noperations or complex preprocessing operations.\n", "link": "http://arxiv.org/abs/2509.01839v2", "date": "2025-09-03", "relevancy": 2.7816, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5745}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HodgeFormer%3A%20Transformers%20for%20Learnable%20Operators%20on%20Triangular%20Meshes%0A%20%20through%20Data-Driven%20Hodge%20Matrices&body=Title%3A%20HodgeFormer%3A%20Transformers%20for%20Learnable%20Operators%20on%20Triangular%20Meshes%0A%20%20through%20Data-Driven%20Hodge%20Matrices%0AAuthor%3A%20Akis%20Nousias%20and%20Stavros%20Nousias%0AAbstract%3A%20%20%20Currently%2C%20prominent%20Transformer%20architectures%20applied%20on%20graphs%20and%20meshes%0Afor%20shape%20analysis%20tasks%20employ%20traditional%20attention%20layers%20that%20heavily%0Autilize%20spectral%20features%20requiring%20costly%20eigenvalue%20decomposition-based%0Amethods.%20To%20encode%20the%20mesh%20structure%2C%20these%20methods%20derive%20positional%0Aembeddings%2C%20that%20heavily%20rely%20on%20eigenvalue%20decomposition%20based%20operations%2C%0Ae.g.%20on%20the%20Laplacian%20matrix%2C%20or%20on%20heat-kernel%20signatures%2C%20which%20are%20then%0Aconcatenated%20to%20the%20input%20features.%20This%20paper%20proposes%20a%20novel%20approach%0Ainspired%20by%20the%20explicit%20construction%20of%20the%20Hodge%20Laplacian%20operator%20in%0ADiscrete%20Exterior%20Calculus%20as%20a%20product%20of%20discrete%20Hodge%20operators%20and%0Aexterior%20derivatives%2C%20i.e.%20%24%28L%20%3A%3D%20%5Cstar_0%5E%7B-1%7D%20d_0%5ET%20%5Cstar_1%20d_0%29%24.%20We%20adjust%0Athe%20Transformer%20architecture%20in%20a%20novel%20deep%20learning%20layer%20that%20utilizes%20the%0Amulti-head%20attention%20mechanism%20to%20approximate%20Hodge%20matrices%20%24%5Cstar_0%24%2C%0A%24%5Cstar_1%24%20and%20%24%5Cstar_2%24%20and%20learn%20families%20of%20discrete%20operators%20%24L%24%20that%20act%0Aon%20mesh%20vertices%2C%20edges%20and%20faces.%20Our%20approach%20results%20in%20a%0Acomputationally-efficient%20architecture%20that%20achieves%20comparable%20performance%20in%0Amesh%20segmentation%20and%20classification%20tasks%2C%20through%20a%20direct%20learning%0Aframework%2C%20while%20eliminating%20the%20need%20for%20costly%20eigenvalue%20decomposition%0Aoperations%20or%20complex%20preprocessing%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHodgeFormer%253A%2520Transformers%2520for%2520Learnable%2520Operators%2520on%2520Triangular%2520Meshes%250A%2520%2520through%2520Data-Driven%2520Hodge%2520Matrices%26entry.906535625%3DAkis%2520Nousias%2520and%2520Stavros%2520Nousias%26entry.1292438233%3D%2520%2520Currently%252C%2520prominent%2520Transformer%2520architectures%2520applied%2520on%2520graphs%2520and%2520meshes%250Afor%2520shape%2520analysis%2520tasks%2520employ%2520traditional%2520attention%2520layers%2520that%2520heavily%250Autilize%2520spectral%2520features%2520requiring%2520costly%2520eigenvalue%2520decomposition-based%250Amethods.%2520To%2520encode%2520the%2520mesh%2520structure%252C%2520these%2520methods%2520derive%2520positional%250Aembeddings%252C%2520that%2520heavily%2520rely%2520on%2520eigenvalue%2520decomposition%2520based%2520operations%252C%250Ae.g.%2520on%2520the%2520Laplacian%2520matrix%252C%2520or%2520on%2520heat-kernel%2520signatures%252C%2520which%2520are%2520then%250Aconcatenated%2520to%2520the%2520input%2520features.%2520This%2520paper%2520proposes%2520a%2520novel%2520approach%250Ainspired%2520by%2520the%2520explicit%2520construction%2520of%2520the%2520Hodge%2520Laplacian%2520operator%2520in%250ADiscrete%2520Exterior%2520Calculus%2520as%2520a%2520product%2520of%2520discrete%2520Hodge%2520operators%2520and%250Aexterior%2520derivatives%252C%2520i.e.%2520%2524%2528L%2520%253A%253D%2520%255Cstar_0%255E%257B-1%257D%2520d_0%255ET%2520%255Cstar_1%2520d_0%2529%2524.%2520We%2520adjust%250Athe%2520Transformer%2520architecture%2520in%2520a%2520novel%2520deep%2520learning%2520layer%2520that%2520utilizes%2520the%250Amulti-head%2520attention%2520mechanism%2520to%2520approximate%2520Hodge%2520matrices%2520%2524%255Cstar_0%2524%252C%250A%2524%255Cstar_1%2524%2520and%2520%2524%255Cstar_2%2524%2520and%2520learn%2520families%2520of%2520discrete%2520operators%2520%2524L%2524%2520that%2520act%250Aon%2520mesh%2520vertices%252C%2520edges%2520and%2520faces.%2520Our%2520approach%2520results%2520in%2520a%250Acomputationally-efficient%2520architecture%2520that%2520achieves%2520comparable%2520performance%2520in%250Amesh%2520segmentation%2520and%2520classification%2520tasks%252C%2520through%2520a%2520direct%2520learning%250Aframework%252C%2520while%2520eliminating%2520the%2520need%2520for%2520costly%2520eigenvalue%2520decomposition%250Aoperations%2520or%2520complex%2520preprocessing%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HodgeFormer%3A%20Transformers%20for%20Learnable%20Operators%20on%20Triangular%20Meshes%0A%20%20through%20Data-Driven%20Hodge%20Matrices&entry.906535625=Akis%20Nousias%20and%20Stavros%20Nousias&entry.1292438233=%20%20Currently%2C%20prominent%20Transformer%20architectures%20applied%20on%20graphs%20and%20meshes%0Afor%20shape%20analysis%20tasks%20employ%20traditional%20attention%20layers%20that%20heavily%0Autilize%20spectral%20features%20requiring%20costly%20eigenvalue%20decomposition-based%0Amethods.%20To%20encode%20the%20mesh%20structure%2C%20these%20methods%20derive%20positional%0Aembeddings%2C%20that%20heavily%20rely%20on%20eigenvalue%20decomposition%20based%20operations%2C%0Ae.g.%20on%20the%20Laplacian%20matrix%2C%20or%20on%20heat-kernel%20signatures%2C%20which%20are%20then%0Aconcatenated%20to%20the%20input%20features.%20This%20paper%20proposes%20a%20novel%20approach%0Ainspired%20by%20the%20explicit%20construction%20of%20the%20Hodge%20Laplacian%20operator%20in%0ADiscrete%20Exterior%20Calculus%20as%20a%20product%20of%20discrete%20Hodge%20operators%20and%0Aexterior%20derivatives%2C%20i.e.%20%24%28L%20%3A%3D%20%5Cstar_0%5E%7B-1%7D%20d_0%5ET%20%5Cstar_1%20d_0%29%24.%20We%20adjust%0Athe%20Transformer%20architecture%20in%20a%20novel%20deep%20learning%20layer%20that%20utilizes%20the%0Amulti-head%20attention%20mechanism%20to%20approximate%20Hodge%20matrices%20%24%5Cstar_0%24%2C%0A%24%5Cstar_1%24%20and%20%24%5Cstar_2%24%20and%20learn%20families%20of%20discrete%20operators%20%24L%24%20that%20act%0Aon%20mesh%20vertices%2C%20edges%20and%20faces.%20Our%20approach%20results%20in%20a%0Acomputationally-efficient%20architecture%20that%20achieves%20comparable%20performance%20in%0Amesh%20segmentation%20and%20classification%20tasks%2C%20through%20a%20direct%20learning%0Aframework%2C%20while%20eliminating%20the%20need%20for%20costly%20eigenvalue%20decomposition%0Aoperations%20or%20complex%20preprocessing%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01839v2&entry.124074799=Read"},
{"title": "Aligning Machine and Human Visual Representations across Abstraction\n  Levels", "author": "Lukas Muttenthaler and Klaus Greff and Frieda Born and Bernhard Spitzer and Simon Kornblith and Michael C. Mozer and Klaus-Robert M\u00fcller and Thomas Unterthiner and Andrew K. Lampinen", "abstract": "  Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior and neural representations\nin vision tasks. However, neural network training and human learning differ in\nfundamental ways, and neural networks often fail to generalize as robustly as\nhumans do raising questions regarding the similarity of their underlying\nrepresentations. What is missing for modern learning systems to exhibit more\nhuman-aligned behavior? We highlight a key misalignment between vision models\nand humans: whereas human conceptual knowledge is hierarchically organized from\nfine- to coarse-scale distinctions, model representations do not accurately\ncapture all these levels of abstraction. To address this misalignment, we first\ntrain a teacher model to imitate human judgments, then transfer human-aligned\nstructure from its representations to refine the representations of pretrained\nstate-of-the-art vision foundation models via finetuning. These human-aligned\nmodels more accurately approximate human behavior and uncertainty across a wide\nrange of similarity tasks, including a new dataset of human judgments spanning\nmultiple levels of semantic abstractions. They also perform better on a diverse\nset of machine learning tasks, increasing generalization and\nout-of-distribution robustness. Thus, infusing neural networks with additional\nhuman knowledge yields a best-of-both-worlds representation that is both more\nconsistent with human cognitive judgments and more practically useful, thus\npaving the way toward more robust, interpretable, and human-aligned artificial\nintelligence systems.\n", "link": "http://arxiv.org/abs/2409.06509v4", "date": "2025-09-03", "relevancy": 2.756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels&body=Title%3A%20Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels%0AAuthor%3A%20Lukas%20Muttenthaler%20and%20Klaus%20Greff%20and%20Frieda%20Born%20and%20Bernhard%20Spitzer%20and%20Simon%20Kornblith%20and%20Michael%20C.%20Mozer%20and%20Klaus-Robert%20M%C3%BCller%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20achieved%20success%20across%20a%20wide%20range%20of%0Aapplications%2C%20including%20as%20models%20of%20human%20behavior%20and%20neural%20representations%0Ain%20vision%20tasks.%20However%2C%20neural%20network%20training%20and%20human%20learning%20differ%20in%0Afundamental%20ways%2C%20and%20neural%20networks%20often%20fail%20to%20generalize%20as%20robustly%20as%0Ahumans%20do%20raising%20questions%20regarding%20the%20similarity%20of%20their%20underlying%0Arepresentations.%20What%20is%20missing%20for%20modern%20learning%20systems%20to%20exhibit%20more%0Ahuman-aligned%20behavior%3F%20We%20highlight%20a%20key%20misalignment%20between%20vision%20models%0Aand%20humans%3A%20whereas%20human%20conceptual%20knowledge%20is%20hierarchically%20organized%20from%0Afine-%20to%20coarse-scale%20distinctions%2C%20model%20representations%20do%20not%20accurately%0Acapture%20all%20these%20levels%20of%20abstraction.%20To%20address%20this%20misalignment%2C%20we%20first%0Atrain%20a%20teacher%20model%20to%20imitate%20human%20judgments%2C%20then%20transfer%20human-aligned%0Astructure%20from%20its%20representations%20to%20refine%20the%20representations%20of%20pretrained%0Astate-of-the-art%20vision%20foundation%20models%20via%20finetuning.%20These%20human-aligned%0Amodels%20more%20accurately%20approximate%20human%20behavior%20and%20uncertainty%20across%20a%20wide%0Arange%20of%20similarity%20tasks%2C%20including%20a%20new%20dataset%20of%20human%20judgments%20spanning%0Amultiple%20levels%20of%20semantic%20abstractions.%20They%20also%20perform%20better%20on%20a%20diverse%0Aset%20of%20machine%20learning%20tasks%2C%20increasing%20generalization%20and%0Aout-of-distribution%20robustness.%20Thus%2C%20infusing%20neural%20networks%20with%20additional%0Ahuman%20knowledge%20yields%20a%20best-of-both-worlds%20representation%20that%20is%20both%20more%0Aconsistent%20with%20human%20cognitive%20judgments%20and%20more%20practically%20useful%2C%20thus%0Apaving%20the%20way%20toward%20more%20robust%2C%20interpretable%2C%20and%20human-aligned%20artificial%0Aintelligence%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06509v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Machine%2520and%2520Human%2520Visual%2520Representations%2520across%2520Abstraction%250A%2520%2520Levels%26entry.906535625%3DLukas%2520Muttenthaler%2520and%2520Klaus%2520Greff%2520and%2520Frieda%2520Born%2520and%2520Bernhard%2520Spitzer%2520and%2520Simon%2520Kornblith%2520and%2520Michael%2520C.%2520Mozer%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Thomas%2520Unterthiner%2520and%2520Andrew%2520K.%2520Lampinen%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520achieved%2520success%2520across%2520a%2520wide%2520range%2520of%250Aapplications%252C%2520including%2520as%2520models%2520of%2520human%2520behavior%2520and%2520neural%2520representations%250Ain%2520vision%2520tasks.%2520However%252C%2520neural%2520network%2520training%2520and%2520human%2520learning%2520differ%2520in%250Afundamental%2520ways%252C%2520and%2520neural%2520networks%2520often%2520fail%2520to%2520generalize%2520as%2520robustly%2520as%250Ahumans%2520do%2520raising%2520questions%2520regarding%2520the%2520similarity%2520of%2520their%2520underlying%250Arepresentations.%2520What%2520is%2520missing%2520for%2520modern%2520learning%2520systems%2520to%2520exhibit%2520more%250Ahuman-aligned%2520behavior%253F%2520We%2520highlight%2520a%2520key%2520misalignment%2520between%2520vision%2520models%250Aand%2520humans%253A%2520whereas%2520human%2520conceptual%2520knowledge%2520is%2520hierarchically%2520organized%2520from%250Afine-%2520to%2520coarse-scale%2520distinctions%252C%2520model%2520representations%2520do%2520not%2520accurately%250Acapture%2520all%2520these%2520levels%2520of%2520abstraction.%2520To%2520address%2520this%2520misalignment%252C%2520we%2520first%250Atrain%2520a%2520teacher%2520model%2520to%2520imitate%2520human%2520judgments%252C%2520then%2520transfer%2520human-aligned%250Astructure%2520from%2520its%2520representations%2520to%2520refine%2520the%2520representations%2520of%2520pretrained%250Astate-of-the-art%2520vision%2520foundation%2520models%2520via%2520finetuning.%2520These%2520human-aligned%250Amodels%2520more%2520accurately%2520approximate%2520human%2520behavior%2520and%2520uncertainty%2520across%2520a%2520wide%250Arange%2520of%2520similarity%2520tasks%252C%2520including%2520a%2520new%2520dataset%2520of%2520human%2520judgments%2520spanning%250Amultiple%2520levels%2520of%2520semantic%2520abstractions.%2520They%2520also%2520perform%2520better%2520on%2520a%2520diverse%250Aset%2520of%2520machine%2520learning%2520tasks%252C%2520increasing%2520generalization%2520and%250Aout-of-distribution%2520robustness.%2520Thus%252C%2520infusing%2520neural%2520networks%2520with%2520additional%250Ahuman%2520knowledge%2520yields%2520a%2520best-of-both-worlds%2520representation%2520that%2520is%2520both%2520more%250Aconsistent%2520with%2520human%2520cognitive%2520judgments%2520and%2520more%2520practically%2520useful%252C%2520thus%250Apaving%2520the%2520way%2520toward%2520more%2520robust%252C%2520interpretable%252C%2520and%2520human-aligned%2520artificial%250Aintelligence%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06509v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels&entry.906535625=Lukas%20Muttenthaler%20and%20Klaus%20Greff%20and%20Frieda%20Born%20and%20Bernhard%20Spitzer%20and%20Simon%20Kornblith%20and%20Michael%20C.%20Mozer%20and%20Klaus-Robert%20M%C3%BCller%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen&entry.1292438233=%20%20Deep%20neural%20networks%20have%20achieved%20success%20across%20a%20wide%20range%20of%0Aapplications%2C%20including%20as%20models%20of%20human%20behavior%20and%20neural%20representations%0Ain%20vision%20tasks.%20However%2C%20neural%20network%20training%20and%20human%20learning%20differ%20in%0Afundamental%20ways%2C%20and%20neural%20networks%20often%20fail%20to%20generalize%20as%20robustly%20as%0Ahumans%20do%20raising%20questions%20regarding%20the%20similarity%20of%20their%20underlying%0Arepresentations.%20What%20is%20missing%20for%20modern%20learning%20systems%20to%20exhibit%20more%0Ahuman-aligned%20behavior%3F%20We%20highlight%20a%20key%20misalignment%20between%20vision%20models%0Aand%20humans%3A%20whereas%20human%20conceptual%20knowledge%20is%20hierarchically%20organized%20from%0Afine-%20to%20coarse-scale%20distinctions%2C%20model%20representations%20do%20not%20accurately%0Acapture%20all%20these%20levels%20of%20abstraction.%20To%20address%20this%20misalignment%2C%20we%20first%0Atrain%20a%20teacher%20model%20to%20imitate%20human%20judgments%2C%20then%20transfer%20human-aligned%0Astructure%20from%20its%20representations%20to%20refine%20the%20representations%20of%20pretrained%0Astate-of-the-art%20vision%20foundation%20models%20via%20finetuning.%20These%20human-aligned%0Amodels%20more%20accurately%20approximate%20human%20behavior%20and%20uncertainty%20across%20a%20wide%0Arange%20of%20similarity%20tasks%2C%20including%20a%20new%20dataset%20of%20human%20judgments%20spanning%0Amultiple%20levels%20of%20semantic%20abstractions.%20They%20also%20perform%20better%20on%20a%20diverse%0Aset%20of%20machine%20learning%20tasks%2C%20increasing%20generalization%20and%0Aout-of-distribution%20robustness.%20Thus%2C%20infusing%20neural%20networks%20with%20additional%0Ahuman%20knowledge%20yields%20a%20best-of-both-worlds%20representation%20that%20is%20both%20more%0Aconsistent%20with%20human%20cognitive%20judgments%20and%20more%20practically%20useful%2C%20thus%0Apaving%20the%20way%20toward%20more%20robust%2C%20interpretable%2C%20and%20human-aligned%20artificial%0Aintelligence%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06509v4&entry.124074799=Read"},
{"title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning\n  via Synthetic Instruction Data", "author": "Honglu Zhou and Xiangyu Peng and Shrikant Kendre and Michael S. Ryoo and Silvio Savarese and Caiming Xiong and Juan Carlos Niebles", "abstract": "  Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs.\n", "link": "http://arxiv.org/abs/2509.03501v1", "date": "2025-09-03", "relevancy": 2.7427, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strefer%3A%20Empowering%20Video%20LLMs%20with%20Space-Time%20Referring%20and%20Reasoning%0A%20%20via%20Synthetic%20Instruction%20Data&body=Title%3A%20Strefer%3A%20Empowering%20Video%20LLMs%20with%20Space-Time%20Referring%20and%20Reasoning%0A%20%20via%20Synthetic%20Instruction%20Data%0AAuthor%3A%20Honglu%20Zhou%20and%20Xiangyu%20Peng%20and%20Shrikant%20Kendre%20and%20Michael%20S.%20Ryoo%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Juan%20Carlos%20Niebles%0AAbstract%3A%20%20%20Next-generation%20AI%20companions%20must%20go%20beyond%20general%20video%20understanding%20to%0Aresolve%20spatial%20and%20temporal%20references%20in%20dynamic%2C%20real-world%20environments.%0AExisting%20Video%20Large%20Language%20Models%20%28Video%20LLMs%29%2C%20while%20capable%20of%0Acoarse-level%20comprehension%2C%20struggle%20with%20fine-grained%2C%20spatiotemporal%0Areasoning%2C%20especially%20when%20user%20queries%20rely%20on%20time-based%20event%20references%20for%0Atemporal%20anchoring%2C%20or%20gestural%20cues%20for%20spatial%20anchoring%20to%20clarify%20object%0Areferences%20and%20positions.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20Strefer%2C%20a%0Asynthetic%20instruction%20data%20generation%20framework%20designed%20to%20equip%20Video%20LLMs%0Awith%20spatiotemporal%20referring%20and%20reasoning%20capabilities.%20Strefer%20produces%0Adiverse%20instruction-tuning%20data%20using%20a%20data%20engine%20that%20pseudo-annotates%0Atemporally%20dense%2C%20fine-grained%20video%20metadata%2C%20capturing%20rich%20spatial%20and%0Atemporal%20information%20in%20a%20structured%20manner%2C%20including%20subjects%2C%20objects%2C%20their%0Alocations%20as%20masklets%2C%20and%20their%20action%20descriptions%20and%20timelines.%20Our%0Aapproach%20enhances%20the%20ability%20of%20Video%20LLMs%20to%20interpret%20spatial%20and%20temporal%0Areferences%2C%20fostering%20more%20versatile%2C%20space-time-aware%20reasoning%20essential%20for%0Areal-world%20AI%20companions.%20Without%20using%20proprietary%20models%2C%20costly%20human%0Aannotation%2C%20or%20the%20need%20to%20annotate%20large%20volumes%20of%20new%20videos%2C%20experimental%0Aevaluations%20show%20that%20models%20trained%20with%20data%20produced%20by%20Strefer%20outperform%0Abaselines%20on%20tasks%20requiring%20spatial%20and%20temporal%20disambiguation.%20Additionally%2C%0Athese%20models%20exhibit%20enhanced%20space-time-aware%20reasoning%2C%20establishing%20a%20new%0Afoundation%20for%20perceptually%20grounded%2C%20instruction-tuned%20Video%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrefer%253A%2520Empowering%2520Video%2520LLMs%2520with%2520Space-Time%2520Referring%2520and%2520Reasoning%250A%2520%2520via%2520Synthetic%2520Instruction%2520Data%26entry.906535625%3DHonglu%2520Zhou%2520and%2520Xiangyu%2520Peng%2520and%2520Shrikant%2520Kendre%2520and%2520Michael%2520S.%2520Ryoo%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%2520and%2520Juan%2520Carlos%2520Niebles%26entry.1292438233%3D%2520%2520Next-generation%2520AI%2520companions%2520must%2520go%2520beyond%2520general%2520video%2520understanding%2520to%250Aresolve%2520spatial%2520and%2520temporal%2520references%2520in%2520dynamic%252C%2520real-world%2520environments.%250AExisting%2520Video%2520Large%2520Language%2520Models%2520%2528Video%2520LLMs%2529%252C%2520while%2520capable%2520of%250Acoarse-level%2520comprehension%252C%2520struggle%2520with%2520fine-grained%252C%2520spatiotemporal%250Areasoning%252C%2520especially%2520when%2520user%2520queries%2520rely%2520on%2520time-based%2520event%2520references%2520for%250Atemporal%2520anchoring%252C%2520or%2520gestural%2520cues%2520for%2520spatial%2520anchoring%2520to%2520clarify%2520object%250Areferences%2520and%2520positions.%2520To%2520bridge%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520Strefer%252C%2520a%250Asynthetic%2520instruction%2520data%2520generation%2520framework%2520designed%2520to%2520equip%2520Video%2520LLMs%250Awith%2520spatiotemporal%2520referring%2520and%2520reasoning%2520capabilities.%2520Strefer%2520produces%250Adiverse%2520instruction-tuning%2520data%2520using%2520a%2520data%2520engine%2520that%2520pseudo-annotates%250Atemporally%2520dense%252C%2520fine-grained%2520video%2520metadata%252C%2520capturing%2520rich%2520spatial%2520and%250Atemporal%2520information%2520in%2520a%2520structured%2520manner%252C%2520including%2520subjects%252C%2520objects%252C%2520their%250Alocations%2520as%2520masklets%252C%2520and%2520their%2520action%2520descriptions%2520and%2520timelines.%2520Our%250Aapproach%2520enhances%2520the%2520ability%2520of%2520Video%2520LLMs%2520to%2520interpret%2520spatial%2520and%2520temporal%250Areferences%252C%2520fostering%2520more%2520versatile%252C%2520space-time-aware%2520reasoning%2520essential%2520for%250Areal-world%2520AI%2520companions.%2520Without%2520using%2520proprietary%2520models%252C%2520costly%2520human%250Aannotation%252C%2520or%2520the%2520need%2520to%2520annotate%2520large%2520volumes%2520of%2520new%2520videos%252C%2520experimental%250Aevaluations%2520show%2520that%2520models%2520trained%2520with%2520data%2520produced%2520by%2520Strefer%2520outperform%250Abaselines%2520on%2520tasks%2520requiring%2520spatial%2520and%2520temporal%2520disambiguation.%2520Additionally%252C%250Athese%2520models%2520exhibit%2520enhanced%2520space-time-aware%2520reasoning%252C%2520establishing%2520a%2520new%250Afoundation%2520for%2520perceptually%2520grounded%252C%2520instruction-tuned%2520Video%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strefer%3A%20Empowering%20Video%20LLMs%20with%20Space-Time%20Referring%20and%20Reasoning%0A%20%20via%20Synthetic%20Instruction%20Data&entry.906535625=Honglu%20Zhou%20and%20Xiangyu%20Peng%20and%20Shrikant%20Kendre%20and%20Michael%20S.%20Ryoo%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Juan%20Carlos%20Niebles&entry.1292438233=%20%20Next-generation%20AI%20companions%20must%20go%20beyond%20general%20video%20understanding%20to%0Aresolve%20spatial%20and%20temporal%20references%20in%20dynamic%2C%20real-world%20environments.%0AExisting%20Video%20Large%20Language%20Models%20%28Video%20LLMs%29%2C%20while%20capable%20of%0Acoarse-level%20comprehension%2C%20struggle%20with%20fine-grained%2C%20spatiotemporal%0Areasoning%2C%20especially%20when%20user%20queries%20rely%20on%20time-based%20event%20references%20for%0Atemporal%20anchoring%2C%20or%20gestural%20cues%20for%20spatial%20anchoring%20to%20clarify%20object%0Areferences%20and%20positions.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20Strefer%2C%20a%0Asynthetic%20instruction%20data%20generation%20framework%20designed%20to%20equip%20Video%20LLMs%0Awith%20spatiotemporal%20referring%20and%20reasoning%20capabilities.%20Strefer%20produces%0Adiverse%20instruction-tuning%20data%20using%20a%20data%20engine%20that%20pseudo-annotates%0Atemporally%20dense%2C%20fine-grained%20video%20metadata%2C%20capturing%20rich%20spatial%20and%0Atemporal%20information%20in%20a%20structured%20manner%2C%20including%20subjects%2C%20objects%2C%20their%0Alocations%20as%20masklets%2C%20and%20their%20action%20descriptions%20and%20timelines.%20Our%0Aapproach%20enhances%20the%20ability%20of%20Video%20LLMs%20to%20interpret%20spatial%20and%20temporal%0Areferences%2C%20fostering%20more%20versatile%2C%20space-time-aware%20reasoning%20essential%20for%0Areal-world%20AI%20companions.%20Without%20using%20proprietary%20models%2C%20costly%20human%0Aannotation%2C%20or%20the%20need%20to%20annotate%20large%20volumes%20of%20new%20videos%2C%20experimental%0Aevaluations%20show%20that%20models%20trained%20with%20data%20produced%20by%20Strefer%20outperform%0Abaselines%20on%20tasks%20requiring%20spatial%20and%20temporal%20disambiguation.%20Additionally%2C%0Athese%20models%20exhibit%20enhanced%20space-time-aware%20reasoning%2C%20establishing%20a%20new%0Afoundation%20for%20perceptually%20grounded%2C%20instruction-tuned%20Video%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03501v1&entry.124074799=Read"},
{"title": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs", "author": "Gaye Colakoglu and G\u00fcrkan Solmaz and Jonathan F\u00fcrst", "abstract": "  This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study investigates the sub-problems\nand methods within these core challenges, such as input representation,\nchunking, prompting, selection of LLMs, and multimodal models. It examines the\neffect of different design choices through LayIE-LLM, a new, open-source,\nlayout-aware IE test suite, benchmarking against traditional, fine-tuned IE\nmodels. The results on two IE datasets show that LLMs require adjustment of the\nIE pipeline to achieve competitive performance: the optimized configuration\nfound with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice\nbaseline configuration using the same LLM. To find a well-working\nconfiguration, we develop a one-factor-at-a-time (OFAT) method that achieves\nnear-optimal results. Our method is only 0.8--1.8 points lower than the best\nfull factorial exploration with a fraction (2.8%) of the required computation.\nOverall, we demonstrate that, if well-configured, general-purpose LLMs match\nthe performance of specialized models, providing a cost-effective,\nfinetuning-free alternative. Our test-suite is available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.\n", "link": "http://arxiv.org/abs/2502.18179v2", "date": "2025-09-03", "relevancy": 2.7027, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Problem%20Solved%3F%20Information%20Extraction%20Design%20Space%20for%20Layout-Rich%0A%20%20Documents%20using%20LLMs&body=Title%3A%20Problem%20Solved%3F%20Information%20Extraction%20Design%20Space%20for%20Layout-Rich%0A%20%20Documents%20using%20LLMs%0AAuthor%3A%20Gaye%20Colakoglu%20and%20G%C3%BCrkan%20Solmaz%20and%20Jonathan%20F%C3%BCrst%0AAbstract%3A%20%20%20This%20paper%20defines%20and%20explores%20the%20design%20space%20for%20information%20extraction%0A%28IE%29%20from%20layout-rich%20documents%20using%20large%20language%20models%20%28LLMs%29.%20The%20three%0Acore%20challenges%20of%20layout-aware%20IE%20with%20LLMs%20are%201%29%20data%20structuring%2C%202%29%20model%0Aengagement%2C%20and%203%29%20output%20refinement.%20Our%20study%20investigates%20the%20sub-problems%0Aand%20methods%20within%20these%20core%20challenges%2C%20such%20as%20input%20representation%2C%0Achunking%2C%20prompting%2C%20selection%20of%20LLMs%2C%20and%20multimodal%20models.%20It%20examines%20the%0Aeffect%20of%20different%20design%20choices%20through%20LayIE-LLM%2C%20a%20new%2C%20open-source%2C%0Alayout-aware%20IE%20test%20suite%2C%20benchmarking%20against%20traditional%2C%20fine-tuned%20IE%0Amodels.%20The%20results%20on%20two%20IE%20datasets%20show%20that%20LLMs%20require%20adjustment%20of%20the%0AIE%20pipeline%20to%20achieve%20competitive%20performance%3A%20the%20optimized%20configuration%0Afound%20with%20LayIE-LLM%20achieves%2013.3--37.5%20F1%20points%20more%20than%20a%20general-practice%0Abaseline%20configuration%20using%20the%20same%20LLM.%20To%20find%20a%20well-working%0Aconfiguration%2C%20we%20develop%20a%20one-factor-at-a-time%20%28OFAT%29%20method%20that%20achieves%0Anear-optimal%20results.%20Our%20method%20is%20only%200.8--1.8%20points%20lower%20than%20the%20best%0Afull%20factorial%20exploration%20with%20a%20fraction%20%282.8%25%29%20of%20the%20required%20computation.%0AOverall%2C%20we%20demonstrate%20that%2C%20if%20well-configured%2C%20general-purpose%20LLMs%20match%0Athe%20performance%20of%20specialized%20models%2C%20providing%20a%20cost-effective%2C%0Afinetuning-free%20alternative.%20Our%20test-suite%20is%20available%20at%0Ahttps%3A//github.com/gayecolakoglu/LayIE-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProblem%2520Solved%253F%2520Information%2520Extraction%2520Design%2520Space%2520for%2520Layout-Rich%250A%2520%2520Documents%2520using%2520LLMs%26entry.906535625%3DGaye%2520Colakoglu%2520and%2520G%25C3%25BCrkan%2520Solmaz%2520and%2520Jonathan%2520F%25C3%25BCrst%26entry.1292438233%3D%2520%2520This%2520paper%2520defines%2520and%2520explores%2520the%2520design%2520space%2520for%2520information%2520extraction%250A%2528IE%2529%2520from%2520layout-rich%2520documents%2520using%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520three%250Acore%2520challenges%2520of%2520layout-aware%2520IE%2520with%2520LLMs%2520are%25201%2529%2520data%2520structuring%252C%25202%2529%2520model%250Aengagement%252C%2520and%25203%2529%2520output%2520refinement.%2520Our%2520study%2520investigates%2520the%2520sub-problems%250Aand%2520methods%2520within%2520these%2520core%2520challenges%252C%2520such%2520as%2520input%2520representation%252C%250Achunking%252C%2520prompting%252C%2520selection%2520of%2520LLMs%252C%2520and%2520multimodal%2520models.%2520It%2520examines%2520the%250Aeffect%2520of%2520different%2520design%2520choices%2520through%2520LayIE-LLM%252C%2520a%2520new%252C%2520open-source%252C%250Alayout-aware%2520IE%2520test%2520suite%252C%2520benchmarking%2520against%2520traditional%252C%2520fine-tuned%2520IE%250Amodels.%2520The%2520results%2520on%2520two%2520IE%2520datasets%2520show%2520that%2520LLMs%2520require%2520adjustment%2520of%2520the%250AIE%2520pipeline%2520to%2520achieve%2520competitive%2520performance%253A%2520the%2520optimized%2520configuration%250Afound%2520with%2520LayIE-LLM%2520achieves%252013.3--37.5%2520F1%2520points%2520more%2520than%2520a%2520general-practice%250Abaseline%2520configuration%2520using%2520the%2520same%2520LLM.%2520To%2520find%2520a%2520well-working%250Aconfiguration%252C%2520we%2520develop%2520a%2520one-factor-at-a-time%2520%2528OFAT%2529%2520method%2520that%2520achieves%250Anear-optimal%2520results.%2520Our%2520method%2520is%2520only%25200.8--1.8%2520points%2520lower%2520than%2520the%2520best%250Afull%2520factorial%2520exploration%2520with%2520a%2520fraction%2520%25282.8%2525%2529%2520of%2520the%2520required%2520computation.%250AOverall%252C%2520we%2520demonstrate%2520that%252C%2520if%2520well-configured%252C%2520general-purpose%2520LLMs%2520match%250Athe%2520performance%2520of%2520specialized%2520models%252C%2520providing%2520a%2520cost-effective%252C%250Afinetuning-free%2520alternative.%2520Our%2520test-suite%2520is%2520available%2520at%250Ahttps%253A//github.com/gayecolakoglu/LayIE-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Problem%20Solved%3F%20Information%20Extraction%20Design%20Space%20for%20Layout-Rich%0A%20%20Documents%20using%20LLMs&entry.906535625=Gaye%20Colakoglu%20and%20G%C3%BCrkan%20Solmaz%20and%20Jonathan%20F%C3%BCrst&entry.1292438233=%20%20This%20paper%20defines%20and%20explores%20the%20design%20space%20for%20information%20extraction%0A%28IE%29%20from%20layout-rich%20documents%20using%20large%20language%20models%20%28LLMs%29.%20The%20three%0Acore%20challenges%20of%20layout-aware%20IE%20with%20LLMs%20are%201%29%20data%20structuring%2C%202%29%20model%0Aengagement%2C%20and%203%29%20output%20refinement.%20Our%20study%20investigates%20the%20sub-problems%0Aand%20methods%20within%20these%20core%20challenges%2C%20such%20as%20input%20representation%2C%0Achunking%2C%20prompting%2C%20selection%20of%20LLMs%2C%20and%20multimodal%20models.%20It%20examines%20the%0Aeffect%20of%20different%20design%20choices%20through%20LayIE-LLM%2C%20a%20new%2C%20open-source%2C%0Alayout-aware%20IE%20test%20suite%2C%20benchmarking%20against%20traditional%2C%20fine-tuned%20IE%0Amodels.%20The%20results%20on%20two%20IE%20datasets%20show%20that%20LLMs%20require%20adjustment%20of%20the%0AIE%20pipeline%20to%20achieve%20competitive%20performance%3A%20the%20optimized%20configuration%0Afound%20with%20LayIE-LLM%20achieves%2013.3--37.5%20F1%20points%20more%20than%20a%20general-practice%0Abaseline%20configuration%20using%20the%20same%20LLM.%20To%20find%20a%20well-working%0Aconfiguration%2C%20we%20develop%20a%20one-factor-at-a-time%20%28OFAT%29%20method%20that%20achieves%0Anear-optimal%20results.%20Our%20method%20is%20only%200.8--1.8%20points%20lower%20than%20the%20best%0Afull%20factorial%20exploration%20with%20a%20fraction%20%282.8%25%29%20of%20the%20required%20computation.%0AOverall%2C%20we%20demonstrate%20that%2C%20if%20well-configured%2C%20general-purpose%20LLMs%20match%0Athe%20performance%20of%20specialized%20models%2C%20providing%20a%20cost-effective%2C%0Afinetuning-free%20alternative.%20Our%20test-suite%20is%20available%20at%0Ahttps%3A//github.com/gayecolakoglu/LayIE-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18179v2&entry.124074799=Read"},
{"title": "LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer\n  Features for Robust Organoid Segmentation and Tracking", "author": "Jing Zhang and Siying Tao and Jiao Li and Tianhe Wang and Junchen Wu and Ruqian Hao and Xiaohui Du and Ruirong Tan and Rui Li", "abstract": "  Organoids replicate organ structure and function, playing a crucial role in\nfields such as tumor treatment and drug screening. Their shape and size can\nindicate their developmental status, but traditional fluorescence labeling\nmethods risk compromising their structure. Therefore, this paper proposes an\nautomated, non-destructive approach to organoid segmentation and tracking. We\nintroduced the LGBP-OrgaNet, a deep learning-based system proficient in\naccurately segmenting, tracking, and quantifying organoids. The model leverages\ncomplementary information extracted from CNN and Transformer modules and\nintroduces the innovative feature fusion module, Learnable Gaussian Band Pass\nFusion, to merge data from two branches. Additionally, in the decoder, the\nmodel proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,\nand finally completes the decoding through progressive concatenation and\nupsampling. SROrga demonstrates satisfactory segmentation accuracy and\nrobustness on organoids segmentation datasets, providing a potent tool for\norganoid research.\n", "link": "http://arxiv.org/abs/2509.03221v1", "date": "2025-09-03", "relevancy": 2.6964, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5468}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5404}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGBP-OrgaNet%3A%20Learnable%20Gaussian%20Band%20Pass%20Fusion%20of%20CNN%20and%20Transformer%0A%20%20Features%20for%20Robust%20Organoid%20Segmentation%20and%20Tracking&body=Title%3A%20LGBP-OrgaNet%3A%20Learnable%20Gaussian%20Band%20Pass%20Fusion%20of%20CNN%20and%20Transformer%0A%20%20Features%20for%20Robust%20Organoid%20Segmentation%20and%20Tracking%0AAuthor%3A%20Jing%20Zhang%20and%20Siying%20Tao%20and%20Jiao%20Li%20and%20Tianhe%20Wang%20and%20Junchen%20Wu%20and%20Ruqian%20Hao%20and%20Xiaohui%20Du%20and%20Ruirong%20Tan%20and%20Rui%20Li%0AAbstract%3A%20%20%20Organoids%20replicate%20organ%20structure%20and%20function%2C%20playing%20a%20crucial%20role%20in%0Afields%20such%20as%20tumor%20treatment%20and%20drug%20screening.%20Their%20shape%20and%20size%20can%0Aindicate%20their%20developmental%20status%2C%20but%20traditional%20fluorescence%20labeling%0Amethods%20risk%20compromising%20their%20structure.%20Therefore%2C%20this%20paper%20proposes%20an%0Aautomated%2C%20non-destructive%20approach%20to%20organoid%20segmentation%20and%20tracking.%20We%0Aintroduced%20the%20LGBP-OrgaNet%2C%20a%20deep%20learning-based%20system%20proficient%20in%0Aaccurately%20segmenting%2C%20tracking%2C%20and%20quantifying%20organoids.%20The%20model%20leverages%0Acomplementary%20information%20extracted%20from%20CNN%20and%20Transformer%20modules%20and%0Aintroduces%20the%20innovative%20feature%20fusion%20module%2C%20Learnable%20Gaussian%20Band%20Pass%0AFusion%2C%20to%20merge%20data%20from%20two%20branches.%20Additionally%2C%20in%20the%20decoder%2C%20the%0Amodel%20proposes%20a%20Bidirectional%20Cross%20Fusion%20Block%20to%20fuse%20multi-scale%20features%2C%0Aand%20finally%20completes%20the%20decoding%20through%20progressive%20concatenation%20and%0Aupsampling.%20SROrga%20demonstrates%20satisfactory%20segmentation%20accuracy%20and%0Arobustness%20on%20organoids%20segmentation%20datasets%2C%20providing%20a%20potent%20tool%20for%0Aorganoid%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGBP-OrgaNet%253A%2520Learnable%2520Gaussian%2520Band%2520Pass%2520Fusion%2520of%2520CNN%2520and%2520Transformer%250A%2520%2520Features%2520for%2520Robust%2520Organoid%2520Segmentation%2520and%2520Tracking%26entry.906535625%3DJing%2520Zhang%2520and%2520Siying%2520Tao%2520and%2520Jiao%2520Li%2520and%2520Tianhe%2520Wang%2520and%2520Junchen%2520Wu%2520and%2520Ruqian%2520Hao%2520and%2520Xiaohui%2520Du%2520and%2520Ruirong%2520Tan%2520and%2520Rui%2520Li%26entry.1292438233%3D%2520%2520Organoids%2520replicate%2520organ%2520structure%2520and%2520function%252C%2520playing%2520a%2520crucial%2520role%2520in%250Afields%2520such%2520as%2520tumor%2520treatment%2520and%2520drug%2520screening.%2520Their%2520shape%2520and%2520size%2520can%250Aindicate%2520their%2520developmental%2520status%252C%2520but%2520traditional%2520fluorescence%2520labeling%250Amethods%2520risk%2520compromising%2520their%2520structure.%2520Therefore%252C%2520this%2520paper%2520proposes%2520an%250Aautomated%252C%2520non-destructive%2520approach%2520to%2520organoid%2520segmentation%2520and%2520tracking.%2520We%250Aintroduced%2520the%2520LGBP-OrgaNet%252C%2520a%2520deep%2520learning-based%2520system%2520proficient%2520in%250Aaccurately%2520segmenting%252C%2520tracking%252C%2520and%2520quantifying%2520organoids.%2520The%2520model%2520leverages%250Acomplementary%2520information%2520extracted%2520from%2520CNN%2520and%2520Transformer%2520modules%2520and%250Aintroduces%2520the%2520innovative%2520feature%2520fusion%2520module%252C%2520Learnable%2520Gaussian%2520Band%2520Pass%250AFusion%252C%2520to%2520merge%2520data%2520from%2520two%2520branches.%2520Additionally%252C%2520in%2520the%2520decoder%252C%2520the%250Amodel%2520proposes%2520a%2520Bidirectional%2520Cross%2520Fusion%2520Block%2520to%2520fuse%2520multi-scale%2520features%252C%250Aand%2520finally%2520completes%2520the%2520decoding%2520through%2520progressive%2520concatenation%2520and%250Aupsampling.%2520SROrga%2520demonstrates%2520satisfactory%2520segmentation%2520accuracy%2520and%250Arobustness%2520on%2520organoids%2520segmentation%2520datasets%252C%2520providing%2520a%2520potent%2520tool%2520for%250Aorganoid%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGBP-OrgaNet%3A%20Learnable%20Gaussian%20Band%20Pass%20Fusion%20of%20CNN%20and%20Transformer%0A%20%20Features%20for%20Robust%20Organoid%20Segmentation%20and%20Tracking&entry.906535625=Jing%20Zhang%20and%20Siying%20Tao%20and%20Jiao%20Li%20and%20Tianhe%20Wang%20and%20Junchen%20Wu%20and%20Ruqian%20Hao%20and%20Xiaohui%20Du%20and%20Ruirong%20Tan%20and%20Rui%20Li&entry.1292438233=%20%20Organoids%20replicate%20organ%20structure%20and%20function%2C%20playing%20a%20crucial%20role%20in%0Afields%20such%20as%20tumor%20treatment%20and%20drug%20screening.%20Their%20shape%20and%20size%20can%0Aindicate%20their%20developmental%20status%2C%20but%20traditional%20fluorescence%20labeling%0Amethods%20risk%20compromising%20their%20structure.%20Therefore%2C%20this%20paper%20proposes%20an%0Aautomated%2C%20non-destructive%20approach%20to%20organoid%20segmentation%20and%20tracking.%20We%0Aintroduced%20the%20LGBP-OrgaNet%2C%20a%20deep%20learning-based%20system%20proficient%20in%0Aaccurately%20segmenting%2C%20tracking%2C%20and%20quantifying%20organoids.%20The%20model%20leverages%0Acomplementary%20information%20extracted%20from%20CNN%20and%20Transformer%20modules%20and%0Aintroduces%20the%20innovative%20feature%20fusion%20module%2C%20Learnable%20Gaussian%20Band%20Pass%0AFusion%2C%20to%20merge%20data%20from%20two%20branches.%20Additionally%2C%20in%20the%20decoder%2C%20the%0Amodel%20proposes%20a%20Bidirectional%20Cross%20Fusion%20Block%20to%20fuse%20multi-scale%20features%2C%0Aand%20finally%20completes%20the%20decoding%20through%20progressive%20concatenation%20and%0Aupsampling.%20SROrga%20demonstrates%20satisfactory%20segmentation%20accuracy%20and%0Arobustness%20on%20organoids%20segmentation%20datasets%2C%20providing%20a%20potent%20tool%20for%0Aorganoid%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03221v1&entry.124074799=Read"},
{"title": "Structure-preserving contrastive learning for spatial time series", "author": "Yiru Jiao and Sander van Cranenburgh and Simeon Calvert and Hans van Lint", "abstract": "  The effectiveness of neural network models largely relies on learning\nmeaningful latent patterns from data, where self-supervised learning of\ninformative representations can enhance model performance and generalisability.\nHowever, self-supervised representation learning for spatially characterised\ntime series, which are ubiquitous in transportation domain, poses unique\nchallenges due to the necessity of maintaining fine-grained spatio-temporal\nsimilarities in the latent space. In this study, we introduce two\nstructure-preserving regularisers for the contrastive learning of spatial time\nseries: one regulariser preserves the topology of similarities between\ninstances, and the other preserves the graph geometry of similarities across\nspatial and temporal dimensions. To balance the contrastive learning objective\nand the need for structure preservation, we propose a dynamic weighting\nmechanism that adaptively manages this trade-off and stabilises training. We\nvalidate the proposed method through extensive experiments, including\nmultivariate time series classification to demonstrate its general\napplicability, as well as macroscopic and microscopic traffic prediction to\nhighlight its particular usefulness in encoding traffic interactions. Across\nall tasks, our method preserves the similarity structures more effectively and\nimproves state-of-the-art task performances. This method can be integrated with\nan arbitrary neural network model and is particularly beneficial for time\nseries data with spatial or geographical features. Furthermore, our findings\nsuggest that well-preserved similarity structures in the latent space indicate\nmore informative and useful representations. This provides insights to design\nmore effective neural networks for data-driven transportation research. Our\ncode is made openly accessible with all resulting data at\nhttps://github.com/yiru-jiao/spclt\n", "link": "http://arxiv.org/abs/2502.06380v4", "date": "2025-09-03", "relevancy": 2.6794, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series&body=Title%3A%20Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series%0AAuthor%3A%20Yiru%20Jiao%20and%20Sander%20van%20Cranenburgh%20and%20Simeon%20Calvert%20and%20Hans%20van%20Lint%0AAbstract%3A%20%20%20The%20effectiveness%20of%20neural%20network%20models%20largely%20relies%20on%20learning%0Ameaningful%20latent%20patterns%20from%20data%2C%20where%20self-supervised%20learning%20of%0Ainformative%20representations%20can%20enhance%20model%20performance%20and%20generalisability.%0AHowever%2C%20self-supervised%20representation%20learning%20for%20spatially%20characterised%0Atime%20series%2C%20which%20are%20ubiquitous%20in%20transportation%20domain%2C%20poses%20unique%0Achallenges%20due%20to%20the%20necessity%20of%20maintaining%20fine-grained%20spatio-temporal%0Asimilarities%20in%20the%20latent%20space.%20In%20this%20study%2C%20we%20introduce%20two%0Astructure-preserving%20regularisers%20for%20the%20contrastive%20learning%20of%20spatial%20time%0Aseries%3A%20one%20regulariser%20preserves%20the%20topology%20of%20similarities%20between%0Ainstances%2C%20and%20the%20other%20preserves%20the%20graph%20geometry%20of%20similarities%20across%0Aspatial%20and%20temporal%20dimensions.%20To%20balance%20the%20contrastive%20learning%20objective%0Aand%20the%20need%20for%20structure%20preservation%2C%20we%20propose%20a%20dynamic%20weighting%0Amechanism%20that%20adaptively%20manages%20this%20trade-off%20and%20stabilises%20training.%20We%0Avalidate%20the%20proposed%20method%20through%20extensive%20experiments%2C%20including%0Amultivariate%20time%20series%20classification%20to%20demonstrate%20its%20general%0Aapplicability%2C%20as%20well%20as%20macroscopic%20and%20microscopic%20traffic%20prediction%20to%0Ahighlight%20its%20particular%20usefulness%20in%20encoding%20traffic%20interactions.%20Across%0Aall%20tasks%2C%20our%20method%20preserves%20the%20similarity%20structures%20more%20effectively%20and%0Aimproves%20state-of-the-art%20task%20performances.%20This%20method%20can%20be%20integrated%20with%0Aan%20arbitrary%20neural%20network%20model%20and%20is%20particularly%20beneficial%20for%20time%0Aseries%20data%20with%20spatial%20or%20geographical%20features.%20Furthermore%2C%20our%20findings%0Asuggest%20that%20well-preserved%20similarity%20structures%20in%20the%20latent%20space%20indicate%0Amore%20informative%20and%20useful%20representations.%20This%20provides%20insights%20to%20design%0Amore%20effective%20neural%20networks%20for%20data-driven%20transportation%20research.%20Our%0Acode%20is%20made%20openly%20accessible%20with%20all%20resulting%20data%20at%0Ahttps%3A//github.com/yiru-jiao/spclt%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06380v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-preserving%2520contrastive%2520learning%2520for%2520spatial%2520time%2520series%26entry.906535625%3DYiru%2520Jiao%2520and%2520Sander%2520van%2520Cranenburgh%2520and%2520Simeon%2520Calvert%2520and%2520Hans%2520van%2520Lint%26entry.1292438233%3D%2520%2520The%2520effectiveness%2520of%2520neural%2520network%2520models%2520largely%2520relies%2520on%2520learning%250Ameaningful%2520latent%2520patterns%2520from%2520data%252C%2520where%2520self-supervised%2520learning%2520of%250Ainformative%2520representations%2520can%2520enhance%2520model%2520performance%2520and%2520generalisability.%250AHowever%252C%2520self-supervised%2520representation%2520learning%2520for%2520spatially%2520characterised%250Atime%2520series%252C%2520which%2520are%2520ubiquitous%2520in%2520transportation%2520domain%252C%2520poses%2520unique%250Achallenges%2520due%2520to%2520the%2520necessity%2520of%2520maintaining%2520fine-grained%2520spatio-temporal%250Asimilarities%2520in%2520the%2520latent%2520space.%2520In%2520this%2520study%252C%2520we%2520introduce%2520two%250Astructure-preserving%2520regularisers%2520for%2520the%2520contrastive%2520learning%2520of%2520spatial%2520time%250Aseries%253A%2520one%2520regulariser%2520preserves%2520the%2520topology%2520of%2520similarities%2520between%250Ainstances%252C%2520and%2520the%2520other%2520preserves%2520the%2520graph%2520geometry%2520of%2520similarities%2520across%250Aspatial%2520and%2520temporal%2520dimensions.%2520To%2520balance%2520the%2520contrastive%2520learning%2520objective%250Aand%2520the%2520need%2520for%2520structure%2520preservation%252C%2520we%2520propose%2520a%2520dynamic%2520weighting%250Amechanism%2520that%2520adaptively%2520manages%2520this%2520trade-off%2520and%2520stabilises%2520training.%2520We%250Avalidate%2520the%2520proposed%2520method%2520through%2520extensive%2520experiments%252C%2520including%250Amultivariate%2520time%2520series%2520classification%2520to%2520demonstrate%2520its%2520general%250Aapplicability%252C%2520as%2520well%2520as%2520macroscopic%2520and%2520microscopic%2520traffic%2520prediction%2520to%250Ahighlight%2520its%2520particular%2520usefulness%2520in%2520encoding%2520traffic%2520interactions.%2520Across%250Aall%2520tasks%252C%2520our%2520method%2520preserves%2520the%2520similarity%2520structures%2520more%2520effectively%2520and%250Aimproves%2520state-of-the-art%2520task%2520performances.%2520This%2520method%2520can%2520be%2520integrated%2520with%250Aan%2520arbitrary%2520neural%2520network%2520model%2520and%2520is%2520particularly%2520beneficial%2520for%2520time%250Aseries%2520data%2520with%2520spatial%2520or%2520geographical%2520features.%2520Furthermore%252C%2520our%2520findings%250Asuggest%2520that%2520well-preserved%2520similarity%2520structures%2520in%2520the%2520latent%2520space%2520indicate%250Amore%2520informative%2520and%2520useful%2520representations.%2520This%2520provides%2520insights%2520to%2520design%250Amore%2520effective%2520neural%2520networks%2520for%2520data-driven%2520transportation%2520research.%2520Our%250Acode%2520is%2520made%2520openly%2520accessible%2520with%2520all%2520resulting%2520data%2520at%250Ahttps%253A//github.com/yiru-jiao/spclt%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06380v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series&entry.906535625=Yiru%20Jiao%20and%20Sander%20van%20Cranenburgh%20and%20Simeon%20Calvert%20and%20Hans%20van%20Lint&entry.1292438233=%20%20The%20effectiveness%20of%20neural%20network%20models%20largely%20relies%20on%20learning%0Ameaningful%20latent%20patterns%20from%20data%2C%20where%20self-supervised%20learning%20of%0Ainformative%20representations%20can%20enhance%20model%20performance%20and%20generalisability.%0AHowever%2C%20self-supervised%20representation%20learning%20for%20spatially%20characterised%0Atime%20series%2C%20which%20are%20ubiquitous%20in%20transportation%20domain%2C%20poses%20unique%0Achallenges%20due%20to%20the%20necessity%20of%20maintaining%20fine-grained%20spatio-temporal%0Asimilarities%20in%20the%20latent%20space.%20In%20this%20study%2C%20we%20introduce%20two%0Astructure-preserving%20regularisers%20for%20the%20contrastive%20learning%20of%20spatial%20time%0Aseries%3A%20one%20regulariser%20preserves%20the%20topology%20of%20similarities%20between%0Ainstances%2C%20and%20the%20other%20preserves%20the%20graph%20geometry%20of%20similarities%20across%0Aspatial%20and%20temporal%20dimensions.%20To%20balance%20the%20contrastive%20learning%20objective%0Aand%20the%20need%20for%20structure%20preservation%2C%20we%20propose%20a%20dynamic%20weighting%0Amechanism%20that%20adaptively%20manages%20this%20trade-off%20and%20stabilises%20training.%20We%0Avalidate%20the%20proposed%20method%20through%20extensive%20experiments%2C%20including%0Amultivariate%20time%20series%20classification%20to%20demonstrate%20its%20general%0Aapplicability%2C%20as%20well%20as%20macroscopic%20and%20microscopic%20traffic%20prediction%20to%0Ahighlight%20its%20particular%20usefulness%20in%20encoding%20traffic%20interactions.%20Across%0Aall%20tasks%2C%20our%20method%20preserves%20the%20similarity%20structures%20more%20effectively%20and%0Aimproves%20state-of-the-art%20task%20performances.%20This%20method%20can%20be%20integrated%20with%0Aan%20arbitrary%20neural%20network%20model%20and%20is%20particularly%20beneficial%20for%20time%0Aseries%20data%20with%20spatial%20or%20geographical%20features.%20Furthermore%2C%20our%20findings%0Asuggest%20that%20well-preserved%20similarity%20structures%20in%20the%20latent%20space%20indicate%0Amore%20informative%20and%20useful%20representations.%20This%20provides%20insights%20to%20design%0Amore%20effective%20neural%20networks%20for%20data-driven%20transportation%20research.%20Our%0Acode%20is%20made%20openly%20accessible%20with%20all%20resulting%20data%20at%0Ahttps%3A//github.com/yiru-jiao/spclt%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06380v4&entry.124074799=Read"},
{"title": "Domain Consistency Representation Learning for Lifelong Person\n  Re-Identification", "author": "Shiben Liu and Huijie Fan and Qiang Wang and Weihong Ren and Yandong Tang and Yang Cong", "abstract": "  Lifelong person re-identification (LReID) exhibits a contradictory\nrelationship between intra-domain discrimination and inter-domain gaps when\nlearning from continuous data. Intra-domain discrimination focuses on\nindividual nuances (i.e., clothing type, accessories, etc.), while inter-domain\ngaps emphasize domain consistency. Achieving a trade-off between maximizing\nintra-domain discrimination and minimizing inter-domain gaps is a crucial\nchallenge for improving LReID performance. Most existing methods strive to\nreduce inter-domain gaps through knowledge distillation to maintain domain\nconsistency. However, they often ignore intra-domain discrimination. To address\nthis challenge, we propose a novel domain consistency representation learning\n(DCR) model that explores global and attribute-wise representations as a bridge\nto balance intra-domain discrimination and inter-domain gaps. At the\nintra-domain level, we explore the complementary relationship between global\nand attribute-wise representations to improve discrimination among similar\nidentities. Excessive learning intra-domain discrimination can lead to\ncatastrophic forgetting. We further develop an attribute-oriented\nanti-forgetting (AF) strategy that explores attribute-wise representations to\nenhance inter-domain consistency, and propose a knowledge consolidation (KC)\nstrategy to facilitate knowledge transfer. Extensive experiments show that our\nDCR achieves superior performance compared to state-of-the-art LReID methods.\nOur code is available at https://github.com/LiuShiBen/DCR.\n", "link": "http://arxiv.org/abs/2409.19954v4", "date": "2025-09-03", "relevancy": 2.6595, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5354}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5304}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Consistency%20Representation%20Learning%20for%20Lifelong%20Person%0A%20%20Re-Identification&body=Title%3A%20Domain%20Consistency%20Representation%20Learning%20for%20Lifelong%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Shiben%20Liu%20and%20Huijie%20Fan%20and%20Qiang%20Wang%20and%20Weihong%20Ren%20and%20Yandong%20Tang%20and%20Yang%20Cong%0AAbstract%3A%20%20%20Lifelong%20person%20re-identification%20%28LReID%29%20exhibits%20a%20contradictory%0Arelationship%20between%20intra-domain%20discrimination%20and%20inter-domain%20gaps%20when%0Alearning%20from%20continuous%20data.%20Intra-domain%20discrimination%20focuses%20on%0Aindividual%20nuances%20%28i.e.%2C%20clothing%20type%2C%20accessories%2C%20etc.%29%2C%20while%20inter-domain%0Agaps%20emphasize%20domain%20consistency.%20Achieving%20a%20trade-off%20between%20maximizing%0Aintra-domain%20discrimination%20and%20minimizing%20inter-domain%20gaps%20is%20a%20crucial%0Achallenge%20for%20improving%20LReID%20performance.%20Most%20existing%20methods%20strive%20to%0Areduce%20inter-domain%20gaps%20through%20knowledge%20distillation%20to%20maintain%20domain%0Aconsistency.%20However%2C%20they%20often%20ignore%20intra-domain%20discrimination.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20novel%20domain%20consistency%20representation%20learning%0A%28DCR%29%20model%20that%20explores%20global%20and%20attribute-wise%20representations%20as%20a%20bridge%0Ato%20balance%20intra-domain%20discrimination%20and%20inter-domain%20gaps.%20At%20the%0Aintra-domain%20level%2C%20we%20explore%20the%20complementary%20relationship%20between%20global%0Aand%20attribute-wise%20representations%20to%20improve%20discrimination%20among%20similar%0Aidentities.%20Excessive%20learning%20intra-domain%20discrimination%20can%20lead%20to%0Acatastrophic%20forgetting.%20We%20further%20develop%20an%20attribute-oriented%0Aanti-forgetting%20%28AF%29%20strategy%20that%20explores%20attribute-wise%20representations%20to%0Aenhance%20inter-domain%20consistency%2C%20and%20propose%20a%20knowledge%20consolidation%20%28KC%29%0Astrategy%20to%20facilitate%20knowledge%20transfer.%20Extensive%20experiments%20show%20that%20our%0ADCR%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20LReID%20methods.%0AOur%20code%20is%20available%20at%20https%3A//github.com/LiuShiBen/DCR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19954v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Consistency%2520Representation%2520Learning%2520for%2520Lifelong%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DShiben%2520Liu%2520and%2520Huijie%2520Fan%2520and%2520Qiang%2520Wang%2520and%2520Weihong%2520Ren%2520and%2520Yandong%2520Tang%2520and%2520Yang%2520Cong%26entry.1292438233%3D%2520%2520Lifelong%2520person%2520re-identification%2520%2528LReID%2529%2520exhibits%2520a%2520contradictory%250Arelationship%2520between%2520intra-domain%2520discrimination%2520and%2520inter-domain%2520gaps%2520when%250Alearning%2520from%2520continuous%2520data.%2520Intra-domain%2520discrimination%2520focuses%2520on%250Aindividual%2520nuances%2520%2528i.e.%252C%2520clothing%2520type%252C%2520accessories%252C%2520etc.%2529%252C%2520while%2520inter-domain%250Agaps%2520emphasize%2520domain%2520consistency.%2520Achieving%2520a%2520trade-off%2520between%2520maximizing%250Aintra-domain%2520discrimination%2520and%2520minimizing%2520inter-domain%2520gaps%2520is%2520a%2520crucial%250Achallenge%2520for%2520improving%2520LReID%2520performance.%2520Most%2520existing%2520methods%2520strive%2520to%250Areduce%2520inter-domain%2520gaps%2520through%2520knowledge%2520distillation%2520to%2520maintain%2520domain%250Aconsistency.%2520However%252C%2520they%2520often%2520ignore%2520intra-domain%2520discrimination.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520domain%2520consistency%2520representation%2520learning%250A%2528DCR%2529%2520model%2520that%2520explores%2520global%2520and%2520attribute-wise%2520representations%2520as%2520a%2520bridge%250Ato%2520balance%2520intra-domain%2520discrimination%2520and%2520inter-domain%2520gaps.%2520At%2520the%250Aintra-domain%2520level%252C%2520we%2520explore%2520the%2520complementary%2520relationship%2520between%2520global%250Aand%2520attribute-wise%2520representations%2520to%2520improve%2520discrimination%2520among%2520similar%250Aidentities.%2520Excessive%2520learning%2520intra-domain%2520discrimination%2520can%2520lead%2520to%250Acatastrophic%2520forgetting.%2520We%2520further%2520develop%2520an%2520attribute-oriented%250Aanti-forgetting%2520%2528AF%2529%2520strategy%2520that%2520explores%2520attribute-wise%2520representations%2520to%250Aenhance%2520inter-domain%2520consistency%252C%2520and%2520propose%2520a%2520knowledge%2520consolidation%2520%2528KC%2529%250Astrategy%2520to%2520facilitate%2520knowledge%2520transfer.%2520Extensive%2520experiments%2520show%2520that%2520our%250ADCR%2520achieves%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520LReID%2520methods.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/LiuShiBen/DCR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19954v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Consistency%20Representation%20Learning%20for%20Lifelong%20Person%0A%20%20Re-Identification&entry.906535625=Shiben%20Liu%20and%20Huijie%20Fan%20and%20Qiang%20Wang%20and%20Weihong%20Ren%20and%20Yandong%20Tang%20and%20Yang%20Cong&entry.1292438233=%20%20Lifelong%20person%20re-identification%20%28LReID%29%20exhibits%20a%20contradictory%0Arelationship%20between%20intra-domain%20discrimination%20and%20inter-domain%20gaps%20when%0Alearning%20from%20continuous%20data.%20Intra-domain%20discrimination%20focuses%20on%0Aindividual%20nuances%20%28i.e.%2C%20clothing%20type%2C%20accessories%2C%20etc.%29%2C%20while%20inter-domain%0Agaps%20emphasize%20domain%20consistency.%20Achieving%20a%20trade-off%20between%20maximizing%0Aintra-domain%20discrimination%20and%20minimizing%20inter-domain%20gaps%20is%20a%20crucial%0Achallenge%20for%20improving%20LReID%20performance.%20Most%20existing%20methods%20strive%20to%0Areduce%20inter-domain%20gaps%20through%20knowledge%20distillation%20to%20maintain%20domain%0Aconsistency.%20However%2C%20they%20often%20ignore%20intra-domain%20discrimination.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20novel%20domain%20consistency%20representation%20learning%0A%28DCR%29%20model%20that%20explores%20global%20and%20attribute-wise%20representations%20as%20a%20bridge%0Ato%20balance%20intra-domain%20discrimination%20and%20inter-domain%20gaps.%20At%20the%0Aintra-domain%20level%2C%20we%20explore%20the%20complementary%20relationship%20between%20global%0Aand%20attribute-wise%20representations%20to%20improve%20discrimination%20among%20similar%0Aidentities.%20Excessive%20learning%20intra-domain%20discrimination%20can%20lead%20to%0Acatastrophic%20forgetting.%20We%20further%20develop%20an%20attribute-oriented%0Aanti-forgetting%20%28AF%29%20strategy%20that%20explores%20attribute-wise%20representations%20to%0Aenhance%20inter-domain%20consistency%2C%20and%20propose%20a%20knowledge%20consolidation%20%28KC%29%0Astrategy%20to%20facilitate%20knowledge%20transfer.%20Extensive%20experiments%20show%20that%20our%0ADCR%20achieves%20superior%20performance%20compared%20to%20state-of-the-art%20LReID%20methods.%0AOur%20code%20is%20available%20at%20https%3A//github.com/LiuShiBen/DCR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19954v4&entry.124074799=Read"},
{"title": "Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral\n  Unmixing", "author": "Hui Chen and Liangyu Liu and Xianchao Xiu and Wanquan Liu", "abstract": "  Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote\nsensing images into a set of endmembers and their corresponding abundances.\nDespite significant progress in this field using deep learning, most methods\nfail to simultaneously characterize global dependencies and local consistency,\nmaking it difficult to preserve both long-range interactions and boundary\ndetails. This letter proposes a novel transformer-guided content-adaptive graph\nunmixing framework (T-CAGU), which overcomes these challenges by employing a\ntransformer to capture global dependencies and introducing a content-adaptive\ngraph neural network to enhance local relationships. Unlike previous work,\nT-CAGU integrates multiple propagation orders to dynamically learn the graph\nstructure, ensuring robustness against noise. Furthermore, T-CAGU leverages a\ngraph residual mechanism to preserve global information and stabilize training.\nExperimental results demonstrate its superiority over the state-of-the-art\nmethods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.\n", "link": "http://arxiv.org/abs/2509.03376v1", "date": "2025-09-03", "relevancy": 2.6587, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5511}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5255}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-Guided%20Content-Adaptive%20Graph%20Learning%20for%20Hyperspectral%0A%20%20Unmixing&body=Title%3A%20Transformer-Guided%20Content-Adaptive%20Graph%20Learning%20for%20Hyperspectral%0A%20%20Unmixing%0AAuthor%3A%20Hui%20Chen%20and%20Liangyu%20Liu%20and%20Xianchao%20Xiu%20and%20Wanquan%20Liu%0AAbstract%3A%20%20%20Hyperspectral%20unmixing%20%28HU%29%20targets%20to%20decompose%20each%20mixed%20pixel%20in%20remote%0Asensing%20images%20into%20a%20set%20of%20endmembers%20and%20their%20corresponding%20abundances.%0ADespite%20significant%20progress%20in%20this%20field%20using%20deep%20learning%2C%20most%20methods%0Afail%20to%20simultaneously%20characterize%20global%20dependencies%20and%20local%20consistency%2C%0Amaking%20it%20difficult%20to%20preserve%20both%20long-range%20interactions%20and%20boundary%0Adetails.%20This%20letter%20proposes%20a%20novel%20transformer-guided%20content-adaptive%20graph%0Aunmixing%20framework%20%28T-CAGU%29%2C%20which%20overcomes%20these%20challenges%20by%20employing%20a%0Atransformer%20to%20capture%20global%20dependencies%20and%20introducing%20a%20content-adaptive%0Agraph%20neural%20network%20to%20enhance%20local%20relationships.%20Unlike%20previous%20work%2C%0AT-CAGU%20integrates%20multiple%20propagation%20orders%20to%20dynamically%20learn%20the%20graph%0Astructure%2C%20ensuring%20robustness%20against%20noise.%20Furthermore%2C%20T-CAGU%20leverages%20a%0Agraph%20residual%20mechanism%20to%20preserve%20global%20information%20and%20stabilize%20training.%0AExperimental%20results%20demonstrate%20its%20superiority%20over%20the%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/xianchaoxiu/T-CAGU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-Guided%2520Content-Adaptive%2520Graph%2520Learning%2520for%2520Hyperspectral%250A%2520%2520Unmixing%26entry.906535625%3DHui%2520Chen%2520and%2520Liangyu%2520Liu%2520and%2520Xianchao%2520Xiu%2520and%2520Wanquan%2520Liu%26entry.1292438233%3D%2520%2520Hyperspectral%2520unmixing%2520%2528HU%2529%2520targets%2520to%2520decompose%2520each%2520mixed%2520pixel%2520in%2520remote%250Asensing%2520images%2520into%2520a%2520set%2520of%2520endmembers%2520and%2520their%2520corresponding%2520abundances.%250ADespite%2520significant%2520progress%2520in%2520this%2520field%2520using%2520deep%2520learning%252C%2520most%2520methods%250Afail%2520to%2520simultaneously%2520characterize%2520global%2520dependencies%2520and%2520local%2520consistency%252C%250Amaking%2520it%2520difficult%2520to%2520preserve%2520both%2520long-range%2520interactions%2520and%2520boundary%250Adetails.%2520This%2520letter%2520proposes%2520a%2520novel%2520transformer-guided%2520content-adaptive%2520graph%250Aunmixing%2520framework%2520%2528T-CAGU%2529%252C%2520which%2520overcomes%2520these%2520challenges%2520by%2520employing%2520a%250Atransformer%2520to%2520capture%2520global%2520dependencies%2520and%2520introducing%2520a%2520content-adaptive%250Agraph%2520neural%2520network%2520to%2520enhance%2520local%2520relationships.%2520Unlike%2520previous%2520work%252C%250AT-CAGU%2520integrates%2520multiple%2520propagation%2520orders%2520to%2520dynamically%2520learn%2520the%2520graph%250Astructure%252C%2520ensuring%2520robustness%2520against%2520noise.%2520Furthermore%252C%2520T-CAGU%2520leverages%2520a%250Agraph%2520residual%2520mechanism%2520to%2520preserve%2520global%2520information%2520and%2520stabilize%2520training.%250AExperimental%2520results%2520demonstrate%2520its%2520superiority%2520over%2520the%2520state-of-the-art%250Amethods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/xianchaoxiu/T-CAGU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-Guided%20Content-Adaptive%20Graph%20Learning%20for%20Hyperspectral%0A%20%20Unmixing&entry.906535625=Hui%20Chen%20and%20Liangyu%20Liu%20and%20Xianchao%20Xiu%20and%20Wanquan%20Liu&entry.1292438233=%20%20Hyperspectral%20unmixing%20%28HU%29%20targets%20to%20decompose%20each%20mixed%20pixel%20in%20remote%0Asensing%20images%20into%20a%20set%20of%20endmembers%20and%20their%20corresponding%20abundances.%0ADespite%20significant%20progress%20in%20this%20field%20using%20deep%20learning%2C%20most%20methods%0Afail%20to%20simultaneously%20characterize%20global%20dependencies%20and%20local%20consistency%2C%0Amaking%20it%20difficult%20to%20preserve%20both%20long-range%20interactions%20and%20boundary%0Adetails.%20This%20letter%20proposes%20a%20novel%20transformer-guided%20content-adaptive%20graph%0Aunmixing%20framework%20%28T-CAGU%29%2C%20which%20overcomes%20these%20challenges%20by%20employing%20a%0Atransformer%20to%20capture%20global%20dependencies%20and%20introducing%20a%20content-adaptive%0Agraph%20neural%20network%20to%20enhance%20local%20relationships.%20Unlike%20previous%20work%2C%0AT-CAGU%20integrates%20multiple%20propagation%20orders%20to%20dynamically%20learn%20the%20graph%0Astructure%2C%20ensuring%20robustness%20against%20noise.%20Furthermore%2C%20T-CAGU%20leverages%20a%0Agraph%20residual%20mechanism%20to%20preserve%20global%20information%20and%20stabilize%20training.%0AExperimental%20results%20demonstrate%20its%20superiority%20over%20the%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/xianchaoxiu/T-CAGU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03376v1&entry.124074799=Read"},
{"title": "Real-Time Per-Garment Virtual Try-On with Temporal Consistency for\n  Loose-Fitting Garments", "author": "Zaiqiang Wu and I-Chao Shen and Takeo Igarashi", "abstract": "  Per-garment virtual try-on methods collect garment-specific datasets and\ntrain networks tailored to each garment to achieve superior results. However,\nthese approaches often struggle with loose-fitting garments due to two key\nlimitations: (1) They rely on human body semantic maps to align garments with\nthe body, but these maps become unreliable when body contours are obscured by\nloose-fitting garments, resulting in degraded outcomes; (2) They train garment\nsynthesis networks on a per-frame basis without utilizing temporal information,\nleading to noticeable jittering artifacts. To address the first limitation, we\npropose a two-stage approach for robust semantic map estimation. First, we\nextract a garment-invariant representation from the raw input image. This\nrepresentation is then passed through an auxiliary network to estimate the\nsemantic map. This enhances the robustness of semantic map estimation under\nloose-fitting garments during garment-specific dataset generation. To address\nthe second limitation, we introduce a recurrent garment synthesis framework\nthat incorporates temporal dependencies to improve frame-to-frame coherence\nwhile maintaining real-time performance. We conducted qualitative and\nquantitative evaluations to demonstrate that our method outperforms existing\napproaches in both image quality and temporal coherence. Ablation studies\nfurther validate the effectiveness of the garment-invariant representation and\nthe recurrent synthesis framework.\n", "link": "http://arxiv.org/abs/2506.12348v2", "date": "2025-09-03", "relevancy": 2.6267, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6715}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6555}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Per-Garment%20Virtual%20Try-On%20with%20Temporal%20Consistency%20for%0A%20%20Loose-Fitting%20Garments&body=Title%3A%20Real-Time%20Per-Garment%20Virtual%20Try-On%20with%20Temporal%20Consistency%20for%0A%20%20Loose-Fitting%20Garments%0AAuthor%3A%20Zaiqiang%20Wu%20and%20I-Chao%20Shen%20and%20Takeo%20Igarashi%0AAbstract%3A%20%20%20Per-garment%20virtual%20try-on%20methods%20collect%20garment-specific%20datasets%20and%0Atrain%20networks%20tailored%20to%20each%20garment%20to%20achieve%20superior%20results.%20However%2C%0Athese%20approaches%20often%20struggle%20with%20loose-fitting%20garments%20due%20to%20two%20key%0Alimitations%3A%20%281%29%20They%20rely%20on%20human%20body%20semantic%20maps%20to%20align%20garments%20with%0Athe%20body%2C%20but%20these%20maps%20become%20unreliable%20when%20body%20contours%20are%20obscured%20by%0Aloose-fitting%20garments%2C%20resulting%20in%20degraded%20outcomes%3B%20%282%29%20They%20train%20garment%0Asynthesis%20networks%20on%20a%20per-frame%20basis%20without%20utilizing%20temporal%20information%2C%0Aleading%20to%20noticeable%20jittering%20artifacts.%20To%20address%20the%20first%20limitation%2C%20we%0Apropose%20a%20two-stage%20approach%20for%20robust%20semantic%20map%20estimation.%20First%2C%20we%0Aextract%20a%20garment-invariant%20representation%20from%20the%20raw%20input%20image.%20This%0Arepresentation%20is%20then%20passed%20through%20an%20auxiliary%20network%20to%20estimate%20the%0Asemantic%20map.%20This%20enhances%20the%20robustness%20of%20semantic%20map%20estimation%20under%0Aloose-fitting%20garments%20during%20garment-specific%20dataset%20generation.%20To%20address%0Athe%20second%20limitation%2C%20we%20introduce%20a%20recurrent%20garment%20synthesis%20framework%0Athat%20incorporates%20temporal%20dependencies%20to%20improve%20frame-to-frame%20coherence%0Awhile%20maintaining%20real-time%20performance.%20We%20conducted%20qualitative%20and%0Aquantitative%20evaluations%20to%20demonstrate%20that%20our%20method%20outperforms%20existing%0Aapproaches%20in%20both%20image%20quality%20and%20temporal%20coherence.%20Ablation%20studies%0Afurther%20validate%20the%20effectiveness%20of%20the%20garment-invariant%20representation%20and%0Athe%20recurrent%20synthesis%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12348v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Per-Garment%2520Virtual%2520Try-On%2520with%2520Temporal%2520Consistency%2520for%250A%2520%2520Loose-Fitting%2520Garments%26entry.906535625%3DZaiqiang%2520Wu%2520and%2520I-Chao%2520Shen%2520and%2520Takeo%2520Igarashi%26entry.1292438233%3D%2520%2520Per-garment%2520virtual%2520try-on%2520methods%2520collect%2520garment-specific%2520datasets%2520and%250Atrain%2520networks%2520tailored%2520to%2520each%2520garment%2520to%2520achieve%2520superior%2520results.%2520However%252C%250Athese%2520approaches%2520often%2520struggle%2520with%2520loose-fitting%2520garments%2520due%2520to%2520two%2520key%250Alimitations%253A%2520%25281%2529%2520They%2520rely%2520on%2520human%2520body%2520semantic%2520maps%2520to%2520align%2520garments%2520with%250Athe%2520body%252C%2520but%2520these%2520maps%2520become%2520unreliable%2520when%2520body%2520contours%2520are%2520obscured%2520by%250Aloose-fitting%2520garments%252C%2520resulting%2520in%2520degraded%2520outcomes%253B%2520%25282%2529%2520They%2520train%2520garment%250Asynthesis%2520networks%2520on%2520a%2520per-frame%2520basis%2520without%2520utilizing%2520temporal%2520information%252C%250Aleading%2520to%2520noticeable%2520jittering%2520artifacts.%2520To%2520address%2520the%2520first%2520limitation%252C%2520we%250Apropose%2520a%2520two-stage%2520approach%2520for%2520robust%2520semantic%2520map%2520estimation.%2520First%252C%2520we%250Aextract%2520a%2520garment-invariant%2520representation%2520from%2520the%2520raw%2520input%2520image.%2520This%250Arepresentation%2520is%2520then%2520passed%2520through%2520an%2520auxiliary%2520network%2520to%2520estimate%2520the%250Asemantic%2520map.%2520This%2520enhances%2520the%2520robustness%2520of%2520semantic%2520map%2520estimation%2520under%250Aloose-fitting%2520garments%2520during%2520garment-specific%2520dataset%2520generation.%2520To%2520address%250Athe%2520second%2520limitation%252C%2520we%2520introduce%2520a%2520recurrent%2520garment%2520synthesis%2520framework%250Athat%2520incorporates%2520temporal%2520dependencies%2520to%2520improve%2520frame-to-frame%2520coherence%250Awhile%2520maintaining%2520real-time%2520performance.%2520We%2520conducted%2520qualitative%2520and%250Aquantitative%2520evaluations%2520to%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%250Aapproaches%2520in%2520both%2520image%2520quality%2520and%2520temporal%2520coherence.%2520Ablation%2520studies%250Afurther%2520validate%2520the%2520effectiveness%2520of%2520the%2520garment-invariant%2520representation%2520and%250Athe%2520recurrent%2520synthesis%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12348v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Per-Garment%20Virtual%20Try-On%20with%20Temporal%20Consistency%20for%0A%20%20Loose-Fitting%20Garments&entry.906535625=Zaiqiang%20Wu%20and%20I-Chao%20Shen%20and%20Takeo%20Igarashi&entry.1292438233=%20%20Per-garment%20virtual%20try-on%20methods%20collect%20garment-specific%20datasets%20and%0Atrain%20networks%20tailored%20to%20each%20garment%20to%20achieve%20superior%20results.%20However%2C%0Athese%20approaches%20often%20struggle%20with%20loose-fitting%20garments%20due%20to%20two%20key%0Alimitations%3A%20%281%29%20They%20rely%20on%20human%20body%20semantic%20maps%20to%20align%20garments%20with%0Athe%20body%2C%20but%20these%20maps%20become%20unreliable%20when%20body%20contours%20are%20obscured%20by%0Aloose-fitting%20garments%2C%20resulting%20in%20degraded%20outcomes%3B%20%282%29%20They%20train%20garment%0Asynthesis%20networks%20on%20a%20per-frame%20basis%20without%20utilizing%20temporal%20information%2C%0Aleading%20to%20noticeable%20jittering%20artifacts.%20To%20address%20the%20first%20limitation%2C%20we%0Apropose%20a%20two-stage%20approach%20for%20robust%20semantic%20map%20estimation.%20First%2C%20we%0Aextract%20a%20garment-invariant%20representation%20from%20the%20raw%20input%20image.%20This%0Arepresentation%20is%20then%20passed%20through%20an%20auxiliary%20network%20to%20estimate%20the%0Asemantic%20map.%20This%20enhances%20the%20robustness%20of%20semantic%20map%20estimation%20under%0Aloose-fitting%20garments%20during%20garment-specific%20dataset%20generation.%20To%20address%0Athe%20second%20limitation%2C%20we%20introduce%20a%20recurrent%20garment%20synthesis%20framework%0Athat%20incorporates%20temporal%20dependencies%20to%20improve%20frame-to-frame%20coherence%0Awhile%20maintaining%20real-time%20performance.%20We%20conducted%20qualitative%20and%0Aquantitative%20evaluations%20to%20demonstrate%20that%20our%20method%20outperforms%20existing%0Aapproaches%20in%20both%20image%20quality%20and%20temporal%20coherence.%20Ablation%20studies%0Afurther%20validate%20the%20effectiveness%20of%20the%20garment-invariant%20representation%20and%0Athe%20recurrent%20synthesis%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12348v2&entry.124074799=Read"},
{"title": "ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain\n  Incremental Learning in CLIP", "author": "Zhiyuan Wang and Bokui Chen", "abstract": "  Continual learning (CL) empowers pre-trained vision-language models to adapt\neffectively to novel or previously underrepresented data distributions without\ncomprehensive retraining, enhancing their adaptability and efficiency. While\nvision-language models like CLIP show great promise, they struggle to maintain\nperformance across domains in incremental learning scenarios. Existing prompt\nlearning methods face two main limitations: 1) they primarily focus on\nclass-incremental learning scenarios, lacking specific strategies for\nmulti-domain task incremental learning; 2) most current approaches employ\nsingle-modal prompts, neglecting the potential benefits of cross-modal\ninformation exchange. To address these challenges, we propose the \\ChordPrompt\nframework, which facilitates a harmonious interplay between visual and textual\nprompts. \\ChordPrompt introduces cross-modal prompts to leverage interactions\nbetween visual and textual information. Our approach also employs\ndomain-adaptive text prompts to select appropriate prompts for continual\nadaptation across multiple domains. Comprehensive experiments on multi-domain\nincremental learning benchmarks demonstrate that \\ChordPrompt outperforms\nstate-of-the-art methods in zero-shot generalization and downstream task\nperformance.\n", "link": "http://arxiv.org/abs/2506.19608v2", "date": "2025-09-03", "relevancy": 2.622, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChordPrompt%3A%20Orchestrating%20Cross-Modal%20Prompt%20Synergy%20for%20Multi-Domain%0A%20%20Incremental%20Learning%20in%20CLIP&body=Title%3A%20ChordPrompt%3A%20Orchestrating%20Cross-Modal%20Prompt%20Synergy%20for%20Multi-Domain%0A%20%20Incremental%20Learning%20in%20CLIP%0AAuthor%3A%20Zhiyuan%20Wang%20and%20Bokui%20Chen%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20empowers%20pre-trained%20vision-language%20models%20to%20adapt%0Aeffectively%20to%20novel%20or%20previously%20underrepresented%20data%20distributions%20without%0Acomprehensive%20retraining%2C%20enhancing%20their%20adaptability%20and%20efficiency.%20While%0Avision-language%20models%20like%20CLIP%20show%20great%20promise%2C%20they%20struggle%20to%20maintain%0Aperformance%20across%20domains%20in%20incremental%20learning%20scenarios.%20Existing%20prompt%0Alearning%20methods%20face%20two%20main%20limitations%3A%201%29%20they%20primarily%20focus%20on%0Aclass-incremental%20learning%20scenarios%2C%20lacking%20specific%20strategies%20for%0Amulti-domain%20task%20incremental%20learning%3B%202%29%20most%20current%20approaches%20employ%0Asingle-modal%20prompts%2C%20neglecting%20the%20potential%20benefits%20of%20cross-modal%0Ainformation%20exchange.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20%5CChordPrompt%0Aframework%2C%20which%20facilitates%20a%20harmonious%20interplay%20between%20visual%20and%20textual%0Aprompts.%20%5CChordPrompt%20introduces%20cross-modal%20prompts%20to%20leverage%20interactions%0Abetween%20visual%20and%20textual%20information.%20Our%20approach%20also%20employs%0Adomain-adaptive%20text%20prompts%20to%20select%20appropriate%20prompts%20for%20continual%0Aadaptation%20across%20multiple%20domains.%20Comprehensive%20experiments%20on%20multi-domain%0Aincremental%20learning%20benchmarks%20demonstrate%20that%20%5CChordPrompt%20outperforms%0Astate-of-the-art%20methods%20in%20zero-shot%20generalization%20and%20downstream%20task%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19608v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChordPrompt%253A%2520Orchestrating%2520Cross-Modal%2520Prompt%2520Synergy%2520for%2520Multi-Domain%250A%2520%2520Incremental%2520Learning%2520in%2520CLIP%26entry.906535625%3DZhiyuan%2520Wang%2520and%2520Bokui%2520Chen%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520empowers%2520pre-trained%2520vision-language%2520models%2520to%2520adapt%250Aeffectively%2520to%2520novel%2520or%2520previously%2520underrepresented%2520data%2520distributions%2520without%250Acomprehensive%2520retraining%252C%2520enhancing%2520their%2520adaptability%2520and%2520efficiency.%2520While%250Avision-language%2520models%2520like%2520CLIP%2520show%2520great%2520promise%252C%2520they%2520struggle%2520to%2520maintain%250Aperformance%2520across%2520domains%2520in%2520incremental%2520learning%2520scenarios.%2520Existing%2520prompt%250Alearning%2520methods%2520face%2520two%2520main%2520limitations%253A%25201%2529%2520they%2520primarily%2520focus%2520on%250Aclass-incremental%2520learning%2520scenarios%252C%2520lacking%2520specific%2520strategies%2520for%250Amulti-domain%2520task%2520incremental%2520learning%253B%25202%2529%2520most%2520current%2520approaches%2520employ%250Asingle-modal%2520prompts%252C%2520neglecting%2520the%2520potential%2520benefits%2520of%2520cross-modal%250Ainformation%2520exchange.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520%255CChordPrompt%250Aframework%252C%2520which%2520facilitates%2520a%2520harmonious%2520interplay%2520between%2520visual%2520and%2520textual%250Aprompts.%2520%255CChordPrompt%2520introduces%2520cross-modal%2520prompts%2520to%2520leverage%2520interactions%250Abetween%2520visual%2520and%2520textual%2520information.%2520Our%2520approach%2520also%2520employs%250Adomain-adaptive%2520text%2520prompts%2520to%2520select%2520appropriate%2520prompts%2520for%2520continual%250Aadaptation%2520across%2520multiple%2520domains.%2520Comprehensive%2520experiments%2520on%2520multi-domain%250Aincremental%2520learning%2520benchmarks%2520demonstrate%2520that%2520%255CChordPrompt%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520zero-shot%2520generalization%2520and%2520downstream%2520task%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19608v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChordPrompt%3A%20Orchestrating%20Cross-Modal%20Prompt%20Synergy%20for%20Multi-Domain%0A%20%20Incremental%20Learning%20in%20CLIP&entry.906535625=Zhiyuan%20Wang%20and%20Bokui%20Chen&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20empowers%20pre-trained%20vision-language%20models%20to%20adapt%0Aeffectively%20to%20novel%20or%20previously%20underrepresented%20data%20distributions%20without%0Acomprehensive%20retraining%2C%20enhancing%20their%20adaptability%20and%20efficiency.%20While%0Avision-language%20models%20like%20CLIP%20show%20great%20promise%2C%20they%20struggle%20to%20maintain%0Aperformance%20across%20domains%20in%20incremental%20learning%20scenarios.%20Existing%20prompt%0Alearning%20methods%20face%20two%20main%20limitations%3A%201%29%20they%20primarily%20focus%20on%0Aclass-incremental%20learning%20scenarios%2C%20lacking%20specific%20strategies%20for%0Amulti-domain%20task%20incremental%20learning%3B%202%29%20most%20current%20approaches%20employ%0Asingle-modal%20prompts%2C%20neglecting%20the%20potential%20benefits%20of%20cross-modal%0Ainformation%20exchange.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20%5CChordPrompt%0Aframework%2C%20which%20facilitates%20a%20harmonious%20interplay%20between%20visual%20and%20textual%0Aprompts.%20%5CChordPrompt%20introduces%20cross-modal%20prompts%20to%20leverage%20interactions%0Abetween%20visual%20and%20textual%20information.%20Our%20approach%20also%20employs%0Adomain-adaptive%20text%20prompts%20to%20select%20appropriate%20prompts%20for%20continual%0Aadaptation%20across%20multiple%20domains.%20Comprehensive%20experiments%20on%20multi-domain%0Aincremental%20learning%20benchmarks%20demonstrate%20that%20%5CChordPrompt%20outperforms%0Astate-of-the-art%20methods%20in%20zero-shot%20generalization%20and%20downstream%20task%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19608v2&entry.124074799=Read"},
{"title": "A theoretical framework for self-supervised contrastive learning for\n  continuous dependent data", "author": "Alexander Marusov and Aleksandr Yugay and Alexey Zaytsev", "abstract": "  Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.\n", "link": "http://arxiv.org/abs/2506.09785v3", "date": "2025-09-03", "relevancy": 2.5932, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5293}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data&body=Title%3A%20A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data%0AAuthor%3A%20Alexander%20Marusov%20and%20Aleksandr%20Yugay%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20learning%0Arepresentations%2C%20particularly%20in%20the%20field%20of%20computer%20vision.%20However%2C%20its%0Aapplication%20to%20dependent%20data%2C%20such%20as%20temporal%20and%20spatio-temporal%20domains%2C%0Aremains%20underexplored.%20Besides%2C%20traditional%20contrastive%20SSL%20methods%20often%0Aassume%20%5Cemph%7Bsemantic%20independence%20between%20samples%7D%2C%20which%20does%20not%20hold%20for%0Adependent%20data%20exhibiting%20complex%20correlations.%20We%20propose%20a%20novel%20theoretical%0Aframework%20for%20contrastive%20SSL%20tailored%20to%20%5Cemph%7Bcontinuous%20dependent%20data%7D%2C%0Awhich%20allows%20the%20nearest%20samples%20to%20be%20semantically%20close%20to%20each%20other.%20In%0Aparticular%2C%20we%20propose%20two%20possible%20%5Ctextit%7Bground%20truth%20similarity%20measures%7D%0Abetween%20objects%20--%20%5Cemph%7Bhard%7D%20and%20%5Cemph%7Bsoft%7D%20closeness.%20Under%20it%2C%20we%20derive%0Aan%20analytical%20form%20for%20the%20%5Ctextit%7Bestimated%20similarity%20matrix%7D%20that%0Aaccommodates%20both%20types%20of%20closeness%20between%20samples%2C%20thereby%20introducing%0Adependency-aware%20loss%20functions.%20We%20validate%20our%20approach%2C%20%5Cemph%7BDependent%0ATS2Vec%7D%2C%20on%20temporal%20and%20spatio-temporal%20downstream%20problems.%20Given%20the%0Adependency%20patterns%20presented%20in%20the%20data%2C%20our%20approach%20surpasses%20modern%20ones%0Afor%20dependent%20data%2C%20highlighting%20the%20effectiveness%20of%20our%20theoretically%0Agrounded%20loss%20functions%20for%20SSL%20in%20capturing%20spatio-temporal%20dependencies.%0ASpecifically%2C%20we%20outperform%20TS2Vec%20on%20the%20standard%20UEA%20and%20UCR%20benchmarks%2C%20with%0Aaccuracy%20improvements%20of%20%244.17%24%5C%25%20and%20%242.08%24%5C%25%2C%20respectively.%20Furthermore%2C%20on%0Athe%20drought%20classification%20task%2C%20which%20involves%20complex%20spatio-temporal%0Apatterns%2C%20our%20method%20achieves%20a%20%247%24%5C%25%20higher%20ROC-AUC%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09785v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520theoretical%2520framework%2520for%2520self-supervised%2520contrastive%2520learning%2520for%250A%2520%2520continuous%2520dependent%2520data%26entry.906535625%3DAlexander%2520Marusov%2520and%2520Aleksandr%2520Yugay%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520approach%2520to%2520learning%250Arepresentations%252C%2520particularly%2520in%2520the%2520field%2520of%2520computer%2520vision.%2520However%252C%2520its%250Aapplication%2520to%2520dependent%2520data%252C%2520such%2520as%2520temporal%2520and%2520spatio-temporal%2520domains%252C%250Aremains%2520underexplored.%2520Besides%252C%2520traditional%2520contrastive%2520SSL%2520methods%2520often%250Aassume%2520%255Cemph%257Bsemantic%2520independence%2520between%2520samples%257D%252C%2520which%2520does%2520not%2520hold%2520for%250Adependent%2520data%2520exhibiting%2520complex%2520correlations.%2520We%2520propose%2520a%2520novel%2520theoretical%250Aframework%2520for%2520contrastive%2520SSL%2520tailored%2520to%2520%255Cemph%257Bcontinuous%2520dependent%2520data%257D%252C%250Awhich%2520allows%2520the%2520nearest%2520samples%2520to%2520be%2520semantically%2520close%2520to%2520each%2520other.%2520In%250Aparticular%252C%2520we%2520propose%2520two%2520possible%2520%255Ctextit%257Bground%2520truth%2520similarity%2520measures%257D%250Abetween%2520objects%2520--%2520%255Cemph%257Bhard%257D%2520and%2520%255Cemph%257Bsoft%257D%2520closeness.%2520Under%2520it%252C%2520we%2520derive%250Aan%2520analytical%2520form%2520for%2520the%2520%255Ctextit%257Bestimated%2520similarity%2520matrix%257D%2520that%250Aaccommodates%2520both%2520types%2520of%2520closeness%2520between%2520samples%252C%2520thereby%2520introducing%250Adependency-aware%2520loss%2520functions.%2520We%2520validate%2520our%2520approach%252C%2520%255Cemph%257BDependent%250ATS2Vec%257D%252C%2520on%2520temporal%2520and%2520spatio-temporal%2520downstream%2520problems.%2520Given%2520the%250Adependency%2520patterns%2520presented%2520in%2520the%2520data%252C%2520our%2520approach%2520surpasses%2520modern%2520ones%250Afor%2520dependent%2520data%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520theoretically%250Agrounded%2520loss%2520functions%2520for%2520SSL%2520in%2520capturing%2520spatio-temporal%2520dependencies.%250ASpecifically%252C%2520we%2520outperform%2520TS2Vec%2520on%2520the%2520standard%2520UEA%2520and%2520UCR%2520benchmarks%252C%2520with%250Aaccuracy%2520improvements%2520of%2520%25244.17%2524%255C%2525%2520and%2520%25242.08%2524%255C%2525%252C%2520respectively.%2520Furthermore%252C%2520on%250Athe%2520drought%2520classification%2520task%252C%2520which%2520involves%2520complex%2520spatio-temporal%250Apatterns%252C%2520our%2520method%2520achieves%2520a%2520%25247%2524%255C%2525%2520higher%2520ROC-AUC%2520score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09785v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20theoretical%20framework%20for%20self-supervised%20contrastive%20learning%20for%0A%20%20continuous%20dependent%20data&entry.906535625=Alexander%20Marusov%20and%20Aleksandr%20Yugay%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20learning%0Arepresentations%2C%20particularly%20in%20the%20field%20of%20computer%20vision.%20However%2C%20its%0Aapplication%20to%20dependent%20data%2C%20such%20as%20temporal%20and%20spatio-temporal%20domains%2C%0Aremains%20underexplored.%20Besides%2C%20traditional%20contrastive%20SSL%20methods%20often%0Aassume%20%5Cemph%7Bsemantic%20independence%20between%20samples%7D%2C%20which%20does%20not%20hold%20for%0Adependent%20data%20exhibiting%20complex%20correlations.%20We%20propose%20a%20novel%20theoretical%0Aframework%20for%20contrastive%20SSL%20tailored%20to%20%5Cemph%7Bcontinuous%20dependent%20data%7D%2C%0Awhich%20allows%20the%20nearest%20samples%20to%20be%20semantically%20close%20to%20each%20other.%20In%0Aparticular%2C%20we%20propose%20two%20possible%20%5Ctextit%7Bground%20truth%20similarity%20measures%7D%0Abetween%20objects%20--%20%5Cemph%7Bhard%7D%20and%20%5Cemph%7Bsoft%7D%20closeness.%20Under%20it%2C%20we%20derive%0Aan%20analytical%20form%20for%20the%20%5Ctextit%7Bestimated%20similarity%20matrix%7D%20that%0Aaccommodates%20both%20types%20of%20closeness%20between%20samples%2C%20thereby%20introducing%0Adependency-aware%20loss%20functions.%20We%20validate%20our%20approach%2C%20%5Cemph%7BDependent%0ATS2Vec%7D%2C%20on%20temporal%20and%20spatio-temporal%20downstream%20problems.%20Given%20the%0Adependency%20patterns%20presented%20in%20the%20data%2C%20our%20approach%20surpasses%20modern%20ones%0Afor%20dependent%20data%2C%20highlighting%20the%20effectiveness%20of%20our%20theoretically%0Agrounded%20loss%20functions%20for%20SSL%20in%20capturing%20spatio-temporal%20dependencies.%0ASpecifically%2C%20we%20outperform%20TS2Vec%20on%20the%20standard%20UEA%20and%20UCR%20benchmarks%2C%20with%0Aaccuracy%20improvements%20of%20%244.17%24%5C%25%20and%20%242.08%24%5C%25%2C%20respectively.%20Furthermore%2C%20on%0Athe%20drought%20classification%20task%2C%20which%20involves%20complex%20spatio-temporal%0Apatterns%2C%20our%20method%20achieves%20a%20%247%24%5C%25%20higher%20ROC-AUC%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09785v3&entry.124074799=Read"},
{"title": "TopoMap: A Feature-based Semantic Discriminator of the Topographical\n  Regions in the Test Input Space", "author": "Gianmarco De Vita and Nargiz Humbatova and Paolo Tonella", "abstract": "  Testing Deep Learning (DL)-based systems is an open challenge. Although it is\nrelatively easy to find inputs that cause a DL model to misbehave, the grouping\nof inputs by features that make the DL model under test fail is largely\nunexplored. Existing approaches for DL testing introduce perturbations that may\nfocus on specific failure-inducing features, while neglecting others that\nbelong to different regions of the feature space. In this paper, we create an\nexplicit topographical map of the input feature space. Our approach, named\nTopoMap, is both black-box and model-agnostic as it relies solely on features\nthat characterise the input space. To discriminate the inputs according to the\nspecific features they share, we first apply dimensionality reduction to obtain\ninput embeddings, which are then subjected to clustering. Each DL model might\nrequire specific embedding computations and clustering algorithms to achieve a\nmeaningful separation of inputs into discriminative groups. We propose a novel\nway to evaluate alternative configurations of embedding and clustering\ntechniques. We used a deep neural network (DNN) as an approximation of a human\nevaluator who could tell whether a pair of clusters can be discriminated based\non the features of the included elements. We use such a DNN to automatically\nselect the optimal topographical map of the inputs among all those that are\nproduced by different embedding/clustering configurations. The evaluation\nresults show that the maps generated by TopoMap consist of distinguishable and\nmeaningful regions. In addition, we evaluate the effectiveness of TopoMap using\nmutation analysis. In particular, we assess whether the clusters in our\ntopographical map allow for an effective selection of mutation-killing inputs.\nExperimental results show that our approach outperforms random selection by 35%\non average on killable mutants; by 61% on non-killable ones.\n", "link": "http://arxiv.org/abs/2509.03242v1", "date": "2025-09-03", "relevancy": 2.5905, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5324}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoMap%3A%20A%20Feature-based%20Semantic%20Discriminator%20of%20the%20Topographical%0A%20%20Regions%20in%20the%20Test%20Input%20Space&body=Title%3A%20TopoMap%3A%20A%20Feature-based%20Semantic%20Discriminator%20of%20the%20Topographical%0A%20%20Regions%20in%20the%20Test%20Input%20Space%0AAuthor%3A%20Gianmarco%20De%20Vita%20and%20Nargiz%20Humbatova%20and%20Paolo%20Tonella%0AAbstract%3A%20%20%20Testing%20Deep%20Learning%20%28DL%29-based%20systems%20is%20an%20open%20challenge.%20Although%20it%20is%0Arelatively%20easy%20to%20find%20inputs%20that%20cause%20a%20DL%20model%20to%20misbehave%2C%20the%20grouping%0Aof%20inputs%20by%20features%20that%20make%20the%20DL%20model%20under%20test%20fail%20is%20largely%0Aunexplored.%20Existing%20approaches%20for%20DL%20testing%20introduce%20perturbations%20that%20may%0Afocus%20on%20specific%20failure-inducing%20features%2C%20while%20neglecting%20others%20that%0Abelong%20to%20different%20regions%20of%20the%20feature%20space.%20In%20this%20paper%2C%20we%20create%20an%0Aexplicit%20topographical%20map%20of%20the%20input%20feature%20space.%20Our%20approach%2C%20named%0ATopoMap%2C%20is%20both%20black-box%20and%20model-agnostic%20as%20it%20relies%20solely%20on%20features%0Athat%20characterise%20the%20input%20space.%20To%20discriminate%20the%20inputs%20according%20to%20the%0Aspecific%20features%20they%20share%2C%20we%20first%20apply%20dimensionality%20reduction%20to%20obtain%0Ainput%20embeddings%2C%20which%20are%20then%20subjected%20to%20clustering.%20Each%20DL%20model%20might%0Arequire%20specific%20embedding%20computations%20and%20clustering%20algorithms%20to%20achieve%20a%0Ameaningful%20separation%20of%20inputs%20into%20discriminative%20groups.%20We%20propose%20a%20novel%0Away%20to%20evaluate%20alternative%20configurations%20of%20embedding%20and%20clustering%0Atechniques.%20We%20used%20a%20deep%20neural%20network%20%28DNN%29%20as%20an%20approximation%20of%20a%20human%0Aevaluator%20who%20could%20tell%20whether%20a%20pair%20of%20clusters%20can%20be%20discriminated%20based%0Aon%20the%20features%20of%20the%20included%20elements.%20We%20use%20such%20a%20DNN%20to%20automatically%0Aselect%20the%20optimal%20topographical%20map%20of%20the%20inputs%20among%20all%20those%20that%20are%0Aproduced%20by%20different%20embedding/clustering%20configurations.%20The%20evaluation%0Aresults%20show%20that%20the%20maps%20generated%20by%20TopoMap%20consist%20of%20distinguishable%20and%0Ameaningful%20regions.%20In%20addition%2C%20we%20evaluate%20the%20effectiveness%20of%20TopoMap%20using%0Amutation%20analysis.%20In%20particular%2C%20we%20assess%20whether%20the%20clusters%20in%20our%0Atopographical%20map%20allow%20for%20an%20effective%20selection%20of%20mutation-killing%20inputs.%0AExperimental%20results%20show%20that%20our%20approach%20outperforms%20random%20selection%20by%2035%25%0Aon%20average%20on%20killable%20mutants%3B%20by%2061%25%20on%20non-killable%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoMap%253A%2520A%2520Feature-based%2520Semantic%2520Discriminator%2520of%2520the%2520Topographical%250A%2520%2520Regions%2520in%2520the%2520Test%2520Input%2520Space%26entry.906535625%3DGianmarco%2520De%2520Vita%2520and%2520Nargiz%2520Humbatova%2520and%2520Paolo%2520Tonella%26entry.1292438233%3D%2520%2520Testing%2520Deep%2520Learning%2520%2528DL%2529-based%2520systems%2520is%2520an%2520open%2520challenge.%2520Although%2520it%2520is%250Arelatively%2520easy%2520to%2520find%2520inputs%2520that%2520cause%2520a%2520DL%2520model%2520to%2520misbehave%252C%2520the%2520grouping%250Aof%2520inputs%2520by%2520features%2520that%2520make%2520the%2520DL%2520model%2520under%2520test%2520fail%2520is%2520largely%250Aunexplored.%2520Existing%2520approaches%2520for%2520DL%2520testing%2520introduce%2520perturbations%2520that%2520may%250Afocus%2520on%2520specific%2520failure-inducing%2520features%252C%2520while%2520neglecting%2520others%2520that%250Abelong%2520to%2520different%2520regions%2520of%2520the%2520feature%2520space.%2520In%2520this%2520paper%252C%2520we%2520create%2520an%250Aexplicit%2520topographical%2520map%2520of%2520the%2520input%2520feature%2520space.%2520Our%2520approach%252C%2520named%250ATopoMap%252C%2520is%2520both%2520black-box%2520and%2520model-agnostic%2520as%2520it%2520relies%2520solely%2520on%2520features%250Athat%2520characterise%2520the%2520input%2520space.%2520To%2520discriminate%2520the%2520inputs%2520according%2520to%2520the%250Aspecific%2520features%2520they%2520share%252C%2520we%2520first%2520apply%2520dimensionality%2520reduction%2520to%2520obtain%250Ainput%2520embeddings%252C%2520which%2520are%2520then%2520subjected%2520to%2520clustering.%2520Each%2520DL%2520model%2520might%250Arequire%2520specific%2520embedding%2520computations%2520and%2520clustering%2520algorithms%2520to%2520achieve%2520a%250Ameaningful%2520separation%2520of%2520inputs%2520into%2520discriminative%2520groups.%2520We%2520propose%2520a%2520novel%250Away%2520to%2520evaluate%2520alternative%2520configurations%2520of%2520embedding%2520and%2520clustering%250Atechniques.%2520We%2520used%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%2520as%2520an%2520approximation%2520of%2520a%2520human%250Aevaluator%2520who%2520could%2520tell%2520whether%2520a%2520pair%2520of%2520clusters%2520can%2520be%2520discriminated%2520based%250Aon%2520the%2520features%2520of%2520the%2520included%2520elements.%2520We%2520use%2520such%2520a%2520DNN%2520to%2520automatically%250Aselect%2520the%2520optimal%2520topographical%2520map%2520of%2520the%2520inputs%2520among%2520all%2520those%2520that%2520are%250Aproduced%2520by%2520different%2520embedding/clustering%2520configurations.%2520The%2520evaluation%250Aresults%2520show%2520that%2520the%2520maps%2520generated%2520by%2520TopoMap%2520consist%2520of%2520distinguishable%2520and%250Ameaningful%2520regions.%2520In%2520addition%252C%2520we%2520evaluate%2520the%2520effectiveness%2520of%2520TopoMap%2520using%250Amutation%2520analysis.%2520In%2520particular%252C%2520we%2520assess%2520whether%2520the%2520clusters%2520in%2520our%250Atopographical%2520map%2520allow%2520for%2520an%2520effective%2520selection%2520of%2520mutation-killing%2520inputs.%250AExperimental%2520results%2520show%2520that%2520our%2520approach%2520outperforms%2520random%2520selection%2520by%252035%2525%250Aon%2520average%2520on%2520killable%2520mutants%253B%2520by%252061%2525%2520on%2520non-killable%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoMap%3A%20A%20Feature-based%20Semantic%20Discriminator%20of%20the%20Topographical%0A%20%20Regions%20in%20the%20Test%20Input%20Space&entry.906535625=Gianmarco%20De%20Vita%20and%20Nargiz%20Humbatova%20and%20Paolo%20Tonella&entry.1292438233=%20%20Testing%20Deep%20Learning%20%28DL%29-based%20systems%20is%20an%20open%20challenge.%20Although%20it%20is%0Arelatively%20easy%20to%20find%20inputs%20that%20cause%20a%20DL%20model%20to%20misbehave%2C%20the%20grouping%0Aof%20inputs%20by%20features%20that%20make%20the%20DL%20model%20under%20test%20fail%20is%20largely%0Aunexplored.%20Existing%20approaches%20for%20DL%20testing%20introduce%20perturbations%20that%20may%0Afocus%20on%20specific%20failure-inducing%20features%2C%20while%20neglecting%20others%20that%0Abelong%20to%20different%20regions%20of%20the%20feature%20space.%20In%20this%20paper%2C%20we%20create%20an%0Aexplicit%20topographical%20map%20of%20the%20input%20feature%20space.%20Our%20approach%2C%20named%0ATopoMap%2C%20is%20both%20black-box%20and%20model-agnostic%20as%20it%20relies%20solely%20on%20features%0Athat%20characterise%20the%20input%20space.%20To%20discriminate%20the%20inputs%20according%20to%20the%0Aspecific%20features%20they%20share%2C%20we%20first%20apply%20dimensionality%20reduction%20to%20obtain%0Ainput%20embeddings%2C%20which%20are%20then%20subjected%20to%20clustering.%20Each%20DL%20model%20might%0Arequire%20specific%20embedding%20computations%20and%20clustering%20algorithms%20to%20achieve%20a%0Ameaningful%20separation%20of%20inputs%20into%20discriminative%20groups.%20We%20propose%20a%20novel%0Away%20to%20evaluate%20alternative%20configurations%20of%20embedding%20and%20clustering%0Atechniques.%20We%20used%20a%20deep%20neural%20network%20%28DNN%29%20as%20an%20approximation%20of%20a%20human%0Aevaluator%20who%20could%20tell%20whether%20a%20pair%20of%20clusters%20can%20be%20discriminated%20based%0Aon%20the%20features%20of%20the%20included%20elements.%20We%20use%20such%20a%20DNN%20to%20automatically%0Aselect%20the%20optimal%20topographical%20map%20of%20the%20inputs%20among%20all%20those%20that%20are%0Aproduced%20by%20different%20embedding/clustering%20configurations.%20The%20evaluation%0Aresults%20show%20that%20the%20maps%20generated%20by%20TopoMap%20consist%20of%20distinguishable%20and%0Ameaningful%20regions.%20In%20addition%2C%20we%20evaluate%20the%20effectiveness%20of%20TopoMap%20using%0Amutation%20analysis.%20In%20particular%2C%20we%20assess%20whether%20the%20clusters%20in%20our%0Atopographical%20map%20allow%20for%20an%20effective%20selection%20of%20mutation-killing%20inputs.%0AExperimental%20results%20show%20that%20our%20approach%20outperforms%20random%20selection%20by%2035%25%0Aon%20average%20on%20killable%20mutants%3B%20by%2061%25%20on%20non-killable%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03242v1&entry.124074799=Read"},
{"title": "Survey on Hand Gesture Recognition from Visual Input", "author": "Manousos Linardakis and Iraklis Varlamis and Georgios Th. Papadopoulos", "abstract": "  Hand gesture recognition has become an important research area, driven by the\ngrowing demand for human-computer interaction in fields such as sign language\nrecognition, virtual and augmented reality, and robotics. Despite the rapid\ngrowth of the field, there are few surveys that comprehensively cover recent\nresearch developments, available solutions, and benchmark datasets. This survey\naddresses this gap by examining the latest advancements in hand gesture and 3D\nhand pose recognition from various types of camera input data including RGB\nimages, depth images, and videos from monocular or multiview cameras, examining\nthe differing methodological requirements of each approach. Furthermore, an\noverview of widely used datasets is provided, detailing their main\ncharacteristics and application domains. Finally, open challenges such as\nachieving robust recognition in real-world environments, handling occlusions,\nensuring generalization across diverse users, and addressing computational\nefficiency for real-time applications are highlighted to guide future research\ndirections. By synthesizing the objectives, methodologies, and applications of\nrecent studies, this survey offers valuable insights into current trends,\nchallenges, and opportunities for future research in human hand gesture\nrecognition.\n", "link": "http://arxiv.org/abs/2501.11992v3", "date": "2025-09-03", "relevancy": 2.5553, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.521}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5183}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20on%20Hand%20Gesture%20Recognition%20from%20Visual%20Input&body=Title%3A%20Survey%20on%20Hand%20Gesture%20Recognition%20from%20Visual%20Input%0AAuthor%3A%20Manousos%20Linardakis%20and%20Iraklis%20Varlamis%20and%20Georgios%20Th.%20Papadopoulos%0AAbstract%3A%20%20%20Hand%20gesture%20recognition%20has%20become%20an%20important%20research%20area%2C%20driven%20by%20the%0Agrowing%20demand%20for%20human-computer%20interaction%20in%20fields%20such%20as%20sign%20language%0Arecognition%2C%20virtual%20and%20augmented%20reality%2C%20and%20robotics.%20Despite%20the%20rapid%0Agrowth%20of%20the%20field%2C%20there%20are%20few%20surveys%20that%20comprehensively%20cover%20recent%0Aresearch%20developments%2C%20available%20solutions%2C%20and%20benchmark%20datasets.%20This%20survey%0Aaddresses%20this%20gap%20by%20examining%20the%20latest%20advancements%20in%20hand%20gesture%20and%203D%0Ahand%20pose%20recognition%20from%20various%20types%20of%20camera%20input%20data%20including%20RGB%0Aimages%2C%20depth%20images%2C%20and%20videos%20from%20monocular%20or%20multiview%20cameras%2C%20examining%0Athe%20differing%20methodological%20requirements%20of%20each%20approach.%20Furthermore%2C%20an%0Aoverview%20of%20widely%20used%20datasets%20is%20provided%2C%20detailing%20their%20main%0Acharacteristics%20and%20application%20domains.%20Finally%2C%20open%20challenges%20such%20as%0Aachieving%20robust%20recognition%20in%20real-world%20environments%2C%20handling%20occlusions%2C%0Aensuring%20generalization%20across%20diverse%20users%2C%20and%20addressing%20computational%0Aefficiency%20for%20real-time%20applications%20are%20highlighted%20to%20guide%20future%20research%0Adirections.%20By%20synthesizing%20the%20objectives%2C%20methodologies%2C%20and%20applications%20of%0Arecent%20studies%2C%20this%20survey%20offers%20valuable%20insights%20into%20current%20trends%2C%0Achallenges%2C%20and%20opportunities%20for%20future%20research%20in%20human%20hand%20gesture%0Arecognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11992v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520on%2520Hand%2520Gesture%2520Recognition%2520from%2520Visual%2520Input%26entry.906535625%3DManousos%2520Linardakis%2520and%2520Iraklis%2520Varlamis%2520and%2520Georgios%2520Th.%2520Papadopoulos%26entry.1292438233%3D%2520%2520Hand%2520gesture%2520recognition%2520has%2520become%2520an%2520important%2520research%2520area%252C%2520driven%2520by%2520the%250Agrowing%2520demand%2520for%2520human-computer%2520interaction%2520in%2520fields%2520such%2520as%2520sign%2520language%250Arecognition%252C%2520virtual%2520and%2520augmented%2520reality%252C%2520and%2520robotics.%2520Despite%2520the%2520rapid%250Agrowth%2520of%2520the%2520field%252C%2520there%2520are%2520few%2520surveys%2520that%2520comprehensively%2520cover%2520recent%250Aresearch%2520developments%252C%2520available%2520solutions%252C%2520and%2520benchmark%2520datasets.%2520This%2520survey%250Aaddresses%2520this%2520gap%2520by%2520examining%2520the%2520latest%2520advancements%2520in%2520hand%2520gesture%2520and%25203D%250Ahand%2520pose%2520recognition%2520from%2520various%2520types%2520of%2520camera%2520input%2520data%2520including%2520RGB%250Aimages%252C%2520depth%2520images%252C%2520and%2520videos%2520from%2520monocular%2520or%2520multiview%2520cameras%252C%2520examining%250Athe%2520differing%2520methodological%2520requirements%2520of%2520each%2520approach.%2520Furthermore%252C%2520an%250Aoverview%2520of%2520widely%2520used%2520datasets%2520is%2520provided%252C%2520detailing%2520their%2520main%250Acharacteristics%2520and%2520application%2520domains.%2520Finally%252C%2520open%2520challenges%2520such%2520as%250Aachieving%2520robust%2520recognition%2520in%2520real-world%2520environments%252C%2520handling%2520occlusions%252C%250Aensuring%2520generalization%2520across%2520diverse%2520users%252C%2520and%2520addressing%2520computational%250Aefficiency%2520for%2520real-time%2520applications%2520are%2520highlighted%2520to%2520guide%2520future%2520research%250Adirections.%2520By%2520synthesizing%2520the%2520objectives%252C%2520methodologies%252C%2520and%2520applications%2520of%250Arecent%2520studies%252C%2520this%2520survey%2520offers%2520valuable%2520insights%2520into%2520current%2520trends%252C%250Achallenges%252C%2520and%2520opportunities%2520for%2520future%2520research%2520in%2520human%2520hand%2520gesture%250Arecognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11992v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20on%20Hand%20Gesture%20Recognition%20from%20Visual%20Input&entry.906535625=Manousos%20Linardakis%20and%20Iraklis%20Varlamis%20and%20Georgios%20Th.%20Papadopoulos&entry.1292438233=%20%20Hand%20gesture%20recognition%20has%20become%20an%20important%20research%20area%2C%20driven%20by%20the%0Agrowing%20demand%20for%20human-computer%20interaction%20in%20fields%20such%20as%20sign%20language%0Arecognition%2C%20virtual%20and%20augmented%20reality%2C%20and%20robotics.%20Despite%20the%20rapid%0Agrowth%20of%20the%20field%2C%20there%20are%20few%20surveys%20that%20comprehensively%20cover%20recent%0Aresearch%20developments%2C%20available%20solutions%2C%20and%20benchmark%20datasets.%20This%20survey%0Aaddresses%20this%20gap%20by%20examining%20the%20latest%20advancements%20in%20hand%20gesture%20and%203D%0Ahand%20pose%20recognition%20from%20various%20types%20of%20camera%20input%20data%20including%20RGB%0Aimages%2C%20depth%20images%2C%20and%20videos%20from%20monocular%20or%20multiview%20cameras%2C%20examining%0Athe%20differing%20methodological%20requirements%20of%20each%20approach.%20Furthermore%2C%20an%0Aoverview%20of%20widely%20used%20datasets%20is%20provided%2C%20detailing%20their%20main%0Acharacteristics%20and%20application%20domains.%20Finally%2C%20open%20challenges%20such%20as%0Aachieving%20robust%20recognition%20in%20real-world%20environments%2C%20handling%20occlusions%2C%0Aensuring%20generalization%20across%20diverse%20users%2C%20and%20addressing%20computational%0Aefficiency%20for%20real-time%20applications%20are%20highlighted%20to%20guide%20future%20research%0Adirections.%20By%20synthesizing%20the%20objectives%2C%20methodologies%2C%20and%20applications%20of%0Arecent%20studies%2C%20this%20survey%20offers%20valuable%20insights%20into%20current%20trends%2C%0Achallenges%2C%20and%20opportunities%20for%20future%20research%20in%20human%20hand%20gesture%0Arecognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11992v3&entry.124074799=Read"},
{"title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding", "author": "Zhan Shi and Song Wang and Junbo Chen and Jianke Zhu", "abstract": "  Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.\n", "link": "http://arxiv.org/abs/2508.01197v2", "date": "2025-09-03", "relevancy": 2.5289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6372}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Coarse-to-Fine%20Approach%20to%20Multi-Modality%203D%20Occupancy%20Grounding&body=Title%3A%20A%20Coarse-to-Fine%20Approach%20to%20Multi-Modality%203D%20Occupancy%20Grounding%0AAuthor%3A%20Zhan%20Shi%20and%20Song%20Wang%20and%20Junbo%20Chen%20and%20Jianke%20Zhu%0AAbstract%3A%20%20%20Visual%20grounding%20aims%20to%20identify%20objects%20or%20regions%20in%20a%20scene%20based%20on%0Anatural%20language%20descriptions%2C%20essential%20for%20spatially%20aware%20perception%20in%0Aautonomous%20driving.%20However%2C%20existing%20visual%20grounding%20tasks%20typically%20depend%0Aon%20bounding%20boxes%20that%20often%20fail%20to%20capture%20fine-grained%20details.%20Not%20all%0Avoxels%20within%20a%20bounding%20box%20are%20occupied%2C%20resulting%20in%20inaccurate%20object%0Arepresentations.%20To%20address%20this%2C%20we%20introduce%20a%20benchmark%20for%203D%20occupancy%0Agrounding%20in%20challenging%20outdoor%20scenes.%20Built%20on%20the%20nuScenes%20dataset%2C%20it%0Aintegrates%20natural%20language%20with%20voxel-level%20occupancy%20annotations%2C%20offering%0Amore%20precise%20object%20perception%20compared%20to%20the%20traditional%20grounding%20task.%0AMoreover%2C%20we%20propose%20GroundingOcc%2C%20an%20end-to-end%20model%20designed%20for%203D%0Aoccupancy%20grounding%20through%20multi-modal%20learning.%20It%20combines%20visual%2C%20textual%2C%0Aand%20point%20cloud%20features%20to%20predict%20object%20location%20and%20occupancy%20information%0Afrom%20coarse%20to%20fine.%20Specifically%2C%20GroundingOcc%20comprises%20a%20multimodal%20encoder%0Afor%20feature%20extraction%2C%20an%20occupancy%20head%20for%20voxel-wise%20predictions%2C%20and%20a%0Agrounding%20head%20to%20refine%20localization.%20Additionally%2C%20a%202D%20grounding%20module%20and%0Aa%20depth%20estimation%20module%20enhance%20geometric%20understanding%2C%20thereby%20boosting%0Amodel%20performance.%20Extensive%20experiments%20on%20the%20benchmark%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20baselines%20on%203D%20occupancy%20grounding.%20The%20dataset%20is%0Aavailable%20at%20https%3A//github.com/RONINGOD/GroundingOcc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Coarse-to-Fine%2520Approach%2520to%2520Multi-Modality%25203D%2520Occupancy%2520Grounding%26entry.906535625%3DZhan%2520Shi%2520and%2520Song%2520Wang%2520and%2520Junbo%2520Chen%2520and%2520Jianke%2520Zhu%26entry.1292438233%3D%2520%2520Visual%2520grounding%2520aims%2520to%2520identify%2520objects%2520or%2520regions%2520in%2520a%2520scene%2520based%2520on%250Anatural%2520language%2520descriptions%252C%2520essential%2520for%2520spatially%2520aware%2520perception%2520in%250Aautonomous%2520driving.%2520However%252C%2520existing%2520visual%2520grounding%2520tasks%2520typically%2520depend%250Aon%2520bounding%2520boxes%2520that%2520often%2520fail%2520to%2520capture%2520fine-grained%2520details.%2520Not%2520all%250Avoxels%2520within%2520a%2520bounding%2520box%2520are%2520occupied%252C%2520resulting%2520in%2520inaccurate%2520object%250Arepresentations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520benchmark%2520for%25203D%2520occupancy%250Agrounding%2520in%2520challenging%2520outdoor%2520scenes.%2520Built%2520on%2520the%2520nuScenes%2520dataset%252C%2520it%250Aintegrates%2520natural%2520language%2520with%2520voxel-level%2520occupancy%2520annotations%252C%2520offering%250Amore%2520precise%2520object%2520perception%2520compared%2520to%2520the%2520traditional%2520grounding%2520task.%250AMoreover%252C%2520we%2520propose%2520GroundingOcc%252C%2520an%2520end-to-end%2520model%2520designed%2520for%25203D%250Aoccupancy%2520grounding%2520through%2520multi-modal%2520learning.%2520It%2520combines%2520visual%252C%2520textual%252C%250Aand%2520point%2520cloud%2520features%2520to%2520predict%2520object%2520location%2520and%2520occupancy%2520information%250Afrom%2520coarse%2520to%2520fine.%2520Specifically%252C%2520GroundingOcc%2520comprises%2520a%2520multimodal%2520encoder%250Afor%2520feature%2520extraction%252C%2520an%2520occupancy%2520head%2520for%2520voxel-wise%2520predictions%252C%2520and%2520a%250Agrounding%2520head%2520to%2520refine%2520localization.%2520Additionally%252C%2520a%25202D%2520grounding%2520module%2520and%250Aa%2520depth%2520estimation%2520module%2520enhance%2520geometric%2520understanding%252C%2520thereby%2520boosting%250Amodel%2520performance.%2520Extensive%2520experiments%2520on%2520the%2520benchmark%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520existing%2520baselines%2520on%25203D%2520occupancy%2520grounding.%2520The%2520dataset%2520is%250Aavailable%2520at%2520https%253A//github.com/RONINGOD/GroundingOcc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Coarse-to-Fine%20Approach%20to%20Multi-Modality%203D%20Occupancy%20Grounding&entry.906535625=Zhan%20Shi%20and%20Song%20Wang%20and%20Junbo%20Chen%20and%20Jianke%20Zhu&entry.1292438233=%20%20Visual%20grounding%20aims%20to%20identify%20objects%20or%20regions%20in%20a%20scene%20based%20on%0Anatural%20language%20descriptions%2C%20essential%20for%20spatially%20aware%20perception%20in%0Aautonomous%20driving.%20However%2C%20existing%20visual%20grounding%20tasks%20typically%20depend%0Aon%20bounding%20boxes%20that%20often%20fail%20to%20capture%20fine-grained%20details.%20Not%20all%0Avoxels%20within%20a%20bounding%20box%20are%20occupied%2C%20resulting%20in%20inaccurate%20object%0Arepresentations.%20To%20address%20this%2C%20we%20introduce%20a%20benchmark%20for%203D%20occupancy%0Agrounding%20in%20challenging%20outdoor%20scenes.%20Built%20on%20the%20nuScenes%20dataset%2C%20it%0Aintegrates%20natural%20language%20with%20voxel-level%20occupancy%20annotations%2C%20offering%0Amore%20precise%20object%20perception%20compared%20to%20the%20traditional%20grounding%20task.%0AMoreover%2C%20we%20propose%20GroundingOcc%2C%20an%20end-to-end%20model%20designed%20for%203D%0Aoccupancy%20grounding%20through%20multi-modal%20learning.%20It%20combines%20visual%2C%20textual%2C%0Aand%20point%20cloud%20features%20to%20predict%20object%20location%20and%20occupancy%20information%0Afrom%20coarse%20to%20fine.%20Specifically%2C%20GroundingOcc%20comprises%20a%20multimodal%20encoder%0Afor%20feature%20extraction%2C%20an%20occupancy%20head%20for%20voxel-wise%20predictions%2C%20and%20a%0Agrounding%20head%20to%20refine%20localization.%20Additionally%2C%20a%202D%20grounding%20module%20and%0Aa%20depth%20estimation%20module%20enhance%20geometric%20understanding%2C%20thereby%20boosting%0Amodel%20performance.%20Extensive%20experiments%20on%20the%20benchmark%20demonstrate%20that%20our%0Amethod%20outperforms%20existing%20baselines%20on%203D%20occupancy%20grounding.%20The%20dataset%20is%0Aavailable%20at%20https%3A//github.com/RONINGOD/GroundingOcc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01197v2&entry.124074799=Read"},
{"title": "Can Large Language Models Act as Ensembler for Multi-GNNs?", "author": "Hanqi Duan and Yao Cheng and Jianxiang Yu and Yao Liu and Xiang Li", "abstract": "  Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://github.com/AquariusAQ/LensGNN.\n", "link": "http://arxiv.org/abs/2410.16822v5", "date": "2025-09-03", "relevancy": 2.5234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Act%20as%20Ensembler%20for%20Multi-GNNs%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Act%20as%20Ensembler%20for%20Multi-GNNs%3F%0AAuthor%3A%20Hanqi%20Duan%20and%20Yao%20Cheng%20and%20Jianxiang%20Yu%20and%20Yao%20Liu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20models%20for%20learning%0Afrom%20graph-structured%20data.%20However%2C%20GNNs%20lack%20the%20inherent%20semantic%0Aunderstanding%20capability%20of%20rich%20textual%20node%20attributes%2C%20limiting%20their%0Aeffectiveness%20in%20applications.%20On%20the%20other%20hand%2C%20we%20empirically%20observe%20that%0Afor%20existing%20GNN%20models%2C%20no%20one%20can%20consistently%20outperforms%20others%20across%0Adiverse%20datasets.%20In%20this%20paper%2C%20we%20study%20whether%20LLMs%20can%20act%20as%20an%20ensembler%0Afor%20multi-GNNs%20and%20propose%20the%20LensGNN%20model.%20The%20model%20first%20aligns%20multiple%0AGNNs%2C%20mapping%20the%20representations%20of%20different%20GNNs%20into%20the%20same%20space.%20Then%2C%0Athrough%20LoRA%20fine-tuning%2C%20it%20aligns%20the%20space%20between%20the%20GNN%20and%20the%20LLM%2C%0Ainjecting%20graph%20tokens%20and%20textual%20information%20into%20LLMs.%20This%20allows%20LensGNN%0Ato%20ensemble%20multiple%20GNNs%20and%20take%20advantage%20of%20the%20strengths%20of%20LLM%2C%20leading%0Ato%20a%20deeper%20understanding%20of%20both%20textual%20semantic%20information%20and%20graph%0Astructural%20information.%20The%20experimental%20results%20show%20that%20LensGNN%20outperforms%0Aexisting%20models.%20This%20research%20advances%20text-attributed%20graph%20ensemble%20learning%0Aby%20providing%20a%20robust%20and%20superior%20solution%20for%20integrating%20semantic%20and%0Astructural%20information.%20We%20provide%20our%20code%20and%20data%20here%3A%0Ahttps%3A//github.com/AquariusAQ/LensGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16822v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Act%2520as%2520Ensembler%2520for%2520Multi-GNNs%253F%26entry.906535625%3DHanqi%2520Duan%2520and%2520Yao%2520Cheng%2520and%2520Jianxiang%2520Yu%2520and%2520Yao%2520Liu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520powerful%2520models%2520for%2520learning%250Afrom%2520graph-structured%2520data.%2520However%252C%2520GNNs%2520lack%2520the%2520inherent%2520semantic%250Aunderstanding%2520capability%2520of%2520rich%2520textual%2520node%2520attributes%252C%2520limiting%2520their%250Aeffectiveness%2520in%2520applications.%2520On%2520the%2520other%2520hand%252C%2520we%2520empirically%2520observe%2520that%250Afor%2520existing%2520GNN%2520models%252C%2520no%2520one%2520can%2520consistently%2520outperforms%2520others%2520across%250Adiverse%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520study%2520whether%2520LLMs%2520can%2520act%2520as%2520an%2520ensembler%250Afor%2520multi-GNNs%2520and%2520propose%2520the%2520LensGNN%2520model.%2520The%2520model%2520first%2520aligns%2520multiple%250AGNNs%252C%2520mapping%2520the%2520representations%2520of%2520different%2520GNNs%2520into%2520the%2520same%2520space.%2520Then%252C%250Athrough%2520LoRA%2520fine-tuning%252C%2520it%2520aligns%2520the%2520space%2520between%2520the%2520GNN%2520and%2520the%2520LLM%252C%250Ainjecting%2520graph%2520tokens%2520and%2520textual%2520information%2520into%2520LLMs.%2520This%2520allows%2520LensGNN%250Ato%2520ensemble%2520multiple%2520GNNs%2520and%2520take%2520advantage%2520of%2520the%2520strengths%2520of%2520LLM%252C%2520leading%250Ato%2520a%2520deeper%2520understanding%2520of%2520both%2520textual%2520semantic%2520information%2520and%2520graph%250Astructural%2520information.%2520The%2520experimental%2520results%2520show%2520that%2520LensGNN%2520outperforms%250Aexisting%2520models.%2520This%2520research%2520advances%2520text-attributed%2520graph%2520ensemble%2520learning%250Aby%2520providing%2520a%2520robust%2520and%2520superior%2520solution%2520for%2520integrating%2520semantic%2520and%250Astructural%2520information.%2520We%2520provide%2520our%2520code%2520and%2520data%2520here%253A%250Ahttps%253A//github.com/AquariusAQ/LensGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16822v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Act%20as%20Ensembler%20for%20Multi-GNNs%3F&entry.906535625=Hanqi%20Duan%20and%20Yao%20Cheng%20and%20Jianxiang%20Yu%20and%20Yao%20Liu%20and%20Xiang%20Li&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20models%20for%20learning%0Afrom%20graph-structured%20data.%20However%2C%20GNNs%20lack%20the%20inherent%20semantic%0Aunderstanding%20capability%20of%20rich%20textual%20node%20attributes%2C%20limiting%20their%0Aeffectiveness%20in%20applications.%20On%20the%20other%20hand%2C%20we%20empirically%20observe%20that%0Afor%20existing%20GNN%20models%2C%20no%20one%20can%20consistently%20outperforms%20others%20across%0Adiverse%20datasets.%20In%20this%20paper%2C%20we%20study%20whether%20LLMs%20can%20act%20as%20an%20ensembler%0Afor%20multi-GNNs%20and%20propose%20the%20LensGNN%20model.%20The%20model%20first%20aligns%20multiple%0AGNNs%2C%20mapping%20the%20representations%20of%20different%20GNNs%20into%20the%20same%20space.%20Then%2C%0Athrough%20LoRA%20fine-tuning%2C%20it%20aligns%20the%20space%20between%20the%20GNN%20and%20the%20LLM%2C%0Ainjecting%20graph%20tokens%20and%20textual%20information%20into%20LLMs.%20This%20allows%20LensGNN%0Ato%20ensemble%20multiple%20GNNs%20and%20take%20advantage%20of%20the%20strengths%20of%20LLM%2C%20leading%0Ato%20a%20deeper%20understanding%20of%20both%20textual%20semantic%20information%20and%20graph%0Astructural%20information.%20The%20experimental%20results%20show%20that%20LensGNN%20outperforms%0Aexisting%20models.%20This%20research%20advances%20text-attributed%20graph%20ensemble%20learning%0Aby%20providing%20a%20robust%20and%20superior%20solution%20for%20integrating%20semantic%20and%0Astructural%20information.%20We%20provide%20our%20code%20and%20data%20here%3A%0Ahttps%3A//github.com/AquariusAQ/LensGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16822v5&entry.124074799=Read"},
{"title": "Rapid Word Learning Through Meta In-Context Learning", "author": "Wentao Wang and Guangyuan Jiang and Tal Linzen and Brenden M. Lake", "abstract": "  Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.\n", "link": "http://arxiv.org/abs/2502.14791v3", "date": "2025-09-03", "relevancy": 2.5005, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning&body=Title%3A%20Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning%0AAuthor%3A%20Wentao%20Wang%20and%20Guangyuan%20Jiang%20and%20Tal%20Linzen%20and%20Brenden%20M.%20Lake%0AAbstract%3A%20%20%20Humans%20can%20quickly%20learn%20a%20new%20word%20from%20a%20few%20illustrative%20examples%2C%20and%0Athen%20systematically%20and%20flexibly%20use%20it%20in%20novel%20contexts.%20Yet%20the%20abilities%20of%0Acurrent%20language%20models%20for%20few-shot%20word%20learning%2C%20and%20methods%20for%20improving%0Athese%20abilities%2C%20are%20underexplored.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20method%2C%0AMeta-training%20for%20IN-context%20learNing%20Of%20Words%20%28Minnow%29.%20This%20method%20trains%0Alanguage%20models%20to%20generate%20new%20examples%20of%20a%20word%27s%20usage%20given%20a%20few%0Ain-context%20examples%2C%20using%20a%20special%20placeholder%20token%20to%20represent%20the%20new%0Aword.%20This%20training%20is%20repeated%20on%20many%20new%20words%20to%20develop%20a%20general%0Aword-learning%20ability.%20We%20find%20that%20training%20models%20from%20scratch%20with%20Minnow%20on%0Ahuman-scale%20child-directed%20language%20enables%20strong%20few-shot%20word%20learning%2C%0Acomparable%20to%20a%20large%20language%20model%20%28LLM%29%20pre-trained%20on%20orders%20of%20magnitude%0Amore%20data.%20Furthermore%2C%20through%20discriminative%20and%20generative%20evaluations%2C%20we%0Ademonstrate%20that%20finetuning%20pre-trained%20LLMs%20with%20Minnow%20improves%20their%20ability%0Ato%20discriminate%20between%20new%20words%2C%20identify%20syntactic%20categories%20of%20new%20words%2C%0Aand%20generate%20reasonable%20new%20usages%20and%20definitions%20for%20new%20words%2C%20based%20on%20one%0Aor%20a%20few%20in-context%20examples.%20These%20findings%20highlight%20the%20data%20efficiency%20of%0AMinnow%20and%20its%20potential%20to%20improve%20language%20model%20performance%20in%20word%20learning%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14791v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Word%2520Learning%2520Through%2520Meta%2520In-Context%2520Learning%26entry.906535625%3DWentao%2520Wang%2520and%2520Guangyuan%2520Jiang%2520and%2520Tal%2520Linzen%2520and%2520Brenden%2520M.%2520Lake%26entry.1292438233%3D%2520%2520Humans%2520can%2520quickly%2520learn%2520a%2520new%2520word%2520from%2520a%2520few%2520illustrative%2520examples%252C%2520and%250Athen%2520systematically%2520and%2520flexibly%2520use%2520it%2520in%2520novel%2520contexts.%2520Yet%2520the%2520abilities%2520of%250Acurrent%2520language%2520models%2520for%2520few-shot%2520word%2520learning%252C%2520and%2520methods%2520for%2520improving%250Athese%2520abilities%252C%2520are%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520method%252C%250AMeta-training%2520for%2520IN-context%2520learNing%2520Of%2520Words%2520%2528Minnow%2529.%2520This%2520method%2520trains%250Alanguage%2520models%2520to%2520generate%2520new%2520examples%2520of%2520a%2520word%2527s%2520usage%2520given%2520a%2520few%250Ain-context%2520examples%252C%2520using%2520a%2520special%2520placeholder%2520token%2520to%2520represent%2520the%2520new%250Aword.%2520This%2520training%2520is%2520repeated%2520on%2520many%2520new%2520words%2520to%2520develop%2520a%2520general%250Aword-learning%2520ability.%2520We%2520find%2520that%2520training%2520models%2520from%2520scratch%2520with%2520Minnow%2520on%250Ahuman-scale%2520child-directed%2520language%2520enables%2520strong%2520few-shot%2520word%2520learning%252C%250Acomparable%2520to%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520pre-trained%2520on%2520orders%2520of%2520magnitude%250Amore%2520data.%2520Furthermore%252C%2520through%2520discriminative%2520and%2520generative%2520evaluations%252C%2520we%250Ademonstrate%2520that%2520finetuning%2520pre-trained%2520LLMs%2520with%2520Minnow%2520improves%2520their%2520ability%250Ato%2520discriminate%2520between%2520new%2520words%252C%2520identify%2520syntactic%2520categories%2520of%2520new%2520words%252C%250Aand%2520generate%2520reasonable%2520new%2520usages%2520and%2520definitions%2520for%2520new%2520words%252C%2520based%2520on%2520one%250Aor%2520a%2520few%2520in-context%2520examples.%2520These%2520findings%2520highlight%2520the%2520data%2520efficiency%2520of%250AMinnow%2520and%2520its%2520potential%2520to%2520improve%2520language%2520model%2520performance%2520in%2520word%2520learning%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14791v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Word%20Learning%20Through%20Meta%20In-Context%20Learning&entry.906535625=Wentao%20Wang%20and%20Guangyuan%20Jiang%20and%20Tal%20Linzen%20and%20Brenden%20M.%20Lake&entry.1292438233=%20%20Humans%20can%20quickly%20learn%20a%20new%20word%20from%20a%20few%20illustrative%20examples%2C%20and%0Athen%20systematically%20and%20flexibly%20use%20it%20in%20novel%20contexts.%20Yet%20the%20abilities%20of%0Acurrent%20language%20models%20for%20few-shot%20word%20learning%2C%20and%20methods%20for%20improving%0Athese%20abilities%2C%20are%20underexplored.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20method%2C%0AMeta-training%20for%20IN-context%20learNing%20Of%20Words%20%28Minnow%29.%20This%20method%20trains%0Alanguage%20models%20to%20generate%20new%20examples%20of%20a%20word%27s%20usage%20given%20a%20few%0Ain-context%20examples%2C%20using%20a%20special%20placeholder%20token%20to%20represent%20the%20new%0Aword.%20This%20training%20is%20repeated%20on%20many%20new%20words%20to%20develop%20a%20general%0Aword-learning%20ability.%20We%20find%20that%20training%20models%20from%20scratch%20with%20Minnow%20on%0Ahuman-scale%20child-directed%20language%20enables%20strong%20few-shot%20word%20learning%2C%0Acomparable%20to%20a%20large%20language%20model%20%28LLM%29%20pre-trained%20on%20orders%20of%20magnitude%0Amore%20data.%20Furthermore%2C%20through%20discriminative%20and%20generative%20evaluations%2C%20we%0Ademonstrate%20that%20finetuning%20pre-trained%20LLMs%20with%20Minnow%20improves%20their%20ability%0Ato%20discriminate%20between%20new%20words%2C%20identify%20syntactic%20categories%20of%20new%20words%2C%0Aand%20generate%20reasonable%20new%20usages%20and%20definitions%20for%20new%20words%2C%20based%20on%20one%0Aor%20a%20few%20in-context%20examples.%20These%20findings%20highlight%20the%20data%20efficiency%20of%0AMinnow%20and%20its%20potential%20to%20improve%20language%20model%20performance%20in%20word%20learning%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14791v3&entry.124074799=Read"},
{"title": "Avoidance Decoding for Diverse Multi-Branch Story Generation", "author": "Kyeongman Park and Nakyeong Yang and Kyomin Jung", "abstract": "  Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity.\n", "link": "http://arxiv.org/abs/2509.02170v2", "date": "2025-09-03", "relevancy": 2.4938, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Avoidance%20Decoding%20for%20Diverse%20Multi-Branch%20Story%20Generation&body=Title%3A%20Avoidance%20Decoding%20for%20Diverse%20Multi-Branch%20Story%20Generation%0AAuthor%3A%20Kyeongman%20Park%20and%20Nakyeong%20Yang%20and%20Kyomin%20Jung%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20generate%20repetitive%20and%20monotonous%0Aoutputs%2C%20especially%20in%20tasks%20like%20story%20generation%2C%20due%20to%20limited%20creative%0Adiversity%20when%20given%20the%20same%20input%20prompt.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20decoding%20strategy%2C%20Avoidance%20Decoding%2C%20that%20modifies%20token%0Alogits%20by%20penalizing%20similarity%20to%20previously%20generated%20outputs%2C%20thereby%0Aencouraging%20more%20diverse%20multi-branch%20stories.%20This%20penalty%20adaptively%20balances%0Atwo%20similarity%20measures%3A%20%281%29%20Concept-level%20Similarity%20Penalty%2C%20which%20is%0Aprioritized%20in%20early%20stages%20to%20diversify%20initial%20story%20concepts%2C%20and%20%282%29%0ANarrative-level%20Similarity%20Penalty%2C%20which%20is%20increasingly%20emphasized%20later%20to%0Aensure%20natural%20yet%20diverse%20plot%20development.%20Notably%2C%20our%20method%20achieves%20up%20to%0A2.6%20times%20higher%20output%20diversity%20and%20reduces%20repetition%20by%20an%20average%20of%2030%25%0Acompared%20to%20strong%20baselines%2C%20while%20effectively%20mitigating%20text%20degeneration.%0AFurthermore%2C%20we%20reveal%20that%20our%20method%20activates%20a%20broader%20range%20of%20neurons%2C%0Ademonstrating%20that%20it%20leverages%20the%20model%27s%20intrinsic%20creativity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvoidance%2520Decoding%2520for%2520Diverse%2520Multi-Branch%2520Story%2520Generation%26entry.906535625%3DKyeongman%2520Park%2520and%2520Nakyeong%2520Yang%2520and%2520Kyomin%2520Jung%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520generate%2520repetitive%2520and%2520monotonous%250Aoutputs%252C%2520especially%2520in%2520tasks%2520like%2520story%2520generation%252C%2520due%2520to%2520limited%2520creative%250Adiversity%2520when%2520given%2520the%2520same%2520input%2520prompt.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520novel%2520decoding%2520strategy%252C%2520Avoidance%2520Decoding%252C%2520that%2520modifies%2520token%250Alogits%2520by%2520penalizing%2520similarity%2520to%2520previously%2520generated%2520outputs%252C%2520thereby%250Aencouraging%2520more%2520diverse%2520multi-branch%2520stories.%2520This%2520penalty%2520adaptively%2520balances%250Atwo%2520similarity%2520measures%253A%2520%25281%2529%2520Concept-level%2520Similarity%2520Penalty%252C%2520which%2520is%250Aprioritized%2520in%2520early%2520stages%2520to%2520diversify%2520initial%2520story%2520concepts%252C%2520and%2520%25282%2529%250ANarrative-level%2520Similarity%2520Penalty%252C%2520which%2520is%2520increasingly%2520emphasized%2520later%2520to%250Aensure%2520natural%2520yet%2520diverse%2520plot%2520development.%2520Notably%252C%2520our%2520method%2520achieves%2520up%2520to%250A2.6%2520times%2520higher%2520output%2520diversity%2520and%2520reduces%2520repetition%2520by%2520an%2520average%2520of%252030%2525%250Acompared%2520to%2520strong%2520baselines%252C%2520while%2520effectively%2520mitigating%2520text%2520degeneration.%250AFurthermore%252C%2520we%2520reveal%2520that%2520our%2520method%2520activates%2520a%2520broader%2520range%2520of%2520neurons%252C%250Ademonstrating%2520that%2520it%2520leverages%2520the%2520model%2527s%2520intrinsic%2520creativity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Avoidance%20Decoding%20for%20Diverse%20Multi-Branch%20Story%20Generation&entry.906535625=Kyeongman%20Park%20and%20Nakyeong%20Yang%20and%20Kyomin%20Jung&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20generate%20repetitive%20and%20monotonous%0Aoutputs%2C%20especially%20in%20tasks%20like%20story%20generation%2C%20due%20to%20limited%20creative%0Adiversity%20when%20given%20the%20same%20input%20prompt.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20novel%20decoding%20strategy%2C%20Avoidance%20Decoding%2C%20that%20modifies%20token%0Alogits%20by%20penalizing%20similarity%20to%20previously%20generated%20outputs%2C%20thereby%0Aencouraging%20more%20diverse%20multi-branch%20stories.%20This%20penalty%20adaptively%20balances%0Atwo%20similarity%20measures%3A%20%281%29%20Concept-level%20Similarity%20Penalty%2C%20which%20is%0Aprioritized%20in%20early%20stages%20to%20diversify%20initial%20story%20concepts%2C%20and%20%282%29%0ANarrative-level%20Similarity%20Penalty%2C%20which%20is%20increasingly%20emphasized%20later%20to%0Aensure%20natural%20yet%20diverse%20plot%20development.%20Notably%2C%20our%20method%20achieves%20up%20to%0A2.6%20times%20higher%20output%20diversity%20and%20reduces%20repetition%20by%20an%20average%20of%2030%25%0Acompared%20to%20strong%20baselines%2C%20while%20effectively%20mitigating%20text%20degeneration.%0AFurthermore%2C%20we%20reveal%20that%20our%20method%20activates%20a%20broader%20range%20of%20neurons%2C%0Ademonstrating%20that%20it%20leverages%20the%20model%27s%20intrinsic%20creativity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02170v2&entry.124074799=Read"},
{"title": "PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly\n  Detection", "author": "Qihang Zhou and Shibo He and Jiangtao Yan and Wenchao Meng and Jiming Chen", "abstract": "  In this paper, we aim to transfer CLIP's robust 2D generalization\ncapabilities to identify 3D anomalies across unseen objects of highly diverse\nclass semantics. To this end, we propose a unified framework to comprehensively\ndetect and segment 3D anomalies by leveraging both point- and pixel-level\ninformation. We first design PointAD, which leverages point-pixel\ncorrespondence to represent 3D anomalies through their associated rendering\npixel representations. This approach is referred to as implicit 3D\nrepresentation, as it focuses solely on rendering pixel anomalies but neglects\nthe inherent spatial relationships within point clouds. Then, we propose\nPointAD+ to further broaden the interpretation of 3D anomalies by introducing\nexplicit 3D representation, emphasizing spatial abnormality to uncover abnormal\nspatial relationships. Hence, we propose G-aggregation to involve geometry\ninformation to enable the aggregated point representations spatially aware. To\nsimultaneously capture rendering and spatial abnormality, PointAD+ proposes\nhierarchical representation learning, incorporating implicit and explicit\nanomaly semantics into hierarchical text prompts: rendering prompts for the\nrendering layer and geometry prompts for the geometry layer. A cross-hierarchy\ncontrastive alignment is further introduced to promote the interaction between\nthe rendering and geometry layers, facilitating mutual anomaly learning.\nFinally, PointAD+ integrates anomaly semantics from both layers to capture the\ngeneralized anomaly semantics. During the test, PointAD+ can integrate RGB\ninformation in a plug-and-play manner and further improve its detection\nperformance. Extensive experiments demonstrate the superiority of PointAD+ in\nZS 3D anomaly detection across unseen objects with highly diverse class\nsemantics, achieving a holistic understanding of abnormality.\n", "link": "http://arxiv.org/abs/2509.03277v1", "date": "2025-09-03", "relevancy": 2.4921, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointAD%2B%3A%20Learning%20Hierarchical%20Representations%20for%20Zero-shot%203D%20Anomaly%0A%20%20Detection&body=Title%3A%20PointAD%2B%3A%20Learning%20Hierarchical%20Representations%20for%20Zero-shot%203D%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Qihang%20Zhou%20and%20Shibo%20He%20and%20Jiangtao%20Yan%20and%20Wenchao%20Meng%20and%20Jiming%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20transfer%20CLIP%27s%20robust%202D%20generalization%0Acapabilities%20to%20identify%203D%20anomalies%20across%20unseen%20objects%20of%20highly%20diverse%0Aclass%20semantics.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20to%20comprehensively%0Adetect%20and%20segment%203D%20anomalies%20by%20leveraging%20both%20point-%20and%20pixel-level%0Ainformation.%20We%20first%20design%20PointAD%2C%20which%20leverages%20point-pixel%0Acorrespondence%20to%20represent%203D%20anomalies%20through%20their%20associated%20rendering%0Apixel%20representations.%20This%20approach%20is%20referred%20to%20as%20implicit%203D%0Arepresentation%2C%20as%20it%20focuses%20solely%20on%20rendering%20pixel%20anomalies%20but%20neglects%0Athe%20inherent%20spatial%20relationships%20within%20point%20clouds.%20Then%2C%20we%20propose%0APointAD%2B%20to%20further%20broaden%20the%20interpretation%20of%203D%20anomalies%20by%20introducing%0Aexplicit%203D%20representation%2C%20emphasizing%20spatial%20abnormality%20to%20uncover%20abnormal%0Aspatial%20relationships.%20Hence%2C%20we%20propose%20G-aggregation%20to%20involve%20geometry%0Ainformation%20to%20enable%20the%20aggregated%20point%20representations%20spatially%20aware.%20To%0Asimultaneously%20capture%20rendering%20and%20spatial%20abnormality%2C%20PointAD%2B%20proposes%0Ahierarchical%20representation%20learning%2C%20incorporating%20implicit%20and%20explicit%0Aanomaly%20semantics%20into%20hierarchical%20text%20prompts%3A%20rendering%20prompts%20for%20the%0Arendering%20layer%20and%20geometry%20prompts%20for%20the%20geometry%20layer.%20A%20cross-hierarchy%0Acontrastive%20alignment%20is%20further%20introduced%20to%20promote%20the%20interaction%20between%0Athe%20rendering%20and%20geometry%20layers%2C%20facilitating%20mutual%20anomaly%20learning.%0AFinally%2C%20PointAD%2B%20integrates%20anomaly%20semantics%20from%20both%20layers%20to%20capture%20the%0Ageneralized%20anomaly%20semantics.%20During%20the%20test%2C%20PointAD%2B%20can%20integrate%20RGB%0Ainformation%20in%20a%20plug-and-play%20manner%20and%20further%20improve%20its%20detection%0Aperformance.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20PointAD%2B%20in%0AZS%203D%20anomaly%20detection%20across%20unseen%20objects%20with%20highly%20diverse%20class%0Asemantics%2C%20achieving%20a%20holistic%20understanding%20of%20abnormality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointAD%252B%253A%2520Learning%2520Hierarchical%2520Representations%2520for%2520Zero-shot%25203D%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DQihang%2520Zhou%2520and%2520Shibo%2520He%2520and%2520Jiangtao%2520Yan%2520and%2520Wenchao%2520Meng%2520and%2520Jiming%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520transfer%2520CLIP%2527s%2520robust%25202D%2520generalization%250Acapabilities%2520to%2520identify%25203D%2520anomalies%2520across%2520unseen%2520objects%2520of%2520highly%2520diverse%250Aclass%2520semantics.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520unified%2520framework%2520to%2520comprehensively%250Adetect%2520and%2520segment%25203D%2520anomalies%2520by%2520leveraging%2520both%2520point-%2520and%2520pixel-level%250Ainformation.%2520We%2520first%2520design%2520PointAD%252C%2520which%2520leverages%2520point-pixel%250Acorrespondence%2520to%2520represent%25203D%2520anomalies%2520through%2520their%2520associated%2520rendering%250Apixel%2520representations.%2520This%2520approach%2520is%2520referred%2520to%2520as%2520implicit%25203D%250Arepresentation%252C%2520as%2520it%2520focuses%2520solely%2520on%2520rendering%2520pixel%2520anomalies%2520but%2520neglects%250Athe%2520inherent%2520spatial%2520relationships%2520within%2520point%2520clouds.%2520Then%252C%2520we%2520propose%250APointAD%252B%2520to%2520further%2520broaden%2520the%2520interpretation%2520of%25203D%2520anomalies%2520by%2520introducing%250Aexplicit%25203D%2520representation%252C%2520emphasizing%2520spatial%2520abnormality%2520to%2520uncover%2520abnormal%250Aspatial%2520relationships.%2520Hence%252C%2520we%2520propose%2520G-aggregation%2520to%2520involve%2520geometry%250Ainformation%2520to%2520enable%2520the%2520aggregated%2520point%2520representations%2520spatially%2520aware.%2520To%250Asimultaneously%2520capture%2520rendering%2520and%2520spatial%2520abnormality%252C%2520PointAD%252B%2520proposes%250Ahierarchical%2520representation%2520learning%252C%2520incorporating%2520implicit%2520and%2520explicit%250Aanomaly%2520semantics%2520into%2520hierarchical%2520text%2520prompts%253A%2520rendering%2520prompts%2520for%2520the%250Arendering%2520layer%2520and%2520geometry%2520prompts%2520for%2520the%2520geometry%2520layer.%2520A%2520cross-hierarchy%250Acontrastive%2520alignment%2520is%2520further%2520introduced%2520to%2520promote%2520the%2520interaction%2520between%250Athe%2520rendering%2520and%2520geometry%2520layers%252C%2520facilitating%2520mutual%2520anomaly%2520learning.%250AFinally%252C%2520PointAD%252B%2520integrates%2520anomaly%2520semantics%2520from%2520both%2520layers%2520to%2520capture%2520the%250Ageneralized%2520anomaly%2520semantics.%2520During%2520the%2520test%252C%2520PointAD%252B%2520can%2520integrate%2520RGB%250Ainformation%2520in%2520a%2520plug-and-play%2520manner%2520and%2520further%2520improve%2520its%2520detection%250Aperformance.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520PointAD%252B%2520in%250AZS%25203D%2520anomaly%2520detection%2520across%2520unseen%2520objects%2520with%2520highly%2520diverse%2520class%250Asemantics%252C%2520achieving%2520a%2520holistic%2520understanding%2520of%2520abnormality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointAD%2B%3A%20Learning%20Hierarchical%20Representations%20for%20Zero-shot%203D%20Anomaly%0A%20%20Detection&entry.906535625=Qihang%20Zhou%20and%20Shibo%20He%20and%20Jiangtao%20Yan%20and%20Wenchao%20Meng%20and%20Jiming%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20transfer%20CLIP%27s%20robust%202D%20generalization%0Acapabilities%20to%20identify%203D%20anomalies%20across%20unseen%20objects%20of%20highly%20diverse%0Aclass%20semantics.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20to%20comprehensively%0Adetect%20and%20segment%203D%20anomalies%20by%20leveraging%20both%20point-%20and%20pixel-level%0Ainformation.%20We%20first%20design%20PointAD%2C%20which%20leverages%20point-pixel%0Acorrespondence%20to%20represent%203D%20anomalies%20through%20their%20associated%20rendering%0Apixel%20representations.%20This%20approach%20is%20referred%20to%20as%20implicit%203D%0Arepresentation%2C%20as%20it%20focuses%20solely%20on%20rendering%20pixel%20anomalies%20but%20neglects%0Athe%20inherent%20spatial%20relationships%20within%20point%20clouds.%20Then%2C%20we%20propose%0APointAD%2B%20to%20further%20broaden%20the%20interpretation%20of%203D%20anomalies%20by%20introducing%0Aexplicit%203D%20representation%2C%20emphasizing%20spatial%20abnormality%20to%20uncover%20abnormal%0Aspatial%20relationships.%20Hence%2C%20we%20propose%20G-aggregation%20to%20involve%20geometry%0Ainformation%20to%20enable%20the%20aggregated%20point%20representations%20spatially%20aware.%20To%0Asimultaneously%20capture%20rendering%20and%20spatial%20abnormality%2C%20PointAD%2B%20proposes%0Ahierarchical%20representation%20learning%2C%20incorporating%20implicit%20and%20explicit%0Aanomaly%20semantics%20into%20hierarchical%20text%20prompts%3A%20rendering%20prompts%20for%20the%0Arendering%20layer%20and%20geometry%20prompts%20for%20the%20geometry%20layer.%20A%20cross-hierarchy%0Acontrastive%20alignment%20is%20further%20introduced%20to%20promote%20the%20interaction%20between%0Athe%20rendering%20and%20geometry%20layers%2C%20facilitating%20mutual%20anomaly%20learning.%0AFinally%2C%20PointAD%2B%20integrates%20anomaly%20semantics%20from%20both%20layers%20to%20capture%20the%0Ageneralized%20anomaly%20semantics.%20During%20the%20test%2C%20PointAD%2B%20can%20integrate%20RGB%0Ainformation%20in%20a%20plug-and-play%20manner%20and%20further%20improve%20its%20detection%0Aperformance.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20PointAD%2B%20in%0AZS%203D%20anomaly%20detection%20across%20unseen%20objects%20with%20highly%20diverse%20class%0Asemantics%2C%20achieving%20a%20holistic%20understanding%20of%20abnormality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03277v1&entry.124074799=Read"},
{"title": "Decoding Visual Neural Representations by Multimodal with Dynamic\n  Balancing", "author": "Kaili sun and Xingyu Miao and Bing Zhai and Haoran Duan and Yang Long", "abstract": "  In this work, we propose an innovative framework that integrates EEG, image,\nand text data, aiming to decode visual neural representations from low\nsignal-to-noise ratio EEG signals. Specifically, we introduce text modality to\nenhance the semantic correspondence between EEG signals and visual content.\nWith the explicit semantic labels provided by text, image and EEG features of\nthe same category can be more closely aligned with the corresponding text\nrepresentations in a shared multimodal space. To fully utilize pre-trained\nvisual and textual representations, we propose an adapter module that\nalleviates the instability of high-dimensional representation while\nfacilitating the alignment and fusion of cross-modal features. Additionally, to\nalleviate the imbalance in multimodal feature contributions introduced by the\ntextual representations, we propose a Modal Consistency Dynamic Balance (MCDB)\nstrategy that dynamically adjusts the contribution weights of each modality. We\nfurther propose a stochastic perturbation regularization (SPR) term to enhance\nthe generalization ability of semantic perturbation-based models by introducing\ndynamic Gaussian noise in the modality optimization process. The evaluation\nresults on the ThingsEEG dataset show that our method surpasses previous\nstate-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by\n2.0\\% and 4.7\\% respectively.\n", "link": "http://arxiv.org/abs/2509.03433v1", "date": "2025-09-03", "relevancy": 2.3626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5974}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Visual%20Neural%20Representations%20by%20Multimodal%20with%20Dynamic%0A%20%20Balancing&body=Title%3A%20Decoding%20Visual%20Neural%20Representations%20by%20Multimodal%20with%20Dynamic%0A%20%20Balancing%0AAuthor%3A%20Kaili%20sun%20and%20Xingyu%20Miao%20and%20Bing%20Zhai%20and%20Haoran%20Duan%20and%20Yang%20Long%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20an%20innovative%20framework%20that%20integrates%20EEG%2C%20image%2C%0Aand%20text%20data%2C%20aiming%20to%20decode%20visual%20neural%20representations%20from%20low%0Asignal-to-noise%20ratio%20EEG%20signals.%20Specifically%2C%20we%20introduce%20text%20modality%20to%0Aenhance%20the%20semantic%20correspondence%20between%20EEG%20signals%20and%20visual%20content.%0AWith%20the%20explicit%20semantic%20labels%20provided%20by%20text%2C%20image%20and%20EEG%20features%20of%0Athe%20same%20category%20can%20be%20more%20closely%20aligned%20with%20the%20corresponding%20text%0Arepresentations%20in%20a%20shared%20multimodal%20space.%20To%20fully%20utilize%20pre-trained%0Avisual%20and%20textual%20representations%2C%20we%20propose%20an%20adapter%20module%20that%0Aalleviates%20the%20instability%20of%20high-dimensional%20representation%20while%0Afacilitating%20the%20alignment%20and%20fusion%20of%20cross-modal%20features.%20Additionally%2C%20to%0Aalleviate%20the%20imbalance%20in%20multimodal%20feature%20contributions%20introduced%20by%20the%0Atextual%20representations%2C%20we%20propose%20a%20Modal%20Consistency%20Dynamic%20Balance%20%28MCDB%29%0Astrategy%20that%20dynamically%20adjusts%20the%20contribution%20weights%20of%20each%20modality.%20We%0Afurther%20propose%20a%20stochastic%20perturbation%20regularization%20%28SPR%29%20term%20to%20enhance%0Athe%20generalization%20ability%20of%20semantic%20perturbation-based%20models%20by%20introducing%0Adynamic%20Gaussian%20noise%20in%20the%20modality%20optimization%20process.%20The%20evaluation%0Aresults%20on%20the%20ThingsEEG%20dataset%20show%20that%20our%20method%20surpasses%20previous%0Astate-of-the-art%20methods%20in%20both%20Top-1%20and%20Top-5%20accuracy%20metrics%2C%20improving%20by%0A2.0%5C%25%20and%204.7%5C%25%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Visual%2520Neural%2520Representations%2520by%2520Multimodal%2520with%2520Dynamic%250A%2520%2520Balancing%26entry.906535625%3DKaili%2520sun%2520and%2520Xingyu%2520Miao%2520and%2520Bing%2520Zhai%2520and%2520Haoran%2520Duan%2520and%2520Yang%2520Long%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520innovative%2520framework%2520that%2520integrates%2520EEG%252C%2520image%252C%250Aand%2520text%2520data%252C%2520aiming%2520to%2520decode%2520visual%2520neural%2520representations%2520from%2520low%250Asignal-to-noise%2520ratio%2520EEG%2520signals.%2520Specifically%252C%2520we%2520introduce%2520text%2520modality%2520to%250Aenhance%2520the%2520semantic%2520correspondence%2520between%2520EEG%2520signals%2520and%2520visual%2520content.%250AWith%2520the%2520explicit%2520semantic%2520labels%2520provided%2520by%2520text%252C%2520image%2520and%2520EEG%2520features%2520of%250Athe%2520same%2520category%2520can%2520be%2520more%2520closely%2520aligned%2520with%2520the%2520corresponding%2520text%250Arepresentations%2520in%2520a%2520shared%2520multimodal%2520space.%2520To%2520fully%2520utilize%2520pre-trained%250Avisual%2520and%2520textual%2520representations%252C%2520we%2520propose%2520an%2520adapter%2520module%2520that%250Aalleviates%2520the%2520instability%2520of%2520high-dimensional%2520representation%2520while%250Afacilitating%2520the%2520alignment%2520and%2520fusion%2520of%2520cross-modal%2520features.%2520Additionally%252C%2520to%250Aalleviate%2520the%2520imbalance%2520in%2520multimodal%2520feature%2520contributions%2520introduced%2520by%2520the%250Atextual%2520representations%252C%2520we%2520propose%2520a%2520Modal%2520Consistency%2520Dynamic%2520Balance%2520%2528MCDB%2529%250Astrategy%2520that%2520dynamically%2520adjusts%2520the%2520contribution%2520weights%2520of%2520each%2520modality.%2520We%250Afurther%2520propose%2520a%2520stochastic%2520perturbation%2520regularization%2520%2528SPR%2529%2520term%2520to%2520enhance%250Athe%2520generalization%2520ability%2520of%2520semantic%2520perturbation-based%2520models%2520by%2520introducing%250Adynamic%2520Gaussian%2520noise%2520in%2520the%2520modality%2520optimization%2520process.%2520The%2520evaluation%250Aresults%2520on%2520the%2520ThingsEEG%2520dataset%2520show%2520that%2520our%2520method%2520surpasses%2520previous%250Astate-of-the-art%2520methods%2520in%2520both%2520Top-1%2520and%2520Top-5%2520accuracy%2520metrics%252C%2520improving%2520by%250A2.0%255C%2525%2520and%25204.7%255C%2525%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Visual%20Neural%20Representations%20by%20Multimodal%20with%20Dynamic%0A%20%20Balancing&entry.906535625=Kaili%20sun%20and%20Xingyu%20Miao%20and%20Bing%20Zhai%20and%20Haoran%20Duan%20and%20Yang%20Long&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20an%20innovative%20framework%20that%20integrates%20EEG%2C%20image%2C%0Aand%20text%20data%2C%20aiming%20to%20decode%20visual%20neural%20representations%20from%20low%0Asignal-to-noise%20ratio%20EEG%20signals.%20Specifically%2C%20we%20introduce%20text%20modality%20to%0Aenhance%20the%20semantic%20correspondence%20between%20EEG%20signals%20and%20visual%20content.%0AWith%20the%20explicit%20semantic%20labels%20provided%20by%20text%2C%20image%20and%20EEG%20features%20of%0Athe%20same%20category%20can%20be%20more%20closely%20aligned%20with%20the%20corresponding%20text%0Arepresentations%20in%20a%20shared%20multimodal%20space.%20To%20fully%20utilize%20pre-trained%0Avisual%20and%20textual%20representations%2C%20we%20propose%20an%20adapter%20module%20that%0Aalleviates%20the%20instability%20of%20high-dimensional%20representation%20while%0Afacilitating%20the%20alignment%20and%20fusion%20of%20cross-modal%20features.%20Additionally%2C%20to%0Aalleviate%20the%20imbalance%20in%20multimodal%20feature%20contributions%20introduced%20by%20the%0Atextual%20representations%2C%20we%20propose%20a%20Modal%20Consistency%20Dynamic%20Balance%20%28MCDB%29%0Astrategy%20that%20dynamically%20adjusts%20the%20contribution%20weights%20of%20each%20modality.%20We%0Afurther%20propose%20a%20stochastic%20perturbation%20regularization%20%28SPR%29%20term%20to%20enhance%0Athe%20generalization%20ability%20of%20semantic%20perturbation-based%20models%20by%20introducing%0Adynamic%20Gaussian%20noise%20in%20the%20modality%20optimization%20process.%20The%20evaluation%0Aresults%20on%20the%20ThingsEEG%20dataset%20show%20that%20our%20method%20surpasses%20previous%0Astate-of-the-art%20methods%20in%20both%20Top-1%20and%20Top-5%20accuracy%20metrics%2C%20improving%20by%0A2.0%5C%25%20and%204.7%5C%25%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03433v1&entry.124074799=Read"},
{"title": "Generalist versus Specialist Vision Foundation Models for Ocular Disease\n  and Oculomics", "author": "Yukun Zhou and Paul Nderitu and Jocelyn Hui Lin Goh and Justin Engelmann and Siegfried K. Wagner and Anran Ran and Hongyang Jiang and Lie Ju and Ke Zou and Sahana Srinivasan and Hyunmin Kim and Takahiro Ninomiya and Zheyuan Wang and Gabriel Dawei Yang and Eden Ruffell and Dominic Williamson and Rui Santos and Gabor Mark Somfai and Carol Y. Cheung and Tien Yin Wong and Daniel C. Alexander and Yih Chung Tham and Pearse A. Keane", "abstract": "  Medical foundation models, pre-trained with large-scale clinical data,\ndemonstrate strong performance in diverse clinically relevant applications.\nRETFound, trained on nearly one million retinal images, exemplifies this\napproach in applications with retinal images. However, the emergence of\nincreasingly powerful and multifold larger generalist foundation models such as\nDINOv2 and DINOv3 raises the question of whether domain-specific pre-training\nremains essential, and if so, what gap persists. To investigate this, we\nsystematically evaluated the adaptability of DINOv2 and DINOv3 in retinal image\napplications, compared to two specialist RETFound models, RETFound-MAE and\nRETFound-DINOv2. We assessed performance on ocular disease detection and\nsystemic disease prediction using two adaptation strategies: fine-tuning and\nlinear probing. Data efficiency and adaptation efficiency were further analysed\nto characterise trade-offs between predictive performance and computational\ncost. Our results show that although scaling generalist models yields strong\nadaptability across diverse tasks, RETFound-DINOv2 consistently outperforms\nthese generalist foundation models in ocular-disease detection and oculomics\ntasks, demonstrating stronger generalisability and data efficiency. These\nfindings suggest that specialist retinal foundation models remain the most\neffective choice for clinical applications, while the narrowing gap with\ngeneralist foundation models suggests that continued data and model scaling can\ndeliver domain-relevant gains and position them as strong foundations for\nfuture medical foundation models.\n", "link": "http://arxiv.org/abs/2509.03421v1", "date": "2025-09-03", "relevancy": 2.3529, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalist%20versus%20Specialist%20Vision%20Foundation%20Models%20for%20Ocular%20Disease%0A%20%20and%20Oculomics&body=Title%3A%20Generalist%20versus%20Specialist%20Vision%20Foundation%20Models%20for%20Ocular%20Disease%0A%20%20and%20Oculomics%0AAuthor%3A%20Yukun%20Zhou%20and%20Paul%20Nderitu%20and%20Jocelyn%20Hui%20Lin%20Goh%20and%20Justin%20Engelmann%20and%20Siegfried%20K.%20Wagner%20and%20Anran%20Ran%20and%20Hongyang%20Jiang%20and%20Lie%20Ju%20and%20Ke%20Zou%20and%20Sahana%20Srinivasan%20and%20Hyunmin%20Kim%20and%20Takahiro%20Ninomiya%20and%20Zheyuan%20Wang%20and%20Gabriel%20Dawei%20Yang%20and%20Eden%20Ruffell%20and%20Dominic%20Williamson%20and%20Rui%20Santos%20and%20Gabor%20Mark%20Somfai%20and%20Carol%20Y.%20Cheung%20and%20Tien%20Yin%20Wong%20and%20Daniel%20C.%20Alexander%20and%20Yih%20Chung%20Tham%20and%20Pearse%20A.%20Keane%0AAbstract%3A%20%20%20Medical%20foundation%20models%2C%20pre-trained%20with%20large-scale%20clinical%20data%2C%0Ademonstrate%20strong%20performance%20in%20diverse%20clinically%20relevant%20applications.%0ARETFound%2C%20trained%20on%20nearly%20one%20million%20retinal%20images%2C%20exemplifies%20this%0Aapproach%20in%20applications%20with%20retinal%20images.%20However%2C%20the%20emergence%20of%0Aincreasingly%20powerful%20and%20multifold%20larger%20generalist%20foundation%20models%20such%20as%0ADINOv2%20and%20DINOv3%20raises%20the%20question%20of%20whether%20domain-specific%20pre-training%0Aremains%20essential%2C%20and%20if%20so%2C%20what%20gap%20persists.%20To%20investigate%20this%2C%20we%0Asystematically%20evaluated%20the%20adaptability%20of%20DINOv2%20and%20DINOv3%20in%20retinal%20image%0Aapplications%2C%20compared%20to%20two%20specialist%20RETFound%20models%2C%20RETFound-MAE%20and%0ARETFound-DINOv2.%20We%20assessed%20performance%20on%20ocular%20disease%20detection%20and%0Asystemic%20disease%20prediction%20using%20two%20adaptation%20strategies%3A%20fine-tuning%20and%0Alinear%20probing.%20Data%20efficiency%20and%20adaptation%20efficiency%20were%20further%20analysed%0Ato%20characterise%20trade-offs%20between%20predictive%20performance%20and%20computational%0Acost.%20Our%20results%20show%20that%20although%20scaling%20generalist%20models%20yields%20strong%0Aadaptability%20across%20diverse%20tasks%2C%20RETFound-DINOv2%20consistently%20outperforms%0Athese%20generalist%20foundation%20models%20in%20ocular-disease%20detection%20and%20oculomics%0Atasks%2C%20demonstrating%20stronger%20generalisability%20and%20data%20efficiency.%20These%0Afindings%20suggest%20that%20specialist%20retinal%20foundation%20models%20remain%20the%20most%0Aeffective%20choice%20for%20clinical%20applications%2C%20while%20the%20narrowing%20gap%20with%0Ageneralist%20foundation%20models%20suggests%20that%20continued%20data%20and%20model%20scaling%20can%0Adeliver%20domain-relevant%20gains%20and%20position%20them%20as%20strong%20foundations%20for%0Afuture%20medical%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralist%2520versus%2520Specialist%2520Vision%2520Foundation%2520Models%2520for%2520Ocular%2520Disease%250A%2520%2520and%2520Oculomics%26entry.906535625%3DYukun%2520Zhou%2520and%2520Paul%2520Nderitu%2520and%2520Jocelyn%2520Hui%2520Lin%2520Goh%2520and%2520Justin%2520Engelmann%2520and%2520Siegfried%2520K.%2520Wagner%2520and%2520Anran%2520Ran%2520and%2520Hongyang%2520Jiang%2520and%2520Lie%2520Ju%2520and%2520Ke%2520Zou%2520and%2520Sahana%2520Srinivasan%2520and%2520Hyunmin%2520Kim%2520and%2520Takahiro%2520Ninomiya%2520and%2520Zheyuan%2520Wang%2520and%2520Gabriel%2520Dawei%2520Yang%2520and%2520Eden%2520Ruffell%2520and%2520Dominic%2520Williamson%2520and%2520Rui%2520Santos%2520and%2520Gabor%2520Mark%2520Somfai%2520and%2520Carol%2520Y.%2520Cheung%2520and%2520Tien%2520Yin%2520Wong%2520and%2520Daniel%2520C.%2520Alexander%2520and%2520Yih%2520Chung%2520Tham%2520and%2520Pearse%2520A.%2520Keane%26entry.1292438233%3D%2520%2520Medical%2520foundation%2520models%252C%2520pre-trained%2520with%2520large-scale%2520clinical%2520data%252C%250Ademonstrate%2520strong%2520performance%2520in%2520diverse%2520clinically%2520relevant%2520applications.%250ARETFound%252C%2520trained%2520on%2520nearly%2520one%2520million%2520retinal%2520images%252C%2520exemplifies%2520this%250Aapproach%2520in%2520applications%2520with%2520retinal%2520images.%2520However%252C%2520the%2520emergence%2520of%250Aincreasingly%2520powerful%2520and%2520multifold%2520larger%2520generalist%2520foundation%2520models%2520such%2520as%250ADINOv2%2520and%2520DINOv3%2520raises%2520the%2520question%2520of%2520whether%2520domain-specific%2520pre-training%250Aremains%2520essential%252C%2520and%2520if%2520so%252C%2520what%2520gap%2520persists.%2520To%2520investigate%2520this%252C%2520we%250Asystematically%2520evaluated%2520the%2520adaptability%2520of%2520DINOv2%2520and%2520DINOv3%2520in%2520retinal%2520image%250Aapplications%252C%2520compared%2520to%2520two%2520specialist%2520RETFound%2520models%252C%2520RETFound-MAE%2520and%250ARETFound-DINOv2.%2520We%2520assessed%2520performance%2520on%2520ocular%2520disease%2520detection%2520and%250Asystemic%2520disease%2520prediction%2520using%2520two%2520adaptation%2520strategies%253A%2520fine-tuning%2520and%250Alinear%2520probing.%2520Data%2520efficiency%2520and%2520adaptation%2520efficiency%2520were%2520further%2520analysed%250Ato%2520characterise%2520trade-offs%2520between%2520predictive%2520performance%2520and%2520computational%250Acost.%2520Our%2520results%2520show%2520that%2520although%2520scaling%2520generalist%2520models%2520yields%2520strong%250Aadaptability%2520across%2520diverse%2520tasks%252C%2520RETFound-DINOv2%2520consistently%2520outperforms%250Athese%2520generalist%2520foundation%2520models%2520in%2520ocular-disease%2520detection%2520and%2520oculomics%250Atasks%252C%2520demonstrating%2520stronger%2520generalisability%2520and%2520data%2520efficiency.%2520These%250Afindings%2520suggest%2520that%2520specialist%2520retinal%2520foundation%2520models%2520remain%2520the%2520most%250Aeffective%2520choice%2520for%2520clinical%2520applications%252C%2520while%2520the%2520narrowing%2520gap%2520with%250Ageneralist%2520foundation%2520models%2520suggests%2520that%2520continued%2520data%2520and%2520model%2520scaling%2520can%250Adeliver%2520domain-relevant%2520gains%2520and%2520position%2520them%2520as%2520strong%2520foundations%2520for%250Afuture%2520medical%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalist%20versus%20Specialist%20Vision%20Foundation%20Models%20for%20Ocular%20Disease%0A%20%20and%20Oculomics&entry.906535625=Yukun%20Zhou%20and%20Paul%20Nderitu%20and%20Jocelyn%20Hui%20Lin%20Goh%20and%20Justin%20Engelmann%20and%20Siegfried%20K.%20Wagner%20and%20Anran%20Ran%20and%20Hongyang%20Jiang%20and%20Lie%20Ju%20and%20Ke%20Zou%20and%20Sahana%20Srinivasan%20and%20Hyunmin%20Kim%20and%20Takahiro%20Ninomiya%20and%20Zheyuan%20Wang%20and%20Gabriel%20Dawei%20Yang%20and%20Eden%20Ruffell%20and%20Dominic%20Williamson%20and%20Rui%20Santos%20and%20Gabor%20Mark%20Somfai%20and%20Carol%20Y.%20Cheung%20and%20Tien%20Yin%20Wong%20and%20Daniel%20C.%20Alexander%20and%20Yih%20Chung%20Tham%20and%20Pearse%20A.%20Keane&entry.1292438233=%20%20Medical%20foundation%20models%2C%20pre-trained%20with%20large-scale%20clinical%20data%2C%0Ademonstrate%20strong%20performance%20in%20diverse%20clinically%20relevant%20applications.%0ARETFound%2C%20trained%20on%20nearly%20one%20million%20retinal%20images%2C%20exemplifies%20this%0Aapproach%20in%20applications%20with%20retinal%20images.%20However%2C%20the%20emergence%20of%0Aincreasingly%20powerful%20and%20multifold%20larger%20generalist%20foundation%20models%20such%20as%0ADINOv2%20and%20DINOv3%20raises%20the%20question%20of%20whether%20domain-specific%20pre-training%0Aremains%20essential%2C%20and%20if%20so%2C%20what%20gap%20persists.%20To%20investigate%20this%2C%20we%0Asystematically%20evaluated%20the%20adaptability%20of%20DINOv2%20and%20DINOv3%20in%20retinal%20image%0Aapplications%2C%20compared%20to%20two%20specialist%20RETFound%20models%2C%20RETFound-MAE%20and%0ARETFound-DINOv2.%20We%20assessed%20performance%20on%20ocular%20disease%20detection%20and%0Asystemic%20disease%20prediction%20using%20two%20adaptation%20strategies%3A%20fine-tuning%20and%0Alinear%20probing.%20Data%20efficiency%20and%20adaptation%20efficiency%20were%20further%20analysed%0Ato%20characterise%20trade-offs%20between%20predictive%20performance%20and%20computational%0Acost.%20Our%20results%20show%20that%20although%20scaling%20generalist%20models%20yields%20strong%0Aadaptability%20across%20diverse%20tasks%2C%20RETFound-DINOv2%20consistently%20outperforms%0Athese%20generalist%20foundation%20models%20in%20ocular-disease%20detection%20and%20oculomics%0Atasks%2C%20demonstrating%20stronger%20generalisability%20and%20data%20efficiency.%20These%0Afindings%20suggest%20that%20specialist%20retinal%20foundation%20models%20remain%20the%20most%0Aeffective%20choice%20for%20clinical%20applications%2C%20while%20the%20narrowing%20gap%20with%0Ageneralist%20foundation%20models%20suggests%20that%20continued%20data%20and%20model%20scaling%20can%0Adeliver%20domain-relevant%20gains%20and%20position%20them%20as%20strong%20foundations%20for%0Afuture%20medical%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03421v1&entry.124074799=Read"},
{"title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?", "author": "Ouxiang Li and Yuan Wang and Xinting Hu and Huijuan Huang and Rui Chen and Jiarong Ou and Xin Tao and Pengfei Wan and Fuli Feng", "abstract": "  Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.\n", "link": "http://arxiv.org/abs/2509.03516v1", "date": "2025-09-03", "relevancy": 2.3492, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Easier%20Painting%20Than%20Thinking%3A%20Can%20Text-to-Image%20Models%20Set%20the%20Stage%2C%0A%20%20but%20Not%20Direct%20the%20Play%3F&body=Title%3A%20Easier%20Painting%20Than%20Thinking%3A%20Can%20Text-to-Image%20Models%20Set%20the%20Stage%2C%0A%20%20but%20Not%20Direct%20the%20Play%3F%0AAuthor%3A%20Ouxiang%20Li%20and%20Yuan%20Wang%20and%20Xinting%20Hu%20and%20Huijuan%20Huang%20and%20Rui%20Chen%20and%20Jiarong%20Ou%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Fuli%20Feng%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20aims%20to%20synthesize%20images%20from%20textual%0Aprompts%2C%20which%20jointly%20specify%20what%20must%20be%20shown%20and%20imply%20what%20can%20be%0Ainferred%2C%20thereby%20corresponding%20to%20two%20core%20capabilities%3A%20composition%20and%0Areasoning.%20However%2C%20with%20the%20emerging%20advances%20of%20T2I%20models%20in%20reasoning%0Abeyond%20composition%2C%20existing%20benchmarks%20reveal%20clear%20limitations%20in%20providing%0Acomprehensive%20evaluations%20across%20and%20within%20these%20capabilities.%20Meanwhile%2C%0Athese%20advances%20also%20enable%20models%20to%20handle%20more%20complex%20prompts%2C%20whereas%0Acurrent%20benchmarks%20remain%20limited%20to%20low%20scene%20density%20and%20simplified%0Aone-to-one%20reasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20T2I-CoReBench%2C%20a%0Acomprehensive%20and%20complex%20benchmark%20that%20evaluates%20both%20composition%20and%0Areasoning%20capabilities%20of%20T2I%20models.%20To%20ensure%20comprehensiveness%2C%20we%20structure%0Acomposition%20around%20scene%20graph%20elements%20%28instance%2C%20attribute%2C%20and%20relation%29%20and%0Areasoning%20around%20the%20philosophical%20framework%20of%20inference%20%28deductive%2C%0Ainductive%2C%20and%20abductive%29%2C%20formulating%20a%2012-dimensional%20evaluation%20taxonomy.%20To%0Aincrease%20complexity%2C%20driven%20by%20the%20inherent%20complexities%20of%20real-world%0Ascenarios%2C%20we%20curate%20each%20prompt%20with%20high%20compositional%20density%20for%0Acomposition%20and%20multi-step%20inference%20for%20reasoning.%20We%20also%20pair%20each%20prompt%0Awith%20a%20checklist%20that%20specifies%20individual%20yes/no%20questions%20to%20assess%20each%0Aintended%20element%20independently%20to%20facilitate%20fine-grained%20and%20reliable%0Aevaluation.%20In%20statistics%2C%20our%20benchmark%20comprises%201%2C080%20challenging%20prompts%0Aand%20around%2013%2C500%20checklist%20questions.%20Experiments%20across%2027%20current%20T2I%20models%0Areveal%20that%20their%20composition%20capability%20still%20remains%20limited%20in%20complex%0Ahigh-density%20scenarios%2C%20while%20the%20reasoning%20capability%20lags%20even%20further%20behind%0Aas%20a%20critical%20bottleneck%2C%20with%20all%20models%20struggling%20to%20infer%20implicit%20elements%0Afrom%20prompts.%20Our%20project%20page%3A%20https%3A//t2i-corebench.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasier%2520Painting%2520Than%2520Thinking%253A%2520Can%2520Text-to-Image%2520Models%2520Set%2520the%2520Stage%252C%250A%2520%2520but%2520Not%2520Direct%2520the%2520Play%253F%26entry.906535625%3DOuxiang%2520Li%2520and%2520Yuan%2520Wang%2520and%2520Xinting%2520Hu%2520and%2520Huijuan%2520Huang%2520and%2520Rui%2520Chen%2520and%2520Jiarong%2520Ou%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Fuli%2520Feng%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520aims%2520to%2520synthesize%2520images%2520from%2520textual%250Aprompts%252C%2520which%2520jointly%2520specify%2520what%2520must%2520be%2520shown%2520and%2520imply%2520what%2520can%2520be%250Ainferred%252C%2520thereby%2520corresponding%2520to%2520two%2520core%2520capabilities%253A%2520composition%2520and%250Areasoning.%2520However%252C%2520with%2520the%2520emerging%2520advances%2520of%2520T2I%2520models%2520in%2520reasoning%250Abeyond%2520composition%252C%2520existing%2520benchmarks%2520reveal%2520clear%2520limitations%2520in%2520providing%250Acomprehensive%2520evaluations%2520across%2520and%2520within%2520these%2520capabilities.%2520Meanwhile%252C%250Athese%2520advances%2520also%2520enable%2520models%2520to%2520handle%2520more%2520complex%2520prompts%252C%2520whereas%250Acurrent%2520benchmarks%2520remain%2520limited%2520to%2520low%2520scene%2520density%2520and%2520simplified%250Aone-to-one%2520reasoning.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520T2I-CoReBench%252C%2520a%250Acomprehensive%2520and%2520complex%2520benchmark%2520that%2520evaluates%2520both%2520composition%2520and%250Areasoning%2520capabilities%2520of%2520T2I%2520models.%2520To%2520ensure%2520comprehensiveness%252C%2520we%2520structure%250Acomposition%2520around%2520scene%2520graph%2520elements%2520%2528instance%252C%2520attribute%252C%2520and%2520relation%2529%2520and%250Areasoning%2520around%2520the%2520philosophical%2520framework%2520of%2520inference%2520%2528deductive%252C%250Ainductive%252C%2520and%2520abductive%2529%252C%2520formulating%2520a%252012-dimensional%2520evaluation%2520taxonomy.%2520To%250Aincrease%2520complexity%252C%2520driven%2520by%2520the%2520inherent%2520complexities%2520of%2520real-world%250Ascenarios%252C%2520we%2520curate%2520each%2520prompt%2520with%2520high%2520compositional%2520density%2520for%250Acomposition%2520and%2520multi-step%2520inference%2520for%2520reasoning.%2520We%2520also%2520pair%2520each%2520prompt%250Awith%2520a%2520checklist%2520that%2520specifies%2520individual%2520yes/no%2520questions%2520to%2520assess%2520each%250Aintended%2520element%2520independently%2520to%2520facilitate%2520fine-grained%2520and%2520reliable%250Aevaluation.%2520In%2520statistics%252C%2520our%2520benchmark%2520comprises%25201%252C080%2520challenging%2520prompts%250Aand%2520around%252013%252C500%2520checklist%2520questions.%2520Experiments%2520across%252027%2520current%2520T2I%2520models%250Areveal%2520that%2520their%2520composition%2520capability%2520still%2520remains%2520limited%2520in%2520complex%250Ahigh-density%2520scenarios%252C%2520while%2520the%2520reasoning%2520capability%2520lags%2520even%2520further%2520behind%250Aas%2520a%2520critical%2520bottleneck%252C%2520with%2520all%2520models%2520struggling%2520to%2520infer%2520implicit%2520elements%250Afrom%2520prompts.%2520Our%2520project%2520page%253A%2520https%253A//t2i-corebench.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Easier%20Painting%20Than%20Thinking%3A%20Can%20Text-to-Image%20Models%20Set%20the%20Stage%2C%0A%20%20but%20Not%20Direct%20the%20Play%3F&entry.906535625=Ouxiang%20Li%20and%20Yuan%20Wang%20and%20Xinting%20Hu%20and%20Huijuan%20Huang%20and%20Rui%20Chen%20and%20Jiarong%20Ou%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Fuli%20Feng&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20aims%20to%20synthesize%20images%20from%20textual%0Aprompts%2C%20which%20jointly%20specify%20what%20must%20be%20shown%20and%20imply%20what%20can%20be%0Ainferred%2C%20thereby%20corresponding%20to%20two%20core%20capabilities%3A%20composition%20and%0Areasoning.%20However%2C%20with%20the%20emerging%20advances%20of%20T2I%20models%20in%20reasoning%0Abeyond%20composition%2C%20existing%20benchmarks%20reveal%20clear%20limitations%20in%20providing%0Acomprehensive%20evaluations%20across%20and%20within%20these%20capabilities.%20Meanwhile%2C%0Athese%20advances%20also%20enable%20models%20to%20handle%20more%20complex%20prompts%2C%20whereas%0Acurrent%20benchmarks%20remain%20limited%20to%20low%20scene%20density%20and%20simplified%0Aone-to-one%20reasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20T2I-CoReBench%2C%20a%0Acomprehensive%20and%20complex%20benchmark%20that%20evaluates%20both%20composition%20and%0Areasoning%20capabilities%20of%20T2I%20models.%20To%20ensure%20comprehensiveness%2C%20we%20structure%0Acomposition%20around%20scene%20graph%20elements%20%28instance%2C%20attribute%2C%20and%20relation%29%20and%0Areasoning%20around%20the%20philosophical%20framework%20of%20inference%20%28deductive%2C%0Ainductive%2C%20and%20abductive%29%2C%20formulating%20a%2012-dimensional%20evaluation%20taxonomy.%20To%0Aincrease%20complexity%2C%20driven%20by%20the%20inherent%20complexities%20of%20real-world%0Ascenarios%2C%20we%20curate%20each%20prompt%20with%20high%20compositional%20density%20for%0Acomposition%20and%20multi-step%20inference%20for%20reasoning.%20We%20also%20pair%20each%20prompt%0Awith%20a%20checklist%20that%20specifies%20individual%20yes/no%20questions%20to%20assess%20each%0Aintended%20element%20independently%20to%20facilitate%20fine-grained%20and%20reliable%0Aevaluation.%20In%20statistics%2C%20our%20benchmark%20comprises%201%2C080%20challenging%20prompts%0Aand%20around%2013%2C500%20checklist%20questions.%20Experiments%20across%2027%20current%20T2I%20models%0Areveal%20that%20their%20composition%20capability%20still%20remains%20limited%20in%20complex%0Ahigh-density%20scenarios%2C%20while%20the%20reasoning%20capability%20lags%20even%20further%20behind%0Aas%20a%20critical%20bottleneck%2C%20with%20all%20models%20struggling%20to%20infer%20implicit%20elements%0Afrom%20prompts.%20Our%20project%20page%3A%20https%3A//t2i-corebench.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03516v1&entry.124074799=Read"},
{"title": "Temporal social network modeling of mobile connectivity data with graph\n  neural networks", "author": "Joel Jaskari and Chandreyee Roy and Fumiko Ogushi and Mikko Saukkoriipi and Jaakko Sahlsten and Kimmo Kaski", "abstract": "  Graph neural networks (GNNs) have emerged as a state-of-the-art data-driven\ntool for modeling connectivity data of graph-structured complex networks and\nintegrating information of their nodes and edges in space and time. However, as\nof yet, the analysis of social networks using the time series of people's\nmobile connectivity data has not been extensively investigated. In the present\nstudy, we investigate four snapshot - based temporal GNNs in predicting the\nphone call and SMS activity between users of a mobile communication network. In\naddition, we develop a simple non - GNN baseline model using recently proposed\nEdgeBank method. Our analysis shows that the ROLAND temporal GNN outperforms\nthe baseline model in most cases, whereas the other three GNNs perform on\naverage worse than the baseline. The results show that GNN based approaches\nhold promise in the analysis of temporal social networks through mobile\nconnectivity data. However, due to the relatively small performance margin\nbetween ROLAND and the baseline model, further research is required on\nspecialized GNN architectures for temporal social network analysis.\n", "link": "http://arxiv.org/abs/2509.03319v1", "date": "2025-09-03", "relevancy": 2.3454, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4787}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4651}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20social%20network%20modeling%20of%20mobile%20connectivity%20data%20with%20graph%0A%20%20neural%20networks&body=Title%3A%20Temporal%20social%20network%20modeling%20of%20mobile%20connectivity%20data%20with%20graph%0A%20%20neural%20networks%0AAuthor%3A%20Joel%20Jaskari%20and%20Chandreyee%20Roy%20and%20Fumiko%20Ogushi%20and%20Mikko%20Saukkoriipi%20and%20Jaakko%20Sahlsten%20and%20Kimmo%20Kaski%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20a%20state-of-the-art%20data-driven%0Atool%20for%20modeling%20connectivity%20data%20of%20graph-structured%20complex%20networks%20and%0Aintegrating%20information%20of%20their%20nodes%20and%20edges%20in%20space%20and%20time.%20However%2C%20as%0Aof%20yet%2C%20the%20analysis%20of%20social%20networks%20using%20the%20time%20series%20of%20people%27s%0Amobile%20connectivity%20data%20has%20not%20been%20extensively%20investigated.%20In%20the%20present%0Astudy%2C%20we%20investigate%20four%20snapshot%20-%20based%20temporal%20GNNs%20in%20predicting%20the%0Aphone%20call%20and%20SMS%20activity%20between%20users%20of%20a%20mobile%20communication%20network.%20In%0Aaddition%2C%20we%20develop%20a%20simple%20non%20-%20GNN%20baseline%20model%20using%20recently%20proposed%0AEdgeBank%20method.%20Our%20analysis%20shows%20that%20the%20ROLAND%20temporal%20GNN%20outperforms%0Athe%20baseline%20model%20in%20most%20cases%2C%20whereas%20the%20other%20three%20GNNs%20perform%20on%0Aaverage%20worse%20than%20the%20baseline.%20The%20results%20show%20that%20GNN%20based%20approaches%0Ahold%20promise%20in%20the%20analysis%20of%20temporal%20social%20networks%20through%20mobile%0Aconnectivity%20data.%20However%2C%20due%20to%20the%20relatively%20small%20performance%20margin%0Abetween%20ROLAND%20and%20the%20baseline%20model%2C%20further%20research%20is%20required%20on%0Aspecialized%20GNN%20architectures%20for%20temporal%20social%20network%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520social%2520network%2520modeling%2520of%2520mobile%2520connectivity%2520data%2520with%2520graph%250A%2520%2520neural%2520networks%26entry.906535625%3DJoel%2520Jaskari%2520and%2520Chandreyee%2520Roy%2520and%2520Fumiko%2520Ogushi%2520and%2520Mikko%2520Saukkoriipi%2520and%2520Jaakko%2520Sahlsten%2520and%2520Kimmo%2520Kaski%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520a%2520state-of-the-art%2520data-driven%250Atool%2520for%2520modeling%2520connectivity%2520data%2520of%2520graph-structured%2520complex%2520networks%2520and%250Aintegrating%2520information%2520of%2520their%2520nodes%2520and%2520edges%2520in%2520space%2520and%2520time.%2520However%252C%2520as%250Aof%2520yet%252C%2520the%2520analysis%2520of%2520social%2520networks%2520using%2520the%2520time%2520series%2520of%2520people%2527s%250Amobile%2520connectivity%2520data%2520has%2520not%2520been%2520extensively%2520investigated.%2520In%2520the%2520present%250Astudy%252C%2520we%2520investigate%2520four%2520snapshot%2520-%2520based%2520temporal%2520GNNs%2520in%2520predicting%2520the%250Aphone%2520call%2520and%2520SMS%2520activity%2520between%2520users%2520of%2520a%2520mobile%2520communication%2520network.%2520In%250Aaddition%252C%2520we%2520develop%2520a%2520simple%2520non%2520-%2520GNN%2520baseline%2520model%2520using%2520recently%2520proposed%250AEdgeBank%2520method.%2520Our%2520analysis%2520shows%2520that%2520the%2520ROLAND%2520temporal%2520GNN%2520outperforms%250Athe%2520baseline%2520model%2520in%2520most%2520cases%252C%2520whereas%2520the%2520other%2520three%2520GNNs%2520perform%2520on%250Aaverage%2520worse%2520than%2520the%2520baseline.%2520The%2520results%2520show%2520that%2520GNN%2520based%2520approaches%250Ahold%2520promise%2520in%2520the%2520analysis%2520of%2520temporal%2520social%2520networks%2520through%2520mobile%250Aconnectivity%2520data.%2520However%252C%2520due%2520to%2520the%2520relatively%2520small%2520performance%2520margin%250Abetween%2520ROLAND%2520and%2520the%2520baseline%2520model%252C%2520further%2520research%2520is%2520required%2520on%250Aspecialized%2520GNN%2520architectures%2520for%2520temporal%2520social%2520network%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20social%20network%20modeling%20of%20mobile%20connectivity%20data%20with%20graph%0A%20%20neural%20networks&entry.906535625=Joel%20Jaskari%20and%20Chandreyee%20Roy%20and%20Fumiko%20Ogushi%20and%20Mikko%20Saukkoriipi%20and%20Jaakko%20Sahlsten%20and%20Kimmo%20Kaski&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20emerged%20as%20a%20state-of-the-art%20data-driven%0Atool%20for%20modeling%20connectivity%20data%20of%20graph-structured%20complex%20networks%20and%0Aintegrating%20information%20of%20their%20nodes%20and%20edges%20in%20space%20and%20time.%20However%2C%20as%0Aof%20yet%2C%20the%20analysis%20of%20social%20networks%20using%20the%20time%20series%20of%20people%27s%0Amobile%20connectivity%20data%20has%20not%20been%20extensively%20investigated.%20In%20the%20present%0Astudy%2C%20we%20investigate%20four%20snapshot%20-%20based%20temporal%20GNNs%20in%20predicting%20the%0Aphone%20call%20and%20SMS%20activity%20between%20users%20of%20a%20mobile%20communication%20network.%20In%0Aaddition%2C%20we%20develop%20a%20simple%20non%20-%20GNN%20baseline%20model%20using%20recently%20proposed%0AEdgeBank%20method.%20Our%20analysis%20shows%20that%20the%20ROLAND%20temporal%20GNN%20outperforms%0Athe%20baseline%20model%20in%20most%20cases%2C%20whereas%20the%20other%20three%20GNNs%20perform%20on%0Aaverage%20worse%20than%20the%20baseline.%20The%20results%20show%20that%20GNN%20based%20approaches%0Ahold%20promise%20in%20the%20analysis%20of%20temporal%20social%20networks%20through%20mobile%0Aconnectivity%20data.%20However%2C%20due%20to%20the%20relatively%20small%20performance%20margin%0Abetween%20ROLAND%20and%20the%20baseline%20model%2C%20further%20research%20is%20required%20on%0Aspecialized%20GNN%20architectures%20for%20temporal%20social%20network%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03319v1&entry.124074799=Read"},
{"title": "Dynamical stability for dense patterns in discrete attractor neural\n  networks", "author": "Uri Cohen and M\u00e1t\u00e9 Lengyel", "abstract": "  Neural networks storing multiple discrete attractors are canonical models of\nbiological memory. Previously, the dynamical stability of such networks could\nonly be guaranteed under highly restrictive conditions. Here, we derive a\ntheory of the local stability of discrete fixed points in a broad class of\nnetworks with graded neural activities and in the presence of noise. By\ndirectly analyzing the bulk and outliers of the Jacobian spectrum, we show that\nall fixed points are stable below a critical load that is distinct from the\nclassical \\textit{critical capacity} and depends on the statistics of neural\nactivities in the fixed points as well as the single-neuron activation\nfunction. Our analysis highlights the computational benefits of\nthreshold-linear activation and sparse-like patterns.\n", "link": "http://arxiv.org/abs/2507.10383v2", "date": "2025-09-03", "relevancy": 2.338, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4844}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4816}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%0A%20%20networks&body=Title%3A%20Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%0A%20%20networks%0AAuthor%3A%20Uri%20Cohen%20and%20M%C3%A1t%C3%A9%20Lengyel%0AAbstract%3A%20%20%20Neural%20networks%20storing%20multiple%20discrete%20attractors%20are%20canonical%20models%20of%0Abiological%20memory.%20Previously%2C%20the%20dynamical%20stability%20of%20such%20networks%20could%0Aonly%20be%20guaranteed%20under%20highly%20restrictive%20conditions.%20Here%2C%20we%20derive%20a%0Atheory%20of%20the%20local%20stability%20of%20discrete%20fixed%20points%20in%20a%20broad%20class%20of%0Anetworks%20with%20graded%20neural%20activities%20and%20in%20the%20presence%20of%20noise.%20By%0Adirectly%20analyzing%20the%20bulk%20and%20outliers%20of%20the%20Jacobian%20spectrum%2C%20we%20show%20that%0Aall%20fixed%20points%20are%20stable%20below%20a%20critical%20load%20that%20is%20distinct%20from%20the%0Aclassical%20%5Ctextit%7Bcritical%20capacity%7D%20and%20depends%20on%20the%20statistics%20of%20neural%0Aactivities%20in%20the%20fixed%20points%20as%20well%20as%20the%20single-neuron%20activation%0Afunction.%20Our%20analysis%20highlights%20the%20computational%20benefits%20of%0Athreshold-linear%20activation%20and%20sparse-like%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamical%2520stability%2520for%2520dense%2520patterns%2520in%2520discrete%2520attractor%2520neural%250A%2520%2520networks%26entry.906535625%3DUri%2520Cohen%2520and%2520M%25C3%25A1t%25C3%25A9%2520Lengyel%26entry.1292438233%3D%2520%2520Neural%2520networks%2520storing%2520multiple%2520discrete%2520attractors%2520are%2520canonical%2520models%2520of%250Abiological%2520memory.%2520Previously%252C%2520the%2520dynamical%2520stability%2520of%2520such%2520networks%2520could%250Aonly%2520be%2520guaranteed%2520under%2520highly%2520restrictive%2520conditions.%2520Here%252C%2520we%2520derive%2520a%250Atheory%2520of%2520the%2520local%2520stability%2520of%2520discrete%2520fixed%2520points%2520in%2520a%2520broad%2520class%2520of%250Anetworks%2520with%2520graded%2520neural%2520activities%2520and%2520in%2520the%2520presence%2520of%2520noise.%2520By%250Adirectly%2520analyzing%2520the%2520bulk%2520and%2520outliers%2520of%2520the%2520Jacobian%2520spectrum%252C%2520we%2520show%2520that%250Aall%2520fixed%2520points%2520are%2520stable%2520below%2520a%2520critical%2520load%2520that%2520is%2520distinct%2520from%2520the%250Aclassical%2520%255Ctextit%257Bcritical%2520capacity%257D%2520and%2520depends%2520on%2520the%2520statistics%2520of%2520neural%250Aactivities%2520in%2520the%2520fixed%2520points%2520as%2520well%2520as%2520the%2520single-neuron%2520activation%250Afunction.%2520Our%2520analysis%2520highlights%2520the%2520computational%2520benefits%2520of%250Athreshold-linear%2520activation%2520and%2520sparse-like%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%0A%20%20networks&entry.906535625=Uri%20Cohen%20and%20M%C3%A1t%C3%A9%20Lengyel&entry.1292438233=%20%20Neural%20networks%20storing%20multiple%20discrete%20attractors%20are%20canonical%20models%20of%0Abiological%20memory.%20Previously%2C%20the%20dynamical%20stability%20of%20such%20networks%20could%0Aonly%20be%20guaranteed%20under%20highly%20restrictive%20conditions.%20Here%2C%20we%20derive%20a%0Atheory%20of%20the%20local%20stability%20of%20discrete%20fixed%20points%20in%20a%20broad%20class%20of%0Anetworks%20with%20graded%20neural%20activities%20and%20in%20the%20presence%20of%20noise.%20By%0Adirectly%20analyzing%20the%20bulk%20and%20outliers%20of%20the%20Jacobian%20spectrum%2C%20we%20show%20that%0Aall%20fixed%20points%20are%20stable%20below%20a%20critical%20load%20that%20is%20distinct%20from%20the%0Aclassical%20%5Ctextit%7Bcritical%20capacity%7D%20and%20depends%20on%20the%20statistics%20of%20neural%0Aactivities%20in%20the%20fixed%20points%20as%20well%20as%20the%20single-neuron%20activation%0Afunction.%20Our%20analysis%20highlights%20the%20computational%20benefits%20of%0Athreshold-linear%20activation%20and%20sparse-like%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10383v2&entry.124074799=Read"},
{"title": "Invariant Features for Global Crop Type Classification", "author": "Xin-Yi Tong and Sherrie Wang", "abstract": "  Accurately obtaining crop type and its spatial distribution at a global scale\nis critical for food security, agricultural policy-making, and sustainable\ndevelopment. Remote sensing offers an efficient solution for large-scale crop\nclassification, but the limited availability of reliable ground samples in many\nregions constrains applicability across geographic areas. To address\nperformance declines under geospatial shifts, this study identifies remote\nsensing features that are invariant to geographic variation and proposes\nstrategies to enhance cross-regional generalization. We construct CropGlobe, a\nglobal crop type dataset with 300,000 pixel-level samples from eight countries\nacross five continents, covering six major food and industrial crops (corn,\nsoybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,\nCropGlobe enables a systematic evaluation under cross-country, cross-continent,\nand cross-hemisphere transfer. We compare the transferability of temporal\nmulti-spectral features (Sentinel-2-based 1D/2D median features and harmonic\ncoefficients) and hyperspectral features (from EMIT). To improve generalization\nunder spectral and phenological shifts, we design CropNet, a lightweight and\nrobust CNN tailored for pixel-level crop classification, coupled with temporal\ndata augmentation (time shift, time scale, and magnitude warping) that\nsimulates realistic cross-regional phenology. Experiments show that 2D median\ntemporal features from Sentinel-2 consistently exhibit the strongest invariance\nacross all transfer scenarios, and augmentation further improves robustness,\nparticularly when training data diversity is limited. Overall, the work\nidentifies more invariant feature representations that enhance geographic\ntransferability and suggests a promising path toward scalable, low-cost crop\ntype applications across globally diverse regions.\n", "link": "http://arxiv.org/abs/2509.03497v1", "date": "2025-09-03", "relevancy": 2.3338, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.467}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Features%20for%20Global%20Crop%20Type%20Classification&body=Title%3A%20Invariant%20Features%20for%20Global%20Crop%20Type%20Classification%0AAuthor%3A%20Xin-Yi%20Tong%20and%20Sherrie%20Wang%0AAbstract%3A%20%20%20Accurately%20obtaining%20crop%20type%20and%20its%20spatial%20distribution%20at%20a%20global%20scale%0Ais%20critical%20for%20food%20security%2C%20agricultural%20policy-making%2C%20and%20sustainable%0Adevelopment.%20Remote%20sensing%20offers%20an%20efficient%20solution%20for%20large-scale%20crop%0Aclassification%2C%20but%20the%20limited%20availability%20of%20reliable%20ground%20samples%20in%20many%0Aregions%20constrains%20applicability%20across%20geographic%20areas.%20To%20address%0Aperformance%20declines%20under%20geospatial%20shifts%2C%20this%20study%20identifies%20remote%0Asensing%20features%20that%20are%20invariant%20to%20geographic%20variation%20and%20proposes%0Astrategies%20to%20enhance%20cross-regional%20generalization.%20We%20construct%20CropGlobe%2C%20a%0Aglobal%20crop%20type%20dataset%20with%20300%2C000%20pixel-level%20samples%20from%20eight%20countries%0Aacross%20five%20continents%2C%20covering%20six%20major%20food%20and%20industrial%20crops%20%28corn%2C%0Asoybeans%2C%20rice%2C%20wheat%2C%20sugarcane%2C%20cotton%29.%20With%20broad%20geographic%20coverage%2C%0ACropGlobe%20enables%20a%20systematic%20evaluation%20under%20cross-country%2C%20cross-continent%2C%0Aand%20cross-hemisphere%20transfer.%20We%20compare%20the%20transferability%20of%20temporal%0Amulti-spectral%20features%20%28Sentinel-2-based%201D/2D%20median%20features%20and%20harmonic%0Acoefficients%29%20and%20hyperspectral%20features%20%28from%20EMIT%29.%20To%20improve%20generalization%0Aunder%20spectral%20and%20phenological%20shifts%2C%20we%20design%20CropNet%2C%20a%20lightweight%20and%0Arobust%20CNN%20tailored%20for%20pixel-level%20crop%20classification%2C%20coupled%20with%20temporal%0Adata%20augmentation%20%28time%20shift%2C%20time%20scale%2C%20and%20magnitude%20warping%29%20that%0Asimulates%20realistic%20cross-regional%20phenology.%20Experiments%20show%20that%202D%20median%0Atemporal%20features%20from%20Sentinel-2%20consistently%20exhibit%20the%20strongest%20invariance%0Aacross%20all%20transfer%20scenarios%2C%20and%20augmentation%20further%20improves%20robustness%2C%0Aparticularly%20when%20training%20data%20diversity%20is%20limited.%20Overall%2C%20the%20work%0Aidentifies%20more%20invariant%20feature%20representations%20that%20enhance%20geographic%0Atransferability%20and%20suggests%20a%20promising%20path%20toward%20scalable%2C%20low-cost%20crop%0Atype%20applications%20across%20globally%20diverse%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Features%2520for%2520Global%2520Crop%2520Type%2520Classification%26entry.906535625%3DXin-Yi%2520Tong%2520and%2520Sherrie%2520Wang%26entry.1292438233%3D%2520%2520Accurately%2520obtaining%2520crop%2520type%2520and%2520its%2520spatial%2520distribution%2520at%2520a%2520global%2520scale%250Ais%2520critical%2520for%2520food%2520security%252C%2520agricultural%2520policy-making%252C%2520and%2520sustainable%250Adevelopment.%2520Remote%2520sensing%2520offers%2520an%2520efficient%2520solution%2520for%2520large-scale%2520crop%250Aclassification%252C%2520but%2520the%2520limited%2520availability%2520of%2520reliable%2520ground%2520samples%2520in%2520many%250Aregions%2520constrains%2520applicability%2520across%2520geographic%2520areas.%2520To%2520address%250Aperformance%2520declines%2520under%2520geospatial%2520shifts%252C%2520this%2520study%2520identifies%2520remote%250Asensing%2520features%2520that%2520are%2520invariant%2520to%2520geographic%2520variation%2520and%2520proposes%250Astrategies%2520to%2520enhance%2520cross-regional%2520generalization.%2520We%2520construct%2520CropGlobe%252C%2520a%250Aglobal%2520crop%2520type%2520dataset%2520with%2520300%252C000%2520pixel-level%2520samples%2520from%2520eight%2520countries%250Aacross%2520five%2520continents%252C%2520covering%2520six%2520major%2520food%2520and%2520industrial%2520crops%2520%2528corn%252C%250Asoybeans%252C%2520rice%252C%2520wheat%252C%2520sugarcane%252C%2520cotton%2529.%2520With%2520broad%2520geographic%2520coverage%252C%250ACropGlobe%2520enables%2520a%2520systematic%2520evaluation%2520under%2520cross-country%252C%2520cross-continent%252C%250Aand%2520cross-hemisphere%2520transfer.%2520We%2520compare%2520the%2520transferability%2520of%2520temporal%250Amulti-spectral%2520features%2520%2528Sentinel-2-based%25201D/2D%2520median%2520features%2520and%2520harmonic%250Acoefficients%2529%2520and%2520hyperspectral%2520features%2520%2528from%2520EMIT%2529.%2520To%2520improve%2520generalization%250Aunder%2520spectral%2520and%2520phenological%2520shifts%252C%2520we%2520design%2520CropNet%252C%2520a%2520lightweight%2520and%250Arobust%2520CNN%2520tailored%2520for%2520pixel-level%2520crop%2520classification%252C%2520coupled%2520with%2520temporal%250Adata%2520augmentation%2520%2528time%2520shift%252C%2520time%2520scale%252C%2520and%2520magnitude%2520warping%2529%2520that%250Asimulates%2520realistic%2520cross-regional%2520phenology.%2520Experiments%2520show%2520that%25202D%2520median%250Atemporal%2520features%2520from%2520Sentinel-2%2520consistently%2520exhibit%2520the%2520strongest%2520invariance%250Aacross%2520all%2520transfer%2520scenarios%252C%2520and%2520augmentation%2520further%2520improves%2520robustness%252C%250Aparticularly%2520when%2520training%2520data%2520diversity%2520is%2520limited.%2520Overall%252C%2520the%2520work%250Aidentifies%2520more%2520invariant%2520feature%2520representations%2520that%2520enhance%2520geographic%250Atransferability%2520and%2520suggests%2520a%2520promising%2520path%2520toward%2520scalable%252C%2520low-cost%2520crop%250Atype%2520applications%2520across%2520globally%2520diverse%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Features%20for%20Global%20Crop%20Type%20Classification&entry.906535625=Xin-Yi%20Tong%20and%20Sherrie%20Wang&entry.1292438233=%20%20Accurately%20obtaining%20crop%20type%20and%20its%20spatial%20distribution%20at%20a%20global%20scale%0Ais%20critical%20for%20food%20security%2C%20agricultural%20policy-making%2C%20and%20sustainable%0Adevelopment.%20Remote%20sensing%20offers%20an%20efficient%20solution%20for%20large-scale%20crop%0Aclassification%2C%20but%20the%20limited%20availability%20of%20reliable%20ground%20samples%20in%20many%0Aregions%20constrains%20applicability%20across%20geographic%20areas.%20To%20address%0Aperformance%20declines%20under%20geospatial%20shifts%2C%20this%20study%20identifies%20remote%0Asensing%20features%20that%20are%20invariant%20to%20geographic%20variation%20and%20proposes%0Astrategies%20to%20enhance%20cross-regional%20generalization.%20We%20construct%20CropGlobe%2C%20a%0Aglobal%20crop%20type%20dataset%20with%20300%2C000%20pixel-level%20samples%20from%20eight%20countries%0Aacross%20five%20continents%2C%20covering%20six%20major%20food%20and%20industrial%20crops%20%28corn%2C%0Asoybeans%2C%20rice%2C%20wheat%2C%20sugarcane%2C%20cotton%29.%20With%20broad%20geographic%20coverage%2C%0ACropGlobe%20enables%20a%20systematic%20evaluation%20under%20cross-country%2C%20cross-continent%2C%0Aand%20cross-hemisphere%20transfer.%20We%20compare%20the%20transferability%20of%20temporal%0Amulti-spectral%20features%20%28Sentinel-2-based%201D/2D%20median%20features%20and%20harmonic%0Acoefficients%29%20and%20hyperspectral%20features%20%28from%20EMIT%29.%20To%20improve%20generalization%0Aunder%20spectral%20and%20phenological%20shifts%2C%20we%20design%20CropNet%2C%20a%20lightweight%20and%0Arobust%20CNN%20tailored%20for%20pixel-level%20crop%20classification%2C%20coupled%20with%20temporal%0Adata%20augmentation%20%28time%20shift%2C%20time%20scale%2C%20and%20magnitude%20warping%29%20that%0Asimulates%20realistic%20cross-regional%20phenology.%20Experiments%20show%20that%202D%20median%0Atemporal%20features%20from%20Sentinel-2%20consistently%20exhibit%20the%20strongest%20invariance%0Aacross%20all%20transfer%20scenarios%2C%20and%20augmentation%20further%20improves%20robustness%2C%0Aparticularly%20when%20training%20data%20diversity%20is%20limited.%20Overall%2C%20the%20work%0Aidentifies%20more%20invariant%20feature%20representations%20that%20enhance%20geographic%0Atransferability%20and%20suggests%20a%20promising%20path%20toward%20scalable%2C%20low-cost%20crop%0Atype%20applications%20across%20globally%20diverse%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03497v1&entry.124074799=Read"},
{"title": "InfraDiffusion: zero-shot depth map restoration with diffusion models\n  and prompted segmentation from sparse infrastructure point clouds", "author": "Yixiong Jing and Cheng Zhang and Haibing Wu and Guangming Wang and Olaf Wysocki and Brian Sheil", "abstract": "  Point clouds are widely used for infrastructure monitoring by providing\ngeometric information, where segmentation is required for downstream tasks such\nas defect detection. Existing research has automated semantic segmentation of\nstructural components, while brick-level segmentation (identifying defects such\nas spalling and mortar loss) has been primarily conducted from RGB images.\nHowever, acquiring high-resolution images is impractical in low-light\nenvironments like masonry tunnels. Point clouds, though robust to dim lighting,\nare typically unstructured, sparse, and noisy, limiting fine-grained\nsegmentation. We present InfraDiffusion, a zero-shot framework that projects\nmasonry point clouds into depth maps using virtual cameras and restores them by\nadapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific\ntraining, InfraDiffusion enhances visual clarity and geometric consistency of\ndepth maps. Experiments on masonry bridge and tunnel point cloud datasets show\nsignificant improvements in brick-level segmentation using the Segment Anything\nModel (SAM), underscoring its potential for automated inspection of masonry\nassets. Our code and data is available at\nhttps://github.com/Jingyixiong/InfraDiffusion-official-implement.\n", "link": "http://arxiv.org/abs/2509.03324v1", "date": "2025-09-03", "relevancy": 2.329, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6002}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5787}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfraDiffusion%3A%20zero-shot%20depth%20map%20restoration%20with%20diffusion%20models%0A%20%20and%20prompted%20segmentation%20from%20sparse%20infrastructure%20point%20clouds&body=Title%3A%20InfraDiffusion%3A%20zero-shot%20depth%20map%20restoration%20with%20diffusion%20models%0A%20%20and%20prompted%20segmentation%20from%20sparse%20infrastructure%20point%20clouds%0AAuthor%3A%20Yixiong%20Jing%20and%20Cheng%20Zhang%20and%20Haibing%20Wu%20and%20Guangming%20Wang%20and%20Olaf%20Wysocki%20and%20Brian%20Sheil%0AAbstract%3A%20%20%20Point%20clouds%20are%20widely%20used%20for%20infrastructure%20monitoring%20by%20providing%0Ageometric%20information%2C%20where%20segmentation%20is%20required%20for%20downstream%20tasks%20such%0Aas%20defect%20detection.%20Existing%20research%20has%20automated%20semantic%20segmentation%20of%0Astructural%20components%2C%20while%20brick-level%20segmentation%20%28identifying%20defects%20such%0Aas%20spalling%20and%20mortar%20loss%29%20has%20been%20primarily%20conducted%20from%20RGB%20images.%0AHowever%2C%20acquiring%20high-resolution%20images%20is%20impractical%20in%20low-light%0Aenvironments%20like%20masonry%20tunnels.%20Point%20clouds%2C%20though%20robust%20to%20dim%20lighting%2C%0Aare%20typically%20unstructured%2C%20sparse%2C%20and%20noisy%2C%20limiting%20fine-grained%0Asegmentation.%20We%20present%20InfraDiffusion%2C%20a%20zero-shot%20framework%20that%20projects%0Amasonry%20point%20clouds%20into%20depth%20maps%20using%20virtual%20cameras%20and%20restores%20them%20by%0Aadapting%20the%20Denoising%20Diffusion%20Null-space%20Model%20%28DDNM%29.%20Without%20task-specific%0Atraining%2C%20InfraDiffusion%20enhances%20visual%20clarity%20and%20geometric%20consistency%20of%0Adepth%20maps.%20Experiments%20on%20masonry%20bridge%20and%20tunnel%20point%20cloud%20datasets%20show%0Asignificant%20improvements%20in%20brick-level%20segmentation%20using%20the%20Segment%20Anything%0AModel%20%28SAM%29%2C%20underscoring%20its%20potential%20for%20automated%20inspection%20of%20masonry%0Aassets.%20Our%20code%20and%20data%20is%20available%20at%0Ahttps%3A//github.com/Jingyixiong/InfraDiffusion-official-implement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfraDiffusion%253A%2520zero-shot%2520depth%2520map%2520restoration%2520with%2520diffusion%2520models%250A%2520%2520and%2520prompted%2520segmentation%2520from%2520sparse%2520infrastructure%2520point%2520clouds%26entry.906535625%3DYixiong%2520Jing%2520and%2520Cheng%2520Zhang%2520and%2520Haibing%2520Wu%2520and%2520Guangming%2520Wang%2520and%2520Olaf%2520Wysocki%2520and%2520Brian%2520Sheil%26entry.1292438233%3D%2520%2520Point%2520clouds%2520are%2520widely%2520used%2520for%2520infrastructure%2520monitoring%2520by%2520providing%250Ageometric%2520information%252C%2520where%2520segmentation%2520is%2520required%2520for%2520downstream%2520tasks%2520such%250Aas%2520defect%2520detection.%2520Existing%2520research%2520has%2520automated%2520semantic%2520segmentation%2520of%250Astructural%2520components%252C%2520while%2520brick-level%2520segmentation%2520%2528identifying%2520defects%2520such%250Aas%2520spalling%2520and%2520mortar%2520loss%2529%2520has%2520been%2520primarily%2520conducted%2520from%2520RGB%2520images.%250AHowever%252C%2520acquiring%2520high-resolution%2520images%2520is%2520impractical%2520in%2520low-light%250Aenvironments%2520like%2520masonry%2520tunnels.%2520Point%2520clouds%252C%2520though%2520robust%2520to%2520dim%2520lighting%252C%250Aare%2520typically%2520unstructured%252C%2520sparse%252C%2520and%2520noisy%252C%2520limiting%2520fine-grained%250Asegmentation.%2520We%2520present%2520InfraDiffusion%252C%2520a%2520zero-shot%2520framework%2520that%2520projects%250Amasonry%2520point%2520clouds%2520into%2520depth%2520maps%2520using%2520virtual%2520cameras%2520and%2520restores%2520them%2520by%250Aadapting%2520the%2520Denoising%2520Diffusion%2520Null-space%2520Model%2520%2528DDNM%2529.%2520Without%2520task-specific%250Atraining%252C%2520InfraDiffusion%2520enhances%2520visual%2520clarity%2520and%2520geometric%2520consistency%2520of%250Adepth%2520maps.%2520Experiments%2520on%2520masonry%2520bridge%2520and%2520tunnel%2520point%2520cloud%2520datasets%2520show%250Asignificant%2520improvements%2520in%2520brick-level%2520segmentation%2520using%2520the%2520Segment%2520Anything%250AModel%2520%2528SAM%2529%252C%2520underscoring%2520its%2520potential%2520for%2520automated%2520inspection%2520of%2520masonry%250Aassets.%2520Our%2520code%2520and%2520data%2520is%2520available%2520at%250Ahttps%253A//github.com/Jingyixiong/InfraDiffusion-official-implement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfraDiffusion%3A%20zero-shot%20depth%20map%20restoration%20with%20diffusion%20models%0A%20%20and%20prompted%20segmentation%20from%20sparse%20infrastructure%20point%20clouds&entry.906535625=Yixiong%20Jing%20and%20Cheng%20Zhang%20and%20Haibing%20Wu%20and%20Guangming%20Wang%20and%20Olaf%20Wysocki%20and%20Brian%20Sheil&entry.1292438233=%20%20Point%20clouds%20are%20widely%20used%20for%20infrastructure%20monitoring%20by%20providing%0Ageometric%20information%2C%20where%20segmentation%20is%20required%20for%20downstream%20tasks%20such%0Aas%20defect%20detection.%20Existing%20research%20has%20automated%20semantic%20segmentation%20of%0Astructural%20components%2C%20while%20brick-level%20segmentation%20%28identifying%20defects%20such%0Aas%20spalling%20and%20mortar%20loss%29%20has%20been%20primarily%20conducted%20from%20RGB%20images.%0AHowever%2C%20acquiring%20high-resolution%20images%20is%20impractical%20in%20low-light%0Aenvironments%20like%20masonry%20tunnels.%20Point%20clouds%2C%20though%20robust%20to%20dim%20lighting%2C%0Aare%20typically%20unstructured%2C%20sparse%2C%20and%20noisy%2C%20limiting%20fine-grained%0Asegmentation.%20We%20present%20InfraDiffusion%2C%20a%20zero-shot%20framework%20that%20projects%0Amasonry%20point%20clouds%20into%20depth%20maps%20using%20virtual%20cameras%20and%20restores%20them%20by%0Aadapting%20the%20Denoising%20Diffusion%20Null-space%20Model%20%28DDNM%29.%20Without%20task-specific%0Atraining%2C%20InfraDiffusion%20enhances%20visual%20clarity%20and%20geometric%20consistency%20of%0Adepth%20maps.%20Experiments%20on%20masonry%20bridge%20and%20tunnel%20point%20cloud%20datasets%20show%0Asignificant%20improvements%20in%20brick-level%20segmentation%20using%20the%20Segment%20Anything%0AModel%20%28SAM%29%2C%20underscoring%20its%20potential%20for%20automated%20inspection%20of%20masonry%0Aassets.%20Our%20code%20and%20data%20is%20available%20at%0Ahttps%3A//github.com/Jingyixiong/InfraDiffusion-official-implement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03324v1&entry.124074799=Read"},
{"title": "Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues", "author": "Mengze Hong and Wailing Ng and Chen Jason Zhang and Yuanfeng Song and Di Jiang", "abstract": "  Discovering customer intentions is crucial for automated service agents, yet\nexisting intent clustering methods often fall short due to their reliance on\nembedding distance metrics and neglect of underlying semantic structures. To\naddress these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent\nclustering framework, integrating the language understanding capabilities of\nLLMs into conventional clustering algorithms. Specifically, this paper (1)\nexamines the effectiveness of fine-tuned LLMs in semantic coherence evaluation\nand intent cluster naming, achieving over 95% accuracy aligned with human\njudgments; (2) designs an LLM-ITL framework that facilitates the iterative\ndiscovery of coherent intent clusters and the optimal number of clusters; and\n(3) introduces context-aware techniques tailored for customer service dialogue.\nSince existing English benchmarks lack sufficient semantic diversity and intent\ncoverage, we further present a comprehensive Chinese dialogue intent dataset\ncomprising over 100k real customer service calls with 1,507 human-annotated\nclusters. The proposed approaches significantly outperform LLM-guided\nbaselines, achieving notable improvements in clustering quality, cost\nefficiency, and downstream applications. Combined with several best practices,\nour findings highlight the prominence of LLM-in-the-loop techniques for\nscalable dialogue data mining.\n", "link": "http://arxiv.org/abs/2412.09049v4", "date": "2025-09-03", "relevancy": 2.3256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dial-In%20LLM%3A%20Human-Aligned%20LLM-in-the-loop%20Intent%20Clustering%20for%0A%20%20Customer%20Service%20Dialogues&body=Title%3A%20Dial-In%20LLM%3A%20Human-Aligned%20LLM-in-the-loop%20Intent%20Clustering%20for%0A%20%20Customer%20Service%20Dialogues%0AAuthor%3A%20Mengze%20Hong%20and%20Wailing%20Ng%20and%20Chen%20Jason%20Zhang%20and%20Yuanfeng%20Song%20and%20Di%20Jiang%0AAbstract%3A%20%20%20Discovering%20customer%20intentions%20is%20crucial%20for%20automated%20service%20agents%2C%20yet%0Aexisting%20intent%20clustering%20methods%20often%20fall%20short%20due%20to%20their%20reliance%20on%0Aembedding%20distance%20metrics%20and%20neglect%20of%20underlying%20semantic%20structures.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20an%20LLM-in-the-loop%20%28LLM-ITL%29%20intent%0Aclustering%20framework%2C%20integrating%20the%20language%20understanding%20capabilities%20of%0ALLMs%20into%20conventional%20clustering%20algorithms.%20Specifically%2C%20this%20paper%20%281%29%0Aexamines%20the%20effectiveness%20of%20fine-tuned%20LLMs%20in%20semantic%20coherence%20evaluation%0Aand%20intent%20cluster%20naming%2C%20achieving%20over%2095%25%20accuracy%20aligned%20with%20human%0Ajudgments%3B%20%282%29%20designs%20an%20LLM-ITL%20framework%20that%20facilitates%20the%20iterative%0Adiscovery%20of%20coherent%20intent%20clusters%20and%20the%20optimal%20number%20of%20clusters%3B%20and%0A%283%29%20introduces%20context-aware%20techniques%20tailored%20for%20customer%20service%20dialogue.%0ASince%20existing%20English%20benchmarks%20lack%20sufficient%20semantic%20diversity%20and%20intent%0Acoverage%2C%20we%20further%20present%20a%20comprehensive%20Chinese%20dialogue%20intent%20dataset%0Acomprising%20over%20100k%20real%20customer%20service%20calls%20with%201%2C507%20human-annotated%0Aclusters.%20The%20proposed%20approaches%20significantly%20outperform%20LLM-guided%0Abaselines%2C%20achieving%20notable%20improvements%20in%20clustering%20quality%2C%20cost%0Aefficiency%2C%20and%20downstream%20applications.%20Combined%20with%20several%20best%20practices%2C%0Aour%20findings%20highlight%20the%20prominence%20of%20LLM-in-the-loop%20techniques%20for%0Ascalable%20dialogue%20data%20mining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09049v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDial-In%2520LLM%253A%2520Human-Aligned%2520LLM-in-the-loop%2520Intent%2520Clustering%2520for%250A%2520%2520Customer%2520Service%2520Dialogues%26entry.906535625%3DMengze%2520Hong%2520and%2520Wailing%2520Ng%2520and%2520Chen%2520Jason%2520Zhang%2520and%2520Yuanfeng%2520Song%2520and%2520Di%2520Jiang%26entry.1292438233%3D%2520%2520Discovering%2520customer%2520intentions%2520is%2520crucial%2520for%2520automated%2520service%2520agents%252C%2520yet%250Aexisting%2520intent%2520clustering%2520methods%2520often%2520fall%2520short%2520due%2520to%2520their%2520reliance%2520on%250Aembedding%2520distance%2520metrics%2520and%2520neglect%2520of%2520underlying%2520semantic%2520structures.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520an%2520LLM-in-the-loop%2520%2528LLM-ITL%2529%2520intent%250Aclustering%2520framework%252C%2520integrating%2520the%2520language%2520understanding%2520capabilities%2520of%250ALLMs%2520into%2520conventional%2520clustering%2520algorithms.%2520Specifically%252C%2520this%2520paper%2520%25281%2529%250Aexamines%2520the%2520effectiveness%2520of%2520fine-tuned%2520LLMs%2520in%2520semantic%2520coherence%2520evaluation%250Aand%2520intent%2520cluster%2520naming%252C%2520achieving%2520over%252095%2525%2520accuracy%2520aligned%2520with%2520human%250Ajudgments%253B%2520%25282%2529%2520designs%2520an%2520LLM-ITL%2520framework%2520that%2520facilitates%2520the%2520iterative%250Adiscovery%2520of%2520coherent%2520intent%2520clusters%2520and%2520the%2520optimal%2520number%2520of%2520clusters%253B%2520and%250A%25283%2529%2520introduces%2520context-aware%2520techniques%2520tailored%2520for%2520customer%2520service%2520dialogue.%250ASince%2520existing%2520English%2520benchmarks%2520lack%2520sufficient%2520semantic%2520diversity%2520and%2520intent%250Acoverage%252C%2520we%2520further%2520present%2520a%2520comprehensive%2520Chinese%2520dialogue%2520intent%2520dataset%250Acomprising%2520over%2520100k%2520real%2520customer%2520service%2520calls%2520with%25201%252C507%2520human-annotated%250Aclusters.%2520The%2520proposed%2520approaches%2520significantly%2520outperform%2520LLM-guided%250Abaselines%252C%2520achieving%2520notable%2520improvements%2520in%2520clustering%2520quality%252C%2520cost%250Aefficiency%252C%2520and%2520downstream%2520applications.%2520Combined%2520with%2520several%2520best%2520practices%252C%250Aour%2520findings%2520highlight%2520the%2520prominence%2520of%2520LLM-in-the-loop%2520techniques%2520for%250Ascalable%2520dialogue%2520data%2520mining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09049v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dial-In%20LLM%3A%20Human-Aligned%20LLM-in-the-loop%20Intent%20Clustering%20for%0A%20%20Customer%20Service%20Dialogues&entry.906535625=Mengze%20Hong%20and%20Wailing%20Ng%20and%20Chen%20Jason%20Zhang%20and%20Yuanfeng%20Song%20and%20Di%20Jiang&entry.1292438233=%20%20Discovering%20customer%20intentions%20is%20crucial%20for%20automated%20service%20agents%2C%20yet%0Aexisting%20intent%20clustering%20methods%20often%20fall%20short%20due%20to%20their%20reliance%20on%0Aembedding%20distance%20metrics%20and%20neglect%20of%20underlying%20semantic%20structures.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20an%20LLM-in-the-loop%20%28LLM-ITL%29%20intent%0Aclustering%20framework%2C%20integrating%20the%20language%20understanding%20capabilities%20of%0ALLMs%20into%20conventional%20clustering%20algorithms.%20Specifically%2C%20this%20paper%20%281%29%0Aexamines%20the%20effectiveness%20of%20fine-tuned%20LLMs%20in%20semantic%20coherence%20evaluation%0Aand%20intent%20cluster%20naming%2C%20achieving%20over%2095%25%20accuracy%20aligned%20with%20human%0Ajudgments%3B%20%282%29%20designs%20an%20LLM-ITL%20framework%20that%20facilitates%20the%20iterative%0Adiscovery%20of%20coherent%20intent%20clusters%20and%20the%20optimal%20number%20of%20clusters%3B%20and%0A%283%29%20introduces%20context-aware%20techniques%20tailored%20for%20customer%20service%20dialogue.%0ASince%20existing%20English%20benchmarks%20lack%20sufficient%20semantic%20diversity%20and%20intent%0Acoverage%2C%20we%20further%20present%20a%20comprehensive%20Chinese%20dialogue%20intent%20dataset%0Acomprising%20over%20100k%20real%20customer%20service%20calls%20with%201%2C507%20human-annotated%0Aclusters.%20The%20proposed%20approaches%20significantly%20outperform%20LLM-guided%0Abaselines%2C%20achieving%20notable%20improvements%20in%20clustering%20quality%2C%20cost%0Aefficiency%2C%20and%20downstream%20applications.%20Combined%20with%20several%20best%20practices%2C%0Aour%20findings%20highlight%20the%20prominence%20of%20LLM-in-the-loop%20techniques%20for%0Ascalable%20dialogue%20data%20mining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09049v4&entry.124074799=Read"},
{"title": "epiGPTope: A machine learning-based epitope generator and classifier", "author": "Natalia Flechas Manrique and Alberto Mart\u00ednez and Elena L\u00f3pez-Mart\u00ednez and Luc Andrea and Rom\u00e1n Orus and Aitor Manteca and Aitziber L. Cortajarena and Lloren\u00e7 Espinosa-Portal\u00e9s", "abstract": "  Epitopes are short antigenic peptide sequences which are recognized by\nantibodies or immune cell receptors. These are central to the development of\nimmunotherapies, vaccines, and diagnostics. However, the rational design of\nsynthetic epitope libraries is challenging due to the large combinatorial\nsequence space, $20^n$ combinations for linear epitopes of n amino acids,\nmaking screening and testing unfeasible, even with high throughput experimental\ntechniques. In this study, we present a large language model, epiGPTope,\npre-trained on protein data and specifically fine-tuned on linear epitopes,\nwhich for the first time can directly generate novel epitope-like sequences,\nwhich are found to possess statistical properties analogous to the ones of\nknown epitopes. This generative approach can be used to prepare libraries of\nepitope candidate sequences. We further train statistical classifiers to\npredict whether an epitope sequence is of bacterial or viral origin, thus\nnarrowing the candidate library and increasing the likelihood of identifying\nspecific epitopes. We propose that such combination of generative and\npredictive models can be of assistance in epitope discovery. The approach uses\nonly primary amino acid sequences of linear epitopes, bypassing the need for a\ngeometric framework or hand-crafted features of the sequences. By developing a\nmethod to create biologically feasible sequences, we anticipate faster and more\ncost-effective generation and screening of synthetic epitopes, with relevant\napplications in the development of new biotechnologies.\n", "link": "http://arxiv.org/abs/2509.03351v1", "date": "2025-09-03", "relevancy": 2.3077, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4821}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4675}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20epiGPTope%3A%20A%20machine%20learning-based%20epitope%20generator%20and%20classifier&body=Title%3A%20epiGPTope%3A%20A%20machine%20learning-based%20epitope%20generator%20and%20classifier%0AAuthor%3A%20Natalia%20Flechas%20Manrique%20and%20Alberto%20Mart%C3%ADnez%20and%20Elena%20L%C3%B3pez-Mart%C3%ADnez%20and%20Luc%20Andrea%20and%20Rom%C3%A1n%20Orus%20and%20Aitor%20Manteca%20and%20Aitziber%20L.%20Cortajarena%20and%20Lloren%C3%A7%20Espinosa-Portal%C3%A9s%0AAbstract%3A%20%20%20Epitopes%20are%20short%20antigenic%20peptide%20sequences%20which%20are%20recognized%20by%0Aantibodies%20or%20immune%20cell%20receptors.%20These%20are%20central%20to%20the%20development%20of%0Aimmunotherapies%2C%20vaccines%2C%20and%20diagnostics.%20However%2C%20the%20rational%20design%20of%0Asynthetic%20epitope%20libraries%20is%20challenging%20due%20to%20the%20large%20combinatorial%0Asequence%20space%2C%20%2420%5En%24%20combinations%20for%20linear%20epitopes%20of%20n%20amino%20acids%2C%0Amaking%20screening%20and%20testing%20unfeasible%2C%20even%20with%20high%20throughput%20experimental%0Atechniques.%20In%20this%20study%2C%20we%20present%20a%20large%20language%20model%2C%20epiGPTope%2C%0Apre-trained%20on%20protein%20data%20and%20specifically%20fine-tuned%20on%20linear%20epitopes%2C%0Awhich%20for%20the%20first%20time%20can%20directly%20generate%20novel%20epitope-like%20sequences%2C%0Awhich%20are%20found%20to%20possess%20statistical%20properties%20analogous%20to%20the%20ones%20of%0Aknown%20epitopes.%20This%20generative%20approach%20can%20be%20used%20to%20prepare%20libraries%20of%0Aepitope%20candidate%20sequences.%20We%20further%20train%20statistical%20classifiers%20to%0Apredict%20whether%20an%20epitope%20sequence%20is%20of%20bacterial%20or%20viral%20origin%2C%20thus%0Anarrowing%20the%20candidate%20library%20and%20increasing%20the%20likelihood%20of%20identifying%0Aspecific%20epitopes.%20We%20propose%20that%20such%20combination%20of%20generative%20and%0Apredictive%20models%20can%20be%20of%20assistance%20in%20epitope%20discovery.%20The%20approach%20uses%0Aonly%20primary%20amino%20acid%20sequences%20of%20linear%20epitopes%2C%20bypassing%20the%20need%20for%20a%0Ageometric%20framework%20or%20hand-crafted%20features%20of%20the%20sequences.%20By%20developing%20a%0Amethod%20to%20create%20biologically%20feasible%20sequences%2C%20we%20anticipate%20faster%20and%20more%0Acost-effective%20generation%20and%20screening%20of%20synthetic%20epitopes%2C%20with%20relevant%0Aapplications%20in%20the%20development%20of%20new%20biotechnologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DepiGPTope%253A%2520A%2520machine%2520learning-based%2520epitope%2520generator%2520and%2520classifier%26entry.906535625%3DNatalia%2520Flechas%2520Manrique%2520and%2520Alberto%2520Mart%25C3%25ADnez%2520and%2520Elena%2520L%25C3%25B3pez-Mart%25C3%25ADnez%2520and%2520Luc%2520Andrea%2520and%2520Rom%25C3%25A1n%2520Orus%2520and%2520Aitor%2520Manteca%2520and%2520Aitziber%2520L.%2520Cortajarena%2520and%2520Lloren%25C3%25A7%2520Espinosa-Portal%25C3%25A9s%26entry.1292438233%3D%2520%2520Epitopes%2520are%2520short%2520antigenic%2520peptide%2520sequences%2520which%2520are%2520recognized%2520by%250Aantibodies%2520or%2520immune%2520cell%2520receptors.%2520These%2520are%2520central%2520to%2520the%2520development%2520of%250Aimmunotherapies%252C%2520vaccines%252C%2520and%2520diagnostics.%2520However%252C%2520the%2520rational%2520design%2520of%250Asynthetic%2520epitope%2520libraries%2520is%2520challenging%2520due%2520to%2520the%2520large%2520combinatorial%250Asequence%2520space%252C%2520%252420%255En%2524%2520combinations%2520for%2520linear%2520epitopes%2520of%2520n%2520amino%2520acids%252C%250Amaking%2520screening%2520and%2520testing%2520unfeasible%252C%2520even%2520with%2520high%2520throughput%2520experimental%250Atechniques.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520large%2520language%2520model%252C%2520epiGPTope%252C%250Apre-trained%2520on%2520protein%2520data%2520and%2520specifically%2520fine-tuned%2520on%2520linear%2520epitopes%252C%250Awhich%2520for%2520the%2520first%2520time%2520can%2520directly%2520generate%2520novel%2520epitope-like%2520sequences%252C%250Awhich%2520are%2520found%2520to%2520possess%2520statistical%2520properties%2520analogous%2520to%2520the%2520ones%2520of%250Aknown%2520epitopes.%2520This%2520generative%2520approach%2520can%2520be%2520used%2520to%2520prepare%2520libraries%2520of%250Aepitope%2520candidate%2520sequences.%2520We%2520further%2520train%2520statistical%2520classifiers%2520to%250Apredict%2520whether%2520an%2520epitope%2520sequence%2520is%2520of%2520bacterial%2520or%2520viral%2520origin%252C%2520thus%250Anarrowing%2520the%2520candidate%2520library%2520and%2520increasing%2520the%2520likelihood%2520of%2520identifying%250Aspecific%2520epitopes.%2520We%2520propose%2520that%2520such%2520combination%2520of%2520generative%2520and%250Apredictive%2520models%2520can%2520be%2520of%2520assistance%2520in%2520epitope%2520discovery.%2520The%2520approach%2520uses%250Aonly%2520primary%2520amino%2520acid%2520sequences%2520of%2520linear%2520epitopes%252C%2520bypassing%2520the%2520need%2520for%2520a%250Ageometric%2520framework%2520or%2520hand-crafted%2520features%2520of%2520the%2520sequences.%2520By%2520developing%2520a%250Amethod%2520to%2520create%2520biologically%2520feasible%2520sequences%252C%2520we%2520anticipate%2520faster%2520and%2520more%250Acost-effective%2520generation%2520and%2520screening%2520of%2520synthetic%2520epitopes%252C%2520with%2520relevant%250Aapplications%2520in%2520the%2520development%2520of%2520new%2520biotechnologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=epiGPTope%3A%20A%20machine%20learning-based%20epitope%20generator%20and%20classifier&entry.906535625=Natalia%20Flechas%20Manrique%20and%20Alberto%20Mart%C3%ADnez%20and%20Elena%20L%C3%B3pez-Mart%C3%ADnez%20and%20Luc%20Andrea%20and%20Rom%C3%A1n%20Orus%20and%20Aitor%20Manteca%20and%20Aitziber%20L.%20Cortajarena%20and%20Lloren%C3%A7%20Espinosa-Portal%C3%A9s&entry.1292438233=%20%20Epitopes%20are%20short%20antigenic%20peptide%20sequences%20which%20are%20recognized%20by%0Aantibodies%20or%20immune%20cell%20receptors.%20These%20are%20central%20to%20the%20development%20of%0Aimmunotherapies%2C%20vaccines%2C%20and%20diagnostics.%20However%2C%20the%20rational%20design%20of%0Asynthetic%20epitope%20libraries%20is%20challenging%20due%20to%20the%20large%20combinatorial%0Asequence%20space%2C%20%2420%5En%24%20combinations%20for%20linear%20epitopes%20of%20n%20amino%20acids%2C%0Amaking%20screening%20and%20testing%20unfeasible%2C%20even%20with%20high%20throughput%20experimental%0Atechniques.%20In%20this%20study%2C%20we%20present%20a%20large%20language%20model%2C%20epiGPTope%2C%0Apre-trained%20on%20protein%20data%20and%20specifically%20fine-tuned%20on%20linear%20epitopes%2C%0Awhich%20for%20the%20first%20time%20can%20directly%20generate%20novel%20epitope-like%20sequences%2C%0Awhich%20are%20found%20to%20possess%20statistical%20properties%20analogous%20to%20the%20ones%20of%0Aknown%20epitopes.%20This%20generative%20approach%20can%20be%20used%20to%20prepare%20libraries%20of%0Aepitope%20candidate%20sequences.%20We%20further%20train%20statistical%20classifiers%20to%0Apredict%20whether%20an%20epitope%20sequence%20is%20of%20bacterial%20or%20viral%20origin%2C%20thus%0Anarrowing%20the%20candidate%20library%20and%20increasing%20the%20likelihood%20of%20identifying%0Aspecific%20epitopes.%20We%20propose%20that%20such%20combination%20of%20generative%20and%0Apredictive%20models%20can%20be%20of%20assistance%20in%20epitope%20discovery.%20The%20approach%20uses%0Aonly%20primary%20amino%20acid%20sequences%20of%20linear%20epitopes%2C%20bypassing%20the%20need%20for%20a%0Ageometric%20framework%20or%20hand-crafted%20features%20of%20the%20sequences.%20By%20developing%20a%0Amethod%20to%20create%20biologically%20feasible%20sequences%2C%20we%20anticipate%20faster%20and%20more%0Acost-effective%20generation%20and%20screening%20of%20synthetic%20epitopes%2C%20with%20relevant%0Aapplications%20in%20the%20development%20of%20new%20biotechnologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03351v1&entry.124074799=Read"},
{"title": "HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through\n  Data-Driven Structural-Temporal Modeling", "author": "Minjung Park and Gyuyeon Na and Soyoun Kim and Sunyoung Moon and HyeonJeong Cha and Sangmi Chai", "abstract": "  Abnormal cryptocurrency transactions - such as mixing services, fraudulent\ntransfers, and pump-and-dump operations -- pose escalating risks to financial\nintegrity but remain notoriously difficult to detect due to class imbalance,\ntemporal volatility, and complex network dependencies. Existing approaches are\npredominantly model-centric and post hoc, flagging anomalies only after they\noccur and thus offering limited preventive value. This paper introduces\nHyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a\ndata-driven early-warning framework that explicitly incorporates lead time into\nanomaly detection. Unlike prior methods, HyPV-LEAD integrates three\ninnovations: (1) window-horizon modeling to guarantee actionable lead-time\nalerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while\npreserving temporal continuity, and (3) hyperbolic embedding to capture the\nhierarchical and scale-free properties of blockchain transaction networks.\nEmpirical evaluation on large-scale Bitcoin transaction data demonstrates that\nHyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a\nPR-AUC of 0.9624 with significant gains in precision and recall. Ablation\nstudies further confirm that each component - PV sampling, hyperbolic\nembedding, and structural-temporal modeling - provides complementary benefits,\nwith the full framework delivering the highest performance. By shifting anomaly\ndetection from reactive classification to proactive early-warning, HyPV-LEAD\nestablishes a robust foundation for real-time risk management, anti-money\nlaundering (AML) compliance, and financial security in dynamic blockchain\nenvironments.\n", "link": "http://arxiv.org/abs/2509.03260v1", "date": "2025-09-03", "relevancy": 2.2728, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4835}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4432}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyPV-LEAD%3A%20Proactive%20Early-Warning%20of%20Cryptocurrency%20Anomalies%20through%0A%20%20Data-Driven%20Structural-Temporal%20Modeling&body=Title%3A%20HyPV-LEAD%3A%20Proactive%20Early-Warning%20of%20Cryptocurrency%20Anomalies%20through%0A%20%20Data-Driven%20Structural-Temporal%20Modeling%0AAuthor%3A%20Minjung%20Park%20and%20Gyuyeon%20Na%20and%20Soyoun%20Kim%20and%20Sunyoung%20Moon%20and%20HyeonJeong%20Cha%20and%20Sangmi%20Chai%0AAbstract%3A%20%20%20Abnormal%20cryptocurrency%20transactions%20-%20such%20as%20mixing%20services%2C%20fraudulent%0Atransfers%2C%20and%20pump-and-dump%20operations%20--%20pose%20escalating%20risks%20to%20financial%0Aintegrity%20but%20remain%20notoriously%20difficult%20to%20detect%20due%20to%20class%20imbalance%2C%0Atemporal%20volatility%2C%20and%20complex%20network%20dependencies.%20Existing%20approaches%20are%0Apredominantly%20model-centric%20and%20post%20hoc%2C%20flagging%20anomalies%20only%20after%20they%0Aoccur%20and%20thus%20offering%20limited%20preventive%20value.%20This%20paper%20introduces%0AHyPV-LEAD%20%28Hyperbolic%20Peak-Valley%20Lead-time%20Enabled%20Anomaly%20Detection%29%2C%20a%0Adata-driven%20early-warning%20framework%20that%20explicitly%20incorporates%20lead%20time%20into%0Aanomaly%20detection.%20Unlike%20prior%20methods%2C%20HyPV-LEAD%20integrates%20three%0Ainnovations%3A%20%281%29%20window-horizon%20modeling%20to%20guarantee%20actionable%20lead-time%0Aalerts%2C%20%282%29%20Peak-Valley%20%28PV%29%20sampling%20to%20mitigate%20class%20imbalance%20while%0Apreserving%20temporal%20continuity%2C%20and%20%283%29%20hyperbolic%20embedding%20to%20capture%20the%0Ahierarchical%20and%20scale-free%20properties%20of%20blockchain%20transaction%20networks.%0AEmpirical%20evaluation%20on%20large-scale%20Bitcoin%20transaction%20data%20demonstrates%20that%0AHyPV-LEAD%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20a%0APR-AUC%20of%200.9624%20with%20significant%20gains%20in%20precision%20and%20recall.%20Ablation%0Astudies%20further%20confirm%20that%20each%20component%20-%20PV%20sampling%2C%20hyperbolic%0Aembedding%2C%20and%20structural-temporal%20modeling%20-%20provides%20complementary%20benefits%2C%0Awith%20the%20full%20framework%20delivering%20the%20highest%20performance.%20By%20shifting%20anomaly%0Adetection%20from%20reactive%20classification%20to%20proactive%20early-warning%2C%20HyPV-LEAD%0Aestablishes%20a%20robust%20foundation%20for%20real-time%20risk%20management%2C%20anti-money%0Alaundering%20%28AML%29%20compliance%2C%20and%20financial%20security%20in%20dynamic%20blockchain%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyPV-LEAD%253A%2520Proactive%2520Early-Warning%2520of%2520Cryptocurrency%2520Anomalies%2520through%250A%2520%2520Data-Driven%2520Structural-Temporal%2520Modeling%26entry.906535625%3DMinjung%2520Park%2520and%2520Gyuyeon%2520Na%2520and%2520Soyoun%2520Kim%2520and%2520Sunyoung%2520Moon%2520and%2520HyeonJeong%2520Cha%2520and%2520Sangmi%2520Chai%26entry.1292438233%3D%2520%2520Abnormal%2520cryptocurrency%2520transactions%2520-%2520such%2520as%2520mixing%2520services%252C%2520fraudulent%250Atransfers%252C%2520and%2520pump-and-dump%2520operations%2520--%2520pose%2520escalating%2520risks%2520to%2520financial%250Aintegrity%2520but%2520remain%2520notoriously%2520difficult%2520to%2520detect%2520due%2520to%2520class%2520imbalance%252C%250Atemporal%2520volatility%252C%2520and%2520complex%2520network%2520dependencies.%2520Existing%2520approaches%2520are%250Apredominantly%2520model-centric%2520and%2520post%2520hoc%252C%2520flagging%2520anomalies%2520only%2520after%2520they%250Aoccur%2520and%2520thus%2520offering%2520limited%2520preventive%2520value.%2520This%2520paper%2520introduces%250AHyPV-LEAD%2520%2528Hyperbolic%2520Peak-Valley%2520Lead-time%2520Enabled%2520Anomaly%2520Detection%2529%252C%2520a%250Adata-driven%2520early-warning%2520framework%2520that%2520explicitly%2520incorporates%2520lead%2520time%2520into%250Aanomaly%2520detection.%2520Unlike%2520prior%2520methods%252C%2520HyPV-LEAD%2520integrates%2520three%250Ainnovations%253A%2520%25281%2529%2520window-horizon%2520modeling%2520to%2520guarantee%2520actionable%2520lead-time%250Aalerts%252C%2520%25282%2529%2520Peak-Valley%2520%2528PV%2529%2520sampling%2520to%2520mitigate%2520class%2520imbalance%2520while%250Apreserving%2520temporal%2520continuity%252C%2520and%2520%25283%2529%2520hyperbolic%2520embedding%2520to%2520capture%2520the%250Ahierarchical%2520and%2520scale-free%2520properties%2520of%2520blockchain%2520transaction%2520networks.%250AEmpirical%2520evaluation%2520on%2520large-scale%2520Bitcoin%2520transaction%2520data%2520demonstrates%2520that%250AHyPV-LEAD%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%252C%2520achieving%2520a%250APR-AUC%2520of%25200.9624%2520with%2520significant%2520gains%2520in%2520precision%2520and%2520recall.%2520Ablation%250Astudies%2520further%2520confirm%2520that%2520each%2520component%2520-%2520PV%2520sampling%252C%2520hyperbolic%250Aembedding%252C%2520and%2520structural-temporal%2520modeling%2520-%2520provides%2520complementary%2520benefits%252C%250Awith%2520the%2520full%2520framework%2520delivering%2520the%2520highest%2520performance.%2520By%2520shifting%2520anomaly%250Adetection%2520from%2520reactive%2520classification%2520to%2520proactive%2520early-warning%252C%2520HyPV-LEAD%250Aestablishes%2520a%2520robust%2520foundation%2520for%2520real-time%2520risk%2520management%252C%2520anti-money%250Alaundering%2520%2528AML%2529%2520compliance%252C%2520and%2520financial%2520security%2520in%2520dynamic%2520blockchain%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyPV-LEAD%3A%20Proactive%20Early-Warning%20of%20Cryptocurrency%20Anomalies%20through%0A%20%20Data-Driven%20Structural-Temporal%20Modeling&entry.906535625=Minjung%20Park%20and%20Gyuyeon%20Na%20and%20Soyoun%20Kim%20and%20Sunyoung%20Moon%20and%20HyeonJeong%20Cha%20and%20Sangmi%20Chai&entry.1292438233=%20%20Abnormal%20cryptocurrency%20transactions%20-%20such%20as%20mixing%20services%2C%20fraudulent%0Atransfers%2C%20and%20pump-and-dump%20operations%20--%20pose%20escalating%20risks%20to%20financial%0Aintegrity%20but%20remain%20notoriously%20difficult%20to%20detect%20due%20to%20class%20imbalance%2C%0Atemporal%20volatility%2C%20and%20complex%20network%20dependencies.%20Existing%20approaches%20are%0Apredominantly%20model-centric%20and%20post%20hoc%2C%20flagging%20anomalies%20only%20after%20they%0Aoccur%20and%20thus%20offering%20limited%20preventive%20value.%20This%20paper%20introduces%0AHyPV-LEAD%20%28Hyperbolic%20Peak-Valley%20Lead-time%20Enabled%20Anomaly%20Detection%29%2C%20a%0Adata-driven%20early-warning%20framework%20that%20explicitly%20incorporates%20lead%20time%20into%0Aanomaly%20detection.%20Unlike%20prior%20methods%2C%20HyPV-LEAD%20integrates%20three%0Ainnovations%3A%20%281%29%20window-horizon%20modeling%20to%20guarantee%20actionable%20lead-time%0Aalerts%2C%20%282%29%20Peak-Valley%20%28PV%29%20sampling%20to%20mitigate%20class%20imbalance%20while%0Apreserving%20temporal%20continuity%2C%20and%20%283%29%20hyperbolic%20embedding%20to%20capture%20the%0Ahierarchical%20and%20scale-free%20properties%20of%20blockchain%20transaction%20networks.%0AEmpirical%20evaluation%20on%20large-scale%20Bitcoin%20transaction%20data%20demonstrates%20that%0AHyPV-LEAD%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20a%0APR-AUC%20of%200.9624%20with%20significant%20gains%20in%20precision%20and%20recall.%20Ablation%0Astudies%20further%20confirm%20that%20each%20component%20-%20PV%20sampling%2C%20hyperbolic%0Aembedding%2C%20and%20structural-temporal%20modeling%20-%20provides%20complementary%20benefits%2C%0Awith%20the%20full%20framework%20delivering%20the%20highest%20performance.%20By%20shifting%20anomaly%0Adetection%20from%20reactive%20classification%20to%20proactive%20early-warning%2C%20HyPV-LEAD%0Aestablishes%20a%20robust%20foundation%20for%20real-time%20risk%20management%2C%20anti-money%0Alaundering%20%28AML%29%20compliance%2C%20and%20financial%20security%20in%20dynamic%20blockchain%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03260v1&entry.124074799=Read"},
{"title": "Multimodal Medical Image Binding via Shared Text Embeddings", "author": "Yunhao Liu and Suyang Xi and Shiqi Liu and Hong Ding and Chicheng Jin and Chong Zhong and Junjun He and Catherine C. Liu and Yiqing Shen", "abstract": "  Medical image analysis increasingly relies on the integration of multiple\nimaging modalities to capture complementary anatomical and functional\ninformation, enabling more accurate diagnosis and treatment planning. Achieving\naligned feature representations across these diverse modalities is therefore\nimportant for effective multimodal analysis. While contrastive language-image\npre-training (CLIP) and its variant have enabled image-text alignments, they\nrequire explicitly paired data between arbitrary two modalities, which is\ndifficult to acquire in medical contexts. To address the gap, we present\nMultimodal Medical Image Binding with Text (M\\textsuperscript{3}Bind), a novel\npre-training framework that enables seamless alignment of multiple medical\nimaging modalities through a shared text representation space without requiring\nexplicit paired data between any two medical image modalities. Specifically,\nbased on the insight that different images can naturally bind with text,\nM\\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text\nmodels to align their modality-specific text embedding space while preserving\ntheir original image-text alignments. Subsequently, we distill these\nmodality-specific text encoders into a unified model, creating a shared text\nembedding space. Experiments on X-ray, CT, retina, ECG, and pathological images\non multiple downstream tasks demonstrate that M\\textsuperscript{3}Bind achieves\nstate-of-the-art performance in zero-shot, few-shot classification and\ncross-modal retrieval tasks compared to its CLIP-like counterparts. These\nresults validate M\\textsuperscript{3}Bind's effectiveness in achieving\ncross-image-modal alignment for medical analysis.\n", "link": "http://arxiv.org/abs/2506.18072v2", "date": "2025-09-03", "relevancy": 2.2502, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6269}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5173}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Medical%20Image%20Binding%20via%20Shared%20Text%20Embeddings&body=Title%3A%20Multimodal%20Medical%20Image%20Binding%20via%20Shared%20Text%20Embeddings%0AAuthor%3A%20Yunhao%20Liu%20and%20Suyang%20Xi%20and%20Shiqi%20Liu%20and%20Hong%20Ding%20and%20Chicheng%20Jin%20and%20Chong%20Zhong%20and%20Junjun%20He%20and%20Catherine%20C.%20Liu%20and%20Yiqing%20Shen%0AAbstract%3A%20%20%20Medical%20image%20analysis%20increasingly%20relies%20on%20the%20integration%20of%20multiple%0Aimaging%20modalities%20to%20capture%20complementary%20anatomical%20and%20functional%0Ainformation%2C%20enabling%20more%20accurate%20diagnosis%20and%20treatment%20planning.%20Achieving%0Aaligned%20feature%20representations%20across%20these%20diverse%20modalities%20is%20therefore%0Aimportant%20for%20effective%20multimodal%20analysis.%20While%20contrastive%20language-image%0Apre-training%20%28CLIP%29%20and%20its%20variant%20have%20enabled%20image-text%20alignments%2C%20they%0Arequire%20explicitly%20paired%20data%20between%20arbitrary%20two%20modalities%2C%20which%20is%0Adifficult%20to%20acquire%20in%20medical%20contexts.%20To%20address%20the%20gap%2C%20we%20present%0AMultimodal%20Medical%20Image%20Binding%20with%20Text%20%28M%5Ctextsuperscript%7B3%7DBind%29%2C%20a%20novel%0Apre-training%20framework%20that%20enables%20seamless%20alignment%20of%20multiple%20medical%0Aimaging%20modalities%20through%20a%20shared%20text%20representation%20space%20without%20requiring%0Aexplicit%20paired%20data%20between%20any%20two%20medical%20image%20modalities.%20Specifically%2C%0Abased%20on%20the%20insight%20that%20different%20images%20can%20naturally%20bind%20with%20text%2C%0AM%5Ctextsuperscript%7B3%7DBind%20first%20fine-tunes%20pre-trained%20CLIP-like%20image-text%0Amodels%20to%20align%20their%20modality-specific%20text%20embedding%20space%20while%20preserving%0Atheir%20original%20image-text%20alignments.%20Subsequently%2C%20we%20distill%20these%0Amodality-specific%20text%20encoders%20into%20a%20unified%20model%2C%20creating%20a%20shared%20text%0Aembedding%20space.%20Experiments%20on%20X-ray%2C%20CT%2C%20retina%2C%20ECG%2C%20and%20pathological%20images%0Aon%20multiple%20downstream%20tasks%20demonstrate%20that%20M%5Ctextsuperscript%7B3%7DBind%20achieves%0Astate-of-the-art%20performance%20in%20zero-shot%2C%20few-shot%20classification%20and%0Across-modal%20retrieval%20tasks%20compared%20to%20its%20CLIP-like%20counterparts.%20These%0Aresults%20validate%20M%5Ctextsuperscript%7B3%7DBind%27s%20effectiveness%20in%20achieving%0Across-image-modal%20alignment%20for%20medical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Medical%2520Image%2520Binding%2520via%2520Shared%2520Text%2520Embeddings%26entry.906535625%3DYunhao%2520Liu%2520and%2520Suyang%2520Xi%2520and%2520Shiqi%2520Liu%2520and%2520Hong%2520Ding%2520and%2520Chicheng%2520Jin%2520and%2520Chong%2520Zhong%2520and%2520Junjun%2520He%2520and%2520Catherine%2520C.%2520Liu%2520and%2520Yiqing%2520Shen%26entry.1292438233%3D%2520%2520Medical%2520image%2520analysis%2520increasingly%2520relies%2520on%2520the%2520integration%2520of%2520multiple%250Aimaging%2520modalities%2520to%2520capture%2520complementary%2520anatomical%2520and%2520functional%250Ainformation%252C%2520enabling%2520more%2520accurate%2520diagnosis%2520and%2520treatment%2520planning.%2520Achieving%250Aaligned%2520feature%2520representations%2520across%2520these%2520diverse%2520modalities%2520is%2520therefore%250Aimportant%2520for%2520effective%2520multimodal%2520analysis.%2520While%2520contrastive%2520language-image%250Apre-training%2520%2528CLIP%2529%2520and%2520its%2520variant%2520have%2520enabled%2520image-text%2520alignments%252C%2520they%250Arequire%2520explicitly%2520paired%2520data%2520between%2520arbitrary%2520two%2520modalities%252C%2520which%2520is%250Adifficult%2520to%2520acquire%2520in%2520medical%2520contexts.%2520To%2520address%2520the%2520gap%252C%2520we%2520present%250AMultimodal%2520Medical%2520Image%2520Binding%2520with%2520Text%2520%2528M%255Ctextsuperscript%257B3%257DBind%2529%252C%2520a%2520novel%250Apre-training%2520framework%2520that%2520enables%2520seamless%2520alignment%2520of%2520multiple%2520medical%250Aimaging%2520modalities%2520through%2520a%2520shared%2520text%2520representation%2520space%2520without%2520requiring%250Aexplicit%2520paired%2520data%2520between%2520any%2520two%2520medical%2520image%2520modalities.%2520Specifically%252C%250Abased%2520on%2520the%2520insight%2520that%2520different%2520images%2520can%2520naturally%2520bind%2520with%2520text%252C%250AM%255Ctextsuperscript%257B3%257DBind%2520first%2520fine-tunes%2520pre-trained%2520CLIP-like%2520image-text%250Amodels%2520to%2520align%2520their%2520modality-specific%2520text%2520embedding%2520space%2520while%2520preserving%250Atheir%2520original%2520image-text%2520alignments.%2520Subsequently%252C%2520we%2520distill%2520these%250Amodality-specific%2520text%2520encoders%2520into%2520a%2520unified%2520model%252C%2520creating%2520a%2520shared%2520text%250Aembedding%2520space.%2520Experiments%2520on%2520X-ray%252C%2520CT%252C%2520retina%252C%2520ECG%252C%2520and%2520pathological%2520images%250Aon%2520multiple%2520downstream%2520tasks%2520demonstrate%2520that%2520M%255Ctextsuperscript%257B3%257DBind%2520achieves%250Astate-of-the-art%2520performance%2520in%2520zero-shot%252C%2520few-shot%2520classification%2520and%250Across-modal%2520retrieval%2520tasks%2520compared%2520to%2520its%2520CLIP-like%2520counterparts.%2520These%250Aresults%2520validate%2520M%255Ctextsuperscript%257B3%257DBind%2527s%2520effectiveness%2520in%2520achieving%250Across-image-modal%2520alignment%2520for%2520medical%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Medical%20Image%20Binding%20via%20Shared%20Text%20Embeddings&entry.906535625=Yunhao%20Liu%20and%20Suyang%20Xi%20and%20Shiqi%20Liu%20and%20Hong%20Ding%20and%20Chicheng%20Jin%20and%20Chong%20Zhong%20and%20Junjun%20He%20and%20Catherine%20C.%20Liu%20and%20Yiqing%20Shen&entry.1292438233=%20%20Medical%20image%20analysis%20increasingly%20relies%20on%20the%20integration%20of%20multiple%0Aimaging%20modalities%20to%20capture%20complementary%20anatomical%20and%20functional%0Ainformation%2C%20enabling%20more%20accurate%20diagnosis%20and%20treatment%20planning.%20Achieving%0Aaligned%20feature%20representations%20across%20these%20diverse%20modalities%20is%20therefore%0Aimportant%20for%20effective%20multimodal%20analysis.%20While%20contrastive%20language-image%0Apre-training%20%28CLIP%29%20and%20its%20variant%20have%20enabled%20image-text%20alignments%2C%20they%0Arequire%20explicitly%20paired%20data%20between%20arbitrary%20two%20modalities%2C%20which%20is%0Adifficult%20to%20acquire%20in%20medical%20contexts.%20To%20address%20the%20gap%2C%20we%20present%0AMultimodal%20Medical%20Image%20Binding%20with%20Text%20%28M%5Ctextsuperscript%7B3%7DBind%29%2C%20a%20novel%0Apre-training%20framework%20that%20enables%20seamless%20alignment%20of%20multiple%20medical%0Aimaging%20modalities%20through%20a%20shared%20text%20representation%20space%20without%20requiring%0Aexplicit%20paired%20data%20between%20any%20two%20medical%20image%20modalities.%20Specifically%2C%0Abased%20on%20the%20insight%20that%20different%20images%20can%20naturally%20bind%20with%20text%2C%0AM%5Ctextsuperscript%7B3%7DBind%20first%20fine-tunes%20pre-trained%20CLIP-like%20image-text%0Amodels%20to%20align%20their%20modality-specific%20text%20embedding%20space%20while%20preserving%0Atheir%20original%20image-text%20alignments.%20Subsequently%2C%20we%20distill%20these%0Amodality-specific%20text%20encoders%20into%20a%20unified%20model%2C%20creating%20a%20shared%20text%0Aembedding%20space.%20Experiments%20on%20X-ray%2C%20CT%2C%20retina%2C%20ECG%2C%20and%20pathological%20images%0Aon%20multiple%20downstream%20tasks%20demonstrate%20that%20M%5Ctextsuperscript%7B3%7DBind%20achieves%0Astate-of-the-art%20performance%20in%20zero-shot%2C%20few-shot%20classification%20and%0Across-modal%20retrieval%20tasks%20compared%20to%20its%20CLIP-like%20counterparts.%20These%0Aresults%20validate%20M%5Ctextsuperscript%7B3%7DBind%27s%20effectiveness%20in%20achieving%0Across-image-modal%20alignment%20for%20medical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18072v2&entry.124074799=Read"},
{"title": "Graph neural networks for learning liquid simulations in dynamic scenes\n  containing kinematic objects", "author": "Niteesh Midlagajni and Constantin A. Rothkopf", "abstract": "  Simulating particle dynamics with high fidelity is crucial for solving\nreal-world interaction and control tasks involving liquids in design, graphics,\nand robotics. Recently, data-driven approaches, particularly those based on\ngraph neural networks (GNNs), have shown progress in tackling such problems.\nHowever, these approaches are often limited to learning fluid behavior in\nstatic free-fall environments or simple manipulation settings involving\nprimitive objects, often overlooking complex interactions with dynamically\nmoving kinematic rigid bodies. Here, we propose a GNN-based framework designed\nfrom the ground up to learn the dynamics of liquids under rigid body\ninteractions and active manipulations, where particles are represented as graph\nnodes and particle-object collisions are handled using surface representations\nwith the bounding volume hierarchy (BVH) algorithm. This approach enables the\nnetwork to model complex interactions between liquid particles and intricate\nsurface geometries. Our model accurately captures fluid behavior in dynamic\nsettings and can also function as a simulator in static free-fall environments.\nDespite being trained on a single-object manipulation task of pouring, our\nmodel generalizes effectively to environments with unseen objects and novel\nmanipulation tasks such as stirring and scooping. Finally, we show that the\nlearned dynamics can be leveraged to solve control and manipulation tasks using\ngradient-based optimization methods.\n", "link": "http://arxiv.org/abs/2509.03446v1", "date": "2025-09-03", "relevancy": 2.2353, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5701}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5588}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20neural%20networks%20for%20learning%20liquid%20simulations%20in%20dynamic%20scenes%0A%20%20containing%20kinematic%20objects&body=Title%3A%20Graph%20neural%20networks%20for%20learning%20liquid%20simulations%20in%20dynamic%20scenes%0A%20%20containing%20kinematic%20objects%0AAuthor%3A%20Niteesh%20Midlagajni%20and%20Constantin%20A.%20Rothkopf%0AAbstract%3A%20%20%20Simulating%20particle%20dynamics%20with%20high%20fidelity%20is%20crucial%20for%20solving%0Areal-world%20interaction%20and%20control%20tasks%20involving%20liquids%20in%20design%2C%20graphics%2C%0Aand%20robotics.%20Recently%2C%20data-driven%20approaches%2C%20particularly%20those%20based%20on%0Agraph%20neural%20networks%20%28GNNs%29%2C%20have%20shown%20progress%20in%20tackling%20such%20problems.%0AHowever%2C%20these%20approaches%20are%20often%20limited%20to%20learning%20fluid%20behavior%20in%0Astatic%20free-fall%20environments%20or%20simple%20manipulation%20settings%20involving%0Aprimitive%20objects%2C%20often%20overlooking%20complex%20interactions%20with%20dynamically%0Amoving%20kinematic%20rigid%20bodies.%20Here%2C%20we%20propose%20a%20GNN-based%20framework%20designed%0Afrom%20the%20ground%20up%20to%20learn%20the%20dynamics%20of%20liquids%20under%20rigid%20body%0Ainteractions%20and%20active%20manipulations%2C%20where%20particles%20are%20represented%20as%20graph%0Anodes%20and%20particle-object%20collisions%20are%20handled%20using%20surface%20representations%0Awith%20the%20bounding%20volume%20hierarchy%20%28BVH%29%20algorithm.%20This%20approach%20enables%20the%0Anetwork%20to%20model%20complex%20interactions%20between%20liquid%20particles%20and%20intricate%0Asurface%20geometries.%20Our%20model%20accurately%20captures%20fluid%20behavior%20in%20dynamic%0Asettings%20and%20can%20also%20function%20as%20a%20simulator%20in%20static%20free-fall%20environments.%0ADespite%20being%20trained%20on%20a%20single-object%20manipulation%20task%20of%20pouring%2C%20our%0Amodel%20generalizes%20effectively%20to%20environments%20with%20unseen%20objects%20and%20novel%0Amanipulation%20tasks%20such%20as%20stirring%20and%20scooping.%20Finally%2C%20we%20show%20that%20the%0Alearned%20dynamics%20can%20be%20leveraged%20to%20solve%20control%20and%20manipulation%20tasks%20using%0Agradient-based%20optimization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520neural%2520networks%2520for%2520learning%2520liquid%2520simulations%2520in%2520dynamic%2520scenes%250A%2520%2520containing%2520kinematic%2520objects%26entry.906535625%3DNiteesh%2520Midlagajni%2520and%2520Constantin%2520A.%2520Rothkopf%26entry.1292438233%3D%2520%2520Simulating%2520particle%2520dynamics%2520with%2520high%2520fidelity%2520is%2520crucial%2520for%2520solving%250Areal-world%2520interaction%2520and%2520control%2520tasks%2520involving%2520liquids%2520in%2520design%252C%2520graphics%252C%250Aand%2520robotics.%2520Recently%252C%2520data-driven%2520approaches%252C%2520particularly%2520those%2520based%2520on%250Agraph%2520neural%2520networks%2520%2528GNNs%2529%252C%2520have%2520shown%2520progress%2520in%2520tackling%2520such%2520problems.%250AHowever%252C%2520these%2520approaches%2520are%2520often%2520limited%2520to%2520learning%2520fluid%2520behavior%2520in%250Astatic%2520free-fall%2520environments%2520or%2520simple%2520manipulation%2520settings%2520involving%250Aprimitive%2520objects%252C%2520often%2520overlooking%2520complex%2520interactions%2520with%2520dynamically%250Amoving%2520kinematic%2520rigid%2520bodies.%2520Here%252C%2520we%2520propose%2520a%2520GNN-based%2520framework%2520designed%250Afrom%2520the%2520ground%2520up%2520to%2520learn%2520the%2520dynamics%2520of%2520liquids%2520under%2520rigid%2520body%250Ainteractions%2520and%2520active%2520manipulations%252C%2520where%2520particles%2520are%2520represented%2520as%2520graph%250Anodes%2520and%2520particle-object%2520collisions%2520are%2520handled%2520using%2520surface%2520representations%250Awith%2520the%2520bounding%2520volume%2520hierarchy%2520%2528BVH%2529%2520algorithm.%2520This%2520approach%2520enables%2520the%250Anetwork%2520to%2520model%2520complex%2520interactions%2520between%2520liquid%2520particles%2520and%2520intricate%250Asurface%2520geometries.%2520Our%2520model%2520accurately%2520captures%2520fluid%2520behavior%2520in%2520dynamic%250Asettings%2520and%2520can%2520also%2520function%2520as%2520a%2520simulator%2520in%2520static%2520free-fall%2520environments.%250ADespite%2520being%2520trained%2520on%2520a%2520single-object%2520manipulation%2520task%2520of%2520pouring%252C%2520our%250Amodel%2520generalizes%2520effectively%2520to%2520environments%2520with%2520unseen%2520objects%2520and%2520novel%250Amanipulation%2520tasks%2520such%2520as%2520stirring%2520and%2520scooping.%2520Finally%252C%2520we%2520show%2520that%2520the%250Alearned%2520dynamics%2520can%2520be%2520leveraged%2520to%2520solve%2520control%2520and%2520manipulation%2520tasks%2520using%250Agradient-based%2520optimization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20neural%20networks%20for%20learning%20liquid%20simulations%20in%20dynamic%20scenes%0A%20%20containing%20kinematic%20objects&entry.906535625=Niteesh%20Midlagajni%20and%20Constantin%20A.%20Rothkopf&entry.1292438233=%20%20Simulating%20particle%20dynamics%20with%20high%20fidelity%20is%20crucial%20for%20solving%0Areal-world%20interaction%20and%20control%20tasks%20involving%20liquids%20in%20design%2C%20graphics%2C%0Aand%20robotics.%20Recently%2C%20data-driven%20approaches%2C%20particularly%20those%20based%20on%0Agraph%20neural%20networks%20%28GNNs%29%2C%20have%20shown%20progress%20in%20tackling%20such%20problems.%0AHowever%2C%20these%20approaches%20are%20often%20limited%20to%20learning%20fluid%20behavior%20in%0Astatic%20free-fall%20environments%20or%20simple%20manipulation%20settings%20involving%0Aprimitive%20objects%2C%20often%20overlooking%20complex%20interactions%20with%20dynamically%0Amoving%20kinematic%20rigid%20bodies.%20Here%2C%20we%20propose%20a%20GNN-based%20framework%20designed%0Afrom%20the%20ground%20up%20to%20learn%20the%20dynamics%20of%20liquids%20under%20rigid%20body%0Ainteractions%20and%20active%20manipulations%2C%20where%20particles%20are%20represented%20as%20graph%0Anodes%20and%20particle-object%20collisions%20are%20handled%20using%20surface%20representations%0Awith%20the%20bounding%20volume%20hierarchy%20%28BVH%29%20algorithm.%20This%20approach%20enables%20the%0Anetwork%20to%20model%20complex%20interactions%20between%20liquid%20particles%20and%20intricate%0Asurface%20geometries.%20Our%20model%20accurately%20captures%20fluid%20behavior%20in%20dynamic%0Asettings%20and%20can%20also%20function%20as%20a%20simulator%20in%20static%20free-fall%20environments.%0ADespite%20being%20trained%20on%20a%20single-object%20manipulation%20task%20of%20pouring%2C%20our%0Amodel%20generalizes%20effectively%20to%20environments%20with%20unseen%20objects%20and%20novel%0Amanipulation%20tasks%20such%20as%20stirring%20and%20scooping.%20Finally%2C%20we%20show%20that%20the%0Alearned%20dynamics%20can%20be%20leveraged%20to%20solve%20control%20and%20manipulation%20tasks%20using%0Agradient-based%20optimization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03446v1&entry.124074799=Read"},
{"title": "From Image Denoisers to Regularizing Imaging Inverse Problems: An\n  Overview", "author": "Hong Ye Tan and Subhadip Mukherjee and Junqi Tang", "abstract": "  Inverse problems lie at the heart of modern imaging science, with broad\napplications in areas such as medical imaging, remote sensing, and microscopy.\nRecent years have witnessed a paradigm shift in solving imaging inverse\nproblems, where data-driven regularizers are used increasingly, leading to\nremarkably high-fidelity reconstruction. A particularly notable approach for\ndata-driven regularization is to use learned image denoisers as implicit priors\nin iterative image reconstruction algorithms. This survey presents a\ncomprehensive overview of this powerful and emerging class of algorithms,\ncommonly referred to as plug-and-play (PnP) methods. We begin by providing a\nbrief background on image denoising and inverse problems, followed by a short\nreview of traditional regularization strategies. We then explore how proximal\nsplitting algorithms, such as the alternating direction method of multipliers\n(ADMM) and proximal gradient descent (PGD), can naturally accommodate learned\ndenoisers in place of proximal operators, and under what conditions such\nreplacements preserve convergence. The role of Tweedie's formula in connecting\noptimal Gaussian denoisers and score estimation is discussed, which lays the\nfoundation for regularization-by-denoising (RED) and more recent\ndiffusion-based posterior sampling methods. We discuss theoretical advances\nregarding the convergence of PnP algorithms, both within the RED and proximal\nsettings, emphasizing the structural assumptions that the denoiser must satisfy\nfor convergence, such as non-expansiveness, Lipschitz continuity, and local\nhomogeneity. We also address practical considerations in algorithm design,\nincluding choices of denoiser architecture and acceleration strategies.\n", "link": "http://arxiv.org/abs/2509.03475v1", "date": "2025-09-03", "relevancy": 2.2191, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5628}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5508}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Image%20Denoisers%20to%20Regularizing%20Imaging%20Inverse%20Problems%3A%20An%0A%20%20Overview&body=Title%3A%20From%20Image%20Denoisers%20to%20Regularizing%20Imaging%20Inverse%20Problems%3A%20An%0A%20%20Overview%0AAuthor%3A%20Hong%20Ye%20Tan%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang%0AAbstract%3A%20%20%20Inverse%20problems%20lie%20at%20the%20heart%20of%20modern%20imaging%20science%2C%20with%20broad%0Aapplications%20in%20areas%20such%20as%20medical%20imaging%2C%20remote%20sensing%2C%20and%20microscopy.%0ARecent%20years%20have%20witnessed%20a%20paradigm%20shift%20in%20solving%20imaging%20inverse%0Aproblems%2C%20where%20data-driven%20regularizers%20are%20used%20increasingly%2C%20leading%20to%0Aremarkably%20high-fidelity%20reconstruction.%20A%20particularly%20notable%20approach%20for%0Adata-driven%20regularization%20is%20to%20use%20learned%20image%20denoisers%20as%20implicit%20priors%0Ain%20iterative%20image%20reconstruction%20algorithms.%20This%20survey%20presents%20a%0Acomprehensive%20overview%20of%20this%20powerful%20and%20emerging%20class%20of%20algorithms%2C%0Acommonly%20referred%20to%20as%20plug-and-play%20%28PnP%29%20methods.%20We%20begin%20by%20providing%20a%0Abrief%20background%20on%20image%20denoising%20and%20inverse%20problems%2C%20followed%20by%20a%20short%0Areview%20of%20traditional%20regularization%20strategies.%20We%20then%20explore%20how%20proximal%0Asplitting%20algorithms%2C%20such%20as%20the%20alternating%20direction%20method%20of%20multipliers%0A%28ADMM%29%20and%20proximal%20gradient%20descent%20%28PGD%29%2C%20can%20naturally%20accommodate%20learned%0Adenoisers%20in%20place%20of%20proximal%20operators%2C%20and%20under%20what%20conditions%20such%0Areplacements%20preserve%20convergence.%20The%20role%20of%20Tweedie%27s%20formula%20in%20connecting%0Aoptimal%20Gaussian%20denoisers%20and%20score%20estimation%20is%20discussed%2C%20which%20lays%20the%0Afoundation%20for%20regularization-by-denoising%20%28RED%29%20and%20more%20recent%0Adiffusion-based%20posterior%20sampling%20methods.%20We%20discuss%20theoretical%20advances%0Aregarding%20the%20convergence%20of%20PnP%20algorithms%2C%20both%20within%20the%20RED%20and%20proximal%0Asettings%2C%20emphasizing%20the%20structural%20assumptions%20that%20the%20denoiser%20must%20satisfy%0Afor%20convergence%2C%20such%20as%20non-expansiveness%2C%20Lipschitz%20continuity%2C%20and%20local%0Ahomogeneity.%20We%20also%20address%20practical%20considerations%20in%20algorithm%20design%2C%0Aincluding%20choices%20of%20denoiser%20architecture%20and%20acceleration%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Image%2520Denoisers%2520to%2520Regularizing%2520Imaging%2520Inverse%2520Problems%253A%2520An%250A%2520%2520Overview%26entry.906535625%3DHong%2520Ye%2520Tan%2520and%2520Subhadip%2520Mukherjee%2520and%2520Junqi%2520Tang%26entry.1292438233%3D%2520%2520Inverse%2520problems%2520lie%2520at%2520the%2520heart%2520of%2520modern%2520imaging%2520science%252C%2520with%2520broad%250Aapplications%2520in%2520areas%2520such%2520as%2520medical%2520imaging%252C%2520remote%2520sensing%252C%2520and%2520microscopy.%250ARecent%2520years%2520have%2520witnessed%2520a%2520paradigm%2520shift%2520in%2520solving%2520imaging%2520inverse%250Aproblems%252C%2520where%2520data-driven%2520regularizers%2520are%2520used%2520increasingly%252C%2520leading%2520to%250Aremarkably%2520high-fidelity%2520reconstruction.%2520A%2520particularly%2520notable%2520approach%2520for%250Adata-driven%2520regularization%2520is%2520to%2520use%2520learned%2520image%2520denoisers%2520as%2520implicit%2520priors%250Ain%2520iterative%2520image%2520reconstruction%2520algorithms.%2520This%2520survey%2520presents%2520a%250Acomprehensive%2520overview%2520of%2520this%2520powerful%2520and%2520emerging%2520class%2520of%2520algorithms%252C%250Acommonly%2520referred%2520to%2520as%2520plug-and-play%2520%2528PnP%2529%2520methods.%2520We%2520begin%2520by%2520providing%2520a%250Abrief%2520background%2520on%2520image%2520denoising%2520and%2520inverse%2520problems%252C%2520followed%2520by%2520a%2520short%250Areview%2520of%2520traditional%2520regularization%2520strategies.%2520We%2520then%2520explore%2520how%2520proximal%250Asplitting%2520algorithms%252C%2520such%2520as%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%250A%2528ADMM%2529%2520and%2520proximal%2520gradient%2520descent%2520%2528PGD%2529%252C%2520can%2520naturally%2520accommodate%2520learned%250Adenoisers%2520in%2520place%2520of%2520proximal%2520operators%252C%2520and%2520under%2520what%2520conditions%2520such%250Areplacements%2520preserve%2520convergence.%2520The%2520role%2520of%2520Tweedie%2527s%2520formula%2520in%2520connecting%250Aoptimal%2520Gaussian%2520denoisers%2520and%2520score%2520estimation%2520is%2520discussed%252C%2520which%2520lays%2520the%250Afoundation%2520for%2520regularization-by-denoising%2520%2528RED%2529%2520and%2520more%2520recent%250Adiffusion-based%2520posterior%2520sampling%2520methods.%2520We%2520discuss%2520theoretical%2520advances%250Aregarding%2520the%2520convergence%2520of%2520PnP%2520algorithms%252C%2520both%2520within%2520the%2520RED%2520and%2520proximal%250Asettings%252C%2520emphasizing%2520the%2520structural%2520assumptions%2520that%2520the%2520denoiser%2520must%2520satisfy%250Afor%2520convergence%252C%2520such%2520as%2520non-expansiveness%252C%2520Lipschitz%2520continuity%252C%2520and%2520local%250Ahomogeneity.%2520We%2520also%2520address%2520practical%2520considerations%2520in%2520algorithm%2520design%252C%250Aincluding%2520choices%2520of%2520denoiser%2520architecture%2520and%2520acceleration%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Image%20Denoisers%20to%20Regularizing%20Imaging%20Inverse%20Problems%3A%20An%0A%20%20Overview&entry.906535625=Hong%20Ye%20Tan%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang&entry.1292438233=%20%20Inverse%20problems%20lie%20at%20the%20heart%20of%20modern%20imaging%20science%2C%20with%20broad%0Aapplications%20in%20areas%20such%20as%20medical%20imaging%2C%20remote%20sensing%2C%20and%20microscopy.%0ARecent%20years%20have%20witnessed%20a%20paradigm%20shift%20in%20solving%20imaging%20inverse%0Aproblems%2C%20where%20data-driven%20regularizers%20are%20used%20increasingly%2C%20leading%20to%0Aremarkably%20high-fidelity%20reconstruction.%20A%20particularly%20notable%20approach%20for%0Adata-driven%20regularization%20is%20to%20use%20learned%20image%20denoisers%20as%20implicit%20priors%0Ain%20iterative%20image%20reconstruction%20algorithms.%20This%20survey%20presents%20a%0Acomprehensive%20overview%20of%20this%20powerful%20and%20emerging%20class%20of%20algorithms%2C%0Acommonly%20referred%20to%20as%20plug-and-play%20%28PnP%29%20methods.%20We%20begin%20by%20providing%20a%0Abrief%20background%20on%20image%20denoising%20and%20inverse%20problems%2C%20followed%20by%20a%20short%0Areview%20of%20traditional%20regularization%20strategies.%20We%20then%20explore%20how%20proximal%0Asplitting%20algorithms%2C%20such%20as%20the%20alternating%20direction%20method%20of%20multipliers%0A%28ADMM%29%20and%20proximal%20gradient%20descent%20%28PGD%29%2C%20can%20naturally%20accommodate%20learned%0Adenoisers%20in%20place%20of%20proximal%20operators%2C%20and%20under%20what%20conditions%20such%0Areplacements%20preserve%20convergence.%20The%20role%20of%20Tweedie%27s%20formula%20in%20connecting%0Aoptimal%20Gaussian%20denoisers%20and%20score%20estimation%20is%20discussed%2C%20which%20lays%20the%0Afoundation%20for%20regularization-by-denoising%20%28RED%29%20and%20more%20recent%0Adiffusion-based%20posterior%20sampling%20methods.%20We%20discuss%20theoretical%20advances%0Aregarding%20the%20convergence%20of%20PnP%20algorithms%2C%20both%20within%20the%20RED%20and%20proximal%0Asettings%2C%20emphasizing%20the%20structural%20assumptions%20that%20the%20denoiser%20must%20satisfy%0Afor%20convergence%2C%20such%20as%20non-expansiveness%2C%20Lipschitz%20continuity%2C%20and%20local%0Ahomogeneity.%20We%20also%20address%20practical%20considerations%20in%20algorithm%20design%2C%0Aincluding%20choices%20of%20denoiser%20architecture%20and%20acceleration%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03475v1&entry.124074799=Read"},
{"title": "Mitigating Hallucination in Large Vision-Language Models through\n  Aligning Attention Distribution to Information Flow", "author": "Jianfei Zhao and Feng Zhang and Xin Sun and Chong Feng", "abstract": "  Due to the unidirectional masking mechanism, Decoder-Only models propagate\ninformation from left to right. LVLMs (Large Vision-Language Models) follow the\nsame architecture, with visual information gradually integrated into semantic\nrepresentations during forward propagation. Through systematic analysis, we\nobserve that the majority of the visual information is absorbed into the\nsemantic representations. However, the model's attention distribution does not\nexhibit sufficient emphasis on semantic representations. This misalignment\nbetween the attention distribution and the actual information flow undermines\nthe model's visual understanding ability and contributes to hallucinations. To\naddress this issue, we enhance the model's visual understanding by leveraging\nthe core information embedded in semantic representations. Specifically, we\nidentify attention heads that focus on core semantic representations based on\ntheir attention distributions. Then, through a two-stage optimization paradigm,\nwe propagate the advantages of these attention heads across the entire model,\naligning the attention distribution with the actual information flow. We\nevaluate our method on three image captioning benchmarks using five different\nLVLMs, demonstrating its effectiveness in significantly reducing\nhallucinations. Further experiments reveal a trade-off between reduced\nhallucinations and richer details. Notably, our method allows for manual\nadjustment of the model's conservativeness, enabling flexible control to meet\ndiverse real-world requirements.\n", "link": "http://arxiv.org/abs/2505.14257v2", "date": "2025-09-03", "relevancy": 2.2114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20through%0A%20%20Aligning%20Attention%20Distribution%20to%20Information%20Flow&body=Title%3A%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20through%0A%20%20Aligning%20Attention%20Distribution%20to%20Information%20Flow%0AAuthor%3A%20Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Chong%20Feng%0AAbstract%3A%20%20%20Due%20to%20the%20unidirectional%20masking%20mechanism%2C%20Decoder-Only%20models%20propagate%0Ainformation%20from%20left%20to%20right.%20LVLMs%20%28Large%20Vision-Language%20Models%29%20follow%20the%0Asame%20architecture%2C%20with%20visual%20information%20gradually%20integrated%20into%20semantic%0Arepresentations%20during%20forward%20propagation.%20Through%20systematic%20analysis%2C%20we%0Aobserve%20that%20the%20majority%20of%20the%20visual%20information%20is%20absorbed%20into%20the%0Asemantic%20representations.%20However%2C%20the%20model%27s%20attention%20distribution%20does%20not%0Aexhibit%20sufficient%20emphasis%20on%20semantic%20representations.%20This%20misalignment%0Abetween%20the%20attention%20distribution%20and%20the%20actual%20information%20flow%20undermines%0Athe%20model%27s%20visual%20understanding%20ability%20and%20contributes%20to%20hallucinations.%20To%0Aaddress%20this%20issue%2C%20we%20enhance%20the%20model%27s%20visual%20understanding%20by%20leveraging%0Athe%20core%20information%20embedded%20in%20semantic%20representations.%20Specifically%2C%20we%0Aidentify%20attention%20heads%20that%20focus%20on%20core%20semantic%20representations%20based%20on%0Atheir%20attention%20distributions.%20Then%2C%20through%20a%20two-stage%20optimization%20paradigm%2C%0Awe%20propagate%20the%20advantages%20of%20these%20attention%20heads%20across%20the%20entire%20model%2C%0Aaligning%20the%20attention%20distribution%20with%20the%20actual%20information%20flow.%20We%0Aevaluate%20our%20method%20on%20three%20image%20captioning%20benchmarks%20using%20five%20different%0ALVLMs%2C%20demonstrating%20its%20effectiveness%20in%20significantly%20reducing%0Ahallucinations.%20Further%20experiments%20reveal%20a%20trade-off%20between%20reduced%0Ahallucinations%20and%20richer%20details.%20Notably%2C%20our%20method%20allows%20for%20manual%0Aadjustment%20of%20the%20model%27s%20conservativeness%2C%20enabling%20flexible%20control%20to%20meet%0Adiverse%20real-world%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucination%2520in%2520Large%2520Vision-Language%2520Models%2520through%250A%2520%2520Aligning%2520Attention%2520Distribution%2520to%2520Information%2520Flow%26entry.906535625%3DJianfei%2520Zhao%2520and%2520Feng%2520Zhang%2520and%2520Xin%2520Sun%2520and%2520Chong%2520Feng%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520unidirectional%2520masking%2520mechanism%252C%2520Decoder-Only%2520models%2520propagate%250Ainformation%2520from%2520left%2520to%2520right.%2520LVLMs%2520%2528Large%2520Vision-Language%2520Models%2529%2520follow%2520the%250Asame%2520architecture%252C%2520with%2520visual%2520information%2520gradually%2520integrated%2520into%2520semantic%250Arepresentations%2520during%2520forward%2520propagation.%2520Through%2520systematic%2520analysis%252C%2520we%250Aobserve%2520that%2520the%2520majority%2520of%2520the%2520visual%2520information%2520is%2520absorbed%2520into%2520the%250Asemantic%2520representations.%2520However%252C%2520the%2520model%2527s%2520attention%2520distribution%2520does%2520not%250Aexhibit%2520sufficient%2520emphasis%2520on%2520semantic%2520representations.%2520This%2520misalignment%250Abetween%2520the%2520attention%2520distribution%2520and%2520the%2520actual%2520information%2520flow%2520undermines%250Athe%2520model%2527s%2520visual%2520understanding%2520ability%2520and%2520contributes%2520to%2520hallucinations.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520enhance%2520the%2520model%2527s%2520visual%2520understanding%2520by%2520leveraging%250Athe%2520core%2520information%2520embedded%2520in%2520semantic%2520representations.%2520Specifically%252C%2520we%250Aidentify%2520attention%2520heads%2520that%2520focus%2520on%2520core%2520semantic%2520representations%2520based%2520on%250Atheir%2520attention%2520distributions.%2520Then%252C%2520through%2520a%2520two-stage%2520optimization%2520paradigm%252C%250Awe%2520propagate%2520the%2520advantages%2520of%2520these%2520attention%2520heads%2520across%2520the%2520entire%2520model%252C%250Aaligning%2520the%2520attention%2520distribution%2520with%2520the%2520actual%2520information%2520flow.%2520We%250Aevaluate%2520our%2520method%2520on%2520three%2520image%2520captioning%2520benchmarks%2520using%2520five%2520different%250ALVLMs%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520significantly%2520reducing%250Ahallucinations.%2520Further%2520experiments%2520reveal%2520a%2520trade-off%2520between%2520reduced%250Ahallucinations%2520and%2520richer%2520details.%2520Notably%252C%2520our%2520method%2520allows%2520for%2520manual%250Aadjustment%2520of%2520the%2520model%2527s%2520conservativeness%252C%2520enabling%2520flexible%2520control%2520to%2520meet%250Adiverse%2520real-world%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20through%0A%20%20Aligning%20Attention%20Distribution%20to%20Information%20Flow&entry.906535625=Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Chong%20Feng&entry.1292438233=%20%20Due%20to%20the%20unidirectional%20masking%20mechanism%2C%20Decoder-Only%20models%20propagate%0Ainformation%20from%20left%20to%20right.%20LVLMs%20%28Large%20Vision-Language%20Models%29%20follow%20the%0Asame%20architecture%2C%20with%20visual%20information%20gradually%20integrated%20into%20semantic%0Arepresentations%20during%20forward%20propagation.%20Through%20systematic%20analysis%2C%20we%0Aobserve%20that%20the%20majority%20of%20the%20visual%20information%20is%20absorbed%20into%20the%0Asemantic%20representations.%20However%2C%20the%20model%27s%20attention%20distribution%20does%20not%0Aexhibit%20sufficient%20emphasis%20on%20semantic%20representations.%20This%20misalignment%0Abetween%20the%20attention%20distribution%20and%20the%20actual%20information%20flow%20undermines%0Athe%20model%27s%20visual%20understanding%20ability%20and%20contributes%20to%20hallucinations.%20To%0Aaddress%20this%20issue%2C%20we%20enhance%20the%20model%27s%20visual%20understanding%20by%20leveraging%0Athe%20core%20information%20embedded%20in%20semantic%20representations.%20Specifically%2C%20we%0Aidentify%20attention%20heads%20that%20focus%20on%20core%20semantic%20representations%20based%20on%0Atheir%20attention%20distributions.%20Then%2C%20through%20a%20two-stage%20optimization%20paradigm%2C%0Awe%20propagate%20the%20advantages%20of%20these%20attention%20heads%20across%20the%20entire%20model%2C%0Aaligning%20the%20attention%20distribution%20with%20the%20actual%20information%20flow.%20We%0Aevaluate%20our%20method%20on%20three%20image%20captioning%20benchmarks%20using%20five%20different%0ALVLMs%2C%20demonstrating%20its%20effectiveness%20in%20significantly%20reducing%0Ahallucinations.%20Further%20experiments%20reveal%20a%20trade-off%20between%20reduced%0Ahallucinations%20and%20richer%20details.%20Notably%2C%20our%20method%20allows%20for%20manual%0Aadjustment%20of%20the%20model%27s%20conservativeness%2C%20enabling%20flexible%20control%20to%20meet%0Adiverse%20real-world%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14257v2&entry.124074799=Read"},
{"title": "Predict, Cluster, Refine: A Joint Embedding Predictive Self-Supervised\n  Framework for Graph Representation Learning", "author": "Srinitish Srinivasan and Omkumar CU", "abstract": "  Graph representation learning has emerged as a cornerstone for tasks like\nnode classification and link prediction, yet prevailing self-supervised\nlearning (SSL) methods face challenges such as computational inefficiency,\nreliance on contrastive objectives, and representation collapse. Existing\napproaches often depend on feature reconstruction, negative sampling, or\ncomplex decoders, which introduce training overhead and hinder generalization.\nFurther, current techniques which address such limitations fail to account for\nthe contribution of node embeddings to a certain prediction in the absence of\nlabeled nodes. To address these limitations, we propose a novel joint embedding\npredictive framework for graph SSL that eliminates contrastive objectives and\nnegative sampling while preserving semantic and structural information.\nAdditionally, we introduce a semantic-aware objective term that incorporates\npseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node\ndiscriminability by evaluating latent feature contributions. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art graph\nSSL methods across benchmarks, achieving superior performance without\ncontrastive loss or complex decoders. Key innovations include (1) a\nnon-contrastive, view-invariant joint embedding predictive architecture, (2)\nLeveraging single context and multiple targets relationship between subgraphs,\nand (3) GMM-based pseudo-label scoring to capture semantic contributions. This\nwork advances graph SSL by offering a computationally efficient,\ncollapse-resistant paradigm that bridges spatial and semantic graph features\nfor downstream tasks. The code for our paper can be found at\nhttps://github.com/Deceptrax123/JPEB-GSSL\n", "link": "http://arxiv.org/abs/2502.01684v4", "date": "2025-09-03", "relevancy": 2.2019, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5972}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5206}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predict%2C%20Cluster%2C%20Refine%3A%20A%20Joint%20Embedding%20Predictive%20Self-Supervised%0A%20%20Framework%20for%20Graph%20Representation%20Learning&body=Title%3A%20Predict%2C%20Cluster%2C%20Refine%3A%20A%20Joint%20Embedding%20Predictive%20Self-Supervised%0A%20%20Framework%20for%20Graph%20Representation%20Learning%0AAuthor%3A%20Srinitish%20Srinivasan%20and%20Omkumar%20CU%0AAbstract%3A%20%20%20Graph%20representation%20learning%20has%20emerged%20as%20a%20cornerstone%20for%20tasks%20like%0Anode%20classification%20and%20link%20prediction%2C%20yet%20prevailing%20self-supervised%0Alearning%20%28SSL%29%20methods%20face%20challenges%20such%20as%20computational%20inefficiency%2C%0Areliance%20on%20contrastive%20objectives%2C%20and%20representation%20collapse.%20Existing%0Aapproaches%20often%20depend%20on%20feature%20reconstruction%2C%20negative%20sampling%2C%20or%0Acomplex%20decoders%2C%20which%20introduce%20training%20overhead%20and%20hinder%20generalization.%0AFurther%2C%20current%20techniques%20which%20address%20such%20limitations%20fail%20to%20account%20for%0Athe%20contribution%20of%20node%20embeddings%20to%20a%20certain%20prediction%20in%20the%20absence%20of%0Alabeled%20nodes.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20joint%20embedding%0Apredictive%20framework%20for%20graph%20SSL%20that%20eliminates%20contrastive%20objectives%20and%0Anegative%20sampling%20while%20preserving%20semantic%20and%20structural%20information.%0AAdditionally%2C%20we%20introduce%20a%20semantic-aware%20objective%20term%20that%20incorporates%0Apseudo-labels%20derived%20from%20Gaussian%20Mixture%20Models%20%28GMMs%29%2C%20enhancing%20node%0Adiscriminability%20by%20evaluating%20latent%20feature%20contributions.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20graph%0ASSL%20methods%20across%20benchmarks%2C%20achieving%20superior%20performance%20without%0Acontrastive%20loss%20or%20complex%20decoders.%20Key%20innovations%20include%20%281%29%20a%0Anon-contrastive%2C%20view-invariant%20joint%20embedding%20predictive%20architecture%2C%20%282%29%0ALeveraging%20single%20context%20and%20multiple%20targets%20relationship%20between%20subgraphs%2C%0Aand%20%283%29%20GMM-based%20pseudo-label%20scoring%20to%20capture%20semantic%20contributions.%20This%0Awork%20advances%20graph%20SSL%20by%20offering%20a%20computationally%20efficient%2C%0Acollapse-resistant%20paradigm%20that%20bridges%20spatial%20and%20semantic%20graph%20features%0Afor%20downstream%20tasks.%20The%20code%20for%20our%20paper%20can%20be%20found%20at%0Ahttps%3A//github.com/Deceptrax123/JPEB-GSSL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01684v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredict%252C%2520Cluster%252C%2520Refine%253A%2520A%2520Joint%2520Embedding%2520Predictive%2520Self-Supervised%250A%2520%2520Framework%2520for%2520Graph%2520Representation%2520Learning%26entry.906535625%3DSrinitish%2520Srinivasan%2520and%2520Omkumar%2520CU%26entry.1292438233%3D%2520%2520Graph%2520representation%2520learning%2520has%2520emerged%2520as%2520a%2520cornerstone%2520for%2520tasks%2520like%250Anode%2520classification%2520and%2520link%2520prediction%252C%2520yet%2520prevailing%2520self-supervised%250Alearning%2520%2528SSL%2529%2520methods%2520face%2520challenges%2520such%2520as%2520computational%2520inefficiency%252C%250Areliance%2520on%2520contrastive%2520objectives%252C%2520and%2520representation%2520collapse.%2520Existing%250Aapproaches%2520often%2520depend%2520on%2520feature%2520reconstruction%252C%2520negative%2520sampling%252C%2520or%250Acomplex%2520decoders%252C%2520which%2520introduce%2520training%2520overhead%2520and%2520hinder%2520generalization.%250AFurther%252C%2520current%2520techniques%2520which%2520address%2520such%2520limitations%2520fail%2520to%2520account%2520for%250Athe%2520contribution%2520of%2520node%2520embeddings%2520to%2520a%2520certain%2520prediction%2520in%2520the%2520absence%2520of%250Alabeled%2520nodes.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520joint%2520embedding%250Apredictive%2520framework%2520for%2520graph%2520SSL%2520that%2520eliminates%2520contrastive%2520objectives%2520and%250Anegative%2520sampling%2520while%2520preserving%2520semantic%2520and%2520structural%2520information.%250AAdditionally%252C%2520we%2520introduce%2520a%2520semantic-aware%2520objective%2520term%2520that%2520incorporates%250Apseudo-labels%2520derived%2520from%2520Gaussian%2520Mixture%2520Models%2520%2528GMMs%2529%252C%2520enhancing%2520node%250Adiscriminability%2520by%2520evaluating%2520latent%2520feature%2520contributions.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520state-of-the-art%2520graph%250ASSL%2520methods%2520across%2520benchmarks%252C%2520achieving%2520superior%2520performance%2520without%250Acontrastive%2520loss%2520or%2520complex%2520decoders.%2520Key%2520innovations%2520include%2520%25281%2529%2520a%250Anon-contrastive%252C%2520view-invariant%2520joint%2520embedding%2520predictive%2520architecture%252C%2520%25282%2529%250ALeveraging%2520single%2520context%2520and%2520multiple%2520targets%2520relationship%2520between%2520subgraphs%252C%250Aand%2520%25283%2529%2520GMM-based%2520pseudo-label%2520scoring%2520to%2520capture%2520semantic%2520contributions.%2520This%250Awork%2520advances%2520graph%2520SSL%2520by%2520offering%2520a%2520computationally%2520efficient%252C%250Acollapse-resistant%2520paradigm%2520that%2520bridges%2520spatial%2520and%2520semantic%2520graph%2520features%250Afor%2520downstream%2520tasks.%2520The%2520code%2520for%2520our%2520paper%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Deceptrax123/JPEB-GSSL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01684v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predict%2C%20Cluster%2C%20Refine%3A%20A%20Joint%20Embedding%20Predictive%20Self-Supervised%0A%20%20Framework%20for%20Graph%20Representation%20Learning&entry.906535625=Srinitish%20Srinivasan%20and%20Omkumar%20CU&entry.1292438233=%20%20Graph%20representation%20learning%20has%20emerged%20as%20a%20cornerstone%20for%20tasks%20like%0Anode%20classification%20and%20link%20prediction%2C%20yet%20prevailing%20self-supervised%0Alearning%20%28SSL%29%20methods%20face%20challenges%20such%20as%20computational%20inefficiency%2C%0Areliance%20on%20contrastive%20objectives%2C%20and%20representation%20collapse.%20Existing%0Aapproaches%20often%20depend%20on%20feature%20reconstruction%2C%20negative%20sampling%2C%20or%0Acomplex%20decoders%2C%20which%20introduce%20training%20overhead%20and%20hinder%20generalization.%0AFurther%2C%20current%20techniques%20which%20address%20such%20limitations%20fail%20to%20account%20for%0Athe%20contribution%20of%20node%20embeddings%20to%20a%20certain%20prediction%20in%20the%20absence%20of%0Alabeled%20nodes.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20joint%20embedding%0Apredictive%20framework%20for%20graph%20SSL%20that%20eliminates%20contrastive%20objectives%20and%0Anegative%20sampling%20while%20preserving%20semantic%20and%20structural%20information.%0AAdditionally%2C%20we%20introduce%20a%20semantic-aware%20objective%20term%20that%20incorporates%0Apseudo-labels%20derived%20from%20Gaussian%20Mixture%20Models%20%28GMMs%29%2C%20enhancing%20node%0Adiscriminability%20by%20evaluating%20latent%20feature%20contributions.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20graph%0ASSL%20methods%20across%20benchmarks%2C%20achieving%20superior%20performance%20without%0Acontrastive%20loss%20or%20complex%20decoders.%20Key%20innovations%20include%20%281%29%20a%0Anon-contrastive%2C%20view-invariant%20joint%20embedding%20predictive%20architecture%2C%20%282%29%0ALeveraging%20single%20context%20and%20multiple%20targets%20relationship%20between%20subgraphs%2C%0Aand%20%283%29%20GMM-based%20pseudo-label%20scoring%20to%20capture%20semantic%20contributions.%20This%0Awork%20advances%20graph%20SSL%20by%20offering%20a%20computationally%20efficient%2C%0Acollapse-resistant%20paradigm%20that%20bridges%20spatial%20and%20semantic%20graph%20features%0Afor%20downstream%20tasks.%20The%20code%20for%20our%20paper%20can%20be%20found%20at%0Ahttps%3A//github.com/Deceptrax123/JPEB-GSSL%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01684v4&entry.124074799=Read"},
{"title": "Efficient Active Training for Deep LiDAR Odometry", "author": "Beibei Zhou and Zhiyuan Zhang and Zhenbo Song and Jianhui Guo and Hui Kong", "abstract": "  Robust and efficient deep LiDAR odometry models are crucial for accurate\nlocalization and 3D reconstruction, but typically require extensive and diverse\ntraining data to adapt to diverse environments, leading to inefficiencies. To\ntackle this, we introduce an active training framework designed to selectively\nextract training data from diverse environments, thereby reducing the training\nload and enhancing model generalization. Our framework is based on two key\nstrategies: Initial Training Set Selection (ITSS) and Active Incremental\nSelection (AIS). ITSS begins by breaking down motion sequences from general\nweather into nodes and edges for detailed trajectory analysis, prioritizing\ndiverse sequences to form a rich initial training dataset for training the base\nmodel. For complex sequences that are difficult to analyze, especially under\nchallenging snowy weather conditions, AIS uses scene reconstruction and\nprediction inconsistency to iteratively select training samples, refining the\nmodel to handle a wide range of real-world scenarios. Experiments across\ndatasets and weather conditions validate our approach's effectiveness. Notably,\nour method matches the performance of full-dataset training with just 52\\% of\nthe sequence volume, demonstrating the training efficiency and robustness of\nour active training paradigm. By optimizing the training process, our approach\nsets the stage for more agile and reliable LiDAR odometry systems, capable of\nnavigating diverse environmental conditions with greater precision.\n", "link": "http://arxiv.org/abs/2509.03211v1", "date": "2025-09-03", "relevancy": 2.1952, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5604}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5425}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Active%20Training%20for%20Deep%20LiDAR%20Odometry&body=Title%3A%20Efficient%20Active%20Training%20for%20Deep%20LiDAR%20Odometry%0AAuthor%3A%20Beibei%20Zhou%20and%20Zhiyuan%20Zhang%20and%20Zhenbo%20Song%20and%20Jianhui%20Guo%20and%20Hui%20Kong%0AAbstract%3A%20%20%20Robust%20and%20efficient%20deep%20LiDAR%20odometry%20models%20are%20crucial%20for%20accurate%0Alocalization%20and%203D%20reconstruction%2C%20but%20typically%20require%20extensive%20and%20diverse%0Atraining%20data%20to%20adapt%20to%20diverse%20environments%2C%20leading%20to%20inefficiencies.%20To%0Atackle%20this%2C%20we%20introduce%20an%20active%20training%20framework%20designed%20to%20selectively%0Aextract%20training%20data%20from%20diverse%20environments%2C%20thereby%20reducing%20the%20training%0Aload%20and%20enhancing%20model%20generalization.%20Our%20framework%20is%20based%20on%20two%20key%0Astrategies%3A%20Initial%20Training%20Set%20Selection%20%28ITSS%29%20and%20Active%20Incremental%0ASelection%20%28AIS%29.%20ITSS%20begins%20by%20breaking%20down%20motion%20sequences%20from%20general%0Aweather%20into%20nodes%20and%20edges%20for%20detailed%20trajectory%20analysis%2C%20prioritizing%0Adiverse%20sequences%20to%20form%20a%20rich%20initial%20training%20dataset%20for%20training%20the%20base%0Amodel.%20For%20complex%20sequences%20that%20are%20difficult%20to%20analyze%2C%20especially%20under%0Achallenging%20snowy%20weather%20conditions%2C%20AIS%20uses%20scene%20reconstruction%20and%0Aprediction%20inconsistency%20to%20iteratively%20select%20training%20samples%2C%20refining%20the%0Amodel%20to%20handle%20a%20wide%20range%20of%20real-world%20scenarios.%20Experiments%20across%0Adatasets%20and%20weather%20conditions%20validate%20our%20approach%27s%20effectiveness.%20Notably%2C%0Aour%20method%20matches%20the%20performance%20of%20full-dataset%20training%20with%20just%2052%5C%25%20of%0Athe%20sequence%20volume%2C%20demonstrating%20the%20training%20efficiency%20and%20robustness%20of%0Aour%20active%20training%20paradigm.%20By%20optimizing%20the%20training%20process%2C%20our%20approach%0Asets%20the%20stage%20for%20more%20agile%20and%20reliable%20LiDAR%20odometry%20systems%2C%20capable%20of%0Anavigating%20diverse%20environmental%20conditions%20with%20greater%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Active%2520Training%2520for%2520Deep%2520LiDAR%2520Odometry%26entry.906535625%3DBeibei%2520Zhou%2520and%2520Zhiyuan%2520Zhang%2520and%2520Zhenbo%2520Song%2520and%2520Jianhui%2520Guo%2520and%2520Hui%2520Kong%26entry.1292438233%3D%2520%2520Robust%2520and%2520efficient%2520deep%2520LiDAR%2520odometry%2520models%2520are%2520crucial%2520for%2520accurate%250Alocalization%2520and%25203D%2520reconstruction%252C%2520but%2520typically%2520require%2520extensive%2520and%2520diverse%250Atraining%2520data%2520to%2520adapt%2520to%2520diverse%2520environments%252C%2520leading%2520to%2520inefficiencies.%2520To%250Atackle%2520this%252C%2520we%2520introduce%2520an%2520active%2520training%2520framework%2520designed%2520to%2520selectively%250Aextract%2520training%2520data%2520from%2520diverse%2520environments%252C%2520thereby%2520reducing%2520the%2520training%250Aload%2520and%2520enhancing%2520model%2520generalization.%2520Our%2520framework%2520is%2520based%2520on%2520two%2520key%250Astrategies%253A%2520Initial%2520Training%2520Set%2520Selection%2520%2528ITSS%2529%2520and%2520Active%2520Incremental%250ASelection%2520%2528AIS%2529.%2520ITSS%2520begins%2520by%2520breaking%2520down%2520motion%2520sequences%2520from%2520general%250Aweather%2520into%2520nodes%2520and%2520edges%2520for%2520detailed%2520trajectory%2520analysis%252C%2520prioritizing%250Adiverse%2520sequences%2520to%2520form%2520a%2520rich%2520initial%2520training%2520dataset%2520for%2520training%2520the%2520base%250Amodel.%2520For%2520complex%2520sequences%2520that%2520are%2520difficult%2520to%2520analyze%252C%2520especially%2520under%250Achallenging%2520snowy%2520weather%2520conditions%252C%2520AIS%2520uses%2520scene%2520reconstruction%2520and%250Aprediction%2520inconsistency%2520to%2520iteratively%2520select%2520training%2520samples%252C%2520refining%2520the%250Amodel%2520to%2520handle%2520a%2520wide%2520range%2520of%2520real-world%2520scenarios.%2520Experiments%2520across%250Adatasets%2520and%2520weather%2520conditions%2520validate%2520our%2520approach%2527s%2520effectiveness.%2520Notably%252C%250Aour%2520method%2520matches%2520the%2520performance%2520of%2520full-dataset%2520training%2520with%2520just%252052%255C%2525%2520of%250Athe%2520sequence%2520volume%252C%2520demonstrating%2520the%2520training%2520efficiency%2520and%2520robustness%2520of%250Aour%2520active%2520training%2520paradigm.%2520By%2520optimizing%2520the%2520training%2520process%252C%2520our%2520approach%250Asets%2520the%2520stage%2520for%2520more%2520agile%2520and%2520reliable%2520LiDAR%2520odometry%2520systems%252C%2520capable%2520of%250Anavigating%2520diverse%2520environmental%2520conditions%2520with%2520greater%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Active%20Training%20for%20Deep%20LiDAR%20Odometry&entry.906535625=Beibei%20Zhou%20and%20Zhiyuan%20Zhang%20and%20Zhenbo%20Song%20and%20Jianhui%20Guo%20and%20Hui%20Kong&entry.1292438233=%20%20Robust%20and%20efficient%20deep%20LiDAR%20odometry%20models%20are%20crucial%20for%20accurate%0Alocalization%20and%203D%20reconstruction%2C%20but%20typically%20require%20extensive%20and%20diverse%0Atraining%20data%20to%20adapt%20to%20diverse%20environments%2C%20leading%20to%20inefficiencies.%20To%0Atackle%20this%2C%20we%20introduce%20an%20active%20training%20framework%20designed%20to%20selectively%0Aextract%20training%20data%20from%20diverse%20environments%2C%20thereby%20reducing%20the%20training%0Aload%20and%20enhancing%20model%20generalization.%20Our%20framework%20is%20based%20on%20two%20key%0Astrategies%3A%20Initial%20Training%20Set%20Selection%20%28ITSS%29%20and%20Active%20Incremental%0ASelection%20%28AIS%29.%20ITSS%20begins%20by%20breaking%20down%20motion%20sequences%20from%20general%0Aweather%20into%20nodes%20and%20edges%20for%20detailed%20trajectory%20analysis%2C%20prioritizing%0Adiverse%20sequences%20to%20form%20a%20rich%20initial%20training%20dataset%20for%20training%20the%20base%0Amodel.%20For%20complex%20sequences%20that%20are%20difficult%20to%20analyze%2C%20especially%20under%0Achallenging%20snowy%20weather%20conditions%2C%20AIS%20uses%20scene%20reconstruction%20and%0Aprediction%20inconsistency%20to%20iteratively%20select%20training%20samples%2C%20refining%20the%0Amodel%20to%20handle%20a%20wide%20range%20of%20real-world%20scenarios.%20Experiments%20across%0Adatasets%20and%20weather%20conditions%20validate%20our%20approach%27s%20effectiveness.%20Notably%2C%0Aour%20method%20matches%20the%20performance%20of%20full-dataset%20training%20with%20just%2052%5C%25%20of%0Athe%20sequence%20volume%2C%20demonstrating%20the%20training%20efficiency%20and%20robustness%20of%0Aour%20active%20training%20paradigm.%20By%20optimizing%20the%20training%20process%2C%20our%20approach%0Asets%20the%20stage%20for%20more%20agile%20and%20reliable%20LiDAR%20odometry%20systems%2C%20capable%20of%0Anavigating%20diverse%20environmental%20conditions%20with%20greater%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03211v1&entry.124074799=Read"},
{"title": "Deep Research Agents: A Systematic Examination And Roadmap", "author": "Yuxuan Huang and Yihang Chen and Haozheng Zhang and Kang Li and Huichi Zhou and Meng Fang and Linyi Yang and Xiaoguang Li and Lifeng Shang and Songcen Xu and Jianye Hao and Kun Shao and Jun Wang", "abstract": "  The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.\n", "link": "http://arxiv.org/abs/2506.18096v2", "date": "2025-09-03", "relevancy": 2.1736, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Research%20Agents%3A%20A%20Systematic%20Examination%20And%20Roadmap&body=Title%3A%20Deep%20Research%20Agents%3A%20A%20Systematic%20Examination%20And%20Roadmap%0AAuthor%3A%20Yuxuan%20Huang%20and%20Yihang%20Chen%20and%20Haozheng%20Zhang%20and%20Kang%20Li%20and%20Huichi%20Zhou%20and%20Meng%20Fang%20and%20Linyi%20Yang%20and%20Xiaoguang%20Li%20and%20Lifeng%20Shang%20and%20Songcen%20Xu%20and%20Jianye%20Hao%20and%20Kun%20Shao%20and%20Jun%20Wang%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20given%20rise%20to%20a%20new%0Acategory%20of%20autonomous%20AI%20systems%2C%20referred%20to%20as%20Deep%20Research%20%28DR%29%20agents.%0AThese%20agents%20are%20designed%20to%20tackle%20complex%2C%20multi-turn%20informational%20research%0Atasks%20by%20leveraging%20a%20combination%20of%20dynamic%20reasoning%2C%20adaptive%20long-horizon%0Aplanning%2C%20multi-hop%20information%20retrieval%2C%20iterative%20tool%20use%2C%20and%20the%0Ageneration%20of%20structured%20analytical%20reports.%20In%20this%20paper%2C%20we%20conduct%20a%0Adetailed%20analysis%20of%20the%20foundational%20technologies%20and%20architectural%20components%0Athat%20constitute%20Deep%20Research%20agents.%20We%20begin%20by%20reviewing%20information%0Aacquisition%20strategies%2C%20contrasting%20API-based%20retrieval%20methods%20with%0Abrowser-based%20exploration.%20We%20then%20examine%20modular%20tool-use%20frameworks%2C%0Aincluding%20code%20execution%2C%20multimodal%20input%20processing%2C%20and%20the%20integration%20of%0AModel%20Context%20Protocols%20%28MCPs%29%20to%20support%20extensibility%20and%20ecosystem%0Adevelopment.%20To%20systematize%20existing%20approaches%2C%20we%20propose%20a%20taxonomy%20that%0Adifferentiates%20between%20static%20and%20dynamic%20workflows%2C%20and%20we%20classify%20agent%0Aarchitectures%20based%20on%20planning%20strategies%20and%20agent%20composition%2C%20including%0Asingle-agent%20and%20multi-agent%20configurations.%20We%20also%20provide%20a%20critical%0Aevaluation%20of%20current%20benchmarks%2C%20highlighting%20key%20limitations%20such%20as%0Arestricted%20access%20to%20external%20knowledge%2C%20sequential%20execution%20inefficiencies%2C%0Aand%20misalignment%20between%20evaluation%20metrics%20and%20the%20practical%20objectives%20of%20DR%0Aagents.%20Finally%2C%20we%20outline%20open%20challenges%20and%20promising%20directions%20for%20future%0Aresearch.%20A%20curated%20and%20continuously%20updated%20repository%20of%20DR%20agent%20research%20is%0Aavailable%20at%3A%20%7Bhttps%3A//github.com/ai-agents-2030/awesome-deep-research-agent%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Research%2520Agents%253A%2520A%2520Systematic%2520Examination%2520And%2520Roadmap%26entry.906535625%3DYuxuan%2520Huang%2520and%2520Yihang%2520Chen%2520and%2520Haozheng%2520Zhang%2520and%2520Kang%2520Li%2520and%2520Huichi%2520Zhou%2520and%2520Meng%2520Fang%2520and%2520Linyi%2520Yang%2520and%2520Xiaoguang%2520Li%2520and%2520Lifeng%2520Shang%2520and%2520Songcen%2520Xu%2520and%2520Jianye%2520Hao%2520and%2520Kun%2520Shao%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520given%2520rise%2520to%2520a%2520new%250Acategory%2520of%2520autonomous%2520AI%2520systems%252C%2520referred%2520to%2520as%2520Deep%2520Research%2520%2528DR%2529%2520agents.%250AThese%2520agents%2520are%2520designed%2520to%2520tackle%2520complex%252C%2520multi-turn%2520informational%2520research%250Atasks%2520by%2520leveraging%2520a%2520combination%2520of%2520dynamic%2520reasoning%252C%2520adaptive%2520long-horizon%250Aplanning%252C%2520multi-hop%2520information%2520retrieval%252C%2520iterative%2520tool%2520use%252C%2520and%2520the%250Ageneration%2520of%2520structured%2520analytical%2520reports.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%250Adetailed%2520analysis%2520of%2520the%2520foundational%2520technologies%2520and%2520architectural%2520components%250Athat%2520constitute%2520Deep%2520Research%2520agents.%2520We%2520begin%2520by%2520reviewing%2520information%250Aacquisition%2520strategies%252C%2520contrasting%2520API-based%2520retrieval%2520methods%2520with%250Abrowser-based%2520exploration.%2520We%2520then%2520examine%2520modular%2520tool-use%2520frameworks%252C%250Aincluding%2520code%2520execution%252C%2520multimodal%2520input%2520processing%252C%2520and%2520the%2520integration%2520of%250AModel%2520Context%2520Protocols%2520%2528MCPs%2529%2520to%2520support%2520extensibility%2520and%2520ecosystem%250Adevelopment.%2520To%2520systematize%2520existing%2520approaches%252C%2520we%2520propose%2520a%2520taxonomy%2520that%250Adifferentiates%2520between%2520static%2520and%2520dynamic%2520workflows%252C%2520and%2520we%2520classify%2520agent%250Aarchitectures%2520based%2520on%2520planning%2520strategies%2520and%2520agent%2520composition%252C%2520including%250Asingle-agent%2520and%2520multi-agent%2520configurations.%2520We%2520also%2520provide%2520a%2520critical%250Aevaluation%2520of%2520current%2520benchmarks%252C%2520highlighting%2520key%2520limitations%2520such%2520as%250Arestricted%2520access%2520to%2520external%2520knowledge%252C%2520sequential%2520execution%2520inefficiencies%252C%250Aand%2520misalignment%2520between%2520evaluation%2520metrics%2520and%2520the%2520practical%2520objectives%2520of%2520DR%250Aagents.%2520Finally%252C%2520we%2520outline%2520open%2520challenges%2520and%2520promising%2520directions%2520for%2520future%250Aresearch.%2520A%2520curated%2520and%2520continuously%2520updated%2520repository%2520of%2520DR%2520agent%2520research%2520is%250Aavailable%2520at%253A%2520%257Bhttps%253A//github.com/ai-agents-2030/awesome-deep-research-agent%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Research%20Agents%3A%20A%20Systematic%20Examination%20And%20Roadmap&entry.906535625=Yuxuan%20Huang%20and%20Yihang%20Chen%20and%20Haozheng%20Zhang%20and%20Kang%20Li%20and%20Huichi%20Zhou%20and%20Meng%20Fang%20and%20Linyi%20Yang%20and%20Xiaoguang%20Li%20and%20Lifeng%20Shang%20and%20Songcen%20Xu%20and%20Jianye%20Hao%20and%20Kun%20Shao%20and%20Jun%20Wang&entry.1292438233=%20%20The%20rapid%20progress%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20given%20rise%20to%20a%20new%0Acategory%20of%20autonomous%20AI%20systems%2C%20referred%20to%20as%20Deep%20Research%20%28DR%29%20agents.%0AThese%20agents%20are%20designed%20to%20tackle%20complex%2C%20multi-turn%20informational%20research%0Atasks%20by%20leveraging%20a%20combination%20of%20dynamic%20reasoning%2C%20adaptive%20long-horizon%0Aplanning%2C%20multi-hop%20information%20retrieval%2C%20iterative%20tool%20use%2C%20and%20the%0Ageneration%20of%20structured%20analytical%20reports.%20In%20this%20paper%2C%20we%20conduct%20a%0Adetailed%20analysis%20of%20the%20foundational%20technologies%20and%20architectural%20components%0Athat%20constitute%20Deep%20Research%20agents.%20We%20begin%20by%20reviewing%20information%0Aacquisition%20strategies%2C%20contrasting%20API-based%20retrieval%20methods%20with%0Abrowser-based%20exploration.%20We%20then%20examine%20modular%20tool-use%20frameworks%2C%0Aincluding%20code%20execution%2C%20multimodal%20input%20processing%2C%20and%20the%20integration%20of%0AModel%20Context%20Protocols%20%28MCPs%29%20to%20support%20extensibility%20and%20ecosystem%0Adevelopment.%20To%20systematize%20existing%20approaches%2C%20we%20propose%20a%20taxonomy%20that%0Adifferentiates%20between%20static%20and%20dynamic%20workflows%2C%20and%20we%20classify%20agent%0Aarchitectures%20based%20on%20planning%20strategies%20and%20agent%20composition%2C%20including%0Asingle-agent%20and%20multi-agent%20configurations.%20We%20also%20provide%20a%20critical%0Aevaluation%20of%20current%20benchmarks%2C%20highlighting%20key%20limitations%20such%20as%0Arestricted%20access%20to%20external%20knowledge%2C%20sequential%20execution%20inefficiencies%2C%0Aand%20misalignment%20between%20evaluation%20metrics%20and%20the%20practical%20objectives%20of%20DR%0Aagents.%20Finally%2C%20we%20outline%20open%20challenges%20and%20promising%20directions%20for%20future%0Aresearch.%20A%20curated%20and%20continuously%20updated%20repository%20of%20DR%20agent%20research%20is%0Aavailable%20at%3A%20%7Bhttps%3A//github.com/ai-agents-2030/awesome-deep-research-agent%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18096v2&entry.124074799=Read"},
{"title": "PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a\n  Geometry-Aware 3DETR", "author": "Fabio F. Oberweger and Michael Schwingshackl and Vanessa Staderini", "abstract": "  We present PI3DETR, an end-to-end framework that directly predicts 3D\nparametric curve instances from raw point clouds, avoiding the intermediate\nrepresentations and multi-stage processing common in prior work. Extending\n3DETR, our model introduces a geometry-aware matching strategy and specialized\nloss functions that enable unified detection of differently parameterized curve\ntypes, including cubic B\\'ezier curves, line segments, circles, and arcs, in a\nsingle forward pass. Optional post-processing steps further refine predictions\nwithout adding complexity. This streamlined design improves robustness to noise\nand varying sampling densities, addressing critical challenges in real world\nLiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC\ndataset and generalizes effectively to real sensor data, offering a simple yet\npowerful solution for 3D edge and curve estimation.\n", "link": "http://arxiv.org/abs/2509.03262v1", "date": "2025-09-03", "relevancy": 2.1732, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5609}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5316}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PI3DETR%3A%20Parametric%20Instance%20Detection%20of%203D%20Point%20Cloud%20Edges%20with%20a%0A%20%20Geometry-Aware%203DETR&body=Title%3A%20PI3DETR%3A%20Parametric%20Instance%20Detection%20of%203D%20Point%20Cloud%20Edges%20with%20a%0A%20%20Geometry-Aware%203DETR%0AAuthor%3A%20Fabio%20F.%20Oberweger%20and%20Michael%20Schwingshackl%20and%20Vanessa%20Staderini%0AAbstract%3A%20%20%20We%20present%20PI3DETR%2C%20an%20end-to-end%20framework%20that%20directly%20predicts%203D%0Aparametric%20curve%20instances%20from%20raw%20point%20clouds%2C%20avoiding%20the%20intermediate%0Arepresentations%20and%20multi-stage%20processing%20common%20in%20prior%20work.%20Extending%0A3DETR%2C%20our%20model%20introduces%20a%20geometry-aware%20matching%20strategy%20and%20specialized%0Aloss%20functions%20that%20enable%20unified%20detection%20of%20differently%20parameterized%20curve%0Atypes%2C%20including%20cubic%20B%5C%27ezier%20curves%2C%20line%20segments%2C%20circles%2C%20and%20arcs%2C%20in%20a%0Asingle%20forward%20pass.%20Optional%20post-processing%20steps%20further%20refine%20predictions%0Awithout%20adding%20complexity.%20This%20streamlined%20design%20improves%20robustness%20to%20noise%0Aand%20varying%20sampling%20densities%2C%20addressing%20critical%20challenges%20in%20real%20world%0ALiDAR%20and%203D%20sensing%20scenarios.%20PI3DETR%20sets%20a%20new%20state-of-the-art%20on%20the%20ABC%0Adataset%20and%20generalizes%20effectively%20to%20real%20sensor%20data%2C%20offering%20a%20simple%20yet%0Apowerful%20solution%20for%203D%20edge%20and%20curve%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPI3DETR%253A%2520Parametric%2520Instance%2520Detection%2520of%25203D%2520Point%2520Cloud%2520Edges%2520with%2520a%250A%2520%2520Geometry-Aware%25203DETR%26entry.906535625%3DFabio%2520F.%2520Oberweger%2520and%2520Michael%2520Schwingshackl%2520and%2520Vanessa%2520Staderini%26entry.1292438233%3D%2520%2520We%2520present%2520PI3DETR%252C%2520an%2520end-to-end%2520framework%2520that%2520directly%2520predicts%25203D%250Aparametric%2520curve%2520instances%2520from%2520raw%2520point%2520clouds%252C%2520avoiding%2520the%2520intermediate%250Arepresentations%2520and%2520multi-stage%2520processing%2520common%2520in%2520prior%2520work.%2520Extending%250A3DETR%252C%2520our%2520model%2520introduces%2520a%2520geometry-aware%2520matching%2520strategy%2520and%2520specialized%250Aloss%2520functions%2520that%2520enable%2520unified%2520detection%2520of%2520differently%2520parameterized%2520curve%250Atypes%252C%2520including%2520cubic%2520B%255C%2527ezier%2520curves%252C%2520line%2520segments%252C%2520circles%252C%2520and%2520arcs%252C%2520in%2520a%250Asingle%2520forward%2520pass.%2520Optional%2520post-processing%2520steps%2520further%2520refine%2520predictions%250Awithout%2520adding%2520complexity.%2520This%2520streamlined%2520design%2520improves%2520robustness%2520to%2520noise%250Aand%2520varying%2520sampling%2520densities%252C%2520addressing%2520critical%2520challenges%2520in%2520real%2520world%250ALiDAR%2520and%25203D%2520sensing%2520scenarios.%2520PI3DETR%2520sets%2520a%2520new%2520state-of-the-art%2520on%2520the%2520ABC%250Adataset%2520and%2520generalizes%2520effectively%2520to%2520real%2520sensor%2520data%252C%2520offering%2520a%2520simple%2520yet%250Apowerful%2520solution%2520for%25203D%2520edge%2520and%2520curve%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PI3DETR%3A%20Parametric%20Instance%20Detection%20of%203D%20Point%20Cloud%20Edges%20with%20a%0A%20%20Geometry-Aware%203DETR&entry.906535625=Fabio%20F.%20Oberweger%20and%20Michael%20Schwingshackl%20and%20Vanessa%20Staderini&entry.1292438233=%20%20We%20present%20PI3DETR%2C%20an%20end-to-end%20framework%20that%20directly%20predicts%203D%0Aparametric%20curve%20instances%20from%20raw%20point%20clouds%2C%20avoiding%20the%20intermediate%0Arepresentations%20and%20multi-stage%20processing%20common%20in%20prior%20work.%20Extending%0A3DETR%2C%20our%20model%20introduces%20a%20geometry-aware%20matching%20strategy%20and%20specialized%0Aloss%20functions%20that%20enable%20unified%20detection%20of%20differently%20parameterized%20curve%0Atypes%2C%20including%20cubic%20B%5C%27ezier%20curves%2C%20line%20segments%2C%20circles%2C%20and%20arcs%2C%20in%20a%0Asingle%20forward%20pass.%20Optional%20post-processing%20steps%20further%20refine%20predictions%0Awithout%20adding%20complexity.%20This%20streamlined%20design%20improves%20robustness%20to%20noise%0Aand%20varying%20sampling%20densities%2C%20addressing%20critical%20challenges%20in%20real%20world%0ALiDAR%20and%203D%20sensing%20scenarios.%20PI3DETR%20sets%20a%20new%20state-of-the-art%20on%20the%20ABC%0Adataset%20and%20generalizes%20effectively%20to%20real%20sensor%20data%2C%20offering%20a%20simple%20yet%0Apowerful%20solution%20for%203D%20edge%20and%20curve%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03262v1&entry.124074799=Read"},
{"title": "IndexTTS2: A Breakthrough in Emotionally Expressive and\n  Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "author": "Siyi Zhou and Yiquan Zhou and Yi He and Xun Zhou and Jinchao Wang and Wei Deng and Jingchen Shu", "abstract": "  Existing autoregressive large-scale text-to-speech (TTS) models have\nadvantages in speech naturalness, but their token-by-token generation mechanism\nmakes it difficult to precisely control the duration of synthesized speech.\nThis becomes a significant limitation in applications requiring strict\naudio-visual synchronization, such as video dubbing. This paper introduces\nIndexTTS2, which proposes a novel, general, and autoregressive model-friendly\nmethod for speech duration control. The method supports two generation modes:\none explicitly specifies the number of generated tokens to precisely control\nspeech duration; the other freely generates speech in an autoregressive manner\nwithout specifying the number of tokens, while faithfully reproducing the\nprosodic features of the input prompt. Furthermore, IndexTTS2 achieves\ndisentanglement between emotional expression and speaker identity, enabling\nindependent control over timbre and emotion. In the zero-shot setting, the\nmodel can accurately reconstruct the target timbre (from the timbre prompt)\nwhile perfectly reproducing the specified emotional tone (from the style\nprompt). To enhance speech clarity in highly emotional expressions, we\nincorporate GPT latent representations and design a novel three-stage training\nparadigm to improve the stability of the generated speech. Additionally, to\nlower the barrier for emotional control, we designed a soft instruction\nmechanism based on text descriptions by fine-tuning Qwen3, effectively guiding\nthe generation of speech with the desired emotional orientation. Finally,\nexperimental results on multiple datasets show that IndexTTS2 outperforms\nstate-of-the-art zero-shot TTS models in terms of word error rate, speaker\nsimilarity, and emotional fidelity. Audio samples are available at:\nhttps://index-tts.github.io/index-tts2.github.io/\n", "link": "http://arxiv.org/abs/2506.21619v2", "date": "2025-09-03", "relevancy": 2.168, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5571}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.543}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IndexTTS2%3A%20A%20Breakthrough%20in%20Emotionally%20Expressive%20and%0A%20%20Duration-Controlled%20Auto-Regressive%20Zero-Shot%20Text-to-Speech&body=Title%3A%20IndexTTS2%3A%20A%20Breakthrough%20in%20Emotionally%20Expressive%20and%0A%20%20Duration-Controlled%20Auto-Regressive%20Zero-Shot%20Text-to-Speech%0AAuthor%3A%20Siyi%20Zhou%20and%20Yiquan%20Zhou%20and%20Yi%20He%20and%20Xun%20Zhou%20and%20Jinchao%20Wang%20and%20Wei%20Deng%20and%20Jingchen%20Shu%0AAbstract%3A%20%20%20Existing%20autoregressive%20large-scale%20text-to-speech%20%28TTS%29%20models%20have%0Aadvantages%20in%20speech%20naturalness%2C%20but%20their%20token-by-token%20generation%20mechanism%0Amakes%20it%20difficult%20to%20precisely%20control%20the%20duration%20of%20synthesized%20speech.%0AThis%20becomes%20a%20significant%20limitation%20in%20applications%20requiring%20strict%0Aaudio-visual%20synchronization%2C%20such%20as%20video%20dubbing.%20This%20paper%20introduces%0AIndexTTS2%2C%20which%20proposes%20a%20novel%2C%20general%2C%20and%20autoregressive%20model-friendly%0Amethod%20for%20speech%20duration%20control.%20The%20method%20supports%20two%20generation%20modes%3A%0Aone%20explicitly%20specifies%20the%20number%20of%20generated%20tokens%20to%20precisely%20control%0Aspeech%20duration%3B%20the%20other%20freely%20generates%20speech%20in%20an%20autoregressive%20manner%0Awithout%20specifying%20the%20number%20of%20tokens%2C%20while%20faithfully%20reproducing%20the%0Aprosodic%20features%20of%20the%20input%20prompt.%20Furthermore%2C%20IndexTTS2%20achieves%0Adisentanglement%20between%20emotional%20expression%20and%20speaker%20identity%2C%20enabling%0Aindependent%20control%20over%20timbre%20and%20emotion.%20In%20the%20zero-shot%20setting%2C%20the%0Amodel%20can%20accurately%20reconstruct%20the%20target%20timbre%20%28from%20the%20timbre%20prompt%29%0Awhile%20perfectly%20reproducing%20the%20specified%20emotional%20tone%20%28from%20the%20style%0Aprompt%29.%20To%20enhance%20speech%20clarity%20in%20highly%20emotional%20expressions%2C%20we%0Aincorporate%20GPT%20latent%20representations%20and%20design%20a%20novel%20three-stage%20training%0Aparadigm%20to%20improve%20the%20stability%20of%20the%20generated%20speech.%20Additionally%2C%20to%0Alower%20the%20barrier%20for%20emotional%20control%2C%20we%20designed%20a%20soft%20instruction%0Amechanism%20based%20on%20text%20descriptions%20by%20fine-tuning%20Qwen3%2C%20effectively%20guiding%0Athe%20generation%20of%20speech%20with%20the%20desired%20emotional%20orientation.%20Finally%2C%0Aexperimental%20results%20on%20multiple%20datasets%20show%20that%20IndexTTS2%20outperforms%0Astate-of-the-art%20zero-shot%20TTS%20models%20in%20terms%20of%20word%20error%20rate%2C%20speaker%0Asimilarity%2C%20and%20emotional%20fidelity.%20Audio%20samples%20are%20available%20at%3A%0Ahttps%3A//index-tts.github.io/index-tts2.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndexTTS2%253A%2520A%2520Breakthrough%2520in%2520Emotionally%2520Expressive%2520and%250A%2520%2520Duration-Controlled%2520Auto-Regressive%2520Zero-Shot%2520Text-to-Speech%26entry.906535625%3DSiyi%2520Zhou%2520and%2520Yiquan%2520Zhou%2520and%2520Yi%2520He%2520and%2520Xun%2520Zhou%2520and%2520Jinchao%2520Wang%2520and%2520Wei%2520Deng%2520and%2520Jingchen%2520Shu%26entry.1292438233%3D%2520%2520Existing%2520autoregressive%2520large-scale%2520text-to-speech%2520%2528TTS%2529%2520models%2520have%250Aadvantages%2520in%2520speech%2520naturalness%252C%2520but%2520their%2520token-by-token%2520generation%2520mechanism%250Amakes%2520it%2520difficult%2520to%2520precisely%2520control%2520the%2520duration%2520of%2520synthesized%2520speech.%250AThis%2520becomes%2520a%2520significant%2520limitation%2520in%2520applications%2520requiring%2520strict%250Aaudio-visual%2520synchronization%252C%2520such%2520as%2520video%2520dubbing.%2520This%2520paper%2520introduces%250AIndexTTS2%252C%2520which%2520proposes%2520a%2520novel%252C%2520general%252C%2520and%2520autoregressive%2520model-friendly%250Amethod%2520for%2520speech%2520duration%2520control.%2520The%2520method%2520supports%2520two%2520generation%2520modes%253A%250Aone%2520explicitly%2520specifies%2520the%2520number%2520of%2520generated%2520tokens%2520to%2520precisely%2520control%250Aspeech%2520duration%253B%2520the%2520other%2520freely%2520generates%2520speech%2520in%2520an%2520autoregressive%2520manner%250Awithout%2520specifying%2520the%2520number%2520of%2520tokens%252C%2520while%2520faithfully%2520reproducing%2520the%250Aprosodic%2520features%2520of%2520the%2520input%2520prompt.%2520Furthermore%252C%2520IndexTTS2%2520achieves%250Adisentanglement%2520between%2520emotional%2520expression%2520and%2520speaker%2520identity%252C%2520enabling%250Aindependent%2520control%2520over%2520timbre%2520and%2520emotion.%2520In%2520the%2520zero-shot%2520setting%252C%2520the%250Amodel%2520can%2520accurately%2520reconstruct%2520the%2520target%2520timbre%2520%2528from%2520the%2520timbre%2520prompt%2529%250Awhile%2520perfectly%2520reproducing%2520the%2520specified%2520emotional%2520tone%2520%2528from%2520the%2520style%250Aprompt%2529.%2520To%2520enhance%2520speech%2520clarity%2520in%2520highly%2520emotional%2520expressions%252C%2520we%250Aincorporate%2520GPT%2520latent%2520representations%2520and%2520design%2520a%2520novel%2520three-stage%2520training%250Aparadigm%2520to%2520improve%2520the%2520stability%2520of%2520the%2520generated%2520speech.%2520Additionally%252C%2520to%250Alower%2520the%2520barrier%2520for%2520emotional%2520control%252C%2520we%2520designed%2520a%2520soft%2520instruction%250Amechanism%2520based%2520on%2520text%2520descriptions%2520by%2520fine-tuning%2520Qwen3%252C%2520effectively%2520guiding%250Athe%2520generation%2520of%2520speech%2520with%2520the%2520desired%2520emotional%2520orientation.%2520Finally%252C%250Aexperimental%2520results%2520on%2520multiple%2520datasets%2520show%2520that%2520IndexTTS2%2520outperforms%250Astate-of-the-art%2520zero-shot%2520TTS%2520models%2520in%2520terms%2520of%2520word%2520error%2520rate%252C%2520speaker%250Asimilarity%252C%2520and%2520emotional%2520fidelity.%2520Audio%2520samples%2520are%2520available%2520at%253A%250Ahttps%253A//index-tts.github.io/index-tts2.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IndexTTS2%3A%20A%20Breakthrough%20in%20Emotionally%20Expressive%20and%0A%20%20Duration-Controlled%20Auto-Regressive%20Zero-Shot%20Text-to-Speech&entry.906535625=Siyi%20Zhou%20and%20Yiquan%20Zhou%20and%20Yi%20He%20and%20Xun%20Zhou%20and%20Jinchao%20Wang%20and%20Wei%20Deng%20and%20Jingchen%20Shu&entry.1292438233=%20%20Existing%20autoregressive%20large-scale%20text-to-speech%20%28TTS%29%20models%20have%0Aadvantages%20in%20speech%20naturalness%2C%20but%20their%20token-by-token%20generation%20mechanism%0Amakes%20it%20difficult%20to%20precisely%20control%20the%20duration%20of%20synthesized%20speech.%0AThis%20becomes%20a%20significant%20limitation%20in%20applications%20requiring%20strict%0Aaudio-visual%20synchronization%2C%20such%20as%20video%20dubbing.%20This%20paper%20introduces%0AIndexTTS2%2C%20which%20proposes%20a%20novel%2C%20general%2C%20and%20autoregressive%20model-friendly%0Amethod%20for%20speech%20duration%20control.%20The%20method%20supports%20two%20generation%20modes%3A%0Aone%20explicitly%20specifies%20the%20number%20of%20generated%20tokens%20to%20precisely%20control%0Aspeech%20duration%3B%20the%20other%20freely%20generates%20speech%20in%20an%20autoregressive%20manner%0Awithout%20specifying%20the%20number%20of%20tokens%2C%20while%20faithfully%20reproducing%20the%0Aprosodic%20features%20of%20the%20input%20prompt.%20Furthermore%2C%20IndexTTS2%20achieves%0Adisentanglement%20between%20emotional%20expression%20and%20speaker%20identity%2C%20enabling%0Aindependent%20control%20over%20timbre%20and%20emotion.%20In%20the%20zero-shot%20setting%2C%20the%0Amodel%20can%20accurately%20reconstruct%20the%20target%20timbre%20%28from%20the%20timbre%20prompt%29%0Awhile%20perfectly%20reproducing%20the%20specified%20emotional%20tone%20%28from%20the%20style%0Aprompt%29.%20To%20enhance%20speech%20clarity%20in%20highly%20emotional%20expressions%2C%20we%0Aincorporate%20GPT%20latent%20representations%20and%20design%20a%20novel%20three-stage%20training%0Aparadigm%20to%20improve%20the%20stability%20of%20the%20generated%20speech.%20Additionally%2C%20to%0Alower%20the%20barrier%20for%20emotional%20control%2C%20we%20designed%20a%20soft%20instruction%0Amechanism%20based%20on%20text%20descriptions%20by%20fine-tuning%20Qwen3%2C%20effectively%20guiding%0Athe%20generation%20of%20speech%20with%20the%20desired%20emotional%20orientation.%20Finally%2C%0Aexperimental%20results%20on%20multiple%20datasets%20show%20that%20IndexTTS2%20outperforms%0Astate-of-the-art%20zero-shot%20TTS%20models%20in%20terms%20of%20word%20error%20rate%2C%20speaker%0Asimilarity%2C%20and%20emotional%20fidelity.%20Audio%20samples%20are%20available%20at%3A%0Ahttps%3A//index-tts.github.io/index-tts2.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21619v2&entry.124074799=Read"},
{"title": "sam-llm: interpretable lane change trajectoryprediction via parametric\n  finetuning", "author": "Zhuo Cao and Yunxiao Shi and Min Xu", "abstract": "  This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency.\n", "link": "http://arxiv.org/abs/2509.03462v1", "date": "2025-09-03", "relevancy": 2.1564, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5429}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20sam-llm%3A%20interpretable%20lane%20change%20trajectoryprediction%20via%20parametric%0A%20%20finetuning&body=Title%3A%20sam-llm%3A%20interpretable%20lane%20change%20trajectoryprediction%20via%20parametric%0A%20%20finetuning%0AAuthor%3A%20Zhuo%20Cao%20and%20Yunxiao%20Shi%20and%20Min%20Xu%0AAbstract%3A%20%20%20This%20work%20introduces%20SAM-LLM%2C%20a%20novel%20hybrid%20architecture%20that%20bridges%20the%0Agap%20between%20the%20contextual%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20the%0Aphysical%20precision%20of%20kinematic%20lane%20change%20models%20for%20autonomous%20driving.%20The%0Asystem%20is%20designed%20for%20interpretable%20lane%20change%20trajectory%20prediction%20by%0Afinetuning%20an%20LLM%20to%20output%20the%20core%20physical%20parameters%20of%20a%20trajectory%20model%0Ainstead%20of%20raw%20coordinates.%20For%20lane-keeping%20scenarios%2C%20the%20model%20predicts%0Adiscrete%20coordinates%2C%20but%20for%20lane%20change%20maneuvers%2C%20it%20generates%20the%0Aparameters%20for%20an%20enhanced%20Sinusoidal%20Acceleration%20Model%20%28SAM%29%2C%20including%0Alateral%20displacement%2C%20maneuver%20duration%2C%20initial%20lateral%20velocity%2C%20and%0Alongitudinal%20velocity%20change.%20This%20parametric%20approach%20yields%20a%20complete%2C%0Acontinuous%2C%20and%20physically%20plausible%20trajectory%20model%20that%20is%20inherently%0Ainterpretable%20and%20computationally%20efficient%2C%20achieving%20an%2080%25%20reduction%20in%0Aoutput%20size%20compared%20to%20coordinate-based%20methods.%20The%20SAM-LLM%20achieves%20a%0Astate-of-the-art%20overall%20intention%20prediction%20accuracy%20of%2098.73%25%2C%20demonstrating%0Aperformance%20equivalent%20to%20traditional%20LLM%20predictors%20while%20offering%20significant%0Aadvantages%20in%20explainability%20and%20resource%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dsam-llm%253A%2520interpretable%2520lane%2520change%2520trajectoryprediction%2520via%2520parametric%250A%2520%2520finetuning%26entry.906535625%3DZhuo%2520Cao%2520and%2520Yunxiao%2520Shi%2520and%2520Min%2520Xu%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520SAM-LLM%252C%2520a%2520novel%2520hybrid%2520architecture%2520that%2520bridges%2520the%250Agap%2520between%2520the%2520contextual%2520reasoning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520the%250Aphysical%2520precision%2520of%2520kinematic%2520lane%2520change%2520models%2520for%2520autonomous%2520driving.%2520The%250Asystem%2520is%2520designed%2520for%2520interpretable%2520lane%2520change%2520trajectory%2520prediction%2520by%250Afinetuning%2520an%2520LLM%2520to%2520output%2520the%2520core%2520physical%2520parameters%2520of%2520a%2520trajectory%2520model%250Ainstead%2520of%2520raw%2520coordinates.%2520For%2520lane-keeping%2520scenarios%252C%2520the%2520model%2520predicts%250Adiscrete%2520coordinates%252C%2520but%2520for%2520lane%2520change%2520maneuvers%252C%2520it%2520generates%2520the%250Aparameters%2520for%2520an%2520enhanced%2520Sinusoidal%2520Acceleration%2520Model%2520%2528SAM%2529%252C%2520including%250Alateral%2520displacement%252C%2520maneuver%2520duration%252C%2520initial%2520lateral%2520velocity%252C%2520and%250Alongitudinal%2520velocity%2520change.%2520This%2520parametric%2520approach%2520yields%2520a%2520complete%252C%250Acontinuous%252C%2520and%2520physically%2520plausible%2520trajectory%2520model%2520that%2520is%2520inherently%250Ainterpretable%2520and%2520computationally%2520efficient%252C%2520achieving%2520an%252080%2525%2520reduction%2520in%250Aoutput%2520size%2520compared%2520to%2520coordinate-based%2520methods.%2520The%2520SAM-LLM%2520achieves%2520a%250Astate-of-the-art%2520overall%2520intention%2520prediction%2520accuracy%2520of%252098.73%2525%252C%2520demonstrating%250Aperformance%2520equivalent%2520to%2520traditional%2520LLM%2520predictors%2520while%2520offering%2520significant%250Aadvantages%2520in%2520explainability%2520and%2520resource%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=sam-llm%3A%20interpretable%20lane%20change%20trajectoryprediction%20via%20parametric%0A%20%20finetuning&entry.906535625=Zhuo%20Cao%20and%20Yunxiao%20Shi%20and%20Min%20Xu&entry.1292438233=%20%20This%20work%20introduces%20SAM-LLM%2C%20a%20novel%20hybrid%20architecture%20that%20bridges%20the%0Agap%20between%20the%20contextual%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20the%0Aphysical%20precision%20of%20kinematic%20lane%20change%20models%20for%20autonomous%20driving.%20The%0Asystem%20is%20designed%20for%20interpretable%20lane%20change%20trajectory%20prediction%20by%0Afinetuning%20an%20LLM%20to%20output%20the%20core%20physical%20parameters%20of%20a%20trajectory%20model%0Ainstead%20of%20raw%20coordinates.%20For%20lane-keeping%20scenarios%2C%20the%20model%20predicts%0Adiscrete%20coordinates%2C%20but%20for%20lane%20change%20maneuvers%2C%20it%20generates%20the%0Aparameters%20for%20an%20enhanced%20Sinusoidal%20Acceleration%20Model%20%28SAM%29%2C%20including%0Alateral%20displacement%2C%20maneuver%20duration%2C%20initial%20lateral%20velocity%2C%20and%0Alongitudinal%20velocity%20change.%20This%20parametric%20approach%20yields%20a%20complete%2C%0Acontinuous%2C%20and%20physically%20plausible%20trajectory%20model%20that%20is%20inherently%0Ainterpretable%20and%20computationally%20efficient%2C%20achieving%20an%2080%25%20reduction%20in%0Aoutput%20size%20compared%20to%20coordinate-based%20methods.%20The%20SAM-LLM%20achieves%20a%0Astate-of-the-art%20overall%20intention%20prediction%20accuracy%20of%2098.73%25%2C%20demonstrating%0Aperformance%20equivalent%20to%20traditional%20LLM%20predictors%20while%20offering%20significant%0Aadvantages%20in%20explainability%20and%20resource%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03462v1&entry.124074799=Read"},
{"title": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive\n  and Abductive Reasoning", "author": "Yunxin Sun and Abulhair Saparov", "abstract": "  Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR.\n", "link": "http://arxiv.org/abs/2509.03345v1", "date": "2025-09-03", "relevancy": 2.1523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Do%20Not%20Follow%20Occam%27s%20Razor%3A%20A%20Benchmark%20for%20Inductive%0A%20%20and%20Abductive%20Reasoning&body=Title%3A%20Language%20Models%20Do%20Not%20Follow%20Occam%27s%20Razor%3A%20A%20Benchmark%20for%20Inductive%0A%20%20and%20Abductive%20Reasoning%0AAuthor%3A%20Yunxin%20Sun%20and%20Abulhair%20Saparov%0AAbstract%3A%20%20%20Reasoning%20is%20a%20core%20capability%20in%20artificial%20intelligence%20systems%2C%20for%20which%0Alarge%20language%20models%20%28LLMs%29%20have%20recently%20shown%20remarkable%20progress.%20However%2C%0Amost%20work%20focuses%20exclusively%20on%20deductive%20reasoning%2C%20which%20is%20problematic%0Asince%20other%20types%20of%20reasoning%20are%20also%20essential%20in%20solving%20real-world%0Aproblems%2C%20and%20they%20are%20less%20explored.%20This%20work%20focuses%20on%20evaluating%20LLMs%27%0Ainductive%20and%20abductive%20reasoning%20capabilities.%20We%20introduce%20a%20programmable%20and%0Asynthetic%20dataset%2C%20InAbHyD%20%28pronounced%20in-a-bid%29%2C%20where%20each%20reasoning%20example%0Aconsists%20of%20an%20incomplete%20world%20model%20and%20a%20set%20of%20observations.%20The%20task%20for%0Athe%20intelligent%20agent%20is%20to%20produce%20hypotheses%20to%20explain%20observations%20under%0Athe%20incomplete%20world%20model%20to%20solve%20each%20reasoning%20example.%20We%20propose%20a%20new%0Ametric%20to%20evaluate%20the%20quality%20of%20hypotheses%20based%20on%20Occam%27s%20Razor.%20We%0Aevaluate%20and%20analyze%20some%20state-of-the-art%20LLMs.%20Our%20analysis%20shows%20that%20LLMs%0Acan%20perform%20inductive%20and%20abductive%20reasoning%20in%20simple%20scenarios%2C%20but%20struggle%0Awith%20complex%20world%20models%20and%20producing%20high-quality%20hypotheses%2C%20even%20with%0Apopular%20reasoning-enhancing%20techniques%20such%20as%20in-context%20learning%20and%20RLVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Do%2520Not%2520Follow%2520Occam%2527s%2520Razor%253A%2520A%2520Benchmark%2520for%2520Inductive%250A%2520%2520and%2520Abductive%2520Reasoning%26entry.906535625%3DYunxin%2520Sun%2520and%2520Abulhair%2520Saparov%26entry.1292438233%3D%2520%2520Reasoning%2520is%2520a%2520core%2520capability%2520in%2520artificial%2520intelligence%2520systems%252C%2520for%2520which%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520shown%2520remarkable%2520progress.%2520However%252C%250Amost%2520work%2520focuses%2520exclusively%2520on%2520deductive%2520reasoning%252C%2520which%2520is%2520problematic%250Asince%2520other%2520types%2520of%2520reasoning%2520are%2520also%2520essential%2520in%2520solving%2520real-world%250Aproblems%252C%2520and%2520they%2520are%2520less%2520explored.%2520This%2520work%2520focuses%2520on%2520evaluating%2520LLMs%2527%250Ainductive%2520and%2520abductive%2520reasoning%2520capabilities.%2520We%2520introduce%2520a%2520programmable%2520and%250Asynthetic%2520dataset%252C%2520InAbHyD%2520%2528pronounced%2520in-a-bid%2529%252C%2520where%2520each%2520reasoning%2520example%250Aconsists%2520of%2520an%2520incomplete%2520world%2520model%2520and%2520a%2520set%2520of%2520observations.%2520The%2520task%2520for%250Athe%2520intelligent%2520agent%2520is%2520to%2520produce%2520hypotheses%2520to%2520explain%2520observations%2520under%250Athe%2520incomplete%2520world%2520model%2520to%2520solve%2520each%2520reasoning%2520example.%2520We%2520propose%2520a%2520new%250Ametric%2520to%2520evaluate%2520the%2520quality%2520of%2520hypotheses%2520based%2520on%2520Occam%2527s%2520Razor.%2520We%250Aevaluate%2520and%2520analyze%2520some%2520state-of-the-art%2520LLMs.%2520Our%2520analysis%2520shows%2520that%2520LLMs%250Acan%2520perform%2520inductive%2520and%2520abductive%2520reasoning%2520in%2520simple%2520scenarios%252C%2520but%2520struggle%250Awith%2520complex%2520world%2520models%2520and%2520producing%2520high-quality%2520hypotheses%252C%2520even%2520with%250Apopular%2520reasoning-enhancing%2520techniques%2520such%2520as%2520in-context%2520learning%2520and%2520RLVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Do%20Not%20Follow%20Occam%27s%20Razor%3A%20A%20Benchmark%20for%20Inductive%0A%20%20and%20Abductive%20Reasoning&entry.906535625=Yunxin%20Sun%20and%20Abulhair%20Saparov&entry.1292438233=%20%20Reasoning%20is%20a%20core%20capability%20in%20artificial%20intelligence%20systems%2C%20for%20which%0Alarge%20language%20models%20%28LLMs%29%20have%20recently%20shown%20remarkable%20progress.%20However%2C%0Amost%20work%20focuses%20exclusively%20on%20deductive%20reasoning%2C%20which%20is%20problematic%0Asince%20other%20types%20of%20reasoning%20are%20also%20essential%20in%20solving%20real-world%0Aproblems%2C%20and%20they%20are%20less%20explored.%20This%20work%20focuses%20on%20evaluating%20LLMs%27%0Ainductive%20and%20abductive%20reasoning%20capabilities.%20We%20introduce%20a%20programmable%20and%0Asynthetic%20dataset%2C%20InAbHyD%20%28pronounced%20in-a-bid%29%2C%20where%20each%20reasoning%20example%0Aconsists%20of%20an%20incomplete%20world%20model%20and%20a%20set%20of%20observations.%20The%20task%20for%0Athe%20intelligent%20agent%20is%20to%20produce%20hypotheses%20to%20explain%20observations%20under%0Athe%20incomplete%20world%20model%20to%20solve%20each%20reasoning%20example.%20We%20propose%20a%20new%0Ametric%20to%20evaluate%20the%20quality%20of%20hypotheses%20based%20on%20Occam%27s%20Razor.%20We%0Aevaluate%20and%20analyze%20some%20state-of-the-art%20LLMs.%20Our%20analysis%20shows%20that%20LLMs%0Acan%20perform%20inductive%20and%20abductive%20reasoning%20in%20simple%20scenarios%2C%20but%20struggle%0Awith%20complex%20world%20models%20and%20producing%20high-quality%20hypotheses%2C%20even%20with%0Apopular%20reasoning-enhancing%20techniques%20such%20as%20in-context%20learning%20and%20RLVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03345v1&entry.124074799=Read"},
{"title": "Joint Training of Image Generator and Detector for Road Defect Detection", "author": "Kuan-Chuan Peng", "abstract": "  Road defect detection is important for road authorities to reduce the vehicle\ndamage caused by road defects. Considering the practical scenarios where the\ndefect detectors are typically deployed on edge devices with limited memory and\ncomputational resource, we aim at performing road defect detection without\nusing ensemble-based methods or test-time augmentation (TTA). To this end, we\npropose to Jointly Train the image Generator and Detector for road defect\ndetection (dubbed as JTGD). We design the dual discriminators for the\ngenerative model to enforce both the synthesized defect patches and overall\nimages to look plausible. The synthesized image quality is improved by our\nproposed CLIP-based Fr\\'echet Inception Distance loss. The generative model in\nJTGD is trained jointly with the detector to encourage the generative model to\nsynthesize harder examples for the detector. Since harder synthesized images of\nbetter quality caused by the aforesaid design are used in the data\naugmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road\ndefect detection benchmark across various countries under the condition of no\nensemble and TTA. JTGD only uses less than 20% of the number of parameters\ncompared with the competing baseline, which makes it more suitable for\ndeployment on edge devices in practice.\n", "link": "http://arxiv.org/abs/2509.03465v1", "date": "2025-09-03", "relevancy": 2.148, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5496}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5375}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Training%20of%20Image%20Generator%20and%20Detector%20for%20Road%20Defect%20Detection&body=Title%3A%20Joint%20Training%20of%20Image%20Generator%20and%20Detector%20for%20Road%20Defect%20Detection%0AAuthor%3A%20Kuan-Chuan%20Peng%0AAbstract%3A%20%20%20Road%20defect%20detection%20is%20important%20for%20road%20authorities%20to%20reduce%20the%20vehicle%0Adamage%20caused%20by%20road%20defects.%20Considering%20the%20practical%20scenarios%20where%20the%0Adefect%20detectors%20are%20typically%20deployed%20on%20edge%20devices%20with%20limited%20memory%20and%0Acomputational%20resource%2C%20we%20aim%20at%20performing%20road%20defect%20detection%20without%0Ausing%20ensemble-based%20methods%20or%20test-time%20augmentation%20%28TTA%29.%20To%20this%20end%2C%20we%0Apropose%20to%20Jointly%20Train%20the%20image%20Generator%20and%20Detector%20for%20road%20defect%0Adetection%20%28dubbed%20as%20JTGD%29.%20We%20design%20the%20dual%20discriminators%20for%20the%0Agenerative%20model%20to%20enforce%20both%20the%20synthesized%20defect%20patches%20and%20overall%0Aimages%20to%20look%20plausible.%20The%20synthesized%20image%20quality%20is%20improved%20by%20our%0Aproposed%20CLIP-based%20Fr%5C%27echet%20Inception%20Distance%20loss.%20The%20generative%20model%20in%0AJTGD%20is%20trained%20jointly%20with%20the%20detector%20to%20encourage%20the%20generative%20model%20to%0Asynthesize%20harder%20examples%20for%20the%20detector.%20Since%20harder%20synthesized%20images%20of%0Abetter%20quality%20caused%20by%20the%20aforesaid%20design%20are%20used%20in%20the%20data%0Aaugmentation%2C%20JTGD%20outperforms%20the%20state-of-the-art%20method%20in%20the%20RDD2022%20road%0Adefect%20detection%20benchmark%20across%20various%20countries%20under%20the%20condition%20of%20no%0Aensemble%20and%20TTA.%20JTGD%20only%20uses%20less%20than%2020%25%20of%20the%20number%20of%20parameters%0Acompared%20with%20the%20competing%20baseline%2C%20which%20makes%20it%20more%20suitable%20for%0Adeployment%20on%20edge%20devices%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Training%2520of%2520Image%2520Generator%2520and%2520Detector%2520for%2520Road%2520Defect%2520Detection%26entry.906535625%3DKuan-Chuan%2520Peng%26entry.1292438233%3D%2520%2520Road%2520defect%2520detection%2520is%2520important%2520for%2520road%2520authorities%2520to%2520reduce%2520the%2520vehicle%250Adamage%2520caused%2520by%2520road%2520defects.%2520Considering%2520the%2520practical%2520scenarios%2520where%2520the%250Adefect%2520detectors%2520are%2520typically%2520deployed%2520on%2520edge%2520devices%2520with%2520limited%2520memory%2520and%250Acomputational%2520resource%252C%2520we%2520aim%2520at%2520performing%2520road%2520defect%2520detection%2520without%250Ausing%2520ensemble-based%2520methods%2520or%2520test-time%2520augmentation%2520%2528TTA%2529.%2520To%2520this%2520end%252C%2520we%250Apropose%2520to%2520Jointly%2520Train%2520the%2520image%2520Generator%2520and%2520Detector%2520for%2520road%2520defect%250Adetection%2520%2528dubbed%2520as%2520JTGD%2529.%2520We%2520design%2520the%2520dual%2520discriminators%2520for%2520the%250Agenerative%2520model%2520to%2520enforce%2520both%2520the%2520synthesized%2520defect%2520patches%2520and%2520overall%250Aimages%2520to%2520look%2520plausible.%2520The%2520synthesized%2520image%2520quality%2520is%2520improved%2520by%2520our%250Aproposed%2520CLIP-based%2520Fr%255C%2527echet%2520Inception%2520Distance%2520loss.%2520The%2520generative%2520model%2520in%250AJTGD%2520is%2520trained%2520jointly%2520with%2520the%2520detector%2520to%2520encourage%2520the%2520generative%2520model%2520to%250Asynthesize%2520harder%2520examples%2520for%2520the%2520detector.%2520Since%2520harder%2520synthesized%2520images%2520of%250Abetter%2520quality%2520caused%2520by%2520the%2520aforesaid%2520design%2520are%2520used%2520in%2520the%2520data%250Aaugmentation%252C%2520JTGD%2520outperforms%2520the%2520state-of-the-art%2520method%2520in%2520the%2520RDD2022%2520road%250Adefect%2520detection%2520benchmark%2520across%2520various%2520countries%2520under%2520the%2520condition%2520of%2520no%250Aensemble%2520and%2520TTA.%2520JTGD%2520only%2520uses%2520less%2520than%252020%2525%2520of%2520the%2520number%2520of%2520parameters%250Acompared%2520with%2520the%2520competing%2520baseline%252C%2520which%2520makes%2520it%2520more%2520suitable%2520for%250Adeployment%2520on%2520edge%2520devices%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Training%20of%20Image%20Generator%20and%20Detector%20for%20Road%20Defect%20Detection&entry.906535625=Kuan-Chuan%20Peng&entry.1292438233=%20%20Road%20defect%20detection%20is%20important%20for%20road%20authorities%20to%20reduce%20the%20vehicle%0Adamage%20caused%20by%20road%20defects.%20Considering%20the%20practical%20scenarios%20where%20the%0Adefect%20detectors%20are%20typically%20deployed%20on%20edge%20devices%20with%20limited%20memory%20and%0Acomputational%20resource%2C%20we%20aim%20at%20performing%20road%20defect%20detection%20without%0Ausing%20ensemble-based%20methods%20or%20test-time%20augmentation%20%28TTA%29.%20To%20this%20end%2C%20we%0Apropose%20to%20Jointly%20Train%20the%20image%20Generator%20and%20Detector%20for%20road%20defect%0Adetection%20%28dubbed%20as%20JTGD%29.%20We%20design%20the%20dual%20discriminators%20for%20the%0Agenerative%20model%20to%20enforce%20both%20the%20synthesized%20defect%20patches%20and%20overall%0Aimages%20to%20look%20plausible.%20The%20synthesized%20image%20quality%20is%20improved%20by%20our%0Aproposed%20CLIP-based%20Fr%5C%27echet%20Inception%20Distance%20loss.%20The%20generative%20model%20in%0AJTGD%20is%20trained%20jointly%20with%20the%20detector%20to%20encourage%20the%20generative%20model%20to%0Asynthesize%20harder%20examples%20for%20the%20detector.%20Since%20harder%20synthesized%20images%20of%0Abetter%20quality%20caused%20by%20the%20aforesaid%20design%20are%20used%20in%20the%20data%0Aaugmentation%2C%20JTGD%20outperforms%20the%20state-of-the-art%20method%20in%20the%20RDD2022%20road%0Adefect%20detection%20benchmark%20across%20various%20countries%20under%20the%20condition%20of%20no%0Aensemble%20and%20TTA.%20JTGD%20only%20uses%20less%20than%2020%25%20of%20the%20number%20of%20parameters%0Acompared%20with%20the%20competing%20baseline%2C%20which%20makes%20it%20more%20suitable%20for%0Adeployment%20on%20edge%20devices%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03465v1&entry.124074799=Read"},
{"title": "INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free\n  Class-incremental Learning", "author": "Zhiyuan Wang and Xiaoyang Qu and Jing Xiao and Bokui Chen and Jianzong Wang", "abstract": "  This paper introduces INCPrompt, an innovative continual learning solution\nthat effectively addresses catastrophic forgetting. INCPrompt's key innovation\nlies in its use of adaptive key-learner and task-aware prompts that capture\ntask-relevant information. This unique combination encapsulates general\nknowledge across tasks and encodes task-specific knowledge. Our comprehensive\nevaluation across multiple continual learning benchmarks demonstrates\nINCPrompt's superiority over existing algorithms, showing its effectiveness in\nmitigating catastrophic forgetting while maintaining high performance. These\nresults highlight the significant impact of task-aware incremental prompting on\ncontinual learning performance.\n", "link": "http://arxiv.org/abs/2401.11667v4", "date": "2025-09-03", "relevancy": 2.1381, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4334}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4281}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INCPrompt%3A%20Task-Aware%20incremental%20Prompting%20for%20Rehearsal-Free%0A%20%20Class-incremental%20Learning&body=Title%3A%20INCPrompt%3A%20Task-Aware%20incremental%20Prompting%20for%20Rehearsal-Free%0A%20%20Class-incremental%20Learning%0AAuthor%3A%20Zhiyuan%20Wang%20and%20Xiaoyang%20Qu%20and%20Jing%20Xiao%20and%20Bokui%20Chen%20and%20Jianzong%20Wang%0AAbstract%3A%20%20%20This%20paper%20introduces%20INCPrompt%2C%20an%20innovative%20continual%20learning%20solution%0Athat%20effectively%20addresses%20catastrophic%20forgetting.%20INCPrompt%27s%20key%20innovation%0Alies%20in%20its%20use%20of%20adaptive%20key-learner%20and%20task-aware%20prompts%20that%20capture%0Atask-relevant%20information.%20This%20unique%20combination%20encapsulates%20general%0Aknowledge%20across%20tasks%20and%20encodes%20task-specific%20knowledge.%20Our%20comprehensive%0Aevaluation%20across%20multiple%20continual%20learning%20benchmarks%20demonstrates%0AINCPrompt%27s%20superiority%20over%20existing%20algorithms%2C%20showing%20its%20effectiveness%20in%0Amitigating%20catastrophic%20forgetting%20while%20maintaining%20high%20performance.%20These%0Aresults%20highlight%20the%20significant%20impact%20of%20task-aware%20incremental%20prompting%20on%0Acontinual%20learning%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11667v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINCPrompt%253A%2520Task-Aware%2520incremental%2520Prompting%2520for%2520Rehearsal-Free%250A%2520%2520Class-incremental%2520Learning%26entry.906535625%3DZhiyuan%2520Wang%2520and%2520Xiaoyang%2520Qu%2520and%2520Jing%2520Xiao%2520and%2520Bokui%2520Chen%2520and%2520Jianzong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520INCPrompt%252C%2520an%2520innovative%2520continual%2520learning%2520solution%250Athat%2520effectively%2520addresses%2520catastrophic%2520forgetting.%2520INCPrompt%2527s%2520key%2520innovation%250Alies%2520in%2520its%2520use%2520of%2520adaptive%2520key-learner%2520and%2520task-aware%2520prompts%2520that%2520capture%250Atask-relevant%2520information.%2520This%2520unique%2520combination%2520encapsulates%2520general%250Aknowledge%2520across%2520tasks%2520and%2520encodes%2520task-specific%2520knowledge.%2520Our%2520comprehensive%250Aevaluation%2520across%2520multiple%2520continual%2520learning%2520benchmarks%2520demonstrates%250AINCPrompt%2527s%2520superiority%2520over%2520existing%2520algorithms%252C%2520showing%2520its%2520effectiveness%2520in%250Amitigating%2520catastrophic%2520forgetting%2520while%2520maintaining%2520high%2520performance.%2520These%250Aresults%2520highlight%2520the%2520significant%2520impact%2520of%2520task-aware%2520incremental%2520prompting%2520on%250Acontinual%2520learning%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11667v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INCPrompt%3A%20Task-Aware%20incremental%20Prompting%20for%20Rehearsal-Free%0A%20%20Class-incremental%20Learning&entry.906535625=Zhiyuan%20Wang%20and%20Xiaoyang%20Qu%20and%20Jing%20Xiao%20and%20Bokui%20Chen%20and%20Jianzong%20Wang&entry.1292438233=%20%20This%20paper%20introduces%20INCPrompt%2C%20an%20innovative%20continual%20learning%20solution%0Athat%20effectively%20addresses%20catastrophic%20forgetting.%20INCPrompt%27s%20key%20innovation%0Alies%20in%20its%20use%20of%20adaptive%20key-learner%20and%20task-aware%20prompts%20that%20capture%0Atask-relevant%20information.%20This%20unique%20combination%20encapsulates%20general%0Aknowledge%20across%20tasks%20and%20encodes%20task-specific%20knowledge.%20Our%20comprehensive%0Aevaluation%20across%20multiple%20continual%20learning%20benchmarks%20demonstrates%0AINCPrompt%27s%20superiority%20over%20existing%20algorithms%2C%20showing%20its%20effectiveness%20in%0Amitigating%20catastrophic%20forgetting%20while%20maintaining%20high%20performance.%20These%0Aresults%20highlight%20the%20significant%20impact%20of%20task-aware%20incremental%20prompting%20on%0Acontinual%20learning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11667v4&entry.124074799=Read"},
{"title": "DPQuant: Efficient and Differentially-Private Model Training via Dynamic\n  Quantization Scheduling", "author": "Yubo Gao and Renbo Tu and Gennady Pekhimenko and Nandita Vijaykumar", "abstract": "  Differentially-Private SGD (DP-SGD) is a powerful technique to protect user\nprivacy when using sensitive data to train neural networks. During training,\nconverting model weights and activations into low-precision formats, i.e.,\nquantization, can drastically reduce training times, energy consumption, and\ncost, and is thus a widely used technique. In this work, we demonstrate that\nquantization causes significantly higher accuracy degradation in DP-SGD\ncompared to regular SGD. We observe that this is caused by noise injection in\nDP-SGD, which amplifies quantization variance, leading to disproportionately\nlarge accuracy degradation. To address this challenge, we present QPQuant, a\ndynamic quantization framework that adaptively selects a changing subset of\nlayers to quantize at each epoch. Our method combines two key ideas that\neffectively reduce quantization variance: (i) probabilistic sampling of the\nlayers that rotates which layers are quantized every epoch, and (ii) loss-aware\nlayer prioritization, which uses a differentially private loss sensitivity\nestimator to identify layers that can be quantized with minimal impact on model\nquality. This estimator consumes a negligible fraction of the overall privacy\nbudget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,\nand DenseNet121 across a range of datasets demonstrate that DPQuant\nconsistently outperforms static quantization baselines, achieving near\nPareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical\nthroughput improvements on low-precision hardware, with less than 2% drop in\nvalidation accuracy.\n", "link": "http://arxiv.org/abs/2509.03472v1", "date": "2025-09-03", "relevancy": 2.1159, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5624}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5423}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPQuant%3A%20Efficient%20and%20Differentially-Private%20Model%20Training%20via%20Dynamic%0A%20%20Quantization%20Scheduling&body=Title%3A%20DPQuant%3A%20Efficient%20and%20Differentially-Private%20Model%20Training%20via%20Dynamic%0A%20%20Quantization%20Scheduling%0AAuthor%3A%20Yubo%20Gao%20and%20Renbo%20Tu%20and%20Gennady%20Pekhimenko%20and%20Nandita%20Vijaykumar%0AAbstract%3A%20%20%20Differentially-Private%20SGD%20%28DP-SGD%29%20is%20a%20powerful%20technique%20to%20protect%20user%0Aprivacy%20when%20using%20sensitive%20data%20to%20train%20neural%20networks.%20During%20training%2C%0Aconverting%20model%20weights%20and%20activations%20into%20low-precision%20formats%2C%20i.e.%2C%0Aquantization%2C%20can%20drastically%20reduce%20training%20times%2C%20energy%20consumption%2C%20and%0Acost%2C%20and%20is%20thus%20a%20widely%20used%20technique.%20In%20this%20work%2C%20we%20demonstrate%20that%0Aquantization%20causes%20significantly%20higher%20accuracy%20degradation%20in%20DP-SGD%0Acompared%20to%20regular%20SGD.%20We%20observe%20that%20this%20is%20caused%20by%20noise%20injection%20in%0ADP-SGD%2C%20which%20amplifies%20quantization%20variance%2C%20leading%20to%20disproportionately%0Alarge%20accuracy%20degradation.%20To%20address%20this%20challenge%2C%20we%20present%20QPQuant%2C%20a%0Adynamic%20quantization%20framework%20that%20adaptively%20selects%20a%20changing%20subset%20of%0Alayers%20to%20quantize%20at%20each%20epoch.%20Our%20method%20combines%20two%20key%20ideas%20that%0Aeffectively%20reduce%20quantization%20variance%3A%20%28i%29%20probabilistic%20sampling%20of%20the%0Alayers%20that%20rotates%20which%20layers%20are%20quantized%20every%20epoch%2C%20and%20%28ii%29%20loss-aware%0Alayer%20prioritization%2C%20which%20uses%20a%20differentially%20private%20loss%20sensitivity%0Aestimator%20to%20identify%20layers%20that%20can%20be%20quantized%20with%20minimal%20impact%20on%20model%0Aquality.%20This%20estimator%20consumes%20a%20negligible%20fraction%20of%20the%20overall%20privacy%0Abudget%2C%20preserving%20DP%20guarantees.%20Empirical%20evaluations%20on%20ResNet18%2C%20ResNet50%2C%0Aand%20DenseNet121%20across%20a%20range%20of%20datasets%20demonstrate%20that%20DPQuant%0Aconsistently%20outperforms%20static%20quantization%20baselines%2C%20achieving%20near%0APareto-optimal%20accuracy-compute%20trade-offs%20and%20up%20to%202.21x%20theoretical%0Athroughput%20improvements%20on%20low-precision%20hardware%2C%20with%20less%20than%202%25%20drop%20in%0Avalidation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPQuant%253A%2520Efficient%2520and%2520Differentially-Private%2520Model%2520Training%2520via%2520Dynamic%250A%2520%2520Quantization%2520Scheduling%26entry.906535625%3DYubo%2520Gao%2520and%2520Renbo%2520Tu%2520and%2520Gennady%2520Pekhimenko%2520and%2520Nandita%2520Vijaykumar%26entry.1292438233%3D%2520%2520Differentially-Private%2520SGD%2520%2528DP-SGD%2529%2520is%2520a%2520powerful%2520technique%2520to%2520protect%2520user%250Aprivacy%2520when%2520using%2520sensitive%2520data%2520to%2520train%2520neural%2520networks.%2520During%2520training%252C%250Aconverting%2520model%2520weights%2520and%2520activations%2520into%2520low-precision%2520formats%252C%2520i.e.%252C%250Aquantization%252C%2520can%2520drastically%2520reduce%2520training%2520times%252C%2520energy%2520consumption%252C%2520and%250Acost%252C%2520and%2520is%2520thus%2520a%2520widely%2520used%2520technique.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%250Aquantization%2520causes%2520significantly%2520higher%2520accuracy%2520degradation%2520in%2520DP-SGD%250Acompared%2520to%2520regular%2520SGD.%2520We%2520observe%2520that%2520this%2520is%2520caused%2520by%2520noise%2520injection%2520in%250ADP-SGD%252C%2520which%2520amplifies%2520quantization%2520variance%252C%2520leading%2520to%2520disproportionately%250Alarge%2520accuracy%2520degradation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520QPQuant%252C%2520a%250Adynamic%2520quantization%2520framework%2520that%2520adaptively%2520selects%2520a%2520changing%2520subset%2520of%250Alayers%2520to%2520quantize%2520at%2520each%2520epoch.%2520Our%2520method%2520combines%2520two%2520key%2520ideas%2520that%250Aeffectively%2520reduce%2520quantization%2520variance%253A%2520%2528i%2529%2520probabilistic%2520sampling%2520of%2520the%250Alayers%2520that%2520rotates%2520which%2520layers%2520are%2520quantized%2520every%2520epoch%252C%2520and%2520%2528ii%2529%2520loss-aware%250Alayer%2520prioritization%252C%2520which%2520uses%2520a%2520differentially%2520private%2520loss%2520sensitivity%250Aestimator%2520to%2520identify%2520layers%2520that%2520can%2520be%2520quantized%2520with%2520minimal%2520impact%2520on%2520model%250Aquality.%2520This%2520estimator%2520consumes%2520a%2520negligible%2520fraction%2520of%2520the%2520overall%2520privacy%250Abudget%252C%2520preserving%2520DP%2520guarantees.%2520Empirical%2520evaluations%2520on%2520ResNet18%252C%2520ResNet50%252C%250Aand%2520DenseNet121%2520across%2520a%2520range%2520of%2520datasets%2520demonstrate%2520that%2520DPQuant%250Aconsistently%2520outperforms%2520static%2520quantization%2520baselines%252C%2520achieving%2520near%250APareto-optimal%2520accuracy-compute%2520trade-offs%2520and%2520up%2520to%25202.21x%2520theoretical%250Athroughput%2520improvements%2520on%2520low-precision%2520hardware%252C%2520with%2520less%2520than%25202%2525%2520drop%2520in%250Avalidation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPQuant%3A%20Efficient%20and%20Differentially-Private%20Model%20Training%20via%20Dynamic%0A%20%20Quantization%20Scheduling&entry.906535625=Yubo%20Gao%20and%20Renbo%20Tu%20and%20Gennady%20Pekhimenko%20and%20Nandita%20Vijaykumar&entry.1292438233=%20%20Differentially-Private%20SGD%20%28DP-SGD%29%20is%20a%20powerful%20technique%20to%20protect%20user%0Aprivacy%20when%20using%20sensitive%20data%20to%20train%20neural%20networks.%20During%20training%2C%0Aconverting%20model%20weights%20and%20activations%20into%20low-precision%20formats%2C%20i.e.%2C%0Aquantization%2C%20can%20drastically%20reduce%20training%20times%2C%20energy%20consumption%2C%20and%0Acost%2C%20and%20is%20thus%20a%20widely%20used%20technique.%20In%20this%20work%2C%20we%20demonstrate%20that%0Aquantization%20causes%20significantly%20higher%20accuracy%20degradation%20in%20DP-SGD%0Acompared%20to%20regular%20SGD.%20We%20observe%20that%20this%20is%20caused%20by%20noise%20injection%20in%0ADP-SGD%2C%20which%20amplifies%20quantization%20variance%2C%20leading%20to%20disproportionately%0Alarge%20accuracy%20degradation.%20To%20address%20this%20challenge%2C%20we%20present%20QPQuant%2C%20a%0Adynamic%20quantization%20framework%20that%20adaptively%20selects%20a%20changing%20subset%20of%0Alayers%20to%20quantize%20at%20each%20epoch.%20Our%20method%20combines%20two%20key%20ideas%20that%0Aeffectively%20reduce%20quantization%20variance%3A%20%28i%29%20probabilistic%20sampling%20of%20the%0Alayers%20that%20rotates%20which%20layers%20are%20quantized%20every%20epoch%2C%20and%20%28ii%29%20loss-aware%0Alayer%20prioritization%2C%20which%20uses%20a%20differentially%20private%20loss%20sensitivity%0Aestimator%20to%20identify%20layers%20that%20can%20be%20quantized%20with%20minimal%20impact%20on%20model%0Aquality.%20This%20estimator%20consumes%20a%20negligible%20fraction%20of%20the%20overall%20privacy%0Abudget%2C%20preserving%20DP%20guarantees.%20Empirical%20evaluations%20on%20ResNet18%2C%20ResNet50%2C%0Aand%20DenseNet121%20across%20a%20range%20of%20datasets%20demonstrate%20that%20DPQuant%0Aconsistently%20outperforms%20static%20quantization%20baselines%2C%20achieving%20near%0APareto-optimal%20accuracy-compute%20trade-offs%20and%20up%20to%202.21x%20theoretical%0Athroughput%20improvements%20on%20low-precision%20hardware%2C%20with%20less%20than%202%25%20drop%20in%0Avalidation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03472v1&entry.124074799=Read"},
{"title": "HydroVision: Predicting Optically Active Parameters in Surface Water\n  Using Computer Vision", "author": "Shubham Laxmikant Deshmukh and Matthew Wilchek and Feras A. Batarseh", "abstract": "  Ongoing advancements in computer vision, particularly in pattern recognition\nand scene classification, have enabled new applications in environmental\nmonitoring. Deep learning now offers non-contact methods for assessing water\nquality and detecting contamination, both critical for disaster response and\npublic health protection. This work introduces HydroVision, a deep\nlearning-based scene classification framework that estimates optically active\nwater quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored\nDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and\nTurbidity from standard Red-Green-Blue (RGB) images of surface water.\nHydroVision supports early detection of contamination trends and strengthens\nmonitoring by regulatory agencies during external environmental stressors,\nindustrial activities, and force majeure events. The model is trained on more\nthan 500,000 seasonally varied images collected from the United States\nGeological Survey Hydrologic Imagery Visualization and Information System\nbetween 2022 and 2024. This approach leverages widely available RGB imagery as\na scalable, cost-effective alternative to traditional multispectral and\nhyperspectral remote sensing. Four state-of-the-art convolutional neural\nnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer\nare evaluated through transfer learning to identify the best-performing\narchitecture. DenseNet121 achieves the highest validation performance, with an\nR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for\nreal-world water quality monitoring across diverse conditions. While the\ncurrent model is optimized for well-lit imagery, future work will focus on\nimproving robustness under low-light and obstructed scenarios to expand its\noperational utility.\n", "link": "http://arxiv.org/abs/2509.01882v2", "date": "2025-09-03", "relevancy": 2.1098, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HydroVision%3A%20Predicting%20Optically%20Active%20Parameters%20in%20Surface%20Water%0A%20%20Using%20Computer%20Vision&body=Title%3A%20HydroVision%3A%20Predicting%20Optically%20Active%20Parameters%20in%20Surface%20Water%0A%20%20Using%20Computer%20Vision%0AAuthor%3A%20Shubham%20Laxmikant%20Deshmukh%20and%20Matthew%20Wilchek%20and%20Feras%20A.%20Batarseh%0AAbstract%3A%20%20%20Ongoing%20advancements%20in%20computer%20vision%2C%20particularly%20in%20pattern%20recognition%0Aand%20scene%20classification%2C%20have%20enabled%20new%20applications%20in%20environmental%0Amonitoring.%20Deep%20learning%20now%20offers%20non-contact%20methods%20for%20assessing%20water%0Aquality%20and%20detecting%20contamination%2C%20both%20critical%20for%20disaster%20response%20and%0Apublic%20health%20protection.%20This%20work%20introduces%20HydroVision%2C%20a%20deep%0Alearning-based%20scene%20classification%20framework%20that%20estimates%20optically%20active%0Awater%20quality%20parameters%20including%20Chlorophyll-Alpha%2C%20Chlorophylls%2C%20Colored%0ADissolved%20Organic%20Matter%20%28CDOM%29%2C%20Phycocyanins%2C%20Suspended%20Sediments%2C%20and%0ATurbidity%20from%20standard%20Red-Green-Blue%20%28RGB%29%20images%20of%20surface%20water.%0AHydroVision%20supports%20early%20detection%20of%20contamination%20trends%20and%20strengthens%0Amonitoring%20by%20regulatory%20agencies%20during%20external%20environmental%20stressors%2C%0Aindustrial%20activities%2C%20and%20force%20majeure%20events.%20The%20model%20is%20trained%20on%20more%0Athan%20500%2C000%20seasonally%20varied%20images%20collected%20from%20the%20United%20States%0AGeological%20Survey%20Hydrologic%20Imagery%20Visualization%20and%20Information%20System%0Abetween%202022%20and%202024.%20This%20approach%20leverages%20widely%20available%20RGB%20imagery%20as%0Aa%20scalable%2C%20cost-effective%20alternative%20to%20traditional%20multispectral%20and%0Ahyperspectral%20remote%20sensing.%20Four%20state-of-the-art%20convolutional%20neural%0Anetworks%20%28VGG-16%2C%20ResNet50%2C%20MobileNetV2%2C%20DenseNet121%29%20and%20a%20Vision%20Transformer%0Aare%20evaluated%20through%20transfer%20learning%20to%20identify%20the%20best-performing%0Aarchitecture.%20DenseNet121%20achieves%20the%20highest%20validation%20performance%2C%20with%20an%0AR2%20score%20of%200.89%20in%20predicting%20CDOM%2C%20demonstrating%20the%20framework%27s%20promise%20for%0Areal-world%20water%20quality%20monitoring%20across%20diverse%20conditions.%20While%20the%0Acurrent%20model%20is%20optimized%20for%20well-lit%20imagery%2C%20future%20work%20will%20focus%20on%0Aimproving%20robustness%20under%20low-light%20and%20obstructed%20scenarios%20to%20expand%20its%0Aoperational%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01882v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHydroVision%253A%2520Predicting%2520Optically%2520Active%2520Parameters%2520in%2520Surface%2520Water%250A%2520%2520Using%2520Computer%2520Vision%26entry.906535625%3DShubham%2520Laxmikant%2520Deshmukh%2520and%2520Matthew%2520Wilchek%2520and%2520Feras%2520A.%2520Batarseh%26entry.1292438233%3D%2520%2520Ongoing%2520advancements%2520in%2520computer%2520vision%252C%2520particularly%2520in%2520pattern%2520recognition%250Aand%2520scene%2520classification%252C%2520have%2520enabled%2520new%2520applications%2520in%2520environmental%250Amonitoring.%2520Deep%2520learning%2520now%2520offers%2520non-contact%2520methods%2520for%2520assessing%2520water%250Aquality%2520and%2520detecting%2520contamination%252C%2520both%2520critical%2520for%2520disaster%2520response%2520and%250Apublic%2520health%2520protection.%2520This%2520work%2520introduces%2520HydroVision%252C%2520a%2520deep%250Alearning-based%2520scene%2520classification%2520framework%2520that%2520estimates%2520optically%2520active%250Awater%2520quality%2520parameters%2520including%2520Chlorophyll-Alpha%252C%2520Chlorophylls%252C%2520Colored%250ADissolved%2520Organic%2520Matter%2520%2528CDOM%2529%252C%2520Phycocyanins%252C%2520Suspended%2520Sediments%252C%2520and%250ATurbidity%2520from%2520standard%2520Red-Green-Blue%2520%2528RGB%2529%2520images%2520of%2520surface%2520water.%250AHydroVision%2520supports%2520early%2520detection%2520of%2520contamination%2520trends%2520and%2520strengthens%250Amonitoring%2520by%2520regulatory%2520agencies%2520during%2520external%2520environmental%2520stressors%252C%250Aindustrial%2520activities%252C%2520and%2520force%2520majeure%2520events.%2520The%2520model%2520is%2520trained%2520on%2520more%250Athan%2520500%252C000%2520seasonally%2520varied%2520images%2520collected%2520from%2520the%2520United%2520States%250AGeological%2520Survey%2520Hydrologic%2520Imagery%2520Visualization%2520and%2520Information%2520System%250Abetween%25202022%2520and%25202024.%2520This%2520approach%2520leverages%2520widely%2520available%2520RGB%2520imagery%2520as%250Aa%2520scalable%252C%2520cost-effective%2520alternative%2520to%2520traditional%2520multispectral%2520and%250Ahyperspectral%2520remote%2520sensing.%2520Four%2520state-of-the-art%2520convolutional%2520neural%250Anetworks%2520%2528VGG-16%252C%2520ResNet50%252C%2520MobileNetV2%252C%2520DenseNet121%2529%2520and%2520a%2520Vision%2520Transformer%250Aare%2520evaluated%2520through%2520transfer%2520learning%2520to%2520identify%2520the%2520best-performing%250Aarchitecture.%2520DenseNet121%2520achieves%2520the%2520highest%2520validation%2520performance%252C%2520with%2520an%250AR2%2520score%2520of%25200.89%2520in%2520predicting%2520CDOM%252C%2520demonstrating%2520the%2520framework%2527s%2520promise%2520for%250Areal-world%2520water%2520quality%2520monitoring%2520across%2520diverse%2520conditions.%2520While%2520the%250Acurrent%2520model%2520is%2520optimized%2520for%2520well-lit%2520imagery%252C%2520future%2520work%2520will%2520focus%2520on%250Aimproving%2520robustness%2520under%2520low-light%2520and%2520obstructed%2520scenarios%2520to%2520expand%2520its%250Aoperational%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01882v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HydroVision%3A%20Predicting%20Optically%20Active%20Parameters%20in%20Surface%20Water%0A%20%20Using%20Computer%20Vision&entry.906535625=Shubham%20Laxmikant%20Deshmukh%20and%20Matthew%20Wilchek%20and%20Feras%20A.%20Batarseh&entry.1292438233=%20%20Ongoing%20advancements%20in%20computer%20vision%2C%20particularly%20in%20pattern%20recognition%0Aand%20scene%20classification%2C%20have%20enabled%20new%20applications%20in%20environmental%0Amonitoring.%20Deep%20learning%20now%20offers%20non-contact%20methods%20for%20assessing%20water%0Aquality%20and%20detecting%20contamination%2C%20both%20critical%20for%20disaster%20response%20and%0Apublic%20health%20protection.%20This%20work%20introduces%20HydroVision%2C%20a%20deep%0Alearning-based%20scene%20classification%20framework%20that%20estimates%20optically%20active%0Awater%20quality%20parameters%20including%20Chlorophyll-Alpha%2C%20Chlorophylls%2C%20Colored%0ADissolved%20Organic%20Matter%20%28CDOM%29%2C%20Phycocyanins%2C%20Suspended%20Sediments%2C%20and%0ATurbidity%20from%20standard%20Red-Green-Blue%20%28RGB%29%20images%20of%20surface%20water.%0AHydroVision%20supports%20early%20detection%20of%20contamination%20trends%20and%20strengthens%0Amonitoring%20by%20regulatory%20agencies%20during%20external%20environmental%20stressors%2C%0Aindustrial%20activities%2C%20and%20force%20majeure%20events.%20The%20model%20is%20trained%20on%20more%0Athan%20500%2C000%20seasonally%20varied%20images%20collected%20from%20the%20United%20States%0AGeological%20Survey%20Hydrologic%20Imagery%20Visualization%20and%20Information%20System%0Abetween%202022%20and%202024.%20This%20approach%20leverages%20widely%20available%20RGB%20imagery%20as%0Aa%20scalable%2C%20cost-effective%20alternative%20to%20traditional%20multispectral%20and%0Ahyperspectral%20remote%20sensing.%20Four%20state-of-the-art%20convolutional%20neural%0Anetworks%20%28VGG-16%2C%20ResNet50%2C%20MobileNetV2%2C%20DenseNet121%29%20and%20a%20Vision%20Transformer%0Aare%20evaluated%20through%20transfer%20learning%20to%20identify%20the%20best-performing%0Aarchitecture.%20DenseNet121%20achieves%20the%20highest%20validation%20performance%2C%20with%20an%0AR2%20score%20of%200.89%20in%20predicting%20CDOM%2C%20demonstrating%20the%20framework%27s%20promise%20for%0Areal-world%20water%20quality%20monitoring%20across%20diverse%20conditions.%20While%20the%0Acurrent%20model%20is%20optimized%20for%20well-lit%20imagery%2C%20future%20work%20will%20focus%20on%0Aimproving%20robustness%20under%20low-light%20and%20obstructed%20scenarios%20to%20expand%20its%0Aoperational%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01882v2&entry.124074799=Read"},
{"title": "On Developers' Self-Declaration of AI-Generated Code: An Analysis of\n  Practices", "author": "Syed Mohammad Kashif and Peng Liang and Amjed Tahir", "abstract": "  AI code generation tools have gained significant popularity among developers,\nwho use them to assist in software development due to their capability to\ngenerate code. Existing studies mainly explored the quality, e.g., correctness\nand security, of AI-generated code, while in real-world software development,\nthe prerequisite is to distinguish AI-generated code from human-written code,\nwhich emphasizes the need to explicitly declare AI-generated code by\ndevelopers. To this end, this study intends to understand the ways developers\nuse to self-declare AI-generated code and explore the reasons why developers\nchoose to self-declare or not. We conducted a mixed-methods study consisting of\ntwo phases. In the first phase, we mined GitHub repositories and collected 613\ninstances of AI-generated code snippets. In the second phase, we conducted a\nfollow-up practitioners' survey, which received 111 valid responses. Our\nresearch revealed the practices followed by developers to self-declare\nAI-generated code. Most practitioners (76.6%) always or sometimes self-declare\nAI-generated code. In contrast, other practitioners (23.4%) noted that they\nnever self-declare AI-generated code. The reasons for self-declaring\nAI-generated code include the need to track and monitor the code for future\nreview and debugging, and ethical considerations. The reasons for not\nself-declaring AI-generated code include extensive modifications to\nAI-generated code and the developers' perception that self-declaration is an\nunnecessary activity. We finally provided guidelines for practitioners to\nself-declare AI-generated code, addressing ethical and code quality concerns.\n", "link": "http://arxiv.org/abs/2504.16485v2", "date": "2025-09-03", "relevancy": 2.1006, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4606}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4116}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Developers%27%20Self-Declaration%20of%20AI-Generated%20Code%3A%20An%20Analysis%20of%0A%20%20Practices&body=Title%3A%20On%20Developers%27%20Self-Declaration%20of%20AI-Generated%20Code%3A%20An%20Analysis%20of%0A%20%20Practices%0AAuthor%3A%20Syed%20Mohammad%20Kashif%20and%20Peng%20Liang%20and%20Amjed%20Tahir%0AAbstract%3A%20%20%20AI%20code%20generation%20tools%20have%20gained%20significant%20popularity%20among%20developers%2C%0Awho%20use%20them%20to%20assist%20in%20software%20development%20due%20to%20their%20capability%20to%0Agenerate%20code.%20Existing%20studies%20mainly%20explored%20the%20quality%2C%20e.g.%2C%20correctness%0Aand%20security%2C%20of%20AI-generated%20code%2C%20while%20in%20real-world%20software%20development%2C%0Athe%20prerequisite%20is%20to%20distinguish%20AI-generated%20code%20from%20human-written%20code%2C%0Awhich%20emphasizes%20the%20need%20to%20explicitly%20declare%20AI-generated%20code%20by%0Adevelopers.%20To%20this%20end%2C%20this%20study%20intends%20to%20understand%20the%20ways%20developers%0Ause%20to%20self-declare%20AI-generated%20code%20and%20explore%20the%20reasons%20why%20developers%0Achoose%20to%20self-declare%20or%20not.%20We%20conducted%20a%20mixed-methods%20study%20consisting%20of%0Atwo%20phases.%20In%20the%20first%20phase%2C%20we%20mined%20GitHub%20repositories%20and%20collected%20613%0Ainstances%20of%20AI-generated%20code%20snippets.%20In%20the%20second%20phase%2C%20we%20conducted%20a%0Afollow-up%20practitioners%27%20survey%2C%20which%20received%20111%20valid%20responses.%20Our%0Aresearch%20revealed%20the%20practices%20followed%20by%20developers%20to%20self-declare%0AAI-generated%20code.%20Most%20practitioners%20%2876.6%25%29%20always%20or%20sometimes%20self-declare%0AAI-generated%20code.%20In%20contrast%2C%20other%20practitioners%20%2823.4%25%29%20noted%20that%20they%0Anever%20self-declare%20AI-generated%20code.%20The%20reasons%20for%20self-declaring%0AAI-generated%20code%20include%20the%20need%20to%20track%20and%20monitor%20the%20code%20for%20future%0Areview%20and%20debugging%2C%20and%20ethical%20considerations.%20The%20reasons%20for%20not%0Aself-declaring%20AI-generated%20code%20include%20extensive%20modifications%20to%0AAI-generated%20code%20and%20the%20developers%27%20perception%20that%20self-declaration%20is%20an%0Aunnecessary%20activity.%20We%20finally%20provided%20guidelines%20for%20practitioners%20to%0Aself-declare%20AI-generated%20code%2C%20addressing%20ethical%20and%20code%20quality%20concerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Developers%2527%2520Self-Declaration%2520of%2520AI-Generated%2520Code%253A%2520An%2520Analysis%2520of%250A%2520%2520Practices%26entry.906535625%3DSyed%2520Mohammad%2520Kashif%2520and%2520Peng%2520Liang%2520and%2520Amjed%2520Tahir%26entry.1292438233%3D%2520%2520AI%2520code%2520generation%2520tools%2520have%2520gained%2520significant%2520popularity%2520among%2520developers%252C%250Awho%2520use%2520them%2520to%2520assist%2520in%2520software%2520development%2520due%2520to%2520their%2520capability%2520to%250Agenerate%2520code.%2520Existing%2520studies%2520mainly%2520explored%2520the%2520quality%252C%2520e.g.%252C%2520correctness%250Aand%2520security%252C%2520of%2520AI-generated%2520code%252C%2520while%2520in%2520real-world%2520software%2520development%252C%250Athe%2520prerequisite%2520is%2520to%2520distinguish%2520AI-generated%2520code%2520from%2520human-written%2520code%252C%250Awhich%2520emphasizes%2520the%2520need%2520to%2520explicitly%2520declare%2520AI-generated%2520code%2520by%250Adevelopers.%2520To%2520this%2520end%252C%2520this%2520study%2520intends%2520to%2520understand%2520the%2520ways%2520developers%250Ause%2520to%2520self-declare%2520AI-generated%2520code%2520and%2520explore%2520the%2520reasons%2520why%2520developers%250Achoose%2520to%2520self-declare%2520or%2520not.%2520We%2520conducted%2520a%2520mixed-methods%2520study%2520consisting%2520of%250Atwo%2520phases.%2520In%2520the%2520first%2520phase%252C%2520we%2520mined%2520GitHub%2520repositories%2520and%2520collected%2520613%250Ainstances%2520of%2520AI-generated%2520code%2520snippets.%2520In%2520the%2520second%2520phase%252C%2520we%2520conducted%2520a%250Afollow-up%2520practitioners%2527%2520survey%252C%2520which%2520received%2520111%2520valid%2520responses.%2520Our%250Aresearch%2520revealed%2520the%2520practices%2520followed%2520by%2520developers%2520to%2520self-declare%250AAI-generated%2520code.%2520Most%2520practitioners%2520%252876.6%2525%2529%2520always%2520or%2520sometimes%2520self-declare%250AAI-generated%2520code.%2520In%2520contrast%252C%2520other%2520practitioners%2520%252823.4%2525%2529%2520noted%2520that%2520they%250Anever%2520self-declare%2520AI-generated%2520code.%2520The%2520reasons%2520for%2520self-declaring%250AAI-generated%2520code%2520include%2520the%2520need%2520to%2520track%2520and%2520monitor%2520the%2520code%2520for%2520future%250Areview%2520and%2520debugging%252C%2520and%2520ethical%2520considerations.%2520The%2520reasons%2520for%2520not%250Aself-declaring%2520AI-generated%2520code%2520include%2520extensive%2520modifications%2520to%250AAI-generated%2520code%2520and%2520the%2520developers%2527%2520perception%2520that%2520self-declaration%2520is%2520an%250Aunnecessary%2520activity.%2520We%2520finally%2520provided%2520guidelines%2520for%2520practitioners%2520to%250Aself-declare%2520AI-generated%2520code%252C%2520addressing%2520ethical%2520and%2520code%2520quality%2520concerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Developers%27%20Self-Declaration%20of%20AI-Generated%20Code%3A%20An%20Analysis%20of%0A%20%20Practices&entry.906535625=Syed%20Mohammad%20Kashif%20and%20Peng%20Liang%20and%20Amjed%20Tahir&entry.1292438233=%20%20AI%20code%20generation%20tools%20have%20gained%20significant%20popularity%20among%20developers%2C%0Awho%20use%20them%20to%20assist%20in%20software%20development%20due%20to%20their%20capability%20to%0Agenerate%20code.%20Existing%20studies%20mainly%20explored%20the%20quality%2C%20e.g.%2C%20correctness%0Aand%20security%2C%20of%20AI-generated%20code%2C%20while%20in%20real-world%20software%20development%2C%0Athe%20prerequisite%20is%20to%20distinguish%20AI-generated%20code%20from%20human-written%20code%2C%0Awhich%20emphasizes%20the%20need%20to%20explicitly%20declare%20AI-generated%20code%20by%0Adevelopers.%20To%20this%20end%2C%20this%20study%20intends%20to%20understand%20the%20ways%20developers%0Ause%20to%20self-declare%20AI-generated%20code%20and%20explore%20the%20reasons%20why%20developers%0Achoose%20to%20self-declare%20or%20not.%20We%20conducted%20a%20mixed-methods%20study%20consisting%20of%0Atwo%20phases.%20In%20the%20first%20phase%2C%20we%20mined%20GitHub%20repositories%20and%20collected%20613%0Ainstances%20of%20AI-generated%20code%20snippets.%20In%20the%20second%20phase%2C%20we%20conducted%20a%0Afollow-up%20practitioners%27%20survey%2C%20which%20received%20111%20valid%20responses.%20Our%0Aresearch%20revealed%20the%20practices%20followed%20by%20developers%20to%20self-declare%0AAI-generated%20code.%20Most%20practitioners%20%2876.6%25%29%20always%20or%20sometimes%20self-declare%0AAI-generated%20code.%20In%20contrast%2C%20other%20practitioners%20%2823.4%25%29%20noted%20that%20they%0Anever%20self-declare%20AI-generated%20code.%20The%20reasons%20for%20self-declaring%0AAI-generated%20code%20include%20the%20need%20to%20track%20and%20monitor%20the%20code%20for%20future%0Areview%20and%20debugging%2C%20and%20ethical%20considerations.%20The%20reasons%20for%20not%0Aself-declaring%20AI-generated%20code%20include%20extensive%20modifications%20to%0AAI-generated%20code%20and%20the%20developers%27%20perception%20that%20self-declaration%20is%20an%0Aunnecessary%20activity.%20We%20finally%20provided%20guidelines%20for%20practitioners%20to%0Aself-declare%20AI-generated%20code%2C%20addressing%20ethical%20and%20code%20quality%20concerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16485v2&entry.124074799=Read"},
{"title": "Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal\n  Gland Segmentation in Computed Tomography Medical Images", "author": "Hania Ghouse and Muzammil Behzad", "abstract": "  Segmentation of small and irregularly shaped abdominal organs, such as the\nadrenal glands in CT imaging, remains a persistent challenge due to severe\nclass imbalance, poor spatial context, and limited annotated data. In this\nwork, we propose a unified framework that combines variational reconstruction,\nsupervised segmentation, and adversarial patch-based feedback to address these\nlimitations in a principled and scalable manner. Our architecture is built upon\na VAE-UNet backbone that jointly reconstructs input patches and generates\nvoxel-level segmentation masks, allowing the model to learn disentangled\nrepresentations of anatomical structure and appearance. We introduce a\npatch-based training pipeline that selectively injects synthetic patches\ngenerated from the learned latent space, and systematically study the effects\nof varying synthetic-to-real patch ratios during training. To further enhance\noutput fidelity, the framework incorporates perceptual reconstruction loss\nusing VGG features, as well as a PatchGAN-style discriminator for adversarial\nsupervision over spatial realism. Comprehensive experiments on the BTCV dataset\ndemonstrate that our approach improves segmentation accuracy, particularly in\nboundary-sensitive regions, while maintaining strong reconstruction quality.\nOur findings highlight the effectiveness of hybrid generative-discriminative\ntraining regimes for small-organ segmentation and provide new insights into\nbalancing realism, diversity, and anatomical consistency in data-scarce\nscenarios.\n", "link": "http://arxiv.org/abs/2509.03188v1", "date": "2025-09-03", "relevancy": 2.1, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5378}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Guided%20Patch%20UNet-VAE%20with%20Adversarial%20Supervision%20for%20Adrenal%0A%20%20Gland%20Segmentation%20in%20Computed%20Tomography%20Medical%20Images&body=Title%3A%20Prompt-Guided%20Patch%20UNet-VAE%20with%20Adversarial%20Supervision%20for%20Adrenal%0A%20%20Gland%20Segmentation%20in%20Computed%20Tomography%20Medical%20Images%0AAuthor%3A%20Hania%20Ghouse%20and%20Muzammil%20Behzad%0AAbstract%3A%20%20%20Segmentation%20of%20small%20and%20irregularly%20shaped%20abdominal%20organs%2C%20such%20as%20the%0Aadrenal%20glands%20in%20CT%20imaging%2C%20remains%20a%20persistent%20challenge%20due%20to%20severe%0Aclass%20imbalance%2C%20poor%20spatial%20context%2C%20and%20limited%20annotated%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20unified%20framework%20that%20combines%20variational%20reconstruction%2C%0Asupervised%20segmentation%2C%20and%20adversarial%20patch-based%20feedback%20to%20address%20these%0Alimitations%20in%20a%20principled%20and%20scalable%20manner.%20Our%20architecture%20is%20built%20upon%0Aa%20VAE-UNet%20backbone%20that%20jointly%20reconstructs%20input%20patches%20and%20generates%0Avoxel-level%20segmentation%20masks%2C%20allowing%20the%20model%20to%20learn%20disentangled%0Arepresentations%20of%20anatomical%20structure%20and%20appearance.%20We%20introduce%20a%0Apatch-based%20training%20pipeline%20that%20selectively%20injects%20synthetic%20patches%0Agenerated%20from%20the%20learned%20latent%20space%2C%20and%20systematically%20study%20the%20effects%0Aof%20varying%20synthetic-to-real%20patch%20ratios%20during%20training.%20To%20further%20enhance%0Aoutput%20fidelity%2C%20the%20framework%20incorporates%20perceptual%20reconstruction%20loss%0Ausing%20VGG%20features%2C%20as%20well%20as%20a%20PatchGAN-style%20discriminator%20for%20adversarial%0Asupervision%20over%20spatial%20realism.%20Comprehensive%20experiments%20on%20the%20BTCV%20dataset%0Ademonstrate%20that%20our%20approach%20improves%20segmentation%20accuracy%2C%20particularly%20in%0Aboundary-sensitive%20regions%2C%20while%20maintaining%20strong%20reconstruction%20quality.%0AOur%20findings%20highlight%20the%20effectiveness%20of%20hybrid%20generative-discriminative%0Atraining%20regimes%20for%20small-organ%20segmentation%20and%20provide%20new%20insights%20into%0Abalancing%20realism%2C%20diversity%2C%20and%20anatomical%20consistency%20in%20data-scarce%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Guided%2520Patch%2520UNet-VAE%2520with%2520Adversarial%2520Supervision%2520for%2520Adrenal%250A%2520%2520Gland%2520Segmentation%2520in%2520Computed%2520Tomography%2520Medical%2520Images%26entry.906535625%3DHania%2520Ghouse%2520and%2520Muzammil%2520Behzad%26entry.1292438233%3D%2520%2520Segmentation%2520of%2520small%2520and%2520irregularly%2520shaped%2520abdominal%2520organs%252C%2520such%2520as%2520the%250Aadrenal%2520glands%2520in%2520CT%2520imaging%252C%2520remains%2520a%2520persistent%2520challenge%2520due%2520to%2520severe%250Aclass%2520imbalance%252C%2520poor%2520spatial%2520context%252C%2520and%2520limited%2520annotated%2520data.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520unified%2520framework%2520that%2520combines%2520variational%2520reconstruction%252C%250Asupervised%2520segmentation%252C%2520and%2520adversarial%2520patch-based%2520feedback%2520to%2520address%2520these%250Alimitations%2520in%2520a%2520principled%2520and%2520scalable%2520manner.%2520Our%2520architecture%2520is%2520built%2520upon%250Aa%2520VAE-UNet%2520backbone%2520that%2520jointly%2520reconstructs%2520input%2520patches%2520and%2520generates%250Avoxel-level%2520segmentation%2520masks%252C%2520allowing%2520the%2520model%2520to%2520learn%2520disentangled%250Arepresentations%2520of%2520anatomical%2520structure%2520and%2520appearance.%2520We%2520introduce%2520a%250Apatch-based%2520training%2520pipeline%2520that%2520selectively%2520injects%2520synthetic%2520patches%250Agenerated%2520from%2520the%2520learned%2520latent%2520space%252C%2520and%2520systematically%2520study%2520the%2520effects%250Aof%2520varying%2520synthetic-to-real%2520patch%2520ratios%2520during%2520training.%2520To%2520further%2520enhance%250Aoutput%2520fidelity%252C%2520the%2520framework%2520incorporates%2520perceptual%2520reconstruction%2520loss%250Ausing%2520VGG%2520features%252C%2520as%2520well%2520as%2520a%2520PatchGAN-style%2520discriminator%2520for%2520adversarial%250Asupervision%2520over%2520spatial%2520realism.%2520Comprehensive%2520experiments%2520on%2520the%2520BTCV%2520dataset%250Ademonstrate%2520that%2520our%2520approach%2520improves%2520segmentation%2520accuracy%252C%2520particularly%2520in%250Aboundary-sensitive%2520regions%252C%2520while%2520maintaining%2520strong%2520reconstruction%2520quality.%250AOur%2520findings%2520highlight%2520the%2520effectiveness%2520of%2520hybrid%2520generative-discriminative%250Atraining%2520regimes%2520for%2520small-organ%2520segmentation%2520and%2520provide%2520new%2520insights%2520into%250Abalancing%2520realism%252C%2520diversity%252C%2520and%2520anatomical%2520consistency%2520in%2520data-scarce%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Guided%20Patch%20UNet-VAE%20with%20Adversarial%20Supervision%20for%20Adrenal%0A%20%20Gland%20Segmentation%20in%20Computed%20Tomography%20Medical%20Images&entry.906535625=Hania%20Ghouse%20and%20Muzammil%20Behzad&entry.1292438233=%20%20Segmentation%20of%20small%20and%20irregularly%20shaped%20abdominal%20organs%2C%20such%20as%20the%0Aadrenal%20glands%20in%20CT%20imaging%2C%20remains%20a%20persistent%20challenge%20due%20to%20severe%0Aclass%20imbalance%2C%20poor%20spatial%20context%2C%20and%20limited%20annotated%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20unified%20framework%20that%20combines%20variational%20reconstruction%2C%0Asupervised%20segmentation%2C%20and%20adversarial%20patch-based%20feedback%20to%20address%20these%0Alimitations%20in%20a%20principled%20and%20scalable%20manner.%20Our%20architecture%20is%20built%20upon%0Aa%20VAE-UNet%20backbone%20that%20jointly%20reconstructs%20input%20patches%20and%20generates%0Avoxel-level%20segmentation%20masks%2C%20allowing%20the%20model%20to%20learn%20disentangled%0Arepresentations%20of%20anatomical%20structure%20and%20appearance.%20We%20introduce%20a%0Apatch-based%20training%20pipeline%20that%20selectively%20injects%20synthetic%20patches%0Agenerated%20from%20the%20learned%20latent%20space%2C%20and%20systematically%20study%20the%20effects%0Aof%20varying%20synthetic-to-real%20patch%20ratios%20during%20training.%20To%20further%20enhance%0Aoutput%20fidelity%2C%20the%20framework%20incorporates%20perceptual%20reconstruction%20loss%0Ausing%20VGG%20features%2C%20as%20well%20as%20a%20PatchGAN-style%20discriminator%20for%20adversarial%0Asupervision%20over%20spatial%20realism.%20Comprehensive%20experiments%20on%20the%20BTCV%20dataset%0Ademonstrate%20that%20our%20approach%20improves%20segmentation%20accuracy%2C%20particularly%20in%0Aboundary-sensitive%20regions%2C%20while%20maintaining%20strong%20reconstruction%20quality.%0AOur%20findings%20highlight%20the%20effectiveness%20of%20hybrid%20generative-discriminative%0Atraining%20regimes%20for%20small-organ%20segmentation%20and%20provide%20new%20insights%20into%0Abalancing%20realism%2C%20diversity%2C%20and%20anatomical%20consistency%20in%20data-scarce%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03188v1&entry.124074799=Read"},
{"title": "P2DT: Mitigating Forgetting in task-incremental Learning with\n  progressive prompt Decision Transformer", "author": "Zhiyuan Wang and Xiaoyang Qu and Jing Xiao and Bokui Chen and Jianzong Wang", "abstract": "  Catastrophic forgetting poses a substantial challenge for managing\nintelligent agents controlled by a large model, causing performance degradation\nwhen these agents face new tasks. In our work, we propose a novel solution -\nthe Progressive Prompt Decision Transformer (P2DT). This method enhances a\ntransformer-based model by dynamically appending decision tokens during new\ntask training, thus fostering task-specific policies. Our approach mitigates\nforgetting in continual and offline reinforcement learning scenarios. Moreover,\nP2DT leverages trajectories collected via traditional reinforcement learning\nfrom all tasks and generates new task-specific tokens during training, thereby\nretaining knowledge from previous studies. Preliminary results demonstrate that\nour model effectively alleviates catastrophic forgetting and scales well with\nincreasing task environments.\n", "link": "http://arxiv.org/abs/2401.11666v2", "date": "2025-09-03", "relevancy": 2.097, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5294}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5279}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P2DT%3A%20Mitigating%20Forgetting%20in%20task-incremental%20Learning%20with%0A%20%20progressive%20prompt%20Decision%20Transformer&body=Title%3A%20P2DT%3A%20Mitigating%20Forgetting%20in%20task-incremental%20Learning%20with%0A%20%20progressive%20prompt%20Decision%20Transformer%0AAuthor%3A%20Zhiyuan%20Wang%20and%20Xiaoyang%20Qu%20and%20Jing%20Xiao%20and%20Bokui%20Chen%20and%20Jianzong%20Wang%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20poses%20a%20substantial%20challenge%20for%20managing%0Aintelligent%20agents%20controlled%20by%20a%20large%20model%2C%20causing%20performance%20degradation%0Awhen%20these%20agents%20face%20new%20tasks.%20In%20our%20work%2C%20we%20propose%20a%20novel%20solution%20-%0Athe%20Progressive%20Prompt%20Decision%20Transformer%20%28P2DT%29.%20This%20method%20enhances%20a%0Atransformer-based%20model%20by%20dynamically%20appending%20decision%20tokens%20during%20new%0Atask%20training%2C%20thus%20fostering%20task-specific%20policies.%20Our%20approach%20mitigates%0Aforgetting%20in%20continual%20and%20offline%20reinforcement%20learning%20scenarios.%20Moreover%2C%0AP2DT%20leverages%20trajectories%20collected%20via%20traditional%20reinforcement%20learning%0Afrom%20all%20tasks%20and%20generates%20new%20task-specific%20tokens%20during%20training%2C%20thereby%0Aretaining%20knowledge%20from%20previous%20studies.%20Preliminary%20results%20demonstrate%20that%0Aour%20model%20effectively%20alleviates%20catastrophic%20forgetting%20and%20scales%20well%20with%0Aincreasing%20task%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP2DT%253A%2520Mitigating%2520Forgetting%2520in%2520task-incremental%2520Learning%2520with%250A%2520%2520progressive%2520prompt%2520Decision%2520Transformer%26entry.906535625%3DZhiyuan%2520Wang%2520and%2520Xiaoyang%2520Qu%2520and%2520Jing%2520Xiao%2520and%2520Bokui%2520Chen%2520and%2520Jianzong%2520Wang%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520poses%2520a%2520substantial%2520challenge%2520for%2520managing%250Aintelligent%2520agents%2520controlled%2520by%2520a%2520large%2520model%252C%2520causing%2520performance%2520degradation%250Awhen%2520these%2520agents%2520face%2520new%2520tasks.%2520In%2520our%2520work%252C%2520we%2520propose%2520a%2520novel%2520solution%2520-%250Athe%2520Progressive%2520Prompt%2520Decision%2520Transformer%2520%2528P2DT%2529.%2520This%2520method%2520enhances%2520a%250Atransformer-based%2520model%2520by%2520dynamically%2520appending%2520decision%2520tokens%2520during%2520new%250Atask%2520training%252C%2520thus%2520fostering%2520task-specific%2520policies.%2520Our%2520approach%2520mitigates%250Aforgetting%2520in%2520continual%2520and%2520offline%2520reinforcement%2520learning%2520scenarios.%2520Moreover%252C%250AP2DT%2520leverages%2520trajectories%2520collected%2520via%2520traditional%2520reinforcement%2520learning%250Afrom%2520all%2520tasks%2520and%2520generates%2520new%2520task-specific%2520tokens%2520during%2520training%252C%2520thereby%250Aretaining%2520knowledge%2520from%2520previous%2520studies.%2520Preliminary%2520results%2520demonstrate%2520that%250Aour%2520model%2520effectively%2520alleviates%2520catastrophic%2520forgetting%2520and%2520scales%2520well%2520with%250Aincreasing%2520task%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P2DT%3A%20Mitigating%20Forgetting%20in%20task-incremental%20Learning%20with%0A%20%20progressive%20prompt%20Decision%20Transformer&entry.906535625=Zhiyuan%20Wang%20and%20Xiaoyang%20Qu%20and%20Jing%20Xiao%20and%20Bokui%20Chen%20and%20Jianzong%20Wang&entry.1292438233=%20%20Catastrophic%20forgetting%20poses%20a%20substantial%20challenge%20for%20managing%0Aintelligent%20agents%20controlled%20by%20a%20large%20model%2C%20causing%20performance%20degradation%0Awhen%20these%20agents%20face%20new%20tasks.%20In%20our%20work%2C%20we%20propose%20a%20novel%20solution%20-%0Athe%20Progressive%20Prompt%20Decision%20Transformer%20%28P2DT%29.%20This%20method%20enhances%20a%0Atransformer-based%20model%20by%20dynamically%20appending%20decision%20tokens%20during%20new%0Atask%20training%2C%20thus%20fostering%20task-specific%20policies.%20Our%20approach%20mitigates%0Aforgetting%20in%20continual%20and%20offline%20reinforcement%20learning%20scenarios.%20Moreover%2C%0AP2DT%20leverages%20trajectories%20collected%20via%20traditional%20reinforcement%20learning%0Afrom%20all%20tasks%20and%20generates%20new%20task-specific%20tokens%20during%20training%2C%20thereby%0Aretaining%20knowledge%20from%20previous%20studies.%20Preliminary%20results%20demonstrate%20that%0Aour%20model%20effectively%20alleviates%20catastrophic%20forgetting%20and%20scales%20well%20with%0Aincreasing%20task%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11666v2&entry.124074799=Read"},
{"title": "Population-Scale Network Embeddings Expose Educational Divides in\n  Network Structure Related to Right-Wing Populist Voting", "author": "Malte L\u00fcken and Javier Garcia-Bernardo and Sreeparna Deb and Flavio Hafner and Megha Khosla", "abstract": "  Administrative registry data can be used to construct population-scale\nnetworks whose ties reflect shared social contexts between persons. With\nmachine learning, such networks can be encoded into numerical representations\n-- embeddings -- that automatically capture individuals' position within the\nnetwork. We created embeddings for all persons in the Dutch population from a\npopulation-scale network that represents five shared contexts: neighborhood,\nwork, family, household, and school. To assess the informativeness of these\nembeddings, we used them to predict right-wing populist voting. Embeddings\nalone predicted right-wing populist voting above chance-level but performed\nworse than individual characteristics. Combining the best subset of embeddings\nwith individual characteristics only slightly improved predictions. After\ntransforming the embeddings to make their dimensions more sparse and\northogonal, we found that one embedding dimension was strongly associated with\nthe outcome. Mapping this dimension back to the population network revealed\ndifferences in network structure related to right-wing populist voting between\ndifferent school ties and achieved education levels. Our study contributes\nmethodologically by demonstrating how population-scale network embeddings can\nbe made interpretable, and substantively by linking structural network\ndifferences in education to right-wing populist voting.\n", "link": "http://arxiv.org/abs/2508.21236v2", "date": "2025-09-03", "relevancy": 2.091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4201}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4201}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Population-Scale%20Network%20Embeddings%20Expose%20Educational%20Divides%20in%0A%20%20Network%20Structure%20Related%20to%20Right-Wing%20Populist%20Voting&body=Title%3A%20Population-Scale%20Network%20Embeddings%20Expose%20Educational%20Divides%20in%0A%20%20Network%20Structure%20Related%20to%20Right-Wing%20Populist%20Voting%0AAuthor%3A%20Malte%20L%C3%BCken%20and%20Javier%20Garcia-Bernardo%20and%20Sreeparna%20Deb%20and%20Flavio%20Hafner%20and%20Megha%20Khosla%0AAbstract%3A%20%20%20Administrative%20registry%20data%20can%20be%20used%20to%20construct%20population-scale%0Anetworks%20whose%20ties%20reflect%20shared%20social%20contexts%20between%20persons.%20With%0Amachine%20learning%2C%20such%20networks%20can%20be%20encoded%20into%20numerical%20representations%0A--%20embeddings%20--%20that%20automatically%20capture%20individuals%27%20position%20within%20the%0Anetwork.%20We%20created%20embeddings%20for%20all%20persons%20in%20the%20Dutch%20population%20from%20a%0Apopulation-scale%20network%20that%20represents%20five%20shared%20contexts%3A%20neighborhood%2C%0Awork%2C%20family%2C%20household%2C%20and%20school.%20To%20assess%20the%20informativeness%20of%20these%0Aembeddings%2C%20we%20used%20them%20to%20predict%20right-wing%20populist%20voting.%20Embeddings%0Aalone%20predicted%20right-wing%20populist%20voting%20above%20chance-level%20but%20performed%0Aworse%20than%20individual%20characteristics.%20Combining%20the%20best%20subset%20of%20embeddings%0Awith%20individual%20characteristics%20only%20slightly%20improved%20predictions.%20After%0Atransforming%20the%20embeddings%20to%20make%20their%20dimensions%20more%20sparse%20and%0Aorthogonal%2C%20we%20found%20that%20one%20embedding%20dimension%20was%20strongly%20associated%20with%0Athe%20outcome.%20Mapping%20this%20dimension%20back%20to%20the%20population%20network%20revealed%0Adifferences%20in%20network%20structure%20related%20to%20right-wing%20populist%20voting%20between%0Adifferent%20school%20ties%20and%20achieved%20education%20levels.%20Our%20study%20contributes%0Amethodologically%20by%20demonstrating%20how%20population-scale%20network%20embeddings%20can%0Abe%20made%20interpretable%2C%20and%20substantively%20by%20linking%20structural%20network%0Adifferences%20in%20education%20to%20right-wing%20populist%20voting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21236v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPopulation-Scale%2520Network%2520Embeddings%2520Expose%2520Educational%2520Divides%2520in%250A%2520%2520Network%2520Structure%2520Related%2520to%2520Right-Wing%2520Populist%2520Voting%26entry.906535625%3DMalte%2520L%25C3%25BCken%2520and%2520Javier%2520Garcia-Bernardo%2520and%2520Sreeparna%2520Deb%2520and%2520Flavio%2520Hafner%2520and%2520Megha%2520Khosla%26entry.1292438233%3D%2520%2520Administrative%2520registry%2520data%2520can%2520be%2520used%2520to%2520construct%2520population-scale%250Anetworks%2520whose%2520ties%2520reflect%2520shared%2520social%2520contexts%2520between%2520persons.%2520With%250Amachine%2520learning%252C%2520such%2520networks%2520can%2520be%2520encoded%2520into%2520numerical%2520representations%250A--%2520embeddings%2520--%2520that%2520automatically%2520capture%2520individuals%2527%2520position%2520within%2520the%250Anetwork.%2520We%2520created%2520embeddings%2520for%2520all%2520persons%2520in%2520the%2520Dutch%2520population%2520from%2520a%250Apopulation-scale%2520network%2520that%2520represents%2520five%2520shared%2520contexts%253A%2520neighborhood%252C%250Awork%252C%2520family%252C%2520household%252C%2520and%2520school.%2520To%2520assess%2520the%2520informativeness%2520of%2520these%250Aembeddings%252C%2520we%2520used%2520them%2520to%2520predict%2520right-wing%2520populist%2520voting.%2520Embeddings%250Aalone%2520predicted%2520right-wing%2520populist%2520voting%2520above%2520chance-level%2520but%2520performed%250Aworse%2520than%2520individual%2520characteristics.%2520Combining%2520the%2520best%2520subset%2520of%2520embeddings%250Awith%2520individual%2520characteristics%2520only%2520slightly%2520improved%2520predictions.%2520After%250Atransforming%2520the%2520embeddings%2520to%2520make%2520their%2520dimensions%2520more%2520sparse%2520and%250Aorthogonal%252C%2520we%2520found%2520that%2520one%2520embedding%2520dimension%2520was%2520strongly%2520associated%2520with%250Athe%2520outcome.%2520Mapping%2520this%2520dimension%2520back%2520to%2520the%2520population%2520network%2520revealed%250Adifferences%2520in%2520network%2520structure%2520related%2520to%2520right-wing%2520populist%2520voting%2520between%250Adifferent%2520school%2520ties%2520and%2520achieved%2520education%2520levels.%2520Our%2520study%2520contributes%250Amethodologically%2520by%2520demonstrating%2520how%2520population-scale%2520network%2520embeddings%2520can%250Abe%2520made%2520interpretable%252C%2520and%2520substantively%2520by%2520linking%2520structural%2520network%250Adifferences%2520in%2520education%2520to%2520right-wing%2520populist%2520voting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21236v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Population-Scale%20Network%20Embeddings%20Expose%20Educational%20Divides%20in%0A%20%20Network%20Structure%20Related%20to%20Right-Wing%20Populist%20Voting&entry.906535625=Malte%20L%C3%BCken%20and%20Javier%20Garcia-Bernardo%20and%20Sreeparna%20Deb%20and%20Flavio%20Hafner%20and%20Megha%20Khosla&entry.1292438233=%20%20Administrative%20registry%20data%20can%20be%20used%20to%20construct%20population-scale%0Anetworks%20whose%20ties%20reflect%20shared%20social%20contexts%20between%20persons.%20With%0Amachine%20learning%2C%20such%20networks%20can%20be%20encoded%20into%20numerical%20representations%0A--%20embeddings%20--%20that%20automatically%20capture%20individuals%27%20position%20within%20the%0Anetwork.%20We%20created%20embeddings%20for%20all%20persons%20in%20the%20Dutch%20population%20from%20a%0Apopulation-scale%20network%20that%20represents%20five%20shared%20contexts%3A%20neighborhood%2C%0Awork%2C%20family%2C%20household%2C%20and%20school.%20To%20assess%20the%20informativeness%20of%20these%0Aembeddings%2C%20we%20used%20them%20to%20predict%20right-wing%20populist%20voting.%20Embeddings%0Aalone%20predicted%20right-wing%20populist%20voting%20above%20chance-level%20but%20performed%0Aworse%20than%20individual%20characteristics.%20Combining%20the%20best%20subset%20of%20embeddings%0Awith%20individual%20characteristics%20only%20slightly%20improved%20predictions.%20After%0Atransforming%20the%20embeddings%20to%20make%20their%20dimensions%20more%20sparse%20and%0Aorthogonal%2C%20we%20found%20that%20one%20embedding%20dimension%20was%20strongly%20associated%20with%0Athe%20outcome.%20Mapping%20this%20dimension%20back%20to%20the%20population%20network%20revealed%0Adifferences%20in%20network%20structure%20related%20to%20right-wing%20populist%20voting%20between%0Adifferent%20school%20ties%20and%20achieved%20education%20levels.%20Our%20study%20contributes%0Amethodologically%20by%20demonstrating%20how%20population-scale%20network%20embeddings%20can%0Abe%20made%20interpretable%2C%20and%20substantively%20by%20linking%20structural%20network%0Adifferences%20in%20education%20to%20right-wing%20populist%20voting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21236v2&entry.124074799=Read"},
{"title": "Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and\n  Self-Supervised Embeddings", "author": "Dyah A. M. G. Wisnu and Ryandhimas E. Zezario and Stefano Rini and Hsin-Min Wang and Yu Tsao", "abstract": "  We present a system for automatic multi-axis perceptual quality prediction of\ngenerative audio, developed for Track 2 of the AudioMOS Challenge 2025. The\ntask is to predict four Audio Aesthetic Scores--Production Quality, Production\nComplexity, Content Enjoyment, and Content Usefulness--for audio generated by\ntext-to-speech (TTS), text-to-audio (TTA), and text-to-music (TTM) systems. A\nmain challenge is the domain shift between natural training data and synthetic\nevaluation data. To address this, we combine BEATs, a pretrained\ntransformer-based audio representation model, with a multi-branch long\nshort-term memory (LSTM) predictor and use a triplet loss with buffer-based\nsampling to structure the embedding space by perceptual similarity. Our results\nshow that this improves embedding discriminability and generalization, enabling\ndomain-robust audio quality assessment without synthetic training data.\n", "link": "http://arxiv.org/abs/2509.03292v1", "date": "2025-09-03", "relevancy": 2.0906, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5241}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Perceptual%20Audio%20Aesthetic%20Assessment%20via%20Triplet%20Loss%20and%0A%20%20Self-Supervised%20Embeddings&body=Title%3A%20Improving%20Perceptual%20Audio%20Aesthetic%20Assessment%20via%20Triplet%20Loss%20and%0A%20%20Self-Supervised%20Embeddings%0AAuthor%3A%20Dyah%20A.%20M.%20G.%20Wisnu%20and%20Ryandhimas%20E.%20Zezario%20and%20Stefano%20Rini%20and%20Hsin-Min%20Wang%20and%20Yu%20Tsao%0AAbstract%3A%20%20%20We%20present%20a%20system%20for%20automatic%20multi-axis%20perceptual%20quality%20prediction%20of%0Agenerative%20audio%2C%20developed%20for%20Track%202%20of%20the%20AudioMOS%20Challenge%202025.%20The%0Atask%20is%20to%20predict%20four%20Audio%20Aesthetic%20Scores--Production%20Quality%2C%20Production%0AComplexity%2C%20Content%20Enjoyment%2C%20and%20Content%20Usefulness--for%20audio%20generated%20by%0Atext-to-speech%20%28TTS%29%2C%20text-to-audio%20%28TTA%29%2C%20and%20text-to-music%20%28TTM%29%20systems.%20A%0Amain%20challenge%20is%20the%20domain%20shift%20between%20natural%20training%20data%20and%20synthetic%0Aevaluation%20data.%20To%20address%20this%2C%20we%20combine%20BEATs%2C%20a%20pretrained%0Atransformer-based%20audio%20representation%20model%2C%20with%20a%20multi-branch%20long%0Ashort-term%20memory%20%28LSTM%29%20predictor%20and%20use%20a%20triplet%20loss%20with%20buffer-based%0Asampling%20to%20structure%20the%20embedding%20space%20by%20perceptual%20similarity.%20Our%20results%0Ashow%20that%20this%20improves%20embedding%20discriminability%20and%20generalization%2C%20enabling%0Adomain-robust%20audio%20quality%20assessment%20without%20synthetic%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Perceptual%2520Audio%2520Aesthetic%2520Assessment%2520via%2520Triplet%2520Loss%2520and%250A%2520%2520Self-Supervised%2520Embeddings%26entry.906535625%3DDyah%2520A.%2520M.%2520G.%2520Wisnu%2520and%2520Ryandhimas%2520E.%2520Zezario%2520and%2520Stefano%2520Rini%2520and%2520Hsin-Min%2520Wang%2520and%2520Yu%2520Tsao%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520system%2520for%2520automatic%2520multi-axis%2520perceptual%2520quality%2520prediction%2520of%250Agenerative%2520audio%252C%2520developed%2520for%2520Track%25202%2520of%2520the%2520AudioMOS%2520Challenge%25202025.%2520The%250Atask%2520is%2520to%2520predict%2520four%2520Audio%2520Aesthetic%2520Scores--Production%2520Quality%252C%2520Production%250AComplexity%252C%2520Content%2520Enjoyment%252C%2520and%2520Content%2520Usefulness--for%2520audio%2520generated%2520by%250Atext-to-speech%2520%2528TTS%2529%252C%2520text-to-audio%2520%2528TTA%2529%252C%2520and%2520text-to-music%2520%2528TTM%2529%2520systems.%2520A%250Amain%2520challenge%2520is%2520the%2520domain%2520shift%2520between%2520natural%2520training%2520data%2520and%2520synthetic%250Aevaluation%2520data.%2520To%2520address%2520this%252C%2520we%2520combine%2520BEATs%252C%2520a%2520pretrained%250Atransformer-based%2520audio%2520representation%2520model%252C%2520with%2520a%2520multi-branch%2520long%250Ashort-term%2520memory%2520%2528LSTM%2529%2520predictor%2520and%2520use%2520a%2520triplet%2520loss%2520with%2520buffer-based%250Asampling%2520to%2520structure%2520the%2520embedding%2520space%2520by%2520perceptual%2520similarity.%2520Our%2520results%250Ashow%2520that%2520this%2520improves%2520embedding%2520discriminability%2520and%2520generalization%252C%2520enabling%250Adomain-robust%2520audio%2520quality%2520assessment%2520without%2520synthetic%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Perceptual%20Audio%20Aesthetic%20Assessment%20via%20Triplet%20Loss%20and%0A%20%20Self-Supervised%20Embeddings&entry.906535625=Dyah%20A.%20M.%20G.%20Wisnu%20and%20Ryandhimas%20E.%20Zezario%20and%20Stefano%20Rini%20and%20Hsin-Min%20Wang%20and%20Yu%20Tsao&entry.1292438233=%20%20We%20present%20a%20system%20for%20automatic%20multi-axis%20perceptual%20quality%20prediction%20of%0Agenerative%20audio%2C%20developed%20for%20Track%202%20of%20the%20AudioMOS%20Challenge%202025.%20The%0Atask%20is%20to%20predict%20four%20Audio%20Aesthetic%20Scores--Production%20Quality%2C%20Production%0AComplexity%2C%20Content%20Enjoyment%2C%20and%20Content%20Usefulness--for%20audio%20generated%20by%0Atext-to-speech%20%28TTS%29%2C%20text-to-audio%20%28TTA%29%2C%20and%20text-to-music%20%28TTM%29%20systems.%20A%0Amain%20challenge%20is%20the%20domain%20shift%20between%20natural%20training%20data%20and%20synthetic%0Aevaluation%20data.%20To%20address%20this%2C%20we%20combine%20BEATs%2C%20a%20pretrained%0Atransformer-based%20audio%20representation%20model%2C%20with%20a%20multi-branch%20long%0Ashort-term%20memory%20%28LSTM%29%20predictor%20and%20use%20a%20triplet%20loss%20with%20buffer-based%0Asampling%20to%20structure%20the%20embedding%20space%20by%20perceptual%20similarity.%20Our%20results%0Ashow%20that%20this%20improves%20embedding%20discriminability%20and%20generalization%2C%20enabling%0Adomain-robust%20audio%20quality%20assessment%20without%20synthetic%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03292v1&entry.124074799=Read"},
{"title": "Autonomous Learning From Success and Failure: Goal-Conditioned\n  Supervised Learning with Negative Feedback", "author": "Zeqiang Zhang and Fabian Wurzberger and Gerrit Schmid and Sebastian Gottwald and Daniel A. Braun", "abstract": "  Reinforcement learning faces significant challenges when applied to tasks\ncharacterized by sparse reward structures. Although imitation learning, within\nthe domain of supervised learning, offers faster convergence, it relies heavily\non human-generated demonstrations. Recently, Goal-Conditioned Supervised\nLearning (GCSL) has emerged as a potential solution by enabling self-imitation\nlearning for autonomous systems. By strategically relabelling goals, agents can\nderive policy insights from their own experiences. Despite the successes of\nthis framework, it presents two notable limitations: (1) Learning exclusively\nfrom self-generated experiences can exacerbate the agents' inherent biases; (2)\nThe relabelling strategy allows agents to focus solely on successful outcomes,\nprecluding them from learning from their mistakes. To address these issues, we\npropose a novel model that integrates contrastive learning principles into the\nGCSL framework to learn from both success and failure. Through empirical\nevaluations, we demonstrate that our algorithm overcomes limitations imposed by\nagents' initial biases and thereby enables more exploratory behavior. This\nfacilitates the identification and adoption of effective policies, leading to\nsuperior performance across a variety of challenging environments.\n", "link": "http://arxiv.org/abs/2509.03206v1", "date": "2025-09-03", "relevancy": 2.0896, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.528}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5256}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Learning%20From%20Success%20and%20Failure%3A%20Goal-Conditioned%0A%20%20Supervised%20Learning%20with%20Negative%20Feedback&body=Title%3A%20Autonomous%20Learning%20From%20Success%20and%20Failure%3A%20Goal-Conditioned%0A%20%20Supervised%20Learning%20with%20Negative%20Feedback%0AAuthor%3A%20Zeqiang%20Zhang%20and%20Fabian%20Wurzberger%20and%20Gerrit%20Schmid%20and%20Sebastian%20Gottwald%20and%20Daniel%20A.%20Braun%0AAbstract%3A%20%20%20Reinforcement%20learning%20faces%20significant%20challenges%20when%20applied%20to%20tasks%0Acharacterized%20by%20sparse%20reward%20structures.%20Although%20imitation%20learning%2C%20within%0Athe%20domain%20of%20supervised%20learning%2C%20offers%20faster%20convergence%2C%20it%20relies%20heavily%0Aon%20human-generated%20demonstrations.%20Recently%2C%20Goal-Conditioned%20Supervised%0ALearning%20%28GCSL%29%20has%20emerged%20as%20a%20potential%20solution%20by%20enabling%20self-imitation%0Alearning%20for%20autonomous%20systems.%20By%20strategically%20relabelling%20goals%2C%20agents%20can%0Aderive%20policy%20insights%20from%20their%20own%20experiences.%20Despite%20the%20successes%20of%0Athis%20framework%2C%20it%20presents%20two%20notable%20limitations%3A%20%281%29%20Learning%20exclusively%0Afrom%20self-generated%20experiences%20can%20exacerbate%20the%20agents%27%20inherent%20biases%3B%20%282%29%0AThe%20relabelling%20strategy%20allows%20agents%20to%20focus%20solely%20on%20successful%20outcomes%2C%0Aprecluding%20them%20from%20learning%20from%20their%20mistakes.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20model%20that%20integrates%20contrastive%20learning%20principles%20into%20the%0AGCSL%20framework%20to%20learn%20from%20both%20success%20and%20failure.%20Through%20empirical%0Aevaluations%2C%20we%20demonstrate%20that%20our%20algorithm%20overcomes%20limitations%20imposed%20by%0Aagents%27%20initial%20biases%20and%20thereby%20enables%20more%20exploratory%20behavior.%20This%0Afacilitates%20the%20identification%20and%20adoption%20of%20effective%20policies%2C%20leading%20to%0Asuperior%20performance%20across%20a%20variety%20of%20challenging%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Learning%2520From%2520Success%2520and%2520Failure%253A%2520Goal-Conditioned%250A%2520%2520Supervised%2520Learning%2520with%2520Negative%2520Feedback%26entry.906535625%3DZeqiang%2520Zhang%2520and%2520Fabian%2520Wurzberger%2520and%2520Gerrit%2520Schmid%2520and%2520Sebastian%2520Gottwald%2520and%2520Daniel%2520A.%2520Braun%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520faces%2520significant%2520challenges%2520when%2520applied%2520to%2520tasks%250Acharacterized%2520by%2520sparse%2520reward%2520structures.%2520Although%2520imitation%2520learning%252C%2520within%250Athe%2520domain%2520of%2520supervised%2520learning%252C%2520offers%2520faster%2520convergence%252C%2520it%2520relies%2520heavily%250Aon%2520human-generated%2520demonstrations.%2520Recently%252C%2520Goal-Conditioned%2520Supervised%250ALearning%2520%2528GCSL%2529%2520has%2520emerged%2520as%2520a%2520potential%2520solution%2520by%2520enabling%2520self-imitation%250Alearning%2520for%2520autonomous%2520systems.%2520By%2520strategically%2520relabelling%2520goals%252C%2520agents%2520can%250Aderive%2520policy%2520insights%2520from%2520their%2520own%2520experiences.%2520Despite%2520the%2520successes%2520of%250Athis%2520framework%252C%2520it%2520presents%2520two%2520notable%2520limitations%253A%2520%25281%2529%2520Learning%2520exclusively%250Afrom%2520self-generated%2520experiences%2520can%2520exacerbate%2520the%2520agents%2527%2520inherent%2520biases%253B%2520%25282%2529%250AThe%2520relabelling%2520strategy%2520allows%2520agents%2520to%2520focus%2520solely%2520on%2520successful%2520outcomes%252C%250Aprecluding%2520them%2520from%2520learning%2520from%2520their%2520mistakes.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520novel%2520model%2520that%2520integrates%2520contrastive%2520learning%2520principles%2520into%2520the%250AGCSL%2520framework%2520to%2520learn%2520from%2520both%2520success%2520and%2520failure.%2520Through%2520empirical%250Aevaluations%252C%2520we%2520demonstrate%2520that%2520our%2520algorithm%2520overcomes%2520limitations%2520imposed%2520by%250Aagents%2527%2520initial%2520biases%2520and%2520thereby%2520enables%2520more%2520exploratory%2520behavior.%2520This%250Afacilitates%2520the%2520identification%2520and%2520adoption%2520of%2520effective%2520policies%252C%2520leading%2520to%250Asuperior%2520performance%2520across%2520a%2520variety%2520of%2520challenging%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Learning%20From%20Success%20and%20Failure%3A%20Goal-Conditioned%0A%20%20Supervised%20Learning%20with%20Negative%20Feedback&entry.906535625=Zeqiang%20Zhang%20and%20Fabian%20Wurzberger%20and%20Gerrit%20Schmid%20and%20Sebastian%20Gottwald%20and%20Daniel%20A.%20Braun&entry.1292438233=%20%20Reinforcement%20learning%20faces%20significant%20challenges%20when%20applied%20to%20tasks%0Acharacterized%20by%20sparse%20reward%20structures.%20Although%20imitation%20learning%2C%20within%0Athe%20domain%20of%20supervised%20learning%2C%20offers%20faster%20convergence%2C%20it%20relies%20heavily%0Aon%20human-generated%20demonstrations.%20Recently%2C%20Goal-Conditioned%20Supervised%0ALearning%20%28GCSL%29%20has%20emerged%20as%20a%20potential%20solution%20by%20enabling%20self-imitation%0Alearning%20for%20autonomous%20systems.%20By%20strategically%20relabelling%20goals%2C%20agents%20can%0Aderive%20policy%20insights%20from%20their%20own%20experiences.%20Despite%20the%20successes%20of%0Athis%20framework%2C%20it%20presents%20two%20notable%20limitations%3A%20%281%29%20Learning%20exclusively%0Afrom%20self-generated%20experiences%20can%20exacerbate%20the%20agents%27%20inherent%20biases%3B%20%282%29%0AThe%20relabelling%20strategy%20allows%20agents%20to%20focus%20solely%20on%20successful%20outcomes%2C%0Aprecluding%20them%20from%20learning%20from%20their%20mistakes.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20model%20that%20integrates%20contrastive%20learning%20principles%20into%20the%0AGCSL%20framework%20to%20learn%20from%20both%20success%20and%20failure.%20Through%20empirical%0Aevaluations%2C%20we%20demonstrate%20that%20our%20algorithm%20overcomes%20limitations%20imposed%20by%0Aagents%27%20initial%20biases%20and%20thereby%20enables%20more%20exploratory%20behavior.%20This%0Afacilitates%20the%20identification%20and%20adoption%20of%20effective%20policies%2C%20leading%20to%0Asuperior%20performance%20across%20a%20variety%20of%20challenging%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03206v1&entry.124074799=Read"},
{"title": "Group-in-Group Policy Optimization for LLM Agent Training", "author": "Lang Feng and Zhenghai Xue and Tingcong Liu and Bo An", "abstract": "  Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to long-horizon LLM agent training\nremains limited. Unlike static tasks, agent-environment interactions unfold\nover many steps and often yield sparse or delayed rewards, making credit\nassignment across individual steps significantly more challenging. In this\nwork, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL\nalgorithm that achieves fine-grained credit assignment for LLM agents while\npreserving the appealing properties of group-based RL: critic-free, low memory,\nand stable convergence. GiGPO introduces a two-level structure for estimating\nrelative advantage: (i) At the episode-level, GiGPO computes macro relative\nadvantages based on groups of complete trajectories; (ii) At the step-level,\nGiGPO introduces an anchor state grouping mechanism that retroactively\nconstructs step-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using\nQwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers\nfine-grained per-step credit signals and achieves performance gains of > 12\\%\non ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining\nthe same GPU memory overhead, identical LLM rollout, and incurring little to no\nadditional time cost.\n", "link": "http://arxiv.org/abs/2505.10978v2", "date": "2025-09-03", "relevancy": 2.0559, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5196}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5168}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group-in-Group%20Policy%20Optimization%20for%20LLM%20Agent%20Training&body=Title%3A%20Group-in-Group%20Policy%20Optimization%20for%20LLM%20Agent%20Training%0AAuthor%3A%20Lang%20Feng%20and%20Zhenghai%20Xue%20and%20Tingcong%20Liu%20and%20Bo%20An%0AAbstract%3A%20%20%20Recent%20advances%20in%20group-based%20reinforcement%20learning%20%28RL%29%20have%20driven%0Afrontier%20large%20language%20models%20%28LLMs%29%20in%20single-turn%20tasks%20like%20mathematical%0Areasoning.%20However%2C%20their%20scalability%20to%20long-horizon%20LLM%20agent%20training%0Aremains%20limited.%20Unlike%20static%20tasks%2C%20agent-environment%20interactions%20unfold%0Aover%20many%20steps%20and%20often%20yield%20sparse%20or%20delayed%20rewards%2C%20making%20credit%0Aassignment%20across%20individual%20steps%20significantly%20more%20challenging.%20In%20this%0Awork%2C%20we%20propose%20Group-in-Group%20Policy%20Optimization%20%28GiGPO%29%2C%20a%20novel%20RL%0Aalgorithm%20that%20achieves%20fine-grained%20credit%20assignment%20for%20LLM%20agents%20while%0Apreserving%20the%20appealing%20properties%20of%20group-based%20RL%3A%20critic-free%2C%20low%20memory%2C%0Aand%20stable%20convergence.%20GiGPO%20introduces%20a%20two-level%20structure%20for%20estimating%0Arelative%20advantage%3A%20%28i%29%20At%20the%20episode-level%2C%20GiGPO%20computes%20macro%20relative%0Aadvantages%20based%20on%20groups%20of%20complete%20trajectories%3B%20%28ii%29%20At%20the%20step-level%2C%0AGiGPO%20introduces%20an%20anchor%20state%20grouping%20mechanism%20that%20retroactively%0Aconstructs%20step-level%20groups%20by%20identifying%20repeated%20environment%20states%20across%0Atrajectories.%20Actions%20stemming%20from%20the%20same%20state%20are%20grouped%20together%2C%0Aenabling%20micro%20relative%20advantage%20estimation.%20This%20hierarchical%20structure%0Aeffectively%20captures%20both%20global%20trajectory%20quality%20and%20local%20step%0Aeffectiveness%20without%20relying%20on%20auxiliary%20models%20or%20additional%20rollouts.%20We%0Aevaluate%20GiGPO%20on%20two%20challenging%20agent%20benchmarks%2C%20ALFWorld%20and%20WebShop%2C%20using%0AQwen2.5-1.5B-Instruct%20and%20Qwen2.5-7B-Instruct.%20Crucially%2C%20GiGPO%20delivers%0Afine-grained%20per-step%20credit%20signals%20and%20achieves%20performance%20gains%20of%20%3E%2012%5C%25%0Aon%20ALFWorld%20and%20%3E%209%5C%25%20on%20WebShop%20over%20the%20GRPO%20baseline%3A%20all%20while%20maintaining%0Athe%20same%20GPU%20memory%20overhead%2C%20identical%20LLM%20rollout%2C%20and%20incurring%20little%20to%20no%0Aadditional%20time%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup-in-Group%2520Policy%2520Optimization%2520for%2520LLM%2520Agent%2520Training%26entry.906535625%3DLang%2520Feng%2520and%2520Zhenghai%2520Xue%2520and%2520Tingcong%2520Liu%2520and%2520Bo%2520An%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520group-based%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520driven%250Afrontier%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520single-turn%2520tasks%2520like%2520mathematical%250Areasoning.%2520However%252C%2520their%2520scalability%2520to%2520long-horizon%2520LLM%2520agent%2520training%250Aremains%2520limited.%2520Unlike%2520static%2520tasks%252C%2520agent-environment%2520interactions%2520unfold%250Aover%2520many%2520steps%2520and%2520often%2520yield%2520sparse%2520or%2520delayed%2520rewards%252C%2520making%2520credit%250Aassignment%2520across%2520individual%2520steps%2520significantly%2520more%2520challenging.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Group-in-Group%2520Policy%2520Optimization%2520%2528GiGPO%2529%252C%2520a%2520novel%2520RL%250Aalgorithm%2520that%2520achieves%2520fine-grained%2520credit%2520assignment%2520for%2520LLM%2520agents%2520while%250Apreserving%2520the%2520appealing%2520properties%2520of%2520group-based%2520RL%253A%2520critic-free%252C%2520low%2520memory%252C%250Aand%2520stable%2520convergence.%2520GiGPO%2520introduces%2520a%2520two-level%2520structure%2520for%2520estimating%250Arelative%2520advantage%253A%2520%2528i%2529%2520At%2520the%2520episode-level%252C%2520GiGPO%2520computes%2520macro%2520relative%250Aadvantages%2520based%2520on%2520groups%2520of%2520complete%2520trajectories%253B%2520%2528ii%2529%2520At%2520the%2520step-level%252C%250AGiGPO%2520introduces%2520an%2520anchor%2520state%2520grouping%2520mechanism%2520that%2520retroactively%250Aconstructs%2520step-level%2520groups%2520by%2520identifying%2520repeated%2520environment%2520states%2520across%250Atrajectories.%2520Actions%2520stemming%2520from%2520the%2520same%2520state%2520are%2520grouped%2520together%252C%250Aenabling%2520micro%2520relative%2520advantage%2520estimation.%2520This%2520hierarchical%2520structure%250Aeffectively%2520captures%2520both%2520global%2520trajectory%2520quality%2520and%2520local%2520step%250Aeffectiveness%2520without%2520relying%2520on%2520auxiliary%2520models%2520or%2520additional%2520rollouts.%2520We%250Aevaluate%2520GiGPO%2520on%2520two%2520challenging%2520agent%2520benchmarks%252C%2520ALFWorld%2520and%2520WebShop%252C%2520using%250AQwen2.5-1.5B-Instruct%2520and%2520Qwen2.5-7B-Instruct.%2520Crucially%252C%2520GiGPO%2520delivers%250Afine-grained%2520per-step%2520credit%2520signals%2520and%2520achieves%2520performance%2520gains%2520of%2520%253E%252012%255C%2525%250Aon%2520ALFWorld%2520and%2520%253E%25209%255C%2525%2520on%2520WebShop%2520over%2520the%2520GRPO%2520baseline%253A%2520all%2520while%2520maintaining%250Athe%2520same%2520GPU%2520memory%2520overhead%252C%2520identical%2520LLM%2520rollout%252C%2520and%2520incurring%2520little%2520to%2520no%250Aadditional%2520time%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group-in-Group%20Policy%20Optimization%20for%20LLM%20Agent%20Training&entry.906535625=Lang%20Feng%20and%20Zhenghai%20Xue%20and%20Tingcong%20Liu%20and%20Bo%20An&entry.1292438233=%20%20Recent%20advances%20in%20group-based%20reinforcement%20learning%20%28RL%29%20have%20driven%0Afrontier%20large%20language%20models%20%28LLMs%29%20in%20single-turn%20tasks%20like%20mathematical%0Areasoning.%20However%2C%20their%20scalability%20to%20long-horizon%20LLM%20agent%20training%0Aremains%20limited.%20Unlike%20static%20tasks%2C%20agent-environment%20interactions%20unfold%0Aover%20many%20steps%20and%20often%20yield%20sparse%20or%20delayed%20rewards%2C%20making%20credit%0Aassignment%20across%20individual%20steps%20significantly%20more%20challenging.%20In%20this%0Awork%2C%20we%20propose%20Group-in-Group%20Policy%20Optimization%20%28GiGPO%29%2C%20a%20novel%20RL%0Aalgorithm%20that%20achieves%20fine-grained%20credit%20assignment%20for%20LLM%20agents%20while%0Apreserving%20the%20appealing%20properties%20of%20group-based%20RL%3A%20critic-free%2C%20low%20memory%2C%0Aand%20stable%20convergence.%20GiGPO%20introduces%20a%20two-level%20structure%20for%20estimating%0Arelative%20advantage%3A%20%28i%29%20At%20the%20episode-level%2C%20GiGPO%20computes%20macro%20relative%0Aadvantages%20based%20on%20groups%20of%20complete%20trajectories%3B%20%28ii%29%20At%20the%20step-level%2C%0AGiGPO%20introduces%20an%20anchor%20state%20grouping%20mechanism%20that%20retroactively%0Aconstructs%20step-level%20groups%20by%20identifying%20repeated%20environment%20states%20across%0Atrajectories.%20Actions%20stemming%20from%20the%20same%20state%20are%20grouped%20together%2C%0Aenabling%20micro%20relative%20advantage%20estimation.%20This%20hierarchical%20structure%0Aeffectively%20captures%20both%20global%20trajectory%20quality%20and%20local%20step%0Aeffectiveness%20without%20relying%20on%20auxiliary%20models%20or%20additional%20rollouts.%20We%0Aevaluate%20GiGPO%20on%20two%20challenging%20agent%20benchmarks%2C%20ALFWorld%20and%20WebShop%2C%20using%0AQwen2.5-1.5B-Instruct%20and%20Qwen2.5-7B-Instruct.%20Crucially%2C%20GiGPO%20delivers%0Afine-grained%20per-step%20credit%20signals%20and%20achieves%20performance%20gains%20of%20%3E%2012%5C%25%0Aon%20ALFWorld%20and%20%3E%209%5C%25%20on%20WebShop%20over%20the%20GRPO%20baseline%3A%20all%20while%20maintaining%0Athe%20same%20GPU%20memory%20overhead%2C%20identical%20LLM%20rollout%2C%20and%20incurring%20little%20to%20no%0Aadditional%20time%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10978v2&entry.124074799=Read"},
{"title": "Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement\n  Learning", "author": "Raphael Trumpp and Ansgar Sch\u00e4fftlein and Mirco Theile and Marco Caccamo", "abstract": "  As image-based deep reinforcement learning tackles more challenging tasks,\nincreasing model size has become an important factor in improving performance.\nRecent studies achieved this by focusing on the parameter efficiency of scaled\nnetworks, typically using Impala-CNN, a 15-layer ResNet-inspired network, as\nthe image encoder. However, while Impala-CNN evidently outperforms older CNN\narchitectures, potential advancements in network design for deep reinforcement\nlearning-specific image encoders remain largely unexplored. We find that\nreplacing the flattening of output feature maps in Impala-CNN with global\naverage pooling leads to a notable performance improvement. This approach\noutperforms larger and more complex models in the Procgen Benchmark,\nparticularly in terms of generalization. We call our proposed encoder model\nImpoola-CNN. A decrease in the network's translation sensitivity may be central\nto this improvement, as we observe the most significant gains in games without\nagent-centered observations. Our results demonstrate that network scaling is\nnot just about increasing model size - efficient network design is also an\nessential factor. We make our code available at\nhttps://github.com/raphajaner/impoola.\n", "link": "http://arxiv.org/abs/2503.05546v2", "date": "2025-09-03", "relevancy": 2.0534, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5191}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5148}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impoola%3A%20The%20Power%20of%20Average%20Pooling%20for%20Image-Based%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20Impoola%3A%20The%20Power%20of%20Average%20Pooling%20for%20Image-Based%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Raphael%20Trumpp%20and%20Ansgar%20Sch%C3%A4fftlein%20and%20Mirco%20Theile%20and%20Marco%20Caccamo%0AAbstract%3A%20%20%20As%20image-based%20deep%20reinforcement%20learning%20tackles%20more%20challenging%20tasks%2C%0Aincreasing%20model%20size%20has%20become%20an%20important%20factor%20in%20improving%20performance.%0ARecent%20studies%20achieved%20this%20by%20focusing%20on%20the%20parameter%20efficiency%20of%20scaled%0Anetworks%2C%20typically%20using%20Impala-CNN%2C%20a%2015-layer%20ResNet-inspired%20network%2C%20as%0Athe%20image%20encoder.%20However%2C%20while%20Impala-CNN%20evidently%20outperforms%20older%20CNN%0Aarchitectures%2C%20potential%20advancements%20in%20network%20design%20for%20deep%20reinforcement%0Alearning-specific%20image%20encoders%20remain%20largely%20unexplored.%20We%20find%20that%0Areplacing%20the%20flattening%20of%20output%20feature%20maps%20in%20Impala-CNN%20with%20global%0Aaverage%20pooling%20leads%20to%20a%20notable%20performance%20improvement.%20This%20approach%0Aoutperforms%20larger%20and%20more%20complex%20models%20in%20the%20Procgen%20Benchmark%2C%0Aparticularly%20in%20terms%20of%20generalization.%20We%20call%20our%20proposed%20encoder%20model%0AImpoola-CNN.%20A%20decrease%20in%20the%20network%27s%20translation%20sensitivity%20may%20be%20central%0Ato%20this%20improvement%2C%20as%20we%20observe%20the%20most%20significant%20gains%20in%20games%20without%0Aagent-centered%20observations.%20Our%20results%20demonstrate%20that%20network%20scaling%20is%0Anot%20just%20about%20increasing%20model%20size%20-%20efficient%20network%20design%20is%20also%20an%0Aessential%20factor.%20We%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/raphajaner/impoola.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpoola%253A%2520The%2520Power%2520of%2520Average%2520Pooling%2520for%2520Image-Based%2520Deep%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DRaphael%2520Trumpp%2520and%2520Ansgar%2520Sch%25C3%25A4fftlein%2520and%2520Mirco%2520Theile%2520and%2520Marco%2520Caccamo%26entry.1292438233%3D%2520%2520As%2520image-based%2520deep%2520reinforcement%2520learning%2520tackles%2520more%2520challenging%2520tasks%252C%250Aincreasing%2520model%2520size%2520has%2520become%2520an%2520important%2520factor%2520in%2520improving%2520performance.%250ARecent%2520studies%2520achieved%2520this%2520by%2520focusing%2520on%2520the%2520parameter%2520efficiency%2520of%2520scaled%250Anetworks%252C%2520typically%2520using%2520Impala-CNN%252C%2520a%252015-layer%2520ResNet-inspired%2520network%252C%2520as%250Athe%2520image%2520encoder.%2520However%252C%2520while%2520Impala-CNN%2520evidently%2520outperforms%2520older%2520CNN%250Aarchitectures%252C%2520potential%2520advancements%2520in%2520network%2520design%2520for%2520deep%2520reinforcement%250Alearning-specific%2520image%2520encoders%2520remain%2520largely%2520unexplored.%2520We%2520find%2520that%250Areplacing%2520the%2520flattening%2520of%2520output%2520feature%2520maps%2520in%2520Impala-CNN%2520with%2520global%250Aaverage%2520pooling%2520leads%2520to%2520a%2520notable%2520performance%2520improvement.%2520This%2520approach%250Aoutperforms%2520larger%2520and%2520more%2520complex%2520models%2520in%2520the%2520Procgen%2520Benchmark%252C%250Aparticularly%2520in%2520terms%2520of%2520generalization.%2520We%2520call%2520our%2520proposed%2520encoder%2520model%250AImpoola-CNN.%2520A%2520decrease%2520in%2520the%2520network%2527s%2520translation%2520sensitivity%2520may%2520be%2520central%250Ato%2520this%2520improvement%252C%2520as%2520we%2520observe%2520the%2520most%2520significant%2520gains%2520in%2520games%2520without%250Aagent-centered%2520observations.%2520Our%2520results%2520demonstrate%2520that%2520network%2520scaling%2520is%250Anot%2520just%2520about%2520increasing%2520model%2520size%2520-%2520efficient%2520network%2520design%2520is%2520also%2520an%250Aessential%2520factor.%2520We%2520make%2520our%2520code%2520available%2520at%250Ahttps%253A//github.com/raphajaner/impoola.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impoola%3A%20The%20Power%20of%20Average%20Pooling%20for%20Image-Based%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Raphael%20Trumpp%20and%20Ansgar%20Sch%C3%A4fftlein%20and%20Mirco%20Theile%20and%20Marco%20Caccamo&entry.1292438233=%20%20As%20image-based%20deep%20reinforcement%20learning%20tackles%20more%20challenging%20tasks%2C%0Aincreasing%20model%20size%20has%20become%20an%20important%20factor%20in%20improving%20performance.%0ARecent%20studies%20achieved%20this%20by%20focusing%20on%20the%20parameter%20efficiency%20of%20scaled%0Anetworks%2C%20typically%20using%20Impala-CNN%2C%20a%2015-layer%20ResNet-inspired%20network%2C%20as%0Athe%20image%20encoder.%20However%2C%20while%20Impala-CNN%20evidently%20outperforms%20older%20CNN%0Aarchitectures%2C%20potential%20advancements%20in%20network%20design%20for%20deep%20reinforcement%0Alearning-specific%20image%20encoders%20remain%20largely%20unexplored.%20We%20find%20that%0Areplacing%20the%20flattening%20of%20output%20feature%20maps%20in%20Impala-CNN%20with%20global%0Aaverage%20pooling%20leads%20to%20a%20notable%20performance%20improvement.%20This%20approach%0Aoutperforms%20larger%20and%20more%20complex%20models%20in%20the%20Procgen%20Benchmark%2C%0Aparticularly%20in%20terms%20of%20generalization.%20We%20call%20our%20proposed%20encoder%20model%0AImpoola-CNN.%20A%20decrease%20in%20the%20network%27s%20translation%20sensitivity%20may%20be%20central%0Ato%20this%20improvement%2C%20as%20we%20observe%20the%20most%20significant%20gains%20in%20games%20without%0Aagent-centered%20observations.%20Our%20results%20demonstrate%20that%20network%20scaling%20is%0Anot%20just%20about%20increasing%20model%20size%20-%20efficient%20network%20design%20is%20also%20an%0Aessential%20factor.%20We%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/raphajaner/impoola.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05546v2&entry.124074799=Read"},
{"title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning", "author": "Carlos G\u00fcemes-Palau and Miquel Ferriol-Galm\u00e9s and Jordi Paillisse-Vilanova and Albert L\u00f3pez-Bresc\u00f3 and Pere Barlet-Ros and Albert Cabellos-Aparicio", "abstract": "  Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators.\n", "link": "http://arxiv.org/abs/2501.08848v2", "date": "2025-09-03", "relevancy": 2.036, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5171}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5122}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RouteNet-Gauss%3A%20Hardware-Enhanced%20Network%20Modeling%20with%20Machine%20Learning&body=Title%3A%20RouteNet-Gauss%3A%20Hardware-Enhanced%20Network%20Modeling%20with%20Machine%20Learning%0AAuthor%3A%20Carlos%20G%C3%BCemes-Palau%20and%20Miquel%20Ferriol-Galm%C3%A9s%20and%20Jordi%20Paillisse-Vilanova%20and%20Albert%20L%C3%B3pez-Bresc%C3%B3%20and%20Pere%20Barlet-Ros%20and%20Albert%20Cabellos-Aparicio%0AAbstract%3A%20%20%20Network%20simulation%20is%20pivotal%20in%20network%20modeling%2C%20assisting%20with%20tasks%0Aranging%20from%20capacity%20planning%20to%20performance%20estimation.%20Traditional%0Aapproaches%20such%20as%20Discrete%20Event%20Simulation%20%28DES%29%20face%20limitations%20in%20terms%20of%0Acomputational%20cost%20and%20accuracy.%20This%20paper%20introduces%20RouteNet-Gauss%2C%20a%20novel%0Aintegration%20of%20a%20testbed%20network%20with%20a%20Machine%20Learning%20%28ML%29%20model%20to%20address%0Athese%20challenges.%20By%20using%20the%20testbed%20as%20a%20hardware%20accelerator%2C%0ARouteNet-Gauss%20generates%20training%20datasets%20rapidly%20and%20simulates%20network%0Ascenarios%20with%20high%20fidelity%20to%20real-world%20conditions.%20Experimental%20results%0Ashow%20that%20RouteNet-Gauss%20significantly%20reduces%20prediction%20errors%20by%20up%20to%2095%25%0Aand%20achieves%20a%20488x%20speedup%20in%20inference%20time%20compared%20to%20state-of-the-art%0ADES-based%20methods.%20RouteNet-Gauss%27s%20modular%20architecture%20is%20dynamically%0Aconstructed%20based%20on%20the%20specific%20characteristics%20of%20the%20network%20scenario%2C%20such%0Aas%20topology%20and%20routing.%20This%20enables%20it%20to%20understand%20and%20generalize%20to%0Adifferent%20network%20configurations%20beyond%20those%20seen%20during%20training%2C%20including%0Anetworks%20up%20to%2010x%20larger.%20Additionally%2C%20it%20supports%20Temporal%20Aggregated%0APerformance%20Estimation%20%28TAPE%29%2C%20providing%20configurable%20temporal%20granularity%20and%0Amaintaining%20high%20accuracy%20in%20flow%20performance%20metrics.%20This%20approach%20shows%0Apromise%20in%20improving%20both%20simulation%20efficiency%20and%20accuracy%2C%20offering%20a%0Avaluable%20tool%20for%20network%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRouteNet-Gauss%253A%2520Hardware-Enhanced%2520Network%2520Modeling%2520with%2520Machine%2520Learning%26entry.906535625%3DCarlos%2520G%25C3%25BCemes-Palau%2520and%2520Miquel%2520Ferriol-Galm%25C3%25A9s%2520and%2520Jordi%2520Paillisse-Vilanova%2520and%2520Albert%2520L%25C3%25B3pez-Bresc%25C3%25B3%2520and%2520Pere%2520Barlet-Ros%2520and%2520Albert%2520Cabellos-Aparicio%26entry.1292438233%3D%2520%2520Network%2520simulation%2520is%2520pivotal%2520in%2520network%2520modeling%252C%2520assisting%2520with%2520tasks%250Aranging%2520from%2520capacity%2520planning%2520to%2520performance%2520estimation.%2520Traditional%250Aapproaches%2520such%2520as%2520Discrete%2520Event%2520Simulation%2520%2528DES%2529%2520face%2520limitations%2520in%2520terms%2520of%250Acomputational%2520cost%2520and%2520accuracy.%2520This%2520paper%2520introduces%2520RouteNet-Gauss%252C%2520a%2520novel%250Aintegration%2520of%2520a%2520testbed%2520network%2520with%2520a%2520Machine%2520Learning%2520%2528ML%2529%2520model%2520to%2520address%250Athese%2520challenges.%2520By%2520using%2520the%2520testbed%2520as%2520a%2520hardware%2520accelerator%252C%250ARouteNet-Gauss%2520generates%2520training%2520datasets%2520rapidly%2520and%2520simulates%2520network%250Ascenarios%2520with%2520high%2520fidelity%2520to%2520real-world%2520conditions.%2520Experimental%2520results%250Ashow%2520that%2520RouteNet-Gauss%2520significantly%2520reduces%2520prediction%2520errors%2520by%2520up%2520to%252095%2525%250Aand%2520achieves%2520a%2520488x%2520speedup%2520in%2520inference%2520time%2520compared%2520to%2520state-of-the-art%250ADES-based%2520methods.%2520RouteNet-Gauss%2527s%2520modular%2520architecture%2520is%2520dynamically%250Aconstructed%2520based%2520on%2520the%2520specific%2520characteristics%2520of%2520the%2520network%2520scenario%252C%2520such%250Aas%2520topology%2520and%2520routing.%2520This%2520enables%2520it%2520to%2520understand%2520and%2520generalize%2520to%250Adifferent%2520network%2520configurations%2520beyond%2520those%2520seen%2520during%2520training%252C%2520including%250Anetworks%2520up%2520to%252010x%2520larger.%2520Additionally%252C%2520it%2520supports%2520Temporal%2520Aggregated%250APerformance%2520Estimation%2520%2528TAPE%2529%252C%2520providing%2520configurable%2520temporal%2520granularity%2520and%250Amaintaining%2520high%2520accuracy%2520in%2520flow%2520performance%2520metrics.%2520This%2520approach%2520shows%250Apromise%2520in%2520improving%2520both%2520simulation%2520efficiency%2520and%2520accuracy%252C%2520offering%2520a%250Avaluable%2520tool%2520for%2520network%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RouteNet-Gauss%3A%20Hardware-Enhanced%20Network%20Modeling%20with%20Machine%20Learning&entry.906535625=Carlos%20G%C3%BCemes-Palau%20and%20Miquel%20Ferriol-Galm%C3%A9s%20and%20Jordi%20Paillisse-Vilanova%20and%20Albert%20L%C3%B3pez-Bresc%C3%B3%20and%20Pere%20Barlet-Ros%20and%20Albert%20Cabellos-Aparicio&entry.1292438233=%20%20Network%20simulation%20is%20pivotal%20in%20network%20modeling%2C%20assisting%20with%20tasks%0Aranging%20from%20capacity%20planning%20to%20performance%20estimation.%20Traditional%0Aapproaches%20such%20as%20Discrete%20Event%20Simulation%20%28DES%29%20face%20limitations%20in%20terms%20of%0Acomputational%20cost%20and%20accuracy.%20This%20paper%20introduces%20RouteNet-Gauss%2C%20a%20novel%0Aintegration%20of%20a%20testbed%20network%20with%20a%20Machine%20Learning%20%28ML%29%20model%20to%20address%0Athese%20challenges.%20By%20using%20the%20testbed%20as%20a%20hardware%20accelerator%2C%0ARouteNet-Gauss%20generates%20training%20datasets%20rapidly%20and%20simulates%20network%0Ascenarios%20with%20high%20fidelity%20to%20real-world%20conditions.%20Experimental%20results%0Ashow%20that%20RouteNet-Gauss%20significantly%20reduces%20prediction%20errors%20by%20up%20to%2095%25%0Aand%20achieves%20a%20488x%20speedup%20in%20inference%20time%20compared%20to%20state-of-the-art%0ADES-based%20methods.%20RouteNet-Gauss%27s%20modular%20architecture%20is%20dynamically%0Aconstructed%20based%20on%20the%20specific%20characteristics%20of%20the%20network%20scenario%2C%20such%0Aas%20topology%20and%20routing.%20This%20enables%20it%20to%20understand%20and%20generalize%20to%0Adifferent%20network%20configurations%20beyond%20those%20seen%20during%20training%2C%20including%0Anetworks%20up%20to%2010x%20larger.%20Additionally%2C%20it%20supports%20Temporal%20Aggregated%0APerformance%20Estimation%20%28TAPE%29%2C%20providing%20configurable%20temporal%20granularity%20and%0Amaintaining%20high%20accuracy%20in%20flow%20performance%20metrics.%20This%20approach%20shows%0Apromise%20in%20improving%20both%20simulation%20efficiency%20and%20accuracy%2C%20offering%20a%0Avaluable%20tool%20for%20network%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08848v2&entry.124074799=Read"},
{"title": "LINKER: Learning Interactions Between Functional Groups and Residues\n  With Chemical Knowledge-Enhanced Reasoning and Explainability", "author": "Phuc Pham and Viet Thanh Duy Nguyen and Truong-Son Hy", "abstract": "  Accurate identification of interactions between protein residues and ligand\nfunctional groups is essential to understand molecular recognition and guide\nrational drug design. Existing deep learning approaches for protein-ligand\ninterpretability often rely on 3D structural input or use distance-based\ncontact labels, limiting both their applicability and biological relevance. We\nintroduce LINKER, the first sequence-based model to predict residue-functional\ngroup interactions in terms of biologically defined interaction types, using\nonly protein sequences and the ligand SMILES as input. LINKER is trained with\nstructure-supervised attention, where interaction labels are derived from 3D\nprotein-ligand complexes via functional group-based motif extraction. By\nabstracting ligand structures into functional groups, the model focuses on\nchemically meaningful substructures while predicting interaction types rather\nthan mere spatial proximity. Crucially, LINKER requires only sequence-level\ninput at inference time, enabling large-scale application in settings where\nstructural data is unavailable. Experiments on the LP-PDBBind benchmark\ndemonstrate that structure-informed supervision over functional group\nabstractions yields interaction predictions closely aligned with ground-truth\nbiochemical annotations.\n", "link": "http://arxiv.org/abs/2509.03425v1", "date": "2025-09-03", "relevancy": 2.0359, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LINKER%3A%20Learning%20Interactions%20Between%20Functional%20Groups%20and%20Residues%0A%20%20With%20Chemical%20Knowledge-Enhanced%20Reasoning%20and%20Explainability&body=Title%3A%20LINKER%3A%20Learning%20Interactions%20Between%20Functional%20Groups%20and%20Residues%0A%20%20With%20Chemical%20Knowledge-Enhanced%20Reasoning%20and%20Explainability%0AAuthor%3A%20Phuc%20Pham%20and%20Viet%20Thanh%20Duy%20Nguyen%20and%20Truong-Son%20Hy%0AAbstract%3A%20%20%20Accurate%20identification%20of%20interactions%20between%20protein%20residues%20and%20ligand%0Afunctional%20groups%20is%20essential%20to%20understand%20molecular%20recognition%20and%20guide%0Arational%20drug%20design.%20Existing%20deep%20learning%20approaches%20for%20protein-ligand%0Ainterpretability%20often%20rely%20on%203D%20structural%20input%20or%20use%20distance-based%0Acontact%20labels%2C%20limiting%20both%20their%20applicability%20and%20biological%20relevance.%20We%0Aintroduce%20LINKER%2C%20the%20first%20sequence-based%20model%20to%20predict%20residue-functional%0Agroup%20interactions%20in%20terms%20of%20biologically%20defined%20interaction%20types%2C%20using%0Aonly%20protein%20sequences%20and%20the%20ligand%20SMILES%20as%20input.%20LINKER%20is%20trained%20with%0Astructure-supervised%20attention%2C%20where%20interaction%20labels%20are%20derived%20from%203D%0Aprotein-ligand%20complexes%20via%20functional%20group-based%20motif%20extraction.%20By%0Aabstracting%20ligand%20structures%20into%20functional%20groups%2C%20the%20model%20focuses%20on%0Achemically%20meaningful%20substructures%20while%20predicting%20interaction%20types%20rather%0Athan%20mere%20spatial%20proximity.%20Crucially%2C%20LINKER%20requires%20only%20sequence-level%0Ainput%20at%20inference%20time%2C%20enabling%20large-scale%20application%20in%20settings%20where%0Astructural%20data%20is%20unavailable.%20Experiments%20on%20the%20LP-PDBBind%20benchmark%0Ademonstrate%20that%20structure-informed%20supervision%20over%20functional%20group%0Aabstractions%20yields%20interaction%20predictions%20closely%20aligned%20with%20ground-truth%0Abiochemical%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLINKER%253A%2520Learning%2520Interactions%2520Between%2520Functional%2520Groups%2520and%2520Residues%250A%2520%2520With%2520Chemical%2520Knowledge-Enhanced%2520Reasoning%2520and%2520Explainability%26entry.906535625%3DPhuc%2520Pham%2520and%2520Viet%2520Thanh%2520Duy%2520Nguyen%2520and%2520Truong-Son%2520Hy%26entry.1292438233%3D%2520%2520Accurate%2520identification%2520of%2520interactions%2520between%2520protein%2520residues%2520and%2520ligand%250Afunctional%2520groups%2520is%2520essential%2520to%2520understand%2520molecular%2520recognition%2520and%2520guide%250Arational%2520drug%2520design.%2520Existing%2520deep%2520learning%2520approaches%2520for%2520protein-ligand%250Ainterpretability%2520often%2520rely%2520on%25203D%2520structural%2520input%2520or%2520use%2520distance-based%250Acontact%2520labels%252C%2520limiting%2520both%2520their%2520applicability%2520and%2520biological%2520relevance.%2520We%250Aintroduce%2520LINKER%252C%2520the%2520first%2520sequence-based%2520model%2520to%2520predict%2520residue-functional%250Agroup%2520interactions%2520in%2520terms%2520of%2520biologically%2520defined%2520interaction%2520types%252C%2520using%250Aonly%2520protein%2520sequences%2520and%2520the%2520ligand%2520SMILES%2520as%2520input.%2520LINKER%2520is%2520trained%2520with%250Astructure-supervised%2520attention%252C%2520where%2520interaction%2520labels%2520are%2520derived%2520from%25203D%250Aprotein-ligand%2520complexes%2520via%2520functional%2520group-based%2520motif%2520extraction.%2520By%250Aabstracting%2520ligand%2520structures%2520into%2520functional%2520groups%252C%2520the%2520model%2520focuses%2520on%250Achemically%2520meaningful%2520substructures%2520while%2520predicting%2520interaction%2520types%2520rather%250Athan%2520mere%2520spatial%2520proximity.%2520Crucially%252C%2520LINKER%2520requires%2520only%2520sequence-level%250Ainput%2520at%2520inference%2520time%252C%2520enabling%2520large-scale%2520application%2520in%2520settings%2520where%250Astructural%2520data%2520is%2520unavailable.%2520Experiments%2520on%2520the%2520LP-PDBBind%2520benchmark%250Ademonstrate%2520that%2520structure-informed%2520supervision%2520over%2520functional%2520group%250Aabstractions%2520yields%2520interaction%2520predictions%2520closely%2520aligned%2520with%2520ground-truth%250Abiochemical%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LINKER%3A%20Learning%20Interactions%20Between%20Functional%20Groups%20and%20Residues%0A%20%20With%20Chemical%20Knowledge-Enhanced%20Reasoning%20and%20Explainability&entry.906535625=Phuc%20Pham%20and%20Viet%20Thanh%20Duy%20Nguyen%20and%20Truong-Son%20Hy&entry.1292438233=%20%20Accurate%20identification%20of%20interactions%20between%20protein%20residues%20and%20ligand%0Afunctional%20groups%20is%20essential%20to%20understand%20molecular%20recognition%20and%20guide%0Arational%20drug%20design.%20Existing%20deep%20learning%20approaches%20for%20protein-ligand%0Ainterpretability%20often%20rely%20on%203D%20structural%20input%20or%20use%20distance-based%0Acontact%20labels%2C%20limiting%20both%20their%20applicability%20and%20biological%20relevance.%20We%0Aintroduce%20LINKER%2C%20the%20first%20sequence-based%20model%20to%20predict%20residue-functional%0Agroup%20interactions%20in%20terms%20of%20biologically%20defined%20interaction%20types%2C%20using%0Aonly%20protein%20sequences%20and%20the%20ligand%20SMILES%20as%20input.%20LINKER%20is%20trained%20with%0Astructure-supervised%20attention%2C%20where%20interaction%20labels%20are%20derived%20from%203D%0Aprotein-ligand%20complexes%20via%20functional%20group-based%20motif%20extraction.%20By%0Aabstracting%20ligand%20structures%20into%20functional%20groups%2C%20the%20model%20focuses%20on%0Achemically%20meaningful%20substructures%20while%20predicting%20interaction%20types%20rather%0Athan%20mere%20spatial%20proximity.%20Crucially%2C%20LINKER%20requires%20only%20sequence-level%0Ainput%20at%20inference%20time%2C%20enabling%20large-scale%20application%20in%20settings%20where%0Astructural%20data%20is%20unavailable.%20Experiments%20on%20the%20LP-PDBBind%20benchmark%0Ademonstrate%20that%20structure-informed%20supervision%20over%20functional%20group%0Aabstractions%20yields%20interaction%20predictions%20closely%20aligned%20with%20ground-truth%0Abiochemical%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03425v1&entry.124074799=Read"},
{"title": "Controlling Deformable Objects with Non-negligible Dynamics: a\n  Shape-Regulation Approach to End-Point Positioning", "author": "Sebastien Tiburzio and Tom\u00e1s Coleman and Daniel Feliu-Talegon and Cosimo Della Santina", "abstract": "  Model-based manipulation of deformable objects has traditionally dealt with\nobjects while neglecting their dynamics, thus mostly focusing on very\nlightweight objects at steady state. At the same time, soft robotic research\nhas made considerable strides toward general modeling and control, despite soft\nrobots and deformable objects being very similar from a mechanical standpoint.\nIn this work, we leverage these recent results to develop a control-oriented,\nfully dynamic framework of slender deformable objects grasped at one end by a\nrobotic manipulator. We introduce a dynamic model of this system using\nfunctional strain parameterizations and describe the manipulation challenge as\na regulation control problem. This enables us to define a fully model-based\ncontrol architecture, for which we can prove analytically closed-loop stability\nand provide sufficient conditions for steady state convergence to the desired\nstate. The nature of this work is intended to be markedly experimental. We\nprovide an extensive experimental validation of the proposed ideas, tasking a\nrobot arm with controlling the distal end of six different cables, in a given\nplanar position and orientation in space.\n", "link": "http://arxiv.org/abs/2402.16114v2", "date": "2025-09-03", "relevancy": 2.0352, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5231}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5207}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20Deformable%20Objects%20with%20Non-negligible%20Dynamics%3A%20a%0A%20%20Shape-Regulation%20Approach%20to%20End-Point%20Positioning&body=Title%3A%20Controlling%20Deformable%20Objects%20with%20Non-negligible%20Dynamics%3A%20a%0A%20%20Shape-Regulation%20Approach%20to%20End-Point%20Positioning%0AAuthor%3A%20Sebastien%20Tiburzio%20and%20Tom%C3%A1s%20Coleman%20and%20Daniel%20Feliu-Talegon%20and%20Cosimo%20Della%20Santina%0AAbstract%3A%20%20%20Model-based%20manipulation%20of%20deformable%20objects%20has%20traditionally%20dealt%20with%0Aobjects%20while%20neglecting%20their%20dynamics%2C%20thus%20mostly%20focusing%20on%20very%0Alightweight%20objects%20at%20steady%20state.%20At%20the%20same%20time%2C%20soft%20robotic%20research%0Ahas%20made%20considerable%20strides%20toward%20general%20modeling%20and%20control%2C%20despite%20soft%0Arobots%20and%20deformable%20objects%20being%20very%20similar%20from%20a%20mechanical%20standpoint.%0AIn%20this%20work%2C%20we%20leverage%20these%20recent%20results%20to%20develop%20a%20control-oriented%2C%0Afully%20dynamic%20framework%20of%20slender%20deformable%20objects%20grasped%20at%20one%20end%20by%20a%0Arobotic%20manipulator.%20We%20introduce%20a%20dynamic%20model%20of%20this%20system%20using%0Afunctional%20strain%20parameterizations%20and%20describe%20the%20manipulation%20challenge%20as%0Aa%20regulation%20control%20problem.%20This%20enables%20us%20to%20define%20a%20fully%20model-based%0Acontrol%20architecture%2C%20for%20which%20we%20can%20prove%20analytically%20closed-loop%20stability%0Aand%20provide%20sufficient%20conditions%20for%20steady%20state%20convergence%20to%20the%20desired%0Astate.%20The%20nature%20of%20this%20work%20is%20intended%20to%20be%20markedly%20experimental.%20We%0Aprovide%20an%20extensive%20experimental%20validation%20of%20the%20proposed%20ideas%2C%20tasking%20a%0Arobot%20arm%20with%20controlling%20the%20distal%20end%20of%20six%20different%20cables%2C%20in%20a%20given%0Aplanar%20position%20and%20orientation%20in%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520Deformable%2520Objects%2520with%2520Non-negligible%2520Dynamics%253A%2520a%250A%2520%2520Shape-Regulation%2520Approach%2520to%2520End-Point%2520Positioning%26entry.906535625%3DSebastien%2520Tiburzio%2520and%2520Tom%25C3%25A1s%2520Coleman%2520and%2520Daniel%2520Feliu-Talegon%2520and%2520Cosimo%2520Della%2520Santina%26entry.1292438233%3D%2520%2520Model-based%2520manipulation%2520of%2520deformable%2520objects%2520has%2520traditionally%2520dealt%2520with%250Aobjects%2520while%2520neglecting%2520their%2520dynamics%252C%2520thus%2520mostly%2520focusing%2520on%2520very%250Alightweight%2520objects%2520at%2520steady%2520state.%2520At%2520the%2520same%2520time%252C%2520soft%2520robotic%2520research%250Ahas%2520made%2520considerable%2520strides%2520toward%2520general%2520modeling%2520and%2520control%252C%2520despite%2520soft%250Arobots%2520and%2520deformable%2520objects%2520being%2520very%2520similar%2520from%2520a%2520mechanical%2520standpoint.%250AIn%2520this%2520work%252C%2520we%2520leverage%2520these%2520recent%2520results%2520to%2520develop%2520a%2520control-oriented%252C%250Afully%2520dynamic%2520framework%2520of%2520slender%2520deformable%2520objects%2520grasped%2520at%2520one%2520end%2520by%2520a%250Arobotic%2520manipulator.%2520We%2520introduce%2520a%2520dynamic%2520model%2520of%2520this%2520system%2520using%250Afunctional%2520strain%2520parameterizations%2520and%2520describe%2520the%2520manipulation%2520challenge%2520as%250Aa%2520regulation%2520control%2520problem.%2520This%2520enables%2520us%2520to%2520define%2520a%2520fully%2520model-based%250Acontrol%2520architecture%252C%2520for%2520which%2520we%2520can%2520prove%2520analytically%2520closed-loop%2520stability%250Aand%2520provide%2520sufficient%2520conditions%2520for%2520steady%2520state%2520convergence%2520to%2520the%2520desired%250Astate.%2520The%2520nature%2520of%2520this%2520work%2520is%2520intended%2520to%2520be%2520markedly%2520experimental.%2520We%250Aprovide%2520an%2520extensive%2520experimental%2520validation%2520of%2520the%2520proposed%2520ideas%252C%2520tasking%2520a%250Arobot%2520arm%2520with%2520controlling%2520the%2520distal%2520end%2520of%2520six%2520different%2520cables%252C%2520in%2520a%2520given%250Aplanar%2520position%2520and%2520orientation%2520in%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Deformable%20Objects%20with%20Non-negligible%20Dynamics%3A%20a%0A%20%20Shape-Regulation%20Approach%20to%20End-Point%20Positioning&entry.906535625=Sebastien%20Tiburzio%20and%20Tom%C3%A1s%20Coleman%20and%20Daniel%20Feliu-Talegon%20and%20Cosimo%20Della%20Santina&entry.1292438233=%20%20Model-based%20manipulation%20of%20deformable%20objects%20has%20traditionally%20dealt%20with%0Aobjects%20while%20neglecting%20their%20dynamics%2C%20thus%20mostly%20focusing%20on%20very%0Alightweight%20objects%20at%20steady%20state.%20At%20the%20same%20time%2C%20soft%20robotic%20research%0Ahas%20made%20considerable%20strides%20toward%20general%20modeling%20and%20control%2C%20despite%20soft%0Arobots%20and%20deformable%20objects%20being%20very%20similar%20from%20a%20mechanical%20standpoint.%0AIn%20this%20work%2C%20we%20leverage%20these%20recent%20results%20to%20develop%20a%20control-oriented%2C%0Afully%20dynamic%20framework%20of%20slender%20deformable%20objects%20grasped%20at%20one%20end%20by%20a%0Arobotic%20manipulator.%20We%20introduce%20a%20dynamic%20model%20of%20this%20system%20using%0Afunctional%20strain%20parameterizations%20and%20describe%20the%20manipulation%20challenge%20as%0Aa%20regulation%20control%20problem.%20This%20enables%20us%20to%20define%20a%20fully%20model-based%0Acontrol%20architecture%2C%20for%20which%20we%20can%20prove%20analytically%20closed-loop%20stability%0Aand%20provide%20sufficient%20conditions%20for%20steady%20state%20convergence%20to%20the%20desired%0Astate.%20The%20nature%20of%20this%20work%20is%20intended%20to%20be%20markedly%20experimental.%20We%0Aprovide%20an%20extensive%20experimental%20validation%20of%20the%20proposed%20ideas%2C%20tasking%20a%0Arobot%20arm%20with%20controlling%20the%20distal%20end%20of%20six%20different%20cables%2C%20in%20a%20given%0Aplanar%20position%20and%20orientation%20in%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16114v2&entry.124074799=Read"},
{"title": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions\n  to Generative Model's Response for Vulnerability Analysis", "author": "Reza Fayyazi and Michael Zuzak and Shanchieh Jay Yang", "abstract": "  Large Language Models (LLMs) are increasingly used for cybersecurity threat\nanalysis, but their deployment in security-sensitive environments raises trust\nand safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual\nanalysis is infeasible, making scalable and verifiable AI support critical.\nWhen querying LLMs, dealing with emerging vulnerabilities is challenging as\nthey have a training cut-off date. While Retrieval-Augmented Generation (RAG)\ncan inject up-to-date context to alleviate the cut-off date limitation, it\nremains unclear how much LLMs rely on retrieved evidence versus the model's\ninternal knowledge, and whether the retrieved information is meaningful or even\ncorrect. This uncertainty could mislead security analysts, mis-prioritize\npatches, and increase security risks. Therefore, this work proposes LLM\nEmbedding-based Attribution (LEA) to analyze the generated responses for\nvulnerability exploitation analysis. More specifically, LEA quantifies the\nrelative contribution of internal knowledge vs. retrieved content in the\ngenerated responses. We evaluate LEA on 500 critical vulnerabilities disclosed\nbetween 2016 and 2025, across three RAG settings -- valid, generic, and\nincorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's\nability to detect clear distinctions between non-retrieval, generic-retrieval,\nand valid-retrieval scenarios with over 95% accuracy on larger models. Finally,\nwe demonstrate the limitations posed by incorrect retrieval of vulnerability\ninformation and raise a cautionary note to the cybersecurity community\nregarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA\noffers security analysts with a metric to audit RAG-enhanced workflows,\nimproving the transparent and trustworthy deployment of AI in cybersecurity\nthreat analysis.\n", "link": "http://arxiv.org/abs/2506.12100v2", "date": "2025-09-03", "relevancy": 2.03, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Embedding-based%20Attribution%20%28LEA%29%3A%20Quantifying%20Source%20Contributions%0A%20%20to%20Generative%20Model%27s%20Response%20for%20Vulnerability%20Analysis&body=Title%3A%20LLM%20Embedding-based%20Attribution%20%28LEA%29%3A%20Quantifying%20Source%20Contributions%0A%20%20to%20Generative%20Model%27s%20Response%20for%20Vulnerability%20Analysis%0AAuthor%3A%20Reza%20Fayyazi%20and%20Michael%20Zuzak%20and%20Shanchieh%20Jay%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20for%20cybersecurity%20threat%0Aanalysis%2C%20but%20their%20deployment%20in%20security-sensitive%20environments%20raises%20trust%0Aand%20safety%20concerns.%20With%20over%2021%2C000%20vulnerabilities%20disclosed%20in%202025%2C%20manual%0Aanalysis%20is%20infeasible%2C%20making%20scalable%20and%20verifiable%20AI%20support%20critical.%0AWhen%20querying%20LLMs%2C%20dealing%20with%20emerging%20vulnerabilities%20is%20challenging%20as%0Athey%20have%20a%20training%20cut-off%20date.%20While%20Retrieval-Augmented%20Generation%20%28RAG%29%0Acan%20inject%20up-to-date%20context%20to%20alleviate%20the%20cut-off%20date%20limitation%2C%20it%0Aremains%20unclear%20how%20much%20LLMs%20rely%20on%20retrieved%20evidence%20versus%20the%20model%27s%0Ainternal%20knowledge%2C%20and%20whether%20the%20retrieved%20information%20is%20meaningful%20or%20even%0Acorrect.%20This%20uncertainty%20could%20mislead%20security%20analysts%2C%20mis-prioritize%0Apatches%2C%20and%20increase%20security%20risks.%20Therefore%2C%20this%20work%20proposes%20LLM%0AEmbedding-based%20Attribution%20%28LEA%29%20to%20analyze%20the%20generated%20responses%20for%0Avulnerability%20exploitation%20analysis.%20More%20specifically%2C%20LEA%20quantifies%20the%0Arelative%20contribution%20of%20internal%20knowledge%20vs.%20retrieved%20content%20in%20the%0Agenerated%20responses.%20We%20evaluate%20LEA%20on%20500%20critical%20vulnerabilities%20disclosed%0Abetween%202016%20and%202025%2C%20across%20three%20RAG%20settings%20--%20valid%2C%20generic%2C%20and%0Aincorrect%20--%20using%20three%20state-of-the-art%20LLMs.%20Our%20results%20demonstrate%20LEA%27s%0Aability%20to%20detect%20clear%20distinctions%20between%20non-retrieval%2C%20generic-retrieval%2C%0Aand%20valid-retrieval%20scenarios%20with%20over%2095%25%20accuracy%20on%20larger%20models.%20Finally%2C%0Awe%20demonstrate%20the%20limitations%20posed%20by%20incorrect%20retrieval%20of%20vulnerability%0Ainformation%20and%20raise%20a%20cautionary%20note%20to%20the%20cybersecurity%20community%0Aregarding%20the%20blind%20reliance%20on%20LLMs%20and%20RAG%20for%20vulnerability%20analysis.%20LEA%0Aoffers%20security%20analysts%20with%20a%20metric%20to%20audit%20RAG-enhanced%20workflows%2C%0Aimproving%20the%20transparent%20and%20trustworthy%20deployment%20of%20AI%20in%20cybersecurity%0Athreat%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Embedding-based%2520Attribution%2520%2528LEA%2529%253A%2520Quantifying%2520Source%2520Contributions%250A%2520%2520to%2520Generative%2520Model%2527s%2520Response%2520for%2520Vulnerability%2520Analysis%26entry.906535625%3DReza%2520Fayyazi%2520and%2520Michael%2520Zuzak%2520and%2520Shanchieh%2520Jay%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520for%2520cybersecurity%2520threat%250Aanalysis%252C%2520but%2520their%2520deployment%2520in%2520security-sensitive%2520environments%2520raises%2520trust%250Aand%2520safety%2520concerns.%2520With%2520over%252021%252C000%2520vulnerabilities%2520disclosed%2520in%25202025%252C%2520manual%250Aanalysis%2520is%2520infeasible%252C%2520making%2520scalable%2520and%2520verifiable%2520AI%2520support%2520critical.%250AWhen%2520querying%2520LLMs%252C%2520dealing%2520with%2520emerging%2520vulnerabilities%2520is%2520challenging%2520as%250Athey%2520have%2520a%2520training%2520cut-off%2520date.%2520While%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%250Acan%2520inject%2520up-to-date%2520context%2520to%2520alleviate%2520the%2520cut-off%2520date%2520limitation%252C%2520it%250Aremains%2520unclear%2520how%2520much%2520LLMs%2520rely%2520on%2520retrieved%2520evidence%2520versus%2520the%2520model%2527s%250Ainternal%2520knowledge%252C%2520and%2520whether%2520the%2520retrieved%2520information%2520is%2520meaningful%2520or%2520even%250Acorrect.%2520This%2520uncertainty%2520could%2520mislead%2520security%2520analysts%252C%2520mis-prioritize%250Apatches%252C%2520and%2520increase%2520security%2520risks.%2520Therefore%252C%2520this%2520work%2520proposes%2520LLM%250AEmbedding-based%2520Attribution%2520%2528LEA%2529%2520to%2520analyze%2520the%2520generated%2520responses%2520for%250Avulnerability%2520exploitation%2520analysis.%2520More%2520specifically%252C%2520LEA%2520quantifies%2520the%250Arelative%2520contribution%2520of%2520internal%2520knowledge%2520vs.%2520retrieved%2520content%2520in%2520the%250Agenerated%2520responses.%2520We%2520evaluate%2520LEA%2520on%2520500%2520critical%2520vulnerabilities%2520disclosed%250Abetween%25202016%2520and%25202025%252C%2520across%2520three%2520RAG%2520settings%2520--%2520valid%252C%2520generic%252C%2520and%250Aincorrect%2520--%2520using%2520three%2520state-of-the-art%2520LLMs.%2520Our%2520results%2520demonstrate%2520LEA%2527s%250Aability%2520to%2520detect%2520clear%2520distinctions%2520between%2520non-retrieval%252C%2520generic-retrieval%252C%250Aand%2520valid-retrieval%2520scenarios%2520with%2520over%252095%2525%2520accuracy%2520on%2520larger%2520models.%2520Finally%252C%250Awe%2520demonstrate%2520the%2520limitations%2520posed%2520by%2520incorrect%2520retrieval%2520of%2520vulnerability%250Ainformation%2520and%2520raise%2520a%2520cautionary%2520note%2520to%2520the%2520cybersecurity%2520community%250Aregarding%2520the%2520blind%2520reliance%2520on%2520LLMs%2520and%2520RAG%2520for%2520vulnerability%2520analysis.%2520LEA%250Aoffers%2520security%2520analysts%2520with%2520a%2520metric%2520to%2520audit%2520RAG-enhanced%2520workflows%252C%250Aimproving%2520the%2520transparent%2520and%2520trustworthy%2520deployment%2520of%2520AI%2520in%2520cybersecurity%250Athreat%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Embedding-based%20Attribution%20%28LEA%29%3A%20Quantifying%20Source%20Contributions%0A%20%20to%20Generative%20Model%27s%20Response%20for%20Vulnerability%20Analysis&entry.906535625=Reza%20Fayyazi%20and%20Michael%20Zuzak%20and%20Shanchieh%20Jay%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20for%20cybersecurity%20threat%0Aanalysis%2C%20but%20their%20deployment%20in%20security-sensitive%20environments%20raises%20trust%0Aand%20safety%20concerns.%20With%20over%2021%2C000%20vulnerabilities%20disclosed%20in%202025%2C%20manual%0Aanalysis%20is%20infeasible%2C%20making%20scalable%20and%20verifiable%20AI%20support%20critical.%0AWhen%20querying%20LLMs%2C%20dealing%20with%20emerging%20vulnerabilities%20is%20challenging%20as%0Athey%20have%20a%20training%20cut-off%20date.%20While%20Retrieval-Augmented%20Generation%20%28RAG%29%0Acan%20inject%20up-to-date%20context%20to%20alleviate%20the%20cut-off%20date%20limitation%2C%20it%0Aremains%20unclear%20how%20much%20LLMs%20rely%20on%20retrieved%20evidence%20versus%20the%20model%27s%0Ainternal%20knowledge%2C%20and%20whether%20the%20retrieved%20information%20is%20meaningful%20or%20even%0Acorrect.%20This%20uncertainty%20could%20mislead%20security%20analysts%2C%20mis-prioritize%0Apatches%2C%20and%20increase%20security%20risks.%20Therefore%2C%20this%20work%20proposes%20LLM%0AEmbedding-based%20Attribution%20%28LEA%29%20to%20analyze%20the%20generated%20responses%20for%0Avulnerability%20exploitation%20analysis.%20More%20specifically%2C%20LEA%20quantifies%20the%0Arelative%20contribution%20of%20internal%20knowledge%20vs.%20retrieved%20content%20in%20the%0Agenerated%20responses.%20We%20evaluate%20LEA%20on%20500%20critical%20vulnerabilities%20disclosed%0Abetween%202016%20and%202025%2C%20across%20three%20RAG%20settings%20--%20valid%2C%20generic%2C%20and%0Aincorrect%20--%20using%20three%20state-of-the-art%20LLMs.%20Our%20results%20demonstrate%20LEA%27s%0Aability%20to%20detect%20clear%20distinctions%20between%20non-retrieval%2C%20generic-retrieval%2C%0Aand%20valid-retrieval%20scenarios%20with%20over%2095%25%20accuracy%20on%20larger%20models.%20Finally%2C%0Awe%20demonstrate%20the%20limitations%20posed%20by%20incorrect%20retrieval%20of%20vulnerability%0Ainformation%20and%20raise%20a%20cautionary%20note%20to%20the%20cybersecurity%20community%0Aregarding%20the%20blind%20reliance%20on%20LLMs%20and%20RAG%20for%20vulnerability%20analysis.%20LEA%0Aoffers%20security%20analysts%20with%20a%20metric%20to%20audit%20RAG-enhanced%20workflows%2C%0Aimproving%20the%20transparent%20and%20trustworthy%20deployment%20of%20AI%20in%20cybersecurity%0Athreat%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12100v2&entry.124074799=Read"},
{"title": "Multimodal Iterative RAG for Knowledge Visual Question Answering", "author": "Changin Choi and Wonseok Lee and Jungmin Ko and Wonjong Rhee", "abstract": "  While Multimodal Large Language Models (MLLMs) have significantly advanced\nmultimodal understanding, their performance remains limited on\nknowledge-intensive visual questions that require external knowledge beyond the\nimage. Retrieval-Augmented Generation (RAG) has become a promising solution for\nproviding models with external knowledge, its conventional single-pass\nframework often fails to gather sufficient knowledge. To overcome this\nlimitation, we propose MI-RAG, a Multimodal Iterative RAG framework that\nleverages reasoning to enhance retrieval and update reasoning over newly\nretrieved knowledge across modalities. At each iteration, MI-RAG leverages an\naccumulated reasoning record to dynamically formulate a multi-query. These\nqueries then drive a joint search across heterogeneous knowledge bases\ncontaining both visually-grounded and textual knowledge. The newly acquired\nknowledge is synthesized into the reasoning record, progressively refining\nunderstanding across iterations. Experiments on challenging benchmarks,\nincluding Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG\nsignificantly improves both retrieval recall and answer accuracy, establishing\na scalable approach for compositional reasoning in knowledge-intensive VQA.\n", "link": "http://arxiv.org/abs/2509.00798v2", "date": "2025-09-03", "relevancy": 2.0191, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Iterative%20RAG%20for%20Knowledge%20Visual%20Question%20Answering&body=Title%3A%20Multimodal%20Iterative%20RAG%20for%20Knowledge%20Visual%20Question%20Answering%0AAuthor%3A%20Changin%20Choi%20and%20Wonseok%20Lee%20and%20Jungmin%20Ko%20and%20Wonjong%20Rhee%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20advanced%0Amultimodal%20understanding%2C%20their%20performance%20remains%20limited%20on%0Aknowledge-intensive%20visual%20questions%20that%20require%20external%20knowledge%20beyond%20the%0Aimage.%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20become%20a%20promising%20solution%20for%0Aproviding%20models%20with%20external%20knowledge%2C%20its%20conventional%20single-pass%0Aframework%20often%20fails%20to%20gather%20sufficient%20knowledge.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20MI-RAG%2C%20a%20Multimodal%20Iterative%20RAG%20framework%20that%0Aleverages%20reasoning%20to%20enhance%20retrieval%20and%20update%20reasoning%20over%20newly%0Aretrieved%20knowledge%20across%20modalities.%20At%20each%20iteration%2C%20MI-RAG%20leverages%20an%0Aaccumulated%20reasoning%20record%20to%20dynamically%20formulate%20a%20multi-query.%20These%0Aqueries%20then%20drive%20a%20joint%20search%20across%20heterogeneous%20knowledge%20bases%0Acontaining%20both%20visually-grounded%20and%20textual%20knowledge.%20The%20newly%20acquired%0Aknowledge%20is%20synthesized%20into%20the%20reasoning%20record%2C%20progressively%20refining%0Aunderstanding%20across%20iterations.%20Experiments%20on%20challenging%20benchmarks%2C%0Aincluding%20Encyclopedic%20VQA%2C%20InfoSeek%2C%20and%20OK-VQA%2C%20show%20that%20MI-RAG%0Asignificantly%20improves%20both%20retrieval%20recall%20and%20answer%20accuracy%2C%20establishing%0Aa%20scalable%20approach%20for%20compositional%20reasoning%20in%20knowledge-intensive%20VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00798v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Iterative%2520RAG%2520for%2520Knowledge%2520Visual%2520Question%2520Answering%26entry.906535625%3DChangin%2520Choi%2520and%2520Wonseok%2520Lee%2520and%2520Jungmin%2520Ko%2520and%2520Wonjong%2520Rhee%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520advanced%250Amultimodal%2520understanding%252C%2520their%2520performance%2520remains%2520limited%2520on%250Aknowledge-intensive%2520visual%2520questions%2520that%2520require%2520external%2520knowledge%2520beyond%2520the%250Aimage.%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520become%2520a%2520promising%2520solution%2520for%250Aproviding%2520models%2520with%2520external%2520knowledge%252C%2520its%2520conventional%2520single-pass%250Aframework%2520often%2520fails%2520to%2520gather%2520sufficient%2520knowledge.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520propose%2520MI-RAG%252C%2520a%2520Multimodal%2520Iterative%2520RAG%2520framework%2520that%250Aleverages%2520reasoning%2520to%2520enhance%2520retrieval%2520and%2520update%2520reasoning%2520over%2520newly%250Aretrieved%2520knowledge%2520across%2520modalities.%2520At%2520each%2520iteration%252C%2520MI-RAG%2520leverages%2520an%250Aaccumulated%2520reasoning%2520record%2520to%2520dynamically%2520formulate%2520a%2520multi-query.%2520These%250Aqueries%2520then%2520drive%2520a%2520joint%2520search%2520across%2520heterogeneous%2520knowledge%2520bases%250Acontaining%2520both%2520visually-grounded%2520and%2520textual%2520knowledge.%2520The%2520newly%2520acquired%250Aknowledge%2520is%2520synthesized%2520into%2520the%2520reasoning%2520record%252C%2520progressively%2520refining%250Aunderstanding%2520across%2520iterations.%2520Experiments%2520on%2520challenging%2520benchmarks%252C%250Aincluding%2520Encyclopedic%2520VQA%252C%2520InfoSeek%252C%2520and%2520OK-VQA%252C%2520show%2520that%2520MI-RAG%250Asignificantly%2520improves%2520both%2520retrieval%2520recall%2520and%2520answer%2520accuracy%252C%2520establishing%250Aa%2520scalable%2520approach%2520for%2520compositional%2520reasoning%2520in%2520knowledge-intensive%2520VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00798v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Iterative%20RAG%20for%20Knowledge%20Visual%20Question%20Answering&entry.906535625=Changin%20Choi%20and%20Wonseok%20Lee%20and%20Jungmin%20Ko%20and%20Wonjong%20Rhee&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20advanced%0Amultimodal%20understanding%2C%20their%20performance%20remains%20limited%20on%0Aknowledge-intensive%20visual%20questions%20that%20require%20external%20knowledge%20beyond%20the%0Aimage.%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20become%20a%20promising%20solution%20for%0Aproviding%20models%20with%20external%20knowledge%2C%20its%20conventional%20single-pass%0Aframework%20often%20fails%20to%20gather%20sufficient%20knowledge.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20MI-RAG%2C%20a%20Multimodal%20Iterative%20RAG%20framework%20that%0Aleverages%20reasoning%20to%20enhance%20retrieval%20and%20update%20reasoning%20over%20newly%0Aretrieved%20knowledge%20across%20modalities.%20At%20each%20iteration%2C%20MI-RAG%20leverages%20an%0Aaccumulated%20reasoning%20record%20to%20dynamically%20formulate%20a%20multi-query.%20These%0Aqueries%20then%20drive%20a%20joint%20search%20across%20heterogeneous%20knowledge%20bases%0Acontaining%20both%20visually-grounded%20and%20textual%20knowledge.%20The%20newly%20acquired%0Aknowledge%20is%20synthesized%20into%20the%20reasoning%20record%2C%20progressively%20refining%0Aunderstanding%20across%20iterations.%20Experiments%20on%20challenging%20benchmarks%2C%0Aincluding%20Encyclopedic%20VQA%2C%20InfoSeek%2C%20and%20OK-VQA%2C%20show%20that%20MI-RAG%0Asignificantly%20improves%20both%20retrieval%20recall%20and%20answer%20accuracy%2C%20establishing%0Aa%20scalable%20approach%20for%20compositional%20reasoning%20in%20knowledge-intensive%20VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00798v2&entry.124074799=Read"},
{"title": "FoMEMO: Towards Foundation Models for Expensive Multi-objective\n  Optimization", "author": "Yiming Yao and Fei Liu and Liang Zhao and Xi Lin and Qingfu Zhang", "abstract": "  Expensive multi-objective optimization is a prevalent and crucial concern in\nmany real-world scenarios, where sample-efficiency is vital due to the limited\nevaluations to recover the true Pareto front for decision making. Existing\nworks either involve rebuilding Gaussian process surrogates from scratch for\neach objective in each new problem encountered, or rely on extensive past\ndomain experiments for pre-training deep learning models, making them hard to\ngeneralize and impractical to cope with various emerging applications in the\nreal world. To address this issue, we propose a new paradigm named FoMEMO\n(Foundation Models for Expensive Multi-objective Optimization), which enables\nthe establishment of a foundation model conditioned on any domain trajectory\nand user preference, and facilitates fast in-context optimization based on the\npredicted preference-wise aggregation posteriors. Rather than accessing\nextensive domain experiments in the real world, we demonstrate that\npre-training the foundation model with a diverse set of hundreds of millions of\nsynthetic data can lead to superior adaptability to unknown problems, without\nnecessitating any subsequent model training or updates in the optimization\nprocess. We evaluate our method across a variety of synthetic benchmarks and\nreal-word applications, and demonstrate its superior generality and competitive\nperformance compared to existing methods.\n", "link": "http://arxiv.org/abs/2509.03244v1", "date": "2025-09-03", "relevancy": 2.0185, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoMEMO%3A%20Towards%20Foundation%20Models%20for%20Expensive%20Multi-objective%0A%20%20Optimization&body=Title%3A%20FoMEMO%3A%20Towards%20Foundation%20Models%20for%20Expensive%20Multi-objective%0A%20%20Optimization%0AAuthor%3A%20Yiming%20Yao%20and%20Fei%20Liu%20and%20Liang%20Zhao%20and%20Xi%20Lin%20and%20Qingfu%20Zhang%0AAbstract%3A%20%20%20Expensive%20multi-objective%20optimization%20is%20a%20prevalent%20and%20crucial%20concern%20in%0Amany%20real-world%20scenarios%2C%20where%20sample-efficiency%20is%20vital%20due%20to%20the%20limited%0Aevaluations%20to%20recover%20the%20true%20Pareto%20front%20for%20decision%20making.%20Existing%0Aworks%20either%20involve%20rebuilding%20Gaussian%20process%20surrogates%20from%20scratch%20for%0Aeach%20objective%20in%20each%20new%20problem%20encountered%2C%20or%20rely%20on%20extensive%20past%0Adomain%20experiments%20for%20pre-training%20deep%20learning%20models%2C%20making%20them%20hard%20to%0Ageneralize%20and%20impractical%20to%20cope%20with%20various%20emerging%20applications%20in%20the%0Areal%20world.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20paradigm%20named%20FoMEMO%0A%28Foundation%20Models%20for%20Expensive%20Multi-objective%20Optimization%29%2C%20which%20enables%0Athe%20establishment%20of%20a%20foundation%20model%20conditioned%20on%20any%20domain%20trajectory%0Aand%20user%20preference%2C%20and%20facilitates%20fast%20in-context%20optimization%20based%20on%20the%0Apredicted%20preference-wise%20aggregation%20posteriors.%20Rather%20than%20accessing%0Aextensive%20domain%20experiments%20in%20the%20real%20world%2C%20we%20demonstrate%20that%0Apre-training%20the%20foundation%20model%20with%20a%20diverse%20set%20of%20hundreds%20of%20millions%20of%0Asynthetic%20data%20can%20lead%20to%20superior%20adaptability%20to%20unknown%20problems%2C%20without%0Anecessitating%20any%20subsequent%20model%20training%20or%20updates%20in%20the%20optimization%0Aprocess.%20We%20evaluate%20our%20method%20across%20a%20variety%20of%20synthetic%20benchmarks%20and%0Areal-word%20applications%2C%20and%20demonstrate%20its%20superior%20generality%20and%20competitive%0Aperformance%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoMEMO%253A%2520Towards%2520Foundation%2520Models%2520for%2520Expensive%2520Multi-objective%250A%2520%2520Optimization%26entry.906535625%3DYiming%2520Yao%2520and%2520Fei%2520Liu%2520and%2520Liang%2520Zhao%2520and%2520Xi%2520Lin%2520and%2520Qingfu%2520Zhang%26entry.1292438233%3D%2520%2520Expensive%2520multi-objective%2520optimization%2520is%2520a%2520prevalent%2520and%2520crucial%2520concern%2520in%250Amany%2520real-world%2520scenarios%252C%2520where%2520sample-efficiency%2520is%2520vital%2520due%2520to%2520the%2520limited%250Aevaluations%2520to%2520recover%2520the%2520true%2520Pareto%2520front%2520for%2520decision%2520making.%2520Existing%250Aworks%2520either%2520involve%2520rebuilding%2520Gaussian%2520process%2520surrogates%2520from%2520scratch%2520for%250Aeach%2520objective%2520in%2520each%2520new%2520problem%2520encountered%252C%2520or%2520rely%2520on%2520extensive%2520past%250Adomain%2520experiments%2520for%2520pre-training%2520deep%2520learning%2520models%252C%2520making%2520them%2520hard%2520to%250Ageneralize%2520and%2520impractical%2520to%2520cope%2520with%2520various%2520emerging%2520applications%2520in%2520the%250Areal%2520world.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520new%2520paradigm%2520named%2520FoMEMO%250A%2528Foundation%2520Models%2520for%2520Expensive%2520Multi-objective%2520Optimization%2529%252C%2520which%2520enables%250Athe%2520establishment%2520of%2520a%2520foundation%2520model%2520conditioned%2520on%2520any%2520domain%2520trajectory%250Aand%2520user%2520preference%252C%2520and%2520facilitates%2520fast%2520in-context%2520optimization%2520based%2520on%2520the%250Apredicted%2520preference-wise%2520aggregation%2520posteriors.%2520Rather%2520than%2520accessing%250Aextensive%2520domain%2520experiments%2520in%2520the%2520real%2520world%252C%2520we%2520demonstrate%2520that%250Apre-training%2520the%2520foundation%2520model%2520with%2520a%2520diverse%2520set%2520of%2520hundreds%2520of%2520millions%2520of%250Asynthetic%2520data%2520can%2520lead%2520to%2520superior%2520adaptability%2520to%2520unknown%2520problems%252C%2520without%250Anecessitating%2520any%2520subsequent%2520model%2520training%2520or%2520updates%2520in%2520the%2520optimization%250Aprocess.%2520We%2520evaluate%2520our%2520method%2520across%2520a%2520variety%2520of%2520synthetic%2520benchmarks%2520and%250Areal-word%2520applications%252C%2520and%2520demonstrate%2520its%2520superior%2520generality%2520and%2520competitive%250Aperformance%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoMEMO%3A%20Towards%20Foundation%20Models%20for%20Expensive%20Multi-objective%0A%20%20Optimization&entry.906535625=Yiming%20Yao%20and%20Fei%20Liu%20and%20Liang%20Zhao%20and%20Xi%20Lin%20and%20Qingfu%20Zhang&entry.1292438233=%20%20Expensive%20multi-objective%20optimization%20is%20a%20prevalent%20and%20crucial%20concern%20in%0Amany%20real-world%20scenarios%2C%20where%20sample-efficiency%20is%20vital%20due%20to%20the%20limited%0Aevaluations%20to%20recover%20the%20true%20Pareto%20front%20for%20decision%20making.%20Existing%0Aworks%20either%20involve%20rebuilding%20Gaussian%20process%20surrogates%20from%20scratch%20for%0Aeach%20objective%20in%20each%20new%20problem%20encountered%2C%20or%20rely%20on%20extensive%20past%0Adomain%20experiments%20for%20pre-training%20deep%20learning%20models%2C%20making%20them%20hard%20to%0Ageneralize%20and%20impractical%20to%20cope%20with%20various%20emerging%20applications%20in%20the%0Areal%20world.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20paradigm%20named%20FoMEMO%0A%28Foundation%20Models%20for%20Expensive%20Multi-objective%20Optimization%29%2C%20which%20enables%0Athe%20establishment%20of%20a%20foundation%20model%20conditioned%20on%20any%20domain%20trajectory%0Aand%20user%20preference%2C%20and%20facilitates%20fast%20in-context%20optimization%20based%20on%20the%0Apredicted%20preference-wise%20aggregation%20posteriors.%20Rather%20than%20accessing%0Aextensive%20domain%20experiments%20in%20the%20real%20world%2C%20we%20demonstrate%20that%0Apre-training%20the%20foundation%20model%20with%20a%20diverse%20set%20of%20hundreds%20of%20millions%20of%0Asynthetic%20data%20can%20lead%20to%20superior%20adaptability%20to%20unknown%20problems%2C%20without%0Anecessitating%20any%20subsequent%20model%20training%20or%20updates%20in%20the%20optimization%0Aprocess.%20We%20evaluate%20our%20method%20across%20a%20variety%20of%20synthetic%20benchmarks%20and%0Areal-word%20applications%2C%20and%20demonstrate%20its%20superior%20generality%20and%20competitive%0Aperformance%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03244v1&entry.124074799=Read"},
{"title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns", "author": "Juntian Zhu and Miguel de Carvalho and Zhouwang Yang and Fengxiang He", "abstract": "  An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost.\n", "link": "http://arxiv.org/abs/2505.13188v2", "date": "2025-09-03", "relevancy": 2.015, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.511}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5017}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20a%20Reinforcement%20Learning%20Agent%20Encounters%20Unknown%20Unknowns&body=Title%3A%20When%20a%20Reinforcement%20Learning%20Agent%20Encounters%20Unknown%20Unknowns%0AAuthor%3A%20Juntian%20Zhu%20and%20Miguel%20de%20Carvalho%20and%20Zhouwang%20Yang%20and%20Fengxiang%20He%0AAbstract%3A%20%20%20An%20AI%20agent%20might%20surprisingly%20find%20she%20has%20reached%20an%20unknown%20state%20which%0Ashe%20has%20never%20been%20aware%20of%20--%20an%20unknown%20unknown.%20We%20mathematically%20ground%0Athis%20scenario%20in%20reinforcement%20learning%3A%20an%20agent%2C%20after%20taking%20an%20action%0Acalculated%20from%20value%20functions%20%24Q%24%20and%20%24V%24%20defined%20on%20the%20%7B%5Cit%20%7Baware%0Adomain%7D%7D%2C%20reaches%20a%20state%20out%20of%20the%20domain.%20To%20enable%20the%20agent%20to%20handle%20this%0Ascenario%2C%20we%20propose%20an%20%7B%5Cit%20episodic%20Markov%20decision%20%7Bprocess%7D%20with%20growing%0Aawareness%7D%20%28EMDP-GA%29%20model%2C%20taking%20a%20new%20%7B%5Cit%20noninformative%20value%20expansion%7D%0A%28NIVE%29%20approach%20to%20expand%20value%20functions%20to%20newly%20aware%20areas%3A%20when%20an%20agent%0Aarrives%20at%20an%20unknown%20unknown%2C%20value%20functions%20%24Q%24%20and%20%24V%24%20whereon%20are%0Ainitialised%20by%20noninformative%20beliefs%20--%20the%20averaged%20values%20on%20the%20aware%0Adomain.%20This%20design%20is%20out%20of%20respect%20for%20the%20complete%20absence%20of%20knowledge%20in%0Athe%20newly%20discovered%20state.%20The%20upper%20confidence%20bound%20momentum%20Q-learning%20is%0Athen%20adapted%20to%20the%20growing%20awareness%20for%20training%20the%20EMDP-GA%20model.%20We%20prove%0Athat%20%281%29%20the%20regret%20of%20our%20approach%20is%20asymptotically%20consistent%20with%20the%20state%0Aof%20the%20art%20%28SOTA%29%20without%20exposure%20to%20unknown%20unknowns%20in%20an%20extremely%0Auncertain%20environment%2C%20and%20%282%29%20our%20computational%20complexity%20and%20space%0Acomplexity%20are%20comparable%20with%20the%20SOTA%20--%20these%20collectively%20suggest%20that%0Athough%20an%20unknown%20unknown%20is%20surprising%2C%20it%20will%20be%20asymptotically%20properly%0Adiscovered%20with%20decent%20speed%20and%20an%20affordable%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520a%2520Reinforcement%2520Learning%2520Agent%2520Encounters%2520Unknown%2520Unknowns%26entry.906535625%3DJuntian%2520Zhu%2520and%2520Miguel%2520de%2520Carvalho%2520and%2520Zhouwang%2520Yang%2520and%2520Fengxiang%2520He%26entry.1292438233%3D%2520%2520An%2520AI%2520agent%2520might%2520surprisingly%2520find%2520she%2520has%2520reached%2520an%2520unknown%2520state%2520which%250Ashe%2520has%2520never%2520been%2520aware%2520of%2520--%2520an%2520unknown%2520unknown.%2520We%2520mathematically%2520ground%250Athis%2520scenario%2520in%2520reinforcement%2520learning%253A%2520an%2520agent%252C%2520after%2520taking%2520an%2520action%250Acalculated%2520from%2520value%2520functions%2520%2524Q%2524%2520and%2520%2524V%2524%2520defined%2520on%2520the%2520%257B%255Cit%2520%257Baware%250Adomain%257D%257D%252C%2520reaches%2520a%2520state%2520out%2520of%2520the%2520domain.%2520To%2520enable%2520the%2520agent%2520to%2520handle%2520this%250Ascenario%252C%2520we%2520propose%2520an%2520%257B%255Cit%2520episodic%2520Markov%2520decision%2520%257Bprocess%257D%2520with%2520growing%250Aawareness%257D%2520%2528EMDP-GA%2529%2520model%252C%2520taking%2520a%2520new%2520%257B%255Cit%2520noninformative%2520value%2520expansion%257D%250A%2528NIVE%2529%2520approach%2520to%2520expand%2520value%2520functions%2520to%2520newly%2520aware%2520areas%253A%2520when%2520an%2520agent%250Aarrives%2520at%2520an%2520unknown%2520unknown%252C%2520value%2520functions%2520%2524Q%2524%2520and%2520%2524V%2524%2520whereon%2520are%250Ainitialised%2520by%2520noninformative%2520beliefs%2520--%2520the%2520averaged%2520values%2520on%2520the%2520aware%250Adomain.%2520This%2520design%2520is%2520out%2520of%2520respect%2520for%2520the%2520complete%2520absence%2520of%2520knowledge%2520in%250Athe%2520newly%2520discovered%2520state.%2520The%2520upper%2520confidence%2520bound%2520momentum%2520Q-learning%2520is%250Athen%2520adapted%2520to%2520the%2520growing%2520awareness%2520for%2520training%2520the%2520EMDP-GA%2520model.%2520We%2520prove%250Athat%2520%25281%2529%2520the%2520regret%2520of%2520our%2520approach%2520is%2520asymptotically%2520consistent%2520with%2520the%2520state%250Aof%2520the%2520art%2520%2528SOTA%2529%2520without%2520exposure%2520to%2520unknown%2520unknowns%2520in%2520an%2520extremely%250Auncertain%2520environment%252C%2520and%2520%25282%2529%2520our%2520computational%2520complexity%2520and%2520space%250Acomplexity%2520are%2520comparable%2520with%2520the%2520SOTA%2520--%2520these%2520collectively%2520suggest%2520that%250Athough%2520an%2520unknown%2520unknown%2520is%2520surprising%252C%2520it%2520will%2520be%2520asymptotically%2520properly%250Adiscovered%2520with%2520decent%2520speed%2520and%2520an%2520affordable%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20a%20Reinforcement%20Learning%20Agent%20Encounters%20Unknown%20Unknowns&entry.906535625=Juntian%20Zhu%20and%20Miguel%20de%20Carvalho%20and%20Zhouwang%20Yang%20and%20Fengxiang%20He&entry.1292438233=%20%20An%20AI%20agent%20might%20surprisingly%20find%20she%20has%20reached%20an%20unknown%20state%20which%0Ashe%20has%20never%20been%20aware%20of%20--%20an%20unknown%20unknown.%20We%20mathematically%20ground%0Athis%20scenario%20in%20reinforcement%20learning%3A%20an%20agent%2C%20after%20taking%20an%20action%0Acalculated%20from%20value%20functions%20%24Q%24%20and%20%24V%24%20defined%20on%20the%20%7B%5Cit%20%7Baware%0Adomain%7D%7D%2C%20reaches%20a%20state%20out%20of%20the%20domain.%20To%20enable%20the%20agent%20to%20handle%20this%0Ascenario%2C%20we%20propose%20an%20%7B%5Cit%20episodic%20Markov%20decision%20%7Bprocess%7D%20with%20growing%0Aawareness%7D%20%28EMDP-GA%29%20model%2C%20taking%20a%20new%20%7B%5Cit%20noninformative%20value%20expansion%7D%0A%28NIVE%29%20approach%20to%20expand%20value%20functions%20to%20newly%20aware%20areas%3A%20when%20an%20agent%0Aarrives%20at%20an%20unknown%20unknown%2C%20value%20functions%20%24Q%24%20and%20%24V%24%20whereon%20are%0Ainitialised%20by%20noninformative%20beliefs%20--%20the%20averaged%20values%20on%20the%20aware%0Adomain.%20This%20design%20is%20out%20of%20respect%20for%20the%20complete%20absence%20of%20knowledge%20in%0Athe%20newly%20discovered%20state.%20The%20upper%20confidence%20bound%20momentum%20Q-learning%20is%0Athen%20adapted%20to%20the%20growing%20awareness%20for%20training%20the%20EMDP-GA%20model.%20We%20prove%0Athat%20%281%29%20the%20regret%20of%20our%20approach%20is%20asymptotically%20consistent%20with%20the%20state%0Aof%20the%20art%20%28SOTA%29%20without%20exposure%20to%20unknown%20unknowns%20in%20an%20extremely%0Auncertain%20environment%2C%20and%20%282%29%20our%20computational%20complexity%20and%20space%0Acomplexity%20are%20comparable%20with%20the%20SOTA%20--%20these%20collectively%20suggest%20that%0Athough%20an%20unknown%20unknown%20is%20surprising%2C%20it%20will%20be%20asymptotically%20properly%0Adiscovered%20with%20decent%20speed%20and%20an%20affordable%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13188v2&entry.124074799=Read"},
{"title": "A Lorentz-Equivariant Transformer for All of the LHC", "author": "Johann Brehmer and V\u00edctor Bres\u00f3 and Pim de Haan and Tilman Plehn and Huilin Qu and Jonas Spinner and Jesse Thaler", "abstract": "  We show that the Lorentz-Equivariant Geometric Algebra Transformer (L-GATr)\nyields state-of-the-art performance for a wide range of machine learning tasks\nat the Large Hadron Collider. L-GATr represents data in a geometric algebra\nover space-time and is equivariant under Lorentz transformations. The\nunderlying architecture is a versatile and scalable transformer, which is able\nto break symmetries if needed. We demonstrate the power of L-GATr for amplitude\nregression and jet classification, and then benchmark it as the first\nLorentz-equivariant generative network. For all three LHC tasks, we find\nsignificant improvements over previous architectures.\n", "link": "http://arxiv.org/abs/2411.00446v3", "date": "2025-09-03", "relevancy": 2.0104, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5472}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5008}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lorentz-Equivariant%20Transformer%20for%20All%20of%20the%20LHC&body=Title%3A%20A%20Lorentz-Equivariant%20Transformer%20for%20All%20of%20the%20LHC%0AAuthor%3A%20Johann%20Brehmer%20and%20V%C3%ADctor%20Bres%C3%B3%20and%20Pim%20de%20Haan%20and%20Tilman%20Plehn%20and%20Huilin%20Qu%20and%20Jonas%20Spinner%20and%20Jesse%20Thaler%0AAbstract%3A%20%20%20We%20show%20that%20the%20Lorentz-Equivariant%20Geometric%20Algebra%20Transformer%20%28L-GATr%29%0Ayields%20state-of-the-art%20performance%20for%20a%20wide%20range%20of%20machine%20learning%20tasks%0Aat%20the%20Large%20Hadron%20Collider.%20L-GATr%20represents%20data%20in%20a%20geometric%20algebra%0Aover%20space-time%20and%20is%20equivariant%20under%20Lorentz%20transformations.%20The%0Aunderlying%20architecture%20is%20a%20versatile%20and%20scalable%20transformer%2C%20which%20is%20able%0Ato%20break%20symmetries%20if%20needed.%20We%20demonstrate%20the%20power%20of%20L-GATr%20for%20amplitude%0Aregression%20and%20jet%20classification%2C%20and%20then%20benchmark%20it%20as%20the%20first%0ALorentz-equivariant%20generative%20network.%20For%20all%20three%20LHC%20tasks%2C%20we%20find%0Asignificant%20improvements%20over%20previous%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00446v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lorentz-Equivariant%2520Transformer%2520for%2520All%2520of%2520the%2520LHC%26entry.906535625%3DJohann%2520Brehmer%2520and%2520V%25C3%25ADctor%2520Bres%25C3%25B3%2520and%2520Pim%2520de%2520Haan%2520and%2520Tilman%2520Plehn%2520and%2520Huilin%2520Qu%2520and%2520Jonas%2520Spinner%2520and%2520Jesse%2520Thaler%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520the%2520Lorentz-Equivariant%2520Geometric%2520Algebra%2520Transformer%2520%2528L-GATr%2529%250Ayields%2520state-of-the-art%2520performance%2520for%2520a%2520wide%2520range%2520of%2520machine%2520learning%2520tasks%250Aat%2520the%2520Large%2520Hadron%2520Collider.%2520L-GATr%2520represents%2520data%2520in%2520a%2520geometric%2520algebra%250Aover%2520space-time%2520and%2520is%2520equivariant%2520under%2520Lorentz%2520transformations.%2520The%250Aunderlying%2520architecture%2520is%2520a%2520versatile%2520and%2520scalable%2520transformer%252C%2520which%2520is%2520able%250Ato%2520break%2520symmetries%2520if%2520needed.%2520We%2520demonstrate%2520the%2520power%2520of%2520L-GATr%2520for%2520amplitude%250Aregression%2520and%2520jet%2520classification%252C%2520and%2520then%2520benchmark%2520it%2520as%2520the%2520first%250ALorentz-equivariant%2520generative%2520network.%2520For%2520all%2520three%2520LHC%2520tasks%252C%2520we%2520find%250Asignificant%2520improvements%2520over%2520previous%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00446v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lorentz-Equivariant%20Transformer%20for%20All%20of%20the%20LHC&entry.906535625=Johann%20Brehmer%20and%20V%C3%ADctor%20Bres%C3%B3%20and%20Pim%20de%20Haan%20and%20Tilman%20Plehn%20and%20Huilin%20Qu%20and%20Jonas%20Spinner%20and%20Jesse%20Thaler&entry.1292438233=%20%20We%20show%20that%20the%20Lorentz-Equivariant%20Geometric%20Algebra%20Transformer%20%28L-GATr%29%0Ayields%20state-of-the-art%20performance%20for%20a%20wide%20range%20of%20machine%20learning%20tasks%0Aat%20the%20Large%20Hadron%20Collider.%20L-GATr%20represents%20data%20in%20a%20geometric%20algebra%0Aover%20space-time%20and%20is%20equivariant%20under%20Lorentz%20transformations.%20The%0Aunderlying%20architecture%20is%20a%20versatile%20and%20scalable%20transformer%2C%20which%20is%20able%0Ato%20break%20symmetries%20if%20needed.%20We%20demonstrate%20the%20power%20of%20L-GATr%20for%20amplitude%0Aregression%20and%20jet%20classification%2C%20and%20then%20benchmark%20it%20as%20the%20first%0ALorentz-equivariant%20generative%20network.%20For%20all%20three%20LHC%20tasks%2C%20we%20find%0Asignificant%20improvements%20over%20previous%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00446v3&entry.124074799=Read"},
{"title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens\n  of Documents", "author": "Tomer Wolfson and Harsh Trivedi and Mor Geva and Yoav Goldberg and Dan Roth and Tushar Khot and Ashish Sabharwal and Reut Tsarfaty", "abstract": "  Automated agents, powered by Large language models (LLMs), are emerging as\nthe go-to tool for querying information. However, evaluation benchmarks for LLM\nagents rarely feature natural questions that are both information-seeking and\ngenuinely time-consuming for humans. To address this gap we introduce MoNaCo, a\nbenchmark of 1,315 natural and time-consuming questions that require dozens,\nand at times hundreds, of intermediate steps to solve -- far more than any\nexisting QA benchmark. To build MoNaCo, we developed a decomposed annotation\npipeline to elicit and manually answer real-world time-consuming questions at\nscale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by\nlow recall and hallucinations. Our results underscore the limitations of\nLLM-powered agents in handling the complexity and sheer breadth of real-world\ninformation-seeking tasks -- with MoNaCo providing an effective resource for\ntracking such progress. The MoNaCo benchmark, codebase, prompts and models\npredictions are all publicly available at:\nhttps://tomerwolgithub.github.io/monaco\n", "link": "http://arxiv.org/abs/2508.11133v2", "date": "2025-09-03", "relevancy": 2.0087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoNaCo%3A%20More%20Natural%20and%20Complex%20Questions%20for%20Reasoning%20Across%20Dozens%0A%20%20of%20Documents&body=Title%3A%20MoNaCo%3A%20More%20Natural%20and%20Complex%20Questions%20for%20Reasoning%20Across%20Dozens%0A%20%20of%20Documents%0AAuthor%3A%20Tomer%20Wolfson%20and%20Harsh%20Trivedi%20and%20Mor%20Geva%20and%20Yoav%20Goldberg%20and%20Dan%20Roth%20and%20Tushar%20Khot%20and%20Ashish%20Sabharwal%20and%20Reut%20Tsarfaty%0AAbstract%3A%20%20%20Automated%20agents%2C%20powered%20by%20Large%20language%20models%20%28LLMs%29%2C%20are%20emerging%20as%0Athe%20go-to%20tool%20for%20querying%20information.%20However%2C%20evaluation%20benchmarks%20for%20LLM%0Aagents%20rarely%20feature%20natural%20questions%20that%20are%20both%20information-seeking%20and%0Agenuinely%20time-consuming%20for%20humans.%20To%20address%20this%20gap%20we%20introduce%20MoNaCo%2C%20a%0Abenchmark%20of%201%2C315%20natural%20and%20time-consuming%20questions%20that%20require%20dozens%2C%0Aand%20at%20times%20hundreds%2C%20of%20intermediate%20steps%20to%20solve%20--%20far%20more%20than%20any%0Aexisting%20QA%20benchmark.%20To%20build%20MoNaCo%2C%20we%20developed%20a%20decomposed%20annotation%0Apipeline%20to%20elicit%20and%20manually%20answer%20real-world%20time-consuming%20questions%20at%0Ascale.%20Frontier%20LLMs%20evaluated%20on%20MoNaCo%20achieve%20at%20most%2061.2%25%20F1%2C%20hampered%20by%0Alow%20recall%20and%20hallucinations.%20Our%20results%20underscore%20the%20limitations%20of%0ALLM-powered%20agents%20in%20handling%20the%20complexity%20and%20sheer%20breadth%20of%20real-world%0Ainformation-seeking%20tasks%20--%20with%20MoNaCo%20providing%20an%20effective%20resource%20for%0Atracking%20such%20progress.%20The%20MoNaCo%20benchmark%2C%20codebase%2C%20prompts%20and%20models%0Apredictions%20are%20all%20publicly%20available%20at%3A%0Ahttps%3A//tomerwolgithub.github.io/monaco%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11133v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoNaCo%253A%2520More%2520Natural%2520and%2520Complex%2520Questions%2520for%2520Reasoning%2520Across%2520Dozens%250A%2520%2520of%2520Documents%26entry.906535625%3DTomer%2520Wolfson%2520and%2520Harsh%2520Trivedi%2520and%2520Mor%2520Geva%2520and%2520Yoav%2520Goldberg%2520and%2520Dan%2520Roth%2520and%2520Tushar%2520Khot%2520and%2520Ashish%2520Sabharwal%2520and%2520Reut%2520Tsarfaty%26entry.1292438233%3D%2520%2520Automated%2520agents%252C%2520powered%2520by%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520are%2520emerging%2520as%250Athe%2520go-to%2520tool%2520for%2520querying%2520information.%2520However%252C%2520evaluation%2520benchmarks%2520for%2520LLM%250Aagents%2520rarely%2520feature%2520natural%2520questions%2520that%2520are%2520both%2520information-seeking%2520and%250Agenuinely%2520time-consuming%2520for%2520humans.%2520To%2520address%2520this%2520gap%2520we%2520introduce%2520MoNaCo%252C%2520a%250Abenchmark%2520of%25201%252C315%2520natural%2520and%2520time-consuming%2520questions%2520that%2520require%2520dozens%252C%250Aand%2520at%2520times%2520hundreds%252C%2520of%2520intermediate%2520steps%2520to%2520solve%2520--%2520far%2520more%2520than%2520any%250Aexisting%2520QA%2520benchmark.%2520To%2520build%2520MoNaCo%252C%2520we%2520developed%2520a%2520decomposed%2520annotation%250Apipeline%2520to%2520elicit%2520and%2520manually%2520answer%2520real-world%2520time-consuming%2520questions%2520at%250Ascale.%2520Frontier%2520LLMs%2520evaluated%2520on%2520MoNaCo%2520achieve%2520at%2520most%252061.2%2525%2520F1%252C%2520hampered%2520by%250Alow%2520recall%2520and%2520hallucinations.%2520Our%2520results%2520underscore%2520the%2520limitations%2520of%250ALLM-powered%2520agents%2520in%2520handling%2520the%2520complexity%2520and%2520sheer%2520breadth%2520of%2520real-world%250Ainformation-seeking%2520tasks%2520--%2520with%2520MoNaCo%2520providing%2520an%2520effective%2520resource%2520for%250Atracking%2520such%2520progress.%2520The%2520MoNaCo%2520benchmark%252C%2520codebase%252C%2520prompts%2520and%2520models%250Apredictions%2520are%2520all%2520publicly%2520available%2520at%253A%250Ahttps%253A//tomerwolgithub.github.io/monaco%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11133v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoNaCo%3A%20More%20Natural%20and%20Complex%20Questions%20for%20Reasoning%20Across%20Dozens%0A%20%20of%20Documents&entry.906535625=Tomer%20Wolfson%20and%20Harsh%20Trivedi%20and%20Mor%20Geva%20and%20Yoav%20Goldberg%20and%20Dan%20Roth%20and%20Tushar%20Khot%20and%20Ashish%20Sabharwal%20and%20Reut%20Tsarfaty&entry.1292438233=%20%20Automated%20agents%2C%20powered%20by%20Large%20language%20models%20%28LLMs%29%2C%20are%20emerging%20as%0Athe%20go-to%20tool%20for%20querying%20information.%20However%2C%20evaluation%20benchmarks%20for%20LLM%0Aagents%20rarely%20feature%20natural%20questions%20that%20are%20both%20information-seeking%20and%0Agenuinely%20time-consuming%20for%20humans.%20To%20address%20this%20gap%20we%20introduce%20MoNaCo%2C%20a%0Abenchmark%20of%201%2C315%20natural%20and%20time-consuming%20questions%20that%20require%20dozens%2C%0Aand%20at%20times%20hundreds%2C%20of%20intermediate%20steps%20to%20solve%20--%20far%20more%20than%20any%0Aexisting%20QA%20benchmark.%20To%20build%20MoNaCo%2C%20we%20developed%20a%20decomposed%20annotation%0Apipeline%20to%20elicit%20and%20manually%20answer%20real-world%20time-consuming%20questions%20at%0Ascale.%20Frontier%20LLMs%20evaluated%20on%20MoNaCo%20achieve%20at%20most%2061.2%25%20F1%2C%20hampered%20by%0Alow%20recall%20and%20hallucinations.%20Our%20results%20underscore%20the%20limitations%20of%0ALLM-powered%20agents%20in%20handling%20the%20complexity%20and%20sheer%20breadth%20of%20real-world%0Ainformation-seeking%20tasks%20--%20with%20MoNaCo%20providing%20an%20effective%20resource%20for%0Atracking%20such%20progress.%20The%20MoNaCo%20benchmark%2C%20codebase%2C%20prompts%20and%20models%0Apredictions%20are%20all%20publicly%20available%20at%3A%0Ahttps%3A//tomerwolgithub.github.io/monaco%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11133v2&entry.124074799=Read"},
{"title": "Beyond Correctness: Harmonizing Process and Outcome Rewards through RL\n  Training", "author": "Chenlu Ye and Zhou Yu and Ziji Zhang and Hao Chen and Narayanan Sadagopan and Jing Huang and Tong Zhang and Anurag Beniwal", "abstract": "  Reinforcement learning with verifiable rewards (RLVR) has emerged to be a\npredominant paradigm for mathematical reasoning tasks, offering stable\nimprovements in reasoning ability. However, Outcome Reward Models (ORMs) in\nRLVR are too coarse-grained to distinguish flawed reasoning within correct\nanswers or valid reasoning within incorrect answers. This lack of granularity\nintroduces noisy and misleading gradients significantly and hinders further\nprogress in reasoning process quality. While Process Reward Models (PRMs) offer\nfine-grained guidance for intermediate steps, they frequently suffer from\ninaccuracies and are susceptible to reward hacking.\n  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an\neffective data process curation method that harmonizes noisy, fine-grained\nprocess rewards with accurate, coarse-grained outcome rewards. Rather than\nnaively blending PRM and ORM in the objective function\n(arXiv:archive/2506.18896), PROF leverages their complementary strengths\nthrough consistency-driven sample selection. Our approach retains correct\nresponses with higher averaged process values and incorrect responses with\nlower averaged process values, while maintaining positive/negative training\nsample balance. Extensive experiments demonstrate that our method not only\nconsistently improves the final accuracy over $4\\%$ compared to the blending\napproaches, but also strengthens the quality of intermediate reasoning steps.\nCodes and training recipes are available at https://github.com/Chenluye99/PROF.\n", "link": "http://arxiv.org/abs/2509.03403v1", "date": "2025-09-03", "relevancy": 2.0015, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5062}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Correctness%3A%20Harmonizing%20Process%20and%20Outcome%20Rewards%20through%20RL%0A%20%20Training&body=Title%3A%20Beyond%20Correctness%3A%20Harmonizing%20Process%20and%20Outcome%20Rewards%20through%20RL%0A%20%20Training%0AAuthor%3A%20Chenlu%20Ye%20and%20Zhou%20Yu%20and%20Ziji%20Zhang%20and%20Hao%20Chen%20and%20Narayanan%20Sadagopan%20and%20Jing%20Huang%20and%20Tong%20Zhang%20and%20Anurag%20Beniwal%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20emerged%20to%20be%20a%0Apredominant%20paradigm%20for%20mathematical%20reasoning%20tasks%2C%20offering%20stable%0Aimprovements%20in%20reasoning%20ability.%20However%2C%20Outcome%20Reward%20Models%20%28ORMs%29%20in%0ARLVR%20are%20too%20coarse-grained%20to%20distinguish%20flawed%20reasoning%20within%20correct%0Aanswers%20or%20valid%20reasoning%20within%20incorrect%20answers.%20This%20lack%20of%20granularity%0Aintroduces%20noisy%20and%20misleading%20gradients%20significantly%20and%20hinders%20further%0Aprogress%20in%20reasoning%20process%20quality.%20While%20Process%20Reward%20Models%20%28PRMs%29%20offer%0Afine-grained%20guidance%20for%20intermediate%20steps%2C%20they%20frequently%20suffer%20from%0Ainaccuracies%20and%20are%20susceptible%20to%20reward%20hacking.%0A%20%20To%20resolve%20this%20dilemma%2C%20we%20introduce%20PRocess%20cOnsistency%20Filter%20%28PROF%29%2C%20an%0Aeffective%20data%20process%20curation%20method%20that%20harmonizes%20noisy%2C%20fine-grained%0Aprocess%20rewards%20with%20accurate%2C%20coarse-grained%20outcome%20rewards.%20Rather%20than%0Anaively%20blending%20PRM%20and%20ORM%20in%20the%20objective%20function%0A%28arXiv%3Aarchive/2506.18896%29%2C%20PROF%20leverages%20their%20complementary%20strengths%0Athrough%20consistency-driven%20sample%20selection.%20Our%20approach%20retains%20correct%0Aresponses%20with%20higher%20averaged%20process%20values%20and%20incorrect%20responses%20with%0Alower%20averaged%20process%20values%2C%20while%20maintaining%20positive/negative%20training%0Asample%20balance.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20not%20only%0Aconsistently%20improves%20the%20final%20accuracy%20over%20%244%5C%25%24%20compared%20to%20the%20blending%0Aapproaches%2C%20but%20also%20strengthens%20the%20quality%20of%20intermediate%20reasoning%20steps.%0ACodes%20and%20training%20recipes%20are%20available%20at%20https%3A//github.com/Chenluye99/PROF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Correctness%253A%2520Harmonizing%2520Process%2520and%2520Outcome%2520Rewards%2520through%2520RL%250A%2520%2520Training%26entry.906535625%3DChenlu%2520Ye%2520and%2520Zhou%2520Yu%2520and%2520Ziji%2520Zhang%2520and%2520Hao%2520Chen%2520and%2520Narayanan%2520Sadagopan%2520and%2520Jing%2520Huang%2520and%2520Tong%2520Zhang%2520and%2520Anurag%2520Beniwal%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520emerged%2520to%2520be%2520a%250Apredominant%2520paradigm%2520for%2520mathematical%2520reasoning%2520tasks%252C%2520offering%2520stable%250Aimprovements%2520in%2520reasoning%2520ability.%2520However%252C%2520Outcome%2520Reward%2520Models%2520%2528ORMs%2529%2520in%250ARLVR%2520are%2520too%2520coarse-grained%2520to%2520distinguish%2520flawed%2520reasoning%2520within%2520correct%250Aanswers%2520or%2520valid%2520reasoning%2520within%2520incorrect%2520answers.%2520This%2520lack%2520of%2520granularity%250Aintroduces%2520noisy%2520and%2520misleading%2520gradients%2520significantly%2520and%2520hinders%2520further%250Aprogress%2520in%2520reasoning%2520process%2520quality.%2520While%2520Process%2520Reward%2520Models%2520%2528PRMs%2529%2520offer%250Afine-grained%2520guidance%2520for%2520intermediate%2520steps%252C%2520they%2520frequently%2520suffer%2520from%250Ainaccuracies%2520and%2520are%2520susceptible%2520to%2520reward%2520hacking.%250A%2520%2520To%2520resolve%2520this%2520dilemma%252C%2520we%2520introduce%2520PRocess%2520cOnsistency%2520Filter%2520%2528PROF%2529%252C%2520an%250Aeffective%2520data%2520process%2520curation%2520method%2520that%2520harmonizes%2520noisy%252C%2520fine-grained%250Aprocess%2520rewards%2520with%2520accurate%252C%2520coarse-grained%2520outcome%2520rewards.%2520Rather%2520than%250Anaively%2520blending%2520PRM%2520and%2520ORM%2520in%2520the%2520objective%2520function%250A%2528arXiv%253Aarchive/2506.18896%2529%252C%2520PROF%2520leverages%2520their%2520complementary%2520strengths%250Athrough%2520consistency-driven%2520sample%2520selection.%2520Our%2520approach%2520retains%2520correct%250Aresponses%2520with%2520higher%2520averaged%2520process%2520values%2520and%2520incorrect%2520responses%2520with%250Alower%2520averaged%2520process%2520values%252C%2520while%2520maintaining%2520positive/negative%2520training%250Asample%2520balance.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520not%2520only%250Aconsistently%2520improves%2520the%2520final%2520accuracy%2520over%2520%25244%255C%2525%2524%2520compared%2520to%2520the%2520blending%250Aapproaches%252C%2520but%2520also%2520strengthens%2520the%2520quality%2520of%2520intermediate%2520reasoning%2520steps.%250ACodes%2520and%2520training%2520recipes%2520are%2520available%2520at%2520https%253A//github.com/Chenluye99/PROF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Correctness%3A%20Harmonizing%20Process%20and%20Outcome%20Rewards%20through%20RL%0A%20%20Training&entry.906535625=Chenlu%20Ye%20and%20Zhou%20Yu%20and%20Ziji%20Zhang%20and%20Hao%20Chen%20and%20Narayanan%20Sadagopan%20and%20Jing%20Huang%20and%20Tong%20Zhang%20and%20Anurag%20Beniwal&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20emerged%20to%20be%20a%0Apredominant%20paradigm%20for%20mathematical%20reasoning%20tasks%2C%20offering%20stable%0Aimprovements%20in%20reasoning%20ability.%20However%2C%20Outcome%20Reward%20Models%20%28ORMs%29%20in%0ARLVR%20are%20too%20coarse-grained%20to%20distinguish%20flawed%20reasoning%20within%20correct%0Aanswers%20or%20valid%20reasoning%20within%20incorrect%20answers.%20This%20lack%20of%20granularity%0Aintroduces%20noisy%20and%20misleading%20gradients%20significantly%20and%20hinders%20further%0Aprogress%20in%20reasoning%20process%20quality.%20While%20Process%20Reward%20Models%20%28PRMs%29%20offer%0Afine-grained%20guidance%20for%20intermediate%20steps%2C%20they%20frequently%20suffer%20from%0Ainaccuracies%20and%20are%20susceptible%20to%20reward%20hacking.%0A%20%20To%20resolve%20this%20dilemma%2C%20we%20introduce%20PRocess%20cOnsistency%20Filter%20%28PROF%29%2C%20an%0Aeffective%20data%20process%20curation%20method%20that%20harmonizes%20noisy%2C%20fine-grained%0Aprocess%20rewards%20with%20accurate%2C%20coarse-grained%20outcome%20rewards.%20Rather%20than%0Anaively%20blending%20PRM%20and%20ORM%20in%20the%20objective%20function%0A%28arXiv%3Aarchive/2506.18896%29%2C%20PROF%20leverages%20their%20complementary%20strengths%0Athrough%20consistency-driven%20sample%20selection.%20Our%20approach%20retains%20correct%0Aresponses%20with%20higher%20averaged%20process%20values%20and%20incorrect%20responses%20with%0Alower%20averaged%20process%20values%2C%20while%20maintaining%20positive/negative%20training%0Asample%20balance.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20not%20only%0Aconsistently%20improves%20the%20final%20accuracy%20over%20%244%5C%25%24%20compared%20to%20the%20blending%0Aapproaches%2C%20but%20also%20strengthens%20the%20quality%20of%20intermediate%20reasoning%20steps.%0ACodes%20and%20training%20recipes%20are%20available%20at%20https%3A//github.com/Chenluye99/PROF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03403v1&entry.124074799=Read"},
{"title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning", "author": "Zhenghai Xue and Longtao Zheng and Qian Liu and Yingru Li and Xiaosen Zheng and Zejun Ma and Bo An", "abstract": "  Large Language Models (LLMs) can significantly improve their reasoning\ncapabilities by interacting with external tools, a paradigm known as\nTool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios\nusing Reinforcement Learning (RL) is often hindered by training instability and\nperformance collapse. We identify that such instability is primarily caused by\na distributional drift from external tool feedback, leading to the generation\nof low-probability tokens. This issue compounds over successive turns, causing\ncatastrophic gradient norm explosions that derail the training process. To\naddress this challenge, we introduce SimpleTIR , a plug-and-play algorithm that\nstabilizes multi-turn TIR training. Its core strategy is to identify and filter\nout trajectories containing void turns, i.e., turns that yield neither a code\nblock nor a final answer. By removing these problematic trajectories from the\npolicy update, SimpleTIR effectively blocks the harmful, high-magnitude\ngradients, thus stabilizing the learning dynamics. Extensive experiments show\nthat SimpleTIR achieves state-of-the-art performance on challenging math\nreasoning benchmarks, notably elevating the AIME24 score from a text-only\nbaseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.\nFurthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR\nencourages the model to discover diverse and sophisticated reasoning patterns,\nsuch as self-correction and cross-validation.\n", "link": "http://arxiv.org/abs/2509.02479v2", "date": "2025-09-03", "relevancy": 1.9935, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5333}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimpleTIR%3A%20End-to-End%20Reinforcement%20Learning%20for%20Multi-Turn%0A%20%20Tool-Integrated%20Reasoning&body=Title%3A%20SimpleTIR%3A%20End-to-End%20Reinforcement%20Learning%20for%20Multi-Turn%0A%20%20Tool-Integrated%20Reasoning%0AAuthor%3A%20Zhenghai%20Xue%20and%20Longtao%20Zheng%20and%20Qian%20Liu%20and%20Yingru%20Li%20and%20Xiaosen%20Zheng%20and%20Zejun%20Ma%20and%20Bo%20An%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20can%20significantly%20improve%20their%20reasoning%0Acapabilities%20by%20interacting%20with%20external%20tools%2C%20a%20paradigm%20known%20as%0ATool-Integrated%20Reasoning%20%28TIR%29.%20However%2C%20extending%20TIR%20to%20multi-turn%20scenarios%0Ausing%20Reinforcement%20Learning%20%28RL%29%20is%20often%20hindered%20by%20training%20instability%20and%0Aperformance%20collapse.%20We%20identify%20that%20such%20instability%20is%20primarily%20caused%20by%0Aa%20distributional%20drift%20from%20external%20tool%20feedback%2C%20leading%20to%20the%20generation%0Aof%20low-probability%20tokens.%20This%20issue%20compounds%20over%20successive%20turns%2C%20causing%0Acatastrophic%20gradient%20norm%20explosions%20that%20derail%20the%20training%20process.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20SimpleTIR%20%2C%20a%20plug-and-play%20algorithm%20that%0Astabilizes%20multi-turn%20TIR%20training.%20Its%20core%20strategy%20is%20to%20identify%20and%20filter%0Aout%20trajectories%20containing%20void%20turns%2C%20i.e.%2C%20turns%20that%20yield%20neither%20a%20code%0Ablock%20nor%20a%20final%20answer.%20By%20removing%20these%20problematic%20trajectories%20from%20the%0Apolicy%20update%2C%20SimpleTIR%20effectively%20blocks%20the%20harmful%2C%20high-magnitude%0Agradients%2C%20thus%20stabilizing%20the%20learning%20dynamics.%20Extensive%20experiments%20show%0Athat%20SimpleTIR%20achieves%20state-of-the-art%20performance%20on%20challenging%20math%0Areasoning%20benchmarks%2C%20notably%20elevating%20the%20AIME24%20score%20from%20a%20text-only%0Abaseline%20of%2022.1%20to%2050.5%20when%20starting%20from%20the%20Qwen2.5-7B%20base%20model.%0AFurthermore%2C%20by%20avoiding%20the%20constraints%20of%20supervised%20fine-tuning%2C%20SimpleTIR%0Aencourages%20the%20model%20to%20discover%20diverse%20and%20sophisticated%20reasoning%20patterns%2C%0Asuch%20as%20self-correction%20and%20cross-validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02479v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimpleTIR%253A%2520End-to-End%2520Reinforcement%2520Learning%2520for%2520Multi-Turn%250A%2520%2520Tool-Integrated%2520Reasoning%26entry.906535625%3DZhenghai%2520Xue%2520and%2520Longtao%2520Zheng%2520and%2520Qian%2520Liu%2520and%2520Yingru%2520Li%2520and%2520Xiaosen%2520Zheng%2520and%2520Zejun%2520Ma%2520and%2520Bo%2520An%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520significantly%2520improve%2520their%2520reasoning%250Acapabilities%2520by%2520interacting%2520with%2520external%2520tools%252C%2520a%2520paradigm%2520known%2520as%250ATool-Integrated%2520Reasoning%2520%2528TIR%2529.%2520However%252C%2520extending%2520TIR%2520to%2520multi-turn%2520scenarios%250Ausing%2520Reinforcement%2520Learning%2520%2528RL%2529%2520is%2520often%2520hindered%2520by%2520training%2520instability%2520and%250Aperformance%2520collapse.%2520We%2520identify%2520that%2520such%2520instability%2520is%2520primarily%2520caused%2520by%250Aa%2520distributional%2520drift%2520from%2520external%2520tool%2520feedback%252C%2520leading%2520to%2520the%2520generation%250Aof%2520low-probability%2520tokens.%2520This%2520issue%2520compounds%2520over%2520successive%2520turns%252C%2520causing%250Acatastrophic%2520gradient%2520norm%2520explosions%2520that%2520derail%2520the%2520training%2520process.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520SimpleTIR%2520%252C%2520a%2520plug-and-play%2520algorithm%2520that%250Astabilizes%2520multi-turn%2520TIR%2520training.%2520Its%2520core%2520strategy%2520is%2520to%2520identify%2520and%2520filter%250Aout%2520trajectories%2520containing%2520void%2520turns%252C%2520i.e.%252C%2520turns%2520that%2520yield%2520neither%2520a%2520code%250Ablock%2520nor%2520a%2520final%2520answer.%2520By%2520removing%2520these%2520problematic%2520trajectories%2520from%2520the%250Apolicy%2520update%252C%2520SimpleTIR%2520effectively%2520blocks%2520the%2520harmful%252C%2520high-magnitude%250Agradients%252C%2520thus%2520stabilizing%2520the%2520learning%2520dynamics.%2520Extensive%2520experiments%2520show%250Athat%2520SimpleTIR%2520achieves%2520state-of-the-art%2520performance%2520on%2520challenging%2520math%250Areasoning%2520benchmarks%252C%2520notably%2520elevating%2520the%2520AIME24%2520score%2520from%2520a%2520text-only%250Abaseline%2520of%252022.1%2520to%252050.5%2520when%2520starting%2520from%2520the%2520Qwen2.5-7B%2520base%2520model.%250AFurthermore%252C%2520by%2520avoiding%2520the%2520constraints%2520of%2520supervised%2520fine-tuning%252C%2520SimpleTIR%250Aencourages%2520the%2520model%2520to%2520discover%2520diverse%2520and%2520sophisticated%2520reasoning%2520patterns%252C%250Asuch%2520as%2520self-correction%2520and%2520cross-validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02479v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimpleTIR%3A%20End-to-End%20Reinforcement%20Learning%20for%20Multi-Turn%0A%20%20Tool-Integrated%20Reasoning&entry.906535625=Zhenghai%20Xue%20and%20Longtao%20Zheng%20and%20Qian%20Liu%20and%20Yingru%20Li%20and%20Xiaosen%20Zheng%20and%20Zejun%20Ma%20and%20Bo%20An&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20can%20significantly%20improve%20their%20reasoning%0Acapabilities%20by%20interacting%20with%20external%20tools%2C%20a%20paradigm%20known%20as%0ATool-Integrated%20Reasoning%20%28TIR%29.%20However%2C%20extending%20TIR%20to%20multi-turn%20scenarios%0Ausing%20Reinforcement%20Learning%20%28RL%29%20is%20often%20hindered%20by%20training%20instability%20and%0Aperformance%20collapse.%20We%20identify%20that%20such%20instability%20is%20primarily%20caused%20by%0Aa%20distributional%20drift%20from%20external%20tool%20feedback%2C%20leading%20to%20the%20generation%0Aof%20low-probability%20tokens.%20This%20issue%20compounds%20over%20successive%20turns%2C%20causing%0Acatastrophic%20gradient%20norm%20explosions%20that%20derail%20the%20training%20process.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20SimpleTIR%20%2C%20a%20plug-and-play%20algorithm%20that%0Astabilizes%20multi-turn%20TIR%20training.%20Its%20core%20strategy%20is%20to%20identify%20and%20filter%0Aout%20trajectories%20containing%20void%20turns%2C%20i.e.%2C%20turns%20that%20yield%20neither%20a%20code%0Ablock%20nor%20a%20final%20answer.%20By%20removing%20these%20problematic%20trajectories%20from%20the%0Apolicy%20update%2C%20SimpleTIR%20effectively%20blocks%20the%20harmful%2C%20high-magnitude%0Agradients%2C%20thus%20stabilizing%20the%20learning%20dynamics.%20Extensive%20experiments%20show%0Athat%20SimpleTIR%20achieves%20state-of-the-art%20performance%20on%20challenging%20math%0Areasoning%20benchmarks%2C%20notably%20elevating%20the%20AIME24%20score%20from%20a%20text-only%0Abaseline%20of%2022.1%20to%2050.5%20when%20starting%20from%20the%20Qwen2.5-7B%20base%20model.%0AFurthermore%2C%20by%20avoiding%20the%20constraints%20of%20supervised%20fine-tuning%2C%20SimpleTIR%0Aencourages%20the%20model%20to%20discover%20diverse%20and%20sophisticated%20reasoning%20patterns%2C%0Asuch%20as%20self-correction%20and%20cross-validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02479v2&entry.124074799=Read"},
{"title": "Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling?\n  A Validation Study with Naturalistic Trajectories", "author": "Yanlin Zhang and Sungyong Chung and Nachuan Li and Dana Monzer and Hani S. Mahmassani and Samer H. Hamdar and Alireza Talebpour", "abstract": "  The Waymo Open Motion Dataset (WOMD) has become a popular resource for\ndata-driven modeling of autonomous vehicles (AVs) behavior. However, its\nvalidity for behavioral analysis remains uncertain due to proprietary\npost-processing, the absence of error quantification, and the segmentation of\ntrajectories into 20-second clips. This study examines whether WOMD accurately\ncaptures the dynamics and interactions observed in real-world AV operations.\nLeveraging an independently collected naturalistic dataset from Level 4 AV\noperations in Phoenix, Arizona (PHX), we perform comparative analyses across\nthree representative urban driving scenarios: discharging at signalized\nintersections, car-following, and lane-changing behaviors. For the discharging\nanalysis, headways are manually extracted from aerial video to ensure\nnegligible measurement error. For the car-following and lane-changing cases, we\napply the Simulation-Extrapolation (SIMEX) method to account for empirically\nestimated error in the PHX data and use Dynamic Time Warping (DTW) distances to\nquantify behavioral differences. Results across all scenarios consistently show\nthat behavior in PHX falls outside the behavioral envelope of WOMD. Notably,\nWOMD underrepresents short headways and abrupt decelerations. These findings\nsuggest that behavioral models calibrated solely on WOMD may systematically\nunderestimate the variability, risk, and complexity of naturalistic driving.\nCaution is therefore warranted when using WOMD for behavior modeling without\nproper validation against independently collected data.\n", "link": "http://arxiv.org/abs/2509.03515v1", "date": "2025-09-03", "relevancy": 1.9644, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5262}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4979}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20the%20Waymo%20Open%20Motion%20Dataset%20Support%20Realistic%20Behavioral%20Modeling%3F%0A%20%20A%20Validation%20Study%20with%20Naturalistic%20Trajectories&body=Title%3A%20Can%20the%20Waymo%20Open%20Motion%20Dataset%20Support%20Realistic%20Behavioral%20Modeling%3F%0A%20%20A%20Validation%20Study%20with%20Naturalistic%20Trajectories%0AAuthor%3A%20Yanlin%20Zhang%20and%20Sungyong%20Chung%20and%20Nachuan%20Li%20and%20Dana%20Monzer%20and%20Hani%20S.%20Mahmassani%20and%20Samer%20H.%20Hamdar%20and%20Alireza%20Talebpour%0AAbstract%3A%20%20%20The%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29%20has%20become%20a%20popular%20resource%20for%0Adata-driven%20modeling%20of%20autonomous%20vehicles%20%28AVs%29%20behavior.%20However%2C%20its%0Avalidity%20for%20behavioral%20analysis%20remains%20uncertain%20due%20to%20proprietary%0Apost-processing%2C%20the%20absence%20of%20error%20quantification%2C%20and%20the%20segmentation%20of%0Atrajectories%20into%2020-second%20clips.%20This%20study%20examines%20whether%20WOMD%20accurately%0Acaptures%20the%20dynamics%20and%20interactions%20observed%20in%20real-world%20AV%20operations.%0ALeveraging%20an%20independently%20collected%20naturalistic%20dataset%20from%20Level%204%20AV%0Aoperations%20in%20Phoenix%2C%20Arizona%20%28PHX%29%2C%20we%20perform%20comparative%20analyses%20across%0Athree%20representative%20urban%20driving%20scenarios%3A%20discharging%20at%20signalized%0Aintersections%2C%20car-following%2C%20and%20lane-changing%20behaviors.%20For%20the%20discharging%0Aanalysis%2C%20headways%20are%20manually%20extracted%20from%20aerial%20video%20to%20ensure%0Anegligible%20measurement%20error.%20For%20the%20car-following%20and%20lane-changing%20cases%2C%20we%0Aapply%20the%20Simulation-Extrapolation%20%28SIMEX%29%20method%20to%20account%20for%20empirically%0Aestimated%20error%20in%20the%20PHX%20data%20and%20use%20Dynamic%20Time%20Warping%20%28DTW%29%20distances%20to%0Aquantify%20behavioral%20differences.%20Results%20across%20all%20scenarios%20consistently%20show%0Athat%20behavior%20in%20PHX%20falls%20outside%20the%20behavioral%20envelope%20of%20WOMD.%20Notably%2C%0AWOMD%20underrepresents%20short%20headways%20and%20abrupt%20decelerations.%20These%20findings%0Asuggest%20that%20behavioral%20models%20calibrated%20solely%20on%20WOMD%20may%20systematically%0Aunderestimate%20the%20variability%2C%20risk%2C%20and%20complexity%20of%20naturalistic%20driving.%0ACaution%20is%20therefore%20warranted%20when%20using%20WOMD%20for%20behavior%20modeling%20without%0Aproper%20validation%20against%20independently%20collected%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%2520Support%2520Realistic%2520Behavioral%2520Modeling%253F%250A%2520%2520A%2520Validation%2520Study%2520with%2520Naturalistic%2520Trajectories%26entry.906535625%3DYanlin%2520Zhang%2520and%2520Sungyong%2520Chung%2520and%2520Nachuan%2520Li%2520and%2520Dana%2520Monzer%2520and%2520Hani%2520S.%2520Mahmassani%2520and%2520Samer%2520H.%2520Hamdar%2520and%2520Alireza%2520Talebpour%26entry.1292438233%3D%2520%2520The%2520Waymo%2520Open%2520Motion%2520Dataset%2520%2528WOMD%2529%2520has%2520become%2520a%2520popular%2520resource%2520for%250Adata-driven%2520modeling%2520of%2520autonomous%2520vehicles%2520%2528AVs%2529%2520behavior.%2520However%252C%2520its%250Avalidity%2520for%2520behavioral%2520analysis%2520remains%2520uncertain%2520due%2520to%2520proprietary%250Apost-processing%252C%2520the%2520absence%2520of%2520error%2520quantification%252C%2520and%2520the%2520segmentation%2520of%250Atrajectories%2520into%252020-second%2520clips.%2520This%2520study%2520examines%2520whether%2520WOMD%2520accurately%250Acaptures%2520the%2520dynamics%2520and%2520interactions%2520observed%2520in%2520real-world%2520AV%2520operations.%250ALeveraging%2520an%2520independently%2520collected%2520naturalistic%2520dataset%2520from%2520Level%25204%2520AV%250Aoperations%2520in%2520Phoenix%252C%2520Arizona%2520%2528PHX%2529%252C%2520we%2520perform%2520comparative%2520analyses%2520across%250Athree%2520representative%2520urban%2520driving%2520scenarios%253A%2520discharging%2520at%2520signalized%250Aintersections%252C%2520car-following%252C%2520and%2520lane-changing%2520behaviors.%2520For%2520the%2520discharging%250Aanalysis%252C%2520headways%2520are%2520manually%2520extracted%2520from%2520aerial%2520video%2520to%2520ensure%250Anegligible%2520measurement%2520error.%2520For%2520the%2520car-following%2520and%2520lane-changing%2520cases%252C%2520we%250Aapply%2520the%2520Simulation-Extrapolation%2520%2528SIMEX%2529%2520method%2520to%2520account%2520for%2520empirically%250Aestimated%2520error%2520in%2520the%2520PHX%2520data%2520and%2520use%2520Dynamic%2520Time%2520Warping%2520%2528DTW%2529%2520distances%2520to%250Aquantify%2520behavioral%2520differences.%2520Results%2520across%2520all%2520scenarios%2520consistently%2520show%250Athat%2520behavior%2520in%2520PHX%2520falls%2520outside%2520the%2520behavioral%2520envelope%2520of%2520WOMD.%2520Notably%252C%250AWOMD%2520underrepresents%2520short%2520headways%2520and%2520abrupt%2520decelerations.%2520These%2520findings%250Asuggest%2520that%2520behavioral%2520models%2520calibrated%2520solely%2520on%2520WOMD%2520may%2520systematically%250Aunderestimate%2520the%2520variability%252C%2520risk%252C%2520and%2520complexity%2520of%2520naturalistic%2520driving.%250ACaution%2520is%2520therefore%2520warranted%2520when%2520using%2520WOMD%2520for%2520behavior%2520modeling%2520without%250Aproper%2520validation%2520against%2520independently%2520collected%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20the%20Waymo%20Open%20Motion%20Dataset%20Support%20Realistic%20Behavioral%20Modeling%3F%0A%20%20A%20Validation%20Study%20with%20Naturalistic%20Trajectories&entry.906535625=Yanlin%20Zhang%20and%20Sungyong%20Chung%20and%20Nachuan%20Li%20and%20Dana%20Monzer%20and%20Hani%20S.%20Mahmassani%20and%20Samer%20H.%20Hamdar%20and%20Alireza%20Talebpour&entry.1292438233=%20%20The%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29%20has%20become%20a%20popular%20resource%20for%0Adata-driven%20modeling%20of%20autonomous%20vehicles%20%28AVs%29%20behavior.%20However%2C%20its%0Avalidity%20for%20behavioral%20analysis%20remains%20uncertain%20due%20to%20proprietary%0Apost-processing%2C%20the%20absence%20of%20error%20quantification%2C%20and%20the%20segmentation%20of%0Atrajectories%20into%2020-second%20clips.%20This%20study%20examines%20whether%20WOMD%20accurately%0Acaptures%20the%20dynamics%20and%20interactions%20observed%20in%20real-world%20AV%20operations.%0ALeveraging%20an%20independently%20collected%20naturalistic%20dataset%20from%20Level%204%20AV%0Aoperations%20in%20Phoenix%2C%20Arizona%20%28PHX%29%2C%20we%20perform%20comparative%20analyses%20across%0Athree%20representative%20urban%20driving%20scenarios%3A%20discharging%20at%20signalized%0Aintersections%2C%20car-following%2C%20and%20lane-changing%20behaviors.%20For%20the%20discharging%0Aanalysis%2C%20headways%20are%20manually%20extracted%20from%20aerial%20video%20to%20ensure%0Anegligible%20measurement%20error.%20For%20the%20car-following%20and%20lane-changing%20cases%2C%20we%0Aapply%20the%20Simulation-Extrapolation%20%28SIMEX%29%20method%20to%20account%20for%20empirically%0Aestimated%20error%20in%20the%20PHX%20data%20and%20use%20Dynamic%20Time%20Warping%20%28DTW%29%20distances%20to%0Aquantify%20behavioral%20differences.%20Results%20across%20all%20scenarios%20consistently%20show%0Athat%20behavior%20in%20PHX%20falls%20outside%20the%20behavioral%20envelope%20of%20WOMD.%20Notably%2C%0AWOMD%20underrepresents%20short%20headways%20and%20abrupt%20decelerations.%20These%20findings%0Asuggest%20that%20behavioral%20models%20calibrated%20solely%20on%20WOMD%20may%20systematically%0Aunderestimate%20the%20variability%2C%20risk%2C%20and%20complexity%20of%20naturalistic%20driving.%0ACaution%20is%20therefore%20warranted%20when%20using%20WOMD%20for%20behavior%20modeling%20without%0Aproper%20validation%20against%20independently%20collected%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03515v1&entry.124074799=Read"},
{"title": "Insertion Language Models: Sequence Generation with Arbitrary-Position\n  Insertions", "author": "Dhruvesh Patel and Aishwarya Sahoo and Avinash Amballa and Tahira Naseem and Tim G. J. Rudner and Andrew McCallum", "abstract": "  Autoregressive models (ARMs), which predict subsequent tokens one-by-one\n``from left to right,'' have achieved significant success across a wide range\nof sequence generation tasks. However, they struggle to accurately represent\nsequences that require satisfying sophisticated constraints or whose sequential\ndependencies are better addressed by out-of-order generation. Masked Diffusion\nModels (MDMs) address some of these limitations, but the process of unmasking\nmultiple tokens simultaneously in MDMs can introduce incoherences, and MDMs\ncannot handle arbitrary infilling constraints when the number of tokens to be\nfilled in is not known in advance. In this work, we introduce Insertion\nLanguage Models (ILMs), which learn to insert tokens at arbitrary positions in\na sequence -- that is, they select jointly both the position and the vocabulary\nelement to be inserted. By inserting tokens one at a time, ILMs can represent\nstrong dependencies between tokens, and their ability to generate sequences in\narbitrary order allows them to accurately model sequences where token\ndependencies do not follow a left-to-right sequential structure. To train ILMs,\nwe propose a tailored network parameterization and use a simple denoising\nobjective. Our empirical evaluation demonstrates that ILMs outperform both ARMs\nand MDMs on common planning tasks. Furthermore, we show that ILMs outperform\nMDMs and perform on par with ARMs in an unconditional text generation task\nwhile offering greater flexibility than MDMs in arbitrary-length text\ninfilling. The code is available at: https://dhruveshp.com/projects/ilm .\n", "link": "http://arxiv.org/abs/2505.05755v3", "date": "2025-09-03", "relevancy": 1.9626, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4998}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4875}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insertion%20Language%20Models%3A%20Sequence%20Generation%20with%20Arbitrary-Position%0A%20%20Insertions&body=Title%3A%20Insertion%20Language%20Models%3A%20Sequence%20Generation%20with%20Arbitrary-Position%0A%20%20Insertions%0AAuthor%3A%20Dhruvesh%20Patel%20and%20Aishwarya%20Sahoo%20and%20Avinash%20Amballa%20and%20Tahira%20Naseem%20and%20Tim%20G.%20J.%20Rudner%20and%20Andrew%20McCallum%0AAbstract%3A%20%20%20Autoregressive%20models%20%28ARMs%29%2C%20which%20predict%20subsequent%20tokens%20one-by-one%0A%60%60from%20left%20to%20right%2C%27%27%20have%20achieved%20significant%20success%20across%20a%20wide%20range%0Aof%20sequence%20generation%20tasks.%20However%2C%20they%20struggle%20to%20accurately%20represent%0Asequences%20that%20require%20satisfying%20sophisticated%20constraints%20or%20whose%20sequential%0Adependencies%20are%20better%20addressed%20by%20out-of-order%20generation.%20Masked%20Diffusion%0AModels%20%28MDMs%29%20address%20some%20of%20these%20limitations%2C%20but%20the%20process%20of%20unmasking%0Amultiple%20tokens%20simultaneously%20in%20MDMs%20can%20introduce%20incoherences%2C%20and%20MDMs%0Acannot%20handle%20arbitrary%20infilling%20constraints%20when%20the%20number%20of%20tokens%20to%20be%0Afilled%20in%20is%20not%20known%20in%20advance.%20In%20this%20work%2C%20we%20introduce%20Insertion%0ALanguage%20Models%20%28ILMs%29%2C%20which%20learn%20to%20insert%20tokens%20at%20arbitrary%20positions%20in%0Aa%20sequence%20--%20that%20is%2C%20they%20select%20jointly%20both%20the%20position%20and%20the%20vocabulary%0Aelement%20to%20be%20inserted.%20By%20inserting%20tokens%20one%20at%20a%20time%2C%20ILMs%20can%20represent%0Astrong%20dependencies%20between%20tokens%2C%20and%20their%20ability%20to%20generate%20sequences%20in%0Aarbitrary%20order%20allows%20them%20to%20accurately%20model%20sequences%20where%20token%0Adependencies%20do%20not%20follow%20a%20left-to-right%20sequential%20structure.%20To%20train%20ILMs%2C%0Awe%20propose%20a%20tailored%20network%20parameterization%20and%20use%20a%20simple%20denoising%0Aobjective.%20Our%20empirical%20evaluation%20demonstrates%20that%20ILMs%20outperform%20both%20ARMs%0Aand%20MDMs%20on%20common%20planning%20tasks.%20Furthermore%2C%20we%20show%20that%20ILMs%20outperform%0AMDMs%20and%20perform%20on%20par%20with%20ARMs%20in%20an%20unconditional%20text%20generation%20task%0Awhile%20offering%20greater%20flexibility%20than%20MDMs%20in%20arbitrary-length%20text%0Ainfilling.%20The%20code%20is%20available%20at%3A%20https%3A//dhruveshp.com/projects/ilm%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05755v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsertion%2520Language%2520Models%253A%2520Sequence%2520Generation%2520with%2520Arbitrary-Position%250A%2520%2520Insertions%26entry.906535625%3DDhruvesh%2520Patel%2520and%2520Aishwarya%2520Sahoo%2520and%2520Avinash%2520Amballa%2520and%2520Tahira%2520Naseem%2520and%2520Tim%2520G.%2520J.%2520Rudner%2520and%2520Andrew%2520McCallum%26entry.1292438233%3D%2520%2520Autoregressive%2520models%2520%2528ARMs%2529%252C%2520which%2520predict%2520subsequent%2520tokens%2520one-by-one%250A%2560%2560from%2520left%2520to%2520right%252C%2527%2527%2520have%2520achieved%2520significant%2520success%2520across%2520a%2520wide%2520range%250Aof%2520sequence%2520generation%2520tasks.%2520However%252C%2520they%2520struggle%2520to%2520accurately%2520represent%250Asequences%2520that%2520require%2520satisfying%2520sophisticated%2520constraints%2520or%2520whose%2520sequential%250Adependencies%2520are%2520better%2520addressed%2520by%2520out-of-order%2520generation.%2520Masked%2520Diffusion%250AModels%2520%2528MDMs%2529%2520address%2520some%2520of%2520these%2520limitations%252C%2520but%2520the%2520process%2520of%2520unmasking%250Amultiple%2520tokens%2520simultaneously%2520in%2520MDMs%2520can%2520introduce%2520incoherences%252C%2520and%2520MDMs%250Acannot%2520handle%2520arbitrary%2520infilling%2520constraints%2520when%2520the%2520number%2520of%2520tokens%2520to%2520be%250Afilled%2520in%2520is%2520not%2520known%2520in%2520advance.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Insertion%250ALanguage%2520Models%2520%2528ILMs%2529%252C%2520which%2520learn%2520to%2520insert%2520tokens%2520at%2520arbitrary%2520positions%2520in%250Aa%2520sequence%2520--%2520that%2520is%252C%2520they%2520select%2520jointly%2520both%2520the%2520position%2520and%2520the%2520vocabulary%250Aelement%2520to%2520be%2520inserted.%2520By%2520inserting%2520tokens%2520one%2520at%2520a%2520time%252C%2520ILMs%2520can%2520represent%250Astrong%2520dependencies%2520between%2520tokens%252C%2520and%2520their%2520ability%2520to%2520generate%2520sequences%2520in%250Aarbitrary%2520order%2520allows%2520them%2520to%2520accurately%2520model%2520sequences%2520where%2520token%250Adependencies%2520do%2520not%2520follow%2520a%2520left-to-right%2520sequential%2520structure.%2520To%2520train%2520ILMs%252C%250Awe%2520propose%2520a%2520tailored%2520network%2520parameterization%2520and%2520use%2520a%2520simple%2520denoising%250Aobjective.%2520Our%2520empirical%2520evaluation%2520demonstrates%2520that%2520ILMs%2520outperform%2520both%2520ARMs%250Aand%2520MDMs%2520on%2520common%2520planning%2520tasks.%2520Furthermore%252C%2520we%2520show%2520that%2520ILMs%2520outperform%250AMDMs%2520and%2520perform%2520on%2520par%2520with%2520ARMs%2520in%2520an%2520unconditional%2520text%2520generation%2520task%250Awhile%2520offering%2520greater%2520flexibility%2520than%2520MDMs%2520in%2520arbitrary-length%2520text%250Ainfilling.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//dhruveshp.com/projects/ilm%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05755v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insertion%20Language%20Models%3A%20Sequence%20Generation%20with%20Arbitrary-Position%0A%20%20Insertions&entry.906535625=Dhruvesh%20Patel%20and%20Aishwarya%20Sahoo%20and%20Avinash%20Amballa%20and%20Tahira%20Naseem%20and%20Tim%20G.%20J.%20Rudner%20and%20Andrew%20McCallum&entry.1292438233=%20%20Autoregressive%20models%20%28ARMs%29%2C%20which%20predict%20subsequent%20tokens%20one-by-one%0A%60%60from%20left%20to%20right%2C%27%27%20have%20achieved%20significant%20success%20across%20a%20wide%20range%0Aof%20sequence%20generation%20tasks.%20However%2C%20they%20struggle%20to%20accurately%20represent%0Asequences%20that%20require%20satisfying%20sophisticated%20constraints%20or%20whose%20sequential%0Adependencies%20are%20better%20addressed%20by%20out-of-order%20generation.%20Masked%20Diffusion%0AModels%20%28MDMs%29%20address%20some%20of%20these%20limitations%2C%20but%20the%20process%20of%20unmasking%0Amultiple%20tokens%20simultaneously%20in%20MDMs%20can%20introduce%20incoherences%2C%20and%20MDMs%0Acannot%20handle%20arbitrary%20infilling%20constraints%20when%20the%20number%20of%20tokens%20to%20be%0Afilled%20in%20is%20not%20known%20in%20advance.%20In%20this%20work%2C%20we%20introduce%20Insertion%0ALanguage%20Models%20%28ILMs%29%2C%20which%20learn%20to%20insert%20tokens%20at%20arbitrary%20positions%20in%0Aa%20sequence%20--%20that%20is%2C%20they%20select%20jointly%20both%20the%20position%20and%20the%20vocabulary%0Aelement%20to%20be%20inserted.%20By%20inserting%20tokens%20one%20at%20a%20time%2C%20ILMs%20can%20represent%0Astrong%20dependencies%20between%20tokens%2C%20and%20their%20ability%20to%20generate%20sequences%20in%0Aarbitrary%20order%20allows%20them%20to%20accurately%20model%20sequences%20where%20token%0Adependencies%20do%20not%20follow%20a%20left-to-right%20sequential%20structure.%20To%20train%20ILMs%2C%0Awe%20propose%20a%20tailored%20network%20parameterization%20and%20use%20a%20simple%20denoising%0Aobjective.%20Our%20empirical%20evaluation%20demonstrates%20that%20ILMs%20outperform%20both%20ARMs%0Aand%20MDMs%20on%20common%20planning%20tasks.%20Furthermore%2C%20we%20show%20that%20ILMs%20outperform%0AMDMs%20and%20perform%20on%20par%20with%20ARMs%20in%20an%20unconditional%20text%20generation%20task%0Awhile%20offering%20greater%20flexibility%20than%20MDMs%20in%20arbitrary-length%20text%0Ainfilling.%20The%20code%20is%20available%20at%3A%20https%3A//dhruveshp.com/projects/ilm%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05755v3&entry.124074799=Read"},
{"title": "Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel\n  GAT-MLP", "author": "Xiang Li and Shanshan Wang and Chenglong Xiao", "abstract": "  Extensive experiments and prior studies show that no single maximum clique\nalgorithm consistently performs best across all instances, highlighting the\nimportance of selecting suitable algorithms based on instance features. Through\nan extensive analysis of relevant studies, it is found that there is a lack of\nresearch work concerning algorithm selection oriented toward the Maximum Clique\nProblem (MCP). In this work, we propose a learning-based framework that\nintegrates both traditional machine learning and graph neural networks to\naddress this gap. We construct a labeled dataset by running four exact MCP\nalgorithms on a diverse collection of graph instances, accompanied by\nstructural and global statistical features extracted from each graph. We first\nevaluate four conventional classifiers: Support Vector Machine (SVM), Random\nForest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple\ndataset variants. Experimental results show that RF consistently shows strong\nperformance across metrics and dataset variants, making it a reliable baseline.\nIn addition, feature importance analysis indicates that connectivity and\ntopological structure are strong predictors of algorithm performance. Building\non these findings, we develop a dual-channel model named GAT-MLP, which\ncombines a Graph Attention Network (GAT) for local structural encoding with a\nMultilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model\nshows strong and consistent performance across all metrics. Our results\nhighlight the effectiveness of dual-channel architectures and the promise of\ngraph neural networks in combinatorial algorithm selection.\n", "link": "http://arxiv.org/abs/2508.08005v2", "date": "2025-09-03", "relevancy": 1.9493, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Select%20MCP%20Algorithms%3A%20From%20Traditional%20ML%20to%20Dual-Channel%0A%20%20GAT-MLP&body=Title%3A%20Learning%20to%20Select%20MCP%20Algorithms%3A%20From%20Traditional%20ML%20to%20Dual-Channel%0A%20%20GAT-MLP%0AAuthor%3A%20Xiang%20Li%20and%20Shanshan%20Wang%20and%20Chenglong%20Xiao%0AAbstract%3A%20%20%20Extensive%20experiments%20and%20prior%20studies%20show%20that%20no%20single%20maximum%20clique%0Aalgorithm%20consistently%20performs%20best%20across%20all%20instances%2C%20highlighting%20the%0Aimportance%20of%20selecting%20suitable%20algorithms%20based%20on%20instance%20features.%20Through%0Aan%20extensive%20analysis%20of%20relevant%20studies%2C%20it%20is%20found%20that%20there%20is%20a%20lack%20of%0Aresearch%20work%20concerning%20algorithm%20selection%20oriented%20toward%20the%20Maximum%20Clique%0AProblem%20%28MCP%29.%20In%20this%20work%2C%20we%20propose%20a%20learning-based%20framework%20that%0Aintegrates%20both%20traditional%20machine%20learning%20and%20graph%20neural%20networks%20to%0Aaddress%20this%20gap.%20We%20construct%20a%20labeled%20dataset%20by%20running%20four%20exact%20MCP%0Aalgorithms%20on%20a%20diverse%20collection%20of%20graph%20instances%2C%20accompanied%20by%0Astructural%20and%20global%20statistical%20features%20extracted%20from%20each%20graph.%20We%20first%0Aevaluate%20four%20conventional%20classifiers%3A%20Support%20Vector%20Machine%20%28SVM%29%2C%20Random%0AForest%20%28RF%29%2C%20Decision%20Tree%20%28DT%29%2C%20and%20K-Nearest%20Neighbors%20%28KNN%29%2C%20across%20multiple%0Adataset%20variants.%20Experimental%20results%20show%20that%20RF%20consistently%20shows%20strong%0Aperformance%20across%20metrics%20and%20dataset%20variants%2C%20making%20it%20a%20reliable%20baseline.%0AIn%20addition%2C%20feature%20importance%20analysis%20indicates%20that%20connectivity%20and%0Atopological%20structure%20are%20strong%20predictors%20of%20algorithm%20performance.%20Building%0Aon%20these%20findings%2C%20we%20develop%20a%20dual-channel%20model%20named%20GAT-MLP%2C%20which%0Acombines%20a%20Graph%20Attention%20Network%20%28GAT%29%20for%20local%20structural%20encoding%20with%20a%0AMultilayer%20Perceptron%20%28MLP%29%20for%20global%20feature%20modeling.%20The%20GAT-MLP%20model%0Ashows%20strong%20and%20consistent%20performance%20across%20all%20metrics.%20Our%20results%0Ahighlight%20the%20effectiveness%20of%20dual-channel%20architectures%20and%20the%20promise%20of%0Agraph%20neural%20networks%20in%20combinatorial%20algorithm%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Select%2520MCP%2520Algorithms%253A%2520From%2520Traditional%2520ML%2520to%2520Dual-Channel%250A%2520%2520GAT-MLP%26entry.906535625%3DXiang%2520Li%2520and%2520Shanshan%2520Wang%2520and%2520Chenglong%2520Xiao%26entry.1292438233%3D%2520%2520Extensive%2520experiments%2520and%2520prior%2520studies%2520show%2520that%2520no%2520single%2520maximum%2520clique%250Aalgorithm%2520consistently%2520performs%2520best%2520across%2520all%2520instances%252C%2520highlighting%2520the%250Aimportance%2520of%2520selecting%2520suitable%2520algorithms%2520based%2520on%2520instance%2520features.%2520Through%250Aan%2520extensive%2520analysis%2520of%2520relevant%2520studies%252C%2520it%2520is%2520found%2520that%2520there%2520is%2520a%2520lack%2520of%250Aresearch%2520work%2520concerning%2520algorithm%2520selection%2520oriented%2520toward%2520the%2520Maximum%2520Clique%250AProblem%2520%2528MCP%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520learning-based%2520framework%2520that%250Aintegrates%2520both%2520traditional%2520machine%2520learning%2520and%2520graph%2520neural%2520networks%2520to%250Aaddress%2520this%2520gap.%2520We%2520construct%2520a%2520labeled%2520dataset%2520by%2520running%2520four%2520exact%2520MCP%250Aalgorithms%2520on%2520a%2520diverse%2520collection%2520of%2520graph%2520instances%252C%2520accompanied%2520by%250Astructural%2520and%2520global%2520statistical%2520features%2520extracted%2520from%2520each%2520graph.%2520We%2520first%250Aevaluate%2520four%2520conventional%2520classifiers%253A%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520Random%250AForest%2520%2528RF%2529%252C%2520Decision%2520Tree%2520%2528DT%2529%252C%2520and%2520K-Nearest%2520Neighbors%2520%2528KNN%2529%252C%2520across%2520multiple%250Adataset%2520variants.%2520Experimental%2520results%2520show%2520that%2520RF%2520consistently%2520shows%2520strong%250Aperformance%2520across%2520metrics%2520and%2520dataset%2520variants%252C%2520making%2520it%2520a%2520reliable%2520baseline.%250AIn%2520addition%252C%2520feature%2520importance%2520analysis%2520indicates%2520that%2520connectivity%2520and%250Atopological%2520structure%2520are%2520strong%2520predictors%2520of%2520algorithm%2520performance.%2520Building%250Aon%2520these%2520findings%252C%2520we%2520develop%2520a%2520dual-channel%2520model%2520named%2520GAT-MLP%252C%2520which%250Acombines%2520a%2520Graph%2520Attention%2520Network%2520%2528GAT%2529%2520for%2520local%2520structural%2520encoding%2520with%2520a%250AMultilayer%2520Perceptron%2520%2528MLP%2529%2520for%2520global%2520feature%2520modeling.%2520The%2520GAT-MLP%2520model%250Ashows%2520strong%2520and%2520consistent%2520performance%2520across%2520all%2520metrics.%2520Our%2520results%250Ahighlight%2520the%2520effectiveness%2520of%2520dual-channel%2520architectures%2520and%2520the%2520promise%2520of%250Agraph%2520neural%2520networks%2520in%2520combinatorial%2520algorithm%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Select%20MCP%20Algorithms%3A%20From%20Traditional%20ML%20to%20Dual-Channel%0A%20%20GAT-MLP&entry.906535625=Xiang%20Li%20and%20Shanshan%20Wang%20and%20Chenglong%20Xiao&entry.1292438233=%20%20Extensive%20experiments%20and%20prior%20studies%20show%20that%20no%20single%20maximum%20clique%0Aalgorithm%20consistently%20performs%20best%20across%20all%20instances%2C%20highlighting%20the%0Aimportance%20of%20selecting%20suitable%20algorithms%20based%20on%20instance%20features.%20Through%0Aan%20extensive%20analysis%20of%20relevant%20studies%2C%20it%20is%20found%20that%20there%20is%20a%20lack%20of%0Aresearch%20work%20concerning%20algorithm%20selection%20oriented%20toward%20the%20Maximum%20Clique%0AProblem%20%28MCP%29.%20In%20this%20work%2C%20we%20propose%20a%20learning-based%20framework%20that%0Aintegrates%20both%20traditional%20machine%20learning%20and%20graph%20neural%20networks%20to%0Aaddress%20this%20gap.%20We%20construct%20a%20labeled%20dataset%20by%20running%20four%20exact%20MCP%0Aalgorithms%20on%20a%20diverse%20collection%20of%20graph%20instances%2C%20accompanied%20by%0Astructural%20and%20global%20statistical%20features%20extracted%20from%20each%20graph.%20We%20first%0Aevaluate%20four%20conventional%20classifiers%3A%20Support%20Vector%20Machine%20%28SVM%29%2C%20Random%0AForest%20%28RF%29%2C%20Decision%20Tree%20%28DT%29%2C%20and%20K-Nearest%20Neighbors%20%28KNN%29%2C%20across%20multiple%0Adataset%20variants.%20Experimental%20results%20show%20that%20RF%20consistently%20shows%20strong%0Aperformance%20across%20metrics%20and%20dataset%20variants%2C%20making%20it%20a%20reliable%20baseline.%0AIn%20addition%2C%20feature%20importance%20analysis%20indicates%20that%20connectivity%20and%0Atopological%20structure%20are%20strong%20predictors%20of%20algorithm%20performance.%20Building%0Aon%20these%20findings%2C%20we%20develop%20a%20dual-channel%20model%20named%20GAT-MLP%2C%20which%0Acombines%20a%20Graph%20Attention%20Network%20%28GAT%29%20for%20local%20structural%20encoding%20with%20a%0AMultilayer%20Perceptron%20%28MLP%29%20for%20global%20feature%20modeling.%20The%20GAT-MLP%20model%0Ashows%20strong%20and%20consistent%20performance%20across%20all%20metrics.%20Our%20results%0Ahighlight%20the%20effectiveness%20of%20dual-channel%20architectures%20and%20the%20promise%20of%0Agraph%20neural%20networks%20in%20combinatorial%20algorithm%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08005v2&entry.124074799=Read"},
{"title": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for\n  Operations Research", "author": "Zhiyuan Wang and Bokui Chen and Yinya Huang and Qingxing Cao and Ming He and Jianping Fan and Xiaodan Liang", "abstract": "  Operations research (OR) is widely deployed to solve critical decision-making\nproblems with complex objectives and constraints, impacting manufacturing,\nlogistics, finance, and healthcare outcomes. While Large Language Models (LLMs)\nhave shown promising results in various domains, their practical application in\nindustry-relevant operations research (OR) problems presents significant\nchallenges and opportunities. Preliminary industrial applications of LLMs for\noperations research face two critical deployment challenges: 1) Self-correction\nfocuses on code syntax rather than mathematical accuracy, causing costly\nerrors; 2) Complex expert selection creates unpredictable workflows that reduce\ntransparency and increase maintenance costs, making them impractical for\ntime-sensitive business applications. To address these business limitations, we\nintroduce ORMind, a cognitive-inspired framework that enhances optimization\nthrough counterfactual reasoning. Our approach emulates human cognition,\nimplementing an end-to-end workflow that systematically transforms requirements\ninto mathematical models and executable solver code. It is currently being\ntested internally in Lenovo's AI Assistant, with plans to enhance optimization\ncapabilities for both business and consumer customers. Experiments demonstrate\nthat ORMind outperforms existing methods, achieving a 9.5\\% improvement on the\nNL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset.\n", "link": "http://arxiv.org/abs/2506.01326v2", "date": "2025-09-03", "relevancy": 1.9468, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORMind%3A%20A%20Cognitive-Inspired%20End-to-End%20Reasoning%20Framework%20for%0A%20%20Operations%20Research&body=Title%3A%20ORMind%3A%20A%20Cognitive-Inspired%20End-to-End%20Reasoning%20Framework%20for%0A%20%20Operations%20Research%0AAuthor%3A%20Zhiyuan%20Wang%20and%20Bokui%20Chen%20and%20Yinya%20Huang%20and%20Qingxing%20Cao%20and%20Ming%20He%20and%20Jianping%20Fan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Operations%20research%20%28OR%29%20is%20widely%20deployed%20to%20solve%20critical%20decision-making%0Aproblems%20with%20complex%20objectives%20and%20constraints%2C%20impacting%20manufacturing%2C%0Alogistics%2C%20finance%2C%20and%20healthcare%20outcomes.%20While%20Large%20Language%20Models%20%28LLMs%29%0Ahave%20shown%20promising%20results%20in%20various%20domains%2C%20their%20practical%20application%20in%0Aindustry-relevant%20operations%20research%20%28OR%29%20problems%20presents%20significant%0Achallenges%20and%20opportunities.%20Preliminary%20industrial%20applications%20of%20LLMs%20for%0Aoperations%20research%20face%20two%20critical%20deployment%20challenges%3A%201%29%20Self-correction%0Afocuses%20on%20code%20syntax%20rather%20than%20mathematical%20accuracy%2C%20causing%20costly%0Aerrors%3B%202%29%20Complex%20expert%20selection%20creates%20unpredictable%20workflows%20that%20reduce%0Atransparency%20and%20increase%20maintenance%20costs%2C%20making%20them%20impractical%20for%0Atime-sensitive%20business%20applications.%20To%20address%20these%20business%20limitations%2C%20we%0Aintroduce%20ORMind%2C%20a%20cognitive-inspired%20framework%20that%20enhances%20optimization%0Athrough%20counterfactual%20reasoning.%20Our%20approach%20emulates%20human%20cognition%2C%0Aimplementing%20an%20end-to-end%20workflow%20that%20systematically%20transforms%20requirements%0Ainto%20mathematical%20models%20and%20executable%20solver%20code.%20It%20is%20currently%20being%0Atested%20internally%20in%20Lenovo%27s%20AI%20Assistant%2C%20with%20plans%20to%20enhance%20optimization%0Acapabilities%20for%20both%20business%20and%20consumer%20customers.%20Experiments%20demonstrate%0Athat%20ORMind%20outperforms%20existing%20methods%2C%20achieving%20a%209.5%5C%25%20improvement%20on%20the%0ANL4Opt%20dataset%20and%20a%2014.6%5C%25%20improvement%20on%20the%20ComplexOR%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01326v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORMind%253A%2520A%2520Cognitive-Inspired%2520End-to-End%2520Reasoning%2520Framework%2520for%250A%2520%2520Operations%2520Research%26entry.906535625%3DZhiyuan%2520Wang%2520and%2520Bokui%2520Chen%2520and%2520Yinya%2520Huang%2520and%2520Qingxing%2520Cao%2520and%2520Ming%2520He%2520and%2520Jianping%2520Fan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Operations%2520research%2520%2528OR%2529%2520is%2520widely%2520deployed%2520to%2520solve%2520critical%2520decision-making%250Aproblems%2520with%2520complex%2520objectives%2520and%2520constraints%252C%2520impacting%2520manufacturing%252C%250Alogistics%252C%2520finance%252C%2520and%2520healthcare%2520outcomes.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ahave%2520shown%2520promising%2520results%2520in%2520various%2520domains%252C%2520their%2520practical%2520application%2520in%250Aindustry-relevant%2520operations%2520research%2520%2528OR%2529%2520problems%2520presents%2520significant%250Achallenges%2520and%2520opportunities.%2520Preliminary%2520industrial%2520applications%2520of%2520LLMs%2520for%250Aoperations%2520research%2520face%2520two%2520critical%2520deployment%2520challenges%253A%25201%2529%2520Self-correction%250Afocuses%2520on%2520code%2520syntax%2520rather%2520than%2520mathematical%2520accuracy%252C%2520causing%2520costly%250Aerrors%253B%25202%2529%2520Complex%2520expert%2520selection%2520creates%2520unpredictable%2520workflows%2520that%2520reduce%250Atransparency%2520and%2520increase%2520maintenance%2520costs%252C%2520making%2520them%2520impractical%2520for%250Atime-sensitive%2520business%2520applications.%2520To%2520address%2520these%2520business%2520limitations%252C%2520we%250Aintroduce%2520ORMind%252C%2520a%2520cognitive-inspired%2520framework%2520that%2520enhances%2520optimization%250Athrough%2520counterfactual%2520reasoning.%2520Our%2520approach%2520emulates%2520human%2520cognition%252C%250Aimplementing%2520an%2520end-to-end%2520workflow%2520that%2520systematically%2520transforms%2520requirements%250Ainto%2520mathematical%2520models%2520and%2520executable%2520solver%2520code.%2520It%2520is%2520currently%2520being%250Atested%2520internally%2520in%2520Lenovo%2527s%2520AI%2520Assistant%252C%2520with%2520plans%2520to%2520enhance%2520optimization%250Acapabilities%2520for%2520both%2520business%2520and%2520consumer%2520customers.%2520Experiments%2520demonstrate%250Athat%2520ORMind%2520outperforms%2520existing%2520methods%252C%2520achieving%2520a%25209.5%255C%2525%2520improvement%2520on%2520the%250ANL4Opt%2520dataset%2520and%2520a%252014.6%255C%2525%2520improvement%2520on%2520the%2520ComplexOR%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01326v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORMind%3A%20A%20Cognitive-Inspired%20End-to-End%20Reasoning%20Framework%20for%0A%20%20Operations%20Research&entry.906535625=Zhiyuan%20Wang%20and%20Bokui%20Chen%20and%20Yinya%20Huang%20and%20Qingxing%20Cao%20and%20Ming%20He%20and%20Jianping%20Fan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Operations%20research%20%28OR%29%20is%20widely%20deployed%20to%20solve%20critical%20decision-making%0Aproblems%20with%20complex%20objectives%20and%20constraints%2C%20impacting%20manufacturing%2C%0Alogistics%2C%20finance%2C%20and%20healthcare%20outcomes.%20While%20Large%20Language%20Models%20%28LLMs%29%0Ahave%20shown%20promising%20results%20in%20various%20domains%2C%20their%20practical%20application%20in%0Aindustry-relevant%20operations%20research%20%28OR%29%20problems%20presents%20significant%0Achallenges%20and%20opportunities.%20Preliminary%20industrial%20applications%20of%20LLMs%20for%0Aoperations%20research%20face%20two%20critical%20deployment%20challenges%3A%201%29%20Self-correction%0Afocuses%20on%20code%20syntax%20rather%20than%20mathematical%20accuracy%2C%20causing%20costly%0Aerrors%3B%202%29%20Complex%20expert%20selection%20creates%20unpredictable%20workflows%20that%20reduce%0Atransparency%20and%20increase%20maintenance%20costs%2C%20making%20them%20impractical%20for%0Atime-sensitive%20business%20applications.%20To%20address%20these%20business%20limitations%2C%20we%0Aintroduce%20ORMind%2C%20a%20cognitive-inspired%20framework%20that%20enhances%20optimization%0Athrough%20counterfactual%20reasoning.%20Our%20approach%20emulates%20human%20cognition%2C%0Aimplementing%20an%20end-to-end%20workflow%20that%20systematically%20transforms%20requirements%0Ainto%20mathematical%20models%20and%20executable%20solver%20code.%20It%20is%20currently%20being%0Atested%20internally%20in%20Lenovo%27s%20AI%20Assistant%2C%20with%20plans%20to%20enhance%20optimization%0Acapabilities%20for%20both%20business%20and%20consumer%20customers.%20Experiments%20demonstrate%0Athat%20ORMind%20outperforms%20existing%20methods%2C%20achieving%20a%209.5%5C%25%20improvement%20on%20the%0ANL4Opt%20dataset%20and%20a%2014.6%5C%25%20improvement%20on%20the%20ComplexOR%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01326v2&entry.124074799=Read"},
{"title": "EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared\n  Shadow Casting", "author": "Vimal Mollyn and Nathan DeVrio and Chris Harrison", "abstract": "  The ability to detect touch events on uninstrumented, everyday surfaces has\nbeen a long-standing goal for mixed reality systems. Prior work has shown that\nvirtual interfaces bound to physical surfaces offer performance and ergonomic\nbenefits over tapping at interfaces floating in the air. A wide variety of\napproaches have been previously developed, to which we contribute a new\nheadset-integrated technique called \\systemname. We use a combination of a\ncomputer-triggered camera and one or more infrared emitters to create\nstructured shadows, from which we can accurately estimate hover distance (mean\nerror of 6.9~mm) and touch contact (98.0\\% accuracy). We discuss how our\ntechnique works across a range of conditions, including surface material,\ninteraction orientation, and environmental lighting.\n", "link": "http://arxiv.org/abs/2509.03430v1", "date": "2025-09-03", "relevancy": 1.9459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4983}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EclipseTouch%3A%20Touch%20Segmentation%20on%20Ad%20Hoc%20Surfaces%20using%20Worn%20Infrared%0A%20%20Shadow%20Casting&body=Title%3A%20EclipseTouch%3A%20Touch%20Segmentation%20on%20Ad%20Hoc%20Surfaces%20using%20Worn%20Infrared%0A%20%20Shadow%20Casting%0AAuthor%3A%20Vimal%20Mollyn%20and%20Nathan%20DeVrio%20and%20Chris%20Harrison%0AAbstract%3A%20%20%20The%20ability%20to%20detect%20touch%20events%20on%20uninstrumented%2C%20everyday%20surfaces%20has%0Abeen%20a%20long-standing%20goal%20for%20mixed%20reality%20systems.%20Prior%20work%20has%20shown%20that%0Avirtual%20interfaces%20bound%20to%20physical%20surfaces%20offer%20performance%20and%20ergonomic%0Abenefits%20over%20tapping%20at%20interfaces%20floating%20in%20the%20air.%20A%20wide%20variety%20of%0Aapproaches%20have%20been%20previously%20developed%2C%20to%20which%20we%20contribute%20a%20new%0Aheadset-integrated%20technique%20called%20%5Csystemname.%20We%20use%20a%20combination%20of%20a%0Acomputer-triggered%20camera%20and%20one%20or%20more%20infrared%20emitters%20to%20create%0Astructured%20shadows%2C%20from%20which%20we%20can%20accurately%20estimate%20hover%20distance%20%28mean%0Aerror%20of%206.9~mm%29%20and%20touch%20contact%20%2898.0%5C%25%20accuracy%29.%20We%20discuss%20how%20our%0Atechnique%20works%20across%20a%20range%20of%20conditions%2C%20including%20surface%20material%2C%0Ainteraction%20orientation%2C%20and%20environmental%20lighting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEclipseTouch%253A%2520Touch%2520Segmentation%2520on%2520Ad%2520Hoc%2520Surfaces%2520using%2520Worn%2520Infrared%250A%2520%2520Shadow%2520Casting%26entry.906535625%3DVimal%2520Mollyn%2520and%2520Nathan%2520DeVrio%2520and%2520Chris%2520Harrison%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520detect%2520touch%2520events%2520on%2520uninstrumented%252C%2520everyday%2520surfaces%2520has%250Abeen%2520a%2520long-standing%2520goal%2520for%2520mixed%2520reality%2520systems.%2520Prior%2520work%2520has%2520shown%2520that%250Avirtual%2520interfaces%2520bound%2520to%2520physical%2520surfaces%2520offer%2520performance%2520and%2520ergonomic%250Abenefits%2520over%2520tapping%2520at%2520interfaces%2520floating%2520in%2520the%2520air.%2520A%2520wide%2520variety%2520of%250Aapproaches%2520have%2520been%2520previously%2520developed%252C%2520to%2520which%2520we%2520contribute%2520a%2520new%250Aheadset-integrated%2520technique%2520called%2520%255Csystemname.%2520We%2520use%2520a%2520combination%2520of%2520a%250Acomputer-triggered%2520camera%2520and%2520one%2520or%2520more%2520infrared%2520emitters%2520to%2520create%250Astructured%2520shadows%252C%2520from%2520which%2520we%2520can%2520accurately%2520estimate%2520hover%2520distance%2520%2528mean%250Aerror%2520of%25206.9~mm%2529%2520and%2520touch%2520contact%2520%252898.0%255C%2525%2520accuracy%2529.%2520We%2520discuss%2520how%2520our%250Atechnique%2520works%2520across%2520a%2520range%2520of%2520conditions%252C%2520including%2520surface%2520material%252C%250Ainteraction%2520orientation%252C%2520and%2520environmental%2520lighting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EclipseTouch%3A%20Touch%20Segmentation%20on%20Ad%20Hoc%20Surfaces%20using%20Worn%20Infrared%0A%20%20Shadow%20Casting&entry.906535625=Vimal%20Mollyn%20and%20Nathan%20DeVrio%20and%20Chris%20Harrison&entry.1292438233=%20%20The%20ability%20to%20detect%20touch%20events%20on%20uninstrumented%2C%20everyday%20surfaces%20has%0Abeen%20a%20long-standing%20goal%20for%20mixed%20reality%20systems.%20Prior%20work%20has%20shown%20that%0Avirtual%20interfaces%20bound%20to%20physical%20surfaces%20offer%20performance%20and%20ergonomic%0Abenefits%20over%20tapping%20at%20interfaces%20floating%20in%20the%20air.%20A%20wide%20variety%20of%0Aapproaches%20have%20been%20previously%20developed%2C%20to%20which%20we%20contribute%20a%20new%0Aheadset-integrated%20technique%20called%20%5Csystemname.%20We%20use%20a%20combination%20of%20a%0Acomputer-triggered%20camera%20and%20one%20or%20more%20infrared%20emitters%20to%20create%0Astructured%20shadows%2C%20from%20which%20we%20can%20accurately%20estimate%20hover%20distance%20%28mean%0Aerror%20of%206.9~mm%29%20and%20touch%20contact%20%2898.0%5C%25%20accuracy%29.%20We%20discuss%20how%20our%0Atechnique%20works%20across%20a%20range%20of%20conditions%2C%20including%20surface%20material%2C%0Ainteraction%20orientation%2C%20and%20environmental%20lighting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03430v1&entry.124074799=Read"},
{"title": "Efficiently Editing Mixture-of-Experts Models with Compressed Experts", "author": "Yifei He and Yang Liu and Chen Liang and Hany Hassan Awadalla", "abstract": "  Mixture-of-Experts (MoE) models have become a key approach for scaling large\nlanguage models efficiently by activating only a subset of experts during\ntraining and inference. Typically, the number of activated experts presents a\ntrade-off: fewer experts reduce computational costs, while more experts improve\nperformance. Recent studies reveal that not all activated experts contribute\nequally to model performance, with some providing minimal utility, particularly\nwhen finetuning pretrained MoE models for specialized downstream tasks. The\nco-existence of significant and redundant parameters in experts provides us an\nopportunity to reduce the number of activated experts while maintaining model\nperformance. In this work, we propose the concept of compressed experts,\nlightweight modules that serve as compact representations of full experts. Our\napproach preserves the most important experts while replacing other auxiliary\nactivated experts with compressed experts. The reduction of active parameters\nsignificantly lowers inference costs while achieving comparable performance.\nExtensive experiments on models including Phi-MoE and OLMoE demonstrate that\ncompressed experts recover over 90% of full expert performance across various\ntasks while reducing more than 30% active parameters and saving 20% in\ninference costs. This approach enables efficient deployment of MoE models in\nresource-constrained settings and facilitates scaling to larger models with\nmanageable overhead. Our code is available at\nhttps://github.com/yifei-he/Compressed-Experts.\n", "link": "http://arxiv.org/abs/2503.00634v2", "date": "2025-09-03", "relevancy": 1.9456, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Editing%20Mixture-of-Experts%20Models%20with%20Compressed%20Experts&body=Title%3A%20Efficiently%20Editing%20Mixture-of-Experts%20Models%20with%20Compressed%20Experts%0AAuthor%3A%20Yifei%20He%20and%20Yang%20Liu%20and%20Chen%20Liang%20and%20Hany%20Hassan%20Awadalla%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20models%20have%20become%20a%20key%20approach%20for%20scaling%20large%0Alanguage%20models%20efficiently%20by%20activating%20only%20a%20subset%20of%20experts%20during%0Atraining%20and%20inference.%20Typically%2C%20the%20number%20of%20activated%20experts%20presents%20a%0Atrade-off%3A%20fewer%20experts%20reduce%20computational%20costs%2C%20while%20more%20experts%20improve%0Aperformance.%20Recent%20studies%20reveal%20that%20not%20all%20activated%20experts%20contribute%0Aequally%20to%20model%20performance%2C%20with%20some%20providing%20minimal%20utility%2C%20particularly%0Awhen%20finetuning%20pretrained%20MoE%20models%20for%20specialized%20downstream%20tasks.%20The%0Aco-existence%20of%20significant%20and%20redundant%20parameters%20in%20experts%20provides%20us%20an%0Aopportunity%20to%20reduce%20the%20number%20of%20activated%20experts%20while%20maintaining%20model%0Aperformance.%20In%20this%20work%2C%20we%20propose%20the%20concept%20of%20compressed%20experts%2C%0Alightweight%20modules%20that%20serve%20as%20compact%20representations%20of%20full%20experts.%20Our%0Aapproach%20preserves%20the%20most%20important%20experts%20while%20replacing%20other%20auxiliary%0Aactivated%20experts%20with%20compressed%20experts.%20The%20reduction%20of%20active%20parameters%0Asignificantly%20lowers%20inference%20costs%20while%20achieving%20comparable%20performance.%0AExtensive%20experiments%20on%20models%20including%20Phi-MoE%20and%20OLMoE%20demonstrate%20that%0Acompressed%20experts%20recover%20over%2090%25%20of%20full%20expert%20performance%20across%20various%0Atasks%20while%20reducing%20more%20than%2030%25%20active%20parameters%20and%20saving%2020%25%20in%0Ainference%20costs.%20This%20approach%20enables%20efficient%20deployment%20of%20MoE%20models%20in%0Aresource-constrained%20settings%20and%20facilitates%20scaling%20to%20larger%20models%20with%0Amanageable%20overhead.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yifei-he/Compressed-Experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Editing%2520Mixture-of-Experts%2520Models%2520with%2520Compressed%2520Experts%26entry.906535625%3DYifei%2520He%2520and%2520Yang%2520Liu%2520and%2520Chen%2520Liang%2520and%2520Hany%2520Hassan%2520Awadalla%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520models%2520have%2520become%2520a%2520key%2520approach%2520for%2520scaling%2520large%250Alanguage%2520models%2520efficiently%2520by%2520activating%2520only%2520a%2520subset%2520of%2520experts%2520during%250Atraining%2520and%2520inference.%2520Typically%252C%2520the%2520number%2520of%2520activated%2520experts%2520presents%2520a%250Atrade-off%253A%2520fewer%2520experts%2520reduce%2520computational%2520costs%252C%2520while%2520more%2520experts%2520improve%250Aperformance.%2520Recent%2520studies%2520reveal%2520that%2520not%2520all%2520activated%2520experts%2520contribute%250Aequally%2520to%2520model%2520performance%252C%2520with%2520some%2520providing%2520minimal%2520utility%252C%2520particularly%250Awhen%2520finetuning%2520pretrained%2520MoE%2520models%2520for%2520specialized%2520downstream%2520tasks.%2520The%250Aco-existence%2520of%2520significant%2520and%2520redundant%2520parameters%2520in%2520experts%2520provides%2520us%2520an%250Aopportunity%2520to%2520reduce%2520the%2520number%2520of%2520activated%2520experts%2520while%2520maintaining%2520model%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520concept%2520of%2520compressed%2520experts%252C%250Alightweight%2520modules%2520that%2520serve%2520as%2520compact%2520representations%2520of%2520full%2520experts.%2520Our%250Aapproach%2520preserves%2520the%2520most%2520important%2520experts%2520while%2520replacing%2520other%2520auxiliary%250Aactivated%2520experts%2520with%2520compressed%2520experts.%2520The%2520reduction%2520of%2520active%2520parameters%250Asignificantly%2520lowers%2520inference%2520costs%2520while%2520achieving%2520comparable%2520performance.%250AExtensive%2520experiments%2520on%2520models%2520including%2520Phi-MoE%2520and%2520OLMoE%2520demonstrate%2520that%250Acompressed%2520experts%2520recover%2520over%252090%2525%2520of%2520full%2520expert%2520performance%2520across%2520various%250Atasks%2520while%2520reducing%2520more%2520than%252030%2525%2520active%2520parameters%2520and%2520saving%252020%2525%2520in%250Ainference%2520costs.%2520This%2520approach%2520enables%2520efficient%2520deployment%2520of%2520MoE%2520models%2520in%250Aresource-constrained%2520settings%2520and%2520facilitates%2520scaling%2520to%2520larger%2520models%2520with%250Amanageable%2520overhead.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/yifei-he/Compressed-Experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Editing%20Mixture-of-Experts%20Models%20with%20Compressed%20Experts&entry.906535625=Yifei%20He%20and%20Yang%20Liu%20and%20Chen%20Liang%20and%20Hany%20Hassan%20Awadalla&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20models%20have%20become%20a%20key%20approach%20for%20scaling%20large%0Alanguage%20models%20efficiently%20by%20activating%20only%20a%20subset%20of%20experts%20during%0Atraining%20and%20inference.%20Typically%2C%20the%20number%20of%20activated%20experts%20presents%20a%0Atrade-off%3A%20fewer%20experts%20reduce%20computational%20costs%2C%20while%20more%20experts%20improve%0Aperformance.%20Recent%20studies%20reveal%20that%20not%20all%20activated%20experts%20contribute%0Aequally%20to%20model%20performance%2C%20with%20some%20providing%20minimal%20utility%2C%20particularly%0Awhen%20finetuning%20pretrained%20MoE%20models%20for%20specialized%20downstream%20tasks.%20The%0Aco-existence%20of%20significant%20and%20redundant%20parameters%20in%20experts%20provides%20us%20an%0Aopportunity%20to%20reduce%20the%20number%20of%20activated%20experts%20while%20maintaining%20model%0Aperformance.%20In%20this%20work%2C%20we%20propose%20the%20concept%20of%20compressed%20experts%2C%0Alightweight%20modules%20that%20serve%20as%20compact%20representations%20of%20full%20experts.%20Our%0Aapproach%20preserves%20the%20most%20important%20experts%20while%20replacing%20other%20auxiliary%0Aactivated%20experts%20with%20compressed%20experts.%20The%20reduction%20of%20active%20parameters%0Asignificantly%20lowers%20inference%20costs%20while%20achieving%20comparable%20performance.%0AExtensive%20experiments%20on%20models%20including%20Phi-MoE%20and%20OLMoE%20demonstrate%20that%0Acompressed%20experts%20recover%20over%2090%25%20of%20full%20expert%20performance%20across%20various%0Atasks%20while%20reducing%20more%20than%2030%25%20active%20parameters%20and%20saving%2020%25%20in%0Ainference%20costs.%20This%20approach%20enables%20efficient%20deployment%20of%20MoE%20models%20in%0Aresource-constrained%20settings%20and%20facilitates%20scaling%20to%20larger%20models%20with%0Amanageable%20overhead.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yifei-he/Compressed-Experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00634v2&entry.124074799=Read"},
{"title": "Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias\n  and Revealing Method-Dependent Performance Patterns", "author": "Serra Aksoy", "abstract": "  Attribution methods explain neural network predictions by identifying\ninfluential input features, but their evaluation suffers from threshold\nselection bias that can reverse method rankings and undermine conclusions.\nCurrent protocols binarize attribution maps at single thresholds, where\nthreshold choice alone can alter rankings by over 200 percentage points. We\naddress this flaw with a threshold-free framework that computes Area Under the\nCurve for Intersection over Union (AUC-IoU), capturing attribution quality\nacross the full threshold spectrum. Evaluating seven attribution methods on\ndermatological imaging, we show single-threshold metrics yield contradictory\nresults, while threshold-free evaluation provides reliable differentiation.\nXRAI achieves 31% improvement over LIME and 204% over vanilla Integrated\nGradients, with size-stratified analysis revealing performance variations up to\n269% across lesion scales. These findings establish methodological standards\nthat eliminate evaluation artifacts and enable evidence-based method selection.\nThe threshold-free framework provides both theoretical insight into attribution\nbehavior and practical guidance for robust comparison in medical imaging and\nbeyond.\n", "link": "http://arxiv.org/abs/2509.03176v1", "date": "2025-09-03", "relevancy": 1.935, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.493}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20Evaluation%20of%20Attribution%20Methods%3A%20Eliminating%20Threshold%20Bias%0A%20%20and%20Revealing%20Method-Dependent%20Performance%20Patterns&body=Title%3A%20Systematic%20Evaluation%20of%20Attribution%20Methods%3A%20Eliminating%20Threshold%20Bias%0A%20%20and%20Revealing%20Method-Dependent%20Performance%20Patterns%0AAuthor%3A%20Serra%20Aksoy%0AAbstract%3A%20%20%20Attribution%20methods%20explain%20neural%20network%20predictions%20by%20identifying%0Ainfluential%20input%20features%2C%20but%20their%20evaluation%20suffers%20from%20threshold%0Aselection%20bias%20that%20can%20reverse%20method%20rankings%20and%20undermine%20conclusions.%0ACurrent%20protocols%20binarize%20attribution%20maps%20at%20single%20thresholds%2C%20where%0Athreshold%20choice%20alone%20can%20alter%20rankings%20by%20over%20200%20percentage%20points.%20We%0Aaddress%20this%20flaw%20with%20a%20threshold-free%20framework%20that%20computes%20Area%20Under%20the%0ACurve%20for%20Intersection%20over%20Union%20%28AUC-IoU%29%2C%20capturing%20attribution%20quality%0Aacross%20the%20full%20threshold%20spectrum.%20Evaluating%20seven%20attribution%20methods%20on%0Adermatological%20imaging%2C%20we%20show%20single-threshold%20metrics%20yield%20contradictory%0Aresults%2C%20while%20threshold-free%20evaluation%20provides%20reliable%20differentiation.%0AXRAI%20achieves%2031%25%20improvement%20over%20LIME%20and%20204%25%20over%20vanilla%20Integrated%0AGradients%2C%20with%20size-stratified%20analysis%20revealing%20performance%20variations%20up%20to%0A269%25%20across%20lesion%20scales.%20These%20findings%20establish%20methodological%20standards%0Athat%20eliminate%20evaluation%20artifacts%20and%20enable%20evidence-based%20method%20selection.%0AThe%20threshold-free%20framework%20provides%20both%20theoretical%20insight%20into%20attribution%0Abehavior%20and%20practical%20guidance%20for%20robust%20comparison%20in%20medical%20imaging%20and%0Abeyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520Evaluation%2520of%2520Attribution%2520Methods%253A%2520Eliminating%2520Threshold%2520Bias%250A%2520%2520and%2520Revealing%2520Method-Dependent%2520Performance%2520Patterns%26entry.906535625%3DSerra%2520Aksoy%26entry.1292438233%3D%2520%2520Attribution%2520methods%2520explain%2520neural%2520network%2520predictions%2520by%2520identifying%250Ainfluential%2520input%2520features%252C%2520but%2520their%2520evaluation%2520suffers%2520from%2520threshold%250Aselection%2520bias%2520that%2520can%2520reverse%2520method%2520rankings%2520and%2520undermine%2520conclusions.%250ACurrent%2520protocols%2520binarize%2520attribution%2520maps%2520at%2520single%2520thresholds%252C%2520where%250Athreshold%2520choice%2520alone%2520can%2520alter%2520rankings%2520by%2520over%2520200%2520percentage%2520points.%2520We%250Aaddress%2520this%2520flaw%2520with%2520a%2520threshold-free%2520framework%2520that%2520computes%2520Area%2520Under%2520the%250ACurve%2520for%2520Intersection%2520over%2520Union%2520%2528AUC-IoU%2529%252C%2520capturing%2520attribution%2520quality%250Aacross%2520the%2520full%2520threshold%2520spectrum.%2520Evaluating%2520seven%2520attribution%2520methods%2520on%250Adermatological%2520imaging%252C%2520we%2520show%2520single-threshold%2520metrics%2520yield%2520contradictory%250Aresults%252C%2520while%2520threshold-free%2520evaluation%2520provides%2520reliable%2520differentiation.%250AXRAI%2520achieves%252031%2525%2520improvement%2520over%2520LIME%2520and%2520204%2525%2520over%2520vanilla%2520Integrated%250AGradients%252C%2520with%2520size-stratified%2520analysis%2520revealing%2520performance%2520variations%2520up%2520to%250A269%2525%2520across%2520lesion%2520scales.%2520These%2520findings%2520establish%2520methodological%2520standards%250Athat%2520eliminate%2520evaluation%2520artifacts%2520and%2520enable%2520evidence-based%2520method%2520selection.%250AThe%2520threshold-free%2520framework%2520provides%2520both%2520theoretical%2520insight%2520into%2520attribution%250Abehavior%2520and%2520practical%2520guidance%2520for%2520robust%2520comparison%2520in%2520medical%2520imaging%2520and%250Abeyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20Evaluation%20of%20Attribution%20Methods%3A%20Eliminating%20Threshold%20Bias%0A%20%20and%20Revealing%20Method-Dependent%20Performance%20Patterns&entry.906535625=Serra%20Aksoy&entry.1292438233=%20%20Attribution%20methods%20explain%20neural%20network%20predictions%20by%20identifying%0Ainfluential%20input%20features%2C%20but%20their%20evaluation%20suffers%20from%20threshold%0Aselection%20bias%20that%20can%20reverse%20method%20rankings%20and%20undermine%20conclusions.%0ACurrent%20protocols%20binarize%20attribution%20maps%20at%20single%20thresholds%2C%20where%0Athreshold%20choice%20alone%20can%20alter%20rankings%20by%20over%20200%20percentage%20points.%20We%0Aaddress%20this%20flaw%20with%20a%20threshold-free%20framework%20that%20computes%20Area%20Under%20the%0ACurve%20for%20Intersection%20over%20Union%20%28AUC-IoU%29%2C%20capturing%20attribution%20quality%0Aacross%20the%20full%20threshold%20spectrum.%20Evaluating%20seven%20attribution%20methods%20on%0Adermatological%20imaging%2C%20we%20show%20single-threshold%20metrics%20yield%20contradictory%0Aresults%2C%20while%20threshold-free%20evaluation%20provides%20reliable%20differentiation.%0AXRAI%20achieves%2031%25%20improvement%20over%20LIME%20and%20204%25%20over%20vanilla%20Integrated%0AGradients%2C%20with%20size-stratified%20analysis%20revealing%20performance%20variations%20up%20to%0A269%25%20across%20lesion%20scales.%20These%20findings%20establish%20methodological%20standards%0Athat%20eliminate%20evaluation%20artifacts%20and%20enable%20evidence-based%20method%20selection.%0AThe%20threshold-free%20framework%20provides%20both%20theoretical%20insight%20into%20attribution%0Abehavior%20and%20practical%20guidance%20for%20robust%20comparison%20in%20medical%20imaging%20and%0Abeyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03176v1&entry.124074799=Read"},
{"title": "Quantum Data Encoding and Variational Algorithms: A Framework for Hybrid\n  Quantum Classical Machine Learning", "author": "Bhavna Bose and Saurav Verma", "abstract": "  The development of quantum computers has been the stimulus that enables the\nrealization of Quantum Machine Learning (QML), an area that integrates the\ncalculational framework of quantum mechanics with the adaptive properties of\nclassical machine learning. This article suggests a broad architecture that\nallows the connection between classical data pipelines and quantum algorithms,\nhybrid quantum-classical models emerge as a promising route to scalable and\nnear-term quantum benefit. At the core of this paradigm lies the\nClassical-Quantum (CQ) paradigm, in which the qubit states of high-dimensional\nclassical data are encoded using sophisticated classical encoding strategies\nwhich encode the data in terms of amplitude and angle of rotation, along with\nsuperposition mapping. These techniques allow compression of information\nexponentially into Hilbert space representations, which, together with reduced\nsample complexity, allows greater feature expressivity. We also examine\nvariational quantum circuits, quantum gates expressed as trainable variables\nthat run with classical optimizers to overcome decoherence, noise, and\ngate-depth constraints of the existing Noisy Intermediate-Scale Quantum (NISQ)\ndevices. Experimental comparisons with a Quantum Naive Bayes classifier prove\nthat even small quantum circuits can approximate probabilistic inference with\ncompetitive accuracy compared to classical benchmarks, and have much better\nrobustness to noisy data distributionsThis model does not only explain the\nalgorithmic and architectural design of QML, it also offers a roadmap to the\nimplementation of quantum kernels, variational algorithms, and hybrid feedback\nloops into practice, including optimization, computer vision, and medical\ndiagnostics. The results support the idea that hybrid architectures with strong\ndata encoding and adaptive error protection are key to moving QML out of theory\nto practice.\n", "link": "http://arxiv.org/abs/2502.11951v2", "date": "2025-09-03", "relevancy": 1.9338, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Data%20Encoding%20and%20Variational%20Algorithms%3A%20A%20Framework%20for%20Hybrid%0A%20%20Quantum%20Classical%20Machine%20Learning&body=Title%3A%20Quantum%20Data%20Encoding%20and%20Variational%20Algorithms%3A%20A%20Framework%20for%20Hybrid%0A%20%20Quantum%20Classical%20Machine%20Learning%0AAuthor%3A%20Bhavna%20Bose%20and%20Saurav%20Verma%0AAbstract%3A%20%20%20The%20development%20of%20quantum%20computers%20has%20been%20the%20stimulus%20that%20enables%20the%0Arealization%20of%20Quantum%20Machine%20Learning%20%28QML%29%2C%20an%20area%20that%20integrates%20the%0Acalculational%20framework%20of%20quantum%20mechanics%20with%20the%20adaptive%20properties%20of%0Aclassical%20machine%20learning.%20This%20article%20suggests%20a%20broad%20architecture%20that%0Aallows%20the%20connection%20between%20classical%20data%20pipelines%20and%20quantum%20algorithms%2C%0Ahybrid%20quantum-classical%20models%20emerge%20as%20a%20promising%20route%20to%20scalable%20and%0Anear-term%20quantum%20benefit.%20At%20the%20core%20of%20this%20paradigm%20lies%20the%0AClassical-Quantum%20%28CQ%29%20paradigm%2C%20in%20which%20the%20qubit%20states%20of%20high-dimensional%0Aclassical%20data%20are%20encoded%20using%20sophisticated%20classical%20encoding%20strategies%0Awhich%20encode%20the%20data%20in%20terms%20of%20amplitude%20and%20angle%20of%20rotation%2C%20along%20with%0Asuperposition%20mapping.%20These%20techniques%20allow%20compression%20of%20information%0Aexponentially%20into%20Hilbert%20space%20representations%2C%20which%2C%20together%20with%20reduced%0Asample%20complexity%2C%20allows%20greater%20feature%20expressivity.%20We%20also%20examine%0Avariational%20quantum%20circuits%2C%20quantum%20gates%20expressed%20as%20trainable%20variables%0Athat%20run%20with%20classical%20optimizers%20to%20overcome%20decoherence%2C%20noise%2C%20and%0Agate-depth%20constraints%20of%20the%20existing%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%0Adevices.%20Experimental%20comparisons%20with%20a%20Quantum%20Naive%20Bayes%20classifier%20prove%0Athat%20even%20small%20quantum%20circuits%20can%20approximate%20probabilistic%20inference%20with%0Acompetitive%20accuracy%20compared%20to%20classical%20benchmarks%2C%20and%20have%20much%20better%0Arobustness%20to%20noisy%20data%20distributionsThis%20model%20does%20not%20only%20explain%20the%0Aalgorithmic%20and%20architectural%20design%20of%20QML%2C%20it%20also%20offers%20a%20roadmap%20to%20the%0Aimplementation%20of%20quantum%20kernels%2C%20variational%20algorithms%2C%20and%20hybrid%20feedback%0Aloops%20into%20practice%2C%20including%20optimization%2C%20computer%20vision%2C%20and%20medical%0Adiagnostics.%20The%20results%20support%20the%20idea%20that%20hybrid%20architectures%20with%20strong%0Adata%20encoding%20and%20adaptive%20error%20protection%20are%20key%20to%20moving%20QML%20out%20of%20theory%0Ato%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11951v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Data%2520Encoding%2520and%2520Variational%2520Algorithms%253A%2520A%2520Framework%2520for%2520Hybrid%250A%2520%2520Quantum%2520Classical%2520Machine%2520Learning%26entry.906535625%3DBhavna%2520Bose%2520and%2520Saurav%2520Verma%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520quantum%2520computers%2520has%2520been%2520the%2520stimulus%2520that%2520enables%2520the%250Arealization%2520of%2520Quantum%2520Machine%2520Learning%2520%2528QML%2529%252C%2520an%2520area%2520that%2520integrates%2520the%250Acalculational%2520framework%2520of%2520quantum%2520mechanics%2520with%2520the%2520adaptive%2520properties%2520of%250Aclassical%2520machine%2520learning.%2520This%2520article%2520suggests%2520a%2520broad%2520architecture%2520that%250Aallows%2520the%2520connection%2520between%2520classical%2520data%2520pipelines%2520and%2520quantum%2520algorithms%252C%250Ahybrid%2520quantum-classical%2520models%2520emerge%2520as%2520a%2520promising%2520route%2520to%2520scalable%2520and%250Anear-term%2520quantum%2520benefit.%2520At%2520the%2520core%2520of%2520this%2520paradigm%2520lies%2520the%250AClassical-Quantum%2520%2528CQ%2529%2520paradigm%252C%2520in%2520which%2520the%2520qubit%2520states%2520of%2520high-dimensional%250Aclassical%2520data%2520are%2520encoded%2520using%2520sophisticated%2520classical%2520encoding%2520strategies%250Awhich%2520encode%2520the%2520data%2520in%2520terms%2520of%2520amplitude%2520and%2520angle%2520of%2520rotation%252C%2520along%2520with%250Asuperposition%2520mapping.%2520These%2520techniques%2520allow%2520compression%2520of%2520information%250Aexponentially%2520into%2520Hilbert%2520space%2520representations%252C%2520which%252C%2520together%2520with%2520reduced%250Asample%2520complexity%252C%2520allows%2520greater%2520feature%2520expressivity.%2520We%2520also%2520examine%250Avariational%2520quantum%2520circuits%252C%2520quantum%2520gates%2520expressed%2520as%2520trainable%2520variables%250Athat%2520run%2520with%2520classical%2520optimizers%2520to%2520overcome%2520decoherence%252C%2520noise%252C%2520and%250Agate-depth%2520constraints%2520of%2520the%2520existing%2520Noisy%2520Intermediate-Scale%2520Quantum%2520%2528NISQ%2529%250Adevices.%2520Experimental%2520comparisons%2520with%2520a%2520Quantum%2520Naive%2520Bayes%2520classifier%2520prove%250Athat%2520even%2520small%2520quantum%2520circuits%2520can%2520approximate%2520probabilistic%2520inference%2520with%250Acompetitive%2520accuracy%2520compared%2520to%2520classical%2520benchmarks%252C%2520and%2520have%2520much%2520better%250Arobustness%2520to%2520noisy%2520data%2520distributionsThis%2520model%2520does%2520not%2520only%2520explain%2520the%250Aalgorithmic%2520and%2520architectural%2520design%2520of%2520QML%252C%2520it%2520also%2520offers%2520a%2520roadmap%2520to%2520the%250Aimplementation%2520of%2520quantum%2520kernels%252C%2520variational%2520algorithms%252C%2520and%2520hybrid%2520feedback%250Aloops%2520into%2520practice%252C%2520including%2520optimization%252C%2520computer%2520vision%252C%2520and%2520medical%250Adiagnostics.%2520The%2520results%2520support%2520the%2520idea%2520that%2520hybrid%2520architectures%2520with%2520strong%250Adata%2520encoding%2520and%2520adaptive%2520error%2520protection%2520are%2520key%2520to%2520moving%2520QML%2520out%2520of%2520theory%250Ato%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11951v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Data%20Encoding%20and%20Variational%20Algorithms%3A%20A%20Framework%20for%20Hybrid%0A%20%20Quantum%20Classical%20Machine%20Learning&entry.906535625=Bhavna%20Bose%20and%20Saurav%20Verma&entry.1292438233=%20%20The%20development%20of%20quantum%20computers%20has%20been%20the%20stimulus%20that%20enables%20the%0Arealization%20of%20Quantum%20Machine%20Learning%20%28QML%29%2C%20an%20area%20that%20integrates%20the%0Acalculational%20framework%20of%20quantum%20mechanics%20with%20the%20adaptive%20properties%20of%0Aclassical%20machine%20learning.%20This%20article%20suggests%20a%20broad%20architecture%20that%0Aallows%20the%20connection%20between%20classical%20data%20pipelines%20and%20quantum%20algorithms%2C%0Ahybrid%20quantum-classical%20models%20emerge%20as%20a%20promising%20route%20to%20scalable%20and%0Anear-term%20quantum%20benefit.%20At%20the%20core%20of%20this%20paradigm%20lies%20the%0AClassical-Quantum%20%28CQ%29%20paradigm%2C%20in%20which%20the%20qubit%20states%20of%20high-dimensional%0Aclassical%20data%20are%20encoded%20using%20sophisticated%20classical%20encoding%20strategies%0Awhich%20encode%20the%20data%20in%20terms%20of%20amplitude%20and%20angle%20of%20rotation%2C%20along%20with%0Asuperposition%20mapping.%20These%20techniques%20allow%20compression%20of%20information%0Aexponentially%20into%20Hilbert%20space%20representations%2C%20which%2C%20together%20with%20reduced%0Asample%20complexity%2C%20allows%20greater%20feature%20expressivity.%20We%20also%20examine%0Avariational%20quantum%20circuits%2C%20quantum%20gates%20expressed%20as%20trainable%20variables%0Athat%20run%20with%20classical%20optimizers%20to%20overcome%20decoherence%2C%20noise%2C%20and%0Agate-depth%20constraints%20of%20the%20existing%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%0Adevices.%20Experimental%20comparisons%20with%20a%20Quantum%20Naive%20Bayes%20classifier%20prove%0Athat%20even%20small%20quantum%20circuits%20can%20approximate%20probabilistic%20inference%20with%0Acompetitive%20accuracy%20compared%20to%20classical%20benchmarks%2C%20and%20have%20much%20better%0Arobustness%20to%20noisy%20data%20distributionsThis%20model%20does%20not%20only%20explain%20the%0Aalgorithmic%20and%20architectural%20design%20of%20QML%2C%20it%20also%20offers%20a%20roadmap%20to%20the%0Aimplementation%20of%20quantum%20kernels%2C%20variational%20algorithms%2C%20and%20hybrid%20feedback%0Aloops%20into%20practice%2C%20including%20optimization%2C%20computer%20vision%2C%20and%20medical%0Adiagnostics.%20The%20results%20support%20the%20idea%20that%20hybrid%20architectures%20with%20strong%0Adata%20encoding%20and%20adaptive%20error%20protection%20are%20key%20to%20moving%20QML%20out%20of%20theory%0Ato%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11951v2&entry.124074799=Read"},
{"title": "AutoDetect: Designing an Autoencoder-based Detection Method for\n  Poisoning Attacks on Object Detection Applications in the Military Domain", "author": "Alma M. Liezenga and Stefan Wijnja and Puck de Haan and Niels W. T. Brink and Jip J. van Stijn and Yori Kamphuis and Klamer Schutte", "abstract": "  Poisoning attacks pose an increasing threat to the security and robustness of\nArtificial Intelligence systems in the military domain. The widespread use of\nopen-source datasets and pretrained models exacerbates this risk. Despite the\nseverity of this threat, there is limited research on the application and\ndetection of poisoning attacks on object detection systems. This is especially\nproblematic in the military domain, where attacks can have grave consequences.\nIn this work, we both investigate the effect of poisoning attacks on military\nobject detectors in practice, and the best approach to detect these attacks. To\nsupport this research, we create a small, custom dataset featuring military\nvehicles: MilCivVeh. We explore the vulnerability of military object detectors\nfor poisoning attacks by implementing a modified version of the BadDet attack:\na patch-based poisoning attack. We then assess its impact, finding that while a\npositive attack success rate is achievable, it requires a substantial portion\nof the data to be poisoned -- raising questions about its practical\napplicability. To address the detection challenge, we test both specialized\npoisoning detection methods and anomaly detection methods from the visual\nindustrial inspection domain. Since our research shows that both classes of\nmethods are lacking, we introduce our own patch detection method: AutoDetect, a\nsimple, fast, and lightweight autoencoder-based method. Our method shows\npromising results in separating clean from poisoned samples using the\nreconstruction error of image slices, outperforming existing methods, while\nbeing less time- and memory-intensive. We urge that the availability of large,\nrepresentative datasets in the military domain is a prerequisite to further\nevaluate risks of poisoning attacks and opportunities patch detection.\n", "link": "http://arxiv.org/abs/2509.03179v1", "date": "2025-09-03", "relevancy": 1.9293, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4922}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4893}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoDetect%3A%20Designing%20an%20Autoencoder-based%20Detection%20Method%20for%0A%20%20Poisoning%20Attacks%20on%20Object%20Detection%20Applications%20in%20the%20Military%20Domain&body=Title%3A%20AutoDetect%3A%20Designing%20an%20Autoencoder-based%20Detection%20Method%20for%0A%20%20Poisoning%20Attacks%20on%20Object%20Detection%20Applications%20in%20the%20Military%20Domain%0AAuthor%3A%20Alma%20M.%20Liezenga%20and%20Stefan%20Wijnja%20and%20Puck%20de%20Haan%20and%20Niels%20W.%20T.%20Brink%20and%20Jip%20J.%20van%20Stijn%20and%20Yori%20Kamphuis%20and%20Klamer%20Schutte%0AAbstract%3A%20%20%20Poisoning%20attacks%20pose%20an%20increasing%20threat%20to%20the%20security%20and%20robustness%20of%0AArtificial%20Intelligence%20systems%20in%20the%20military%20domain.%20The%20widespread%20use%20of%0Aopen-source%20datasets%20and%20pretrained%20models%20exacerbates%20this%20risk.%20Despite%20the%0Aseverity%20of%20this%20threat%2C%20there%20is%20limited%20research%20on%20the%20application%20and%0Adetection%20of%20poisoning%20attacks%20on%20object%20detection%20systems.%20This%20is%20especially%0Aproblematic%20in%20the%20military%20domain%2C%20where%20attacks%20can%20have%20grave%20consequences.%0AIn%20this%20work%2C%20we%20both%20investigate%20the%20effect%20of%20poisoning%20attacks%20on%20military%0Aobject%20detectors%20in%20practice%2C%20and%20the%20best%20approach%20to%20detect%20these%20attacks.%20To%0Asupport%20this%20research%2C%20we%20create%20a%20small%2C%20custom%20dataset%20featuring%20military%0Avehicles%3A%20MilCivVeh.%20We%20explore%20the%20vulnerability%20of%20military%20object%20detectors%0Afor%20poisoning%20attacks%20by%20implementing%20a%20modified%20version%20of%20the%20BadDet%20attack%3A%0Aa%20patch-based%20poisoning%20attack.%20We%20then%20assess%20its%20impact%2C%20finding%20that%20while%20a%0Apositive%20attack%20success%20rate%20is%20achievable%2C%20it%20requires%20a%20substantial%20portion%0Aof%20the%20data%20to%20be%20poisoned%20--%20raising%20questions%20about%20its%20practical%0Aapplicability.%20To%20address%20the%20detection%20challenge%2C%20we%20test%20both%20specialized%0Apoisoning%20detection%20methods%20and%20anomaly%20detection%20methods%20from%20the%20visual%0Aindustrial%20inspection%20domain.%20Since%20our%20research%20shows%20that%20both%20classes%20of%0Amethods%20are%20lacking%2C%20we%20introduce%20our%20own%20patch%20detection%20method%3A%20AutoDetect%2C%20a%0Asimple%2C%20fast%2C%20and%20lightweight%20autoencoder-based%20method.%20Our%20method%20shows%0Apromising%20results%20in%20separating%20clean%20from%20poisoned%20samples%20using%20the%0Areconstruction%20error%20of%20image%20slices%2C%20outperforming%20existing%20methods%2C%20while%0Abeing%20less%20time-%20and%20memory-intensive.%20We%20urge%20that%20the%20availability%20of%20large%2C%0Arepresentative%20datasets%20in%20the%20military%20domain%20is%20a%20prerequisite%20to%20further%0Aevaluate%20risks%20of%20poisoning%20attacks%20and%20opportunities%20patch%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoDetect%253A%2520Designing%2520an%2520Autoencoder-based%2520Detection%2520Method%2520for%250A%2520%2520Poisoning%2520Attacks%2520on%2520Object%2520Detection%2520Applications%2520in%2520the%2520Military%2520Domain%26entry.906535625%3DAlma%2520M.%2520Liezenga%2520and%2520Stefan%2520Wijnja%2520and%2520Puck%2520de%2520Haan%2520and%2520Niels%2520W.%2520T.%2520Brink%2520and%2520Jip%2520J.%2520van%2520Stijn%2520and%2520Yori%2520Kamphuis%2520and%2520Klamer%2520Schutte%26entry.1292438233%3D%2520%2520Poisoning%2520attacks%2520pose%2520an%2520increasing%2520threat%2520to%2520the%2520security%2520and%2520robustness%2520of%250AArtificial%2520Intelligence%2520systems%2520in%2520the%2520military%2520domain.%2520The%2520widespread%2520use%2520of%250Aopen-source%2520datasets%2520and%2520pretrained%2520models%2520exacerbates%2520this%2520risk.%2520Despite%2520the%250Aseverity%2520of%2520this%2520threat%252C%2520there%2520is%2520limited%2520research%2520on%2520the%2520application%2520and%250Adetection%2520of%2520poisoning%2520attacks%2520on%2520object%2520detection%2520systems.%2520This%2520is%2520especially%250Aproblematic%2520in%2520the%2520military%2520domain%252C%2520where%2520attacks%2520can%2520have%2520grave%2520consequences.%250AIn%2520this%2520work%252C%2520we%2520both%2520investigate%2520the%2520effect%2520of%2520poisoning%2520attacks%2520on%2520military%250Aobject%2520detectors%2520in%2520practice%252C%2520and%2520the%2520best%2520approach%2520to%2520detect%2520these%2520attacks.%2520To%250Asupport%2520this%2520research%252C%2520we%2520create%2520a%2520small%252C%2520custom%2520dataset%2520featuring%2520military%250Avehicles%253A%2520MilCivVeh.%2520We%2520explore%2520the%2520vulnerability%2520of%2520military%2520object%2520detectors%250Afor%2520poisoning%2520attacks%2520by%2520implementing%2520a%2520modified%2520version%2520of%2520the%2520BadDet%2520attack%253A%250Aa%2520patch-based%2520poisoning%2520attack.%2520We%2520then%2520assess%2520its%2520impact%252C%2520finding%2520that%2520while%2520a%250Apositive%2520attack%2520success%2520rate%2520is%2520achievable%252C%2520it%2520requires%2520a%2520substantial%2520portion%250Aof%2520the%2520data%2520to%2520be%2520poisoned%2520--%2520raising%2520questions%2520about%2520its%2520practical%250Aapplicability.%2520To%2520address%2520the%2520detection%2520challenge%252C%2520we%2520test%2520both%2520specialized%250Apoisoning%2520detection%2520methods%2520and%2520anomaly%2520detection%2520methods%2520from%2520the%2520visual%250Aindustrial%2520inspection%2520domain.%2520Since%2520our%2520research%2520shows%2520that%2520both%2520classes%2520of%250Amethods%2520are%2520lacking%252C%2520we%2520introduce%2520our%2520own%2520patch%2520detection%2520method%253A%2520AutoDetect%252C%2520a%250Asimple%252C%2520fast%252C%2520and%2520lightweight%2520autoencoder-based%2520method.%2520Our%2520method%2520shows%250Apromising%2520results%2520in%2520separating%2520clean%2520from%2520poisoned%2520samples%2520using%2520the%250Areconstruction%2520error%2520of%2520image%2520slices%252C%2520outperforming%2520existing%2520methods%252C%2520while%250Abeing%2520less%2520time-%2520and%2520memory-intensive.%2520We%2520urge%2520that%2520the%2520availability%2520of%2520large%252C%250Arepresentative%2520datasets%2520in%2520the%2520military%2520domain%2520is%2520a%2520prerequisite%2520to%2520further%250Aevaluate%2520risks%2520of%2520poisoning%2520attacks%2520and%2520opportunities%2520patch%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoDetect%3A%20Designing%20an%20Autoencoder-based%20Detection%20Method%20for%0A%20%20Poisoning%20Attacks%20on%20Object%20Detection%20Applications%20in%20the%20Military%20Domain&entry.906535625=Alma%20M.%20Liezenga%20and%20Stefan%20Wijnja%20and%20Puck%20de%20Haan%20and%20Niels%20W.%20T.%20Brink%20and%20Jip%20J.%20van%20Stijn%20and%20Yori%20Kamphuis%20and%20Klamer%20Schutte&entry.1292438233=%20%20Poisoning%20attacks%20pose%20an%20increasing%20threat%20to%20the%20security%20and%20robustness%20of%0AArtificial%20Intelligence%20systems%20in%20the%20military%20domain.%20The%20widespread%20use%20of%0Aopen-source%20datasets%20and%20pretrained%20models%20exacerbates%20this%20risk.%20Despite%20the%0Aseverity%20of%20this%20threat%2C%20there%20is%20limited%20research%20on%20the%20application%20and%0Adetection%20of%20poisoning%20attacks%20on%20object%20detection%20systems.%20This%20is%20especially%0Aproblematic%20in%20the%20military%20domain%2C%20where%20attacks%20can%20have%20grave%20consequences.%0AIn%20this%20work%2C%20we%20both%20investigate%20the%20effect%20of%20poisoning%20attacks%20on%20military%0Aobject%20detectors%20in%20practice%2C%20and%20the%20best%20approach%20to%20detect%20these%20attacks.%20To%0Asupport%20this%20research%2C%20we%20create%20a%20small%2C%20custom%20dataset%20featuring%20military%0Avehicles%3A%20MilCivVeh.%20We%20explore%20the%20vulnerability%20of%20military%20object%20detectors%0Afor%20poisoning%20attacks%20by%20implementing%20a%20modified%20version%20of%20the%20BadDet%20attack%3A%0Aa%20patch-based%20poisoning%20attack.%20We%20then%20assess%20its%20impact%2C%20finding%20that%20while%20a%0Apositive%20attack%20success%20rate%20is%20achievable%2C%20it%20requires%20a%20substantial%20portion%0Aof%20the%20data%20to%20be%20poisoned%20--%20raising%20questions%20about%20its%20practical%0Aapplicability.%20To%20address%20the%20detection%20challenge%2C%20we%20test%20both%20specialized%0Apoisoning%20detection%20methods%20and%20anomaly%20detection%20methods%20from%20the%20visual%0Aindustrial%20inspection%20domain.%20Since%20our%20research%20shows%20that%20both%20classes%20of%0Amethods%20are%20lacking%2C%20we%20introduce%20our%20own%20patch%20detection%20method%3A%20AutoDetect%2C%20a%0Asimple%2C%20fast%2C%20and%20lightweight%20autoencoder-based%20method.%20Our%20method%20shows%0Apromising%20results%20in%20separating%20clean%20from%20poisoned%20samples%20using%20the%0Areconstruction%20error%20of%20image%20slices%2C%20outperforming%20existing%20methods%2C%20while%0Abeing%20less%20time-%20and%20memory-intensive.%20We%20urge%20that%20the%20availability%20of%20large%2C%0Arepresentative%20datasets%20in%20the%20military%20domain%20is%20a%20prerequisite%20to%20further%0Aevaluate%20risks%20of%20poisoning%20attacks%20and%20opportunities%20patch%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03179v1&entry.124074799=Read"},
{"title": "Tabular foundation model for GEOAI benchmark problems\n  BM/AirportSoilProperties/2/2025", "author": "Taiga Saito and Yu Otake and Stephen Wu", "abstract": "  This paper presents a novel application of the Tabular Prior-Data Fitted\nNetwork (TabPFN) - a transformer-based foundation model for tabular data - to\ngeotechnical site characterization problems defined in the GEOAI benchmark\nBM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the\nspatial variation of undrained shear strength (su) across borehole depth\nprofiles, and (2) imputing missing mechanical parameters in a dense-site\ndataset. We apply TabPFN in a zero-training, few-shot, in-context learning\nsetting - without hyper-parameter tuning - and provide it with additional\ncontext from the big indirect database (BID). The study demonstrates that\nTabPFN, as a general-purpose foundation model, achieved superior accuracy and\nwell-calibrated predictive distributions compared to a conventional\nhierarchical Bayesian model (HBM) baseline, while also offering significant\ngains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),\nTabPFN outperformed the HBM in prediction accuracy and delivered an\norder-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical\nparameter imputation), TabPFN likewise achieved lower RMSE for all target\nparameters with well-quantified uncertainties, though its cumulative\ncomputation cost was higher than HBM's due to its one-variable-at-a-time\ninference. These results mark the first successful use of a tabular foundation\nmodel in geotechnical modeling, suggesting a potential paradigm shift in\nprobabilistic site characterization.\n", "link": "http://arxiv.org/abs/2509.03191v1", "date": "2025-09-03", "relevancy": 1.9287, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5321}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4739}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tabular%20foundation%20model%20for%20GEOAI%20benchmark%20problems%0A%20%20BM/AirportSoilProperties/2/2025&body=Title%3A%20Tabular%20foundation%20model%20for%20GEOAI%20benchmark%20problems%0A%20%20BM/AirportSoilProperties/2/2025%0AAuthor%3A%20Taiga%20Saito%20and%20Yu%20Otake%20and%20Stephen%20Wu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20application%20of%20the%20Tabular%20Prior-Data%20Fitted%0ANetwork%20%28TabPFN%29%20-%20a%20transformer-based%20foundation%20model%20for%20tabular%20data%20-%20to%0Ageotechnical%20site%20characterization%20problems%20defined%20in%20the%20GEOAI%20benchmark%0ABM/AirportSoilProperties/2/2025.%20Two%20tasks%20are%20addressed%3A%20%281%29%20predicting%20the%0Aspatial%20variation%20of%20undrained%20shear%20strength%20%28su%29%20across%20borehole%20depth%0Aprofiles%2C%20and%20%282%29%20imputing%20missing%20mechanical%20parameters%20in%20a%20dense-site%0Adataset.%20We%20apply%20TabPFN%20in%20a%20zero-training%2C%20few-shot%2C%20in-context%20learning%0Asetting%20-%20without%20hyper-parameter%20tuning%20-%20and%20provide%20it%20with%20additional%0Acontext%20from%20the%20big%20indirect%20database%20%28BID%29.%20The%20study%20demonstrates%20that%0ATabPFN%2C%20as%20a%20general-purpose%20foundation%20model%2C%20achieved%20superior%20accuracy%20and%0Awell-calibrated%20predictive%20distributions%20compared%20to%20a%20conventional%0Ahierarchical%20Bayesian%20model%20%28HBM%29%20baseline%2C%20while%20also%20offering%20significant%0Agains%20in%20inference%20efficiency.%20In%20Benchmark%20Problem%20%231%20%28spatial%20su%20prediction%29%2C%0ATabPFN%20outperformed%20the%20HBM%20in%20prediction%20accuracy%20and%20delivered%20an%0Aorder-of-magnitude%20faster%20runtime.%20In%20Benchmark%20Problem%20%232%20%28missing%20mechanical%0Aparameter%20imputation%29%2C%20TabPFN%20likewise%20achieved%20lower%20RMSE%20for%20all%20target%0Aparameters%20with%20well-quantified%20uncertainties%2C%20though%20its%20cumulative%0Acomputation%20cost%20was%20higher%20than%20HBM%27s%20due%20to%20its%20one-variable-at-a-time%0Ainference.%20These%20results%20mark%20the%20first%20successful%20use%20of%20a%20tabular%20foundation%0Amodel%20in%20geotechnical%20modeling%2C%20suggesting%20a%20potential%20paradigm%20shift%20in%0Aprobabilistic%20site%20characterization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabular%2520foundation%2520model%2520for%2520GEOAI%2520benchmark%2520problems%250A%2520%2520BM/AirportSoilProperties/2/2025%26entry.906535625%3DTaiga%2520Saito%2520and%2520Yu%2520Otake%2520and%2520Stephen%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520application%2520of%2520the%2520Tabular%2520Prior-Data%2520Fitted%250ANetwork%2520%2528TabPFN%2529%2520-%2520a%2520transformer-based%2520foundation%2520model%2520for%2520tabular%2520data%2520-%2520to%250Ageotechnical%2520site%2520characterization%2520problems%2520defined%2520in%2520the%2520GEOAI%2520benchmark%250ABM/AirportSoilProperties/2/2025.%2520Two%2520tasks%2520are%2520addressed%253A%2520%25281%2529%2520predicting%2520the%250Aspatial%2520variation%2520of%2520undrained%2520shear%2520strength%2520%2528su%2529%2520across%2520borehole%2520depth%250Aprofiles%252C%2520and%2520%25282%2529%2520imputing%2520missing%2520mechanical%2520parameters%2520in%2520a%2520dense-site%250Adataset.%2520We%2520apply%2520TabPFN%2520in%2520a%2520zero-training%252C%2520few-shot%252C%2520in-context%2520learning%250Asetting%2520-%2520without%2520hyper-parameter%2520tuning%2520-%2520and%2520provide%2520it%2520with%2520additional%250Acontext%2520from%2520the%2520big%2520indirect%2520database%2520%2528BID%2529.%2520The%2520study%2520demonstrates%2520that%250ATabPFN%252C%2520as%2520a%2520general-purpose%2520foundation%2520model%252C%2520achieved%2520superior%2520accuracy%2520and%250Awell-calibrated%2520predictive%2520distributions%2520compared%2520to%2520a%2520conventional%250Ahierarchical%2520Bayesian%2520model%2520%2528HBM%2529%2520baseline%252C%2520while%2520also%2520offering%2520significant%250Agains%2520in%2520inference%2520efficiency.%2520In%2520Benchmark%2520Problem%2520%25231%2520%2528spatial%2520su%2520prediction%2529%252C%250ATabPFN%2520outperformed%2520the%2520HBM%2520in%2520prediction%2520accuracy%2520and%2520delivered%2520an%250Aorder-of-magnitude%2520faster%2520runtime.%2520In%2520Benchmark%2520Problem%2520%25232%2520%2528missing%2520mechanical%250Aparameter%2520imputation%2529%252C%2520TabPFN%2520likewise%2520achieved%2520lower%2520RMSE%2520for%2520all%2520target%250Aparameters%2520with%2520well-quantified%2520uncertainties%252C%2520though%2520its%2520cumulative%250Acomputation%2520cost%2520was%2520higher%2520than%2520HBM%2527s%2520due%2520to%2520its%2520one-variable-at-a-time%250Ainference.%2520These%2520results%2520mark%2520the%2520first%2520successful%2520use%2520of%2520a%2520tabular%2520foundation%250Amodel%2520in%2520geotechnical%2520modeling%252C%2520suggesting%2520a%2520potential%2520paradigm%2520shift%2520in%250Aprobabilistic%2520site%2520characterization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tabular%20foundation%20model%20for%20GEOAI%20benchmark%20problems%0A%20%20BM/AirportSoilProperties/2/2025&entry.906535625=Taiga%20Saito%20and%20Yu%20Otake%20and%20Stephen%20Wu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20application%20of%20the%20Tabular%20Prior-Data%20Fitted%0ANetwork%20%28TabPFN%29%20-%20a%20transformer-based%20foundation%20model%20for%20tabular%20data%20-%20to%0Ageotechnical%20site%20characterization%20problems%20defined%20in%20the%20GEOAI%20benchmark%0ABM/AirportSoilProperties/2/2025.%20Two%20tasks%20are%20addressed%3A%20%281%29%20predicting%20the%0Aspatial%20variation%20of%20undrained%20shear%20strength%20%28su%29%20across%20borehole%20depth%0Aprofiles%2C%20and%20%282%29%20imputing%20missing%20mechanical%20parameters%20in%20a%20dense-site%0Adataset.%20We%20apply%20TabPFN%20in%20a%20zero-training%2C%20few-shot%2C%20in-context%20learning%0Asetting%20-%20without%20hyper-parameter%20tuning%20-%20and%20provide%20it%20with%20additional%0Acontext%20from%20the%20big%20indirect%20database%20%28BID%29.%20The%20study%20demonstrates%20that%0ATabPFN%2C%20as%20a%20general-purpose%20foundation%20model%2C%20achieved%20superior%20accuracy%20and%0Awell-calibrated%20predictive%20distributions%20compared%20to%20a%20conventional%0Ahierarchical%20Bayesian%20model%20%28HBM%29%20baseline%2C%20while%20also%20offering%20significant%0Agains%20in%20inference%20efficiency.%20In%20Benchmark%20Problem%20%231%20%28spatial%20su%20prediction%29%2C%0ATabPFN%20outperformed%20the%20HBM%20in%20prediction%20accuracy%20and%20delivered%20an%0Aorder-of-magnitude%20faster%20runtime.%20In%20Benchmark%20Problem%20%232%20%28missing%20mechanical%0Aparameter%20imputation%29%2C%20TabPFN%20likewise%20achieved%20lower%20RMSE%20for%20all%20target%0Aparameters%20with%20well-quantified%20uncertainties%2C%20though%20its%20cumulative%0Acomputation%20cost%20was%20higher%20than%20HBM%27s%20due%20to%20its%20one-variable-at-a-time%0Ainference.%20These%20results%20mark%20the%20first%20successful%20use%20of%20a%20tabular%20foundation%0Amodel%20in%20geotechnical%20modeling%2C%20suggesting%20a%20potential%20paradigm%20shift%20in%0Aprobabilistic%20site%20characterization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03191v1&entry.124074799=Read"},
{"title": "Continuous Saudi Sign Language Recognition: A Vision Transformer\n  Approach", "author": "Soukeina Elhassen and Lama Al Khuzayem and Areej Alhothali and Ohoud Alzamzami and Nahed Alowaidi", "abstract": "  Sign language (SL) is an essential communication form for hearing-impaired\nand deaf people, enabling engagement within the broader society. Despite its\nsignificance, limited public awareness of SL often leads to inequitable access\nto educational and professional opportunities, thereby contributing to social\nexclusion, particularly in Saudi Arabia, where over 84,000 individuals depend\non Saudi Sign Language (SSL) as their primary form of communication. Although\ncertain technological approaches have helped to improve communication for\nindividuals with hearing impairments, there continues to be an urgent\nrequirement for more precise and dependable translation techniques, especially\nfor Arabic sign language variants like SSL. Most state-of-the-art solutions\nhave primarily focused on non-Arabic sign languages, resulting in a\nconsiderable absence of resources dedicated to Arabic sign language,\nspecifically SSL. The complexity of the Arabic language and the prevalence of\nisolated sign language datasets that concentrate on individual words instead of\ncontinuous speech contribute to this issue. To address this gap, our research\nrepresents an important step in developing SSL resources. To address this, we\nintroduce the first continuous Saudi Sign Language dataset called KAU-CSSL,\nfocusing on complete sentences to facilitate further research and enable\nsophisticated recognition systems for SSL recognition and translation.\nAdditionally, we propose a transformer-based model, utilizing a pretrained\nResNet-18 for spatial feature extraction and a Transformer Encoder with\nBidirectional LSTM for temporal dependencies, achieving 99.02\\% accuracy at\nsigner dependent mode and 77.71\\% accuracy at signer independent mode. This\ndevelopment leads the way to not only improving communication tools for the SSL\ncommunity but also making a substantial contribution to the wider field of sign\nlanguage.\n", "link": "http://arxiv.org/abs/2509.03467v1", "date": "2025-09-03", "relevancy": 1.9286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Saudi%20Sign%20Language%20Recognition%3A%20A%20Vision%20Transformer%0A%20%20Approach&body=Title%3A%20Continuous%20Saudi%20Sign%20Language%20Recognition%3A%20A%20Vision%20Transformer%0A%20%20Approach%0AAuthor%3A%20Soukeina%20Elhassen%20and%20Lama%20Al%20Khuzayem%20and%20Areej%20Alhothali%20and%20Ohoud%20Alzamzami%20and%20Nahed%20Alowaidi%0AAbstract%3A%20%20%20Sign%20language%20%28SL%29%20is%20an%20essential%20communication%20form%20for%20hearing-impaired%0Aand%20deaf%20people%2C%20enabling%20engagement%20within%20the%20broader%20society.%20Despite%20its%0Asignificance%2C%20limited%20public%20awareness%20of%20SL%20often%20leads%20to%20inequitable%20access%0Ato%20educational%20and%20professional%20opportunities%2C%20thereby%20contributing%20to%20social%0Aexclusion%2C%20particularly%20in%20Saudi%20Arabia%2C%20where%20over%2084%2C000%20individuals%20depend%0Aon%20Saudi%20Sign%20Language%20%28SSL%29%20as%20their%20primary%20form%20of%20communication.%20Although%0Acertain%20technological%20approaches%20have%20helped%20to%20improve%20communication%20for%0Aindividuals%20with%20hearing%20impairments%2C%20there%20continues%20to%20be%20an%20urgent%0Arequirement%20for%20more%20precise%20and%20dependable%20translation%20techniques%2C%20especially%0Afor%20Arabic%20sign%20language%20variants%20like%20SSL.%20Most%20state-of-the-art%20solutions%0Ahave%20primarily%20focused%20on%20non-Arabic%20sign%20languages%2C%20resulting%20in%20a%0Aconsiderable%20absence%20of%20resources%20dedicated%20to%20Arabic%20sign%20language%2C%0Aspecifically%20SSL.%20The%20complexity%20of%20the%20Arabic%20language%20and%20the%20prevalence%20of%0Aisolated%20sign%20language%20datasets%20that%20concentrate%20on%20individual%20words%20instead%20of%0Acontinuous%20speech%20contribute%20to%20this%20issue.%20To%20address%20this%20gap%2C%20our%20research%0Arepresents%20an%20important%20step%20in%20developing%20SSL%20resources.%20To%20address%20this%2C%20we%0Aintroduce%20the%20first%20continuous%20Saudi%20Sign%20Language%20dataset%20called%20KAU-CSSL%2C%0Afocusing%20on%20complete%20sentences%20to%20facilitate%20further%20research%20and%20enable%0Asophisticated%20recognition%20systems%20for%20SSL%20recognition%20and%20translation.%0AAdditionally%2C%20we%20propose%20a%20transformer-based%20model%2C%20utilizing%20a%20pretrained%0AResNet-18%20for%20spatial%20feature%20extraction%20and%20a%20Transformer%20Encoder%20with%0ABidirectional%20LSTM%20for%20temporal%20dependencies%2C%20achieving%2099.02%5C%25%20accuracy%20at%0Asigner%20dependent%20mode%20and%2077.71%5C%25%20accuracy%20at%20signer%20independent%20mode.%20This%0Adevelopment%20leads%20the%20way%20to%20not%20only%20improving%20communication%20tools%20for%20the%20SSL%0Acommunity%20but%20also%20making%20a%20substantial%20contribution%20to%20the%20wider%20field%20of%20sign%0Alanguage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Saudi%2520Sign%2520Language%2520Recognition%253A%2520A%2520Vision%2520Transformer%250A%2520%2520Approach%26entry.906535625%3DSoukeina%2520Elhassen%2520and%2520Lama%2520Al%2520Khuzayem%2520and%2520Areej%2520Alhothali%2520and%2520Ohoud%2520Alzamzami%2520and%2520Nahed%2520Alowaidi%26entry.1292438233%3D%2520%2520Sign%2520language%2520%2528SL%2529%2520is%2520an%2520essential%2520communication%2520form%2520for%2520hearing-impaired%250Aand%2520deaf%2520people%252C%2520enabling%2520engagement%2520within%2520the%2520broader%2520society.%2520Despite%2520its%250Asignificance%252C%2520limited%2520public%2520awareness%2520of%2520SL%2520often%2520leads%2520to%2520inequitable%2520access%250Ato%2520educational%2520and%2520professional%2520opportunities%252C%2520thereby%2520contributing%2520to%2520social%250Aexclusion%252C%2520particularly%2520in%2520Saudi%2520Arabia%252C%2520where%2520over%252084%252C000%2520individuals%2520depend%250Aon%2520Saudi%2520Sign%2520Language%2520%2528SSL%2529%2520as%2520their%2520primary%2520form%2520of%2520communication.%2520Although%250Acertain%2520technological%2520approaches%2520have%2520helped%2520to%2520improve%2520communication%2520for%250Aindividuals%2520with%2520hearing%2520impairments%252C%2520there%2520continues%2520to%2520be%2520an%2520urgent%250Arequirement%2520for%2520more%2520precise%2520and%2520dependable%2520translation%2520techniques%252C%2520especially%250Afor%2520Arabic%2520sign%2520language%2520variants%2520like%2520SSL.%2520Most%2520state-of-the-art%2520solutions%250Ahave%2520primarily%2520focused%2520on%2520non-Arabic%2520sign%2520languages%252C%2520resulting%2520in%2520a%250Aconsiderable%2520absence%2520of%2520resources%2520dedicated%2520to%2520Arabic%2520sign%2520language%252C%250Aspecifically%2520SSL.%2520The%2520complexity%2520of%2520the%2520Arabic%2520language%2520and%2520the%2520prevalence%2520of%250Aisolated%2520sign%2520language%2520datasets%2520that%2520concentrate%2520on%2520individual%2520words%2520instead%2520of%250Acontinuous%2520speech%2520contribute%2520to%2520this%2520issue.%2520To%2520address%2520this%2520gap%252C%2520our%2520research%250Arepresents%2520an%2520important%2520step%2520in%2520developing%2520SSL%2520resources.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520the%2520first%2520continuous%2520Saudi%2520Sign%2520Language%2520dataset%2520called%2520KAU-CSSL%252C%250Afocusing%2520on%2520complete%2520sentences%2520to%2520facilitate%2520further%2520research%2520and%2520enable%250Asophisticated%2520recognition%2520systems%2520for%2520SSL%2520recognition%2520and%2520translation.%250AAdditionally%252C%2520we%2520propose%2520a%2520transformer-based%2520model%252C%2520utilizing%2520a%2520pretrained%250AResNet-18%2520for%2520spatial%2520feature%2520extraction%2520and%2520a%2520Transformer%2520Encoder%2520with%250ABidirectional%2520LSTM%2520for%2520temporal%2520dependencies%252C%2520achieving%252099.02%255C%2525%2520accuracy%2520at%250Asigner%2520dependent%2520mode%2520and%252077.71%255C%2525%2520accuracy%2520at%2520signer%2520independent%2520mode.%2520This%250Adevelopment%2520leads%2520the%2520way%2520to%2520not%2520only%2520improving%2520communication%2520tools%2520for%2520the%2520SSL%250Acommunity%2520but%2520also%2520making%2520a%2520substantial%2520contribution%2520to%2520the%2520wider%2520field%2520of%2520sign%250Alanguage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Saudi%20Sign%20Language%20Recognition%3A%20A%20Vision%20Transformer%0A%20%20Approach&entry.906535625=Soukeina%20Elhassen%20and%20Lama%20Al%20Khuzayem%20and%20Areej%20Alhothali%20and%20Ohoud%20Alzamzami%20and%20Nahed%20Alowaidi&entry.1292438233=%20%20Sign%20language%20%28SL%29%20is%20an%20essential%20communication%20form%20for%20hearing-impaired%0Aand%20deaf%20people%2C%20enabling%20engagement%20within%20the%20broader%20society.%20Despite%20its%0Asignificance%2C%20limited%20public%20awareness%20of%20SL%20often%20leads%20to%20inequitable%20access%0Ato%20educational%20and%20professional%20opportunities%2C%20thereby%20contributing%20to%20social%0Aexclusion%2C%20particularly%20in%20Saudi%20Arabia%2C%20where%20over%2084%2C000%20individuals%20depend%0Aon%20Saudi%20Sign%20Language%20%28SSL%29%20as%20their%20primary%20form%20of%20communication.%20Although%0Acertain%20technological%20approaches%20have%20helped%20to%20improve%20communication%20for%0Aindividuals%20with%20hearing%20impairments%2C%20there%20continues%20to%20be%20an%20urgent%0Arequirement%20for%20more%20precise%20and%20dependable%20translation%20techniques%2C%20especially%0Afor%20Arabic%20sign%20language%20variants%20like%20SSL.%20Most%20state-of-the-art%20solutions%0Ahave%20primarily%20focused%20on%20non-Arabic%20sign%20languages%2C%20resulting%20in%20a%0Aconsiderable%20absence%20of%20resources%20dedicated%20to%20Arabic%20sign%20language%2C%0Aspecifically%20SSL.%20The%20complexity%20of%20the%20Arabic%20language%20and%20the%20prevalence%20of%0Aisolated%20sign%20language%20datasets%20that%20concentrate%20on%20individual%20words%20instead%20of%0Acontinuous%20speech%20contribute%20to%20this%20issue.%20To%20address%20this%20gap%2C%20our%20research%0Arepresents%20an%20important%20step%20in%20developing%20SSL%20resources.%20To%20address%20this%2C%20we%0Aintroduce%20the%20first%20continuous%20Saudi%20Sign%20Language%20dataset%20called%20KAU-CSSL%2C%0Afocusing%20on%20complete%20sentences%20to%20facilitate%20further%20research%20and%20enable%0Asophisticated%20recognition%20systems%20for%20SSL%20recognition%20and%20translation.%0AAdditionally%2C%20we%20propose%20a%20transformer-based%20model%2C%20utilizing%20a%20pretrained%0AResNet-18%20for%20spatial%20feature%20extraction%20and%20a%20Transformer%20Encoder%20with%0ABidirectional%20LSTM%20for%20temporal%20dependencies%2C%20achieving%2099.02%5C%25%20accuracy%20at%0Asigner%20dependent%20mode%20and%2077.71%5C%25%20accuracy%20at%20signer%20independent%20mode.%20This%0Adevelopment%20leads%20the%20way%20to%20not%20only%20improving%20communication%20tools%20for%20the%20SSL%0Acommunity%20but%20also%20making%20a%20substantial%20contribution%20to%20the%20wider%20field%20of%20sign%0Alanguage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03467v1&entry.124074799=Read"},
{"title": "TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of\n  Large Language Models", "author": "Yuxuan Gu and Wuyang Zhou and Giorgos Iacovides and Danilo Mandic", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have significantly reduced the number of trainable parameters needed in\nfine-tuning large language models (LLMs). Subsequent developments of LoRA-style\nadapters have diverged into two main directions: (1) enhancing model\nexpressivity with high-rank adapters, and (2) pushing for further parameter\nreduction, as exemplified by vector-based methods. However, these approaches\npresent a trade-off, as achieving the expressivity of high-rank weight updates\ntypically comes at the cost of sacrificing the extreme parameter efficiency\noffered by vector-based techniques. To address this issue, we propose a\nvector-based random \\underline{\\textbf{Te}}nsor network for\nhigh-\\underline{\\textbf{R}}ank \\underline{\\textbf{A}}daptation (TeRA), a novel\nPEFT method that achieves high-rank weight updates while retaining the\nparameter efficiency of vector-based PEFT adapters. This is achieved by\nparameterizing the tensorized weight update matrix as a Tucker-like tensor\nnetwork (TN), in which large randomly initialized factors are frozen and shared\nacross layers, while only small layer-specific scaling vectors, formed by\nentries in diagonal factor matrices, are trained. This design effectively\ndecouples the rank of the weight update matrix from the number of trainable\nparameters. Comprehensive experiments demonstrate that TeRA matches or even\noutperforms high-rank adapters, while requiring a trainable parameter count\nsimilar to vector-based methods. Theoretical analysis and ablation studies\nfurther validate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2509.03234v1", "date": "2025-09-03", "relevancy": 1.9244, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4965}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4773}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeRA%3A%20Vector-based%20Random%20Tensor%20Network%20for%20High-Rank%20Adaptation%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20TeRA%3A%20Vector-based%20Random%20Tensor%20Network%20for%20High-Rank%20Adaptation%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yuxuan%20Gu%20and%20Wuyang%20Zhou%20and%20Giorgos%20Iacovides%20and%20Danilo%20Mandic%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20Low-Rank%20Adaptation%0A%28LoRA%29%2C%20have%20significantly%20reduced%20the%20number%20of%20trainable%20parameters%20needed%20in%0Afine-tuning%20large%20language%20models%20%28LLMs%29.%20Subsequent%20developments%20of%20LoRA-style%0Aadapters%20have%20diverged%20into%20two%20main%20directions%3A%20%281%29%20enhancing%20model%0Aexpressivity%20with%20high-rank%20adapters%2C%20and%20%282%29%20pushing%20for%20further%20parameter%0Areduction%2C%20as%20exemplified%20by%20vector-based%20methods.%20However%2C%20these%20approaches%0Apresent%20a%20trade-off%2C%20as%20achieving%20the%20expressivity%20of%20high-rank%20weight%20updates%0Atypically%20comes%20at%20the%20cost%20of%20sacrificing%20the%20extreme%20parameter%20efficiency%0Aoffered%20by%20vector-based%20techniques.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Avector-based%20random%20%5Cunderline%7B%5Ctextbf%7BTe%7D%7Dnsor%20network%20for%0Ahigh-%5Cunderline%7B%5Ctextbf%7BR%7D%7Dank%20%5Cunderline%7B%5Ctextbf%7BA%7D%7Ddaptation%20%28TeRA%29%2C%20a%20novel%0APEFT%20method%20that%20achieves%20high-rank%20weight%20updates%20while%20retaining%20the%0Aparameter%20efficiency%20of%20vector-based%20PEFT%20adapters.%20This%20is%20achieved%20by%0Aparameterizing%20the%20tensorized%20weight%20update%20matrix%20as%20a%20Tucker-like%20tensor%0Anetwork%20%28TN%29%2C%20in%20which%20large%20randomly%20initialized%20factors%20are%20frozen%20and%20shared%0Aacross%20layers%2C%20while%20only%20small%20layer-specific%20scaling%20vectors%2C%20formed%20by%0Aentries%20in%20diagonal%20factor%20matrices%2C%20are%20trained.%20This%20design%20effectively%0Adecouples%20the%20rank%20of%20the%20weight%20update%20matrix%20from%20the%20number%20of%20trainable%0Aparameters.%20Comprehensive%20experiments%20demonstrate%20that%20TeRA%20matches%20or%20even%0Aoutperforms%20high-rank%20adapters%2C%20while%20requiring%20a%20trainable%20parameter%20count%0Asimilar%20to%20vector-based%20methods.%20Theoretical%20analysis%20and%20ablation%20studies%0Afurther%20validate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeRA%253A%2520Vector-based%2520Random%2520Tensor%2520Network%2520for%2520High-Rank%2520Adaptation%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYuxuan%2520Gu%2520and%2520Wuyang%2520Zhou%2520and%2520Giorgos%2520Iacovides%2520and%2520Danilo%2520Mandic%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%252C%2520have%2520significantly%2520reduced%2520the%2520number%2520of%2520trainable%2520parameters%2520needed%2520in%250Afine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Subsequent%2520developments%2520of%2520LoRA-style%250Aadapters%2520have%2520diverged%2520into%2520two%2520main%2520directions%253A%2520%25281%2529%2520enhancing%2520model%250Aexpressivity%2520with%2520high-rank%2520adapters%252C%2520and%2520%25282%2529%2520pushing%2520for%2520further%2520parameter%250Areduction%252C%2520as%2520exemplified%2520by%2520vector-based%2520methods.%2520However%252C%2520these%2520approaches%250Apresent%2520a%2520trade-off%252C%2520as%2520achieving%2520the%2520expressivity%2520of%2520high-rank%2520weight%2520updates%250Atypically%2520comes%2520at%2520the%2520cost%2520of%2520sacrificing%2520the%2520extreme%2520parameter%2520efficiency%250Aoffered%2520by%2520vector-based%2520techniques.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Avector-based%2520random%2520%255Cunderline%257B%255Ctextbf%257BTe%257D%257Dnsor%2520network%2520for%250Ahigh-%255Cunderline%257B%255Ctextbf%257BR%257D%257Dank%2520%255Cunderline%257B%255Ctextbf%257BA%257D%257Ddaptation%2520%2528TeRA%2529%252C%2520a%2520novel%250APEFT%2520method%2520that%2520achieves%2520high-rank%2520weight%2520updates%2520while%2520retaining%2520the%250Aparameter%2520efficiency%2520of%2520vector-based%2520PEFT%2520adapters.%2520This%2520is%2520achieved%2520by%250Aparameterizing%2520the%2520tensorized%2520weight%2520update%2520matrix%2520as%2520a%2520Tucker-like%2520tensor%250Anetwork%2520%2528TN%2529%252C%2520in%2520which%2520large%2520randomly%2520initialized%2520factors%2520are%2520frozen%2520and%2520shared%250Aacross%2520layers%252C%2520while%2520only%2520small%2520layer-specific%2520scaling%2520vectors%252C%2520formed%2520by%250Aentries%2520in%2520diagonal%2520factor%2520matrices%252C%2520are%2520trained.%2520This%2520design%2520effectively%250Adecouples%2520the%2520rank%2520of%2520the%2520weight%2520update%2520matrix%2520from%2520the%2520number%2520of%2520trainable%250Aparameters.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520TeRA%2520matches%2520or%2520even%250Aoutperforms%2520high-rank%2520adapters%252C%2520while%2520requiring%2520a%2520trainable%2520parameter%2520count%250Asimilar%2520to%2520vector-based%2520methods.%2520Theoretical%2520analysis%2520and%2520ablation%2520studies%250Afurther%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeRA%3A%20Vector-based%20Random%20Tensor%20Network%20for%20High-Rank%20Adaptation%20of%0A%20%20Large%20Language%20Models&entry.906535625=Yuxuan%20Gu%20and%20Wuyang%20Zhou%20and%20Giorgos%20Iacovides%20and%20Danilo%20Mandic&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20Low-Rank%20Adaptation%0A%28LoRA%29%2C%20have%20significantly%20reduced%20the%20number%20of%20trainable%20parameters%20needed%20in%0Afine-tuning%20large%20language%20models%20%28LLMs%29.%20Subsequent%20developments%20of%20LoRA-style%0Aadapters%20have%20diverged%20into%20two%20main%20directions%3A%20%281%29%20enhancing%20model%0Aexpressivity%20with%20high-rank%20adapters%2C%20and%20%282%29%20pushing%20for%20further%20parameter%0Areduction%2C%20as%20exemplified%20by%20vector-based%20methods.%20However%2C%20these%20approaches%0Apresent%20a%20trade-off%2C%20as%20achieving%20the%20expressivity%20of%20high-rank%20weight%20updates%0Atypically%20comes%20at%20the%20cost%20of%20sacrificing%20the%20extreme%20parameter%20efficiency%0Aoffered%20by%20vector-based%20techniques.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Avector-based%20random%20%5Cunderline%7B%5Ctextbf%7BTe%7D%7Dnsor%20network%20for%0Ahigh-%5Cunderline%7B%5Ctextbf%7BR%7D%7Dank%20%5Cunderline%7B%5Ctextbf%7BA%7D%7Ddaptation%20%28TeRA%29%2C%20a%20novel%0APEFT%20method%20that%20achieves%20high-rank%20weight%20updates%20while%20retaining%20the%0Aparameter%20efficiency%20of%20vector-based%20PEFT%20adapters.%20This%20is%20achieved%20by%0Aparameterizing%20the%20tensorized%20weight%20update%20matrix%20as%20a%20Tucker-like%20tensor%0Anetwork%20%28TN%29%2C%20in%20which%20large%20randomly%20initialized%20factors%20are%20frozen%20and%20shared%0Aacross%20layers%2C%20while%20only%20small%20layer-specific%20scaling%20vectors%2C%20formed%20by%0Aentries%20in%20diagonal%20factor%20matrices%2C%20are%20trained.%20This%20design%20effectively%0Adecouples%20the%20rank%20of%20the%20weight%20update%20matrix%20from%20the%20number%20of%20trainable%0Aparameters.%20Comprehensive%20experiments%20demonstrate%20that%20TeRA%20matches%20or%20even%0Aoutperforms%20high-rank%20adapters%2C%20while%20requiring%20a%20trainable%20parameter%20count%0Asimilar%20to%20vector-based%20methods.%20Theoretical%20analysis%20and%20ablation%20studies%0Afurther%20validate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03234v1&entry.124074799=Read"},
{"title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning\n  and non-reasoning tasks", "author": "Ping Yu and Jack Lanchantin and Tianlu Wang and Weizhe Yuan and Olga Golovneva and Ilia Kulikov and Sainbayar Sukhbaatar and Jason Weston and Jing Xu", "abstract": "  We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on\ngiven seed tasks, and then generate a new synthetic example of similar quality\nand complexity. This is followed by a filtering step to select high-quality\ndata using automatic metrics, which are then used for LLM training. In\nverifiable reasoning, our synthetic data significantly outperforms existing\ntraining datasets, such as s1k and OpenMathReasoning, when evaluated on\nMATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable\ninstruction-following tasks, our method surpasses the performance of both human\nand standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard\nbenchmarks.\n", "link": "http://arxiv.org/abs/2507.23751v2", "date": "2025-09-03", "relevancy": 1.8936, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4847}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4758}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-Self-Instruct%3A%20Building%20high-quality%20synthetic%20prompts%20for%20reasoning%0A%20%20and%20non-reasoning%20tasks&body=Title%3A%20CoT-Self-Instruct%3A%20Building%20high-quality%20synthetic%20prompts%20for%20reasoning%0A%20%20and%20non-reasoning%20tasks%0AAuthor%3A%20Ping%20Yu%20and%20Jack%20Lanchantin%20and%20Tianlu%20Wang%20and%20Weizhe%20Yuan%20and%20Olga%20Golovneva%20and%20Ilia%20Kulikov%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%20and%20Jing%20Xu%0AAbstract%3A%20%20%20We%20propose%20CoT-Self-Instruct%2C%20a%20synthetic%20data%20generation%20method%20that%0Ainstructs%20LLMs%20to%20first%20reason%20and%20plan%20via%20Chain-of-Thought%20%28CoT%29%20based%20on%0Agiven%20seed%20tasks%2C%20and%20then%20generate%20a%20new%20synthetic%20example%20of%20similar%20quality%0Aand%20complexity.%20This%20is%20followed%20by%20a%20filtering%20step%20to%20select%20high-quality%0Adata%20using%20automatic%20metrics%2C%20which%20are%20then%20used%20for%20LLM%20training.%20In%0Averifiable%20reasoning%2C%20our%20synthetic%20data%20significantly%20outperforms%20existing%0Atraining%20datasets%2C%20such%20as%20s1k%20and%20OpenMathReasoning%2C%20when%20evaluated%20on%0AMATH500%2C%20AMC23%2C%20AIME24%2C%20and%20GPQA-Diamond.%20For%20non-verifiable%0Ainstruction-following%20tasks%2C%20our%20method%20surpasses%20the%20performance%20of%20both%20human%0Aand%20standard%20Self-Instruct%20training%20data%20on%20the%20AlpacaEval%202.0%20and%20Arena-Hard%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-Self-Instruct%253A%2520Building%2520high-quality%2520synthetic%2520prompts%2520for%2520reasoning%250A%2520%2520and%2520non-reasoning%2520tasks%26entry.906535625%3DPing%2520Yu%2520and%2520Jack%2520Lanchantin%2520and%2520Tianlu%2520Wang%2520and%2520Weizhe%2520Yuan%2520and%2520Olga%2520Golovneva%2520and%2520Ilia%2520Kulikov%2520and%2520Sainbayar%2520Sukhbaatar%2520and%2520Jason%2520Weston%2520and%2520Jing%2520Xu%26entry.1292438233%3D%2520%2520We%2520propose%2520CoT-Self-Instruct%252C%2520a%2520synthetic%2520data%2520generation%2520method%2520that%250Ainstructs%2520LLMs%2520to%2520first%2520reason%2520and%2520plan%2520via%2520Chain-of-Thought%2520%2528CoT%2529%2520based%2520on%250Agiven%2520seed%2520tasks%252C%2520and%2520then%2520generate%2520a%2520new%2520synthetic%2520example%2520of%2520similar%2520quality%250Aand%2520complexity.%2520This%2520is%2520followed%2520by%2520a%2520filtering%2520step%2520to%2520select%2520high-quality%250Adata%2520using%2520automatic%2520metrics%252C%2520which%2520are%2520then%2520used%2520for%2520LLM%2520training.%2520In%250Averifiable%2520reasoning%252C%2520our%2520synthetic%2520data%2520significantly%2520outperforms%2520existing%250Atraining%2520datasets%252C%2520such%2520as%2520s1k%2520and%2520OpenMathReasoning%252C%2520when%2520evaluated%2520on%250AMATH500%252C%2520AMC23%252C%2520AIME24%252C%2520and%2520GPQA-Diamond.%2520For%2520non-verifiable%250Ainstruction-following%2520tasks%252C%2520our%2520method%2520surpasses%2520the%2520performance%2520of%2520both%2520human%250Aand%2520standard%2520Self-Instruct%2520training%2520data%2520on%2520the%2520AlpacaEval%25202.0%2520and%2520Arena-Hard%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-Self-Instruct%3A%20Building%20high-quality%20synthetic%20prompts%20for%20reasoning%0A%20%20and%20non-reasoning%20tasks&entry.906535625=Ping%20Yu%20and%20Jack%20Lanchantin%20and%20Tianlu%20Wang%20and%20Weizhe%20Yuan%20and%20Olga%20Golovneva%20and%20Ilia%20Kulikov%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%20and%20Jing%20Xu&entry.1292438233=%20%20We%20propose%20CoT-Self-Instruct%2C%20a%20synthetic%20data%20generation%20method%20that%0Ainstructs%20LLMs%20to%20first%20reason%20and%20plan%20via%20Chain-of-Thought%20%28CoT%29%20based%20on%0Agiven%20seed%20tasks%2C%20and%20then%20generate%20a%20new%20synthetic%20example%20of%20similar%20quality%0Aand%20complexity.%20This%20is%20followed%20by%20a%20filtering%20step%20to%20select%20high-quality%0Adata%20using%20automatic%20metrics%2C%20which%20are%20then%20used%20for%20LLM%20training.%20In%0Averifiable%20reasoning%2C%20our%20synthetic%20data%20significantly%20outperforms%20existing%0Atraining%20datasets%2C%20such%20as%20s1k%20and%20OpenMathReasoning%2C%20when%20evaluated%20on%0AMATH500%2C%20AMC23%2C%20AIME24%2C%20and%20GPQA-Diamond.%20For%20non-verifiable%0Ainstruction-following%20tasks%2C%20our%20method%20surpasses%20the%20performance%20of%20both%20human%0Aand%20standard%20Self-Instruct%20training%20data%20on%20the%20AlpacaEval%202.0%20and%20Arena-Hard%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23751v2&entry.124074799=Read"},
{"title": "Geometric Foundations of Tuning without Forgetting in Neural ODEs", "author": "Erkan Bayram and Mohamed-Ali Belabbas and Tamer Ba\u015far", "abstract": "  In our earlier work, we introduced the principle of Tuning without Forgetting\n(TwF) for sequential training of neural ODEs, where training samples are added\niteratively and parameters are updated within the subspace of control functions\nthat preserves the end-point mapping at previously learned samples on the\nmanifold of output labels in the first-order approximation sense. In this\nletter, we prove that this parameter subspace forms a Banach submanifold of\nfinite codimension under nonsingular controls, and we characterize its tangent\nspace. This reveals that TwF corresponds to a continuation/deformation of the\ncontrol function along the tangent space of this Banach submanifold, providing\na theoretical foundation for its mapping-preserving (not forgetting) during the\nsequential training exactly, beyond first-order approximation.\n", "link": "http://arxiv.org/abs/2509.03474v1", "date": "2025-09-03", "relevancy": 1.8905, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5078}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4501}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Foundations%20of%20Tuning%20without%20Forgetting%20in%20Neural%20ODEs&body=Title%3A%20Geometric%20Foundations%20of%20Tuning%20without%20Forgetting%20in%20Neural%20ODEs%0AAuthor%3A%20Erkan%20Bayram%20and%20Mohamed-Ali%20Belabbas%20and%20Tamer%20Ba%C5%9Far%0AAbstract%3A%20%20%20In%20our%20earlier%20work%2C%20we%20introduced%20the%20principle%20of%20Tuning%20without%20Forgetting%0A%28TwF%29%20for%20sequential%20training%20of%20neural%20ODEs%2C%20where%20training%20samples%20are%20added%0Aiteratively%20and%20parameters%20are%20updated%20within%20the%20subspace%20of%20control%20functions%0Athat%20preserves%20the%20end-point%20mapping%20at%20previously%20learned%20samples%20on%20the%0Amanifold%20of%20output%20labels%20in%20the%20first-order%20approximation%20sense.%20In%20this%0Aletter%2C%20we%20prove%20that%20this%20parameter%20subspace%20forms%20a%20Banach%20submanifold%20of%0Afinite%20codimension%20under%20nonsingular%20controls%2C%20and%20we%20characterize%20its%20tangent%0Aspace.%20This%20reveals%20that%20TwF%20corresponds%20to%20a%20continuation/deformation%20of%20the%0Acontrol%20function%20along%20the%20tangent%20space%20of%20this%20Banach%20submanifold%2C%20providing%0Aa%20theoretical%20foundation%20for%20its%20mapping-preserving%20%28not%20forgetting%29%20during%20the%0Asequential%20training%20exactly%2C%20beyond%20first-order%20approximation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Foundations%2520of%2520Tuning%2520without%2520Forgetting%2520in%2520Neural%2520ODEs%26entry.906535625%3DErkan%2520Bayram%2520and%2520Mohamed-Ali%2520Belabbas%2520and%2520Tamer%2520Ba%25C5%259Far%26entry.1292438233%3D%2520%2520In%2520our%2520earlier%2520work%252C%2520we%2520introduced%2520the%2520principle%2520of%2520Tuning%2520without%2520Forgetting%250A%2528TwF%2529%2520for%2520sequential%2520training%2520of%2520neural%2520ODEs%252C%2520where%2520training%2520samples%2520are%2520added%250Aiteratively%2520and%2520parameters%2520are%2520updated%2520within%2520the%2520subspace%2520of%2520control%2520functions%250Athat%2520preserves%2520the%2520end-point%2520mapping%2520at%2520previously%2520learned%2520samples%2520on%2520the%250Amanifold%2520of%2520output%2520labels%2520in%2520the%2520first-order%2520approximation%2520sense.%2520In%2520this%250Aletter%252C%2520we%2520prove%2520that%2520this%2520parameter%2520subspace%2520forms%2520a%2520Banach%2520submanifold%2520of%250Afinite%2520codimension%2520under%2520nonsingular%2520controls%252C%2520and%2520we%2520characterize%2520its%2520tangent%250Aspace.%2520This%2520reveals%2520that%2520TwF%2520corresponds%2520to%2520a%2520continuation/deformation%2520of%2520the%250Acontrol%2520function%2520along%2520the%2520tangent%2520space%2520of%2520this%2520Banach%2520submanifold%252C%2520providing%250Aa%2520theoretical%2520foundation%2520for%2520its%2520mapping-preserving%2520%2528not%2520forgetting%2529%2520during%2520the%250Asequential%2520training%2520exactly%252C%2520beyond%2520first-order%2520approximation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Foundations%20of%20Tuning%20without%20Forgetting%20in%20Neural%20ODEs&entry.906535625=Erkan%20Bayram%20and%20Mohamed-Ali%20Belabbas%20and%20Tamer%20Ba%C5%9Far&entry.1292438233=%20%20In%20our%20earlier%20work%2C%20we%20introduced%20the%20principle%20of%20Tuning%20without%20Forgetting%0A%28TwF%29%20for%20sequential%20training%20of%20neural%20ODEs%2C%20where%20training%20samples%20are%20added%0Aiteratively%20and%20parameters%20are%20updated%20within%20the%20subspace%20of%20control%20functions%0Athat%20preserves%20the%20end-point%20mapping%20at%20previously%20learned%20samples%20on%20the%0Amanifold%20of%20output%20labels%20in%20the%20first-order%20approximation%20sense.%20In%20this%0Aletter%2C%20we%20prove%20that%20this%20parameter%20subspace%20forms%20a%20Banach%20submanifold%20of%0Afinite%20codimension%20under%20nonsingular%20controls%2C%20and%20we%20characterize%20its%20tangent%0Aspace.%20This%20reveals%20that%20TwF%20corresponds%20to%20a%20continuation/deformation%20of%20the%0Acontrol%20function%20along%20the%20tangent%20space%20of%20this%20Banach%20submanifold%2C%20providing%0Aa%20theoretical%20foundation%20for%20its%20mapping-preserving%20%28not%20forgetting%29%20during%20the%0Asequential%20training%20exactly%2C%20beyond%20first-order%20approximation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03474v1&entry.124074799=Read"},
{"title": "A comprehensive Persian offline handwritten database for investigating\n  the effects of heritability and family relationships on handwriting", "author": "Abbas Zohrevand and Javad Sadri and Zahra Imani", "abstract": "  This paper introduces a comprehensive database for research and investigation\non the effects of inheritance on handwriting. A database has been created that\ncan be used to answer questions such as: Is there a genetic component to\nhandwriting? Is handwriting inherited? Do family relationships affect\nhandwriting? Varieties of samples of handwritten components such as: digits,\nletters, shapes and free paragraphs of 210 families including (grandparents,\nparents, uncles, aunts, siblings, cousins, nephews and nieces) have been\ncollected using specially designed forms, and family relationships of all\nwriters are captured. To the best of our knowledge, no such database is\npresently available. Based on comparisons and investigation of features of\nhandwritings of family members, similarities among their features and writing\nstyles are detected. Our database is freely available to the pattern\nrecognition community and hope it will pave the way for investigations on the\neffects of inheritance and family relationships on handwritings.\n", "link": "http://arxiv.org/abs/2509.03510v1", "date": "2025-09-03", "relevancy": 1.8875, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3873}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3775}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comprehensive%20Persian%20offline%20handwritten%20database%20for%20investigating%0A%20%20the%20effects%20of%20heritability%20and%20family%20relationships%20on%20handwriting&body=Title%3A%20A%20comprehensive%20Persian%20offline%20handwritten%20database%20for%20investigating%0A%20%20the%20effects%20of%20heritability%20and%20family%20relationships%20on%20handwriting%0AAuthor%3A%20Abbas%20Zohrevand%20and%20Javad%20Sadri%20and%20Zahra%20Imani%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20comprehensive%20database%20for%20research%20and%20investigation%0Aon%20the%20effects%20of%20inheritance%20on%20handwriting.%20A%20database%20has%20been%20created%20that%0Acan%20be%20used%20to%20answer%20questions%20such%20as%3A%20Is%20there%20a%20genetic%20component%20to%0Ahandwriting%3F%20Is%20handwriting%20inherited%3F%20Do%20family%20relationships%20affect%0Ahandwriting%3F%20Varieties%20of%20samples%20of%20handwritten%20components%20such%20as%3A%20digits%2C%0Aletters%2C%20shapes%20and%20free%20paragraphs%20of%20210%20families%20including%20%28grandparents%2C%0Aparents%2C%20uncles%2C%20aunts%2C%20siblings%2C%20cousins%2C%20nephews%20and%20nieces%29%20have%20been%0Acollected%20using%20specially%20designed%20forms%2C%20and%20family%20relationships%20of%20all%0Awriters%20are%20captured.%20To%20the%20best%20of%20our%20knowledge%2C%20no%20such%20database%20is%0Apresently%20available.%20Based%20on%20comparisons%20and%20investigation%20of%20features%20of%0Ahandwritings%20of%20family%20members%2C%20similarities%20among%20their%20features%20and%20writing%0Astyles%20are%20detected.%20Our%20database%20is%20freely%20available%20to%20the%20pattern%0Arecognition%20community%20and%20hope%20it%20will%20pave%20the%20way%20for%20investigations%20on%20the%0Aeffects%20of%20inheritance%20and%20family%20relationships%20on%20handwritings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comprehensive%2520Persian%2520offline%2520handwritten%2520database%2520for%2520investigating%250A%2520%2520the%2520effects%2520of%2520heritability%2520and%2520family%2520relationships%2520on%2520handwriting%26entry.906535625%3DAbbas%2520Zohrevand%2520and%2520Javad%2520Sadri%2520and%2520Zahra%2520Imani%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520comprehensive%2520database%2520for%2520research%2520and%2520investigation%250Aon%2520the%2520effects%2520of%2520inheritance%2520on%2520handwriting.%2520A%2520database%2520has%2520been%2520created%2520that%250Acan%2520be%2520used%2520to%2520answer%2520questions%2520such%2520as%253A%2520Is%2520there%2520a%2520genetic%2520component%2520to%250Ahandwriting%253F%2520Is%2520handwriting%2520inherited%253F%2520Do%2520family%2520relationships%2520affect%250Ahandwriting%253F%2520Varieties%2520of%2520samples%2520of%2520handwritten%2520components%2520such%2520as%253A%2520digits%252C%250Aletters%252C%2520shapes%2520and%2520free%2520paragraphs%2520of%2520210%2520families%2520including%2520%2528grandparents%252C%250Aparents%252C%2520uncles%252C%2520aunts%252C%2520siblings%252C%2520cousins%252C%2520nephews%2520and%2520nieces%2529%2520have%2520been%250Acollected%2520using%2520specially%2520designed%2520forms%252C%2520and%2520family%2520relationships%2520of%2520all%250Awriters%2520are%2520captured.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520no%2520such%2520database%2520is%250Apresently%2520available.%2520Based%2520on%2520comparisons%2520and%2520investigation%2520of%2520features%2520of%250Ahandwritings%2520of%2520family%2520members%252C%2520similarities%2520among%2520their%2520features%2520and%2520writing%250Astyles%2520are%2520detected.%2520Our%2520database%2520is%2520freely%2520available%2520to%2520the%2520pattern%250Arecognition%2520community%2520and%2520hope%2520it%2520will%2520pave%2520the%2520way%2520for%2520investigations%2520on%2520the%250Aeffects%2520of%2520inheritance%2520and%2520family%2520relationships%2520on%2520handwritings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comprehensive%20Persian%20offline%20handwritten%20database%20for%20investigating%0A%20%20the%20effects%20of%20heritability%20and%20family%20relationships%20on%20handwriting&entry.906535625=Abbas%20Zohrevand%20and%20Javad%20Sadri%20and%20Zahra%20Imani&entry.1292438233=%20%20This%20paper%20introduces%20a%20comprehensive%20database%20for%20research%20and%20investigation%0Aon%20the%20effects%20of%20inheritance%20on%20handwriting.%20A%20database%20has%20been%20created%20that%0Acan%20be%20used%20to%20answer%20questions%20such%20as%3A%20Is%20there%20a%20genetic%20component%20to%0Ahandwriting%3F%20Is%20handwriting%20inherited%3F%20Do%20family%20relationships%20affect%0Ahandwriting%3F%20Varieties%20of%20samples%20of%20handwritten%20components%20such%20as%3A%20digits%2C%0Aletters%2C%20shapes%20and%20free%20paragraphs%20of%20210%20families%20including%20%28grandparents%2C%0Aparents%2C%20uncles%2C%20aunts%2C%20siblings%2C%20cousins%2C%20nephews%20and%20nieces%29%20have%20been%0Acollected%20using%20specially%20designed%20forms%2C%20and%20family%20relationships%20of%20all%0Awriters%20are%20captured.%20To%20the%20best%20of%20our%20knowledge%2C%20no%20such%20database%20is%0Apresently%20available.%20Based%20on%20comparisons%20and%20investigation%20of%20features%20of%0Ahandwritings%20of%20family%20members%2C%20similarities%20among%20their%20features%20and%20writing%0Astyles%20are%20detected.%20Our%20database%20is%20freely%20available%20to%20the%20pattern%0Arecognition%20community%20and%20hope%20it%20will%20pave%20the%20way%20for%20investigations%20on%20the%0Aeffects%20of%20inheritance%20and%20family%20relationships%20on%20handwritings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03510v1&entry.124074799=Read"},
{"title": "Empowering Lightweight MLLMs with Reasoning via Long CoT SFT", "author": "Linyu Ou", "abstract": "  While Reinforcement Learning with Verifiable Rewards has enhanced the\nreasoning of large-scale language models (LLMs), its efficacy for lightweight\nmultimodal language models (MLLMs) with fewer than seven billion parameters\nremains underexplored. This paper investigates the role of long\nChain-of-Thought (long CoT) data in enhancing the reasoning abilities of such\nMLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT\ndata significantly improves MLLM reasoning. Furthermore, we observe that after\nthis initial SFT phase, MLLMs can achieve additional performance gains through\na subsequent RL stage. We conclude that a SFT stage with long CoT data is a\ncritical prerequisite for developing the reasoning capabilities of lightweight\nMLLMs.\n", "link": "http://arxiv.org/abs/2509.03321v1", "date": "2025-09-03", "relevancy": 1.8874, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Lightweight%20MLLMs%20with%20Reasoning%20via%20Long%20CoT%20SFT&body=Title%3A%20Empowering%20Lightweight%20MLLMs%20with%20Reasoning%20via%20Long%20CoT%20SFT%0AAuthor%3A%20Linyu%20Ou%0AAbstract%3A%20%20%20While%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20has%20enhanced%20the%0Areasoning%20of%20large-scale%20language%20models%20%28LLMs%29%2C%20its%20efficacy%20for%20lightweight%0Amultimodal%20language%20models%20%28MLLMs%29%20with%20fewer%20than%20seven%20billion%20parameters%0Aremains%20underexplored.%20This%20paper%20investigates%20the%20role%20of%20long%0AChain-of-Thought%20%28long%20CoT%29%20data%20in%20enhancing%20the%20reasoning%20abilities%20of%20such%0AMLLMs.%20Our%20findings%20demonstrate%20that%20Supervised%20Fine-Tuning%20%28SFT%29%20with%20long%20CoT%0Adata%20significantly%20improves%20MLLM%20reasoning.%20Furthermore%2C%20we%20observe%20that%20after%0Athis%20initial%20SFT%20phase%2C%20MLLMs%20can%20achieve%20additional%20performance%20gains%20through%0Aa%20subsequent%20RL%20stage.%20We%20conclude%20that%20a%20SFT%20stage%20with%20long%20CoT%20data%20is%20a%0Acritical%20prerequisite%20for%20developing%20the%20reasoning%20capabilities%20of%20lightweight%0AMLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Lightweight%2520MLLMs%2520with%2520Reasoning%2520via%2520Long%2520CoT%2520SFT%26entry.906535625%3DLinyu%2520Ou%26entry.1292438233%3D%2520%2520While%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520has%2520enhanced%2520the%250Areasoning%2520of%2520large-scale%2520language%2520models%2520%2528LLMs%2529%252C%2520its%2520efficacy%2520for%2520lightweight%250Amultimodal%2520language%2520models%2520%2528MLLMs%2529%2520with%2520fewer%2520than%2520seven%2520billion%2520parameters%250Aremains%2520underexplored.%2520This%2520paper%2520investigates%2520the%2520role%2520of%2520long%250AChain-of-Thought%2520%2528long%2520CoT%2529%2520data%2520in%2520enhancing%2520the%2520reasoning%2520abilities%2520of%2520such%250AMLLMs.%2520Our%2520findings%2520demonstrate%2520that%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520with%2520long%2520CoT%250Adata%2520significantly%2520improves%2520MLLM%2520reasoning.%2520Furthermore%252C%2520we%2520observe%2520that%2520after%250Athis%2520initial%2520SFT%2520phase%252C%2520MLLMs%2520can%2520achieve%2520additional%2520performance%2520gains%2520through%250Aa%2520subsequent%2520RL%2520stage.%2520We%2520conclude%2520that%2520a%2520SFT%2520stage%2520with%2520long%2520CoT%2520data%2520is%2520a%250Acritical%2520prerequisite%2520for%2520developing%2520the%2520reasoning%2520capabilities%2520of%2520lightweight%250AMLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Lightweight%20MLLMs%20with%20Reasoning%20via%20Long%20CoT%20SFT&entry.906535625=Linyu%20Ou&entry.1292438233=%20%20While%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20has%20enhanced%20the%0Areasoning%20of%20large-scale%20language%20models%20%28LLMs%29%2C%20its%20efficacy%20for%20lightweight%0Amultimodal%20language%20models%20%28MLLMs%29%20with%20fewer%20than%20seven%20billion%20parameters%0Aremains%20underexplored.%20This%20paper%20investigates%20the%20role%20of%20long%0AChain-of-Thought%20%28long%20CoT%29%20data%20in%20enhancing%20the%20reasoning%20abilities%20of%20such%0AMLLMs.%20Our%20findings%20demonstrate%20that%20Supervised%20Fine-Tuning%20%28SFT%29%20with%20long%20CoT%0Adata%20significantly%20improves%20MLLM%20reasoning.%20Furthermore%2C%20we%20observe%20that%20after%0Athis%20initial%20SFT%20phase%2C%20MLLMs%20can%20achieve%20additional%20performance%20gains%20through%0Aa%20subsequent%20RL%20stage.%20We%20conclude%20that%20a%20SFT%20stage%20with%20long%20CoT%20data%20is%20a%0Acritical%20prerequisite%20for%20developing%20the%20reasoning%20capabilities%20of%20lightweight%0AMLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03321v1&entry.124074799=Read"},
{"title": "SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation\n  Models", "author": "Jigang Fan and Zhenghong Zhou and Ruofan Jin and Le Cong and Mengdi Wang and Zaixi Zhang", "abstract": "  Proteins play crucial roles in almost all biological processes. The\nadvancement of deep learning has greatly accelerated the development of protein\nfoundation models, leading to significant successes in protein understanding\nand design. However, the lack of systematic red-teaming for these models has\nraised serious concerns about their potential misuse, such as generating\nproteins with biological safety risks. This paper introduces SafeProtein, the\nfirst red-teaming framework designed for protein foundation models to the best\nof our knowledge. SafeProtein combines multimodal prompt engineering and\nheuristic beam search to systematically design red-teaming methods and conduct\ntests on protein foundation models. We also curated SafeProtein-Bench, which\nincludes a manually constructed red-teaming benchmark dataset and a\ncomprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks\non state-of-the-art protein foundation models (up to 70% attack success rate\nfor ESM3), revealing potential biological safety risks in current protein\nfoundation models and providing insights for the development of robust security\nprotection technologies for frontier models. The codes will be made publicly\navailable at https://github.com/jigang-fan/SafeProtein.\n", "link": "http://arxiv.org/abs/2509.03487v1", "date": "2025-09-03", "relevancy": 1.8828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeProtein%3A%20Red-Teaming%20Framework%20and%20Benchmark%20for%20Protein%20Foundation%0A%20%20Models&body=Title%3A%20SafeProtein%3A%20Red-Teaming%20Framework%20and%20Benchmark%20for%20Protein%20Foundation%0A%20%20Models%0AAuthor%3A%20Jigang%20Fan%20and%20Zhenghong%20Zhou%20and%20Ruofan%20Jin%20and%20Le%20Cong%20and%20Mengdi%20Wang%20and%20Zaixi%20Zhang%0AAbstract%3A%20%20%20Proteins%20play%20crucial%20roles%20in%20almost%20all%20biological%20processes.%20The%0Aadvancement%20of%20deep%20learning%20has%20greatly%20accelerated%20the%20development%20of%20protein%0Afoundation%20models%2C%20leading%20to%20significant%20successes%20in%20protein%20understanding%0Aand%20design.%20However%2C%20the%20lack%20of%20systematic%20red-teaming%20for%20these%20models%20has%0Araised%20serious%20concerns%20about%20their%20potential%20misuse%2C%20such%20as%20generating%0Aproteins%20with%20biological%20safety%20risks.%20This%20paper%20introduces%20SafeProtein%2C%20the%0Afirst%20red-teaming%20framework%20designed%20for%20protein%20foundation%20models%20to%20the%20best%0Aof%20our%20knowledge.%20SafeProtein%20combines%20multimodal%20prompt%20engineering%20and%0Aheuristic%20beam%20search%20to%20systematically%20design%20red-teaming%20methods%20and%20conduct%0Atests%20on%20protein%20foundation%20models.%20We%20also%20curated%20SafeProtein-Bench%2C%20which%0Aincludes%20a%20manually%20constructed%20red-teaming%20benchmark%20dataset%20and%20a%0Acomprehensive%20evaluation%20protocol.%20SafeProtein%20achieved%20continuous%20jailbreaks%0Aon%20state-of-the-art%20protein%20foundation%20models%20%28up%20to%2070%25%20attack%20success%20rate%0Afor%20ESM3%29%2C%20revealing%20potential%20biological%20safety%20risks%20in%20current%20protein%0Afoundation%20models%20and%20providing%20insights%20for%20the%20development%20of%20robust%20security%0Aprotection%20technologies%20for%20frontier%20models.%20The%20codes%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/jigang-fan/SafeProtein.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeProtein%253A%2520Red-Teaming%2520Framework%2520and%2520Benchmark%2520for%2520Protein%2520Foundation%250A%2520%2520Models%26entry.906535625%3DJigang%2520Fan%2520and%2520Zhenghong%2520Zhou%2520and%2520Ruofan%2520Jin%2520and%2520Le%2520Cong%2520and%2520Mengdi%2520Wang%2520and%2520Zaixi%2520Zhang%26entry.1292438233%3D%2520%2520Proteins%2520play%2520crucial%2520roles%2520in%2520almost%2520all%2520biological%2520processes.%2520The%250Aadvancement%2520of%2520deep%2520learning%2520has%2520greatly%2520accelerated%2520the%2520development%2520of%2520protein%250Afoundation%2520models%252C%2520leading%2520to%2520significant%2520successes%2520in%2520protein%2520understanding%250Aand%2520design.%2520However%252C%2520the%2520lack%2520of%2520systematic%2520red-teaming%2520for%2520these%2520models%2520has%250Araised%2520serious%2520concerns%2520about%2520their%2520potential%2520misuse%252C%2520such%2520as%2520generating%250Aproteins%2520with%2520biological%2520safety%2520risks.%2520This%2520paper%2520introduces%2520SafeProtein%252C%2520the%250Afirst%2520red-teaming%2520framework%2520designed%2520for%2520protein%2520foundation%2520models%2520to%2520the%2520best%250Aof%2520our%2520knowledge.%2520SafeProtein%2520combines%2520multimodal%2520prompt%2520engineering%2520and%250Aheuristic%2520beam%2520search%2520to%2520systematically%2520design%2520red-teaming%2520methods%2520and%2520conduct%250Atests%2520on%2520protein%2520foundation%2520models.%2520We%2520also%2520curated%2520SafeProtein-Bench%252C%2520which%250Aincludes%2520a%2520manually%2520constructed%2520red-teaming%2520benchmark%2520dataset%2520and%2520a%250Acomprehensive%2520evaluation%2520protocol.%2520SafeProtein%2520achieved%2520continuous%2520jailbreaks%250Aon%2520state-of-the-art%2520protein%2520foundation%2520models%2520%2528up%2520to%252070%2525%2520attack%2520success%2520rate%250Afor%2520ESM3%2529%252C%2520revealing%2520potential%2520biological%2520safety%2520risks%2520in%2520current%2520protein%250Afoundation%2520models%2520and%2520providing%2520insights%2520for%2520the%2520development%2520of%2520robust%2520security%250Aprotection%2520technologies%2520for%2520frontier%2520models.%2520The%2520codes%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/jigang-fan/SafeProtein.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeProtein%3A%20Red-Teaming%20Framework%20and%20Benchmark%20for%20Protein%20Foundation%0A%20%20Models&entry.906535625=Jigang%20Fan%20and%20Zhenghong%20Zhou%20and%20Ruofan%20Jin%20and%20Le%20Cong%20and%20Mengdi%20Wang%20and%20Zaixi%20Zhang&entry.1292438233=%20%20Proteins%20play%20crucial%20roles%20in%20almost%20all%20biological%20processes.%20The%0Aadvancement%20of%20deep%20learning%20has%20greatly%20accelerated%20the%20development%20of%20protein%0Afoundation%20models%2C%20leading%20to%20significant%20successes%20in%20protein%20understanding%0Aand%20design.%20However%2C%20the%20lack%20of%20systematic%20red-teaming%20for%20these%20models%20has%0Araised%20serious%20concerns%20about%20their%20potential%20misuse%2C%20such%20as%20generating%0Aproteins%20with%20biological%20safety%20risks.%20This%20paper%20introduces%20SafeProtein%2C%20the%0Afirst%20red-teaming%20framework%20designed%20for%20protein%20foundation%20models%20to%20the%20best%0Aof%20our%20knowledge.%20SafeProtein%20combines%20multimodal%20prompt%20engineering%20and%0Aheuristic%20beam%20search%20to%20systematically%20design%20red-teaming%20methods%20and%20conduct%0Atests%20on%20protein%20foundation%20models.%20We%20also%20curated%20SafeProtein-Bench%2C%20which%0Aincludes%20a%20manually%20constructed%20red-teaming%20benchmark%20dataset%20and%20a%0Acomprehensive%20evaluation%20protocol.%20SafeProtein%20achieved%20continuous%20jailbreaks%0Aon%20state-of-the-art%20protein%20foundation%20models%20%28up%20to%2070%25%20attack%20success%20rate%0Afor%20ESM3%29%2C%20revealing%20potential%20biological%20safety%20risks%20in%20current%20protein%0Afoundation%20models%20and%20providing%20insights%20for%20the%20development%20of%20robust%20security%0Aprotection%20technologies%20for%20frontier%20models.%20The%20codes%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/jigang-fan/SafeProtein.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03487v1&entry.124074799=Read"},
{"title": "FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual\n  LLMs", "author": "Royson Lee and Minyoung Kim and Fady Rezk and Rui Li and Stylianos I. Venieris and Timothy Hospedales", "abstract": "  Federated learning (FL) has enabled the training of multilingual large\nlanguage models (LLMs) on diverse and decentralized multilingual data,\nespecially on low-resource languages. To improve client-specific performance,\npersonalization via the use of parameter-efficient fine-tuning (PEFT) modules\nsuch as LoRA is common. This involves a personalization strategy (PS), such as\nthe design of the PEFT adapter structures (e.g., in which layers to add LoRAs\nand what ranks) and choice of hyperparameters (e.g., learning rates) for\nfine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a\nfederated learning-to-personalize method for multilingual LLMs in cross-device\nFL settings. Unlike most existing PEFT structure selection methods, which are\nprone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the\noptimal personalized PEFT structure for each client via Bayesian sparse rank\nselection. Evaluations on both simulated and real-world multilingual FL\nbenchmarks demonstrate that FedP$^2$EFT largely outperforms existing\npersonalized fine-tuning methods, while complementing other existing FL\nmethods. Code is available at https://github.com/SamsungLabs/fedp2eft.\n", "link": "http://arxiv.org/abs/2502.04387v2", "date": "2025-09-03", "relevancy": 1.8747, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4786}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedP%24%5E2%24EFT%3A%20Federated%20Learning%20to%20Personalize%20PEFT%20for%20Multilingual%0A%20%20LLMs&body=Title%3A%20FedP%24%5E2%24EFT%3A%20Federated%20Learning%20to%20Personalize%20PEFT%20for%20Multilingual%0A%20%20LLMs%0AAuthor%3A%20Royson%20Lee%20and%20Minyoung%20Kim%20and%20Fady%20Rezk%20and%20Rui%20Li%20and%20Stylianos%20I.%20Venieris%20and%20Timothy%20Hospedales%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20enabled%20the%20training%20of%20multilingual%20large%0Alanguage%20models%20%28LLMs%29%20on%20diverse%20and%20decentralized%20multilingual%20data%2C%0Aespecially%20on%20low-resource%20languages.%20To%20improve%20client-specific%20performance%2C%0Apersonalization%20via%20the%20use%20of%20parameter-efficient%20fine-tuning%20%28PEFT%29%20modules%0Asuch%20as%20LoRA%20is%20common.%20This%20involves%20a%20personalization%20strategy%20%28PS%29%2C%20such%20as%0Athe%20design%20of%20the%20PEFT%20adapter%20structures%20%28e.g.%2C%20in%20which%20layers%20to%20add%20LoRAs%0Aand%20what%20ranks%29%20and%20choice%20of%20hyperparameters%20%28e.g.%2C%20learning%20rates%29%20for%0Afine-tuning.%20Instead%20of%20manual%20PS%20configuration%2C%20we%20propose%20FedP%24%5E2%24EFT%2C%20a%0Afederated%20learning-to-personalize%20method%20for%20multilingual%20LLMs%20in%20cross-device%0AFL%20settings.%20Unlike%20most%20existing%20PEFT%20structure%20selection%20methods%2C%20which%20are%0Aprone%20to%20overfitting%20low-data%20regimes%2C%20FedP%24%5E2%24EFT%20collaboratively%20learns%20the%0Aoptimal%20personalized%20PEFT%20structure%20for%20each%20client%20via%20Bayesian%20sparse%20rank%0Aselection.%20Evaluations%20on%20both%20simulated%20and%20real-world%20multilingual%20FL%0Abenchmarks%20demonstrate%20that%20FedP%24%5E2%24EFT%20largely%20outperforms%20existing%0Apersonalized%20fine-tuning%20methods%2C%20while%20complementing%20other%20existing%20FL%0Amethods.%20Code%20is%20available%20at%20https%3A//github.com/SamsungLabs/fedp2eft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedP%2524%255E2%2524EFT%253A%2520Federated%2520Learning%2520to%2520Personalize%2520PEFT%2520for%2520Multilingual%250A%2520%2520LLMs%26entry.906535625%3DRoyson%2520Lee%2520and%2520Minyoung%2520Kim%2520and%2520Fady%2520Rezk%2520and%2520Rui%2520Li%2520and%2520Stylianos%2520I.%2520Venieris%2520and%2520Timothy%2520Hospedales%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520enabled%2520the%2520training%2520of%2520multilingual%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520on%2520diverse%2520and%2520decentralized%2520multilingual%2520data%252C%250Aespecially%2520on%2520low-resource%2520languages.%2520To%2520improve%2520client-specific%2520performance%252C%250Apersonalization%2520via%2520the%2520use%2520of%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520modules%250Asuch%2520as%2520LoRA%2520is%2520common.%2520This%2520involves%2520a%2520personalization%2520strategy%2520%2528PS%2529%252C%2520such%2520as%250Athe%2520design%2520of%2520the%2520PEFT%2520adapter%2520structures%2520%2528e.g.%252C%2520in%2520which%2520layers%2520to%2520add%2520LoRAs%250Aand%2520what%2520ranks%2529%2520and%2520choice%2520of%2520hyperparameters%2520%2528e.g.%252C%2520learning%2520rates%2529%2520for%250Afine-tuning.%2520Instead%2520of%2520manual%2520PS%2520configuration%252C%2520we%2520propose%2520FedP%2524%255E2%2524EFT%252C%2520a%250Afederated%2520learning-to-personalize%2520method%2520for%2520multilingual%2520LLMs%2520in%2520cross-device%250AFL%2520settings.%2520Unlike%2520most%2520existing%2520PEFT%2520structure%2520selection%2520methods%252C%2520which%2520are%250Aprone%2520to%2520overfitting%2520low-data%2520regimes%252C%2520FedP%2524%255E2%2524EFT%2520collaboratively%2520learns%2520the%250Aoptimal%2520personalized%2520PEFT%2520structure%2520for%2520each%2520client%2520via%2520Bayesian%2520sparse%2520rank%250Aselection.%2520Evaluations%2520on%2520both%2520simulated%2520and%2520real-world%2520multilingual%2520FL%250Abenchmarks%2520demonstrate%2520that%2520FedP%2524%255E2%2524EFT%2520largely%2520outperforms%2520existing%250Apersonalized%2520fine-tuning%2520methods%252C%2520while%2520complementing%2520other%2520existing%2520FL%250Amethods.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/SamsungLabs/fedp2eft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedP%24%5E2%24EFT%3A%20Federated%20Learning%20to%20Personalize%20PEFT%20for%20Multilingual%0A%20%20LLMs&entry.906535625=Royson%20Lee%20and%20Minyoung%20Kim%20and%20Fady%20Rezk%20and%20Rui%20Li%20and%20Stylianos%20I.%20Venieris%20and%20Timothy%20Hospedales&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20enabled%20the%20training%20of%20multilingual%20large%0Alanguage%20models%20%28LLMs%29%20on%20diverse%20and%20decentralized%20multilingual%20data%2C%0Aespecially%20on%20low-resource%20languages.%20To%20improve%20client-specific%20performance%2C%0Apersonalization%20via%20the%20use%20of%20parameter-efficient%20fine-tuning%20%28PEFT%29%20modules%0Asuch%20as%20LoRA%20is%20common.%20This%20involves%20a%20personalization%20strategy%20%28PS%29%2C%20such%20as%0Athe%20design%20of%20the%20PEFT%20adapter%20structures%20%28e.g.%2C%20in%20which%20layers%20to%20add%20LoRAs%0Aand%20what%20ranks%29%20and%20choice%20of%20hyperparameters%20%28e.g.%2C%20learning%20rates%29%20for%0Afine-tuning.%20Instead%20of%20manual%20PS%20configuration%2C%20we%20propose%20FedP%24%5E2%24EFT%2C%20a%0Afederated%20learning-to-personalize%20method%20for%20multilingual%20LLMs%20in%20cross-device%0AFL%20settings.%20Unlike%20most%20existing%20PEFT%20structure%20selection%20methods%2C%20which%20are%0Aprone%20to%20overfitting%20low-data%20regimes%2C%20FedP%24%5E2%24EFT%20collaboratively%20learns%20the%0Aoptimal%20personalized%20PEFT%20structure%20for%20each%20client%20via%20Bayesian%20sparse%20rank%0Aselection.%20Evaluations%20on%20both%20simulated%20and%20real-world%20multilingual%20FL%0Abenchmarks%20demonstrate%20that%20FedP%24%5E2%24EFT%20largely%20outperforms%20existing%0Apersonalized%20fine-tuning%20methods%2C%20while%20complementing%20other%20existing%20FL%0Amethods.%20Code%20is%20available%20at%20https%3A//github.com/SamsungLabs/fedp2eft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04387v2&entry.124074799=Read"},
{"title": "Warming Up for Zeroth-Order Federated Pre-Training with Low Resource\n  Clients", "author": "Gwen Legate and Irina Rish and Eugene Belilovsky", "abstract": "  Federated learning enables collaborative model training across numerous edge\ndevices without requiring participants to share data; however, memory and\ncommunication constraints on these edge devices may preclude their\nparticipation in training. We consider a setting in which a subset of edge\ndevices are below a critical memory or communication threshold required to\nconduct model updates. Under typical federated optimization algorithms, these\ndevices are excluded from training which renders their data inaccessible and\nincreases system induced bias. We are inspired by MeZO, a zeroth-order method\nused for memory-efficient fine-tuning. The increased variance inherent to\nzeroth-order gradient approximations has relegated previous zeroth-order\noptimizers exclusively to the domain of fine tuning; a limitation we seek to\ncorrect. We devise a federated, memory-efficient zeroth-order optimizer,\nZOWarmUp that permits zeroth-order training from a random initialization.\nZOWarmUp leverages differing client capabilities and careful variance reduction\ntechniques to facilitate participation of under-represented, low-resource\nclients in model training. Like other federated zeroth-order methods, ZOWarmUp\neliminates the need for edge devices to transmit their full gradients to the\nserver and instead relies on only a small set of random seeds, rendering the\nup-link communication cost negligible. We present experiments using various\ndatasets and model architectures to show that ZOWarmUp is a robust algorithm\nthat can can be applied under a wide variety of circumstances. For systems with\na high proportion of edge devices that would otherwise be excluded from\ntraining, this algorithm provides access to a greater volume and diversity of\ndata, thus improving training outcomes.\n", "link": "http://arxiv.org/abs/2509.03503v1", "date": "2025-09-03", "relevancy": 1.8696, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4846}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4645}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Warming%20Up%20for%20Zeroth-Order%20Federated%20Pre-Training%20with%20Low%20Resource%0A%20%20Clients&body=Title%3A%20Warming%20Up%20for%20Zeroth-Order%20Federated%20Pre-Training%20with%20Low%20Resource%0A%20%20Clients%0AAuthor%3A%20Gwen%20Legate%20and%20Irina%20Rish%20and%20Eugene%20Belilovsky%0AAbstract%3A%20%20%20Federated%20learning%20enables%20collaborative%20model%20training%20across%20numerous%20edge%0Adevices%20without%20requiring%20participants%20to%20share%20data%3B%20however%2C%20memory%20and%0Acommunication%20constraints%20on%20these%20edge%20devices%20may%20preclude%20their%0Aparticipation%20in%20training.%20We%20consider%20a%20setting%20in%20which%20a%20subset%20of%20edge%0Adevices%20are%20below%20a%20critical%20memory%20or%20communication%20threshold%20required%20to%0Aconduct%20model%20updates.%20Under%20typical%20federated%20optimization%20algorithms%2C%20these%0Adevices%20are%20excluded%20from%20training%20which%20renders%20their%20data%20inaccessible%20and%0Aincreases%20system%20induced%20bias.%20We%20are%20inspired%20by%20MeZO%2C%20a%20zeroth-order%20method%0Aused%20for%20memory-efficient%20fine-tuning.%20The%20increased%20variance%20inherent%20to%0Azeroth-order%20gradient%20approximations%20has%20relegated%20previous%20zeroth-order%0Aoptimizers%20exclusively%20to%20the%20domain%20of%20fine%20tuning%3B%20a%20limitation%20we%20seek%20to%0Acorrect.%20We%20devise%20a%20federated%2C%20memory-efficient%20zeroth-order%20optimizer%2C%0AZOWarmUp%20that%20permits%20zeroth-order%20training%20from%20a%20random%20initialization.%0AZOWarmUp%20leverages%20differing%20client%20capabilities%20and%20careful%20variance%20reduction%0Atechniques%20to%20facilitate%20participation%20of%20under-represented%2C%20low-resource%0Aclients%20in%20model%20training.%20Like%20other%20federated%20zeroth-order%20methods%2C%20ZOWarmUp%0Aeliminates%20the%20need%20for%20edge%20devices%20to%20transmit%20their%20full%20gradients%20to%20the%0Aserver%20and%20instead%20relies%20on%20only%20a%20small%20set%20of%20random%20seeds%2C%20rendering%20the%0Aup-link%20communication%20cost%20negligible.%20We%20present%20experiments%20using%20various%0Adatasets%20and%20model%20architectures%20to%20show%20that%20ZOWarmUp%20is%20a%20robust%20algorithm%0Athat%20can%20can%20be%20applied%20under%20a%20wide%20variety%20of%20circumstances.%20For%20systems%20with%0Aa%20high%20proportion%20of%20edge%20devices%20that%20would%20otherwise%20be%20excluded%20from%0Atraining%2C%20this%20algorithm%20provides%20access%20to%20a%20greater%20volume%20and%20diversity%20of%0Adata%2C%20thus%20improving%20training%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWarming%2520Up%2520for%2520Zeroth-Order%2520Federated%2520Pre-Training%2520with%2520Low%2520Resource%250A%2520%2520Clients%26entry.906535625%3DGwen%2520Legate%2520and%2520Irina%2520Rish%2520and%2520Eugene%2520Belilovsky%26entry.1292438233%3D%2520%2520Federated%2520learning%2520enables%2520collaborative%2520model%2520training%2520across%2520numerous%2520edge%250Adevices%2520without%2520requiring%2520participants%2520to%2520share%2520data%253B%2520however%252C%2520memory%2520and%250Acommunication%2520constraints%2520on%2520these%2520edge%2520devices%2520may%2520preclude%2520their%250Aparticipation%2520in%2520training.%2520We%2520consider%2520a%2520setting%2520in%2520which%2520a%2520subset%2520of%2520edge%250Adevices%2520are%2520below%2520a%2520critical%2520memory%2520or%2520communication%2520threshold%2520required%2520to%250Aconduct%2520model%2520updates.%2520Under%2520typical%2520federated%2520optimization%2520algorithms%252C%2520these%250Adevices%2520are%2520excluded%2520from%2520training%2520which%2520renders%2520their%2520data%2520inaccessible%2520and%250Aincreases%2520system%2520induced%2520bias.%2520We%2520are%2520inspired%2520by%2520MeZO%252C%2520a%2520zeroth-order%2520method%250Aused%2520for%2520memory-efficient%2520fine-tuning.%2520The%2520increased%2520variance%2520inherent%2520to%250Azeroth-order%2520gradient%2520approximations%2520has%2520relegated%2520previous%2520zeroth-order%250Aoptimizers%2520exclusively%2520to%2520the%2520domain%2520of%2520fine%2520tuning%253B%2520a%2520limitation%2520we%2520seek%2520to%250Acorrect.%2520We%2520devise%2520a%2520federated%252C%2520memory-efficient%2520zeroth-order%2520optimizer%252C%250AZOWarmUp%2520that%2520permits%2520zeroth-order%2520training%2520from%2520a%2520random%2520initialization.%250AZOWarmUp%2520leverages%2520differing%2520client%2520capabilities%2520and%2520careful%2520variance%2520reduction%250Atechniques%2520to%2520facilitate%2520participation%2520of%2520under-represented%252C%2520low-resource%250Aclients%2520in%2520model%2520training.%2520Like%2520other%2520federated%2520zeroth-order%2520methods%252C%2520ZOWarmUp%250Aeliminates%2520the%2520need%2520for%2520edge%2520devices%2520to%2520transmit%2520their%2520full%2520gradients%2520to%2520the%250Aserver%2520and%2520instead%2520relies%2520on%2520only%2520a%2520small%2520set%2520of%2520random%2520seeds%252C%2520rendering%2520the%250Aup-link%2520communication%2520cost%2520negligible.%2520We%2520present%2520experiments%2520using%2520various%250Adatasets%2520and%2520model%2520architectures%2520to%2520show%2520that%2520ZOWarmUp%2520is%2520a%2520robust%2520algorithm%250Athat%2520can%2520can%2520be%2520applied%2520under%2520a%2520wide%2520variety%2520of%2520circumstances.%2520For%2520systems%2520with%250Aa%2520high%2520proportion%2520of%2520edge%2520devices%2520that%2520would%2520otherwise%2520be%2520excluded%2520from%250Atraining%252C%2520this%2520algorithm%2520provides%2520access%2520to%2520a%2520greater%2520volume%2520and%2520diversity%2520of%250Adata%252C%2520thus%2520improving%2520training%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Warming%20Up%20for%20Zeroth-Order%20Federated%20Pre-Training%20with%20Low%20Resource%0A%20%20Clients&entry.906535625=Gwen%20Legate%20and%20Irina%20Rish%20and%20Eugene%20Belilovsky&entry.1292438233=%20%20Federated%20learning%20enables%20collaborative%20model%20training%20across%20numerous%20edge%0Adevices%20without%20requiring%20participants%20to%20share%20data%3B%20however%2C%20memory%20and%0Acommunication%20constraints%20on%20these%20edge%20devices%20may%20preclude%20their%0Aparticipation%20in%20training.%20We%20consider%20a%20setting%20in%20which%20a%20subset%20of%20edge%0Adevices%20are%20below%20a%20critical%20memory%20or%20communication%20threshold%20required%20to%0Aconduct%20model%20updates.%20Under%20typical%20federated%20optimization%20algorithms%2C%20these%0Adevices%20are%20excluded%20from%20training%20which%20renders%20their%20data%20inaccessible%20and%0Aincreases%20system%20induced%20bias.%20We%20are%20inspired%20by%20MeZO%2C%20a%20zeroth-order%20method%0Aused%20for%20memory-efficient%20fine-tuning.%20The%20increased%20variance%20inherent%20to%0Azeroth-order%20gradient%20approximations%20has%20relegated%20previous%20zeroth-order%0Aoptimizers%20exclusively%20to%20the%20domain%20of%20fine%20tuning%3B%20a%20limitation%20we%20seek%20to%0Acorrect.%20We%20devise%20a%20federated%2C%20memory-efficient%20zeroth-order%20optimizer%2C%0AZOWarmUp%20that%20permits%20zeroth-order%20training%20from%20a%20random%20initialization.%0AZOWarmUp%20leverages%20differing%20client%20capabilities%20and%20careful%20variance%20reduction%0Atechniques%20to%20facilitate%20participation%20of%20under-represented%2C%20low-resource%0Aclients%20in%20model%20training.%20Like%20other%20federated%20zeroth-order%20methods%2C%20ZOWarmUp%0Aeliminates%20the%20need%20for%20edge%20devices%20to%20transmit%20their%20full%20gradients%20to%20the%0Aserver%20and%20instead%20relies%20on%20only%20a%20small%20set%20of%20random%20seeds%2C%20rendering%20the%0Aup-link%20communication%20cost%20negligible.%20We%20present%20experiments%20using%20various%0Adatasets%20and%20model%20architectures%20to%20show%20that%20ZOWarmUp%20is%20a%20robust%20algorithm%0Athat%20can%20can%20be%20applied%20under%20a%20wide%20variety%20of%20circumstances.%20For%20systems%20with%0Aa%20high%20proportion%20of%20edge%20devices%20that%20would%20otherwise%20be%20excluded%20from%0Atraining%2C%20this%20algorithm%20provides%20access%20to%20a%20greater%20volume%20and%20diversity%20of%0Adata%2C%20thus%20improving%20training%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03503v1&entry.124074799=Read"},
{"title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models", "author": "Qiang Liu and Xinlong Chen and Yue Ding and Bowen Song and Weiqiang Wang and Shu Wu and Liang Wang", "abstract": "  Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.\n", "link": "http://arxiv.org/abs/2501.09997v3", "date": "2025-09-03", "relevancy": 1.8446, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4639}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-guided%20Self-reflection%20for%20Zero-shot%20Hallucination%20Detection%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20Attention-guided%20Self-reflection%20for%20Zero-shot%20Hallucination%20Detection%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Qiang%20Liu%20and%20Xinlong%20Chen%20and%20Yue%20Ding%20and%20Bowen%20Song%20and%20Weiqiang%20Wang%20and%20Shu%20Wu%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Hallucination%20has%20emerged%20as%20a%20significant%20barrier%20to%20the%20effective%0Aapplication%20of%20Large%20Language%20Models%20%28LLMs%29.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0AAttention-Guided%20SElf-Reflection%20%28AGSER%29%20approach%20for%20zero-shot%20hallucination%0Adetection%20in%20LLMs.%20The%20AGSER%20method%20utilizes%20attention%20contributions%20to%0Acategorize%20the%20input%20query%20into%20attentive%20and%20non-attentive%20queries.%20Each%20query%0Ais%20then%20processed%20separately%20through%20the%20LLMs%2C%20allowing%20us%20to%20compute%0Aconsistency%20scores%20between%20the%20generated%20responses%20and%20the%20original%20answer.%20The%0Adifference%20between%20the%20two%20consistency%20scores%20serves%20as%20a%20hallucination%0Aestimator.%20In%20addition%20to%20its%20efficacy%20in%20detecting%20hallucinations%2C%20AGSER%0Anotably%20reduces%20computational%20overhead%2C%20requiring%20only%20three%20passes%20through%20the%0ALLM%20and%20utilizing%20two%20sets%20of%20tokens.%20We%20have%20conducted%20extensive%20experiments%0Awith%20four%20widely-used%20LLMs%20across%20three%20different%20hallucination%20benchmarks%2C%0Ademonstrating%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%20in%0Azero-shot%20hallucination%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09997v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-guided%2520Self-reflection%2520for%2520Zero-shot%2520Hallucination%2520Detection%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DQiang%2520Liu%2520and%2520Xinlong%2520Chen%2520and%2520Yue%2520Ding%2520and%2520Bowen%2520Song%2520and%2520Weiqiang%2520Wang%2520and%2520Shu%2520Wu%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Hallucination%2520has%2520emerged%2520as%2520a%2520significant%2520barrier%2520to%2520the%2520effective%250Aapplication%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250AAttention-Guided%2520SElf-Reflection%2520%2528AGSER%2529%2520approach%2520for%2520zero-shot%2520hallucination%250Adetection%2520in%2520LLMs.%2520The%2520AGSER%2520method%2520utilizes%2520attention%2520contributions%2520to%250Acategorize%2520the%2520input%2520query%2520into%2520attentive%2520and%2520non-attentive%2520queries.%2520Each%2520query%250Ais%2520then%2520processed%2520separately%2520through%2520the%2520LLMs%252C%2520allowing%2520us%2520to%2520compute%250Aconsistency%2520scores%2520between%2520the%2520generated%2520responses%2520and%2520the%2520original%2520answer.%2520The%250Adifference%2520between%2520the%2520two%2520consistency%2520scores%2520serves%2520as%2520a%2520hallucination%250Aestimator.%2520In%2520addition%2520to%2520its%2520efficacy%2520in%2520detecting%2520hallucinations%252C%2520AGSER%250Anotably%2520reduces%2520computational%2520overhead%252C%2520requiring%2520only%2520three%2520passes%2520through%2520the%250ALLM%2520and%2520utilizing%2520two%2520sets%2520of%2520tokens.%2520We%2520have%2520conducted%2520extensive%2520experiments%250Awith%2520four%2520widely-used%2520LLMs%2520across%2520three%2520different%2520hallucination%2520benchmarks%252C%250Ademonstrating%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%2520methods%2520in%250Azero-shot%2520hallucination%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09997v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-guided%20Self-reflection%20for%20Zero-shot%20Hallucination%20Detection%0A%20%20in%20Large%20Language%20Models&entry.906535625=Qiang%20Liu%20and%20Xinlong%20Chen%20and%20Yue%20Ding%20and%20Bowen%20Song%20and%20Weiqiang%20Wang%20and%20Shu%20Wu%20and%20Liang%20Wang&entry.1292438233=%20%20Hallucination%20has%20emerged%20as%20a%20significant%20barrier%20to%20the%20effective%0Aapplication%20of%20Large%20Language%20Models%20%28LLMs%29.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0AAttention-Guided%20SElf-Reflection%20%28AGSER%29%20approach%20for%20zero-shot%20hallucination%0Adetection%20in%20LLMs.%20The%20AGSER%20method%20utilizes%20attention%20contributions%20to%0Acategorize%20the%20input%20query%20into%20attentive%20and%20non-attentive%20queries.%20Each%20query%0Ais%20then%20processed%20separately%20through%20the%20LLMs%2C%20allowing%20us%20to%20compute%0Aconsistency%20scores%20between%20the%20generated%20responses%20and%20the%20original%20answer.%20The%0Adifference%20between%20the%20two%20consistency%20scores%20serves%20as%20a%20hallucination%0Aestimator.%20In%20addition%20to%20its%20efficacy%20in%20detecting%20hallucinations%2C%20AGSER%0Anotably%20reduces%20computational%20overhead%2C%20requiring%20only%20three%20passes%20through%20the%0ALLM%20and%20utilizing%20two%20sets%20of%20tokens.%20We%20have%20conducted%20extensive%20experiments%0Awith%20four%20widely-used%20LLMs%20across%20three%20different%20hallucination%20benchmarks%2C%0Ademonstrating%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%20in%0Azero-shot%20hallucination%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09997v3&entry.124074799=Read"},
{"title": "Stretchable Electrohydraulic Artificial Muscle for Full Motion Ranges in\n  Musculoskeletal Antagonistic Joints", "author": "Amirhossein Kazemipour and Ronan Hinchet and Robert K. Katzschmann", "abstract": "  Artificial muscles play a crucial role in musculoskeletal robotics and\nprosthetics to approximate the force-generating functionality of biological\nmuscle. However, current artificial muscle systems are typically limited to\neither contraction or extension, not both. This limitation hinders the\ndevelopment of fully functional artificial musculoskeletal systems. We address\nthis challenge by introducing an artificial antagonistic muscle system capable\nof both contraction and extension. Our design integrates non-stretchable\nelectrohydraulic soft actuators (HASELs) with electrostatic clutches within an\nantagonistic musculoskeletal framework. This configuration enables an\nantagonistic joint to achieve a full range of motion without displacement loss\ndue to tendon slack. We implement a synchronization method to coordinate muscle\nand clutch units, ensuring smooth motion profiles and speeds. This approach\nfacilitates seamless transitions between antagonistic muscles at operational\nfrequencies of up to 3.2 Hz. While our prototype utilizes electrohydraulic\nactuators, this muscle-clutch concept is adaptable to other non-stretchable\nartificial muscles, such as McKibben actuators, expanding their capability for\nextension and full range of motion in antagonistic setups. Our design\nrepresents a significant advancement in the development of fundamental\ncomponents for more functional and efficient artificial musculoskeletal\nsystems, bringing their capabilities closer to those of their biological\ncounterparts.\n", "link": "http://arxiv.org/abs/2409.11017v3", "date": "2025-09-03", "relevancy": 1.8434, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.472}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4639}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stretchable%20Electrohydraulic%20Artificial%20Muscle%20for%20Full%20Motion%20Ranges%20in%0A%20%20Musculoskeletal%20Antagonistic%20Joints&body=Title%3A%20Stretchable%20Electrohydraulic%20Artificial%20Muscle%20for%20Full%20Motion%20Ranges%20in%0A%20%20Musculoskeletal%20Antagonistic%20Joints%0AAuthor%3A%20Amirhossein%20Kazemipour%20and%20Ronan%20Hinchet%20and%20Robert%20K.%20Katzschmann%0AAbstract%3A%20%20%20Artificial%20muscles%20play%20a%20crucial%20role%20in%20musculoskeletal%20robotics%20and%0Aprosthetics%20to%20approximate%20the%20force-generating%20functionality%20of%20biological%0Amuscle.%20However%2C%20current%20artificial%20muscle%20systems%20are%20typically%20limited%20to%0Aeither%20contraction%20or%20extension%2C%20not%20both.%20This%20limitation%20hinders%20the%0Adevelopment%20of%20fully%20functional%20artificial%20musculoskeletal%20systems.%20We%20address%0Athis%20challenge%20by%20introducing%20an%20artificial%20antagonistic%20muscle%20system%20capable%0Aof%20both%20contraction%20and%20extension.%20Our%20design%20integrates%20non-stretchable%0Aelectrohydraulic%20soft%20actuators%20%28HASELs%29%20with%20electrostatic%20clutches%20within%20an%0Aantagonistic%20musculoskeletal%20framework.%20This%20configuration%20enables%20an%0Aantagonistic%20joint%20to%20achieve%20a%20full%20range%20of%20motion%20without%20displacement%20loss%0Adue%20to%20tendon%20slack.%20We%20implement%20a%20synchronization%20method%20to%20coordinate%20muscle%0Aand%20clutch%20units%2C%20ensuring%20smooth%20motion%20profiles%20and%20speeds.%20This%20approach%0Afacilitates%20seamless%20transitions%20between%20antagonistic%20muscles%20at%20operational%0Afrequencies%20of%20up%20to%203.2%20Hz.%20While%20our%20prototype%20utilizes%20electrohydraulic%0Aactuators%2C%20this%20muscle-clutch%20concept%20is%20adaptable%20to%20other%20non-stretchable%0Aartificial%20muscles%2C%20such%20as%20McKibben%20actuators%2C%20expanding%20their%20capability%20for%0Aextension%20and%20full%20range%20of%20motion%20in%20antagonistic%20setups.%20Our%20design%0Arepresents%20a%20significant%20advancement%20in%20the%20development%20of%20fundamental%0Acomponents%20for%20more%20functional%20and%20efficient%20artificial%20musculoskeletal%0Asystems%2C%20bringing%20their%20capabilities%20closer%20to%20those%20of%20their%20biological%0Acounterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11017v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStretchable%2520Electrohydraulic%2520Artificial%2520Muscle%2520for%2520Full%2520Motion%2520Ranges%2520in%250A%2520%2520Musculoskeletal%2520Antagonistic%2520Joints%26entry.906535625%3DAmirhossein%2520Kazemipour%2520and%2520Ronan%2520Hinchet%2520and%2520Robert%2520K.%2520Katzschmann%26entry.1292438233%3D%2520%2520Artificial%2520muscles%2520play%2520a%2520crucial%2520role%2520in%2520musculoskeletal%2520robotics%2520and%250Aprosthetics%2520to%2520approximate%2520the%2520force-generating%2520functionality%2520of%2520biological%250Amuscle.%2520However%252C%2520current%2520artificial%2520muscle%2520systems%2520are%2520typically%2520limited%2520to%250Aeither%2520contraction%2520or%2520extension%252C%2520not%2520both.%2520This%2520limitation%2520hinders%2520the%250Adevelopment%2520of%2520fully%2520functional%2520artificial%2520musculoskeletal%2520systems.%2520We%2520address%250Athis%2520challenge%2520by%2520introducing%2520an%2520artificial%2520antagonistic%2520muscle%2520system%2520capable%250Aof%2520both%2520contraction%2520and%2520extension.%2520Our%2520design%2520integrates%2520non-stretchable%250Aelectrohydraulic%2520soft%2520actuators%2520%2528HASELs%2529%2520with%2520electrostatic%2520clutches%2520within%2520an%250Aantagonistic%2520musculoskeletal%2520framework.%2520This%2520configuration%2520enables%2520an%250Aantagonistic%2520joint%2520to%2520achieve%2520a%2520full%2520range%2520of%2520motion%2520without%2520displacement%2520loss%250Adue%2520to%2520tendon%2520slack.%2520We%2520implement%2520a%2520synchronization%2520method%2520to%2520coordinate%2520muscle%250Aand%2520clutch%2520units%252C%2520ensuring%2520smooth%2520motion%2520profiles%2520and%2520speeds.%2520This%2520approach%250Afacilitates%2520seamless%2520transitions%2520between%2520antagonistic%2520muscles%2520at%2520operational%250Afrequencies%2520of%2520up%2520to%25203.2%2520Hz.%2520While%2520our%2520prototype%2520utilizes%2520electrohydraulic%250Aactuators%252C%2520this%2520muscle-clutch%2520concept%2520is%2520adaptable%2520to%2520other%2520non-stretchable%250Aartificial%2520muscles%252C%2520such%2520as%2520McKibben%2520actuators%252C%2520expanding%2520their%2520capability%2520for%250Aextension%2520and%2520full%2520range%2520of%2520motion%2520in%2520antagonistic%2520setups.%2520Our%2520design%250Arepresents%2520a%2520significant%2520advancement%2520in%2520the%2520development%2520of%2520fundamental%250Acomponents%2520for%2520more%2520functional%2520and%2520efficient%2520artificial%2520musculoskeletal%250Asystems%252C%2520bringing%2520their%2520capabilities%2520closer%2520to%2520those%2520of%2520their%2520biological%250Acounterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11017v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stretchable%20Electrohydraulic%20Artificial%20Muscle%20for%20Full%20Motion%20Ranges%20in%0A%20%20Musculoskeletal%20Antagonistic%20Joints&entry.906535625=Amirhossein%20Kazemipour%20and%20Ronan%20Hinchet%20and%20Robert%20K.%20Katzschmann&entry.1292438233=%20%20Artificial%20muscles%20play%20a%20crucial%20role%20in%20musculoskeletal%20robotics%20and%0Aprosthetics%20to%20approximate%20the%20force-generating%20functionality%20of%20biological%0Amuscle.%20However%2C%20current%20artificial%20muscle%20systems%20are%20typically%20limited%20to%0Aeither%20contraction%20or%20extension%2C%20not%20both.%20This%20limitation%20hinders%20the%0Adevelopment%20of%20fully%20functional%20artificial%20musculoskeletal%20systems.%20We%20address%0Athis%20challenge%20by%20introducing%20an%20artificial%20antagonistic%20muscle%20system%20capable%0Aof%20both%20contraction%20and%20extension.%20Our%20design%20integrates%20non-stretchable%0Aelectrohydraulic%20soft%20actuators%20%28HASELs%29%20with%20electrostatic%20clutches%20within%20an%0Aantagonistic%20musculoskeletal%20framework.%20This%20configuration%20enables%20an%0Aantagonistic%20joint%20to%20achieve%20a%20full%20range%20of%20motion%20without%20displacement%20loss%0Adue%20to%20tendon%20slack.%20We%20implement%20a%20synchronization%20method%20to%20coordinate%20muscle%0Aand%20clutch%20units%2C%20ensuring%20smooth%20motion%20profiles%20and%20speeds.%20This%20approach%0Afacilitates%20seamless%20transitions%20between%20antagonistic%20muscles%20at%20operational%0Afrequencies%20of%20up%20to%203.2%20Hz.%20While%20our%20prototype%20utilizes%20electrohydraulic%0Aactuators%2C%20this%20muscle-clutch%20concept%20is%20adaptable%20to%20other%20non-stretchable%0Aartificial%20muscles%2C%20such%20as%20McKibben%20actuators%2C%20expanding%20their%20capability%20for%0Aextension%20and%20full%20range%20of%20motion%20in%20antagonistic%20setups.%20Our%20design%0Arepresents%20a%20significant%20advancement%20in%20the%20development%20of%20fundamental%0Acomponents%20for%20more%20functional%20and%20efficient%20artificial%20musculoskeletal%0Asystems%2C%20bringing%20their%20capabilities%20closer%20to%20those%20of%20their%20biological%0Acounterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11017v3&entry.124074799=Read"},
{"title": "Revisiting Clustering of Neural Bandits: Selective Reinitialization for\n  Mitigating Loss of Plasticity", "author": "Zhiyuan Su and Sunhao Dai and Xiao Zhang", "abstract": "  Clustering of Bandits (CB) methods enhance sequential decision-making by\ngrouping bandits into clusters based on similarity and incorporating\ncluster-level contextual information, demonstrating effectiveness and\nadaptability in applications like personalized streaming recommendations.\nHowever, when extending CB algorithms to their neural version (commonly\nreferred to as Clustering of Neural Bandits, or CNB), they suffer from loss of\nplasticity, where neural network parameters become rigid and less adaptable\nover time, limiting their ability to adapt to non-stationary environments\n(e.g., dynamic user preferences in recommendation). To address this challenge,\nwe propose Selective Reinitialization (SeRe), a novel bandit learning framework\nthat dynamically preserves the adaptability of CNB algorithms in evolving\nenvironments. SeRe leverages a contribution utility metric to identify and\nselectively reset underutilized units, mitigating loss of plasticity while\nmaintaining stable knowledge retention. Furthermore, when combining SeRe with\nCNB algorithms, the adaptive change detection mechanism adjusts the\nreinitialization frequency according to the degree of non-stationarity,\nensuring effective adaptation without unnecessary resets. Theoretically, we\nprove that SeRe enables sublinear cumulative regret in piecewise-stationary\nenvironments, outperforming traditional CNB approaches in long-term\nperformances. Extensive experiments on six real-world recommendation datasets\ndemonstrate that SeRe-enhanced CNB algorithms can effectively mitigate the loss\nof plasticity with lower regrets, improving adaptability and robustness in\ndynamic settings.\n", "link": "http://arxiv.org/abs/2506.12389v2", "date": "2025-09-03", "relevancy": 1.8431, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4916}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4656}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Clustering%20of%20Neural%20Bandits%3A%20Selective%20Reinitialization%20for%0A%20%20Mitigating%20Loss%20of%20Plasticity&body=Title%3A%20Revisiting%20Clustering%20of%20Neural%20Bandits%3A%20Selective%20Reinitialization%20for%0A%20%20Mitigating%20Loss%20of%20Plasticity%0AAuthor%3A%20Zhiyuan%20Su%20and%20Sunhao%20Dai%20and%20Xiao%20Zhang%0AAbstract%3A%20%20%20Clustering%20of%20Bandits%20%28CB%29%20methods%20enhance%20sequential%20decision-making%20by%0Agrouping%20bandits%20into%20clusters%20based%20on%20similarity%20and%20incorporating%0Acluster-level%20contextual%20information%2C%20demonstrating%20effectiveness%20and%0Aadaptability%20in%20applications%20like%20personalized%20streaming%20recommendations.%0AHowever%2C%20when%20extending%20CB%20algorithms%20to%20their%20neural%20version%20%28commonly%0Areferred%20to%20as%20Clustering%20of%20Neural%20Bandits%2C%20or%20CNB%29%2C%20they%20suffer%20from%20loss%20of%0Aplasticity%2C%20where%20neural%20network%20parameters%20become%20rigid%20and%20less%20adaptable%0Aover%20time%2C%20limiting%20their%20ability%20to%20adapt%20to%20non-stationary%20environments%0A%28e.g.%2C%20dynamic%20user%20preferences%20in%20recommendation%29.%20To%20address%20this%20challenge%2C%0Awe%20propose%20Selective%20Reinitialization%20%28SeRe%29%2C%20a%20novel%20bandit%20learning%20framework%0Athat%20dynamically%20preserves%20the%20adaptability%20of%20CNB%20algorithms%20in%20evolving%0Aenvironments.%20SeRe%20leverages%20a%20contribution%20utility%20metric%20to%20identify%20and%0Aselectively%20reset%20underutilized%20units%2C%20mitigating%20loss%20of%20plasticity%20while%0Amaintaining%20stable%20knowledge%20retention.%20Furthermore%2C%20when%20combining%20SeRe%20with%0ACNB%20algorithms%2C%20the%20adaptive%20change%20detection%20mechanism%20adjusts%20the%0Areinitialization%20frequency%20according%20to%20the%20degree%20of%20non-stationarity%2C%0Aensuring%20effective%20adaptation%20without%20unnecessary%20resets.%20Theoretically%2C%20we%0Aprove%20that%20SeRe%20enables%20sublinear%20cumulative%20regret%20in%20piecewise-stationary%0Aenvironments%2C%20outperforming%20traditional%20CNB%20approaches%20in%20long-term%0Aperformances.%20Extensive%20experiments%20on%20six%20real-world%20recommendation%20datasets%0Ademonstrate%20that%20SeRe-enhanced%20CNB%20algorithms%20can%20effectively%20mitigate%20the%20loss%0Aof%20plasticity%20with%20lower%20regrets%2C%20improving%20adaptability%20and%20robustness%20in%0Adynamic%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Clustering%2520of%2520Neural%2520Bandits%253A%2520Selective%2520Reinitialization%2520for%250A%2520%2520Mitigating%2520Loss%2520of%2520Plasticity%26entry.906535625%3DZhiyuan%2520Su%2520and%2520Sunhao%2520Dai%2520and%2520Xiao%2520Zhang%26entry.1292438233%3D%2520%2520Clustering%2520of%2520Bandits%2520%2528CB%2529%2520methods%2520enhance%2520sequential%2520decision-making%2520by%250Agrouping%2520bandits%2520into%2520clusters%2520based%2520on%2520similarity%2520and%2520incorporating%250Acluster-level%2520contextual%2520information%252C%2520demonstrating%2520effectiveness%2520and%250Aadaptability%2520in%2520applications%2520like%2520personalized%2520streaming%2520recommendations.%250AHowever%252C%2520when%2520extending%2520CB%2520algorithms%2520to%2520their%2520neural%2520version%2520%2528commonly%250Areferred%2520to%2520as%2520Clustering%2520of%2520Neural%2520Bandits%252C%2520or%2520CNB%2529%252C%2520they%2520suffer%2520from%2520loss%2520of%250Aplasticity%252C%2520where%2520neural%2520network%2520parameters%2520become%2520rigid%2520and%2520less%2520adaptable%250Aover%2520time%252C%2520limiting%2520their%2520ability%2520to%2520adapt%2520to%2520non-stationary%2520environments%250A%2528e.g.%252C%2520dynamic%2520user%2520preferences%2520in%2520recommendation%2529.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520Selective%2520Reinitialization%2520%2528SeRe%2529%252C%2520a%2520novel%2520bandit%2520learning%2520framework%250Athat%2520dynamically%2520preserves%2520the%2520adaptability%2520of%2520CNB%2520algorithms%2520in%2520evolving%250Aenvironments.%2520SeRe%2520leverages%2520a%2520contribution%2520utility%2520metric%2520to%2520identify%2520and%250Aselectively%2520reset%2520underutilized%2520units%252C%2520mitigating%2520loss%2520of%2520plasticity%2520while%250Amaintaining%2520stable%2520knowledge%2520retention.%2520Furthermore%252C%2520when%2520combining%2520SeRe%2520with%250ACNB%2520algorithms%252C%2520the%2520adaptive%2520change%2520detection%2520mechanism%2520adjusts%2520the%250Areinitialization%2520frequency%2520according%2520to%2520the%2520degree%2520of%2520non-stationarity%252C%250Aensuring%2520effective%2520adaptation%2520without%2520unnecessary%2520resets.%2520Theoretically%252C%2520we%250Aprove%2520that%2520SeRe%2520enables%2520sublinear%2520cumulative%2520regret%2520in%2520piecewise-stationary%250Aenvironments%252C%2520outperforming%2520traditional%2520CNB%2520approaches%2520in%2520long-term%250Aperformances.%2520Extensive%2520experiments%2520on%2520six%2520real-world%2520recommendation%2520datasets%250Ademonstrate%2520that%2520SeRe-enhanced%2520CNB%2520algorithms%2520can%2520effectively%2520mitigate%2520the%2520loss%250Aof%2520plasticity%2520with%2520lower%2520regrets%252C%2520improving%2520adaptability%2520and%2520robustness%2520in%250Adynamic%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Clustering%20of%20Neural%20Bandits%3A%20Selective%20Reinitialization%20for%0A%20%20Mitigating%20Loss%20of%20Plasticity&entry.906535625=Zhiyuan%20Su%20and%20Sunhao%20Dai%20and%20Xiao%20Zhang&entry.1292438233=%20%20Clustering%20of%20Bandits%20%28CB%29%20methods%20enhance%20sequential%20decision-making%20by%0Agrouping%20bandits%20into%20clusters%20based%20on%20similarity%20and%20incorporating%0Acluster-level%20contextual%20information%2C%20demonstrating%20effectiveness%20and%0Aadaptability%20in%20applications%20like%20personalized%20streaming%20recommendations.%0AHowever%2C%20when%20extending%20CB%20algorithms%20to%20their%20neural%20version%20%28commonly%0Areferred%20to%20as%20Clustering%20of%20Neural%20Bandits%2C%20or%20CNB%29%2C%20they%20suffer%20from%20loss%20of%0Aplasticity%2C%20where%20neural%20network%20parameters%20become%20rigid%20and%20less%20adaptable%0Aover%20time%2C%20limiting%20their%20ability%20to%20adapt%20to%20non-stationary%20environments%0A%28e.g.%2C%20dynamic%20user%20preferences%20in%20recommendation%29.%20To%20address%20this%20challenge%2C%0Awe%20propose%20Selective%20Reinitialization%20%28SeRe%29%2C%20a%20novel%20bandit%20learning%20framework%0Athat%20dynamically%20preserves%20the%20adaptability%20of%20CNB%20algorithms%20in%20evolving%0Aenvironments.%20SeRe%20leverages%20a%20contribution%20utility%20metric%20to%20identify%20and%0Aselectively%20reset%20underutilized%20units%2C%20mitigating%20loss%20of%20plasticity%20while%0Amaintaining%20stable%20knowledge%20retention.%20Furthermore%2C%20when%20combining%20SeRe%20with%0ACNB%20algorithms%2C%20the%20adaptive%20change%20detection%20mechanism%20adjusts%20the%0Areinitialization%20frequency%20according%20to%20the%20degree%20of%20non-stationarity%2C%0Aensuring%20effective%20adaptation%20without%20unnecessary%20resets.%20Theoretically%2C%20we%0Aprove%20that%20SeRe%20enables%20sublinear%20cumulative%20regret%20in%20piecewise-stationary%0Aenvironments%2C%20outperforming%20traditional%20CNB%20approaches%20in%20long-term%0Aperformances.%20Extensive%20experiments%20on%20six%20real-world%20recommendation%20datasets%0Ademonstrate%20that%20SeRe-enhanced%20CNB%20algorithms%20can%20effectively%20mitigate%20the%20loss%0Aof%20plasticity%20with%20lower%20regrets%2C%20improving%20adaptability%20and%20robustness%20in%0Adynamic%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12389v2&entry.124074799=Read"},
{"title": "CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural\n  Additive Models", "author": "Dhanesh Ramachandram and Ananya Raval", "abstract": "  Competing risks are crucial considerations in survival modelling,\nparticularly in healthcare domains where patients may experience multiple\ndistinct event types. We propose CRISP-NAM (Competing Risks Interpretable\nSurvival Prediction with Neural Additive Models), an interpretable neural\nadditive model for competing risks survival analysis which extends the neural\nadditive architecture to model cause-specific hazards while preserving\nfeature-level interpretability. Each feature contributes independently to risk\nestimation through dedicated neural networks, allowing for visualization of\ncomplex non-linear relationships between covariates and each competing risk. We\ndemonstrate competitive performance on multiple datasets compared to existing\napproaches.\n", "link": "http://arxiv.org/abs/2505.21360v4", "date": "2025-09-03", "relevancy": 1.459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4926}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRISP-NAM%3A%20Competing%20Risks%20Interpretable%20Survival%20Prediction%20with%20Neural%0A%20%20Additive%20Models&body=Title%3A%20CRISP-NAM%3A%20Competing%20Risks%20Interpretable%20Survival%20Prediction%20with%20Neural%0A%20%20Additive%20Models%0AAuthor%3A%20Dhanesh%20Ramachandram%20and%20Ananya%20Raval%0AAbstract%3A%20%20%20Competing%20risks%20are%20crucial%20considerations%20in%20survival%20modelling%2C%0Aparticularly%20in%20healthcare%20domains%20where%20patients%20may%20experience%20multiple%0Adistinct%20event%20types.%20We%20propose%20CRISP-NAM%20%28Competing%20Risks%20Interpretable%0ASurvival%20Prediction%20with%20Neural%20Additive%20Models%29%2C%20an%20interpretable%20neural%0Aadditive%20model%20for%20competing%20risks%20survival%20analysis%20which%20extends%20the%20neural%0Aadditive%20architecture%20to%20model%20cause-specific%20hazards%20while%20preserving%0Afeature-level%20interpretability.%20Each%20feature%20contributes%20independently%20to%20risk%0Aestimation%20through%20dedicated%20neural%20networks%2C%20allowing%20for%20visualization%20of%0Acomplex%20non-linear%20relationships%20between%20covariates%20and%20each%20competing%20risk.%20We%0Ademonstrate%20competitive%20performance%20on%20multiple%20datasets%20compared%20to%20existing%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21360v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRISP-NAM%253A%2520Competing%2520Risks%2520Interpretable%2520Survival%2520Prediction%2520with%2520Neural%250A%2520%2520Additive%2520Models%26entry.906535625%3DDhanesh%2520Ramachandram%2520and%2520Ananya%2520Raval%26entry.1292438233%3D%2520%2520Competing%2520risks%2520are%2520crucial%2520considerations%2520in%2520survival%2520modelling%252C%250Aparticularly%2520in%2520healthcare%2520domains%2520where%2520patients%2520may%2520experience%2520multiple%250Adistinct%2520event%2520types.%2520We%2520propose%2520CRISP-NAM%2520%2528Competing%2520Risks%2520Interpretable%250ASurvival%2520Prediction%2520with%2520Neural%2520Additive%2520Models%2529%252C%2520an%2520interpretable%2520neural%250Aadditive%2520model%2520for%2520competing%2520risks%2520survival%2520analysis%2520which%2520extends%2520the%2520neural%250Aadditive%2520architecture%2520to%2520model%2520cause-specific%2520hazards%2520while%2520preserving%250Afeature-level%2520interpretability.%2520Each%2520feature%2520contributes%2520independently%2520to%2520risk%250Aestimation%2520through%2520dedicated%2520neural%2520networks%252C%2520allowing%2520for%2520visualization%2520of%250Acomplex%2520non-linear%2520relationships%2520between%2520covariates%2520and%2520each%2520competing%2520risk.%2520We%250Ademonstrate%2520competitive%2520performance%2520on%2520multiple%2520datasets%2520compared%2520to%2520existing%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21360v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRISP-NAM%3A%20Competing%20Risks%20Interpretable%20Survival%20Prediction%20with%20Neural%0A%20%20Additive%20Models&entry.906535625=Dhanesh%20Ramachandram%20and%20Ananya%20Raval&entry.1292438233=%20%20Competing%20risks%20are%20crucial%20considerations%20in%20survival%20modelling%2C%0Aparticularly%20in%20healthcare%20domains%20where%20patients%20may%20experience%20multiple%0Adistinct%20event%20types.%20We%20propose%20CRISP-NAM%20%28Competing%20Risks%20Interpretable%0ASurvival%20Prediction%20with%20Neural%20Additive%20Models%29%2C%20an%20interpretable%20neural%0Aadditive%20model%20for%20competing%20risks%20survival%20analysis%20which%20extends%20the%20neural%0Aadditive%20architecture%20to%20model%20cause-specific%20hazards%20while%20preserving%0Afeature-level%20interpretability.%20Each%20feature%20contributes%20independently%20to%20risk%0Aestimation%20through%20dedicated%20neural%20networks%2C%20allowing%20for%20visualization%20of%0Acomplex%20non-linear%20relationships%20between%20covariates%20and%20each%20competing%20risk.%20We%0Ademonstrate%20competitive%20performance%20on%20multiple%20datasets%20compared%20to%20existing%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21360v4&entry.124074799=Read"},
{"title": "Real-Time Instrument Planning and Perception for Novel Measurements of\n  Dynamic Phenomena", "author": "Itai Zilberstein and Alberto Candela and Steve Chien", "abstract": "  Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.\n", "link": "http://arxiv.org/abs/2509.03500v1", "date": "2025-09-03", "relevancy": 1.6652, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5649}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.56}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Instrument%20Planning%20and%20Perception%20for%20Novel%20Measurements%20of%0A%20%20Dynamic%20Phenomena&body=Title%3A%20Real-Time%20Instrument%20Planning%20and%20Perception%20for%20Novel%20Measurements%20of%0A%20%20Dynamic%20Phenomena%0AAuthor%3A%20Itai%20Zilberstein%20and%20Alberto%20Candela%20and%20Steve%20Chien%0AAbstract%3A%20%20%20Advancements%20in%20onboard%20computing%20mean%20remote%20sensing%20agents%20can%20employ%0Astate-of-the-art%20computer%20vision%20and%20machine%20learning%20at%20the%20edge.%20These%0Acapabilities%20can%20be%20leveraged%20to%20unlock%20new%20rare%2C%20transient%2C%20and%20pinpoint%0Ameasurements%20of%20dynamic%20science%20phenomena.%20In%20this%20paper%2C%20we%20present%20an%0Aautomated%20workflow%20that%20synthesizes%20the%20detection%20of%20these%20dynamic%20events%20in%0Alook-ahead%20satellite%20imagery%20with%20autonomous%20trajectory%20planning%20for%20a%0Afollow-up%20high-resolution%20sensor%20to%20obtain%20pinpoint%20measurements.%20We%20apply%20this%0Aworkflow%20to%20the%20use%20case%20of%20observing%20volcanic%20plumes.%20We%20analyze%0Aclassification%20approaches%20including%20traditional%20machine%20learning%20algorithms%20and%0Aconvolutional%20neural%20networks.%20We%20present%20several%20trajectory%20planning%0Aalgorithms%20that%20track%20the%20morphological%20features%20of%20a%20plume%20and%20integrate%20these%0Aalgorithms%20with%20the%20classifiers.%20We%20show%20through%20simulation%20an%20order%20of%0Amagnitude%20increase%20in%20the%20utility%20return%20of%20the%20high-resolution%20instrument%0Acompared%20to%20baselines%20while%20maintaining%20efficient%20runtimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Instrument%2520Planning%2520and%2520Perception%2520for%2520Novel%2520Measurements%2520of%250A%2520%2520Dynamic%2520Phenomena%26entry.906535625%3DItai%2520Zilberstein%2520and%2520Alberto%2520Candela%2520and%2520Steve%2520Chien%26entry.1292438233%3D%2520%2520Advancements%2520in%2520onboard%2520computing%2520mean%2520remote%2520sensing%2520agents%2520can%2520employ%250Astate-of-the-art%2520computer%2520vision%2520and%2520machine%2520learning%2520at%2520the%2520edge.%2520These%250Acapabilities%2520can%2520be%2520leveraged%2520to%2520unlock%2520new%2520rare%252C%2520transient%252C%2520and%2520pinpoint%250Ameasurements%2520of%2520dynamic%2520science%2520phenomena.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%250Aautomated%2520workflow%2520that%2520synthesizes%2520the%2520detection%2520of%2520these%2520dynamic%2520events%2520in%250Alook-ahead%2520satellite%2520imagery%2520with%2520autonomous%2520trajectory%2520planning%2520for%2520a%250Afollow-up%2520high-resolution%2520sensor%2520to%2520obtain%2520pinpoint%2520measurements.%2520We%2520apply%2520this%250Aworkflow%2520to%2520the%2520use%2520case%2520of%2520observing%2520volcanic%2520plumes.%2520We%2520analyze%250Aclassification%2520approaches%2520including%2520traditional%2520machine%2520learning%2520algorithms%2520and%250Aconvolutional%2520neural%2520networks.%2520We%2520present%2520several%2520trajectory%2520planning%250Aalgorithms%2520that%2520track%2520the%2520morphological%2520features%2520of%2520a%2520plume%2520and%2520integrate%2520these%250Aalgorithms%2520with%2520the%2520classifiers.%2520We%2520show%2520through%2520simulation%2520an%2520order%2520of%250Amagnitude%2520increase%2520in%2520the%2520utility%2520return%2520of%2520the%2520high-resolution%2520instrument%250Acompared%2520to%2520baselines%2520while%2520maintaining%2520efficient%2520runtimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Instrument%20Planning%20and%20Perception%20for%20Novel%20Measurements%20of%0A%20%20Dynamic%20Phenomena&entry.906535625=Itai%20Zilberstein%20and%20Alberto%20Candela%20and%20Steve%20Chien&entry.1292438233=%20%20Advancements%20in%20onboard%20computing%20mean%20remote%20sensing%20agents%20can%20employ%0Astate-of-the-art%20computer%20vision%20and%20machine%20learning%20at%20the%20edge.%20These%0Acapabilities%20can%20be%20leveraged%20to%20unlock%20new%20rare%2C%20transient%2C%20and%20pinpoint%0Ameasurements%20of%20dynamic%20science%20phenomena.%20In%20this%20paper%2C%20we%20present%20an%0Aautomated%20workflow%20that%20synthesizes%20the%20detection%20of%20these%20dynamic%20events%20in%0Alook-ahead%20satellite%20imagery%20with%20autonomous%20trajectory%20planning%20for%20a%0Afollow-up%20high-resolution%20sensor%20to%20obtain%20pinpoint%20measurements.%20We%20apply%20this%0Aworkflow%20to%20the%20use%20case%20of%20observing%20volcanic%20plumes.%20We%20analyze%0Aclassification%20approaches%20including%20traditional%20machine%20learning%20algorithms%20and%0Aconvolutional%20neural%20networks.%20We%20present%20several%20trajectory%20planning%0Aalgorithms%20that%20track%20the%20morphological%20features%20of%20a%20plume%20and%20integrate%20these%0Aalgorithms%20with%20the%20classifiers.%20We%20show%20through%20simulation%20an%20order%20of%0Amagnitude%20increase%20in%20the%20utility%20return%20of%20the%20high-resolution%20instrument%0Acompared%20to%20baselines%20while%20maintaining%20efficient%20runtimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03500v1&entry.124074799=Read"},
{"title": "Murakkab: Resource-Efficient Agentic Workflow Orchestration in Cloud\n  Platforms", "author": "Gohar Irfan Chaudhry and Esha Choukse and Haoran Qiu and \u00cd\u00f1igo Goiri and Rodrigo Fonseca and Adam Belay and Ricardo Bianchini", "abstract": "  Agentic workflows commonly coordinate multiple models and tools with complex\ncontrol logic. They are quickly becoming the dominant paradigm for AI\napplications. However, serving them remains inefficient with today's\nframeworks. The key problem is that they expose workflows as opaque sequences\nof model and tool calls that tightly couple agent logic with model and hardware\nchoices. Often, these workflow components are fragmented across different\nentities, preventing systems from reasoning about trade-offs across accuracy,\nlatency, energy, and cost. This leads to resource waste and degraded\nservice-level objectives (SLOs).\n  We present Murakkab, a resource-efficient serving system for agentic\nworkflows. Murakkab introduces a declarative abstraction that decouples\nworkflow specification from execution configuration. A profile-guided optimizer\nand adaptive runtime jointly manage the full stack: orchestrating workflow\ncomponents, mapping them to models and hardware, and dynamically reconfiguring\nexecution to satisfy user-defined SLOs. By exposing the internal structure of\nagentic workflows, Murakkab enables cross-layer optimization that existing\nframeworks and cloud schedulers cannot achieve.\n  Our evaluation on diverse workflows shows that Murakkab reduces GPU usage by\nup to 2.8$\\times$, energy consumption by 3.7$\\times$, and cost by 4.3$\\times$\nwhile maintaining SLOs.\n", "link": "http://arxiv.org/abs/2508.18298v2", "date": "2025-09-03", "relevancy": 0.9165, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4802}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Murakkab%3A%20Resource-Efficient%20Agentic%20Workflow%20Orchestration%20in%20Cloud%0A%20%20Platforms&body=Title%3A%20Murakkab%3A%20Resource-Efficient%20Agentic%20Workflow%20Orchestration%20in%20Cloud%0A%20%20Platforms%0AAuthor%3A%20Gohar%20Irfan%20Chaudhry%20and%20Esha%20Choukse%20and%20Haoran%20Qiu%20and%20%C3%8D%C3%B1igo%20Goiri%20and%20Rodrigo%20Fonseca%20and%20Adam%20Belay%20and%20Ricardo%20Bianchini%0AAbstract%3A%20%20%20Agentic%20workflows%20commonly%20coordinate%20multiple%20models%20and%20tools%20with%20complex%0Acontrol%20logic.%20They%20are%20quickly%20becoming%20the%20dominant%20paradigm%20for%20AI%0Aapplications.%20However%2C%20serving%20them%20remains%20inefficient%20with%20today%27s%0Aframeworks.%20The%20key%20problem%20is%20that%20they%20expose%20workflows%20as%20opaque%20sequences%0Aof%20model%20and%20tool%20calls%20that%20tightly%20couple%20agent%20logic%20with%20model%20and%20hardware%0Achoices.%20Often%2C%20these%20workflow%20components%20are%20fragmented%20across%20different%0Aentities%2C%20preventing%20systems%20from%20reasoning%20about%20trade-offs%20across%20accuracy%2C%0Alatency%2C%20energy%2C%20and%20cost.%20This%20leads%20to%20resource%20waste%20and%20degraded%0Aservice-level%20objectives%20%28SLOs%29.%0A%20%20We%20present%20Murakkab%2C%20a%20resource-efficient%20serving%20system%20for%20agentic%0Aworkflows.%20Murakkab%20introduces%20a%20declarative%20abstraction%20that%20decouples%0Aworkflow%20specification%20from%20execution%20configuration.%20A%20profile-guided%20optimizer%0Aand%20adaptive%20runtime%20jointly%20manage%20the%20full%20stack%3A%20orchestrating%20workflow%0Acomponents%2C%20mapping%20them%20to%20models%20and%20hardware%2C%20and%20dynamically%20reconfiguring%0Aexecution%20to%20satisfy%20user-defined%20SLOs.%20By%20exposing%20the%20internal%20structure%20of%0Aagentic%20workflows%2C%20Murakkab%20enables%20cross-layer%20optimization%20that%20existing%0Aframeworks%20and%20cloud%20schedulers%20cannot%20achieve.%0A%20%20Our%20evaluation%20on%20diverse%20workflows%20shows%20that%20Murakkab%20reduces%20GPU%20usage%20by%0Aup%20to%202.8%24%5Ctimes%24%2C%20energy%20consumption%20by%203.7%24%5Ctimes%24%2C%20and%20cost%20by%204.3%24%5Ctimes%24%0Awhile%20maintaining%20SLOs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMurakkab%253A%2520Resource-Efficient%2520Agentic%2520Workflow%2520Orchestration%2520in%2520Cloud%250A%2520%2520Platforms%26entry.906535625%3DGohar%2520Irfan%2520Chaudhry%2520and%2520Esha%2520Choukse%2520and%2520Haoran%2520Qiu%2520and%2520%25C3%258D%25C3%25B1igo%2520Goiri%2520and%2520Rodrigo%2520Fonseca%2520and%2520Adam%2520Belay%2520and%2520Ricardo%2520Bianchini%26entry.1292438233%3D%2520%2520Agentic%2520workflows%2520commonly%2520coordinate%2520multiple%2520models%2520and%2520tools%2520with%2520complex%250Acontrol%2520logic.%2520They%2520are%2520quickly%2520becoming%2520the%2520dominant%2520paradigm%2520for%2520AI%250Aapplications.%2520However%252C%2520serving%2520them%2520remains%2520inefficient%2520with%2520today%2527s%250Aframeworks.%2520The%2520key%2520problem%2520is%2520that%2520they%2520expose%2520workflows%2520as%2520opaque%2520sequences%250Aof%2520model%2520and%2520tool%2520calls%2520that%2520tightly%2520couple%2520agent%2520logic%2520with%2520model%2520and%2520hardware%250Achoices.%2520Often%252C%2520these%2520workflow%2520components%2520are%2520fragmented%2520across%2520different%250Aentities%252C%2520preventing%2520systems%2520from%2520reasoning%2520about%2520trade-offs%2520across%2520accuracy%252C%250Alatency%252C%2520energy%252C%2520and%2520cost.%2520This%2520leads%2520to%2520resource%2520waste%2520and%2520degraded%250Aservice-level%2520objectives%2520%2528SLOs%2529.%250A%2520%2520We%2520present%2520Murakkab%252C%2520a%2520resource-efficient%2520serving%2520system%2520for%2520agentic%250Aworkflows.%2520Murakkab%2520introduces%2520a%2520declarative%2520abstraction%2520that%2520decouples%250Aworkflow%2520specification%2520from%2520execution%2520configuration.%2520A%2520profile-guided%2520optimizer%250Aand%2520adaptive%2520runtime%2520jointly%2520manage%2520the%2520full%2520stack%253A%2520orchestrating%2520workflow%250Acomponents%252C%2520mapping%2520them%2520to%2520models%2520and%2520hardware%252C%2520and%2520dynamically%2520reconfiguring%250Aexecution%2520to%2520satisfy%2520user-defined%2520SLOs.%2520By%2520exposing%2520the%2520internal%2520structure%2520of%250Aagentic%2520workflows%252C%2520Murakkab%2520enables%2520cross-layer%2520optimization%2520that%2520existing%250Aframeworks%2520and%2520cloud%2520schedulers%2520cannot%2520achieve.%250A%2520%2520Our%2520evaluation%2520on%2520diverse%2520workflows%2520shows%2520that%2520Murakkab%2520reduces%2520GPU%2520usage%2520by%250Aup%2520to%25202.8%2524%255Ctimes%2524%252C%2520energy%2520consumption%2520by%25203.7%2524%255Ctimes%2524%252C%2520and%2520cost%2520by%25204.3%2524%255Ctimes%2524%250Awhile%2520maintaining%2520SLOs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Murakkab%3A%20Resource-Efficient%20Agentic%20Workflow%20Orchestration%20in%20Cloud%0A%20%20Platforms&entry.906535625=Gohar%20Irfan%20Chaudhry%20and%20Esha%20Choukse%20and%20Haoran%20Qiu%20and%20%C3%8D%C3%B1igo%20Goiri%20and%20Rodrigo%20Fonseca%20and%20Adam%20Belay%20and%20Ricardo%20Bianchini&entry.1292438233=%20%20Agentic%20workflows%20commonly%20coordinate%20multiple%20models%20and%20tools%20with%20complex%0Acontrol%20logic.%20They%20are%20quickly%20becoming%20the%20dominant%20paradigm%20for%20AI%0Aapplications.%20However%2C%20serving%20them%20remains%20inefficient%20with%20today%27s%0Aframeworks.%20The%20key%20problem%20is%20that%20they%20expose%20workflows%20as%20opaque%20sequences%0Aof%20model%20and%20tool%20calls%20that%20tightly%20couple%20agent%20logic%20with%20model%20and%20hardware%0Achoices.%20Often%2C%20these%20workflow%20components%20are%20fragmented%20across%20different%0Aentities%2C%20preventing%20systems%20from%20reasoning%20about%20trade-offs%20across%20accuracy%2C%0Alatency%2C%20energy%2C%20and%20cost.%20This%20leads%20to%20resource%20waste%20and%20degraded%0Aservice-level%20objectives%20%28SLOs%29.%0A%20%20We%20present%20Murakkab%2C%20a%20resource-efficient%20serving%20system%20for%20agentic%0Aworkflows.%20Murakkab%20introduces%20a%20declarative%20abstraction%20that%20decouples%0Aworkflow%20specification%20from%20execution%20configuration.%20A%20profile-guided%20optimizer%0Aand%20adaptive%20runtime%20jointly%20manage%20the%20full%20stack%3A%20orchestrating%20workflow%0Acomponents%2C%20mapping%20them%20to%20models%20and%20hardware%2C%20and%20dynamically%20reconfiguring%0Aexecution%20to%20satisfy%20user-defined%20SLOs.%20By%20exposing%20the%20internal%20structure%20of%0Aagentic%20workflows%2C%20Murakkab%20enables%20cross-layer%20optimization%20that%20existing%0Aframeworks%20and%20cloud%20schedulers%20cannot%20achieve.%0A%20%20Our%20evaluation%20on%20diverse%20workflows%20shows%20that%20Murakkab%20reduces%20GPU%20usage%20by%0Aup%20to%202.8%24%5Ctimes%24%2C%20energy%20consumption%20by%203.7%24%5Ctimes%24%2C%20and%20cost%20by%204.3%24%5Ctimes%24%0Awhile%20maintaining%20SLOs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18298v2&entry.124074799=Read"},
{"title": "Estudio de la eficiencia en la escalabilidad de GPUs para el\n  entrenamiento de Inteligencia Artificial", "author": "David Cortes and Carlos Juiz and Belen Bermejo", "abstract": "  Training large-scale deep learning models has become a key challenge for the\nscientific community and industry. While the massive use of GPUs can\nsignificantly speed up training times, this approach has a negative impact on\nefficiency. In this article, we present a detailed analysis of the times\nreported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,\nRetinaNet, and Stable Diffusion, showing that there are configurations that\noptimise the relationship between performance, GPU usage, and efficiency. The\nresults point to a break-even point that allows training times to be reduced\nwhile maximising efficiency.\n", "link": "http://arxiv.org/abs/2509.03263v1", "date": "2025-09-03", "relevancy": 1.507, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5162}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4985}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estudio%20de%20la%20eficiencia%20en%20la%20escalabilidad%20de%20GPUs%20para%20el%0A%20%20entrenamiento%20de%20Inteligencia%20Artificial&body=Title%3A%20Estudio%20de%20la%20eficiencia%20en%20la%20escalabilidad%20de%20GPUs%20para%20el%0A%20%20entrenamiento%20de%20Inteligencia%20Artificial%0AAuthor%3A%20David%20Cortes%20and%20Carlos%20Juiz%20and%20Belen%20Bermejo%0AAbstract%3A%20%20%20Training%20large-scale%20deep%20learning%20models%20has%20become%20a%20key%20challenge%20for%20the%0Ascientific%20community%20and%20industry.%20While%20the%20massive%20use%20of%20GPUs%20can%0Asignificantly%20speed%20up%20training%20times%2C%20this%20approach%20has%20a%20negative%20impact%20on%0Aefficiency.%20In%20this%20article%2C%20we%20present%20a%20detailed%20analysis%20of%20the%20times%0Areported%20by%20MLPerf%20Training%20v4.1%20on%20four%20workloads%3A%20BERT%2C%20Llama2%20LoRA%2C%0ARetinaNet%2C%20and%20Stable%20Diffusion%2C%20showing%20that%20there%20are%20configurations%20that%0Aoptimise%20the%20relationship%20between%20performance%2C%20GPU%20usage%2C%20and%20efficiency.%20The%0Aresults%20point%20to%20a%20break-even%20point%20that%20allows%20training%20times%20to%20be%20reduced%0Awhile%20maximising%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstudio%2520de%2520la%2520eficiencia%2520en%2520la%2520escalabilidad%2520de%2520GPUs%2520para%2520el%250A%2520%2520entrenamiento%2520de%2520Inteligencia%2520Artificial%26entry.906535625%3DDavid%2520Cortes%2520and%2520Carlos%2520Juiz%2520and%2520Belen%2520Bermejo%26entry.1292438233%3D%2520%2520Training%2520large-scale%2520deep%2520learning%2520models%2520has%2520become%2520a%2520key%2520challenge%2520for%2520the%250Ascientific%2520community%2520and%2520industry.%2520While%2520the%2520massive%2520use%2520of%2520GPUs%2520can%250Asignificantly%2520speed%2520up%2520training%2520times%252C%2520this%2520approach%2520has%2520a%2520negative%2520impact%2520on%250Aefficiency.%2520In%2520this%2520article%252C%2520we%2520present%2520a%2520detailed%2520analysis%2520of%2520the%2520times%250Areported%2520by%2520MLPerf%2520Training%2520v4.1%2520on%2520four%2520workloads%253A%2520BERT%252C%2520Llama2%2520LoRA%252C%250ARetinaNet%252C%2520and%2520Stable%2520Diffusion%252C%2520showing%2520that%2520there%2520are%2520configurations%2520that%250Aoptimise%2520the%2520relationship%2520between%2520performance%252C%2520GPU%2520usage%252C%2520and%2520efficiency.%2520The%250Aresults%2520point%2520to%2520a%2520break-even%2520point%2520that%2520allows%2520training%2520times%2520to%2520be%2520reduced%250Awhile%2520maximising%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estudio%20de%20la%20eficiencia%20en%20la%20escalabilidad%20de%20GPUs%20para%20el%0A%20%20entrenamiento%20de%20Inteligencia%20Artificial&entry.906535625=David%20Cortes%20and%20Carlos%20Juiz%20and%20Belen%20Bermejo&entry.1292438233=%20%20Training%20large-scale%20deep%20learning%20models%20has%20become%20a%20key%20challenge%20for%20the%0Ascientific%20community%20and%20industry.%20While%20the%20massive%20use%20of%20GPUs%20can%0Asignificantly%20speed%20up%20training%20times%2C%20this%20approach%20has%20a%20negative%20impact%20on%0Aefficiency.%20In%20this%20article%2C%20we%20present%20a%20detailed%20analysis%20of%20the%20times%0Areported%20by%20MLPerf%20Training%20v4.1%20on%20four%20workloads%3A%20BERT%2C%20Llama2%20LoRA%2C%0ARetinaNet%2C%20and%20Stable%20Diffusion%2C%20showing%20that%20there%20are%20configurations%20that%0Aoptimise%20the%20relationship%20between%20performance%2C%20GPU%20usage%2C%20and%20efficiency.%20The%0Aresults%20point%20to%20a%20break-even%20point%20that%20allows%20training%20times%20to%20be%20reduced%0Awhile%20maximising%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03263v1&entry.124074799=Read"},
{"title": "WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada", "author": "Braeden Sherritt and Isar Nejadgholi and Efstratios Aivaliotis and Khaled Mslmani and Marzieh Amini", "abstract": "  Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. In this work, we focus on\nmultimodal wildfire social media data, which, although existing in current\ndatasets, is currently underrepresented in Canadian contexts. We present\nWildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian\nwildfires, annotated across twelve key themes. We evaluate zero-shot\nvision-language models on this dataset and compare their results with those of\ncustom-trained and baseline classifiers. We show that while baseline methods\nand zero-shot prompting offer quick deployment, custom-trained models\noutperform them when labelled data is available. Our best-performing custom\nmodel reaches 84.48% f-score, outperforming VLMs and baseline classifiers. We\nalso demonstrate how this model can be used to uncover trends during wildfires,\nthrough the collection and analysis of a large unlabeled dataset. Our dataset\nfacilitates future research in wildfire response, and our findings highlight\nthe importance of tailored datasets and task-specific training. Importantly,\nsuch datasets should be localized, as disaster response requirements vary\nacross regions and contexts.\n", "link": "http://arxiv.org/abs/2504.13231v3", "date": "2025-09-03", "relevancy": 1.3872, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4797}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WildFireCan-MMD%3A%20A%20Multimodal%20Dataset%20for%20Classification%20of%0A%20%20User-Generated%20Content%20During%20Wildfires%20in%20Canada&body=Title%3A%20WildFireCan-MMD%3A%20A%20Multimodal%20Dataset%20for%20Classification%20of%0A%20%20User-Generated%20Content%20During%20Wildfires%20in%20Canada%0AAuthor%3A%20Braeden%20Sherritt%20and%20Isar%20Nejadgholi%20and%20Efstratios%20Aivaliotis%20and%20Khaled%20Mslmani%20and%20Marzieh%20Amini%0AAbstract%3A%20%20%20Rapid%20information%20access%20is%20vital%20during%20wildfires%2C%20yet%20traditional%20data%0Asources%20are%20slow%20and%20costly.%20Social%20media%20offers%20real-time%20updates%2C%20but%0Aextracting%20relevant%20insights%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20focus%20on%0Amultimodal%20wildfire%20social%20media%20data%2C%20which%2C%20although%20existing%20in%20current%0Adatasets%2C%20is%20currently%20underrepresented%20in%20Canadian%20contexts.%20We%20present%0AWildFireCan-MMD%2C%20a%20new%20multimodal%20dataset%20of%20X%20posts%20from%20recent%20Canadian%0Awildfires%2C%20annotated%20across%20twelve%20key%20themes.%20We%20evaluate%20zero-shot%0Avision-language%20models%20on%20this%20dataset%20and%20compare%20their%20results%20with%20those%20of%0Acustom-trained%20and%20baseline%20classifiers.%20We%20show%20that%20while%20baseline%20methods%0Aand%20zero-shot%20prompting%20offer%20quick%20deployment%2C%20custom-trained%20models%0Aoutperform%20them%20when%20labelled%20data%20is%20available.%20Our%20best-performing%20custom%0Amodel%20reaches%2084.48%25%20f-score%2C%20outperforming%20VLMs%20and%20baseline%20classifiers.%20We%0Aalso%20demonstrate%20how%20this%20model%20can%20be%20used%20to%20uncover%20trends%20during%20wildfires%2C%0Athrough%20the%20collection%20and%20analysis%20of%20a%20large%20unlabeled%20dataset.%20Our%20dataset%0Afacilitates%20future%20research%20in%20wildfire%20response%2C%20and%20our%20findings%20highlight%0Athe%20importance%20of%20tailored%20datasets%20and%20task-specific%20training.%20Importantly%2C%0Asuch%20datasets%20should%20be%20localized%2C%20as%20disaster%20response%20requirements%20vary%0Aacross%20regions%20and%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13231v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildFireCan-MMD%253A%2520A%2520Multimodal%2520Dataset%2520for%2520Classification%2520of%250A%2520%2520User-Generated%2520Content%2520During%2520Wildfires%2520in%2520Canada%26entry.906535625%3DBraeden%2520Sherritt%2520and%2520Isar%2520Nejadgholi%2520and%2520Efstratios%2520Aivaliotis%2520and%2520Khaled%2520Mslmani%2520and%2520Marzieh%2520Amini%26entry.1292438233%3D%2520%2520Rapid%2520information%2520access%2520is%2520vital%2520during%2520wildfires%252C%2520yet%2520traditional%2520data%250Asources%2520are%2520slow%2520and%2520costly.%2520Social%2520media%2520offers%2520real-time%2520updates%252C%2520but%250Aextracting%2520relevant%2520insights%2520remains%2520a%2520challenge.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%250Amultimodal%2520wildfire%2520social%2520media%2520data%252C%2520which%252C%2520although%2520existing%2520in%2520current%250Adatasets%252C%2520is%2520currently%2520underrepresented%2520in%2520Canadian%2520contexts.%2520We%2520present%250AWildFireCan-MMD%252C%2520a%2520new%2520multimodal%2520dataset%2520of%2520X%2520posts%2520from%2520recent%2520Canadian%250Awildfires%252C%2520annotated%2520across%2520twelve%2520key%2520themes.%2520We%2520evaluate%2520zero-shot%250Avision-language%2520models%2520on%2520this%2520dataset%2520and%2520compare%2520their%2520results%2520with%2520those%2520of%250Acustom-trained%2520and%2520baseline%2520classifiers.%2520We%2520show%2520that%2520while%2520baseline%2520methods%250Aand%2520zero-shot%2520prompting%2520offer%2520quick%2520deployment%252C%2520custom-trained%2520models%250Aoutperform%2520them%2520when%2520labelled%2520data%2520is%2520available.%2520Our%2520best-performing%2520custom%250Amodel%2520reaches%252084.48%2525%2520f-score%252C%2520outperforming%2520VLMs%2520and%2520baseline%2520classifiers.%2520We%250Aalso%2520demonstrate%2520how%2520this%2520model%2520can%2520be%2520used%2520to%2520uncover%2520trends%2520during%2520wildfires%252C%250Athrough%2520the%2520collection%2520and%2520analysis%2520of%2520a%2520large%2520unlabeled%2520dataset.%2520Our%2520dataset%250Afacilitates%2520future%2520research%2520in%2520wildfire%2520response%252C%2520and%2520our%2520findings%2520highlight%250Athe%2520importance%2520of%2520tailored%2520datasets%2520and%2520task-specific%2520training.%2520Importantly%252C%250Asuch%2520datasets%2520should%2520be%2520localized%252C%2520as%2520disaster%2520response%2520requirements%2520vary%250Aacross%2520regions%2520and%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13231v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WildFireCan-MMD%3A%20A%20Multimodal%20Dataset%20for%20Classification%20of%0A%20%20User-Generated%20Content%20During%20Wildfires%20in%20Canada&entry.906535625=Braeden%20Sherritt%20and%20Isar%20Nejadgholi%20and%20Efstratios%20Aivaliotis%20and%20Khaled%20Mslmani%20and%20Marzieh%20Amini&entry.1292438233=%20%20Rapid%20information%20access%20is%20vital%20during%20wildfires%2C%20yet%20traditional%20data%0Asources%20are%20slow%20and%20costly.%20Social%20media%20offers%20real-time%20updates%2C%20but%0Aextracting%20relevant%20insights%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20focus%20on%0Amultimodal%20wildfire%20social%20media%20data%2C%20which%2C%20although%20existing%20in%20current%0Adatasets%2C%20is%20currently%20underrepresented%20in%20Canadian%20contexts.%20We%20present%0AWildFireCan-MMD%2C%20a%20new%20multimodal%20dataset%20of%20X%20posts%20from%20recent%20Canadian%0Awildfires%2C%20annotated%20across%20twelve%20key%20themes.%20We%20evaluate%20zero-shot%0Avision-language%20models%20on%20this%20dataset%20and%20compare%20their%20results%20with%20those%20of%0Acustom-trained%20and%20baseline%20classifiers.%20We%20show%20that%20while%20baseline%20methods%0Aand%20zero-shot%20prompting%20offer%20quick%20deployment%2C%20custom-trained%20models%0Aoutperform%20them%20when%20labelled%20data%20is%20available.%20Our%20best-performing%20custom%0Amodel%20reaches%2084.48%25%20f-score%2C%20outperforming%20VLMs%20and%20baseline%20classifiers.%20We%0Aalso%20demonstrate%20how%20this%20model%20can%20be%20used%20to%20uncover%20trends%20during%20wildfires%2C%0Athrough%20the%20collection%20and%20analysis%20of%20a%20large%20unlabeled%20dataset.%20Our%20dataset%0Afacilitates%20future%20research%20in%20wildfire%20response%2C%20and%20our%20findings%20highlight%0Athe%20importance%20of%20tailored%20datasets%20and%20task-specific%20training.%20Importantly%2C%0Asuch%20datasets%20should%20be%20localized%2C%20as%20disaster%20response%20requirements%20vary%0Aacross%20regions%20and%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13231v3&entry.124074799=Read"},
{"title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving", "author": "Ruoyu Qin and Zheming Li and Weiran He and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu", "abstract": "  Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.\n", "link": "http://arxiv.org/abs/2407.00079v4", "date": "2025-09-03", "relevancy": 1.6502, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4184}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4166}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mooncake%3A%20A%20KVCache-centric%20Disaggregated%20Architecture%20for%20LLM%20Serving&body=Title%3A%20Mooncake%3A%20A%20KVCache-centric%20Disaggregated%20Architecture%20for%20LLM%20Serving%0AAuthor%3A%20Ruoyu%20Qin%20and%20Zheming%20Li%20and%20Weiran%20He%20and%20Mingxing%20Zhang%20and%20Yongwei%20Wu%20and%20Weimin%20Zheng%20and%20Xinran%20Xu%0AAbstract%3A%20%20%20Mooncake%20is%20the%20serving%20platform%20for%20Kimi%2C%20a%20leading%20LLM%20service%20provided%20by%0AMoonshot%20AI.%20It%20features%20a%20KVCache-centric%20disaggregated%20architecture%20that%0Aseparates%20the%20prefill%20and%20decoding%20clusters.%20It%20also%20leverages%20the%0Aunderutilized%20CPU%2C%20DRAM%2C%20and%20SSD%20resources%20of%20the%20GPU%20cluster%20to%20implement%20a%0Adisaggregated%20cache%20of%20KVCache.%20The%20core%20of%20Mooncake%20is%20its%20KVCache-centric%0Ascheduler%2C%20which%20balances%20maximizing%20overall%20effective%20throughput%20while%20meeting%0Alatency-related%20Service%20Level%20Objectives%20%28SLOs%29.%20Unlike%20traditional%20studies%0Athat%20assume%20all%20requests%20will%20be%20processed%2C%20Mooncake%20faces%20challenges%20due%20to%0Ahighly%20overloaded%20scenarios.%20To%20mitigate%20these%2C%20we%20developed%20a%20prediction-based%0Aearly%20rejection%20policy.%20Experiments%20show%20that%20Mooncake%20excels%20in%20long-context%0Ascenarios.%20Compared%20to%20the%20baseline%20method%2C%20Mooncake%20can%20achieve%20up%20to%20a%20525%25%0Aincrease%20in%20throughput%20in%20certain%20simulated%20scenarios%20while%20adhering%20to%20SLOs.%0AUnder%20real%20workloads%2C%20Mooncake%27s%20innovative%20architecture%20enables%20Kimi%20to%20handle%0A75%25%20more%20requests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00079v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMooncake%253A%2520A%2520KVCache-centric%2520Disaggregated%2520Architecture%2520for%2520LLM%2520Serving%26entry.906535625%3DRuoyu%2520Qin%2520and%2520Zheming%2520Li%2520and%2520Weiran%2520He%2520and%2520Mingxing%2520Zhang%2520and%2520Yongwei%2520Wu%2520and%2520Weimin%2520Zheng%2520and%2520Xinran%2520Xu%26entry.1292438233%3D%2520%2520Mooncake%2520is%2520the%2520serving%2520platform%2520for%2520Kimi%252C%2520a%2520leading%2520LLM%2520service%2520provided%2520by%250AMoonshot%2520AI.%2520It%2520features%2520a%2520KVCache-centric%2520disaggregated%2520architecture%2520that%250Aseparates%2520the%2520prefill%2520and%2520decoding%2520clusters.%2520It%2520also%2520leverages%2520the%250Aunderutilized%2520CPU%252C%2520DRAM%252C%2520and%2520SSD%2520resources%2520of%2520the%2520GPU%2520cluster%2520to%2520implement%2520a%250Adisaggregated%2520cache%2520of%2520KVCache.%2520The%2520core%2520of%2520Mooncake%2520is%2520its%2520KVCache-centric%250Ascheduler%252C%2520which%2520balances%2520maximizing%2520overall%2520effective%2520throughput%2520while%2520meeting%250Alatency-related%2520Service%2520Level%2520Objectives%2520%2528SLOs%2529.%2520Unlike%2520traditional%2520studies%250Athat%2520assume%2520all%2520requests%2520will%2520be%2520processed%252C%2520Mooncake%2520faces%2520challenges%2520due%2520to%250Ahighly%2520overloaded%2520scenarios.%2520To%2520mitigate%2520these%252C%2520we%2520developed%2520a%2520prediction-based%250Aearly%2520rejection%2520policy.%2520Experiments%2520show%2520that%2520Mooncake%2520excels%2520in%2520long-context%250Ascenarios.%2520Compared%2520to%2520the%2520baseline%2520method%252C%2520Mooncake%2520can%2520achieve%2520up%2520to%2520a%2520525%2525%250Aincrease%2520in%2520throughput%2520in%2520certain%2520simulated%2520scenarios%2520while%2520adhering%2520to%2520SLOs.%250AUnder%2520real%2520workloads%252C%2520Mooncake%2527s%2520innovative%2520architecture%2520enables%2520Kimi%2520to%2520handle%250A75%2525%2520more%2520requests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00079v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mooncake%3A%20A%20KVCache-centric%20Disaggregated%20Architecture%20for%20LLM%20Serving&entry.906535625=Ruoyu%20Qin%20and%20Zheming%20Li%20and%20Weiran%20He%20and%20Mingxing%20Zhang%20and%20Yongwei%20Wu%20and%20Weimin%20Zheng%20and%20Xinran%20Xu&entry.1292438233=%20%20Mooncake%20is%20the%20serving%20platform%20for%20Kimi%2C%20a%20leading%20LLM%20service%20provided%20by%0AMoonshot%20AI.%20It%20features%20a%20KVCache-centric%20disaggregated%20architecture%20that%0Aseparates%20the%20prefill%20and%20decoding%20clusters.%20It%20also%20leverages%20the%0Aunderutilized%20CPU%2C%20DRAM%2C%20and%20SSD%20resources%20of%20the%20GPU%20cluster%20to%20implement%20a%0Adisaggregated%20cache%20of%20KVCache.%20The%20core%20of%20Mooncake%20is%20its%20KVCache-centric%0Ascheduler%2C%20which%20balances%20maximizing%20overall%20effective%20throughput%20while%20meeting%0Alatency-related%20Service%20Level%20Objectives%20%28SLOs%29.%20Unlike%20traditional%20studies%0Athat%20assume%20all%20requests%20will%20be%20processed%2C%20Mooncake%20faces%20challenges%20due%20to%0Ahighly%20overloaded%20scenarios.%20To%20mitigate%20these%2C%20we%20developed%20a%20prediction-based%0Aearly%20rejection%20policy.%20Experiments%20show%20that%20Mooncake%20excels%20in%20long-context%0Ascenarios.%20Compared%20to%20the%20baseline%20method%2C%20Mooncake%20can%20achieve%20up%20to%20a%20525%25%0Aincrease%20in%20throughput%20in%20certain%20simulated%20scenarios%20while%20adhering%20to%20SLOs.%0AUnder%20real%20workloads%2C%20Mooncake%27s%20innovative%20architecture%20enables%20Kimi%20to%20handle%0A75%25%20more%20requests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00079v4&entry.124074799=Read"},
{"title": "Improving Bayesian Optimization for Portfolio Management with an\n  Adaptive Scheduling", "author": "Zinuo You and John Cartlidge and Karen Elliott and Menghan Ge and Daniel Gold", "abstract": "  Existing black-box portfolio management systems are prevalent in the\nfinancial industry due to commercial and safety constraints, though their\nperformance can fluctuate dramatically with changing market regimes. Evaluating\nthese non-transparent systems is computationally expensive, as fixed budgets\nlimit the number of possible observations. Therefore, achieving stable and\nsample-efficient optimization for these systems has become a critical\nchallenge. This work presents a novel Bayesian optimization framework (TPE-AS)\nthat improves search stability and efficiency for black-box portfolio models\nunder these limited observation budgets. Standard Bayesian optimization, which\nsolely maximizes expected return, can yield erratic search trajectories and\nmisalign the surrogate model with the true objective, thereby wasting the\nlimited evaluation budget. To mitigate these issues, we propose a weighted\nLagrangian estimator that leverages an adaptive schedule and importance\nsampling. This estimator dynamically balances exploration and exploitation by\nincorporating both the maximization of model performance and the minimization\nof the variance of model observations. It guides the search from broad,\nperformance-seeking exploration towards stable and desirable regions as the\noptimization progresses. Extensive experiments and ablation studies, which\nestablish our proposed method as the primary approach and other configurations\nas baselines, demonstrate its effectiveness across four backtest settings with\nthree distinct black-box portfolio management models.\n", "link": "http://arxiv.org/abs/2504.13529v2", "date": "2025-09-03", "relevancy": 1.38, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4556}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Bayesian%20Optimization%20for%20Portfolio%20Management%20with%20an%0A%20%20Adaptive%20Scheduling&body=Title%3A%20Improving%20Bayesian%20Optimization%20for%20Portfolio%20Management%20with%20an%0A%20%20Adaptive%20Scheduling%0AAuthor%3A%20Zinuo%20You%20and%20John%20Cartlidge%20and%20Karen%20Elliott%20and%20Menghan%20Ge%20and%20Daniel%20Gold%0AAbstract%3A%20%20%20Existing%20black-box%20portfolio%20management%20systems%20are%20prevalent%20in%20the%0Afinancial%20industry%20due%20to%20commercial%20and%20safety%20constraints%2C%20though%20their%0Aperformance%20can%20fluctuate%20dramatically%20with%20changing%20market%20regimes.%20Evaluating%0Athese%20non-transparent%20systems%20is%20computationally%20expensive%2C%20as%20fixed%20budgets%0Alimit%20the%20number%20of%20possible%20observations.%20Therefore%2C%20achieving%20stable%20and%0Asample-efficient%20optimization%20for%20these%20systems%20has%20become%20a%20critical%0Achallenge.%20This%20work%20presents%20a%20novel%20Bayesian%20optimization%20framework%20%28TPE-AS%29%0Athat%20improves%20search%20stability%20and%20efficiency%20for%20black-box%20portfolio%20models%0Aunder%20these%20limited%20observation%20budgets.%20Standard%20Bayesian%20optimization%2C%20which%0Asolely%20maximizes%20expected%20return%2C%20can%20yield%20erratic%20search%20trajectories%20and%0Amisalign%20the%20surrogate%20model%20with%20the%20true%20objective%2C%20thereby%20wasting%20the%0Alimited%20evaluation%20budget.%20To%20mitigate%20these%20issues%2C%20we%20propose%20a%20weighted%0ALagrangian%20estimator%20that%20leverages%20an%20adaptive%20schedule%20and%20importance%0Asampling.%20This%20estimator%20dynamically%20balances%20exploration%20and%20exploitation%20by%0Aincorporating%20both%20the%20maximization%20of%20model%20performance%20and%20the%20minimization%0Aof%20the%20variance%20of%20model%20observations.%20It%20guides%20the%20search%20from%20broad%2C%0Aperformance-seeking%20exploration%20towards%20stable%20and%20desirable%20regions%20as%20the%0Aoptimization%20progresses.%20Extensive%20experiments%20and%20ablation%20studies%2C%20which%0Aestablish%20our%20proposed%20method%20as%20the%20primary%20approach%20and%20other%20configurations%0Aas%20baselines%2C%20demonstrate%20its%20effectiveness%20across%20four%20backtest%20settings%20with%0Athree%20distinct%20black-box%20portfolio%20management%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Bayesian%2520Optimization%2520for%2520Portfolio%2520Management%2520with%2520an%250A%2520%2520Adaptive%2520Scheduling%26entry.906535625%3DZinuo%2520You%2520and%2520John%2520Cartlidge%2520and%2520Karen%2520Elliott%2520and%2520Menghan%2520Ge%2520and%2520Daniel%2520Gold%26entry.1292438233%3D%2520%2520Existing%2520black-box%2520portfolio%2520management%2520systems%2520are%2520prevalent%2520in%2520the%250Afinancial%2520industry%2520due%2520to%2520commercial%2520and%2520safety%2520constraints%252C%2520though%2520their%250Aperformance%2520can%2520fluctuate%2520dramatically%2520with%2520changing%2520market%2520regimes.%2520Evaluating%250Athese%2520non-transparent%2520systems%2520is%2520computationally%2520expensive%252C%2520as%2520fixed%2520budgets%250Alimit%2520the%2520number%2520of%2520possible%2520observations.%2520Therefore%252C%2520achieving%2520stable%2520and%250Asample-efficient%2520optimization%2520for%2520these%2520systems%2520has%2520become%2520a%2520critical%250Achallenge.%2520This%2520work%2520presents%2520a%2520novel%2520Bayesian%2520optimization%2520framework%2520%2528TPE-AS%2529%250Athat%2520improves%2520search%2520stability%2520and%2520efficiency%2520for%2520black-box%2520portfolio%2520models%250Aunder%2520these%2520limited%2520observation%2520budgets.%2520Standard%2520Bayesian%2520optimization%252C%2520which%250Asolely%2520maximizes%2520expected%2520return%252C%2520can%2520yield%2520erratic%2520search%2520trajectories%2520and%250Amisalign%2520the%2520surrogate%2520model%2520with%2520the%2520true%2520objective%252C%2520thereby%2520wasting%2520the%250Alimited%2520evaluation%2520budget.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520propose%2520a%2520weighted%250ALagrangian%2520estimator%2520that%2520leverages%2520an%2520adaptive%2520schedule%2520and%2520importance%250Asampling.%2520This%2520estimator%2520dynamically%2520balances%2520exploration%2520and%2520exploitation%2520by%250Aincorporating%2520both%2520the%2520maximization%2520of%2520model%2520performance%2520and%2520the%2520minimization%250Aof%2520the%2520variance%2520of%2520model%2520observations.%2520It%2520guides%2520the%2520search%2520from%2520broad%252C%250Aperformance-seeking%2520exploration%2520towards%2520stable%2520and%2520desirable%2520regions%2520as%2520the%250Aoptimization%2520progresses.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%252C%2520which%250Aestablish%2520our%2520proposed%2520method%2520as%2520the%2520primary%2520approach%2520and%2520other%2520configurations%250Aas%2520baselines%252C%2520demonstrate%2520its%2520effectiveness%2520across%2520four%2520backtest%2520settings%2520with%250Athree%2520distinct%2520black-box%2520portfolio%2520management%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Bayesian%20Optimization%20for%20Portfolio%20Management%20with%20an%0A%20%20Adaptive%20Scheduling&entry.906535625=Zinuo%20You%20and%20John%20Cartlidge%20and%20Karen%20Elliott%20and%20Menghan%20Ge%20and%20Daniel%20Gold&entry.1292438233=%20%20Existing%20black-box%20portfolio%20management%20systems%20are%20prevalent%20in%20the%0Afinancial%20industry%20due%20to%20commercial%20and%20safety%20constraints%2C%20though%20their%0Aperformance%20can%20fluctuate%20dramatically%20with%20changing%20market%20regimes.%20Evaluating%0Athese%20non-transparent%20systems%20is%20computationally%20expensive%2C%20as%20fixed%20budgets%0Alimit%20the%20number%20of%20possible%20observations.%20Therefore%2C%20achieving%20stable%20and%0Asample-efficient%20optimization%20for%20these%20systems%20has%20become%20a%20critical%0Achallenge.%20This%20work%20presents%20a%20novel%20Bayesian%20optimization%20framework%20%28TPE-AS%29%0Athat%20improves%20search%20stability%20and%20efficiency%20for%20black-box%20portfolio%20models%0Aunder%20these%20limited%20observation%20budgets.%20Standard%20Bayesian%20optimization%2C%20which%0Asolely%20maximizes%20expected%20return%2C%20can%20yield%20erratic%20search%20trajectories%20and%0Amisalign%20the%20surrogate%20model%20with%20the%20true%20objective%2C%20thereby%20wasting%20the%0Alimited%20evaluation%20budget.%20To%20mitigate%20these%20issues%2C%20we%20propose%20a%20weighted%0ALagrangian%20estimator%20that%20leverages%20an%20adaptive%20schedule%20and%20importance%0Asampling.%20This%20estimator%20dynamically%20balances%20exploration%20and%20exploitation%20by%0Aincorporating%20both%20the%20maximization%20of%20model%20performance%20and%20the%20minimization%0Aof%20the%20variance%20of%20model%20observations.%20It%20guides%20the%20search%20from%20broad%2C%0Aperformance-seeking%20exploration%20towards%20stable%20and%20desirable%20regions%20as%20the%0Aoptimization%20progresses.%20Extensive%20experiments%20and%20ablation%20studies%2C%20which%0Aestablish%20our%20proposed%20method%20as%20the%20primary%20approach%20and%20other%20configurations%0Aas%20baselines%2C%20demonstrate%20its%20effectiveness%20across%20four%20backtest%20settings%20with%0Athree%20distinct%20black-box%20portfolio%20management%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13529v2&entry.124074799=Read"},
{"title": "Non-Linear Counterfactual Aggregate Optimization", "author": "Benjamin Heymann and Otmane Sakhi", "abstract": "  We consider the problem of directly optimizing a non-linear function of an\noutcome, where this outcome itself is the sum of many small contributions. The\nnon-linearity of the function means that the problem is not equivalent to the\nmaximization of the expectation of the individual contribution. By leveraging\nthe concentration properties of the sum of individual outcomes, we derive a\nscalable descent algorithm that directly optimizes for our stated objective.\nThis allows for instance to maximize the probability of successful A/B test,\nfor which it can be wiser to target a success criterion, such as exceeding a\ngiven uplift, rather than chasing the highest expected payoff.\n", "link": "http://arxiv.org/abs/2509.03438v1", "date": "2025-09-03", "relevancy": 1.3521, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4703}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Linear%20Counterfactual%20Aggregate%20Optimization&body=Title%3A%20Non-Linear%20Counterfactual%20Aggregate%20Optimization%0AAuthor%3A%20Benjamin%20Heymann%20and%20Otmane%20Sakhi%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20directly%20optimizing%20a%20non-linear%20function%20of%20an%0Aoutcome%2C%20where%20this%20outcome%20itself%20is%20the%20sum%20of%20many%20small%20contributions.%20The%0Anon-linearity%20of%20the%20function%20means%20that%20the%20problem%20is%20not%20equivalent%20to%20the%0Amaximization%20of%20the%20expectation%20of%20the%20individual%20contribution.%20By%20leveraging%0Athe%20concentration%20properties%20of%20the%20sum%20of%20individual%20outcomes%2C%20we%20derive%20a%0Ascalable%20descent%20algorithm%20that%20directly%20optimizes%20for%20our%20stated%20objective.%0AThis%20allows%20for%20instance%20to%20maximize%20the%20probability%20of%20successful%20A/B%20test%2C%0Afor%20which%20it%20can%20be%20wiser%20to%20target%20a%20success%20criterion%2C%20such%20as%20exceeding%20a%0Agiven%20uplift%2C%20rather%20than%20chasing%20the%20highest%20expected%20payoff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Linear%2520Counterfactual%2520Aggregate%2520Optimization%26entry.906535625%3DBenjamin%2520Heymann%2520and%2520Otmane%2520Sakhi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520directly%2520optimizing%2520a%2520non-linear%2520function%2520of%2520an%250Aoutcome%252C%2520where%2520this%2520outcome%2520itself%2520is%2520the%2520sum%2520of%2520many%2520small%2520contributions.%2520The%250Anon-linearity%2520of%2520the%2520function%2520means%2520that%2520the%2520problem%2520is%2520not%2520equivalent%2520to%2520the%250Amaximization%2520of%2520the%2520expectation%2520of%2520the%2520individual%2520contribution.%2520By%2520leveraging%250Athe%2520concentration%2520properties%2520of%2520the%2520sum%2520of%2520individual%2520outcomes%252C%2520we%2520derive%2520a%250Ascalable%2520descent%2520algorithm%2520that%2520directly%2520optimizes%2520for%2520our%2520stated%2520objective.%250AThis%2520allows%2520for%2520instance%2520to%2520maximize%2520the%2520probability%2520of%2520successful%2520A/B%2520test%252C%250Afor%2520which%2520it%2520can%2520be%2520wiser%2520to%2520target%2520a%2520success%2520criterion%252C%2520such%2520as%2520exceeding%2520a%250Agiven%2520uplift%252C%2520rather%2520than%2520chasing%2520the%2520highest%2520expected%2520payoff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Linear%20Counterfactual%20Aggregate%20Optimization&entry.906535625=Benjamin%20Heymann%20and%20Otmane%20Sakhi&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20directly%20optimizing%20a%20non-linear%20function%20of%20an%0Aoutcome%2C%20where%20this%20outcome%20itself%20is%20the%20sum%20of%20many%20small%20contributions.%20The%0Anon-linearity%20of%20the%20function%20means%20that%20the%20problem%20is%20not%20equivalent%20to%20the%0Amaximization%20of%20the%20expectation%20of%20the%20individual%20contribution.%20By%20leveraging%0Athe%20concentration%20properties%20of%20the%20sum%20of%20individual%20outcomes%2C%20we%20derive%20a%0Ascalable%20descent%20algorithm%20that%20directly%20optimizes%20for%20our%20stated%20objective.%0AThis%20allows%20for%20instance%20to%20maximize%20the%20probability%20of%20successful%20A/B%20test%2C%0Afor%20which%20it%20can%20be%20wiser%20to%20target%20a%20success%20criterion%2C%20such%20as%20exceeding%20a%0Agiven%20uplift%2C%20rather%20than%20chasing%20the%20highest%20expected%20payoff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03438v1&entry.124074799=Read"},
{"title": "Comparing Next-Day Wildfire Predictability of MODIS and VIIRS Satellite\n  Data", "author": "Justus Karlsson and Yonghao Xu and Amanda Berg and Leif Haglund", "abstract": "  Multiple studies have performed next-day fire prediction using satellite\nimagery. Two main satellites are used to detect wildfires: MODIS and VIIRS.\nBoth satellites provide fire mask products, called MOD14 and VNP14,\nrespectively. Studies have used one or the other, but there has been no\ncomparison between them to determine which might be more suitable for next-day\nfire prediction. In this paper, we first evaluate how well VIIRS and MODIS data\ncan be used to forecast wildfire spread one day ahead. We find that the model\nusing VIIRS as input and VNP14 as target achieves the best results.\nInterestingly, the model using MODIS as input and VNP14 as target performs\nsignificantly better than using VNP14 as input and MOD14 as target. Next, we\ndiscuss why MOD14 might be harder to use for predicting next-day fires. We find\nthat the MOD14 fire mask is highly stochastic and does not correlate with\nreasonable fire spread patterns. This is detrimental for machine learning\ntasks, as the model learns irrational patterns. Therefore, we conclude that\nMOD14 is unsuitable for next-day fire prediction and that VNP14 is a much\nbetter option. However, using MODIS input and VNP14 as target, we achieve a\nsignificant improvement in predictability. This indicates that an improved fire\ndetection model is possible for MODIS. The full code and dataset is available\nonline: https://github.com/justuskarlsson/wildfire-mod14-vnp14\n", "link": "http://arxiv.org/abs/2503.08580v4", "date": "2025-09-03", "relevancy": 0.8404, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4329}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4236}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Next-Day%20Wildfire%20Predictability%20of%20MODIS%20and%20VIIRS%20Satellite%0A%20%20Data&body=Title%3A%20Comparing%20Next-Day%20Wildfire%20Predictability%20of%20MODIS%20and%20VIIRS%20Satellite%0A%20%20Data%0AAuthor%3A%20Justus%20Karlsson%20and%20Yonghao%20Xu%20and%20Amanda%20Berg%20and%20Leif%20Haglund%0AAbstract%3A%20%20%20Multiple%20studies%20have%20performed%20next-day%20fire%20prediction%20using%20satellite%0Aimagery.%20Two%20main%20satellites%20are%20used%20to%20detect%20wildfires%3A%20MODIS%20and%20VIIRS.%0ABoth%20satellites%20provide%20fire%20mask%20products%2C%20called%20MOD14%20and%20VNP14%2C%0Arespectively.%20Studies%20have%20used%20one%20or%20the%20other%2C%20but%20there%20has%20been%20no%0Acomparison%20between%20them%20to%20determine%20which%20might%20be%20more%20suitable%20for%20next-day%0Afire%20prediction.%20In%20this%20paper%2C%20we%20first%20evaluate%20how%20well%20VIIRS%20and%20MODIS%20data%0Acan%20be%20used%20to%20forecast%20wildfire%20spread%20one%20day%20ahead.%20We%20find%20that%20the%20model%0Ausing%20VIIRS%20as%20input%20and%20VNP14%20as%20target%20achieves%20the%20best%20results.%0AInterestingly%2C%20the%20model%20using%20MODIS%20as%20input%20and%20VNP14%20as%20target%20performs%0Asignificantly%20better%20than%20using%20VNP14%20as%20input%20and%20MOD14%20as%20target.%20Next%2C%20we%0Adiscuss%20why%20MOD14%20might%20be%20harder%20to%20use%20for%20predicting%20next-day%20fires.%20We%20find%0Athat%20the%20MOD14%20fire%20mask%20is%20highly%20stochastic%20and%20does%20not%20correlate%20with%0Areasonable%20fire%20spread%20patterns.%20This%20is%20detrimental%20for%20machine%20learning%0Atasks%2C%20as%20the%20model%20learns%20irrational%20patterns.%20Therefore%2C%20we%20conclude%20that%0AMOD14%20is%20unsuitable%20for%20next-day%20fire%20prediction%20and%20that%20VNP14%20is%20a%20much%0Abetter%20option.%20However%2C%20using%20MODIS%20input%20and%20VNP14%20as%20target%2C%20we%20achieve%20a%0Asignificant%20improvement%20in%20predictability.%20This%20indicates%20that%20an%20improved%20fire%0Adetection%20model%20is%20possible%20for%20MODIS.%20The%20full%20code%20and%20dataset%20is%20available%0Aonline%3A%20https%3A//github.com/justuskarlsson/wildfire-mod14-vnp14%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08580v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Next-Day%2520Wildfire%2520Predictability%2520of%2520MODIS%2520and%2520VIIRS%2520Satellite%250A%2520%2520Data%26entry.906535625%3DJustus%2520Karlsson%2520and%2520Yonghao%2520Xu%2520and%2520Amanda%2520Berg%2520and%2520Leif%2520Haglund%26entry.1292438233%3D%2520%2520Multiple%2520studies%2520have%2520performed%2520next-day%2520fire%2520prediction%2520using%2520satellite%250Aimagery.%2520Two%2520main%2520satellites%2520are%2520used%2520to%2520detect%2520wildfires%253A%2520MODIS%2520and%2520VIIRS.%250ABoth%2520satellites%2520provide%2520fire%2520mask%2520products%252C%2520called%2520MOD14%2520and%2520VNP14%252C%250Arespectively.%2520Studies%2520have%2520used%2520one%2520or%2520the%2520other%252C%2520but%2520there%2520has%2520been%2520no%250Acomparison%2520between%2520them%2520to%2520determine%2520which%2520might%2520be%2520more%2520suitable%2520for%2520next-day%250Afire%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520first%2520evaluate%2520how%2520well%2520VIIRS%2520and%2520MODIS%2520data%250Acan%2520be%2520used%2520to%2520forecast%2520wildfire%2520spread%2520one%2520day%2520ahead.%2520We%2520find%2520that%2520the%2520model%250Ausing%2520VIIRS%2520as%2520input%2520and%2520VNP14%2520as%2520target%2520achieves%2520the%2520best%2520results.%250AInterestingly%252C%2520the%2520model%2520using%2520MODIS%2520as%2520input%2520and%2520VNP14%2520as%2520target%2520performs%250Asignificantly%2520better%2520than%2520using%2520VNP14%2520as%2520input%2520and%2520MOD14%2520as%2520target.%2520Next%252C%2520we%250Adiscuss%2520why%2520MOD14%2520might%2520be%2520harder%2520to%2520use%2520for%2520predicting%2520next-day%2520fires.%2520We%2520find%250Athat%2520the%2520MOD14%2520fire%2520mask%2520is%2520highly%2520stochastic%2520and%2520does%2520not%2520correlate%2520with%250Areasonable%2520fire%2520spread%2520patterns.%2520This%2520is%2520detrimental%2520for%2520machine%2520learning%250Atasks%252C%2520as%2520the%2520model%2520learns%2520irrational%2520patterns.%2520Therefore%252C%2520we%2520conclude%2520that%250AMOD14%2520is%2520unsuitable%2520for%2520next-day%2520fire%2520prediction%2520and%2520that%2520VNP14%2520is%2520a%2520much%250Abetter%2520option.%2520However%252C%2520using%2520MODIS%2520input%2520and%2520VNP14%2520as%2520target%252C%2520we%2520achieve%2520a%250Asignificant%2520improvement%2520in%2520predictability.%2520This%2520indicates%2520that%2520an%2520improved%2520fire%250Adetection%2520model%2520is%2520possible%2520for%2520MODIS.%2520The%2520full%2520code%2520and%2520dataset%2520is%2520available%250Aonline%253A%2520https%253A//github.com/justuskarlsson/wildfire-mod14-vnp14%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08580v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Next-Day%20Wildfire%20Predictability%20of%20MODIS%20and%20VIIRS%20Satellite%0A%20%20Data&entry.906535625=Justus%20Karlsson%20and%20Yonghao%20Xu%20and%20Amanda%20Berg%20and%20Leif%20Haglund&entry.1292438233=%20%20Multiple%20studies%20have%20performed%20next-day%20fire%20prediction%20using%20satellite%0Aimagery.%20Two%20main%20satellites%20are%20used%20to%20detect%20wildfires%3A%20MODIS%20and%20VIIRS.%0ABoth%20satellites%20provide%20fire%20mask%20products%2C%20called%20MOD14%20and%20VNP14%2C%0Arespectively.%20Studies%20have%20used%20one%20or%20the%20other%2C%20but%20there%20has%20been%20no%0Acomparison%20between%20them%20to%20determine%20which%20might%20be%20more%20suitable%20for%20next-day%0Afire%20prediction.%20In%20this%20paper%2C%20we%20first%20evaluate%20how%20well%20VIIRS%20and%20MODIS%20data%0Acan%20be%20used%20to%20forecast%20wildfire%20spread%20one%20day%20ahead.%20We%20find%20that%20the%20model%0Ausing%20VIIRS%20as%20input%20and%20VNP14%20as%20target%20achieves%20the%20best%20results.%0AInterestingly%2C%20the%20model%20using%20MODIS%20as%20input%20and%20VNP14%20as%20target%20performs%0Asignificantly%20better%20than%20using%20VNP14%20as%20input%20and%20MOD14%20as%20target.%20Next%2C%20we%0Adiscuss%20why%20MOD14%20might%20be%20harder%20to%20use%20for%20predicting%20next-day%20fires.%20We%20find%0Athat%20the%20MOD14%20fire%20mask%20is%20highly%20stochastic%20and%20does%20not%20correlate%20with%0Areasonable%20fire%20spread%20patterns.%20This%20is%20detrimental%20for%20machine%20learning%0Atasks%2C%20as%20the%20model%20learns%20irrational%20patterns.%20Therefore%2C%20we%20conclude%20that%0AMOD14%20is%20unsuitable%20for%20next-day%20fire%20prediction%20and%20that%20VNP14%20is%20a%20much%0Abetter%20option.%20However%2C%20using%20MODIS%20input%20and%20VNP14%20as%20target%2C%20we%20achieve%20a%0Asignificant%20improvement%20in%20predictability.%20This%20indicates%20that%20an%20improved%20fire%0Adetection%20model%20is%20possible%20for%20MODIS.%20The%20full%20code%20and%20dataset%20is%20available%0Aonline%3A%20https%3A//github.com/justuskarlsson/wildfire-mod14-vnp14%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08580v4&entry.124074799=Read"},
{"title": "Human Preference-Aligned Concept Customization Benchmark via Decomposed\n  Evaluation", "author": "Reina Ishikawa and Ryo Fujii and Hideo Saito and Ryo Hachiuma", "abstract": "  Evaluating concept customization is challenging, as it requires a\ncomprehensive assessment of fidelity to generative prompts and concept images.\nMoreover, evaluating multiple concepts is considerably more difficult than\nevaluating a single concept, as it demands detailed assessment not only for\neach individual concept but also for the interactions among concepts. While\nhumans can intuitively assess generated images, existing metrics often provide\neither overly narrow or overly generalized evaluations, resulting in\nmisalignment with human preference. To address this, we propose Decomposed GPT\nScore (D-GPTScore), a novel human-aligned evaluation method that decomposes\nevaluation criteria into finer aspects and incorporates aspect-wise assessments\nusing Multimodal Large Language Model (MLLM). Additionally, we release Human\nPreference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark\ndataset containing both single- and multi-concept tasks, enabling stage-wise\nevaluation across a wide difficulty range -- from individual actions to\nmulti-person interactions. Our method significantly outperforms existing\napproaches on this benchmark, exhibiting higher correlation with human\npreferences. This work establishes a new standard for evaluating concept\ncustomization and highlights key challenges for future research. The benchmark\nand associated materials are available at\nhttps://github.com/ReinaIshikawa/D-GPTScore.\n", "link": "http://arxiv.org/abs/2509.03385v1", "date": "2025-09-03", "relevancy": 1.0179, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5224}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5188}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Preference-Aligned%20Concept%20Customization%20Benchmark%20via%20Decomposed%0A%20%20Evaluation&body=Title%3A%20Human%20Preference-Aligned%20Concept%20Customization%20Benchmark%20via%20Decomposed%0A%20%20Evaluation%0AAuthor%3A%20Reina%20Ishikawa%20and%20Ryo%20Fujii%20and%20Hideo%20Saito%20and%20Ryo%20Hachiuma%0AAbstract%3A%20%20%20Evaluating%20concept%20customization%20is%20challenging%2C%20as%20it%20requires%20a%0Acomprehensive%20assessment%20of%20fidelity%20to%20generative%20prompts%20and%20concept%20images.%0AMoreover%2C%20evaluating%20multiple%20concepts%20is%20considerably%20more%20difficult%20than%0Aevaluating%20a%20single%20concept%2C%20as%20it%20demands%20detailed%20assessment%20not%20only%20for%0Aeach%20individual%20concept%20but%20also%20for%20the%20interactions%20among%20concepts.%20While%0Ahumans%20can%20intuitively%20assess%20generated%20images%2C%20existing%20metrics%20often%20provide%0Aeither%20overly%20narrow%20or%20overly%20generalized%20evaluations%2C%20resulting%20in%0Amisalignment%20with%20human%20preference.%20To%20address%20this%2C%20we%20propose%20Decomposed%20GPT%0AScore%20%28D-GPTScore%29%2C%20a%20novel%20human-aligned%20evaluation%20method%20that%20decomposes%0Aevaluation%20criteria%20into%20finer%20aspects%20and%20incorporates%20aspect-wise%20assessments%0Ausing%20Multimodal%20Large%20Language%20Model%20%28MLLM%29.%20Additionally%2C%20we%20release%20Human%0APreference-Aligned%20Concept%20Customization%20Benchmark%20%28CC-AlignBench%29%2C%20a%20benchmark%0Adataset%20containing%20both%20single-%20and%20multi-concept%20tasks%2C%20enabling%20stage-wise%0Aevaluation%20across%20a%20wide%20difficulty%20range%20--%20from%20individual%20actions%20to%0Amulti-person%20interactions.%20Our%20method%20significantly%20outperforms%20existing%0Aapproaches%20on%20this%20benchmark%2C%20exhibiting%20higher%20correlation%20with%20human%0Apreferences.%20This%20work%20establishes%20a%20new%20standard%20for%20evaluating%20concept%0Acustomization%20and%20highlights%20key%20challenges%20for%20future%20research.%20The%20benchmark%0Aand%20associated%20materials%20are%20available%20at%0Ahttps%3A//github.com/ReinaIshikawa/D-GPTScore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Preference-Aligned%2520Concept%2520Customization%2520Benchmark%2520via%2520Decomposed%250A%2520%2520Evaluation%26entry.906535625%3DReina%2520Ishikawa%2520and%2520Ryo%2520Fujii%2520and%2520Hideo%2520Saito%2520and%2520Ryo%2520Hachiuma%26entry.1292438233%3D%2520%2520Evaluating%2520concept%2520customization%2520is%2520challenging%252C%2520as%2520it%2520requires%2520a%250Acomprehensive%2520assessment%2520of%2520fidelity%2520to%2520generative%2520prompts%2520and%2520concept%2520images.%250AMoreover%252C%2520evaluating%2520multiple%2520concepts%2520is%2520considerably%2520more%2520difficult%2520than%250Aevaluating%2520a%2520single%2520concept%252C%2520as%2520it%2520demands%2520detailed%2520assessment%2520not%2520only%2520for%250Aeach%2520individual%2520concept%2520but%2520also%2520for%2520the%2520interactions%2520among%2520concepts.%2520While%250Ahumans%2520can%2520intuitively%2520assess%2520generated%2520images%252C%2520existing%2520metrics%2520often%2520provide%250Aeither%2520overly%2520narrow%2520or%2520overly%2520generalized%2520evaluations%252C%2520resulting%2520in%250Amisalignment%2520with%2520human%2520preference.%2520To%2520address%2520this%252C%2520we%2520propose%2520Decomposed%2520GPT%250AScore%2520%2528D-GPTScore%2529%252C%2520a%2520novel%2520human-aligned%2520evaluation%2520method%2520that%2520decomposes%250Aevaluation%2520criteria%2520into%2520finer%2520aspects%2520and%2520incorporates%2520aspect-wise%2520assessments%250Ausing%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529.%2520Additionally%252C%2520we%2520release%2520Human%250APreference-Aligned%2520Concept%2520Customization%2520Benchmark%2520%2528CC-AlignBench%2529%252C%2520a%2520benchmark%250Adataset%2520containing%2520both%2520single-%2520and%2520multi-concept%2520tasks%252C%2520enabling%2520stage-wise%250Aevaluation%2520across%2520a%2520wide%2520difficulty%2520range%2520--%2520from%2520individual%2520actions%2520to%250Amulti-person%2520interactions.%2520Our%2520method%2520significantly%2520outperforms%2520existing%250Aapproaches%2520on%2520this%2520benchmark%252C%2520exhibiting%2520higher%2520correlation%2520with%2520human%250Apreferences.%2520This%2520work%2520establishes%2520a%2520new%2520standard%2520for%2520evaluating%2520concept%250Acustomization%2520and%2520highlights%2520key%2520challenges%2520for%2520future%2520research.%2520The%2520benchmark%250Aand%2520associated%2520materials%2520are%2520available%2520at%250Ahttps%253A//github.com/ReinaIshikawa/D-GPTScore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Preference-Aligned%20Concept%20Customization%20Benchmark%20via%20Decomposed%0A%20%20Evaluation&entry.906535625=Reina%20Ishikawa%20and%20Ryo%20Fujii%20and%20Hideo%20Saito%20and%20Ryo%20Hachiuma&entry.1292438233=%20%20Evaluating%20concept%20customization%20is%20challenging%2C%20as%20it%20requires%20a%0Acomprehensive%20assessment%20of%20fidelity%20to%20generative%20prompts%20and%20concept%20images.%0AMoreover%2C%20evaluating%20multiple%20concepts%20is%20considerably%20more%20difficult%20than%0Aevaluating%20a%20single%20concept%2C%20as%20it%20demands%20detailed%20assessment%20not%20only%20for%0Aeach%20individual%20concept%20but%20also%20for%20the%20interactions%20among%20concepts.%20While%0Ahumans%20can%20intuitively%20assess%20generated%20images%2C%20existing%20metrics%20often%20provide%0Aeither%20overly%20narrow%20or%20overly%20generalized%20evaluations%2C%20resulting%20in%0Amisalignment%20with%20human%20preference.%20To%20address%20this%2C%20we%20propose%20Decomposed%20GPT%0AScore%20%28D-GPTScore%29%2C%20a%20novel%20human-aligned%20evaluation%20method%20that%20decomposes%0Aevaluation%20criteria%20into%20finer%20aspects%20and%20incorporates%20aspect-wise%20assessments%0Ausing%20Multimodal%20Large%20Language%20Model%20%28MLLM%29.%20Additionally%2C%20we%20release%20Human%0APreference-Aligned%20Concept%20Customization%20Benchmark%20%28CC-AlignBench%29%2C%20a%20benchmark%0Adataset%20containing%20both%20single-%20and%20multi-concept%20tasks%2C%20enabling%20stage-wise%0Aevaluation%20across%20a%20wide%20difficulty%20range%20--%20from%20individual%20actions%20to%0Amulti-person%20interactions.%20Our%20method%20significantly%20outperforms%20existing%0Aapproaches%20on%20this%20benchmark%2C%20exhibiting%20higher%20correlation%20with%20human%0Apreferences.%20This%20work%20establishes%20a%20new%20standard%20for%20evaluating%20concept%0Acustomization%20and%20highlights%20key%20challenges%20for%20future%20research.%20The%20benchmark%0Aand%20associated%20materials%20are%20available%20at%0Ahttps%3A//github.com/ReinaIshikawa/D-GPTScore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03385v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


