<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251002.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion", "author": "David Romero and Ariana Bermudez and Hao Li and Fabio Pizzati and Ivan Laptev", "abstract": "  Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.\n", "link": "http://arxiv.org/abs/2510.02284v1", "date": "2025-10-02", "relevancy": 3.4588, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7255}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6893}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Object%20Interactions%20with%20Physics-Guided%20Video%0A%20%20Diffusion&body=Title%3A%20Learning%20to%20Generate%20Object%20Interactions%20with%20Physics-Guided%20Video%0A%20%20Diffusion%0AAuthor%3A%20David%20Romero%20and%20Ariana%20Bermudez%20and%20Hao%20Li%20and%20Fabio%20Pizzati%20and%20Ivan%20Laptev%0AAbstract%3A%20%20%20Recent%20models%20for%20video%20generation%20have%20achieved%20remarkable%20progress%20and%20are%0Anow%20deployed%20in%20film%2C%20social%20media%20production%2C%20and%20advertising.%20Beyond%20their%0Acreative%20potential%2C%20such%20models%20also%20hold%20promise%20as%20world%20simulators%20for%0Arobotics%20and%20embodied%20decision%20making.%20Despite%20strong%20advances%2C%20however%2C%0Acurrent%20approaches%20still%20struggle%20to%20generate%20physically%20plausible%20object%0Ainteractions%20and%20lack%20physics-grounded%20control%20mechanisms.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20KineMask%2C%20an%20approach%20for%20physics-guided%20video%0Ageneration%20that%20enables%20realistic%20rigid%20body%20control%2C%20interactions%2C%20and%0Aeffects.%20Given%20a%20single%20image%20and%20a%20specified%20object%20velocity%2C%20our%20method%0Agenerates%20videos%20with%20inferred%20motions%20and%20future%20object%20interactions.%20We%0Apropose%20a%20two-stage%20training%20strategy%20that%20gradually%20removes%20future%20motion%0Asupervision%20via%20object%20masks.%20Using%20this%20strategy%20we%20train%20video%20diffusion%0Amodels%20%28VDMs%29%20on%20synthetic%20scenes%20of%20simple%20interactions%20and%20demonstrate%0Asignificant%20improvements%20of%20object%20interactions%20in%20real%20scenes.%20Furthermore%2C%0AKineMask%20integrates%20low-level%20motion%20control%20with%20high-level%20textual%0Aconditioning%20via%20predictive%20scene%20descriptions%2C%20leading%20to%20effective%20support%0Afor%20synthesis%20of%20complex%20dynamical%20phenomena.%20Extensive%20experiments%20show%20that%0AKineMask%20achieves%20strong%20improvements%20over%20recent%20models%20of%20comparable%20size.%0AAblation%20studies%20further%20highlight%20the%20complementary%20roles%20of%20low-%20and%0Ahigh-level%20conditioning%20in%20VDMs.%20Our%20code%2C%20model%2C%20and%20data%20will%20be%20made%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Object%2520Interactions%2520with%2520Physics-Guided%2520Video%250A%2520%2520Diffusion%26entry.906535625%3DDavid%2520Romero%2520and%2520Ariana%2520Bermudez%2520and%2520Hao%2520Li%2520and%2520Fabio%2520Pizzati%2520and%2520Ivan%2520Laptev%26entry.1292438233%3D%2520%2520Recent%2520models%2520for%2520video%2520generation%2520have%2520achieved%2520remarkable%2520progress%2520and%2520are%250Anow%2520deployed%2520in%2520film%252C%2520social%2520media%2520production%252C%2520and%2520advertising.%2520Beyond%2520their%250Acreative%2520potential%252C%2520such%2520models%2520also%2520hold%2520promise%2520as%2520world%2520simulators%2520for%250Arobotics%2520and%2520embodied%2520decision%2520making.%2520Despite%2520strong%2520advances%252C%2520however%252C%250Acurrent%2520approaches%2520still%2520struggle%2520to%2520generate%2520physically%2520plausible%2520object%250Ainteractions%2520and%2520lack%2520physics-grounded%2520control%2520mechanisms.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520introduce%2520KineMask%252C%2520an%2520approach%2520for%2520physics-guided%2520video%250Ageneration%2520that%2520enables%2520realistic%2520rigid%2520body%2520control%252C%2520interactions%252C%2520and%250Aeffects.%2520Given%2520a%2520single%2520image%2520and%2520a%2520specified%2520object%2520velocity%252C%2520our%2520method%250Agenerates%2520videos%2520with%2520inferred%2520motions%2520and%2520future%2520object%2520interactions.%2520We%250Apropose%2520a%2520two-stage%2520training%2520strategy%2520that%2520gradually%2520removes%2520future%2520motion%250Asupervision%2520via%2520object%2520masks.%2520Using%2520this%2520strategy%2520we%2520train%2520video%2520diffusion%250Amodels%2520%2528VDMs%2529%2520on%2520synthetic%2520scenes%2520of%2520simple%2520interactions%2520and%2520demonstrate%250Asignificant%2520improvements%2520of%2520object%2520interactions%2520in%2520real%2520scenes.%2520Furthermore%252C%250AKineMask%2520integrates%2520low-level%2520motion%2520control%2520with%2520high-level%2520textual%250Aconditioning%2520via%2520predictive%2520scene%2520descriptions%252C%2520leading%2520to%2520effective%2520support%250Afor%2520synthesis%2520of%2520complex%2520dynamical%2520phenomena.%2520Extensive%2520experiments%2520show%2520that%250AKineMask%2520achieves%2520strong%2520improvements%2520over%2520recent%2520models%2520of%2520comparable%2520size.%250AAblation%2520studies%2520further%2520highlight%2520the%2520complementary%2520roles%2520of%2520low-%2520and%250Ahigh-level%2520conditioning%2520in%2520VDMs.%2520Our%2520code%252C%2520model%252C%2520and%2520data%2520will%2520be%2520made%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Object%20Interactions%20with%20Physics-Guided%20Video%0A%20%20Diffusion&entry.906535625=David%20Romero%20and%20Ariana%20Bermudez%20and%20Hao%20Li%20and%20Fabio%20Pizzati%20and%20Ivan%20Laptev&entry.1292438233=%20%20Recent%20models%20for%20video%20generation%20have%20achieved%20remarkable%20progress%20and%20are%0Anow%20deployed%20in%20film%2C%20social%20media%20production%2C%20and%20advertising.%20Beyond%20their%0Acreative%20potential%2C%20such%20models%20also%20hold%20promise%20as%20world%20simulators%20for%0Arobotics%20and%20embodied%20decision%20making.%20Despite%20strong%20advances%2C%20however%2C%0Acurrent%20approaches%20still%20struggle%20to%20generate%20physically%20plausible%20object%0Ainteractions%20and%20lack%20physics-grounded%20control%20mechanisms.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20KineMask%2C%20an%20approach%20for%20physics-guided%20video%0Ageneration%20that%20enables%20realistic%20rigid%20body%20control%2C%20interactions%2C%20and%0Aeffects.%20Given%20a%20single%20image%20and%20a%20specified%20object%20velocity%2C%20our%20method%0Agenerates%20videos%20with%20inferred%20motions%20and%20future%20object%20interactions.%20We%0Apropose%20a%20two-stage%20training%20strategy%20that%20gradually%20removes%20future%20motion%0Asupervision%20via%20object%20masks.%20Using%20this%20strategy%20we%20train%20video%20diffusion%0Amodels%20%28VDMs%29%20on%20synthetic%20scenes%20of%20simple%20interactions%20and%20demonstrate%0Asignificant%20improvements%20of%20object%20interactions%20in%20real%20scenes.%20Furthermore%2C%0AKineMask%20integrates%20low-level%20motion%20control%20with%20high-level%20textual%0Aconditioning%20via%20predictive%20scene%20descriptions%2C%20leading%20to%20effective%20support%0Afor%20synthesis%20of%20complex%20dynamical%20phenomena.%20Extensive%20experiments%20show%20that%0AKineMask%20achieves%20strong%20improvements%20over%20recent%20models%20of%20comparable%20size.%0AAblation%20studies%20further%20highlight%20the%20complementary%20roles%20of%20low-%20and%0Ahigh-level%20conditioning%20in%20VDMs.%20Our%20code%2C%20model%2C%20and%20data%20will%20be%20made%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02284v1&entry.124074799=Read"},
{"title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation", "author": "Justin Cui and Jie Wu and Ming Li and Tao Yang and Xiaojie Li and Rui Wang and Andrew Bai and Yuanhao Ban and Cho-Jui Hsieh", "abstract": "  Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/\n", "link": "http://arxiv.org/abs/2510.02283v1", "date": "2025-10-02", "relevancy": 3.2018, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6488}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6396}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Forcing%2B%2B%3A%20Towards%20Minute-Scale%20High-Quality%20Video%20Generation&body=Title%3A%20Self-Forcing%2B%2B%3A%20Towards%20Minute-Scale%20High-Quality%20Video%20Generation%0AAuthor%3A%20Justin%20Cui%20and%20Jie%20Wu%20and%20Ming%20Li%20and%20Tao%20Yang%20and%20Xiaojie%20Li%20and%20Rui%20Wang%20and%20Andrew%20Bai%20and%20Yuanhao%20Ban%20and%20Cho-Jui%20Hsieh%0AAbstract%3A%20%20%20Diffusion%20models%20have%20revolutionized%20image%20and%20video%20generation%2C%20achieving%0Aunprecedented%20visual%20quality.%20However%2C%20their%20reliance%20on%20transformer%0Aarchitectures%20incurs%20prohibitively%20high%20computational%20costs%2C%20particularly%20when%0Aextending%20generation%20to%20long%20videos.%20Recent%20work%20has%20explored%20autoregressive%0Aformulations%20for%20long%20video%20generation%2C%20typically%20by%20distilling%20from%0Ashort-horizon%20bidirectional%20teachers.%20Nevertheless%2C%20given%20that%20teacher%20models%0Acannot%20synthesize%20long%20videos%2C%20the%20extrapolation%20of%20student%20models%20beyond%20their%0Atraining%20horizon%20often%20leads%20to%20pronounced%20quality%20degradation%2C%20arising%20from%0Athe%20compounding%20of%20errors%20within%20the%20continuous%20latent%20space.%20In%20this%20paper%2C%20we%0Apropose%20a%20simple%20yet%20effective%20approach%20to%20mitigate%20quality%20degradation%20in%0Along-horizon%20video%20generation%20without%20requiring%20supervision%20from%20long-video%0Ateachers%20or%20retraining%20on%20long%20video%20datasets.%20Our%20approach%20centers%20on%0Aexploiting%20the%20rich%20knowledge%20of%20teacher%20models%20to%20provide%20guidance%20for%20the%0Astudent%20model%20through%20sampled%20segments%20drawn%20from%20self-generated%20long%20videos.%0AOur%20method%20maintains%20temporal%20consistency%20while%20scaling%20video%20length%20by%20up%20to%0A20x%20beyond%20teacher%27s%20capability%2C%20avoiding%20common%20issues%20such%20as%20over-exposure%0Aand%20error-accumulation%20without%20recomputing%20overlapping%20frames%20like%20previous%0Amethods.%20When%20scaling%20up%20the%20computation%2C%20our%20method%20shows%20the%20capability%20of%0Agenerating%20videos%20up%20to%204%20minutes%20and%2015%20seconds%2C%20equivalent%20to%2099.9%25%20of%20the%0Amaximum%20span%20supported%20by%20our%20base%20model%27s%20position%20embedding%20and%20more%20than%2050x%0Alonger%20than%20that%20of%20our%20baseline%20model.%20Experiments%20on%20standard%20benchmarks%20and%0Aour%20proposed%20improved%20benchmark%20demonstrate%20that%20our%20approach%20substantially%0Aoutperforms%20baseline%20methods%20in%20both%20fidelity%20and%20consistency.%20Our%20long-horizon%0Avideos%20demo%20can%20be%20found%20at%20https%3A//self-forcing-plus-plus.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Forcing%252B%252B%253A%2520Towards%2520Minute-Scale%2520High-Quality%2520Video%2520Generation%26entry.906535625%3DJustin%2520Cui%2520and%2520Jie%2520Wu%2520and%2520Ming%2520Li%2520and%2520Tao%2520Yang%2520and%2520Xiaojie%2520Li%2520and%2520Rui%2520Wang%2520and%2520Andrew%2520Bai%2520and%2520Yuanhao%2520Ban%2520and%2520Cho-Jui%2520Hsieh%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520revolutionized%2520image%2520and%2520video%2520generation%252C%2520achieving%250Aunprecedented%2520visual%2520quality.%2520However%252C%2520their%2520reliance%2520on%2520transformer%250Aarchitectures%2520incurs%2520prohibitively%2520high%2520computational%2520costs%252C%2520particularly%2520when%250Aextending%2520generation%2520to%2520long%2520videos.%2520Recent%2520work%2520has%2520explored%2520autoregressive%250Aformulations%2520for%2520long%2520video%2520generation%252C%2520typically%2520by%2520distilling%2520from%250Ashort-horizon%2520bidirectional%2520teachers.%2520Nevertheless%252C%2520given%2520that%2520teacher%2520models%250Acannot%2520synthesize%2520long%2520videos%252C%2520the%2520extrapolation%2520of%2520student%2520models%2520beyond%2520their%250Atraining%2520horizon%2520often%2520leads%2520to%2520pronounced%2520quality%2520degradation%252C%2520arising%2520from%250Athe%2520compounding%2520of%2520errors%2520within%2520the%2520continuous%2520latent%2520space.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520effective%2520approach%2520to%2520mitigate%2520quality%2520degradation%2520in%250Along-horizon%2520video%2520generation%2520without%2520requiring%2520supervision%2520from%2520long-video%250Ateachers%2520or%2520retraining%2520on%2520long%2520video%2520datasets.%2520Our%2520approach%2520centers%2520on%250Aexploiting%2520the%2520rich%2520knowledge%2520of%2520teacher%2520models%2520to%2520provide%2520guidance%2520for%2520the%250Astudent%2520model%2520through%2520sampled%2520segments%2520drawn%2520from%2520self-generated%2520long%2520videos.%250AOur%2520method%2520maintains%2520temporal%2520consistency%2520while%2520scaling%2520video%2520length%2520by%2520up%2520to%250A20x%2520beyond%2520teacher%2527s%2520capability%252C%2520avoiding%2520common%2520issues%2520such%2520as%2520over-exposure%250Aand%2520error-accumulation%2520without%2520recomputing%2520overlapping%2520frames%2520like%2520previous%250Amethods.%2520When%2520scaling%2520up%2520the%2520computation%252C%2520our%2520method%2520shows%2520the%2520capability%2520of%250Agenerating%2520videos%2520up%2520to%25204%2520minutes%2520and%252015%2520seconds%252C%2520equivalent%2520to%252099.9%2525%2520of%2520the%250Amaximum%2520span%2520supported%2520by%2520our%2520base%2520model%2527s%2520position%2520embedding%2520and%2520more%2520than%252050x%250Alonger%2520than%2520that%2520of%2520our%2520baseline%2520model.%2520Experiments%2520on%2520standard%2520benchmarks%2520and%250Aour%2520proposed%2520improved%2520benchmark%2520demonstrate%2520that%2520our%2520approach%2520substantially%250Aoutperforms%2520baseline%2520methods%2520in%2520both%2520fidelity%2520and%2520consistency.%2520Our%2520long-horizon%250Avideos%2520demo%2520can%2520be%2520found%2520at%2520https%253A//self-forcing-plus-plus.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Forcing%2B%2B%3A%20Towards%20Minute-Scale%20High-Quality%20Video%20Generation&entry.906535625=Justin%20Cui%20and%20Jie%20Wu%20and%20Ming%20Li%20and%20Tao%20Yang%20and%20Xiaojie%20Li%20and%20Rui%20Wang%20and%20Andrew%20Bai%20and%20Yuanhao%20Ban%20and%20Cho-Jui%20Hsieh&entry.1292438233=%20%20Diffusion%20models%20have%20revolutionized%20image%20and%20video%20generation%2C%20achieving%0Aunprecedented%20visual%20quality.%20However%2C%20their%20reliance%20on%20transformer%0Aarchitectures%20incurs%20prohibitively%20high%20computational%20costs%2C%20particularly%20when%0Aextending%20generation%20to%20long%20videos.%20Recent%20work%20has%20explored%20autoregressive%0Aformulations%20for%20long%20video%20generation%2C%20typically%20by%20distilling%20from%0Ashort-horizon%20bidirectional%20teachers.%20Nevertheless%2C%20given%20that%20teacher%20models%0Acannot%20synthesize%20long%20videos%2C%20the%20extrapolation%20of%20student%20models%20beyond%20their%0Atraining%20horizon%20often%20leads%20to%20pronounced%20quality%20degradation%2C%20arising%20from%0Athe%20compounding%20of%20errors%20within%20the%20continuous%20latent%20space.%20In%20this%20paper%2C%20we%0Apropose%20a%20simple%20yet%20effective%20approach%20to%20mitigate%20quality%20degradation%20in%0Along-horizon%20video%20generation%20without%20requiring%20supervision%20from%20long-video%0Ateachers%20or%20retraining%20on%20long%20video%20datasets.%20Our%20approach%20centers%20on%0Aexploiting%20the%20rich%20knowledge%20of%20teacher%20models%20to%20provide%20guidance%20for%20the%0Astudent%20model%20through%20sampled%20segments%20drawn%20from%20self-generated%20long%20videos.%0AOur%20method%20maintains%20temporal%20consistency%20while%20scaling%20video%20length%20by%20up%20to%0A20x%20beyond%20teacher%27s%20capability%2C%20avoiding%20common%20issues%20such%20as%20over-exposure%0Aand%20error-accumulation%20without%20recomputing%20overlapping%20frames%20like%20previous%0Amethods.%20When%20scaling%20up%20the%20computation%2C%20our%20method%20shows%20the%20capability%20of%0Agenerating%20videos%20up%20to%204%20minutes%20and%2015%20seconds%2C%20equivalent%20to%2099.9%25%20of%20the%0Amaximum%20span%20supported%20by%20our%20base%20model%27s%20position%20embedding%20and%20more%20than%2050x%0Alonger%20than%20that%20of%20our%20baseline%20model.%20Experiments%20on%20standard%20benchmarks%20and%0Aour%20proposed%20improved%20benchmark%20demonstrate%20that%20our%20approach%20substantially%0Aoutperforms%20baseline%20methods%20in%20both%20fidelity%20and%20consistency.%20Our%20long-horizon%0Avideos%20demo%20can%20be%20found%20at%20https%3A//self-forcing-plus-plus.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02283v1&entry.124074799=Read"},
{"title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM\n  Reconstruction", "author": "Huaizhi Qu and Xiao Wang and Gengwei Zhang and Jie Peng and Tianlong Chen", "abstract": "  Cryo-electron microscopy (cryo-EM) has become a central tool for\nhigh-resolution structural biology, yet the massive scale of datasets (often\nexceeding 100k particle images) renders 3D reconstruction both computationally\nexpensive and memory intensive. Traditional Fourier-space methods are efficient\nbut lose fidelity due to repeated transforms, while recent real-space\napproaches based on neural radiance fields (NeRFs) improve accuracy but incur\ncubic memory and computation overhead. Therefore, we introduce GEM, a novel\ncryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that\noperates directly in real-space while maintaining high efficiency. Instead of\nmodeling the entire density volume, GEM represents proteins with compact 3D\nGaussians, each parameterized by only 11 values. To further improve the\ntraining efficiency, we designed a novel gradient computation to 3D Gaussians\nthat contribute to each voxel. This design substantially reduced both memory\nfootprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to\n48% faster training and 12% lower memory usage compared to state-of-the-art\nmethods, while improving local resolution by as much as 38.8%. These results\nestablish GEM as a practical and scalable paradigm for cryo-EM reconstruction,\nunifying speed, efficiency, and high-resolution accuracy. Our code is available\nat https://github.com/UNITES-Lab/GEM.\n", "link": "http://arxiv.org/abs/2509.25075v2", "date": "2025-10-02", "relevancy": 2.9959, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6311}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5843}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEM%3A%203D%20Gaussian%20Splatting%20for%20Efficient%20and%20Accurate%20Cryo-EM%0A%20%20Reconstruction&body=Title%3A%20GEM%3A%203D%20Gaussian%20Splatting%20for%20Efficient%20and%20Accurate%20Cryo-EM%0A%20%20Reconstruction%0AAuthor%3A%20Huaizhi%20Qu%20and%20Xiao%20Wang%20and%20Gengwei%20Zhang%20and%20Jie%20Peng%20and%20Tianlong%20Chen%0AAbstract%3A%20%20%20Cryo-electron%20microscopy%20%28cryo-EM%29%20has%20become%20a%20central%20tool%20for%0Ahigh-resolution%20structural%20biology%2C%20yet%20the%20massive%20scale%20of%20datasets%20%28often%0Aexceeding%20100k%20particle%20images%29%20renders%203D%20reconstruction%20both%20computationally%0Aexpensive%20and%20memory%20intensive.%20Traditional%20Fourier-space%20methods%20are%20efficient%0Abut%20lose%20fidelity%20due%20to%20repeated%20transforms%2C%20while%20recent%20real-space%0Aapproaches%20based%20on%20neural%20radiance%20fields%20%28NeRFs%29%20improve%20accuracy%20but%20incur%0Acubic%20memory%20and%20computation%20overhead.%20Therefore%2C%20we%20introduce%20GEM%2C%20a%20novel%0Acryo-EM%20reconstruction%20framework%20built%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20that%0Aoperates%20directly%20in%20real-space%20while%20maintaining%20high%20efficiency.%20Instead%20of%0Amodeling%20the%20entire%20density%20volume%2C%20GEM%20represents%20proteins%20with%20compact%203D%0AGaussians%2C%20each%20parameterized%20by%20only%2011%20values.%20To%20further%20improve%20the%0Atraining%20efficiency%2C%20we%20designed%20a%20novel%20gradient%20computation%20to%203D%20Gaussians%0Athat%20contribute%20to%20each%20voxel.%20This%20design%20substantially%20reduced%20both%20memory%0Afootprint%20and%20training%20cost.%20On%20standard%20cryo-EM%20benchmarks%2C%20GEM%20achieves%20up%20to%0A48%25%20faster%20training%20and%2012%25%20lower%20memory%20usage%20compared%20to%20state-of-the-art%0Amethods%2C%20while%20improving%20local%20resolution%20by%20as%20much%20as%2038.8%25.%20These%20results%0Aestablish%20GEM%20as%20a%20practical%20and%20scalable%20paradigm%20for%20cryo-EM%20reconstruction%2C%0Aunifying%20speed%2C%20efficiency%2C%20and%20high-resolution%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/UNITES-Lab/GEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEM%253A%25203D%2520Gaussian%2520Splatting%2520for%2520Efficient%2520and%2520Accurate%2520Cryo-EM%250A%2520%2520Reconstruction%26entry.906535625%3DHuaizhi%2520Qu%2520and%2520Xiao%2520Wang%2520and%2520Gengwei%2520Zhang%2520and%2520Jie%2520Peng%2520and%2520Tianlong%2520Chen%26entry.1292438233%3D%2520%2520Cryo-electron%2520microscopy%2520%2528cryo-EM%2529%2520has%2520become%2520a%2520central%2520tool%2520for%250Ahigh-resolution%2520structural%2520biology%252C%2520yet%2520the%2520massive%2520scale%2520of%2520datasets%2520%2528often%250Aexceeding%2520100k%2520particle%2520images%2529%2520renders%25203D%2520reconstruction%2520both%2520computationally%250Aexpensive%2520and%2520memory%2520intensive.%2520Traditional%2520Fourier-space%2520methods%2520are%2520efficient%250Abut%2520lose%2520fidelity%2520due%2520to%2520repeated%2520transforms%252C%2520while%2520recent%2520real-space%250Aapproaches%2520based%2520on%2520neural%2520radiance%2520fields%2520%2528NeRFs%2529%2520improve%2520accuracy%2520but%2520incur%250Acubic%2520memory%2520and%2520computation%2520overhead.%2520Therefore%252C%2520we%2520introduce%2520GEM%252C%2520a%2520novel%250Acryo-EM%2520reconstruction%2520framework%2520built%2520on%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520that%250Aoperates%2520directly%2520in%2520real-space%2520while%2520maintaining%2520high%2520efficiency.%2520Instead%2520of%250Amodeling%2520the%2520entire%2520density%2520volume%252C%2520GEM%2520represents%2520proteins%2520with%2520compact%25203D%250AGaussians%252C%2520each%2520parameterized%2520by%2520only%252011%2520values.%2520To%2520further%2520improve%2520the%250Atraining%2520efficiency%252C%2520we%2520designed%2520a%2520novel%2520gradient%2520computation%2520to%25203D%2520Gaussians%250Athat%2520contribute%2520to%2520each%2520voxel.%2520This%2520design%2520substantially%2520reduced%2520both%2520memory%250Afootprint%2520and%2520training%2520cost.%2520On%2520standard%2520cryo-EM%2520benchmarks%252C%2520GEM%2520achieves%2520up%2520to%250A48%2525%2520faster%2520training%2520and%252012%2525%2520lower%2520memory%2520usage%2520compared%2520to%2520state-of-the-art%250Amethods%252C%2520while%2520improving%2520local%2520resolution%2520by%2520as%2520much%2520as%252038.8%2525.%2520These%2520results%250Aestablish%2520GEM%2520as%2520a%2520practical%2520and%2520scalable%2520paradigm%2520for%2520cryo-EM%2520reconstruction%252C%250Aunifying%2520speed%252C%2520efficiency%252C%2520and%2520high-resolution%2520accuracy.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/UNITES-Lab/GEM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEM%3A%203D%20Gaussian%20Splatting%20for%20Efficient%20and%20Accurate%20Cryo-EM%0A%20%20Reconstruction&entry.906535625=Huaizhi%20Qu%20and%20Xiao%20Wang%20and%20Gengwei%20Zhang%20and%20Jie%20Peng%20and%20Tianlong%20Chen&entry.1292438233=%20%20Cryo-electron%20microscopy%20%28cryo-EM%29%20has%20become%20a%20central%20tool%20for%0Ahigh-resolution%20structural%20biology%2C%20yet%20the%20massive%20scale%20of%20datasets%20%28often%0Aexceeding%20100k%20particle%20images%29%20renders%203D%20reconstruction%20both%20computationally%0Aexpensive%20and%20memory%20intensive.%20Traditional%20Fourier-space%20methods%20are%20efficient%0Abut%20lose%20fidelity%20due%20to%20repeated%20transforms%2C%20while%20recent%20real-space%0Aapproaches%20based%20on%20neural%20radiance%20fields%20%28NeRFs%29%20improve%20accuracy%20but%20incur%0Acubic%20memory%20and%20computation%20overhead.%20Therefore%2C%20we%20introduce%20GEM%2C%20a%20novel%0Acryo-EM%20reconstruction%20framework%20built%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20that%0Aoperates%20directly%20in%20real-space%20while%20maintaining%20high%20efficiency.%20Instead%20of%0Amodeling%20the%20entire%20density%20volume%2C%20GEM%20represents%20proteins%20with%20compact%203D%0AGaussians%2C%20each%20parameterized%20by%20only%2011%20values.%20To%20further%20improve%20the%0Atraining%20efficiency%2C%20we%20designed%20a%20novel%20gradient%20computation%20to%203D%20Gaussians%0Athat%20contribute%20to%20each%20voxel.%20This%20design%20substantially%20reduced%20both%20memory%0Afootprint%20and%20training%20cost.%20On%20standard%20cryo-EM%20benchmarks%2C%20GEM%20achieves%20up%20to%0A48%25%20faster%20training%20and%2012%25%20lower%20memory%20usage%20compared%20to%20state-of-the-art%0Amethods%2C%20while%20improving%20local%20resolution%20by%20as%20much%20as%2038.8%25.%20These%20results%0Aestablish%20GEM%20as%20a%20practical%20and%20scalable%20paradigm%20for%20cryo-EM%20reconstruction%2C%0Aunifying%20speed%2C%20efficiency%2C%20and%20high-resolution%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/UNITES-Lab/GEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25075v2&entry.124074799=Read"},
{"title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning\n  MLLMs and RL", "author": "Kyoungjun Park and Yifan Yang and Juheon Yi and Shicheng Zheng and Yifei Shen and Dongqi Han and Caihua Shan and Muhammad Muaz and Lili Qiu", "abstract": "  With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.\n", "link": "http://arxiv.org/abs/2510.02282v1", "date": "2025-10-02", "relevancy": 2.9779, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6152}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6051}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidGuard-R1%3A%20AI-Generated%20Video%20Detection%20and%20Explanation%20via%20Reasoning%0A%20%20MLLMs%20and%20RL&body=Title%3A%20VidGuard-R1%3A%20AI-Generated%20Video%20Detection%20and%20Explanation%20via%20Reasoning%0A%20%20MLLMs%20and%20RL%0AAuthor%3A%20Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Juheon%20Yi%20and%20Shicheng%20Zheng%20and%20Yifei%20Shen%20and%20Dongqi%20Han%20and%20Caihua%20Shan%20and%20Muhammad%20Muaz%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20AI-generated%20videos%2C%20there%20is%20an%20urgent%20need%0Afor%20effective%20detection%20tools%20to%20mitigate%20societal%20risks%20such%20as%20misinformation%0Aand%20reputational%20harm.%20In%20addition%20to%20accurate%20classification%2C%20it%20is%20essential%0Athat%20detection%20models%20provide%20interpretable%20explanations%20to%20ensure%20transparency%0Afor%20regulators%20and%20end%20users.%20To%20address%20these%20challenges%2C%20we%20introduce%0AVidGuard-R1%2C%20the%20first%20video%20authenticity%20detector%20that%20fine-tunes%20a%0Amulti-modal%20large%20language%20model%20%28MLLM%29%20using%20group%20relative%20policy%0Aoptimization%20%28GRPO%29.%20Our%20model%20delivers%20both%20highly%20accurate%20judgments%20and%0Ainsightful%20reasoning.%20We%20curate%20a%20challenging%20dataset%20of%20140k%20real%20and%0AAI-generated%20videos%20produced%20by%20state-of-the-art%20generation%20models%2C%20carefully%0Adesigning%20the%20generation%20process%20to%20maximize%20discrimination%20difficulty.%20We%20then%0Afine-tune%20Qwen-VL%20using%20GRPO%20with%20two%20specialized%20reward%20models%20that%20target%0Atemporal%20artifacts%20and%20generation%20complexity.%20Extensive%20experiments%20demonstrate%0Athat%20VidGuard-R1%20achieves%20state-of-the-art%20zero-shot%20performance%20on%20existing%0Abenchmarks%2C%20with%20additional%20training%20pushing%20accuracy%20above%2095%25.%20Case%20studies%0Afurther%20show%20that%20VidGuard-R1%20produces%20precise%20and%20interpretable%20rationales%0Abehind%20its%20predictions.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//VidGuard-R1.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidGuard-R1%253A%2520AI-Generated%2520Video%2520Detection%2520and%2520Explanation%2520via%2520Reasoning%250A%2520%2520MLLMs%2520and%2520RL%26entry.906535625%3DKyoungjun%2520Park%2520and%2520Yifan%2520Yang%2520and%2520Juheon%2520Yi%2520and%2520Shicheng%2520Zheng%2520and%2520Yifei%2520Shen%2520and%2520Dongqi%2520Han%2520and%2520Caihua%2520Shan%2520and%2520Muhammad%2520Muaz%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520AI-generated%2520videos%252C%2520there%2520is%2520an%2520urgent%2520need%250Afor%2520effective%2520detection%2520tools%2520to%2520mitigate%2520societal%2520risks%2520such%2520as%2520misinformation%250Aand%2520reputational%2520harm.%2520In%2520addition%2520to%2520accurate%2520classification%252C%2520it%2520is%2520essential%250Athat%2520detection%2520models%2520provide%2520interpretable%2520explanations%2520to%2520ensure%2520transparency%250Afor%2520regulators%2520and%2520end%2520users.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AVidGuard-R1%252C%2520the%2520first%2520video%2520authenticity%2520detector%2520that%2520fine-tunes%2520a%250Amulti-modal%2520large%2520language%2520model%2520%2528MLLM%2529%2520using%2520group%2520relative%2520policy%250Aoptimization%2520%2528GRPO%2529.%2520Our%2520model%2520delivers%2520both%2520highly%2520accurate%2520judgments%2520and%250Ainsightful%2520reasoning.%2520We%2520curate%2520a%2520challenging%2520dataset%2520of%2520140k%2520real%2520and%250AAI-generated%2520videos%2520produced%2520by%2520state-of-the-art%2520generation%2520models%252C%2520carefully%250Adesigning%2520the%2520generation%2520process%2520to%2520maximize%2520discrimination%2520difficulty.%2520We%2520then%250Afine-tune%2520Qwen-VL%2520using%2520GRPO%2520with%2520two%2520specialized%2520reward%2520models%2520that%2520target%250Atemporal%2520artifacts%2520and%2520generation%2520complexity.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520VidGuard-R1%2520achieves%2520state-of-the-art%2520zero-shot%2520performance%2520on%2520existing%250Abenchmarks%252C%2520with%2520additional%2520training%2520pushing%2520accuracy%2520above%252095%2525.%2520Case%2520studies%250Afurther%2520show%2520that%2520VidGuard-R1%2520produces%2520precise%2520and%2520interpretable%2520rationales%250Abehind%2520its%2520predictions.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//VidGuard-R1.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidGuard-R1%3A%20AI-Generated%20Video%20Detection%20and%20Explanation%20via%20Reasoning%0A%20%20MLLMs%20and%20RL&entry.906535625=Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Juheon%20Yi%20and%20Shicheng%20Zheng%20and%20Yifei%20Shen%20and%20Dongqi%20Han%20and%20Caihua%20Shan%20and%20Muhammad%20Muaz%20and%20Lili%20Qiu&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20AI-generated%20videos%2C%20there%20is%20an%20urgent%20need%0Afor%20effective%20detection%20tools%20to%20mitigate%20societal%20risks%20such%20as%20misinformation%0Aand%20reputational%20harm.%20In%20addition%20to%20accurate%20classification%2C%20it%20is%20essential%0Athat%20detection%20models%20provide%20interpretable%20explanations%20to%20ensure%20transparency%0Afor%20regulators%20and%20end%20users.%20To%20address%20these%20challenges%2C%20we%20introduce%0AVidGuard-R1%2C%20the%20first%20video%20authenticity%20detector%20that%20fine-tunes%20a%0Amulti-modal%20large%20language%20model%20%28MLLM%29%20using%20group%20relative%20policy%0Aoptimization%20%28GRPO%29.%20Our%20model%20delivers%20both%20highly%20accurate%20judgments%20and%0Ainsightful%20reasoning.%20We%20curate%20a%20challenging%20dataset%20of%20140k%20real%20and%0AAI-generated%20videos%20produced%20by%20state-of-the-art%20generation%20models%2C%20carefully%0Adesigning%20the%20generation%20process%20to%20maximize%20discrimination%20difficulty.%20We%20then%0Afine-tune%20Qwen-VL%20using%20GRPO%20with%20two%20specialized%20reward%20models%20that%20target%0Atemporal%20artifacts%20and%20generation%20complexity.%20Extensive%20experiments%20demonstrate%0Athat%20VidGuard-R1%20achieves%20state-of-the-art%20zero-shot%20performance%20on%20existing%0Abenchmarks%2C%20with%20additional%20training%20pushing%20accuracy%20above%2095%25.%20Case%20studies%0Afurther%20show%20that%20VidGuard-R1%20produces%20precise%20and%20interpretable%20rationales%0Abehind%20its%20predictions.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//VidGuard-R1.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02282v1&entry.124074799=Read"},
{"title": "From Behavioral Performance to Internal Competence: Interpreting\n  Vision-Language Models with VLM-Lens", "author": "Hala Sheta and Eric Huang and Shuyu Wu and Ilia Alenabi and Jiajun Hong and Ryker Lin and Ruoxi Ning and Daniel Wei and Jialin Yang and Jiawei Zhou and Ziqiao Ma and Freda Shi", "abstract": "  We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,\nanalysis, and interpretation of vision-language models (VLMs) by supporting the\nextraction of intermediate outputs from any layer during the forward pass of\nopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that\nabstracts away model-specific complexities and supports user-friendly operation\nacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs and\ntheir over 30 variants, and is extensible to accommodate new models without\nchanging the core logic.\n  The toolkit integrates easily with various interpretability and analysis\nmethods. We demonstrate its usage with two simple analytical experiments,\nrevealing systematic differences in the hidden representations of VLMs across\nlayers and target concepts. VLM-Lens is released as an open-sourced project to\naccelerate community efforts in understanding and improving VLMs.\n", "link": "http://arxiv.org/abs/2510.02292v1", "date": "2025-10-02", "relevancy": 2.9446, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6146}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Behavioral%20Performance%20to%20Internal%20Competence%3A%20Interpreting%0A%20%20Vision-Language%20Models%20with%20VLM-Lens&body=Title%3A%20From%20Behavioral%20Performance%20to%20Internal%20Competence%3A%20Interpreting%0A%20%20Vision-Language%20Models%20with%20VLM-Lens%0AAuthor%3A%20Hala%20Sheta%20and%20Eric%20Huang%20and%20Shuyu%20Wu%20and%20Ilia%20Alenabi%20and%20Jiajun%20Hong%20and%20Ryker%20Lin%20and%20Ruoxi%20Ning%20and%20Daniel%20Wei%20and%20Jialin%20Yang%20and%20Jiawei%20Zhou%20and%20Ziqiao%20Ma%20and%20Freda%20Shi%0AAbstract%3A%20%20%20We%20introduce%20VLM-Lens%2C%20a%20toolkit%20designed%20to%20enable%20systematic%20benchmarking%2C%0Aanalysis%2C%20and%20interpretation%20of%20vision-language%20models%20%28VLMs%29%20by%20supporting%20the%0Aextraction%20of%20intermediate%20outputs%20from%20any%20layer%20during%20the%20forward%20pass%20of%0Aopen-source%20VLMs.%20VLM-Lens%20provides%20a%20unified%2C%20YAML-configurable%20interface%20that%0Aabstracts%20away%20model-specific%20complexities%20and%20supports%20user-friendly%20operation%0Aacross%20diverse%20VLMs.%20It%20currently%20supports%2016%20state-of-the-art%20base%20VLMs%20and%0Atheir%20over%2030%20variants%2C%20and%20is%20extensible%20to%20accommodate%20new%20models%20without%0Achanging%20the%20core%20logic.%0A%20%20The%20toolkit%20integrates%20easily%20with%20various%20interpretability%20and%20analysis%0Amethods.%20We%20demonstrate%20its%20usage%20with%20two%20simple%20analytical%20experiments%2C%0Arevealing%20systematic%20differences%20in%20the%20hidden%20representations%20of%20VLMs%20across%0Alayers%20and%20target%20concepts.%20VLM-Lens%20is%20released%20as%20an%20open-sourced%20project%20to%0Aaccelerate%20community%20efforts%20in%20understanding%20and%20improving%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Behavioral%2520Performance%2520to%2520Internal%2520Competence%253A%2520Interpreting%250A%2520%2520Vision-Language%2520Models%2520with%2520VLM-Lens%26entry.906535625%3DHala%2520Sheta%2520and%2520Eric%2520Huang%2520and%2520Shuyu%2520Wu%2520and%2520Ilia%2520Alenabi%2520and%2520Jiajun%2520Hong%2520and%2520Ryker%2520Lin%2520and%2520Ruoxi%2520Ning%2520and%2520Daniel%2520Wei%2520and%2520Jialin%2520Yang%2520and%2520Jiawei%2520Zhou%2520and%2520Ziqiao%2520Ma%2520and%2520Freda%2520Shi%26entry.1292438233%3D%2520%2520We%2520introduce%2520VLM-Lens%252C%2520a%2520toolkit%2520designed%2520to%2520enable%2520systematic%2520benchmarking%252C%250Aanalysis%252C%2520and%2520interpretation%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520by%2520supporting%2520the%250Aextraction%2520of%2520intermediate%2520outputs%2520from%2520any%2520layer%2520during%2520the%2520forward%2520pass%2520of%250Aopen-source%2520VLMs.%2520VLM-Lens%2520provides%2520a%2520unified%252C%2520YAML-configurable%2520interface%2520that%250Aabstracts%2520away%2520model-specific%2520complexities%2520and%2520supports%2520user-friendly%2520operation%250Aacross%2520diverse%2520VLMs.%2520It%2520currently%2520supports%252016%2520state-of-the-art%2520base%2520VLMs%2520and%250Atheir%2520over%252030%2520variants%252C%2520and%2520is%2520extensible%2520to%2520accommodate%2520new%2520models%2520without%250Achanging%2520the%2520core%2520logic.%250A%2520%2520The%2520toolkit%2520integrates%2520easily%2520with%2520various%2520interpretability%2520and%2520analysis%250Amethods.%2520We%2520demonstrate%2520its%2520usage%2520with%2520two%2520simple%2520analytical%2520experiments%252C%250Arevealing%2520systematic%2520differences%2520in%2520the%2520hidden%2520representations%2520of%2520VLMs%2520across%250Alayers%2520and%2520target%2520concepts.%2520VLM-Lens%2520is%2520released%2520as%2520an%2520open-sourced%2520project%2520to%250Aaccelerate%2520community%2520efforts%2520in%2520understanding%2520and%2520improving%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Behavioral%20Performance%20to%20Internal%20Competence%3A%20Interpreting%0A%20%20Vision-Language%20Models%20with%20VLM-Lens&entry.906535625=Hala%20Sheta%20and%20Eric%20Huang%20and%20Shuyu%20Wu%20and%20Ilia%20Alenabi%20and%20Jiajun%20Hong%20and%20Ryker%20Lin%20and%20Ruoxi%20Ning%20and%20Daniel%20Wei%20and%20Jialin%20Yang%20and%20Jiawei%20Zhou%20and%20Ziqiao%20Ma%20and%20Freda%20Shi&entry.1292438233=%20%20We%20introduce%20VLM-Lens%2C%20a%20toolkit%20designed%20to%20enable%20systematic%20benchmarking%2C%0Aanalysis%2C%20and%20interpretation%20of%20vision-language%20models%20%28VLMs%29%20by%20supporting%20the%0Aextraction%20of%20intermediate%20outputs%20from%20any%20layer%20during%20the%20forward%20pass%20of%0Aopen-source%20VLMs.%20VLM-Lens%20provides%20a%20unified%2C%20YAML-configurable%20interface%20that%0Aabstracts%20away%20model-specific%20complexities%20and%20supports%20user-friendly%20operation%0Aacross%20diverse%20VLMs.%20It%20currently%20supports%2016%20state-of-the-art%20base%20VLMs%20and%0Atheir%20over%2030%20variants%2C%20and%20is%20extensible%20to%20accommodate%20new%20models%20without%0Achanging%20the%20core%20logic.%0A%20%20The%20toolkit%20integrates%20easily%20with%20various%20interpretability%20and%20analysis%0Amethods.%20We%20demonstrate%20its%20usage%20with%20two%20simple%20analytical%20experiments%2C%0Arevealing%20systematic%20differences%20in%20the%20hidden%20representations%20of%20VLMs%20across%0Alayers%20and%20target%20concepts.%20VLM-Lens%20is%20released%20as%20an%20open-sourced%20project%20to%0Aaccelerate%20community%20efforts%20in%20understanding%20and%20improving%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02292v1&entry.124074799=Read"},
{"title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions", "author": "Bo-Hsu Ke and You-Zhe Xie and Yu-Lun Liu and Wei-Chen Chiu", "abstract": "  3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/\n", "link": "http://arxiv.org/abs/2510.02314v1", "date": "2025-10-02", "relevancy": 2.8702, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5974}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5805}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StealthAttack%3A%20Robust%203D%20Gaussian%20Splatting%20Poisoning%20via%20Density-Guided%0A%20%20Illusions&body=Title%3A%20StealthAttack%3A%20Robust%203D%20Gaussian%20Splatting%20Poisoning%20via%20Density-Guided%0A%20%20Illusions%0AAuthor%3A%20Bo-Hsu%20Ke%20and%20You-Zhe%20Xie%20and%20Yu-Lun%20Liu%20and%20Wei-Chen%20Chiu%0AAbstract%3A%20%20%203D%20scene%20representation%20methods%20like%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20significantly%20advanced%20novel%20view%20synthesis.%20As%0Athese%20methods%20become%20prevalent%2C%20addressing%20their%20vulnerabilities%20becomes%0Acritical.%20We%20analyze%203DGS%20robustness%20against%20image-level%20poisoning%20attacks%20and%0Apropose%20a%20novel%20density-guided%20poisoning%20method.%20Our%20method%20strategically%0Ainjects%20Gaussian%20points%20into%20low-density%20regions%20identified%20via%20Kernel%20Density%0AEstimation%20%28KDE%29%2C%20embedding%20viewpoint-dependent%20illusory%20objects%20clearly%0Avisible%20from%20poisoned%20views%20while%20minimally%20affecting%20innocent%20views.%0AAdditionally%2C%20we%20introduce%20an%20adaptive%20noise%20strategy%20to%20disrupt%20multi-view%0Aconsistency%2C%20further%20enhancing%20attack%20effectiveness.%20We%20propose%20a%20KDE-based%0Aevaluation%20protocol%20to%20assess%20attack%20difficulty%20systematically%2C%20enabling%0Aobjective%20benchmarking%20for%20future%20research.%20Extensive%20experiments%20demonstrate%0Aour%20method%27s%20superior%20performance%20compared%20to%20state-of-the-art%20techniques.%0AProject%20page%3A%20https%3A//hentci.github.io/stealthattack/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStealthAttack%253A%2520Robust%25203D%2520Gaussian%2520Splatting%2520Poisoning%2520via%2520Density-Guided%250A%2520%2520Illusions%26entry.906535625%3DBo-Hsu%2520Ke%2520and%2520You-Zhe%2520Xie%2520and%2520Yu-Lun%2520Liu%2520and%2520Wei-Chen%2520Chiu%26entry.1292438233%3D%2520%25203D%2520scene%2520representation%2520methods%2520like%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520and%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520have%2520significantly%2520advanced%2520novel%2520view%2520synthesis.%2520As%250Athese%2520methods%2520become%2520prevalent%252C%2520addressing%2520their%2520vulnerabilities%2520becomes%250Acritical.%2520We%2520analyze%25203DGS%2520robustness%2520against%2520image-level%2520poisoning%2520attacks%2520and%250Apropose%2520a%2520novel%2520density-guided%2520poisoning%2520method.%2520Our%2520method%2520strategically%250Ainjects%2520Gaussian%2520points%2520into%2520low-density%2520regions%2520identified%2520via%2520Kernel%2520Density%250AEstimation%2520%2528KDE%2529%252C%2520embedding%2520viewpoint-dependent%2520illusory%2520objects%2520clearly%250Avisible%2520from%2520poisoned%2520views%2520while%2520minimally%2520affecting%2520innocent%2520views.%250AAdditionally%252C%2520we%2520introduce%2520an%2520adaptive%2520noise%2520strategy%2520to%2520disrupt%2520multi-view%250Aconsistency%252C%2520further%2520enhancing%2520attack%2520effectiveness.%2520We%2520propose%2520a%2520KDE-based%250Aevaluation%2520protocol%2520to%2520assess%2520attack%2520difficulty%2520systematically%252C%2520enabling%250Aobjective%2520benchmarking%2520for%2520future%2520research.%2520Extensive%2520experiments%2520demonstrate%250Aour%2520method%2527s%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520techniques.%250AProject%2520page%253A%2520https%253A//hentci.github.io/stealthattack/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StealthAttack%3A%20Robust%203D%20Gaussian%20Splatting%20Poisoning%20via%20Density-Guided%0A%20%20Illusions&entry.906535625=Bo-Hsu%20Ke%20and%20You-Zhe%20Xie%20and%20Yu-Lun%20Liu%20and%20Wei-Chen%20Chiu&entry.1292438233=%20%203D%20scene%20representation%20methods%20like%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20significantly%20advanced%20novel%20view%20synthesis.%20As%0Athese%20methods%20become%20prevalent%2C%20addressing%20their%20vulnerabilities%20becomes%0Acritical.%20We%20analyze%203DGS%20robustness%20against%20image-level%20poisoning%20attacks%20and%0Apropose%20a%20novel%20density-guided%20poisoning%20method.%20Our%20method%20strategically%0Ainjects%20Gaussian%20points%20into%20low-density%20regions%20identified%20via%20Kernel%20Density%0AEstimation%20%28KDE%29%2C%20embedding%20viewpoint-dependent%20illusory%20objects%20clearly%0Avisible%20from%20poisoned%20views%20while%20minimally%20affecting%20innocent%20views.%0AAdditionally%2C%20we%20introduce%20an%20adaptive%20noise%20strategy%20to%20disrupt%20multi-view%0Aconsistency%2C%20further%20enhancing%20attack%20effectiveness.%20We%20propose%20a%20KDE-based%0Aevaluation%20protocol%20to%20assess%20attack%20difficulty%20systematically%2C%20enabling%0Aobjective%20benchmarking%20for%20future%20research.%20Extensive%20experiments%20demonstrate%0Aour%20method%27s%20superior%20performance%20compared%20to%20state-of-the-art%20techniques.%0AProject%20page%3A%20https%3A//hentci.github.io/stealthattack/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02314v1&entry.124074799=Read"},
{"title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification", "author": "Sathira Silva and Eman Ali and Chetan Arora and Muhammad Haris Khan", "abstract": "  Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.\n", "link": "http://arxiv.org/abs/2510.02270v1", "date": "2025-10-02", "relevancy": 2.8507, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6452}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.543}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20microCLIP%3A%20Unsupervised%20CLIP%20Adaptation%20via%20Coarse-Fine%20Token%20Fusion%20for%0A%20%20Fine-Grained%20Image%20Classification&body=Title%3A%20microCLIP%3A%20Unsupervised%20CLIP%20Adaptation%20via%20Coarse-Fine%20Token%20Fusion%20for%0A%20%20Fine-Grained%20Image%20Classification%0AAuthor%3A%20Sathira%20Silva%20and%20Eman%20Ali%20and%20Chetan%20Arora%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20Unsupervised%20adaptation%20of%20CLIP-based%20vision-language%20models%20%28VLMs%29%20for%0Afine-grained%20image%20classification%20requires%20sensitivity%20to%20microscopic%20local%0Acues.%20While%20CLIP%20exhibits%20strong%20zero-shot%20transfer%2C%20its%20reliance%20on%20coarse%0Aglobal%20features%20restricts%20its%20performance%20on%20fine-grained%20classification%20tasks.%0APrior%20efforts%20inject%20fine-grained%20knowledge%20by%20aligning%20large%20language%20model%0A%28LLM%29%20descriptions%20with%20the%20CLIP%20%24%5Ctexttt%7B%5BCLS%5D%7D%24%20token%3B%20however%2C%20this%20approach%0Aoverlooks%20spatial%20precision.%20We%20propose%20%24%5Ctextbf%7BmicroCLIP%7D%24%2C%20a%20self-training%0Aframework%20that%20jointly%20refines%20CLIP%27s%20visual%20and%20textual%20representations%20using%0Afine-grained%20cues.%20At%20its%20core%20is%20Saliency-Oriented%20Attention%20Pooling%20%28SOAP%29%0Awithin%20a%20lightweight%20TokenFusion%20module%2C%20which%20builds%20a%20saliency-guided%0A%24%5Ctexttt%7B%5BFG%5D%7D%24%20token%20from%20patch%20embeddings%20and%20fuses%20it%20with%20the%20global%0A%24%5Ctexttt%7B%5BCLS%5D%7D%24%20token%20for%20coarse-fine%20alignment.%20To%20stabilize%20adaptation%2C%20we%0Aintroduce%20a%20two-headed%20LLM-derived%20classifier%3A%20a%20frozen%20classifier%20that%2C%20via%0Amulti-view%20alignment%2C%20provides%20a%20stable%20text-based%20prior%20for%20pseudo-labeling%2C%0Aand%20a%20learnable%20classifier%20initialized%20from%20LLM%20descriptions%20and%20fine-tuned%0Awith%20TokenFusion.%20We%20further%20develop%20Dynamic%20Knowledge%20Aggregation%2C%20which%0Aconvexly%20combines%20fixed%20LLM/CLIP%20priors%20with%20TokenFusion%27s%20evolving%20logits%20to%0Aiteratively%20refine%20pseudo-labels.%20Together%2C%20these%20components%20uncover%20latent%0Afine-grained%20signals%20in%20CLIP%2C%20yielding%20a%20consistent%20%242.90%5C%25%24%20average%20accuracy%0Again%20across%2013%20fine-grained%20benchmarks%20while%20requiring%20only%20light%20adaptation.%0AOur%20code%20is%20available%20at%20https%3A//github.com/sathiiii/microCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmicroCLIP%253A%2520Unsupervised%2520CLIP%2520Adaptation%2520via%2520Coarse-Fine%2520Token%2520Fusion%2520for%250A%2520%2520Fine-Grained%2520Image%2520Classification%26entry.906535625%3DSathira%2520Silva%2520and%2520Eman%2520Ali%2520and%2520Chetan%2520Arora%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520Unsupervised%2520adaptation%2520of%2520CLIP-based%2520vision-language%2520models%2520%2528VLMs%2529%2520for%250Afine-grained%2520image%2520classification%2520requires%2520sensitivity%2520to%2520microscopic%2520local%250Acues.%2520While%2520CLIP%2520exhibits%2520strong%2520zero-shot%2520transfer%252C%2520its%2520reliance%2520on%2520coarse%250Aglobal%2520features%2520restricts%2520its%2520performance%2520on%2520fine-grained%2520classification%2520tasks.%250APrior%2520efforts%2520inject%2520fine-grained%2520knowledge%2520by%2520aligning%2520large%2520language%2520model%250A%2528LLM%2529%2520descriptions%2520with%2520the%2520CLIP%2520%2524%255Ctexttt%257B%255BCLS%255D%257D%2524%2520token%253B%2520however%252C%2520this%2520approach%250Aoverlooks%2520spatial%2520precision.%2520We%2520propose%2520%2524%255Ctextbf%257BmicroCLIP%257D%2524%252C%2520a%2520self-training%250Aframework%2520that%2520jointly%2520refines%2520CLIP%2527s%2520visual%2520and%2520textual%2520representations%2520using%250Afine-grained%2520cues.%2520At%2520its%2520core%2520is%2520Saliency-Oriented%2520Attention%2520Pooling%2520%2528SOAP%2529%250Awithin%2520a%2520lightweight%2520TokenFusion%2520module%252C%2520which%2520builds%2520a%2520saliency-guided%250A%2524%255Ctexttt%257B%255BFG%255D%257D%2524%2520token%2520from%2520patch%2520embeddings%2520and%2520fuses%2520it%2520with%2520the%2520global%250A%2524%255Ctexttt%257B%255BCLS%255D%257D%2524%2520token%2520for%2520coarse-fine%2520alignment.%2520To%2520stabilize%2520adaptation%252C%2520we%250Aintroduce%2520a%2520two-headed%2520LLM-derived%2520classifier%253A%2520a%2520frozen%2520classifier%2520that%252C%2520via%250Amulti-view%2520alignment%252C%2520provides%2520a%2520stable%2520text-based%2520prior%2520for%2520pseudo-labeling%252C%250Aand%2520a%2520learnable%2520classifier%2520initialized%2520from%2520LLM%2520descriptions%2520and%2520fine-tuned%250Awith%2520TokenFusion.%2520We%2520further%2520develop%2520Dynamic%2520Knowledge%2520Aggregation%252C%2520which%250Aconvexly%2520combines%2520fixed%2520LLM/CLIP%2520priors%2520with%2520TokenFusion%2527s%2520evolving%2520logits%2520to%250Aiteratively%2520refine%2520pseudo-labels.%2520Together%252C%2520these%2520components%2520uncover%2520latent%250Afine-grained%2520signals%2520in%2520CLIP%252C%2520yielding%2520a%2520consistent%2520%25242.90%255C%2525%2524%2520average%2520accuracy%250Again%2520across%252013%2520fine-grained%2520benchmarks%2520while%2520requiring%2520only%2520light%2520adaptation.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/sathiiii/microCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=microCLIP%3A%20Unsupervised%20CLIP%20Adaptation%20via%20Coarse-Fine%20Token%20Fusion%20for%0A%20%20Fine-Grained%20Image%20Classification&entry.906535625=Sathira%20Silva%20and%20Eman%20Ali%20and%20Chetan%20Arora%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20Unsupervised%20adaptation%20of%20CLIP-based%20vision-language%20models%20%28VLMs%29%20for%0Afine-grained%20image%20classification%20requires%20sensitivity%20to%20microscopic%20local%0Acues.%20While%20CLIP%20exhibits%20strong%20zero-shot%20transfer%2C%20its%20reliance%20on%20coarse%0Aglobal%20features%20restricts%20its%20performance%20on%20fine-grained%20classification%20tasks.%0APrior%20efforts%20inject%20fine-grained%20knowledge%20by%20aligning%20large%20language%20model%0A%28LLM%29%20descriptions%20with%20the%20CLIP%20%24%5Ctexttt%7B%5BCLS%5D%7D%24%20token%3B%20however%2C%20this%20approach%0Aoverlooks%20spatial%20precision.%20We%20propose%20%24%5Ctextbf%7BmicroCLIP%7D%24%2C%20a%20self-training%0Aframework%20that%20jointly%20refines%20CLIP%27s%20visual%20and%20textual%20representations%20using%0Afine-grained%20cues.%20At%20its%20core%20is%20Saliency-Oriented%20Attention%20Pooling%20%28SOAP%29%0Awithin%20a%20lightweight%20TokenFusion%20module%2C%20which%20builds%20a%20saliency-guided%0A%24%5Ctexttt%7B%5BFG%5D%7D%24%20token%20from%20patch%20embeddings%20and%20fuses%20it%20with%20the%20global%0A%24%5Ctexttt%7B%5BCLS%5D%7D%24%20token%20for%20coarse-fine%20alignment.%20To%20stabilize%20adaptation%2C%20we%0Aintroduce%20a%20two-headed%20LLM-derived%20classifier%3A%20a%20frozen%20classifier%20that%2C%20via%0Amulti-view%20alignment%2C%20provides%20a%20stable%20text-based%20prior%20for%20pseudo-labeling%2C%0Aand%20a%20learnable%20classifier%20initialized%20from%20LLM%20descriptions%20and%20fine-tuned%0Awith%20TokenFusion.%20We%20further%20develop%20Dynamic%20Knowledge%20Aggregation%2C%20which%0Aconvexly%20combines%20fixed%20LLM/CLIP%20priors%20with%20TokenFusion%27s%20evolving%20logits%20to%0Aiteratively%20refine%20pseudo-labels.%20Together%2C%20these%20components%20uncover%20latent%0Afine-grained%20signals%20in%20CLIP%2C%20yielding%20a%20consistent%20%242.90%5C%25%24%20average%20accuracy%0Again%20across%2013%20fine-grained%20benchmarks%20while%20requiring%20only%20light%20adaptation.%0AOur%20code%20is%20available%20at%20https%3A//github.com/sathiiii/microCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02270v1&entry.124074799=Read"},
{"title": "Interactive Training: Feedback-Driven Neural Network Optimization", "author": "Wentao Zhang and Yang Young Lu and Yuntian Deng", "abstract": "  Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.\n", "link": "http://arxiv.org/abs/2510.02297v1", "date": "2025-10-02", "relevancy": 2.7881, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5932}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5755}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Training%3A%20Feedback-Driven%20Neural%20Network%20Optimization&body=Title%3A%20Interactive%20Training%3A%20Feedback-Driven%20Neural%20Network%20Optimization%0AAuthor%3A%20Wentao%20Zhang%20and%20Yang%20Young%20Lu%20and%20Yuntian%20Deng%0AAbstract%3A%20%20%20Traditional%20neural%20network%20training%20typically%20follows%20fixed%2C%20predefined%0Aoptimization%20recipes%2C%20lacking%20the%20flexibility%20to%20dynamically%20respond%20to%0Ainstabilities%20or%20emerging%20training%20issues.%20In%20this%20paper%2C%20we%20introduce%0AInteractive%20Training%2C%20an%20open-source%20framework%20that%20enables%20real-time%2C%0Afeedback-driven%20intervention%20during%20neural%20network%20training%20by%20human%20experts%20or%0Aautomated%20AI%20agents.%20At%20its%20core%2C%20Interactive%20Training%20uses%20a%20control%20server%20to%0Amediate%20communication%20between%20users%20or%20agents%20and%20the%20ongoing%20training%20process%2C%0Aallowing%20users%20to%20dynamically%20adjust%20optimizer%20hyperparameters%2C%20training%20data%2C%0Aand%20model%20checkpoints.%20Through%20three%20case%20studies%2C%20we%20demonstrate%20that%0AInteractive%20Training%20achieves%20superior%20training%20stability%2C%20reduced%20sensitivity%0Ato%20initial%20hyperparameters%2C%20and%20improved%20adaptability%20to%20evolving%20user%20needs%2C%0Apaving%20the%20way%20toward%20a%20future%20training%20paradigm%20where%20AI%20agents%20autonomously%0Amonitor%20training%20logs%2C%20proactively%20resolve%20instabilities%2C%20and%20optimize%20training%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Training%253A%2520Feedback-Driven%2520Neural%2520Network%2520Optimization%26entry.906535625%3DWentao%2520Zhang%2520and%2520Yang%2520Young%2520Lu%2520and%2520Yuntian%2520Deng%26entry.1292438233%3D%2520%2520Traditional%2520neural%2520network%2520training%2520typically%2520follows%2520fixed%252C%2520predefined%250Aoptimization%2520recipes%252C%2520lacking%2520the%2520flexibility%2520to%2520dynamically%2520respond%2520to%250Ainstabilities%2520or%2520emerging%2520training%2520issues.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AInteractive%2520Training%252C%2520an%2520open-source%2520framework%2520that%2520enables%2520real-time%252C%250Afeedback-driven%2520intervention%2520during%2520neural%2520network%2520training%2520by%2520human%2520experts%2520or%250Aautomated%2520AI%2520agents.%2520At%2520its%2520core%252C%2520Interactive%2520Training%2520uses%2520a%2520control%2520server%2520to%250Amediate%2520communication%2520between%2520users%2520or%2520agents%2520and%2520the%2520ongoing%2520training%2520process%252C%250Aallowing%2520users%2520to%2520dynamically%2520adjust%2520optimizer%2520hyperparameters%252C%2520training%2520data%252C%250Aand%2520model%2520checkpoints.%2520Through%2520three%2520case%2520studies%252C%2520we%2520demonstrate%2520that%250AInteractive%2520Training%2520achieves%2520superior%2520training%2520stability%252C%2520reduced%2520sensitivity%250Ato%2520initial%2520hyperparameters%252C%2520and%2520improved%2520adaptability%2520to%2520evolving%2520user%2520needs%252C%250Apaving%2520the%2520way%2520toward%2520a%2520future%2520training%2520paradigm%2520where%2520AI%2520agents%2520autonomously%250Amonitor%2520training%2520logs%252C%2520proactively%2520resolve%2520instabilities%252C%2520and%2520optimize%2520training%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Training%3A%20Feedback-Driven%20Neural%20Network%20Optimization&entry.906535625=Wentao%20Zhang%20and%20Yang%20Young%20Lu%20and%20Yuntian%20Deng&entry.1292438233=%20%20Traditional%20neural%20network%20training%20typically%20follows%20fixed%2C%20predefined%0Aoptimization%20recipes%2C%20lacking%20the%20flexibility%20to%20dynamically%20respond%20to%0Ainstabilities%20or%20emerging%20training%20issues.%20In%20this%20paper%2C%20we%20introduce%0AInteractive%20Training%2C%20an%20open-source%20framework%20that%20enables%20real-time%2C%0Afeedback-driven%20intervention%20during%20neural%20network%20training%20by%20human%20experts%20or%0Aautomated%20AI%20agents.%20At%20its%20core%2C%20Interactive%20Training%20uses%20a%20control%20server%20to%0Amediate%20communication%20between%20users%20or%20agents%20and%20the%20ongoing%20training%20process%2C%0Aallowing%20users%20to%20dynamically%20adjust%20optimizer%20hyperparameters%2C%20training%20data%2C%0Aand%20model%20checkpoints.%20Through%20three%20case%20studies%2C%20we%20demonstrate%20that%0AInteractive%20Training%20achieves%20superior%20training%20stability%2C%20reduced%20sensitivity%0Ato%20initial%20hyperparameters%2C%20and%20improved%20adaptability%20to%20evolving%20user%20needs%2C%0Apaving%20the%20way%20toward%20a%20future%20training%20paradigm%20where%20AI%20agents%20autonomously%0Amonitor%20training%20logs%2C%20proactively%20resolve%20instabilities%2C%20and%20optimize%20training%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02297v1&entry.124074799=Read"},
{"title": "Performance-Guided Refinement for Visual Aerial Navigation using\n  Editable Gaussian Splatting in FalconGym 2.0", "author": "Yan Miao and Ege Yuceel and Georgios Fainekos and Bardh Hoxha and Hideki Okamoto and Sayan Mitra", "abstract": "  Visual policy design is crucial for aerial navigation. However,\nstate-of-the-art visual policies often overfit to a single track and their\nperformance degrades when track geometry changes. We develop FalconGym 2.0, a\nphotorealistic simulation framework built on Gaussian Splatting (GSplat) with\nan Edit API that programmatically generates diverse static and dynamic tracks\nin milliseconds. Leveraging FalconGym 2.0's editability, we propose a\nPerformance-Guided Refinement (PGR) algorithm, which concentrates visual\npolicy's training on challenging tracks while iteratively improving its\nperformance. Across two case studies (fixed-wing UAVs and quadrotors) with\ndistinct dynamics and environments, we show that a single visual policy trained\nwith PGR in FalconGym 2.0 outperforms state-of-the-art baselines in\ngeneralization and robustness: it generalizes to three unseen tracks with 100%\nsuccess without per-track retraining and maintains higher success rates under\ngate-pose perturbations. Finally, we demonstrate that the visual policy trained\nwith PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a\nquadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30\ntrials spanning two three-gate tracks and a moving-gate track.\n", "link": "http://arxiv.org/abs/2510.02248v1", "date": "2025-10-02", "relevancy": 2.7534, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.582}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5386}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance-Guided%20Refinement%20for%20Visual%20Aerial%20Navigation%20using%0A%20%20Editable%20Gaussian%20Splatting%20in%20FalconGym%202.0&body=Title%3A%20Performance-Guided%20Refinement%20for%20Visual%20Aerial%20Navigation%20using%0A%20%20Editable%20Gaussian%20Splatting%20in%20FalconGym%202.0%0AAuthor%3A%20Yan%20Miao%20and%20Ege%20Yuceel%20and%20Georgios%20Fainekos%20and%20Bardh%20Hoxha%20and%20Hideki%20Okamoto%20and%20Sayan%20Mitra%0AAbstract%3A%20%20%20Visual%20policy%20design%20is%20crucial%20for%20aerial%20navigation.%20However%2C%0Astate-of-the-art%20visual%20policies%20often%20overfit%20to%20a%20single%20track%20and%20their%0Aperformance%20degrades%20when%20track%20geometry%20changes.%20We%20develop%20FalconGym%202.0%2C%20a%0Aphotorealistic%20simulation%20framework%20built%20on%20Gaussian%20Splatting%20%28GSplat%29%20with%0Aan%20Edit%20API%20that%20programmatically%20generates%20diverse%20static%20and%20dynamic%20tracks%0Ain%20milliseconds.%20Leveraging%20FalconGym%202.0%27s%20editability%2C%20we%20propose%20a%0APerformance-Guided%20Refinement%20%28PGR%29%20algorithm%2C%20which%20concentrates%20visual%0Apolicy%27s%20training%20on%20challenging%20tracks%20while%20iteratively%20improving%20its%0Aperformance.%20Across%20two%20case%20studies%20%28fixed-wing%20UAVs%20and%20quadrotors%29%20with%0Adistinct%20dynamics%20and%20environments%2C%20we%20show%20that%20a%20single%20visual%20policy%20trained%0Awith%20PGR%20in%20FalconGym%202.0%20outperforms%20state-of-the-art%20baselines%20in%0Ageneralization%20and%20robustness%3A%20it%20generalizes%20to%20three%20unseen%20tracks%20with%20100%25%0Asuccess%20without%20per-track%20retraining%20and%20maintains%20higher%20success%20rates%20under%0Agate-pose%20perturbations.%20Finally%2C%20we%20demonstrate%20that%20the%20visual%20policy%20trained%0Awith%20PGR%20in%20FalconGym%202.0%20can%20be%20zero-shot%20sim-to-real%20transferred%20to%20a%0Aquadrotor%20hardware%2C%20achieving%20a%2098.6%25%20success%20rate%20%2869%20/%2070%20gates%29%20over%2030%0Atrials%20spanning%20two%20three-gate%20tracks%20and%20a%20moving-gate%20track.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance-Guided%2520Refinement%2520for%2520Visual%2520Aerial%2520Navigation%2520using%250A%2520%2520Editable%2520Gaussian%2520Splatting%2520in%2520FalconGym%25202.0%26entry.906535625%3DYan%2520Miao%2520and%2520Ege%2520Yuceel%2520and%2520Georgios%2520Fainekos%2520and%2520Bardh%2520Hoxha%2520and%2520Hideki%2520Okamoto%2520and%2520Sayan%2520Mitra%26entry.1292438233%3D%2520%2520Visual%2520policy%2520design%2520is%2520crucial%2520for%2520aerial%2520navigation.%2520However%252C%250Astate-of-the-art%2520visual%2520policies%2520often%2520overfit%2520to%2520a%2520single%2520track%2520and%2520their%250Aperformance%2520degrades%2520when%2520track%2520geometry%2520changes.%2520We%2520develop%2520FalconGym%25202.0%252C%2520a%250Aphotorealistic%2520simulation%2520framework%2520built%2520on%2520Gaussian%2520Splatting%2520%2528GSplat%2529%2520with%250Aan%2520Edit%2520API%2520that%2520programmatically%2520generates%2520diverse%2520static%2520and%2520dynamic%2520tracks%250Ain%2520milliseconds.%2520Leveraging%2520FalconGym%25202.0%2527s%2520editability%252C%2520we%2520propose%2520a%250APerformance-Guided%2520Refinement%2520%2528PGR%2529%2520algorithm%252C%2520which%2520concentrates%2520visual%250Apolicy%2527s%2520training%2520on%2520challenging%2520tracks%2520while%2520iteratively%2520improving%2520its%250Aperformance.%2520Across%2520two%2520case%2520studies%2520%2528fixed-wing%2520UAVs%2520and%2520quadrotors%2529%2520with%250Adistinct%2520dynamics%2520and%2520environments%252C%2520we%2520show%2520that%2520a%2520single%2520visual%2520policy%2520trained%250Awith%2520PGR%2520in%2520FalconGym%25202.0%2520outperforms%2520state-of-the-art%2520baselines%2520in%250Ageneralization%2520and%2520robustness%253A%2520it%2520generalizes%2520to%2520three%2520unseen%2520tracks%2520with%2520100%2525%250Asuccess%2520without%2520per-track%2520retraining%2520and%2520maintains%2520higher%2520success%2520rates%2520under%250Agate-pose%2520perturbations.%2520Finally%252C%2520we%2520demonstrate%2520that%2520the%2520visual%2520policy%2520trained%250Awith%2520PGR%2520in%2520FalconGym%25202.0%2520can%2520be%2520zero-shot%2520sim-to-real%2520transferred%2520to%2520a%250Aquadrotor%2520hardware%252C%2520achieving%2520a%252098.6%2525%2520success%2520rate%2520%252869%2520/%252070%2520gates%2529%2520over%252030%250Atrials%2520spanning%2520two%2520three-gate%2520tracks%2520and%2520a%2520moving-gate%2520track.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance-Guided%20Refinement%20for%20Visual%20Aerial%20Navigation%20using%0A%20%20Editable%20Gaussian%20Splatting%20in%20FalconGym%202.0&entry.906535625=Yan%20Miao%20and%20Ege%20Yuceel%20and%20Georgios%20Fainekos%20and%20Bardh%20Hoxha%20and%20Hideki%20Okamoto%20and%20Sayan%20Mitra&entry.1292438233=%20%20Visual%20policy%20design%20is%20crucial%20for%20aerial%20navigation.%20However%2C%0Astate-of-the-art%20visual%20policies%20often%20overfit%20to%20a%20single%20track%20and%20their%0Aperformance%20degrades%20when%20track%20geometry%20changes.%20We%20develop%20FalconGym%202.0%2C%20a%0Aphotorealistic%20simulation%20framework%20built%20on%20Gaussian%20Splatting%20%28GSplat%29%20with%0Aan%20Edit%20API%20that%20programmatically%20generates%20diverse%20static%20and%20dynamic%20tracks%0Ain%20milliseconds.%20Leveraging%20FalconGym%202.0%27s%20editability%2C%20we%20propose%20a%0APerformance-Guided%20Refinement%20%28PGR%29%20algorithm%2C%20which%20concentrates%20visual%0Apolicy%27s%20training%20on%20challenging%20tracks%20while%20iteratively%20improving%20its%0Aperformance.%20Across%20two%20case%20studies%20%28fixed-wing%20UAVs%20and%20quadrotors%29%20with%0Adistinct%20dynamics%20and%20environments%2C%20we%20show%20that%20a%20single%20visual%20policy%20trained%0Awith%20PGR%20in%20FalconGym%202.0%20outperforms%20state-of-the-art%20baselines%20in%0Ageneralization%20and%20robustness%3A%20it%20generalizes%20to%20three%20unseen%20tracks%20with%20100%25%0Asuccess%20without%20per-track%20retraining%20and%20maintains%20higher%20success%20rates%20under%0Agate-pose%20perturbations.%20Finally%2C%20we%20demonstrate%20that%20the%20visual%20policy%20trained%0Awith%20PGR%20in%20FalconGym%202.0%20can%20be%20zero-shot%20sim-to-real%20transferred%20to%20a%0Aquadrotor%20hardware%2C%20achieving%20a%2098.6%25%20success%20rate%20%2869%20/%2070%20gates%29%20over%2030%0Atrials%20spanning%20two%20three-gate%20tracks%20and%20a%20moving-gate%20track.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02248v1&entry.124074799=Read"},
{"title": "Clink! Chop! Thud! -- Learning Object Sounds from Real-World\n  Interactions", "author": "Mengyu Yang and Yiming Chen and Haozheng Pei and Siddhant Agarwal and Arun Balajee Vasudevan and James Hays", "abstract": "  Can a model distinguish between the sound of a spoon hitting a hardwood floor\nversus a carpeted one? Everyday object interactions produce sounds unique to\nthe objects involved. We introduce the sounding object detection task to\nevaluate a model's ability to link these sounds to the objects directly\ninvolved. Inspired by human perception, our multimodal object-aware framework\nlearns from in-the-wild egocentric videos. To encourage an object-centric\napproach, we first develop an automatic pipeline to compute segmentation masks\nof the objects involved to guide the model's focus during training towards the\nmost informative regions of the interaction. A slot attention visual encoder is\nused to further enforce an object prior. We demonstrate state of the art\nperformance on our new task along with existing multimodal action understanding\ntasks.\n", "link": "http://arxiv.org/abs/2510.02313v1", "date": "2025-10-02", "relevancy": 2.7309, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clink%21%20Chop%21%20Thud%21%20--%20Learning%20Object%20Sounds%20from%20Real-World%0A%20%20Interactions&body=Title%3A%20Clink%21%20Chop%21%20Thud%21%20--%20Learning%20Object%20Sounds%20from%20Real-World%0A%20%20Interactions%0AAuthor%3A%20Mengyu%20Yang%20and%20Yiming%20Chen%20and%20Haozheng%20Pei%20and%20Siddhant%20Agarwal%20and%20Arun%20Balajee%20Vasudevan%20and%20James%20Hays%0AAbstract%3A%20%20%20Can%20a%20model%20distinguish%20between%20the%20sound%20of%20a%20spoon%20hitting%20a%20hardwood%20floor%0Aversus%20a%20carpeted%20one%3F%20Everyday%20object%20interactions%20produce%20sounds%20unique%20to%0Athe%20objects%20involved.%20We%20introduce%20the%20sounding%20object%20detection%20task%20to%0Aevaluate%20a%20model%27s%20ability%20to%20link%20these%20sounds%20to%20the%20objects%20directly%0Ainvolved.%20Inspired%20by%20human%20perception%2C%20our%20multimodal%20object-aware%20framework%0Alearns%20from%20in-the-wild%20egocentric%20videos.%20To%20encourage%20an%20object-centric%0Aapproach%2C%20we%20first%20develop%20an%20automatic%20pipeline%20to%20compute%20segmentation%20masks%0Aof%20the%20objects%20involved%20to%20guide%20the%20model%27s%20focus%20during%20training%20towards%20the%0Amost%20informative%20regions%20of%20the%20interaction.%20A%20slot%20attention%20visual%20encoder%20is%0Aused%20to%20further%20enforce%20an%20object%20prior.%20We%20demonstrate%20state%20of%20the%20art%0Aperformance%20on%20our%20new%20task%20along%20with%20existing%20multimodal%20action%20understanding%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClink%2521%2520Chop%2521%2520Thud%2521%2520--%2520Learning%2520Object%2520Sounds%2520from%2520Real-World%250A%2520%2520Interactions%26entry.906535625%3DMengyu%2520Yang%2520and%2520Yiming%2520Chen%2520and%2520Haozheng%2520Pei%2520and%2520Siddhant%2520Agarwal%2520and%2520Arun%2520Balajee%2520Vasudevan%2520and%2520James%2520Hays%26entry.1292438233%3D%2520%2520Can%2520a%2520model%2520distinguish%2520between%2520the%2520sound%2520of%2520a%2520spoon%2520hitting%2520a%2520hardwood%2520floor%250Aversus%2520a%2520carpeted%2520one%253F%2520Everyday%2520object%2520interactions%2520produce%2520sounds%2520unique%2520to%250Athe%2520objects%2520involved.%2520We%2520introduce%2520the%2520sounding%2520object%2520detection%2520task%2520to%250Aevaluate%2520a%2520model%2527s%2520ability%2520to%2520link%2520these%2520sounds%2520to%2520the%2520objects%2520directly%250Ainvolved.%2520Inspired%2520by%2520human%2520perception%252C%2520our%2520multimodal%2520object-aware%2520framework%250Alearns%2520from%2520in-the-wild%2520egocentric%2520videos.%2520To%2520encourage%2520an%2520object-centric%250Aapproach%252C%2520we%2520first%2520develop%2520an%2520automatic%2520pipeline%2520to%2520compute%2520segmentation%2520masks%250Aof%2520the%2520objects%2520involved%2520to%2520guide%2520the%2520model%2527s%2520focus%2520during%2520training%2520towards%2520the%250Amost%2520informative%2520regions%2520of%2520the%2520interaction.%2520A%2520slot%2520attention%2520visual%2520encoder%2520is%250Aused%2520to%2520further%2520enforce%2520an%2520object%2520prior.%2520We%2520demonstrate%2520state%2520of%2520the%2520art%250Aperformance%2520on%2520our%2520new%2520task%2520along%2520with%2520existing%2520multimodal%2520action%2520understanding%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clink%21%20Chop%21%20Thud%21%20--%20Learning%20Object%20Sounds%20from%20Real-World%0A%20%20Interactions&entry.906535625=Mengyu%20Yang%20and%20Yiming%20Chen%20and%20Haozheng%20Pei%20and%20Siddhant%20Agarwal%20and%20Arun%20Balajee%20Vasudevan%20and%20James%20Hays&entry.1292438233=%20%20Can%20a%20model%20distinguish%20between%20the%20sound%20of%20a%20spoon%20hitting%20a%20hardwood%20floor%0Aversus%20a%20carpeted%20one%3F%20Everyday%20object%20interactions%20produce%20sounds%20unique%20to%0Athe%20objects%20involved.%20We%20introduce%20the%20sounding%20object%20detection%20task%20to%0Aevaluate%20a%20model%27s%20ability%20to%20link%20these%20sounds%20to%20the%20objects%20directly%0Ainvolved.%20Inspired%20by%20human%20perception%2C%20our%20multimodal%20object-aware%20framework%0Alearns%20from%20in-the-wild%20egocentric%20videos.%20To%20encourage%20an%20object-centric%0Aapproach%2C%20we%20first%20develop%20an%20automatic%20pipeline%20to%20compute%20segmentation%20masks%0Aof%20the%20objects%20involved%20to%20guide%20the%20model%27s%20focus%20during%20training%20towards%20the%0Amost%20informative%20regions%20of%20the%20interaction.%20A%20slot%20attention%20visual%20encoder%20is%0Aused%20to%20further%20enforce%20an%20object%20prior.%20We%20demonstrate%20state%20of%20the%20art%0Aperformance%20on%20our%20new%20task%20along%20with%20existing%20multimodal%20action%20understanding%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02313v1&entry.124074799=Read"},
{"title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features", "author": "Xudong Zhu and Mohammad Mahdi Khalili and Zhihui Zhu", "abstract": "  Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.\n", "link": "http://arxiv.org/abs/2510.00404v2", "date": "2025-10-02", "relevancy": 2.6464, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AbsTopK%3A%20Rethinking%20Sparse%20Autoencoders%20For%20Bidirectional%20Features&body=Title%3A%20AbsTopK%3A%20Rethinking%20Sparse%20Autoencoders%20For%20Bidirectional%20Features%0AAuthor%3A%20Xudong%20Zhu%20and%20Mohammad%20Mahdi%20Khalili%20and%20Zhihui%20Zhu%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20emerged%20as%20powerful%20techniques%20for%0Ainterpretability%20of%20large%20language%20models%20%28LLMs%29%2C%20aiming%20to%20decompose%20hidden%0Astates%20into%20meaningful%20semantic%20features.%20While%20several%20SAE%20variants%20have%20been%0Aproposed%2C%20there%20remains%20no%20principled%20framework%20to%20derive%20SAEs%20from%20the%0Aoriginal%20dictionary%20learning%20formulation.%20In%20this%20work%2C%20we%20introduce%20such%20a%0Aframework%20by%20unrolling%20the%20proximal%20gradient%20method%20for%20sparse%20coding.%20We%20show%0Athat%20a%20single-step%20update%20naturally%20recovers%20common%20SAE%20variants%2C%20including%0AReLU%2C%20JumpReLU%2C%20and%20TopK.%20Through%20this%20lens%2C%20we%20reveal%20a%20fundamental%20limitation%0Aof%20existing%20SAEs%3A%20their%20sparsity-inducing%20regularizers%20enforce%20non-negativity%2C%0Apreventing%20a%20single%20feature%20from%20representing%20bidirectional%20concepts%20%28e.g.%2C%0Amale%20vs.%20female%29.%20This%20structural%20constraint%20fragments%20semantic%20axes%20into%0Aseparate%2C%20redundant%20features%2C%20limiting%20representational%20completeness.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20AbsTopK%20SAE%2C%20a%20new%20variant%20derived%20from%20the%0A%24%5Cell_0%24%20sparsity%20constraint%20that%20applies%20hard%20thresholding%20over%20the%0Alargest-magnitude%20activations.%20By%20preserving%20both%20positive%20and%20negative%0Aactivations%2C%20AbsTopK%20uncovers%20richer%2C%20bidirectional%20conceptual%20representations.%0AComprehensive%20experiments%20across%20four%20LLMs%20and%20seven%20probing%20and%20steering%20tasks%0Ashow%20that%20AbsTopK%20improves%20reconstruction%20fidelity%2C%20enhances%20interpretability%2C%0Aand%20enables%20single%20features%20to%20encode%20contrasting%20concepts.%20Remarkably%2C%20AbsTopK%0Amatches%20or%20even%20surpasses%20the%20Difference-in-Mean%20method%2C%20a%20supervised%20approach%0Athat%20requires%20labeled%20data%20for%20each%20concept%20and%20has%20been%20shown%20in%20prior%20work%20to%0Aoutperform%20SAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbsTopK%253A%2520Rethinking%2520Sparse%2520Autoencoders%2520For%2520Bidirectional%2520Features%26entry.906535625%3DXudong%2520Zhu%2520and%2520Mohammad%2520Mahdi%2520Khalili%2520and%2520Zhihui%2520Zhu%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520powerful%2520techniques%2520for%250Ainterpretability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520aiming%2520to%2520decompose%2520hidden%250Astates%2520into%2520meaningful%2520semantic%2520features.%2520While%2520several%2520SAE%2520variants%2520have%2520been%250Aproposed%252C%2520there%2520remains%2520no%2520principled%2520framework%2520to%2520derive%2520SAEs%2520from%2520the%250Aoriginal%2520dictionary%2520learning%2520formulation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520such%2520a%250Aframework%2520by%2520unrolling%2520the%2520proximal%2520gradient%2520method%2520for%2520sparse%2520coding.%2520We%2520show%250Athat%2520a%2520single-step%2520update%2520naturally%2520recovers%2520common%2520SAE%2520variants%252C%2520including%250AReLU%252C%2520JumpReLU%252C%2520and%2520TopK.%2520Through%2520this%2520lens%252C%2520we%2520reveal%2520a%2520fundamental%2520limitation%250Aof%2520existing%2520SAEs%253A%2520their%2520sparsity-inducing%2520regularizers%2520enforce%2520non-negativity%252C%250Apreventing%2520a%2520single%2520feature%2520from%2520representing%2520bidirectional%2520concepts%2520%2528e.g.%252C%250Amale%2520vs.%2520female%2529.%2520This%2520structural%2520constraint%2520fragments%2520semantic%2520axes%2520into%250Aseparate%252C%2520redundant%2520features%252C%2520limiting%2520representational%2520completeness.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520AbsTopK%2520SAE%252C%2520a%2520new%2520variant%2520derived%2520from%2520the%250A%2524%255Cell_0%2524%2520sparsity%2520constraint%2520that%2520applies%2520hard%2520thresholding%2520over%2520the%250Alargest-magnitude%2520activations.%2520By%2520preserving%2520both%2520positive%2520and%2520negative%250Aactivations%252C%2520AbsTopK%2520uncovers%2520richer%252C%2520bidirectional%2520conceptual%2520representations.%250AComprehensive%2520experiments%2520across%2520four%2520LLMs%2520and%2520seven%2520probing%2520and%2520steering%2520tasks%250Ashow%2520that%2520AbsTopK%2520improves%2520reconstruction%2520fidelity%252C%2520enhances%2520interpretability%252C%250Aand%2520enables%2520single%2520features%2520to%2520encode%2520contrasting%2520concepts.%2520Remarkably%252C%2520AbsTopK%250Amatches%2520or%2520even%2520surpasses%2520the%2520Difference-in-Mean%2520method%252C%2520a%2520supervised%2520approach%250Athat%2520requires%2520labeled%2520data%2520for%2520each%2520concept%2520and%2520has%2520been%2520shown%2520in%2520prior%2520work%2520to%250Aoutperform%2520SAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AbsTopK%3A%20Rethinking%20Sparse%20Autoencoders%20For%20Bidirectional%20Features&entry.906535625=Xudong%20Zhu%20and%20Mohammad%20Mahdi%20Khalili%20and%20Zhihui%20Zhu&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20emerged%20as%20powerful%20techniques%20for%0Ainterpretability%20of%20large%20language%20models%20%28LLMs%29%2C%20aiming%20to%20decompose%20hidden%0Astates%20into%20meaningful%20semantic%20features.%20While%20several%20SAE%20variants%20have%20been%0Aproposed%2C%20there%20remains%20no%20principled%20framework%20to%20derive%20SAEs%20from%20the%0Aoriginal%20dictionary%20learning%20formulation.%20In%20this%20work%2C%20we%20introduce%20such%20a%0Aframework%20by%20unrolling%20the%20proximal%20gradient%20method%20for%20sparse%20coding.%20We%20show%0Athat%20a%20single-step%20update%20naturally%20recovers%20common%20SAE%20variants%2C%20including%0AReLU%2C%20JumpReLU%2C%20and%20TopK.%20Through%20this%20lens%2C%20we%20reveal%20a%20fundamental%20limitation%0Aof%20existing%20SAEs%3A%20their%20sparsity-inducing%20regularizers%20enforce%20non-negativity%2C%0Apreventing%20a%20single%20feature%20from%20representing%20bidirectional%20concepts%20%28e.g.%2C%0Amale%20vs.%20female%29.%20This%20structural%20constraint%20fragments%20semantic%20axes%20into%0Aseparate%2C%20redundant%20features%2C%20limiting%20representational%20completeness.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20AbsTopK%20SAE%2C%20a%20new%20variant%20derived%20from%20the%0A%24%5Cell_0%24%20sparsity%20constraint%20that%20applies%20hard%20thresholding%20over%20the%0Alargest-magnitude%20activations.%20By%20preserving%20both%20positive%20and%20negative%0Aactivations%2C%20AbsTopK%20uncovers%20richer%2C%20bidirectional%20conceptual%20representations.%0AComprehensive%20experiments%20across%20four%20LLMs%20and%20seven%20probing%20and%20steering%20tasks%0Ashow%20that%20AbsTopK%20improves%20reconstruction%20fidelity%2C%20enhances%20interpretability%2C%0Aand%20enables%20single%20features%20to%20encode%20contrasting%20concepts.%20Remarkably%2C%20AbsTopK%0Amatches%20or%20even%20surpasses%20the%20Difference-in-Mean%20method%2C%20a%20supervised%20approach%0Athat%20requires%20labeled%20data%20for%20each%20concept%20and%20has%20been%20shown%20in%20prior%20work%20to%0Aoutperform%20SAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00404v2&entry.124074799=Read"},
{"title": "BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge\n  Transfer across Biosignals", "author": "Chenqi Li and Yu Liu and Timothy Denison and Tingting Zhu", "abstract": "  Biosignals offer valuable insights into the physiological states of the human\nbody. Although biosignal modalities differ in functionality, signal fidelity,\nsensor comfort, and cost, they are often intercorrelated, reflecting the\nholistic and interconnected nature of human physiology. This opens up the\npossibility of performing the same tasks using alternative biosignal\nmodalities, thereby improving the accessibility, usability, and adaptability of\nhealth monitoring systems. However, the limited availability of large labeled\ndatasets presents challenges for training models tailored to specific tasks and\nmodalities of interest. Unsupervised cross-modal knowledge transfer offers a\npromising solution by leveraging knowledge from an existing modality to support\nmodel training for a new modality. Existing methods are typically based on\nknowledge distillation, which requires running a teacher model alongside\nstudent model training, resulting in high computational and memory overhead.\nThis challenge is further exacerbated by the recent development of foundation\nmodels that demonstrate superior performance and generalization across tasks at\nthe cost of large model sizes. To this end, we explore a new framework for\nunsupervised cross-modal knowledge transfer of biosignals by training a\nlightweight bridge network to align the intermediate representations and enable\ninformation flow between foundation models and across modalities. Specifically,\nwe introduce an efficient strategy for selecting alignment positions where the\nbridge should be constructed, along with a flexible prototype network as the\nbridge architecture. Extensive experiments across multiple biosignal\nmodalities, tasks, and datasets show that BioX-Bridge reduces the number of\ntrainable parameters by 88--99\\% while maintaining or even improving transfer\nperformance compared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2510.02276v1", "date": "2025-10-02", "relevancy": 2.6228, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BioX-Bridge%3A%20Model%20Bridging%20for%20Unsupervised%20Cross-Modal%20Knowledge%0A%20%20Transfer%20across%20Biosignals&body=Title%3A%20BioX-Bridge%3A%20Model%20Bridging%20for%20Unsupervised%20Cross-Modal%20Knowledge%0A%20%20Transfer%20across%20Biosignals%0AAuthor%3A%20Chenqi%20Li%20and%20Yu%20Liu%20and%20Timothy%20Denison%20and%20Tingting%20Zhu%0AAbstract%3A%20%20%20Biosignals%20offer%20valuable%20insights%20into%20the%20physiological%20states%20of%20the%20human%0Abody.%20Although%20biosignal%20modalities%20differ%20in%20functionality%2C%20signal%20fidelity%2C%0Asensor%20comfort%2C%20and%20cost%2C%20they%20are%20often%20intercorrelated%2C%20reflecting%20the%0Aholistic%20and%20interconnected%20nature%20of%20human%20physiology.%20This%20opens%20up%20the%0Apossibility%20of%20performing%20the%20same%20tasks%20using%20alternative%20biosignal%0Amodalities%2C%20thereby%20improving%20the%20accessibility%2C%20usability%2C%20and%20adaptability%20of%0Ahealth%20monitoring%20systems.%20However%2C%20the%20limited%20availability%20of%20large%20labeled%0Adatasets%20presents%20challenges%20for%20training%20models%20tailored%20to%20specific%20tasks%20and%0Amodalities%20of%20interest.%20Unsupervised%20cross-modal%20knowledge%20transfer%20offers%20a%0Apromising%20solution%20by%20leveraging%20knowledge%20from%20an%20existing%20modality%20to%20support%0Amodel%20training%20for%20a%20new%20modality.%20Existing%20methods%20are%20typically%20based%20on%0Aknowledge%20distillation%2C%20which%20requires%20running%20a%20teacher%20model%20alongside%0Astudent%20model%20training%2C%20resulting%20in%20high%20computational%20and%20memory%20overhead.%0AThis%20challenge%20is%20further%20exacerbated%20by%20the%20recent%20development%20of%20foundation%0Amodels%20that%20demonstrate%20superior%20performance%20and%20generalization%20across%20tasks%20at%0Athe%20cost%20of%20large%20model%20sizes.%20To%20this%20end%2C%20we%20explore%20a%20new%20framework%20for%0Aunsupervised%20cross-modal%20knowledge%20transfer%20of%20biosignals%20by%20training%20a%0Alightweight%20bridge%20network%20to%20align%20the%20intermediate%20representations%20and%20enable%0Ainformation%20flow%20between%20foundation%20models%20and%20across%20modalities.%20Specifically%2C%0Awe%20introduce%20an%20efficient%20strategy%20for%20selecting%20alignment%20positions%20where%20the%0Abridge%20should%20be%20constructed%2C%20along%20with%20a%20flexible%20prototype%20network%20as%20the%0Abridge%20architecture.%20Extensive%20experiments%20across%20multiple%20biosignal%0Amodalities%2C%20tasks%2C%20and%20datasets%20show%20that%20BioX-Bridge%20reduces%20the%20number%20of%0Atrainable%20parameters%20by%2088--99%5C%25%20while%20maintaining%20or%20even%20improving%20transfer%0Aperformance%20compared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioX-Bridge%253A%2520Model%2520Bridging%2520for%2520Unsupervised%2520Cross-Modal%2520Knowledge%250A%2520%2520Transfer%2520across%2520Biosignals%26entry.906535625%3DChenqi%2520Li%2520and%2520Yu%2520Liu%2520and%2520Timothy%2520Denison%2520and%2520Tingting%2520Zhu%26entry.1292438233%3D%2520%2520Biosignals%2520offer%2520valuable%2520insights%2520into%2520the%2520physiological%2520states%2520of%2520the%2520human%250Abody.%2520Although%2520biosignal%2520modalities%2520differ%2520in%2520functionality%252C%2520signal%2520fidelity%252C%250Asensor%2520comfort%252C%2520and%2520cost%252C%2520they%2520are%2520often%2520intercorrelated%252C%2520reflecting%2520the%250Aholistic%2520and%2520interconnected%2520nature%2520of%2520human%2520physiology.%2520This%2520opens%2520up%2520the%250Apossibility%2520of%2520performing%2520the%2520same%2520tasks%2520using%2520alternative%2520biosignal%250Amodalities%252C%2520thereby%2520improving%2520the%2520accessibility%252C%2520usability%252C%2520and%2520adaptability%2520of%250Ahealth%2520monitoring%2520systems.%2520However%252C%2520the%2520limited%2520availability%2520of%2520large%2520labeled%250Adatasets%2520presents%2520challenges%2520for%2520training%2520models%2520tailored%2520to%2520specific%2520tasks%2520and%250Amodalities%2520of%2520interest.%2520Unsupervised%2520cross-modal%2520knowledge%2520transfer%2520offers%2520a%250Apromising%2520solution%2520by%2520leveraging%2520knowledge%2520from%2520an%2520existing%2520modality%2520to%2520support%250Amodel%2520training%2520for%2520a%2520new%2520modality.%2520Existing%2520methods%2520are%2520typically%2520based%2520on%250Aknowledge%2520distillation%252C%2520which%2520requires%2520running%2520a%2520teacher%2520model%2520alongside%250Astudent%2520model%2520training%252C%2520resulting%2520in%2520high%2520computational%2520and%2520memory%2520overhead.%250AThis%2520challenge%2520is%2520further%2520exacerbated%2520by%2520the%2520recent%2520development%2520of%2520foundation%250Amodels%2520that%2520demonstrate%2520superior%2520performance%2520and%2520generalization%2520across%2520tasks%2520at%250Athe%2520cost%2520of%2520large%2520model%2520sizes.%2520To%2520this%2520end%252C%2520we%2520explore%2520a%2520new%2520framework%2520for%250Aunsupervised%2520cross-modal%2520knowledge%2520transfer%2520of%2520biosignals%2520by%2520training%2520a%250Alightweight%2520bridge%2520network%2520to%2520align%2520the%2520intermediate%2520representations%2520and%2520enable%250Ainformation%2520flow%2520between%2520foundation%2520models%2520and%2520across%2520modalities.%2520Specifically%252C%250Awe%2520introduce%2520an%2520efficient%2520strategy%2520for%2520selecting%2520alignment%2520positions%2520where%2520the%250Abridge%2520should%2520be%2520constructed%252C%2520along%2520with%2520a%2520flexible%2520prototype%2520network%2520as%2520the%250Abridge%2520architecture.%2520Extensive%2520experiments%2520across%2520multiple%2520biosignal%250Amodalities%252C%2520tasks%252C%2520and%2520datasets%2520show%2520that%2520BioX-Bridge%2520reduces%2520the%2520number%2520of%250Atrainable%2520parameters%2520by%252088--99%255C%2525%2520while%2520maintaining%2520or%2520even%2520improving%2520transfer%250Aperformance%2520compared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioX-Bridge%3A%20Model%20Bridging%20for%20Unsupervised%20Cross-Modal%20Knowledge%0A%20%20Transfer%20across%20Biosignals&entry.906535625=Chenqi%20Li%20and%20Yu%20Liu%20and%20Timothy%20Denison%20and%20Tingting%20Zhu&entry.1292438233=%20%20Biosignals%20offer%20valuable%20insights%20into%20the%20physiological%20states%20of%20the%20human%0Abody.%20Although%20biosignal%20modalities%20differ%20in%20functionality%2C%20signal%20fidelity%2C%0Asensor%20comfort%2C%20and%20cost%2C%20they%20are%20often%20intercorrelated%2C%20reflecting%20the%0Aholistic%20and%20interconnected%20nature%20of%20human%20physiology.%20This%20opens%20up%20the%0Apossibility%20of%20performing%20the%20same%20tasks%20using%20alternative%20biosignal%0Amodalities%2C%20thereby%20improving%20the%20accessibility%2C%20usability%2C%20and%20adaptability%20of%0Ahealth%20monitoring%20systems.%20However%2C%20the%20limited%20availability%20of%20large%20labeled%0Adatasets%20presents%20challenges%20for%20training%20models%20tailored%20to%20specific%20tasks%20and%0Amodalities%20of%20interest.%20Unsupervised%20cross-modal%20knowledge%20transfer%20offers%20a%0Apromising%20solution%20by%20leveraging%20knowledge%20from%20an%20existing%20modality%20to%20support%0Amodel%20training%20for%20a%20new%20modality.%20Existing%20methods%20are%20typically%20based%20on%0Aknowledge%20distillation%2C%20which%20requires%20running%20a%20teacher%20model%20alongside%0Astudent%20model%20training%2C%20resulting%20in%20high%20computational%20and%20memory%20overhead.%0AThis%20challenge%20is%20further%20exacerbated%20by%20the%20recent%20development%20of%20foundation%0Amodels%20that%20demonstrate%20superior%20performance%20and%20generalization%20across%20tasks%20at%0Athe%20cost%20of%20large%20model%20sizes.%20To%20this%20end%2C%20we%20explore%20a%20new%20framework%20for%0Aunsupervised%20cross-modal%20knowledge%20transfer%20of%20biosignals%20by%20training%20a%0Alightweight%20bridge%20network%20to%20align%20the%20intermediate%20representations%20and%20enable%0Ainformation%20flow%20between%20foundation%20models%20and%20across%20modalities.%20Specifically%2C%0Awe%20introduce%20an%20efficient%20strategy%20for%20selecting%20alignment%20positions%20where%20the%0Abridge%20should%20be%20constructed%2C%20along%20with%20a%20flexible%20prototype%20network%20as%20the%0Abridge%20architecture.%20Extensive%20experiments%20across%20multiple%20biosignal%0Amodalities%2C%20tasks%2C%20and%20datasets%20show%20that%20BioX-Bridge%20reduces%20the%20number%20of%0Atrainable%20parameters%20by%2088--99%5C%25%20while%20maintaining%20or%20even%20improving%20transfer%0Aperformance%20compared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02276v1&entry.124074799=Read"},
{"title": "ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models", "author": "Raghav Singhal and Kaustubh Ponkshe and Rohit Vartak and Praneeth Vepakomma", "abstract": "  Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget, a property we validate through matrix reconstruction experiments.\nEmpirically, ABBA achieves state-of-the-art results on arithmetic and\ncommonsense reasoning benchmarks, consistently outperforming existing PEFT\nmethods by a significant margin across multiple models. Our code is publicly\navailable at: https://github.com/CERT-Lab/abba.\n", "link": "http://arxiv.org/abs/2505.14238v3", "date": "2025-10-02", "relevancy": 2.6196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ABBA-Adapters%3A%20Efficient%20and%20Expressive%20Fine-Tuning%20of%20Foundation%20Models&body=Title%3A%20ABBA-Adapters%3A%20Efficient%20and%20Expressive%20Fine-Tuning%20of%20Foundation%20Models%0AAuthor%3A%20Raghav%20Singhal%20and%20Kaustubh%20Ponkshe%20and%20Rohit%20Vartak%20and%20Praneeth%20Vepakomma%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20demonstrated%20strong%20performance%20across%20a%20wide%0Arange%20of%20tasks%2C%20but%20adapting%20them%20efficiently%20to%20new%20domains%20remains%20a%20key%0Achallenge.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20this%20by%0Aintroducing%20lightweight%2C%20trainable%20modules%20while%20keeping%20most%20pre-trained%0Aweights%20fixed.%20The%20prevailing%20approach%2C%20LoRA%2C%20models%20updates%20using%20a%20low-rank%0Adecomposition%2C%20but%20its%20expressivity%20is%20inherently%20constrained%20by%20the%20rank.%0ARecent%20methods%20like%20HiRA%20aim%20to%20increase%20expressivity%20by%20incorporating%20a%0AHadamard%20product%20with%20the%20frozen%20weights%2C%20but%20still%20rely%20on%20the%20structure%20of%0Athe%20pre-trained%20model.%20We%20introduce%20ABBA%2C%20a%20new%20PEFT%20architecture%20that%0Areparameterizes%20the%20update%20as%20a%20Hadamard%20product%20of%20two%20independently%20learnable%0Alow-rank%20matrices.%20In%20contrast%20to%20prior%20work%2C%20ABBA%20fully%20decouples%20the%20update%0Afrom%20the%20pre-trained%20weights%2C%20enabling%20both%20components%20to%20be%20optimized%20freely.%0AThis%20leads%20to%20significantly%20higher%20expressivity%20under%20the%20same%20parameter%0Abudget%2C%20a%20property%20we%20validate%20through%20matrix%20reconstruction%20experiments.%0AEmpirically%2C%20ABBA%20achieves%20state-of-the-art%20results%20on%20arithmetic%20and%0Acommonsense%20reasoning%20benchmarks%2C%20consistently%20outperforming%20existing%20PEFT%0Amethods%20by%20a%20significant%20margin%20across%20multiple%20models.%20Our%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/CERT-Lab/abba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14238v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DABBA-Adapters%253A%2520Efficient%2520and%2520Expressive%2520Fine-Tuning%2520of%2520Foundation%2520Models%26entry.906535625%3DRaghav%2520Singhal%2520and%2520Kaustubh%2520Ponkshe%2520and%2520Rohit%2520Vartak%2520and%2520Praneeth%2520Vepakomma%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520strong%2520performance%2520across%2520a%2520wide%250Arange%2520of%2520tasks%252C%2520but%2520adapting%2520them%2520efficiently%2520to%2520new%2520domains%2520remains%2520a%2520key%250Achallenge.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520address%2520this%2520by%250Aintroducing%2520lightweight%252C%2520trainable%2520modules%2520while%2520keeping%2520most%2520pre-trained%250Aweights%2520fixed.%2520The%2520prevailing%2520approach%252C%2520LoRA%252C%2520models%2520updates%2520using%2520a%2520low-rank%250Adecomposition%252C%2520but%2520its%2520expressivity%2520is%2520inherently%2520constrained%2520by%2520the%2520rank.%250ARecent%2520methods%2520like%2520HiRA%2520aim%2520to%2520increase%2520expressivity%2520by%2520incorporating%2520a%250AHadamard%2520product%2520with%2520the%2520frozen%2520weights%252C%2520but%2520still%2520rely%2520on%2520the%2520structure%2520of%250Athe%2520pre-trained%2520model.%2520We%2520introduce%2520ABBA%252C%2520a%2520new%2520PEFT%2520architecture%2520that%250Areparameterizes%2520the%2520update%2520as%2520a%2520Hadamard%2520product%2520of%2520two%2520independently%2520learnable%250Alow-rank%2520matrices.%2520In%2520contrast%2520to%2520prior%2520work%252C%2520ABBA%2520fully%2520decouples%2520the%2520update%250Afrom%2520the%2520pre-trained%2520weights%252C%2520enabling%2520both%2520components%2520to%2520be%2520optimized%2520freely.%250AThis%2520leads%2520to%2520significantly%2520higher%2520expressivity%2520under%2520the%2520same%2520parameter%250Abudget%252C%2520a%2520property%2520we%2520validate%2520through%2520matrix%2520reconstruction%2520experiments.%250AEmpirically%252C%2520ABBA%2520achieves%2520state-of-the-art%2520results%2520on%2520arithmetic%2520and%250Acommonsense%2520reasoning%2520benchmarks%252C%2520consistently%2520outperforming%2520existing%2520PEFT%250Amethods%2520by%2520a%2520significant%2520margin%2520across%2520multiple%2520models.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/CERT-Lab/abba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14238v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ABBA-Adapters%3A%20Efficient%20and%20Expressive%20Fine-Tuning%20of%20Foundation%20Models&entry.906535625=Raghav%20Singhal%20and%20Kaustubh%20Ponkshe%20and%20Rohit%20Vartak%20and%20Praneeth%20Vepakomma&entry.1292438233=%20%20Large%20Language%20Models%20have%20demonstrated%20strong%20performance%20across%20a%20wide%0Arange%20of%20tasks%2C%20but%20adapting%20them%20efficiently%20to%20new%20domains%20remains%20a%20key%0Achallenge.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20address%20this%20by%0Aintroducing%20lightweight%2C%20trainable%20modules%20while%20keeping%20most%20pre-trained%0Aweights%20fixed.%20The%20prevailing%20approach%2C%20LoRA%2C%20models%20updates%20using%20a%20low-rank%0Adecomposition%2C%20but%20its%20expressivity%20is%20inherently%20constrained%20by%20the%20rank.%0ARecent%20methods%20like%20HiRA%20aim%20to%20increase%20expressivity%20by%20incorporating%20a%0AHadamard%20product%20with%20the%20frozen%20weights%2C%20but%20still%20rely%20on%20the%20structure%20of%0Athe%20pre-trained%20model.%20We%20introduce%20ABBA%2C%20a%20new%20PEFT%20architecture%20that%0Areparameterizes%20the%20update%20as%20a%20Hadamard%20product%20of%20two%20independently%20learnable%0Alow-rank%20matrices.%20In%20contrast%20to%20prior%20work%2C%20ABBA%20fully%20decouples%20the%20update%0Afrom%20the%20pre-trained%20weights%2C%20enabling%20both%20components%20to%20be%20optimized%20freely.%0AThis%20leads%20to%20significantly%20higher%20expressivity%20under%20the%20same%20parameter%0Abudget%2C%20a%20property%20we%20validate%20through%20matrix%20reconstruction%20experiments.%0AEmpirically%2C%20ABBA%20achieves%20state-of-the-art%20results%20on%20arithmetic%20and%0Acommonsense%20reasoning%20benchmarks%2C%20consistently%20outperforming%20existing%20PEFT%0Amethods%20by%20a%20significant%20margin%20across%20multiple%20models.%20Our%20code%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/CERT-Lab/abba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14238v3&entry.124074799=Read"},
{"title": "C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale\n  Recommendation Systems", "author": "Mertcan Cokbas and Ziteng Liu and Zeyi Tao and Chengkai Zhang and Elder Veliz and Qin Huang and Ellie Wen and Huayu Li and Qiang Jin and Murat Duman and Benjamin Au and Guy Lebanon and Sagar Chordia", "abstract": "  Training large-scale recommendation models under a single global objective\nimplicitly assumes homogeneity across user populations. However, real-world\ndata are composites of heterogeneous cohorts with distinct conditional\ndistributions. As models increase in scale and complexity and as more data is\nused for training, they become dominated by central distribution patterns,\nneglecting head and tail regions. This imbalance limits the model's learning\nability and can result in inactive attention weights or dead neurons. In this\npaper, we reveal how the attention mechanism can play a key role in\nfactorization machines for shared embedding selection, and propose to address\nthis challenge by analyzing the substructures in the dataset and exposing those\nwith strong distributional contrast through auxiliary learning. Unlike previous\nresearch, which heuristically applies weighted labels or multi-task heads to\nmitigate such biases, we leverage partially conflicting auxiliary labels to\nregularize the shared representation. This approach customizes the learning\nprocess of attention layers to preserve mutual information with minority\ncohorts while improving global performance. We evaluated C2AL on massive\nproduction datasets with billions of data points each for six SOTA models.\nExperiments show that the factorization machine is able to capture fine-grained\nuser-ad interactions using the proposed method, achieving up to a 0.16%\nreduction in normalized entropy overall and delivering gains exceeding 0.30% on\ntargeted minority cohorts.\n", "link": "http://arxiv.org/abs/2510.02215v1", "date": "2025-10-02", "relevancy": 2.6132, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5316}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2AL%3A%20Cohort-Contrastive%20Auxiliary%20Learning%20for%20Large-scale%0A%20%20Recommendation%20Systems&body=Title%3A%20C2AL%3A%20Cohort-Contrastive%20Auxiliary%20Learning%20for%20Large-scale%0A%20%20Recommendation%20Systems%0AAuthor%3A%20Mertcan%20Cokbas%20and%20Ziteng%20Liu%20and%20Zeyi%20Tao%20and%20Chengkai%20Zhang%20and%20Elder%20Veliz%20and%20Qin%20Huang%20and%20Ellie%20Wen%20and%20Huayu%20Li%20and%20Qiang%20Jin%20and%20Murat%20Duman%20and%20Benjamin%20Au%20and%20Guy%20Lebanon%20and%20Sagar%20Chordia%0AAbstract%3A%20%20%20Training%20large-scale%20recommendation%20models%20under%20a%20single%20global%20objective%0Aimplicitly%20assumes%20homogeneity%20across%20user%20populations.%20However%2C%20real-world%0Adata%20are%20composites%20of%20heterogeneous%20cohorts%20with%20distinct%20conditional%0Adistributions.%20As%20models%20increase%20in%20scale%20and%20complexity%20and%20as%20more%20data%20is%0Aused%20for%20training%2C%20they%20become%20dominated%20by%20central%20distribution%20patterns%2C%0Aneglecting%20head%20and%20tail%20regions.%20This%20imbalance%20limits%20the%20model%27s%20learning%0Aability%20and%20can%20result%20in%20inactive%20attention%20weights%20or%20dead%20neurons.%20In%20this%0Apaper%2C%20we%20reveal%20how%20the%20attention%20mechanism%20can%20play%20a%20key%20role%20in%0Afactorization%20machines%20for%20shared%20embedding%20selection%2C%20and%20propose%20to%20address%0Athis%20challenge%20by%20analyzing%20the%20substructures%20in%20the%20dataset%20and%20exposing%20those%0Awith%20strong%20distributional%20contrast%20through%20auxiliary%20learning.%20Unlike%20previous%0Aresearch%2C%20which%20heuristically%20applies%20weighted%20labels%20or%20multi-task%20heads%20to%0Amitigate%20such%20biases%2C%20we%20leverage%20partially%20conflicting%20auxiliary%20labels%20to%0Aregularize%20the%20shared%20representation.%20This%20approach%20customizes%20the%20learning%0Aprocess%20of%20attention%20layers%20to%20preserve%20mutual%20information%20with%20minority%0Acohorts%20while%20improving%20global%20performance.%20We%20evaluated%20C2AL%20on%20massive%0Aproduction%20datasets%20with%20billions%20of%20data%20points%20each%20for%20six%20SOTA%20models.%0AExperiments%20show%20that%20the%20factorization%20machine%20is%20able%20to%20capture%20fine-grained%0Auser-ad%20interactions%20using%20the%20proposed%20method%2C%20achieving%20up%20to%20a%200.16%25%0Areduction%20in%20normalized%20entropy%20overall%20and%20delivering%20gains%20exceeding%200.30%25%20on%0Atargeted%20minority%20cohorts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2AL%253A%2520Cohort-Contrastive%2520Auxiliary%2520Learning%2520for%2520Large-scale%250A%2520%2520Recommendation%2520Systems%26entry.906535625%3DMertcan%2520Cokbas%2520and%2520Ziteng%2520Liu%2520and%2520Zeyi%2520Tao%2520and%2520Chengkai%2520Zhang%2520and%2520Elder%2520Veliz%2520and%2520Qin%2520Huang%2520and%2520Ellie%2520Wen%2520and%2520Huayu%2520Li%2520and%2520Qiang%2520Jin%2520and%2520Murat%2520Duman%2520and%2520Benjamin%2520Au%2520and%2520Guy%2520Lebanon%2520and%2520Sagar%2520Chordia%26entry.1292438233%3D%2520%2520Training%2520large-scale%2520recommendation%2520models%2520under%2520a%2520single%2520global%2520objective%250Aimplicitly%2520assumes%2520homogeneity%2520across%2520user%2520populations.%2520However%252C%2520real-world%250Adata%2520are%2520composites%2520of%2520heterogeneous%2520cohorts%2520with%2520distinct%2520conditional%250Adistributions.%2520As%2520models%2520increase%2520in%2520scale%2520and%2520complexity%2520and%2520as%2520more%2520data%2520is%250Aused%2520for%2520training%252C%2520they%2520become%2520dominated%2520by%2520central%2520distribution%2520patterns%252C%250Aneglecting%2520head%2520and%2520tail%2520regions.%2520This%2520imbalance%2520limits%2520the%2520model%2527s%2520learning%250Aability%2520and%2520can%2520result%2520in%2520inactive%2520attention%2520weights%2520or%2520dead%2520neurons.%2520In%2520this%250Apaper%252C%2520we%2520reveal%2520how%2520the%2520attention%2520mechanism%2520can%2520play%2520a%2520key%2520role%2520in%250Afactorization%2520machines%2520for%2520shared%2520embedding%2520selection%252C%2520and%2520propose%2520to%2520address%250Athis%2520challenge%2520by%2520analyzing%2520the%2520substructures%2520in%2520the%2520dataset%2520and%2520exposing%2520those%250Awith%2520strong%2520distributional%2520contrast%2520through%2520auxiliary%2520learning.%2520Unlike%2520previous%250Aresearch%252C%2520which%2520heuristically%2520applies%2520weighted%2520labels%2520or%2520multi-task%2520heads%2520to%250Amitigate%2520such%2520biases%252C%2520we%2520leverage%2520partially%2520conflicting%2520auxiliary%2520labels%2520to%250Aregularize%2520the%2520shared%2520representation.%2520This%2520approach%2520customizes%2520the%2520learning%250Aprocess%2520of%2520attention%2520layers%2520to%2520preserve%2520mutual%2520information%2520with%2520minority%250Acohorts%2520while%2520improving%2520global%2520performance.%2520We%2520evaluated%2520C2AL%2520on%2520massive%250Aproduction%2520datasets%2520with%2520billions%2520of%2520data%2520points%2520each%2520for%2520six%2520SOTA%2520models.%250AExperiments%2520show%2520that%2520the%2520factorization%2520machine%2520is%2520able%2520to%2520capture%2520fine-grained%250Auser-ad%2520interactions%2520using%2520the%2520proposed%2520method%252C%2520achieving%2520up%2520to%2520a%25200.16%2525%250Areduction%2520in%2520normalized%2520entropy%2520overall%2520and%2520delivering%2520gains%2520exceeding%25200.30%2525%2520on%250Atargeted%2520minority%2520cohorts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2AL%3A%20Cohort-Contrastive%20Auxiliary%20Learning%20for%20Large-scale%0A%20%20Recommendation%20Systems&entry.906535625=Mertcan%20Cokbas%20and%20Ziteng%20Liu%20and%20Zeyi%20Tao%20and%20Chengkai%20Zhang%20and%20Elder%20Veliz%20and%20Qin%20Huang%20and%20Ellie%20Wen%20and%20Huayu%20Li%20and%20Qiang%20Jin%20and%20Murat%20Duman%20and%20Benjamin%20Au%20and%20Guy%20Lebanon%20and%20Sagar%20Chordia&entry.1292438233=%20%20Training%20large-scale%20recommendation%20models%20under%20a%20single%20global%20objective%0Aimplicitly%20assumes%20homogeneity%20across%20user%20populations.%20However%2C%20real-world%0Adata%20are%20composites%20of%20heterogeneous%20cohorts%20with%20distinct%20conditional%0Adistributions.%20As%20models%20increase%20in%20scale%20and%20complexity%20and%20as%20more%20data%20is%0Aused%20for%20training%2C%20they%20become%20dominated%20by%20central%20distribution%20patterns%2C%0Aneglecting%20head%20and%20tail%20regions.%20This%20imbalance%20limits%20the%20model%27s%20learning%0Aability%20and%20can%20result%20in%20inactive%20attention%20weights%20or%20dead%20neurons.%20In%20this%0Apaper%2C%20we%20reveal%20how%20the%20attention%20mechanism%20can%20play%20a%20key%20role%20in%0Afactorization%20machines%20for%20shared%20embedding%20selection%2C%20and%20propose%20to%20address%0Athis%20challenge%20by%20analyzing%20the%20substructures%20in%20the%20dataset%20and%20exposing%20those%0Awith%20strong%20distributional%20contrast%20through%20auxiliary%20learning.%20Unlike%20previous%0Aresearch%2C%20which%20heuristically%20applies%20weighted%20labels%20or%20multi-task%20heads%20to%0Amitigate%20such%20biases%2C%20we%20leverage%20partially%20conflicting%20auxiliary%20labels%20to%0Aregularize%20the%20shared%20representation.%20This%20approach%20customizes%20the%20learning%0Aprocess%20of%20attention%20layers%20to%20preserve%20mutual%20information%20with%20minority%0Acohorts%20while%20improving%20global%20performance.%20We%20evaluated%20C2AL%20on%20massive%0Aproduction%20datasets%20with%20billions%20of%20data%20points%20each%20for%20six%20SOTA%20models.%0AExperiments%20show%20that%20the%20factorization%20machine%20is%20able%20to%20capture%20fine-grained%0Auser-ad%20interactions%20using%20the%20proposed%20method%2C%20achieving%20up%20to%20a%200.16%25%0Areduction%20in%20normalized%20entropy%20overall%20and%20delivering%20gains%20exceeding%200.30%25%20on%0Atargeted%20minority%20cohorts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02215v1&entry.124074799=Read"},
{"title": "NoiseShift: Resolution-Aware Noise Recalibration for Better\n  Low-Resolution Image Generation", "author": "Ruozhen He and Moayed Haji-Ali and Ziyan Yang and Vicente Ordonez", "abstract": "  Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.\n", "link": "http://arxiv.org/abs/2510.02307v1", "date": "2025-10-02", "relevancy": 2.6131, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6641}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6625}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoiseShift%3A%20Resolution-Aware%20Noise%20Recalibration%20for%20Better%0A%20%20Low-Resolution%20Image%20Generation&body=Title%3A%20NoiseShift%3A%20Resolution-Aware%20Noise%20Recalibration%20for%20Better%0A%20%20Low-Resolution%20Image%20Generation%0AAuthor%3A%20Ruozhen%20He%20and%20Moayed%20Haji-Ali%20and%20Ziyan%20Yang%20and%20Vicente%20Ordonez%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20trained%20on%20a%20fixed%20set%20of%20resolutions%20often%0Afail%20to%20generalize%2C%20even%20when%20asked%20to%20generate%20images%20at%20lower%20resolutions%0Athan%20those%20seen%20during%20training.%20High-resolution%20text-to-image%20generators%20are%0Acurrently%20unable%20to%20easily%20offer%20an%20out-of-the-box%20budget-efficient%20alternative%0Ato%20their%20users%20who%20might%20not%20need%20high-resolution%20images.%20We%20identify%20a%20key%0Atechnical%20insight%20in%20diffusion%20models%20that%20when%20addressed%20can%20help%20tackle%20this%0Alimitation%3A%20Noise%20schedulers%20have%20unequal%20perceptual%20effects%20across%0Aresolutions.%20The%20same%20level%20of%20noise%20removes%20disproportionately%20more%20signal%0Afrom%20lower-resolution%20images%20than%20from%20high-resolution%20images%2C%20leading%20to%20a%0Atrain-test%20mismatch.%20We%20propose%20NoiseShift%2C%20a%20training-free%20method%20that%0Arecalibrates%20the%20noise%20level%20of%20the%20denoiser%20conditioned%20on%20resolution%20size.%0ANoiseShift%20requires%20no%20changes%20to%20model%20architecture%20or%20sampling%20schedule%20and%0Ais%20compatible%20with%20existing%20models.%20When%20applied%20to%20Stable%20Diffusion%203%2C%20Stable%0ADiffusion%203.5%2C%20and%20Flux-Dev%2C%20quality%20at%20low%20resolutions%20is%20significantly%0Aimproved.%20On%20LAION-COCO%2C%20NoiseShift%20improves%20SD3.5%20by%2015.89%25%2C%20SD3%20by%208.56%25%2C%20and%0AFlux-Dev%20by%202.44%25%20in%20FID%20on%20average.%20On%20CelebA%2C%20NoiseShift%20improves%20SD3.5%20by%0A10.36%25%2C%20SD3%20by%205.19%25%2C%20and%20Flux-Dev%20by%203.02%25%20in%20FID%20on%20average.%20These%20results%0Ademonstrate%20the%20effectiveness%20of%20NoiseShift%20in%20mitigating%20resolution-dependent%0Aartifacts%20and%20enhancing%20the%20quality%20of%20low-resolution%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoiseShift%253A%2520Resolution-Aware%2520Noise%2520Recalibration%2520for%2520Better%250A%2520%2520Low-Resolution%2520Image%2520Generation%26entry.906535625%3DRuozhen%2520He%2520and%2520Moayed%2520Haji-Ali%2520and%2520Ziyan%2520Yang%2520and%2520Vicente%2520Ordonez%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520trained%2520on%2520a%2520fixed%2520set%2520of%2520resolutions%2520often%250Afail%2520to%2520generalize%252C%2520even%2520when%2520asked%2520to%2520generate%2520images%2520at%2520lower%2520resolutions%250Athan%2520those%2520seen%2520during%2520training.%2520High-resolution%2520text-to-image%2520generators%2520are%250Acurrently%2520unable%2520to%2520easily%2520offer%2520an%2520out-of-the-box%2520budget-efficient%2520alternative%250Ato%2520their%2520users%2520who%2520might%2520not%2520need%2520high-resolution%2520images.%2520We%2520identify%2520a%2520key%250Atechnical%2520insight%2520in%2520diffusion%2520models%2520that%2520when%2520addressed%2520can%2520help%2520tackle%2520this%250Alimitation%253A%2520Noise%2520schedulers%2520have%2520unequal%2520perceptual%2520effects%2520across%250Aresolutions.%2520The%2520same%2520level%2520of%2520noise%2520removes%2520disproportionately%2520more%2520signal%250Afrom%2520lower-resolution%2520images%2520than%2520from%2520high-resolution%2520images%252C%2520leading%2520to%2520a%250Atrain-test%2520mismatch.%2520We%2520propose%2520NoiseShift%252C%2520a%2520training-free%2520method%2520that%250Arecalibrates%2520the%2520noise%2520level%2520of%2520the%2520denoiser%2520conditioned%2520on%2520resolution%2520size.%250ANoiseShift%2520requires%2520no%2520changes%2520to%2520model%2520architecture%2520or%2520sampling%2520schedule%2520and%250Ais%2520compatible%2520with%2520existing%2520models.%2520When%2520applied%2520to%2520Stable%2520Diffusion%25203%252C%2520Stable%250ADiffusion%25203.5%252C%2520and%2520Flux-Dev%252C%2520quality%2520at%2520low%2520resolutions%2520is%2520significantly%250Aimproved.%2520On%2520LAION-COCO%252C%2520NoiseShift%2520improves%2520SD3.5%2520by%252015.89%2525%252C%2520SD3%2520by%25208.56%2525%252C%2520and%250AFlux-Dev%2520by%25202.44%2525%2520in%2520FID%2520on%2520average.%2520On%2520CelebA%252C%2520NoiseShift%2520improves%2520SD3.5%2520by%250A10.36%2525%252C%2520SD3%2520by%25205.19%2525%252C%2520and%2520Flux-Dev%2520by%25203.02%2525%2520in%2520FID%2520on%2520average.%2520These%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520NoiseShift%2520in%2520mitigating%2520resolution-dependent%250Aartifacts%2520and%2520enhancing%2520the%2520quality%2520of%2520low-resolution%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoiseShift%3A%20Resolution-Aware%20Noise%20Recalibration%20for%20Better%0A%20%20Low-Resolution%20Image%20Generation&entry.906535625=Ruozhen%20He%20and%20Moayed%20Haji-Ali%20and%20Ziyan%20Yang%20and%20Vicente%20Ordonez&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20trained%20on%20a%20fixed%20set%20of%20resolutions%20often%0Afail%20to%20generalize%2C%20even%20when%20asked%20to%20generate%20images%20at%20lower%20resolutions%0Athan%20those%20seen%20during%20training.%20High-resolution%20text-to-image%20generators%20are%0Acurrently%20unable%20to%20easily%20offer%20an%20out-of-the-box%20budget-efficient%20alternative%0Ato%20their%20users%20who%20might%20not%20need%20high-resolution%20images.%20We%20identify%20a%20key%0Atechnical%20insight%20in%20diffusion%20models%20that%20when%20addressed%20can%20help%20tackle%20this%0Alimitation%3A%20Noise%20schedulers%20have%20unequal%20perceptual%20effects%20across%0Aresolutions.%20The%20same%20level%20of%20noise%20removes%20disproportionately%20more%20signal%0Afrom%20lower-resolution%20images%20than%20from%20high-resolution%20images%2C%20leading%20to%20a%0Atrain-test%20mismatch.%20We%20propose%20NoiseShift%2C%20a%20training-free%20method%20that%0Arecalibrates%20the%20noise%20level%20of%20the%20denoiser%20conditioned%20on%20resolution%20size.%0ANoiseShift%20requires%20no%20changes%20to%20model%20architecture%20or%20sampling%20schedule%20and%0Ais%20compatible%20with%20existing%20models.%20When%20applied%20to%20Stable%20Diffusion%203%2C%20Stable%0ADiffusion%203.5%2C%20and%20Flux-Dev%2C%20quality%20at%20low%20resolutions%20is%20significantly%0Aimproved.%20On%20LAION-COCO%2C%20NoiseShift%20improves%20SD3.5%20by%2015.89%25%2C%20SD3%20by%208.56%25%2C%20and%0AFlux-Dev%20by%202.44%25%20in%20FID%20on%20average.%20On%20CelebA%2C%20NoiseShift%20improves%20SD3.5%20by%0A10.36%25%2C%20SD3%20by%205.19%25%2C%20and%20Flux-Dev%20by%203.02%25%20in%20FID%20on%20average.%20These%20results%0Ademonstrate%20the%20effectiveness%20of%20NoiseShift%20in%20mitigating%20resolution-dependent%0Aartifacts%20and%20enhancing%20the%20quality%20of%20low-resolution%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02307v1&entry.124074799=Read"},
{"title": "NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields", "author": "Amandine Brunetto and Sascha Hornauer and Fabien Moutarde", "abstract": "  Sound plays a major role in human perception. Along with vision, it provides\nessential information for understanding our surroundings. Despite advances in\nneural implicit representations, learning acoustics that align with visual\nscenes remains a challenge. We propose NeRAF, a method that jointly learns\nacoustic and radiance fields. NeRAF synthesizes both novel views and\nspatialized room impulse responses (RIR) at new positions by conditioning the\nacoustic field on 3D scene geometric and appearance priors from the radiance\nfield. The generated RIR can be applied to auralize any audio signal. Each\nmodality can be rendered independently and at spatially distinct positions,\noffering greater versatility. We demonstrate that NeRAF generates high-quality\naudio on SoundSpaces and RAF datasets, achieving significant performance\nimprovements over prior methods while being more data-efficient. Additionally,\nNeRAF enhances novel view synthesis of complex scenes trained with sparse data\nthrough cross-modal learning. NeRAF is designed as a Nerfstudio module,\nproviding convenient access to realistic audio-visual generation.\n", "link": "http://arxiv.org/abs/2405.18213v4", "date": "2025-10-02", "relevancy": 2.6056, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5381}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&body=Title%3A%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields%0AAuthor%3A%20Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde%0AAbstract%3A%20%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception.%20Along%20with%20vision%2C%20it%20provides%0Aessential%20information%20for%20understanding%20our%20surroundings.%20Despite%20advances%20in%0Aneural%20implicit%20representations%2C%20learning%20acoustics%20that%20align%20with%20visual%0Ascenes%20remains%20a%20challenge.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%20learns%0Aacoustic%20and%20radiance%20fields.%20NeRAF%20synthesizes%20both%20novel%20views%20and%0Aspatialized%20room%20impulse%20responses%20%28RIR%29%20at%20new%20positions%20by%20conditioning%20the%0Aacoustic%20field%20on%203D%20scene%20geometric%20and%20appearance%20priors%20from%20the%20radiance%0Afield.%20The%20generated%20RIR%20can%20be%20applied%20to%20auralize%20any%20audio%20signal.%20Each%0Amodality%20can%20be%20rendered%20independently%20and%20at%20spatially%20distinct%20positions%2C%0Aoffering%20greater%20versatility.%20We%20demonstrate%20that%20NeRAF%20generates%20high-quality%0Aaudio%20on%20SoundSpaces%20and%20RAF%20datasets%2C%20achieving%20significant%20performance%0Aimprovements%20over%20prior%20methods%20while%20being%20more%20data-efficient.%20Additionally%2C%0ANeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%20sparse%20data%0Athrough%20cross-modal%20learning.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%2C%0Aproviding%20convenient%20access%20to%20realistic%20audio-visual%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18213v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRAF%253A%25203D%2520Scene%2520Infused%2520Neural%2520Radiance%2520and%2520Acoustic%2520Fields%26entry.906535625%3DAmandine%2520Brunetto%2520and%2520Sascha%2520Hornauer%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3D%2520%2520Sound%2520plays%2520a%2520major%2520role%2520in%2520human%2520perception.%2520Along%2520with%2520vision%252C%2520it%2520provides%250Aessential%2520information%2520for%2520understanding%2520our%2520surroundings.%2520Despite%2520advances%2520in%250Aneural%2520implicit%2520representations%252C%2520learning%2520acoustics%2520that%2520align%2520with%2520visual%250Ascenes%2520remains%2520a%2520challenge.%2520We%2520propose%2520NeRAF%252C%2520a%2520method%2520that%2520jointly%2520learns%250Aacoustic%2520and%2520radiance%2520fields.%2520NeRAF%2520synthesizes%2520both%2520novel%2520views%2520and%250Aspatialized%2520room%2520impulse%2520responses%2520%2528RIR%2529%2520at%2520new%2520positions%2520by%2520conditioning%2520the%250Aacoustic%2520field%2520on%25203D%2520scene%2520geometric%2520and%2520appearance%2520priors%2520from%2520the%2520radiance%250Afield.%2520The%2520generated%2520RIR%2520can%2520be%2520applied%2520to%2520auralize%2520any%2520audio%2520signal.%2520Each%250Amodality%2520can%2520be%2520rendered%2520independently%2520and%2520at%2520spatially%2520distinct%2520positions%252C%250Aoffering%2520greater%2520versatility.%2520We%2520demonstrate%2520that%2520NeRAF%2520generates%2520high-quality%250Aaudio%2520on%2520SoundSpaces%2520and%2520RAF%2520datasets%252C%2520achieving%2520significant%2520performance%250Aimprovements%2520over%2520prior%2520methods%2520while%2520being%2520more%2520data-efficient.%2520Additionally%252C%250ANeRAF%2520enhances%2520novel%2520view%2520synthesis%2520of%2520complex%2520scenes%2520trained%2520with%2520sparse%2520data%250Athrough%2520cross-modal%2520learning.%2520NeRAF%2520is%2520designed%2520as%2520a%2520Nerfstudio%2520module%252C%250Aproviding%2520convenient%2520access%2520to%2520realistic%2520audio-visual%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18213v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&entry.906535625=Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde&entry.1292438233=%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception.%20Along%20with%20vision%2C%20it%20provides%0Aessential%20information%20for%20understanding%20our%20surroundings.%20Despite%20advances%20in%0Aneural%20implicit%20representations%2C%20learning%20acoustics%20that%20align%20with%20visual%0Ascenes%20remains%20a%20challenge.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%20learns%0Aacoustic%20and%20radiance%20fields.%20NeRAF%20synthesizes%20both%20novel%20views%20and%0Aspatialized%20room%20impulse%20responses%20%28RIR%29%20at%20new%20positions%20by%20conditioning%20the%0Aacoustic%20field%20on%203D%20scene%20geometric%20and%20appearance%20priors%20from%20the%20radiance%0Afield.%20The%20generated%20RIR%20can%20be%20applied%20to%20auralize%20any%20audio%20signal.%20Each%0Amodality%20can%20be%20rendered%20independently%20and%20at%20spatially%20distinct%20positions%2C%0Aoffering%20greater%20versatility.%20We%20demonstrate%20that%20NeRAF%20generates%20high-quality%0Aaudio%20on%20SoundSpaces%20and%20RAF%20datasets%2C%20achieving%20significant%20performance%0Aimprovements%20over%20prior%20methods%20while%20being%20more%20data-efficient.%20Additionally%2C%0ANeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%20sparse%20data%0Athrough%20cross-modal%20learning.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%2C%0Aproviding%20convenient%20access%20to%20realistic%20audio-visual%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18213v4&entry.124074799=Read"},
{"title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data", "author": "Ziyin Zhang and Zihan Liao and Hang Yu and Peng Di and Rui Wang", "abstract": "  We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.\n", "link": "http://arxiv.org/abs/2510.02294v1", "date": "2025-10-02", "relevancy": 2.5466, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F2LLM%20Technical%20Report%3A%20Matching%20SOTA%20Embedding%20Performance%20with%206%0A%20%20Million%20Open-Source%20Data&body=Title%3A%20F2LLM%20Technical%20Report%3A%20Matching%20SOTA%20Embedding%20Performance%20with%206%0A%20%20Million%20Open-Source%20Data%0AAuthor%3A%20Ziyin%20Zhang%20and%20Zihan%20Liao%20and%20Hang%20Yu%20and%20Peng%20Di%20and%20Rui%20Wang%0AAbstract%3A%20%20%20We%20introduce%20F2LLM%20-%20Foundation%20to%20Feature%20Large%20Language%20Models%2C%20a%20suite%20of%0Astate-of-the-art%20embedding%20models%20in%20three%20sizes%3A%200.6B%2C%201.7B%2C%20and%204B.%20Unlike%0Aprevious%20top-ranking%20embedding%20models%20that%20require%20massive%20contrastive%0Apretraining%2C%20sophisticated%20training%20pipelines%2C%20and%20costly%20synthetic%20training%0Adata%2C%20F2LLM%20is%20directly%20finetuned%20from%20foundation%20models%20on%206%20million%0Aquery-document-negative%20tuples%20curated%20from%20open-source%2C%20non-synthetic%0Adatasets%2C%20striking%20a%20strong%20balance%20between%20training%20cost%2C%20model%20size%2C%20and%0Aembedding%20performance.%20On%20the%20MTEB%20English%20leaderboard%2C%20F2LLM-4B%20ranks%202nd%0Aamong%20models%20with%20approximately%204B%20parameters%20and%207th%20overall%2C%20while%20F2LLM-1.7B%0Aranks%201st%20among%20models%20in%20the%201B-2B%20size%20range.%20To%20facilitate%20future%20research%0Ain%20the%20field%2C%20we%20release%20the%20models%2C%20training%20dataset%2C%20and%20code%2C%20positioning%0AF2LLM%20as%20a%20strong%2C%20reproducible%2C%20and%20budget-friendly%20baseline%20for%20future%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF2LLM%2520Technical%2520Report%253A%2520Matching%2520SOTA%2520Embedding%2520Performance%2520with%25206%250A%2520%2520Million%2520Open-Source%2520Data%26entry.906535625%3DZiyin%2520Zhang%2520and%2520Zihan%2520Liao%2520and%2520Hang%2520Yu%2520and%2520Peng%2520Di%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520F2LLM%2520-%2520Foundation%2520to%2520Feature%2520Large%2520Language%2520Models%252C%2520a%2520suite%2520of%250Astate-of-the-art%2520embedding%2520models%2520in%2520three%2520sizes%253A%25200.6B%252C%25201.7B%252C%2520and%25204B.%2520Unlike%250Aprevious%2520top-ranking%2520embedding%2520models%2520that%2520require%2520massive%2520contrastive%250Apretraining%252C%2520sophisticated%2520training%2520pipelines%252C%2520and%2520costly%2520synthetic%2520training%250Adata%252C%2520F2LLM%2520is%2520directly%2520finetuned%2520from%2520foundation%2520models%2520on%25206%2520million%250Aquery-document-negative%2520tuples%2520curated%2520from%2520open-source%252C%2520non-synthetic%250Adatasets%252C%2520striking%2520a%2520strong%2520balance%2520between%2520training%2520cost%252C%2520model%2520size%252C%2520and%250Aembedding%2520performance.%2520On%2520the%2520MTEB%2520English%2520leaderboard%252C%2520F2LLM-4B%2520ranks%25202nd%250Aamong%2520models%2520with%2520approximately%25204B%2520parameters%2520and%25207th%2520overall%252C%2520while%2520F2LLM-1.7B%250Aranks%25201st%2520among%2520models%2520in%2520the%25201B-2B%2520size%2520range.%2520To%2520facilitate%2520future%2520research%250Ain%2520the%2520field%252C%2520we%2520release%2520the%2520models%252C%2520training%2520dataset%252C%2520and%2520code%252C%2520positioning%250AF2LLM%2520as%2520a%2520strong%252C%2520reproducible%252C%2520and%2520budget-friendly%2520baseline%2520for%2520future%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F2LLM%20Technical%20Report%3A%20Matching%20SOTA%20Embedding%20Performance%20with%206%0A%20%20Million%20Open-Source%20Data&entry.906535625=Ziyin%20Zhang%20and%20Zihan%20Liao%20and%20Hang%20Yu%20and%20Peng%20Di%20and%20Rui%20Wang&entry.1292438233=%20%20We%20introduce%20F2LLM%20-%20Foundation%20to%20Feature%20Large%20Language%20Models%2C%20a%20suite%20of%0Astate-of-the-art%20embedding%20models%20in%20three%20sizes%3A%200.6B%2C%201.7B%2C%20and%204B.%20Unlike%0Aprevious%20top-ranking%20embedding%20models%20that%20require%20massive%20contrastive%0Apretraining%2C%20sophisticated%20training%20pipelines%2C%20and%20costly%20synthetic%20training%0Adata%2C%20F2LLM%20is%20directly%20finetuned%20from%20foundation%20models%20on%206%20million%0Aquery-document-negative%20tuples%20curated%20from%20open-source%2C%20non-synthetic%0Adatasets%2C%20striking%20a%20strong%20balance%20between%20training%20cost%2C%20model%20size%2C%20and%0Aembedding%20performance.%20On%20the%20MTEB%20English%20leaderboard%2C%20F2LLM-4B%20ranks%202nd%0Aamong%20models%20with%20approximately%204B%20parameters%20and%207th%20overall%2C%20while%20F2LLM-1.7B%0Aranks%201st%20among%20models%20in%20the%201B-2B%20size%20range.%20To%20facilitate%20future%20research%0Ain%20the%20field%2C%20we%20release%20the%20models%2C%20training%20dataset%2C%20and%20code%2C%20positioning%0AF2LLM%20as%20a%20strong%2C%20reproducible%2C%20and%20budget-friendly%20baseline%20for%20future%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02294v1&entry.124074799=Read"},
{"title": "MultiModal Action Conditioned Video Generation", "author": "Yichen Li and Antonio Torralba", "abstract": "  Current video models fail as world model as they lack fine-graiend control.\nGeneral-purpose household robots require real-time fine motor control to handle\ndelicate tasks and urgent situations. In this work, we introduce fine-grained\nmultimodal actions to capture such precise control. We consider senses of\nproprioception, kinesthesia, force haptics, and muscle activation. Such\nmultimodal senses naturally enables fine-grained interactions that are\ndifficult to simulate with text-conditioned generative models. To effectively\nsimulate fine-grained multisensory actions, we develop a feature learning\nparadigm that aligns these modalities while preserving the unique information\neach modality provides. We further propose a regularization scheme to enhance\ncausality of the action trajectory features in representing intricate\ninteraction dynamics. Experiments show that incorporating multimodal senses\nimproves simulation accuracy and reduces temporal drift. Extensive ablation\nstudies and downstream applications demonstrate the effectiveness and\npracticality of our work.\n", "link": "http://arxiv.org/abs/2510.02287v1", "date": "2025-10-02", "relevancy": 2.5098, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6539}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6314}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiModal%20Action%20Conditioned%20Video%20Generation&body=Title%3A%20MultiModal%20Action%20Conditioned%20Video%20Generation%0AAuthor%3A%20Yichen%20Li%20and%20Antonio%20Torralba%0AAbstract%3A%20%20%20Current%20video%20models%20fail%20as%20world%20model%20as%20they%20lack%20fine-graiend%20control.%0AGeneral-purpose%20household%20robots%20require%20real-time%20fine%20motor%20control%20to%20handle%0Adelicate%20tasks%20and%20urgent%20situations.%20In%20this%20work%2C%20we%20introduce%20fine-grained%0Amultimodal%20actions%20to%20capture%20such%20precise%20control.%20We%20consider%20senses%20of%0Aproprioception%2C%20kinesthesia%2C%20force%20haptics%2C%20and%20muscle%20activation.%20Such%0Amultimodal%20senses%20naturally%20enables%20fine-grained%20interactions%20that%20are%0Adifficult%20to%20simulate%20with%20text-conditioned%20generative%20models.%20To%20effectively%0Asimulate%20fine-grained%20multisensory%20actions%2C%20we%20develop%20a%20feature%20learning%0Aparadigm%20that%20aligns%20these%20modalities%20while%20preserving%20the%20unique%20information%0Aeach%20modality%20provides.%20We%20further%20propose%20a%20regularization%20scheme%20to%20enhance%0Acausality%20of%20the%20action%20trajectory%20features%20in%20representing%20intricate%0Ainteraction%20dynamics.%20Experiments%20show%20that%20incorporating%20multimodal%20senses%0Aimproves%20simulation%20accuracy%20and%20reduces%20temporal%20drift.%20Extensive%20ablation%0Astudies%20and%20downstream%20applications%20demonstrate%20the%20effectiveness%20and%0Apracticality%20of%20our%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiModal%2520Action%2520Conditioned%2520Video%2520Generation%26entry.906535625%3DYichen%2520Li%2520and%2520Antonio%2520Torralba%26entry.1292438233%3D%2520%2520Current%2520video%2520models%2520fail%2520as%2520world%2520model%2520as%2520they%2520lack%2520fine-graiend%2520control.%250AGeneral-purpose%2520household%2520robots%2520require%2520real-time%2520fine%2520motor%2520control%2520to%2520handle%250Adelicate%2520tasks%2520and%2520urgent%2520situations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520fine-grained%250Amultimodal%2520actions%2520to%2520capture%2520such%2520precise%2520control.%2520We%2520consider%2520senses%2520of%250Aproprioception%252C%2520kinesthesia%252C%2520force%2520haptics%252C%2520and%2520muscle%2520activation.%2520Such%250Amultimodal%2520senses%2520naturally%2520enables%2520fine-grained%2520interactions%2520that%2520are%250Adifficult%2520to%2520simulate%2520with%2520text-conditioned%2520generative%2520models.%2520To%2520effectively%250Asimulate%2520fine-grained%2520multisensory%2520actions%252C%2520we%2520develop%2520a%2520feature%2520learning%250Aparadigm%2520that%2520aligns%2520these%2520modalities%2520while%2520preserving%2520the%2520unique%2520information%250Aeach%2520modality%2520provides.%2520We%2520further%2520propose%2520a%2520regularization%2520scheme%2520to%2520enhance%250Acausality%2520of%2520the%2520action%2520trajectory%2520features%2520in%2520representing%2520intricate%250Ainteraction%2520dynamics.%2520Experiments%2520show%2520that%2520incorporating%2520multimodal%2520senses%250Aimproves%2520simulation%2520accuracy%2520and%2520reduces%2520temporal%2520drift.%2520Extensive%2520ablation%250Astudies%2520and%2520downstream%2520applications%2520demonstrate%2520the%2520effectiveness%2520and%250Apracticality%2520of%2520our%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiModal%20Action%20Conditioned%20Video%20Generation&entry.906535625=Yichen%20Li%20and%20Antonio%20Torralba&entry.1292438233=%20%20Current%20video%20models%20fail%20as%20world%20model%20as%20they%20lack%20fine-graiend%20control.%0AGeneral-purpose%20household%20robots%20require%20real-time%20fine%20motor%20control%20to%20handle%0Adelicate%20tasks%20and%20urgent%20situations.%20In%20this%20work%2C%20we%20introduce%20fine-grained%0Amultimodal%20actions%20to%20capture%20such%20precise%20control.%20We%20consider%20senses%20of%0Aproprioception%2C%20kinesthesia%2C%20force%20haptics%2C%20and%20muscle%20activation.%20Such%0Amultimodal%20senses%20naturally%20enables%20fine-grained%20interactions%20that%20are%0Adifficult%20to%20simulate%20with%20text-conditioned%20generative%20models.%20To%20effectively%0Asimulate%20fine-grained%20multisensory%20actions%2C%20we%20develop%20a%20feature%20learning%0Aparadigm%20that%20aligns%20these%20modalities%20while%20preserving%20the%20unique%20information%0Aeach%20modality%20provides.%20We%20further%20propose%20a%20regularization%20scheme%20to%20enhance%0Acausality%20of%20the%20action%20trajectory%20features%20in%20representing%20intricate%0Ainteraction%20dynamics.%20Experiments%20show%20that%20incorporating%20multimodal%20senses%0Aimproves%20simulation%20accuracy%20and%20reduces%20temporal%20drift.%20Extensive%20ablation%0Astudies%20and%20downstream%20applications%20demonstrate%20the%20effectiveness%20and%0Apracticality%20of%20our%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02287v1&entry.124074799=Read"},
{"title": "Inferring Dynamic Physical Properties from Video Foundation Models", "author": "Guanqi Zhan and Xianzheng Ma and Weidi Xie and Andrew Zisserman", "abstract": "  We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.\n", "link": "http://arxiv.org/abs/2510.02311v1", "date": "2025-10-02", "relevancy": 2.4344, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6756}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6282}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inferring%20Dynamic%20Physical%20Properties%20from%20Video%20Foundation%20Models&body=Title%3A%20Inferring%20Dynamic%20Physical%20Properties%20from%20Video%20Foundation%20Models%0AAuthor%3A%20Guanqi%20Zhan%20and%20Xianzheng%20Ma%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20predicting%20dynamic%20physical%20properties%20from%20videos.%20More%0Aspecifically%2C%20we%20consider%20physical%20properties%20that%20require%20temporal%20information%0Ato%20be%20inferred%3A%20elasticity%20of%20a%20bouncing%20object%2C%20viscosity%20of%20a%20flowing%20liquid%2C%0Aand%20dynamic%20friction%20of%20an%20object%20sliding%20on%20a%20surface.%20To%20this%20end%2C%20we%20make%0Athe%20following%20contributions%3A%20%28i%29%20We%20collect%20a%20new%20video%20dataset%20for%20each%0Aphysical%20property%2C%20consisting%20of%20synthetic%20training%20and%20testing%20splits%2C%20as%20well%0Aas%20a%20real%20split%20for%20real%20world%20evaluation.%20%28ii%29%20We%20explore%20three%20ways%20to%20infer%0Athe%20physical%20property%20from%20videos%3A%20%28a%29%20an%20oracle%20method%20where%20we%20supply%20the%0Avisual%20cues%20that%20intrinsically%20reflect%20the%20property%20using%20classical%20computer%0Avision%20techniques%3B%20%28b%29%20a%20simple%20read%20out%20mechanism%20using%20a%20visual%20prompt%20and%0Atrainable%20prompt%20vector%20for%20cross-attention%20on%20pre-trained%20video%20generative%20and%0Aself-supervised%20models%3B%20and%20%28c%29%20prompt%20strategies%20for%20Multi-modal%20Large%0ALanguage%20Models%20%28MLLMs%29.%20%28iii%29%20We%20show%20that%20video%20foundation%20models%20trained%20in%0Aa%20generative%20or%20self-supervised%20manner%20achieve%20a%20similar%20performance%2C%20though%0Abehind%20that%20of%20the%20oracle%2C%20and%20MLLMs%20are%20currently%20inferior%20to%20the%20other%0Amodels%2C%20though%20their%20performance%20can%20be%20improved%20through%20suitable%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInferring%2520Dynamic%2520Physical%2520Properties%2520from%2520Video%2520Foundation%2520Models%26entry.906535625%3DGuanqi%2520Zhan%2520and%2520Xianzheng%2520Ma%2520and%2520Weidi%2520Xie%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520predicting%2520dynamic%2520physical%2520properties%2520from%2520videos.%2520More%250Aspecifically%252C%2520we%2520consider%2520physical%2520properties%2520that%2520require%2520temporal%2520information%250Ato%2520be%2520inferred%253A%2520elasticity%2520of%2520a%2520bouncing%2520object%252C%2520viscosity%2520of%2520a%2520flowing%2520liquid%252C%250Aand%2520dynamic%2520friction%2520of%2520an%2520object%2520sliding%2520on%2520a%2520surface.%2520To%2520this%2520end%252C%2520we%2520make%250Athe%2520following%2520contributions%253A%2520%2528i%2529%2520We%2520collect%2520a%2520new%2520video%2520dataset%2520for%2520each%250Aphysical%2520property%252C%2520consisting%2520of%2520synthetic%2520training%2520and%2520testing%2520splits%252C%2520as%2520well%250Aas%2520a%2520real%2520split%2520for%2520real%2520world%2520evaluation.%2520%2528ii%2529%2520We%2520explore%2520three%2520ways%2520to%2520infer%250Athe%2520physical%2520property%2520from%2520videos%253A%2520%2528a%2529%2520an%2520oracle%2520method%2520where%2520we%2520supply%2520the%250Avisual%2520cues%2520that%2520intrinsically%2520reflect%2520the%2520property%2520using%2520classical%2520computer%250Avision%2520techniques%253B%2520%2528b%2529%2520a%2520simple%2520read%2520out%2520mechanism%2520using%2520a%2520visual%2520prompt%2520and%250Atrainable%2520prompt%2520vector%2520for%2520cross-attention%2520on%2520pre-trained%2520video%2520generative%2520and%250Aself-supervised%2520models%253B%2520and%2520%2528c%2529%2520prompt%2520strategies%2520for%2520Multi-modal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529.%2520%2528iii%2529%2520We%2520show%2520that%2520video%2520foundation%2520models%2520trained%2520in%250Aa%2520generative%2520or%2520self-supervised%2520manner%2520achieve%2520a%2520similar%2520performance%252C%2520though%250Abehind%2520that%2520of%2520the%2520oracle%252C%2520and%2520MLLMs%2520are%2520currently%2520inferior%2520to%2520the%2520other%250Amodels%252C%2520though%2520their%2520performance%2520can%2520be%2520improved%2520through%2520suitable%2520prompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20Dynamic%20Physical%20Properties%20from%20Video%20Foundation%20Models&entry.906535625=Guanqi%20Zhan%20and%20Xianzheng%20Ma%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20We%20study%20the%20task%20of%20predicting%20dynamic%20physical%20properties%20from%20videos.%20More%0Aspecifically%2C%20we%20consider%20physical%20properties%20that%20require%20temporal%20information%0Ato%20be%20inferred%3A%20elasticity%20of%20a%20bouncing%20object%2C%20viscosity%20of%20a%20flowing%20liquid%2C%0Aand%20dynamic%20friction%20of%20an%20object%20sliding%20on%20a%20surface.%20To%20this%20end%2C%20we%20make%0Athe%20following%20contributions%3A%20%28i%29%20We%20collect%20a%20new%20video%20dataset%20for%20each%0Aphysical%20property%2C%20consisting%20of%20synthetic%20training%20and%20testing%20splits%2C%20as%20well%0Aas%20a%20real%20split%20for%20real%20world%20evaluation.%20%28ii%29%20We%20explore%20three%20ways%20to%20infer%0Athe%20physical%20property%20from%20videos%3A%20%28a%29%20an%20oracle%20method%20where%20we%20supply%20the%0Avisual%20cues%20that%20intrinsically%20reflect%20the%20property%20using%20classical%20computer%0Avision%20techniques%3B%20%28b%29%20a%20simple%20read%20out%20mechanism%20using%20a%20visual%20prompt%20and%0Atrainable%20prompt%20vector%20for%20cross-attention%20on%20pre-trained%20video%20generative%20and%0Aself-supervised%20models%3B%20and%20%28c%29%20prompt%20strategies%20for%20Multi-modal%20Large%0ALanguage%20Models%20%28MLLMs%29.%20%28iii%29%20We%20show%20that%20video%20foundation%20models%20trained%20in%0Aa%20generative%20or%20self-supervised%20manner%20achieve%20a%20similar%20performance%2C%20though%0Abehind%20that%20of%20the%20oracle%2C%20and%20MLLMs%20are%20currently%20inferior%20to%20the%20other%0Amodels%2C%20though%20their%20performance%20can%20be%20improved%20through%20suitable%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02311v1&entry.124074799=Read"},
{"title": "Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks", "author": "Fedor Velikonivtsev and Oleg Platonov and Gleb Bazhenov and Liudmila Prokhorenkova", "abstract": "  Traffic forecasting on road networks is a complex task of significant\npractical importance that has recently attracted considerable attention from\nthe machine learning community, with spatiotemporal graph neural networks\n(GNNs) becoming the most popular approach. The proper evaluation of traffic\nforecasting methods requires realistic datasets, but current publicly available\nbenchmarks have significant drawbacks, including the absence of information\nabout road connectivity for road graph construction, limited information about\nroad properties, and a relatively small number of road segments that falls\nshort of real-world applications. Further, current datasets mostly contain\ninformation about intercity highways with sparsely located sensors, while city\nroad networks arguably present a more challenging forecasting task due to much\ndenser roads and more complex urban traffic patterns. In this work, we provide\na more complete, realistic, and challenging benchmark for traffic forecasting\nby releasing datasets representing the road networks of two major cities, with\nthe largest containing almost 100,000 road segments (more than a 10-fold\nincrease relative to existing datasets). Our datasets contain rich road\nfeatures and provide fine-grained data about both traffic volume and traffic\nspeed, allowing for building more holistic traffic forecasting systems. We show\nthat most current implementations of neural spatiotemporal models for traffic\nforecasting have problems scaling to datasets of our size. To overcome this\nissue, we propose an alternative approach to neural traffic forecasting that\nuses a GNN without a dedicated module for temporal sequence processing, thus\nachieving much better scalability, while also demonstrating stronger\nforecasting performance. We hope our datasets and modeling insights will serve\nas a valuable resource for research in traffic forecasting.\n", "link": "http://arxiv.org/abs/2510.02278v1", "date": "2025-10-02", "relevancy": 2.379, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4789}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4784}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Urban%20Traffic%20Forecasting%20on%20Metropolis-Scale%20Road%20Networks&body=Title%3A%20Fine-Grained%20Urban%20Traffic%20Forecasting%20on%20Metropolis-Scale%20Road%20Networks%0AAuthor%3A%20Fedor%20Velikonivtsev%20and%20Oleg%20Platonov%20and%20Gleb%20Bazhenov%20and%20Liudmila%20Prokhorenkova%0AAbstract%3A%20%20%20Traffic%20forecasting%20on%20road%20networks%20is%20a%20complex%20task%20of%20significant%0Apractical%20importance%20that%20has%20recently%20attracted%20considerable%20attention%20from%0Athe%20machine%20learning%20community%2C%20with%20spatiotemporal%20graph%20neural%20networks%0A%28GNNs%29%20becoming%20the%20most%20popular%20approach.%20The%20proper%20evaluation%20of%20traffic%0Aforecasting%20methods%20requires%20realistic%20datasets%2C%20but%20current%20publicly%20available%0Abenchmarks%20have%20significant%20drawbacks%2C%20including%20the%20absence%20of%20information%0Aabout%20road%20connectivity%20for%20road%20graph%20construction%2C%20limited%20information%20about%0Aroad%20properties%2C%20and%20a%20relatively%20small%20number%20of%20road%20segments%20that%20falls%0Ashort%20of%20real-world%20applications.%20Further%2C%20current%20datasets%20mostly%20contain%0Ainformation%20about%20intercity%20highways%20with%20sparsely%20located%20sensors%2C%20while%20city%0Aroad%20networks%20arguably%20present%20a%20more%20challenging%20forecasting%20task%20due%20to%20much%0Adenser%20roads%20and%20more%20complex%20urban%20traffic%20patterns.%20In%20this%20work%2C%20we%20provide%0Aa%20more%20complete%2C%20realistic%2C%20and%20challenging%20benchmark%20for%20traffic%20forecasting%0Aby%20releasing%20datasets%20representing%20the%20road%20networks%20of%20two%20major%20cities%2C%20with%0Athe%20largest%20containing%20almost%20100%2C000%20road%20segments%20%28more%20than%20a%2010-fold%0Aincrease%20relative%20to%20existing%20datasets%29.%20Our%20datasets%20contain%20rich%20road%0Afeatures%20and%20provide%20fine-grained%20data%20about%20both%20traffic%20volume%20and%20traffic%0Aspeed%2C%20allowing%20for%20building%20more%20holistic%20traffic%20forecasting%20systems.%20We%20show%0Athat%20most%20current%20implementations%20of%20neural%20spatiotemporal%20models%20for%20traffic%0Aforecasting%20have%20problems%20scaling%20to%20datasets%20of%20our%20size.%20To%20overcome%20this%0Aissue%2C%20we%20propose%20an%20alternative%20approach%20to%20neural%20traffic%20forecasting%20that%0Auses%20a%20GNN%20without%20a%20dedicated%20module%20for%20temporal%20sequence%20processing%2C%20thus%0Aachieving%20much%20better%20scalability%2C%20while%20also%20demonstrating%20stronger%0Aforecasting%20performance.%20We%20hope%20our%20datasets%20and%20modeling%20insights%20will%20serve%0Aas%20a%20valuable%20resource%20for%20research%20in%20traffic%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Urban%2520Traffic%2520Forecasting%2520on%2520Metropolis-Scale%2520Road%2520Networks%26entry.906535625%3DFedor%2520Velikonivtsev%2520and%2520Oleg%2520Platonov%2520and%2520Gleb%2520Bazhenov%2520and%2520Liudmila%2520Prokhorenkova%26entry.1292438233%3D%2520%2520Traffic%2520forecasting%2520on%2520road%2520networks%2520is%2520a%2520complex%2520task%2520of%2520significant%250Apractical%2520importance%2520that%2520has%2520recently%2520attracted%2520considerable%2520attention%2520from%250Athe%2520machine%2520learning%2520community%252C%2520with%2520spatiotemporal%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520becoming%2520the%2520most%2520popular%2520approach.%2520The%2520proper%2520evaluation%2520of%2520traffic%250Aforecasting%2520methods%2520requires%2520realistic%2520datasets%252C%2520but%2520current%2520publicly%2520available%250Abenchmarks%2520have%2520significant%2520drawbacks%252C%2520including%2520the%2520absence%2520of%2520information%250Aabout%2520road%2520connectivity%2520for%2520road%2520graph%2520construction%252C%2520limited%2520information%2520about%250Aroad%2520properties%252C%2520and%2520a%2520relatively%2520small%2520number%2520of%2520road%2520segments%2520that%2520falls%250Ashort%2520of%2520real-world%2520applications.%2520Further%252C%2520current%2520datasets%2520mostly%2520contain%250Ainformation%2520about%2520intercity%2520highways%2520with%2520sparsely%2520located%2520sensors%252C%2520while%2520city%250Aroad%2520networks%2520arguably%2520present%2520a%2520more%2520challenging%2520forecasting%2520task%2520due%2520to%2520much%250Adenser%2520roads%2520and%2520more%2520complex%2520urban%2520traffic%2520patterns.%2520In%2520this%2520work%252C%2520we%2520provide%250Aa%2520more%2520complete%252C%2520realistic%252C%2520and%2520challenging%2520benchmark%2520for%2520traffic%2520forecasting%250Aby%2520releasing%2520datasets%2520representing%2520the%2520road%2520networks%2520of%2520two%2520major%2520cities%252C%2520with%250Athe%2520largest%2520containing%2520almost%2520100%252C000%2520road%2520segments%2520%2528more%2520than%2520a%252010-fold%250Aincrease%2520relative%2520to%2520existing%2520datasets%2529.%2520Our%2520datasets%2520contain%2520rich%2520road%250Afeatures%2520and%2520provide%2520fine-grained%2520data%2520about%2520both%2520traffic%2520volume%2520and%2520traffic%250Aspeed%252C%2520allowing%2520for%2520building%2520more%2520holistic%2520traffic%2520forecasting%2520systems.%2520We%2520show%250Athat%2520most%2520current%2520implementations%2520of%2520neural%2520spatiotemporal%2520models%2520for%2520traffic%250Aforecasting%2520have%2520problems%2520scaling%2520to%2520datasets%2520of%2520our%2520size.%2520To%2520overcome%2520this%250Aissue%252C%2520we%2520propose%2520an%2520alternative%2520approach%2520to%2520neural%2520traffic%2520forecasting%2520that%250Auses%2520a%2520GNN%2520without%2520a%2520dedicated%2520module%2520for%2520temporal%2520sequence%2520processing%252C%2520thus%250Aachieving%2520much%2520better%2520scalability%252C%2520while%2520also%2520demonstrating%2520stronger%250Aforecasting%2520performance.%2520We%2520hope%2520our%2520datasets%2520and%2520modeling%2520insights%2520will%2520serve%250Aas%2520a%2520valuable%2520resource%2520for%2520research%2520in%2520traffic%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Urban%20Traffic%20Forecasting%20on%20Metropolis-Scale%20Road%20Networks&entry.906535625=Fedor%20Velikonivtsev%20and%20Oleg%20Platonov%20and%20Gleb%20Bazhenov%20and%20Liudmila%20Prokhorenkova&entry.1292438233=%20%20Traffic%20forecasting%20on%20road%20networks%20is%20a%20complex%20task%20of%20significant%0Apractical%20importance%20that%20has%20recently%20attracted%20considerable%20attention%20from%0Athe%20machine%20learning%20community%2C%20with%20spatiotemporal%20graph%20neural%20networks%0A%28GNNs%29%20becoming%20the%20most%20popular%20approach.%20The%20proper%20evaluation%20of%20traffic%0Aforecasting%20methods%20requires%20realistic%20datasets%2C%20but%20current%20publicly%20available%0Abenchmarks%20have%20significant%20drawbacks%2C%20including%20the%20absence%20of%20information%0Aabout%20road%20connectivity%20for%20road%20graph%20construction%2C%20limited%20information%20about%0Aroad%20properties%2C%20and%20a%20relatively%20small%20number%20of%20road%20segments%20that%20falls%0Ashort%20of%20real-world%20applications.%20Further%2C%20current%20datasets%20mostly%20contain%0Ainformation%20about%20intercity%20highways%20with%20sparsely%20located%20sensors%2C%20while%20city%0Aroad%20networks%20arguably%20present%20a%20more%20challenging%20forecasting%20task%20due%20to%20much%0Adenser%20roads%20and%20more%20complex%20urban%20traffic%20patterns.%20In%20this%20work%2C%20we%20provide%0Aa%20more%20complete%2C%20realistic%2C%20and%20challenging%20benchmark%20for%20traffic%20forecasting%0Aby%20releasing%20datasets%20representing%20the%20road%20networks%20of%20two%20major%20cities%2C%20with%0Athe%20largest%20containing%20almost%20100%2C000%20road%20segments%20%28more%20than%20a%2010-fold%0Aincrease%20relative%20to%20existing%20datasets%29.%20Our%20datasets%20contain%20rich%20road%0Afeatures%20and%20provide%20fine-grained%20data%20about%20both%20traffic%20volume%20and%20traffic%0Aspeed%2C%20allowing%20for%20building%20more%20holistic%20traffic%20forecasting%20systems.%20We%20show%0Athat%20most%20current%20implementations%20of%20neural%20spatiotemporal%20models%20for%20traffic%0Aforecasting%20have%20problems%20scaling%20to%20datasets%20of%20our%20size.%20To%20overcome%20this%0Aissue%2C%20we%20propose%20an%20alternative%20approach%20to%20neural%20traffic%20forecasting%20that%0Auses%20a%20GNN%20without%20a%20dedicated%20module%20for%20temporal%20sequence%20processing%2C%20thus%0Aachieving%20much%20better%20scalability%2C%20while%20also%20demonstrating%20stronger%0Aforecasting%20performance.%20We%20hope%20our%20datasets%20and%20modeling%20insights%20will%20serve%0Aas%20a%20valuable%20resource%20for%20research%20in%20traffic%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02278v1&entry.124074799=Read"},
{"title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A\n  Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose\n  Estimators Against Inertial Sensors in Daily Living Activities", "author": "Mario Medrano-Paredes and Carmen Fern\u00e1ndez-Gonz\u00e1lez and Francisco-Javier D\u00edaz-Pernas and Hichem Saoudi and Javier Gonz\u00e1lez-Alonso and Mario Mart\u00ednez-Zarzuela", "abstract": "  Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.\n", "link": "http://arxiv.org/abs/2510.02264v1", "date": "2025-10-02", "relevancy": 2.3789, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6118}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6097}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paving%20the%20Way%20Towards%20Kinematic%20Assessment%20Using%20Monocular%20Video%3A%20A%0A%20%20Preclinical%20Benchmark%20of%20State-of-the-Art%20Deep-Learning-Based%203D%20Human%20Pose%0A%20%20Estimators%20Against%20Inertial%20Sensors%20in%20Daily%20Living%20Activities&body=Title%3A%20Paving%20the%20Way%20Towards%20Kinematic%20Assessment%20Using%20Monocular%20Video%3A%20A%0A%20%20Preclinical%20Benchmark%20of%20State-of-the-Art%20Deep-Learning-Based%203D%20Human%20Pose%0A%20%20Estimators%20Against%20Inertial%20Sensors%20in%20Daily%20Living%20Activities%0AAuthor%3A%20Mario%20Medrano-Paredes%20and%20Carmen%20Fern%C3%A1ndez-Gonz%C3%A1lez%20and%20Francisco-Javier%20D%C3%ADaz-Pernas%20and%20Hichem%20Saoudi%20and%20Javier%20Gonz%C3%A1lez-Alonso%20and%20Mario%20Mart%C3%ADnez-Zarzuela%0AAbstract%3A%20%20%20Advances%20in%20machine%20learning%20and%20wearable%20sensors%20offer%20new%20opportunities%20for%0Acapturing%20and%20analyzing%20human%20movement%20outside%20specialized%20laboratories.%0AAccurate%20assessment%20of%20human%20movement%20under%20real-world%20conditions%20is%20essential%0Afor%20telemedicine%2C%20sports%20science%2C%20and%20rehabilitation.%20This%20preclinical%0Abenchmark%20compares%20monocular%20video-based%203D%20human%20pose%20estimation%20models%20with%0Ainertial%20measurement%20units%20%28IMUs%29%2C%20leveraging%20the%20VIDIMU%20dataset%20containing%20a%0Atotal%20of%2013%20clinically%20relevant%20daily%20activities%20which%20were%20captured%20using%20both%0Acommodity%20video%20cameras%20and%20five%20IMUs.%20During%20this%20initial%20study%20only%20healthy%0Asubjects%20were%20recorded%2C%20so%20results%20cannot%20be%20generalized%20to%20pathological%0Acohorts.%20Joint%20angles%20derived%20from%20state-of-the-art%20deep%20learning%20frameworks%0A%28MotionAGFormer%2C%20MotionBERT%2C%20MMPose%202D-to-3D%20pose%20lifting%2C%20and%20NVIDIA%0ABodyTrack%29%20were%20evaluated%20against%20joint%20angles%20computed%20from%20IMU%20data%20using%0AOpenSim%20inverse%20kinematics%20following%20the%20Human3.6M%20dataset%20format%20with%2017%0Akeypoints.%20Among%20them%2C%20MotionAGFormer%20demonstrated%20superior%20performance%2C%0Aachieving%20the%20lowest%20overall%20RMSE%20%28%249.27%5Cdeg%20%5Cpm%204.80%5Cdeg%24%29%20and%20MAE%20%28%247.86%5Cdeg%0A%5Cpm%204.18%5Cdeg%24%29%2C%20as%20well%20as%20the%20highest%20Pearson%20correlation%20%28%240.86%20%5Cpm%200.15%24%29%0Aand%20the%20highest%20coefficient%20of%20determination%20%24R%5E%7B2%7D%24%20%28%240.67%20%5Cpm%200.28%24%29.%20The%0Aresults%20reveal%20that%20both%20technologies%20are%20viable%20for%20out-of-the-lab%20kinematic%0Aassessment.%20However%2C%20they%20also%20highlight%20key%20trade-offs%20between%20video-%20and%0Asensor-based%20approaches%20including%20costs%2C%20accessibility%2C%20and%20precision.%20This%0Astudy%20clarifies%20where%20off-the-shelf%20video%20models%20already%20provide%20clinically%0Apromising%20kinematics%20in%20healthy%20adults%20and%20where%20they%20lag%20behind%20IMU-based%0Aestimates%20while%20establishing%20valuable%20guidelines%20for%20researchers%20and%20clinicians%0Aseeking%20to%20develop%20robust%2C%20cost-effective%2C%20and%20user-friendly%20solutions%20for%0Atelehealth%20and%20remote%20patient%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaving%2520the%2520Way%2520Towards%2520Kinematic%2520Assessment%2520Using%2520Monocular%2520Video%253A%2520A%250A%2520%2520Preclinical%2520Benchmark%2520of%2520State-of-the-Art%2520Deep-Learning-Based%25203D%2520Human%2520Pose%250A%2520%2520Estimators%2520Against%2520Inertial%2520Sensors%2520in%2520Daily%2520Living%2520Activities%26entry.906535625%3DMario%2520Medrano-Paredes%2520and%2520Carmen%2520Fern%25C3%25A1ndez-Gonz%25C3%25A1lez%2520and%2520Francisco-Javier%2520D%25C3%25ADaz-Pernas%2520and%2520Hichem%2520Saoudi%2520and%2520Javier%2520Gonz%25C3%25A1lez-Alonso%2520and%2520Mario%2520Mart%25C3%25ADnez-Zarzuela%26entry.1292438233%3D%2520%2520Advances%2520in%2520machine%2520learning%2520and%2520wearable%2520sensors%2520offer%2520new%2520opportunities%2520for%250Acapturing%2520and%2520analyzing%2520human%2520movement%2520outside%2520specialized%2520laboratories.%250AAccurate%2520assessment%2520of%2520human%2520movement%2520under%2520real-world%2520conditions%2520is%2520essential%250Afor%2520telemedicine%252C%2520sports%2520science%252C%2520and%2520rehabilitation.%2520This%2520preclinical%250Abenchmark%2520compares%2520monocular%2520video-based%25203D%2520human%2520pose%2520estimation%2520models%2520with%250Ainertial%2520measurement%2520units%2520%2528IMUs%2529%252C%2520leveraging%2520the%2520VIDIMU%2520dataset%2520containing%2520a%250Atotal%2520of%252013%2520clinically%2520relevant%2520daily%2520activities%2520which%2520were%2520captured%2520using%2520both%250Acommodity%2520video%2520cameras%2520and%2520five%2520IMUs.%2520During%2520this%2520initial%2520study%2520only%2520healthy%250Asubjects%2520were%2520recorded%252C%2520so%2520results%2520cannot%2520be%2520generalized%2520to%2520pathological%250Acohorts.%2520Joint%2520angles%2520derived%2520from%2520state-of-the-art%2520deep%2520learning%2520frameworks%250A%2528MotionAGFormer%252C%2520MotionBERT%252C%2520MMPose%25202D-to-3D%2520pose%2520lifting%252C%2520and%2520NVIDIA%250ABodyTrack%2529%2520were%2520evaluated%2520against%2520joint%2520angles%2520computed%2520from%2520IMU%2520data%2520using%250AOpenSim%2520inverse%2520kinematics%2520following%2520the%2520Human3.6M%2520dataset%2520format%2520with%252017%250Akeypoints.%2520Among%2520them%252C%2520MotionAGFormer%2520demonstrated%2520superior%2520performance%252C%250Aachieving%2520the%2520lowest%2520overall%2520RMSE%2520%2528%25249.27%255Cdeg%2520%255Cpm%25204.80%255Cdeg%2524%2529%2520and%2520MAE%2520%2528%25247.86%255Cdeg%250A%255Cpm%25204.18%255Cdeg%2524%2529%252C%2520as%2520well%2520as%2520the%2520highest%2520Pearson%2520correlation%2520%2528%25240.86%2520%255Cpm%25200.15%2524%2529%250Aand%2520the%2520highest%2520coefficient%2520of%2520determination%2520%2524R%255E%257B2%257D%2524%2520%2528%25240.67%2520%255Cpm%25200.28%2524%2529.%2520The%250Aresults%2520reveal%2520that%2520both%2520technologies%2520are%2520viable%2520for%2520out-of-the-lab%2520kinematic%250Aassessment.%2520However%252C%2520they%2520also%2520highlight%2520key%2520trade-offs%2520between%2520video-%2520and%250Asensor-based%2520approaches%2520including%2520costs%252C%2520accessibility%252C%2520and%2520precision.%2520This%250Astudy%2520clarifies%2520where%2520off-the-shelf%2520video%2520models%2520already%2520provide%2520clinically%250Apromising%2520kinematics%2520in%2520healthy%2520adults%2520and%2520where%2520they%2520lag%2520behind%2520IMU-based%250Aestimates%2520while%2520establishing%2520valuable%2520guidelines%2520for%2520researchers%2520and%2520clinicians%250Aseeking%2520to%2520develop%2520robust%252C%2520cost-effective%252C%2520and%2520user-friendly%2520solutions%2520for%250Atelehealth%2520and%2520remote%2520patient%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paving%20the%20Way%20Towards%20Kinematic%20Assessment%20Using%20Monocular%20Video%3A%20A%0A%20%20Preclinical%20Benchmark%20of%20State-of-the-Art%20Deep-Learning-Based%203D%20Human%20Pose%0A%20%20Estimators%20Against%20Inertial%20Sensors%20in%20Daily%20Living%20Activities&entry.906535625=Mario%20Medrano-Paredes%20and%20Carmen%20Fern%C3%A1ndez-Gonz%C3%A1lez%20and%20Francisco-Javier%20D%C3%ADaz-Pernas%20and%20Hichem%20Saoudi%20and%20Javier%20Gonz%C3%A1lez-Alonso%20and%20Mario%20Mart%C3%ADnez-Zarzuela&entry.1292438233=%20%20Advances%20in%20machine%20learning%20and%20wearable%20sensors%20offer%20new%20opportunities%20for%0Acapturing%20and%20analyzing%20human%20movement%20outside%20specialized%20laboratories.%0AAccurate%20assessment%20of%20human%20movement%20under%20real-world%20conditions%20is%20essential%0Afor%20telemedicine%2C%20sports%20science%2C%20and%20rehabilitation.%20This%20preclinical%0Abenchmark%20compares%20monocular%20video-based%203D%20human%20pose%20estimation%20models%20with%0Ainertial%20measurement%20units%20%28IMUs%29%2C%20leveraging%20the%20VIDIMU%20dataset%20containing%20a%0Atotal%20of%2013%20clinically%20relevant%20daily%20activities%20which%20were%20captured%20using%20both%0Acommodity%20video%20cameras%20and%20five%20IMUs.%20During%20this%20initial%20study%20only%20healthy%0Asubjects%20were%20recorded%2C%20so%20results%20cannot%20be%20generalized%20to%20pathological%0Acohorts.%20Joint%20angles%20derived%20from%20state-of-the-art%20deep%20learning%20frameworks%0A%28MotionAGFormer%2C%20MotionBERT%2C%20MMPose%202D-to-3D%20pose%20lifting%2C%20and%20NVIDIA%0ABodyTrack%29%20were%20evaluated%20against%20joint%20angles%20computed%20from%20IMU%20data%20using%0AOpenSim%20inverse%20kinematics%20following%20the%20Human3.6M%20dataset%20format%20with%2017%0Akeypoints.%20Among%20them%2C%20MotionAGFormer%20demonstrated%20superior%20performance%2C%0Aachieving%20the%20lowest%20overall%20RMSE%20%28%249.27%5Cdeg%20%5Cpm%204.80%5Cdeg%24%29%20and%20MAE%20%28%247.86%5Cdeg%0A%5Cpm%204.18%5Cdeg%24%29%2C%20as%20well%20as%20the%20highest%20Pearson%20correlation%20%28%240.86%20%5Cpm%200.15%24%29%0Aand%20the%20highest%20coefficient%20of%20determination%20%24R%5E%7B2%7D%24%20%28%240.67%20%5Cpm%200.28%24%29.%20The%0Aresults%20reveal%20that%20both%20technologies%20are%20viable%20for%20out-of-the-lab%20kinematic%0Aassessment.%20However%2C%20they%20also%20highlight%20key%20trade-offs%20between%20video-%20and%0Asensor-based%20approaches%20including%20costs%2C%20accessibility%2C%20and%20precision.%20This%0Astudy%20clarifies%20where%20off-the-shelf%20video%20models%20already%20provide%20clinically%0Apromising%20kinematics%20in%20healthy%20adults%20and%20where%20they%20lag%20behind%20IMU-based%0Aestimates%20while%20establishing%20valuable%20guidelines%20for%20researchers%20and%20clinicians%0Aseeking%20to%20develop%20robust%2C%20cost-effective%2C%20and%20user-friendly%20solutions%20for%0Atelehealth%20and%20remote%20patient%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02264v1&entry.124074799=Read"},
{"title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps", "author": "Kyoungjun Park and Yifan Yang and Changhan Ge and Lili Qiu and Shiqi Jiang", "abstract": "  Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.\n", "link": "http://arxiv.org/abs/2510.02274v1", "date": "2025-10-02", "relevancy": 2.3538, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5913}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5913}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%5E2%3A%20Turning%203D%20Environments%20into%20Radio%20Frequency%20Heatmaps&body=Title%3A%20Diffusion%5E2%3A%20Turning%203D%20Environments%20into%20Radio%20Frequency%20Heatmaps%0AAuthor%3A%20Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Changhan%20Ge%20and%20Lili%20Qiu%20and%20Shiqi%20Jiang%0AAbstract%3A%20%20%20Modeling%20radio%20frequency%20%28RF%29%20signal%20propagation%20is%20essential%20for%0Aunderstanding%20the%20environment%2C%20as%20RF%20signals%20offer%20valuable%20insights%20beyond%20the%0Acapabilities%20of%20RGB%20cameras%2C%20which%20are%20limited%20by%20the%20visible-light%20spectrum%2C%0Alens%20coverage%2C%20and%20occlusions.%20It%20is%20also%20useful%20for%20supporting%20wireless%0Adiagnosis%2C%20deployment%2C%20and%20optimization.%20However%2C%20accurately%20predicting%20RF%0Asignals%20in%20complex%20environments%20remains%20a%20challenge%20due%20to%20interactions%20with%0Aobstacles%20such%20as%20absorption%20and%20reflection.%20We%20introduce%20Diffusion%5E2%2C%20a%0Adiffusion-based%20approach%20that%20uses%203D%20point%20clouds%20to%20model%20the%20propagation%20of%0ARF%20signals%20across%20a%20wide%20range%20of%20frequencies%2C%20from%20Wi-Fi%20to%20millimeter%20waves.%0ATo%20effectively%20capture%20RF-related%20features%20from%203D%20data%2C%20we%20present%20the%20RF-3D%0AEncoder%2C%20which%20encapsulates%20the%20complexities%20of%203D%20geometry%20along%20with%0Asignal-specific%20details.%20These%20features%20undergo%20multi-scale%20embedding%20to%0Asimulate%20the%20actual%20RF%20signal%20dissemination%20process.%20Our%20evaluation%2C%20based%20on%0Asynthetic%20and%20real-world%20measurements%2C%20demonstrates%20that%20Diffusion%5E2%20accurately%0Aestimates%20the%20behavior%20of%20RF%20signals%20in%20various%20frequency%20bands%20and%0Aenvironmental%20conditions%2C%20with%20an%20error%20margin%20of%20just%201.9%20dB%20and%2027x%20faster%0Athan%20existing%20methods%2C%20marking%20a%20significant%20advancement%20in%20the%20field.%20Refer%20to%0Ahttps%3A//rfvision-project.github.io/%20for%20more%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%255E2%253A%2520Turning%25203D%2520Environments%2520into%2520Radio%2520Frequency%2520Heatmaps%26entry.906535625%3DKyoungjun%2520Park%2520and%2520Yifan%2520Yang%2520and%2520Changhan%2520Ge%2520and%2520Lili%2520Qiu%2520and%2520Shiqi%2520Jiang%26entry.1292438233%3D%2520%2520Modeling%2520radio%2520frequency%2520%2528RF%2529%2520signal%2520propagation%2520is%2520essential%2520for%250Aunderstanding%2520the%2520environment%252C%2520as%2520RF%2520signals%2520offer%2520valuable%2520insights%2520beyond%2520the%250Acapabilities%2520of%2520RGB%2520cameras%252C%2520which%2520are%2520limited%2520by%2520the%2520visible-light%2520spectrum%252C%250Alens%2520coverage%252C%2520and%2520occlusions.%2520It%2520is%2520also%2520useful%2520for%2520supporting%2520wireless%250Adiagnosis%252C%2520deployment%252C%2520and%2520optimization.%2520However%252C%2520accurately%2520predicting%2520RF%250Asignals%2520in%2520complex%2520environments%2520remains%2520a%2520challenge%2520due%2520to%2520interactions%2520with%250Aobstacles%2520such%2520as%2520absorption%2520and%2520reflection.%2520We%2520introduce%2520Diffusion%255E2%252C%2520a%250Adiffusion-based%2520approach%2520that%2520uses%25203D%2520point%2520clouds%2520to%2520model%2520the%2520propagation%2520of%250ARF%2520signals%2520across%2520a%2520wide%2520range%2520of%2520frequencies%252C%2520from%2520Wi-Fi%2520to%2520millimeter%2520waves.%250ATo%2520effectively%2520capture%2520RF-related%2520features%2520from%25203D%2520data%252C%2520we%2520present%2520the%2520RF-3D%250AEncoder%252C%2520which%2520encapsulates%2520the%2520complexities%2520of%25203D%2520geometry%2520along%2520with%250Asignal-specific%2520details.%2520These%2520features%2520undergo%2520multi-scale%2520embedding%2520to%250Asimulate%2520the%2520actual%2520RF%2520signal%2520dissemination%2520process.%2520Our%2520evaluation%252C%2520based%2520on%250Asynthetic%2520and%2520real-world%2520measurements%252C%2520demonstrates%2520that%2520Diffusion%255E2%2520accurately%250Aestimates%2520the%2520behavior%2520of%2520RF%2520signals%2520in%2520various%2520frequency%2520bands%2520and%250Aenvironmental%2520conditions%252C%2520with%2520an%2520error%2520margin%2520of%2520just%25201.9%2520dB%2520and%252027x%2520faster%250Athan%2520existing%2520methods%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520the%2520field.%2520Refer%2520to%250Ahttps%253A//rfvision-project.github.io/%2520for%2520more%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%5E2%3A%20Turning%203D%20Environments%20into%20Radio%20Frequency%20Heatmaps&entry.906535625=Kyoungjun%20Park%20and%20Yifan%20Yang%20and%20Changhan%20Ge%20and%20Lili%20Qiu%20and%20Shiqi%20Jiang&entry.1292438233=%20%20Modeling%20radio%20frequency%20%28RF%29%20signal%20propagation%20is%20essential%20for%0Aunderstanding%20the%20environment%2C%20as%20RF%20signals%20offer%20valuable%20insights%20beyond%20the%0Acapabilities%20of%20RGB%20cameras%2C%20which%20are%20limited%20by%20the%20visible-light%20spectrum%2C%0Alens%20coverage%2C%20and%20occlusions.%20It%20is%20also%20useful%20for%20supporting%20wireless%0Adiagnosis%2C%20deployment%2C%20and%20optimization.%20However%2C%20accurately%20predicting%20RF%0Asignals%20in%20complex%20environments%20remains%20a%20challenge%20due%20to%20interactions%20with%0Aobstacles%20such%20as%20absorption%20and%20reflection.%20We%20introduce%20Diffusion%5E2%2C%20a%0Adiffusion-based%20approach%20that%20uses%203D%20point%20clouds%20to%20model%20the%20propagation%20of%0ARF%20signals%20across%20a%20wide%20range%20of%20frequencies%2C%20from%20Wi-Fi%20to%20millimeter%20waves.%0ATo%20effectively%20capture%20RF-related%20features%20from%203D%20data%2C%20we%20present%20the%20RF-3D%0AEncoder%2C%20which%20encapsulates%20the%20complexities%20of%203D%20geometry%20along%20with%0Asignal-specific%20details.%20These%20features%20undergo%20multi-scale%20embedding%20to%0Asimulate%20the%20actual%20RF%20signal%20dissemination%20process.%20Our%20evaluation%2C%20based%20on%0Asynthetic%20and%20real-world%20measurements%2C%20demonstrates%20that%20Diffusion%5E2%20accurately%0Aestimates%20the%20behavior%20of%20RF%20signals%20in%20various%20frequency%20bands%20and%0Aenvironmental%20conditions%2C%20with%20an%20error%20margin%20of%20just%201.9%20dB%20and%2027x%20faster%0Athan%20existing%20methods%2C%20marking%20a%20significant%20advancement%20in%20the%20field.%20Refer%20to%0Ahttps%3A//rfvision-project.github.io/%20for%20more%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02274v1&entry.124074799=Read"},
{"title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for\n  Open-Vocabulary 3D Segmentation", "author": "Weijia Dou and Xu Zhang and Yi Bin and Jian Liu and Bo Peng and Guoqing Wang and Yang Yang and Heng Tao Shen", "abstract": "  Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).\n", "link": "http://arxiv.org/abs/2510.02186v1", "date": "2025-10-02", "relevancy": 2.3497, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5899}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5896}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoPurify%3A%20A%20Data-Efficient%20Geometric%20Distillation%20Framework%20for%0A%20%20Open-Vocabulary%203D%20Segmentation&body=Title%3A%20GeoPurify%3A%20A%20Data-Efficient%20Geometric%20Distillation%20Framework%20for%0A%20%20Open-Vocabulary%203D%20Segmentation%0AAuthor%3A%20Weijia%20Dou%20and%20Xu%20Zhang%20and%20Yi%20Bin%20and%20Jian%20Liu%20and%20Bo%20Peng%20and%20Guoqing%20Wang%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20%20%20Recent%20attempts%20to%20transfer%20features%20from%202D%20Vision-Language%20Models%20%28VLMs%29%20to%0A3D%20semantic%20segmentation%20expose%20a%20persistent%20trade-off.%20Directly%20projecting%202D%0Afeatures%20into%203D%20yields%20noisy%20and%20fragmented%20predictions%2C%20whereas%20enforcing%0Ageometric%20coherence%20necessitates%20costly%20training%20pipelines%20and%20large-scale%0Aannotated%203D%20data.%20We%20argue%20that%20this%20limitation%20stems%20from%20the%20dominant%0Asegmentation-and-matching%20paradigm%2C%20which%20fails%20to%20reconcile%202D%20semantics%20with%0A3D%20geometric%20structure.%20The%20geometric%20cues%20are%20not%20eliminated%20during%20the%0A2D-to-3D%20transfer%20but%20remain%20latent%20within%20the%20noisy%20and%20view-aggregated%0Afeatures.%20To%20exploit%20this%20property%2C%20we%20propose%20GeoPurify%20that%20applies%20a%20small%0AStudent%20Affinity%20Network%20to%20purify%202D%20VLM-generated%203D%20point%20features%20using%0Ageometric%20priors%20distilled%20from%20a%203D%20self-supervised%20teacher%20model.%20During%0Ainference%2C%20we%20devise%20a%20Geometry-Guided%20Pooling%20module%20to%20further%20denoise%20the%0Apoint%20cloud%20and%20ensure%20the%20semantic%20and%20structural%20consistency.%20Benefiting%20from%0Alatent%20geometric%20information%20and%20the%20learned%20affinity%20network%2C%20GeoPurify%0Aeffectively%20mitigates%20the%20trade-off%20and%20achieves%20superior%20data%20efficiency.%0AExtensive%20experiments%20on%20major%203D%20benchmarks%20demonstrate%20that%20GeoPurify%0Aachieves%20or%20surpasses%20state-of-the-art%20performance%20while%20utilizing%20only%20about%0A1.5%25%20of%20the%20training%20data.%20Our%20codes%20and%20checkpoints%20are%20available%20at%0A%5Bhttps%3A//github.com/tj12323/GeoPurify%5D%28https%3A//github.com/tj12323/GeoPurify%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoPurify%253A%2520A%2520Data-Efficient%2520Geometric%2520Distillation%2520Framework%2520for%250A%2520%2520Open-Vocabulary%25203D%2520Segmentation%26entry.906535625%3DWeijia%2520Dou%2520and%2520Xu%2520Zhang%2520and%2520Yi%2520Bin%2520and%2520Jian%2520Liu%2520and%2520Bo%2520Peng%2520and%2520Guoqing%2520Wang%2520and%2520Yang%2520Yang%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520attempts%2520to%2520transfer%2520features%2520from%25202D%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%250A3D%2520semantic%2520segmentation%2520expose%2520a%2520persistent%2520trade-off.%2520Directly%2520projecting%25202D%250Afeatures%2520into%25203D%2520yields%2520noisy%2520and%2520fragmented%2520predictions%252C%2520whereas%2520enforcing%250Ageometric%2520coherence%2520necessitates%2520costly%2520training%2520pipelines%2520and%2520large-scale%250Aannotated%25203D%2520data.%2520We%2520argue%2520that%2520this%2520limitation%2520stems%2520from%2520the%2520dominant%250Asegmentation-and-matching%2520paradigm%252C%2520which%2520fails%2520to%2520reconcile%25202D%2520semantics%2520with%250A3D%2520geometric%2520structure.%2520The%2520geometric%2520cues%2520are%2520not%2520eliminated%2520during%2520the%250A2D-to-3D%2520transfer%2520but%2520remain%2520latent%2520within%2520the%2520noisy%2520and%2520view-aggregated%250Afeatures.%2520To%2520exploit%2520this%2520property%252C%2520we%2520propose%2520GeoPurify%2520that%2520applies%2520a%2520small%250AStudent%2520Affinity%2520Network%2520to%2520purify%25202D%2520VLM-generated%25203D%2520point%2520features%2520using%250Ageometric%2520priors%2520distilled%2520from%2520a%25203D%2520self-supervised%2520teacher%2520model.%2520During%250Ainference%252C%2520we%2520devise%2520a%2520Geometry-Guided%2520Pooling%2520module%2520to%2520further%2520denoise%2520the%250Apoint%2520cloud%2520and%2520ensure%2520the%2520semantic%2520and%2520structural%2520consistency.%2520Benefiting%2520from%250Alatent%2520geometric%2520information%2520and%2520the%2520learned%2520affinity%2520network%252C%2520GeoPurify%250Aeffectively%2520mitigates%2520the%2520trade-off%2520and%2520achieves%2520superior%2520data%2520efficiency.%250AExtensive%2520experiments%2520on%2520major%25203D%2520benchmarks%2520demonstrate%2520that%2520GeoPurify%250Aachieves%2520or%2520surpasses%2520state-of-the-art%2520performance%2520while%2520utilizing%2520only%2520about%250A1.5%2525%2520of%2520the%2520training%2520data.%2520Our%2520codes%2520and%2520checkpoints%2520are%2520available%2520at%250A%255Bhttps%253A//github.com/tj12323/GeoPurify%255D%2528https%253A//github.com/tj12323/GeoPurify%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoPurify%3A%20A%20Data-Efficient%20Geometric%20Distillation%20Framework%20for%0A%20%20Open-Vocabulary%203D%20Segmentation&entry.906535625=Weijia%20Dou%20and%20Xu%20Zhang%20and%20Yi%20Bin%20and%20Jian%20Liu%20and%20Bo%20Peng%20and%20Guoqing%20Wang%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen&entry.1292438233=%20%20Recent%20attempts%20to%20transfer%20features%20from%202D%20Vision-Language%20Models%20%28VLMs%29%20to%0A3D%20semantic%20segmentation%20expose%20a%20persistent%20trade-off.%20Directly%20projecting%202D%0Afeatures%20into%203D%20yields%20noisy%20and%20fragmented%20predictions%2C%20whereas%20enforcing%0Ageometric%20coherence%20necessitates%20costly%20training%20pipelines%20and%20large-scale%0Aannotated%203D%20data.%20We%20argue%20that%20this%20limitation%20stems%20from%20the%20dominant%0Asegmentation-and-matching%20paradigm%2C%20which%20fails%20to%20reconcile%202D%20semantics%20with%0A3D%20geometric%20structure.%20The%20geometric%20cues%20are%20not%20eliminated%20during%20the%0A2D-to-3D%20transfer%20but%20remain%20latent%20within%20the%20noisy%20and%20view-aggregated%0Afeatures.%20To%20exploit%20this%20property%2C%20we%20propose%20GeoPurify%20that%20applies%20a%20small%0AStudent%20Affinity%20Network%20to%20purify%202D%20VLM-generated%203D%20point%20features%20using%0Ageometric%20priors%20distilled%20from%20a%203D%20self-supervised%20teacher%20model.%20During%0Ainference%2C%20we%20devise%20a%20Geometry-Guided%20Pooling%20module%20to%20further%20denoise%20the%0Apoint%20cloud%20and%20ensure%20the%20semantic%20and%20structural%20consistency.%20Benefiting%20from%0Alatent%20geometric%20information%20and%20the%20learned%20affinity%20network%2C%20GeoPurify%0Aeffectively%20mitigates%20the%20trade-off%20and%20achieves%20superior%20data%20efficiency.%0AExtensive%20experiments%20on%20major%203D%20benchmarks%20demonstrate%20that%20GeoPurify%0Aachieves%20or%20surpasses%20state-of-the-art%20performance%20while%20utilizing%20only%20about%0A1.5%25%20of%20the%20training%20data.%20Our%20codes%20and%20checkpoints%20are%20available%20at%0A%5Bhttps%3A//github.com/tj12323/GeoPurify%5D%28https%3A//github.com/tj12323/GeoPurify%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02186v1&entry.124074799=Read"},
{"title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video\n  Understanding", "author": "Guangyu Sun and Archit Singhal and Burak Uzkent and Mubarak Shah and Chen Chen and Garin Kessler", "abstract": "  Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c .\n", "link": "http://arxiv.org/abs/2510.02262v1", "date": "2025-10-02", "relevancy": 2.3335, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Frames%20to%20Clips%3A%20Efficient%20Key%20Clip%20Selection%20for%20Long-Form%20Video%0A%20%20Understanding&body=Title%3A%20From%20Frames%20to%20Clips%3A%20Efficient%20Key%20Clip%20Selection%20for%20Long-Form%20Video%0A%20%20Understanding%0AAuthor%3A%20Guangyu%20Sun%20and%20Archit%20Singhal%20and%20Burak%20Uzkent%20and%20Mubarak%20Shah%20and%20Chen%20Chen%20and%20Garin%20Kessler%0AAbstract%3A%20%20%20Video%20Large%20Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20results%20on%20a%0Avariety%20of%20vision%20language%20tasks%2C%20yet%20their%20practical%20use%20is%20limited%20by%20the%0A%22needle%20in%20a%20haystack%22%20problem%3A%20the%20massive%20number%20of%20visual%20tokens%20produced%0Afrom%20raw%20video%20frames%20exhausts%20the%20model%27s%20context%20window.%20Existing%20solutions%0Aalleviate%20this%20issue%20by%20selecting%20a%20sparse%20set%20of%20frames%2C%20thereby%20reducing%0Atoken%20count%2C%20but%20such%20frame-wise%20selection%20discards%20essential%20temporal%0Adynamics%2C%20leading%20to%20suboptimal%20reasoning%20about%20motion%20and%20event%20continuity.%20In%0Athis%20work%20we%20systematically%20explore%20the%20impact%20of%20temporal%20information%20and%0Ademonstrate%20that%20extending%20selection%20from%20isolated%20key%20frames%20to%20key%20clips%2C%0Awhich%20are%20short%2C%20temporally%20coherent%20segments%2C%20improves%20video%20understanding.%20To%0Amaintain%20a%20fixed%20computational%20budget%20while%20accommodating%20the%20larger%20token%0Afootprint%20of%20clips%2C%20we%20propose%20an%20adaptive%20resolution%20strategy%20that%20dynamically%0Abalances%20spatial%20resolution%20and%20clip%20length%2C%20ensuring%20a%20constant%20token%20count%0Aper%20video.%20Experiments%20on%20three%20long-form%20video%20benchmarks%20demonstrate%20that%20our%0Atraining-free%20approach%2C%20F2C%2C%20outperforms%20uniform%20sampling%20up%20to%208.1%25%2C%205.6%25%2C%20and%0A10.3%25%20on%20Video-MME%2C%20LongVideoBench%20and%20MLVU%20benchmarks%2C%20respectively.%20These%0Aresults%20highlight%20the%20importance%20of%20preserving%20temporal%20coherence%20in%20frame%0Aselection%20and%20provide%20a%20practical%20pathway%20for%20scaling%20Video%20LLMs%20to%20real%20world%0Avideo%20understanding%20applications.%20Project%20webpage%20is%20available%20at%0Ahttps%3A//guangyusun.com/f2c%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Frames%2520to%2520Clips%253A%2520Efficient%2520Key%2520Clip%2520Selection%2520for%2520Long-Form%2520Video%250A%2520%2520Understanding%26entry.906535625%3DGuangyu%2520Sun%2520and%2520Archit%2520Singhal%2520and%2520Burak%2520Uzkent%2520and%2520Mubarak%2520Shah%2520and%2520Chen%2520Chen%2520and%2520Garin%2520Kessler%26entry.1292438233%3D%2520%2520Video%2520Large%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520results%2520on%2520a%250Avariety%2520of%2520vision%2520language%2520tasks%252C%2520yet%2520their%2520practical%2520use%2520is%2520limited%2520by%2520the%250A%2522needle%2520in%2520a%2520haystack%2522%2520problem%253A%2520the%2520massive%2520number%2520of%2520visual%2520tokens%2520produced%250Afrom%2520raw%2520video%2520frames%2520exhausts%2520the%2520model%2527s%2520context%2520window.%2520Existing%2520solutions%250Aalleviate%2520this%2520issue%2520by%2520selecting%2520a%2520sparse%2520set%2520of%2520frames%252C%2520thereby%2520reducing%250Atoken%2520count%252C%2520but%2520such%2520frame-wise%2520selection%2520discards%2520essential%2520temporal%250Adynamics%252C%2520leading%2520to%2520suboptimal%2520reasoning%2520about%2520motion%2520and%2520event%2520continuity.%2520In%250Athis%2520work%2520we%2520systematically%2520explore%2520the%2520impact%2520of%2520temporal%2520information%2520and%250Ademonstrate%2520that%2520extending%2520selection%2520from%2520isolated%2520key%2520frames%2520to%2520key%2520clips%252C%250Awhich%2520are%2520short%252C%2520temporally%2520coherent%2520segments%252C%2520improves%2520video%2520understanding.%2520To%250Amaintain%2520a%2520fixed%2520computational%2520budget%2520while%2520accommodating%2520the%2520larger%2520token%250Afootprint%2520of%2520clips%252C%2520we%2520propose%2520an%2520adaptive%2520resolution%2520strategy%2520that%2520dynamically%250Abalances%2520spatial%2520resolution%2520and%2520clip%2520length%252C%2520ensuring%2520a%2520constant%2520token%2520count%250Aper%2520video.%2520Experiments%2520on%2520three%2520long-form%2520video%2520benchmarks%2520demonstrate%2520that%2520our%250Atraining-free%2520approach%252C%2520F2C%252C%2520outperforms%2520uniform%2520sampling%2520up%2520to%25208.1%2525%252C%25205.6%2525%252C%2520and%250A10.3%2525%2520on%2520Video-MME%252C%2520LongVideoBench%2520and%2520MLVU%2520benchmarks%252C%2520respectively.%2520These%250Aresults%2520highlight%2520the%2520importance%2520of%2520preserving%2520temporal%2520coherence%2520in%2520frame%250Aselection%2520and%2520provide%2520a%2520practical%2520pathway%2520for%2520scaling%2520Video%2520LLMs%2520to%2520real%2520world%250Avideo%2520understanding%2520applications.%2520Project%2520webpage%2520is%2520available%2520at%250Ahttps%253A//guangyusun.com/f2c%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Frames%20to%20Clips%3A%20Efficient%20Key%20Clip%20Selection%20for%20Long-Form%20Video%0A%20%20Understanding&entry.906535625=Guangyu%20Sun%20and%20Archit%20Singhal%20and%20Burak%20Uzkent%20and%20Mubarak%20Shah%20and%20Chen%20Chen%20and%20Garin%20Kessler&entry.1292438233=%20%20Video%20Large%20Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20results%20on%20a%0Avariety%20of%20vision%20language%20tasks%2C%20yet%20their%20practical%20use%20is%20limited%20by%20the%0A%22needle%20in%20a%20haystack%22%20problem%3A%20the%20massive%20number%20of%20visual%20tokens%20produced%0Afrom%20raw%20video%20frames%20exhausts%20the%20model%27s%20context%20window.%20Existing%20solutions%0Aalleviate%20this%20issue%20by%20selecting%20a%20sparse%20set%20of%20frames%2C%20thereby%20reducing%0Atoken%20count%2C%20but%20such%20frame-wise%20selection%20discards%20essential%20temporal%0Adynamics%2C%20leading%20to%20suboptimal%20reasoning%20about%20motion%20and%20event%20continuity.%20In%0Athis%20work%20we%20systematically%20explore%20the%20impact%20of%20temporal%20information%20and%0Ademonstrate%20that%20extending%20selection%20from%20isolated%20key%20frames%20to%20key%20clips%2C%0Awhich%20are%20short%2C%20temporally%20coherent%20segments%2C%20improves%20video%20understanding.%20To%0Amaintain%20a%20fixed%20computational%20budget%20while%20accommodating%20the%20larger%20token%0Afootprint%20of%20clips%2C%20we%20propose%20an%20adaptive%20resolution%20strategy%20that%20dynamically%0Abalances%20spatial%20resolution%20and%20clip%20length%2C%20ensuring%20a%20constant%20token%20count%0Aper%20video.%20Experiments%20on%20three%20long-form%20video%20benchmarks%20demonstrate%20that%20our%0Atraining-free%20approach%2C%20F2C%2C%20outperforms%20uniform%20sampling%20up%20to%208.1%25%2C%205.6%25%2C%20and%0A10.3%25%20on%20Video-MME%2C%20LongVideoBench%20and%20MLVU%20benchmarks%2C%20respectively.%20These%0Aresults%20highlight%20the%20importance%20of%20preserving%20temporal%20coherence%20in%20frame%0Aselection%20and%20provide%20a%20practical%20pathway%20for%20scaling%20Video%20LLMs%20to%20real%20world%0Avideo%20understanding%20applications.%20Project%20webpage%20is%20available%20at%0Ahttps%3A//guangyusun.com/f2c%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02262v1&entry.124074799=Read"},
{"title": "NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual\n  Reconstruction of Complex Scenes", "author": "Shiyi Zhang and Dong Liang and Yihang Zhou", "abstract": "  Reconstructing visual information from brain activity via computer vision\ntechnology provides an intuitive understanding of visual neural mechanisms.\nDespite progress in decoding fMRI data with generative models, achieving\naccurate cross-subject reconstruction of visual stimuli remains challenging and\ncomputationally demanding. This difficulty arises from inter-subject\nvariability in neural representations and the brain's abstract encoding of core\nsemantic features in complex visual inputs. To address these challenges, we\npropose NeuroSwift, which integrates complementary adapters via diffusion:\nAutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter\nis trained on Stable Diffusion generated images paired with COCO captions to\nemulate higher visual cortex encoding. For cross-subject generalization, we\npretrain on one subject and then fine-tune only 17 percent of parameters (fully\nconnected layers) for new subjects, while freezing other components. This\nenables state-of-the-art performance with only one hour of training per subject\non lightweight GPUs (three RTX 4090), and it outperforms existing methods.\n", "link": "http://arxiv.org/abs/2510.02266v1", "date": "2025-10-02", "relevancy": 2.3148, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5799}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroSwift%3A%20A%20Lightweight%20Cross-Subject%20Framework%20for%20fMRI%20Visual%0A%20%20Reconstruction%20of%20Complex%20Scenes&body=Title%3A%20NeuroSwift%3A%20A%20Lightweight%20Cross-Subject%20Framework%20for%20fMRI%20Visual%0A%20%20Reconstruction%20of%20Complex%20Scenes%0AAuthor%3A%20Shiyi%20Zhang%20and%20Dong%20Liang%20and%20Yihang%20Zhou%0AAbstract%3A%20%20%20Reconstructing%20visual%20information%20from%20brain%20activity%20via%20computer%20vision%0Atechnology%20provides%20an%20intuitive%20understanding%20of%20visual%20neural%20mechanisms.%0ADespite%20progress%20in%20decoding%20fMRI%20data%20with%20generative%20models%2C%20achieving%0Aaccurate%20cross-subject%20reconstruction%20of%20visual%20stimuli%20remains%20challenging%20and%0Acomputationally%20demanding.%20This%20difficulty%20arises%20from%20inter-subject%0Avariability%20in%20neural%20representations%20and%20the%20brain%27s%20abstract%20encoding%20of%20core%0Asemantic%20features%20in%20complex%20visual%20inputs.%20To%20address%20these%20challenges%2C%20we%0Apropose%20NeuroSwift%2C%20which%20integrates%20complementary%20adapters%20via%20diffusion%3A%0AAutoKL%20for%20low-level%20features%20and%20CLIP%20for%20semantics.%20NeuroSwift%27s%20CLIP%20Adapter%0Ais%20trained%20on%20Stable%20Diffusion%20generated%20images%20paired%20with%20COCO%20captions%20to%0Aemulate%20higher%20visual%20cortex%20encoding.%20For%20cross-subject%20generalization%2C%20we%0Apretrain%20on%20one%20subject%20and%20then%20fine-tune%20only%2017%20percent%20of%20parameters%20%28fully%0Aconnected%20layers%29%20for%20new%20subjects%2C%20while%20freezing%20other%20components.%20This%0Aenables%20state-of-the-art%20performance%20with%20only%20one%20hour%20of%20training%20per%20subject%0Aon%20lightweight%20GPUs%20%28three%20RTX%204090%29%2C%20and%20it%20outperforms%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroSwift%253A%2520A%2520Lightweight%2520Cross-Subject%2520Framework%2520for%2520fMRI%2520Visual%250A%2520%2520Reconstruction%2520of%2520Complex%2520Scenes%26entry.906535625%3DShiyi%2520Zhang%2520and%2520Dong%2520Liang%2520and%2520Yihang%2520Zhou%26entry.1292438233%3D%2520%2520Reconstructing%2520visual%2520information%2520from%2520brain%2520activity%2520via%2520computer%2520vision%250Atechnology%2520provides%2520an%2520intuitive%2520understanding%2520of%2520visual%2520neural%2520mechanisms.%250ADespite%2520progress%2520in%2520decoding%2520fMRI%2520data%2520with%2520generative%2520models%252C%2520achieving%250Aaccurate%2520cross-subject%2520reconstruction%2520of%2520visual%2520stimuli%2520remains%2520challenging%2520and%250Acomputationally%2520demanding.%2520This%2520difficulty%2520arises%2520from%2520inter-subject%250Avariability%2520in%2520neural%2520representations%2520and%2520the%2520brain%2527s%2520abstract%2520encoding%2520of%2520core%250Asemantic%2520features%2520in%2520complex%2520visual%2520inputs.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520NeuroSwift%252C%2520which%2520integrates%2520complementary%2520adapters%2520via%2520diffusion%253A%250AAutoKL%2520for%2520low-level%2520features%2520and%2520CLIP%2520for%2520semantics.%2520NeuroSwift%2527s%2520CLIP%2520Adapter%250Ais%2520trained%2520on%2520Stable%2520Diffusion%2520generated%2520images%2520paired%2520with%2520COCO%2520captions%2520to%250Aemulate%2520higher%2520visual%2520cortex%2520encoding.%2520For%2520cross-subject%2520generalization%252C%2520we%250Apretrain%2520on%2520one%2520subject%2520and%2520then%2520fine-tune%2520only%252017%2520percent%2520of%2520parameters%2520%2528fully%250Aconnected%2520layers%2529%2520for%2520new%2520subjects%252C%2520while%2520freezing%2520other%2520components.%2520This%250Aenables%2520state-of-the-art%2520performance%2520with%2520only%2520one%2520hour%2520of%2520training%2520per%2520subject%250Aon%2520lightweight%2520GPUs%2520%2528three%2520RTX%25204090%2529%252C%2520and%2520it%2520outperforms%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroSwift%3A%20A%20Lightweight%20Cross-Subject%20Framework%20for%20fMRI%20Visual%0A%20%20Reconstruction%20of%20Complex%20Scenes&entry.906535625=Shiyi%20Zhang%20and%20Dong%20Liang%20and%20Yihang%20Zhou&entry.1292438233=%20%20Reconstructing%20visual%20information%20from%20brain%20activity%20via%20computer%20vision%0Atechnology%20provides%20an%20intuitive%20understanding%20of%20visual%20neural%20mechanisms.%0ADespite%20progress%20in%20decoding%20fMRI%20data%20with%20generative%20models%2C%20achieving%0Aaccurate%20cross-subject%20reconstruction%20of%20visual%20stimuli%20remains%20challenging%20and%0Acomputationally%20demanding.%20This%20difficulty%20arises%20from%20inter-subject%0Avariability%20in%20neural%20representations%20and%20the%20brain%27s%20abstract%20encoding%20of%20core%0Asemantic%20features%20in%20complex%20visual%20inputs.%20To%20address%20these%20challenges%2C%20we%0Apropose%20NeuroSwift%2C%20which%20integrates%20complementary%20adapters%20via%20diffusion%3A%0AAutoKL%20for%20low-level%20features%20and%20CLIP%20for%20semantics.%20NeuroSwift%27s%20CLIP%20Adapter%0Ais%20trained%20on%20Stable%20Diffusion%20generated%20images%20paired%20with%20COCO%20captions%20to%0Aemulate%20higher%20visual%20cortex%20encoding.%20For%20cross-subject%20generalization%2C%20we%0Apretrain%20on%20one%20subject%20and%20then%20fine-tune%20only%2017%20percent%20of%20parameters%20%28fully%0Aconnected%20layers%29%20for%20new%20subjects%2C%20while%20freezing%20other%20components.%20This%0Aenables%20state-of-the-art%20performance%20with%20only%20one%20hour%20of%20training%20per%20subject%0Aon%20lightweight%20GPUs%20%28three%20RTX%204090%29%2C%20and%20it%20outperforms%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02266v1&entry.124074799=Read"},
{"title": "Multiple Queries with Multiple Keys: A Precise Prompt Matching Paradigm\n  for Prompt-based Continual Learning", "author": "Dunwei Tu and Huiyu Yi and Yuchi Wang and Baile Xu and Jian Zhao and Furao Shen", "abstract": "  Continual learning requires machine learning models to continuously acquire\nnew knowledge in dynamic environments while avoiding the forgetting of previous\nknowledge. Prompt-based continual learning methods effectively address the\nissue of catastrophic forgetting through prompt expansion and selection.\nHowever, existing approaches often suffer from low accuracy in prompt\nselection, which can result in the model receiving biased knowledge and making\nbiased predictions. To address this issue, we propose the Multiple Queries with\nMultiple Keys (MQMK) prompt matching paradigm for precise prompt selection. The\ngoal of MQMK is to select the prompts whose training data distribution most\nclosely matches that of the test sample. Specifically, Multiple Queries enable\nprecise breadth search by introducing task-specific knowledge, while Multiple\nKeys perform deep search by representing the feature distribution of training\nsamples at a fine-grained level. Each query is designed to perform local\nmatching with a designated task to reduce interference across queries.\nExperiments show that MQMK enhances the prompt matching rate by over 30\\% in\nchallenging scenarios and achieves state-of-the-art performance on three widely\nadopted continual learning benchmarks. The code is available at\nhttps://github.com/DunweiTu/MQMK.\n", "link": "http://arxiv.org/abs/2501.12635v3", "date": "2025-10-02", "relevancy": 2.3043, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple%20Queries%20with%20Multiple%20Keys%3A%20A%20Precise%20Prompt%20Matching%20Paradigm%0A%20%20for%20Prompt-based%20Continual%20Learning&body=Title%3A%20Multiple%20Queries%20with%20Multiple%20Keys%3A%20A%20Precise%20Prompt%20Matching%20Paradigm%0A%20%20for%20Prompt-based%20Continual%20Learning%0AAuthor%3A%20Dunwei%20Tu%20and%20Huiyu%20Yi%20and%20Yuchi%20Wang%20and%20Baile%20Xu%20and%20Jian%20Zhao%20and%20Furao%20Shen%0AAbstract%3A%20%20%20Continual%20learning%20requires%20machine%20learning%20models%20to%20continuously%20acquire%0Anew%20knowledge%20in%20dynamic%20environments%20while%20avoiding%20the%20forgetting%20of%20previous%0Aknowledge.%20Prompt-based%20continual%20learning%20methods%20effectively%20address%20the%0Aissue%20of%20catastrophic%20forgetting%20through%20prompt%20expansion%20and%20selection.%0AHowever%2C%20existing%20approaches%20often%20suffer%20from%20low%20accuracy%20in%20prompt%0Aselection%2C%20which%20can%20result%20in%20the%20model%20receiving%20biased%20knowledge%20and%20making%0Abiased%20predictions.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Multiple%20Queries%20with%0AMultiple%20Keys%20%28MQMK%29%20prompt%20matching%20paradigm%20for%20precise%20prompt%20selection.%20The%0Agoal%20of%20MQMK%20is%20to%20select%20the%20prompts%20whose%20training%20data%20distribution%20most%0Aclosely%20matches%20that%20of%20the%20test%20sample.%20Specifically%2C%20Multiple%20Queries%20enable%0Aprecise%20breadth%20search%20by%20introducing%20task-specific%20knowledge%2C%20while%20Multiple%0AKeys%20perform%20deep%20search%20by%20representing%20the%20feature%20distribution%20of%20training%0Asamples%20at%20a%20fine-grained%20level.%20Each%20query%20is%20designed%20to%20perform%20local%0Amatching%20with%20a%20designated%20task%20to%20reduce%20interference%20across%20queries.%0AExperiments%20show%20that%20MQMK%20enhances%20the%20prompt%20matching%20rate%20by%20over%2030%5C%25%20in%0Achallenging%20scenarios%20and%20achieves%20state-of-the-art%20performance%20on%20three%20widely%0Aadopted%20continual%20learning%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/DunweiTu/MQMK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12635v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple%2520Queries%2520with%2520Multiple%2520Keys%253A%2520A%2520Precise%2520Prompt%2520Matching%2520Paradigm%250A%2520%2520for%2520Prompt-based%2520Continual%2520Learning%26entry.906535625%3DDunwei%2520Tu%2520and%2520Huiyu%2520Yi%2520and%2520Yuchi%2520Wang%2520and%2520Baile%2520Xu%2520and%2520Jian%2520Zhao%2520and%2520Furao%2520Shen%26entry.1292438233%3D%2520%2520Continual%2520learning%2520requires%2520machine%2520learning%2520models%2520to%2520continuously%2520acquire%250Anew%2520knowledge%2520in%2520dynamic%2520environments%2520while%2520avoiding%2520the%2520forgetting%2520of%2520previous%250Aknowledge.%2520Prompt-based%2520continual%2520learning%2520methods%2520effectively%2520address%2520the%250Aissue%2520of%2520catastrophic%2520forgetting%2520through%2520prompt%2520expansion%2520and%2520selection.%250AHowever%252C%2520existing%2520approaches%2520often%2520suffer%2520from%2520low%2520accuracy%2520in%2520prompt%250Aselection%252C%2520which%2520can%2520result%2520in%2520the%2520model%2520receiving%2520biased%2520knowledge%2520and%2520making%250Abiased%2520predictions.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520Multiple%2520Queries%2520with%250AMultiple%2520Keys%2520%2528MQMK%2529%2520prompt%2520matching%2520paradigm%2520for%2520precise%2520prompt%2520selection.%2520The%250Agoal%2520of%2520MQMK%2520is%2520to%2520select%2520the%2520prompts%2520whose%2520training%2520data%2520distribution%2520most%250Aclosely%2520matches%2520that%2520of%2520the%2520test%2520sample.%2520Specifically%252C%2520Multiple%2520Queries%2520enable%250Aprecise%2520breadth%2520search%2520by%2520introducing%2520task-specific%2520knowledge%252C%2520while%2520Multiple%250AKeys%2520perform%2520deep%2520search%2520by%2520representing%2520the%2520feature%2520distribution%2520of%2520training%250Asamples%2520at%2520a%2520fine-grained%2520level.%2520Each%2520query%2520is%2520designed%2520to%2520perform%2520local%250Amatching%2520with%2520a%2520designated%2520task%2520to%2520reduce%2520interference%2520across%2520queries.%250AExperiments%2520show%2520that%2520MQMK%2520enhances%2520the%2520prompt%2520matching%2520rate%2520by%2520over%252030%255C%2525%2520in%250Achallenging%2520scenarios%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520three%2520widely%250Aadopted%2520continual%2520learning%2520benchmarks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/DunweiTu/MQMK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12635v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple%20Queries%20with%20Multiple%20Keys%3A%20A%20Precise%20Prompt%20Matching%20Paradigm%0A%20%20for%20Prompt-based%20Continual%20Learning&entry.906535625=Dunwei%20Tu%20and%20Huiyu%20Yi%20and%20Yuchi%20Wang%20and%20Baile%20Xu%20and%20Jian%20Zhao%20and%20Furao%20Shen&entry.1292438233=%20%20Continual%20learning%20requires%20machine%20learning%20models%20to%20continuously%20acquire%0Anew%20knowledge%20in%20dynamic%20environments%20while%20avoiding%20the%20forgetting%20of%20previous%0Aknowledge.%20Prompt-based%20continual%20learning%20methods%20effectively%20address%20the%0Aissue%20of%20catastrophic%20forgetting%20through%20prompt%20expansion%20and%20selection.%0AHowever%2C%20existing%20approaches%20often%20suffer%20from%20low%20accuracy%20in%20prompt%0Aselection%2C%20which%20can%20result%20in%20the%20model%20receiving%20biased%20knowledge%20and%20making%0Abiased%20predictions.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Multiple%20Queries%20with%0AMultiple%20Keys%20%28MQMK%29%20prompt%20matching%20paradigm%20for%20precise%20prompt%20selection.%20The%0Agoal%20of%20MQMK%20is%20to%20select%20the%20prompts%20whose%20training%20data%20distribution%20most%0Aclosely%20matches%20that%20of%20the%20test%20sample.%20Specifically%2C%20Multiple%20Queries%20enable%0Aprecise%20breadth%20search%20by%20introducing%20task-specific%20knowledge%2C%20while%20Multiple%0AKeys%20perform%20deep%20search%20by%20representing%20the%20feature%20distribution%20of%20training%0Asamples%20at%20a%20fine-grained%20level.%20Each%20query%20is%20designed%20to%20perform%20local%0Amatching%20with%20a%20designated%20task%20to%20reduce%20interference%20across%20queries.%0AExperiments%20show%20that%20MQMK%20enhances%20the%20prompt%20matching%20rate%20by%20over%2030%5C%25%20in%0Achallenging%20scenarios%20and%20achieves%20state-of-the-art%20performance%20on%20three%20widely%0Aadopted%20continual%20learning%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/DunweiTu/MQMK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12635v3&entry.124074799=Read"},
{"title": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex\n  with Mutual Information-Guided Diffusion", "author": "Yule Wang and Joseph Yu and Chengrui Li and Weihan Li and Anqi Wu", "abstract": "  Understanding how neural populations in higher visual areas encode\nobject-centered visual information remains a central challenge in computational\nneuroscience. Prior works have investigated representational alignment between\nartificial neural networks and the visual cortex. Nevertheless, these findings\nare indirect and offer limited insights to the structure of neural populations\nthemselves. Similarly, decoding-based methods have quantified semantic features\nfrom neural populations but have not uncovered their underlying organizations.\nThis leaves open a scientific question: \"how feature-specific visual\ninformation is distributed across neural populations in higher visual areas,\nand whether it is organized into structured, semantically meaningful\nsubspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages\nthe generative power of diffusion models to visualize and validate the\nvisual-semantic attributes encoded in neural latent subspaces. Our method first\nuses a variational autoencoder to infer a group-wise disentangled neural latent\nsubspace from neural populations. Subsequently, we propose a mutual information\n(MI)-guided diffusion synthesis procedure to visualize the specific\nvisual-semantic features encoded by each latent group. We validate MIG-Vis on\nmulti-session neural spiking datasets from the inferior temporal (IT) cortex of\ntwo macaques. The synthesized results demonstrate that our method identifies\nneural latent groups with clear semantic selectivity to diverse visual\nfeatures, including object pose, inter-category transformations, and\nintra-class content. These findings provide direct, interpretable evidence of\nstructured semantic representation in the higher visual cortex and advance our\nunderstanding of its encoding principles.\n", "link": "http://arxiv.org/abs/2510.02182v1", "date": "2025-10-02", "relevancy": 2.3003, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5761}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Semantic%20Selectivity%20of%20Latent%20Groups%20in%20Higher%20Visual%20Cortex%0A%20%20with%20Mutual%20Information-Guided%20Diffusion&body=Title%3A%20Uncovering%20Semantic%20Selectivity%20of%20Latent%20Groups%20in%20Higher%20Visual%20Cortex%0A%20%20with%20Mutual%20Information-Guided%20Diffusion%0AAuthor%3A%20Yule%20Wang%20and%20Joseph%20Yu%20and%20Chengrui%20Li%20and%20Weihan%20Li%20and%20Anqi%20Wu%0AAbstract%3A%20%20%20Understanding%20how%20neural%20populations%20in%20higher%20visual%20areas%20encode%0Aobject-centered%20visual%20information%20remains%20a%20central%20challenge%20in%20computational%0Aneuroscience.%20Prior%20works%20have%20investigated%20representational%20alignment%20between%0Aartificial%20neural%20networks%20and%20the%20visual%20cortex.%20Nevertheless%2C%20these%20findings%0Aare%20indirect%20and%20offer%20limited%20insights%20to%20the%20structure%20of%20neural%20populations%0Athemselves.%20Similarly%2C%20decoding-based%20methods%20have%20quantified%20semantic%20features%0Afrom%20neural%20populations%20but%20have%20not%20uncovered%20their%20underlying%20organizations.%0AThis%20leaves%20open%20a%20scientific%20question%3A%20%22how%20feature-specific%20visual%0Ainformation%20is%20distributed%20across%20neural%20populations%20in%20higher%20visual%20areas%2C%0Aand%20whether%20it%20is%20organized%20into%20structured%2C%20semantically%20meaningful%0Asubspaces.%22%20To%20tackle%20this%20problem%2C%20we%20present%20MIG-Vis%2C%20a%20method%20that%20leverages%0Athe%20generative%20power%20of%20diffusion%20models%20to%20visualize%20and%20validate%20the%0Avisual-semantic%20attributes%20encoded%20in%20neural%20latent%20subspaces.%20Our%20method%20first%0Auses%20a%20variational%20autoencoder%20to%20infer%20a%20group-wise%20disentangled%20neural%20latent%0Asubspace%20from%20neural%20populations.%20Subsequently%2C%20we%20propose%20a%20mutual%20information%0A%28MI%29-guided%20diffusion%20synthesis%20procedure%20to%20visualize%20the%20specific%0Avisual-semantic%20features%20encoded%20by%20each%20latent%20group.%20We%20validate%20MIG-Vis%20on%0Amulti-session%20neural%20spiking%20datasets%20from%20the%20inferior%20temporal%20%28IT%29%20cortex%20of%0Atwo%20macaques.%20The%20synthesized%20results%20demonstrate%20that%20our%20method%20identifies%0Aneural%20latent%20groups%20with%20clear%20semantic%20selectivity%20to%20diverse%20visual%0Afeatures%2C%20including%20object%20pose%2C%20inter-category%20transformations%2C%20and%0Aintra-class%20content.%20These%20findings%20provide%20direct%2C%20interpretable%20evidence%20of%0Astructured%20semantic%20representation%20in%20the%20higher%20visual%20cortex%20and%20advance%20our%0Aunderstanding%20of%20its%20encoding%20principles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Semantic%2520Selectivity%2520of%2520Latent%2520Groups%2520in%2520Higher%2520Visual%2520Cortex%250A%2520%2520with%2520Mutual%2520Information-Guided%2520Diffusion%26entry.906535625%3DYule%2520Wang%2520and%2520Joseph%2520Yu%2520and%2520Chengrui%2520Li%2520and%2520Weihan%2520Li%2520and%2520Anqi%2520Wu%26entry.1292438233%3D%2520%2520Understanding%2520how%2520neural%2520populations%2520in%2520higher%2520visual%2520areas%2520encode%250Aobject-centered%2520visual%2520information%2520remains%2520a%2520central%2520challenge%2520in%2520computational%250Aneuroscience.%2520Prior%2520works%2520have%2520investigated%2520representational%2520alignment%2520between%250Aartificial%2520neural%2520networks%2520and%2520the%2520visual%2520cortex.%2520Nevertheless%252C%2520these%2520findings%250Aare%2520indirect%2520and%2520offer%2520limited%2520insights%2520to%2520the%2520structure%2520of%2520neural%2520populations%250Athemselves.%2520Similarly%252C%2520decoding-based%2520methods%2520have%2520quantified%2520semantic%2520features%250Afrom%2520neural%2520populations%2520but%2520have%2520not%2520uncovered%2520their%2520underlying%2520organizations.%250AThis%2520leaves%2520open%2520a%2520scientific%2520question%253A%2520%2522how%2520feature-specific%2520visual%250Ainformation%2520is%2520distributed%2520across%2520neural%2520populations%2520in%2520higher%2520visual%2520areas%252C%250Aand%2520whether%2520it%2520is%2520organized%2520into%2520structured%252C%2520semantically%2520meaningful%250Asubspaces.%2522%2520To%2520tackle%2520this%2520problem%252C%2520we%2520present%2520MIG-Vis%252C%2520a%2520method%2520that%2520leverages%250Athe%2520generative%2520power%2520of%2520diffusion%2520models%2520to%2520visualize%2520and%2520validate%2520the%250Avisual-semantic%2520attributes%2520encoded%2520in%2520neural%2520latent%2520subspaces.%2520Our%2520method%2520first%250Auses%2520a%2520variational%2520autoencoder%2520to%2520infer%2520a%2520group-wise%2520disentangled%2520neural%2520latent%250Asubspace%2520from%2520neural%2520populations.%2520Subsequently%252C%2520we%2520propose%2520a%2520mutual%2520information%250A%2528MI%2529-guided%2520diffusion%2520synthesis%2520procedure%2520to%2520visualize%2520the%2520specific%250Avisual-semantic%2520features%2520encoded%2520by%2520each%2520latent%2520group.%2520We%2520validate%2520MIG-Vis%2520on%250Amulti-session%2520neural%2520spiking%2520datasets%2520from%2520the%2520inferior%2520temporal%2520%2528IT%2529%2520cortex%2520of%250Atwo%2520macaques.%2520The%2520synthesized%2520results%2520demonstrate%2520that%2520our%2520method%2520identifies%250Aneural%2520latent%2520groups%2520with%2520clear%2520semantic%2520selectivity%2520to%2520diverse%2520visual%250Afeatures%252C%2520including%2520object%2520pose%252C%2520inter-category%2520transformations%252C%2520and%250Aintra-class%2520content.%2520These%2520findings%2520provide%2520direct%252C%2520interpretable%2520evidence%2520of%250Astructured%2520semantic%2520representation%2520in%2520the%2520higher%2520visual%2520cortex%2520and%2520advance%2520our%250Aunderstanding%2520of%2520its%2520encoding%2520principles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Semantic%20Selectivity%20of%20Latent%20Groups%20in%20Higher%20Visual%20Cortex%0A%20%20with%20Mutual%20Information-Guided%20Diffusion&entry.906535625=Yule%20Wang%20and%20Joseph%20Yu%20and%20Chengrui%20Li%20and%20Weihan%20Li%20and%20Anqi%20Wu&entry.1292438233=%20%20Understanding%20how%20neural%20populations%20in%20higher%20visual%20areas%20encode%0Aobject-centered%20visual%20information%20remains%20a%20central%20challenge%20in%20computational%0Aneuroscience.%20Prior%20works%20have%20investigated%20representational%20alignment%20between%0Aartificial%20neural%20networks%20and%20the%20visual%20cortex.%20Nevertheless%2C%20these%20findings%0Aare%20indirect%20and%20offer%20limited%20insights%20to%20the%20structure%20of%20neural%20populations%0Athemselves.%20Similarly%2C%20decoding-based%20methods%20have%20quantified%20semantic%20features%0Afrom%20neural%20populations%20but%20have%20not%20uncovered%20their%20underlying%20organizations.%0AThis%20leaves%20open%20a%20scientific%20question%3A%20%22how%20feature-specific%20visual%0Ainformation%20is%20distributed%20across%20neural%20populations%20in%20higher%20visual%20areas%2C%0Aand%20whether%20it%20is%20organized%20into%20structured%2C%20semantically%20meaningful%0Asubspaces.%22%20To%20tackle%20this%20problem%2C%20we%20present%20MIG-Vis%2C%20a%20method%20that%20leverages%0Athe%20generative%20power%20of%20diffusion%20models%20to%20visualize%20and%20validate%20the%0Avisual-semantic%20attributes%20encoded%20in%20neural%20latent%20subspaces.%20Our%20method%20first%0Auses%20a%20variational%20autoencoder%20to%20infer%20a%20group-wise%20disentangled%20neural%20latent%0Asubspace%20from%20neural%20populations.%20Subsequently%2C%20we%20propose%20a%20mutual%20information%0A%28MI%29-guided%20diffusion%20synthesis%20procedure%20to%20visualize%20the%20specific%0Avisual-semantic%20features%20encoded%20by%20each%20latent%20group.%20We%20validate%20MIG-Vis%20on%0Amulti-session%20neural%20spiking%20datasets%20from%20the%20inferior%20temporal%20%28IT%29%20cortex%20of%0Atwo%20macaques.%20The%20synthesized%20results%20demonstrate%20that%20our%20method%20identifies%0Aneural%20latent%20groups%20with%20clear%20semantic%20selectivity%20to%20diverse%20visual%0Afeatures%2C%20including%20object%20pose%2C%20inter-category%20transformations%2C%20and%0Aintra-class%20content.%20These%20findings%20provide%20direct%2C%20interpretable%20evidence%20of%0Astructured%20semantic%20representation%20in%20the%20higher%20visual%20cortex%20and%20advance%20our%0Aunderstanding%20of%20its%20encoding%20principles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02182v1&entry.124074799=Read"},
{"title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion\n  Tracking", "author": "Joao Pedro Araujo and Yanjie Ze and Pei Xu and Jiajun Wu and C. Karen Liu", "abstract": "  Humanoid motion tracking policies are central to building teleoperation\npipelines and hierarchical controllers, yet they face a fundamental challenge:\nthe embodiment gap between humans and humanoid robots. Current approaches\naddress this gap by retargeting human motion data to humanoid embodiments and\nthen training reinforcement learning (RL) policies to imitate these reference\ntrajectories. However, artifacts introduced during retargeting, such as foot\nsliding, self-penetration, and physically infeasible motion are often left in\nthe reference trajectories for the RL policy to correct. While prior work has\ndemonstrated motion tracking abilities, they often require extensive reward\nengineering and domain randomization to succeed. In this paper, we\nsystematically evaluate how retargeting quality affects policy performance when\nexcessive reward tuning is suppressed. To address issues that we identify with\nexisting retargeting methods, we propose a new retargeting method, General\nMotion Retargeting (GMR). We evaluate GMR alongside two open-source\nretargeters, PHC and ProtoMotions, as well as with a high-quality closed-source\ndataset from Unitree. Using BeyondMimic for policy training, we isolate\nretargeting effects without reward tuning. Our experiments on a diverse subset\nof the LAFAN1 dataset reveal that while most motions can be tracked, artifacts\nin retargeted data significantly reduce policy robustness, particularly for\ndynamic or long sequences. GMR consistently outperforms existing open-source\nmethods in both tracking performance and faithfulness to the source motion,\nachieving perceptual fidelity and policy success rates close to the\nclosed-source baseline. Website:\nhttps://jaraujo98.github.io/retargeting_matters. Code:\nhttps://github.com/YanjieZe/GMR.\n", "link": "http://arxiv.org/abs/2510.02252v1", "date": "2025-10-02", "relevancy": 2.2753, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6034}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retargeting%20Matters%3A%20General%20Motion%20Retargeting%20for%20Humanoid%20Motion%0A%20%20Tracking&body=Title%3A%20Retargeting%20Matters%3A%20General%20Motion%20Retargeting%20for%20Humanoid%20Motion%0A%20%20Tracking%0AAuthor%3A%20Joao%20Pedro%20Araujo%20and%20Yanjie%20Ze%20and%20Pei%20Xu%20and%20Jiajun%20Wu%20and%20C.%20Karen%20Liu%0AAbstract%3A%20%20%20Humanoid%20motion%20tracking%20policies%20are%20central%20to%20building%20teleoperation%0Apipelines%20and%20hierarchical%20controllers%2C%20yet%20they%20face%20a%20fundamental%20challenge%3A%0Athe%20embodiment%20gap%20between%20humans%20and%20humanoid%20robots.%20Current%20approaches%0Aaddress%20this%20gap%20by%20retargeting%20human%20motion%20data%20to%20humanoid%20embodiments%20and%0Athen%20training%20reinforcement%20learning%20%28RL%29%20policies%20to%20imitate%20these%20reference%0Atrajectories.%20However%2C%20artifacts%20introduced%20during%20retargeting%2C%20such%20as%20foot%0Asliding%2C%20self-penetration%2C%20and%20physically%20infeasible%20motion%20are%20often%20left%20in%0Athe%20reference%20trajectories%20for%20the%20RL%20policy%20to%20correct.%20While%20prior%20work%20has%0Ademonstrated%20motion%20tracking%20abilities%2C%20they%20often%20require%20extensive%20reward%0Aengineering%20and%20domain%20randomization%20to%20succeed.%20In%20this%20paper%2C%20we%0Asystematically%20evaluate%20how%20retargeting%20quality%20affects%20policy%20performance%20when%0Aexcessive%20reward%20tuning%20is%20suppressed.%20To%20address%20issues%20that%20we%20identify%20with%0Aexisting%20retargeting%20methods%2C%20we%20propose%20a%20new%20retargeting%20method%2C%20General%0AMotion%20Retargeting%20%28GMR%29.%20We%20evaluate%20GMR%20alongside%20two%20open-source%0Aretargeters%2C%20PHC%20and%20ProtoMotions%2C%20as%20well%20as%20with%20a%20high-quality%20closed-source%0Adataset%20from%20Unitree.%20Using%20BeyondMimic%20for%20policy%20training%2C%20we%20isolate%0Aretargeting%20effects%20without%20reward%20tuning.%20Our%20experiments%20on%20a%20diverse%20subset%0Aof%20the%20LAFAN1%20dataset%20reveal%20that%20while%20most%20motions%20can%20be%20tracked%2C%20artifacts%0Ain%20retargeted%20data%20significantly%20reduce%20policy%20robustness%2C%20particularly%20for%0Adynamic%20or%20long%20sequences.%20GMR%20consistently%20outperforms%20existing%20open-source%0Amethods%20in%20both%20tracking%20performance%20and%20faithfulness%20to%20the%20source%20motion%2C%0Aachieving%20perceptual%20fidelity%20and%20policy%20success%20rates%20close%20to%20the%0Aclosed-source%20baseline.%20Website%3A%0Ahttps%3A//jaraujo98.github.io/retargeting_matters.%20Code%3A%0Ahttps%3A//github.com/YanjieZe/GMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetargeting%2520Matters%253A%2520General%2520Motion%2520Retargeting%2520for%2520Humanoid%2520Motion%250A%2520%2520Tracking%26entry.906535625%3DJoao%2520Pedro%2520Araujo%2520and%2520Yanjie%2520Ze%2520and%2520Pei%2520Xu%2520and%2520Jiajun%2520Wu%2520and%2520C.%2520Karen%2520Liu%26entry.1292438233%3D%2520%2520Humanoid%2520motion%2520tracking%2520policies%2520are%2520central%2520to%2520building%2520teleoperation%250Apipelines%2520and%2520hierarchical%2520controllers%252C%2520yet%2520they%2520face%2520a%2520fundamental%2520challenge%253A%250Athe%2520embodiment%2520gap%2520between%2520humans%2520and%2520humanoid%2520robots.%2520Current%2520approaches%250Aaddress%2520this%2520gap%2520by%2520retargeting%2520human%2520motion%2520data%2520to%2520humanoid%2520embodiments%2520and%250Athen%2520training%2520reinforcement%2520learning%2520%2528RL%2529%2520policies%2520to%2520imitate%2520these%2520reference%250Atrajectories.%2520However%252C%2520artifacts%2520introduced%2520during%2520retargeting%252C%2520such%2520as%2520foot%250Asliding%252C%2520self-penetration%252C%2520and%2520physically%2520infeasible%2520motion%2520are%2520often%2520left%2520in%250Athe%2520reference%2520trajectories%2520for%2520the%2520RL%2520policy%2520to%2520correct.%2520While%2520prior%2520work%2520has%250Ademonstrated%2520motion%2520tracking%2520abilities%252C%2520they%2520often%2520require%2520extensive%2520reward%250Aengineering%2520and%2520domain%2520randomization%2520to%2520succeed.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520evaluate%2520how%2520retargeting%2520quality%2520affects%2520policy%2520performance%2520when%250Aexcessive%2520reward%2520tuning%2520is%2520suppressed.%2520To%2520address%2520issues%2520that%2520we%2520identify%2520with%250Aexisting%2520retargeting%2520methods%252C%2520we%2520propose%2520a%2520new%2520retargeting%2520method%252C%2520General%250AMotion%2520Retargeting%2520%2528GMR%2529.%2520We%2520evaluate%2520GMR%2520alongside%2520two%2520open-source%250Aretargeters%252C%2520PHC%2520and%2520ProtoMotions%252C%2520as%2520well%2520as%2520with%2520a%2520high-quality%2520closed-source%250Adataset%2520from%2520Unitree.%2520Using%2520BeyondMimic%2520for%2520policy%2520training%252C%2520we%2520isolate%250Aretargeting%2520effects%2520without%2520reward%2520tuning.%2520Our%2520experiments%2520on%2520a%2520diverse%2520subset%250Aof%2520the%2520LAFAN1%2520dataset%2520reveal%2520that%2520while%2520most%2520motions%2520can%2520be%2520tracked%252C%2520artifacts%250Ain%2520retargeted%2520data%2520significantly%2520reduce%2520policy%2520robustness%252C%2520particularly%2520for%250Adynamic%2520or%2520long%2520sequences.%2520GMR%2520consistently%2520outperforms%2520existing%2520open-source%250Amethods%2520in%2520both%2520tracking%2520performance%2520and%2520faithfulness%2520to%2520the%2520source%2520motion%252C%250Aachieving%2520perceptual%2520fidelity%2520and%2520policy%2520success%2520rates%2520close%2520to%2520the%250Aclosed-source%2520baseline.%2520Website%253A%250Ahttps%253A//jaraujo98.github.io/retargeting_matters.%2520Code%253A%250Ahttps%253A//github.com/YanjieZe/GMR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retargeting%20Matters%3A%20General%20Motion%20Retargeting%20for%20Humanoid%20Motion%0A%20%20Tracking&entry.906535625=Joao%20Pedro%20Araujo%20and%20Yanjie%20Ze%20and%20Pei%20Xu%20and%20Jiajun%20Wu%20and%20C.%20Karen%20Liu&entry.1292438233=%20%20Humanoid%20motion%20tracking%20policies%20are%20central%20to%20building%20teleoperation%0Apipelines%20and%20hierarchical%20controllers%2C%20yet%20they%20face%20a%20fundamental%20challenge%3A%0Athe%20embodiment%20gap%20between%20humans%20and%20humanoid%20robots.%20Current%20approaches%0Aaddress%20this%20gap%20by%20retargeting%20human%20motion%20data%20to%20humanoid%20embodiments%20and%0Athen%20training%20reinforcement%20learning%20%28RL%29%20policies%20to%20imitate%20these%20reference%0Atrajectories.%20However%2C%20artifacts%20introduced%20during%20retargeting%2C%20such%20as%20foot%0Asliding%2C%20self-penetration%2C%20and%20physically%20infeasible%20motion%20are%20often%20left%20in%0Athe%20reference%20trajectories%20for%20the%20RL%20policy%20to%20correct.%20While%20prior%20work%20has%0Ademonstrated%20motion%20tracking%20abilities%2C%20they%20often%20require%20extensive%20reward%0Aengineering%20and%20domain%20randomization%20to%20succeed.%20In%20this%20paper%2C%20we%0Asystematically%20evaluate%20how%20retargeting%20quality%20affects%20policy%20performance%20when%0Aexcessive%20reward%20tuning%20is%20suppressed.%20To%20address%20issues%20that%20we%20identify%20with%0Aexisting%20retargeting%20methods%2C%20we%20propose%20a%20new%20retargeting%20method%2C%20General%0AMotion%20Retargeting%20%28GMR%29.%20We%20evaluate%20GMR%20alongside%20two%20open-source%0Aretargeters%2C%20PHC%20and%20ProtoMotions%2C%20as%20well%20as%20with%20a%20high-quality%20closed-source%0Adataset%20from%20Unitree.%20Using%20BeyondMimic%20for%20policy%20training%2C%20we%20isolate%0Aretargeting%20effects%20without%20reward%20tuning.%20Our%20experiments%20on%20a%20diverse%20subset%0Aof%20the%20LAFAN1%20dataset%20reveal%20that%20while%20most%20motions%20can%20be%20tracked%2C%20artifacts%0Ain%20retargeted%20data%20significantly%20reduce%20policy%20robustness%2C%20particularly%20for%0Adynamic%20or%20long%20sequences.%20GMR%20consistently%20outperforms%20existing%20open-source%0Amethods%20in%20both%20tracking%20performance%20and%20faithfulness%20to%20the%20source%20motion%2C%0Aachieving%20perceptual%20fidelity%20and%20policy%20success%20rates%20close%20to%20the%0Aclosed-source%20baseline.%20Website%3A%0Ahttps%3A//jaraujo98.github.io/retargeting_matters.%20Code%3A%0Ahttps%3A//github.com/YanjieZe/GMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02252v1&entry.124074799=Read"},
{"title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation", "author": "Mykyta Ielanskyi and Kajetan Schweighofer and Lukas Aichberger and Sepp Hochreiter", "abstract": "  Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.\n", "link": "http://arxiv.org/abs/2510.02279v1", "date": "2025-10-02", "relevancy": 2.2521, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6146}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5627}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Pitfalls%20in%20the%20Evaluation%20of%20Uncertainty%20Estimation%20Methods%0A%20%20for%20Natural%20Language%20Generation&body=Title%3A%20Addressing%20Pitfalls%20in%20the%20Evaluation%20of%20Uncertainty%20Estimation%20Methods%0A%20%20for%20Natural%20Language%20Generation%0AAuthor%3A%20Mykyta%20Ielanskyi%20and%20Kajetan%20Schweighofer%20and%20Lukas%20Aichberger%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20Hallucinations%20are%20a%20common%20issue%20that%20undermine%20the%20reliability%20of%20large%0Alanguage%20models%20%28LLMs%29.%20Recent%20studies%20have%20identified%20a%20specific%20subset%20of%0Ahallucinations%2C%20known%20as%20confabulations%2C%20which%20arise%20due%20to%20predictive%0Auncertainty%20of%20LLMs.%20To%20detect%20confabulations%2C%20various%20methods%20for%20estimating%0Apredictive%20uncertainty%20in%20natural%20language%20generation%20%28NLG%29%20have%20been%0Adeveloped.%20These%20methods%20are%20typically%20evaluated%20by%20correlating%20uncertainty%0Aestimates%20with%20the%20correctness%20of%20generated%20text%2C%20with%20question-answering%20%28QA%29%0Adatasets%20serving%20as%20the%20standard%20benchmark.%20However%2C%20commonly%20used%20approximate%0Acorrectness%20functions%20have%20substantial%20disagreement%20between%20each%20other%20and%2C%0Aconsequently%2C%20in%20the%20ranking%20of%20the%20uncertainty%20estimation%20methods.%20This%20allows%0Aone%20to%20inflate%20the%20apparent%20performance%20of%20uncertainty%20estimation%20methods.%20We%0Apropose%20using%20several%20alternative%20risk%20indicators%20for%20risk%20correlation%0Aexperiments%20that%20improve%20robustness%20of%20empirical%20assessment%20of%20UE%20algorithms%0Afor%20NLG.%20For%20QA%20tasks%2C%20we%20show%20that%20marginalizing%20over%20multiple%20LLM-as-a-judge%0Avariants%20leads%20to%20reducing%20the%20evaluation%20biases.%20Furthermore%2C%20we%20explore%0Astructured%20tasks%20as%20well%20as%20out%20of%20distribution%20and%20perturbation%20detection%0Atasks%20which%20provide%20robust%20and%20controllable%20risk%20indicators.%20Finally%2C%20we%0Apropose%20to%20use%20an%20Elo%20rating%20of%20uncertainty%20estimation%20methods%20to%20give%20an%0Aobjective%20summarization%20over%20extensive%20evaluation%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Pitfalls%2520in%2520the%2520Evaluation%2520of%2520Uncertainty%2520Estimation%2520Methods%250A%2520%2520for%2520Natural%2520Language%2520Generation%26entry.906535625%3DMykyta%2520Ielanskyi%2520and%2520Kajetan%2520Schweighofer%2520and%2520Lukas%2520Aichberger%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520Hallucinations%2520are%2520a%2520common%2520issue%2520that%2520undermine%2520the%2520reliability%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520Recent%2520studies%2520have%2520identified%2520a%2520specific%2520subset%2520of%250Ahallucinations%252C%2520known%2520as%2520confabulations%252C%2520which%2520arise%2520due%2520to%2520predictive%250Auncertainty%2520of%2520LLMs.%2520To%2520detect%2520confabulations%252C%2520various%2520methods%2520for%2520estimating%250Apredictive%2520uncertainty%2520in%2520natural%2520language%2520generation%2520%2528NLG%2529%2520have%2520been%250Adeveloped.%2520These%2520methods%2520are%2520typically%2520evaluated%2520by%2520correlating%2520uncertainty%250Aestimates%2520with%2520the%2520correctness%2520of%2520generated%2520text%252C%2520with%2520question-answering%2520%2528QA%2529%250Adatasets%2520serving%2520as%2520the%2520standard%2520benchmark.%2520However%252C%2520commonly%2520used%2520approximate%250Acorrectness%2520functions%2520have%2520substantial%2520disagreement%2520between%2520each%2520other%2520and%252C%250Aconsequently%252C%2520in%2520the%2520ranking%2520of%2520the%2520uncertainty%2520estimation%2520methods.%2520This%2520allows%250Aone%2520to%2520inflate%2520the%2520apparent%2520performance%2520of%2520uncertainty%2520estimation%2520methods.%2520We%250Apropose%2520using%2520several%2520alternative%2520risk%2520indicators%2520for%2520risk%2520correlation%250Aexperiments%2520that%2520improve%2520robustness%2520of%2520empirical%2520assessment%2520of%2520UE%2520algorithms%250Afor%2520NLG.%2520For%2520QA%2520tasks%252C%2520we%2520show%2520that%2520marginalizing%2520over%2520multiple%2520LLM-as-a-judge%250Avariants%2520leads%2520to%2520reducing%2520the%2520evaluation%2520biases.%2520Furthermore%252C%2520we%2520explore%250Astructured%2520tasks%2520as%2520well%2520as%2520out%2520of%2520distribution%2520and%2520perturbation%2520detection%250Atasks%2520which%2520provide%2520robust%2520and%2520controllable%2520risk%2520indicators.%2520Finally%252C%2520we%250Apropose%2520to%2520use%2520an%2520Elo%2520rating%2520of%2520uncertainty%2520estimation%2520methods%2520to%2520give%2520an%250Aobjective%2520summarization%2520over%2520extensive%2520evaluation%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Pitfalls%20in%20the%20Evaluation%20of%20Uncertainty%20Estimation%20Methods%0A%20%20for%20Natural%20Language%20Generation&entry.906535625=Mykyta%20Ielanskyi%20and%20Kajetan%20Schweighofer%20and%20Lukas%20Aichberger%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20Hallucinations%20are%20a%20common%20issue%20that%20undermine%20the%20reliability%20of%20large%0Alanguage%20models%20%28LLMs%29.%20Recent%20studies%20have%20identified%20a%20specific%20subset%20of%0Ahallucinations%2C%20known%20as%20confabulations%2C%20which%20arise%20due%20to%20predictive%0Auncertainty%20of%20LLMs.%20To%20detect%20confabulations%2C%20various%20methods%20for%20estimating%0Apredictive%20uncertainty%20in%20natural%20language%20generation%20%28NLG%29%20have%20been%0Adeveloped.%20These%20methods%20are%20typically%20evaluated%20by%20correlating%20uncertainty%0Aestimates%20with%20the%20correctness%20of%20generated%20text%2C%20with%20question-answering%20%28QA%29%0Adatasets%20serving%20as%20the%20standard%20benchmark.%20However%2C%20commonly%20used%20approximate%0Acorrectness%20functions%20have%20substantial%20disagreement%20between%20each%20other%20and%2C%0Aconsequently%2C%20in%20the%20ranking%20of%20the%20uncertainty%20estimation%20methods.%20This%20allows%0Aone%20to%20inflate%20the%20apparent%20performance%20of%20uncertainty%20estimation%20methods.%20We%0Apropose%20using%20several%20alternative%20risk%20indicators%20for%20risk%20correlation%0Aexperiments%20that%20improve%20robustness%20of%20empirical%20assessment%20of%20UE%20algorithms%0Afor%20NLG.%20For%20QA%20tasks%2C%20we%20show%20that%20marginalizing%20over%20multiple%20LLM-as-a-judge%0Avariants%20leads%20to%20reducing%20the%20evaluation%20biases.%20Furthermore%2C%20we%20explore%0Astructured%20tasks%20as%20well%20as%20out%20of%20distribution%20and%20perturbation%20detection%0Atasks%20which%20provide%20robust%20and%20controllable%20risk%20indicators.%20Finally%2C%20we%0Apropose%20to%20use%20an%20Elo%20rating%20of%20uncertainty%20estimation%20methods%20to%20give%20an%0Aobjective%20summarization%20over%20extensive%20evaluation%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02279v1&entry.124074799=Read"},
{"title": "Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier", "author": "Craig W. Schmidt and Varshini Reddy and Chris Tanner and Yuval Pinter", "abstract": "  Pre-tokenization, the initial step in many modern tokenization pipelines,\nsegments text into smaller units called pretokens, typically splitting on\nwhitespace and punctuation. While this process encourages having full,\nindividual words as tokens, it introduces a fundamental limitation in most\ntokenization algorithms such as Byte Pair Encoding (BPE). Specifically,\npre-tokenization causes the distribution of tokens in a corpus to heavily skew\ntowards common, full-length words. This skewed distribution limits the benefits\nof expanding to larger vocabularies, since the additional tokens appear with\nprogressively lower counts. To overcome this barrier, we propose BoundlessBPE,\na modified BPE algorithm that relaxes the pretoken boundary constraint. Our\napproach selectively merges two complete pretokens into a larger unit we term a\nsuperword. Superwords are not necessarily semantically cohesive. For example,\nthe pretokens \" of\" and \" the\" might be combined to form the superword \" of\nthe\". This merging strategy results in a substantially more uniform\ndistribution of tokens across a corpus than standard BPE, and compresses text\nmore effectively, with up to a 15% increase in bytes per token.\n", "link": "http://arxiv.org/abs/2504.00178v2", "date": "2025-10-02", "relevancy": 2.2428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4522}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundless%20Byte%20Pair%20Encoding%3A%20Breaking%20the%20Pre-tokenization%20Barrier&body=Title%3A%20Boundless%20Byte%20Pair%20Encoding%3A%20Breaking%20the%20Pre-tokenization%20Barrier%0AAuthor%3A%20Craig%20W.%20Schmidt%20and%20Varshini%20Reddy%20and%20Chris%20Tanner%20and%20Yuval%20Pinter%0AAbstract%3A%20%20%20Pre-tokenization%2C%20the%20initial%20step%20in%20many%20modern%20tokenization%20pipelines%2C%0Asegments%20text%20into%20smaller%20units%20called%20pretokens%2C%20typically%20splitting%20on%0Awhitespace%20and%20punctuation.%20While%20this%20process%20encourages%20having%20full%2C%0Aindividual%20words%20as%20tokens%2C%20it%20introduces%20a%20fundamental%20limitation%20in%20most%0Atokenization%20algorithms%20such%20as%20Byte%20Pair%20Encoding%20%28BPE%29.%20Specifically%2C%0Apre-tokenization%20causes%20the%20distribution%20of%20tokens%20in%20a%20corpus%20to%20heavily%20skew%0Atowards%20common%2C%20full-length%20words.%20This%20skewed%20distribution%20limits%20the%20benefits%0Aof%20expanding%20to%20larger%20vocabularies%2C%20since%20the%20additional%20tokens%20appear%20with%0Aprogressively%20lower%20counts.%20To%20overcome%20this%20barrier%2C%20we%20propose%20BoundlessBPE%2C%0Aa%20modified%20BPE%20algorithm%20that%20relaxes%20the%20pretoken%20boundary%20constraint.%20Our%0Aapproach%20selectively%20merges%20two%20complete%20pretokens%20into%20a%20larger%20unit%20we%20term%20a%0Asuperword.%20Superwords%20are%20not%20necessarily%20semantically%20cohesive.%20For%20example%2C%0Athe%20pretokens%20%22%20of%22%20and%20%22%20the%22%20might%20be%20combined%20to%20form%20the%20superword%20%22%20of%0Athe%22.%20This%20merging%20strategy%20results%20in%20a%20substantially%20more%20uniform%0Adistribution%20of%20tokens%20across%20a%20corpus%20than%20standard%20BPE%2C%20and%20compresses%20text%0Amore%20effectively%2C%20with%20up%20to%20a%2015%25%20increase%20in%20bytes%20per%20token.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundless%2520Byte%2520Pair%2520Encoding%253A%2520Breaking%2520the%2520Pre-tokenization%2520Barrier%26entry.906535625%3DCraig%2520W.%2520Schmidt%2520and%2520Varshini%2520Reddy%2520and%2520Chris%2520Tanner%2520and%2520Yuval%2520Pinter%26entry.1292438233%3D%2520%2520Pre-tokenization%252C%2520the%2520initial%2520step%2520in%2520many%2520modern%2520tokenization%2520pipelines%252C%250Asegments%2520text%2520into%2520smaller%2520units%2520called%2520pretokens%252C%2520typically%2520splitting%2520on%250Awhitespace%2520and%2520punctuation.%2520While%2520this%2520process%2520encourages%2520having%2520full%252C%250Aindividual%2520words%2520as%2520tokens%252C%2520it%2520introduces%2520a%2520fundamental%2520limitation%2520in%2520most%250Atokenization%2520algorithms%2520such%2520as%2520Byte%2520Pair%2520Encoding%2520%2528BPE%2529.%2520Specifically%252C%250Apre-tokenization%2520causes%2520the%2520distribution%2520of%2520tokens%2520in%2520a%2520corpus%2520to%2520heavily%2520skew%250Atowards%2520common%252C%2520full-length%2520words.%2520This%2520skewed%2520distribution%2520limits%2520the%2520benefits%250Aof%2520expanding%2520to%2520larger%2520vocabularies%252C%2520since%2520the%2520additional%2520tokens%2520appear%2520with%250Aprogressively%2520lower%2520counts.%2520To%2520overcome%2520this%2520barrier%252C%2520we%2520propose%2520BoundlessBPE%252C%250Aa%2520modified%2520BPE%2520algorithm%2520that%2520relaxes%2520the%2520pretoken%2520boundary%2520constraint.%2520Our%250Aapproach%2520selectively%2520merges%2520two%2520complete%2520pretokens%2520into%2520a%2520larger%2520unit%2520we%2520term%2520a%250Asuperword.%2520Superwords%2520are%2520not%2520necessarily%2520semantically%2520cohesive.%2520For%2520example%252C%250Athe%2520pretokens%2520%2522%2520of%2522%2520and%2520%2522%2520the%2522%2520might%2520be%2520combined%2520to%2520form%2520the%2520superword%2520%2522%2520of%250Athe%2522.%2520This%2520merging%2520strategy%2520results%2520in%2520a%2520substantially%2520more%2520uniform%250Adistribution%2520of%2520tokens%2520across%2520a%2520corpus%2520than%2520standard%2520BPE%252C%2520and%2520compresses%2520text%250Amore%2520effectively%252C%2520with%2520up%2520to%2520a%252015%2525%2520increase%2520in%2520bytes%2520per%2520token.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundless%20Byte%20Pair%20Encoding%3A%20Breaking%20the%20Pre-tokenization%20Barrier&entry.906535625=Craig%20W.%20Schmidt%20and%20Varshini%20Reddy%20and%20Chris%20Tanner%20and%20Yuval%20Pinter&entry.1292438233=%20%20Pre-tokenization%2C%20the%20initial%20step%20in%20many%20modern%20tokenization%20pipelines%2C%0Asegments%20text%20into%20smaller%20units%20called%20pretokens%2C%20typically%20splitting%20on%0Awhitespace%20and%20punctuation.%20While%20this%20process%20encourages%20having%20full%2C%0Aindividual%20words%20as%20tokens%2C%20it%20introduces%20a%20fundamental%20limitation%20in%20most%0Atokenization%20algorithms%20such%20as%20Byte%20Pair%20Encoding%20%28BPE%29.%20Specifically%2C%0Apre-tokenization%20causes%20the%20distribution%20of%20tokens%20in%20a%20corpus%20to%20heavily%20skew%0Atowards%20common%2C%20full-length%20words.%20This%20skewed%20distribution%20limits%20the%20benefits%0Aof%20expanding%20to%20larger%20vocabularies%2C%20since%20the%20additional%20tokens%20appear%20with%0Aprogressively%20lower%20counts.%20To%20overcome%20this%20barrier%2C%20we%20propose%20BoundlessBPE%2C%0Aa%20modified%20BPE%20algorithm%20that%20relaxes%20the%20pretoken%20boundary%20constraint.%20Our%0Aapproach%20selectively%20merges%20two%20complete%20pretokens%20into%20a%20larger%20unit%20we%20term%20a%0Asuperword.%20Superwords%20are%20not%20necessarily%20semantically%20cohesive.%20For%20example%2C%0Athe%20pretokens%20%22%20of%22%20and%20%22%20the%22%20might%20be%20combined%20to%20form%20the%20superword%20%22%20of%0Athe%22.%20This%20merging%20strategy%20results%20in%20a%20substantially%20more%20uniform%0Adistribution%20of%20tokens%20across%20a%20corpus%20than%20standard%20BPE%2C%20and%20compresses%20text%0Amore%20effectively%2C%20with%20up%20to%20a%2015%25%20increase%20in%20bytes%20per%20token.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00178v2&entry.124074799=Read"},
{"title": "VideoNSA: Native Sparse Attention Scales Video Understanding", "author": "Enxin Song and Wenhao Chai and Shusheng Yang and Ethan Armand and Xiaojun Shan and Haiyang Xu and Jianwen Xie and Zhuowen Tu", "abstract": "  Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.\n", "link": "http://arxiv.org/abs/2510.02295v1", "date": "2025-10-02", "relevancy": 2.2317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoNSA%3A%20Native%20Sparse%20Attention%20Scales%20Video%20Understanding&body=Title%3A%20VideoNSA%3A%20Native%20Sparse%20Attention%20Scales%20Video%20Understanding%0AAuthor%3A%20Enxin%20Song%20and%20Wenhao%20Chai%20and%20Shusheng%20Yang%20and%20Ethan%20Armand%20and%20Xiaojun%20Shan%20and%20Haiyang%20Xu%20and%20Jianwen%20Xie%20and%20Zhuowen%20Tu%0AAbstract%3A%20%20%20Video%20understanding%20in%20multimodal%20language%20models%20remains%20limited%20by%20context%0Alength%3A%20models%20often%20miss%20key%20transition%20frames%20and%20struggle%20to%20maintain%0Acoherence%20across%20long%20time%20scales.%20To%20address%20this%2C%20we%20adapt%20Native%20Sparse%0AAttention%20%28NSA%29%20to%20video-language%20models.%20Our%20method%2C%20VideoNSA%2C%20adapts%0AQwen2.5-VL%20through%20end-to-end%20training%20on%20a%20216K%20video%20instruction%20dataset.%20We%0Aemploy%20a%20hardware-aware%20hybrid%20approach%20to%20attention%2C%20preserving%20dense%0Aattention%20for%20text%2C%20while%20employing%20NSA%20for%20video.%20Compared%20to%0Atoken-compression%20and%20training-free%20sparse%20baselines%2C%20VideoNSA%20achieves%0Aimproved%20performance%20on%20long-video%20understanding%2C%20temporal%20reasoning%2C%20and%0Aspatial%20benchmarks.%20Further%20ablation%20analysis%20reveals%20four%20key%20findings%3A%20%281%29%0Areliable%20scaling%20to%20128K%20tokens%3B%20%282%29%20an%20optimal%20global-local%20attention%0Aallocation%20at%20a%20fixed%20budget%3B%20%283%29%20task-dependent%20branch%20usage%20patterns%3B%20and%20%284%29%0Athe%20learnable%20combined%20sparse%20attention%20help%20induce%20dynamic%20attention%20sinks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoNSA%253A%2520Native%2520Sparse%2520Attention%2520Scales%2520Video%2520Understanding%26entry.906535625%3DEnxin%2520Song%2520and%2520Wenhao%2520Chai%2520and%2520Shusheng%2520Yang%2520and%2520Ethan%2520Armand%2520and%2520Xiaojun%2520Shan%2520and%2520Haiyang%2520Xu%2520and%2520Jianwen%2520Xie%2520and%2520Zhuowen%2520Tu%26entry.1292438233%3D%2520%2520Video%2520understanding%2520in%2520multimodal%2520language%2520models%2520remains%2520limited%2520by%2520context%250Alength%253A%2520models%2520often%2520miss%2520key%2520transition%2520frames%2520and%2520struggle%2520to%2520maintain%250Acoherence%2520across%2520long%2520time%2520scales.%2520To%2520address%2520this%252C%2520we%2520adapt%2520Native%2520Sparse%250AAttention%2520%2528NSA%2529%2520to%2520video-language%2520models.%2520Our%2520method%252C%2520VideoNSA%252C%2520adapts%250AQwen2.5-VL%2520through%2520end-to-end%2520training%2520on%2520a%2520216K%2520video%2520instruction%2520dataset.%2520We%250Aemploy%2520a%2520hardware-aware%2520hybrid%2520approach%2520to%2520attention%252C%2520preserving%2520dense%250Aattention%2520for%2520text%252C%2520while%2520employing%2520NSA%2520for%2520video.%2520Compared%2520to%250Atoken-compression%2520and%2520training-free%2520sparse%2520baselines%252C%2520VideoNSA%2520achieves%250Aimproved%2520performance%2520on%2520long-video%2520understanding%252C%2520temporal%2520reasoning%252C%2520and%250Aspatial%2520benchmarks.%2520Further%2520ablation%2520analysis%2520reveals%2520four%2520key%2520findings%253A%2520%25281%2529%250Areliable%2520scaling%2520to%2520128K%2520tokens%253B%2520%25282%2529%2520an%2520optimal%2520global-local%2520attention%250Aallocation%2520at%2520a%2520fixed%2520budget%253B%2520%25283%2529%2520task-dependent%2520branch%2520usage%2520patterns%253B%2520and%2520%25284%2529%250Athe%2520learnable%2520combined%2520sparse%2520attention%2520help%2520induce%2520dynamic%2520attention%2520sinks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoNSA%3A%20Native%20Sparse%20Attention%20Scales%20Video%20Understanding&entry.906535625=Enxin%20Song%20and%20Wenhao%20Chai%20and%20Shusheng%20Yang%20and%20Ethan%20Armand%20and%20Xiaojun%20Shan%20and%20Haiyang%20Xu%20and%20Jianwen%20Xie%20and%20Zhuowen%20Tu&entry.1292438233=%20%20Video%20understanding%20in%20multimodal%20language%20models%20remains%20limited%20by%20context%0Alength%3A%20models%20often%20miss%20key%20transition%20frames%20and%20struggle%20to%20maintain%0Acoherence%20across%20long%20time%20scales.%20To%20address%20this%2C%20we%20adapt%20Native%20Sparse%0AAttention%20%28NSA%29%20to%20video-language%20models.%20Our%20method%2C%20VideoNSA%2C%20adapts%0AQwen2.5-VL%20through%20end-to-end%20training%20on%20a%20216K%20video%20instruction%20dataset.%20We%0Aemploy%20a%20hardware-aware%20hybrid%20approach%20to%20attention%2C%20preserving%20dense%0Aattention%20for%20text%2C%20while%20employing%20NSA%20for%20video.%20Compared%20to%0Atoken-compression%20and%20training-free%20sparse%20baselines%2C%20VideoNSA%20achieves%0Aimproved%20performance%20on%20long-video%20understanding%2C%20temporal%20reasoning%2C%20and%0Aspatial%20benchmarks.%20Further%20ablation%20analysis%20reveals%20four%20key%20findings%3A%20%281%29%0Areliable%20scaling%20to%20128K%20tokens%3B%20%282%29%20an%20optimal%20global-local%20attention%0Aallocation%20at%20a%20fixed%20budget%3B%20%283%29%20task-dependent%20branch%20usage%20patterns%3B%20and%20%284%29%0Athe%20learnable%20combined%20sparse%20attention%20help%20induce%20dynamic%20attention%20sinks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02295v1&entry.124074799=Read"},
{"title": "GenExam: A Multidisciplinary Text-to-Image Exam", "author": "Zhaokai Wang and Penghao Yin and Xiangyu Zhao and Changyao Tian and Yu Qiao and Wenhai Wang and Jifeng Dai and Gen Luo", "abstract": "  Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate\nunderstanding, reasoning, and generation, providing insights on the path to\ngeneral AGI. Our benchmark and evaluation code are released at\nhttps://github.com/OpenGVLab/GenExam.\n", "link": "http://arxiv.org/abs/2509.14232v2", "date": "2025-10-02", "relevancy": 2.2297, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5703}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5516}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenExam%3A%20A%20Multidisciplinary%20Text-to-Image%20Exam&body=Title%3A%20GenExam%3A%20A%20Multidisciplinary%20Text-to-Image%20Exam%0AAuthor%3A%20Zhaokai%20Wang%20and%20Penghao%20Yin%20and%20Xiangyu%20Zhao%20and%20Changyao%20Tian%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Gen%20Luo%0AAbstract%3A%20%20%20Exams%20are%20a%20fundamental%20test%20of%20expert-level%20intelligence%20and%20require%0Aintegrated%20understanding%2C%20reasoning%2C%20and%20generation.%20Existing%20exam-style%0Abenchmarks%20mainly%20focus%20on%20understanding%20and%20reasoning%20tasks%2C%20and%20current%0Ageneration%20benchmarks%20emphasize%20the%20illustration%20of%20world%20knowledge%20and%20visual%0Aconcepts%2C%20neglecting%20the%20evaluation%20of%20rigorous%20drawing%20exams.%20We%20introduce%0AGenExam%2C%20the%20first%20benchmark%20for%20multidisciplinary%20text-to-image%20exams%2C%0Afeaturing%201%2C000%20samples%20across%2010%20subjects%20with%20exam-style%20prompts%20organized%0Aunder%20a%20four-level%20taxonomy.%20Each%20problem%20is%20equipped%20with%20ground-truth%20images%0Aand%20fine-grained%20scoring%20points%20to%20enable%20a%20precise%20evaluation%20of%20semantic%0Acorrectness%20and%20visual%20plausibility.%20Experiments%20show%20that%20even%0Astate-of-the-art%20models%20such%20as%20GPT-Image-1%20and%20Gemini-2.5-Flash-Image%20achieve%0Aless%20than%2015%25%20strict%20scores%2C%20and%20most%20models%20yield%20almost%200%25%2C%20suggesting%20the%0Agreat%20challenge%20of%20our%20benchmark.%20By%20framing%20image%20generation%20as%20an%20exam%2C%0AGenExam%20offers%20a%20rigorous%20assessment%20of%20models%27%20ability%20to%20integrate%0Aunderstanding%2C%20reasoning%2C%20and%20generation%2C%20providing%20insights%20on%20the%20path%20to%0Ageneral%20AGI.%20Our%20benchmark%20and%20evaluation%20code%20are%20released%20at%0Ahttps%3A//github.com/OpenGVLab/GenExam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenExam%253A%2520A%2520Multidisciplinary%2520Text-to-Image%2520Exam%26entry.906535625%3DZhaokai%2520Wang%2520and%2520Penghao%2520Yin%2520and%2520Xiangyu%2520Zhao%2520and%2520Changyao%2520Tian%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%2520and%2520Jifeng%2520Dai%2520and%2520Gen%2520Luo%26entry.1292438233%3D%2520%2520Exams%2520are%2520a%2520fundamental%2520test%2520of%2520expert-level%2520intelligence%2520and%2520require%250Aintegrated%2520understanding%252C%2520reasoning%252C%2520and%2520generation.%2520Existing%2520exam-style%250Abenchmarks%2520mainly%2520focus%2520on%2520understanding%2520and%2520reasoning%2520tasks%252C%2520and%2520current%250Ageneration%2520benchmarks%2520emphasize%2520the%2520illustration%2520of%2520world%2520knowledge%2520and%2520visual%250Aconcepts%252C%2520neglecting%2520the%2520evaluation%2520of%2520rigorous%2520drawing%2520exams.%2520We%2520introduce%250AGenExam%252C%2520the%2520first%2520benchmark%2520for%2520multidisciplinary%2520text-to-image%2520exams%252C%250Afeaturing%25201%252C000%2520samples%2520across%252010%2520subjects%2520with%2520exam-style%2520prompts%2520organized%250Aunder%2520a%2520four-level%2520taxonomy.%2520Each%2520problem%2520is%2520equipped%2520with%2520ground-truth%2520images%250Aand%2520fine-grained%2520scoring%2520points%2520to%2520enable%2520a%2520precise%2520evaluation%2520of%2520semantic%250Acorrectness%2520and%2520visual%2520plausibility.%2520Experiments%2520show%2520that%2520even%250Astate-of-the-art%2520models%2520such%2520as%2520GPT-Image-1%2520and%2520Gemini-2.5-Flash-Image%2520achieve%250Aless%2520than%252015%2525%2520strict%2520scores%252C%2520and%2520most%2520models%2520yield%2520almost%25200%2525%252C%2520suggesting%2520the%250Agreat%2520challenge%2520of%2520our%2520benchmark.%2520By%2520framing%2520image%2520generation%2520as%2520an%2520exam%252C%250AGenExam%2520offers%2520a%2520rigorous%2520assessment%2520of%2520models%2527%2520ability%2520to%2520integrate%250Aunderstanding%252C%2520reasoning%252C%2520and%2520generation%252C%2520providing%2520insights%2520on%2520the%2520path%2520to%250Ageneral%2520AGI.%2520Our%2520benchmark%2520and%2520evaluation%2520code%2520are%2520released%2520at%250Ahttps%253A//github.com/OpenGVLab/GenExam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenExam%3A%20A%20Multidisciplinary%20Text-to-Image%20Exam&entry.906535625=Zhaokai%20Wang%20and%20Penghao%20Yin%20and%20Xiangyu%20Zhao%20and%20Changyao%20Tian%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Gen%20Luo&entry.1292438233=%20%20Exams%20are%20a%20fundamental%20test%20of%20expert-level%20intelligence%20and%20require%0Aintegrated%20understanding%2C%20reasoning%2C%20and%20generation.%20Existing%20exam-style%0Abenchmarks%20mainly%20focus%20on%20understanding%20and%20reasoning%20tasks%2C%20and%20current%0Ageneration%20benchmarks%20emphasize%20the%20illustration%20of%20world%20knowledge%20and%20visual%0Aconcepts%2C%20neglecting%20the%20evaluation%20of%20rigorous%20drawing%20exams.%20We%20introduce%0AGenExam%2C%20the%20first%20benchmark%20for%20multidisciplinary%20text-to-image%20exams%2C%0Afeaturing%201%2C000%20samples%20across%2010%20subjects%20with%20exam-style%20prompts%20organized%0Aunder%20a%20four-level%20taxonomy.%20Each%20problem%20is%20equipped%20with%20ground-truth%20images%0Aand%20fine-grained%20scoring%20points%20to%20enable%20a%20precise%20evaluation%20of%20semantic%0Acorrectness%20and%20visual%20plausibility.%20Experiments%20show%20that%20even%0Astate-of-the-art%20models%20such%20as%20GPT-Image-1%20and%20Gemini-2.5-Flash-Image%20achieve%0Aless%20than%2015%25%20strict%20scores%2C%20and%20most%20models%20yield%20almost%200%25%2C%20suggesting%20the%0Agreat%20challenge%20of%20our%20benchmark.%20By%20framing%20image%20generation%20as%20an%20exam%2C%0AGenExam%20offers%20a%20rigorous%20assessment%20of%20models%27%20ability%20to%20integrate%0Aunderstanding%2C%20reasoning%2C%20and%20generation%2C%20providing%20insights%20on%20the%20path%20to%0Ageneral%20AGI.%20Our%20benchmark%20and%20evaluation%20code%20are%20released%20at%0Ahttps%3A//github.com/OpenGVLab/GenExam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14232v2&entry.124074799=Read"},
{"title": "Segmentor-Guided Counterfactual Fine-Tuning for Locally Coherent and\n  Targeted Image Synthesis", "author": "Tian Xia and Matthew Sinclair and Andreas Schuh and Fabio De Sousa Ribeiro and Raghav Mehta and Rajat Rasal and Esther Puyol-Ant\u00f3n and Samuel Gerber and Kersten Petersen and Michiel Schaap and Ben Glocker", "abstract": "  Counterfactual image generation is a powerful tool for augmenting training\ndata, de-biasing datasets, and modeling disease. Current approaches rely on\nexternal classifiers or regressors to increase the effectiveness of\nsubject-level interventions (e.g., changing the patient's age). For\nstructure-specific interventions (e.g., changing the area of the left lung in a\nchest radiograph), we show that this is insufficient, and can result in\nundesirable global effects across the image domain. Previous work used\npixel-level label maps as guidance, requiring a user to provide hypothetical\nsegmentations which are tedious and difficult to obtain. We propose\nSegmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the\nsimplicity of intervening on scalar-valued, structure-specific variables while\nproducing locally coherent and effective counterfactuals. We demonstrate the\ncapability of generating realistic chest radiographs, and we show promising\nresults for modeling coronary artery disease. Code:\nhttps://github.com/biomedia-mira/seg-cft.\n", "link": "http://arxiv.org/abs/2509.24913v2", "date": "2025-10-02", "relevancy": 2.2079, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5681}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentor-Guided%20Counterfactual%20Fine-Tuning%20for%20Locally%20Coherent%20and%0A%20%20Targeted%20Image%20Synthesis&body=Title%3A%20Segmentor-Guided%20Counterfactual%20Fine-Tuning%20for%20Locally%20Coherent%20and%0A%20%20Targeted%20Image%20Synthesis%0AAuthor%3A%20Tian%20Xia%20and%20Matthew%20Sinclair%20and%20Andreas%20Schuh%20and%20Fabio%20De%20Sousa%20Ribeiro%20and%20Raghav%20Mehta%20and%20Rajat%20Rasal%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Samuel%20Gerber%20and%20Kersten%20Petersen%20and%20Michiel%20Schaap%20and%20Ben%20Glocker%0AAbstract%3A%20%20%20Counterfactual%20image%20generation%20is%20a%20powerful%20tool%20for%20augmenting%20training%0Adata%2C%20de-biasing%20datasets%2C%20and%20modeling%20disease.%20Current%20approaches%20rely%20on%0Aexternal%20classifiers%20or%20regressors%20to%20increase%20the%20effectiveness%20of%0Asubject-level%20interventions%20%28e.g.%2C%20changing%20the%20patient%27s%20age%29.%20For%0Astructure-specific%20interventions%20%28e.g.%2C%20changing%20the%20area%20of%20the%20left%20lung%20in%20a%0Achest%20radiograph%29%2C%20we%20show%20that%20this%20is%20insufficient%2C%20and%20can%20result%20in%0Aundesirable%20global%20effects%20across%20the%20image%20domain.%20Previous%20work%20used%0Apixel-level%20label%20maps%20as%20guidance%2C%20requiring%20a%20user%20to%20provide%20hypothetical%0Asegmentations%20which%20are%20tedious%20and%20difficult%20to%20obtain.%20We%20propose%0ASegmentor-guided%20Counterfactual%20Fine-Tuning%20%28Seg-CFT%29%2C%20which%20preserves%20the%0Asimplicity%20of%20intervening%20on%20scalar-valued%2C%20structure-specific%20variables%20while%0Aproducing%20locally%20coherent%20and%20effective%20counterfactuals.%20We%20demonstrate%20the%0Acapability%20of%20generating%20realistic%20chest%20radiographs%2C%20and%20we%20show%20promising%0Aresults%20for%20modeling%20coronary%20artery%20disease.%20Code%3A%0Ahttps%3A//github.com/biomedia-mira/seg-cft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentor-Guided%2520Counterfactual%2520Fine-Tuning%2520for%2520Locally%2520Coherent%2520and%250A%2520%2520Targeted%2520Image%2520Synthesis%26entry.906535625%3DTian%2520Xia%2520and%2520Matthew%2520Sinclair%2520and%2520Andreas%2520Schuh%2520and%2520Fabio%2520De%2520Sousa%2520Ribeiro%2520and%2520Raghav%2520Mehta%2520and%2520Rajat%2520Rasal%2520and%2520Esther%2520Puyol-Ant%25C3%25B3n%2520and%2520Samuel%2520Gerber%2520and%2520Kersten%2520Petersen%2520and%2520Michiel%2520Schaap%2520and%2520Ben%2520Glocker%26entry.1292438233%3D%2520%2520Counterfactual%2520image%2520generation%2520is%2520a%2520powerful%2520tool%2520for%2520augmenting%2520training%250Adata%252C%2520de-biasing%2520datasets%252C%2520and%2520modeling%2520disease.%2520Current%2520approaches%2520rely%2520on%250Aexternal%2520classifiers%2520or%2520regressors%2520to%2520increase%2520the%2520effectiveness%2520of%250Asubject-level%2520interventions%2520%2528e.g.%252C%2520changing%2520the%2520patient%2527s%2520age%2529.%2520For%250Astructure-specific%2520interventions%2520%2528e.g.%252C%2520changing%2520the%2520area%2520of%2520the%2520left%2520lung%2520in%2520a%250Achest%2520radiograph%2529%252C%2520we%2520show%2520that%2520this%2520is%2520insufficient%252C%2520and%2520can%2520result%2520in%250Aundesirable%2520global%2520effects%2520across%2520the%2520image%2520domain.%2520Previous%2520work%2520used%250Apixel-level%2520label%2520maps%2520as%2520guidance%252C%2520requiring%2520a%2520user%2520to%2520provide%2520hypothetical%250Asegmentations%2520which%2520are%2520tedious%2520and%2520difficult%2520to%2520obtain.%2520We%2520propose%250ASegmentor-guided%2520Counterfactual%2520Fine-Tuning%2520%2528Seg-CFT%2529%252C%2520which%2520preserves%2520the%250Asimplicity%2520of%2520intervening%2520on%2520scalar-valued%252C%2520structure-specific%2520variables%2520while%250Aproducing%2520locally%2520coherent%2520and%2520effective%2520counterfactuals.%2520We%2520demonstrate%2520the%250Acapability%2520of%2520generating%2520realistic%2520chest%2520radiographs%252C%2520and%2520we%2520show%2520promising%250Aresults%2520for%2520modeling%2520coronary%2520artery%2520disease.%2520Code%253A%250Ahttps%253A//github.com/biomedia-mira/seg-cft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentor-Guided%20Counterfactual%20Fine-Tuning%20for%20Locally%20Coherent%20and%0A%20%20Targeted%20Image%20Synthesis&entry.906535625=Tian%20Xia%20and%20Matthew%20Sinclair%20and%20Andreas%20Schuh%20and%20Fabio%20De%20Sousa%20Ribeiro%20and%20Raghav%20Mehta%20and%20Rajat%20Rasal%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Samuel%20Gerber%20and%20Kersten%20Petersen%20and%20Michiel%20Schaap%20and%20Ben%20Glocker&entry.1292438233=%20%20Counterfactual%20image%20generation%20is%20a%20powerful%20tool%20for%20augmenting%20training%0Adata%2C%20de-biasing%20datasets%2C%20and%20modeling%20disease.%20Current%20approaches%20rely%20on%0Aexternal%20classifiers%20or%20regressors%20to%20increase%20the%20effectiveness%20of%0Asubject-level%20interventions%20%28e.g.%2C%20changing%20the%20patient%27s%20age%29.%20For%0Astructure-specific%20interventions%20%28e.g.%2C%20changing%20the%20area%20of%20the%20left%20lung%20in%20a%0Achest%20radiograph%29%2C%20we%20show%20that%20this%20is%20insufficient%2C%20and%20can%20result%20in%0Aundesirable%20global%20effects%20across%20the%20image%20domain.%20Previous%20work%20used%0Apixel-level%20label%20maps%20as%20guidance%2C%20requiring%20a%20user%20to%20provide%20hypothetical%0Asegmentations%20which%20are%20tedious%20and%20difficult%20to%20obtain.%20We%20propose%0ASegmentor-guided%20Counterfactual%20Fine-Tuning%20%28Seg-CFT%29%2C%20which%20preserves%20the%0Asimplicity%20of%20intervening%20on%20scalar-valued%2C%20structure-specific%20variables%20while%0Aproducing%20locally%20coherent%20and%20effective%20counterfactuals.%20We%20demonstrate%20the%0Acapability%20of%20generating%20realistic%20chest%20radiographs%2C%20and%20we%20show%20promising%0Aresults%20for%20modeling%20coronary%20artery%20disease.%20Code%3A%0Ahttps%3A//github.com/biomedia-mira/seg-cft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24913v2&entry.124074799=Read"},
{"title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models", "author": "Phuc Minh Nguyen and Chinh D. La and Duy M. H. Nguyen and Nitesh V. Chawla and Binh T. Nguyen and Khoa D. Doan", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.\n", "link": "http://arxiv.org/abs/2510.02230v1", "date": "2025-10-02", "relevancy": 2.1603, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Reasoning%20Boundary%20Paradox%3A%20How%20Reinforcement%20Learning%20Constrains%0A%20%20Language%20Models&body=Title%3A%20The%20Reasoning%20Boundary%20Paradox%3A%20How%20Reinforcement%20Learning%20Constrains%0A%20%20Language%20Models%0AAuthor%3A%20Phuc%20Minh%20Nguyen%20and%20Chinh%20D.%20La%20and%20Duy%20M.%20H.%20Nguyen%20and%20Nitesh%20V.%20Chawla%20and%20Binh%20T.%20Nguyen%20and%20Khoa%20D.%20Doan%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%20key%0Amethod%20for%20improving%20Large%20Language%20Models%27%20reasoning%20capabilities%2C%20yet%20recent%0Aevidence%20suggests%20it%20may%20paradoxically%20shrink%20the%20reasoning%20boundary%20rather%0Athan%20expand%20it.%20This%20paper%20investigates%20the%20shrinkage%20issue%20of%20RLVR%20by%0Aanalyzing%20its%20learning%20dynamics%20and%20reveals%20two%20critical%20phenomena%20that%20explain%0Athis%20failure.%20First%2C%20we%20expose%20negative%20interference%20in%20RLVR%2C%20where%20learning%20to%0Asolve%20certain%20training%20problems%20actively%20reduces%20the%20likelihood%20of%20correct%0Asolutions%20for%20others%2C%20leading%20to%20the%20decline%20of%20Pass%40%24k%24%20performance%2C%20or%20the%0Aprobability%20of%20generating%20a%20correct%20solution%20within%20%24k%24%20attempts.%20Second%2C%20we%0Auncover%20the%20winner-take-all%20phenomenon%3A%20RLVR%20disproportionately%20reinforces%0Aproblems%20with%20high%20likelihood%2C%20correct%20solutions%2C%20under%20the%20base%20model%2C%20while%0Asuppressing%20other%20initially%20low-likelihood%20ones.%20Through%20extensive%20theoretical%0Aand%20empirical%20analysis%20on%20multiple%20mathematical%20reasoning%20benchmarks%2C%20we%20show%0Athat%20this%20effect%20arises%20from%20the%20inherent%20on-policy%20sampling%20in%20standard%20RL%0Aobjectives%2C%20causing%20the%20model%20to%20converge%20toward%20narrow%20solution%20strategies.%0ABased%20on%20these%20insights%2C%20we%20propose%20a%20simple%20yet%20effective%20data%20curation%0Aalgorithm%20that%20focuses%20RLVR%20learning%20on%20low-likelihood%20problems%2C%20achieving%0Anotable%20improvement%20in%20Pass%40%24k%24%20performance.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mail-research/SELF-llm-interference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Reasoning%2520Boundary%2520Paradox%253A%2520How%2520Reinforcement%2520Learning%2520Constrains%250A%2520%2520Language%2520Models%26entry.906535625%3DPhuc%2520Minh%2520Nguyen%2520and%2520Chinh%2520D.%2520La%2520and%2520Duy%2520M.%2520H.%2520Nguyen%2520and%2520Nitesh%2520V.%2520Chawla%2520and%2520Binh%2520T.%2520Nguyen%2520and%2520Khoa%2520D.%2520Doan%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%2520key%250Amethod%2520for%2520improving%2520Large%2520Language%2520Models%2527%2520reasoning%2520capabilities%252C%2520yet%2520recent%250Aevidence%2520suggests%2520it%2520may%2520paradoxically%2520shrink%2520the%2520reasoning%2520boundary%2520rather%250Athan%2520expand%2520it.%2520This%2520paper%2520investigates%2520the%2520shrinkage%2520issue%2520of%2520RLVR%2520by%250Aanalyzing%2520its%2520learning%2520dynamics%2520and%2520reveals%2520two%2520critical%2520phenomena%2520that%2520explain%250Athis%2520failure.%2520First%252C%2520we%2520expose%2520negative%2520interference%2520in%2520RLVR%252C%2520where%2520learning%2520to%250Asolve%2520certain%2520training%2520problems%2520actively%2520reduces%2520the%2520likelihood%2520of%2520correct%250Asolutions%2520for%2520others%252C%2520leading%2520to%2520the%2520decline%2520of%2520Pass%2540%2524k%2524%2520performance%252C%2520or%2520the%250Aprobability%2520of%2520generating%2520a%2520correct%2520solution%2520within%2520%2524k%2524%2520attempts.%2520Second%252C%2520we%250Auncover%2520the%2520winner-take-all%2520phenomenon%253A%2520RLVR%2520disproportionately%2520reinforces%250Aproblems%2520with%2520high%2520likelihood%252C%2520correct%2520solutions%252C%2520under%2520the%2520base%2520model%252C%2520while%250Asuppressing%2520other%2520initially%2520low-likelihood%2520ones.%2520Through%2520extensive%2520theoretical%250Aand%2520empirical%2520analysis%2520on%2520multiple%2520mathematical%2520reasoning%2520benchmarks%252C%2520we%2520show%250Athat%2520this%2520effect%2520arises%2520from%2520the%2520inherent%2520on-policy%2520sampling%2520in%2520standard%2520RL%250Aobjectives%252C%2520causing%2520the%2520model%2520to%2520converge%2520toward%2520narrow%2520solution%2520strategies.%250ABased%2520on%2520these%2520insights%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520data%2520curation%250Aalgorithm%2520that%2520focuses%2520RLVR%2520learning%2520on%2520low-likelihood%2520problems%252C%2520achieving%250Anotable%2520improvement%2520in%2520Pass%2540%2524k%2524%2520performance.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mail-research/SELF-llm-interference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Reasoning%20Boundary%20Paradox%3A%20How%20Reinforcement%20Learning%20Constrains%0A%20%20Language%20Models&entry.906535625=Phuc%20Minh%20Nguyen%20and%20Chinh%20D.%20La%20and%20Duy%20M.%20H.%20Nguyen%20and%20Nitesh%20V.%20Chawla%20and%20Binh%20T.%20Nguyen%20and%20Khoa%20D.%20Doan&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%20key%0Amethod%20for%20improving%20Large%20Language%20Models%27%20reasoning%20capabilities%2C%20yet%20recent%0Aevidence%20suggests%20it%20may%20paradoxically%20shrink%20the%20reasoning%20boundary%20rather%0Athan%20expand%20it.%20This%20paper%20investigates%20the%20shrinkage%20issue%20of%20RLVR%20by%0Aanalyzing%20its%20learning%20dynamics%20and%20reveals%20two%20critical%20phenomena%20that%20explain%0Athis%20failure.%20First%2C%20we%20expose%20negative%20interference%20in%20RLVR%2C%20where%20learning%20to%0Asolve%20certain%20training%20problems%20actively%20reduces%20the%20likelihood%20of%20correct%0Asolutions%20for%20others%2C%20leading%20to%20the%20decline%20of%20Pass%40%24k%24%20performance%2C%20or%20the%0Aprobability%20of%20generating%20a%20correct%20solution%20within%20%24k%24%20attempts.%20Second%2C%20we%0Auncover%20the%20winner-take-all%20phenomenon%3A%20RLVR%20disproportionately%20reinforces%0Aproblems%20with%20high%20likelihood%2C%20correct%20solutions%2C%20under%20the%20base%20model%2C%20while%0Asuppressing%20other%20initially%20low-likelihood%20ones.%20Through%20extensive%20theoretical%0Aand%20empirical%20analysis%20on%20multiple%20mathematical%20reasoning%20benchmarks%2C%20we%20show%0Athat%20this%20effect%20arises%20from%20the%20inherent%20on-policy%20sampling%20in%20standard%20RL%0Aobjectives%2C%20causing%20the%20model%20to%20converge%20toward%20narrow%20solution%20strategies.%0ABased%20on%20these%20insights%2C%20we%20propose%20a%20simple%20yet%20effective%20data%20curation%0Aalgorithm%20that%20focuses%20RLVR%20learning%20on%20low-likelihood%20problems%2C%20achieving%0Anotable%20improvement%20in%20Pass%40%24k%24%20performance.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mail-research/SELF-llm-interference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02230v1&entry.124074799=Read"},
{"title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems", "author": "Yuxiao Qu and Anikait Singh and Yoonho Lee and Amrith Setlur and Ruslan Salakhutdinov and Chelsea Finn and Aviral Kumar", "abstract": "  Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.\n", "link": "http://arxiv.org/abs/2510.02263v1", "date": "2025-10-02", "relevancy": 2.1531, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5755}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLAD%3A%20Training%20LLMs%20to%20Discover%20Abstractions%20for%20Solving%20Reasoning%0A%20%20Problems&body=Title%3A%20RLAD%3A%20Training%20LLMs%20to%20Discover%20Abstractions%20for%20Solving%20Reasoning%0A%20%20Problems%0AAuthor%3A%20Yuxiao%20Qu%20and%20Anikait%20Singh%20and%20Yoonho%20Lee%20and%20Amrith%20Setlur%20and%20Ruslan%20Salakhutdinov%20and%20Chelsea%20Finn%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20Reasoning%20requires%20going%20beyond%20pattern%20matching%20or%20memorization%20of%20solutions%0Ato%20identify%20and%20implement%20%22algorithmic%20procedures%22%20that%20can%20be%20used%20to%20deduce%0Aanswers%20to%20hard%20problems.%20Doing%20so%20requires%20realizing%20the%20most%20relevant%0Aprimitives%2C%20intermediate%20results%2C%20or%20shared%20procedures%2C%20and%20building%20upon%20them.%0AWhile%20RL%20post-training%20on%20long%20chains%20of%20thought%20ultimately%20aims%20to%20uncover%0Athis%20kind%20of%20algorithmic%20behavior%2C%20most%20reasoning%20traces%20learned%20by%20large%0Amodels%20fail%20to%20consistently%20capture%20or%20reuse%20procedures%2C%20instead%20drifting%20into%0Averbose%20and%20degenerate%20exploration.%20To%20address%20more%20effective%20reasoning%2C%20we%0Aintroduce%20reasoning%20abstractions%3A%20concise%20natural%20language%20descriptions%20of%0Aprocedural%20and%20factual%20knowledge%20that%20guide%20the%20model%20toward%20learning%0Asuccessful%20reasoning.%20We%20train%20models%20to%20be%20capable%20of%20proposing%20multiple%0Aabstractions%20given%20a%20problem%2C%20followed%20by%20RL%20that%20incentivizes%20building%20a%0Asolution%20while%20using%20the%20information%20provided%20by%20these%20abstractions.%20This%0Aresults%20in%20a%20two-player%20RL%20training%20paradigm%2C%20abbreviated%20as%20RLAD%2C%20that%20jointly%0Atrains%20an%20abstraction%20generator%20and%20a%20solution%20generator.%20This%20setup%0Aeffectively%20enables%20structured%20exploration%2C%20decouples%20learning%20signals%20of%0Aabstraction%20proposal%20and%20solution%20generation%2C%20and%20improves%20generalization%20to%0Aharder%20problems.%20We%20also%20show%20that%20allocating%20more%20test-time%20compute%20to%0Agenerating%20abstractions%20is%20more%20beneficial%20for%20performance%20than%20generating%20more%0Asolutions%20at%20large%20test%20budgets%2C%20illustrating%20the%20role%20of%20abstractions%20in%0Aguiding%20meaningful%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLAD%253A%2520Training%2520LLMs%2520to%2520Discover%2520Abstractions%2520for%2520Solving%2520Reasoning%250A%2520%2520Problems%26entry.906535625%3DYuxiao%2520Qu%2520and%2520Anikait%2520Singh%2520and%2520Yoonho%2520Lee%2520and%2520Amrith%2520Setlur%2520and%2520Ruslan%2520Salakhutdinov%2520and%2520Chelsea%2520Finn%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520Reasoning%2520requires%2520going%2520beyond%2520pattern%2520matching%2520or%2520memorization%2520of%2520solutions%250Ato%2520identify%2520and%2520implement%2520%2522algorithmic%2520procedures%2522%2520that%2520can%2520be%2520used%2520to%2520deduce%250Aanswers%2520to%2520hard%2520problems.%2520Doing%2520so%2520requires%2520realizing%2520the%2520most%2520relevant%250Aprimitives%252C%2520intermediate%2520results%252C%2520or%2520shared%2520procedures%252C%2520and%2520building%2520upon%2520them.%250AWhile%2520RL%2520post-training%2520on%2520long%2520chains%2520of%2520thought%2520ultimately%2520aims%2520to%2520uncover%250Athis%2520kind%2520of%2520algorithmic%2520behavior%252C%2520most%2520reasoning%2520traces%2520learned%2520by%2520large%250Amodels%2520fail%2520to%2520consistently%2520capture%2520or%2520reuse%2520procedures%252C%2520instead%2520drifting%2520into%250Averbose%2520and%2520degenerate%2520exploration.%2520To%2520address%2520more%2520effective%2520reasoning%252C%2520we%250Aintroduce%2520reasoning%2520abstractions%253A%2520concise%2520natural%2520language%2520descriptions%2520of%250Aprocedural%2520and%2520factual%2520knowledge%2520that%2520guide%2520the%2520model%2520toward%2520learning%250Asuccessful%2520reasoning.%2520We%2520train%2520models%2520to%2520be%2520capable%2520of%2520proposing%2520multiple%250Aabstractions%2520given%2520a%2520problem%252C%2520followed%2520by%2520RL%2520that%2520incentivizes%2520building%2520a%250Asolution%2520while%2520using%2520the%2520information%2520provided%2520by%2520these%2520abstractions.%2520This%250Aresults%2520in%2520a%2520two-player%2520RL%2520training%2520paradigm%252C%2520abbreviated%2520as%2520RLAD%252C%2520that%2520jointly%250Atrains%2520an%2520abstraction%2520generator%2520and%2520a%2520solution%2520generator.%2520This%2520setup%250Aeffectively%2520enables%2520structured%2520exploration%252C%2520decouples%2520learning%2520signals%2520of%250Aabstraction%2520proposal%2520and%2520solution%2520generation%252C%2520and%2520improves%2520generalization%2520to%250Aharder%2520problems.%2520We%2520also%2520show%2520that%2520allocating%2520more%2520test-time%2520compute%2520to%250Agenerating%2520abstractions%2520is%2520more%2520beneficial%2520for%2520performance%2520than%2520generating%2520more%250Asolutions%2520at%2520large%2520test%2520budgets%252C%2520illustrating%2520the%2520role%2520of%2520abstractions%2520in%250Aguiding%2520meaningful%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLAD%3A%20Training%20LLMs%20to%20Discover%20Abstractions%20for%20Solving%20Reasoning%0A%20%20Problems&entry.906535625=Yuxiao%20Qu%20and%20Anikait%20Singh%20and%20Yoonho%20Lee%20and%20Amrith%20Setlur%20and%20Ruslan%20Salakhutdinov%20and%20Chelsea%20Finn%20and%20Aviral%20Kumar&entry.1292438233=%20%20Reasoning%20requires%20going%20beyond%20pattern%20matching%20or%20memorization%20of%20solutions%0Ato%20identify%20and%20implement%20%22algorithmic%20procedures%22%20that%20can%20be%20used%20to%20deduce%0Aanswers%20to%20hard%20problems.%20Doing%20so%20requires%20realizing%20the%20most%20relevant%0Aprimitives%2C%20intermediate%20results%2C%20or%20shared%20procedures%2C%20and%20building%20upon%20them.%0AWhile%20RL%20post-training%20on%20long%20chains%20of%20thought%20ultimately%20aims%20to%20uncover%0Athis%20kind%20of%20algorithmic%20behavior%2C%20most%20reasoning%20traces%20learned%20by%20large%0Amodels%20fail%20to%20consistently%20capture%20or%20reuse%20procedures%2C%20instead%20drifting%20into%0Averbose%20and%20degenerate%20exploration.%20To%20address%20more%20effective%20reasoning%2C%20we%0Aintroduce%20reasoning%20abstractions%3A%20concise%20natural%20language%20descriptions%20of%0Aprocedural%20and%20factual%20knowledge%20that%20guide%20the%20model%20toward%20learning%0Asuccessful%20reasoning.%20We%20train%20models%20to%20be%20capable%20of%20proposing%20multiple%0Aabstractions%20given%20a%20problem%2C%20followed%20by%20RL%20that%20incentivizes%20building%20a%0Asolution%20while%20using%20the%20information%20provided%20by%20these%20abstractions.%20This%0Aresults%20in%20a%20two-player%20RL%20training%20paradigm%2C%20abbreviated%20as%20RLAD%2C%20that%20jointly%0Atrains%20an%20abstraction%20generator%20and%20a%20solution%20generator.%20This%20setup%0Aeffectively%20enables%20structured%20exploration%2C%20decouples%20learning%20signals%20of%0Aabstraction%20proposal%20and%20solution%20generation%2C%20and%20improves%20generalization%20to%0Aharder%20problems.%20We%20also%20show%20that%20allocating%20more%20test-time%20compute%20to%0Agenerating%20abstractions%20is%20more%20beneficial%20for%20performance%20than%20generating%20more%0Asolutions%20at%20large%20test%20budgets%2C%20illustrating%20the%20role%20of%20abstractions%20in%0Aguiding%20meaningful%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02263v1&entry.124074799=Read"},
{"title": "Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition:\n  A Machine Learning Approach for Small-Scale Farming Applications", "author": "Emmanuel Nsengiyumvaa and Leonard Niyitegekaa and Eric Umuhoza", "abstract": "  Accurate livestock identification is a cornerstone of modern farming: it\nsupports health monitoring, breeding programs, and productivity tracking.\nHowever, common pig identification methods, such as ear tags and microchips,\nare often unreliable, costly, target pure breeds, and thus impractical for\nsmall-scale farmers. To address this gap, we propose a noninvasive biometric\nidentification approach that leverages uniqueness of the auricular vein\npatterns. To this end, we have collected 800 ear images from 20 mixed-breed\npigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a\nstandard smartphone and simple back lighting. A multistage computer vision\npipeline was developed to enhance vein visibility, extract structural and\nspatial features, and generate biometric signatures. These features were then\nclassified using machine learning models. Support Vector Machines (SVM)\nachieved the highest accuracy: correctly identifying pigs with 98.12% precision\nacross mixed-breed populations. The entire process from image processing to\nclassification was completed in an average of 8.3 seconds, demonstrating\nfeasibility for real-time farm deployment. We believe that by replacing fragile\nphysical identifiers with permanent biological markers, this system provides\nfarmers with a cost-effective and stress-free method of animal identification.\nMore broadly, the findings confirm the practicality of auricular vein\nbiometrics for digitizing livestock management, reinforcing its potential to\nextend the benefits of precision farming to resource-constrained agricultural\ncommunities.\n", "link": "http://arxiv.org/abs/2510.02197v1", "date": "2025-10-02", "relevancy": 2.152, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4328}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4319}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Breed%20Pig%20Identification%20Using%20Auricular%20Vein%20Pattern%20Recognition%3A%0A%20%20A%20Machine%20Learning%20Approach%20for%20Small-Scale%20Farming%20Applications&body=Title%3A%20Cross-Breed%20Pig%20Identification%20Using%20Auricular%20Vein%20Pattern%20Recognition%3A%0A%20%20A%20Machine%20Learning%20Approach%20for%20Small-Scale%20Farming%20Applications%0AAuthor%3A%20Emmanuel%20Nsengiyumvaa%20and%20Leonard%20Niyitegekaa%20and%20Eric%20Umuhoza%0AAbstract%3A%20%20%20Accurate%20livestock%20identification%20is%20a%20cornerstone%20of%20modern%20farming%3A%20it%0Asupports%20health%20monitoring%2C%20breeding%20programs%2C%20and%20productivity%20tracking.%0AHowever%2C%20common%20pig%20identification%20methods%2C%20such%20as%20ear%20tags%20and%20microchips%2C%0Aare%20often%20unreliable%2C%20costly%2C%20target%20pure%20breeds%2C%20and%20thus%20impractical%20for%0Asmall-scale%20farmers.%20To%20address%20this%20gap%2C%20we%20propose%20a%20noninvasive%20biometric%0Aidentification%20approach%20that%20leverages%20uniqueness%20of%20the%20auricular%20vein%0Apatterns.%20To%20this%20end%2C%20we%20have%20collected%20800%20ear%20images%20from%2020%20mixed-breed%0Apigs%20%28Landrace%20cross%20Pietrain%20and%20Duroc%20cross%20Pietrain%29%2C%20captured%20using%20a%0Astandard%20smartphone%20and%20simple%20back%20lighting.%20A%20multistage%20computer%20vision%0Apipeline%20was%20developed%20to%20enhance%20vein%20visibility%2C%20extract%20structural%20and%0Aspatial%20features%2C%20and%20generate%20biometric%20signatures.%20These%20features%20were%20then%0Aclassified%20using%20machine%20learning%20models.%20Support%20Vector%20Machines%20%28SVM%29%0Aachieved%20the%20highest%20accuracy%3A%20correctly%20identifying%20pigs%20with%2098.12%25%20precision%0Aacross%20mixed-breed%20populations.%20The%20entire%20process%20from%20image%20processing%20to%0Aclassification%20was%20completed%20in%20an%20average%20of%208.3%20seconds%2C%20demonstrating%0Afeasibility%20for%20real-time%20farm%20deployment.%20We%20believe%20that%20by%20replacing%20fragile%0Aphysical%20identifiers%20with%20permanent%20biological%20markers%2C%20this%20system%20provides%0Afarmers%20with%20a%20cost-effective%20and%20stress-free%20method%20of%20animal%20identification.%0AMore%20broadly%2C%20the%20findings%20confirm%20the%20practicality%20of%20auricular%20vein%0Abiometrics%20for%20digitizing%20livestock%20management%2C%20reinforcing%20its%20potential%20to%0Aextend%20the%20benefits%20of%20precision%20farming%20to%20resource-constrained%20agricultural%0Acommunities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Breed%2520Pig%2520Identification%2520Using%2520Auricular%2520Vein%2520Pattern%2520Recognition%253A%250A%2520%2520A%2520Machine%2520Learning%2520Approach%2520for%2520Small-Scale%2520Farming%2520Applications%26entry.906535625%3DEmmanuel%2520Nsengiyumvaa%2520and%2520Leonard%2520Niyitegekaa%2520and%2520Eric%2520Umuhoza%26entry.1292438233%3D%2520%2520Accurate%2520livestock%2520identification%2520is%2520a%2520cornerstone%2520of%2520modern%2520farming%253A%2520it%250Asupports%2520health%2520monitoring%252C%2520breeding%2520programs%252C%2520and%2520productivity%2520tracking.%250AHowever%252C%2520common%2520pig%2520identification%2520methods%252C%2520such%2520as%2520ear%2520tags%2520and%2520microchips%252C%250Aare%2520often%2520unreliable%252C%2520costly%252C%2520target%2520pure%2520breeds%252C%2520and%2520thus%2520impractical%2520for%250Asmall-scale%2520farmers.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520noninvasive%2520biometric%250Aidentification%2520approach%2520that%2520leverages%2520uniqueness%2520of%2520the%2520auricular%2520vein%250Apatterns.%2520To%2520this%2520end%252C%2520we%2520have%2520collected%2520800%2520ear%2520images%2520from%252020%2520mixed-breed%250Apigs%2520%2528Landrace%2520cross%2520Pietrain%2520and%2520Duroc%2520cross%2520Pietrain%2529%252C%2520captured%2520using%2520a%250Astandard%2520smartphone%2520and%2520simple%2520back%2520lighting.%2520A%2520multistage%2520computer%2520vision%250Apipeline%2520was%2520developed%2520to%2520enhance%2520vein%2520visibility%252C%2520extract%2520structural%2520and%250Aspatial%2520features%252C%2520and%2520generate%2520biometric%2520signatures.%2520These%2520features%2520were%2520then%250Aclassified%2520using%2520machine%2520learning%2520models.%2520Support%2520Vector%2520Machines%2520%2528SVM%2529%250Aachieved%2520the%2520highest%2520accuracy%253A%2520correctly%2520identifying%2520pigs%2520with%252098.12%2525%2520precision%250Aacross%2520mixed-breed%2520populations.%2520The%2520entire%2520process%2520from%2520image%2520processing%2520to%250Aclassification%2520was%2520completed%2520in%2520an%2520average%2520of%25208.3%2520seconds%252C%2520demonstrating%250Afeasibility%2520for%2520real-time%2520farm%2520deployment.%2520We%2520believe%2520that%2520by%2520replacing%2520fragile%250Aphysical%2520identifiers%2520with%2520permanent%2520biological%2520markers%252C%2520this%2520system%2520provides%250Afarmers%2520with%2520a%2520cost-effective%2520and%2520stress-free%2520method%2520of%2520animal%2520identification.%250AMore%2520broadly%252C%2520the%2520findings%2520confirm%2520the%2520practicality%2520of%2520auricular%2520vein%250Abiometrics%2520for%2520digitizing%2520livestock%2520management%252C%2520reinforcing%2520its%2520potential%2520to%250Aextend%2520the%2520benefits%2520of%2520precision%2520farming%2520to%2520resource-constrained%2520agricultural%250Acommunities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Breed%20Pig%20Identification%20Using%20Auricular%20Vein%20Pattern%20Recognition%3A%0A%20%20A%20Machine%20Learning%20Approach%20for%20Small-Scale%20Farming%20Applications&entry.906535625=Emmanuel%20Nsengiyumvaa%20and%20Leonard%20Niyitegekaa%20and%20Eric%20Umuhoza&entry.1292438233=%20%20Accurate%20livestock%20identification%20is%20a%20cornerstone%20of%20modern%20farming%3A%20it%0Asupports%20health%20monitoring%2C%20breeding%20programs%2C%20and%20productivity%20tracking.%0AHowever%2C%20common%20pig%20identification%20methods%2C%20such%20as%20ear%20tags%20and%20microchips%2C%0Aare%20often%20unreliable%2C%20costly%2C%20target%20pure%20breeds%2C%20and%20thus%20impractical%20for%0Asmall-scale%20farmers.%20To%20address%20this%20gap%2C%20we%20propose%20a%20noninvasive%20biometric%0Aidentification%20approach%20that%20leverages%20uniqueness%20of%20the%20auricular%20vein%0Apatterns.%20To%20this%20end%2C%20we%20have%20collected%20800%20ear%20images%20from%2020%20mixed-breed%0Apigs%20%28Landrace%20cross%20Pietrain%20and%20Duroc%20cross%20Pietrain%29%2C%20captured%20using%20a%0Astandard%20smartphone%20and%20simple%20back%20lighting.%20A%20multistage%20computer%20vision%0Apipeline%20was%20developed%20to%20enhance%20vein%20visibility%2C%20extract%20structural%20and%0Aspatial%20features%2C%20and%20generate%20biometric%20signatures.%20These%20features%20were%20then%0Aclassified%20using%20machine%20learning%20models.%20Support%20Vector%20Machines%20%28SVM%29%0Aachieved%20the%20highest%20accuracy%3A%20correctly%20identifying%20pigs%20with%2098.12%25%20precision%0Aacross%20mixed-breed%20populations.%20The%20entire%20process%20from%20image%20processing%20to%0Aclassification%20was%20completed%20in%20an%20average%20of%208.3%20seconds%2C%20demonstrating%0Afeasibility%20for%20real-time%20farm%20deployment.%20We%20believe%20that%20by%20replacing%20fragile%0Aphysical%20identifiers%20with%20permanent%20biological%20markers%2C%20this%20system%20provides%0Afarmers%20with%20a%20cost-effective%20and%20stress-free%20method%20of%20animal%20identification.%0AMore%20broadly%2C%20the%20findings%20confirm%20the%20practicality%20of%20auricular%20vein%0Abiometrics%20for%20digitizing%20livestock%20management%2C%20reinforcing%20its%20potential%20to%0Aextend%20the%20benefits%20of%20precision%20farming%20to%20resource-constrained%20agricultural%0Acommunities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02197v1&entry.124074799=Read"},
{"title": "A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead\n  Adversarial Example Detection", "author": "Sanggeon Yun and Ryozo Masukawa and Hyunwoo Oh and Nathaniel D. Bastian and Mohsen Imani", "abstract": "  Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle, imperceptible perturbations that can lead to incorrect\npredictions. While detection-based defenses offer a practical alternative to\nadversarial training, many existing methods depend on external models, complex\narchitectures, or adversarial data, limiting their efficiency and\ngeneralizability. We introduce a lightweight, plug-in detection framework that\nleverages internal layer-wise inconsistencies within the target model itself,\nrequiring only benign data for calibration. Our approach is grounded in the A\nFew Large Shifts Assumption, which posits that adversarial perturbations induce\nlarge, localized violations of layer-wise Lipschitz continuity in a small\nsubset of layers. Building on this, we propose two complementary\nstrategies--Recovery Testing (RT) and Logit-layer Testing (LT)--to empirically\nmeasure these violations and expose internal disruptions caused by adversaries.\nEvaluated on CIFAR-10, CIFAR-100, and ImageNet under both standard and adaptive\nthreat models, our method achieves state-of-the-art detection performance with\nnegligible computational overhead. Furthermore, our system-level analysis\nprovides a practical method for selecting a detection threshold with a formal\nlower-bound guarantee on accuracy. The code is available here:\nhttps://github.com/c0510gy/AFLS-AED.\n", "link": "http://arxiv.org/abs/2505.12586v5", "date": "2025-10-02", "relevancy": 2.1497, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5557}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5334}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Few%20Large%20Shifts%3A%20Layer-Inconsistency%20Based%20Minimal%20Overhead%0A%20%20Adversarial%20Example%20Detection&body=Title%3A%20A%20Few%20Large%20Shifts%3A%20Layer-Inconsistency%20Based%20Minimal%20Overhead%0A%20%20Adversarial%20Example%20Detection%0AAuthor%3A%20Sanggeon%20Yun%20and%20Ryozo%20Masukawa%20and%20Hyunwoo%20Oh%20and%20Nathaniel%20D.%20Bastian%20and%20Mohsen%20Imani%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20highly%20susceptible%20to%20adversarial%0Aexamples--subtle%2C%20imperceptible%20perturbations%20that%20can%20lead%20to%20incorrect%0Apredictions.%20While%20detection-based%20defenses%20offer%20a%20practical%20alternative%20to%0Aadversarial%20training%2C%20many%20existing%20methods%20depend%20on%20external%20models%2C%20complex%0Aarchitectures%2C%20or%20adversarial%20data%2C%20limiting%20their%20efficiency%20and%0Ageneralizability.%20We%20introduce%20a%20lightweight%2C%20plug-in%20detection%20framework%20that%0Aleverages%20internal%20layer-wise%20inconsistencies%20within%20the%20target%20model%20itself%2C%0Arequiring%20only%20benign%20data%20for%20calibration.%20Our%20approach%20is%20grounded%20in%20the%20A%0AFew%20Large%20Shifts%20Assumption%2C%20which%20posits%20that%20adversarial%20perturbations%20induce%0Alarge%2C%20localized%20violations%20of%20layer-wise%20Lipschitz%20continuity%20in%20a%20small%0Asubset%20of%20layers.%20Building%20on%20this%2C%20we%20propose%20two%20complementary%0Astrategies--Recovery%20Testing%20%28RT%29%20and%20Logit-layer%20Testing%20%28LT%29--to%20empirically%0Ameasure%20these%20violations%20and%20expose%20internal%20disruptions%20caused%20by%20adversaries.%0AEvaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%20under%20both%20standard%20and%20adaptive%0Athreat%20models%2C%20our%20method%20achieves%20state-of-the-art%20detection%20performance%20with%0Anegligible%20computational%20overhead.%20Furthermore%2C%20our%20system-level%20analysis%0Aprovides%20a%20practical%20method%20for%20selecting%20a%20detection%20threshold%20with%20a%20formal%0Alower-bound%20guarantee%20on%20accuracy.%20The%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/c0510gy/AFLS-AED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12586v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Few%2520Large%2520Shifts%253A%2520Layer-Inconsistency%2520Based%2520Minimal%2520Overhead%250A%2520%2520Adversarial%2520Example%2520Detection%26entry.906535625%3DSanggeon%2520Yun%2520and%2520Ryozo%2520Masukawa%2520and%2520Hyunwoo%2520Oh%2520and%2520Nathaniel%2520D.%2520Bastian%2520and%2520Mohsen%2520Imani%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520highly%2520susceptible%2520to%2520adversarial%250Aexamples--subtle%252C%2520imperceptible%2520perturbations%2520that%2520can%2520lead%2520to%2520incorrect%250Apredictions.%2520While%2520detection-based%2520defenses%2520offer%2520a%2520practical%2520alternative%2520to%250Aadversarial%2520training%252C%2520many%2520existing%2520methods%2520depend%2520on%2520external%2520models%252C%2520complex%250Aarchitectures%252C%2520or%2520adversarial%2520data%252C%2520limiting%2520their%2520efficiency%2520and%250Ageneralizability.%2520We%2520introduce%2520a%2520lightweight%252C%2520plug-in%2520detection%2520framework%2520that%250Aleverages%2520internal%2520layer-wise%2520inconsistencies%2520within%2520the%2520target%2520model%2520itself%252C%250Arequiring%2520only%2520benign%2520data%2520for%2520calibration.%2520Our%2520approach%2520is%2520grounded%2520in%2520the%2520A%250AFew%2520Large%2520Shifts%2520Assumption%252C%2520which%2520posits%2520that%2520adversarial%2520perturbations%2520induce%250Alarge%252C%2520localized%2520violations%2520of%2520layer-wise%2520Lipschitz%2520continuity%2520in%2520a%2520small%250Asubset%2520of%2520layers.%2520Building%2520on%2520this%252C%2520we%2520propose%2520two%2520complementary%250Astrategies--Recovery%2520Testing%2520%2528RT%2529%2520and%2520Logit-layer%2520Testing%2520%2528LT%2529--to%2520empirically%250Ameasure%2520these%2520violations%2520and%2520expose%2520internal%2520disruptions%2520caused%2520by%2520adversaries.%250AEvaluated%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520ImageNet%2520under%2520both%2520standard%2520and%2520adaptive%250Athreat%2520models%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520detection%2520performance%2520with%250Anegligible%2520computational%2520overhead.%2520Furthermore%252C%2520our%2520system-level%2520analysis%250Aprovides%2520a%2520practical%2520method%2520for%2520selecting%2520a%2520detection%2520threshold%2520with%2520a%2520formal%250Alower-bound%2520guarantee%2520on%2520accuracy.%2520The%2520code%2520is%2520available%2520here%253A%250Ahttps%253A//github.com/c0510gy/AFLS-AED.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12586v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Few%20Large%20Shifts%3A%20Layer-Inconsistency%20Based%20Minimal%20Overhead%0A%20%20Adversarial%20Example%20Detection&entry.906535625=Sanggeon%20Yun%20and%20Ryozo%20Masukawa%20and%20Hyunwoo%20Oh%20and%20Nathaniel%20D.%20Bastian%20and%20Mohsen%20Imani&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20highly%20susceptible%20to%20adversarial%0Aexamples--subtle%2C%20imperceptible%20perturbations%20that%20can%20lead%20to%20incorrect%0Apredictions.%20While%20detection-based%20defenses%20offer%20a%20practical%20alternative%20to%0Aadversarial%20training%2C%20many%20existing%20methods%20depend%20on%20external%20models%2C%20complex%0Aarchitectures%2C%20or%20adversarial%20data%2C%20limiting%20their%20efficiency%20and%0Ageneralizability.%20We%20introduce%20a%20lightweight%2C%20plug-in%20detection%20framework%20that%0Aleverages%20internal%20layer-wise%20inconsistencies%20within%20the%20target%20model%20itself%2C%0Arequiring%20only%20benign%20data%20for%20calibration.%20Our%20approach%20is%20grounded%20in%20the%20A%0AFew%20Large%20Shifts%20Assumption%2C%20which%20posits%20that%20adversarial%20perturbations%20induce%0Alarge%2C%20localized%20violations%20of%20layer-wise%20Lipschitz%20continuity%20in%20a%20small%0Asubset%20of%20layers.%20Building%20on%20this%2C%20we%20propose%20two%20complementary%0Astrategies--Recovery%20Testing%20%28RT%29%20and%20Logit-layer%20Testing%20%28LT%29--to%20empirically%0Ameasure%20these%20violations%20and%20expose%20internal%20disruptions%20caused%20by%20adversaries.%0AEvaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet%20under%20both%20standard%20and%20adaptive%0Athreat%20models%2C%20our%20method%20achieves%20state-of-the-art%20detection%20performance%20with%0Anegligible%20computational%20overhead.%20Furthermore%2C%20our%20system-level%20analysis%0Aprovides%20a%20practical%20method%20for%20selecting%20a%20detection%20threshold%20with%20a%20formal%0Alower-bound%20guarantee%20on%20accuracy.%20The%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/c0510gy/AFLS-AED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12586v5&entry.124074799=Read"},
{"title": "VITA: Vision-to-Action Flow Matching Policy", "author": "Dechen Gao and Boqi Zhao and Andrew Lee and Ian Chuang and Hanchu Zhou and Hang Wang and Zhe Zhao and Junshan Zhang and Iman Soltani", "abstract": "  Conventional flow matching and diffusion-based policies sample through\niterative denoising from standard noise distributions (e.g., Gaussian), and\nrequire conditioning mechanisms to incorporate visual information during the\ngenerative process, incurring substantial time and memory overhead. To reduce\nthe complexity, we develop VITA(VIsion-To-Action policy), a noise-free and\nconditioning-free policy learning framework that directly maps visual\nrepresentations to latent actions using flow matching. VITA treats latent\nvisual representations as the source of the flow, thus eliminating the need of\nconditioning. As expected, bridging vision and action is challenging, because\nactions are lower-dimensional, less structured, and sparser than visual\nrepresentations; moreover, flow matching requires the source and target to have\nthe same dimensionality. To overcome this, we introduce an action autoencoder\nthat maps raw actions into a structured latent space aligned with visual\nlatents, trained jointly with flow matching. To further prevent latent space\ncollapse, we propose flow latent decoding, which anchors the latent generation\nprocess by backpropagating the action reconstruction loss through the flow\nmatching ODE (ordinary differential equations) solving steps. We evaluate VITA\non 8 simulation and 2 real-world tasks from ALOHA and Robomimic. VITA\noutperforms or matches state-of-the-art generative policies, while achieving\n1.5-2.3x faster inference compared to conventional methods with conditioning.\nProject page: https://ucd-dare.github.io/VITA/\n", "link": "http://arxiv.org/abs/2507.13231v2", "date": "2025-10-02", "relevancy": 2.1438, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5388}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5367}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA%3A%20Vision-to-Action%20Flow%20Matching%20Policy&body=Title%3A%20VITA%3A%20Vision-to-Action%20Flow%20Matching%20Policy%0AAuthor%3A%20Dechen%20Gao%20and%20Boqi%20Zhao%20and%20Andrew%20Lee%20and%20Ian%20Chuang%20and%20Hanchu%20Zhou%20and%20Hang%20Wang%20and%20Zhe%20Zhao%20and%20Junshan%20Zhang%20and%20Iman%20Soltani%0AAbstract%3A%20%20%20Conventional%20flow%20matching%20and%20diffusion-based%20policies%20sample%20through%0Aiterative%20denoising%20from%20standard%20noise%20distributions%20%28e.g.%2C%20Gaussian%29%2C%20and%0Arequire%20conditioning%20mechanisms%20to%20incorporate%20visual%20information%20during%20the%0Agenerative%20process%2C%20incurring%20substantial%20time%20and%20memory%20overhead.%20To%20reduce%0Athe%20complexity%2C%20we%20develop%20VITA%28VIsion-To-Action%20policy%29%2C%20a%20noise-free%20and%0Aconditioning-free%20policy%20learning%20framework%20that%20directly%20maps%20visual%0Arepresentations%20to%20latent%20actions%20using%20flow%20matching.%20VITA%20treats%20latent%0Avisual%20representations%20as%20the%20source%20of%20the%20flow%2C%20thus%20eliminating%20the%20need%20of%0Aconditioning.%20As%20expected%2C%20bridging%20vision%20and%20action%20is%20challenging%2C%20because%0Aactions%20are%20lower-dimensional%2C%20less%20structured%2C%20and%20sparser%20than%20visual%0Arepresentations%3B%20moreover%2C%20flow%20matching%20requires%20the%20source%20and%20target%20to%20have%0Athe%20same%20dimensionality.%20To%20overcome%20this%2C%20we%20introduce%20an%20action%20autoencoder%0Athat%20maps%20raw%20actions%20into%20a%20structured%20latent%20space%20aligned%20with%20visual%0Alatents%2C%20trained%20jointly%20with%20flow%20matching.%20To%20further%20prevent%20latent%20space%0Acollapse%2C%20we%20propose%20flow%20latent%20decoding%2C%20which%20anchors%20the%20latent%20generation%0Aprocess%20by%20backpropagating%20the%20action%20reconstruction%20loss%20through%20the%20flow%0Amatching%20ODE%20%28ordinary%20differential%20equations%29%20solving%20steps.%20We%20evaluate%20VITA%0Aon%208%20simulation%20and%202%20real-world%20tasks%20from%20ALOHA%20and%20Robomimic.%20VITA%0Aoutperforms%20or%20matches%20state-of-the-art%20generative%20policies%2C%20while%20achieving%0A1.5-2.3x%20faster%20inference%20compared%20to%20conventional%20methods%20with%20conditioning.%0AProject%20page%3A%20https%3A//ucd-dare.github.io/VITA/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA%253A%2520Vision-to-Action%2520Flow%2520Matching%2520Policy%26entry.906535625%3DDechen%2520Gao%2520and%2520Boqi%2520Zhao%2520and%2520Andrew%2520Lee%2520and%2520Ian%2520Chuang%2520and%2520Hanchu%2520Zhou%2520and%2520Hang%2520Wang%2520and%2520Zhe%2520Zhao%2520and%2520Junshan%2520Zhang%2520and%2520Iman%2520Soltani%26entry.1292438233%3D%2520%2520Conventional%2520flow%2520matching%2520and%2520diffusion-based%2520policies%2520sample%2520through%250Aiterative%2520denoising%2520from%2520standard%2520noise%2520distributions%2520%2528e.g.%252C%2520Gaussian%2529%252C%2520and%250Arequire%2520conditioning%2520mechanisms%2520to%2520incorporate%2520visual%2520information%2520during%2520the%250Agenerative%2520process%252C%2520incurring%2520substantial%2520time%2520and%2520memory%2520overhead.%2520To%2520reduce%250Athe%2520complexity%252C%2520we%2520develop%2520VITA%2528VIsion-To-Action%2520policy%2529%252C%2520a%2520noise-free%2520and%250Aconditioning-free%2520policy%2520learning%2520framework%2520that%2520directly%2520maps%2520visual%250Arepresentations%2520to%2520latent%2520actions%2520using%2520flow%2520matching.%2520VITA%2520treats%2520latent%250Avisual%2520representations%2520as%2520the%2520source%2520of%2520the%2520flow%252C%2520thus%2520eliminating%2520the%2520need%2520of%250Aconditioning.%2520As%2520expected%252C%2520bridging%2520vision%2520and%2520action%2520is%2520challenging%252C%2520because%250Aactions%2520are%2520lower-dimensional%252C%2520less%2520structured%252C%2520and%2520sparser%2520than%2520visual%250Arepresentations%253B%2520moreover%252C%2520flow%2520matching%2520requires%2520the%2520source%2520and%2520target%2520to%2520have%250Athe%2520same%2520dimensionality.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520an%2520action%2520autoencoder%250Athat%2520maps%2520raw%2520actions%2520into%2520a%2520structured%2520latent%2520space%2520aligned%2520with%2520visual%250Alatents%252C%2520trained%2520jointly%2520with%2520flow%2520matching.%2520To%2520further%2520prevent%2520latent%2520space%250Acollapse%252C%2520we%2520propose%2520flow%2520latent%2520decoding%252C%2520which%2520anchors%2520the%2520latent%2520generation%250Aprocess%2520by%2520backpropagating%2520the%2520action%2520reconstruction%2520loss%2520through%2520the%2520flow%250Amatching%2520ODE%2520%2528ordinary%2520differential%2520equations%2529%2520solving%2520steps.%2520We%2520evaluate%2520VITA%250Aon%25208%2520simulation%2520and%25202%2520real-world%2520tasks%2520from%2520ALOHA%2520and%2520Robomimic.%2520VITA%250Aoutperforms%2520or%2520matches%2520state-of-the-art%2520generative%2520policies%252C%2520while%2520achieving%250A1.5-2.3x%2520faster%2520inference%2520compared%2520to%2520conventional%2520methods%2520with%2520conditioning.%250AProject%2520page%253A%2520https%253A//ucd-dare.github.io/VITA/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA%3A%20Vision-to-Action%20Flow%20Matching%20Policy&entry.906535625=Dechen%20Gao%20and%20Boqi%20Zhao%20and%20Andrew%20Lee%20and%20Ian%20Chuang%20and%20Hanchu%20Zhou%20and%20Hang%20Wang%20and%20Zhe%20Zhao%20and%20Junshan%20Zhang%20and%20Iman%20Soltani&entry.1292438233=%20%20Conventional%20flow%20matching%20and%20diffusion-based%20policies%20sample%20through%0Aiterative%20denoising%20from%20standard%20noise%20distributions%20%28e.g.%2C%20Gaussian%29%2C%20and%0Arequire%20conditioning%20mechanisms%20to%20incorporate%20visual%20information%20during%20the%0Agenerative%20process%2C%20incurring%20substantial%20time%20and%20memory%20overhead.%20To%20reduce%0Athe%20complexity%2C%20we%20develop%20VITA%28VIsion-To-Action%20policy%29%2C%20a%20noise-free%20and%0Aconditioning-free%20policy%20learning%20framework%20that%20directly%20maps%20visual%0Arepresentations%20to%20latent%20actions%20using%20flow%20matching.%20VITA%20treats%20latent%0Avisual%20representations%20as%20the%20source%20of%20the%20flow%2C%20thus%20eliminating%20the%20need%20of%0Aconditioning.%20As%20expected%2C%20bridging%20vision%20and%20action%20is%20challenging%2C%20because%0Aactions%20are%20lower-dimensional%2C%20less%20structured%2C%20and%20sparser%20than%20visual%0Arepresentations%3B%20moreover%2C%20flow%20matching%20requires%20the%20source%20and%20target%20to%20have%0Athe%20same%20dimensionality.%20To%20overcome%20this%2C%20we%20introduce%20an%20action%20autoencoder%0Athat%20maps%20raw%20actions%20into%20a%20structured%20latent%20space%20aligned%20with%20visual%0Alatents%2C%20trained%20jointly%20with%20flow%20matching.%20To%20further%20prevent%20latent%20space%0Acollapse%2C%20we%20propose%20flow%20latent%20decoding%2C%20which%20anchors%20the%20latent%20generation%0Aprocess%20by%20backpropagating%20the%20action%20reconstruction%20loss%20through%20the%20flow%0Amatching%20ODE%20%28ordinary%20differential%20equations%29%20solving%20steps.%20We%20evaluate%20VITA%0Aon%208%20simulation%20and%202%20real-world%20tasks%20from%20ALOHA%20and%20Robomimic.%20VITA%0Aoutperforms%20or%20matches%20state-of-the-art%20generative%20policies%2C%20while%20achieving%0A1.5-2.3x%20faster%20inference%20compared%20to%20conventional%20methods%20with%20conditioning.%0AProject%20page%3A%20https%3A//ucd-dare.github.io/VITA/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13231v2&entry.124074799=Read"},
{"title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control\n  Empower Scalable Real-world Deployment and Adaptation", "author": "Wenye Yu and Jun Lv and Zixi Ying and Yang Jin and Chuan Wen and Cewu Lu", "abstract": "  Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.\n", "link": "http://arxiv.org/abs/2510.02298v1", "date": "2025-10-02", "relevancy": 2.1409, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5441}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5335}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARMADA%3A%20Autonomous%20Online%20Failure%20Detection%20and%20Human%20Shared%20Control%0A%20%20Empower%20Scalable%20Real-world%20Deployment%20and%20Adaptation&body=Title%3A%20ARMADA%3A%20Autonomous%20Online%20Failure%20Detection%20and%20Human%20Shared%20Control%0A%20%20Empower%20Scalable%20Real-world%20Deployment%20and%20Adaptation%0AAuthor%3A%20Wenye%20Yu%20and%20Jun%20Lv%20and%20Zixi%20Ying%20and%20Yang%20Jin%20and%20Chuan%20Wen%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Imitation%20learning%20has%20shown%20promise%20in%20learning%20from%20large-scale%20real-world%0Adatasets.%20However%2C%20pretrained%20policies%20usually%20perform%20poorly%20without%0Asufficient%20in-domain%20data.%20Besides%2C%20human-collected%20demonstrations%20entail%0Asubstantial%20labour%20and%20tend%20to%20encompass%20mixed-quality%20data%20and%20redundant%0Ainformation.%20As%20a%20workaround%2C%20human-in-the-loop%20systems%20gather%20domain-specific%0Adata%20for%20policy%20post-training%2C%20and%20exploit%20closed-loop%20policy%20feedback%20to%20offer%0Ainformative%20guidance%2C%20but%20usually%20require%20full-time%20human%20surveillance%20during%0Apolicy%20rollout.%20In%20this%20work%2C%20we%20devise%20ARMADA%2C%20a%20multi-robot%20deployment%20and%0Aadaptation%20system%20with%20human-in-the-loop%20shared%20control%2C%20featuring%20an%0Aautonomous%20online%20failure%20detection%20method%20named%20FLOAT.%20Thanks%20to%20FLOAT%2C%20ARMADA%0Aenables%20paralleled%20policy%20rollout%20and%20requests%20human%20intervention%20only%20when%0Anecessary%2C%20significantly%20reducing%20reliance%20on%20human%20supervision.%20Hence%2C%20ARMADA%0Aenables%20efficient%20acquisition%20of%20in-domain%20data%2C%20and%20leads%20to%20more%20scalable%0Adeployment%20and%20faster%20adaptation%20to%20new%20scenarios.%20We%20evaluate%20the%20performance%0Aof%20ARMADA%20on%20four%20real-world%20tasks.%20FLOAT%20achieves%20nearly%2095%25%20accuracy%20on%0Aaverage%2C%20surpassing%20prior%20state-of-the-art%20failure%20detection%20approaches%20by%20over%0A20%25.%20Besides%2C%20ARMADA%20manifests%20more%20than%204%24%5Ctimes%24%20increase%20in%20success%20rate%20and%0Agreater%20than%202%24%5Ctimes%24%20reduction%20in%20human%20intervention%20rate%20over%20multiple%0Arounds%20of%20policy%20rollout%20and%20post-training%2C%20compared%20to%20previous%0Ahuman-in-the-loop%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARMADA%253A%2520Autonomous%2520Online%2520Failure%2520Detection%2520and%2520Human%2520Shared%2520Control%250A%2520%2520Empower%2520Scalable%2520Real-world%2520Deployment%2520and%2520Adaptation%26entry.906535625%3DWenye%2520Yu%2520and%2520Jun%2520Lv%2520and%2520Zixi%2520Ying%2520and%2520Yang%2520Jin%2520and%2520Chuan%2520Wen%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520has%2520shown%2520promise%2520in%2520learning%2520from%2520large-scale%2520real-world%250Adatasets.%2520However%252C%2520pretrained%2520policies%2520usually%2520perform%2520poorly%2520without%250Asufficient%2520in-domain%2520data.%2520Besides%252C%2520human-collected%2520demonstrations%2520entail%250Asubstantial%2520labour%2520and%2520tend%2520to%2520encompass%2520mixed-quality%2520data%2520and%2520redundant%250Ainformation.%2520As%2520a%2520workaround%252C%2520human-in-the-loop%2520systems%2520gather%2520domain-specific%250Adata%2520for%2520policy%2520post-training%252C%2520and%2520exploit%2520closed-loop%2520policy%2520feedback%2520to%2520offer%250Ainformative%2520guidance%252C%2520but%2520usually%2520require%2520full-time%2520human%2520surveillance%2520during%250Apolicy%2520rollout.%2520In%2520this%2520work%252C%2520we%2520devise%2520ARMADA%252C%2520a%2520multi-robot%2520deployment%2520and%250Aadaptation%2520system%2520with%2520human-in-the-loop%2520shared%2520control%252C%2520featuring%2520an%250Aautonomous%2520online%2520failure%2520detection%2520method%2520named%2520FLOAT.%2520Thanks%2520to%2520FLOAT%252C%2520ARMADA%250Aenables%2520paralleled%2520policy%2520rollout%2520and%2520requests%2520human%2520intervention%2520only%2520when%250Anecessary%252C%2520significantly%2520reducing%2520reliance%2520on%2520human%2520supervision.%2520Hence%252C%2520ARMADA%250Aenables%2520efficient%2520acquisition%2520of%2520in-domain%2520data%252C%2520and%2520leads%2520to%2520more%2520scalable%250Adeployment%2520and%2520faster%2520adaptation%2520to%2520new%2520scenarios.%2520We%2520evaluate%2520the%2520performance%250Aof%2520ARMADA%2520on%2520four%2520real-world%2520tasks.%2520FLOAT%2520achieves%2520nearly%252095%2525%2520accuracy%2520on%250Aaverage%252C%2520surpassing%2520prior%2520state-of-the-art%2520failure%2520detection%2520approaches%2520by%2520over%250A20%2525.%2520Besides%252C%2520ARMADA%2520manifests%2520more%2520than%25204%2524%255Ctimes%2524%2520increase%2520in%2520success%2520rate%2520and%250Agreater%2520than%25202%2524%255Ctimes%2524%2520reduction%2520in%2520human%2520intervention%2520rate%2520over%2520multiple%250Arounds%2520of%2520policy%2520rollout%2520and%2520post-training%252C%2520compared%2520to%2520previous%250Ahuman-in-the-loop%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARMADA%3A%20Autonomous%20Online%20Failure%20Detection%20and%20Human%20Shared%20Control%0A%20%20Empower%20Scalable%20Real-world%20Deployment%20and%20Adaptation&entry.906535625=Wenye%20Yu%20and%20Jun%20Lv%20and%20Zixi%20Ying%20and%20Yang%20Jin%20and%20Chuan%20Wen%20and%20Cewu%20Lu&entry.1292438233=%20%20Imitation%20learning%20has%20shown%20promise%20in%20learning%20from%20large-scale%20real-world%0Adatasets.%20However%2C%20pretrained%20policies%20usually%20perform%20poorly%20without%0Asufficient%20in-domain%20data.%20Besides%2C%20human-collected%20demonstrations%20entail%0Asubstantial%20labour%20and%20tend%20to%20encompass%20mixed-quality%20data%20and%20redundant%0Ainformation.%20As%20a%20workaround%2C%20human-in-the-loop%20systems%20gather%20domain-specific%0Adata%20for%20policy%20post-training%2C%20and%20exploit%20closed-loop%20policy%20feedback%20to%20offer%0Ainformative%20guidance%2C%20but%20usually%20require%20full-time%20human%20surveillance%20during%0Apolicy%20rollout.%20In%20this%20work%2C%20we%20devise%20ARMADA%2C%20a%20multi-robot%20deployment%20and%0Aadaptation%20system%20with%20human-in-the-loop%20shared%20control%2C%20featuring%20an%0Aautonomous%20online%20failure%20detection%20method%20named%20FLOAT.%20Thanks%20to%20FLOAT%2C%20ARMADA%0Aenables%20paralleled%20policy%20rollout%20and%20requests%20human%20intervention%20only%20when%0Anecessary%2C%20significantly%20reducing%20reliance%20on%20human%20supervision.%20Hence%2C%20ARMADA%0Aenables%20efficient%20acquisition%20of%20in-domain%20data%2C%20and%20leads%20to%20more%20scalable%0Adeployment%20and%20faster%20adaptation%20to%20new%20scenarios.%20We%20evaluate%20the%20performance%0Aof%20ARMADA%20on%20four%20real-world%20tasks.%20FLOAT%20achieves%20nearly%2095%25%20accuracy%20on%0Aaverage%2C%20surpassing%20prior%20state-of-the-art%20failure%20detection%20approaches%20by%20over%0A20%25.%20Besides%2C%20ARMADA%20manifests%20more%20than%204%24%5Ctimes%24%20increase%20in%20success%20rate%20and%0Agreater%20than%202%24%5Ctimes%24%20reduction%20in%20human%20intervention%20rate%20over%20multiple%0Arounds%20of%20policy%20rollout%20and%20post-training%2C%20compared%20to%20previous%0Ahuman-in-the-loop%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02298v1&entry.124074799=Read"},
{"title": "FalconWing: An Ultra-Light Indoor Fixed-Wing UAV Platform for\n  Vision-Based Autonomy", "author": "Yan Miao and Will Shen and Hang Cui and Sayan Mitra", "abstract": "  We introduce FalconWing, an ultra-light (150 g) indoor fixed-wing UAV\nplatform for vision-based autonomy. Controlled indoor environment enables\nyear-round repeatable UAV experiment but imposes strict weight and\nmaneuverability limits on the UAV, motivating our ultra-light FalconWing\ndesign. FalconWing couples a lightweight hardware stack (137g airframe with a\n9g camera) and offboard computation with a software stack featuring a\nphotorealistic 3D Gaussian Splat (GSplat) simulator for developing and\nevaluating vision-based controllers. We validate FalconWing on two challenging\nvision-based aerial case studies. In the leader-follower case study, our best\nvision-based controller, trained via imitation learning on GSplat-rendered data\naugmented with domain randomization, achieves 100% tracking success across 3\ntypes of leader maneuvers over 30 trials and shows robustness to leader's\nappearance shifts in simulation. In the autonomous landing case study, our\nvision-based controller trained purely in simulation transfers zero-shot to\nreal hardware, achieving an 80% success rate over ten landing trials. We will\nrelease hardware designs, GSplat scenes, and dynamics models upon publication\nto make FalconWing an open-source flight kit for engineering students and\nresearch labs.\n", "link": "http://arxiv.org/abs/2505.01383v2", "date": "2025-10-02", "relevancy": 2.1342, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5503}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5233}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FalconWing%3A%20An%20Ultra-Light%20Indoor%20Fixed-Wing%20UAV%20Platform%20for%0A%20%20Vision-Based%20Autonomy&body=Title%3A%20FalconWing%3A%20An%20Ultra-Light%20Indoor%20Fixed-Wing%20UAV%20Platform%20for%0A%20%20Vision-Based%20Autonomy%0AAuthor%3A%20Yan%20Miao%20and%20Will%20Shen%20and%20Hang%20Cui%20and%20Sayan%20Mitra%0AAbstract%3A%20%20%20We%20introduce%20FalconWing%2C%20an%20ultra-light%20%28150%20g%29%20indoor%20fixed-wing%20UAV%0Aplatform%20for%20vision-based%20autonomy.%20Controlled%20indoor%20environment%20enables%0Ayear-round%20repeatable%20UAV%20experiment%20but%20imposes%20strict%20weight%20and%0Amaneuverability%20limits%20on%20the%20UAV%2C%20motivating%20our%20ultra-light%20FalconWing%0Adesign.%20FalconWing%20couples%20a%20lightweight%20hardware%20stack%20%28137g%20airframe%20with%20a%0A9g%20camera%29%20and%20offboard%20computation%20with%20a%20software%20stack%20featuring%20a%0Aphotorealistic%203D%20Gaussian%20Splat%20%28GSplat%29%20simulator%20for%20developing%20and%0Aevaluating%20vision-based%20controllers.%20We%20validate%20FalconWing%20on%20two%20challenging%0Avision-based%20aerial%20case%20studies.%20In%20the%20leader-follower%20case%20study%2C%20our%20best%0Avision-based%20controller%2C%20trained%20via%20imitation%20learning%20on%20GSplat-rendered%20data%0Aaugmented%20with%20domain%20randomization%2C%20achieves%20100%25%20tracking%20success%20across%203%0Atypes%20of%20leader%20maneuvers%20over%2030%20trials%20and%20shows%20robustness%20to%20leader%27s%0Aappearance%20shifts%20in%20simulation.%20In%20the%20autonomous%20landing%20case%20study%2C%20our%0Avision-based%20controller%20trained%20purely%20in%20simulation%20transfers%20zero-shot%20to%0Areal%20hardware%2C%20achieving%20an%2080%25%20success%20rate%20over%20ten%20landing%20trials.%20We%20will%0Arelease%20hardware%20designs%2C%20GSplat%20scenes%2C%20and%20dynamics%20models%20upon%20publication%0Ato%20make%20FalconWing%20an%20open-source%20flight%20kit%20for%20engineering%20students%20and%0Aresearch%20labs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFalconWing%253A%2520An%2520Ultra-Light%2520Indoor%2520Fixed-Wing%2520UAV%2520Platform%2520for%250A%2520%2520Vision-Based%2520Autonomy%26entry.906535625%3DYan%2520Miao%2520and%2520Will%2520Shen%2520and%2520Hang%2520Cui%2520and%2520Sayan%2520Mitra%26entry.1292438233%3D%2520%2520We%2520introduce%2520FalconWing%252C%2520an%2520ultra-light%2520%2528150%2520g%2529%2520indoor%2520fixed-wing%2520UAV%250Aplatform%2520for%2520vision-based%2520autonomy.%2520Controlled%2520indoor%2520environment%2520enables%250Ayear-round%2520repeatable%2520UAV%2520experiment%2520but%2520imposes%2520strict%2520weight%2520and%250Amaneuverability%2520limits%2520on%2520the%2520UAV%252C%2520motivating%2520our%2520ultra-light%2520FalconWing%250Adesign.%2520FalconWing%2520couples%2520a%2520lightweight%2520hardware%2520stack%2520%2528137g%2520airframe%2520with%2520a%250A9g%2520camera%2529%2520and%2520offboard%2520computation%2520with%2520a%2520software%2520stack%2520featuring%2520a%250Aphotorealistic%25203D%2520Gaussian%2520Splat%2520%2528GSplat%2529%2520simulator%2520for%2520developing%2520and%250Aevaluating%2520vision-based%2520controllers.%2520We%2520validate%2520FalconWing%2520on%2520two%2520challenging%250Avision-based%2520aerial%2520case%2520studies.%2520In%2520the%2520leader-follower%2520case%2520study%252C%2520our%2520best%250Avision-based%2520controller%252C%2520trained%2520via%2520imitation%2520learning%2520on%2520GSplat-rendered%2520data%250Aaugmented%2520with%2520domain%2520randomization%252C%2520achieves%2520100%2525%2520tracking%2520success%2520across%25203%250Atypes%2520of%2520leader%2520maneuvers%2520over%252030%2520trials%2520and%2520shows%2520robustness%2520to%2520leader%2527s%250Aappearance%2520shifts%2520in%2520simulation.%2520In%2520the%2520autonomous%2520landing%2520case%2520study%252C%2520our%250Avision-based%2520controller%2520trained%2520purely%2520in%2520simulation%2520transfers%2520zero-shot%2520to%250Areal%2520hardware%252C%2520achieving%2520an%252080%2525%2520success%2520rate%2520over%2520ten%2520landing%2520trials.%2520We%2520will%250Arelease%2520hardware%2520designs%252C%2520GSplat%2520scenes%252C%2520and%2520dynamics%2520models%2520upon%2520publication%250Ato%2520make%2520FalconWing%2520an%2520open-source%2520flight%2520kit%2520for%2520engineering%2520students%2520and%250Aresearch%2520labs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FalconWing%3A%20An%20Ultra-Light%20Indoor%20Fixed-Wing%20UAV%20Platform%20for%0A%20%20Vision-Based%20Autonomy&entry.906535625=Yan%20Miao%20and%20Will%20Shen%20and%20Hang%20Cui%20and%20Sayan%20Mitra&entry.1292438233=%20%20We%20introduce%20FalconWing%2C%20an%20ultra-light%20%28150%20g%29%20indoor%20fixed-wing%20UAV%0Aplatform%20for%20vision-based%20autonomy.%20Controlled%20indoor%20environment%20enables%0Ayear-round%20repeatable%20UAV%20experiment%20but%20imposes%20strict%20weight%20and%0Amaneuverability%20limits%20on%20the%20UAV%2C%20motivating%20our%20ultra-light%20FalconWing%0Adesign.%20FalconWing%20couples%20a%20lightweight%20hardware%20stack%20%28137g%20airframe%20with%20a%0A9g%20camera%29%20and%20offboard%20computation%20with%20a%20software%20stack%20featuring%20a%0Aphotorealistic%203D%20Gaussian%20Splat%20%28GSplat%29%20simulator%20for%20developing%20and%0Aevaluating%20vision-based%20controllers.%20We%20validate%20FalconWing%20on%20two%20challenging%0Avision-based%20aerial%20case%20studies.%20In%20the%20leader-follower%20case%20study%2C%20our%20best%0Avision-based%20controller%2C%20trained%20via%20imitation%20learning%20on%20GSplat-rendered%20data%0Aaugmented%20with%20domain%20randomization%2C%20achieves%20100%25%20tracking%20success%20across%203%0Atypes%20of%20leader%20maneuvers%20over%2030%20trials%20and%20shows%20robustness%20to%20leader%27s%0Aappearance%20shifts%20in%20simulation.%20In%20the%20autonomous%20landing%20case%20study%2C%20our%0Avision-based%20controller%20trained%20purely%20in%20simulation%20transfers%20zero-shot%20to%0Areal%20hardware%2C%20achieving%20an%2080%25%20success%20rate%20over%20ten%20landing%20trials.%20We%20will%0Arelease%20hardware%20designs%2C%20GSplat%20scenes%2C%20and%20dynamics%20models%20upon%20publication%0Ato%20make%20FalconWing%20an%20open-source%20flight%20kit%20for%20engineering%20students%20and%0Aresearch%20labs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01383v2&entry.124074799=Read"},
{"title": "Riemannian Variational Flow Matching for Material and Protein Design", "author": "Olga Zaghen and Floor Eijkelboom and Alison Pouplin and Cong Liu and Max Welling and Jan-Willem van de Meent and Erik J. Bekkers", "abstract": "  We present Riemannian Gaussian Variational Flow Matching (RG-VFM), a\ngeometric extension of Variational Flow Matching (VFM) for generative modeling\non manifolds. In Euclidean space, predicting endpoints (VFM), velocities (FM),\nor noise (diffusion) are largely equivalent due to affine interpolations. On\ncurved manifolds this equivalence breaks down, and we hypothesize that endpoint\nprediction provides a stronger learning signal by directly minimizing geodesic\ndistances. Building on this insight, we derive a variational flow matching\nobjective based on Riemannian Gaussian distributions, applicable to manifolds\nwith closed-form geodesics. We formally analyze its relationship to Riemannian\nFlow Matching (RFM), exposing that the RFM objective lacks a\ncurvature-dependent penalty - encoded via Jacobi fields - that is naturally\npresent in RG-VFM. Experiments on synthetic spherical and hyperbolic\nbenchmarks, as well as real-world tasks in material and protein generation,\ndemonstrate that RG-VFM more effectively captures manifold structure and\nimproves downstream performance over Euclidean and velocity-based baselines.\n", "link": "http://arxiv.org/abs/2502.12981v2", "date": "2025-10-02", "relevancy": 2.1236, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5639}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.529}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Variational%20Flow%20Matching%20for%20Material%20and%20Protein%20Design&body=Title%3A%20Riemannian%20Variational%20Flow%20Matching%20for%20Material%20and%20Protein%20Design%0AAuthor%3A%20Olga%20Zaghen%20and%20Floor%20Eijkelboom%20and%20Alison%20Pouplin%20and%20Cong%20Liu%20and%20Max%20Welling%20and%20Jan-Willem%20van%20de%20Meent%20and%20Erik%20J.%20Bekkers%0AAbstract%3A%20%20%20We%20present%20Riemannian%20Gaussian%20Variational%20Flow%20Matching%20%28RG-VFM%29%2C%20a%0Ageometric%20extension%20of%20Variational%20Flow%20Matching%20%28VFM%29%20for%20generative%20modeling%0Aon%20manifolds.%20In%20Euclidean%20space%2C%20predicting%20endpoints%20%28VFM%29%2C%20velocities%20%28FM%29%2C%0Aor%20noise%20%28diffusion%29%20are%20largely%20equivalent%20due%20to%20affine%20interpolations.%20On%0Acurved%20manifolds%20this%20equivalence%20breaks%20down%2C%20and%20we%20hypothesize%20that%20endpoint%0Aprediction%20provides%20a%20stronger%20learning%20signal%20by%20directly%20minimizing%20geodesic%0Adistances.%20Building%20on%20this%20insight%2C%20we%20derive%20a%20variational%20flow%20matching%0Aobjective%20based%20on%20Riemannian%20Gaussian%20distributions%2C%20applicable%20to%20manifolds%0Awith%20closed-form%20geodesics.%20We%20formally%20analyze%20its%20relationship%20to%20Riemannian%0AFlow%20Matching%20%28RFM%29%2C%20exposing%20that%20the%20RFM%20objective%20lacks%20a%0Acurvature-dependent%20penalty%20-%20encoded%20via%20Jacobi%20fields%20-%20that%20is%20naturally%0Apresent%20in%20RG-VFM.%20Experiments%20on%20synthetic%20spherical%20and%20hyperbolic%0Abenchmarks%2C%20as%20well%20as%20real-world%20tasks%20in%20material%20and%20protein%20generation%2C%0Ademonstrate%20that%20RG-VFM%20more%20effectively%20captures%20manifold%20structure%20and%0Aimproves%20downstream%20performance%20over%20Euclidean%20and%20velocity-based%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Variational%2520Flow%2520Matching%2520for%2520Material%2520and%2520Protein%2520Design%26entry.906535625%3DOlga%2520Zaghen%2520and%2520Floor%2520Eijkelboom%2520and%2520Alison%2520Pouplin%2520and%2520Cong%2520Liu%2520and%2520Max%2520Welling%2520and%2520Jan-Willem%2520van%2520de%2520Meent%2520and%2520Erik%2520J.%2520Bekkers%26entry.1292438233%3D%2520%2520We%2520present%2520Riemannian%2520Gaussian%2520Variational%2520Flow%2520Matching%2520%2528RG-VFM%2529%252C%2520a%250Ageometric%2520extension%2520of%2520Variational%2520Flow%2520Matching%2520%2528VFM%2529%2520for%2520generative%2520modeling%250Aon%2520manifolds.%2520In%2520Euclidean%2520space%252C%2520predicting%2520endpoints%2520%2528VFM%2529%252C%2520velocities%2520%2528FM%2529%252C%250Aor%2520noise%2520%2528diffusion%2529%2520are%2520largely%2520equivalent%2520due%2520to%2520affine%2520interpolations.%2520On%250Acurved%2520manifolds%2520this%2520equivalence%2520breaks%2520down%252C%2520and%2520we%2520hypothesize%2520that%2520endpoint%250Aprediction%2520provides%2520a%2520stronger%2520learning%2520signal%2520by%2520directly%2520minimizing%2520geodesic%250Adistances.%2520Building%2520on%2520this%2520insight%252C%2520we%2520derive%2520a%2520variational%2520flow%2520matching%250Aobjective%2520based%2520on%2520Riemannian%2520Gaussian%2520distributions%252C%2520applicable%2520to%2520manifolds%250Awith%2520closed-form%2520geodesics.%2520We%2520formally%2520analyze%2520its%2520relationship%2520to%2520Riemannian%250AFlow%2520Matching%2520%2528RFM%2529%252C%2520exposing%2520that%2520the%2520RFM%2520objective%2520lacks%2520a%250Acurvature-dependent%2520penalty%2520-%2520encoded%2520via%2520Jacobi%2520fields%2520-%2520that%2520is%2520naturally%250Apresent%2520in%2520RG-VFM.%2520Experiments%2520on%2520synthetic%2520spherical%2520and%2520hyperbolic%250Abenchmarks%252C%2520as%2520well%2520as%2520real-world%2520tasks%2520in%2520material%2520and%2520protein%2520generation%252C%250Ademonstrate%2520that%2520RG-VFM%2520more%2520effectively%2520captures%2520manifold%2520structure%2520and%250Aimproves%2520downstream%2520performance%2520over%2520Euclidean%2520and%2520velocity-based%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Variational%20Flow%20Matching%20for%20Material%20and%20Protein%20Design&entry.906535625=Olga%20Zaghen%20and%20Floor%20Eijkelboom%20and%20Alison%20Pouplin%20and%20Cong%20Liu%20and%20Max%20Welling%20and%20Jan-Willem%20van%20de%20Meent%20and%20Erik%20J.%20Bekkers&entry.1292438233=%20%20We%20present%20Riemannian%20Gaussian%20Variational%20Flow%20Matching%20%28RG-VFM%29%2C%20a%0Ageometric%20extension%20of%20Variational%20Flow%20Matching%20%28VFM%29%20for%20generative%20modeling%0Aon%20manifolds.%20In%20Euclidean%20space%2C%20predicting%20endpoints%20%28VFM%29%2C%20velocities%20%28FM%29%2C%0Aor%20noise%20%28diffusion%29%20are%20largely%20equivalent%20due%20to%20affine%20interpolations.%20On%0Acurved%20manifolds%20this%20equivalence%20breaks%20down%2C%20and%20we%20hypothesize%20that%20endpoint%0Aprediction%20provides%20a%20stronger%20learning%20signal%20by%20directly%20minimizing%20geodesic%0Adistances.%20Building%20on%20this%20insight%2C%20we%20derive%20a%20variational%20flow%20matching%0Aobjective%20based%20on%20Riemannian%20Gaussian%20distributions%2C%20applicable%20to%20manifolds%0Awith%20closed-form%20geodesics.%20We%20formally%20analyze%20its%20relationship%20to%20Riemannian%0AFlow%20Matching%20%28RFM%29%2C%20exposing%20that%20the%20RFM%20objective%20lacks%20a%0Acurvature-dependent%20penalty%20-%20encoded%20via%20Jacobi%20fields%20-%20that%20is%20naturally%0Apresent%20in%20RG-VFM.%20Experiments%20on%20synthetic%20spherical%20and%20hyperbolic%0Abenchmarks%2C%20as%20well%20as%20real-world%20tasks%20in%20material%20and%20protein%20generation%2C%0Ademonstrate%20that%20RG-VFM%20more%20effectively%20captures%20manifold%20structure%20and%0Aimproves%20downstream%20performance%20over%20Euclidean%20and%20velocity-based%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12981v2&entry.124074799=Read"},
{"title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning", "author": "Sicheng Feng and Kaiwen Tuo and Song Wang and Lingdong Kong and Jianke Zhu and Huan Wang", "abstract": "  Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.\n", "link": "http://arxiv.org/abs/2510.02240v1", "date": "2025-10-02", "relevancy": 2.1216, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5289}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RewardMap%3A%20Tackling%20Sparse%20Rewards%20in%20Fine-grained%20Visual%20Reasoning%20via%0A%20%20Multi-Stage%20Reinforcement%20Learning&body=Title%3A%20RewardMap%3A%20Tackling%20Sparse%20Rewards%20in%20Fine-grained%20Visual%20Reasoning%20via%0A%20%20Multi-Stage%20Reinforcement%20Learning%0AAuthor%3A%20Sicheng%20Feng%20and%20Kaiwen%20Tuo%20and%20Song%20Wang%20and%20Lingdong%20Kong%20and%20Jianke%20Zhu%20and%20Huan%20Wang%0AAbstract%3A%20%20%20Fine-grained%20visual%20reasoning%20remains%20a%20core%20challenge%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20The%20recently%20introduced%20ReasonMap%20highlights%20this%20gap%0Aby%20showing%20that%20even%20advanced%20MLLMs%20struggle%20with%20spatial%20reasoning%20in%0Astructured%20and%20information-rich%20settings%20such%20as%20transit%20maps%2C%20a%20task%20of%20clear%0Apractical%20and%20scientific%20importance.%20However%2C%20standard%20reinforcement%20learning%0A%28RL%29%20on%20such%20tasks%20is%20impeded%20by%20sparse%20rewards%20and%20unstable%20optimization.%20To%0Aaddress%20this%2C%20we%20first%20construct%20ReasonMap-Plus%2C%20an%20extended%20dataset%20that%0Aintroduces%20dense%20reward%20signals%20through%20Visual%20Question%20Answering%20%28VQA%29%20tasks%2C%0Aenabling%20effective%20cold-start%20training%20of%20fine-grained%20visual%20understanding%0Askills.%20Next%2C%20we%20propose%20RewardMap%2C%20a%20multi-stage%20RL%20framework%20designed%20to%0Aimprove%20both%20visual%20understanding%20and%20reasoning%20capabilities%20of%20MLLMs.%0ARewardMap%20incorporates%20two%20key%20designs.%20First%2C%20we%20introduce%20a%20difficulty-aware%0Areward%20design%20that%20incorporates%20detail%20rewards%2C%20directly%20tackling%20the%20sparse%0Arewards%20while%20providing%20richer%20supervision.%20Second%2C%20we%20propose%20a%20multi-stage%20RL%0Ascheme%20that%20bootstraps%20training%20from%20simple%20perception%20to%20complex%20reasoning%0Atasks%2C%20offering%20a%20more%20effective%20cold-start%20strategy%20than%20conventional%0ASupervised%20Fine-Tuning%20%28SFT%29.%20Experiments%20on%20ReasonMap%20and%20ReasonMap-Plus%0Ademonstrate%20that%20each%20component%20of%20RewardMap%20contributes%20to%20consistent%0Aperformance%20gains%2C%20while%20their%20combination%20yields%20the%20best%20results.%20Moreover%2C%0Amodels%20trained%20with%20RewardMap%20achieve%20an%20average%20improvement%20of%203.47%25%20across%206%0Abenchmarks%20spanning%20spatial%20reasoning%2C%20fine-grained%20visual%20reasoning%2C%20and%0Ageneral%20tasks%20beyond%20transit%20maps%2C%20underscoring%20enhanced%20visual%20understanding%0Aand%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRewardMap%253A%2520Tackling%2520Sparse%2520Rewards%2520in%2520Fine-grained%2520Visual%2520Reasoning%2520via%250A%2520%2520Multi-Stage%2520Reinforcement%2520Learning%26entry.906535625%3DSicheng%2520Feng%2520and%2520Kaiwen%2520Tuo%2520and%2520Song%2520Wang%2520and%2520Lingdong%2520Kong%2520and%2520Jianke%2520Zhu%2520and%2520Huan%2520Wang%26entry.1292438233%3D%2520%2520Fine-grained%2520visual%2520reasoning%2520remains%2520a%2520core%2520challenge%2520for%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529.%2520The%2520recently%2520introduced%2520ReasonMap%2520highlights%2520this%2520gap%250Aby%2520showing%2520that%2520even%2520advanced%2520MLLMs%2520struggle%2520with%2520spatial%2520reasoning%2520in%250Astructured%2520and%2520information-rich%2520settings%2520such%2520as%2520transit%2520maps%252C%2520a%2520task%2520of%2520clear%250Apractical%2520and%2520scientific%2520importance.%2520However%252C%2520standard%2520reinforcement%2520learning%250A%2528RL%2529%2520on%2520such%2520tasks%2520is%2520impeded%2520by%2520sparse%2520rewards%2520and%2520unstable%2520optimization.%2520To%250Aaddress%2520this%252C%2520we%2520first%2520construct%2520ReasonMap-Plus%252C%2520an%2520extended%2520dataset%2520that%250Aintroduces%2520dense%2520reward%2520signals%2520through%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520tasks%252C%250Aenabling%2520effective%2520cold-start%2520training%2520of%2520fine-grained%2520visual%2520understanding%250Askills.%2520Next%252C%2520we%2520propose%2520RewardMap%252C%2520a%2520multi-stage%2520RL%2520framework%2520designed%2520to%250Aimprove%2520both%2520visual%2520understanding%2520and%2520reasoning%2520capabilities%2520of%2520MLLMs.%250ARewardMap%2520incorporates%2520two%2520key%2520designs.%2520First%252C%2520we%2520introduce%2520a%2520difficulty-aware%250Areward%2520design%2520that%2520incorporates%2520detail%2520rewards%252C%2520directly%2520tackling%2520the%2520sparse%250Arewards%2520while%2520providing%2520richer%2520supervision.%2520Second%252C%2520we%2520propose%2520a%2520multi-stage%2520RL%250Ascheme%2520that%2520bootstraps%2520training%2520from%2520simple%2520perception%2520to%2520complex%2520reasoning%250Atasks%252C%2520offering%2520a%2520more%2520effective%2520cold-start%2520strategy%2520than%2520conventional%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529.%2520Experiments%2520on%2520ReasonMap%2520and%2520ReasonMap-Plus%250Ademonstrate%2520that%2520each%2520component%2520of%2520RewardMap%2520contributes%2520to%2520consistent%250Aperformance%2520gains%252C%2520while%2520their%2520combination%2520yields%2520the%2520best%2520results.%2520Moreover%252C%250Amodels%2520trained%2520with%2520RewardMap%2520achieve%2520an%2520average%2520improvement%2520of%25203.47%2525%2520across%25206%250Abenchmarks%2520spanning%2520spatial%2520reasoning%252C%2520fine-grained%2520visual%2520reasoning%252C%2520and%250Ageneral%2520tasks%2520beyond%2520transit%2520maps%252C%2520underscoring%2520enhanced%2520visual%2520understanding%250Aand%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RewardMap%3A%20Tackling%20Sparse%20Rewards%20in%20Fine-grained%20Visual%20Reasoning%20via%0A%20%20Multi-Stage%20Reinforcement%20Learning&entry.906535625=Sicheng%20Feng%20and%20Kaiwen%20Tuo%20and%20Song%20Wang%20and%20Lingdong%20Kong%20and%20Jianke%20Zhu%20and%20Huan%20Wang&entry.1292438233=%20%20Fine-grained%20visual%20reasoning%20remains%20a%20core%20challenge%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20The%20recently%20introduced%20ReasonMap%20highlights%20this%20gap%0Aby%20showing%20that%20even%20advanced%20MLLMs%20struggle%20with%20spatial%20reasoning%20in%0Astructured%20and%20information-rich%20settings%20such%20as%20transit%20maps%2C%20a%20task%20of%20clear%0Apractical%20and%20scientific%20importance.%20However%2C%20standard%20reinforcement%20learning%0A%28RL%29%20on%20such%20tasks%20is%20impeded%20by%20sparse%20rewards%20and%20unstable%20optimization.%20To%0Aaddress%20this%2C%20we%20first%20construct%20ReasonMap-Plus%2C%20an%20extended%20dataset%20that%0Aintroduces%20dense%20reward%20signals%20through%20Visual%20Question%20Answering%20%28VQA%29%20tasks%2C%0Aenabling%20effective%20cold-start%20training%20of%20fine-grained%20visual%20understanding%0Askills.%20Next%2C%20we%20propose%20RewardMap%2C%20a%20multi-stage%20RL%20framework%20designed%20to%0Aimprove%20both%20visual%20understanding%20and%20reasoning%20capabilities%20of%20MLLMs.%0ARewardMap%20incorporates%20two%20key%20designs.%20First%2C%20we%20introduce%20a%20difficulty-aware%0Areward%20design%20that%20incorporates%20detail%20rewards%2C%20directly%20tackling%20the%20sparse%0Arewards%20while%20providing%20richer%20supervision.%20Second%2C%20we%20propose%20a%20multi-stage%20RL%0Ascheme%20that%20bootstraps%20training%20from%20simple%20perception%20to%20complex%20reasoning%0Atasks%2C%20offering%20a%20more%20effective%20cold-start%20strategy%20than%20conventional%0ASupervised%20Fine-Tuning%20%28SFT%29.%20Experiments%20on%20ReasonMap%20and%20ReasonMap-Plus%0Ademonstrate%20that%20each%20component%20of%20RewardMap%20contributes%20to%20consistent%0Aperformance%20gains%2C%20while%20their%20combination%20yields%20the%20best%20results.%20Moreover%2C%0Amodels%20trained%20with%20RewardMap%20achieve%20an%20average%20improvement%20of%203.47%25%20across%206%0Abenchmarks%20spanning%20spatial%20reasoning%2C%20fine-grained%20visual%20reasoning%2C%20and%0Ageneral%20tasks%20beyond%20transit%20maps%2C%20underscoring%20enhanced%20visual%20understanding%0Aand%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02240v1&entry.124074799=Read"},
{"title": "High-Fidelity Speech Enhancement via Discrete Audio Tokens", "author": "Luca A. Lanzend\u00f6rfer and Fr\u00e9d\u00e9ric Berdoz and Antonis Asonitis and Roger Wattenhofer", "abstract": "  Recent autoregressive transformer-based speech enhancement (SE) methods have\nshown promising results by leveraging advanced semantic understanding and\ncontextual modeling of speech. However, these approaches often rely on complex\nmulti-stage pipelines and low sampling rate codecs, limiting them to narrow and\ntask-specific speech enhancement. In this work, we introduce DAC-SE1, a\nsimplified language model-based SE framework leveraging discrete\nhigh-resolution audio representations; DAC-SE1 preserves fine-grained acoustic\ndetails while maintaining semantic coherence. Our experiments show that DAC-SE1\nsurpasses state-of-the-art autoregressive SE methods on both objective\nperceptual metrics and in a MUSHRA human evaluation. We release our codebase\nand model checkpoints to support further research in scalable, unified, and\nhigh-quality speech enhancement.\n", "link": "http://arxiv.org/abs/2510.02187v1", "date": "2025-10-02", "relevancy": 2.0803, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5374}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5192}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Fidelity%20Speech%20Enhancement%20via%20Discrete%20Audio%20Tokens&body=Title%3A%20High-Fidelity%20Speech%20Enhancement%20via%20Discrete%20Audio%20Tokens%0AAuthor%3A%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Fr%C3%A9d%C3%A9ric%20Berdoz%20and%20Antonis%20Asonitis%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20Recent%20autoregressive%20transformer-based%20speech%20enhancement%20%28SE%29%20methods%20have%0Ashown%20promising%20results%20by%20leveraging%20advanced%20semantic%20understanding%20and%0Acontextual%20modeling%20of%20speech.%20However%2C%20these%20approaches%20often%20rely%20on%20complex%0Amulti-stage%20pipelines%20and%20low%20sampling%20rate%20codecs%2C%20limiting%20them%20to%20narrow%20and%0Atask-specific%20speech%20enhancement.%20In%20this%20work%2C%20we%20introduce%20DAC-SE1%2C%20a%0Asimplified%20language%20model-based%20SE%20framework%20leveraging%20discrete%0Ahigh-resolution%20audio%20representations%3B%20DAC-SE1%20preserves%20fine-grained%20acoustic%0Adetails%20while%20maintaining%20semantic%20coherence.%20Our%20experiments%20show%20that%20DAC-SE1%0Asurpasses%20state-of-the-art%20autoregressive%20SE%20methods%20on%20both%20objective%0Aperceptual%20metrics%20and%20in%20a%20MUSHRA%20human%20evaluation.%20We%20release%20our%20codebase%0Aand%20model%20checkpoints%20to%20support%20further%20research%20in%20scalable%2C%20unified%2C%20and%0Ahigh-quality%20speech%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Fidelity%2520Speech%2520Enhancement%2520via%2520Discrete%2520Audio%2520Tokens%26entry.906535625%3DLuca%2520A.%2520Lanzend%25C3%25B6rfer%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Berdoz%2520and%2520Antonis%2520Asonitis%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520Recent%2520autoregressive%2520transformer-based%2520speech%2520enhancement%2520%2528SE%2529%2520methods%2520have%250Ashown%2520promising%2520results%2520by%2520leveraging%2520advanced%2520semantic%2520understanding%2520and%250Acontextual%2520modeling%2520of%2520speech.%2520However%252C%2520these%2520approaches%2520often%2520rely%2520on%2520complex%250Amulti-stage%2520pipelines%2520and%2520low%2520sampling%2520rate%2520codecs%252C%2520limiting%2520them%2520to%2520narrow%2520and%250Atask-specific%2520speech%2520enhancement.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DAC-SE1%252C%2520a%250Asimplified%2520language%2520model-based%2520SE%2520framework%2520leveraging%2520discrete%250Ahigh-resolution%2520audio%2520representations%253B%2520DAC-SE1%2520preserves%2520fine-grained%2520acoustic%250Adetails%2520while%2520maintaining%2520semantic%2520coherence.%2520Our%2520experiments%2520show%2520that%2520DAC-SE1%250Asurpasses%2520state-of-the-art%2520autoregressive%2520SE%2520methods%2520on%2520both%2520objective%250Aperceptual%2520metrics%2520and%2520in%2520a%2520MUSHRA%2520human%2520evaluation.%2520We%2520release%2520our%2520codebase%250Aand%2520model%2520checkpoints%2520to%2520support%2520further%2520research%2520in%2520scalable%252C%2520unified%252C%2520and%250Ahigh-quality%2520speech%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20Speech%20Enhancement%20via%20Discrete%20Audio%20Tokens&entry.906535625=Luca%20A.%20Lanzend%C3%B6rfer%20and%20Fr%C3%A9d%C3%A9ric%20Berdoz%20and%20Antonis%20Asonitis%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20Recent%20autoregressive%20transformer-based%20speech%20enhancement%20%28SE%29%20methods%20have%0Ashown%20promising%20results%20by%20leveraging%20advanced%20semantic%20understanding%20and%0Acontextual%20modeling%20of%20speech.%20However%2C%20these%20approaches%20often%20rely%20on%20complex%0Amulti-stage%20pipelines%20and%20low%20sampling%20rate%20codecs%2C%20limiting%20them%20to%20narrow%20and%0Atask-specific%20speech%20enhancement.%20In%20this%20work%2C%20we%20introduce%20DAC-SE1%2C%20a%0Asimplified%20language%20model-based%20SE%20framework%20leveraging%20discrete%0Ahigh-resolution%20audio%20representations%3B%20DAC-SE1%20preserves%20fine-grained%20acoustic%0Adetails%20while%20maintaining%20semantic%20coherence.%20Our%20experiments%20show%20that%20DAC-SE1%0Asurpasses%20state-of-the-art%20autoregressive%20SE%20methods%20on%20both%20objective%0Aperceptual%20metrics%20and%20in%20a%20MUSHRA%20human%20evaluation.%20We%20release%20our%20codebase%0Aand%20model%20checkpoints%20to%20support%20further%20research%20in%20scalable%2C%20unified%2C%20and%0Ahigh-quality%20speech%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02187v1&entry.124074799=Read"},
{"title": "Flatness-Aware Stochastic Gradient Langevin Dynamics", "author": "Stefano Bruno and Youngsik Hwang and Jaehyeon An and Sotirios Sabanis and Dong-Young Lim", "abstract": "  Generalization in deep learning is closely tied to the pursuit of flat minima\nin the loss landscape, yet classical Stochastic Gradient Langevin Dynamics\n(SGLD) offers no mechanism to bias its dynamics toward such low-curvature\nsolutions. This work introduces Flatness-Aware Stochastic Gradient Langevin\nDynamics (fSGLD), designed to efficiently and provably seek flat minima in\nhigh-dimensional nonconvex optimization problems. At each iteration, fSGLD uses\nthe stochastic gradient evaluated at parameters perturbed by isotropic Gaussian\nnoise, commonly referred to as Random Weight Perturbation (RWP), thereby\noptimizing a randomized-smoothing objective that implicitly captures curvature\ninformation. Leveraging these properties, we prove that the invariant measure\nof fSGLD stays close to a stationary measure concentrated on the global\nminimizers of a loss function regularized by the Hessian trace whenever the\ninverse temperature and the scale of random weight perturbation are properly\ncoupled. This result provides a rigorous theoretical explanation for the\nbenefits of random weight perturbation. In particular, we establish\nnon-asymptotic convergence guarantees in Wasserstein distance with the best\nknown rate and derive an excess-risk bound for the Hessian-trace regularized\nobjective. Extensive experiments on noisy-label and large-scale vision tasks,\nin both training-from-scratch and fine-tuning settings, demonstrate that fSGLD\nachieves superior or comparable generalization and robustness to baseline\nalgorithms while maintaining the computational cost of SGD, about half that of\nSAM. Hessian-spectrum analysis further confirms that fSGLD converges to\nsignificantly flatter minima.\n", "link": "http://arxiv.org/abs/2510.02174v1", "date": "2025-10-02", "relevancy": 2.0737, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5258}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5181}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flatness-Aware%20Stochastic%20Gradient%20Langevin%20Dynamics&body=Title%3A%20Flatness-Aware%20Stochastic%20Gradient%20Langevin%20Dynamics%0AAuthor%3A%20Stefano%20Bruno%20and%20Youngsik%20Hwang%20and%20Jaehyeon%20An%20and%20Sotirios%20Sabanis%20and%20Dong-Young%20Lim%0AAbstract%3A%20%20%20Generalization%20in%20deep%20learning%20is%20closely%20tied%20to%20the%20pursuit%20of%20flat%20minima%0Ain%20the%20loss%20landscape%2C%20yet%20classical%20Stochastic%20Gradient%20Langevin%20Dynamics%0A%28SGLD%29%20offers%20no%20mechanism%20to%20bias%20its%20dynamics%20toward%20such%20low-curvature%0Asolutions.%20This%20work%20introduces%20Flatness-Aware%20Stochastic%20Gradient%20Langevin%0ADynamics%20%28fSGLD%29%2C%20designed%20to%20efficiently%20and%20provably%20seek%20flat%20minima%20in%0Ahigh-dimensional%20nonconvex%20optimization%20problems.%20At%20each%20iteration%2C%20fSGLD%20uses%0Athe%20stochastic%20gradient%20evaluated%20at%20parameters%20perturbed%20by%20isotropic%20Gaussian%0Anoise%2C%20commonly%20referred%20to%20as%20Random%20Weight%20Perturbation%20%28RWP%29%2C%20thereby%0Aoptimizing%20a%20randomized-smoothing%20objective%20that%20implicitly%20captures%20curvature%0Ainformation.%20Leveraging%20these%20properties%2C%20we%20prove%20that%20the%20invariant%20measure%0Aof%20fSGLD%20stays%20close%20to%20a%20stationary%20measure%20concentrated%20on%20the%20global%0Aminimizers%20of%20a%20loss%20function%20regularized%20by%20the%20Hessian%20trace%20whenever%20the%0Ainverse%20temperature%20and%20the%20scale%20of%20random%20weight%20perturbation%20are%20properly%0Acoupled.%20This%20result%20provides%20a%20rigorous%20theoretical%20explanation%20for%20the%0Abenefits%20of%20random%20weight%20perturbation.%20In%20particular%2C%20we%20establish%0Anon-asymptotic%20convergence%20guarantees%20in%20Wasserstein%20distance%20with%20the%20best%0Aknown%20rate%20and%20derive%20an%20excess-risk%20bound%20for%20the%20Hessian-trace%20regularized%0Aobjective.%20Extensive%20experiments%20on%20noisy-label%20and%20large-scale%20vision%20tasks%2C%0Ain%20both%20training-from-scratch%20and%20fine-tuning%20settings%2C%20demonstrate%20that%20fSGLD%0Aachieves%20superior%20or%20comparable%20generalization%20and%20robustness%20to%20baseline%0Aalgorithms%20while%20maintaining%20the%20computational%20cost%20of%20SGD%2C%20about%20half%20that%20of%0ASAM.%20Hessian-spectrum%20analysis%20further%20confirms%20that%20fSGLD%20converges%20to%0Asignificantly%20flatter%20minima.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlatness-Aware%2520Stochastic%2520Gradient%2520Langevin%2520Dynamics%26entry.906535625%3DStefano%2520Bruno%2520and%2520Youngsik%2520Hwang%2520and%2520Jaehyeon%2520An%2520and%2520Sotirios%2520Sabanis%2520and%2520Dong-Young%2520Lim%26entry.1292438233%3D%2520%2520Generalization%2520in%2520deep%2520learning%2520is%2520closely%2520tied%2520to%2520the%2520pursuit%2520of%2520flat%2520minima%250Ain%2520the%2520loss%2520landscape%252C%2520yet%2520classical%2520Stochastic%2520Gradient%2520Langevin%2520Dynamics%250A%2528SGLD%2529%2520offers%2520no%2520mechanism%2520to%2520bias%2520its%2520dynamics%2520toward%2520such%2520low-curvature%250Asolutions.%2520This%2520work%2520introduces%2520Flatness-Aware%2520Stochastic%2520Gradient%2520Langevin%250ADynamics%2520%2528fSGLD%2529%252C%2520designed%2520to%2520efficiently%2520and%2520provably%2520seek%2520flat%2520minima%2520in%250Ahigh-dimensional%2520nonconvex%2520optimization%2520problems.%2520At%2520each%2520iteration%252C%2520fSGLD%2520uses%250Athe%2520stochastic%2520gradient%2520evaluated%2520at%2520parameters%2520perturbed%2520by%2520isotropic%2520Gaussian%250Anoise%252C%2520commonly%2520referred%2520to%2520as%2520Random%2520Weight%2520Perturbation%2520%2528RWP%2529%252C%2520thereby%250Aoptimizing%2520a%2520randomized-smoothing%2520objective%2520that%2520implicitly%2520captures%2520curvature%250Ainformation.%2520Leveraging%2520these%2520properties%252C%2520we%2520prove%2520that%2520the%2520invariant%2520measure%250Aof%2520fSGLD%2520stays%2520close%2520to%2520a%2520stationary%2520measure%2520concentrated%2520on%2520the%2520global%250Aminimizers%2520of%2520a%2520loss%2520function%2520regularized%2520by%2520the%2520Hessian%2520trace%2520whenever%2520the%250Ainverse%2520temperature%2520and%2520the%2520scale%2520of%2520random%2520weight%2520perturbation%2520are%2520properly%250Acoupled.%2520This%2520result%2520provides%2520a%2520rigorous%2520theoretical%2520explanation%2520for%2520the%250Abenefits%2520of%2520random%2520weight%2520perturbation.%2520In%2520particular%252C%2520we%2520establish%250Anon-asymptotic%2520convergence%2520guarantees%2520in%2520Wasserstein%2520distance%2520with%2520the%2520best%250Aknown%2520rate%2520and%2520derive%2520an%2520excess-risk%2520bound%2520for%2520the%2520Hessian-trace%2520regularized%250Aobjective.%2520Extensive%2520experiments%2520on%2520noisy-label%2520and%2520large-scale%2520vision%2520tasks%252C%250Ain%2520both%2520training-from-scratch%2520and%2520fine-tuning%2520settings%252C%2520demonstrate%2520that%2520fSGLD%250Aachieves%2520superior%2520or%2520comparable%2520generalization%2520and%2520robustness%2520to%2520baseline%250Aalgorithms%2520while%2520maintaining%2520the%2520computational%2520cost%2520of%2520SGD%252C%2520about%2520half%2520that%2520of%250ASAM.%2520Hessian-spectrum%2520analysis%2520further%2520confirms%2520that%2520fSGLD%2520converges%2520to%250Asignificantly%2520flatter%2520minima.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flatness-Aware%20Stochastic%20Gradient%20Langevin%20Dynamics&entry.906535625=Stefano%20Bruno%20and%20Youngsik%20Hwang%20and%20Jaehyeon%20An%20and%20Sotirios%20Sabanis%20and%20Dong-Young%20Lim&entry.1292438233=%20%20Generalization%20in%20deep%20learning%20is%20closely%20tied%20to%20the%20pursuit%20of%20flat%20minima%0Ain%20the%20loss%20landscape%2C%20yet%20classical%20Stochastic%20Gradient%20Langevin%20Dynamics%0A%28SGLD%29%20offers%20no%20mechanism%20to%20bias%20its%20dynamics%20toward%20such%20low-curvature%0Asolutions.%20This%20work%20introduces%20Flatness-Aware%20Stochastic%20Gradient%20Langevin%0ADynamics%20%28fSGLD%29%2C%20designed%20to%20efficiently%20and%20provably%20seek%20flat%20minima%20in%0Ahigh-dimensional%20nonconvex%20optimization%20problems.%20At%20each%20iteration%2C%20fSGLD%20uses%0Athe%20stochastic%20gradient%20evaluated%20at%20parameters%20perturbed%20by%20isotropic%20Gaussian%0Anoise%2C%20commonly%20referred%20to%20as%20Random%20Weight%20Perturbation%20%28RWP%29%2C%20thereby%0Aoptimizing%20a%20randomized-smoothing%20objective%20that%20implicitly%20captures%20curvature%0Ainformation.%20Leveraging%20these%20properties%2C%20we%20prove%20that%20the%20invariant%20measure%0Aof%20fSGLD%20stays%20close%20to%20a%20stationary%20measure%20concentrated%20on%20the%20global%0Aminimizers%20of%20a%20loss%20function%20regularized%20by%20the%20Hessian%20trace%20whenever%20the%0Ainverse%20temperature%20and%20the%20scale%20of%20random%20weight%20perturbation%20are%20properly%0Acoupled.%20This%20result%20provides%20a%20rigorous%20theoretical%20explanation%20for%20the%0Abenefits%20of%20random%20weight%20perturbation.%20In%20particular%2C%20we%20establish%0Anon-asymptotic%20convergence%20guarantees%20in%20Wasserstein%20distance%20with%20the%20best%0Aknown%20rate%20and%20derive%20an%20excess-risk%20bound%20for%20the%20Hessian-trace%20regularized%0Aobjective.%20Extensive%20experiments%20on%20noisy-label%20and%20large-scale%20vision%20tasks%2C%0Ain%20both%20training-from-scratch%20and%20fine-tuning%20settings%2C%20demonstrate%20that%20fSGLD%0Aachieves%20superior%20or%20comparable%20generalization%20and%20robustness%20to%20baseline%0Aalgorithms%20while%20maintaining%20the%20computational%20cost%20of%20SGD%2C%20about%20half%20that%20of%0ASAM.%20Hessian-spectrum%20analysis%20further%20confirms%20that%20fSGLD%20converges%20to%0Asignificantly%20flatter%20minima.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02174v1&entry.124074799=Read"},
{"title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement\n  Learning", "author": "Silvia Sapora and Devon Hjelm and Alexander Toshev and Omar Attia and Bogdan Mazoure", "abstract": "  Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.\n", "link": "http://arxiv.org/abs/2510.02180v1", "date": "2025-10-02", "relevancy": 2.0685, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5359}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRACE%3A%20A%20Language%20Model%20Framework%20for%20Explainable%20Inverse%20Reinforcement%0A%20%20Learning&body=Title%3A%20GRACE%3A%20A%20Language%20Model%20Framework%20for%20Explainable%20Inverse%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Silvia%20Sapora%20and%20Devon%20Hjelm%20and%20Alexander%20Toshev%20and%20Omar%20Attia%20and%20Bogdan%20Mazoure%0AAbstract%3A%20%20%20Inverse%20Reinforcement%20Learning%20aims%20to%20recover%20reward%20models%20from%20expert%0Ademonstrations%2C%20but%20traditional%20methods%20yield%20%22black-box%22%20models%20that%20are%0Adifficult%20to%20interpret%20and%20debug.%20In%20this%20work%2C%20we%20introduce%20GRACE%20%28Generating%0ARewards%20As%20CodE%29%2C%20a%20method%20for%20using%20Large%20Language%20Models%20within%20an%0Aevolutionary%20search%20to%20reverse-engineer%20an%20interpretable%2C%20code-based%20reward%0Afunction%20directly%20from%20expert%20trajectories.%20The%20resulting%20reward%20function%20is%0Aexecutable%20code%20that%20can%20be%20inspected%20and%20verified.%20We%20empirically%20validate%0AGRACE%20on%20the%20BabyAI%20and%20AndroidWorld%20benchmarks%2C%20where%20it%20efficiently%20learns%0Ahighly%20accurate%20rewards%2C%20even%20in%20complex%2C%20multi-task%20settings.%20Further%2C%20we%0Ademonstrate%20that%20the%20resulting%20reward%20leads%20to%20strong%20policies%2C%20compared%20to%0Aboth%20competitive%20Imitation%20Learning%20and%20online%20RL%20approaches%20with%20ground-truth%0Arewards.%20Finally%2C%20we%20show%20that%20GRACE%20is%20able%20to%20build%20complex%20reward%20APIs%20in%0Amulti-task%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRACE%253A%2520A%2520Language%2520Model%2520Framework%2520for%2520Explainable%2520Inverse%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DSilvia%2520Sapora%2520and%2520Devon%2520Hjelm%2520and%2520Alexander%2520Toshev%2520and%2520Omar%2520Attia%2520and%2520Bogdan%2520Mazoure%26entry.1292438233%3D%2520%2520Inverse%2520Reinforcement%2520Learning%2520aims%2520to%2520recover%2520reward%2520models%2520from%2520expert%250Ademonstrations%252C%2520but%2520traditional%2520methods%2520yield%2520%2522black-box%2522%2520models%2520that%2520are%250Adifficult%2520to%2520interpret%2520and%2520debug.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GRACE%2520%2528Generating%250ARewards%2520As%2520CodE%2529%252C%2520a%2520method%2520for%2520using%2520Large%2520Language%2520Models%2520within%2520an%250Aevolutionary%2520search%2520to%2520reverse-engineer%2520an%2520interpretable%252C%2520code-based%2520reward%250Afunction%2520directly%2520from%2520expert%2520trajectories.%2520The%2520resulting%2520reward%2520function%2520is%250Aexecutable%2520code%2520that%2520can%2520be%2520inspected%2520and%2520verified.%2520We%2520empirically%2520validate%250AGRACE%2520on%2520the%2520BabyAI%2520and%2520AndroidWorld%2520benchmarks%252C%2520where%2520it%2520efficiently%2520learns%250Ahighly%2520accurate%2520rewards%252C%2520even%2520in%2520complex%252C%2520multi-task%2520settings.%2520Further%252C%2520we%250Ademonstrate%2520that%2520the%2520resulting%2520reward%2520leads%2520to%2520strong%2520policies%252C%2520compared%2520to%250Aboth%2520competitive%2520Imitation%2520Learning%2520and%2520online%2520RL%2520approaches%2520with%2520ground-truth%250Arewards.%2520Finally%252C%2520we%2520show%2520that%2520GRACE%2520is%2520able%2520to%2520build%2520complex%2520reward%2520APIs%2520in%250Amulti-task%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRACE%3A%20A%20Language%20Model%20Framework%20for%20Explainable%20Inverse%20Reinforcement%0A%20%20Learning&entry.906535625=Silvia%20Sapora%20and%20Devon%20Hjelm%20and%20Alexander%20Toshev%20and%20Omar%20Attia%20and%20Bogdan%20Mazoure&entry.1292438233=%20%20Inverse%20Reinforcement%20Learning%20aims%20to%20recover%20reward%20models%20from%20expert%0Ademonstrations%2C%20but%20traditional%20methods%20yield%20%22black-box%22%20models%20that%20are%0Adifficult%20to%20interpret%20and%20debug.%20In%20this%20work%2C%20we%20introduce%20GRACE%20%28Generating%0ARewards%20As%20CodE%29%2C%20a%20method%20for%20using%20Large%20Language%20Models%20within%20an%0Aevolutionary%20search%20to%20reverse-engineer%20an%20interpretable%2C%20code-based%20reward%0Afunction%20directly%20from%20expert%20trajectories.%20The%20resulting%20reward%20function%20is%0Aexecutable%20code%20that%20can%20be%20inspected%20and%20verified.%20We%20empirically%20validate%0AGRACE%20on%20the%20BabyAI%20and%20AndroidWorld%20benchmarks%2C%20where%20it%20efficiently%20learns%0Ahighly%20accurate%20rewards%2C%20even%20in%20complex%2C%20multi-task%20settings.%20Further%2C%20we%0Ademonstrate%20that%20the%20resulting%20reward%20leads%20to%20strong%20policies%2C%20compared%20to%0Aboth%20competitive%20Imitation%20Learning%20and%20online%20RL%20approaches%20with%20ground-truth%0Arewards.%20Finally%2C%20we%20show%20that%20GRACE%20is%20able%20to%20build%20complex%20reward%20APIs%20in%0Amulti-task%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02180v1&entry.124074799=Read"},
{"title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports", "author": "Yang Yao and Yixu Wang and Yuxuan Zhang and Yi Lu and Tianle Gu and Lingyu Li and Dingyi Zhao and Keming Wu and Haozhe Wang and Ping Nie and Yan Teng and Yingchun Wang", "abstract": "  Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.\n", "link": "http://arxiv.org/abs/2510.02190v1", "date": "2025-10-02", "relevancy": 2.0606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Rigorous%20Benchmark%20with%20Multidimensional%20Evaluation%20for%20Deep%20Research%0A%20%20Agents%3A%20From%20Answers%20to%20Reports&body=Title%3A%20A%20Rigorous%20Benchmark%20with%20Multidimensional%20Evaluation%20for%20Deep%20Research%0A%20%20Agents%3A%20From%20Answers%20to%20Reports%0AAuthor%3A%20Yang%20Yao%20and%20Yixu%20Wang%20and%20Yuxuan%20Zhang%20and%20Yi%20Lu%20and%20Tianle%20Gu%20and%20Lingyu%20Li%20and%20Dingyi%20Zhao%20and%20Keming%20Wu%20and%20Haozhe%20Wang%20and%20Ping%20Nie%20and%20Yan%20Teng%20and%20Yingchun%20Wang%0AAbstract%3A%20%20%20Artificial%20intelligence%20is%20undergoing%20the%20paradigm%20shift%20from%20closed%20language%0Amodels%20to%20interconnected%20agent%20systems%20capable%20of%20external%20perception%20and%0Ainformation%20integration.%20As%20a%20representative%20embodiment%2C%20Deep%20Research%20Agents%0A%28DRAs%29%20systematically%20exhibit%20the%20capabilities%20for%20task%20decomposition%2C%0Across-source%20retrieval%2C%20multi-stage%20reasoning%2C%20and%20structured%20output%2C%20which%0Amarkedly%20enhance%20performance%20on%20complex%20and%20open-ended%20tasks.%20However%2C%20existing%0Abenchmarks%20remain%20deficient%20in%20evaluation%20dimensions%2C%20response%20formatting%2C%20and%0Ascoring%20mechanisms%2C%20limiting%20their%20capacity%20to%20assess%20such%20systems%20effectively.%0AThis%20paper%20introduces%20a%20rigorous%20benchmark%20and%20a%20multidimensional%20evaluation%0Aframework%20tailored%20to%20DRAs%20and%20report-style%20responses.%20The%20benchmark%20comprises%0A214%20expert-curated%20challenging%20queries%20distributed%20across%2010%20broad%20thematic%0Adomains%2C%20each%20accompanied%20by%20manually%20constructed%20reference%20bundles%20to%20support%0Acomposite%20evaluation.%20The%20framework%20enables%20comprehensive%20evaluation%20of%0Along-form%20reports%20generated%20by%20DRAs%2C%20incorporating%20integrated%20scoring%20metrics%0Afor%20semantic%20quality%2C%20topical%20focus%2C%20and%20retrieval%20trustworthiness.%20Extensive%0Aexperimentation%20confirms%20the%20superior%20performance%20of%20mainstream%20DRAs%20over%0Aweb-search-tool-augmented%20reasoning%20models%2C%20yet%20reveals%20considerable%20scope%20for%0Afurther%20improvement.%20This%20study%20provides%20a%20robust%20foundation%20for%20capability%0Aassessment%2C%20architectural%20refinement%2C%20and%20paradigm%20advancement%20in%20DRA%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Rigorous%2520Benchmark%2520with%2520Multidimensional%2520Evaluation%2520for%2520Deep%2520Research%250A%2520%2520Agents%253A%2520From%2520Answers%2520to%2520Reports%26entry.906535625%3DYang%2520Yao%2520and%2520Yixu%2520Wang%2520and%2520Yuxuan%2520Zhang%2520and%2520Yi%2520Lu%2520and%2520Tianle%2520Gu%2520and%2520Lingyu%2520Li%2520and%2520Dingyi%2520Zhao%2520and%2520Keming%2520Wu%2520and%2520Haozhe%2520Wang%2520and%2520Ping%2520Nie%2520and%2520Yan%2520Teng%2520and%2520Yingchun%2520Wang%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520is%2520undergoing%2520the%2520paradigm%2520shift%2520from%2520closed%2520language%250Amodels%2520to%2520interconnected%2520agent%2520systems%2520capable%2520of%2520external%2520perception%2520and%250Ainformation%2520integration.%2520As%2520a%2520representative%2520embodiment%252C%2520Deep%2520Research%2520Agents%250A%2528DRAs%2529%2520systematically%2520exhibit%2520the%2520capabilities%2520for%2520task%2520decomposition%252C%250Across-source%2520retrieval%252C%2520multi-stage%2520reasoning%252C%2520and%2520structured%2520output%252C%2520which%250Amarkedly%2520enhance%2520performance%2520on%2520complex%2520and%2520open-ended%2520tasks.%2520However%252C%2520existing%250Abenchmarks%2520remain%2520deficient%2520in%2520evaluation%2520dimensions%252C%2520response%2520formatting%252C%2520and%250Ascoring%2520mechanisms%252C%2520limiting%2520their%2520capacity%2520to%2520assess%2520such%2520systems%2520effectively.%250AThis%2520paper%2520introduces%2520a%2520rigorous%2520benchmark%2520and%2520a%2520multidimensional%2520evaluation%250Aframework%2520tailored%2520to%2520DRAs%2520and%2520report-style%2520responses.%2520The%2520benchmark%2520comprises%250A214%2520expert-curated%2520challenging%2520queries%2520distributed%2520across%252010%2520broad%2520thematic%250Adomains%252C%2520each%2520accompanied%2520by%2520manually%2520constructed%2520reference%2520bundles%2520to%2520support%250Acomposite%2520evaluation.%2520The%2520framework%2520enables%2520comprehensive%2520evaluation%2520of%250Along-form%2520reports%2520generated%2520by%2520DRAs%252C%2520incorporating%2520integrated%2520scoring%2520metrics%250Afor%2520semantic%2520quality%252C%2520topical%2520focus%252C%2520and%2520retrieval%2520trustworthiness.%2520Extensive%250Aexperimentation%2520confirms%2520the%2520superior%2520performance%2520of%2520mainstream%2520DRAs%2520over%250Aweb-search-tool-augmented%2520reasoning%2520models%252C%2520yet%2520reveals%2520considerable%2520scope%2520for%250Afurther%2520improvement.%2520This%2520study%2520provides%2520a%2520robust%2520foundation%2520for%2520capability%250Aassessment%252C%2520architectural%2520refinement%252C%2520and%2520paradigm%2520advancement%2520in%2520DRA%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Rigorous%20Benchmark%20with%20Multidimensional%20Evaluation%20for%20Deep%20Research%0A%20%20Agents%3A%20From%20Answers%20to%20Reports&entry.906535625=Yang%20Yao%20and%20Yixu%20Wang%20and%20Yuxuan%20Zhang%20and%20Yi%20Lu%20and%20Tianle%20Gu%20and%20Lingyu%20Li%20and%20Dingyi%20Zhao%20and%20Keming%20Wu%20and%20Haozhe%20Wang%20and%20Ping%20Nie%20and%20Yan%20Teng%20and%20Yingchun%20Wang&entry.1292438233=%20%20Artificial%20intelligence%20is%20undergoing%20the%20paradigm%20shift%20from%20closed%20language%0Amodels%20to%20interconnected%20agent%20systems%20capable%20of%20external%20perception%20and%0Ainformation%20integration.%20As%20a%20representative%20embodiment%2C%20Deep%20Research%20Agents%0A%28DRAs%29%20systematically%20exhibit%20the%20capabilities%20for%20task%20decomposition%2C%0Across-source%20retrieval%2C%20multi-stage%20reasoning%2C%20and%20structured%20output%2C%20which%0Amarkedly%20enhance%20performance%20on%20complex%20and%20open-ended%20tasks.%20However%2C%20existing%0Abenchmarks%20remain%20deficient%20in%20evaluation%20dimensions%2C%20response%20formatting%2C%20and%0Ascoring%20mechanisms%2C%20limiting%20their%20capacity%20to%20assess%20such%20systems%20effectively.%0AThis%20paper%20introduces%20a%20rigorous%20benchmark%20and%20a%20multidimensional%20evaluation%0Aframework%20tailored%20to%20DRAs%20and%20report-style%20responses.%20The%20benchmark%20comprises%0A214%20expert-curated%20challenging%20queries%20distributed%20across%2010%20broad%20thematic%0Adomains%2C%20each%20accompanied%20by%20manually%20constructed%20reference%20bundles%20to%20support%0Acomposite%20evaluation.%20The%20framework%20enables%20comprehensive%20evaluation%20of%0Along-form%20reports%20generated%20by%20DRAs%2C%20incorporating%20integrated%20scoring%20metrics%0Afor%20semantic%20quality%2C%20topical%20focus%2C%20and%20retrieval%20trustworthiness.%20Extensive%0Aexperimentation%20confirms%20the%20superior%20performance%20of%20mainstream%20DRAs%20over%0Aweb-search-tool-augmented%20reasoning%20models%2C%20yet%20reveals%20considerable%20scope%20for%0Afurther%20improvement.%20This%20study%20provides%20a%20robust%20foundation%20for%20capability%0Aassessment%2C%20architectural%20refinement%2C%20and%20paradigm%20advancement%20in%20DRA%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02190v1&entry.124074799=Read"},
{"title": "Learning to Weight Parameters for Training Data Attribution", "author": "Shuangqi Li and Hieu Le and Jingyi Xu and Mathieu Salzmann", "abstract": "  We study gradient-based data attribution, aiming to identify which training\nexamples most influence a given output. Existing methods for this task either\ntreat network parameters uniformly or rely on implicit weighting derived from\nHessian approximations, which do not fully model functional heterogeneity of\nnetwork parameters. To address this, we propose a method to explicitly learn\nparameter importance weights directly from data, without requiring annotated\nlabels. Our approach improves attribution accuracy across diverse tasks,\nincluding image classification, language modeling, and diffusion, and enables\nfine-grained attribution for concepts like subject and style.\n", "link": "http://arxiv.org/abs/2506.05647v2", "date": "2025-10-02", "relevancy": 2.0245, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5169}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5055}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Weight%20Parameters%20for%20Training%20Data%20Attribution&body=Title%3A%20Learning%20to%20Weight%20Parameters%20for%20Training%20Data%20Attribution%0AAuthor%3A%20Shuangqi%20Li%20and%20Hieu%20Le%20and%20Jingyi%20Xu%20and%20Mathieu%20Salzmann%0AAbstract%3A%20%20%20We%20study%20gradient-based%20data%20attribution%2C%20aiming%20to%20identify%20which%20training%0Aexamples%20most%20influence%20a%20given%20output.%20Existing%20methods%20for%20this%20task%20either%0Atreat%20network%20parameters%20uniformly%20or%20rely%20on%20implicit%20weighting%20derived%20from%0AHessian%20approximations%2C%20which%20do%20not%20fully%20model%20functional%20heterogeneity%20of%0Anetwork%20parameters.%20To%20address%20this%2C%20we%20propose%20a%20method%20to%20explicitly%20learn%0Aparameter%20importance%20weights%20directly%20from%20data%2C%20without%20requiring%20annotated%0Alabels.%20Our%20approach%20improves%20attribution%20accuracy%20across%20diverse%20tasks%2C%0Aincluding%20image%20classification%2C%20language%20modeling%2C%20and%20diffusion%2C%20and%20enables%0Afine-grained%20attribution%20for%20concepts%20like%20subject%20and%20style.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Weight%2520Parameters%2520for%2520Training%2520Data%2520Attribution%26entry.906535625%3DShuangqi%2520Li%2520and%2520Hieu%2520Le%2520and%2520Jingyi%2520Xu%2520and%2520Mathieu%2520Salzmann%26entry.1292438233%3D%2520%2520We%2520study%2520gradient-based%2520data%2520attribution%252C%2520aiming%2520to%2520identify%2520which%2520training%250Aexamples%2520most%2520influence%2520a%2520given%2520output.%2520Existing%2520methods%2520for%2520this%2520task%2520either%250Atreat%2520network%2520parameters%2520uniformly%2520or%2520rely%2520on%2520implicit%2520weighting%2520derived%2520from%250AHessian%2520approximations%252C%2520which%2520do%2520not%2520fully%2520model%2520functional%2520heterogeneity%2520of%250Anetwork%2520parameters.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520method%2520to%2520explicitly%2520learn%250Aparameter%2520importance%2520weights%2520directly%2520from%2520data%252C%2520without%2520requiring%2520annotated%250Alabels.%2520Our%2520approach%2520improves%2520attribution%2520accuracy%2520across%2520diverse%2520tasks%252C%250Aincluding%2520image%2520classification%252C%2520language%2520modeling%252C%2520and%2520diffusion%252C%2520and%2520enables%250Afine-grained%2520attribution%2520for%2520concepts%2520like%2520subject%2520and%2520style.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Weight%20Parameters%20for%20Training%20Data%20Attribution&entry.906535625=Shuangqi%20Li%20and%20Hieu%20Le%20and%20Jingyi%20Xu%20and%20Mathieu%20Salzmann&entry.1292438233=%20%20We%20study%20gradient-based%20data%20attribution%2C%20aiming%20to%20identify%20which%20training%0Aexamples%20most%20influence%20a%20given%20output.%20Existing%20methods%20for%20this%20task%20either%0Atreat%20network%20parameters%20uniformly%20or%20rely%20on%20implicit%20weighting%20derived%20from%0AHessian%20approximations%2C%20which%20do%20not%20fully%20model%20functional%20heterogeneity%20of%0Anetwork%20parameters.%20To%20address%20this%2C%20we%20propose%20a%20method%20to%20explicitly%20learn%0Aparameter%20importance%20weights%20directly%20from%20data%2C%20without%20requiring%20annotated%0Alabels.%20Our%20approach%20improves%20attribution%20accuracy%20across%20diverse%20tasks%2C%0Aincluding%20image%20classification%2C%20language%20modeling%2C%20and%20diffusion%2C%20and%20enables%0Afine-grained%20attribution%20for%20concepts%20like%20subject%20and%20style.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05647v2&entry.124074799=Read"},
{"title": "Drop-Muon: Update Less, Converge Faster", "author": "Kaja Gruntkowska and Yassine Maziane and Zheng Qu and Peter Richt\u00e1rik", "abstract": "  Conventional wisdom in deep learning optimization dictates updating all\nlayers at every step-a principle followed by all recent state-of-the-art\noptimizers such as Muon. In this work, we challenge this assumption, showing\nthat full-network updates can be fundamentally suboptimal, both in theory and\nin practice. We introduce a non-Euclidean Randomized Progressive Training\nmethod-Drop-Muon-a simple yet powerful framework that updates only a subset of\nlayers per step according to a randomized schedule, combining the efficiency of\nprogressive training with layer-specific non-Euclidean updates for top-tier\nperformance. We provide rigorous convergence guarantees under both layer-wise\nsmoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and\nstochastic gradient settings, marking the first such results for progressive\ntraining in the stochastic and non-smooth regime. Our cost analysis further\nreveals that full-network updates are not optimal unless a very specific\nrelationship between layer smoothness constants holds. Through controlled CNN\nexperiments, we empirically demonstrate that Drop-Muon consistently outperforms\nfull-network Muon, achieving the same accuracy up to $1.4\\times$ faster in\nwall-clock time. Together, our results suggest a shift in how large-scale\nmodels can be efficiently trained, challenging the status quo and offering a\nhighly efficient, theoretically grounded alternative to full-network updates.\n", "link": "http://arxiv.org/abs/2510.02239v1", "date": "2025-10-02", "relevancy": 2.023, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5103}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5097}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drop-Muon%3A%20Update%20Less%2C%20Converge%20Faster&body=Title%3A%20Drop-Muon%3A%20Update%20Less%2C%20Converge%20Faster%0AAuthor%3A%20Kaja%20Gruntkowska%20and%20Yassine%20Maziane%20and%20Zheng%20Qu%20and%20Peter%20Richt%C3%A1rik%0AAbstract%3A%20%20%20Conventional%20wisdom%20in%20deep%20learning%20optimization%20dictates%20updating%20all%0Alayers%20at%20every%20step-a%20principle%20followed%20by%20all%20recent%20state-of-the-art%0Aoptimizers%20such%20as%20Muon.%20In%20this%20work%2C%20we%20challenge%20this%20assumption%2C%20showing%0Athat%20full-network%20updates%20can%20be%20fundamentally%20suboptimal%2C%20both%20in%20theory%20and%0Ain%20practice.%20We%20introduce%20a%20non-Euclidean%20Randomized%20Progressive%20Training%0Amethod-Drop-Muon-a%20simple%20yet%20powerful%20framework%20that%20updates%20only%20a%20subset%20of%0Alayers%20per%20step%20according%20to%20a%20randomized%20schedule%2C%20combining%20the%20efficiency%20of%0Aprogressive%20training%20with%20layer-specific%20non-Euclidean%20updates%20for%20top-tier%0Aperformance.%20We%20provide%20rigorous%20convergence%20guarantees%20under%20both%20layer-wise%0Asmoothness%20and%20layer-wise%20%24%28L%5E0%2C%20L%5E1%29%24-smoothness%2C%20covering%20deterministic%20and%0Astochastic%20gradient%20settings%2C%20marking%20the%20first%20such%20results%20for%20progressive%0Atraining%20in%20the%20stochastic%20and%20non-smooth%20regime.%20Our%20cost%20analysis%20further%0Areveals%20that%20full-network%20updates%20are%20not%20optimal%20unless%20a%20very%20specific%0Arelationship%20between%20layer%20smoothness%20constants%20holds.%20Through%20controlled%20CNN%0Aexperiments%2C%20we%20empirically%20demonstrate%20that%20Drop-Muon%20consistently%20outperforms%0Afull-network%20Muon%2C%20achieving%20the%20same%20accuracy%20up%20to%20%241.4%5Ctimes%24%20faster%20in%0Awall-clock%20time.%20Together%2C%20our%20results%20suggest%20a%20shift%20in%20how%20large-scale%0Amodels%20can%20be%20efficiently%20trained%2C%20challenging%20the%20status%20quo%20and%20offering%20a%0Ahighly%20efficient%2C%20theoretically%20grounded%20alternative%20to%20full-network%20updates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrop-Muon%253A%2520Update%2520Less%252C%2520Converge%2520Faster%26entry.906535625%3DKaja%2520Gruntkowska%2520and%2520Yassine%2520Maziane%2520and%2520Zheng%2520Qu%2520and%2520Peter%2520Richt%25C3%25A1rik%26entry.1292438233%3D%2520%2520Conventional%2520wisdom%2520in%2520deep%2520learning%2520optimization%2520dictates%2520updating%2520all%250Alayers%2520at%2520every%2520step-a%2520principle%2520followed%2520by%2520all%2520recent%2520state-of-the-art%250Aoptimizers%2520such%2520as%2520Muon.%2520In%2520this%2520work%252C%2520we%2520challenge%2520this%2520assumption%252C%2520showing%250Athat%2520full-network%2520updates%2520can%2520be%2520fundamentally%2520suboptimal%252C%2520both%2520in%2520theory%2520and%250Ain%2520practice.%2520We%2520introduce%2520a%2520non-Euclidean%2520Randomized%2520Progressive%2520Training%250Amethod-Drop-Muon-a%2520simple%2520yet%2520powerful%2520framework%2520that%2520updates%2520only%2520a%2520subset%2520of%250Alayers%2520per%2520step%2520according%2520to%2520a%2520randomized%2520schedule%252C%2520combining%2520the%2520efficiency%2520of%250Aprogressive%2520training%2520with%2520layer-specific%2520non-Euclidean%2520updates%2520for%2520top-tier%250Aperformance.%2520We%2520provide%2520rigorous%2520convergence%2520guarantees%2520under%2520both%2520layer-wise%250Asmoothness%2520and%2520layer-wise%2520%2524%2528L%255E0%252C%2520L%255E1%2529%2524-smoothness%252C%2520covering%2520deterministic%2520and%250Astochastic%2520gradient%2520settings%252C%2520marking%2520the%2520first%2520such%2520results%2520for%2520progressive%250Atraining%2520in%2520the%2520stochastic%2520and%2520non-smooth%2520regime.%2520Our%2520cost%2520analysis%2520further%250Areveals%2520that%2520full-network%2520updates%2520are%2520not%2520optimal%2520unless%2520a%2520very%2520specific%250Arelationship%2520between%2520layer%2520smoothness%2520constants%2520holds.%2520Through%2520controlled%2520CNN%250Aexperiments%252C%2520we%2520empirically%2520demonstrate%2520that%2520Drop-Muon%2520consistently%2520outperforms%250Afull-network%2520Muon%252C%2520achieving%2520the%2520same%2520accuracy%2520up%2520to%2520%25241.4%255Ctimes%2524%2520faster%2520in%250Awall-clock%2520time.%2520Together%252C%2520our%2520results%2520suggest%2520a%2520shift%2520in%2520how%2520large-scale%250Amodels%2520can%2520be%2520efficiently%2520trained%252C%2520challenging%2520the%2520status%2520quo%2520and%2520offering%2520a%250Ahighly%2520efficient%252C%2520theoretically%2520grounded%2520alternative%2520to%2520full-network%2520updates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drop-Muon%3A%20Update%20Less%2C%20Converge%20Faster&entry.906535625=Kaja%20Gruntkowska%20and%20Yassine%20Maziane%20and%20Zheng%20Qu%20and%20Peter%20Richt%C3%A1rik&entry.1292438233=%20%20Conventional%20wisdom%20in%20deep%20learning%20optimization%20dictates%20updating%20all%0Alayers%20at%20every%20step-a%20principle%20followed%20by%20all%20recent%20state-of-the-art%0Aoptimizers%20such%20as%20Muon.%20In%20this%20work%2C%20we%20challenge%20this%20assumption%2C%20showing%0Athat%20full-network%20updates%20can%20be%20fundamentally%20suboptimal%2C%20both%20in%20theory%20and%0Ain%20practice.%20We%20introduce%20a%20non-Euclidean%20Randomized%20Progressive%20Training%0Amethod-Drop-Muon-a%20simple%20yet%20powerful%20framework%20that%20updates%20only%20a%20subset%20of%0Alayers%20per%20step%20according%20to%20a%20randomized%20schedule%2C%20combining%20the%20efficiency%20of%0Aprogressive%20training%20with%20layer-specific%20non-Euclidean%20updates%20for%20top-tier%0Aperformance.%20We%20provide%20rigorous%20convergence%20guarantees%20under%20both%20layer-wise%0Asmoothness%20and%20layer-wise%20%24%28L%5E0%2C%20L%5E1%29%24-smoothness%2C%20covering%20deterministic%20and%0Astochastic%20gradient%20settings%2C%20marking%20the%20first%20such%20results%20for%20progressive%0Atraining%20in%20the%20stochastic%20and%20non-smooth%20regime.%20Our%20cost%20analysis%20further%0Areveals%20that%20full-network%20updates%20are%20not%20optimal%20unless%20a%20very%20specific%0Arelationship%20between%20layer%20smoothness%20constants%20holds.%20Through%20controlled%20CNN%0Aexperiments%2C%20we%20empirically%20demonstrate%20that%20Drop-Muon%20consistently%20outperforms%0Afull-network%20Muon%2C%20achieving%20the%20same%20accuracy%20up%20to%20%241.4%5Ctimes%24%20faster%20in%0Awall-clock%20time.%20Together%2C%20our%20results%20suggest%20a%20shift%20in%20how%20large-scale%0Amodels%20can%20be%20efficiently%20trained%2C%20challenging%20the%20status%20quo%20and%20offering%20a%0Ahighly%20efficient%2C%20theoretically%20grounded%20alternative%20to%20full-network%20updates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02239v1&entry.124074799=Read"},
{"title": "Unraveling Indirect In-Context Learning Using Influence Functions", "author": "Hadi Askari and Shivanshu Gupta and Terry Tong and Fei Wang and Anshuman Chhabra and Muhao Chen", "abstract": "  In this work, we introduce a novel paradigm for generalized In-Context\nLearning (ICL), termed Indirect In-Context Learning. In Indirect ICL, we\nexplore demonstration selection strategies tailored for two distinct real-world\nscenarios: Mixture of Tasks and Noisy ICL. We systematically evaluate the\neffectiveness of Influence Functions (IFs) as a selection tool for these\nsettings, highlighting the potential of IFs to better capture the\ninformativeness of examples within the demonstration pool. For the Mixture of\nTasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU,\nBigBench, StrategyQA, and CommonsenseQA. We demonstrate that combining\nBertScore-Recall (BSR) with an IF surrogate model can further improve\nperformance, leading to average absolute accuracy gains of 0.37\\% and 1.45\\%\nfor 3-shot and 5-shot setups when compared to traditional ICL metrics. In the\nNoisy ICL setting, we examine scenarios where demonstrations might be\nmislabeled or have adversarial noise. Our experiments show that reweighting\ntraditional ICL selectors (BSR and Cosine Similarity) with IF-based selectors\nboosts accuracy by an average of 2.90\\% for Cosine Similarity and 2.94\\% for\nBSR on noisy GLUE benchmarks. For the adversarial sub-setting, we show the\nutility of using IFs for task-agnostic demonstration selection for backdoor\nattack mitigation. Showing a 32.89\\% reduction in Attack Success Rate compared\nto task-aware methods. In sum, we propose a robust framework for demonstration\nselection that generalizes beyond traditional ICL, offering valuable insights\ninto the role of IFs for Indirect ICL.\n", "link": "http://arxiv.org/abs/2501.01473v3", "date": "2025-10-02", "relevancy": 2.0083, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5148}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4943}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20Indirect%20In-Context%20Learning%20Using%20Influence%20Functions&body=Title%3A%20Unraveling%20Indirect%20In-Context%20Learning%20Using%20Influence%20Functions%0AAuthor%3A%20Hadi%20Askari%20and%20Shivanshu%20Gupta%20and%20Terry%20Tong%20and%20Fei%20Wang%20and%20Anshuman%20Chhabra%20and%20Muhao%20Chen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20paradigm%20for%20generalized%20In-Context%0ALearning%20%28ICL%29%2C%20termed%20Indirect%20In-Context%20Learning.%20In%20Indirect%20ICL%2C%20we%0Aexplore%20demonstration%20selection%20strategies%20tailored%20for%20two%20distinct%20real-world%0Ascenarios%3A%20Mixture%20of%20Tasks%20and%20Noisy%20ICL.%20We%20systematically%20evaluate%20the%0Aeffectiveness%20of%20Influence%20Functions%20%28IFs%29%20as%20a%20selection%20tool%20for%20these%0Asettings%2C%20highlighting%20the%20potential%20of%20IFs%20to%20better%20capture%20the%0Ainformativeness%20of%20examples%20within%20the%20demonstration%20pool.%20For%20the%20Mixture%20of%0ATasks%20setting%2C%20demonstrations%20are%20drawn%20from%2028%20diverse%20tasks%2C%20including%20MMLU%2C%0ABigBench%2C%20StrategyQA%2C%20and%20CommonsenseQA.%20We%20demonstrate%20that%20combining%0ABertScore-Recall%20%28BSR%29%20with%20an%20IF%20surrogate%20model%20can%20further%20improve%0Aperformance%2C%20leading%20to%20average%20absolute%20accuracy%20gains%20of%200.37%5C%25%20and%201.45%5C%25%0Afor%203-shot%20and%205-shot%20setups%20when%20compared%20to%20traditional%20ICL%20metrics.%20In%20the%0ANoisy%20ICL%20setting%2C%20we%20examine%20scenarios%20where%20demonstrations%20might%20be%0Amislabeled%20or%20have%20adversarial%20noise.%20Our%20experiments%20show%20that%20reweighting%0Atraditional%20ICL%20selectors%20%28BSR%20and%20Cosine%20Similarity%29%20with%20IF-based%20selectors%0Aboosts%20accuracy%20by%20an%20average%20of%202.90%5C%25%20for%20Cosine%20Similarity%20and%202.94%5C%25%20for%0ABSR%20on%20noisy%20GLUE%20benchmarks.%20For%20the%20adversarial%20sub-setting%2C%20we%20show%20the%0Autility%20of%20using%20IFs%20for%20task-agnostic%20demonstration%20selection%20for%20backdoor%0Aattack%20mitigation.%20Showing%20a%2032.89%5C%25%20reduction%20in%20Attack%20Success%20Rate%20compared%0Ato%20task-aware%20methods.%20In%20sum%2C%20we%20propose%20a%20robust%20framework%20for%20demonstration%0Aselection%20that%20generalizes%20beyond%20traditional%20ICL%2C%20offering%20valuable%20insights%0Ainto%20the%20role%20of%20IFs%20for%20Indirect%20ICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01473v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520Indirect%2520In-Context%2520Learning%2520Using%2520Influence%2520Functions%26entry.906535625%3DHadi%2520Askari%2520and%2520Shivanshu%2520Gupta%2520and%2520Terry%2520Tong%2520and%2520Fei%2520Wang%2520and%2520Anshuman%2520Chhabra%2520and%2520Muhao%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520paradigm%2520for%2520generalized%2520In-Context%250ALearning%2520%2528ICL%2529%252C%2520termed%2520Indirect%2520In-Context%2520Learning.%2520In%2520Indirect%2520ICL%252C%2520we%250Aexplore%2520demonstration%2520selection%2520strategies%2520tailored%2520for%2520two%2520distinct%2520real-world%250Ascenarios%253A%2520Mixture%2520of%2520Tasks%2520and%2520Noisy%2520ICL.%2520We%2520systematically%2520evaluate%2520the%250Aeffectiveness%2520of%2520Influence%2520Functions%2520%2528IFs%2529%2520as%2520a%2520selection%2520tool%2520for%2520these%250Asettings%252C%2520highlighting%2520the%2520potential%2520of%2520IFs%2520to%2520better%2520capture%2520the%250Ainformativeness%2520of%2520examples%2520within%2520the%2520demonstration%2520pool.%2520For%2520the%2520Mixture%2520of%250ATasks%2520setting%252C%2520demonstrations%2520are%2520drawn%2520from%252028%2520diverse%2520tasks%252C%2520including%2520MMLU%252C%250ABigBench%252C%2520StrategyQA%252C%2520and%2520CommonsenseQA.%2520We%2520demonstrate%2520that%2520combining%250ABertScore-Recall%2520%2528BSR%2529%2520with%2520an%2520IF%2520surrogate%2520model%2520can%2520further%2520improve%250Aperformance%252C%2520leading%2520to%2520average%2520absolute%2520accuracy%2520gains%2520of%25200.37%255C%2525%2520and%25201.45%255C%2525%250Afor%25203-shot%2520and%25205-shot%2520setups%2520when%2520compared%2520to%2520traditional%2520ICL%2520metrics.%2520In%2520the%250ANoisy%2520ICL%2520setting%252C%2520we%2520examine%2520scenarios%2520where%2520demonstrations%2520might%2520be%250Amislabeled%2520or%2520have%2520adversarial%2520noise.%2520Our%2520experiments%2520show%2520that%2520reweighting%250Atraditional%2520ICL%2520selectors%2520%2528BSR%2520and%2520Cosine%2520Similarity%2529%2520with%2520IF-based%2520selectors%250Aboosts%2520accuracy%2520by%2520an%2520average%2520of%25202.90%255C%2525%2520for%2520Cosine%2520Similarity%2520and%25202.94%255C%2525%2520for%250ABSR%2520on%2520noisy%2520GLUE%2520benchmarks.%2520For%2520the%2520adversarial%2520sub-setting%252C%2520we%2520show%2520the%250Autility%2520of%2520using%2520IFs%2520for%2520task-agnostic%2520demonstration%2520selection%2520for%2520backdoor%250Aattack%2520mitigation.%2520Showing%2520a%252032.89%255C%2525%2520reduction%2520in%2520Attack%2520Success%2520Rate%2520compared%250Ato%2520task-aware%2520methods.%2520In%2520sum%252C%2520we%2520propose%2520a%2520robust%2520framework%2520for%2520demonstration%250Aselection%2520that%2520generalizes%2520beyond%2520traditional%2520ICL%252C%2520offering%2520valuable%2520insights%250Ainto%2520the%2520role%2520of%2520IFs%2520for%2520Indirect%2520ICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01473v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20Indirect%20In-Context%20Learning%20Using%20Influence%20Functions&entry.906535625=Hadi%20Askari%20and%20Shivanshu%20Gupta%20and%20Terry%20Tong%20and%20Fei%20Wang%20and%20Anshuman%20Chhabra%20and%20Muhao%20Chen&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20paradigm%20for%20generalized%20In-Context%0ALearning%20%28ICL%29%2C%20termed%20Indirect%20In-Context%20Learning.%20In%20Indirect%20ICL%2C%20we%0Aexplore%20demonstration%20selection%20strategies%20tailored%20for%20two%20distinct%20real-world%0Ascenarios%3A%20Mixture%20of%20Tasks%20and%20Noisy%20ICL.%20We%20systematically%20evaluate%20the%0Aeffectiveness%20of%20Influence%20Functions%20%28IFs%29%20as%20a%20selection%20tool%20for%20these%0Asettings%2C%20highlighting%20the%20potential%20of%20IFs%20to%20better%20capture%20the%0Ainformativeness%20of%20examples%20within%20the%20demonstration%20pool.%20For%20the%20Mixture%20of%0ATasks%20setting%2C%20demonstrations%20are%20drawn%20from%2028%20diverse%20tasks%2C%20including%20MMLU%2C%0ABigBench%2C%20StrategyQA%2C%20and%20CommonsenseQA.%20We%20demonstrate%20that%20combining%0ABertScore-Recall%20%28BSR%29%20with%20an%20IF%20surrogate%20model%20can%20further%20improve%0Aperformance%2C%20leading%20to%20average%20absolute%20accuracy%20gains%20of%200.37%5C%25%20and%201.45%5C%25%0Afor%203-shot%20and%205-shot%20setups%20when%20compared%20to%20traditional%20ICL%20metrics.%20In%20the%0ANoisy%20ICL%20setting%2C%20we%20examine%20scenarios%20where%20demonstrations%20might%20be%0Amislabeled%20or%20have%20adversarial%20noise.%20Our%20experiments%20show%20that%20reweighting%0Atraditional%20ICL%20selectors%20%28BSR%20and%20Cosine%20Similarity%29%20with%20IF-based%20selectors%0Aboosts%20accuracy%20by%20an%20average%20of%202.90%5C%25%20for%20Cosine%20Similarity%20and%202.94%5C%25%20for%0ABSR%20on%20noisy%20GLUE%20benchmarks.%20For%20the%20adversarial%20sub-setting%2C%20we%20show%20the%0Autility%20of%20using%20IFs%20for%20task-agnostic%20demonstration%20selection%20for%20backdoor%0Aattack%20mitigation.%20Showing%20a%2032.89%5C%25%20reduction%20in%20Attack%20Success%20Rate%20compared%0Ato%20task-aware%20methods.%20In%20sum%2C%20we%20propose%20a%20robust%20framework%20for%20demonstration%0Aselection%20that%20generalizes%20beyond%20traditional%20ICL%2C%20offering%20valuable%20insights%0Ainto%20the%20role%20of%20IFs%20for%20Indirect%20ICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01473v3&entry.124074799=Read"},
{"title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective", "author": "Wen Yang and Junhong Wu and Chong Li and Chengqing Zong and Jiajun Zhang", "abstract": "  Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\n$\\textit{Does the reasoning capability achieved from English RPT effectively\ntransfer to other languages?}$ We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\n$\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.\n", "link": "http://arxiv.org/abs/2510.02272v1", "date": "2025-10-02", "relevancy": 2.0069, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20Scaling%20Law%3A%20Unveiling%20Reasoning%20Generalization%20through%20A%0A%20%20Cross-Linguistic%20Perspective&body=Title%3A%20Parallel%20Scaling%20Law%3A%20Unveiling%20Reasoning%20Generalization%20through%20A%0A%20%20Cross-Linguistic%20Perspective%0AAuthor%3A%20Wen%20Yang%20and%20Junhong%20Wu%20and%20Chong%20Li%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Reinforcement%20Post-Training%20%28RPT%29%20have%20significantly%0Aenhanced%20the%20capabilities%20of%20Large%20Reasoning%20Models%20%28LRMs%29%2C%20sparking%20increased%0Ainterest%20in%20the%20generalization%20of%20RL-based%20reasoning.%20While%20existing%20work%20has%0Aprimarily%20focused%20on%20investigating%20its%20generalization%20across%20tasks%20or%0Amodalities%2C%20this%20study%20proposes%20a%20novel%20cross-linguistic%20perspective%20to%0Ainvestigate%20reasoning%20generalization.%20This%20raises%20a%20crucial%20question%3A%0A%24%5Ctextit%7BDoes%20the%20reasoning%20capability%20achieved%20from%20English%20RPT%20effectively%0Atransfer%20to%20other%20languages%3F%7D%24%20We%20address%20this%20by%20systematically%20evaluating%0AEnglish-centric%20LRMs%20on%20multilingual%20reasoning%20benchmarks%20and%20introducing%20a%0Ametric%20to%20quantify%20cross-lingual%20transferability.%20Our%20findings%20reveal%20that%0Across-lingual%20transferability%20varies%20significantly%20across%20initial%20model%2C%20target%0Alanguage%2C%20and%20training%20paradigm.%20Through%20interventional%20studies%2C%20we%20find%20that%0Amodels%20with%20stronger%20initial%20English%20capabilities%20tend%20to%20over-rely%20on%0AEnglish-specific%20patterns%2C%20leading%20to%20diminished%20cross-lingual%20generalization.%0ATo%20address%20this%2C%20we%20conduct%20a%20thorough%20parallel%20training%20study.%20Experimental%0Aresults%20yield%20three%20key%20findings%3A%20%24%5Ctextbf%7BFirst-Parallel%20Leap%7D%24%2C%20a%20substantial%0Aleap%20in%20performance%20when%20transitioning%20from%20monolingual%20to%20just%20a%20single%0Aparallel%20language%2C%20and%20a%20predictable%20%24%5Ctextbf%7BParallel%20Scaling%20Law%7D%24%2C%20revealing%0Athat%20cross-lingual%20reasoning%20transfer%20follows%20a%20power-law%20with%20the%20number%20of%0Atraining%20parallel%20languages.%20Moreover%2C%20we%20identify%20the%20discrepancy%20between%0Aactual%20monolingual%20performance%20and%20the%20power-law%20prediction%20as%0A%24%5Ctextbf%7BMonolingual%20Generalization%20Gap%7D%24%2C%20indicating%20that%20English-centric%20LRMs%0Afail%20to%20fully%20generalize%20across%20languages.%20Our%20study%20challenges%20the%20assumption%0Athat%20LRM%20reasoning%20mirrors%20human%20cognition%2C%20providing%20critical%20insights%20for%20the%0Adevelopment%20of%20more%20language-agnostic%20LRMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520Scaling%2520Law%253A%2520Unveiling%2520Reasoning%2520Generalization%2520through%2520A%250A%2520%2520Cross-Linguistic%2520Perspective%26entry.906535625%3DWen%2520Yang%2520and%2520Junhong%2520Wu%2520and%2520Chong%2520Li%2520and%2520Chengqing%2520Zong%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Reinforcement%2520Post-Training%2520%2528RPT%2529%2520have%2520significantly%250Aenhanced%2520the%2520capabilities%2520of%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%252C%2520sparking%2520increased%250Ainterest%2520in%2520the%2520generalization%2520of%2520RL-based%2520reasoning.%2520While%2520existing%2520work%2520has%250Aprimarily%2520focused%2520on%2520investigating%2520its%2520generalization%2520across%2520tasks%2520or%250Amodalities%252C%2520this%2520study%2520proposes%2520a%2520novel%2520cross-linguistic%2520perspective%2520to%250Ainvestigate%2520reasoning%2520generalization.%2520This%2520raises%2520a%2520crucial%2520question%253A%250A%2524%255Ctextit%257BDoes%2520the%2520reasoning%2520capability%2520achieved%2520from%2520English%2520RPT%2520effectively%250Atransfer%2520to%2520other%2520languages%253F%257D%2524%2520We%2520address%2520this%2520by%2520systematically%2520evaluating%250AEnglish-centric%2520LRMs%2520on%2520multilingual%2520reasoning%2520benchmarks%2520and%2520introducing%2520a%250Ametric%2520to%2520quantify%2520cross-lingual%2520transferability.%2520Our%2520findings%2520reveal%2520that%250Across-lingual%2520transferability%2520varies%2520significantly%2520across%2520initial%2520model%252C%2520target%250Alanguage%252C%2520and%2520training%2520paradigm.%2520Through%2520interventional%2520studies%252C%2520we%2520find%2520that%250Amodels%2520with%2520stronger%2520initial%2520English%2520capabilities%2520tend%2520to%2520over-rely%2520on%250AEnglish-specific%2520patterns%252C%2520leading%2520to%2520diminished%2520cross-lingual%2520generalization.%250ATo%2520address%2520this%252C%2520we%2520conduct%2520a%2520thorough%2520parallel%2520training%2520study.%2520Experimental%250Aresults%2520yield%2520three%2520key%2520findings%253A%2520%2524%255Ctextbf%257BFirst-Parallel%2520Leap%257D%2524%252C%2520a%2520substantial%250Aleap%2520in%2520performance%2520when%2520transitioning%2520from%2520monolingual%2520to%2520just%2520a%2520single%250Aparallel%2520language%252C%2520and%2520a%2520predictable%2520%2524%255Ctextbf%257BParallel%2520Scaling%2520Law%257D%2524%252C%2520revealing%250Athat%2520cross-lingual%2520reasoning%2520transfer%2520follows%2520a%2520power-law%2520with%2520the%2520number%2520of%250Atraining%2520parallel%2520languages.%2520Moreover%252C%2520we%2520identify%2520the%2520discrepancy%2520between%250Aactual%2520monolingual%2520performance%2520and%2520the%2520power-law%2520prediction%2520as%250A%2524%255Ctextbf%257BMonolingual%2520Generalization%2520Gap%257D%2524%252C%2520indicating%2520that%2520English-centric%2520LRMs%250Afail%2520to%2520fully%2520generalize%2520across%2520languages.%2520Our%2520study%2520challenges%2520the%2520assumption%250Athat%2520LRM%2520reasoning%2520mirrors%2520human%2520cognition%252C%2520providing%2520critical%2520insights%2520for%2520the%250Adevelopment%2520of%2520more%2520language-agnostic%2520LRMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20Scaling%20Law%3A%20Unveiling%20Reasoning%20Generalization%20through%20A%0A%20%20Cross-Linguistic%20Perspective&entry.906535625=Wen%20Yang%20and%20Junhong%20Wu%20and%20Chong%20Li%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20Reinforcement%20Post-Training%20%28RPT%29%20have%20significantly%0Aenhanced%20the%20capabilities%20of%20Large%20Reasoning%20Models%20%28LRMs%29%2C%20sparking%20increased%0Ainterest%20in%20the%20generalization%20of%20RL-based%20reasoning.%20While%20existing%20work%20has%0Aprimarily%20focused%20on%20investigating%20its%20generalization%20across%20tasks%20or%0Amodalities%2C%20this%20study%20proposes%20a%20novel%20cross-linguistic%20perspective%20to%0Ainvestigate%20reasoning%20generalization.%20This%20raises%20a%20crucial%20question%3A%0A%24%5Ctextit%7BDoes%20the%20reasoning%20capability%20achieved%20from%20English%20RPT%20effectively%0Atransfer%20to%20other%20languages%3F%7D%24%20We%20address%20this%20by%20systematically%20evaluating%0AEnglish-centric%20LRMs%20on%20multilingual%20reasoning%20benchmarks%20and%20introducing%20a%0Ametric%20to%20quantify%20cross-lingual%20transferability.%20Our%20findings%20reveal%20that%0Across-lingual%20transferability%20varies%20significantly%20across%20initial%20model%2C%20target%0Alanguage%2C%20and%20training%20paradigm.%20Through%20interventional%20studies%2C%20we%20find%20that%0Amodels%20with%20stronger%20initial%20English%20capabilities%20tend%20to%20over-rely%20on%0AEnglish-specific%20patterns%2C%20leading%20to%20diminished%20cross-lingual%20generalization.%0ATo%20address%20this%2C%20we%20conduct%20a%20thorough%20parallel%20training%20study.%20Experimental%0Aresults%20yield%20three%20key%20findings%3A%20%24%5Ctextbf%7BFirst-Parallel%20Leap%7D%24%2C%20a%20substantial%0Aleap%20in%20performance%20when%20transitioning%20from%20monolingual%20to%20just%20a%20single%0Aparallel%20language%2C%20and%20a%20predictable%20%24%5Ctextbf%7BParallel%20Scaling%20Law%7D%24%2C%20revealing%0Athat%20cross-lingual%20reasoning%20transfer%20follows%20a%20power-law%20with%20the%20number%20of%0Atraining%20parallel%20languages.%20Moreover%2C%20we%20identify%20the%20discrepancy%20between%0Aactual%20monolingual%20performance%20and%20the%20power-law%20prediction%20as%0A%24%5Ctextbf%7BMonolingual%20Generalization%20Gap%7D%24%2C%20indicating%20that%20English-centric%20LRMs%0Afail%20to%20fully%20generalize%20across%20languages.%20Our%20study%20challenges%20the%20assumption%0Athat%20LRM%20reasoning%20mirrors%20human%20cognition%2C%20providing%20critical%20insights%20for%20the%0Adevelopment%20of%20more%20language-agnostic%20LRMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02272v1&entry.124074799=Read"},
{"title": "Knowledge Distillation Detection for Open-weights Models", "author": "Qin Shi and Amber Yijia Zheng and Qifan Song and Raymond A. Yeh", "abstract": "  We propose the task of knowledge distillation detection, which aims to\ndetermine whether a student model has been distilled from a given teacher,\nunder a practical setting where only the student's weights and the teacher's\nAPI are available. This problem is motivated by growing concerns about model\nprovenance and unauthorized replication through distillation. To address this\ntask, we introduce a model-agnostic framework that combines data-free input\nsynthesis and statistical score computation for detecting distillation. Our\napproach is applicable to both classification and generative models.\nExperiments on diverse architectures for image classification and text-to-image\ngeneration show that our method improves detection accuracy over the strongest\nbaselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image\ngeneration. The code is available at\nhttps://github.com/shqii1j/distillation_detection.\n", "link": "http://arxiv.org/abs/2510.02302v1", "date": "2025-10-02", "relevancy": 2.0044, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5435}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5055}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Distillation%20Detection%20for%20Open-weights%20Models&body=Title%3A%20Knowledge%20Distillation%20Detection%20for%20Open-weights%20Models%0AAuthor%3A%20Qin%20Shi%20and%20Amber%20Yijia%20Zheng%20and%20Qifan%20Song%20and%20Raymond%20A.%20Yeh%0AAbstract%3A%20%20%20We%20propose%20the%20task%20of%20knowledge%20distillation%20detection%2C%20which%20aims%20to%0Adetermine%20whether%20a%20student%20model%20has%20been%20distilled%20from%20a%20given%20teacher%2C%0Aunder%20a%20practical%20setting%20where%20only%20the%20student%27s%20weights%20and%20the%20teacher%27s%0AAPI%20are%20available.%20This%20problem%20is%20motivated%20by%20growing%20concerns%20about%20model%0Aprovenance%20and%20unauthorized%20replication%20through%20distillation.%20To%20address%20this%0Atask%2C%20we%20introduce%20a%20model-agnostic%20framework%20that%20combines%20data-free%20input%0Asynthesis%20and%20statistical%20score%20computation%20for%20detecting%20distillation.%20Our%0Aapproach%20is%20applicable%20to%20both%20classification%20and%20generative%20models.%0AExperiments%20on%20diverse%20architectures%20for%20image%20classification%20and%20text-to-image%0Ageneration%20show%20that%20our%20method%20improves%20detection%20accuracy%20over%20the%20strongest%0Abaselines%20by%2059.6%25%20on%20CIFAR-10%2C%2071.2%25%20on%20ImageNet%2C%20and%2020.0%25%20for%20text-to-image%0Ageneration.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shqii1j/distillation_detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Distillation%2520Detection%2520for%2520Open-weights%2520Models%26entry.906535625%3DQin%2520Shi%2520and%2520Amber%2520Yijia%2520Zheng%2520and%2520Qifan%2520Song%2520and%2520Raymond%2520A.%2520Yeh%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520task%2520of%2520knowledge%2520distillation%2520detection%252C%2520which%2520aims%2520to%250Adetermine%2520whether%2520a%2520student%2520model%2520has%2520been%2520distilled%2520from%2520a%2520given%2520teacher%252C%250Aunder%2520a%2520practical%2520setting%2520where%2520only%2520the%2520student%2527s%2520weights%2520and%2520the%2520teacher%2527s%250AAPI%2520are%2520available.%2520This%2520problem%2520is%2520motivated%2520by%2520growing%2520concerns%2520about%2520model%250Aprovenance%2520and%2520unauthorized%2520replication%2520through%2520distillation.%2520To%2520address%2520this%250Atask%252C%2520we%2520introduce%2520a%2520model-agnostic%2520framework%2520that%2520combines%2520data-free%2520input%250Asynthesis%2520and%2520statistical%2520score%2520computation%2520for%2520detecting%2520distillation.%2520Our%250Aapproach%2520is%2520applicable%2520to%2520both%2520classification%2520and%2520generative%2520models.%250AExperiments%2520on%2520diverse%2520architectures%2520for%2520image%2520classification%2520and%2520text-to-image%250Ageneration%2520show%2520that%2520our%2520method%2520improves%2520detection%2520accuracy%2520over%2520the%2520strongest%250Abaselines%2520by%252059.6%2525%2520on%2520CIFAR-10%252C%252071.2%2525%2520on%2520ImageNet%252C%2520and%252020.0%2525%2520for%2520text-to-image%250Ageneration.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shqii1j/distillation_detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Distillation%20Detection%20for%20Open-weights%20Models&entry.906535625=Qin%20Shi%20and%20Amber%20Yijia%20Zheng%20and%20Qifan%20Song%20and%20Raymond%20A.%20Yeh&entry.1292438233=%20%20We%20propose%20the%20task%20of%20knowledge%20distillation%20detection%2C%20which%20aims%20to%0Adetermine%20whether%20a%20student%20model%20has%20been%20distilled%20from%20a%20given%20teacher%2C%0Aunder%20a%20practical%20setting%20where%20only%20the%20student%27s%20weights%20and%20the%20teacher%27s%0AAPI%20are%20available.%20This%20problem%20is%20motivated%20by%20growing%20concerns%20about%20model%0Aprovenance%20and%20unauthorized%20replication%20through%20distillation.%20To%20address%20this%0Atask%2C%20we%20introduce%20a%20model-agnostic%20framework%20that%20combines%20data-free%20input%0Asynthesis%20and%20statistical%20score%20computation%20for%20detecting%20distillation.%20Our%0Aapproach%20is%20applicable%20to%20both%20classification%20and%20generative%20models.%0AExperiments%20on%20diverse%20architectures%20for%20image%20classification%20and%20text-to-image%0Ageneration%20show%20that%20our%20method%20improves%20detection%20accuracy%20over%20the%20strongest%0Abaselines%20by%2059.6%25%20on%20CIFAR-10%2C%2071.2%25%20on%20ImageNet%2C%20and%2020.0%25%20for%20text-to-image%0Ageneration.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shqii1j/distillation_detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02302v1&entry.124074799=Read"},
{"title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation", "author": "Anna Kuzina and Maciej Pioro and Paul N. Whatmough and Babak Ehteshami Bejnordi", "abstract": "  Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.\n", "link": "http://arxiv.org/abs/2510.02312v1", "date": "2025-10-02", "relevancy": 1.998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KaVa%3A%20Latent%20Reasoning%20via%20Compressed%20KV-Cache%20Distillation&body=Title%3A%20KaVa%3A%20Latent%20Reasoning%20via%20Compressed%20KV-Cache%20Distillation%0AAuthor%3A%20Anna%20Kuzina%20and%20Maciej%20Pioro%20and%20Paul%20N.%20Whatmough%20and%20Babak%20Ehteshami%20Bejnordi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20multi-step%20reasoning%20problems%20with%0Aexplicit%20chain-of-thought%20%28CoT%29%2C%20but%20verbose%20traces%20incur%20significant%0Acomputational%20costs%20and%20memory%20overhead%2C%20and%20often%20carry%20redundant%2C%20stylistic%0Aartifacts.%20Latent%20reasoning%20has%20emerged%20as%20an%20efficient%20alternative%20that%0Ainternalizes%20the%20thought%20process%2C%20but%20it%20suffers%20from%20a%20critical%20lack%20of%0Asupervision%2C%20limiting%20its%20effectiveness%20on%20complex%2C%20natural-language%20reasoning%0Atraces.%20In%20this%20work%2C%20we%20propose%20KaVa%2C%20the%20first%20framework%20that%20bridges%20this%0Agap%20by%20distilling%20knowledge%20directly%20from%20a%20compressed%20KV-cache%20of%20the%20teacher%0Ainto%20a%20latent-reasoning%20student%20via%20self-distillation%2C%20leveraging%20the%0Arepresentational%20flexibility%20of%20continuous%20latent%20tokens%20to%20align%20stepwise%20KV%0Atrajectories.%20We%20show%20that%20the%20abstract%2C%20unstructured%20knowledge%20within%0Acompressed%20KV-cache%2C%20which%20lacks%20direct%20token%20correspondence%2C%20can%20serve%20as%20a%0Arich%20supervisory%20signal%20for%20a%20latent%20reasoning%20student.%20Empirically%2C%20the%0Aapproach%20consistently%20outperforms%20strong%20latent%20baselines%2C%20exhibits%20markedly%0Asmaller%20degradation%20from%20equation-only%20to%20natural-language%20traces%2C%20and%20scales%0Ato%20larger%20backbones%20while%20preserving%20efficiency.%20These%20results%20establish%0Acompressed%20KV-cache%20distillation%20as%20a%20scalable%20supervision%20signal%20for%20latent%0Areasoning%2C%20combining%20the%20accuracy%20of%20CoT-trained%20teachers%20with%20the%20efficiency%0Aand%20deployability%20of%20latent%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKaVa%253A%2520Latent%2520Reasoning%2520via%2520Compressed%2520KV-Cache%2520Distillation%26entry.906535625%3DAnna%2520Kuzina%2520and%2520Maciej%2520Pioro%2520and%2520Paul%2520N.%2520Whatmough%2520and%2520Babak%2520Ehteshami%2520Bejnordi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520multi-step%2520reasoning%2520problems%2520with%250Aexplicit%2520chain-of-thought%2520%2528CoT%2529%252C%2520but%2520verbose%2520traces%2520incur%2520significant%250Acomputational%2520costs%2520and%2520memory%2520overhead%252C%2520and%2520often%2520carry%2520redundant%252C%2520stylistic%250Aartifacts.%2520Latent%2520reasoning%2520has%2520emerged%2520as%2520an%2520efficient%2520alternative%2520that%250Ainternalizes%2520the%2520thought%2520process%252C%2520but%2520it%2520suffers%2520from%2520a%2520critical%2520lack%2520of%250Asupervision%252C%2520limiting%2520its%2520effectiveness%2520on%2520complex%252C%2520natural-language%2520reasoning%250Atraces.%2520In%2520this%2520work%252C%2520we%2520propose%2520KaVa%252C%2520the%2520first%2520framework%2520that%2520bridges%2520this%250Agap%2520by%2520distilling%2520knowledge%2520directly%2520from%2520a%2520compressed%2520KV-cache%2520of%2520the%2520teacher%250Ainto%2520a%2520latent-reasoning%2520student%2520via%2520self-distillation%252C%2520leveraging%2520the%250Arepresentational%2520flexibility%2520of%2520continuous%2520latent%2520tokens%2520to%2520align%2520stepwise%2520KV%250Atrajectories.%2520We%2520show%2520that%2520the%2520abstract%252C%2520unstructured%2520knowledge%2520within%250Acompressed%2520KV-cache%252C%2520which%2520lacks%2520direct%2520token%2520correspondence%252C%2520can%2520serve%2520as%2520a%250Arich%2520supervisory%2520signal%2520for%2520a%2520latent%2520reasoning%2520student.%2520Empirically%252C%2520the%250Aapproach%2520consistently%2520outperforms%2520strong%2520latent%2520baselines%252C%2520exhibits%2520markedly%250Asmaller%2520degradation%2520from%2520equation-only%2520to%2520natural-language%2520traces%252C%2520and%2520scales%250Ato%2520larger%2520backbones%2520while%2520preserving%2520efficiency.%2520These%2520results%2520establish%250Acompressed%2520KV-cache%2520distillation%2520as%2520a%2520scalable%2520supervision%2520signal%2520for%2520latent%250Areasoning%252C%2520combining%2520the%2520accuracy%2520of%2520CoT-trained%2520teachers%2520with%2520the%2520efficiency%250Aand%2520deployability%2520of%2520latent%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KaVa%3A%20Latent%20Reasoning%20via%20Compressed%20KV-Cache%20Distillation&entry.906535625=Anna%20Kuzina%20and%20Maciej%20Pioro%20and%20Paul%20N.%20Whatmough%20and%20Babak%20Ehteshami%20Bejnordi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20multi-step%20reasoning%20problems%20with%0Aexplicit%20chain-of-thought%20%28CoT%29%2C%20but%20verbose%20traces%20incur%20significant%0Acomputational%20costs%20and%20memory%20overhead%2C%20and%20often%20carry%20redundant%2C%20stylistic%0Aartifacts.%20Latent%20reasoning%20has%20emerged%20as%20an%20efficient%20alternative%20that%0Ainternalizes%20the%20thought%20process%2C%20but%20it%20suffers%20from%20a%20critical%20lack%20of%0Asupervision%2C%20limiting%20its%20effectiveness%20on%20complex%2C%20natural-language%20reasoning%0Atraces.%20In%20this%20work%2C%20we%20propose%20KaVa%2C%20the%20first%20framework%20that%20bridges%20this%0Agap%20by%20distilling%20knowledge%20directly%20from%20a%20compressed%20KV-cache%20of%20the%20teacher%0Ainto%20a%20latent-reasoning%20student%20via%20self-distillation%2C%20leveraging%20the%0Arepresentational%20flexibility%20of%20continuous%20latent%20tokens%20to%20align%20stepwise%20KV%0Atrajectories.%20We%20show%20that%20the%20abstract%2C%20unstructured%20knowledge%20within%0Acompressed%20KV-cache%2C%20which%20lacks%20direct%20token%20correspondence%2C%20can%20serve%20as%20a%0Arich%20supervisory%20signal%20for%20a%20latent%20reasoning%20student.%20Empirically%2C%20the%0Aapproach%20consistently%20outperforms%20strong%20latent%20baselines%2C%20exhibits%20markedly%0Asmaller%20degradation%20from%20equation-only%20to%20natural-language%20traces%2C%20and%20scales%0Ato%20larger%20backbones%20while%20preserving%20efficiency.%20These%20results%20establish%0Acompressed%20KV-cache%20distillation%20as%20a%20scalable%20supervision%20signal%20for%20latent%0Areasoning%2C%20combining%20the%20accuracy%20of%20CoT-trained%20teachers%20with%20the%20efficiency%0Aand%20deployability%20of%20latent%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02312v1&entry.124074799=Read"},
{"title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient\n  Orthogonalization", "author": "Dhruv Kohli and Sawyer J. Robertson and Gal Mishne and Alexander Cloninger", "abstract": "  Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation.\n", "link": "http://arxiv.org/abs/2510.02308v1", "date": "2025-10-02", "relevancy": 1.9978, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5131}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Tangent%20Space%20Estimation%20via%20Laplacian%20Eigenvector%20Gradient%0A%20%20Orthogonalization&body=Title%3A%20Robust%20Tangent%20Space%20Estimation%20via%20Laplacian%20Eigenvector%20Gradient%0A%20%20Orthogonalization%0AAuthor%3A%20Dhruv%20Kohli%20and%20Sawyer%20J.%20Robertson%20and%20Gal%20Mishne%20and%20Alexander%20Cloninger%0AAbstract%3A%20%20%20Estimating%20the%20tangent%20spaces%20of%20a%20data%20manifold%20is%20a%20fundamental%20problem%20in%0Adata%20analysis.%20The%20standard%20approach%2C%20Local%20Principal%20Component%20Analysis%0A%28LPCA%29%2C%20struggles%20in%20high-noise%20settings%20due%20to%20a%20critical%20trade-off%20in%0Achoosing%20the%20neighborhood%20size.%20Selecting%20an%20optimal%20size%20requires%20prior%0Aknowledge%20of%20the%20geometric%20and%20noise%20characteristics%20of%20the%20data%20that%20are%20often%0Aunavailable.%20In%20this%20paper%2C%20we%20propose%20a%20spectral%20method%2C%20Laplacian%20Eigenvector%0AGradient%20Orthogonalization%20%28LEGO%29%2C%20that%20utilizes%20the%20global%20structure%20of%20the%0Adata%20to%20guide%20local%20tangent%20space%20estimation.%20Instead%20of%20relying%20solely%20on%0Alocal%20neighborhoods%2C%20LEGO%20estimates%20the%20tangent%20space%20at%20each%20data%20point%20by%0Aorthogonalizing%20the%20gradients%20of%20low-frequency%20eigenvectors%20of%20the%20graph%0ALaplacian.%20We%20provide%20two%20theoretical%20justifications%20of%20our%20method.%20First%2C%20a%0Adifferential%20geometric%20analysis%20on%20a%20tubular%20neighborhood%20of%20a%20manifold%20shows%0Athat%20gradients%20of%20the%20low-frequency%20Laplacian%20eigenfunctions%20of%20the%20tube%20align%0Aclosely%20with%20the%20manifold%27s%20tangent%20bundle%2C%20while%20an%20eigenfunction%20with%20high%0Agradient%20in%20directions%20orthogonal%20to%20the%20manifold%20lie%20deeper%20in%20the%20spectrum.%0ASecond%2C%20a%20random%20matrix%20theoretic%20analysis%20also%20demonstrates%20that%20low-frequency%0Aeigenvectors%20are%20robust%20to%20sub-Gaussian%20noise.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20LEGO%20yields%20tangent%20space%20estimates%20that%20are%0Asignificantly%20more%20robust%20to%20noise%20than%20those%20from%20LPCA%2C%20resulting%20in%20marked%0Aimprovements%20in%20downstream%20tasks%20such%20as%20manifold%20learning%2C%20boundary%20detection%2C%0Aand%20local%20intrinsic%20dimension%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Tangent%2520Space%2520Estimation%2520via%2520Laplacian%2520Eigenvector%2520Gradient%250A%2520%2520Orthogonalization%26entry.906535625%3DDhruv%2520Kohli%2520and%2520Sawyer%2520J.%2520Robertson%2520and%2520Gal%2520Mishne%2520and%2520Alexander%2520Cloninger%26entry.1292438233%3D%2520%2520Estimating%2520the%2520tangent%2520spaces%2520of%2520a%2520data%2520manifold%2520is%2520a%2520fundamental%2520problem%2520in%250Adata%2520analysis.%2520The%2520standard%2520approach%252C%2520Local%2520Principal%2520Component%2520Analysis%250A%2528LPCA%2529%252C%2520struggles%2520in%2520high-noise%2520settings%2520due%2520to%2520a%2520critical%2520trade-off%2520in%250Achoosing%2520the%2520neighborhood%2520size.%2520Selecting%2520an%2520optimal%2520size%2520requires%2520prior%250Aknowledge%2520of%2520the%2520geometric%2520and%2520noise%2520characteristics%2520of%2520the%2520data%2520that%2520are%2520often%250Aunavailable.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520spectral%2520method%252C%2520Laplacian%2520Eigenvector%250AGradient%2520Orthogonalization%2520%2528LEGO%2529%252C%2520that%2520utilizes%2520the%2520global%2520structure%2520of%2520the%250Adata%2520to%2520guide%2520local%2520tangent%2520space%2520estimation.%2520Instead%2520of%2520relying%2520solely%2520on%250Alocal%2520neighborhoods%252C%2520LEGO%2520estimates%2520the%2520tangent%2520space%2520at%2520each%2520data%2520point%2520by%250Aorthogonalizing%2520the%2520gradients%2520of%2520low-frequency%2520eigenvectors%2520of%2520the%2520graph%250ALaplacian.%2520We%2520provide%2520two%2520theoretical%2520justifications%2520of%2520our%2520method.%2520First%252C%2520a%250Adifferential%2520geometric%2520analysis%2520on%2520a%2520tubular%2520neighborhood%2520of%2520a%2520manifold%2520shows%250Athat%2520gradients%2520of%2520the%2520low-frequency%2520Laplacian%2520eigenfunctions%2520of%2520the%2520tube%2520align%250Aclosely%2520with%2520the%2520manifold%2527s%2520tangent%2520bundle%252C%2520while%2520an%2520eigenfunction%2520with%2520high%250Agradient%2520in%2520directions%2520orthogonal%2520to%2520the%2520manifold%2520lie%2520deeper%2520in%2520the%2520spectrum.%250ASecond%252C%2520a%2520random%2520matrix%2520theoretic%2520analysis%2520also%2520demonstrates%2520that%2520low-frequency%250Aeigenvectors%2520are%2520robust%2520to%2520sub-Gaussian%2520noise.%2520Through%2520comprehensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520LEGO%2520yields%2520tangent%2520space%2520estimates%2520that%2520are%250Asignificantly%2520more%2520robust%2520to%2520noise%2520than%2520those%2520from%2520LPCA%252C%2520resulting%2520in%2520marked%250Aimprovements%2520in%2520downstream%2520tasks%2520such%2520as%2520manifold%2520learning%252C%2520boundary%2520detection%252C%250Aand%2520local%2520intrinsic%2520dimension%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Tangent%20Space%20Estimation%20via%20Laplacian%20Eigenvector%20Gradient%0A%20%20Orthogonalization&entry.906535625=Dhruv%20Kohli%20and%20Sawyer%20J.%20Robertson%20and%20Gal%20Mishne%20and%20Alexander%20Cloninger&entry.1292438233=%20%20Estimating%20the%20tangent%20spaces%20of%20a%20data%20manifold%20is%20a%20fundamental%20problem%20in%0Adata%20analysis.%20The%20standard%20approach%2C%20Local%20Principal%20Component%20Analysis%0A%28LPCA%29%2C%20struggles%20in%20high-noise%20settings%20due%20to%20a%20critical%20trade-off%20in%0Achoosing%20the%20neighborhood%20size.%20Selecting%20an%20optimal%20size%20requires%20prior%0Aknowledge%20of%20the%20geometric%20and%20noise%20characteristics%20of%20the%20data%20that%20are%20often%0Aunavailable.%20In%20this%20paper%2C%20we%20propose%20a%20spectral%20method%2C%20Laplacian%20Eigenvector%0AGradient%20Orthogonalization%20%28LEGO%29%2C%20that%20utilizes%20the%20global%20structure%20of%20the%0Adata%20to%20guide%20local%20tangent%20space%20estimation.%20Instead%20of%20relying%20solely%20on%0Alocal%20neighborhoods%2C%20LEGO%20estimates%20the%20tangent%20space%20at%20each%20data%20point%20by%0Aorthogonalizing%20the%20gradients%20of%20low-frequency%20eigenvectors%20of%20the%20graph%0ALaplacian.%20We%20provide%20two%20theoretical%20justifications%20of%20our%20method.%20First%2C%20a%0Adifferential%20geometric%20analysis%20on%20a%20tubular%20neighborhood%20of%20a%20manifold%20shows%0Athat%20gradients%20of%20the%20low-frequency%20Laplacian%20eigenfunctions%20of%20the%20tube%20align%0Aclosely%20with%20the%20manifold%27s%20tangent%20bundle%2C%20while%20an%20eigenfunction%20with%20high%0Agradient%20in%20directions%20orthogonal%20to%20the%20manifold%20lie%20deeper%20in%20the%20spectrum.%0ASecond%2C%20a%20random%20matrix%20theoretic%20analysis%20also%20demonstrates%20that%20low-frequency%0Aeigenvectors%20are%20robust%20to%20sub-Gaussian%20noise.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20LEGO%20yields%20tangent%20space%20estimates%20that%20are%0Asignificantly%20more%20robust%20to%20noise%20than%20those%20from%20LPCA%2C%20resulting%20in%20marked%0Aimprovements%20in%20downstream%20tasks%20such%20as%20manifold%20learning%2C%20boundary%20detection%2C%0Aand%20local%20intrinsic%20dimension%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02308v1&entry.124074799=Read"},
{"title": "Enhancing Corpus Callosum Segmentation in Fetal MRI via\n  Pathology-Informed Domain Randomization", "author": "Marina Grifell i Plana and Vladyslav Zalevskyi and L\u00e9a Schmidt and Yvan Gomez and Thomas Sanchez and Vincent Dunet and M\u00e9riam Koob and Vanessa Siffredi and Meritxell Bach Cuadra", "abstract": "  Accurate fetal brain segmentation is crucial for extracting biomarkers and\nassessing neurodevelopment, especially in conditions such as corpus callosum\ndysgenesis (CCD), which can induce drastic anatomical changes. However, the\nrarity of CCD severely limits annotated data, hindering the generalization of\ndeep learning models. To address this, we propose a pathology-informed domain\nrandomization strategy that embeds prior knowledge of CCD manifestations into a\nsynthetic data generation pipeline. By simulating diverse brain alterations\nfrom healthy data alone, our approach enables robust segmentation without\nrequiring pathological annotations.\n  We validate our method on a cohort comprising 248 healthy fetuses, 26 with\nCCD, and 47 with other brain pathologies, achieving substantial improvements on\nCCD cases while maintaining performance on both healthy fetuses and those with\nother pathologies. From the predicted segmentations, we derive clinically\nrelevant biomarkers, such as corpus callosum length (LCC) and volume, and show\ntheir utility in distinguishing CCD subtypes. Our pathology-informed\naugmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in\nhealthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these\nquantitative gains, our approach yields segmentations with improved topological\nconsistency relative to available ground truth, enabling more reliable\nshape-based analyses. Overall, this work demonstrates that incorporating\ndomain-specific anatomical priors into synthetic data pipelines can effectively\nmitigate data scarcity and enhance analysis of rare but clinically significant\nmalformations.\n", "link": "http://arxiv.org/abs/2508.20475v2", "date": "2025-10-02", "relevancy": 1.9918, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Corpus%20Callosum%20Segmentation%20in%20Fetal%20MRI%20via%0A%20%20Pathology-Informed%20Domain%20Randomization&body=Title%3A%20Enhancing%20Corpus%20Callosum%20Segmentation%20in%20Fetal%20MRI%20via%0A%20%20Pathology-Informed%20Domain%20Randomization%0AAuthor%3A%20Marina%20Grifell%20i%20Plana%20and%20Vladyslav%20Zalevskyi%20and%20L%C3%A9a%20Schmidt%20and%20Yvan%20Gomez%20and%20Thomas%20Sanchez%20and%20Vincent%20Dunet%20and%20M%C3%A9riam%20Koob%20and%20Vanessa%20Siffredi%20and%20Meritxell%20Bach%20Cuadra%0AAbstract%3A%20%20%20Accurate%20fetal%20brain%20segmentation%20is%20crucial%20for%20extracting%20biomarkers%20and%0Aassessing%20neurodevelopment%2C%20especially%20in%20conditions%20such%20as%20corpus%20callosum%0Adysgenesis%20%28CCD%29%2C%20which%20can%20induce%20drastic%20anatomical%20changes.%20However%2C%20the%0Ararity%20of%20CCD%20severely%20limits%20annotated%20data%2C%20hindering%20the%20generalization%20of%0Adeep%20learning%20models.%20To%20address%20this%2C%20we%20propose%20a%20pathology-informed%20domain%0Arandomization%20strategy%20that%20embeds%20prior%20knowledge%20of%20CCD%20manifestations%20into%20a%0Asynthetic%20data%20generation%20pipeline.%20By%20simulating%20diverse%20brain%20alterations%0Afrom%20healthy%20data%20alone%2C%20our%20approach%20enables%20robust%20segmentation%20without%0Arequiring%20pathological%20annotations.%0A%20%20We%20validate%20our%20method%20on%20a%20cohort%20comprising%20248%20healthy%20fetuses%2C%2026%20with%0ACCD%2C%20and%2047%20with%20other%20brain%20pathologies%2C%20achieving%20substantial%20improvements%20on%0ACCD%20cases%20while%20maintaining%20performance%20on%20both%20healthy%20fetuses%20and%20those%20with%0Aother%20pathologies.%20From%20the%20predicted%20segmentations%2C%20we%20derive%20clinically%0Arelevant%20biomarkers%2C%20such%20as%20corpus%20callosum%20length%20%28LCC%29%20and%20volume%2C%20and%20show%0Atheir%20utility%20in%20distinguishing%20CCD%20subtypes.%20Our%20pathology-informed%0Aaugmentation%20reduces%20the%20LCC%20estimation%20error%20from%201.89%20mm%20to%200.80%20mm%20in%0Ahealthy%20cases%20and%20from%2010.9%20mm%20to%200.7%20mm%20in%20CCD%20cases.%20Beyond%20these%0Aquantitative%20gains%2C%20our%20approach%20yields%20segmentations%20with%20improved%20topological%0Aconsistency%20relative%20to%20available%20ground%20truth%2C%20enabling%20more%20reliable%0Ashape-based%20analyses.%20Overall%2C%20this%20work%20demonstrates%20that%20incorporating%0Adomain-specific%20anatomical%20priors%20into%20synthetic%20data%20pipelines%20can%20effectively%0Amitigate%20data%20scarcity%20and%20enhance%20analysis%20of%20rare%20but%20clinically%20significant%0Amalformations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Corpus%2520Callosum%2520Segmentation%2520in%2520Fetal%2520MRI%2520via%250A%2520%2520Pathology-Informed%2520Domain%2520Randomization%26entry.906535625%3DMarina%2520Grifell%2520i%2520Plana%2520and%2520Vladyslav%2520Zalevskyi%2520and%2520L%25C3%25A9a%2520Schmidt%2520and%2520Yvan%2520Gomez%2520and%2520Thomas%2520Sanchez%2520and%2520Vincent%2520Dunet%2520and%2520M%25C3%25A9riam%2520Koob%2520and%2520Vanessa%2520Siffredi%2520and%2520Meritxell%2520Bach%2520Cuadra%26entry.1292438233%3D%2520%2520Accurate%2520fetal%2520brain%2520segmentation%2520is%2520crucial%2520for%2520extracting%2520biomarkers%2520and%250Aassessing%2520neurodevelopment%252C%2520especially%2520in%2520conditions%2520such%2520as%2520corpus%2520callosum%250Adysgenesis%2520%2528CCD%2529%252C%2520which%2520can%2520induce%2520drastic%2520anatomical%2520changes.%2520However%252C%2520the%250Ararity%2520of%2520CCD%2520severely%2520limits%2520annotated%2520data%252C%2520hindering%2520the%2520generalization%2520of%250Adeep%2520learning%2520models.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520pathology-informed%2520domain%250Arandomization%2520strategy%2520that%2520embeds%2520prior%2520knowledge%2520of%2520CCD%2520manifestations%2520into%2520a%250Asynthetic%2520data%2520generation%2520pipeline.%2520By%2520simulating%2520diverse%2520brain%2520alterations%250Afrom%2520healthy%2520data%2520alone%252C%2520our%2520approach%2520enables%2520robust%2520segmentation%2520without%250Arequiring%2520pathological%2520annotations.%250A%2520%2520We%2520validate%2520our%2520method%2520on%2520a%2520cohort%2520comprising%2520248%2520healthy%2520fetuses%252C%252026%2520with%250ACCD%252C%2520and%252047%2520with%2520other%2520brain%2520pathologies%252C%2520achieving%2520substantial%2520improvements%2520on%250ACCD%2520cases%2520while%2520maintaining%2520performance%2520on%2520both%2520healthy%2520fetuses%2520and%2520those%2520with%250Aother%2520pathologies.%2520From%2520the%2520predicted%2520segmentations%252C%2520we%2520derive%2520clinically%250Arelevant%2520biomarkers%252C%2520such%2520as%2520corpus%2520callosum%2520length%2520%2528LCC%2529%2520and%2520volume%252C%2520and%2520show%250Atheir%2520utility%2520in%2520distinguishing%2520CCD%2520subtypes.%2520Our%2520pathology-informed%250Aaugmentation%2520reduces%2520the%2520LCC%2520estimation%2520error%2520from%25201.89%2520mm%2520to%25200.80%2520mm%2520in%250Ahealthy%2520cases%2520and%2520from%252010.9%2520mm%2520to%25200.7%2520mm%2520in%2520CCD%2520cases.%2520Beyond%2520these%250Aquantitative%2520gains%252C%2520our%2520approach%2520yields%2520segmentations%2520with%2520improved%2520topological%250Aconsistency%2520relative%2520to%2520available%2520ground%2520truth%252C%2520enabling%2520more%2520reliable%250Ashape-based%2520analyses.%2520Overall%252C%2520this%2520work%2520demonstrates%2520that%2520incorporating%250Adomain-specific%2520anatomical%2520priors%2520into%2520synthetic%2520data%2520pipelines%2520can%2520effectively%250Amitigate%2520data%2520scarcity%2520and%2520enhance%2520analysis%2520of%2520rare%2520but%2520clinically%2520significant%250Amalformations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Corpus%20Callosum%20Segmentation%20in%20Fetal%20MRI%20via%0A%20%20Pathology-Informed%20Domain%20Randomization&entry.906535625=Marina%20Grifell%20i%20Plana%20and%20Vladyslav%20Zalevskyi%20and%20L%C3%A9a%20Schmidt%20and%20Yvan%20Gomez%20and%20Thomas%20Sanchez%20and%20Vincent%20Dunet%20and%20M%C3%A9riam%20Koob%20and%20Vanessa%20Siffredi%20and%20Meritxell%20Bach%20Cuadra&entry.1292438233=%20%20Accurate%20fetal%20brain%20segmentation%20is%20crucial%20for%20extracting%20biomarkers%20and%0Aassessing%20neurodevelopment%2C%20especially%20in%20conditions%20such%20as%20corpus%20callosum%0Adysgenesis%20%28CCD%29%2C%20which%20can%20induce%20drastic%20anatomical%20changes.%20However%2C%20the%0Ararity%20of%20CCD%20severely%20limits%20annotated%20data%2C%20hindering%20the%20generalization%20of%0Adeep%20learning%20models.%20To%20address%20this%2C%20we%20propose%20a%20pathology-informed%20domain%0Arandomization%20strategy%20that%20embeds%20prior%20knowledge%20of%20CCD%20manifestations%20into%20a%0Asynthetic%20data%20generation%20pipeline.%20By%20simulating%20diverse%20brain%20alterations%0Afrom%20healthy%20data%20alone%2C%20our%20approach%20enables%20robust%20segmentation%20without%0Arequiring%20pathological%20annotations.%0A%20%20We%20validate%20our%20method%20on%20a%20cohort%20comprising%20248%20healthy%20fetuses%2C%2026%20with%0ACCD%2C%20and%2047%20with%20other%20brain%20pathologies%2C%20achieving%20substantial%20improvements%20on%0ACCD%20cases%20while%20maintaining%20performance%20on%20both%20healthy%20fetuses%20and%20those%20with%0Aother%20pathologies.%20From%20the%20predicted%20segmentations%2C%20we%20derive%20clinically%0Arelevant%20biomarkers%2C%20such%20as%20corpus%20callosum%20length%20%28LCC%29%20and%20volume%2C%20and%20show%0Atheir%20utility%20in%20distinguishing%20CCD%20subtypes.%20Our%20pathology-informed%0Aaugmentation%20reduces%20the%20LCC%20estimation%20error%20from%201.89%20mm%20to%200.80%20mm%20in%0Ahealthy%20cases%20and%20from%2010.9%20mm%20to%200.7%20mm%20in%20CCD%20cases.%20Beyond%20these%0Aquantitative%20gains%2C%20our%20approach%20yields%20segmentations%20with%20improved%20topological%0Aconsistency%20relative%20to%20available%20ground%20truth%2C%20enabling%20more%20reliable%0Ashape-based%20analyses.%20Overall%2C%20this%20work%20demonstrates%20that%20incorporating%0Adomain-specific%20anatomical%20priors%20into%20synthetic%20data%20pipelines%20can%20effectively%0Amitigate%20data%20scarcity%20and%20enhance%20analysis%20of%20rare%20but%20clinically%20significant%0Amalformations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20475v2&entry.124074799=Read"},
{"title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity", "author": "Maximilian Beck and Kajetan Schweighofer and Sebastian B\u00f6ck and Sebastian Lehner and Sepp Hochreiter", "abstract": "  Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow.\n", "link": "http://arxiv.org/abs/2510.02228v1", "date": "2025-10-02", "relevancy": 1.9792, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xLSTM%20Scaling%20Laws%3A%20Competitive%20Performance%20with%20Linear%20Time-Complexity&body=Title%3A%20xLSTM%20Scaling%20Laws%3A%20Competitive%20Performance%20with%20Linear%20Time-Complexity%0AAuthor%3A%20Maximilian%20Beck%20and%20Kajetan%20Schweighofer%20and%20Sebastian%20B%C3%B6ck%20and%20Sebastian%20Lehner%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20Scaling%20laws%20play%20a%20central%20role%20in%20the%20success%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20enabling%20the%20prediction%20of%20model%20performance%20relative%20to%20compute%0Abudgets%20prior%20to%20training.%20While%20Transformers%20have%20been%20the%20dominant%0Aarchitecture%2C%20recent%20alternatives%20such%20as%20xLSTM%20offer%20linear%20complexity%20with%0Arespect%20to%20context%20length%20while%20remaining%20competitive%20in%20the%20billion-parameter%0Aregime.%20We%20conduct%20a%20comparative%20investigation%20on%20the%20scaling%20behavior%20of%0ATransformers%20and%20xLSTM%20along%20the%20following%20lines%2C%20providing%20insights%20to%20guide%0Afuture%20model%20design%20and%20deployment.%20First%2C%20we%20study%20the%20scaling%20behavior%20for%0AxLSTM%20in%20compute-optimal%20and%20over-training%20regimes%20using%20both%20IsoFLOP%20and%0Aparametric%20fit%20approaches%20on%20a%20wide%20range%20of%20model%20sizes%20%2880M-7B%29%20and%20number%20of%0Atraining%20tokens%20%282B-2T%29.%20Second%2C%20we%20examine%20the%20dependence%20of%20optimal%20model%0Asizes%20on%20context%20length%2C%20a%20pivotal%20aspect%20that%20was%20largely%20ignored%20in%20previous%0Awork.%20Finally%2C%20we%20analyze%20inference-time%20scaling%20characteristics.%20Our%20findings%0Areveal%20that%20in%20typical%20LLM%20training%20and%20inference%20scenarios%2C%20xLSTM%20scales%0Afavorably%20compared%20to%20Transformers.%20Importantly%2C%20xLSTM%27s%20advantage%20widens%20as%0Atraining%20and%20inference%20contexts%20grow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxLSTM%2520Scaling%2520Laws%253A%2520Competitive%2520Performance%2520with%2520Linear%2520Time-Complexity%26entry.906535625%3DMaximilian%2520Beck%2520and%2520Kajetan%2520Schweighofer%2520and%2520Sebastian%2520B%25C3%25B6ck%2520and%2520Sebastian%2520Lehner%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520Scaling%2520laws%2520play%2520a%2520central%2520role%2520in%2520the%2520success%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520enabling%2520the%2520prediction%2520of%2520model%2520performance%2520relative%2520to%2520compute%250Abudgets%2520prior%2520to%2520training.%2520While%2520Transformers%2520have%2520been%2520the%2520dominant%250Aarchitecture%252C%2520recent%2520alternatives%2520such%2520as%2520xLSTM%2520offer%2520linear%2520complexity%2520with%250Arespect%2520to%2520context%2520length%2520while%2520remaining%2520competitive%2520in%2520the%2520billion-parameter%250Aregime.%2520We%2520conduct%2520a%2520comparative%2520investigation%2520on%2520the%2520scaling%2520behavior%2520of%250ATransformers%2520and%2520xLSTM%2520along%2520the%2520following%2520lines%252C%2520providing%2520insights%2520to%2520guide%250Afuture%2520model%2520design%2520and%2520deployment.%2520First%252C%2520we%2520study%2520the%2520scaling%2520behavior%2520for%250AxLSTM%2520in%2520compute-optimal%2520and%2520over-training%2520regimes%2520using%2520both%2520IsoFLOP%2520and%250Aparametric%2520fit%2520approaches%2520on%2520a%2520wide%2520range%2520of%2520model%2520sizes%2520%252880M-7B%2529%2520and%2520number%2520of%250Atraining%2520tokens%2520%25282B-2T%2529.%2520Second%252C%2520we%2520examine%2520the%2520dependence%2520of%2520optimal%2520model%250Asizes%2520on%2520context%2520length%252C%2520a%2520pivotal%2520aspect%2520that%2520was%2520largely%2520ignored%2520in%2520previous%250Awork.%2520Finally%252C%2520we%2520analyze%2520inference-time%2520scaling%2520characteristics.%2520Our%2520findings%250Areveal%2520that%2520in%2520typical%2520LLM%2520training%2520and%2520inference%2520scenarios%252C%2520xLSTM%2520scales%250Afavorably%2520compared%2520to%2520Transformers.%2520Importantly%252C%2520xLSTM%2527s%2520advantage%2520widens%2520as%250Atraining%2520and%2520inference%2520contexts%2520grow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xLSTM%20Scaling%20Laws%3A%20Competitive%20Performance%20with%20Linear%20Time-Complexity&entry.906535625=Maximilian%20Beck%20and%20Kajetan%20Schweighofer%20and%20Sebastian%20B%C3%B6ck%20and%20Sebastian%20Lehner%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20Scaling%20laws%20play%20a%20central%20role%20in%20the%20success%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20enabling%20the%20prediction%20of%20model%20performance%20relative%20to%20compute%0Abudgets%20prior%20to%20training.%20While%20Transformers%20have%20been%20the%20dominant%0Aarchitecture%2C%20recent%20alternatives%20such%20as%20xLSTM%20offer%20linear%20complexity%20with%0Arespect%20to%20context%20length%20while%20remaining%20competitive%20in%20the%20billion-parameter%0Aregime.%20We%20conduct%20a%20comparative%20investigation%20on%20the%20scaling%20behavior%20of%0ATransformers%20and%20xLSTM%20along%20the%20following%20lines%2C%20providing%20insights%20to%20guide%0Afuture%20model%20design%20and%20deployment.%20First%2C%20we%20study%20the%20scaling%20behavior%20for%0AxLSTM%20in%20compute-optimal%20and%20over-training%20regimes%20using%20both%20IsoFLOP%20and%0Aparametric%20fit%20approaches%20on%20a%20wide%20range%20of%20model%20sizes%20%2880M-7B%29%20and%20number%20of%0Atraining%20tokens%20%282B-2T%29.%20Second%2C%20we%20examine%20the%20dependence%20of%20optimal%20model%0Asizes%20on%20context%20length%2C%20a%20pivotal%20aspect%20that%20was%20largely%20ignored%20in%20previous%0Awork.%20Finally%2C%20we%20analyze%20inference-time%20scaling%20characteristics.%20Our%20findings%0Areveal%20that%20in%20typical%20LLM%20training%20and%20inference%20scenarios%2C%20xLSTM%20scales%0Afavorably%20compared%20to%20Transformers.%20Importantly%2C%20xLSTM%27s%20advantage%20widens%20as%0Atraining%20and%20inference%20contexts%20grow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02228v1&entry.124074799=Read"},
{"title": "Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning", "author": "Kaustubh Ponkshe and Raghav Singhal and Eduard Gorbunov and Alexey Tumanov and Samuel Horvath and Praneeth Vepakomma", "abstract": "  Low-rank adapters have become standard for efficiently fine-tuning large\nlanguage models, but they often fall short of achieving the performance of full\nfine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that\napproximates full fine-tuning within low-rank subspaces using a carefully\ndesigned initialization strategy. We theoretically demonstrate that the\narchitecture of LoRA-XS, which inserts a learnable r x r matrix between B and A\nwhile keeping other matrices fixed, provides the precise conditions needed for\nthis approximation. We leverage its constrained update space to achieve optimal\nscaling for high-rank gradient updates while removing the need for scaling\nfactor tuning. We prove that our initialization offers an optimal low-rank\napproximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of LoRA (and baselines) while using 27-90\ntimes fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our\nfindings establish that it is possible to simulate full fine-tuning in low-rank\nsubspaces, and achieve significant parameter efficiency gains without\nsacrificing performance. Our code is publicly available at:\nhttps://github.com/CERT-Lab/lora-sb.\n", "link": "http://arxiv.org/abs/2411.19557v4", "date": "2025-10-02", "relevancy": 1.9786, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4989}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4946}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Initialization%20using%20Update%20Approximation%20is%20a%20Silver%20Bullet%20for%0A%20%20Extremely%20Efficient%20Low-Rank%20Fine-Tuning&body=Title%3A%20Initialization%20using%20Update%20Approximation%20is%20a%20Silver%20Bullet%20for%0A%20%20Extremely%20Efficient%20Low-Rank%20Fine-Tuning%0AAuthor%3A%20Kaustubh%20Ponkshe%20and%20Raghav%20Singhal%20and%20Eduard%20Gorbunov%20and%20Alexey%20Tumanov%20and%20Samuel%20Horvath%20and%20Praneeth%20Vepakomma%0AAbstract%3A%20%20%20Low-rank%20adapters%20have%20become%20standard%20for%20efficiently%20fine-tuning%20large%0Alanguage%20models%2C%20but%20they%20often%20fall%20short%20of%20achieving%20the%20performance%20of%20full%0Afine-tuning.%20We%20propose%20a%20method%2C%20LoRA%20Silver%20Bullet%20or%20LoRA-SB%2C%20that%0Aapproximates%20full%20fine-tuning%20within%20low-rank%20subspaces%20using%20a%20carefully%0Adesigned%20initialization%20strategy.%20We%20theoretically%20demonstrate%20that%20the%0Aarchitecture%20of%20LoRA-XS%2C%20which%20inserts%20a%20learnable%20r%20x%20r%20matrix%20between%20B%20and%20A%0Awhile%20keeping%20other%20matrices%20fixed%2C%20provides%20the%20precise%20conditions%20needed%20for%0Athis%20approximation.%20We%20leverage%20its%20constrained%20update%20space%20to%20achieve%20optimal%0Ascaling%20for%20high-rank%20gradient%20updates%20while%20removing%20the%20need%20for%20scaling%0Afactor%20tuning.%20We%20prove%20that%20our%20initialization%20offers%20an%20optimal%20low-rank%0Aapproximation%20of%20the%20initial%20gradient%20and%20preserves%20update%20directions%0Athroughout%20training.%20Extensive%20experiments%20across%20mathematical%20reasoning%2C%0Acommonsense%20reasoning%2C%20and%20language%20understanding%20tasks%20demonstrate%20that%20our%0Aapproach%20exceeds%20the%20performance%20of%20LoRA%20%28and%20baselines%29%20while%20using%2027-90%0Atimes%20fewer%20learnable%20parameters%2C%20and%20comprehensively%20outperforms%20LoRA-XS.%20Our%0Afindings%20establish%20that%20it%20is%20possible%20to%20simulate%20full%20fine-tuning%20in%20low-rank%0Asubspaces%2C%20and%20achieve%20significant%20parameter%20efficiency%20gains%20without%0Asacrificing%20performance.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/CERT-Lab/lora-sb.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19557v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInitialization%2520using%2520Update%2520Approximation%2520is%2520a%2520Silver%2520Bullet%2520for%250A%2520%2520Extremely%2520Efficient%2520Low-Rank%2520Fine-Tuning%26entry.906535625%3DKaustubh%2520Ponkshe%2520and%2520Raghav%2520Singhal%2520and%2520Eduard%2520Gorbunov%2520and%2520Alexey%2520Tumanov%2520and%2520Samuel%2520Horvath%2520and%2520Praneeth%2520Vepakomma%26entry.1292438233%3D%2520%2520Low-rank%2520adapters%2520have%2520become%2520standard%2520for%2520efficiently%2520fine-tuning%2520large%250Alanguage%2520models%252C%2520but%2520they%2520often%2520fall%2520short%2520of%2520achieving%2520the%2520performance%2520of%2520full%250Afine-tuning.%2520We%2520propose%2520a%2520method%252C%2520LoRA%2520Silver%2520Bullet%2520or%2520LoRA-SB%252C%2520that%250Aapproximates%2520full%2520fine-tuning%2520within%2520low-rank%2520subspaces%2520using%2520a%2520carefully%250Adesigned%2520initialization%2520strategy.%2520We%2520theoretically%2520demonstrate%2520that%2520the%250Aarchitecture%2520of%2520LoRA-XS%252C%2520which%2520inserts%2520a%2520learnable%2520r%2520x%2520r%2520matrix%2520between%2520B%2520and%2520A%250Awhile%2520keeping%2520other%2520matrices%2520fixed%252C%2520provides%2520the%2520precise%2520conditions%2520needed%2520for%250Athis%2520approximation.%2520We%2520leverage%2520its%2520constrained%2520update%2520space%2520to%2520achieve%2520optimal%250Ascaling%2520for%2520high-rank%2520gradient%2520updates%2520while%2520removing%2520the%2520need%2520for%2520scaling%250Afactor%2520tuning.%2520We%2520prove%2520that%2520our%2520initialization%2520offers%2520an%2520optimal%2520low-rank%250Aapproximation%2520of%2520the%2520initial%2520gradient%2520and%2520preserves%2520update%2520directions%250Athroughout%2520training.%2520Extensive%2520experiments%2520across%2520mathematical%2520reasoning%252C%250Acommonsense%2520reasoning%252C%2520and%2520language%2520understanding%2520tasks%2520demonstrate%2520that%2520our%250Aapproach%2520exceeds%2520the%2520performance%2520of%2520LoRA%2520%2528and%2520baselines%2529%2520while%2520using%252027-90%250Atimes%2520fewer%2520learnable%2520parameters%252C%2520and%2520comprehensively%2520outperforms%2520LoRA-XS.%2520Our%250Afindings%2520establish%2520that%2520it%2520is%2520possible%2520to%2520simulate%2520full%2520fine-tuning%2520in%2520low-rank%250Asubspaces%252C%2520and%2520achieve%2520significant%2520parameter%2520efficiency%2520gains%2520without%250Asacrificing%2520performance.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/CERT-Lab/lora-sb.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19557v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Initialization%20using%20Update%20Approximation%20is%20a%20Silver%20Bullet%20for%0A%20%20Extremely%20Efficient%20Low-Rank%20Fine-Tuning&entry.906535625=Kaustubh%20Ponkshe%20and%20Raghav%20Singhal%20and%20Eduard%20Gorbunov%20and%20Alexey%20Tumanov%20and%20Samuel%20Horvath%20and%20Praneeth%20Vepakomma&entry.1292438233=%20%20Low-rank%20adapters%20have%20become%20standard%20for%20efficiently%20fine-tuning%20large%0Alanguage%20models%2C%20but%20they%20often%20fall%20short%20of%20achieving%20the%20performance%20of%20full%0Afine-tuning.%20We%20propose%20a%20method%2C%20LoRA%20Silver%20Bullet%20or%20LoRA-SB%2C%20that%0Aapproximates%20full%20fine-tuning%20within%20low-rank%20subspaces%20using%20a%20carefully%0Adesigned%20initialization%20strategy.%20We%20theoretically%20demonstrate%20that%20the%0Aarchitecture%20of%20LoRA-XS%2C%20which%20inserts%20a%20learnable%20r%20x%20r%20matrix%20between%20B%20and%20A%0Awhile%20keeping%20other%20matrices%20fixed%2C%20provides%20the%20precise%20conditions%20needed%20for%0Athis%20approximation.%20We%20leverage%20its%20constrained%20update%20space%20to%20achieve%20optimal%0Ascaling%20for%20high-rank%20gradient%20updates%20while%20removing%20the%20need%20for%20scaling%0Afactor%20tuning.%20We%20prove%20that%20our%20initialization%20offers%20an%20optimal%20low-rank%0Aapproximation%20of%20the%20initial%20gradient%20and%20preserves%20update%20directions%0Athroughout%20training.%20Extensive%20experiments%20across%20mathematical%20reasoning%2C%0Acommonsense%20reasoning%2C%20and%20language%20understanding%20tasks%20demonstrate%20that%20our%0Aapproach%20exceeds%20the%20performance%20of%20LoRA%20%28and%20baselines%29%20while%20using%2027-90%0Atimes%20fewer%20learnable%20parameters%2C%20and%20comprehensively%20outperforms%20LoRA-XS.%20Our%0Afindings%20establish%20that%20it%20is%20possible%20to%20simulate%20full%20fine-tuning%20in%20low-rank%0Asubspaces%2C%20and%20achieve%20significant%20parameter%20efficiency%20gains%20without%0Asacrificing%20performance.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/CERT-Lab/lora-sb.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19557v4&entry.124074799=Read"},
{"title": "ExGRPO: Learning to Reason from Experience", "author": "Runzhe Zhan and Yafu Li and Zhi Wang and Xiaoye Qu and Dongrui Liu and Jing Shao and Derek F. Wong and Yu Cheng", "abstract": "  Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.\n", "link": "http://arxiv.org/abs/2510.02245v1", "date": "2025-10-02", "relevancy": 1.971, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExGRPO%3A%20Learning%20to%20Reason%20from%20Experience&body=Title%3A%20ExGRPO%3A%20Learning%20to%20Reason%20from%20Experience%0AAuthor%3A%20Runzhe%20Zhan%20and%20Yafu%20Li%20and%20Zhi%20Wang%20and%20Xiaoye%20Qu%20and%20Dongrui%20Liu%20and%20Jing%20Shao%20and%20Derek%20F.%20Wong%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20verifiable%20rewards%20%28RLVR%29%20is%20an%20emerging%20paradigm%0Afor%20improving%20the%20reasoning%20ability%20of%20large%20language%20models.%20However%2C%20standard%0Aon-policy%20training%20discards%20rollout%20experiences%20after%20a%20single%20update%2C%20leading%0Ato%20computational%20inefficiency%20and%20instability.%20While%20prior%20work%20on%20RL%20has%0Ahighlighted%20the%20benefits%20of%20reusing%20past%20experience%2C%20the%20role%20of%20experience%0Acharacteristics%20in%20shaping%20learning%20dynamics%20of%20large%20reasoning%20models%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20are%20the%20first%20to%20investigate%20what%20makes%20a%0Areasoning%20experience%20valuable%20and%20identify%20rollout%20correctness%20and%20entropy%20as%0Aeffective%20indicators%20of%20experience%20value.%20Based%20on%20these%20insights%2C%20we%20propose%0AExGRPO%20%28Experiential%20Group%20Relative%20Policy%20Optimization%29%2C%20a%20framework%20that%0Aorganizes%20and%20prioritizes%20valuable%20experiences%2C%20and%20employs%20a%20mixed-policy%0Aobjective%20to%20balance%20exploration%20with%20experience%20exploitation.%20Experiments%20on%0Afive%20backbone%20models%20%281.5B-8B%20parameters%29%20show%20that%20ExGRPO%20consistently%0Aimproves%20reasoning%20performance%20on%20mathematical/general%20benchmarks%2C%20with%20an%0Aaverage%20gain%20of%20%2B3.5/7.6%20points%20over%20on-policy%20RLVR.%20Moreover%2C%20ExGRPO%0Astabilizes%20training%20on%20both%20stronger%20and%20weaker%20models%20where%20on-policy%20methods%0Afail.%20These%20results%20highlight%20principled%20experience%20management%20as%20a%20key%0Aingredient%20for%20efficient%20and%20scalable%20RLVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExGRPO%253A%2520Learning%2520to%2520Reason%2520from%2520Experience%26entry.906535625%3DRunzhe%2520Zhan%2520and%2520Yafu%2520Li%2520and%2520Zhi%2520Wang%2520and%2520Xiaoye%2520Qu%2520and%2520Dongrui%2520Liu%2520and%2520Jing%2520Shao%2520and%2520Derek%2520F.%2520Wong%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520from%2520verifiable%2520rewards%2520%2528RLVR%2529%2520is%2520an%2520emerging%2520paradigm%250Afor%2520improving%2520the%2520reasoning%2520ability%2520of%2520large%2520language%2520models.%2520However%252C%2520standard%250Aon-policy%2520training%2520discards%2520rollout%2520experiences%2520after%2520a%2520single%2520update%252C%2520leading%250Ato%2520computational%2520inefficiency%2520and%2520instability.%2520While%2520prior%2520work%2520on%2520RL%2520has%250Ahighlighted%2520the%2520benefits%2520of%2520reusing%2520past%2520experience%252C%2520the%2520role%2520of%2520experience%250Acharacteristics%2520in%2520shaping%2520learning%2520dynamics%2520of%2520large%2520reasoning%2520models%2520remains%250Aunderexplored.%2520In%2520this%2520paper%252C%2520we%2520are%2520the%2520first%2520to%2520investigate%2520what%2520makes%2520a%250Areasoning%2520experience%2520valuable%2520and%2520identify%2520rollout%2520correctness%2520and%2520entropy%2520as%250Aeffective%2520indicators%2520of%2520experience%2520value.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%250AExGRPO%2520%2528Experiential%2520Group%2520Relative%2520Policy%2520Optimization%2529%252C%2520a%2520framework%2520that%250Aorganizes%2520and%2520prioritizes%2520valuable%2520experiences%252C%2520and%2520employs%2520a%2520mixed-policy%250Aobjective%2520to%2520balance%2520exploration%2520with%2520experience%2520exploitation.%2520Experiments%2520on%250Afive%2520backbone%2520models%2520%25281.5B-8B%2520parameters%2529%2520show%2520that%2520ExGRPO%2520consistently%250Aimproves%2520reasoning%2520performance%2520on%2520mathematical/general%2520benchmarks%252C%2520with%2520an%250Aaverage%2520gain%2520of%2520%252B3.5/7.6%2520points%2520over%2520on-policy%2520RLVR.%2520Moreover%252C%2520ExGRPO%250Astabilizes%2520training%2520on%2520both%2520stronger%2520and%2520weaker%2520models%2520where%2520on-policy%2520methods%250Afail.%2520These%2520results%2520highlight%2520principled%2520experience%2520management%2520as%2520a%2520key%250Aingredient%2520for%2520efficient%2520and%2520scalable%2520RLVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExGRPO%3A%20Learning%20to%20Reason%20from%20Experience&entry.906535625=Runzhe%20Zhan%20and%20Yafu%20Li%20and%20Zhi%20Wang%20and%20Xiaoye%20Qu%20and%20Dongrui%20Liu%20and%20Jing%20Shao%20and%20Derek%20F.%20Wong%20and%20Yu%20Cheng&entry.1292438233=%20%20Reinforcement%20learning%20from%20verifiable%20rewards%20%28RLVR%29%20is%20an%20emerging%20paradigm%0Afor%20improving%20the%20reasoning%20ability%20of%20large%20language%20models.%20However%2C%20standard%0Aon-policy%20training%20discards%20rollout%20experiences%20after%20a%20single%20update%2C%20leading%0Ato%20computational%20inefficiency%20and%20instability.%20While%20prior%20work%20on%20RL%20has%0Ahighlighted%20the%20benefits%20of%20reusing%20past%20experience%2C%20the%20role%20of%20experience%0Acharacteristics%20in%20shaping%20learning%20dynamics%20of%20large%20reasoning%20models%20remains%0Aunderexplored.%20In%20this%20paper%2C%20we%20are%20the%20first%20to%20investigate%20what%20makes%20a%0Areasoning%20experience%20valuable%20and%20identify%20rollout%20correctness%20and%20entropy%20as%0Aeffective%20indicators%20of%20experience%20value.%20Based%20on%20these%20insights%2C%20we%20propose%0AExGRPO%20%28Experiential%20Group%20Relative%20Policy%20Optimization%29%2C%20a%20framework%20that%0Aorganizes%20and%20prioritizes%20valuable%20experiences%2C%20and%20employs%20a%20mixed-policy%0Aobjective%20to%20balance%20exploration%20with%20experience%20exploitation.%20Experiments%20on%0Afive%20backbone%20models%20%281.5B-8B%20parameters%29%20show%20that%20ExGRPO%20consistently%0Aimproves%20reasoning%20performance%20on%20mathematical/general%20benchmarks%2C%20with%20an%0Aaverage%20gain%20of%20%2B3.5/7.6%20points%20over%20on-policy%20RLVR.%20Moreover%2C%20ExGRPO%0Astabilizes%20training%20on%20both%20stronger%20and%20weaker%20models%20where%20on-policy%20methods%0Afail.%20These%20results%20highlight%20principled%20experience%20management%20as%20a%20key%0Aingredient%20for%20efficient%20and%20scalable%20RLVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02245v1&entry.124074799=Read"},
{"title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation", "author": "Tianyi Jiang and Yi Bin and Yujuan Ding and Kainian Zhu and Fei Ma and Jingkuan Song and Heng Tao Shen", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.\n", "link": "http://arxiv.org/abs/2510.02249v1", "date": "2025-10-02", "relevancy": 1.96, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explore%20Briefly%2C%20Then%20Decide%3A%20Mitigating%20LLM%20Overthinking%20via%20Cumulative%0A%20%20Entropy%20Regulation&body=Title%3A%20Explore%20Briefly%2C%20Then%20Decide%3A%20Mitigating%20LLM%20Overthinking%20via%20Cumulative%0A%20%20Entropy%20Regulation%0AAuthor%3A%20Tianyi%20Jiang%20and%20Yi%20Bin%20and%20Yujuan%20Ding%20and%20Kainian%20Zhu%20and%20Fei%20Ma%20and%20Jingkuan%20Song%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20reasoning%20abilities%0Aon%20complex%20problems%20using%20long%20Chain-of-Thought%20%28CoT%29%20reasoning.%20However%2C%20they%0Aoften%20suffer%20from%20overthinking%2C%20meaning%20generating%20unnecessarily%20lengthy%0Areasoning%20steps%20for%20simpler%20problems.%20This%20issue%20may%20degrade%20the%20efficiency%20of%0Athe%20models%20and%20make%20them%20difficult%20to%20adapt%20the%20reasoning%20depth%20to%20the%0Acomplexity%20of%20problems.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20metric%20Token%0AEntropy%20Cumulative%20Average%20%28TECA%29%2C%20which%20measures%20the%20extent%20of%20exploration%0Athroughout%20the%20reasoning%20process.%20We%20further%20propose%20a%20novel%20reasoning%20paradigm%0A--%20Explore%20Briefly%2C%20Then%20Decide%20--%20with%20an%20associated%20Cumulative%20Entropy%0ARegulation%20%28CER%29%20mechanism.%20This%20paradigm%20leverages%20TECA%20to%20help%20the%20model%0Adynamically%20determine%20the%20optimal%20point%20to%20conclude%20its%20thought%20process%20and%0Aprovide%20a%20final%20answer%2C%20thus%20achieving%20efficient%20reasoning.%20Experimental%0Aresults%20across%20diverse%20mathematical%20benchmarks%20show%20that%20our%20approach%0Asubstantially%20mitigates%20overthinking%20without%20sacrificing%20problem-solving%0Aability.%20With%20our%20thinking%20paradigm%2C%20the%20average%20response%20length%20decreases%20by%0Aup%20to%2071%25%20on%20simpler%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20our%20method%20in%0Acreating%20a%20more%20efficient%20and%20adaptive%20reasoning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplore%2520Briefly%252C%2520Then%2520Decide%253A%2520Mitigating%2520LLM%2520Overthinking%2520via%2520Cumulative%250A%2520%2520Entropy%2520Regulation%26entry.906535625%3DTianyi%2520Jiang%2520and%2520Yi%2520Bin%2520and%2520Yujuan%2520Ding%2520and%2520Kainian%2520Zhu%2520and%2520Fei%2520Ma%2520and%2520Jingkuan%2520Song%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520reasoning%2520abilities%250Aon%2520complex%2520problems%2520using%2520long%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning.%2520However%252C%2520they%250Aoften%2520suffer%2520from%2520overthinking%252C%2520meaning%2520generating%2520unnecessarily%2520lengthy%250Areasoning%2520steps%2520for%2520simpler%2520problems.%2520This%2520issue%2520may%2520degrade%2520the%2520efficiency%2520of%250Athe%2520models%2520and%2520make%2520them%2520difficult%2520to%2520adapt%2520the%2520reasoning%2520depth%2520to%2520the%250Acomplexity%2520of%2520problems.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520metric%2520Token%250AEntropy%2520Cumulative%2520Average%2520%2528TECA%2529%252C%2520which%2520measures%2520the%2520extent%2520of%2520exploration%250Athroughout%2520the%2520reasoning%2520process.%2520We%2520further%2520propose%2520a%2520novel%2520reasoning%2520paradigm%250A--%2520Explore%2520Briefly%252C%2520Then%2520Decide%2520--%2520with%2520an%2520associated%2520Cumulative%2520Entropy%250ARegulation%2520%2528CER%2529%2520mechanism.%2520This%2520paradigm%2520leverages%2520TECA%2520to%2520help%2520the%2520model%250Adynamically%2520determine%2520the%2520optimal%2520point%2520to%2520conclude%2520its%2520thought%2520process%2520and%250Aprovide%2520a%2520final%2520answer%252C%2520thus%2520achieving%2520efficient%2520reasoning.%2520Experimental%250Aresults%2520across%2520diverse%2520mathematical%2520benchmarks%2520show%2520that%2520our%2520approach%250Asubstantially%2520mitigates%2520overthinking%2520without%2520sacrificing%2520problem-solving%250Aability.%2520With%2520our%2520thinking%2520paradigm%252C%2520the%2520average%2520response%2520length%2520decreases%2520by%250Aup%2520to%252071%2525%2520on%2520simpler%2520datasets%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520method%2520in%250Acreating%2520a%2520more%2520efficient%2520and%2520adaptive%2520reasoning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explore%20Briefly%2C%20Then%20Decide%3A%20Mitigating%20LLM%20Overthinking%20via%20Cumulative%0A%20%20Entropy%20Regulation&entry.906535625=Tianyi%20Jiang%20and%20Yi%20Bin%20and%20Yujuan%20Ding%20and%20Kainian%20Zhu%20and%20Fei%20Ma%20and%20Jingkuan%20Song%20and%20Heng%20Tao%20Shen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20reasoning%20abilities%0Aon%20complex%20problems%20using%20long%20Chain-of-Thought%20%28CoT%29%20reasoning.%20However%2C%20they%0Aoften%20suffer%20from%20overthinking%2C%20meaning%20generating%20unnecessarily%20lengthy%0Areasoning%20steps%20for%20simpler%20problems.%20This%20issue%20may%20degrade%20the%20efficiency%20of%0Athe%20models%20and%20make%20them%20difficult%20to%20adapt%20the%20reasoning%20depth%20to%20the%0Acomplexity%20of%20problems.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20metric%20Token%0AEntropy%20Cumulative%20Average%20%28TECA%29%2C%20which%20measures%20the%20extent%20of%20exploration%0Athroughout%20the%20reasoning%20process.%20We%20further%20propose%20a%20novel%20reasoning%20paradigm%0A--%20Explore%20Briefly%2C%20Then%20Decide%20--%20with%20an%20associated%20Cumulative%20Entropy%0ARegulation%20%28CER%29%20mechanism.%20This%20paradigm%20leverages%20TECA%20to%20help%20the%20model%0Adynamically%20determine%20the%20optimal%20point%20to%20conclude%20its%20thought%20process%20and%0Aprovide%20a%20final%20answer%2C%20thus%20achieving%20efficient%20reasoning.%20Experimental%0Aresults%20across%20diverse%20mathematical%20benchmarks%20show%20that%20our%20approach%0Asubstantially%20mitigates%20overthinking%20without%20sacrificing%20problem-solving%0Aability.%20With%20our%20thinking%20paradigm%2C%20the%20average%20response%20length%20decreases%20by%0Aup%20to%2071%25%20on%20simpler%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20our%20method%20in%0Acreating%20a%20more%20efficient%20and%20adaptive%20reasoning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02249v1&entry.124074799=Read"},
{"title": "Learning to Reason for Hallucination Span Detection", "author": "Hsuan Su and Ting-Yao Hu and Hema Swetha Koppula and Kundan Krishna and Hadi Pouransari and Cheng-Yu Hsieh and Cem Koc and Joseph Yitan Cheng and Oncel Tuzel and Raviteja Vemulapalli", "abstract": "  Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.\n", "link": "http://arxiv.org/abs/2510.02173v1", "date": "2025-10-02", "relevancy": 1.9572, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Reason%20for%20Hallucination%20Span%20Detection&body=Title%3A%20Learning%20to%20Reason%20for%20Hallucination%20Span%20Detection%0AAuthor%3A%20Hsuan%20Su%20and%20Ting-Yao%20Hu%20and%20Hema%20Swetha%20Koppula%20and%20Kundan%20Krishna%20and%20Hadi%20Pouransari%20and%20Cheng-Yu%20Hsieh%20and%20Cem%20Koc%20and%20Joseph%20Yitan%20Cheng%20and%20Oncel%20Tuzel%20and%20Raviteja%20Vemulapalli%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20generate%20hallucinations%20--%20unsupported%0Acontent%20that%20undermines%20reliability.%20While%20most%20prior%20works%20frame%20hallucination%0Adetection%20as%20a%20binary%20task%2C%20many%20real-world%20applications%20require%20identifying%0Ahallucinated%20spans%2C%20which%20is%20a%20multi-step%20decision%20making%20process.%20This%0Anaturally%20raises%20the%20question%20of%20whether%20explicit%20reasoning%20can%20help%20the%0Acomplex%20task%20of%20detecting%20hallucination%20spans.%20To%20answer%20this%20question%2C%20we%0Afirst%20evaluate%20pretrained%20models%20with%20and%20without%20Chain-of-Thought%20%28CoT%29%0Areasoning%2C%20and%20show%20that%20CoT%20reasoning%20has%20the%20potential%20to%20generate%20at%20least%0Aone%20correct%20answer%20when%20sampled%20multiple%20times.%20Motivated%20by%20this%2C%20we%20propose%0ARL4HS%2C%20a%20reinforcement%20learning%20framework%20that%20incentivizes%20reasoning%20with%20a%0Aspan-level%20reward%20function.%20RL4HS%20builds%20on%20Group%20Relative%20Policy%20Optimization%0Aand%20introduces%20Class-Aware%20Policy%20Optimization%20to%20mitigate%20reward%20imbalance%0Aissue.%20Experiments%20on%20the%20RAGTruth%20benchmark%20%28summarization%2C%20question%0Aanswering%2C%20data-to-text%29%20show%20that%20RL4HS%20surpasses%20pretrained%20reasoning%20models%0Aand%20supervised%20fine-tuning%2C%20demonstrating%20the%20necessity%20of%20reinforcement%0Alearning%20with%20span-level%20rewards%20for%20detecting%20hallucination%20spans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Reason%2520for%2520Hallucination%2520Span%2520Detection%26entry.906535625%3DHsuan%2520Su%2520and%2520Ting-Yao%2520Hu%2520and%2520Hema%2520Swetha%2520Koppula%2520and%2520Kundan%2520Krishna%2520and%2520Hadi%2520Pouransari%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Cem%2520Koc%2520and%2520Joseph%2520Yitan%2520Cheng%2520and%2520Oncel%2520Tuzel%2520and%2520Raviteja%2520Vemulapalli%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520generate%2520hallucinations%2520--%2520unsupported%250Acontent%2520that%2520undermines%2520reliability.%2520While%2520most%2520prior%2520works%2520frame%2520hallucination%250Adetection%2520as%2520a%2520binary%2520task%252C%2520many%2520real-world%2520applications%2520require%2520identifying%250Ahallucinated%2520spans%252C%2520which%2520is%2520a%2520multi-step%2520decision%2520making%2520process.%2520This%250Anaturally%2520raises%2520the%2520question%2520of%2520whether%2520explicit%2520reasoning%2520can%2520help%2520the%250Acomplex%2520task%2520of%2520detecting%2520hallucination%2520spans.%2520To%2520answer%2520this%2520question%252C%2520we%250Afirst%2520evaluate%2520pretrained%2520models%2520with%2520and%2520without%2520Chain-of-Thought%2520%2528CoT%2529%250Areasoning%252C%2520and%2520show%2520that%2520CoT%2520reasoning%2520has%2520the%2520potential%2520to%2520generate%2520at%2520least%250Aone%2520correct%2520answer%2520when%2520sampled%2520multiple%2520times.%2520Motivated%2520by%2520this%252C%2520we%2520propose%250ARL4HS%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%2520incentivizes%2520reasoning%2520with%2520a%250Aspan-level%2520reward%2520function.%2520RL4HS%2520builds%2520on%2520Group%2520Relative%2520Policy%2520Optimization%250Aand%2520introduces%2520Class-Aware%2520Policy%2520Optimization%2520to%2520mitigate%2520reward%2520imbalance%250Aissue.%2520Experiments%2520on%2520the%2520RAGTruth%2520benchmark%2520%2528summarization%252C%2520question%250Aanswering%252C%2520data-to-text%2529%2520show%2520that%2520RL4HS%2520surpasses%2520pretrained%2520reasoning%2520models%250Aand%2520supervised%2520fine-tuning%252C%2520demonstrating%2520the%2520necessity%2520of%2520reinforcement%250Alearning%2520with%2520span-level%2520rewards%2520for%2520detecting%2520hallucination%2520spans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Reason%20for%20Hallucination%20Span%20Detection&entry.906535625=Hsuan%20Su%20and%20Ting-Yao%20Hu%20and%20Hema%20Swetha%20Koppula%20and%20Kundan%20Krishna%20and%20Hadi%20Pouransari%20and%20Cheng-Yu%20Hsieh%20and%20Cem%20Koc%20and%20Joseph%20Yitan%20Cheng%20and%20Oncel%20Tuzel%20and%20Raviteja%20Vemulapalli&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20generate%20hallucinations%20--%20unsupported%0Acontent%20that%20undermines%20reliability.%20While%20most%20prior%20works%20frame%20hallucination%0Adetection%20as%20a%20binary%20task%2C%20many%20real-world%20applications%20require%20identifying%0Ahallucinated%20spans%2C%20which%20is%20a%20multi-step%20decision%20making%20process.%20This%0Anaturally%20raises%20the%20question%20of%20whether%20explicit%20reasoning%20can%20help%20the%0Acomplex%20task%20of%20detecting%20hallucination%20spans.%20To%20answer%20this%20question%2C%20we%0Afirst%20evaluate%20pretrained%20models%20with%20and%20without%20Chain-of-Thought%20%28CoT%29%0Areasoning%2C%20and%20show%20that%20CoT%20reasoning%20has%20the%20potential%20to%20generate%20at%20least%0Aone%20correct%20answer%20when%20sampled%20multiple%20times.%20Motivated%20by%20this%2C%20we%20propose%0ARL4HS%2C%20a%20reinforcement%20learning%20framework%20that%20incentivizes%20reasoning%20with%20a%0Aspan-level%20reward%20function.%20RL4HS%20builds%20on%20Group%20Relative%20Policy%20Optimization%0Aand%20introduces%20Class-Aware%20Policy%20Optimization%20to%20mitigate%20reward%20imbalance%0Aissue.%20Experiments%20on%20the%20RAGTruth%20benchmark%20%28summarization%2C%20question%0Aanswering%2C%20data-to-text%29%20show%20that%20RL4HS%20surpasses%20pretrained%20reasoning%20models%0Aand%20supervised%20fine-tuning%2C%20demonstrating%20the%20necessity%20of%20reinforcement%0Alearning%20with%20span-level%20rewards%20for%20detecting%20hallucination%20spans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02173v1&entry.124074799=Read"},
{"title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling", "author": "Daniel Gallo Fern\u00e1ndez", "abstract": "  Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos.\n", "link": "http://arxiv.org/abs/2510.02206v1", "date": "2025-10-02", "relevancy": 1.9476, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5383}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4808}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poolformer%3A%20Recurrent%20Networks%20with%20Pooling%20for%20Long-Sequence%20Modeling&body=Title%3A%20Poolformer%3A%20Recurrent%20Networks%20with%20Pooling%20for%20Long-Sequence%20Modeling%0AAuthor%3A%20Daniel%20Gallo%20Fern%C3%A1ndez%0AAbstract%3A%20%20%20Sequence-to-sequence%20models%20have%20become%20central%20in%20Artificial%20Intelligence%2C%0Aparticularly%20following%20the%20introduction%20of%20the%20transformer%20architecture.%20While%0Ainitially%20developed%20for%20Natural%20Language%20Processing%2C%20these%20models%20have%0Ademonstrated%20utility%20across%20domains%2C%20including%20Computer%20Vision.%20Such%20models%0Arequire%20mechanisms%20to%20exchange%20information%20along%20the%20time%20dimension%2C%20typically%0Ausing%20recurrent%20or%20self-attention%20layers.%20However%2C%20self-attention%20scales%0Aquadratically%20with%20sequence%20length%2C%20limiting%20its%20practicality%20for%20very%20long%0Asequences.%0A%20%20We%20introduce%20Poolformer%2C%20a%20sequence-to-sequence%20model%20that%20replaces%0Aself-attention%20with%20recurrent%20layers%20and%20incorporates%20pooling%20operations%20to%0Areduce%20sequence%20length.%20Poolformer%20is%20defined%20recursively%20using%20SkipBlocks%2C%0Awhich%20contain%20residual%20blocks%2C%20a%20down-pooling%20layer%2C%20a%20nested%20SkipBlock%2C%20an%0Aup-pooling%20layer%2C%20and%20additional%20residual%20blocks.%20We%20conduct%20extensive%0Aexperiments%20to%20support%20our%20architectural%20choices.%0A%20%20Our%20results%20show%20that%20pooling%20greatly%20accelerates%20training%2C%20improves%0Aperceptual%20metrics%20%28FID%20and%20IS%29%2C%20and%20prevents%20overfitting.%20Our%20experiments%20also%0Asuggest%20that%20long-range%20dependencies%20are%20handled%20by%20deep%20layers%2C%20while%20shallow%0Alayers%20take%20care%20of%20short-term%20features.%0A%20%20Evaluated%20on%20raw%20audio%2C%20which%20naturally%20features%20long%20sequence%20lengths%2C%0APoolformer%20outperforms%20state-of-the-art%20models%20such%20as%20SaShiMi%20and%20Mamba.%0AFuture%20directions%20include%20applications%20to%20text%20and%20vision%2C%20as%20well%20as%0Amulti-modal%20scenarios%2C%20where%20a%20Poolformer-based%20LLM%20could%20effectively%20process%0Adense%20representations%20of%20images%20and%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoolformer%253A%2520Recurrent%2520Networks%2520with%2520Pooling%2520for%2520Long-Sequence%2520Modeling%26entry.906535625%3DDaniel%2520Gallo%2520Fern%25C3%25A1ndez%26entry.1292438233%3D%2520%2520Sequence-to-sequence%2520models%2520have%2520become%2520central%2520in%2520Artificial%2520Intelligence%252C%250Aparticularly%2520following%2520the%2520introduction%2520of%2520the%2520transformer%2520architecture.%2520While%250Ainitially%2520developed%2520for%2520Natural%2520Language%2520Processing%252C%2520these%2520models%2520have%250Ademonstrated%2520utility%2520across%2520domains%252C%2520including%2520Computer%2520Vision.%2520Such%2520models%250Arequire%2520mechanisms%2520to%2520exchange%2520information%2520along%2520the%2520time%2520dimension%252C%2520typically%250Ausing%2520recurrent%2520or%2520self-attention%2520layers.%2520However%252C%2520self-attention%2520scales%250Aquadratically%2520with%2520sequence%2520length%252C%2520limiting%2520its%2520practicality%2520for%2520very%2520long%250Asequences.%250A%2520%2520We%2520introduce%2520Poolformer%252C%2520a%2520sequence-to-sequence%2520model%2520that%2520replaces%250Aself-attention%2520with%2520recurrent%2520layers%2520and%2520incorporates%2520pooling%2520operations%2520to%250Areduce%2520sequence%2520length.%2520Poolformer%2520is%2520defined%2520recursively%2520using%2520SkipBlocks%252C%250Awhich%2520contain%2520residual%2520blocks%252C%2520a%2520down-pooling%2520layer%252C%2520a%2520nested%2520SkipBlock%252C%2520an%250Aup-pooling%2520layer%252C%2520and%2520additional%2520residual%2520blocks.%2520We%2520conduct%2520extensive%250Aexperiments%2520to%2520support%2520our%2520architectural%2520choices.%250A%2520%2520Our%2520results%2520show%2520that%2520pooling%2520greatly%2520accelerates%2520training%252C%2520improves%250Aperceptual%2520metrics%2520%2528FID%2520and%2520IS%2529%252C%2520and%2520prevents%2520overfitting.%2520Our%2520experiments%2520also%250Asuggest%2520that%2520long-range%2520dependencies%2520are%2520handled%2520by%2520deep%2520layers%252C%2520while%2520shallow%250Alayers%2520take%2520care%2520of%2520short-term%2520features.%250A%2520%2520Evaluated%2520on%2520raw%2520audio%252C%2520which%2520naturally%2520features%2520long%2520sequence%2520lengths%252C%250APoolformer%2520outperforms%2520state-of-the-art%2520models%2520such%2520as%2520SaShiMi%2520and%2520Mamba.%250AFuture%2520directions%2520include%2520applications%2520to%2520text%2520and%2520vision%252C%2520as%2520well%2520as%250Amulti-modal%2520scenarios%252C%2520where%2520a%2520Poolformer-based%2520LLM%2520could%2520effectively%2520process%250Adense%2520representations%2520of%2520images%2520and%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poolformer%3A%20Recurrent%20Networks%20with%20Pooling%20for%20Long-Sequence%20Modeling&entry.906535625=Daniel%20Gallo%20Fern%C3%A1ndez&entry.1292438233=%20%20Sequence-to-sequence%20models%20have%20become%20central%20in%20Artificial%20Intelligence%2C%0Aparticularly%20following%20the%20introduction%20of%20the%20transformer%20architecture.%20While%0Ainitially%20developed%20for%20Natural%20Language%20Processing%2C%20these%20models%20have%0Ademonstrated%20utility%20across%20domains%2C%20including%20Computer%20Vision.%20Such%20models%0Arequire%20mechanisms%20to%20exchange%20information%20along%20the%20time%20dimension%2C%20typically%0Ausing%20recurrent%20or%20self-attention%20layers.%20However%2C%20self-attention%20scales%0Aquadratically%20with%20sequence%20length%2C%20limiting%20its%20practicality%20for%20very%20long%0Asequences.%0A%20%20We%20introduce%20Poolformer%2C%20a%20sequence-to-sequence%20model%20that%20replaces%0Aself-attention%20with%20recurrent%20layers%20and%20incorporates%20pooling%20operations%20to%0Areduce%20sequence%20length.%20Poolformer%20is%20defined%20recursively%20using%20SkipBlocks%2C%0Awhich%20contain%20residual%20blocks%2C%20a%20down-pooling%20layer%2C%20a%20nested%20SkipBlock%2C%20an%0Aup-pooling%20layer%2C%20and%20additional%20residual%20blocks.%20We%20conduct%20extensive%0Aexperiments%20to%20support%20our%20architectural%20choices.%0A%20%20Our%20results%20show%20that%20pooling%20greatly%20accelerates%20training%2C%20improves%0Aperceptual%20metrics%20%28FID%20and%20IS%29%2C%20and%20prevents%20overfitting.%20Our%20experiments%20also%0Asuggest%20that%20long-range%20dependencies%20are%20handled%20by%20deep%20layers%2C%20while%20shallow%0Alayers%20take%20care%20of%20short-term%20features.%0A%20%20Evaluated%20on%20raw%20audio%2C%20which%20naturally%20features%20long%20sequence%20lengths%2C%0APoolformer%20outperforms%20state-of-the-art%20models%20such%20as%20SaShiMi%20and%20Mamba.%0AFuture%20directions%20include%20applications%20to%20text%20and%20vision%2C%20as%20well%20as%0Amulti-modal%20scenarios%2C%20where%20a%20Poolformer-based%20LLM%20could%20effectively%20process%0Adense%20representations%20of%20images%20and%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02206v1&entry.124074799=Read"},
{"title": "Differential Information Distribution: A Bayesian Perspective on Direct\n  Preference Optimization", "author": "Yunjae Won and Hyunji Lee and Hyeonbin Hwang and Minjoon Seo", "abstract": "  Direct Preference Optimization (DPO) has been widely used for aligning\nlanguage models with human preferences in a supervised manner. However, several\nkey questions remain unresolved: the rationale behind its log-ratio reward, how\nthe statistical structure of preference datasets shapes its training dynamics,\nand how those dynamics impact downstream capabilities. We approach these\nquestions from a Bayesian perspective, interpreting the goal of preference\noptimization as learning the differential information required to update a\nreference policy into a target policy. To formalize this view, we introduce the\nDifferential Information Distribution (DID), defined as the distribution over\nsamples that carry the Bayesian evidence required to update policies. We\nintroduce three complementary insights by viewing preference optimization\nthrough the DID. First, we find that DPO's log-ratio reward is uniquely\njustified when preferences encode the Differential Information needed to update\na reference policy into the target policy. Second, we discuss how commonly\nobserved training dynamics in DPO, including changes in log-likelihood and\npolicy exploration, stem from a power-law DID relationship. Finally, we analyze\nhow training dynamics influence downstream performance using the entropy of\nDID, a principled measure of uncertainty in the learned information. We observe\nthat learning high-entropy DID improves open-ended instruction-following, while\nlow-entropy DID benefits knowledge-intensive QA. Taken together, our results\nshow that DPO's reward design, training dynamics, and downstream capabilities\nall emerge as natural consequences of learning Differential Information,\noffering both a principled theoretical foundation and practical guidance for\npreference-based alignment.\n", "link": "http://arxiv.org/abs/2505.23761v2", "date": "2025-10-02", "relevancy": 1.911, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential%20Information%20Distribution%3A%20A%20Bayesian%20Perspective%20on%20Direct%0A%20%20Preference%20Optimization&body=Title%3A%20Differential%20Information%20Distribution%3A%20A%20Bayesian%20Perspective%20on%20Direct%0A%20%20Preference%20Optimization%0AAuthor%3A%20Yunjae%20Won%20and%20Hyunji%20Lee%20and%20Hyeonbin%20Hwang%20and%20Minjoon%20Seo%0AAbstract%3A%20%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20been%20widely%20used%20for%20aligning%0Alanguage%20models%20with%20human%20preferences%20in%20a%20supervised%20manner.%20However%2C%20several%0Akey%20questions%20remain%20unresolved%3A%20the%20rationale%20behind%20its%20log-ratio%20reward%2C%20how%0Athe%20statistical%20structure%20of%20preference%20datasets%20shapes%20its%20training%20dynamics%2C%0Aand%20how%20those%20dynamics%20impact%20downstream%20capabilities.%20We%20approach%20these%0Aquestions%20from%20a%20Bayesian%20perspective%2C%20interpreting%20the%20goal%20of%20preference%0Aoptimization%20as%20learning%20the%20differential%20information%20required%20to%20update%20a%0Areference%20policy%20into%20a%20target%20policy.%20To%20formalize%20this%20view%2C%20we%20introduce%20the%0ADifferential%20Information%20Distribution%20%28DID%29%2C%20defined%20as%20the%20distribution%20over%0Asamples%20that%20carry%20the%20Bayesian%20evidence%20required%20to%20update%20policies.%20We%0Aintroduce%20three%20complementary%20insights%20by%20viewing%20preference%20optimization%0Athrough%20the%20DID.%20First%2C%20we%20find%20that%20DPO%27s%20log-ratio%20reward%20is%20uniquely%0Ajustified%20when%20preferences%20encode%20the%20Differential%20Information%20needed%20to%20update%0Aa%20reference%20policy%20into%20the%20target%20policy.%20Second%2C%20we%20discuss%20how%20commonly%0Aobserved%20training%20dynamics%20in%20DPO%2C%20including%20changes%20in%20log-likelihood%20and%0Apolicy%20exploration%2C%20stem%20from%20a%20power-law%20DID%20relationship.%20Finally%2C%20we%20analyze%0Ahow%20training%20dynamics%20influence%20downstream%20performance%20using%20the%20entropy%20of%0ADID%2C%20a%20principled%20measure%20of%20uncertainty%20in%20the%20learned%20information.%20We%20observe%0Athat%20learning%20high-entropy%20DID%20improves%20open-ended%20instruction-following%2C%20while%0Alow-entropy%20DID%20benefits%20knowledge-intensive%20QA.%20Taken%20together%2C%20our%20results%0Ashow%20that%20DPO%27s%20reward%20design%2C%20training%20dynamics%2C%20and%20downstream%20capabilities%0Aall%20emerge%20as%20natural%20consequences%20of%20learning%20Differential%20Information%2C%0Aoffering%20both%20a%20principled%20theoretical%20foundation%20and%20practical%20guidance%20for%0Apreference-based%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential%2520Information%2520Distribution%253A%2520A%2520Bayesian%2520Perspective%2520on%2520Direct%250A%2520%2520Preference%2520Optimization%26entry.906535625%3DYunjae%2520Won%2520and%2520Hyunji%2520Lee%2520and%2520Hyeonbin%2520Hwang%2520and%2520Minjoon%2520Seo%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%2520been%2520widely%2520used%2520for%2520aligning%250Alanguage%2520models%2520with%2520human%2520preferences%2520in%2520a%2520supervised%2520manner.%2520However%252C%2520several%250Akey%2520questions%2520remain%2520unresolved%253A%2520the%2520rationale%2520behind%2520its%2520log-ratio%2520reward%252C%2520how%250Athe%2520statistical%2520structure%2520of%2520preference%2520datasets%2520shapes%2520its%2520training%2520dynamics%252C%250Aand%2520how%2520those%2520dynamics%2520impact%2520downstream%2520capabilities.%2520We%2520approach%2520these%250Aquestions%2520from%2520a%2520Bayesian%2520perspective%252C%2520interpreting%2520the%2520goal%2520of%2520preference%250Aoptimization%2520as%2520learning%2520the%2520differential%2520information%2520required%2520to%2520update%2520a%250Areference%2520policy%2520into%2520a%2520target%2520policy.%2520To%2520formalize%2520this%2520view%252C%2520we%2520introduce%2520the%250ADifferential%2520Information%2520Distribution%2520%2528DID%2529%252C%2520defined%2520as%2520the%2520distribution%2520over%250Asamples%2520that%2520carry%2520the%2520Bayesian%2520evidence%2520required%2520to%2520update%2520policies.%2520We%250Aintroduce%2520three%2520complementary%2520insights%2520by%2520viewing%2520preference%2520optimization%250Athrough%2520the%2520DID.%2520First%252C%2520we%2520find%2520that%2520DPO%2527s%2520log-ratio%2520reward%2520is%2520uniquely%250Ajustified%2520when%2520preferences%2520encode%2520the%2520Differential%2520Information%2520needed%2520to%2520update%250Aa%2520reference%2520policy%2520into%2520the%2520target%2520policy.%2520Second%252C%2520we%2520discuss%2520how%2520commonly%250Aobserved%2520training%2520dynamics%2520in%2520DPO%252C%2520including%2520changes%2520in%2520log-likelihood%2520and%250Apolicy%2520exploration%252C%2520stem%2520from%2520a%2520power-law%2520DID%2520relationship.%2520Finally%252C%2520we%2520analyze%250Ahow%2520training%2520dynamics%2520influence%2520downstream%2520performance%2520using%2520the%2520entropy%2520of%250ADID%252C%2520a%2520principled%2520measure%2520of%2520uncertainty%2520in%2520the%2520learned%2520information.%2520We%2520observe%250Athat%2520learning%2520high-entropy%2520DID%2520improves%2520open-ended%2520instruction-following%252C%2520while%250Alow-entropy%2520DID%2520benefits%2520knowledge-intensive%2520QA.%2520Taken%2520together%252C%2520our%2520results%250Ashow%2520that%2520DPO%2527s%2520reward%2520design%252C%2520training%2520dynamics%252C%2520and%2520downstream%2520capabilities%250Aall%2520emerge%2520as%2520natural%2520consequences%2520of%2520learning%2520Differential%2520Information%252C%250Aoffering%2520both%2520a%2520principled%2520theoretical%2520foundation%2520and%2520practical%2520guidance%2520for%250Apreference-based%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential%20Information%20Distribution%3A%20A%20Bayesian%20Perspective%20on%20Direct%0A%20%20Preference%20Optimization&entry.906535625=Yunjae%20Won%20and%20Hyunji%20Lee%20and%20Hyeonbin%20Hwang%20and%20Minjoon%20Seo&entry.1292438233=%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20been%20widely%20used%20for%20aligning%0Alanguage%20models%20with%20human%20preferences%20in%20a%20supervised%20manner.%20However%2C%20several%0Akey%20questions%20remain%20unresolved%3A%20the%20rationale%20behind%20its%20log-ratio%20reward%2C%20how%0Athe%20statistical%20structure%20of%20preference%20datasets%20shapes%20its%20training%20dynamics%2C%0Aand%20how%20those%20dynamics%20impact%20downstream%20capabilities.%20We%20approach%20these%0Aquestions%20from%20a%20Bayesian%20perspective%2C%20interpreting%20the%20goal%20of%20preference%0Aoptimization%20as%20learning%20the%20differential%20information%20required%20to%20update%20a%0Areference%20policy%20into%20a%20target%20policy.%20To%20formalize%20this%20view%2C%20we%20introduce%20the%0ADifferential%20Information%20Distribution%20%28DID%29%2C%20defined%20as%20the%20distribution%20over%0Asamples%20that%20carry%20the%20Bayesian%20evidence%20required%20to%20update%20policies.%20We%0Aintroduce%20three%20complementary%20insights%20by%20viewing%20preference%20optimization%0Athrough%20the%20DID.%20First%2C%20we%20find%20that%20DPO%27s%20log-ratio%20reward%20is%20uniquely%0Ajustified%20when%20preferences%20encode%20the%20Differential%20Information%20needed%20to%20update%0Aa%20reference%20policy%20into%20the%20target%20policy.%20Second%2C%20we%20discuss%20how%20commonly%0Aobserved%20training%20dynamics%20in%20DPO%2C%20including%20changes%20in%20log-likelihood%20and%0Apolicy%20exploration%2C%20stem%20from%20a%20power-law%20DID%20relationship.%20Finally%2C%20we%20analyze%0Ahow%20training%20dynamics%20influence%20downstream%20performance%20using%20the%20entropy%20of%0ADID%2C%20a%20principled%20measure%20of%20uncertainty%20in%20the%20learned%20information.%20We%20observe%0Athat%20learning%20high-entropy%20DID%20improves%20open-ended%20instruction-following%2C%20while%0Alow-entropy%20DID%20benefits%20knowledge-intensive%20QA.%20Taken%20together%2C%20our%20results%0Ashow%20that%20DPO%27s%20reward%20design%2C%20training%20dynamics%2C%20and%20downstream%20capabilities%0Aall%20emerge%20as%20natural%20consequences%20of%20learning%20Differential%20Information%2C%0Aoffering%20both%20a%20principled%20theoretical%20foundation%20and%20practical%20guidance%20for%0Apreference-based%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23761v2&entry.124074799=Read"},
{"title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities", "author": "Felix Brei and Lorenz B\u00fchmann and Johannes Frey and Daniel Gerber and Lars-Peter Meyer and Claus Stadler and Kirill Bulert", "abstract": "  Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.\n", "link": "http://arxiv.org/abs/2510.02200v1", "date": "2025-10-02", "relevancy": 1.8846, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARUQULA%20--%20An%20LLM%20based%20Text2SPARQL%20Approach%20using%20ReAct%20and%20Knowledge%0A%20%20Graph%20Exploration%20Utilities&body=Title%3A%20ARUQULA%20--%20An%20LLM%20based%20Text2SPARQL%20Approach%20using%20ReAct%20and%20Knowledge%0A%20%20Graph%20Exploration%20Utilities%0AAuthor%3A%20Felix%20Brei%20and%20Lorenz%20B%C3%BChmann%20and%20Johannes%20Frey%20and%20Daniel%20Gerber%20and%20Lars-Peter%20Meyer%20and%20Claus%20Stadler%20and%20Kirill%20Bulert%0AAbstract%3A%20%20%20Interacting%20with%20knowledge%20graphs%20can%20be%20a%20daunting%20task%20for%20people%20without%20a%0Abackground%20in%20computer%20science%20since%20the%20query%20language%20that%20is%20used%20%28SPARQL%29%0Ahas%20a%20high%20barrier%20of%20entry.%20Large%20language%20models%20%28LLMs%29%20can%20lower%20that%0Abarrier%20by%20providing%20support%20in%20the%20form%20of%20Text2SPARQL%20translation.%20In%20this%0Apaper%20we%20introduce%20a%20generalized%20method%20based%20on%20SPINACH%2C%20an%20LLM%20backed%20agent%0Athat%20translates%20natural%20language%20questions%20to%20SPARQL%20queries%20not%20in%20a%20single%0Ashot%2C%20but%20as%20an%20iterative%20process%20of%20exploration%20and%20execution.%20We%20describe%20the%0Aoverall%20architecture%20and%20reasoning%20behind%20our%20design%20decisions%2C%20and%20also%0Aconduct%20a%20thorough%20analysis%20of%20the%20agent%20behavior%20to%20gain%20insights%20into%20future%0Aareas%20for%20targeted%20improvements.%20This%20work%20was%20motivated%20by%20the%20Text2SPARQL%0Achallenge%2C%20a%20challenge%20that%20was%20held%20to%20facilitate%20improvements%20in%20the%0AText2SPARQL%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARUQULA%2520--%2520An%2520LLM%2520based%2520Text2SPARQL%2520Approach%2520using%2520ReAct%2520and%2520Knowledge%250A%2520%2520Graph%2520Exploration%2520Utilities%26entry.906535625%3DFelix%2520Brei%2520and%2520Lorenz%2520B%25C3%25BChmann%2520and%2520Johannes%2520Frey%2520and%2520Daniel%2520Gerber%2520and%2520Lars-Peter%2520Meyer%2520and%2520Claus%2520Stadler%2520and%2520Kirill%2520Bulert%26entry.1292438233%3D%2520%2520Interacting%2520with%2520knowledge%2520graphs%2520can%2520be%2520a%2520daunting%2520task%2520for%2520people%2520without%2520a%250Abackground%2520in%2520computer%2520science%2520since%2520the%2520query%2520language%2520that%2520is%2520used%2520%2528SPARQL%2529%250Ahas%2520a%2520high%2520barrier%2520of%2520entry.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520lower%2520that%250Abarrier%2520by%2520providing%2520support%2520in%2520the%2520form%2520of%2520Text2SPARQL%2520translation.%2520In%2520this%250Apaper%2520we%2520introduce%2520a%2520generalized%2520method%2520based%2520on%2520SPINACH%252C%2520an%2520LLM%2520backed%2520agent%250Athat%2520translates%2520natural%2520language%2520questions%2520to%2520SPARQL%2520queries%2520not%2520in%2520a%2520single%250Ashot%252C%2520but%2520as%2520an%2520iterative%2520process%2520of%2520exploration%2520and%2520execution.%2520We%2520describe%2520the%250Aoverall%2520architecture%2520and%2520reasoning%2520behind%2520our%2520design%2520decisions%252C%2520and%2520also%250Aconduct%2520a%2520thorough%2520analysis%2520of%2520the%2520agent%2520behavior%2520to%2520gain%2520insights%2520into%2520future%250Aareas%2520for%2520targeted%2520improvements.%2520This%2520work%2520was%2520motivated%2520by%2520the%2520Text2SPARQL%250Achallenge%252C%2520a%2520challenge%2520that%2520was%2520held%2520to%2520facilitate%2520improvements%2520in%2520the%250AText2SPARQL%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARUQULA%20--%20An%20LLM%20based%20Text2SPARQL%20Approach%20using%20ReAct%20and%20Knowledge%0A%20%20Graph%20Exploration%20Utilities&entry.906535625=Felix%20Brei%20and%20Lorenz%20B%C3%BChmann%20and%20Johannes%20Frey%20and%20Daniel%20Gerber%20and%20Lars-Peter%20Meyer%20and%20Claus%20Stadler%20and%20Kirill%20Bulert&entry.1292438233=%20%20Interacting%20with%20knowledge%20graphs%20can%20be%20a%20daunting%20task%20for%20people%20without%20a%0Abackground%20in%20computer%20science%20since%20the%20query%20language%20that%20is%20used%20%28SPARQL%29%0Ahas%20a%20high%20barrier%20of%20entry.%20Large%20language%20models%20%28LLMs%29%20can%20lower%20that%0Abarrier%20by%20providing%20support%20in%20the%20form%20of%20Text2SPARQL%20translation.%20In%20this%0Apaper%20we%20introduce%20a%20generalized%20method%20based%20on%20SPINACH%2C%20an%20LLM%20backed%20agent%0Athat%20translates%20natural%20language%20questions%20to%20SPARQL%20queries%20not%20in%20a%20single%0Ashot%2C%20but%20as%20an%20iterative%20process%20of%20exploration%20and%20execution.%20We%20describe%20the%0Aoverall%20architecture%20and%20reasoning%20behind%20our%20design%20decisions%2C%20and%20also%0Aconduct%20a%20thorough%20analysis%20of%20the%20agent%20behavior%20to%20gain%20insights%20into%20future%0Aareas%20for%20targeted%20improvements.%20This%20work%20was%20motivated%20by%20the%20Text2SPARQL%0Achallenge%2C%20a%20challenge%20that%20was%20held%20to%20facilitate%20improvements%20in%20the%0AText2SPARQL%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02200v1&entry.124074799=Read"},
{"title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet\n  Challenge 2025", "author": "Matthew A. Reyna and Zuzana Koscova and Jan Pavlus and Soheil Saghafi and James Weigle and Andoni Elola and Salman Seyedi and Kiersten Campbell and Qiao Li and Ali Bahrami Rad and Ant\u00f4nio H. Ribeiro and Antonio Luiz P. Ribeiro and Reza Sameni and Gari D. Clifford", "abstract": "  Objective: Chagas disease is a parasitic infection that is endemic to South\nAmerica, Central America, and, more recently, the U.S., primarily transmitted\nby insects. Chronic Chagas disease can cause cardiovascular diseases and\ndigestive problems. Serological testing capacities for Chagas disease are\nlimited, but Chagas cardiomyopathy often manifests in ECGs, providing an\nopportunity to prioritize patients for testing and treatment. Approach: The\nGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic\napproaches for identifying Chagas disease from electrocardiograms (ECGs). Main\nresults: This Challenge provides multiple innovations. First, we leveraged\nseveral datasets with labels from patient reports and serological testing,\nprovided a large dataset with weak labels and smaller datasets with strong\nlabels. Second, we augmented the data to support model robustness and\ngeneralizability to unseen data sources. Third, we applied an evaluation metric\nthat captured the local serological testing capacity for Chagas disease to\nframe the machine learning problem as a triage task. Significance: Over 630\nparticipants from 111 teams submitted over 1300 entries during the Challenge,\nrepresenting diverse approaches from academia and industry worldwide.\n", "link": "http://arxiv.org/abs/2510.02202v1", "date": "2025-10-02", "relevancy": 1.8827, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3881}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3832}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20of%20Chagas%20Disease%20from%20the%20ECG%3A%20The%20George%20B.%20Moody%20PhysioNet%0A%20%20Challenge%202025&body=Title%3A%20Detection%20of%20Chagas%20Disease%20from%20the%20ECG%3A%20The%20George%20B.%20Moody%20PhysioNet%0A%20%20Challenge%202025%0AAuthor%3A%20Matthew%20A.%20Reyna%20and%20Zuzana%20Koscova%20and%20Jan%20Pavlus%20and%20Soheil%20Saghafi%20and%20James%20Weigle%20and%20Andoni%20Elola%20and%20Salman%20Seyedi%20and%20Kiersten%20Campbell%20and%20Qiao%20Li%20and%20Ali%20Bahrami%20Rad%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20Antonio%20Luiz%20P.%20Ribeiro%20and%20Reza%20Sameni%20and%20Gari%20D.%20Clifford%0AAbstract%3A%20%20%20Objective%3A%20Chagas%20disease%20is%20a%20parasitic%20infection%20that%20is%20endemic%20to%20South%0AAmerica%2C%20Central%20America%2C%20and%2C%20more%20recently%2C%20the%20U.S.%2C%20primarily%20transmitted%0Aby%20insects.%20Chronic%20Chagas%20disease%20can%20cause%20cardiovascular%20diseases%20and%0Adigestive%20problems.%20Serological%20testing%20capacities%20for%20Chagas%20disease%20are%0Alimited%2C%20but%20Chagas%20cardiomyopathy%20often%20manifests%20in%20ECGs%2C%20providing%20an%0Aopportunity%20to%20prioritize%20patients%20for%20testing%20and%20treatment.%20Approach%3A%20The%0AGeorge%20B.%20Moody%20PhysioNet%20Challenge%202025%20invites%20teams%20to%20develop%20algorithmic%0Aapproaches%20for%20identifying%20Chagas%20disease%20from%20electrocardiograms%20%28ECGs%29.%20Main%0Aresults%3A%20This%20Challenge%20provides%20multiple%20innovations.%20First%2C%20we%20leveraged%0Aseveral%20datasets%20with%20labels%20from%20patient%20reports%20and%20serological%20testing%2C%0Aprovided%20a%20large%20dataset%20with%20weak%20labels%20and%20smaller%20datasets%20with%20strong%0Alabels.%20Second%2C%20we%20augmented%20the%20data%20to%20support%20model%20robustness%20and%0Ageneralizability%20to%20unseen%20data%20sources.%20Third%2C%20we%20applied%20an%20evaluation%20metric%0Athat%20captured%20the%20local%20serological%20testing%20capacity%20for%20Chagas%20disease%20to%0Aframe%20the%20machine%20learning%20problem%20as%20a%20triage%20task.%20Significance%3A%20Over%20630%0Aparticipants%20from%20111%20teams%20submitted%20over%201300%20entries%20during%20the%20Challenge%2C%0Arepresenting%20diverse%20approaches%20from%20academia%20and%20industry%20worldwide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520of%2520Chagas%2520Disease%2520from%2520the%2520ECG%253A%2520The%2520George%2520B.%2520Moody%2520PhysioNet%250A%2520%2520Challenge%25202025%26entry.906535625%3DMatthew%2520A.%2520Reyna%2520and%2520Zuzana%2520Koscova%2520and%2520Jan%2520Pavlus%2520and%2520Soheil%2520Saghafi%2520and%2520James%2520Weigle%2520and%2520Andoni%2520Elola%2520and%2520Salman%2520Seyedi%2520and%2520Kiersten%2520Campbell%2520and%2520Qiao%2520Li%2520and%2520Ali%2520Bahrami%2520Rad%2520and%2520Ant%25C3%25B4nio%2520H.%2520Ribeiro%2520and%2520Antonio%2520Luiz%2520P.%2520Ribeiro%2520and%2520Reza%2520Sameni%2520and%2520Gari%2520D.%2520Clifford%26entry.1292438233%3D%2520%2520Objective%253A%2520Chagas%2520disease%2520is%2520a%2520parasitic%2520infection%2520that%2520is%2520endemic%2520to%2520South%250AAmerica%252C%2520Central%2520America%252C%2520and%252C%2520more%2520recently%252C%2520the%2520U.S.%252C%2520primarily%2520transmitted%250Aby%2520insects.%2520Chronic%2520Chagas%2520disease%2520can%2520cause%2520cardiovascular%2520diseases%2520and%250Adigestive%2520problems.%2520Serological%2520testing%2520capacities%2520for%2520Chagas%2520disease%2520are%250Alimited%252C%2520but%2520Chagas%2520cardiomyopathy%2520often%2520manifests%2520in%2520ECGs%252C%2520providing%2520an%250Aopportunity%2520to%2520prioritize%2520patients%2520for%2520testing%2520and%2520treatment.%2520Approach%253A%2520The%250AGeorge%2520B.%2520Moody%2520PhysioNet%2520Challenge%25202025%2520invites%2520teams%2520to%2520develop%2520algorithmic%250Aapproaches%2520for%2520identifying%2520Chagas%2520disease%2520from%2520electrocardiograms%2520%2528ECGs%2529.%2520Main%250Aresults%253A%2520This%2520Challenge%2520provides%2520multiple%2520innovations.%2520First%252C%2520we%2520leveraged%250Aseveral%2520datasets%2520with%2520labels%2520from%2520patient%2520reports%2520and%2520serological%2520testing%252C%250Aprovided%2520a%2520large%2520dataset%2520with%2520weak%2520labels%2520and%2520smaller%2520datasets%2520with%2520strong%250Alabels.%2520Second%252C%2520we%2520augmented%2520the%2520data%2520to%2520support%2520model%2520robustness%2520and%250Ageneralizability%2520to%2520unseen%2520data%2520sources.%2520Third%252C%2520we%2520applied%2520an%2520evaluation%2520metric%250Athat%2520captured%2520the%2520local%2520serological%2520testing%2520capacity%2520for%2520Chagas%2520disease%2520to%250Aframe%2520the%2520machine%2520learning%2520problem%2520as%2520a%2520triage%2520task.%2520Significance%253A%2520Over%2520630%250Aparticipants%2520from%2520111%2520teams%2520submitted%2520over%25201300%2520entries%2520during%2520the%2520Challenge%252C%250Arepresenting%2520diverse%2520approaches%2520from%2520academia%2520and%2520industry%2520worldwide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20Chagas%20Disease%20from%20the%20ECG%3A%20The%20George%20B.%20Moody%20PhysioNet%0A%20%20Challenge%202025&entry.906535625=Matthew%20A.%20Reyna%20and%20Zuzana%20Koscova%20and%20Jan%20Pavlus%20and%20Soheil%20Saghafi%20and%20James%20Weigle%20and%20Andoni%20Elola%20and%20Salman%20Seyedi%20and%20Kiersten%20Campbell%20and%20Qiao%20Li%20and%20Ali%20Bahrami%20Rad%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20Antonio%20Luiz%20P.%20Ribeiro%20and%20Reza%20Sameni%20and%20Gari%20D.%20Clifford&entry.1292438233=%20%20Objective%3A%20Chagas%20disease%20is%20a%20parasitic%20infection%20that%20is%20endemic%20to%20South%0AAmerica%2C%20Central%20America%2C%20and%2C%20more%20recently%2C%20the%20U.S.%2C%20primarily%20transmitted%0Aby%20insects.%20Chronic%20Chagas%20disease%20can%20cause%20cardiovascular%20diseases%20and%0Adigestive%20problems.%20Serological%20testing%20capacities%20for%20Chagas%20disease%20are%0Alimited%2C%20but%20Chagas%20cardiomyopathy%20often%20manifests%20in%20ECGs%2C%20providing%20an%0Aopportunity%20to%20prioritize%20patients%20for%20testing%20and%20treatment.%20Approach%3A%20The%0AGeorge%20B.%20Moody%20PhysioNet%20Challenge%202025%20invites%20teams%20to%20develop%20algorithmic%0Aapproaches%20for%20identifying%20Chagas%20disease%20from%20electrocardiograms%20%28ECGs%29.%20Main%0Aresults%3A%20This%20Challenge%20provides%20multiple%20innovations.%20First%2C%20we%20leveraged%0Aseveral%20datasets%20with%20labels%20from%20patient%20reports%20and%20serological%20testing%2C%0Aprovided%20a%20large%20dataset%20with%20weak%20labels%20and%20smaller%20datasets%20with%20strong%0Alabels.%20Second%2C%20we%20augmented%20the%20data%20to%20support%20model%20robustness%20and%0Ageneralizability%20to%20unseen%20data%20sources.%20Third%2C%20we%20applied%20an%20evaluation%20metric%0Athat%20captured%20the%20local%20serological%20testing%20capacity%20for%20Chagas%20disease%20to%0Aframe%20the%20machine%20learning%20problem%20as%20a%20triage%20task.%20Significance%3A%20Over%20630%0Aparticipants%20from%20111%20teams%20submitted%20over%201300%20entries%20during%20the%20Challenge%2C%0Arepresenting%20diverse%20approaches%20from%20academia%20and%20industry%20worldwide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02202v1&entry.124074799=Read"},
{"title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models", "author": "Shira Schiber and Ofir Lindenbaum and Idan Schwartz", "abstract": "  Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.\n", "link": "http://arxiv.org/abs/2510.02226v1", "date": "2025-10-02", "relevancy": 1.8803, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6527}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6227}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempoControl%3A%20Temporal%20Attention%20Guidance%20for%20Text-to-Video%20Models&body=Title%3A%20TempoControl%3A%20Temporal%20Attention%20Guidance%20for%20Text-to-Video%20Models%0AAuthor%3A%20Shira%20Schiber%20and%20Ofir%20Lindenbaum%20and%20Idan%20Schwartz%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20video%20models%20have%20enabled%20the%20creation%20of%0Ahigh-quality%20videos%20based%20on%20natural%20language%20prompts.%20However%2C%20these%20models%0Afrequently%20lack%20fine-grained%20temporal%20control%2C%20meaning%20they%20do%20not%20allow%20users%0Ato%20specify%20when%20particular%20visual%20elements%20should%20appear%20within%20a%20generated%0Asequence.%20In%20this%20work%2C%20we%20introduce%20TempoControl%2C%20a%20method%20that%20allows%20for%0Atemporal%20alignment%20of%20visual%20concepts%20during%20inference%2C%20without%20requiring%0Aretraining%20or%20additional%20supervision.%20TempoControl%20utilizes%20cross-attention%0Amaps%2C%20a%20key%20component%20of%20text-to-video%20diffusion%20models%2C%20to%20guide%20the%20timing%20of%0Aconcepts%20through%20a%20novel%20optimization%20approach.%20Our%20method%20steers%20attention%0Ausing%20three%20complementary%20principles%3A%20aligning%20its%20temporal%20shape%20with%20a%0Acontrol%20signal%20%28via%20correlation%29%2C%20amplifying%20it%20where%20visibility%20is%20needed%20%28via%0Aenergy%29%2C%20and%20maintaining%20spatial%20focus%20%28via%20entropy%29.%20TempoControl%20allows%0Aprecise%20control%20over%20timing%20while%20ensuring%20high%20video%20quality%20and%20diversity.%20We%0Ademonstrate%20its%20effectiveness%20across%20various%20video%20generation%20applications%2C%0Aincluding%20temporal%20reordering%20for%20single%20and%20multiple%20objects%2C%20as%20well%20as%0Aaction%20and%20audio-aligned%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempoControl%253A%2520Temporal%2520Attention%2520Guidance%2520for%2520Text-to-Video%2520Models%26entry.906535625%3DShira%2520Schiber%2520and%2520Ofir%2520Lindenbaum%2520and%2520Idan%2520Schwartz%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520video%2520models%2520have%2520enabled%2520the%2520creation%2520of%250Ahigh-quality%2520videos%2520based%2520on%2520natural%2520language%2520prompts.%2520However%252C%2520these%2520models%250Afrequently%2520lack%2520fine-grained%2520temporal%2520control%252C%2520meaning%2520they%2520do%2520not%2520allow%2520users%250Ato%2520specify%2520when%2520particular%2520visual%2520elements%2520should%2520appear%2520within%2520a%2520generated%250Asequence.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TempoControl%252C%2520a%2520method%2520that%2520allows%2520for%250Atemporal%2520alignment%2520of%2520visual%2520concepts%2520during%2520inference%252C%2520without%2520requiring%250Aretraining%2520or%2520additional%2520supervision.%2520TempoControl%2520utilizes%2520cross-attention%250Amaps%252C%2520a%2520key%2520component%2520of%2520text-to-video%2520diffusion%2520models%252C%2520to%2520guide%2520the%2520timing%2520of%250Aconcepts%2520through%2520a%2520novel%2520optimization%2520approach.%2520Our%2520method%2520steers%2520attention%250Ausing%2520three%2520complementary%2520principles%253A%2520aligning%2520its%2520temporal%2520shape%2520with%2520a%250Acontrol%2520signal%2520%2528via%2520correlation%2529%252C%2520amplifying%2520it%2520where%2520visibility%2520is%2520needed%2520%2528via%250Aenergy%2529%252C%2520and%2520maintaining%2520spatial%2520focus%2520%2528via%2520entropy%2529.%2520TempoControl%2520allows%250Aprecise%2520control%2520over%2520timing%2520while%2520ensuring%2520high%2520video%2520quality%2520and%2520diversity.%2520We%250Ademonstrate%2520its%2520effectiveness%2520across%2520various%2520video%2520generation%2520applications%252C%250Aincluding%2520temporal%2520reordering%2520for%2520single%2520and%2520multiple%2520objects%252C%2520as%2520well%2520as%250Aaction%2520and%2520audio-aligned%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempoControl%3A%20Temporal%20Attention%20Guidance%20for%20Text-to-Video%20Models&entry.906535625=Shira%20Schiber%20and%20Ofir%20Lindenbaum%20and%20Idan%20Schwartz&entry.1292438233=%20%20Recent%20advances%20in%20generative%20video%20models%20have%20enabled%20the%20creation%20of%0Ahigh-quality%20videos%20based%20on%20natural%20language%20prompts.%20However%2C%20these%20models%0Afrequently%20lack%20fine-grained%20temporal%20control%2C%20meaning%20they%20do%20not%20allow%20users%0Ato%20specify%20when%20particular%20visual%20elements%20should%20appear%20within%20a%20generated%0Asequence.%20In%20this%20work%2C%20we%20introduce%20TempoControl%2C%20a%20method%20that%20allows%20for%0Atemporal%20alignment%20of%20visual%20concepts%20during%20inference%2C%20without%20requiring%0Aretraining%20or%20additional%20supervision.%20TempoControl%20utilizes%20cross-attention%0Amaps%2C%20a%20key%20component%20of%20text-to-video%20diffusion%20models%2C%20to%20guide%20the%20timing%20of%0Aconcepts%20through%20a%20novel%20optimization%20approach.%20Our%20method%20steers%20attention%0Ausing%20three%20complementary%20principles%3A%20aligning%20its%20temporal%20shape%20with%20a%0Acontrol%20signal%20%28via%20correlation%29%2C%20amplifying%20it%20where%20visibility%20is%20needed%20%28via%0Aenergy%29%2C%20and%20maintaining%20spatial%20focus%20%28via%20entropy%29.%20TempoControl%20allows%0Aprecise%20control%20over%20timing%20while%20ensuring%20high%20video%20quality%20and%20diversity.%20We%0Ademonstrate%20its%20effectiveness%20across%20various%20video%20generation%20applications%2C%0Aincluding%20temporal%20reordering%20for%20single%20and%20multiple%20objects%2C%20as%20well%20as%0Aaction%20and%20audio-aligned%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02226v1&entry.124074799=Read"},
{"title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks", "author": "Ruohao Guo and Afshin Oroojlooy and Roshan Sridhar and Miguel Ballesteros and Alan Ritter and Dan Roth", "abstract": "  Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.\n", "link": "http://arxiv.org/abs/2510.02286v1", "date": "2025-10-02", "relevancy": 1.8602, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4606}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree-based%20Dialogue%20Reinforced%20Policy%20Optimization%20for%20Red-Teaming%0A%20%20Attacks&body=Title%3A%20Tree-based%20Dialogue%20Reinforced%20Policy%20Optimization%20for%20Red-Teaming%0A%20%20Attacks%0AAuthor%3A%20Ruohao%20Guo%20and%20Afshin%20Oroojlooy%20and%20Roshan%20Sridhar%20and%20Miguel%20Ballesteros%20and%20Alan%20Ritter%20and%20Dan%20Roth%0AAbstract%3A%20%20%20Despite%20recent%20rapid%20progress%20in%20AI%20safety%2C%20current%20large%20language%20models%0Aremain%20vulnerable%20to%20adversarial%20attacks%20in%20multi-turn%20interaction%20settings%2C%0Awhere%20attackers%20strategically%20adapt%20their%20prompts%20across%20conversation%20turns%20and%0Apose%20a%20more%20critical%20yet%20realistic%20challenge.%20Existing%20approaches%20that%20discover%0Asafety%20vulnerabilities%20either%20rely%20on%20manual%20red-teaming%20with%20human%20experts%20or%0Aemploy%20automated%20methods%20using%20pre-defined%20templates%20and%20human-curated%20attack%0Adata%2C%20with%20most%20focusing%20on%20single-turn%20attacks.%20However%2C%20these%20methods%20did%20not%0Aexplore%20the%20vast%20space%20of%20possible%20multi-turn%20attacks%2C%20failing%20to%20consider%0Anovel%20attack%20trajectories%20that%20emerge%20from%20complex%20dialogue%20dynamics%20and%0Astrategic%20conversation%20planning.%20This%20gap%20is%20particularly%20critical%20given%20recent%0Afindings%20that%20LLMs%20exhibit%20significantly%20higher%20vulnerability%20to%20multi-turn%0Aattacks%20compared%20to%20single-turn%20attacks.%20We%20propose%20DialTree-RPO%2C%20an%20on-policy%0Areinforcement%20learning%20framework%20integrated%20with%20tree%20search%20that%20autonomously%0Adiscovers%20diverse%20multi-turn%20attack%20strategies%20by%20treating%20the%20dialogue%20as%20a%0Asequential%20decision-making%20problem%2C%20enabling%20systematic%20exploration%20without%0Amanually%20curated%20data.%20Through%20extensive%20experiments%2C%20our%20approach%20not%20only%0Aachieves%20more%20than%2025.9%25%20higher%20ASR%20across%2010%20target%20models%20compared%20to%0Aprevious%20state-of-the-art%20approaches%2C%20but%20also%20effectively%20uncovers%20new%20attack%0Astrategies%20by%20learning%20optimal%20dialogue%20policies%20that%20maximize%20attack%20success%0Aacross%20multiple%20turns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree-based%2520Dialogue%2520Reinforced%2520Policy%2520Optimization%2520for%2520Red-Teaming%250A%2520%2520Attacks%26entry.906535625%3DRuohao%2520Guo%2520and%2520Afshin%2520Oroojlooy%2520and%2520Roshan%2520Sridhar%2520and%2520Miguel%2520Ballesteros%2520and%2520Alan%2520Ritter%2520and%2520Dan%2520Roth%26entry.1292438233%3D%2520%2520Despite%2520recent%2520rapid%2520progress%2520in%2520AI%2520safety%252C%2520current%2520large%2520language%2520models%250Aremain%2520vulnerable%2520to%2520adversarial%2520attacks%2520in%2520multi-turn%2520interaction%2520settings%252C%250Awhere%2520attackers%2520strategically%2520adapt%2520their%2520prompts%2520across%2520conversation%2520turns%2520and%250Apose%2520a%2520more%2520critical%2520yet%2520realistic%2520challenge.%2520Existing%2520approaches%2520that%2520discover%250Asafety%2520vulnerabilities%2520either%2520rely%2520on%2520manual%2520red-teaming%2520with%2520human%2520experts%2520or%250Aemploy%2520automated%2520methods%2520using%2520pre-defined%2520templates%2520and%2520human-curated%2520attack%250Adata%252C%2520with%2520most%2520focusing%2520on%2520single-turn%2520attacks.%2520However%252C%2520these%2520methods%2520did%2520not%250Aexplore%2520the%2520vast%2520space%2520of%2520possible%2520multi-turn%2520attacks%252C%2520failing%2520to%2520consider%250Anovel%2520attack%2520trajectories%2520that%2520emerge%2520from%2520complex%2520dialogue%2520dynamics%2520and%250Astrategic%2520conversation%2520planning.%2520This%2520gap%2520is%2520particularly%2520critical%2520given%2520recent%250Afindings%2520that%2520LLMs%2520exhibit%2520significantly%2520higher%2520vulnerability%2520to%2520multi-turn%250Aattacks%2520compared%2520to%2520single-turn%2520attacks.%2520We%2520propose%2520DialTree-RPO%252C%2520an%2520on-policy%250Areinforcement%2520learning%2520framework%2520integrated%2520with%2520tree%2520search%2520that%2520autonomously%250Adiscovers%2520diverse%2520multi-turn%2520attack%2520strategies%2520by%2520treating%2520the%2520dialogue%2520as%2520a%250Asequential%2520decision-making%2520problem%252C%2520enabling%2520systematic%2520exploration%2520without%250Amanually%2520curated%2520data.%2520Through%2520extensive%2520experiments%252C%2520our%2520approach%2520not%2520only%250Aachieves%2520more%2520than%252025.9%2525%2520higher%2520ASR%2520across%252010%2520target%2520models%2520compared%2520to%250Aprevious%2520state-of-the-art%2520approaches%252C%2520but%2520also%2520effectively%2520uncovers%2520new%2520attack%250Astrategies%2520by%2520learning%2520optimal%2520dialogue%2520policies%2520that%2520maximize%2520attack%2520success%250Aacross%2520multiple%2520turns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree-based%20Dialogue%20Reinforced%20Policy%20Optimization%20for%20Red-Teaming%0A%20%20Attacks&entry.906535625=Ruohao%20Guo%20and%20Afshin%20Oroojlooy%20and%20Roshan%20Sridhar%20and%20Miguel%20Ballesteros%20and%20Alan%20Ritter%20and%20Dan%20Roth&entry.1292438233=%20%20Despite%20recent%20rapid%20progress%20in%20AI%20safety%2C%20current%20large%20language%20models%0Aremain%20vulnerable%20to%20adversarial%20attacks%20in%20multi-turn%20interaction%20settings%2C%0Awhere%20attackers%20strategically%20adapt%20their%20prompts%20across%20conversation%20turns%20and%0Apose%20a%20more%20critical%20yet%20realistic%20challenge.%20Existing%20approaches%20that%20discover%0Asafety%20vulnerabilities%20either%20rely%20on%20manual%20red-teaming%20with%20human%20experts%20or%0Aemploy%20automated%20methods%20using%20pre-defined%20templates%20and%20human-curated%20attack%0Adata%2C%20with%20most%20focusing%20on%20single-turn%20attacks.%20However%2C%20these%20methods%20did%20not%0Aexplore%20the%20vast%20space%20of%20possible%20multi-turn%20attacks%2C%20failing%20to%20consider%0Anovel%20attack%20trajectories%20that%20emerge%20from%20complex%20dialogue%20dynamics%20and%0Astrategic%20conversation%20planning.%20This%20gap%20is%20particularly%20critical%20given%20recent%0Afindings%20that%20LLMs%20exhibit%20significantly%20higher%20vulnerability%20to%20multi-turn%0Aattacks%20compared%20to%20single-turn%20attacks.%20We%20propose%20DialTree-RPO%2C%20an%20on-policy%0Areinforcement%20learning%20framework%20integrated%20with%20tree%20search%20that%20autonomously%0Adiscovers%20diverse%20multi-turn%20attack%20strategies%20by%20treating%20the%20dialogue%20as%20a%0Asequential%20decision-making%20problem%2C%20enabling%20systematic%20exploration%20without%0Amanually%20curated%20data.%20Through%20extensive%20experiments%2C%20our%20approach%20not%20only%0Aachieves%20more%20than%2025.9%25%20higher%20ASR%20across%2010%20target%20models%20compared%20to%0Aprevious%20state-of-the-art%20approaches%2C%20but%20also%20effectively%20uncovers%20new%20attack%0Astrategies%20by%20learning%20optimal%20dialogue%20policies%20that%20maximize%20attack%20success%0Aacross%20multiple%20turns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02286v1&entry.124074799=Read"},
{"title": "EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative\n  Captioning", "author": "Liang-Yuan Wu and Dhruv Jain", "abstract": "  Automatic Speech Recognition (ASR) systems often fail to accurately\ntranscribe speech from Deaf and Hard of Hearing (DHH) individuals, especially\nduring real-time conversations. Existing personalization approaches typically\nrequire extensive pre-recorded data and place the burden of adaptation on the\nDHH speaker. We present EvolveCaptions, a real-time, collaborative ASR\nadaptation system that supports in-situ personalization with minimal effort.\nHearing participants correct ASR errors during live conversations. Based on\nthese corrections, the system generates short, phonetically targeted prompts\nfor the DHH speaker to record, which are then used to fine-tune the ASR model.\nIn a study with 12 DHH and six hearing participants, EvolveCaptions reduced\nWord Error Rate (WER) across all DHH users within one hour of use, using only\nfive minutes of recording time on average. Participants described the system as\nintuitive, low-effort, and well-integrated into communication. These findings\ndemonstrate the promise of collaborative, real-time ASR adaptation for more\nequitable communication.\n", "link": "http://arxiv.org/abs/2510.02181v1", "date": "2025-10-02", "relevancy": 1.8344, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4896}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4583}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvolveCaptions%3A%20Empowering%20DHH%20Users%20Through%20Real-Time%20Collaborative%0A%20%20Captioning&body=Title%3A%20EvolveCaptions%3A%20Empowering%20DHH%20Users%20Through%20Real-Time%20Collaborative%0A%20%20Captioning%0AAuthor%3A%20Liang-Yuan%20Wu%20and%20Dhruv%20Jain%0AAbstract%3A%20%20%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems%20often%20fail%20to%20accurately%0Atranscribe%20speech%20from%20Deaf%20and%20Hard%20of%20Hearing%20%28DHH%29%20individuals%2C%20especially%0Aduring%20real-time%20conversations.%20Existing%20personalization%20approaches%20typically%0Arequire%20extensive%20pre-recorded%20data%20and%20place%20the%20burden%20of%20adaptation%20on%20the%0ADHH%20speaker.%20We%20present%20EvolveCaptions%2C%20a%20real-time%2C%20collaborative%20ASR%0Aadaptation%20system%20that%20supports%20in-situ%20personalization%20with%20minimal%20effort.%0AHearing%20participants%20correct%20ASR%20errors%20during%20live%20conversations.%20Based%20on%0Athese%20corrections%2C%20the%20system%20generates%20short%2C%20phonetically%20targeted%20prompts%0Afor%20the%20DHH%20speaker%20to%20record%2C%20which%20are%20then%20used%20to%20fine-tune%20the%20ASR%20model.%0AIn%20a%20study%20with%2012%20DHH%20and%20six%20hearing%20participants%2C%20EvolveCaptions%20reduced%0AWord%20Error%20Rate%20%28WER%29%20across%20all%20DHH%20users%20within%20one%20hour%20of%20use%2C%20using%20only%0Afive%20minutes%20of%20recording%20time%20on%20average.%20Participants%20described%20the%20system%20as%0Aintuitive%2C%20low-effort%2C%20and%20well-integrated%20into%20communication.%20These%20findings%0Ademonstrate%20the%20promise%20of%20collaborative%2C%20real-time%20ASR%20adaptation%20for%20more%0Aequitable%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolveCaptions%253A%2520Empowering%2520DHH%2520Users%2520Through%2520Real-Time%2520Collaborative%250A%2520%2520Captioning%26entry.906535625%3DLiang-Yuan%2520Wu%2520and%2520Dhruv%2520Jain%26entry.1292438233%3D%2520%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520systems%2520often%2520fail%2520to%2520accurately%250Atranscribe%2520speech%2520from%2520Deaf%2520and%2520Hard%2520of%2520Hearing%2520%2528DHH%2529%2520individuals%252C%2520especially%250Aduring%2520real-time%2520conversations.%2520Existing%2520personalization%2520approaches%2520typically%250Arequire%2520extensive%2520pre-recorded%2520data%2520and%2520place%2520the%2520burden%2520of%2520adaptation%2520on%2520the%250ADHH%2520speaker.%2520We%2520present%2520EvolveCaptions%252C%2520a%2520real-time%252C%2520collaborative%2520ASR%250Aadaptation%2520system%2520that%2520supports%2520in-situ%2520personalization%2520with%2520minimal%2520effort.%250AHearing%2520participants%2520correct%2520ASR%2520errors%2520during%2520live%2520conversations.%2520Based%2520on%250Athese%2520corrections%252C%2520the%2520system%2520generates%2520short%252C%2520phonetically%2520targeted%2520prompts%250Afor%2520the%2520DHH%2520speaker%2520to%2520record%252C%2520which%2520are%2520then%2520used%2520to%2520fine-tune%2520the%2520ASR%2520model.%250AIn%2520a%2520study%2520with%252012%2520DHH%2520and%2520six%2520hearing%2520participants%252C%2520EvolveCaptions%2520reduced%250AWord%2520Error%2520Rate%2520%2528WER%2529%2520across%2520all%2520DHH%2520users%2520within%2520one%2520hour%2520of%2520use%252C%2520using%2520only%250Afive%2520minutes%2520of%2520recording%2520time%2520on%2520average.%2520Participants%2520described%2520the%2520system%2520as%250Aintuitive%252C%2520low-effort%252C%2520and%2520well-integrated%2520into%2520communication.%2520These%2520findings%250Ademonstrate%2520the%2520promise%2520of%2520collaborative%252C%2520real-time%2520ASR%2520adaptation%2520for%2520more%250Aequitable%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvolveCaptions%3A%20Empowering%20DHH%20Users%20Through%20Real-Time%20Collaborative%0A%20%20Captioning&entry.906535625=Liang-Yuan%20Wu%20and%20Dhruv%20Jain&entry.1292438233=%20%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems%20often%20fail%20to%20accurately%0Atranscribe%20speech%20from%20Deaf%20and%20Hard%20of%20Hearing%20%28DHH%29%20individuals%2C%20especially%0Aduring%20real-time%20conversations.%20Existing%20personalization%20approaches%20typically%0Arequire%20extensive%20pre-recorded%20data%20and%20place%20the%20burden%20of%20adaptation%20on%20the%0ADHH%20speaker.%20We%20present%20EvolveCaptions%2C%20a%20real-time%2C%20collaborative%20ASR%0Aadaptation%20system%20that%20supports%20in-situ%20personalization%20with%20minimal%20effort.%0AHearing%20participants%20correct%20ASR%20errors%20during%20live%20conversations.%20Based%20on%0Athese%20corrections%2C%20the%20system%20generates%20short%2C%20phonetically%20targeted%20prompts%0Afor%20the%20DHH%20speaker%20to%20record%2C%20which%20are%20then%20used%20to%20fine-tune%20the%20ASR%20model.%0AIn%20a%20study%20with%2012%20DHH%20and%20six%20hearing%20participants%2C%20EvolveCaptions%20reduced%0AWord%20Error%20Rate%20%28WER%29%20across%20all%20DHH%20users%20within%20one%20hour%20of%20use%2C%20using%20only%0Afive%20minutes%20of%20recording%20time%20on%20average.%20Participants%20described%20the%20system%20as%0Aintuitive%2C%20low-effort%2C%20and%20well-integrated%20into%20communication.%20These%20findings%0Ademonstrate%20the%20promise%20of%20collaborative%2C%20real-time%20ASR%20adaptation%20for%20more%0Aequitable%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02181v1&entry.124074799=Read"},
{"title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?", "author": "Yanxu Chen and Zijun Yao and Yantao Liu and Jin Ye and Jianing Yu and Lei Hou and Juanzi Li", "abstract": "  Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.\n", "link": "http://arxiv.org/abs/2510.02209v1", "date": "2025-10-02", "relevancy": 1.8238, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StockBench%3A%20Can%20LLM%20Agents%20Trade%20Stocks%20Profitably%20In%20Real-world%0A%20%20Markets%3F&body=Title%3A%20StockBench%3A%20Can%20LLM%20Agents%20Trade%20Stocks%20Profitably%20In%20Real-world%0A%20%20Markets%3F%0AAuthor%3A%20Yanxu%20Chen%20and%20Zijun%20Yao%20and%20Yantao%20Liu%20and%20Jin%20Ye%20and%20Jianing%20Yu%20and%20Lei%20Hou%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20demonstrated%20strong%20capabilities%0Aas%20autonomous%20agents%2C%20showing%20promise%20in%20reasoning%2C%20tool%20use%2C%20and%20sequential%0Adecision-making.%20While%20prior%20benchmarks%20have%20evaluated%20LLM%20agents%20in%20domains%0Asuch%20as%20software%20engineering%20and%20scientific%20discovery%2C%20the%20finance%20domain%0Aremains%20underexplored%2C%20despite%20its%20direct%20relevance%20to%20economic%20value%20and%0Ahigh-stakes%20decision-making.%20Existing%20financial%20benchmarks%20primarily%20test%0Astatic%20knowledge%20through%20question%20answering%2C%20but%20they%20fall%20short%20of%20capturing%0Athe%20dynamic%20and%20iterative%20nature%20of%20trading.%20To%20address%20this%20gap%2C%20we%20introduce%0AStockBench%2C%20a%20contamination-free%20benchmark%20designed%20to%20evaluate%20LLM%20agents%20in%0Arealistic%2C%20multi-month%20stock%20trading%20environments.%20Agents%20receive%20daily%20market%0Asignals%20--%20including%20prices%2C%20fundamentals%2C%20and%20news%20--%20and%20must%20make%20sequential%0Abuy%2C%20sell%2C%20or%20hold%20decisions.%20Performance%20is%20assessed%20using%20financial%20metrics%0Asuch%20as%20cumulative%20return%2C%20maximum%20drawdown%2C%20and%20the%20Sortino%20ratio.%20Our%0Aevaluation%20of%20state-of-the-art%20proprietary%20%28e.g.%2C%20GPT-5%2C%20Claude-4%29%20and%0Aopen-weight%20%28e.g.%2C%20Qwen3%2C%20Kimi-K2%2C%20GLM-4.5%29%20models%20shows%20that%20while%20most%20LLM%0Aagents%20struggle%20to%20outperform%20the%20simple%20buy-and-hold%20baseline%2C%20several%20models%0Ademonstrate%20the%20potential%20to%20deliver%20higher%20returns%20and%20manage%20risk%20more%0Aeffectively.%20These%20findings%20highlight%20both%20the%20challenges%20and%20opportunities%20in%0Adeveloping%20LLM-powered%20financial%20agents%2C%20showing%20that%20excelling%20at%20static%0Afinancial%20knowledge%20tasks%20does%20not%20necessarily%20translate%20into%20successful%0Atrading%20strategies.%20We%20release%20StockBench%20as%20an%20open-source%20resource%20to%20support%0Areproducibility%20and%20advance%20future%20research%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStockBench%253A%2520Can%2520LLM%2520Agents%2520Trade%2520Stocks%2520Profitably%2520In%2520Real-world%250A%2520%2520Markets%253F%26entry.906535625%3DYanxu%2520Chen%2520and%2520Zijun%2520Yao%2520and%2520Yantao%2520Liu%2520and%2520Jin%2520Ye%2520and%2520Jianing%2520Yu%2520and%2520Lei%2520Hou%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520demonstrated%2520strong%2520capabilities%250Aas%2520autonomous%2520agents%252C%2520showing%2520promise%2520in%2520reasoning%252C%2520tool%2520use%252C%2520and%2520sequential%250Adecision-making.%2520While%2520prior%2520benchmarks%2520have%2520evaluated%2520LLM%2520agents%2520in%2520domains%250Asuch%2520as%2520software%2520engineering%2520and%2520scientific%2520discovery%252C%2520the%2520finance%2520domain%250Aremains%2520underexplored%252C%2520despite%2520its%2520direct%2520relevance%2520to%2520economic%2520value%2520and%250Ahigh-stakes%2520decision-making.%2520Existing%2520financial%2520benchmarks%2520primarily%2520test%250Astatic%2520knowledge%2520through%2520question%2520answering%252C%2520but%2520they%2520fall%2520short%2520of%2520capturing%250Athe%2520dynamic%2520and%2520iterative%2520nature%2520of%2520trading.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AStockBench%252C%2520a%2520contamination-free%2520benchmark%2520designed%2520to%2520evaluate%2520LLM%2520agents%2520in%250Arealistic%252C%2520multi-month%2520stock%2520trading%2520environments.%2520Agents%2520receive%2520daily%2520market%250Asignals%2520--%2520including%2520prices%252C%2520fundamentals%252C%2520and%2520news%2520--%2520and%2520must%2520make%2520sequential%250Abuy%252C%2520sell%252C%2520or%2520hold%2520decisions.%2520Performance%2520is%2520assessed%2520using%2520financial%2520metrics%250Asuch%2520as%2520cumulative%2520return%252C%2520maximum%2520drawdown%252C%2520and%2520the%2520Sortino%2520ratio.%2520Our%250Aevaluation%2520of%2520state-of-the-art%2520proprietary%2520%2528e.g.%252C%2520GPT-5%252C%2520Claude-4%2529%2520and%250Aopen-weight%2520%2528e.g.%252C%2520Qwen3%252C%2520Kimi-K2%252C%2520GLM-4.5%2529%2520models%2520shows%2520that%2520while%2520most%2520LLM%250Aagents%2520struggle%2520to%2520outperform%2520the%2520simple%2520buy-and-hold%2520baseline%252C%2520several%2520models%250Ademonstrate%2520the%2520potential%2520to%2520deliver%2520higher%2520returns%2520and%2520manage%2520risk%2520more%250Aeffectively.%2520These%2520findings%2520highlight%2520both%2520the%2520challenges%2520and%2520opportunities%2520in%250Adeveloping%2520LLM-powered%2520financial%2520agents%252C%2520showing%2520that%2520excelling%2520at%2520static%250Afinancial%2520knowledge%2520tasks%2520does%2520not%2520necessarily%2520translate%2520into%2520successful%250Atrading%2520strategies.%2520We%2520release%2520StockBench%2520as%2520an%2520open-source%2520resource%2520to%2520support%250Areproducibility%2520and%2520advance%2520future%2520research%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StockBench%3A%20Can%20LLM%20Agents%20Trade%20Stocks%20Profitably%20In%20Real-world%0A%20%20Markets%3F&entry.906535625=Yanxu%20Chen%20and%20Zijun%20Yao%20and%20Yantao%20Liu%20and%20Jin%20Ye%20and%20Jianing%20Yu%20and%20Lei%20Hou%20and%20Juanzi%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20demonstrated%20strong%20capabilities%0Aas%20autonomous%20agents%2C%20showing%20promise%20in%20reasoning%2C%20tool%20use%2C%20and%20sequential%0Adecision-making.%20While%20prior%20benchmarks%20have%20evaluated%20LLM%20agents%20in%20domains%0Asuch%20as%20software%20engineering%20and%20scientific%20discovery%2C%20the%20finance%20domain%0Aremains%20underexplored%2C%20despite%20its%20direct%20relevance%20to%20economic%20value%20and%0Ahigh-stakes%20decision-making.%20Existing%20financial%20benchmarks%20primarily%20test%0Astatic%20knowledge%20through%20question%20answering%2C%20but%20they%20fall%20short%20of%20capturing%0Athe%20dynamic%20and%20iterative%20nature%20of%20trading.%20To%20address%20this%20gap%2C%20we%20introduce%0AStockBench%2C%20a%20contamination-free%20benchmark%20designed%20to%20evaluate%20LLM%20agents%20in%0Arealistic%2C%20multi-month%20stock%20trading%20environments.%20Agents%20receive%20daily%20market%0Asignals%20--%20including%20prices%2C%20fundamentals%2C%20and%20news%20--%20and%20must%20make%20sequential%0Abuy%2C%20sell%2C%20or%20hold%20decisions.%20Performance%20is%20assessed%20using%20financial%20metrics%0Asuch%20as%20cumulative%20return%2C%20maximum%20drawdown%2C%20and%20the%20Sortino%20ratio.%20Our%0Aevaluation%20of%20state-of-the-art%20proprietary%20%28e.g.%2C%20GPT-5%2C%20Claude-4%29%20and%0Aopen-weight%20%28e.g.%2C%20Qwen3%2C%20Kimi-K2%2C%20GLM-4.5%29%20models%20shows%20that%20while%20most%20LLM%0Aagents%20struggle%20to%20outperform%20the%20simple%20buy-and-hold%20baseline%2C%20several%20models%0Ademonstrate%20the%20potential%20to%20deliver%20higher%20returns%20and%20manage%20risk%20more%0Aeffectively.%20These%20findings%20highlight%20both%20the%20challenges%20and%20opportunities%20in%0Adeveloping%20LLM-powered%20financial%20agents%2C%20showing%20that%20excelling%20at%20static%0Afinancial%20knowledge%20tasks%20does%20not%20necessarily%20translate%20into%20successful%0Atrading%20strategies.%20We%20release%20StockBench%20as%20an%20open-source%20resource%20to%20support%0Areproducibility%20and%20advance%20future%20research%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02209v1&entry.124074799=Read"},
{"title": "Go witheFlow: Real-time Emotion Driven Audio Effects Modulation", "author": "Edmund Dervakos and Spyridon Kantarelis and Vassilis Lyberatos and Jason Liartis and Giorgos Stamou", "abstract": "  Music performance is a distinctly human activity, intrinsically linked to the\nperformer's ability to convey, evoke, or express emotion. Machines cannot\nperform music in the human sense; they can produce, reproduce, execute, or\nsynthesize music, but they lack the capacity for affective or emotional\nexperience. As such, music performance is an ideal candidate through which to\nexplore aspects of collaboration between humans and machines. In this paper, we\nintroduce the witheFlow system, designed to enhance real-time music performance\nby automatically modulating audio effects based on features extracted from both\nbiosignals and the audio itself. The system, currently in a proof-of-concept\nphase, is designed to be lightweight, able to run locally on a laptop, and is\nopen-source given the availability of a compatible Digital Audio Workstation\nand sensors.\n", "link": "http://arxiv.org/abs/2510.02171v1", "date": "2025-10-02", "relevancy": 1.8193, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4663}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4601}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Go%20witheFlow%3A%20Real-time%20Emotion%20Driven%20Audio%20Effects%20Modulation&body=Title%3A%20Go%20witheFlow%3A%20Real-time%20Emotion%20Driven%20Audio%20Effects%20Modulation%0AAuthor%3A%20Edmund%20Dervakos%20and%20Spyridon%20Kantarelis%20and%20Vassilis%20Lyberatos%20and%20Jason%20Liartis%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20Music%20performance%20is%20a%20distinctly%20human%20activity%2C%20intrinsically%20linked%20to%20the%0Aperformer%27s%20ability%20to%20convey%2C%20evoke%2C%20or%20express%20emotion.%20Machines%20cannot%0Aperform%20music%20in%20the%20human%20sense%3B%20they%20can%20produce%2C%20reproduce%2C%20execute%2C%20or%0Asynthesize%20music%2C%20but%20they%20lack%20the%20capacity%20for%20affective%20or%20emotional%0Aexperience.%20As%20such%2C%20music%20performance%20is%20an%20ideal%20candidate%20through%20which%20to%0Aexplore%20aspects%20of%20collaboration%20between%20humans%20and%20machines.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20witheFlow%20system%2C%20designed%20to%20enhance%20real-time%20music%20performance%0Aby%20automatically%20modulating%20audio%20effects%20based%20on%20features%20extracted%20from%20both%0Abiosignals%20and%20the%20audio%20itself.%20The%20system%2C%20currently%20in%20a%20proof-of-concept%0Aphase%2C%20is%20designed%20to%20be%20lightweight%2C%20able%20to%20run%20locally%20on%20a%20laptop%2C%20and%20is%0Aopen-source%20given%20the%20availability%20of%20a%20compatible%20Digital%20Audio%20Workstation%0Aand%20sensors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGo%2520witheFlow%253A%2520Real-time%2520Emotion%2520Driven%2520Audio%2520Effects%2520Modulation%26entry.906535625%3DEdmund%2520Dervakos%2520and%2520Spyridon%2520Kantarelis%2520and%2520Vassilis%2520Lyberatos%2520and%2520Jason%2520Liartis%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520Music%2520performance%2520is%2520a%2520distinctly%2520human%2520activity%252C%2520intrinsically%2520linked%2520to%2520the%250Aperformer%2527s%2520ability%2520to%2520convey%252C%2520evoke%252C%2520or%2520express%2520emotion.%2520Machines%2520cannot%250Aperform%2520music%2520in%2520the%2520human%2520sense%253B%2520they%2520can%2520produce%252C%2520reproduce%252C%2520execute%252C%2520or%250Asynthesize%2520music%252C%2520but%2520they%2520lack%2520the%2520capacity%2520for%2520affective%2520or%2520emotional%250Aexperience.%2520As%2520such%252C%2520music%2520performance%2520is%2520an%2520ideal%2520candidate%2520through%2520which%2520to%250Aexplore%2520aspects%2520of%2520collaboration%2520between%2520humans%2520and%2520machines.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520witheFlow%2520system%252C%2520designed%2520to%2520enhance%2520real-time%2520music%2520performance%250Aby%2520automatically%2520modulating%2520audio%2520effects%2520based%2520on%2520features%2520extracted%2520from%2520both%250Abiosignals%2520and%2520the%2520audio%2520itself.%2520The%2520system%252C%2520currently%2520in%2520a%2520proof-of-concept%250Aphase%252C%2520is%2520designed%2520to%2520be%2520lightweight%252C%2520able%2520to%2520run%2520locally%2520on%2520a%2520laptop%252C%2520and%2520is%250Aopen-source%2520given%2520the%2520availability%2520of%2520a%2520compatible%2520Digital%2520Audio%2520Workstation%250Aand%2520sensors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Go%20witheFlow%3A%20Real-time%20Emotion%20Driven%20Audio%20Effects%20Modulation&entry.906535625=Edmund%20Dervakos%20and%20Spyridon%20Kantarelis%20and%20Vassilis%20Lyberatos%20and%20Jason%20Liartis%20and%20Giorgos%20Stamou&entry.1292438233=%20%20Music%20performance%20is%20a%20distinctly%20human%20activity%2C%20intrinsically%20linked%20to%20the%0Aperformer%27s%20ability%20to%20convey%2C%20evoke%2C%20or%20express%20emotion.%20Machines%20cannot%0Aperform%20music%20in%20the%20human%20sense%3B%20they%20can%20produce%2C%20reproduce%2C%20execute%2C%20or%0Asynthesize%20music%2C%20but%20they%20lack%20the%20capacity%20for%20affective%20or%20emotional%0Aexperience.%20As%20such%2C%20music%20performance%20is%20an%20ideal%20candidate%20through%20which%20to%0Aexplore%20aspects%20of%20collaboration%20between%20humans%20and%20machines.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20witheFlow%20system%2C%20designed%20to%20enhance%20real-time%20music%20performance%0Aby%20automatically%20modulating%20audio%20effects%20based%20on%20features%20extracted%20from%20both%0Abiosignals%20and%20the%20audio%20itself.%20The%20system%2C%20currently%20in%20a%20proof-of-concept%0Aphase%2C%20is%20designed%20to%20be%20lightweight%2C%20able%20to%20run%20locally%20on%20a%20laptop%2C%20and%20is%0Aopen-source%20given%20the%20availability%20of%20a%20compatible%20Digital%20Audio%20Workstation%0Aand%20sensors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02171v1&entry.124074799=Read"},
{"title": "What Makes a Good Dataset for Knowledge Distillation?", "author": "Logan Frank and Jim Davis", "abstract": "  Knowledge distillation (KD) has been a popular and effective method for model\ncompression. One important assumption of KD is that the teacher's original\ndataset will also be available when training the student. However, in\nsituations such as continual learning and distilling large models trained on\ncompany-withheld datasets, having access to the original data may not always be\npossible. This leads practitioners towards utilizing other sources of\nsupplemental data, which could yield mixed results. One must then ask: \"what\nmakes a good dataset for transferring knowledge from teacher to student?\" Many\nwould assume that only real in-domain imagery is viable, but is that the only\noption? In this work, we explore multiple possible surrogate distillation\ndatasets and demonstrate that many different datasets, even unnatural synthetic\nimagery, can serve as a suitable alternative in KD. From examining these\nalternative datasets, we identify and present various criteria describing what\nmakes a good dataset for distillation. Source code is available at\nhttps://github.com/osu-cvl/good-kd-dataset.\n", "link": "http://arxiv.org/abs/2411.12817v2", "date": "2025-10-02", "relevancy": 1.798, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4513}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4502}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20a%20Good%20Dataset%20for%20Knowledge%20Distillation%3F&body=Title%3A%20What%20Makes%20a%20Good%20Dataset%20for%20Knowledge%20Distillation%3F%0AAuthor%3A%20Logan%20Frank%20and%20Jim%20Davis%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20has%20been%20a%20popular%20and%20effective%20method%20for%20model%0Acompression.%20One%20important%20assumption%20of%20KD%20is%20that%20the%20teacher%27s%20original%0Adataset%20will%20also%20be%20available%20when%20training%20the%20student.%20However%2C%20in%0Asituations%20such%20as%20continual%20learning%20and%20distilling%20large%20models%20trained%20on%0Acompany-withheld%20datasets%2C%20having%20access%20to%20the%20original%20data%20may%20not%20always%20be%0Apossible.%20This%20leads%20practitioners%20towards%20utilizing%20other%20sources%20of%0Asupplemental%20data%2C%20which%20could%20yield%20mixed%20results.%20One%20must%20then%20ask%3A%20%22what%0Amakes%20a%20good%20dataset%20for%20transferring%20knowledge%20from%20teacher%20to%20student%3F%22%20Many%0Awould%20assume%20that%20only%20real%20in-domain%20imagery%20is%20viable%2C%20but%20is%20that%20the%20only%0Aoption%3F%20In%20this%20work%2C%20we%20explore%20multiple%20possible%20surrogate%20distillation%0Adatasets%20and%20demonstrate%20that%20many%20different%20datasets%2C%20even%20unnatural%20synthetic%0Aimagery%2C%20can%20serve%20as%20a%20suitable%20alternative%20in%20KD.%20From%20examining%20these%0Aalternative%20datasets%2C%20we%20identify%20and%20present%20various%20criteria%20describing%20what%0Amakes%20a%20good%20dataset%20for%20distillation.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/osu-cvl/good-kd-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520a%2520Good%2520Dataset%2520for%2520Knowledge%2520Distillation%253F%26entry.906535625%3DLogan%2520Frank%2520and%2520Jim%2520Davis%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520has%2520been%2520a%2520popular%2520and%2520effective%2520method%2520for%2520model%250Acompression.%2520One%2520important%2520assumption%2520of%2520KD%2520is%2520that%2520the%2520teacher%2527s%2520original%250Adataset%2520will%2520also%2520be%2520available%2520when%2520training%2520the%2520student.%2520However%252C%2520in%250Asituations%2520such%2520as%2520continual%2520learning%2520and%2520distilling%2520large%2520models%2520trained%2520on%250Acompany-withheld%2520datasets%252C%2520having%2520access%2520to%2520the%2520original%2520data%2520may%2520not%2520always%2520be%250Apossible.%2520This%2520leads%2520practitioners%2520towards%2520utilizing%2520other%2520sources%2520of%250Asupplemental%2520data%252C%2520which%2520could%2520yield%2520mixed%2520results.%2520One%2520must%2520then%2520ask%253A%2520%2522what%250Amakes%2520a%2520good%2520dataset%2520for%2520transferring%2520knowledge%2520from%2520teacher%2520to%2520student%253F%2522%2520Many%250Awould%2520assume%2520that%2520only%2520real%2520in-domain%2520imagery%2520is%2520viable%252C%2520but%2520is%2520that%2520the%2520only%250Aoption%253F%2520In%2520this%2520work%252C%2520we%2520explore%2520multiple%2520possible%2520surrogate%2520distillation%250Adatasets%2520and%2520demonstrate%2520that%2520many%2520different%2520datasets%252C%2520even%2520unnatural%2520synthetic%250Aimagery%252C%2520can%2520serve%2520as%2520a%2520suitable%2520alternative%2520in%2520KD.%2520From%2520examining%2520these%250Aalternative%2520datasets%252C%2520we%2520identify%2520and%2520present%2520various%2520criteria%2520describing%2520what%250Amakes%2520a%2520good%2520dataset%2520for%2520distillation.%2520Source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/osu-cvl/good-kd-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20a%20Good%20Dataset%20for%20Knowledge%20Distillation%3F&entry.906535625=Logan%20Frank%20and%20Jim%20Davis&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20has%20been%20a%20popular%20and%20effective%20method%20for%20model%0Acompression.%20One%20important%20assumption%20of%20KD%20is%20that%20the%20teacher%27s%20original%0Adataset%20will%20also%20be%20available%20when%20training%20the%20student.%20However%2C%20in%0Asituations%20such%20as%20continual%20learning%20and%20distilling%20large%20models%20trained%20on%0Acompany-withheld%20datasets%2C%20having%20access%20to%20the%20original%20data%20may%20not%20always%20be%0Apossible.%20This%20leads%20practitioners%20towards%20utilizing%20other%20sources%20of%0Asupplemental%20data%2C%20which%20could%20yield%20mixed%20results.%20One%20must%20then%20ask%3A%20%22what%0Amakes%20a%20good%20dataset%20for%20transferring%20knowledge%20from%20teacher%20to%20student%3F%22%20Many%0Awould%20assume%20that%20only%20real%20in-domain%20imagery%20is%20viable%2C%20but%20is%20that%20the%20only%0Aoption%3F%20In%20this%20work%2C%20we%20explore%20multiple%20possible%20surrogate%20distillation%0Adatasets%20and%20demonstrate%20that%20many%20different%20datasets%2C%20even%20unnatural%20synthetic%0Aimagery%2C%20can%20serve%20as%20a%20suitable%20alternative%20in%20KD.%20From%20examining%20these%0Aalternative%20datasets%2C%20we%20identify%20and%20present%20various%20criteria%20describing%20what%0Amakes%20a%20good%20dataset%20for%20distillation.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/osu-cvl/good-kd-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12817v2&entry.124074799=Read"},
{"title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling", "author": "Litu Rout and Andreas Lugmayr and Yasamin Jafarian and Srivatsan Varadharajan and Constantine Caramanis and Sanjay Shakkottai and Ira Kemelmacher-Shlizerman", "abstract": "  We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.\n", "link": "http://arxiv.org/abs/2510.02291v1", "date": "2025-10-02", "relevancy": 1.792, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6395}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5913}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Anchoring%20for%20Discrete%20Diffusion%20Posterior%20Sampling&body=Title%3A%20Test-Time%20Anchoring%20for%20Discrete%20Diffusion%20Posterior%20Sampling%0AAuthor%3A%20Litu%20Rout%20and%20Andreas%20Lugmayr%20and%20Yasamin%20Jafarian%20and%20Srivatsan%20Varadharajan%20and%20Constantine%20Caramanis%20and%20Sanjay%20Shakkottai%20and%20Ira%20Kemelmacher-Shlizerman%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20posterior%20sampling%20using%20pretrained%20discrete%0Adiffusion%20foundation%20models%2C%20aiming%20to%20recover%20images%20from%20noisy%20measurements%0Awithout%20retraining%20task-specific%20models.%20While%20diffusion%20models%20have%20achieved%0Aremarkable%20success%20in%20generative%20modeling%2C%20most%20advances%20rely%20on%20continuous%0AGaussian%20diffusion.%20In%20contrast%2C%20discrete%20diffusion%20offers%20a%20unified%20framework%0Afor%20jointly%20modeling%20categorical%20data%20such%20as%20text%20and%20images.%20Beyond%0Aunification%2C%20discrete%20diffusion%20provides%20faster%20inference%2C%20finer%20control%2C%20and%0Aprincipled%20training-free%20Bayesian%20inference%2C%20making%20it%20particularly%20well-suited%0Afor%20posterior%20sampling.%20However%2C%20existing%20approaches%20to%20discrete%20diffusion%0Aposterior%20sampling%20face%20severe%20challenges%3A%20derivative-free%20guidance%20yields%0Asparse%20signals%2C%20continuous%20relaxations%20limit%20applicability%2C%20and%20split%20Gibbs%0Asamplers%20suffer%20from%20the%20curse%20of%20dimensionality.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20Anchored%20Posterior%20Sampling%20%28APS%29%20for%20masked%0Adiffusion%20foundation%20models%2C%20built%20on%20two%20key%20innovations%20--%20quantized%0Aexpectation%20for%20gradient-like%20guidance%20in%20discrete%20embedding%20space%2C%20and%0Aanchored%20remasking%20for%20adaptive%20decoding.%20Our%20approach%20achieves%0Astate-of-the-art%20performance%20among%20discrete%20diffusion%20samplers%20across%20linear%0Aand%20nonlinear%20inverse%20problems%20on%20the%20standard%20benchmarks.%20We%20further%0Ademonstrate%20the%20benefits%20of%20our%20approach%20in%20training-free%20stylization%20and%0Atext-guided%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Anchoring%2520for%2520Discrete%2520Diffusion%2520Posterior%2520Sampling%26entry.906535625%3DLitu%2520Rout%2520and%2520Andreas%2520Lugmayr%2520and%2520Yasamin%2520Jafarian%2520and%2520Srivatsan%2520Varadharajan%2520and%2520Constantine%2520Caramanis%2520and%2520Sanjay%2520Shakkottai%2520and%2520Ira%2520Kemelmacher-Shlizerman%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520posterior%2520sampling%2520using%2520pretrained%2520discrete%250Adiffusion%2520foundation%2520models%252C%2520aiming%2520to%2520recover%2520images%2520from%2520noisy%2520measurements%250Awithout%2520retraining%2520task-specific%2520models.%2520While%2520diffusion%2520models%2520have%2520achieved%250Aremarkable%2520success%2520in%2520generative%2520modeling%252C%2520most%2520advances%2520rely%2520on%2520continuous%250AGaussian%2520diffusion.%2520In%2520contrast%252C%2520discrete%2520diffusion%2520offers%2520a%2520unified%2520framework%250Afor%2520jointly%2520modeling%2520categorical%2520data%2520such%2520as%2520text%2520and%2520images.%2520Beyond%250Aunification%252C%2520discrete%2520diffusion%2520provides%2520faster%2520inference%252C%2520finer%2520control%252C%2520and%250Aprincipled%2520training-free%2520Bayesian%2520inference%252C%2520making%2520it%2520particularly%2520well-suited%250Afor%2520posterior%2520sampling.%2520However%252C%2520existing%2520approaches%2520to%2520discrete%2520diffusion%250Aposterior%2520sampling%2520face%2520severe%2520challenges%253A%2520derivative-free%2520guidance%2520yields%250Asparse%2520signals%252C%2520continuous%2520relaxations%2520limit%2520applicability%252C%2520and%2520split%2520Gibbs%250Asamplers%2520suffer%2520from%2520the%2520curse%2520of%2520dimensionality.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520introduce%2520Anchored%2520Posterior%2520Sampling%2520%2528APS%2529%2520for%2520masked%250Adiffusion%2520foundation%2520models%252C%2520built%2520on%2520two%2520key%2520innovations%2520--%2520quantized%250Aexpectation%2520for%2520gradient-like%2520guidance%2520in%2520discrete%2520embedding%2520space%252C%2520and%250Aanchored%2520remasking%2520for%2520adaptive%2520decoding.%2520Our%2520approach%2520achieves%250Astate-of-the-art%2520performance%2520among%2520discrete%2520diffusion%2520samplers%2520across%2520linear%250Aand%2520nonlinear%2520inverse%2520problems%2520on%2520the%2520standard%2520benchmarks.%2520We%2520further%250Ademonstrate%2520the%2520benefits%2520of%2520our%2520approach%2520in%2520training-free%2520stylization%2520and%250Atext-guided%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Anchoring%20for%20Discrete%20Diffusion%20Posterior%20Sampling&entry.906535625=Litu%20Rout%20and%20Andreas%20Lugmayr%20and%20Yasamin%20Jafarian%20and%20Srivatsan%20Varadharajan%20and%20Constantine%20Caramanis%20and%20Sanjay%20Shakkottai%20and%20Ira%20Kemelmacher-Shlizerman&entry.1292438233=%20%20We%20study%20the%20problem%20of%20posterior%20sampling%20using%20pretrained%20discrete%0Adiffusion%20foundation%20models%2C%20aiming%20to%20recover%20images%20from%20noisy%20measurements%0Awithout%20retraining%20task-specific%20models.%20While%20diffusion%20models%20have%20achieved%0Aremarkable%20success%20in%20generative%20modeling%2C%20most%20advances%20rely%20on%20continuous%0AGaussian%20diffusion.%20In%20contrast%2C%20discrete%20diffusion%20offers%20a%20unified%20framework%0Afor%20jointly%20modeling%20categorical%20data%20such%20as%20text%20and%20images.%20Beyond%0Aunification%2C%20discrete%20diffusion%20provides%20faster%20inference%2C%20finer%20control%2C%20and%0Aprincipled%20training-free%20Bayesian%20inference%2C%20making%20it%20particularly%20well-suited%0Afor%20posterior%20sampling.%20However%2C%20existing%20approaches%20to%20discrete%20diffusion%0Aposterior%20sampling%20face%20severe%20challenges%3A%20derivative-free%20guidance%20yields%0Asparse%20signals%2C%20continuous%20relaxations%20limit%20applicability%2C%20and%20split%20Gibbs%0Asamplers%20suffer%20from%20the%20curse%20of%20dimensionality.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20Anchored%20Posterior%20Sampling%20%28APS%29%20for%20masked%0Adiffusion%20foundation%20models%2C%20built%20on%20two%20key%20innovations%20--%20quantized%0Aexpectation%20for%20gradient-like%20guidance%20in%20discrete%20embedding%20space%2C%20and%0Aanchored%20remasking%20for%20adaptive%20decoding.%20Our%20approach%20achieves%0Astate-of-the-art%20performance%20among%20discrete%20diffusion%20samplers%20across%20linear%0Aand%20nonlinear%20inverse%20problems%20on%20the%20standard%20benchmarks.%20We%20further%0Ademonstrate%20the%20benefits%20of%20our%20approach%20in%20training-free%20stylization%20and%0Atext-guided%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02291v1&entry.124074799=Read"},
{"title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing", "author": "Zihan Zhou and Shilin Lu and Shuli Leng and Shaocong Zhang and Zhuming Lian and Xinlei Yu and Adams Wai-Kin Kong", "abstract": "  Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.\n", "link": "http://arxiv.org/abs/2510.02253v1", "date": "2025-10-02", "relevancy": 1.7631, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6249}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6021}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DragFlow%3A%20Unleashing%20DiT%20Priors%20with%20Region%20Based%20Supervision%20for%20Drag%0A%20%20Editing&body=Title%3A%20DragFlow%3A%20Unleashing%20DiT%20Priors%20with%20Region%20Based%20Supervision%20for%20Drag%0A%20%20Editing%0AAuthor%3A%20Zihan%20Zhou%20and%20Shilin%20Lu%20and%20Shuli%20Leng%20and%20Shaocong%20Zhang%20and%20Zhuming%20Lian%20and%20Xinlei%20Yu%20and%20Adams%20Wai-Kin%20Kong%0AAbstract%3A%20%20%20Drag-based%20image%20editing%20has%20long%20suffered%20from%20distortions%20in%20the%20target%0Aregion%2C%20largely%20because%20the%20priors%20of%20earlier%20base%20models%2C%20Stable%20Diffusion%2C%0Aare%20insufficient%20to%20project%20optimized%20latents%20back%20onto%20the%20natural%20image%0Amanifold.%20With%20the%20shift%20from%20UNet-based%20DDPMs%20to%20more%20scalable%20DiT%20with%20flow%0Amatching%20%28e.g.%2C%20SD3.5%2C%20FLUX%29%2C%20generative%20priors%20have%20become%20significantly%0Astronger%2C%20enabling%20advances%20across%20diverse%20editing%20tasks.%20However%2C%20drag-based%0Aediting%20has%20yet%20to%20benefit%20from%20these%20stronger%20priors.%20This%20work%20proposes%20the%0Afirst%20framework%20to%20effectively%20harness%20FLUX%27s%20rich%20prior%20for%20drag-based%0Aediting%2C%20dubbed%20DragFlow%2C%20achieving%20substantial%20gains%20over%20baselines.%20We%20first%0Ashow%20that%20directly%20applying%20point-based%20drag%20editing%20to%20DiTs%20performs%20poorly%3A%0Aunlike%20the%20highly%20compressed%20features%20of%20UNets%2C%20DiT%20features%20are%20insufficiently%0Astructured%20to%20provide%20reliable%20guidance%20for%20point-wise%20motion%20supervision.%20To%0Aovercome%20this%20limitation%2C%20DragFlow%20introduces%20a%20region-based%20editing%20paradigm%2C%0Awhere%20affine%20transformations%20enable%20richer%20and%20more%20consistent%20feature%0Asupervision.%20Additionally%2C%20we%20integrate%20pretrained%20open-domain%20personalization%0Aadapters%20%28e.g.%2C%20IP-Adapter%29%20to%20enhance%20subject%20consistency%2C%20while%20preserving%0Abackground%20fidelity%20through%20gradient%20mask-based%20hard%20constraints.%20Multimodal%0Alarge%20language%20models%20%28MLLMs%29%20are%20further%20employed%20to%20resolve%20task%20ambiguities.%0AFor%20evaluation%2C%20we%20curate%20a%20novel%20Region-based%20Dragging%20benchmark%20%28ReD%20Bench%29%0Afeaturing%20region-level%20dragging%20instructions.%20Extensive%20experiments%20on%0ADragBench-DR%20and%20ReD%20Bench%20show%20that%20DragFlow%20surpasses%20both%20point-based%20and%0Aregion-based%20baselines%2C%20setting%20a%20new%20state-of-the-art%20in%20drag-based%20image%0Aediting.%20Code%20and%20datasets%20will%20be%20publicly%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDragFlow%253A%2520Unleashing%2520DiT%2520Priors%2520with%2520Region%2520Based%2520Supervision%2520for%2520Drag%250A%2520%2520Editing%26entry.906535625%3DZihan%2520Zhou%2520and%2520Shilin%2520Lu%2520and%2520Shuli%2520Leng%2520and%2520Shaocong%2520Zhang%2520and%2520Zhuming%2520Lian%2520and%2520Xinlei%2520Yu%2520and%2520Adams%2520Wai-Kin%2520Kong%26entry.1292438233%3D%2520%2520Drag-based%2520image%2520editing%2520has%2520long%2520suffered%2520from%2520distortions%2520in%2520the%2520target%250Aregion%252C%2520largely%2520because%2520the%2520priors%2520of%2520earlier%2520base%2520models%252C%2520Stable%2520Diffusion%252C%250Aare%2520insufficient%2520to%2520project%2520optimized%2520latents%2520back%2520onto%2520the%2520natural%2520image%250Amanifold.%2520With%2520the%2520shift%2520from%2520UNet-based%2520DDPMs%2520to%2520more%2520scalable%2520DiT%2520with%2520flow%250Amatching%2520%2528e.g.%252C%2520SD3.5%252C%2520FLUX%2529%252C%2520generative%2520priors%2520have%2520become%2520significantly%250Astronger%252C%2520enabling%2520advances%2520across%2520diverse%2520editing%2520tasks.%2520However%252C%2520drag-based%250Aediting%2520has%2520yet%2520to%2520benefit%2520from%2520these%2520stronger%2520priors.%2520This%2520work%2520proposes%2520the%250Afirst%2520framework%2520to%2520effectively%2520harness%2520FLUX%2527s%2520rich%2520prior%2520for%2520drag-based%250Aediting%252C%2520dubbed%2520DragFlow%252C%2520achieving%2520substantial%2520gains%2520over%2520baselines.%2520We%2520first%250Ashow%2520that%2520directly%2520applying%2520point-based%2520drag%2520editing%2520to%2520DiTs%2520performs%2520poorly%253A%250Aunlike%2520the%2520highly%2520compressed%2520features%2520of%2520UNets%252C%2520DiT%2520features%2520are%2520insufficiently%250Astructured%2520to%2520provide%2520reliable%2520guidance%2520for%2520point-wise%2520motion%2520supervision.%2520To%250Aovercome%2520this%2520limitation%252C%2520DragFlow%2520introduces%2520a%2520region-based%2520editing%2520paradigm%252C%250Awhere%2520affine%2520transformations%2520enable%2520richer%2520and%2520more%2520consistent%2520feature%250Asupervision.%2520Additionally%252C%2520we%2520integrate%2520pretrained%2520open-domain%2520personalization%250Aadapters%2520%2528e.g.%252C%2520IP-Adapter%2529%2520to%2520enhance%2520subject%2520consistency%252C%2520while%2520preserving%250Abackground%2520fidelity%2520through%2520gradient%2520mask-based%2520hard%2520constraints.%2520Multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520are%2520further%2520employed%2520to%2520resolve%2520task%2520ambiguities.%250AFor%2520evaluation%252C%2520we%2520curate%2520a%2520novel%2520Region-based%2520Dragging%2520benchmark%2520%2528ReD%2520Bench%2529%250Afeaturing%2520region-level%2520dragging%2520instructions.%2520Extensive%2520experiments%2520on%250ADragBench-DR%2520and%2520ReD%2520Bench%2520show%2520that%2520DragFlow%2520surpasses%2520both%2520point-based%2520and%250Aregion-based%2520baselines%252C%2520setting%2520a%2520new%2520state-of-the-art%2520in%2520drag-based%2520image%250Aediting.%2520Code%2520and%2520datasets%2520will%2520be%2520publicly%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragFlow%3A%20Unleashing%20DiT%20Priors%20with%20Region%20Based%20Supervision%20for%20Drag%0A%20%20Editing&entry.906535625=Zihan%20Zhou%20and%20Shilin%20Lu%20and%20Shuli%20Leng%20and%20Shaocong%20Zhang%20and%20Zhuming%20Lian%20and%20Xinlei%20Yu%20and%20Adams%20Wai-Kin%20Kong&entry.1292438233=%20%20Drag-based%20image%20editing%20has%20long%20suffered%20from%20distortions%20in%20the%20target%0Aregion%2C%20largely%20because%20the%20priors%20of%20earlier%20base%20models%2C%20Stable%20Diffusion%2C%0Aare%20insufficient%20to%20project%20optimized%20latents%20back%20onto%20the%20natural%20image%0Amanifold.%20With%20the%20shift%20from%20UNet-based%20DDPMs%20to%20more%20scalable%20DiT%20with%20flow%0Amatching%20%28e.g.%2C%20SD3.5%2C%20FLUX%29%2C%20generative%20priors%20have%20become%20significantly%0Astronger%2C%20enabling%20advances%20across%20diverse%20editing%20tasks.%20However%2C%20drag-based%0Aediting%20has%20yet%20to%20benefit%20from%20these%20stronger%20priors.%20This%20work%20proposes%20the%0Afirst%20framework%20to%20effectively%20harness%20FLUX%27s%20rich%20prior%20for%20drag-based%0Aediting%2C%20dubbed%20DragFlow%2C%20achieving%20substantial%20gains%20over%20baselines.%20We%20first%0Ashow%20that%20directly%20applying%20point-based%20drag%20editing%20to%20DiTs%20performs%20poorly%3A%0Aunlike%20the%20highly%20compressed%20features%20of%20UNets%2C%20DiT%20features%20are%20insufficiently%0Astructured%20to%20provide%20reliable%20guidance%20for%20point-wise%20motion%20supervision.%20To%0Aovercome%20this%20limitation%2C%20DragFlow%20introduces%20a%20region-based%20editing%20paradigm%2C%0Awhere%20affine%20transformations%20enable%20richer%20and%20more%20consistent%20feature%0Asupervision.%20Additionally%2C%20we%20integrate%20pretrained%20open-domain%20personalization%0Aadapters%20%28e.g.%2C%20IP-Adapter%29%20to%20enhance%20subject%20consistency%2C%20while%20preserving%0Abackground%20fidelity%20through%20gradient%20mask-based%20hard%20constraints.%20Multimodal%0Alarge%20language%20models%20%28MLLMs%29%20are%20further%20employed%20to%20resolve%20task%20ambiguities.%0AFor%20evaluation%2C%20we%20curate%20a%20novel%20Region-based%20Dragging%20benchmark%20%28ReD%20Bench%29%0Afeaturing%20region-level%20dragging%20instructions.%20Extensive%20experiments%20on%0ADragBench-DR%20and%20ReD%20Bench%20show%20that%20DragFlow%20surpasses%20both%20point-based%20and%0Aregion-based%20baselines%2C%20setting%20a%20new%20state-of-the-art%20in%20drag-based%20image%0Aediting.%20Code%20and%20datasets%20will%20be%20publicly%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02253v1&entry.124074799=Read"},
{"title": "LPAC: Learnable Perception-Action-Communication Loops with Applications\n  to Coverage Control", "author": "Saurav Agarwal and Ramya Muthukrishnan and Walker Gosrich and Vijay Kumar and Alejandro Ribeiro", "abstract": "  Coverage control is the problem of navigating a robot swarm to\ncollaboratively monitor features or a phenomenon of interest not known a\npriori. The problem is challenging in decentralized settings with robots that\nhave limited communication and sensing capabilities. We propose a learnable\nPerception-Action-Communication (LPAC) architecture for the problem, wherein a\nconvolutional neural network (CNN) processes localized perception; a graph\nneural network (GNN) facilitates robot communications; finally, a shallow\nmulti-layer perceptron (MLP) computes robot actions. The GNN enables\ncollaboration in the robot swarm by computing what information to communicate\nwith nearby robots and how to incorporate received information. Evaluations\nshow that the LPAC models -- trained using imitation learning -- outperform\nstandard decentralized and centralized coverage control algorithms. The learned\npolicy generalizes to environments different from the training dataset,\ntransfers to larger environments with more robots, and is robust to noisy\nposition estimates. The results indicate the suitability of LPAC architectures\nfor decentralized navigation in robot swarms to achieve collaborative behavior.\n", "link": "http://arxiv.org/abs/2401.04855v4", "date": "2025-10-02", "relevancy": 1.7587, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6105}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5679}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPAC%3A%20Learnable%20Perception-Action-Communication%20Loops%20with%20Applications%0A%20%20to%20Coverage%20Control&body=Title%3A%20LPAC%3A%20Learnable%20Perception-Action-Communication%20Loops%20with%20Applications%0A%20%20to%20Coverage%20Control%0AAuthor%3A%20Saurav%20Agarwal%20and%20Ramya%20Muthukrishnan%20and%20Walker%20Gosrich%20and%20Vijay%20Kumar%20and%20Alejandro%20Ribeiro%0AAbstract%3A%20%20%20Coverage%20control%20is%20the%20problem%20of%20navigating%20a%20robot%20swarm%20to%0Acollaboratively%20monitor%20features%20or%20a%20phenomenon%20of%20interest%20not%20known%20a%0Apriori.%20The%20problem%20is%20challenging%20in%20decentralized%20settings%20with%20robots%20that%0Ahave%20limited%20communication%20and%20sensing%20capabilities.%20We%20propose%20a%20learnable%0APerception-Action-Communication%20%28LPAC%29%20architecture%20for%20the%20problem%2C%20wherein%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20processes%20localized%20perception%3B%20a%20graph%0Aneural%20network%20%28GNN%29%20facilitates%20robot%20communications%3B%20finally%2C%20a%20shallow%0Amulti-layer%20perceptron%20%28MLP%29%20computes%20robot%20actions.%20The%20GNN%20enables%0Acollaboration%20in%20the%20robot%20swarm%20by%20computing%20what%20information%20to%20communicate%0Awith%20nearby%20robots%20and%20how%20to%20incorporate%20received%20information.%20Evaluations%0Ashow%20that%20the%20LPAC%20models%20--%20trained%20using%20imitation%20learning%20--%20outperform%0Astandard%20decentralized%20and%20centralized%20coverage%20control%20algorithms.%20The%20learned%0Apolicy%20generalizes%20to%20environments%20different%20from%20the%20training%20dataset%2C%0Atransfers%20to%20larger%20environments%20with%20more%20robots%2C%20and%20is%20robust%20to%20noisy%0Aposition%20estimates.%20The%20results%20indicate%20the%20suitability%20of%20LPAC%20architectures%0Afor%20decentralized%20navigation%20in%20robot%20swarms%20to%20achieve%20collaborative%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04855v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPAC%253A%2520Learnable%2520Perception-Action-Communication%2520Loops%2520with%2520Applications%250A%2520%2520to%2520Coverage%2520Control%26entry.906535625%3DSaurav%2520Agarwal%2520and%2520Ramya%2520Muthukrishnan%2520and%2520Walker%2520Gosrich%2520and%2520Vijay%2520Kumar%2520and%2520Alejandro%2520Ribeiro%26entry.1292438233%3D%2520%2520Coverage%2520control%2520is%2520the%2520problem%2520of%2520navigating%2520a%2520robot%2520swarm%2520to%250Acollaboratively%2520monitor%2520features%2520or%2520a%2520phenomenon%2520of%2520interest%2520not%2520known%2520a%250Apriori.%2520The%2520problem%2520is%2520challenging%2520in%2520decentralized%2520settings%2520with%2520robots%2520that%250Ahave%2520limited%2520communication%2520and%2520sensing%2520capabilities.%2520We%2520propose%2520a%2520learnable%250APerception-Action-Communication%2520%2528LPAC%2529%2520architecture%2520for%2520the%2520problem%252C%2520wherein%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520processes%2520localized%2520perception%253B%2520a%2520graph%250Aneural%2520network%2520%2528GNN%2529%2520facilitates%2520robot%2520communications%253B%2520finally%252C%2520a%2520shallow%250Amulti-layer%2520perceptron%2520%2528MLP%2529%2520computes%2520robot%2520actions.%2520The%2520GNN%2520enables%250Acollaboration%2520in%2520the%2520robot%2520swarm%2520by%2520computing%2520what%2520information%2520to%2520communicate%250Awith%2520nearby%2520robots%2520and%2520how%2520to%2520incorporate%2520received%2520information.%2520Evaluations%250Ashow%2520that%2520the%2520LPAC%2520models%2520--%2520trained%2520using%2520imitation%2520learning%2520--%2520outperform%250Astandard%2520decentralized%2520and%2520centralized%2520coverage%2520control%2520algorithms.%2520The%2520learned%250Apolicy%2520generalizes%2520to%2520environments%2520different%2520from%2520the%2520training%2520dataset%252C%250Atransfers%2520to%2520larger%2520environments%2520with%2520more%2520robots%252C%2520and%2520is%2520robust%2520to%2520noisy%250Aposition%2520estimates.%2520The%2520results%2520indicate%2520the%2520suitability%2520of%2520LPAC%2520architectures%250Afor%2520decentralized%2520navigation%2520in%2520robot%2520swarms%2520to%2520achieve%2520collaborative%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04855v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPAC%3A%20Learnable%20Perception-Action-Communication%20Loops%20with%20Applications%0A%20%20to%20Coverage%20Control&entry.906535625=Saurav%20Agarwal%20and%20Ramya%20Muthukrishnan%20and%20Walker%20Gosrich%20and%20Vijay%20Kumar%20and%20Alejandro%20Ribeiro&entry.1292438233=%20%20Coverage%20control%20is%20the%20problem%20of%20navigating%20a%20robot%20swarm%20to%0Acollaboratively%20monitor%20features%20or%20a%20phenomenon%20of%20interest%20not%20known%20a%0Apriori.%20The%20problem%20is%20challenging%20in%20decentralized%20settings%20with%20robots%20that%0Ahave%20limited%20communication%20and%20sensing%20capabilities.%20We%20propose%20a%20learnable%0APerception-Action-Communication%20%28LPAC%29%20architecture%20for%20the%20problem%2C%20wherein%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20processes%20localized%20perception%3B%20a%20graph%0Aneural%20network%20%28GNN%29%20facilitates%20robot%20communications%3B%20finally%2C%20a%20shallow%0Amulti-layer%20perceptron%20%28MLP%29%20computes%20robot%20actions.%20The%20GNN%20enables%0Acollaboration%20in%20the%20robot%20swarm%20by%20computing%20what%20information%20to%20communicate%0Awith%20nearby%20robots%20and%20how%20to%20incorporate%20received%20information.%20Evaluations%0Ashow%20that%20the%20LPAC%20models%20--%20trained%20using%20imitation%20learning%20--%20outperform%0Astandard%20decentralized%20and%20centralized%20coverage%20control%20algorithms.%20The%20learned%0Apolicy%20generalizes%20to%20environments%20different%20from%20the%20training%20dataset%2C%0Atransfers%20to%20larger%20environments%20with%20more%20robots%2C%20and%20is%20robust%20to%20noisy%0Aposition%20estimates.%20The%20results%20indicate%20the%20suitability%20of%20LPAC%20architectures%0Afor%20decentralized%20navigation%20in%20robot%20swarms%20to%20achieve%20collaborative%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04855v4&entry.124074799=Read"},
{"title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement\n  Learning", "author": "Yalin E. Sagduyu and Tugba Erpek and Kemal Davaslioglu and Sastry Kompella", "abstract": "  This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.\n", "link": "http://arxiv.org/abs/2510.02265v1", "date": "2025-10-02", "relevancy": 1.7346, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.425}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Combat%20Reactive%20and%20Dynamic%20Jamming%20Attacks%20with%20Reinforcement%0A%20%20Learning&body=Title%3A%20How%20to%20Combat%20Reactive%20and%20Dynamic%20Jamming%20Attacks%20with%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Yalin%20E.%20Sagduyu%20and%20Tugba%20Erpek%20and%20Kemal%20Davaslioglu%20and%20Sastry%20Kompella%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20mitigating%20reactive%20jamming%2C%20where%20a%20jammer%0Aadopts%20a%20dynamic%20policy%20of%20selecting%20channels%20and%20sensing%20thresholds%20to%20detect%0Aand%20jam%20ongoing%20transmissions.%20The%20transmitter-receiver%20pair%20learns%20to%20avoid%0Ajamming%20and%20optimize%20throughput%20over%20time%20%28without%20prior%20knowledge%20of%20channel%0Aconditions%20or%20jamming%20strategies%29%20by%20using%20reinforcement%20learning%20%28RL%29%20to%20adapt%0Atransmit%20power%2C%20modulation%2C%20and%20channel%20selection.%20Q-learning%20is%20employed%20for%0Adiscrete%20jamming-event%20states%2C%20while%20Deep%20Q-Networks%20%28DQN%29%20are%20employed%20for%0Acontinuous%20states%20based%20on%20received%20power.%20Through%20different%20reward%20functions%0Aand%20action%20sets%2C%20the%20results%20show%20that%20RL%20can%20adapt%20rapidly%20to%20spectrum%0Adynamics%20and%20sustain%20high%20rates%20as%20channels%20and%20jamming%20policies%20change%20over%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Combat%2520Reactive%2520and%2520Dynamic%2520Jamming%2520Attacks%2520with%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DYalin%2520E.%2520Sagduyu%2520and%2520Tugba%2520Erpek%2520and%2520Kemal%2520Davaslioglu%2520and%2520Sastry%2520Kompella%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520problem%2520of%2520mitigating%2520reactive%2520jamming%252C%2520where%2520a%2520jammer%250Aadopts%2520a%2520dynamic%2520policy%2520of%2520selecting%2520channels%2520and%2520sensing%2520thresholds%2520to%2520detect%250Aand%2520jam%2520ongoing%2520transmissions.%2520The%2520transmitter-receiver%2520pair%2520learns%2520to%2520avoid%250Ajamming%2520and%2520optimize%2520throughput%2520over%2520time%2520%2528without%2520prior%2520knowledge%2520of%2520channel%250Aconditions%2520or%2520jamming%2520strategies%2529%2520by%2520using%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520adapt%250Atransmit%2520power%252C%2520modulation%252C%2520and%2520channel%2520selection.%2520Q-learning%2520is%2520employed%2520for%250Adiscrete%2520jamming-event%2520states%252C%2520while%2520Deep%2520Q-Networks%2520%2528DQN%2529%2520are%2520employed%2520for%250Acontinuous%2520states%2520based%2520on%2520received%2520power.%2520Through%2520different%2520reward%2520functions%250Aand%2520action%2520sets%252C%2520the%2520results%2520show%2520that%2520RL%2520can%2520adapt%2520rapidly%2520to%2520spectrum%250Adynamics%2520and%2520sustain%2520high%2520rates%2520as%2520channels%2520and%2520jamming%2520policies%2520change%2520over%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Combat%20Reactive%20and%20Dynamic%20Jamming%20Attacks%20with%20Reinforcement%0A%20%20Learning&entry.906535625=Yalin%20E.%20Sagduyu%20and%20Tugba%20Erpek%20and%20Kemal%20Davaslioglu%20and%20Sastry%20Kompella&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20mitigating%20reactive%20jamming%2C%20where%20a%20jammer%0Aadopts%20a%20dynamic%20policy%20of%20selecting%20channels%20and%20sensing%20thresholds%20to%20detect%0Aand%20jam%20ongoing%20transmissions.%20The%20transmitter-receiver%20pair%20learns%20to%20avoid%0Ajamming%20and%20optimize%20throughput%20over%20time%20%28without%20prior%20knowledge%20of%20channel%0Aconditions%20or%20jamming%20strategies%29%20by%20using%20reinforcement%20learning%20%28RL%29%20to%20adapt%0Atransmit%20power%2C%20modulation%2C%20and%20channel%20selection.%20Q-learning%20is%20employed%20for%0Adiscrete%20jamming-event%20states%2C%20while%20Deep%20Q-Networks%20%28DQN%29%20are%20employed%20for%0Acontinuous%20states%20based%20on%20received%20power.%20Through%20different%20reward%20functions%0Aand%20action%20sets%2C%20the%20results%20show%20that%20RL%20can%20adapt%20rapidly%20to%20spectrum%0Adynamics%20and%20sustain%20high%20rates%20as%20channels%20and%20jamming%20policies%20change%20over%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02265v1&entry.124074799=Read"},
{"title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical\n  Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis", "author": "Jialin Gao and Donghao Zhou and Mingjian Liang and Lihao Liu and Chi-Wing Fu and Xiaowei Hu and Pheng-Ann Heng", "abstract": "  3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.\n", "link": "http://arxiv.org/abs/2510.02178v1", "date": "2025-10-02", "relevancy": 1.7055, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5871}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5659}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCo-Layout%3A%20Disentangling%20and%20Coordinating%20Semantic%20and%20Physical%0A%20%20Refinement%20in%20a%20Multi-Agent%20Framework%20for%203D%20Indoor%20Layout%20Synthesis&body=Title%3A%20DisCo-Layout%3A%20Disentangling%20and%20Coordinating%20Semantic%20and%20Physical%0A%20%20Refinement%20in%20a%20Multi-Agent%20Framework%20for%203D%20Indoor%20Layout%20Synthesis%0AAuthor%3A%20Jialin%20Gao%20and%20Donghao%20Zhou%20and%20Mingjian%20Liang%20and%20Lihao%20Liu%20and%20Chi-Wing%20Fu%20and%20Xiaowei%20Hu%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%203D%20indoor%20layout%20synthesis%20is%20crucial%20for%20creating%20virtual%20environments.%0ATraditional%20methods%20struggle%20with%20generalization%20due%20to%20fixed%20datasets.%20While%0Arecent%20LLM%20and%20VLM-based%20approaches%20offer%20improved%20semantic%20richness%2C%20they%0Aoften%20lack%20robust%20and%20flexible%20refinement%2C%20resulting%20in%20suboptimal%20layouts.%20We%0Adevelop%20DisCo-Layout%2C%20a%20novel%20framework%20that%20disentangles%20and%20coordinates%0Aphysical%20and%20semantic%20refinement.%20For%20independent%20refinement%2C%20our%20Semantic%0ARefinement%20Tool%20%28SRT%29%20corrects%20abstract%20object%20relationships%2C%20while%20the%0APhysical%20Refinement%20Tool%20%28PRT%29%20resolves%20concrete%20spatial%20issues%20via%20a%0Agrid-matching%20algorithm.%20For%20collaborative%20refinement%2C%20a%20multi-agent%20framework%0Aintelligently%20orchestrates%20these%20tools%2C%20featuring%20a%20planner%20for%20placement%0Arules%2C%20a%20designer%20for%20initial%20layouts%2C%20and%20an%20evaluator%20for%20assessment.%0AExperiments%20demonstrate%20DisCo-Layout%27s%20state-of-the-art%20performance%2C%20generating%0Arealistic%2C%20coherent%2C%20and%20generalizable%203D%20indoor%20layouts.%20Our%20code%20will%20be%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCo-Layout%253A%2520Disentangling%2520and%2520Coordinating%2520Semantic%2520and%2520Physical%250A%2520%2520Refinement%2520in%2520a%2520Multi-Agent%2520Framework%2520for%25203D%2520Indoor%2520Layout%2520Synthesis%26entry.906535625%3DJialin%2520Gao%2520and%2520Donghao%2520Zhou%2520and%2520Mingjian%2520Liang%2520and%2520Lihao%2520Liu%2520and%2520Chi-Wing%2520Fu%2520and%2520Xiaowei%2520Hu%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3D%2520%25203D%2520indoor%2520layout%2520synthesis%2520is%2520crucial%2520for%2520creating%2520virtual%2520environments.%250ATraditional%2520methods%2520struggle%2520with%2520generalization%2520due%2520to%2520fixed%2520datasets.%2520While%250Arecent%2520LLM%2520and%2520VLM-based%2520approaches%2520offer%2520improved%2520semantic%2520richness%252C%2520they%250Aoften%2520lack%2520robust%2520and%2520flexible%2520refinement%252C%2520resulting%2520in%2520suboptimal%2520layouts.%2520We%250Adevelop%2520DisCo-Layout%252C%2520a%2520novel%2520framework%2520that%2520disentangles%2520and%2520coordinates%250Aphysical%2520and%2520semantic%2520refinement.%2520For%2520independent%2520refinement%252C%2520our%2520Semantic%250ARefinement%2520Tool%2520%2528SRT%2529%2520corrects%2520abstract%2520object%2520relationships%252C%2520while%2520the%250APhysical%2520Refinement%2520Tool%2520%2528PRT%2529%2520resolves%2520concrete%2520spatial%2520issues%2520via%2520a%250Agrid-matching%2520algorithm.%2520For%2520collaborative%2520refinement%252C%2520a%2520multi-agent%2520framework%250Aintelligently%2520orchestrates%2520these%2520tools%252C%2520featuring%2520a%2520planner%2520for%2520placement%250Arules%252C%2520a%2520designer%2520for%2520initial%2520layouts%252C%2520and%2520an%2520evaluator%2520for%2520assessment.%250AExperiments%2520demonstrate%2520DisCo-Layout%2527s%2520state-of-the-art%2520performance%252C%2520generating%250Arealistic%252C%2520coherent%252C%2520and%2520generalizable%25203D%2520indoor%2520layouts.%2520Our%2520code%2520will%2520be%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCo-Layout%3A%20Disentangling%20and%20Coordinating%20Semantic%20and%20Physical%0A%20%20Refinement%20in%20a%20Multi-Agent%20Framework%20for%203D%20Indoor%20Layout%20Synthesis&entry.906535625=Jialin%20Gao%20and%20Donghao%20Zhou%20and%20Mingjian%20Liang%20and%20Lihao%20Liu%20and%20Chi-Wing%20Fu%20and%20Xiaowei%20Hu%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%203D%20indoor%20layout%20synthesis%20is%20crucial%20for%20creating%20virtual%20environments.%0ATraditional%20methods%20struggle%20with%20generalization%20due%20to%20fixed%20datasets.%20While%0Arecent%20LLM%20and%20VLM-based%20approaches%20offer%20improved%20semantic%20richness%2C%20they%0Aoften%20lack%20robust%20and%20flexible%20refinement%2C%20resulting%20in%20suboptimal%20layouts.%20We%0Adevelop%20DisCo-Layout%2C%20a%20novel%20framework%20that%20disentangles%20and%20coordinates%0Aphysical%20and%20semantic%20refinement.%20For%20independent%20refinement%2C%20our%20Semantic%0ARefinement%20Tool%20%28SRT%29%20corrects%20abstract%20object%20relationships%2C%20while%20the%0APhysical%20Refinement%20Tool%20%28PRT%29%20resolves%20concrete%20spatial%20issues%20via%20a%0Agrid-matching%20algorithm.%20For%20collaborative%20refinement%2C%20a%20multi-agent%20framework%0Aintelligently%20orchestrates%20these%20tools%2C%20featuring%20a%20planner%20for%20placement%0Arules%2C%20a%20designer%20for%20initial%20layouts%2C%20and%20an%20evaluator%20for%20assessment.%0AExperiments%20demonstrate%20DisCo-Layout%27s%20state-of-the-art%20performance%2C%20generating%0Arealistic%2C%20coherent%2C%20and%20generalizable%203D%20indoor%20layouts.%20Our%20code%20will%20be%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02178v1&entry.124074799=Read"},
{"title": "SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation", "author": "Qianzhong Chen and Justin Yu and Mac Schwager and Pieter Abbeel and Fred Shentu and Philipp Wu", "abstract": "  Large-scale robot learning has recently shown promise for enabling robots to\nperform complex tasks by integrating perception, control, and language\nunderstanding. Yet, it struggles with long-horizon, contact-rich manipulation\nsuch as deformable object handling, where demonstration quality is\ninconsistent. Reward modeling offers a natural solution: by providing grounded\nprogress signals, it transforms noisy demonstrations into stable supervision\nthat generalizes across diverse trajectories. We introduce a stage-aware,\nvideo-based reward modeling framework that jointly predicts high-level task\nstages and fine-grained progress. Reward labels are automatically derived from\nnatural language subtask annotations, ensuring consistent progress estimation\nacross variable-length demonstrations. This design overcomes frame-index\nlabeling, which fails in variable-duration tasks like folding a T-shirt. Our\nreward model demonstrates robustness to variability, generalization to\nout-of-distribution settings, and strong utility for policy training. Building\non it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters\nhigh-quality data and reweights samples by reward. Experiments show the reward\nmodel alone outperforms baselines on validation and real robot rollouts.\nIntegrated into RA-BC, our approach achieves 83\\% success on folding T-shirts\nfrom the flattened state and 67\\% from the crumpled state -- far surpassing\nvanilla behavior cloning, which attains only 8\\% and 0\\% success. Overall, our\nresults highlight reward modeling as a key enabler for scalable,\nannotation-efficient, and robust imitation learning in long-horizon\nmanipulation.\n", "link": "http://arxiv.org/abs/2509.25358v2", "date": "2025-10-02", "relevancy": 1.7052, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6083}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARM%3A%20Stage-Aware%20Reward%20Modeling%20for%20Long%20Horizon%20Robot%20Manipulation&body=Title%3A%20SARM%3A%20Stage-Aware%20Reward%20Modeling%20for%20Long%20Horizon%20Robot%20Manipulation%0AAuthor%3A%20Qianzhong%20Chen%20and%20Justin%20Yu%20and%20Mac%20Schwager%20and%20Pieter%20Abbeel%20and%20Fred%20Shentu%20and%20Philipp%20Wu%0AAbstract%3A%20%20%20Large-scale%20robot%20learning%20has%20recently%20shown%20promise%20for%20enabling%20robots%20to%0Aperform%20complex%20tasks%20by%20integrating%20perception%2C%20control%2C%20and%20language%0Aunderstanding.%20Yet%2C%20it%20struggles%20with%20long-horizon%2C%20contact-rich%20manipulation%0Asuch%20as%20deformable%20object%20handling%2C%20where%20demonstration%20quality%20is%0Ainconsistent.%20Reward%20modeling%20offers%20a%20natural%20solution%3A%20by%20providing%20grounded%0Aprogress%20signals%2C%20it%20transforms%20noisy%20demonstrations%20into%20stable%20supervision%0Athat%20generalizes%20across%20diverse%20trajectories.%20We%20introduce%20a%20stage-aware%2C%0Avideo-based%20reward%20modeling%20framework%20that%20jointly%20predicts%20high-level%20task%0Astages%20and%20fine-grained%20progress.%20Reward%20labels%20are%20automatically%20derived%20from%0Anatural%20language%20subtask%20annotations%2C%20ensuring%20consistent%20progress%20estimation%0Aacross%20variable-length%20demonstrations.%20This%20design%20overcomes%20frame-index%0Alabeling%2C%20which%20fails%20in%20variable-duration%20tasks%20like%20folding%20a%20T-shirt.%20Our%0Areward%20model%20demonstrates%20robustness%20to%20variability%2C%20generalization%20to%0Aout-of-distribution%20settings%2C%20and%20strong%20utility%20for%20policy%20training.%20Building%0Aon%20it%2C%20we%20propose%20Reward-Aligned%20Behavior%20Cloning%20%28RA-BC%29%2C%20which%20filters%0Ahigh-quality%20data%20and%20reweights%20samples%20by%20reward.%20Experiments%20show%20the%20reward%0Amodel%20alone%20outperforms%20baselines%20on%20validation%20and%20real%20robot%20rollouts.%0AIntegrated%20into%20RA-BC%2C%20our%20approach%20achieves%2083%5C%25%20success%20on%20folding%20T-shirts%0Afrom%20the%20flattened%20state%20and%2067%5C%25%20from%20the%20crumpled%20state%20--%20far%20surpassing%0Avanilla%20behavior%20cloning%2C%20which%20attains%20only%208%5C%25%20and%200%5C%25%20success.%20Overall%2C%20our%0Aresults%20highlight%20reward%20modeling%20as%20a%20key%20enabler%20for%20scalable%2C%0Aannotation-efficient%2C%20and%20robust%20imitation%20learning%20in%20long-horizon%0Amanipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARM%253A%2520Stage-Aware%2520Reward%2520Modeling%2520for%2520Long%2520Horizon%2520Robot%2520Manipulation%26entry.906535625%3DQianzhong%2520Chen%2520and%2520Justin%2520Yu%2520and%2520Mac%2520Schwager%2520and%2520Pieter%2520Abbeel%2520and%2520Fred%2520Shentu%2520and%2520Philipp%2520Wu%26entry.1292438233%3D%2520%2520Large-scale%2520robot%2520learning%2520has%2520recently%2520shown%2520promise%2520for%2520enabling%2520robots%2520to%250Aperform%2520complex%2520tasks%2520by%2520integrating%2520perception%252C%2520control%252C%2520and%2520language%250Aunderstanding.%2520Yet%252C%2520it%2520struggles%2520with%2520long-horizon%252C%2520contact-rich%2520manipulation%250Asuch%2520as%2520deformable%2520object%2520handling%252C%2520where%2520demonstration%2520quality%2520is%250Ainconsistent.%2520Reward%2520modeling%2520offers%2520a%2520natural%2520solution%253A%2520by%2520providing%2520grounded%250Aprogress%2520signals%252C%2520it%2520transforms%2520noisy%2520demonstrations%2520into%2520stable%2520supervision%250Athat%2520generalizes%2520across%2520diverse%2520trajectories.%2520We%2520introduce%2520a%2520stage-aware%252C%250Avideo-based%2520reward%2520modeling%2520framework%2520that%2520jointly%2520predicts%2520high-level%2520task%250Astages%2520and%2520fine-grained%2520progress.%2520Reward%2520labels%2520are%2520automatically%2520derived%2520from%250Anatural%2520language%2520subtask%2520annotations%252C%2520ensuring%2520consistent%2520progress%2520estimation%250Aacross%2520variable-length%2520demonstrations.%2520This%2520design%2520overcomes%2520frame-index%250Alabeling%252C%2520which%2520fails%2520in%2520variable-duration%2520tasks%2520like%2520folding%2520a%2520T-shirt.%2520Our%250Areward%2520model%2520demonstrates%2520robustness%2520to%2520variability%252C%2520generalization%2520to%250Aout-of-distribution%2520settings%252C%2520and%2520strong%2520utility%2520for%2520policy%2520training.%2520Building%250Aon%2520it%252C%2520we%2520propose%2520Reward-Aligned%2520Behavior%2520Cloning%2520%2528RA-BC%2529%252C%2520which%2520filters%250Ahigh-quality%2520data%2520and%2520reweights%2520samples%2520by%2520reward.%2520Experiments%2520show%2520the%2520reward%250Amodel%2520alone%2520outperforms%2520baselines%2520on%2520validation%2520and%2520real%2520robot%2520rollouts.%250AIntegrated%2520into%2520RA-BC%252C%2520our%2520approach%2520achieves%252083%255C%2525%2520success%2520on%2520folding%2520T-shirts%250Afrom%2520the%2520flattened%2520state%2520and%252067%255C%2525%2520from%2520the%2520crumpled%2520state%2520--%2520far%2520surpassing%250Avanilla%2520behavior%2520cloning%252C%2520which%2520attains%2520only%25208%255C%2525%2520and%25200%255C%2525%2520success.%2520Overall%252C%2520our%250Aresults%2520highlight%2520reward%2520modeling%2520as%2520a%2520key%2520enabler%2520for%2520scalable%252C%250Aannotation-efficient%252C%2520and%2520robust%2520imitation%2520learning%2520in%2520long-horizon%250Amanipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARM%3A%20Stage-Aware%20Reward%20Modeling%20for%20Long%20Horizon%20Robot%20Manipulation&entry.906535625=Qianzhong%20Chen%20and%20Justin%20Yu%20and%20Mac%20Schwager%20and%20Pieter%20Abbeel%20and%20Fred%20Shentu%20and%20Philipp%20Wu&entry.1292438233=%20%20Large-scale%20robot%20learning%20has%20recently%20shown%20promise%20for%20enabling%20robots%20to%0Aperform%20complex%20tasks%20by%20integrating%20perception%2C%20control%2C%20and%20language%0Aunderstanding.%20Yet%2C%20it%20struggles%20with%20long-horizon%2C%20contact-rich%20manipulation%0Asuch%20as%20deformable%20object%20handling%2C%20where%20demonstration%20quality%20is%0Ainconsistent.%20Reward%20modeling%20offers%20a%20natural%20solution%3A%20by%20providing%20grounded%0Aprogress%20signals%2C%20it%20transforms%20noisy%20demonstrations%20into%20stable%20supervision%0Athat%20generalizes%20across%20diverse%20trajectories.%20We%20introduce%20a%20stage-aware%2C%0Avideo-based%20reward%20modeling%20framework%20that%20jointly%20predicts%20high-level%20task%0Astages%20and%20fine-grained%20progress.%20Reward%20labels%20are%20automatically%20derived%20from%0Anatural%20language%20subtask%20annotations%2C%20ensuring%20consistent%20progress%20estimation%0Aacross%20variable-length%20demonstrations.%20This%20design%20overcomes%20frame-index%0Alabeling%2C%20which%20fails%20in%20variable-duration%20tasks%20like%20folding%20a%20T-shirt.%20Our%0Areward%20model%20demonstrates%20robustness%20to%20variability%2C%20generalization%20to%0Aout-of-distribution%20settings%2C%20and%20strong%20utility%20for%20policy%20training.%20Building%0Aon%20it%2C%20we%20propose%20Reward-Aligned%20Behavior%20Cloning%20%28RA-BC%29%2C%20which%20filters%0Ahigh-quality%20data%20and%20reweights%20samples%20by%20reward.%20Experiments%20show%20the%20reward%0Amodel%20alone%20outperforms%20baselines%20on%20validation%20and%20real%20robot%20rollouts.%0AIntegrated%20into%20RA-BC%2C%20our%20approach%20achieves%2083%5C%25%20success%20on%20folding%20T-shirts%0Afrom%20the%20flattened%20state%20and%2067%5C%25%20from%20the%20crumpled%20state%20--%20far%20surpassing%0Avanilla%20behavior%20cloning%2C%20which%20attains%20only%208%5C%25%20and%200%5C%25%20success.%20Overall%2C%20our%0Aresults%20highlight%20reward%20modeling%20as%20a%20key%20enabler%20for%20scalable%2C%0Aannotation-efficient%2C%20and%20robust%20imitation%20learning%20in%20long-horizon%0Amanipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25358v2&entry.124074799=Read"},
{"title": "Diffusion Transformers for Imputation: Statistical Efficiency and\n  Uncertainty Quantification", "author": "Zeqi Ye and Minshuo Chen", "abstract": "  Imputation methods play a critical role in enhancing the quality of practical\ntime-series data, which often suffer from pervasive missing values. Recently,\ndiffusion-based generative imputation methods have demonstrated remarkable\nsuccess compared to autoregressive and conventional statistical approaches.\nDespite their empirical success, the theoretical understanding of how well\ndiffusion-based models capture complex spatial and temporal dependencies\nbetween the missing values and observed ones remains limited. Our work\naddresses this gap by investigating the statistical efficiency of conditional\ndiffusion transformers for imputation and quantifying the uncertainty in\nmissing values. Specifically, we derive statistical sample complexity bounds\nbased on a novel approximation theory for conditional score functions using\ntransformers, and, through this, construct tight confidence regions for missing\nvalues. Our findings also reveal that the efficiency and accuracy of imputation\nare significantly influenced by the missing patterns. Furthermore, we validate\nthese theoretical insights through simulation and propose a mixed-masking\ntraining strategy to enhance the imputation performance.\n", "link": "http://arxiv.org/abs/2510.02216v1", "date": "2025-10-02", "relevancy": 1.7011, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6327}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5748}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Transformers%20for%20Imputation%3A%20Statistical%20Efficiency%20and%0A%20%20Uncertainty%20Quantification&body=Title%3A%20Diffusion%20Transformers%20for%20Imputation%3A%20Statistical%20Efficiency%20and%0A%20%20Uncertainty%20Quantification%0AAuthor%3A%20Zeqi%20Ye%20and%20Minshuo%20Chen%0AAbstract%3A%20%20%20Imputation%20methods%20play%20a%20critical%20role%20in%20enhancing%20the%20quality%20of%20practical%0Atime-series%20data%2C%20which%20often%20suffer%20from%20pervasive%20missing%20values.%20Recently%2C%0Adiffusion-based%20generative%20imputation%20methods%20have%20demonstrated%20remarkable%0Asuccess%20compared%20to%20autoregressive%20and%20conventional%20statistical%20approaches.%0ADespite%20their%20empirical%20success%2C%20the%20theoretical%20understanding%20of%20how%20well%0Adiffusion-based%20models%20capture%20complex%20spatial%20and%20temporal%20dependencies%0Abetween%20the%20missing%20values%20and%20observed%20ones%20remains%20limited.%20Our%20work%0Aaddresses%20this%20gap%20by%20investigating%20the%20statistical%20efficiency%20of%20conditional%0Adiffusion%20transformers%20for%20imputation%20and%20quantifying%20the%20uncertainty%20in%0Amissing%20values.%20Specifically%2C%20we%20derive%20statistical%20sample%20complexity%20bounds%0Abased%20on%20a%20novel%20approximation%20theory%20for%20conditional%20score%20functions%20using%0Atransformers%2C%20and%2C%20through%20this%2C%20construct%20tight%20confidence%20regions%20for%20missing%0Avalues.%20Our%20findings%20also%20reveal%20that%20the%20efficiency%20and%20accuracy%20of%20imputation%0Aare%20significantly%20influenced%20by%20the%20missing%20patterns.%20Furthermore%2C%20we%20validate%0Athese%20theoretical%20insights%20through%20simulation%20and%20propose%20a%20mixed-masking%0Atraining%20strategy%20to%20enhance%20the%20imputation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Transformers%2520for%2520Imputation%253A%2520Statistical%2520Efficiency%2520and%250A%2520%2520Uncertainty%2520Quantification%26entry.906535625%3DZeqi%2520Ye%2520and%2520Minshuo%2520Chen%26entry.1292438233%3D%2520%2520Imputation%2520methods%2520play%2520a%2520critical%2520role%2520in%2520enhancing%2520the%2520quality%2520of%2520practical%250Atime-series%2520data%252C%2520which%2520often%2520suffer%2520from%2520pervasive%2520missing%2520values.%2520Recently%252C%250Adiffusion-based%2520generative%2520imputation%2520methods%2520have%2520demonstrated%2520remarkable%250Asuccess%2520compared%2520to%2520autoregressive%2520and%2520conventional%2520statistical%2520approaches.%250ADespite%2520their%2520empirical%2520success%252C%2520the%2520theoretical%2520understanding%2520of%2520how%2520well%250Adiffusion-based%2520models%2520capture%2520complex%2520spatial%2520and%2520temporal%2520dependencies%250Abetween%2520the%2520missing%2520values%2520and%2520observed%2520ones%2520remains%2520limited.%2520Our%2520work%250Aaddresses%2520this%2520gap%2520by%2520investigating%2520the%2520statistical%2520efficiency%2520of%2520conditional%250Adiffusion%2520transformers%2520for%2520imputation%2520and%2520quantifying%2520the%2520uncertainty%2520in%250Amissing%2520values.%2520Specifically%252C%2520we%2520derive%2520statistical%2520sample%2520complexity%2520bounds%250Abased%2520on%2520a%2520novel%2520approximation%2520theory%2520for%2520conditional%2520score%2520functions%2520using%250Atransformers%252C%2520and%252C%2520through%2520this%252C%2520construct%2520tight%2520confidence%2520regions%2520for%2520missing%250Avalues.%2520Our%2520findings%2520also%2520reveal%2520that%2520the%2520efficiency%2520and%2520accuracy%2520of%2520imputation%250Aare%2520significantly%2520influenced%2520by%2520the%2520missing%2520patterns.%2520Furthermore%252C%2520we%2520validate%250Athese%2520theoretical%2520insights%2520through%2520simulation%2520and%2520propose%2520a%2520mixed-masking%250Atraining%2520strategy%2520to%2520enhance%2520the%2520imputation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Transformers%20for%20Imputation%3A%20Statistical%20Efficiency%20and%0A%20%20Uncertainty%20Quantification&entry.906535625=Zeqi%20Ye%20and%20Minshuo%20Chen&entry.1292438233=%20%20Imputation%20methods%20play%20a%20critical%20role%20in%20enhancing%20the%20quality%20of%20practical%0Atime-series%20data%2C%20which%20often%20suffer%20from%20pervasive%20missing%20values.%20Recently%2C%0Adiffusion-based%20generative%20imputation%20methods%20have%20demonstrated%20remarkable%0Asuccess%20compared%20to%20autoregressive%20and%20conventional%20statistical%20approaches.%0ADespite%20their%20empirical%20success%2C%20the%20theoretical%20understanding%20of%20how%20well%0Adiffusion-based%20models%20capture%20complex%20spatial%20and%20temporal%20dependencies%0Abetween%20the%20missing%20values%20and%20observed%20ones%20remains%20limited.%20Our%20work%0Aaddresses%20this%20gap%20by%20investigating%20the%20statistical%20efficiency%20of%20conditional%0Adiffusion%20transformers%20for%20imputation%20and%20quantifying%20the%20uncertainty%20in%0Amissing%20values.%20Specifically%2C%20we%20derive%20statistical%20sample%20complexity%20bounds%0Abased%20on%20a%20novel%20approximation%20theory%20for%20conditional%20score%20functions%20using%0Atransformers%2C%20and%2C%20through%20this%2C%20construct%20tight%20confidence%20regions%20for%20missing%0Avalues.%20Our%20findings%20also%20reveal%20that%20the%20efficiency%20and%20accuracy%20of%20imputation%0Aare%20significantly%20influenced%20by%20the%20missing%20patterns.%20Furthermore%2C%20we%20validate%0Athese%20theoretical%20insights%20through%20simulation%20and%20propose%20a%20mixed-masking%0Atraining%20strategy%20to%20enhance%20the%20imputation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02216v1&entry.124074799=Read"},
{"title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models", "author": "Runqian Wang and Yilun Du", "abstract": "  We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.\n", "link": "http://arxiv.org/abs/2510.02300v1", "date": "2025-10-02", "relevancy": 1.6829, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5765}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5566}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equilibrium%20Matching%3A%20Generative%20Modeling%20with%20Implicit%20Energy-Based%0A%20%20Models&body=Title%3A%20Equilibrium%20Matching%3A%20Generative%20Modeling%20with%20Implicit%20Energy-Based%0A%20%20Models%0AAuthor%3A%20Runqian%20Wang%20and%20Yilun%20Du%0AAbstract%3A%20%20%20We%20introduce%20Equilibrium%20Matching%20%28EqM%29%2C%20a%20generative%20modeling%20framework%0Abuilt%20from%20an%20equilibrium%20dynamics%20perspective.%20EqM%20discards%20the%0Anon-equilibrium%2C%20time-conditional%20dynamics%20in%20traditional%20diffusion%20and%0Aflow-based%20generative%20models%20and%20instead%20learns%20the%20equilibrium%20gradient%20of%20an%0Aimplicit%20energy%20landscape.%20Through%20this%20approach%2C%20we%20can%20adopt%20an%0Aoptimization-based%20sampling%20process%20at%20inference%20time%2C%20where%20samples%20are%0Aobtained%20by%20gradient%20descent%20on%20the%20learned%20landscape%20with%20adjustable%20step%0Asizes%2C%20adaptive%20optimizers%2C%20and%20adaptive%20compute.%20EqM%20surpasses%20the%20generation%0Aperformance%20of%20diffusion/flow%20models%20empirically%2C%20achieving%20an%20FID%20of%201.90%20on%0AImageNet%20256%24%5Ctimes%24256.%20EqM%20is%20also%20theoretically%20justified%20to%20learn%20and%0Asample%20from%20the%20data%20manifold.%20Beyond%20generation%2C%20EqM%20is%20a%20flexible%20framework%0Athat%20naturally%20handles%20tasks%20including%20partially%20noised%20image%20denoising%2C%20OOD%0Adetection%2C%20and%20image%20composition.%20By%20replacing%20time-conditional%20velocities%20with%0Aa%20unified%20equilibrium%20landscape%2C%20EqM%20offers%20a%20tighter%20bridge%20between%20flow%20and%0Aenergy-based%20models%20and%20a%20simple%20route%20to%20optimization-driven%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquilibrium%2520Matching%253A%2520Generative%2520Modeling%2520with%2520Implicit%2520Energy-Based%250A%2520%2520Models%26entry.906535625%3DRunqian%2520Wang%2520and%2520Yilun%2520Du%26entry.1292438233%3D%2520%2520We%2520introduce%2520Equilibrium%2520Matching%2520%2528EqM%2529%252C%2520a%2520generative%2520modeling%2520framework%250Abuilt%2520from%2520an%2520equilibrium%2520dynamics%2520perspective.%2520EqM%2520discards%2520the%250Anon-equilibrium%252C%2520time-conditional%2520dynamics%2520in%2520traditional%2520diffusion%2520and%250Aflow-based%2520generative%2520models%2520and%2520instead%2520learns%2520the%2520equilibrium%2520gradient%2520of%2520an%250Aimplicit%2520energy%2520landscape.%2520Through%2520this%2520approach%252C%2520we%2520can%2520adopt%2520an%250Aoptimization-based%2520sampling%2520process%2520at%2520inference%2520time%252C%2520where%2520samples%2520are%250Aobtained%2520by%2520gradient%2520descent%2520on%2520the%2520learned%2520landscape%2520with%2520adjustable%2520step%250Asizes%252C%2520adaptive%2520optimizers%252C%2520and%2520adaptive%2520compute.%2520EqM%2520surpasses%2520the%2520generation%250Aperformance%2520of%2520diffusion/flow%2520models%2520empirically%252C%2520achieving%2520an%2520FID%2520of%25201.90%2520on%250AImageNet%2520256%2524%255Ctimes%2524256.%2520EqM%2520is%2520also%2520theoretically%2520justified%2520to%2520learn%2520and%250Asample%2520from%2520the%2520data%2520manifold.%2520Beyond%2520generation%252C%2520EqM%2520is%2520a%2520flexible%2520framework%250Athat%2520naturally%2520handles%2520tasks%2520including%2520partially%2520noised%2520image%2520denoising%252C%2520OOD%250Adetection%252C%2520and%2520image%2520composition.%2520By%2520replacing%2520time-conditional%2520velocities%2520with%250Aa%2520unified%2520equilibrium%2520landscape%252C%2520EqM%2520offers%2520a%2520tighter%2520bridge%2520between%2520flow%2520and%250Aenergy-based%2520models%2520and%2520a%2520simple%2520route%2520to%2520optimization-driven%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equilibrium%20Matching%3A%20Generative%20Modeling%20with%20Implicit%20Energy-Based%0A%20%20Models&entry.906535625=Runqian%20Wang%20and%20Yilun%20Du&entry.1292438233=%20%20We%20introduce%20Equilibrium%20Matching%20%28EqM%29%2C%20a%20generative%20modeling%20framework%0Abuilt%20from%20an%20equilibrium%20dynamics%20perspective.%20EqM%20discards%20the%0Anon-equilibrium%2C%20time-conditional%20dynamics%20in%20traditional%20diffusion%20and%0Aflow-based%20generative%20models%20and%20instead%20learns%20the%20equilibrium%20gradient%20of%20an%0Aimplicit%20energy%20landscape.%20Through%20this%20approach%2C%20we%20can%20adopt%20an%0Aoptimization-based%20sampling%20process%20at%20inference%20time%2C%20where%20samples%20are%0Aobtained%20by%20gradient%20descent%20on%20the%20learned%20landscape%20with%20adjustable%20step%0Asizes%2C%20adaptive%20optimizers%2C%20and%20adaptive%20compute.%20EqM%20surpasses%20the%20generation%0Aperformance%20of%20diffusion/flow%20models%20empirically%2C%20achieving%20an%20FID%20of%201.90%20on%0AImageNet%20256%24%5Ctimes%24256.%20EqM%20is%20also%20theoretically%20justified%20to%20learn%20and%0Asample%20from%20the%20data%20manifold.%20Beyond%20generation%2C%20EqM%20is%20a%20flexible%20framework%0Athat%20naturally%20handles%20tasks%20including%20partially%20noised%20image%20denoising%2C%20OOD%0Adetection%2C%20and%20image%20composition.%20By%20replacing%20time-conditional%20velocities%20with%0Aa%20unified%20equilibrium%20landscape%2C%20EqM%20offers%20a%20tighter%20bridge%20between%20flow%20and%0Aenergy-based%20models%20and%20a%20simple%20route%20to%20optimization-driven%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02300v1&entry.124074799=Read"},
{"title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity", "author": "Eric Tillmann Bill and Enis Simsar and Thomas Hofmann", "abstract": "  Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.\n", "link": "http://arxiv.org/abs/2510.02315v1", "date": "2025-10-02", "relevancy": 1.6477, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5563}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5489}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Control%20Meets%20Flow%20Matching%3A%20A%20Principled%20Route%20to%20Multi-Subject%0A%20%20Fidelity&body=Title%3A%20Optimal%20Control%20Meets%20Flow%20Matching%3A%20A%20Principled%20Route%20to%20Multi-Subject%0A%20%20Fidelity%0AAuthor%3A%20Eric%20Tillmann%20Bill%20and%20Enis%20Simsar%20and%20Thomas%20Hofmann%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20models%20excel%20on%20single-entity%20prompts%20but%20struggle%20with%0Amulti-subject%20descriptions%2C%20often%20showing%20attribute%20leakage%2C%20identity%0Aentanglement%2C%20and%20subject%20omissions.%20We%20introduce%20the%20first%20theoretical%0Aframework%20with%20a%20principled%2C%20optimizable%20objective%20for%20steering%20sampling%0Adynamics%20toward%20multi-subject%20fidelity.%20Viewing%20flow%20matching%20%28FM%29%20through%0Astochastic%20optimal%20control%20%28SOC%29%2C%20we%20formulate%20subject%20disentanglement%20as%0Acontrol%20over%20a%20trained%20FM%20sampler.%20This%20yields%20two%20architecture-agnostic%0Aalgorithms%3A%20%28i%29%20a%20training-free%20test-time%20controller%20that%20perturbs%20the%20base%0Avelocity%20with%20a%20single-pass%20update%2C%20and%20%28ii%29%20Adjoint%20Matching%2C%20a%20lightweight%0Afine-tuning%20rule%20that%20regresses%20a%20control%20network%20to%20a%20backward%20adjoint%20signal%0Awhile%20preserving%20base-model%20capabilities.%20The%20same%20formulation%20unifies%20prior%0Aattention%20heuristics%2C%20extends%20to%20diffusion%20models%20via%20a%20flow-diffusion%0Acorrespondence%2C%20and%20provides%20the%20first%20fine-tuning%20route%20explicitly%20designed%0Afor%20multi-subject%20fidelity.%20Empirically%2C%20on%20Stable%20Diffusion%203.5%2C%20FLUX%2C%20and%0AStable%20Diffusion%20XL%2C%20both%20algorithms%20consistently%20improve%20multi-subject%0Aalignment%20while%20maintaining%20base-model%20style.%20Test-time%20control%20runs%0Aefficiently%20on%20commodity%20GPUs%2C%20and%20fine-tuned%20controllers%20trained%20on%20limited%0Aprompts%20generalize%20to%20unseen%20ones.%20We%20further%20highlight%20FOCUS%20%28Flow%20Optimal%0AControl%20for%20Unentangled%20Subjects%29%2C%20which%20achieves%20state-of-the-art%0Amulti-subject%20fidelity%20across%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Control%2520Meets%2520Flow%2520Matching%253A%2520A%2520Principled%2520Route%2520to%2520Multi-Subject%250A%2520%2520Fidelity%26entry.906535625%3DEric%2520Tillmann%2520Bill%2520and%2520Enis%2520Simsar%2520and%2520Thomas%2520Hofmann%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520models%2520excel%2520on%2520single-entity%2520prompts%2520but%2520struggle%2520with%250Amulti-subject%2520descriptions%252C%2520often%2520showing%2520attribute%2520leakage%252C%2520identity%250Aentanglement%252C%2520and%2520subject%2520omissions.%2520We%2520introduce%2520the%2520first%2520theoretical%250Aframework%2520with%2520a%2520principled%252C%2520optimizable%2520objective%2520for%2520steering%2520sampling%250Adynamics%2520toward%2520multi-subject%2520fidelity.%2520Viewing%2520flow%2520matching%2520%2528FM%2529%2520through%250Astochastic%2520optimal%2520control%2520%2528SOC%2529%252C%2520we%2520formulate%2520subject%2520disentanglement%2520as%250Acontrol%2520over%2520a%2520trained%2520FM%2520sampler.%2520This%2520yields%2520two%2520architecture-agnostic%250Aalgorithms%253A%2520%2528i%2529%2520a%2520training-free%2520test-time%2520controller%2520that%2520perturbs%2520the%2520base%250Avelocity%2520with%2520a%2520single-pass%2520update%252C%2520and%2520%2528ii%2529%2520Adjoint%2520Matching%252C%2520a%2520lightweight%250Afine-tuning%2520rule%2520that%2520regresses%2520a%2520control%2520network%2520to%2520a%2520backward%2520adjoint%2520signal%250Awhile%2520preserving%2520base-model%2520capabilities.%2520The%2520same%2520formulation%2520unifies%2520prior%250Aattention%2520heuristics%252C%2520extends%2520to%2520diffusion%2520models%2520via%2520a%2520flow-diffusion%250Acorrespondence%252C%2520and%2520provides%2520the%2520first%2520fine-tuning%2520route%2520explicitly%2520designed%250Afor%2520multi-subject%2520fidelity.%2520Empirically%252C%2520on%2520Stable%2520Diffusion%25203.5%252C%2520FLUX%252C%2520and%250AStable%2520Diffusion%2520XL%252C%2520both%2520algorithms%2520consistently%2520improve%2520multi-subject%250Aalignment%2520while%2520maintaining%2520base-model%2520style.%2520Test-time%2520control%2520runs%250Aefficiently%2520on%2520commodity%2520GPUs%252C%2520and%2520fine-tuned%2520controllers%2520trained%2520on%2520limited%250Aprompts%2520generalize%2520to%2520unseen%2520ones.%2520We%2520further%2520highlight%2520FOCUS%2520%2528Flow%2520Optimal%250AControl%2520for%2520Unentangled%2520Subjects%2529%252C%2520which%2520achieves%2520state-of-the-art%250Amulti-subject%2520fidelity%2520across%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Control%20Meets%20Flow%20Matching%3A%20A%20Principled%20Route%20to%20Multi-Subject%0A%20%20Fidelity&entry.906535625=Eric%20Tillmann%20Bill%20and%20Enis%20Simsar%20and%20Thomas%20Hofmann&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20models%20excel%20on%20single-entity%20prompts%20but%20struggle%20with%0Amulti-subject%20descriptions%2C%20often%20showing%20attribute%20leakage%2C%20identity%0Aentanglement%2C%20and%20subject%20omissions.%20We%20introduce%20the%20first%20theoretical%0Aframework%20with%20a%20principled%2C%20optimizable%20objective%20for%20steering%20sampling%0Adynamics%20toward%20multi-subject%20fidelity.%20Viewing%20flow%20matching%20%28FM%29%20through%0Astochastic%20optimal%20control%20%28SOC%29%2C%20we%20formulate%20subject%20disentanglement%20as%0Acontrol%20over%20a%20trained%20FM%20sampler.%20This%20yields%20two%20architecture-agnostic%0Aalgorithms%3A%20%28i%29%20a%20training-free%20test-time%20controller%20that%20perturbs%20the%20base%0Avelocity%20with%20a%20single-pass%20update%2C%20and%20%28ii%29%20Adjoint%20Matching%2C%20a%20lightweight%0Afine-tuning%20rule%20that%20regresses%20a%20control%20network%20to%20a%20backward%20adjoint%20signal%0Awhile%20preserving%20base-model%20capabilities.%20The%20same%20formulation%20unifies%20prior%0Aattention%20heuristics%2C%20extends%20to%20diffusion%20models%20via%20a%20flow-diffusion%0Acorrespondence%2C%20and%20provides%20the%20first%20fine-tuning%20route%20explicitly%20designed%0Afor%20multi-subject%20fidelity.%20Empirically%2C%20on%20Stable%20Diffusion%203.5%2C%20FLUX%2C%20and%0AStable%20Diffusion%20XL%2C%20both%20algorithms%20consistently%20improve%20multi-subject%0Aalignment%20while%20maintaining%20base-model%20style.%20Test-time%20control%20runs%0Aefficiently%20on%20commodity%20GPUs%2C%20and%20fine-tuned%20controllers%20trained%20on%20limited%0Aprompts%20generalize%20to%20unseen%20ones.%20We%20further%20highlight%20FOCUS%20%28Flow%20Optimal%0AControl%20for%20Unentangled%20Subjects%29%2C%20which%20achieves%20state-of-the-art%0Amulti-subject%20fidelity%20across%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02315v1&entry.124074799=Read"},
{"title": "Measurement-Guided Consistency Model Sampling for Inverse Problems", "author": "Amirreza Tanevardi and Pooria Abbas Rad Moghadam and Sajjad Amini", "abstract": "  Diffusion models have become powerful generative priors for solving inverse\nimaging problems, but their reliance on slow multi-step sampling limits\npractical deployment. Consistency models address this bottleneck by enabling\nhigh-quality generation in a single or only a few steps, yet their direct\nadaptation to inverse problems is underexplored. In this paper, we present a\nmodified consistency sampling approach tailored for inverse problem\nreconstruction: the sampler's stochasticity is guided by a\nmeasurement-consistency mechanism tied to the measurement operator, which\nenforces fidelity to the acquired measurements while retaining the efficiency\nof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom\ndatasets demonstrate consistent improvements in perceptual and pixel-level\nmetrics, including Fr\\'echet Inception Distance, Kernel Inception Distance,\npeak signal-to-noise ratio, and structural similarity index measure, compared\nto baseline consistency sampling, yielding competitive or superior\nreconstructions with only a handful of steps.\n", "link": "http://arxiv.org/abs/2510.02208v1", "date": "2025-10-02", "relevancy": 1.6316, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5582}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5448}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measurement-Guided%20Consistency%20Model%20Sampling%20for%20Inverse%20Problems&body=Title%3A%20Measurement-Guided%20Consistency%20Model%20Sampling%20for%20Inverse%20Problems%0AAuthor%3A%20Amirreza%20Tanevardi%20and%20Pooria%20Abbas%20Rad%20Moghadam%20and%20Sajjad%20Amini%0AAbstract%3A%20%20%20Diffusion%20models%20have%20become%20powerful%20generative%20priors%20for%20solving%20inverse%0Aimaging%20problems%2C%20but%20their%20reliance%20on%20slow%20multi-step%20sampling%20limits%0Apractical%20deployment.%20Consistency%20models%20address%20this%20bottleneck%20by%20enabling%0Ahigh-quality%20generation%20in%20a%20single%20or%20only%20a%20few%20steps%2C%20yet%20their%20direct%0Aadaptation%20to%20inverse%20problems%20is%20underexplored.%20In%20this%20paper%2C%20we%20present%20a%0Amodified%20consistency%20sampling%20approach%20tailored%20for%20inverse%20problem%0Areconstruction%3A%20the%20sampler%27s%20stochasticity%20is%20guided%20by%20a%0Ameasurement-consistency%20mechanism%20tied%20to%20the%20measurement%20operator%2C%20which%0Aenforces%20fidelity%20to%20the%20acquired%20measurements%20while%20retaining%20the%20efficiency%0Aof%20consistency-based%20generation.%20Experiments%20on%20Fashion-MNIST%20and%20LSUN%20Bedroom%0Adatasets%20demonstrate%20consistent%20improvements%20in%20perceptual%20and%20pixel-level%0Ametrics%2C%20including%20Fr%5C%27echet%20Inception%20Distance%2C%20Kernel%20Inception%20Distance%2C%0Apeak%20signal-to-noise%20ratio%2C%20and%20structural%20similarity%20index%20measure%2C%20compared%0Ato%20baseline%20consistency%20sampling%2C%20yielding%20competitive%20or%20superior%0Areconstructions%20with%20only%20a%20handful%20of%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasurement-Guided%2520Consistency%2520Model%2520Sampling%2520for%2520Inverse%2520Problems%26entry.906535625%3DAmirreza%2520Tanevardi%2520and%2520Pooria%2520Abbas%2520Rad%2520Moghadam%2520and%2520Sajjad%2520Amini%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520become%2520powerful%2520generative%2520priors%2520for%2520solving%2520inverse%250Aimaging%2520problems%252C%2520but%2520their%2520reliance%2520on%2520slow%2520multi-step%2520sampling%2520limits%250Apractical%2520deployment.%2520Consistency%2520models%2520address%2520this%2520bottleneck%2520by%2520enabling%250Ahigh-quality%2520generation%2520in%2520a%2520single%2520or%2520only%2520a%2520few%2520steps%252C%2520yet%2520their%2520direct%250Aadaptation%2520to%2520inverse%2520problems%2520is%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Amodified%2520consistency%2520sampling%2520approach%2520tailored%2520for%2520inverse%2520problem%250Areconstruction%253A%2520the%2520sampler%2527s%2520stochasticity%2520is%2520guided%2520by%2520a%250Ameasurement-consistency%2520mechanism%2520tied%2520to%2520the%2520measurement%2520operator%252C%2520which%250Aenforces%2520fidelity%2520to%2520the%2520acquired%2520measurements%2520while%2520retaining%2520the%2520efficiency%250Aof%2520consistency-based%2520generation.%2520Experiments%2520on%2520Fashion-MNIST%2520and%2520LSUN%2520Bedroom%250Adatasets%2520demonstrate%2520consistent%2520improvements%2520in%2520perceptual%2520and%2520pixel-level%250Ametrics%252C%2520including%2520Fr%255C%2527echet%2520Inception%2520Distance%252C%2520Kernel%2520Inception%2520Distance%252C%250Apeak%2520signal-to-noise%2520ratio%252C%2520and%2520structural%2520similarity%2520index%2520measure%252C%2520compared%250Ato%2520baseline%2520consistency%2520sampling%252C%2520yielding%2520competitive%2520or%2520superior%250Areconstructions%2520with%2520only%2520a%2520handful%2520of%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measurement-Guided%20Consistency%20Model%20Sampling%20for%20Inverse%20Problems&entry.906535625=Amirreza%20Tanevardi%20and%20Pooria%20Abbas%20Rad%20Moghadam%20and%20Sajjad%20Amini&entry.1292438233=%20%20Diffusion%20models%20have%20become%20powerful%20generative%20priors%20for%20solving%20inverse%0Aimaging%20problems%2C%20but%20their%20reliance%20on%20slow%20multi-step%20sampling%20limits%0Apractical%20deployment.%20Consistency%20models%20address%20this%20bottleneck%20by%20enabling%0Ahigh-quality%20generation%20in%20a%20single%20or%20only%20a%20few%20steps%2C%20yet%20their%20direct%0Aadaptation%20to%20inverse%20problems%20is%20underexplored.%20In%20this%20paper%2C%20we%20present%20a%0Amodified%20consistency%20sampling%20approach%20tailored%20for%20inverse%20problem%0Areconstruction%3A%20the%20sampler%27s%20stochasticity%20is%20guided%20by%20a%0Ameasurement-consistency%20mechanism%20tied%20to%20the%20measurement%20operator%2C%20which%0Aenforces%20fidelity%20to%20the%20acquired%20measurements%20while%20retaining%20the%20efficiency%0Aof%20consistency-based%20generation.%20Experiments%20on%20Fashion-MNIST%20and%20LSUN%20Bedroom%0Adatasets%20demonstrate%20consistent%20improvements%20in%20perceptual%20and%20pixel-level%0Ametrics%2C%20including%20Fr%5C%27echet%20Inception%20Distance%2C%20Kernel%20Inception%20Distance%2C%0Apeak%20signal-to-noise%20ratio%2C%20and%20structural%20similarity%20index%20measure%2C%20compared%0Ato%20baseline%20consistency%20sampling%2C%20yielding%20competitive%20or%20superior%0Areconstructions%20with%20only%20a%20handful%20of%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02208v1&entry.124074799=Read"},
{"title": "MMDEW: Multipurpose Multiclass Density Estimation in the Wild", "author": "Villanelle O'Reilly and Jonathan Cox and Georgios Leontidis and Marc Hanheide and Petra Bosilj and James Brown", "abstract": "  Density map estimation can be used to estimate object counts in dense and\noccluded scenes where discrete counting-by-detection methods fail. We propose a\nmulticategory counting framework that leverages a Twins pyramid\nvision-transformer backbone and a specialised multi-class counting head built\non a state-of-the-art multiscale decoding approach. A two-task design adds a\nsegmentation-based Category Focus Module, suppressing inter-category cross-talk\nat training time. Training and evaluation on the VisDrone and iSAID benchmarks\ndemonstrates superior performance versus prior multicategory crowd-counting\napproaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11\nunderscores the necessity of crowd counting methods in dense scenes. The\nmethod's regional loss opens up multi-class crowd counting to new domains,\ndemonstrated through the application to a biodiversity monitoring dataset,\nhighlighting its capacity to inform conservation efforts and enable scalable\necological insights.\n", "link": "http://arxiv.org/abs/2510.02213v1", "date": "2025-10-02", "relevancy": 1.6272, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMDEW%3A%20Multipurpose%20Multiclass%20Density%20Estimation%20in%20the%20Wild&body=Title%3A%20MMDEW%3A%20Multipurpose%20Multiclass%20Density%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Villanelle%20O%27Reilly%20and%20Jonathan%20Cox%20and%20Georgios%20Leontidis%20and%20Marc%20Hanheide%20and%20Petra%20Bosilj%20and%20James%20Brown%0AAbstract%3A%20%20%20Density%20map%20estimation%20can%20be%20used%20to%20estimate%20object%20counts%20in%20dense%20and%0Aoccluded%20scenes%20where%20discrete%20counting-by-detection%20methods%20fail.%20We%20propose%20a%0Amulticategory%20counting%20framework%20that%20leverages%20a%20Twins%20pyramid%0Avision-transformer%20backbone%20and%20a%20specialised%20multi-class%20counting%20head%20built%0Aon%20a%20state-of-the-art%20multiscale%20decoding%20approach.%20A%20two-task%20design%20adds%20a%0Asegmentation-based%20Category%20Focus%20Module%2C%20suppressing%20inter-category%20cross-talk%0Aat%20training%20time.%20Training%20and%20evaluation%20on%20the%20VisDrone%20and%20iSAID%20benchmarks%0Ademonstrates%20superior%20performance%20versus%20prior%20multicategory%20crowd-counting%0Aapproaches%20%2833%25%2C%2043%25%20and%2064%25%20reduction%20to%20MAE%29%2C%20and%20the%20comparison%20with%20YOLOv11%0Aunderscores%20the%20necessity%20of%20crowd%20counting%20methods%20in%20dense%20scenes.%20The%0Amethod%27s%20regional%20loss%20opens%20up%20multi-class%20crowd%20counting%20to%20new%20domains%2C%0Ademonstrated%20through%20the%20application%20to%20a%20biodiversity%20monitoring%20dataset%2C%0Ahighlighting%20its%20capacity%20to%20inform%20conservation%20efforts%20and%20enable%20scalable%0Aecological%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMDEW%253A%2520Multipurpose%2520Multiclass%2520Density%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DVillanelle%2520O%2527Reilly%2520and%2520Jonathan%2520Cox%2520and%2520Georgios%2520Leontidis%2520and%2520Marc%2520Hanheide%2520and%2520Petra%2520Bosilj%2520and%2520James%2520Brown%26entry.1292438233%3D%2520%2520Density%2520map%2520estimation%2520can%2520be%2520used%2520to%2520estimate%2520object%2520counts%2520in%2520dense%2520and%250Aoccluded%2520scenes%2520where%2520discrete%2520counting-by-detection%2520methods%2520fail.%2520We%2520propose%2520a%250Amulticategory%2520counting%2520framework%2520that%2520leverages%2520a%2520Twins%2520pyramid%250Avision-transformer%2520backbone%2520and%2520a%2520specialised%2520multi-class%2520counting%2520head%2520built%250Aon%2520a%2520state-of-the-art%2520multiscale%2520decoding%2520approach.%2520A%2520two-task%2520design%2520adds%2520a%250Asegmentation-based%2520Category%2520Focus%2520Module%252C%2520suppressing%2520inter-category%2520cross-talk%250Aat%2520training%2520time.%2520Training%2520and%2520evaluation%2520on%2520the%2520VisDrone%2520and%2520iSAID%2520benchmarks%250Ademonstrates%2520superior%2520performance%2520versus%2520prior%2520multicategory%2520crowd-counting%250Aapproaches%2520%252833%2525%252C%252043%2525%2520and%252064%2525%2520reduction%2520to%2520MAE%2529%252C%2520and%2520the%2520comparison%2520with%2520YOLOv11%250Aunderscores%2520the%2520necessity%2520of%2520crowd%2520counting%2520methods%2520in%2520dense%2520scenes.%2520The%250Amethod%2527s%2520regional%2520loss%2520opens%2520up%2520multi-class%2520crowd%2520counting%2520to%2520new%2520domains%252C%250Ademonstrated%2520through%2520the%2520application%2520to%2520a%2520biodiversity%2520monitoring%2520dataset%252C%250Ahighlighting%2520its%2520capacity%2520to%2520inform%2520conservation%2520efforts%2520and%2520enable%2520scalable%250Aecological%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMDEW%3A%20Multipurpose%20Multiclass%20Density%20Estimation%20in%20the%20Wild&entry.906535625=Villanelle%20O%27Reilly%20and%20Jonathan%20Cox%20and%20Georgios%20Leontidis%20and%20Marc%20Hanheide%20and%20Petra%20Bosilj%20and%20James%20Brown&entry.1292438233=%20%20Density%20map%20estimation%20can%20be%20used%20to%20estimate%20object%20counts%20in%20dense%20and%0Aoccluded%20scenes%20where%20discrete%20counting-by-detection%20methods%20fail.%20We%20propose%20a%0Amulticategory%20counting%20framework%20that%20leverages%20a%20Twins%20pyramid%0Avision-transformer%20backbone%20and%20a%20specialised%20multi-class%20counting%20head%20built%0Aon%20a%20state-of-the-art%20multiscale%20decoding%20approach.%20A%20two-task%20design%20adds%20a%0Asegmentation-based%20Category%20Focus%20Module%2C%20suppressing%20inter-category%20cross-talk%0Aat%20training%20time.%20Training%20and%20evaluation%20on%20the%20VisDrone%20and%20iSAID%20benchmarks%0Ademonstrates%20superior%20performance%20versus%20prior%20multicategory%20crowd-counting%0Aapproaches%20%2833%25%2C%2043%25%20and%2064%25%20reduction%20to%20MAE%29%2C%20and%20the%20comparison%20with%20YOLOv11%0Aunderscores%20the%20necessity%20of%20crowd%20counting%20methods%20in%20dense%20scenes.%20The%0Amethod%27s%20regional%20loss%20opens%20up%20multi-class%20crowd%20counting%20to%20new%20domains%2C%0Ademonstrated%20through%20the%20application%20to%20a%20biodiversity%20monitoring%20dataset%2C%0Ahighlighting%20its%20capacity%20to%20inform%20conservation%20efforts%20and%20enable%20scalable%0Aecological%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02213v1&entry.124074799=Read"},
{"title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with\n  Camera Conditioning", "author": "Tianchong Jiang and Jingtian Ji and Xiangshan Tan and Jiading Fang and Anand Bhattad and Vitor Guizilini and Matthew R. Walter", "abstract": "  We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .\n", "link": "http://arxiv.org/abs/2510.02268v1", "date": "2025-10-02", "relevancy": 1.6262, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5351}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20You%20Know%20Where%20Your%20Camera%20Is%3F%20View-Invariant%20Policy%20Learning%20with%0A%20%20Camera%20Conditioning&body=Title%3A%20Do%20You%20Know%20Where%20Your%20Camera%20Is%3F%20View-Invariant%20Policy%20Learning%20with%0A%20%20Camera%20Conditioning%0AAuthor%3A%20Tianchong%20Jiang%20and%20Jingtian%20Ji%20and%20Xiangshan%20Tan%20and%20Jiading%20Fang%20and%20Anand%20Bhattad%20and%20Vitor%20Guizilini%20and%20Matthew%20R.%20Walter%0AAbstract%3A%20%20%20We%20study%20view-invariant%20imitation%20learning%20by%20explicitly%20conditioning%0Apolicies%20on%20camera%20extrinsics.%20Using%20Plucker%20embeddings%20of%20per-pixel%20rays%2C%20we%0Ashow%20that%20conditioning%20on%20extrinsics%20significantly%20improves%20generalization%0Aacross%20viewpoints%20for%20standard%20behavior%20cloning%20policies%2C%20including%20ACT%2C%0ADiffusion%20Policy%2C%20and%20SmolVLA.%20To%20evaluate%20policy%20robustness%20under%20realistic%0Aviewpoint%20shifts%2C%20we%20introduce%20six%20manipulation%20tasks%20in%20RoboSuite%20and%0AManiSkill%20that%20pair%20%22fixed%22%20and%20%22randomized%22%20scene%20variants%2C%20decoupling%0Abackground%20cues%20from%20camera%20pose.%20Our%20analysis%20reveals%20that%20policies%20without%0Aextrinsics%20often%20infer%20camera%20pose%20using%20visual%20cues%20from%20static%20backgrounds%20in%0Afixed%20scenes%3B%20this%20shortcut%20collapses%20when%20workspace%20geometry%20or%20camera%0Aplacement%20shifts.%20Conditioning%20on%20extrinsics%20restores%20performance%20and%20yields%0Arobust%20RGB-only%20control%20without%20depth.%20We%20release%20the%20tasks%2C%20demonstrations%2C%0Aand%20code%20at%20https%3A//ripl.github.io/know_your_camera/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520You%2520Know%2520Where%2520Your%2520Camera%2520Is%253F%2520View-Invariant%2520Policy%2520Learning%2520with%250A%2520%2520Camera%2520Conditioning%26entry.906535625%3DTianchong%2520Jiang%2520and%2520Jingtian%2520Ji%2520and%2520Xiangshan%2520Tan%2520and%2520Jiading%2520Fang%2520and%2520Anand%2520Bhattad%2520and%2520Vitor%2520Guizilini%2520and%2520Matthew%2520R.%2520Walter%26entry.1292438233%3D%2520%2520We%2520study%2520view-invariant%2520imitation%2520learning%2520by%2520explicitly%2520conditioning%250Apolicies%2520on%2520camera%2520extrinsics.%2520Using%2520Plucker%2520embeddings%2520of%2520per-pixel%2520rays%252C%2520we%250Ashow%2520that%2520conditioning%2520on%2520extrinsics%2520significantly%2520improves%2520generalization%250Aacross%2520viewpoints%2520for%2520standard%2520behavior%2520cloning%2520policies%252C%2520including%2520ACT%252C%250ADiffusion%2520Policy%252C%2520and%2520SmolVLA.%2520To%2520evaluate%2520policy%2520robustness%2520under%2520realistic%250Aviewpoint%2520shifts%252C%2520we%2520introduce%2520six%2520manipulation%2520tasks%2520in%2520RoboSuite%2520and%250AManiSkill%2520that%2520pair%2520%2522fixed%2522%2520and%2520%2522randomized%2522%2520scene%2520variants%252C%2520decoupling%250Abackground%2520cues%2520from%2520camera%2520pose.%2520Our%2520analysis%2520reveals%2520that%2520policies%2520without%250Aextrinsics%2520often%2520infer%2520camera%2520pose%2520using%2520visual%2520cues%2520from%2520static%2520backgrounds%2520in%250Afixed%2520scenes%253B%2520this%2520shortcut%2520collapses%2520when%2520workspace%2520geometry%2520or%2520camera%250Aplacement%2520shifts.%2520Conditioning%2520on%2520extrinsics%2520restores%2520performance%2520and%2520yields%250Arobust%2520RGB-only%2520control%2520without%2520depth.%2520We%2520release%2520the%2520tasks%252C%2520demonstrations%252C%250Aand%2520code%2520at%2520https%253A//ripl.github.io/know_your_camera/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20You%20Know%20Where%20Your%20Camera%20Is%3F%20View-Invariant%20Policy%20Learning%20with%0A%20%20Camera%20Conditioning&entry.906535625=Tianchong%20Jiang%20and%20Jingtian%20Ji%20and%20Xiangshan%20Tan%20and%20Jiading%20Fang%20and%20Anand%20Bhattad%20and%20Vitor%20Guizilini%20and%20Matthew%20R.%20Walter&entry.1292438233=%20%20We%20study%20view-invariant%20imitation%20learning%20by%20explicitly%20conditioning%0Apolicies%20on%20camera%20extrinsics.%20Using%20Plucker%20embeddings%20of%20per-pixel%20rays%2C%20we%0Ashow%20that%20conditioning%20on%20extrinsics%20significantly%20improves%20generalization%0Aacross%20viewpoints%20for%20standard%20behavior%20cloning%20policies%2C%20including%20ACT%2C%0ADiffusion%20Policy%2C%20and%20SmolVLA.%20To%20evaluate%20policy%20robustness%20under%20realistic%0Aviewpoint%20shifts%2C%20we%20introduce%20six%20manipulation%20tasks%20in%20RoboSuite%20and%0AManiSkill%20that%20pair%20%22fixed%22%20and%20%22randomized%22%20scene%20variants%2C%20decoupling%0Abackground%20cues%20from%20camera%20pose.%20Our%20analysis%20reveals%20that%20policies%20without%0Aextrinsics%20often%20infer%20camera%20pose%20using%20visual%20cues%20from%20static%20backgrounds%20in%0Afixed%20scenes%3B%20this%20shortcut%20collapses%20when%20workspace%20geometry%20or%20camera%0Aplacement%20shifts.%20Conditioning%20on%20extrinsics%20restores%20performance%20and%20yields%0Arobust%20RGB-only%20control%20without%20depth.%20We%20release%20the%20tasks%2C%20demonstrations%2C%0Aand%20code%20at%20https%3A//ripl.github.io/know_your_camera/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02268v1&entry.124074799=Read"},
{"title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning", "author": "Hanyang Zhao and Dawen Liang and Wenpin Tang and David Yao and Nathan Kallus", "abstract": "  We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.\n", "link": "http://arxiv.org/abs/2510.02212v1", "date": "2025-10-02", "relevancy": 1.594, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5715}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5271}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiFFPO%3A%20Training%20Diffusion%20LLMs%20to%20Reason%20Fast%20and%20Furious%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20DiFFPO%3A%20Training%20Diffusion%20LLMs%20to%20Reason%20Fast%20and%20Furious%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Hanyang%20Zhao%20and%20Dawen%20Liang%20and%20Wenpin%20Tang%20and%20David%20Yao%20and%20Nathan%20Kallus%0AAbstract%3A%20%20%20We%20propose%20DiFFPO%2C%20Diffusion%20Fast%20and%20Furious%20Policy%20Optimization%2C%20a%20unified%0Aframework%20for%20training%20masked%20diffusion%20large%20language%20models%20%28dLLMs%29%20to%20reason%0Anot%20only%20better%20%28furious%29%2C%20but%20also%20faster%20via%20reinforcement%20learning%20%28RL%29.%20We%0Afirst%20unify%20the%20existing%20baseline%20approach%20such%20as%20d1%20by%20proposing%20to%20train%0Asurrogate%20policies%20via%20off-policy%20RL%2C%20whose%20likelihood%20is%20much%20more%20tractable%0Aas%20an%20approximation%20to%20the%20true%20dLLM%20policy.%20This%20naturally%20motivates%20a%20more%0Aaccurate%20and%20informative%20two-stage%20likelihood%20approximation%20combined%20with%0Aimportance%20sampling%20correction%2C%20which%20leads%20to%20generalized%20RL%20algorithms%20with%0Abetter%20sample%20efficiency%20and%20superior%20task%20performance.%20Second%2C%20we%20propose%20a%0Anew%20direction%20of%20joint%20training%20efficient%20samplers/controllers%20of%20dLLMs%20policy.%0AVia%20RL%2C%20we%20incentivize%20dLLMs%27%20natural%20multi-token%20prediction%20capabilities%20by%0Aletting%20the%20model%20learn%20to%20adaptively%20allocate%20an%20inference%20threshold%20for%20each%0Aprompt.%20By%20jointly%20training%20the%20sampler%2C%20we%20yield%20better%20accuracies%20with%20lower%0Anumber%20of%20function%20evaluations%20%28NFEs%29%20compared%20to%20training%20the%20model%20only%2C%0Aobtaining%20the%20best%20performance%20in%20improving%20the%20Pareto%20frontier%20of%20the%0Ainference-time%20compute%20of%20dLLMs.%20We%20showcase%20the%20effectiveness%20of%20our%20pipeline%0Aby%20training%20open%20source%20large%20diffusion%20language%20models%20over%20benchmark%20math%20and%0Aplanning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiFFPO%253A%2520Training%2520Diffusion%2520LLMs%2520to%2520Reason%2520Fast%2520and%2520Furious%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DHanyang%2520Zhao%2520and%2520Dawen%2520Liang%2520and%2520Wenpin%2520Tang%2520and%2520David%2520Yao%2520and%2520Nathan%2520Kallus%26entry.1292438233%3D%2520%2520We%2520propose%2520DiFFPO%252C%2520Diffusion%2520Fast%2520and%2520Furious%2520Policy%2520Optimization%252C%2520a%2520unified%250Aframework%2520for%2520training%2520masked%2520diffusion%2520large%2520language%2520models%2520%2528dLLMs%2529%2520to%2520reason%250Anot%2520only%2520better%2520%2528furious%2529%252C%2520but%2520also%2520faster%2520via%2520reinforcement%2520learning%2520%2528RL%2529.%2520We%250Afirst%2520unify%2520the%2520existing%2520baseline%2520approach%2520such%2520as%2520d1%2520by%2520proposing%2520to%2520train%250Asurrogate%2520policies%2520via%2520off-policy%2520RL%252C%2520whose%2520likelihood%2520is%2520much%2520more%2520tractable%250Aas%2520an%2520approximation%2520to%2520the%2520true%2520dLLM%2520policy.%2520This%2520naturally%2520motivates%2520a%2520more%250Aaccurate%2520and%2520informative%2520two-stage%2520likelihood%2520approximation%2520combined%2520with%250Aimportance%2520sampling%2520correction%252C%2520which%2520leads%2520to%2520generalized%2520RL%2520algorithms%2520with%250Abetter%2520sample%2520efficiency%2520and%2520superior%2520task%2520performance.%2520Second%252C%2520we%2520propose%2520a%250Anew%2520direction%2520of%2520joint%2520training%2520efficient%2520samplers/controllers%2520of%2520dLLMs%2520policy.%250AVia%2520RL%252C%2520we%2520incentivize%2520dLLMs%2527%2520natural%2520multi-token%2520prediction%2520capabilities%2520by%250Aletting%2520the%2520model%2520learn%2520to%2520adaptively%2520allocate%2520an%2520inference%2520threshold%2520for%2520each%250Aprompt.%2520By%2520jointly%2520training%2520the%2520sampler%252C%2520we%2520yield%2520better%2520accuracies%2520with%2520lower%250Anumber%2520of%2520function%2520evaluations%2520%2528NFEs%2529%2520compared%2520to%2520training%2520the%2520model%2520only%252C%250Aobtaining%2520the%2520best%2520performance%2520in%2520improving%2520the%2520Pareto%2520frontier%2520of%2520the%250Ainference-time%2520compute%2520of%2520dLLMs.%2520We%2520showcase%2520the%2520effectiveness%2520of%2520our%2520pipeline%250Aby%2520training%2520open%2520source%2520large%2520diffusion%2520language%2520models%2520over%2520benchmark%2520math%2520and%250Aplanning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiFFPO%3A%20Training%20Diffusion%20LLMs%20to%20Reason%20Fast%20and%20Furious%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Hanyang%20Zhao%20and%20Dawen%20Liang%20and%20Wenpin%20Tang%20and%20David%20Yao%20and%20Nathan%20Kallus&entry.1292438233=%20%20We%20propose%20DiFFPO%2C%20Diffusion%20Fast%20and%20Furious%20Policy%20Optimization%2C%20a%20unified%0Aframework%20for%20training%20masked%20diffusion%20large%20language%20models%20%28dLLMs%29%20to%20reason%0Anot%20only%20better%20%28furious%29%2C%20but%20also%20faster%20via%20reinforcement%20learning%20%28RL%29.%20We%0Afirst%20unify%20the%20existing%20baseline%20approach%20such%20as%20d1%20by%20proposing%20to%20train%0Asurrogate%20policies%20via%20off-policy%20RL%2C%20whose%20likelihood%20is%20much%20more%20tractable%0Aas%20an%20approximation%20to%20the%20true%20dLLM%20policy.%20This%20naturally%20motivates%20a%20more%0Aaccurate%20and%20informative%20two-stage%20likelihood%20approximation%20combined%20with%0Aimportance%20sampling%20correction%2C%20which%20leads%20to%20generalized%20RL%20algorithms%20with%0Abetter%20sample%20efficiency%20and%20superior%20task%20performance.%20Second%2C%20we%20propose%20a%0Anew%20direction%20of%20joint%20training%20efficient%20samplers/controllers%20of%20dLLMs%20policy.%0AVia%20RL%2C%20we%20incentivize%20dLLMs%27%20natural%20multi-token%20prediction%20capabilities%20by%0Aletting%20the%20model%20learn%20to%20adaptively%20allocate%20an%20inference%20threshold%20for%20each%0Aprompt.%20By%20jointly%20training%20the%20sampler%2C%20we%20yield%20better%20accuracies%20with%20lower%0Anumber%20of%20function%20evaluations%20%28NFEs%29%20compared%20to%20training%20the%20model%20only%2C%0Aobtaining%20the%20best%20performance%20in%20improving%20the%20Pareto%20frontier%20of%20the%0Ainference-time%20compute%20of%20dLLMs.%20We%20showcase%20the%20effectiveness%20of%20our%20pipeline%0Aby%20training%20open%20source%20large%20diffusion%20language%20models%20over%20benchmark%20math%20and%0Aplanning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02212v1&entry.124074799=Read"},
{"title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration", "author": "Xiaoyang Yuan and Yujuan Ding and Yi Bin and Wenqi Shao and Jinyu Cai and Jingkuan Song and Yang Yang and Hengtao Shen", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.\n", "link": "http://arxiv.org/abs/2510.02227v1", "date": "2025-10-02", "relevancy": 1.57, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5295}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Than%20One%20Teacher%3A%20Adaptive%20Multi-Guidance%20Policy%20Optimization%20for%0A%20%20Diverse%20Exploration&body=Title%3A%20More%20Than%20One%20Teacher%3A%20Adaptive%20Multi-Guidance%20Policy%20Optimization%20for%0A%20%20Diverse%20Exploration%0AAuthor%3A%20Xiaoyang%20Yuan%20and%20Yujuan%20Ding%20and%20Yi%20Bin%20and%20Wenqi%20Shao%20and%20Jinyu%20Cai%20and%20Jingkuan%20Song%20and%20Yang%20Yang%20and%20Hengtao%20Shen%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20is%20a%20promising%20paradigm%0Afor%20enhancing%20the%20reasoning%20ability%20in%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%0Aprevailing%20methods%20primarily%20rely%20on%20self-exploration%20or%20a%20single%20off-policy%0Ateacher%20to%20elicit%20long%20chain-of-thought%20%28LongCoT%29%20reasoning%2C%20which%20may%0Aintroduce%20intrinsic%20model%20biases%20and%20restrict%20exploration%2C%20ultimately%20limiting%0Areasoning%20diversity%20and%20performance.%20Drawing%20inspiration%20from%20multi-teacher%0Astrategies%20in%20knowledge%20distillation%2C%20we%20introduce%20Adaptive%20Multi-Guidance%0APolicy%20Optimization%20%28AMPO%29%2C%20a%20novel%20framework%20that%20adaptively%20leverages%0Aguidance%20from%20multiple%20proficient%20teacher%20models%2C%20but%20only%20when%20the%20on-policy%0Amodel%20fails%20to%20generate%20correct%20solutions.%20This%20%22guidance-on-demand%22%20approach%0Aexpands%20exploration%20while%20preserving%20the%20value%20of%20self-discovery.%20Moreover%2C%0AAMPO%20incorporates%20a%20comprehension-based%20selection%20mechanism%2C%20prompting%20the%0Astudent%20to%20learn%20from%20the%20reasoning%20paths%20that%20it%20is%20most%20likely%20to%20comprehend%2C%0Athus%20balancing%20broad%20exploration%20with%20effective%20exploitation.%20Extensive%0Aexperiments%20show%20AMPO%20substantially%20outperforms%20a%20strong%20baseline%20%28GRPO%29%2C%20with%0Aa%204.3%25%20improvement%20on%20mathematical%20reasoning%20tasks%20and%2012.2%25%20on%0Aout-of-distribution%20tasks%2C%20while%20significantly%20boosting%20Pass%40k%20performance%20and%0Aenabling%20more%20diverse%20exploration.%20Notably%2C%20using%20four%20peer-sized%20teachers%2C%20our%0Amethod%20achieves%20comparable%20results%20to%20approaches%20that%20leverage%20a%20single%2C%20more%0Apowerful%20teacher%20%28e.g.%2C%20DeepSeek-R1%29%20with%20more%20data.%20These%20results%20demonstrate%0Aa%20more%20efficient%20and%20scalable%20path%20to%20superior%20reasoning%20and%20generalizability.%0AOur%20code%20is%20available%20at%20https%3A//github.com/SII-Enigma/AMPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Than%2520One%2520Teacher%253A%2520Adaptive%2520Multi-Guidance%2520Policy%2520Optimization%2520for%250A%2520%2520Diverse%2520Exploration%26entry.906535625%3DXiaoyang%2520Yuan%2520and%2520Yujuan%2520Ding%2520and%2520Yi%2520Bin%2520and%2520Wenqi%2520Shao%2520and%2520Jinyu%2520Cai%2520and%2520Jingkuan%2520Song%2520and%2520Yang%2520Yang%2520and%2520Hengtao%2520Shen%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520is%2520a%2520promising%2520paradigm%250Afor%2520enhancing%2520the%2520reasoning%2520ability%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%250Aprevailing%2520methods%2520primarily%2520rely%2520on%2520self-exploration%2520or%2520a%2520single%2520off-policy%250Ateacher%2520to%2520elicit%2520long%2520chain-of-thought%2520%2528LongCoT%2529%2520reasoning%252C%2520which%2520may%250Aintroduce%2520intrinsic%2520model%2520biases%2520and%2520restrict%2520exploration%252C%2520ultimately%2520limiting%250Areasoning%2520diversity%2520and%2520performance.%2520Drawing%2520inspiration%2520from%2520multi-teacher%250Astrategies%2520in%2520knowledge%2520distillation%252C%2520we%2520introduce%2520Adaptive%2520Multi-Guidance%250APolicy%2520Optimization%2520%2528AMPO%2529%252C%2520a%2520novel%2520framework%2520that%2520adaptively%2520leverages%250Aguidance%2520from%2520multiple%2520proficient%2520teacher%2520models%252C%2520but%2520only%2520when%2520the%2520on-policy%250Amodel%2520fails%2520to%2520generate%2520correct%2520solutions.%2520This%2520%2522guidance-on-demand%2522%2520approach%250Aexpands%2520exploration%2520while%2520preserving%2520the%2520value%2520of%2520self-discovery.%2520Moreover%252C%250AAMPO%2520incorporates%2520a%2520comprehension-based%2520selection%2520mechanism%252C%2520prompting%2520the%250Astudent%2520to%2520learn%2520from%2520the%2520reasoning%2520paths%2520that%2520it%2520is%2520most%2520likely%2520to%2520comprehend%252C%250Athus%2520balancing%2520broad%2520exploration%2520with%2520effective%2520exploitation.%2520Extensive%250Aexperiments%2520show%2520AMPO%2520substantially%2520outperforms%2520a%2520strong%2520baseline%2520%2528GRPO%2529%252C%2520with%250Aa%25204.3%2525%2520improvement%2520on%2520mathematical%2520reasoning%2520tasks%2520and%252012.2%2525%2520on%250Aout-of-distribution%2520tasks%252C%2520while%2520significantly%2520boosting%2520Pass%2540k%2520performance%2520and%250Aenabling%2520more%2520diverse%2520exploration.%2520Notably%252C%2520using%2520four%2520peer-sized%2520teachers%252C%2520our%250Amethod%2520achieves%2520comparable%2520results%2520to%2520approaches%2520that%2520leverage%2520a%2520single%252C%2520more%250Apowerful%2520teacher%2520%2528e.g.%252C%2520DeepSeek-R1%2529%2520with%2520more%2520data.%2520These%2520results%2520demonstrate%250Aa%2520more%2520efficient%2520and%2520scalable%2520path%2520to%2520superior%2520reasoning%2520and%2520generalizability.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/SII-Enigma/AMPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Than%20One%20Teacher%3A%20Adaptive%20Multi-Guidance%20Policy%20Optimization%20for%0A%20%20Diverse%20Exploration&entry.906535625=Xiaoyang%20Yuan%20and%20Yujuan%20Ding%20and%20Yi%20Bin%20and%20Wenqi%20Shao%20and%20Jinyu%20Cai%20and%20Jingkuan%20Song%20and%20Yang%20Yang%20and%20Hengtao%20Shen&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20is%20a%20promising%20paradigm%0Afor%20enhancing%20the%20reasoning%20ability%20in%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%0Aprevailing%20methods%20primarily%20rely%20on%20self-exploration%20or%20a%20single%20off-policy%0Ateacher%20to%20elicit%20long%20chain-of-thought%20%28LongCoT%29%20reasoning%2C%20which%20may%0Aintroduce%20intrinsic%20model%20biases%20and%20restrict%20exploration%2C%20ultimately%20limiting%0Areasoning%20diversity%20and%20performance.%20Drawing%20inspiration%20from%20multi-teacher%0Astrategies%20in%20knowledge%20distillation%2C%20we%20introduce%20Adaptive%20Multi-Guidance%0APolicy%20Optimization%20%28AMPO%29%2C%20a%20novel%20framework%20that%20adaptively%20leverages%0Aguidance%20from%20multiple%20proficient%20teacher%20models%2C%20but%20only%20when%20the%20on-policy%0Amodel%20fails%20to%20generate%20correct%20solutions.%20This%20%22guidance-on-demand%22%20approach%0Aexpands%20exploration%20while%20preserving%20the%20value%20of%20self-discovery.%20Moreover%2C%0AAMPO%20incorporates%20a%20comprehension-based%20selection%20mechanism%2C%20prompting%20the%0Astudent%20to%20learn%20from%20the%20reasoning%20paths%20that%20it%20is%20most%20likely%20to%20comprehend%2C%0Athus%20balancing%20broad%20exploration%20with%20effective%20exploitation.%20Extensive%0Aexperiments%20show%20AMPO%20substantially%20outperforms%20a%20strong%20baseline%20%28GRPO%29%2C%20with%0Aa%204.3%25%20improvement%20on%20mathematical%20reasoning%20tasks%20and%2012.2%25%20on%0Aout-of-distribution%20tasks%2C%20while%20significantly%20boosting%20Pass%40k%20performance%20and%0Aenabling%20more%20diverse%20exploration.%20Notably%2C%20using%20four%20peer-sized%20teachers%2C%20our%0Amethod%20achieves%20comparable%20results%20to%20approaches%20that%20leverage%20a%20single%2C%20more%0Apowerful%20teacher%20%28e.g.%2C%20DeepSeek-R1%29%20with%20more%20data.%20These%20results%20demonstrate%0Aa%20more%20efficient%20and%20scalable%20path%20to%20superior%20reasoning%20and%20generalizability.%0AOur%20code%20is%20available%20at%20https%3A//github.com/SII-Enigma/AMPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02227v1&entry.124074799=Read"},
{"title": "Probabilistic Reasoning with LLMs for k-anonymity Estimation", "author": "Jonathan Zheng and Sauvik Das and Alan Ritter and Wei Xu", "abstract": "  Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average.\n", "link": "http://arxiv.org/abs/2503.09674v4", "date": "2025-10-02", "relevancy": 1.5615, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.53}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Reasoning%20with%20LLMs%20for%20k-anonymity%20Estimation&body=Title%3A%20Probabilistic%20Reasoning%20with%20LLMs%20for%20k-anonymity%20Estimation%0AAuthor%3A%20Jonathan%20Zheng%20and%20Sauvik%20Das%20and%20Alan%20Ritter%20and%20Wei%20Xu%0AAbstract%3A%20%20%20Probabilistic%20reasoning%20is%20a%20key%20aspect%20of%20both%20human%20and%20artificial%0Aintelligence%20that%20allows%20for%20handling%20uncertainty%20and%20ambiguity%20in%0Adecision-making.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20numerical%20reasoning%20task%0Aunder%20uncertainty%20for%20large%20language%20models%2C%20focusing%20on%20estimating%20the%20privacy%0Arisk%20of%20user-generated%20documents%20containing%20privacy-sensitive%20information.%20We%0Apropose%20BRANCH%2C%20a%20new%20LLM%20methodology%20that%20estimates%20the%20k-privacy%20value%20of%20a%0Atext-the%20size%20of%20the%20population%20matching%20the%20given%20information.%20BRANCH%0Afactorizes%20a%20joint%20probability%20distribution%20of%20personal%20information%20as%20random%0Avariables.%20The%20probability%20of%20each%20factor%20in%20a%20population%20is%20estimated%0Aseparately%20using%20a%20Bayesian%20network%20and%20combined%20to%20compute%20the%20final%20k-value.%0AOur%20experiments%20show%20that%20this%20method%20successfully%20estimates%20the%20k-value%2073%25%20of%0Athe%20time%2C%20a%2013%25%20increase%20compared%20to%20o3-mini%20with%20chain-of-thought%20reasoning.%0AWe%20also%20find%20that%20LLM%20uncertainty%20is%20a%20good%20indicator%20for%20accuracy%2C%20as%0Ahigh-variance%20predictions%20are%2037.47%25%20less%20accurate%20on%20average.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09674v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Reasoning%2520with%2520LLMs%2520for%2520k-anonymity%2520Estimation%26entry.906535625%3DJonathan%2520Zheng%2520and%2520Sauvik%2520Das%2520and%2520Alan%2520Ritter%2520and%2520Wei%2520Xu%26entry.1292438233%3D%2520%2520Probabilistic%2520reasoning%2520is%2520a%2520key%2520aspect%2520of%2520both%2520human%2520and%2520artificial%250Aintelligence%2520that%2520allows%2520for%2520handling%2520uncertainty%2520and%2520ambiguity%2520in%250Adecision-making.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520numerical%2520reasoning%2520task%250Aunder%2520uncertainty%2520for%2520large%2520language%2520models%252C%2520focusing%2520on%2520estimating%2520the%2520privacy%250Arisk%2520of%2520user-generated%2520documents%2520containing%2520privacy-sensitive%2520information.%2520We%250Apropose%2520BRANCH%252C%2520a%2520new%2520LLM%2520methodology%2520that%2520estimates%2520the%2520k-privacy%2520value%2520of%2520a%250Atext-the%2520size%2520of%2520the%2520population%2520matching%2520the%2520given%2520information.%2520BRANCH%250Afactorizes%2520a%2520joint%2520probability%2520distribution%2520of%2520personal%2520information%2520as%2520random%250Avariables.%2520The%2520probability%2520of%2520each%2520factor%2520in%2520a%2520population%2520is%2520estimated%250Aseparately%2520using%2520a%2520Bayesian%2520network%2520and%2520combined%2520to%2520compute%2520the%2520final%2520k-value.%250AOur%2520experiments%2520show%2520that%2520this%2520method%2520successfully%2520estimates%2520the%2520k-value%252073%2525%2520of%250Athe%2520time%252C%2520a%252013%2525%2520increase%2520compared%2520to%2520o3-mini%2520with%2520chain-of-thought%2520reasoning.%250AWe%2520also%2520find%2520that%2520LLM%2520uncertainty%2520is%2520a%2520good%2520indicator%2520for%2520accuracy%252C%2520as%250Ahigh-variance%2520predictions%2520are%252037.47%2525%2520less%2520accurate%2520on%2520average.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09674v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Reasoning%20with%20LLMs%20for%20k-anonymity%20Estimation&entry.906535625=Jonathan%20Zheng%20and%20Sauvik%20Das%20and%20Alan%20Ritter%20and%20Wei%20Xu&entry.1292438233=%20%20Probabilistic%20reasoning%20is%20a%20key%20aspect%20of%20both%20human%20and%20artificial%0Aintelligence%20that%20allows%20for%20handling%20uncertainty%20and%20ambiguity%20in%0Adecision-making.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20numerical%20reasoning%20task%0Aunder%20uncertainty%20for%20large%20language%20models%2C%20focusing%20on%20estimating%20the%20privacy%0Arisk%20of%20user-generated%20documents%20containing%20privacy-sensitive%20information.%20We%0Apropose%20BRANCH%2C%20a%20new%20LLM%20methodology%20that%20estimates%20the%20k-privacy%20value%20of%20a%0Atext-the%20size%20of%20the%20population%20matching%20the%20given%20information.%20BRANCH%0Afactorizes%20a%20joint%20probability%20distribution%20of%20personal%20information%20as%20random%0Avariables.%20The%20probability%20of%20each%20factor%20in%20a%20population%20is%20estimated%0Aseparately%20using%20a%20Bayesian%20network%20and%20combined%20to%20compute%20the%20final%20k-value.%0AOur%20experiments%20show%20that%20this%20method%20successfully%20estimates%20the%20k-value%2073%25%20of%0Athe%20time%2C%20a%2013%25%20increase%20compared%20to%20o3-mini%20with%20chain-of-thought%20reasoning.%0AWe%20also%20find%20that%20LLM%20uncertainty%20is%20a%20good%20indicator%20for%20accuracy%2C%20as%0Ahigh-variance%20predictions%20are%2037.47%25%20less%20accurate%20on%20average.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09674v4&entry.124074799=Read"},
{"title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is\n  Geometry Adaptive", "author": "Tyler Farghly and Peter Potaptchik and Samuel Howard and George Deligiannidis and Jakiw Pidstrigach", "abstract": "  Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.\n", "link": "http://arxiv.org/abs/2510.02305v1", "date": "2025-10-02", "relevancy": 1.5551, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5461}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5134}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Models%20and%20the%20Manifold%20Hypothesis%3A%20Log-Domain%20Smoothing%20is%0A%20%20Geometry%20Adaptive&body=Title%3A%20Diffusion%20Models%20and%20the%20Manifold%20Hypothesis%3A%20Log-Domain%20Smoothing%20is%0A%20%20Geometry%20Adaptive%0AAuthor%3A%20Tyler%20Farghly%20and%20Peter%20Potaptchik%20and%20Samuel%20Howard%20and%20George%20Deligiannidis%20and%20Jakiw%20Pidstrigach%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20state-of-the-art%20performance%2C%20demonstrating%0Aremarkable%20generalisation%20capabilities%20across%20diverse%20domains.%20However%2C%20the%0Amechanisms%20underpinning%20these%20strong%20capabilities%20remain%20only%20partially%0Aunderstood.%20A%20leading%20conjecture%2C%20based%20on%20the%20manifold%20hypothesis%2C%20attributes%0Athis%20success%20to%20their%20ability%20to%20adapt%20to%20low-dimensional%20geometric%20structure%0Awithin%20the%20data.%20This%20work%20provides%20evidence%20for%20this%20conjecture%2C%20focusing%20on%0Ahow%20such%20phenomena%20could%20result%20from%20the%20formulation%20of%20the%20learning%20problem%0Athrough%20score%20matching.%20We%20inspect%20the%20role%20of%20implicit%20regularisation%20by%0Ainvestigating%20the%20effect%20of%20smoothing%20minimisers%20of%20the%20empirical%20score%0Amatching%20objective.%20Our%20theoretical%20and%20empirical%20results%20confirm%20that%0Asmoothing%20the%20score%20function%20--%20or%20equivalently%2C%20smoothing%20in%20the%20log-density%0Adomain%20--%20produces%20smoothing%20tangential%20to%20the%20data%20manifold.%20In%20addition%2C%20we%0Ashow%20that%20the%20manifold%20along%20which%20the%20diffusion%20model%20generalises%20can%20be%0Acontrolled%20by%20choosing%20an%20appropriate%20smoothing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Models%2520and%2520the%2520Manifold%2520Hypothesis%253A%2520Log-Domain%2520Smoothing%2520is%250A%2520%2520Geometry%2520Adaptive%26entry.906535625%3DTyler%2520Farghly%2520and%2520Peter%2520Potaptchik%2520and%2520Samuel%2520Howard%2520and%2520George%2520Deligiannidis%2520and%2520Jakiw%2520Pidstrigach%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520achieved%2520state-of-the-art%2520performance%252C%2520demonstrating%250Aremarkable%2520generalisation%2520capabilities%2520across%2520diverse%2520domains.%2520However%252C%2520the%250Amechanisms%2520underpinning%2520these%2520strong%2520capabilities%2520remain%2520only%2520partially%250Aunderstood.%2520A%2520leading%2520conjecture%252C%2520based%2520on%2520the%2520manifold%2520hypothesis%252C%2520attributes%250Athis%2520success%2520to%2520their%2520ability%2520to%2520adapt%2520to%2520low-dimensional%2520geometric%2520structure%250Awithin%2520the%2520data.%2520This%2520work%2520provides%2520evidence%2520for%2520this%2520conjecture%252C%2520focusing%2520on%250Ahow%2520such%2520phenomena%2520could%2520result%2520from%2520the%2520formulation%2520of%2520the%2520learning%2520problem%250Athrough%2520score%2520matching.%2520We%2520inspect%2520the%2520role%2520of%2520implicit%2520regularisation%2520by%250Ainvestigating%2520the%2520effect%2520of%2520smoothing%2520minimisers%2520of%2520the%2520empirical%2520score%250Amatching%2520objective.%2520Our%2520theoretical%2520and%2520empirical%2520results%2520confirm%2520that%250Asmoothing%2520the%2520score%2520function%2520--%2520or%2520equivalently%252C%2520smoothing%2520in%2520the%2520log-density%250Adomain%2520--%2520produces%2520smoothing%2520tangential%2520to%2520the%2520data%2520manifold.%2520In%2520addition%252C%2520we%250Ashow%2520that%2520the%2520manifold%2520along%2520which%2520the%2520diffusion%2520model%2520generalises%2520can%2520be%250Acontrolled%2520by%2520choosing%2520an%2520appropriate%2520smoothing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Models%20and%20the%20Manifold%20Hypothesis%3A%20Log-Domain%20Smoothing%20is%0A%20%20Geometry%20Adaptive&entry.906535625=Tyler%20Farghly%20and%20Peter%20Potaptchik%20and%20Samuel%20Howard%20and%20George%20Deligiannidis%20and%20Jakiw%20Pidstrigach&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20state-of-the-art%20performance%2C%20demonstrating%0Aremarkable%20generalisation%20capabilities%20across%20diverse%20domains.%20However%2C%20the%0Amechanisms%20underpinning%20these%20strong%20capabilities%20remain%20only%20partially%0Aunderstood.%20A%20leading%20conjecture%2C%20based%20on%20the%20manifold%20hypothesis%2C%20attributes%0Athis%20success%20to%20their%20ability%20to%20adapt%20to%20low-dimensional%20geometric%20structure%0Awithin%20the%20data.%20This%20work%20provides%20evidence%20for%20this%20conjecture%2C%20focusing%20on%0Ahow%20such%20phenomena%20could%20result%20from%20the%20formulation%20of%20the%20learning%20problem%0Athrough%20score%20matching.%20We%20inspect%20the%20role%20of%20implicit%20regularisation%20by%0Ainvestigating%20the%20effect%20of%20smoothing%20minimisers%20of%20the%20empirical%20score%0Amatching%20objective.%20Our%20theoretical%20and%20empirical%20results%20confirm%20that%0Asmoothing%20the%20score%20function%20--%20or%20equivalently%2C%20smoothing%20in%20the%20log-density%0Adomain%20--%20produces%20smoothing%20tangential%20to%20the%20data%20manifold.%20In%20addition%2C%20we%0Ashow%20that%20the%20manifold%20along%20which%20the%20diffusion%20model%20generalises%20can%20be%0Acontrolled%20by%20choosing%20an%20appropriate%20smoothing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02305v1&entry.124074799=Read"},
{"title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents", "author": "Yaxin Du and Yuanshuo Zhang and Xiyuan Yang and Yifan Zhou and Cheng Wang and Gongyi Zou and Xianghe Pang and Wenhao Wang and Menglan Chen and Shuo Tang and Zhiyu Li and Siheng Chen", "abstract": "  Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.\n", "link": "http://arxiv.org/abs/2510.02271v1", "date": "2025-10-02", "relevancy": 1.5479, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5256}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoMosaic-Bench%3A%20Evaluating%20Multi-Source%20Information%20Seeking%20in%0A%20%20Tool-Augmented%20Agents&body=Title%3A%20InfoMosaic-Bench%3A%20Evaluating%20Multi-Source%20Information%20Seeking%20in%0A%20%20Tool-Augmented%20Agents%0AAuthor%3A%20Yaxin%20Du%20and%20Yuanshuo%20Zhang%20and%20Xiyuan%20Yang%20and%20Yifan%20Zhou%20and%20Cheng%20Wang%20and%20Gongyi%20Zou%20and%20Xianghe%20Pang%20and%20Wenhao%20Wang%20and%20Menglan%20Chen%20and%20Shuo%20Tang%20and%20Zhiyu%20Li%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20Information%20seeking%20is%20a%20fundamental%20requirement%20for%20humans.%20However%2C%0Aexisting%20LLM%20agents%20rely%20heavily%20on%20open-web%20search%2C%20which%20exposes%20two%0Afundamental%20weaknesses%3A%20online%20content%20is%20noisy%20and%20unreliable%2C%20and%20many%0Areal-world%20tasks%20require%20precise%2C%20domain-specific%20knowledge%20unavailable%20from%0Athe%20web.%20The%20emergence%20of%20the%20Model%20Context%20Protocol%20%28MCP%29%20now%20allows%20agents%20to%0Ainterface%20with%20thousands%20of%20specialized%20tools%2C%20seemingly%20resolving%20this%0Alimitation.%20Yet%20it%20remains%20unclear%20whether%20agents%20can%20effectively%20leverage%20such%0Atools%20--%20and%20more%20importantly%2C%20whether%20they%20can%20integrate%20them%20with%0Ageneral-purpose%20search%20to%20solve%20complex%20tasks.%20Therefore%2C%20we%20introduce%0AInfoMosaic-Bench%2C%20the%20first%20benchmark%20dedicated%20to%20multi-source%20information%0Aseeking%20in%20tool-augmented%20agents.%20Covering%20six%20representative%20domains%0A%28medicine%2C%20finance%2C%20maps%2C%20video%2C%20web%2C%20and%20multi-domain%20integration%29%2C%0AInfoMosaic-Bench%20requires%20agents%20to%20combine%20general-purpose%20search%20with%0Adomain-specific%20tools.%20Tasks%20are%20synthesized%20with%20InfoMosaic-Flow%2C%20a%20scalable%0Apipeline%20that%20grounds%20task%20conditions%20in%20verified%20tool%20outputs%2C%20enforces%0Across-source%20dependencies%2C%20and%20filters%20out%20shortcut%20cases%20solvable%20by%20trivial%0Alookup.%20This%20design%20guarantees%20both%20reliability%20and%20non-triviality.%20Experiments%0Awith%2014%20state-of-the-art%20LLM%20agents%20reveal%20three%20findings%3A%20%28i%29%20web%20information%0Aalone%20is%20insufficient%2C%20with%20GPT-5%20achieving%20only%2038.2%25%20accuracy%20and%2067.5%25%20pass%0Arate%3B%20%28ii%29%20domain%20tools%20provide%20selective%20but%20inconsistent%20benefits%2C%20improving%0Asome%20domains%20while%20degrading%20others%3B%20and%20%28iii%29%2022.4%25%20of%20failures%20arise%20from%0Aincorrect%20tool%20usage%20or%20selection%2C%20highlighting%20that%20current%20LLMs%20still%0Astruggle%20with%20even%20basic%20tool%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoMosaic-Bench%253A%2520Evaluating%2520Multi-Source%2520Information%2520Seeking%2520in%250A%2520%2520Tool-Augmented%2520Agents%26entry.906535625%3DYaxin%2520Du%2520and%2520Yuanshuo%2520Zhang%2520and%2520Xiyuan%2520Yang%2520and%2520Yifan%2520Zhou%2520and%2520Cheng%2520Wang%2520and%2520Gongyi%2520Zou%2520and%2520Xianghe%2520Pang%2520and%2520Wenhao%2520Wang%2520and%2520Menglan%2520Chen%2520and%2520Shuo%2520Tang%2520and%2520Zhiyu%2520Li%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520Information%2520seeking%2520is%2520a%2520fundamental%2520requirement%2520for%2520humans.%2520However%252C%250Aexisting%2520LLM%2520agents%2520rely%2520heavily%2520on%2520open-web%2520search%252C%2520which%2520exposes%2520two%250Afundamental%2520weaknesses%253A%2520online%2520content%2520is%2520noisy%2520and%2520unreliable%252C%2520and%2520many%250Areal-world%2520tasks%2520require%2520precise%252C%2520domain-specific%2520knowledge%2520unavailable%2520from%250Athe%2520web.%2520The%2520emergence%2520of%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%2520now%2520allows%2520agents%2520to%250Ainterface%2520with%2520thousands%2520of%2520specialized%2520tools%252C%2520seemingly%2520resolving%2520this%250Alimitation.%2520Yet%2520it%2520remains%2520unclear%2520whether%2520agents%2520can%2520effectively%2520leverage%2520such%250Atools%2520--%2520and%2520more%2520importantly%252C%2520whether%2520they%2520can%2520integrate%2520them%2520with%250Ageneral-purpose%2520search%2520to%2520solve%2520complex%2520tasks.%2520Therefore%252C%2520we%2520introduce%250AInfoMosaic-Bench%252C%2520the%2520first%2520benchmark%2520dedicated%2520to%2520multi-source%2520information%250Aseeking%2520in%2520tool-augmented%2520agents.%2520Covering%2520six%2520representative%2520domains%250A%2528medicine%252C%2520finance%252C%2520maps%252C%2520video%252C%2520web%252C%2520and%2520multi-domain%2520integration%2529%252C%250AInfoMosaic-Bench%2520requires%2520agents%2520to%2520combine%2520general-purpose%2520search%2520with%250Adomain-specific%2520tools.%2520Tasks%2520are%2520synthesized%2520with%2520InfoMosaic-Flow%252C%2520a%2520scalable%250Apipeline%2520that%2520grounds%2520task%2520conditions%2520in%2520verified%2520tool%2520outputs%252C%2520enforces%250Across-source%2520dependencies%252C%2520and%2520filters%2520out%2520shortcut%2520cases%2520solvable%2520by%2520trivial%250Alookup.%2520This%2520design%2520guarantees%2520both%2520reliability%2520and%2520non-triviality.%2520Experiments%250Awith%252014%2520state-of-the-art%2520LLM%2520agents%2520reveal%2520three%2520findings%253A%2520%2528i%2529%2520web%2520information%250Aalone%2520is%2520insufficient%252C%2520with%2520GPT-5%2520achieving%2520only%252038.2%2525%2520accuracy%2520and%252067.5%2525%2520pass%250Arate%253B%2520%2528ii%2529%2520domain%2520tools%2520provide%2520selective%2520but%2520inconsistent%2520benefits%252C%2520improving%250Asome%2520domains%2520while%2520degrading%2520others%253B%2520and%2520%2528iii%2529%252022.4%2525%2520of%2520failures%2520arise%2520from%250Aincorrect%2520tool%2520usage%2520or%2520selection%252C%2520highlighting%2520that%2520current%2520LLMs%2520still%250Astruggle%2520with%2520even%2520basic%2520tool%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoMosaic-Bench%3A%20Evaluating%20Multi-Source%20Information%20Seeking%20in%0A%20%20Tool-Augmented%20Agents&entry.906535625=Yaxin%20Du%20and%20Yuanshuo%20Zhang%20and%20Xiyuan%20Yang%20and%20Yifan%20Zhou%20and%20Cheng%20Wang%20and%20Gongyi%20Zou%20and%20Xianghe%20Pang%20and%20Wenhao%20Wang%20and%20Menglan%20Chen%20and%20Shuo%20Tang%20and%20Zhiyu%20Li%20and%20Siheng%20Chen&entry.1292438233=%20%20Information%20seeking%20is%20a%20fundamental%20requirement%20for%20humans.%20However%2C%0Aexisting%20LLM%20agents%20rely%20heavily%20on%20open-web%20search%2C%20which%20exposes%20two%0Afundamental%20weaknesses%3A%20online%20content%20is%20noisy%20and%20unreliable%2C%20and%20many%0Areal-world%20tasks%20require%20precise%2C%20domain-specific%20knowledge%20unavailable%20from%0Athe%20web.%20The%20emergence%20of%20the%20Model%20Context%20Protocol%20%28MCP%29%20now%20allows%20agents%20to%0Ainterface%20with%20thousands%20of%20specialized%20tools%2C%20seemingly%20resolving%20this%0Alimitation.%20Yet%20it%20remains%20unclear%20whether%20agents%20can%20effectively%20leverage%20such%0Atools%20--%20and%20more%20importantly%2C%20whether%20they%20can%20integrate%20them%20with%0Ageneral-purpose%20search%20to%20solve%20complex%20tasks.%20Therefore%2C%20we%20introduce%0AInfoMosaic-Bench%2C%20the%20first%20benchmark%20dedicated%20to%20multi-source%20information%0Aseeking%20in%20tool-augmented%20agents.%20Covering%20six%20representative%20domains%0A%28medicine%2C%20finance%2C%20maps%2C%20video%2C%20web%2C%20and%20multi-domain%20integration%29%2C%0AInfoMosaic-Bench%20requires%20agents%20to%20combine%20general-purpose%20search%20with%0Adomain-specific%20tools.%20Tasks%20are%20synthesized%20with%20InfoMosaic-Flow%2C%20a%20scalable%0Apipeline%20that%20grounds%20task%20conditions%20in%20verified%20tool%20outputs%2C%20enforces%0Across-source%20dependencies%2C%20and%20filters%20out%20shortcut%20cases%20solvable%20by%20trivial%0Alookup.%20This%20design%20guarantees%20both%20reliability%20and%20non-triviality.%20Experiments%0Awith%2014%20state-of-the-art%20LLM%20agents%20reveal%20three%20findings%3A%20%28i%29%20web%20information%0Aalone%20is%20insufficient%2C%20with%20GPT-5%20achieving%20only%2038.2%25%20accuracy%20and%2067.5%25%20pass%0Arate%3B%20%28ii%29%20domain%20tools%20provide%20selective%20but%20inconsistent%20benefits%2C%20improving%0Asome%20domains%20while%20degrading%20others%3B%20and%20%28iii%29%2022.4%25%20of%20failures%20arise%20from%0Aincorrect%20tool%20usage%20or%20selection%2C%20highlighting%20that%20current%20LLMs%20still%0Astruggle%20with%20even%20basic%20tool%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02271v1&entry.124074799=Read"},
{"title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models", "author": "Yuhao Sun and Zhuoer Xu and Shiwen Cui and Kun Yang and Lingyun Yu and Yongdong Zhang and Hongtao Xie", "abstract": "  Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.\n", "link": "http://arxiv.org/abs/2510.02194v1", "date": "2025-10-02", "relevancy": 1.5045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5123}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4989}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UpSafe%24%5E%5Ccirc%24C%3A%20Upcycling%20for%20Controllable%20Safety%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20UpSafe%24%5E%5Ccirc%24C%3A%20Upcycling%20for%20Controllable%20Safety%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yuhao%20Sun%20and%20Zhuoer%20Xu%20and%20Shiwen%20Cui%20and%20Kun%20Yang%20and%20Lingyun%20Yu%20and%20Yongdong%20Zhang%20and%20Hongtao%20Xie%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20across%20a%20wide%0Arange%20of%20tasks%2C%20but%20remain%20vulnerable%20to%20safety%20risks%20such%20as%20harmful%20content%0Ageneration%20and%20jailbreak%20attacks.%20Existing%20safety%20techniques%20--%20including%0Aexternal%20guardrails%2C%20inference-time%20guidance%2C%20and%20post-training%20alignment%20--%0Aeach%20face%20limitations%20in%20balancing%20safety%2C%20utility%2C%20and%20controllability.%20In%0Athis%20work%2C%20we%20propose%20UpSafe%24%5E%5Ccirc%24C%2C%20a%20unified%20framework%20for%20enhancing%20LLM%0Asafety%20through%20safety-aware%20upcycling.%20Our%20approach%20first%20identifies%0Asafety-critical%20layers%20and%20upcycles%20them%20into%20a%20sparse%20Mixture-of-Experts%20%28MoE%29%0Astructure%2C%20where%20the%20router%20acts%20as%20a%20soft%20guardrail%20that%20selectively%20activates%0Aoriginal%20MLPs%20and%20added%20safety%20experts.%20We%20further%20introduce%20a%20two-stage%20SFT%0Astrategy%20to%20strengthen%20safety%20discrimination%20while%20preserving%20general%0Acapabilities.%20To%20enable%20flexible%20control%20at%20inference%20time%2C%20we%20introduce%20a%0Asafety%20temperature%20mechanism%2C%20allowing%20dynamic%20adjustment%20of%20the%20trade-off%0Abetween%20safety%20and%20utility.%20Experiments%20across%20multiple%20benchmarks%2C%20base%20model%2C%0Aand%20model%20scales%20demonstrate%20that%20UpSafe%24%5E%5Ccirc%24C%20achieves%20robust%20safety%0Aimprovements%20against%20harmful%20and%20jailbreak%20inputs%2C%20while%20maintaining%0Acompetitive%20performance%20on%20general%20tasks.%20Moreover%2C%20analysis%20shows%20that%20safety%0Atemperature%20provides%20fine-grained%20inference-time%20control%20that%20achieves%20the%0APareto-optimal%20frontier%20between%20utility%20and%20safety.%20Our%20results%20highlight%20a%20new%0Adirection%20for%20LLM%20safety%3A%20moving%20from%20static%20alignment%20toward%20dynamic%2C%20modular%2C%0Aand%20inference-aware%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpSafe%2524%255E%255Ccirc%2524C%253A%2520Upcycling%2520for%2520Controllable%2520Safety%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYuhao%2520Sun%2520and%2520Zhuoer%2520Xu%2520and%2520Shiwen%2520Cui%2520and%2520Kun%2520Yang%2520and%2520Lingyun%2520Yu%2520and%2520Yongdong%2520Zhang%2520and%2520Hongtao%2520Xie%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520across%2520a%2520wide%250Arange%2520of%2520tasks%252C%2520but%2520remain%2520vulnerable%2520to%2520safety%2520risks%2520such%2520as%2520harmful%2520content%250Ageneration%2520and%2520jailbreak%2520attacks.%2520Existing%2520safety%2520techniques%2520--%2520including%250Aexternal%2520guardrails%252C%2520inference-time%2520guidance%252C%2520and%2520post-training%2520alignment%2520--%250Aeach%2520face%2520limitations%2520in%2520balancing%2520safety%252C%2520utility%252C%2520and%2520controllability.%2520In%250Athis%2520work%252C%2520we%2520propose%2520UpSafe%2524%255E%255Ccirc%2524C%252C%2520a%2520unified%2520framework%2520for%2520enhancing%2520LLM%250Asafety%2520through%2520safety-aware%2520upcycling.%2520Our%2520approach%2520first%2520identifies%250Asafety-critical%2520layers%2520and%2520upcycles%2520them%2520into%2520a%2520sparse%2520Mixture-of-Experts%2520%2528MoE%2529%250Astructure%252C%2520where%2520the%2520router%2520acts%2520as%2520a%2520soft%2520guardrail%2520that%2520selectively%2520activates%250Aoriginal%2520MLPs%2520and%2520added%2520safety%2520experts.%2520We%2520further%2520introduce%2520a%2520two-stage%2520SFT%250Astrategy%2520to%2520strengthen%2520safety%2520discrimination%2520while%2520preserving%2520general%250Acapabilities.%2520To%2520enable%2520flexible%2520control%2520at%2520inference%2520time%252C%2520we%2520introduce%2520a%250Asafety%2520temperature%2520mechanism%252C%2520allowing%2520dynamic%2520adjustment%2520of%2520the%2520trade-off%250Abetween%2520safety%2520and%2520utility.%2520Experiments%2520across%2520multiple%2520benchmarks%252C%2520base%2520model%252C%250Aand%2520model%2520scales%2520demonstrate%2520that%2520UpSafe%2524%255E%255Ccirc%2524C%2520achieves%2520robust%2520safety%250Aimprovements%2520against%2520harmful%2520and%2520jailbreak%2520inputs%252C%2520while%2520maintaining%250Acompetitive%2520performance%2520on%2520general%2520tasks.%2520Moreover%252C%2520analysis%2520shows%2520that%2520safety%250Atemperature%2520provides%2520fine-grained%2520inference-time%2520control%2520that%2520achieves%2520the%250APareto-optimal%2520frontier%2520between%2520utility%2520and%2520safety.%2520Our%2520results%2520highlight%2520a%2520new%250Adirection%2520for%2520LLM%2520safety%253A%2520moving%2520from%2520static%2520alignment%2520toward%2520dynamic%252C%2520modular%252C%250Aand%2520inference-aware%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UpSafe%24%5E%5Ccirc%24C%3A%20Upcycling%20for%20Controllable%20Safety%20in%20Large%20Language%0A%20%20Models&entry.906535625=Yuhao%20Sun%20and%20Zhuoer%20Xu%20and%20Shiwen%20Cui%20and%20Kun%20Yang%20and%20Lingyun%20Yu%20and%20Yongdong%20Zhang%20and%20Hongtao%20Xie&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20across%20a%20wide%0Arange%20of%20tasks%2C%20but%20remain%20vulnerable%20to%20safety%20risks%20such%20as%20harmful%20content%0Ageneration%20and%20jailbreak%20attacks.%20Existing%20safety%20techniques%20--%20including%0Aexternal%20guardrails%2C%20inference-time%20guidance%2C%20and%20post-training%20alignment%20--%0Aeach%20face%20limitations%20in%20balancing%20safety%2C%20utility%2C%20and%20controllability.%20In%0Athis%20work%2C%20we%20propose%20UpSafe%24%5E%5Ccirc%24C%2C%20a%20unified%20framework%20for%20enhancing%20LLM%0Asafety%20through%20safety-aware%20upcycling.%20Our%20approach%20first%20identifies%0Asafety-critical%20layers%20and%20upcycles%20them%20into%20a%20sparse%20Mixture-of-Experts%20%28MoE%29%0Astructure%2C%20where%20the%20router%20acts%20as%20a%20soft%20guardrail%20that%20selectively%20activates%0Aoriginal%20MLPs%20and%20added%20safety%20experts.%20We%20further%20introduce%20a%20two-stage%20SFT%0Astrategy%20to%20strengthen%20safety%20discrimination%20while%20preserving%20general%0Acapabilities.%20To%20enable%20flexible%20control%20at%20inference%20time%2C%20we%20introduce%20a%0Asafety%20temperature%20mechanism%2C%20allowing%20dynamic%20adjustment%20of%20the%20trade-off%0Abetween%20safety%20and%20utility.%20Experiments%20across%20multiple%20benchmarks%2C%20base%20model%2C%0Aand%20model%20scales%20demonstrate%20that%20UpSafe%24%5E%5Ccirc%24C%20achieves%20robust%20safety%0Aimprovements%20against%20harmful%20and%20jailbreak%20inputs%2C%20while%20maintaining%0Acompetitive%20performance%20on%20general%20tasks.%20Moreover%2C%20analysis%20shows%20that%20safety%0Atemperature%20provides%20fine-grained%20inference-time%20control%20that%20achieves%20the%0APareto-optimal%20frontier%20between%20utility%20and%20safety.%20Our%20results%20highlight%20a%20new%0Adirection%20for%20LLM%20safety%3A%20moving%20from%20static%20alignment%20toward%20dynamic%2C%20modular%2C%0Aand%20inference-aware%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02194v1&entry.124074799=Read"},
{"title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\n  Completion across General and Biomedical Domains", "author": "Yongkang Xiao and Sinian Zhang and Yi Dai and Huixue Zhou and Jue Hou and Jie Ding and Rui Zhang", "abstract": "  Knowledge graph completion (KGC) aims to predict missing triples in knowledge\ngraphs (KGs) by leveraging existing triples and textual information. Recently,\ngenerative large language models (LLMs) have been increasingly employed for\ngraph tasks. However, current approaches typically encode graph context in\ntextual form, which fails to fully exploit the potential of LLMs for perceiving\nand reasoning about graph structures. To address this limitation, we propose\nDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\nCompletion). DrKGC employs a flexible lightweight model training strategy to\nlearn structural embeddings and logical rules within the KG. It then leverages\na novel bottom-up graph retrieval method to extract a subgraph for each query\nguided by the learned rules. Finally, a graph convolutional network (GCN)\nadapter uses the retrieved subgraph to enhance the structural embeddings, which\nare then integrated into the prompt for effective LLM fine-tuning. Experimental\nresults on two general domain benchmark datasets and two biomedical datasets\ndemonstrate the superior performance of DrKGC. Furthermore, a realistic case\nstudy in the biomedical domain highlights its interpretability and practical\nutility.\n", "link": "http://arxiv.org/abs/2506.00708v2", "date": "2025-10-02", "relevancy": 1.5042, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5103}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4927}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DrKGC%3A%20Dynamic%20Subgraph%20Retrieval-Augmented%20LLMs%20for%20Knowledge%20Graph%0A%20%20Completion%20across%20General%20and%20Biomedical%20Domains&body=Title%3A%20DrKGC%3A%20Dynamic%20Subgraph%20Retrieval-Augmented%20LLMs%20for%20Knowledge%20Graph%0A%20%20Completion%20across%20General%20and%20Biomedical%20Domains%0AAuthor%3A%20Yongkang%20Xiao%20and%20Sinian%20Zhang%20and%20Yi%20Dai%20and%20Huixue%20Zhou%20and%20Jue%20Hou%20and%20Jie%20Ding%20and%20Rui%20Zhang%0AAbstract%3A%20%20%20Knowledge%20graph%20completion%20%28KGC%29%20aims%20to%20predict%20missing%20triples%20in%20knowledge%0Agraphs%20%28KGs%29%20by%20leveraging%20existing%20triples%20and%20textual%20information.%20Recently%2C%0Agenerative%20large%20language%20models%20%28LLMs%29%20have%20been%20increasingly%20employed%20for%0Agraph%20tasks.%20However%2C%20current%20approaches%20typically%20encode%20graph%20context%20in%0Atextual%20form%2C%20which%20fails%20to%20fully%20exploit%20the%20potential%20of%20LLMs%20for%20perceiving%0Aand%20reasoning%20about%20graph%20structures.%20To%20address%20this%20limitation%2C%20we%20propose%0ADrKGC%20%28Dynamic%20Subgraph%20Retrieval-Augmented%20LLMs%20for%20Knowledge%20Graph%0ACompletion%29.%20DrKGC%20employs%20a%20flexible%20lightweight%20model%20training%20strategy%20to%0Alearn%20structural%20embeddings%20and%20logical%20rules%20within%20the%20KG.%20It%20then%20leverages%0Aa%20novel%20bottom-up%20graph%20retrieval%20method%20to%20extract%20a%20subgraph%20for%20each%20query%0Aguided%20by%20the%20learned%20rules.%20Finally%2C%20a%20graph%20convolutional%20network%20%28GCN%29%0Aadapter%20uses%20the%20retrieved%20subgraph%20to%20enhance%20the%20structural%20embeddings%2C%20which%0Aare%20then%20integrated%20into%20the%20prompt%20for%20effective%20LLM%20fine-tuning.%20Experimental%0Aresults%20on%20two%20general%20domain%20benchmark%20datasets%20and%20two%20biomedical%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20DrKGC.%20Furthermore%2C%20a%20realistic%20case%0Astudy%20in%20the%20biomedical%20domain%20highlights%20its%20interpretability%20and%20practical%0Autility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrKGC%253A%2520Dynamic%2520Subgraph%2520Retrieval-Augmented%2520LLMs%2520for%2520Knowledge%2520Graph%250A%2520%2520Completion%2520across%2520General%2520and%2520Biomedical%2520Domains%26entry.906535625%3DYongkang%2520Xiao%2520and%2520Sinian%2520Zhang%2520and%2520Yi%2520Dai%2520and%2520Huixue%2520Zhou%2520and%2520Jue%2520Hou%2520and%2520Jie%2520Ding%2520and%2520Rui%2520Zhang%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520completion%2520%2528KGC%2529%2520aims%2520to%2520predict%2520missing%2520triples%2520in%2520knowledge%250Agraphs%2520%2528KGs%2529%2520by%2520leveraging%2520existing%2520triples%2520and%2520textual%2520information.%2520Recently%252C%250Agenerative%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520increasingly%2520employed%2520for%250Agraph%2520tasks.%2520However%252C%2520current%2520approaches%2520typically%2520encode%2520graph%2520context%2520in%250Atextual%2520form%252C%2520which%2520fails%2520to%2520fully%2520exploit%2520the%2520potential%2520of%2520LLMs%2520for%2520perceiving%250Aand%2520reasoning%2520about%2520graph%2520structures.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250ADrKGC%2520%2528Dynamic%2520Subgraph%2520Retrieval-Augmented%2520LLMs%2520for%2520Knowledge%2520Graph%250ACompletion%2529.%2520DrKGC%2520employs%2520a%2520flexible%2520lightweight%2520model%2520training%2520strategy%2520to%250Alearn%2520structural%2520embeddings%2520and%2520logical%2520rules%2520within%2520the%2520KG.%2520It%2520then%2520leverages%250Aa%2520novel%2520bottom-up%2520graph%2520retrieval%2520method%2520to%2520extract%2520a%2520subgraph%2520for%2520each%2520query%250Aguided%2520by%2520the%2520learned%2520rules.%2520Finally%252C%2520a%2520graph%2520convolutional%2520network%2520%2528GCN%2529%250Aadapter%2520uses%2520the%2520retrieved%2520subgraph%2520to%2520enhance%2520the%2520structural%2520embeddings%252C%2520which%250Aare%2520then%2520integrated%2520into%2520the%2520prompt%2520for%2520effective%2520LLM%2520fine-tuning.%2520Experimental%250Aresults%2520on%2520two%2520general%2520domain%2520benchmark%2520datasets%2520and%2520two%2520biomedical%2520datasets%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520DrKGC.%2520Furthermore%252C%2520a%2520realistic%2520case%250Astudy%2520in%2520the%2520biomedical%2520domain%2520highlights%2520its%2520interpretability%2520and%2520practical%250Autility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrKGC%3A%20Dynamic%20Subgraph%20Retrieval-Augmented%20LLMs%20for%20Knowledge%20Graph%0A%20%20Completion%20across%20General%20and%20Biomedical%20Domains&entry.906535625=Yongkang%20Xiao%20and%20Sinian%20Zhang%20and%20Yi%20Dai%20and%20Huixue%20Zhou%20and%20Jue%20Hou%20and%20Jie%20Ding%20and%20Rui%20Zhang&entry.1292438233=%20%20Knowledge%20graph%20completion%20%28KGC%29%20aims%20to%20predict%20missing%20triples%20in%20knowledge%0Agraphs%20%28KGs%29%20by%20leveraging%20existing%20triples%20and%20textual%20information.%20Recently%2C%0Agenerative%20large%20language%20models%20%28LLMs%29%20have%20been%20increasingly%20employed%20for%0Agraph%20tasks.%20However%2C%20current%20approaches%20typically%20encode%20graph%20context%20in%0Atextual%20form%2C%20which%20fails%20to%20fully%20exploit%20the%20potential%20of%20LLMs%20for%20perceiving%0Aand%20reasoning%20about%20graph%20structures.%20To%20address%20this%20limitation%2C%20we%20propose%0ADrKGC%20%28Dynamic%20Subgraph%20Retrieval-Augmented%20LLMs%20for%20Knowledge%20Graph%0ACompletion%29.%20DrKGC%20employs%20a%20flexible%20lightweight%20model%20training%20strategy%20to%0Alearn%20structural%20embeddings%20and%20logical%20rules%20within%20the%20KG.%20It%20then%20leverages%0Aa%20novel%20bottom-up%20graph%20retrieval%20method%20to%20extract%20a%20subgraph%20for%20each%20query%0Aguided%20by%20the%20learned%20rules.%20Finally%2C%20a%20graph%20convolutional%20network%20%28GCN%29%0Aadapter%20uses%20the%20retrieved%20subgraph%20to%20enhance%20the%20structural%20embeddings%2C%20which%0Aare%20then%20integrated%20into%20the%20prompt%20for%20effective%20LLM%20fine-tuning.%20Experimental%0Aresults%20on%20two%20general%20domain%20benchmark%20datasets%20and%20two%20biomedical%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20DrKGC.%20Furthermore%2C%20a%20realistic%20case%0Astudy%20in%20the%20biomedical%20domain%20highlights%20its%20interpretability%20and%20practical%0Autility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00708v2&entry.124074799=Read"},
{"title": "Transformers Discover Molecular Structure Without Graph Priors", "author": "Tobias Kreiman and Yutong Bai and Fadi Atieh and Elizabeth Weaver and Eric Qu and Aditi S. Krishnapriyan", "abstract": "  Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.\n", "link": "http://arxiv.org/abs/2510.02259v1", "date": "2025-10-02", "relevancy": 1.5031, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5501}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4928}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20Discover%20Molecular%20Structure%20Without%20Graph%20Priors&body=Title%3A%20Transformers%20Discover%20Molecular%20Structure%20Without%20Graph%20Priors%0AAuthor%3A%20Tobias%20Kreiman%20and%20Yutong%20Bai%20and%20Fadi%20Atieh%20and%20Elizabeth%20Weaver%20and%20Eric%20Qu%20and%20Aditi%20S.%20Krishnapriyan%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20the%20dominant%20architecture%20for%20molecular%0Amachine%20learning%2C%20particularly%20for%20molecular%20property%20prediction%20and%20machine%0Alearning%20interatomic%20potentials%20%28MLIPs%29.%20GNNs%20perform%20message%20passing%20on%0Apredefined%20graphs%20often%20induced%20by%20a%20fixed%20radius%20cutoff%20or%20k-nearest%20neighbor%0Ascheme.%20While%20this%20design%20aligns%20with%20the%20locality%20present%20in%20many%20molecular%0Atasks%2C%20a%20hard-coded%20graph%20can%20limit%20expressivity%20due%20to%20the%20fixed%20receptive%0Afield%20and%20slows%20down%20inference%20with%20sparse%20graph%20operations.%20In%20this%20work%2C%20we%0Ainvestigate%20whether%20pure%2C%20unmodified%20Transformers%20trained%20directly%20on%20Cartesian%0Acoordinates%24%5Cunicode%7Bx2013%7D%24without%20predefined%20graphs%20or%20physical%0Apriors%24%5Cunicode%7Bx2013%7D%24can%20approximate%20molecular%20energies%20and%20forces.%20As%20a%0Astarting%20point%20for%20our%20analysis%2C%20we%20demonstrate%20how%20to%20train%20a%20Transformer%20to%0Acompetitive%20energy%20and%20force%20mean%20absolute%20errors%20under%20a%20matched%20training%0Acompute%20budget%2C%20relative%20to%20a%20state-of-the-art%20equivariant%20GNN%20on%20the%20OMol25%0Adataset.%20We%20discover%20that%20the%20Transformer%20learns%20physically%20consistent%0Apatterns%24%5Cunicode%7Bx2013%7D%24such%20as%20attention%20weights%20that%20decay%20inversely%20with%0Ainteratomic%20distance%24%5Cunicode%7Bx2013%7D%24and%20flexibly%20adapts%20them%20across%20different%0Amolecular%20environments%20due%20to%20the%20absence%20of%20hard-coded%20biases.%20The%20use%20of%20a%0Astandard%20Transformer%20also%20unlocks%20predictable%20improvements%20with%20respect%20to%0Ascaling%20training%20resources%2C%20consistent%20with%20empirical%20scaling%20laws%20observed%20in%0Aother%20domains.%20Our%20results%20demonstrate%20that%20many%20favorable%20properties%20of%20GNNs%0Acan%20emerge%20adaptively%20in%20Transformers%2C%20challenging%20the%20necessity%20of%20hard-coded%0Agraph%20inductive%20biases%20and%20pointing%20toward%20standardized%2C%20scalable%20architectures%0Afor%20molecular%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520Discover%2520Molecular%2520Structure%2520Without%2520Graph%2520Priors%26entry.906535625%3DTobias%2520Kreiman%2520and%2520Yutong%2520Bai%2520and%2520Fadi%2520Atieh%2520and%2520Elizabeth%2520Weaver%2520and%2520Eric%2520Qu%2520and%2520Aditi%2520S.%2520Krishnapriyan%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520the%2520dominant%2520architecture%2520for%2520molecular%250Amachine%2520learning%252C%2520particularly%2520for%2520molecular%2520property%2520prediction%2520and%2520machine%250Alearning%2520interatomic%2520potentials%2520%2528MLIPs%2529.%2520GNNs%2520perform%2520message%2520passing%2520on%250Apredefined%2520graphs%2520often%2520induced%2520by%2520a%2520fixed%2520radius%2520cutoff%2520or%2520k-nearest%2520neighbor%250Ascheme.%2520While%2520this%2520design%2520aligns%2520with%2520the%2520locality%2520present%2520in%2520many%2520molecular%250Atasks%252C%2520a%2520hard-coded%2520graph%2520can%2520limit%2520expressivity%2520due%2520to%2520the%2520fixed%2520receptive%250Afield%2520and%2520slows%2520down%2520inference%2520with%2520sparse%2520graph%2520operations.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520whether%2520pure%252C%2520unmodified%2520Transformers%2520trained%2520directly%2520on%2520Cartesian%250Acoordinates%2524%255Cunicode%257Bx2013%257D%2524without%2520predefined%2520graphs%2520or%2520physical%250Apriors%2524%255Cunicode%257Bx2013%257D%2524can%2520approximate%2520molecular%2520energies%2520and%2520forces.%2520As%2520a%250Astarting%2520point%2520for%2520our%2520analysis%252C%2520we%2520demonstrate%2520how%2520to%2520train%2520a%2520Transformer%2520to%250Acompetitive%2520energy%2520and%2520force%2520mean%2520absolute%2520errors%2520under%2520a%2520matched%2520training%250Acompute%2520budget%252C%2520relative%2520to%2520a%2520state-of-the-art%2520equivariant%2520GNN%2520on%2520the%2520OMol25%250Adataset.%2520We%2520discover%2520that%2520the%2520Transformer%2520learns%2520physically%2520consistent%250Apatterns%2524%255Cunicode%257Bx2013%257D%2524such%2520as%2520attention%2520weights%2520that%2520decay%2520inversely%2520with%250Ainteratomic%2520distance%2524%255Cunicode%257Bx2013%257D%2524and%2520flexibly%2520adapts%2520them%2520across%2520different%250Amolecular%2520environments%2520due%2520to%2520the%2520absence%2520of%2520hard-coded%2520biases.%2520The%2520use%2520of%2520a%250Astandard%2520Transformer%2520also%2520unlocks%2520predictable%2520improvements%2520with%2520respect%2520to%250Ascaling%2520training%2520resources%252C%2520consistent%2520with%2520empirical%2520scaling%2520laws%2520observed%2520in%250Aother%2520domains.%2520Our%2520results%2520demonstrate%2520that%2520many%2520favorable%2520properties%2520of%2520GNNs%250Acan%2520emerge%2520adaptively%2520in%2520Transformers%252C%2520challenging%2520the%2520necessity%2520of%2520hard-coded%250Agraph%2520inductive%2520biases%2520and%2520pointing%2520toward%2520standardized%252C%2520scalable%2520architectures%250Afor%2520molecular%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20Discover%20Molecular%20Structure%20Without%20Graph%20Priors&entry.906535625=Tobias%20Kreiman%20and%20Yutong%20Bai%20and%20Fadi%20Atieh%20and%20Elizabeth%20Weaver%20and%20Eric%20Qu%20and%20Aditi%20S.%20Krishnapriyan&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20the%20dominant%20architecture%20for%20molecular%0Amachine%20learning%2C%20particularly%20for%20molecular%20property%20prediction%20and%20machine%0Alearning%20interatomic%20potentials%20%28MLIPs%29.%20GNNs%20perform%20message%20passing%20on%0Apredefined%20graphs%20often%20induced%20by%20a%20fixed%20radius%20cutoff%20or%20k-nearest%20neighbor%0Ascheme.%20While%20this%20design%20aligns%20with%20the%20locality%20present%20in%20many%20molecular%0Atasks%2C%20a%20hard-coded%20graph%20can%20limit%20expressivity%20due%20to%20the%20fixed%20receptive%0Afield%20and%20slows%20down%20inference%20with%20sparse%20graph%20operations.%20In%20this%20work%2C%20we%0Ainvestigate%20whether%20pure%2C%20unmodified%20Transformers%20trained%20directly%20on%20Cartesian%0Acoordinates%24%5Cunicode%7Bx2013%7D%24without%20predefined%20graphs%20or%20physical%0Apriors%24%5Cunicode%7Bx2013%7D%24can%20approximate%20molecular%20energies%20and%20forces.%20As%20a%0Astarting%20point%20for%20our%20analysis%2C%20we%20demonstrate%20how%20to%20train%20a%20Transformer%20to%0Acompetitive%20energy%20and%20force%20mean%20absolute%20errors%20under%20a%20matched%20training%0Acompute%20budget%2C%20relative%20to%20a%20state-of-the-art%20equivariant%20GNN%20on%20the%20OMol25%0Adataset.%20We%20discover%20that%20the%20Transformer%20learns%20physically%20consistent%0Apatterns%24%5Cunicode%7Bx2013%7D%24such%20as%20attention%20weights%20that%20decay%20inversely%20with%0Ainteratomic%20distance%24%5Cunicode%7Bx2013%7D%24and%20flexibly%20adapts%20them%20across%20different%0Amolecular%20environments%20due%20to%20the%20absence%20of%20hard-coded%20biases.%20The%20use%20of%20a%0Astandard%20Transformer%20also%20unlocks%20predictable%20improvements%20with%20respect%20to%0Ascaling%20training%20resources%2C%20consistent%20with%20empirical%20scaling%20laws%20observed%20in%0Aother%20domains.%20Our%20results%20demonstrate%20that%20many%20favorable%20properties%20of%20GNNs%0Acan%20emerge%20adaptively%20in%20Transformers%2C%20challenging%20the%20necessity%20of%20hard-coded%0Agraph%20inductive%20biases%20and%20pointing%20toward%20standardized%2C%20scalable%20architectures%0Afor%20molecular%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02259v1&entry.124074799=Read"},
{"title": "Quantum Fisher information matrices from R\u00e9nyi relative entropies", "author": "Mark M. Wilde", "abstract": "  Quantum generalizations of the Fisher information are important in quantum\ninformation science, with applications in high energy and condensed matter\nphysics and in quantum estimation theory, machine learning, and optimization.\nOne can derive a quantum generalization of the Fisher information matrix in a\nnatural way as the Hessian matrix arising in a Taylor expansion of a smooth\ndivergence. Such an approach is appealing for quantum information theorists,\ngiven the ubiquity of divergences in quantum information theory. In contrast to\nthe classical case, there is not a unique quantum generalization of the Fisher\ninformation matrix, similar to how there is not a unique quantum generalization\nof the relative entropy or the R\\'enyi relative entropy. In this paper, I\nderive information matrices arising from the log-Euclidean, $\\alpha$-$z$, and\ngeometric R\\'enyi relative entropies, with the main technical tool for doing so\nbeing the method of divided differences for calculating matrix derivatives.\nInterestingly, for all non-negative values of the R\\'enyi parameter $\\alpha$,\nthe log-Euclidean R\\'enyi relative entropy leads to the Kubo-Mori information\nmatrix, and the geometric R\\'enyi relative entropy leads to the\nright-logarithmic derivative Fisher information matrix. Thus, the resulting\ninformation matrices obey the data-processing inequality for all non-negative\nvalues of the R\\'enyi parameter $\\alpha$ even though the original quantities do\nnot. Additionally, I derive and establish basic properties of $\\alpha$-$z$\ninformation matrices resulting from the $\\alpha$-$z$ R\\'enyi relative\nentropies. For parameterized thermal states, I establish formulas for their\n$\\alpha$-$z$ information matrices and hybrid quantum-classical algorithms for\nestimating them, with applications in quantum Boltzmann machine learning.\n", "link": "http://arxiv.org/abs/2510.02218v1", "date": "2025-10-02", "relevancy": 1.4904, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3915}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3732}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Fisher%20information%20matrices%20from%20R%C3%A9nyi%20relative%20entropies&body=Title%3A%20Quantum%20Fisher%20information%20matrices%20from%20R%C3%A9nyi%20relative%20entropies%0AAuthor%3A%20Mark%20M.%20Wilde%0AAbstract%3A%20%20%20Quantum%20generalizations%20of%20the%20Fisher%20information%20are%20important%20in%20quantum%0Ainformation%20science%2C%20with%20applications%20in%20high%20energy%20and%20condensed%20matter%0Aphysics%20and%20in%20quantum%20estimation%20theory%2C%20machine%20learning%2C%20and%20optimization.%0AOne%20can%20derive%20a%20quantum%20generalization%20of%20the%20Fisher%20information%20matrix%20in%20a%0Anatural%20way%20as%20the%20Hessian%20matrix%20arising%20in%20a%20Taylor%20expansion%20of%20a%20smooth%0Adivergence.%20Such%20an%20approach%20is%20appealing%20for%20quantum%20information%20theorists%2C%0Agiven%20the%20ubiquity%20of%20divergences%20in%20quantum%20information%20theory.%20In%20contrast%20to%0Athe%20classical%20case%2C%20there%20is%20not%20a%20unique%20quantum%20generalization%20of%20the%20Fisher%0Ainformation%20matrix%2C%20similar%20to%20how%20there%20is%20not%20a%20unique%20quantum%20generalization%0Aof%20the%20relative%20entropy%20or%20the%20R%5C%27enyi%20relative%20entropy.%20In%20this%20paper%2C%20I%0Aderive%20information%20matrices%20arising%20from%20the%20log-Euclidean%2C%20%24%5Calpha%24-%24z%24%2C%20and%0Ageometric%20R%5C%27enyi%20relative%20entropies%2C%20with%20the%20main%20technical%20tool%20for%20doing%20so%0Abeing%20the%20method%20of%20divided%20differences%20for%20calculating%20matrix%20derivatives.%0AInterestingly%2C%20for%20all%20non-negative%20values%20of%20the%20R%5C%27enyi%20parameter%20%24%5Calpha%24%2C%0Athe%20log-Euclidean%20R%5C%27enyi%20relative%20entropy%20leads%20to%20the%20Kubo-Mori%20information%0Amatrix%2C%20and%20the%20geometric%20R%5C%27enyi%20relative%20entropy%20leads%20to%20the%0Aright-logarithmic%20derivative%20Fisher%20information%20matrix.%20Thus%2C%20the%20resulting%0Ainformation%20matrices%20obey%20the%20data-processing%20inequality%20for%20all%20non-negative%0Avalues%20of%20the%20R%5C%27enyi%20parameter%20%24%5Calpha%24%20even%20though%20the%20original%20quantities%20do%0Anot.%20Additionally%2C%20I%20derive%20and%20establish%20basic%20properties%20of%20%24%5Calpha%24-%24z%24%0Ainformation%20matrices%20resulting%20from%20the%20%24%5Calpha%24-%24z%24%20R%5C%27enyi%20relative%0Aentropies.%20For%20parameterized%20thermal%20states%2C%20I%20establish%20formulas%20for%20their%0A%24%5Calpha%24-%24z%24%20information%20matrices%20and%20hybrid%20quantum-classical%20algorithms%20for%0Aestimating%20them%2C%20with%20applications%20in%20quantum%20Boltzmann%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Fisher%2520information%2520matrices%2520from%2520R%25C3%25A9nyi%2520relative%2520entropies%26entry.906535625%3DMark%2520M.%2520Wilde%26entry.1292438233%3D%2520%2520Quantum%2520generalizations%2520of%2520the%2520Fisher%2520information%2520are%2520important%2520in%2520quantum%250Ainformation%2520science%252C%2520with%2520applications%2520in%2520high%2520energy%2520and%2520condensed%2520matter%250Aphysics%2520and%2520in%2520quantum%2520estimation%2520theory%252C%2520machine%2520learning%252C%2520and%2520optimization.%250AOne%2520can%2520derive%2520a%2520quantum%2520generalization%2520of%2520the%2520Fisher%2520information%2520matrix%2520in%2520a%250Anatural%2520way%2520as%2520the%2520Hessian%2520matrix%2520arising%2520in%2520a%2520Taylor%2520expansion%2520of%2520a%2520smooth%250Adivergence.%2520Such%2520an%2520approach%2520is%2520appealing%2520for%2520quantum%2520information%2520theorists%252C%250Agiven%2520the%2520ubiquity%2520of%2520divergences%2520in%2520quantum%2520information%2520theory.%2520In%2520contrast%2520to%250Athe%2520classical%2520case%252C%2520there%2520is%2520not%2520a%2520unique%2520quantum%2520generalization%2520of%2520the%2520Fisher%250Ainformation%2520matrix%252C%2520similar%2520to%2520how%2520there%2520is%2520not%2520a%2520unique%2520quantum%2520generalization%250Aof%2520the%2520relative%2520entropy%2520or%2520the%2520R%255C%2527enyi%2520relative%2520entropy.%2520In%2520this%2520paper%252C%2520I%250Aderive%2520information%2520matrices%2520arising%2520from%2520the%2520log-Euclidean%252C%2520%2524%255Calpha%2524-%2524z%2524%252C%2520and%250Ageometric%2520R%255C%2527enyi%2520relative%2520entropies%252C%2520with%2520the%2520main%2520technical%2520tool%2520for%2520doing%2520so%250Abeing%2520the%2520method%2520of%2520divided%2520differences%2520for%2520calculating%2520matrix%2520derivatives.%250AInterestingly%252C%2520for%2520all%2520non-negative%2520values%2520of%2520the%2520R%255C%2527enyi%2520parameter%2520%2524%255Calpha%2524%252C%250Athe%2520log-Euclidean%2520R%255C%2527enyi%2520relative%2520entropy%2520leads%2520to%2520the%2520Kubo-Mori%2520information%250Amatrix%252C%2520and%2520the%2520geometric%2520R%255C%2527enyi%2520relative%2520entropy%2520leads%2520to%2520the%250Aright-logarithmic%2520derivative%2520Fisher%2520information%2520matrix.%2520Thus%252C%2520the%2520resulting%250Ainformation%2520matrices%2520obey%2520the%2520data-processing%2520inequality%2520for%2520all%2520non-negative%250Avalues%2520of%2520the%2520R%255C%2527enyi%2520parameter%2520%2524%255Calpha%2524%2520even%2520though%2520the%2520original%2520quantities%2520do%250Anot.%2520Additionally%252C%2520I%2520derive%2520and%2520establish%2520basic%2520properties%2520of%2520%2524%255Calpha%2524-%2524z%2524%250Ainformation%2520matrices%2520resulting%2520from%2520the%2520%2524%255Calpha%2524-%2524z%2524%2520R%255C%2527enyi%2520relative%250Aentropies.%2520For%2520parameterized%2520thermal%2520states%252C%2520I%2520establish%2520formulas%2520for%2520their%250A%2524%255Calpha%2524-%2524z%2524%2520information%2520matrices%2520and%2520hybrid%2520quantum-classical%2520algorithms%2520for%250Aestimating%2520them%252C%2520with%2520applications%2520in%2520quantum%2520Boltzmann%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Fisher%20information%20matrices%20from%20R%C3%A9nyi%20relative%20entropies&entry.906535625=Mark%20M.%20Wilde&entry.1292438233=%20%20Quantum%20generalizations%20of%20the%20Fisher%20information%20are%20important%20in%20quantum%0Ainformation%20science%2C%20with%20applications%20in%20high%20energy%20and%20condensed%20matter%0Aphysics%20and%20in%20quantum%20estimation%20theory%2C%20machine%20learning%2C%20and%20optimization.%0AOne%20can%20derive%20a%20quantum%20generalization%20of%20the%20Fisher%20information%20matrix%20in%20a%0Anatural%20way%20as%20the%20Hessian%20matrix%20arising%20in%20a%20Taylor%20expansion%20of%20a%20smooth%0Adivergence.%20Such%20an%20approach%20is%20appealing%20for%20quantum%20information%20theorists%2C%0Agiven%20the%20ubiquity%20of%20divergences%20in%20quantum%20information%20theory.%20In%20contrast%20to%0Athe%20classical%20case%2C%20there%20is%20not%20a%20unique%20quantum%20generalization%20of%20the%20Fisher%0Ainformation%20matrix%2C%20similar%20to%20how%20there%20is%20not%20a%20unique%20quantum%20generalization%0Aof%20the%20relative%20entropy%20or%20the%20R%5C%27enyi%20relative%20entropy.%20In%20this%20paper%2C%20I%0Aderive%20information%20matrices%20arising%20from%20the%20log-Euclidean%2C%20%24%5Calpha%24-%24z%24%2C%20and%0Ageometric%20R%5C%27enyi%20relative%20entropies%2C%20with%20the%20main%20technical%20tool%20for%20doing%20so%0Abeing%20the%20method%20of%20divided%20differences%20for%20calculating%20matrix%20derivatives.%0AInterestingly%2C%20for%20all%20non-negative%20values%20of%20the%20R%5C%27enyi%20parameter%20%24%5Calpha%24%2C%0Athe%20log-Euclidean%20R%5C%27enyi%20relative%20entropy%20leads%20to%20the%20Kubo-Mori%20information%0Amatrix%2C%20and%20the%20geometric%20R%5C%27enyi%20relative%20entropy%20leads%20to%20the%0Aright-logarithmic%20derivative%20Fisher%20information%20matrix.%20Thus%2C%20the%20resulting%0Ainformation%20matrices%20obey%20the%20data-processing%20inequality%20for%20all%20non-negative%0Avalues%20of%20the%20R%5C%27enyi%20parameter%20%24%5Calpha%24%20even%20though%20the%20original%20quantities%20do%0Anot.%20Additionally%2C%20I%20derive%20and%20establish%20basic%20properties%20of%20%24%5Calpha%24-%24z%24%0Ainformation%20matrices%20resulting%20from%20the%20%24%5Calpha%24-%24z%24%20R%5C%27enyi%20relative%0Aentropies.%20For%20parameterized%20thermal%20states%2C%20I%20establish%20formulas%20for%20their%0A%24%5Calpha%24-%24z%24%20information%20matrices%20and%20hybrid%20quantum-classical%20algorithms%20for%0Aestimating%20them%2C%20with%20applications%20in%20quantum%20Boltzmann%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02218v1&entry.124074799=Read"},
{"title": "Machine learning for accuracy in density functional approximations", "author": "Johannes Voss", "abstract": "  Machine learning techniques have found their way into computational chemistry\nas indispensable tools to accelerate atomistic simulations and materials\ndesign. In addition, machine learning approaches hold the potential to boost\nthe predictive power of computationally efficient electronic structure methods,\nsuch as density functional theory, to chemical accuracy and to correct for\nfundamental errors in density functional approaches. Here, recent progress in\napplying machine learning to improve the accuracy of density functional and\nrelated approximations is reviewed. Promises and challenges in devising machine\nlearning models transferable between different chemistries and materials\nclasses are discussed with the help of examples applying promising models to\nsystems far outside their training sets.\n", "link": "http://arxiv.org/abs/2311.00196v2", "date": "2025-10-02", "relevancy": 0.8128, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4258}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3973}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20learning%20for%20accuracy%20in%20density%20functional%20approximations&body=Title%3A%20Machine%20learning%20for%20accuracy%20in%20density%20functional%20approximations%0AAuthor%3A%20Johannes%20Voss%0AAbstract%3A%20%20%20Machine%20learning%20techniques%20have%20found%20their%20way%20into%20computational%20chemistry%0Aas%20indispensable%20tools%20to%20accelerate%20atomistic%20simulations%20and%20materials%0Adesign.%20In%20addition%2C%20machine%20learning%20approaches%20hold%20the%20potential%20to%20boost%0Athe%20predictive%20power%20of%20computationally%20efficient%20electronic%20structure%20methods%2C%0Asuch%20as%20density%20functional%20theory%2C%20to%20chemical%20accuracy%20and%20to%20correct%20for%0Afundamental%20errors%20in%20density%20functional%20approaches.%20Here%2C%20recent%20progress%20in%0Aapplying%20machine%20learning%20to%20improve%20the%20accuracy%20of%20density%20functional%20and%0Arelated%20approximations%20is%20reviewed.%20Promises%20and%20challenges%20in%20devising%20machine%0Alearning%20models%20transferable%20between%20different%20chemistries%20and%20materials%0Aclasses%20are%20discussed%20with%20the%20help%20of%20examples%20applying%20promising%20models%20to%0Asystems%20far%20outside%20their%20training%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520learning%2520for%2520accuracy%2520in%2520density%2520functional%2520approximations%26entry.906535625%3DJohannes%2520Voss%26entry.1292438233%3D%2520%2520Machine%2520learning%2520techniques%2520have%2520found%2520their%2520way%2520into%2520computational%2520chemistry%250Aas%2520indispensable%2520tools%2520to%2520accelerate%2520atomistic%2520simulations%2520and%2520materials%250Adesign.%2520In%2520addition%252C%2520machine%2520learning%2520approaches%2520hold%2520the%2520potential%2520to%2520boost%250Athe%2520predictive%2520power%2520of%2520computationally%2520efficient%2520electronic%2520structure%2520methods%252C%250Asuch%2520as%2520density%2520functional%2520theory%252C%2520to%2520chemical%2520accuracy%2520and%2520to%2520correct%2520for%250Afundamental%2520errors%2520in%2520density%2520functional%2520approaches.%2520Here%252C%2520recent%2520progress%2520in%250Aapplying%2520machine%2520learning%2520to%2520improve%2520the%2520accuracy%2520of%2520density%2520functional%2520and%250Arelated%2520approximations%2520is%2520reviewed.%2520Promises%2520and%2520challenges%2520in%2520devising%2520machine%250Alearning%2520models%2520transferable%2520between%2520different%2520chemistries%2520and%2520materials%250Aclasses%2520are%2520discussed%2520with%2520the%2520help%2520of%2520examples%2520applying%2520promising%2520models%2520to%250Asystems%2520far%2520outside%2520their%2520training%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20learning%20for%20accuracy%20in%20density%20functional%20approximations&entry.906535625=Johannes%20Voss&entry.1292438233=%20%20Machine%20learning%20techniques%20have%20found%20their%20way%20into%20computational%20chemistry%0Aas%20indispensable%20tools%20to%20accelerate%20atomistic%20simulations%20and%20materials%0Adesign.%20In%20addition%2C%20machine%20learning%20approaches%20hold%20the%20potential%20to%20boost%0Athe%20predictive%20power%20of%20computationally%20efficient%20electronic%20structure%20methods%2C%0Asuch%20as%20density%20functional%20theory%2C%20to%20chemical%20accuracy%20and%20to%20correct%20for%0Afundamental%20errors%20in%20density%20functional%20approaches.%20Here%2C%20recent%20progress%20in%0Aapplying%20machine%20learning%20to%20improve%20the%20accuracy%20of%20density%20functional%20and%0Arelated%20approximations%20is%20reviewed.%20Promises%20and%20challenges%20in%20devising%20machine%0Alearning%20models%20transferable%20between%20different%20chemistries%20and%20materials%0Aclasses%20are%20discussed%20with%20the%20help%20of%20examples%20applying%20promising%20models%20to%0Asystems%20far%20outside%20their%20training%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00196v2&entry.124074799=Read"},
{"title": "Neurosymbolic Association Rule Mining from Tabular Data", "author": "Erkan Karabulut and Paul Groth and Victoria Degeler", "abstract": "  Association Rule Mining (ARM) is the task of mining patterns among data\nfeatures in the form of logical rules, with applications across a myriad of\ndomains. However, high-dimensional datasets often result in an excessive number\nof rules, increasing execution time and negatively impacting downstream task\nperformance. Managing this rule explosion remains a central challenge in ARM\nresearch. To address this, we introduce Aerial+, a novel neurosymbolic ARM\nmethod. Aerial+ leverages an under-complete autoencoder to create a neural\nrepresentation of the data, capturing associations between features. It\nextracts rules from this neural representation by exploiting the model's\nreconstruction mechanism. Extensive evaluations on five datasets against seven\nbaselines demonstrate that Aerial+ achieves state-of-the-art results by\nlearning more concise, high-quality rule sets with full data coverage. When\nintegrated into rule-based interpretable machine learning models, Aerial+\nsignificantly reduces execution time while maintaining or improving accuracy.\n", "link": "http://arxiv.org/abs/2504.19354v3", "date": "2025-10-02", "relevancy": 1.3091, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4467}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4374}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neurosymbolic%20Association%20Rule%20Mining%20from%20Tabular%20Data&body=Title%3A%20Neurosymbolic%20Association%20Rule%20Mining%20from%20Tabular%20Data%0AAuthor%3A%20Erkan%20Karabulut%20and%20Paul%20Groth%20and%20Victoria%20Degeler%0AAbstract%3A%20%20%20Association%20Rule%20Mining%20%28ARM%29%20is%20the%20task%20of%20mining%20patterns%20among%20data%0Afeatures%20in%20the%20form%20of%20logical%20rules%2C%20with%20applications%20across%20a%20myriad%20of%0Adomains.%20However%2C%20high-dimensional%20datasets%20often%20result%20in%20an%20excessive%20number%0Aof%20rules%2C%20increasing%20execution%20time%20and%20negatively%20impacting%20downstream%20task%0Aperformance.%20Managing%20this%20rule%20explosion%20remains%20a%20central%20challenge%20in%20ARM%0Aresearch.%20To%20address%20this%2C%20we%20introduce%20Aerial%2B%2C%20a%20novel%20neurosymbolic%20ARM%0Amethod.%20Aerial%2B%20leverages%20an%20under-complete%20autoencoder%20to%20create%20a%20neural%0Arepresentation%20of%20the%20data%2C%20capturing%20associations%20between%20features.%20It%0Aextracts%20rules%20from%20this%20neural%20representation%20by%20exploiting%20the%20model%27s%0Areconstruction%20mechanism.%20Extensive%20evaluations%20on%20five%20datasets%20against%20seven%0Abaselines%20demonstrate%20that%20Aerial%2B%20achieves%20state-of-the-art%20results%20by%0Alearning%20more%20concise%2C%20high-quality%20rule%20sets%20with%20full%20data%20coverage.%20When%0Aintegrated%20into%20rule-based%20interpretable%20machine%20learning%20models%2C%20Aerial%2B%0Asignificantly%20reduces%20execution%20time%20while%20maintaining%20or%20improving%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19354v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeurosymbolic%2520Association%2520Rule%2520Mining%2520from%2520Tabular%2520Data%26entry.906535625%3DErkan%2520Karabulut%2520and%2520Paul%2520Groth%2520and%2520Victoria%2520Degeler%26entry.1292438233%3D%2520%2520Association%2520Rule%2520Mining%2520%2528ARM%2529%2520is%2520the%2520task%2520of%2520mining%2520patterns%2520among%2520data%250Afeatures%2520in%2520the%2520form%2520of%2520logical%2520rules%252C%2520with%2520applications%2520across%2520a%2520myriad%2520of%250Adomains.%2520However%252C%2520high-dimensional%2520datasets%2520often%2520result%2520in%2520an%2520excessive%2520number%250Aof%2520rules%252C%2520increasing%2520execution%2520time%2520and%2520negatively%2520impacting%2520downstream%2520task%250Aperformance.%2520Managing%2520this%2520rule%2520explosion%2520remains%2520a%2520central%2520challenge%2520in%2520ARM%250Aresearch.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Aerial%252B%252C%2520a%2520novel%2520neurosymbolic%2520ARM%250Amethod.%2520Aerial%252B%2520leverages%2520an%2520under-complete%2520autoencoder%2520to%2520create%2520a%2520neural%250Arepresentation%2520of%2520the%2520data%252C%2520capturing%2520associations%2520between%2520features.%2520It%250Aextracts%2520rules%2520from%2520this%2520neural%2520representation%2520by%2520exploiting%2520the%2520model%2527s%250Areconstruction%2520mechanism.%2520Extensive%2520evaluations%2520on%2520five%2520datasets%2520against%2520seven%250Abaselines%2520demonstrate%2520that%2520Aerial%252B%2520achieves%2520state-of-the-art%2520results%2520by%250Alearning%2520more%2520concise%252C%2520high-quality%2520rule%2520sets%2520with%2520full%2520data%2520coverage.%2520When%250Aintegrated%2520into%2520rule-based%2520interpretable%2520machine%2520learning%2520models%252C%2520Aerial%252B%250Asignificantly%2520reduces%2520execution%2520time%2520while%2520maintaining%2520or%2520improving%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19354v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neurosymbolic%20Association%20Rule%20Mining%20from%20Tabular%20Data&entry.906535625=Erkan%20Karabulut%20and%20Paul%20Groth%20and%20Victoria%20Degeler&entry.1292438233=%20%20Association%20Rule%20Mining%20%28ARM%29%20is%20the%20task%20of%20mining%20patterns%20among%20data%0Afeatures%20in%20the%20form%20of%20logical%20rules%2C%20with%20applications%20across%20a%20myriad%20of%0Adomains.%20However%2C%20high-dimensional%20datasets%20often%20result%20in%20an%20excessive%20number%0Aof%20rules%2C%20increasing%20execution%20time%20and%20negatively%20impacting%20downstream%20task%0Aperformance.%20Managing%20this%20rule%20explosion%20remains%20a%20central%20challenge%20in%20ARM%0Aresearch.%20To%20address%20this%2C%20we%20introduce%20Aerial%2B%2C%20a%20novel%20neurosymbolic%20ARM%0Amethod.%20Aerial%2B%20leverages%20an%20under-complete%20autoencoder%20to%20create%20a%20neural%0Arepresentation%20of%20the%20data%2C%20capturing%20associations%20between%20features.%20It%0Aextracts%20rules%20from%20this%20neural%20representation%20by%20exploiting%20the%20model%27s%0Areconstruction%20mechanism.%20Extensive%20evaluations%20on%20five%20datasets%20against%20seven%0Abaselines%20demonstrate%20that%20Aerial%2B%20achieves%20state-of-the-art%20results%20by%0Alearning%20more%20concise%2C%20high-quality%20rule%20sets%20with%20full%20data%20coverage.%20When%0Aintegrated%20into%20rule-based%20interpretable%20machine%20learning%20models%2C%20Aerial%2B%0Asignificantly%20reduces%20execution%20time%20while%20maintaining%20or%20improving%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19354v3&entry.124074799=Read"},
{"title": "Towards end-to-end ASP computation", "author": "Taisuke Sato and Akihiro Takemura and Katsumi Inoue", "abstract": "  We propose an end-to-end approach for Answer Set Programming (ASP) and linear\nalgebraically compute stable models satisfying given constraints. The idea is\nto implement Lin-Zhao's theorem together with constraints directly in vector\nspaces as numerical minimization of a cost function constructed from a\nmatricized normal logic program, loop formulas in Lin-Zhao's theorem and\nconstraints, thereby no use of symbolic ASP or SAT solvers involved in our\napproach. We also propose precomputation that shrinks the program size and\nheuristics for loop formulas to reduce computational difficulty. We empirically\ntest our approach with programming examples including the 3-coloring and\nHamiltonian cycle problems.\n", "link": "http://arxiv.org/abs/2306.06821v3", "date": "2025-10-02", "relevancy": 1.2453, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4355}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20end-to-end%20ASP%20computation&body=Title%3A%20Towards%20end-to-end%20ASP%20computation%0AAuthor%3A%20Taisuke%20Sato%20and%20Akihiro%20Takemura%20and%20Katsumi%20Inoue%0AAbstract%3A%20%20%20We%20propose%20an%20end-to-end%20approach%20for%20Answer%20Set%20Programming%20%28ASP%29%20and%20linear%0Aalgebraically%20compute%20stable%20models%20satisfying%20given%20constraints.%20The%20idea%20is%0Ato%20implement%20Lin-Zhao%27s%20theorem%20together%20with%20constraints%20directly%20in%20vector%0Aspaces%20as%20numerical%20minimization%20of%20a%20cost%20function%20constructed%20from%20a%0Amatricized%20normal%20logic%20program%2C%20loop%20formulas%20in%20Lin-Zhao%27s%20theorem%20and%0Aconstraints%2C%20thereby%20no%20use%20of%20symbolic%20ASP%20or%20SAT%20solvers%20involved%20in%20our%0Aapproach.%20We%20also%20propose%20precomputation%20that%20shrinks%20the%20program%20size%20and%0Aheuristics%20for%20loop%20formulas%20to%20reduce%20computational%20difficulty.%20We%20empirically%0Atest%20our%20approach%20with%20programming%20examples%20including%20the%203-coloring%20and%0AHamiltonian%20cycle%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520end-to-end%2520ASP%2520computation%26entry.906535625%3DTaisuke%2520Sato%2520and%2520Akihiro%2520Takemura%2520and%2520Katsumi%2520Inoue%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520end-to-end%2520approach%2520for%2520Answer%2520Set%2520Programming%2520%2528ASP%2529%2520and%2520linear%250Aalgebraically%2520compute%2520stable%2520models%2520satisfying%2520given%2520constraints.%2520The%2520idea%2520is%250Ato%2520implement%2520Lin-Zhao%2527s%2520theorem%2520together%2520with%2520constraints%2520directly%2520in%2520vector%250Aspaces%2520as%2520numerical%2520minimization%2520of%2520a%2520cost%2520function%2520constructed%2520from%2520a%250Amatricized%2520normal%2520logic%2520program%252C%2520loop%2520formulas%2520in%2520Lin-Zhao%2527s%2520theorem%2520and%250Aconstraints%252C%2520thereby%2520no%2520use%2520of%2520symbolic%2520ASP%2520or%2520SAT%2520solvers%2520involved%2520in%2520our%250Aapproach.%2520We%2520also%2520propose%2520precomputation%2520that%2520shrinks%2520the%2520program%2520size%2520and%250Aheuristics%2520for%2520loop%2520formulas%2520to%2520reduce%2520computational%2520difficulty.%2520We%2520empirically%250Atest%2520our%2520approach%2520with%2520programming%2520examples%2520including%2520the%25203-coloring%2520and%250AHamiltonian%2520cycle%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20end-to-end%20ASP%20computation&entry.906535625=Taisuke%20Sato%20and%20Akihiro%20Takemura%20and%20Katsumi%20Inoue&entry.1292438233=%20%20We%20propose%20an%20end-to-end%20approach%20for%20Answer%20Set%20Programming%20%28ASP%29%20and%20linear%0Aalgebraically%20compute%20stable%20models%20satisfying%20given%20constraints.%20The%20idea%20is%0Ato%20implement%20Lin-Zhao%27s%20theorem%20together%20with%20constraints%20directly%20in%20vector%0Aspaces%20as%20numerical%20minimization%20of%20a%20cost%20function%20constructed%20from%20a%0Amatricized%20normal%20logic%20program%2C%20loop%20formulas%20in%20Lin-Zhao%27s%20theorem%20and%0Aconstraints%2C%20thereby%20no%20use%20of%20symbolic%20ASP%20or%20SAT%20solvers%20involved%20in%20our%0Aapproach.%20We%20also%20propose%20precomputation%20that%20shrinks%20the%20program%20size%20and%0Aheuristics%20for%20loop%20formulas%20to%20reduce%20computational%20difficulty.%20We%20empirically%0Atest%20our%20approach%20with%20programming%20examples%20including%20the%203-coloring%20and%0AHamiltonian%20cycle%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06821v3&entry.124074799=Read"},
{"title": "Continual Personalization for Diffusion Models", "author": "Yu-Chien Liao and Jr-Jen Chen and Chi-Pin Huang and Ci-Siang Lin and Meng-Lin Wu and Yu-Chiang Frank Wang", "abstract": "  Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.\n", "link": "http://arxiv.org/abs/2510.02296v1", "date": "2025-10-02", "relevancy": 1.1741, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6146}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5796}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Personalization%20for%20Diffusion%20Models&body=Title%3A%20Continual%20Personalization%20for%20Diffusion%20Models%0AAuthor%3A%20Yu-Chien%20Liao%20and%20Jr-Jen%20Chen%20and%20Chi-Pin%20Huang%20and%20Ci-Siang%20Lin%20and%20Meng-Lin%20Wu%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20Updating%20diffusion%20models%20in%20an%20incremental%20setting%20would%20be%20practical%20in%0Areal-world%20applications%20yet%20computationally%20challenging.%20We%20present%20a%20novel%0Alearning%20strategy%20of%20Concept%20Neuron%20Selection%20%28CNS%29%2C%20a%20simple%20yet%20effective%0Aapproach%20to%20perform%20personalization%20in%20a%20continual%20learning%20scheme.%20CNS%0Auniquely%20identifies%20neurons%20in%20diffusion%20models%20that%20are%20closely%20related%20to%20the%0Atarget%20concepts.%20In%20order%20to%20mitigate%20catastrophic%20forgetting%20problems%20while%0Apreserving%20zero-shot%20text-to-image%20generation%20ability%2C%20CNS%20finetunes%20concept%0Aneurons%20in%20an%20incremental%20manner%20and%20jointly%20preserves%20knowledge%20learned%20of%0Aprevious%20concepts.%20Evaluation%20of%20real-world%20datasets%20demonstrates%20that%20CNS%0Aachieves%20state-of-the-art%20performance%20with%20minimal%20parameter%20adjustments%2C%0Aoutperforming%20previous%20methods%20in%20both%20single%20and%20multi-concept%20personalization%0Aworks.%20CNS%20also%20achieves%20fusion-free%20operation%2C%20reducing%20memory%20storage%20and%0Aprocessing%20time%20for%20continual%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Personalization%2520for%2520Diffusion%2520Models%26entry.906535625%3DYu-Chien%2520Liao%2520and%2520Jr-Jen%2520Chen%2520and%2520Chi-Pin%2520Huang%2520and%2520Ci-Siang%2520Lin%2520and%2520Meng-Lin%2520Wu%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3D%2520%2520Updating%2520diffusion%2520models%2520in%2520an%2520incremental%2520setting%2520would%2520be%2520practical%2520in%250Areal-world%2520applications%2520yet%2520computationally%2520challenging.%2520We%2520present%2520a%2520novel%250Alearning%2520strategy%2520of%2520Concept%2520Neuron%2520Selection%2520%2528CNS%2529%252C%2520a%2520simple%2520yet%2520effective%250Aapproach%2520to%2520perform%2520personalization%2520in%2520a%2520continual%2520learning%2520scheme.%2520CNS%250Auniquely%2520identifies%2520neurons%2520in%2520diffusion%2520models%2520that%2520are%2520closely%2520related%2520to%2520the%250Atarget%2520concepts.%2520In%2520order%2520to%2520mitigate%2520catastrophic%2520forgetting%2520problems%2520while%250Apreserving%2520zero-shot%2520text-to-image%2520generation%2520ability%252C%2520CNS%2520finetunes%2520concept%250Aneurons%2520in%2520an%2520incremental%2520manner%2520and%2520jointly%2520preserves%2520knowledge%2520learned%2520of%250Aprevious%2520concepts.%2520Evaluation%2520of%2520real-world%2520datasets%2520demonstrates%2520that%2520CNS%250Aachieves%2520state-of-the-art%2520performance%2520with%2520minimal%2520parameter%2520adjustments%252C%250Aoutperforming%2520previous%2520methods%2520in%2520both%2520single%2520and%2520multi-concept%2520personalization%250Aworks.%2520CNS%2520also%2520achieves%2520fusion-free%2520operation%252C%2520reducing%2520memory%2520storage%2520and%250Aprocessing%2520time%2520for%2520continual%2520personalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Personalization%20for%20Diffusion%20Models&entry.906535625=Yu-Chien%20Liao%20and%20Jr-Jen%20Chen%20and%20Chi-Pin%20Huang%20and%20Ci-Siang%20Lin%20and%20Meng-Lin%20Wu%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20Updating%20diffusion%20models%20in%20an%20incremental%20setting%20would%20be%20practical%20in%0Areal-world%20applications%20yet%20computationally%20challenging.%20We%20present%20a%20novel%0Alearning%20strategy%20of%20Concept%20Neuron%20Selection%20%28CNS%29%2C%20a%20simple%20yet%20effective%0Aapproach%20to%20perform%20personalization%20in%20a%20continual%20learning%20scheme.%20CNS%0Auniquely%20identifies%20neurons%20in%20diffusion%20models%20that%20are%20closely%20related%20to%20the%0Atarget%20concepts.%20In%20order%20to%20mitigate%20catastrophic%20forgetting%20problems%20while%0Apreserving%20zero-shot%20text-to-image%20generation%20ability%2C%20CNS%20finetunes%20concept%0Aneurons%20in%20an%20incremental%20manner%20and%20jointly%20preserves%20knowledge%20learned%20of%0Aprevious%20concepts.%20Evaluation%20of%20real-world%20datasets%20demonstrates%20that%20CNS%0Aachieves%20state-of-the-art%20performance%20with%20minimal%20parameter%20adjustments%2C%0Aoutperforming%20previous%20methods%20in%20both%20single%20and%20multi-concept%20personalization%0Aworks.%20CNS%20also%20achieves%20fusion-free%20operation%2C%20reducing%20memory%20storage%20and%0Aprocessing%20time%20for%20continual%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02296v1&entry.124074799=Read"},
{"title": "Forecasting Generative Amplification", "author": "Henning Bahl and Sascha Diefenbacher and Nina Elmer and Tilman Plehn and Jonas Spinner", "abstract": "  Generative networks are perfect tools to enhance the speed and precision of\nLHC simulations. It is important to understand their statistical precision,\nespecially when generating events beyond the size of the training dataset. We\npresent two complementary methods to estimate the amplification factor without\nlarge holdout datasets. Averaging amplification uses Bayesian networks or\nensembling to estimate amplification from the precision of integrals over given\nphase-space volumes. Differential amplification uses hypothesis testing to\nquantify amplification without any resolution loss. Applied to state-of-the-art\nevent generators, both methods indicate that amplification is possible in\nspecific regions of phase space, but not yet across the entire distribution.\n", "link": "http://arxiv.org/abs/2509.08048v2", "date": "2025-10-02", "relevancy": 1.483, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5033}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4861}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20Generative%20Amplification&body=Title%3A%20Forecasting%20Generative%20Amplification%0AAuthor%3A%20Henning%20Bahl%20and%20Sascha%20Diefenbacher%20and%20Nina%20Elmer%20and%20Tilman%20Plehn%20and%20Jonas%20Spinner%0AAbstract%3A%20%20%20Generative%20networks%20are%20perfect%20tools%20to%20enhance%20the%20speed%20and%20precision%20of%0ALHC%20simulations.%20It%20is%20important%20to%20understand%20their%20statistical%20precision%2C%0Aespecially%20when%20generating%20events%20beyond%20the%20size%20of%20the%20training%20dataset.%20We%0Apresent%20two%20complementary%20methods%20to%20estimate%20the%20amplification%20factor%20without%0Alarge%20holdout%20datasets.%20Averaging%20amplification%20uses%20Bayesian%20networks%20or%0Aensembling%20to%20estimate%20amplification%20from%20the%20precision%20of%20integrals%20over%20given%0Aphase-space%20volumes.%20Differential%20amplification%20uses%20hypothesis%20testing%20to%0Aquantify%20amplification%20without%20any%20resolution%20loss.%20Applied%20to%20state-of-the-art%0Aevent%20generators%2C%20both%20methods%20indicate%20that%20amplification%20is%20possible%20in%0Aspecific%20regions%20of%20phase%20space%2C%20but%20not%20yet%20across%20the%20entire%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520Generative%2520Amplification%26entry.906535625%3DHenning%2520Bahl%2520and%2520Sascha%2520Diefenbacher%2520and%2520Nina%2520Elmer%2520and%2520Tilman%2520Plehn%2520and%2520Jonas%2520Spinner%26entry.1292438233%3D%2520%2520Generative%2520networks%2520are%2520perfect%2520tools%2520to%2520enhance%2520the%2520speed%2520and%2520precision%2520of%250ALHC%2520simulations.%2520It%2520is%2520important%2520to%2520understand%2520their%2520statistical%2520precision%252C%250Aespecially%2520when%2520generating%2520events%2520beyond%2520the%2520size%2520of%2520the%2520training%2520dataset.%2520We%250Apresent%2520two%2520complementary%2520methods%2520to%2520estimate%2520the%2520amplification%2520factor%2520without%250Alarge%2520holdout%2520datasets.%2520Averaging%2520amplification%2520uses%2520Bayesian%2520networks%2520or%250Aensembling%2520to%2520estimate%2520amplification%2520from%2520the%2520precision%2520of%2520integrals%2520over%2520given%250Aphase-space%2520volumes.%2520Differential%2520amplification%2520uses%2520hypothesis%2520testing%2520to%250Aquantify%2520amplification%2520without%2520any%2520resolution%2520loss.%2520Applied%2520to%2520state-of-the-art%250Aevent%2520generators%252C%2520both%2520methods%2520indicate%2520that%2520amplification%2520is%2520possible%2520in%250Aspecific%2520regions%2520of%2520phase%2520space%252C%2520but%2520not%2520yet%2520across%2520the%2520entire%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20Generative%20Amplification&entry.906535625=Henning%20Bahl%20and%20Sascha%20Diefenbacher%20and%20Nina%20Elmer%20and%20Tilman%20Plehn%20and%20Jonas%20Spinner&entry.1292438233=%20%20Generative%20networks%20are%20perfect%20tools%20to%20enhance%20the%20speed%20and%20precision%20of%0ALHC%20simulations.%20It%20is%20important%20to%20understand%20their%20statistical%20precision%2C%0Aespecially%20when%20generating%20events%20beyond%20the%20size%20of%20the%20training%20dataset.%20We%0Apresent%20two%20complementary%20methods%20to%20estimate%20the%20amplification%20factor%20without%0Alarge%20holdout%20datasets.%20Averaging%20amplification%20uses%20Bayesian%20networks%20or%0Aensembling%20to%20estimate%20amplification%20from%20the%20precision%20of%20integrals%20over%20given%0Aphase-space%20volumes.%20Differential%20amplification%20uses%20hypothesis%20testing%20to%0Aquantify%20amplification%20without%20any%20resolution%20loss.%20Applied%20to%20state-of-the-art%0Aevent%20generators%2C%20both%20methods%20indicate%20that%20amplification%20is%20possible%20in%0Aspecific%20regions%20of%20phase%20space%2C%20but%20not%20yet%20across%20the%20entire%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08048v2&entry.124074799=Read"},
{"title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use", "author": "Gonzalo Gonzalez-Pumariega and Vincent Tu and Chih-Lun Lee and Jiachen Yang and Ang Li and Xin Eric Wang", "abstract": "  Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.\n", "link": "http://arxiv.org/abs/2510.02250v1", "date": "2025-10-02", "relevancy": 1.4761, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4971}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Unreasonable%20Effectiveness%20of%20Scaling%20Agents%20for%20Computer%20Use&body=Title%3A%20The%20Unreasonable%20Effectiveness%20of%20Scaling%20Agents%20for%20Computer%20Use%0AAuthor%3A%20Gonzalo%20Gonzalez-Pumariega%20and%20Vincent%20Tu%20and%20Chih-Lun%20Lee%20and%20Jiachen%20Yang%20and%20Ang%20Li%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20Computer-use%20agents%20%28CUAs%29%20hold%20promise%20for%20automating%20everyday%20digital%0Atasks%2C%20but%20their%20unreliability%20and%20high%20variance%20hinder%20their%20application%20to%0Along-horizon%2C%20complex%20tasks.%20We%20introduce%20Behavior%20Best-of-N%20%28bBoN%29%2C%20a%20method%0Athat%20scales%20over%20agents%20by%20generating%20multiple%20rollouts%20and%20selecting%20among%0Athem%20using%20behavior%20narratives%20that%20describe%20the%20agents%27%20rollouts.%20It%20enables%0Aboth%20wide%20exploration%20and%20principled%20trajectory%20selection%2C%20substantially%0Aimproving%20robustness%20and%20success%20rates.%20On%20OSWorld%2C%20our%20bBoN%20scaling%20method%0Aestablishes%20a%20new%20state%20of%20the%20art%20%28SoTA%29%20at%2069.9%25%2C%20significantly%20outperforming%0Aprior%20methods%20and%20approaching%20human-level%20performance%20at%2072%25%2C%20with%0Acomprehensive%20ablations%20validating%20key%20design%20choices.%20We%20further%20demonstrate%0Astrong%20generalization%20results%20to%20different%20operating%20systems%20on%0AWindowsAgentArena%20and%20AndroidWorld.%20Crucially%2C%20our%20results%20highlight%20the%0Aunreasonable%20effectiveness%20of%20scaling%20CUAs%2C%20when%20you%20do%20it%20right%3A%20effective%0Ascaling%20requires%20structured%20trajectory%20understanding%20and%20selection%2C%20and%20bBoN%0Aprovides%20a%20practical%20framework%20to%20achieve%20this.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Unreasonable%2520Effectiveness%2520of%2520Scaling%2520Agents%2520for%2520Computer%2520Use%26entry.906535625%3DGonzalo%2520Gonzalez-Pumariega%2520and%2520Vincent%2520Tu%2520and%2520Chih-Lun%2520Lee%2520and%2520Jiachen%2520Yang%2520and%2520Ang%2520Li%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520Computer-use%2520agents%2520%2528CUAs%2529%2520hold%2520promise%2520for%2520automating%2520everyday%2520digital%250Atasks%252C%2520but%2520their%2520unreliability%2520and%2520high%2520variance%2520hinder%2520their%2520application%2520to%250Along-horizon%252C%2520complex%2520tasks.%2520We%2520introduce%2520Behavior%2520Best-of-N%2520%2528bBoN%2529%252C%2520a%2520method%250Athat%2520scales%2520over%2520agents%2520by%2520generating%2520multiple%2520rollouts%2520and%2520selecting%2520among%250Athem%2520using%2520behavior%2520narratives%2520that%2520describe%2520the%2520agents%2527%2520rollouts.%2520It%2520enables%250Aboth%2520wide%2520exploration%2520and%2520principled%2520trajectory%2520selection%252C%2520substantially%250Aimproving%2520robustness%2520and%2520success%2520rates.%2520On%2520OSWorld%252C%2520our%2520bBoN%2520scaling%2520method%250Aestablishes%2520a%2520new%2520state%2520of%2520the%2520art%2520%2528SoTA%2529%2520at%252069.9%2525%252C%2520significantly%2520outperforming%250Aprior%2520methods%2520and%2520approaching%2520human-level%2520performance%2520at%252072%2525%252C%2520with%250Acomprehensive%2520ablations%2520validating%2520key%2520design%2520choices.%2520We%2520further%2520demonstrate%250Astrong%2520generalization%2520results%2520to%2520different%2520operating%2520systems%2520on%250AWindowsAgentArena%2520and%2520AndroidWorld.%2520Crucially%252C%2520our%2520results%2520highlight%2520the%250Aunreasonable%2520effectiveness%2520of%2520scaling%2520CUAs%252C%2520when%2520you%2520do%2520it%2520right%253A%2520effective%250Ascaling%2520requires%2520structured%2520trajectory%2520understanding%2520and%2520selection%252C%2520and%2520bBoN%250Aprovides%2520a%2520practical%2520framework%2520to%2520achieve%2520this.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unreasonable%20Effectiveness%20of%20Scaling%20Agents%20for%20Computer%20Use&entry.906535625=Gonzalo%20Gonzalez-Pumariega%20and%20Vincent%20Tu%20and%20Chih-Lun%20Lee%20and%20Jiachen%20Yang%20and%20Ang%20Li%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20Computer-use%20agents%20%28CUAs%29%20hold%20promise%20for%20automating%20everyday%20digital%0Atasks%2C%20but%20their%20unreliability%20and%20high%20variance%20hinder%20their%20application%20to%0Along-horizon%2C%20complex%20tasks.%20We%20introduce%20Behavior%20Best-of-N%20%28bBoN%29%2C%20a%20method%0Athat%20scales%20over%20agents%20by%20generating%20multiple%20rollouts%20and%20selecting%20among%0Athem%20using%20behavior%20narratives%20that%20describe%20the%20agents%27%20rollouts.%20It%20enables%0Aboth%20wide%20exploration%20and%20principled%20trajectory%20selection%2C%20substantially%0Aimproving%20robustness%20and%20success%20rates.%20On%20OSWorld%2C%20our%20bBoN%20scaling%20method%0Aestablishes%20a%20new%20state%20of%20the%20art%20%28SoTA%29%20at%2069.9%25%2C%20significantly%20outperforming%0Aprior%20methods%20and%20approaching%20human-level%20performance%20at%2072%25%2C%20with%0Acomprehensive%20ablations%20validating%20key%20design%20choices.%20We%20further%20demonstrate%0Astrong%20generalization%20results%20to%20different%20operating%20systems%20on%0AWindowsAgentArena%20and%20AndroidWorld.%20Crucially%2C%20our%20results%20highlight%20the%0Aunreasonable%20effectiveness%20of%20scaling%20CUAs%2C%20when%20you%20do%20it%20right%3A%20effective%0Ascaling%20requires%20structured%20trajectory%20understanding%20and%20selection%2C%20and%20bBoN%0Aprovides%20a%20practical%20framework%20to%20achieve%20this.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02250v1&entry.124074799=Read"},
{"title": "Efficiently Generating Correlated Sample Paths from Multi-step Time\n  Series Foundation Models", "author": "Ethan Baron and Boris Oreshkin and Ruijun Ma and Hanyu Zhang and Kari Torkkola and Michael W. Mahoney and Andrew Gordon Wilson and Tatiana Konstantinova", "abstract": "  Many time series applications require access to multi-step forecast\ntrajectories in the form of sample paths. Recently, time series foundation\nmodels have leveraged multi-step lookahead predictions to improve the quality\nand efficiency of multi-step forecasts. However, these models only predict\nindependent marginal distributions for each time step, rather than a full joint\npredictive distribution. To generate forecast sample paths with realistic\ncorrelation structures, one typically resorts to autoregressive sampling, which\ncan be extremely expensive. In this paper, we present a copula-based approach\nto efficiently generate accurate, correlated sample paths from existing\nmulti-step time series foundation models in one forward pass. Our copula-based\napproach generates correlated sample paths orders of magnitude faster than\nautoregressive sampling, and it yields improved sample path quality by\nmitigating the snowballing error phenomenon.\n", "link": "http://arxiv.org/abs/2510.02224v1", "date": "2025-10-02", "relevancy": 0.8783, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4496}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4398}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Generating%20Correlated%20Sample%20Paths%20from%20Multi-step%20Time%0A%20%20Series%20Foundation%20Models&body=Title%3A%20Efficiently%20Generating%20Correlated%20Sample%20Paths%20from%20Multi-step%20Time%0A%20%20Series%20Foundation%20Models%0AAuthor%3A%20Ethan%20Baron%20and%20Boris%20Oreshkin%20and%20Ruijun%20Ma%20and%20Hanyu%20Zhang%20and%20Kari%20Torkkola%20and%20Michael%20W.%20Mahoney%20and%20Andrew%20Gordon%20Wilson%20and%20Tatiana%20Konstantinova%0AAbstract%3A%20%20%20Many%20time%20series%20applications%20require%20access%20to%20multi-step%20forecast%0Atrajectories%20in%20the%20form%20of%20sample%20paths.%20Recently%2C%20time%20series%20foundation%0Amodels%20have%20leveraged%20multi-step%20lookahead%20predictions%20to%20improve%20the%20quality%0Aand%20efficiency%20of%20multi-step%20forecasts.%20However%2C%20these%20models%20only%20predict%0Aindependent%20marginal%20distributions%20for%20each%20time%20step%2C%20rather%20than%20a%20full%20joint%0Apredictive%20distribution.%20To%20generate%20forecast%20sample%20paths%20with%20realistic%0Acorrelation%20structures%2C%20one%20typically%20resorts%20to%20autoregressive%20sampling%2C%20which%0Acan%20be%20extremely%20expensive.%20In%20this%20paper%2C%20we%20present%20a%20copula-based%20approach%0Ato%20efficiently%20generate%20accurate%2C%20correlated%20sample%20paths%20from%20existing%0Amulti-step%20time%20series%20foundation%20models%20in%20one%20forward%20pass.%20Our%20copula-based%0Aapproach%20generates%20correlated%20sample%20paths%20orders%20of%20magnitude%20faster%20than%0Aautoregressive%20sampling%2C%20and%20it%20yields%20improved%20sample%20path%20quality%20by%0Amitigating%20the%20snowballing%20error%20phenomenon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Generating%2520Correlated%2520Sample%2520Paths%2520from%2520Multi-step%2520Time%250A%2520%2520Series%2520Foundation%2520Models%26entry.906535625%3DEthan%2520Baron%2520and%2520Boris%2520Oreshkin%2520and%2520Ruijun%2520Ma%2520and%2520Hanyu%2520Zhang%2520and%2520Kari%2520Torkkola%2520and%2520Michael%2520W.%2520Mahoney%2520and%2520Andrew%2520Gordon%2520Wilson%2520and%2520Tatiana%2520Konstantinova%26entry.1292438233%3D%2520%2520Many%2520time%2520series%2520applications%2520require%2520access%2520to%2520multi-step%2520forecast%250Atrajectories%2520in%2520the%2520form%2520of%2520sample%2520paths.%2520Recently%252C%2520time%2520series%2520foundation%250Amodels%2520have%2520leveraged%2520multi-step%2520lookahead%2520predictions%2520to%2520improve%2520the%2520quality%250Aand%2520efficiency%2520of%2520multi-step%2520forecasts.%2520However%252C%2520these%2520models%2520only%2520predict%250Aindependent%2520marginal%2520distributions%2520for%2520each%2520time%2520step%252C%2520rather%2520than%2520a%2520full%2520joint%250Apredictive%2520distribution.%2520To%2520generate%2520forecast%2520sample%2520paths%2520with%2520realistic%250Acorrelation%2520structures%252C%2520one%2520typically%2520resorts%2520to%2520autoregressive%2520sampling%252C%2520which%250Acan%2520be%2520extremely%2520expensive.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520copula-based%2520approach%250Ato%2520efficiently%2520generate%2520accurate%252C%2520correlated%2520sample%2520paths%2520from%2520existing%250Amulti-step%2520time%2520series%2520foundation%2520models%2520in%2520one%2520forward%2520pass.%2520Our%2520copula-based%250Aapproach%2520generates%2520correlated%2520sample%2520paths%2520orders%2520of%2520magnitude%2520faster%2520than%250Aautoregressive%2520sampling%252C%2520and%2520it%2520yields%2520improved%2520sample%2520path%2520quality%2520by%250Amitigating%2520the%2520snowballing%2520error%2520phenomenon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Generating%20Correlated%20Sample%20Paths%20from%20Multi-step%20Time%0A%20%20Series%20Foundation%20Models&entry.906535625=Ethan%20Baron%20and%20Boris%20Oreshkin%20and%20Ruijun%20Ma%20and%20Hanyu%20Zhang%20and%20Kari%20Torkkola%20and%20Michael%20W.%20Mahoney%20and%20Andrew%20Gordon%20Wilson%20and%20Tatiana%20Konstantinova&entry.1292438233=%20%20Many%20time%20series%20applications%20require%20access%20to%20multi-step%20forecast%0Atrajectories%20in%20the%20form%20of%20sample%20paths.%20Recently%2C%20time%20series%20foundation%0Amodels%20have%20leveraged%20multi-step%20lookahead%20predictions%20to%20improve%20the%20quality%0Aand%20efficiency%20of%20multi-step%20forecasts.%20However%2C%20these%20models%20only%20predict%0Aindependent%20marginal%20distributions%20for%20each%20time%20step%2C%20rather%20than%20a%20full%20joint%0Apredictive%20distribution.%20To%20generate%20forecast%20sample%20paths%20with%20realistic%0Acorrelation%20structures%2C%20one%20typically%20resorts%20to%20autoregressive%20sampling%2C%20which%0Acan%20be%20extremely%20expensive.%20In%20this%20paper%2C%20we%20present%20a%20copula-based%20approach%0Ato%20efficiently%20generate%20accurate%2C%20correlated%20sample%20paths%20from%20existing%0Amulti-step%20time%20series%20foundation%20models%20in%20one%20forward%20pass.%20Our%20copula-based%0Aapproach%20generates%20correlated%20sample%20paths%20orders%20of%20magnitude%20faster%20than%0Aautoregressive%20sampling%2C%20and%20it%20yields%20improved%20sample%20path%20quality%20by%0Amitigating%20the%20snowballing%20error%20phenomenon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02224v1&entry.124074799=Read"},
{"title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed\n  Slice Mobility Attacks", "author": "Ricardo Misael Ayala Molina and Hyame Assem Alameddine and Makan Pourzandi and Chadi Assi", "abstract": "  Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.\n", "link": "http://arxiv.org/abs/2510.02236v1", "date": "2025-10-02", "relevancy": 1.4522, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4885}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4829}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PUL-Inter-slice%20Defender%3A%20An%20Anomaly%20Detection%20Solution%20for%20Distributed%0A%20%20Slice%20Mobility%20Attacks&body=Title%3A%20PUL-Inter-slice%20Defender%3A%20An%20Anomaly%20Detection%20Solution%20for%20Distributed%0A%20%20Slice%20Mobility%20Attacks%0AAuthor%3A%20Ricardo%20Misael%20Ayala%20Molina%20and%20Hyame%20Assem%20Alameddine%20and%20Makan%20Pourzandi%20and%20Chadi%20Assi%0AAbstract%3A%20%20%20Network%20Slices%20%28NSs%29%20are%20virtual%20networks%20operating%20over%20a%20shared%20physical%0Ainfrastructure%2C%20each%20designed%20to%20meet%20specific%20application%20requirements%20while%0Amaintaining%20consistent%20Quality%20of%20Service%20%28QoS%29.%20In%20Fifth%20Generation%20%285G%29%0Anetworks%2C%20User%20Equipment%20%28UE%29%20can%20connect%20to%20and%20seamlessly%20switch%20between%0Amultiple%20NSs%20to%20access%20diverse%20services.%20However%2C%20this%20flexibility%2C%20known%20as%0AInter-Slice%20Switching%20%28ISS%29%2C%20introduces%20a%20potential%20vulnerability%20that%20can%20be%0Aexploited%20to%20launch%20Distributed%20Slice%20Mobility%20%28DSM%29%20attacks%2C%20a%20form%20of%0ADistributed%20Denial%20of%20Service%20%28DDoS%29%20attack.%20To%20secure%205G%20networks%20and%20their%0ANSs%20against%20DSM%20attacks%2C%20we%20present%20in%20this%20work%2C%20PUL-Inter-Slice%20Defender%3B%20an%0Aanomaly%20detection%20solution%20that%20leverages%20Positive%20Unlabeled%20Learning%20%28PUL%29%20and%0Aincorporates%20a%20combination%20of%20Long%20Short-Term%20Memory%20Autoencoders%20and%20K-Means%0Aclustering.%20PUL-Inter-Slice%20Defender%20leverages%20the%20Third%20Generation%20Partnership%0AProject%20%283GPP%29%20key%20performance%20indicators%20and%20performance%20measurement%20counters%0Aas%20features%20for%20its%20machine%20learning%20models%20to%20detect%20DSM%20attack%20variants%20while%0Amaintaining%20robustness%20in%20the%20presence%20of%20contaminated%20training%20data.%20When%0Aevaluated%20on%20data%20collected%20from%20our%205G%20testbed%20based%20on%20the%20open-source%0Afree5GC%20and%20UERANSIM%2C%20a%20UE/%20Radio%20Access%20Network%20%28RAN%29%20simulator%3B%0APUL-Inter-Slice%20Defender%20achieved%20F1-scores%20exceeding%2098.50%25%20on%20training%0Adatasets%20with%2010%25%20to%2040%25%20attack%20contamination%2C%20consistently%20outperforming%20its%0Acounterpart%20Inter-Slice%20Defender%20and%20other%20PUL%20based%20solutions%20combining%0AOne-Class%20Support%20Vector%20Machine%20%28OCSVM%29%20with%20Random%20Forest%20and%20XGBoost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPUL-Inter-slice%2520Defender%253A%2520An%2520Anomaly%2520Detection%2520Solution%2520for%2520Distributed%250A%2520%2520Slice%2520Mobility%2520Attacks%26entry.906535625%3DRicardo%2520Misael%2520Ayala%2520Molina%2520and%2520Hyame%2520Assem%2520Alameddine%2520and%2520Makan%2520Pourzandi%2520and%2520Chadi%2520Assi%26entry.1292438233%3D%2520%2520Network%2520Slices%2520%2528NSs%2529%2520are%2520virtual%2520networks%2520operating%2520over%2520a%2520shared%2520physical%250Ainfrastructure%252C%2520each%2520designed%2520to%2520meet%2520specific%2520application%2520requirements%2520while%250Amaintaining%2520consistent%2520Quality%2520of%2520Service%2520%2528QoS%2529.%2520In%2520Fifth%2520Generation%2520%25285G%2529%250Anetworks%252C%2520User%2520Equipment%2520%2528UE%2529%2520can%2520connect%2520to%2520and%2520seamlessly%2520switch%2520between%250Amultiple%2520NSs%2520to%2520access%2520diverse%2520services.%2520However%252C%2520this%2520flexibility%252C%2520known%2520as%250AInter-Slice%2520Switching%2520%2528ISS%2529%252C%2520introduces%2520a%2520potential%2520vulnerability%2520that%2520can%2520be%250Aexploited%2520to%2520launch%2520Distributed%2520Slice%2520Mobility%2520%2528DSM%2529%2520attacks%252C%2520a%2520form%2520of%250ADistributed%2520Denial%2520of%2520Service%2520%2528DDoS%2529%2520attack.%2520To%2520secure%25205G%2520networks%2520and%2520their%250ANSs%2520against%2520DSM%2520attacks%252C%2520we%2520present%2520in%2520this%2520work%252C%2520PUL-Inter-Slice%2520Defender%253B%2520an%250Aanomaly%2520detection%2520solution%2520that%2520leverages%2520Positive%2520Unlabeled%2520Learning%2520%2528PUL%2529%2520and%250Aincorporates%2520a%2520combination%2520of%2520Long%2520Short-Term%2520Memory%2520Autoencoders%2520and%2520K-Means%250Aclustering.%2520PUL-Inter-Slice%2520Defender%2520leverages%2520the%2520Third%2520Generation%2520Partnership%250AProject%2520%25283GPP%2529%2520key%2520performance%2520indicators%2520and%2520performance%2520measurement%2520counters%250Aas%2520features%2520for%2520its%2520machine%2520learning%2520models%2520to%2520detect%2520DSM%2520attack%2520variants%2520while%250Amaintaining%2520robustness%2520in%2520the%2520presence%2520of%2520contaminated%2520training%2520data.%2520When%250Aevaluated%2520on%2520data%2520collected%2520from%2520our%25205G%2520testbed%2520based%2520on%2520the%2520open-source%250Afree5GC%2520and%2520UERANSIM%252C%2520a%2520UE/%2520Radio%2520Access%2520Network%2520%2528RAN%2529%2520simulator%253B%250APUL-Inter-Slice%2520Defender%2520achieved%2520F1-scores%2520exceeding%252098.50%2525%2520on%2520training%250Adatasets%2520with%252010%2525%2520to%252040%2525%2520attack%2520contamination%252C%2520consistently%2520outperforming%2520its%250Acounterpart%2520Inter-Slice%2520Defender%2520and%2520other%2520PUL%2520based%2520solutions%2520combining%250AOne-Class%2520Support%2520Vector%2520Machine%2520%2528OCSVM%2529%2520with%2520Random%2520Forest%2520and%2520XGBoost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PUL-Inter-slice%20Defender%3A%20An%20Anomaly%20Detection%20Solution%20for%20Distributed%0A%20%20Slice%20Mobility%20Attacks&entry.906535625=Ricardo%20Misael%20Ayala%20Molina%20and%20Hyame%20Assem%20Alameddine%20and%20Makan%20Pourzandi%20and%20Chadi%20Assi&entry.1292438233=%20%20Network%20Slices%20%28NSs%29%20are%20virtual%20networks%20operating%20over%20a%20shared%20physical%0Ainfrastructure%2C%20each%20designed%20to%20meet%20specific%20application%20requirements%20while%0Amaintaining%20consistent%20Quality%20of%20Service%20%28QoS%29.%20In%20Fifth%20Generation%20%285G%29%0Anetworks%2C%20User%20Equipment%20%28UE%29%20can%20connect%20to%20and%20seamlessly%20switch%20between%0Amultiple%20NSs%20to%20access%20diverse%20services.%20However%2C%20this%20flexibility%2C%20known%20as%0AInter-Slice%20Switching%20%28ISS%29%2C%20introduces%20a%20potential%20vulnerability%20that%20can%20be%0Aexploited%20to%20launch%20Distributed%20Slice%20Mobility%20%28DSM%29%20attacks%2C%20a%20form%20of%0ADistributed%20Denial%20of%20Service%20%28DDoS%29%20attack.%20To%20secure%205G%20networks%20and%20their%0ANSs%20against%20DSM%20attacks%2C%20we%20present%20in%20this%20work%2C%20PUL-Inter-Slice%20Defender%3B%20an%0Aanomaly%20detection%20solution%20that%20leverages%20Positive%20Unlabeled%20Learning%20%28PUL%29%20and%0Aincorporates%20a%20combination%20of%20Long%20Short-Term%20Memory%20Autoencoders%20and%20K-Means%0Aclustering.%20PUL-Inter-Slice%20Defender%20leverages%20the%20Third%20Generation%20Partnership%0AProject%20%283GPP%29%20key%20performance%20indicators%20and%20performance%20measurement%20counters%0Aas%20features%20for%20its%20machine%20learning%20models%20to%20detect%20DSM%20attack%20variants%20while%0Amaintaining%20robustness%20in%20the%20presence%20of%20contaminated%20training%20data.%20When%0Aevaluated%20on%20data%20collected%20from%20our%205G%20testbed%20based%20on%20the%20open-source%0Afree5GC%20and%20UERANSIM%2C%20a%20UE/%20Radio%20Access%20Network%20%28RAN%29%20simulator%3B%0APUL-Inter-Slice%20Defender%20achieved%20F1-scores%20exceeding%2098.50%25%20on%20training%0Adatasets%20with%2010%25%20to%2040%25%20attack%20contamination%2C%20consistently%20outperforming%20its%0Acounterpart%20Inter-Slice%20Defender%20and%20other%20PUL%20based%20solutions%20combining%0AOne-Class%20Support%20Vector%20Machine%20%28OCSVM%29%20with%20Random%20Forest%20and%20XGBoost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02236v1&entry.124074799=Read"},
{"title": "Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure\n  Risk at Record 2.9-Million Observation Scale", "author": "Boris Kriuk", "abstract": "  Arctic warming threatens over 100 billion in permafrost-dependent\ninfrastructure across Northern territories, yet existing risk assessment\nframeworks lack spatiotemporal validation, uncertainty quantification, and\noperational decision-support capabilities. We present a hybrid physics-machine\nlearning framework integrating 2.9 million observations from 171,605 locations\n(2005-2021) combining permafrost fraction data with climate reanalysis. Our\nstacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic\nNet) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal\ncross-validation preventing data leakage. To address machine learning\nlimitations in extrapolative climate scenarios, we develop a hybrid approach\ncombining learned climate-permafrost relationships (60%) with physical\npermafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over\n10 years), we project mean permafrost fraction decline of -20.3 pp (median:\n-20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point\nloss. Infrastructure risk classification identifies 15% high-risk zones (25%\nmedium-risk) with spatially explicit uncertainty maps. Our framework represents\nthe largest validated permafrost ML dataset globally, provides the first\noperational hybrid physics-ML forecasting system for Arctic infrastructure, and\ndelivers open-source tools enabling probabilistic permafrost projections for\nengineering design codes and climate adaptation planning. The methodology is\ngeneralizable to other permafrost regions and demonstrates how hybrid\napproaches can overcome pure data-driven limitations in climate change\napplications.\n", "link": "http://arxiv.org/abs/2510.02189v1", "date": "2025-10-02", "relevancy": 1.473, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5017}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Physics-ML%20Framework%20for%20Pan-Arctic%20Permafrost%20Infrastructure%0A%20%20Risk%20at%20Record%202.9-Million%20Observation%20Scale&body=Title%3A%20Hybrid%20Physics-ML%20Framework%20for%20Pan-Arctic%20Permafrost%20Infrastructure%0A%20%20Risk%20at%20Record%202.9-Million%20Observation%20Scale%0AAuthor%3A%20Boris%20Kriuk%0AAbstract%3A%20%20%20Arctic%20warming%20threatens%20over%20100%20billion%20in%20permafrost-dependent%0Ainfrastructure%20across%20Northern%20territories%2C%20yet%20existing%20risk%20assessment%0Aframeworks%20lack%20spatiotemporal%20validation%2C%20uncertainty%20quantification%2C%20and%0Aoperational%20decision-support%20capabilities.%20We%20present%20a%20hybrid%20physics-machine%0Alearning%20framework%20integrating%202.9%20million%20observations%20from%20171%2C605%20locations%0A%282005-2021%29%20combining%20permafrost%20fraction%20data%20with%20climate%20reanalysis.%20Our%0Astacked%20ensemble%20model%20%28Random%20Forest%20%2B%20Histogram%20Gradient%20Boosting%20%2B%20Elastic%0ANet%29%20achieves%20R2%3D0.980%20%28RMSE%3D5.01%20pp%29%20with%20rigorous%20spatiotemporal%0Across-validation%20preventing%20data%20leakage.%20To%20address%20machine%20learning%0Alimitations%20in%20extrapolative%20climate%20scenarios%2C%20we%20develop%20a%20hybrid%20approach%0Acombining%20learned%20climate-permafrost%20relationships%20%2860%25%29%20with%20physical%0Apermafrost%20sensitivity%20models%20%2840%25%2C%20-10%20pp/C%29.%20Under%20RCP8.5%20forcing%20%28%2B5C%20over%0A10%20years%29%2C%20we%20project%20mean%20permafrost%20fraction%20decline%20of%20-20.3%20pp%20%28median%3A%0A-20.0%20pp%29%2C%20with%2051.5%25%20of%20Arctic%20Russia%20experiencing%20over%2020%20percentage%20point%0Aloss.%20Infrastructure%20risk%20classification%20identifies%2015%25%20high-risk%20zones%20%2825%25%0Amedium-risk%29%20with%20spatially%20explicit%20uncertainty%20maps.%20Our%20framework%20represents%0Athe%20largest%20validated%20permafrost%20ML%20dataset%20globally%2C%20provides%20the%20first%0Aoperational%20hybrid%20physics-ML%20forecasting%20system%20for%20Arctic%20infrastructure%2C%20and%0Adelivers%20open-source%20tools%20enabling%20probabilistic%20permafrost%20projections%20for%0Aengineering%20design%20codes%20and%20climate%20adaptation%20planning.%20The%20methodology%20is%0Ageneralizable%20to%20other%20permafrost%20regions%20and%20demonstrates%20how%20hybrid%0Aapproaches%20can%20overcome%20pure%20data-driven%20limitations%20in%20climate%20change%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Physics-ML%2520Framework%2520for%2520Pan-Arctic%2520Permafrost%2520Infrastructure%250A%2520%2520Risk%2520at%2520Record%25202.9-Million%2520Observation%2520Scale%26entry.906535625%3DBoris%2520Kriuk%26entry.1292438233%3D%2520%2520Arctic%2520warming%2520threatens%2520over%2520100%2520billion%2520in%2520permafrost-dependent%250Ainfrastructure%2520across%2520Northern%2520territories%252C%2520yet%2520existing%2520risk%2520assessment%250Aframeworks%2520lack%2520spatiotemporal%2520validation%252C%2520uncertainty%2520quantification%252C%2520and%250Aoperational%2520decision-support%2520capabilities.%2520We%2520present%2520a%2520hybrid%2520physics-machine%250Alearning%2520framework%2520integrating%25202.9%2520million%2520observations%2520from%2520171%252C605%2520locations%250A%25282005-2021%2529%2520combining%2520permafrost%2520fraction%2520data%2520with%2520climate%2520reanalysis.%2520Our%250Astacked%2520ensemble%2520model%2520%2528Random%2520Forest%2520%252B%2520Histogram%2520Gradient%2520Boosting%2520%252B%2520Elastic%250ANet%2529%2520achieves%2520R2%253D0.980%2520%2528RMSE%253D5.01%2520pp%2529%2520with%2520rigorous%2520spatiotemporal%250Across-validation%2520preventing%2520data%2520leakage.%2520To%2520address%2520machine%2520learning%250Alimitations%2520in%2520extrapolative%2520climate%2520scenarios%252C%2520we%2520develop%2520a%2520hybrid%2520approach%250Acombining%2520learned%2520climate-permafrost%2520relationships%2520%252860%2525%2529%2520with%2520physical%250Apermafrost%2520sensitivity%2520models%2520%252840%2525%252C%2520-10%2520pp/C%2529.%2520Under%2520RCP8.5%2520forcing%2520%2528%252B5C%2520over%250A10%2520years%2529%252C%2520we%2520project%2520mean%2520permafrost%2520fraction%2520decline%2520of%2520-20.3%2520pp%2520%2528median%253A%250A-20.0%2520pp%2529%252C%2520with%252051.5%2525%2520of%2520Arctic%2520Russia%2520experiencing%2520over%252020%2520percentage%2520point%250Aloss.%2520Infrastructure%2520risk%2520classification%2520identifies%252015%2525%2520high-risk%2520zones%2520%252825%2525%250Amedium-risk%2529%2520with%2520spatially%2520explicit%2520uncertainty%2520maps.%2520Our%2520framework%2520represents%250Athe%2520largest%2520validated%2520permafrost%2520ML%2520dataset%2520globally%252C%2520provides%2520the%2520first%250Aoperational%2520hybrid%2520physics-ML%2520forecasting%2520system%2520for%2520Arctic%2520infrastructure%252C%2520and%250Adelivers%2520open-source%2520tools%2520enabling%2520probabilistic%2520permafrost%2520projections%2520for%250Aengineering%2520design%2520codes%2520and%2520climate%2520adaptation%2520planning.%2520The%2520methodology%2520is%250Ageneralizable%2520to%2520other%2520permafrost%2520regions%2520and%2520demonstrates%2520how%2520hybrid%250Aapproaches%2520can%2520overcome%2520pure%2520data-driven%2520limitations%2520in%2520climate%2520change%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Physics-ML%20Framework%20for%20Pan-Arctic%20Permafrost%20Infrastructure%0A%20%20Risk%20at%20Record%202.9-Million%20Observation%20Scale&entry.906535625=Boris%20Kriuk&entry.1292438233=%20%20Arctic%20warming%20threatens%20over%20100%20billion%20in%20permafrost-dependent%0Ainfrastructure%20across%20Northern%20territories%2C%20yet%20existing%20risk%20assessment%0Aframeworks%20lack%20spatiotemporal%20validation%2C%20uncertainty%20quantification%2C%20and%0Aoperational%20decision-support%20capabilities.%20We%20present%20a%20hybrid%20physics-machine%0Alearning%20framework%20integrating%202.9%20million%20observations%20from%20171%2C605%20locations%0A%282005-2021%29%20combining%20permafrost%20fraction%20data%20with%20climate%20reanalysis.%20Our%0Astacked%20ensemble%20model%20%28Random%20Forest%20%2B%20Histogram%20Gradient%20Boosting%20%2B%20Elastic%0ANet%29%20achieves%20R2%3D0.980%20%28RMSE%3D5.01%20pp%29%20with%20rigorous%20spatiotemporal%0Across-validation%20preventing%20data%20leakage.%20To%20address%20machine%20learning%0Alimitations%20in%20extrapolative%20climate%20scenarios%2C%20we%20develop%20a%20hybrid%20approach%0Acombining%20learned%20climate-permafrost%20relationships%20%2860%25%29%20with%20physical%0Apermafrost%20sensitivity%20models%20%2840%25%2C%20-10%20pp/C%29.%20Under%20RCP8.5%20forcing%20%28%2B5C%20over%0A10%20years%29%2C%20we%20project%20mean%20permafrost%20fraction%20decline%20of%20-20.3%20pp%20%28median%3A%0A-20.0%20pp%29%2C%20with%2051.5%25%20of%20Arctic%20Russia%20experiencing%20over%2020%20percentage%20point%0Aloss.%20Infrastructure%20risk%20classification%20identifies%2015%25%20high-risk%20zones%20%2825%25%0Amedium-risk%29%20with%20spatially%20explicit%20uncertainty%20maps.%20Our%20framework%20represents%0Athe%20largest%20validated%20permafrost%20ML%20dataset%20globally%2C%20provides%20the%20first%0Aoperational%20hybrid%20physics-ML%20forecasting%20system%20for%20Arctic%20infrastructure%2C%20and%0Adelivers%20open-source%20tools%20enabling%20probabilistic%20permafrost%20projections%20for%0Aengineering%20design%20codes%20and%20climate%20adaptation%20planning.%20The%20methodology%20is%0Ageneralizable%20to%20other%20permafrost%20regions%20and%20demonstrates%20how%20hybrid%0Aapproaches%20can%20overcome%20pure%20data-driven%20limitations%20in%20climate%20change%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02189v1&entry.124074799=Read"},
{"title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward", "author": "Yanming Wan and Jiaxing Wu and Marwa Abdulhai and Lior Shani and Natasha Jaques", "abstract": "  Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents.\n", "link": "http://arxiv.org/abs/2504.03206v3", "date": "2025-10-02", "relevancy": 1.4213, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4772}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Personalized%20Multi-Turn%20Dialogue%20with%20Curiosity%20Reward&body=Title%3A%20Enhancing%20Personalized%20Multi-Turn%20Dialogue%20with%20Curiosity%20Reward%0AAuthor%3A%20Yanming%20Wan%20and%20Jiaxing%20Wu%20and%20Marwa%20Abdulhai%20and%20Lior%20Shani%20and%20Natasha%20Jaques%0AAbstract%3A%20%20%20Effective%20conversational%20agents%20like%20large%20language%20models%20%28LLMs%29%20must%0Apersonalize%20their%20interactions%20to%20adapt%20to%20user%20preferences%2C%20personalities%2C%20and%0Aattributes%20across%20diverse%20domains%20like%20education%20and%20healthcare.%20Current%0Amethods%20like%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20often%0Aprioritize%20helpfulness%20and%20safety%20but%20fall%20short%20in%20fostering%20truly%20empathetic%2C%0Aadaptive%2C%20and%20personalized%20dialogues.%20Existing%20personalization%20approaches%0Atypically%20rely%20on%20extensive%20user%20history%2C%20limiting%20their%20effectiveness%20for%20new%0Aor%20context-limited%20users.%20To%20address%20these%20limitations%2C%20we%20propose%20leveraging%20a%0Auser%20model%20to%20incorporate%20a%20curiosity-based%20intrinsic%20reward%20into%20multi-turn%0ARLHF.%20This%20novel%20reward%20mechanism%20encourages%20the%20LLM%20agent%20to%20actively%20infer%0Auser%20traits%20by%20optimizing%20conversations%20to%20improve%20its%20user%20model%27s%20accuracy.%0AConsequently%2C%20the%20agent%20delivers%20more%20personalized%20interactions%20by%20learning%0Amore%20about%20the%20user.%20We%20demonstrate%20our%20method%27s%20effectiveness%20in%20two%20distinct%0Adomains%3A%20significantly%20improving%20personalization%20performance%20in%20a%0Aconversational%20recommendation%20task%2C%20and%20personalizing%20conversations%20for%0Adifferent%20learning%20styles%20in%20an%20educational%20setting.%20We%20show%20improved%0Ageneralization%20capabilities%20compared%20to%20traditional%20multi-turn%20RLHF%2C%20all%20while%0Amaintaining%20conversation%20quality.%20Our%20method%20offers%20a%20promising%20solution%20for%0Acreating%20more%20personalized%2C%20adaptive%2C%20and%20engaging%20conversational%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03206v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Personalized%2520Multi-Turn%2520Dialogue%2520with%2520Curiosity%2520Reward%26entry.906535625%3DYanming%2520Wan%2520and%2520Jiaxing%2520Wu%2520and%2520Marwa%2520Abdulhai%2520and%2520Lior%2520Shani%2520and%2520Natasha%2520Jaques%26entry.1292438233%3D%2520%2520Effective%2520conversational%2520agents%2520like%2520large%2520language%2520models%2520%2528LLMs%2529%2520must%250Apersonalize%2520their%2520interactions%2520to%2520adapt%2520to%2520user%2520preferences%252C%2520personalities%252C%2520and%250Aattributes%2520across%2520diverse%2520domains%2520like%2520education%2520and%2520healthcare.%2520Current%250Amethods%2520like%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%252C%2520often%250Aprioritize%2520helpfulness%2520and%2520safety%2520but%2520fall%2520short%2520in%2520fostering%2520truly%2520empathetic%252C%250Aadaptive%252C%2520and%2520personalized%2520dialogues.%2520Existing%2520personalization%2520approaches%250Atypically%2520rely%2520on%2520extensive%2520user%2520history%252C%2520limiting%2520their%2520effectiveness%2520for%2520new%250Aor%2520context-limited%2520users.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520leveraging%2520a%250Auser%2520model%2520to%2520incorporate%2520a%2520curiosity-based%2520intrinsic%2520reward%2520into%2520multi-turn%250ARLHF.%2520This%2520novel%2520reward%2520mechanism%2520encourages%2520the%2520LLM%2520agent%2520to%2520actively%2520infer%250Auser%2520traits%2520by%2520optimizing%2520conversations%2520to%2520improve%2520its%2520user%2520model%2527s%2520accuracy.%250AConsequently%252C%2520the%2520agent%2520delivers%2520more%2520personalized%2520interactions%2520by%2520learning%250Amore%2520about%2520the%2520user.%2520We%2520demonstrate%2520our%2520method%2527s%2520effectiveness%2520in%2520two%2520distinct%250Adomains%253A%2520significantly%2520improving%2520personalization%2520performance%2520in%2520a%250Aconversational%2520recommendation%2520task%252C%2520and%2520personalizing%2520conversations%2520for%250Adifferent%2520learning%2520styles%2520in%2520an%2520educational%2520setting.%2520We%2520show%2520improved%250Ageneralization%2520capabilities%2520compared%2520to%2520traditional%2520multi-turn%2520RLHF%252C%2520all%2520while%250Amaintaining%2520conversation%2520quality.%2520Our%2520method%2520offers%2520a%2520promising%2520solution%2520for%250Acreating%2520more%2520personalized%252C%2520adaptive%252C%2520and%2520engaging%2520conversational%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03206v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Personalized%20Multi-Turn%20Dialogue%20with%20Curiosity%20Reward&entry.906535625=Yanming%20Wan%20and%20Jiaxing%20Wu%20and%20Marwa%20Abdulhai%20and%20Lior%20Shani%20and%20Natasha%20Jaques&entry.1292438233=%20%20Effective%20conversational%20agents%20like%20large%20language%20models%20%28LLMs%29%20must%0Apersonalize%20their%20interactions%20to%20adapt%20to%20user%20preferences%2C%20personalities%2C%20and%0Aattributes%20across%20diverse%20domains%20like%20education%20and%20healthcare.%20Current%0Amethods%20like%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20often%0Aprioritize%20helpfulness%20and%20safety%20but%20fall%20short%20in%20fostering%20truly%20empathetic%2C%0Aadaptive%2C%20and%20personalized%20dialogues.%20Existing%20personalization%20approaches%0Atypically%20rely%20on%20extensive%20user%20history%2C%20limiting%20their%20effectiveness%20for%20new%0Aor%20context-limited%20users.%20To%20address%20these%20limitations%2C%20we%20propose%20leveraging%20a%0Auser%20model%20to%20incorporate%20a%20curiosity-based%20intrinsic%20reward%20into%20multi-turn%0ARLHF.%20This%20novel%20reward%20mechanism%20encourages%20the%20LLM%20agent%20to%20actively%20infer%0Auser%20traits%20by%20optimizing%20conversations%20to%20improve%20its%20user%20model%27s%20accuracy.%0AConsequently%2C%20the%20agent%20delivers%20more%20personalized%20interactions%20by%20learning%0Amore%20about%20the%20user.%20We%20demonstrate%20our%20method%27s%20effectiveness%20in%20two%20distinct%0Adomains%3A%20significantly%20improving%20personalization%20performance%20in%20a%0Aconversational%20recommendation%20task%2C%20and%20personalizing%20conversations%20for%0Adifferent%20learning%20styles%20in%20an%20educational%20setting.%20We%20show%20improved%0Ageneralization%20capabilities%20compared%20to%20traditional%20multi-turn%20RLHF%2C%20all%20while%0Amaintaining%20conversation%20quality.%20Our%20method%20offers%20a%20promising%20solution%20for%0Acreating%20more%20personalized%2C%20adaptive%2C%20and%20engaging%20conversational%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03206v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


