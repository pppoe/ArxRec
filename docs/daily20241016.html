<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241015.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian\n  Surfel Head Avatars", "author": "Jaeseong Lee and Taewoong Kang and Marcel C. B\u00fchler and Min-Jung Kim and Sungwon Hwang and Junha Hyung and Hyojin Jang and Jaegul Choo", "abstract": "  Recent advancements in head avatar rendering using Gaussian primitives have\nachieved significantly high-fidelity results. Although precise head geometry is\ncrucial for applications like mesh reconstruction and relighting, current\nmethods struggle to capture intricate geometric details and render unseen poses\ndue to their reliance on similarity transformations, which cannot handle\nstretch and shear transforms essential for detailed deformations of geometry.\nTo address this, we propose SurFhead, a novel method that reconstructs riggable\nhead geometry from RGB videos using 2D Gaussian surfels, which offer\nwell-defined geometric properties, such as precise depth from fixed ray\nintersections and normals derived from their surface orientation, making them\nadvantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of\nboth normals and images, even in extreme poses, by leveraging classical\nmesh-based deformation transfer and affine transformation interpolation.\nSurFhead introduces precise geometric deformation and blends surfels through\npolar decomposition of transformations, including those affecting normals. Our\nkey contribution lies in bridging classical graphics techniques, such as\nmesh-based deformation, with modern Gaussian primitives, achieving\nstate-of-the-art geometry reconstruction and rendering quality. Unlike previous\navatar rendering approaches, SurFhead enables efficient reconstruction driven\nby Gaussian primitives while preserving high-fidelity geometry.\n", "link": "http://arxiv.org/abs/2410.11682v1", "date": "2024-10-15", "relevancy": 3.5073, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7446}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7446}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurFhead%3A%20Affine%20Rig%20Blending%20for%20Geometrically%20Accurate%202D%20Gaussian%0A%20%20Surfel%20Head%20Avatars&body=Title%3A%20SurFhead%3A%20Affine%20Rig%20Blending%20for%20Geometrically%20Accurate%202D%20Gaussian%0A%20%20Surfel%20Head%20Avatars%0AAuthor%3A%20Jaeseong%20Lee%20and%20Taewoong%20Kang%20and%20Marcel%20C.%20B%C3%BChler%20and%20Min-Jung%20Kim%20and%20Sungwon%20Hwang%20and%20Junha%20Hyung%20and%20Hyojin%20Jang%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20Recent%20advancements%20in%20head%20avatar%20rendering%20using%20Gaussian%20primitives%20have%0Aachieved%20significantly%20high-fidelity%20results.%20Although%20precise%20head%20geometry%20is%0Acrucial%20for%20applications%20like%20mesh%20reconstruction%20and%20relighting%2C%20current%0Amethods%20struggle%20to%20capture%20intricate%20geometric%20details%20and%20render%20unseen%20poses%0Adue%20to%20their%20reliance%20on%20similarity%20transformations%2C%20which%20cannot%20handle%0Astretch%20and%20shear%20transforms%20essential%20for%20detailed%20deformations%20of%20geometry.%0ATo%20address%20this%2C%20we%20propose%20SurFhead%2C%20a%20novel%20method%20that%20reconstructs%20riggable%0Ahead%20geometry%20from%20RGB%20videos%20using%202D%20Gaussian%20surfels%2C%20which%20offer%0Awell-defined%20geometric%20properties%2C%20such%20as%20precise%20depth%20from%20fixed%20ray%0Aintersections%20and%20normals%20derived%20from%20their%20surface%20orientation%2C%20making%20them%0Aadvantageous%20over%203D%20counterparts.%20SurFhead%20ensures%20high-fidelity%20rendering%20of%0Aboth%20normals%20and%20images%2C%20even%20in%20extreme%20poses%2C%20by%20leveraging%20classical%0Amesh-based%20deformation%20transfer%20and%20affine%20transformation%20interpolation.%0ASurFhead%20introduces%20precise%20geometric%20deformation%20and%20blends%20surfels%20through%0Apolar%20decomposition%20of%20transformations%2C%20including%20those%20affecting%20normals.%20Our%0Akey%20contribution%20lies%20in%20bridging%20classical%20graphics%20techniques%2C%20such%20as%0Amesh-based%20deformation%2C%20with%20modern%20Gaussian%20primitives%2C%20achieving%0Astate-of-the-art%20geometry%20reconstruction%20and%20rendering%20quality.%20Unlike%20previous%0Aavatar%20rendering%20approaches%2C%20SurFhead%20enables%20efficient%20reconstruction%20driven%0Aby%20Gaussian%20primitives%20while%20preserving%20high-fidelity%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurFhead%253A%2520Affine%2520Rig%2520Blending%2520for%2520Geometrically%2520Accurate%25202D%2520Gaussian%250A%2520%2520Surfel%2520Head%2520Avatars%26entry.906535625%3DJaeseong%2520Lee%2520and%2520Taewoong%2520Kang%2520and%2520Marcel%2520C.%2520B%25C3%25BChler%2520and%2520Min-Jung%2520Kim%2520and%2520Sungwon%2520Hwang%2520and%2520Junha%2520Hyung%2520and%2520Hyojin%2520Jang%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520head%2520avatar%2520rendering%2520using%2520Gaussian%2520primitives%2520have%250Aachieved%2520significantly%2520high-fidelity%2520results.%2520Although%2520precise%2520head%2520geometry%2520is%250Acrucial%2520for%2520applications%2520like%2520mesh%2520reconstruction%2520and%2520relighting%252C%2520current%250Amethods%2520struggle%2520to%2520capture%2520intricate%2520geometric%2520details%2520and%2520render%2520unseen%2520poses%250Adue%2520to%2520their%2520reliance%2520on%2520similarity%2520transformations%252C%2520which%2520cannot%2520handle%250Astretch%2520and%2520shear%2520transforms%2520essential%2520for%2520detailed%2520deformations%2520of%2520geometry.%250ATo%2520address%2520this%252C%2520we%2520propose%2520SurFhead%252C%2520a%2520novel%2520method%2520that%2520reconstructs%2520riggable%250Ahead%2520geometry%2520from%2520RGB%2520videos%2520using%25202D%2520Gaussian%2520surfels%252C%2520which%2520offer%250Awell-defined%2520geometric%2520properties%252C%2520such%2520as%2520precise%2520depth%2520from%2520fixed%2520ray%250Aintersections%2520and%2520normals%2520derived%2520from%2520their%2520surface%2520orientation%252C%2520making%2520them%250Aadvantageous%2520over%25203D%2520counterparts.%2520SurFhead%2520ensures%2520high-fidelity%2520rendering%2520of%250Aboth%2520normals%2520and%2520images%252C%2520even%2520in%2520extreme%2520poses%252C%2520by%2520leveraging%2520classical%250Amesh-based%2520deformation%2520transfer%2520and%2520affine%2520transformation%2520interpolation.%250ASurFhead%2520introduces%2520precise%2520geometric%2520deformation%2520and%2520blends%2520surfels%2520through%250Apolar%2520decomposition%2520of%2520transformations%252C%2520including%2520those%2520affecting%2520normals.%2520Our%250Akey%2520contribution%2520lies%2520in%2520bridging%2520classical%2520graphics%2520techniques%252C%2520such%2520as%250Amesh-based%2520deformation%252C%2520with%2520modern%2520Gaussian%2520primitives%252C%2520achieving%250Astate-of-the-art%2520geometry%2520reconstruction%2520and%2520rendering%2520quality.%2520Unlike%2520previous%250Aavatar%2520rendering%2520approaches%252C%2520SurFhead%2520enables%2520efficient%2520reconstruction%2520driven%250Aby%2520Gaussian%2520primitives%2520while%2520preserving%2520high-fidelity%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurFhead%3A%20Affine%20Rig%20Blending%20for%20Geometrically%20Accurate%202D%20Gaussian%0A%20%20Surfel%20Head%20Avatars&entry.906535625=Jaeseong%20Lee%20and%20Taewoong%20Kang%20and%20Marcel%20C.%20B%C3%BChler%20and%20Min-Jung%20Kim%20and%20Sungwon%20Hwang%20and%20Junha%20Hyung%20and%20Hyojin%20Jang%20and%20Jaegul%20Choo&entry.1292438233=%20%20Recent%20advancements%20in%20head%20avatar%20rendering%20using%20Gaussian%20primitives%20have%0Aachieved%20significantly%20high-fidelity%20results.%20Although%20precise%20head%20geometry%20is%0Acrucial%20for%20applications%20like%20mesh%20reconstruction%20and%20relighting%2C%20current%0Amethods%20struggle%20to%20capture%20intricate%20geometric%20details%20and%20render%20unseen%20poses%0Adue%20to%20their%20reliance%20on%20similarity%20transformations%2C%20which%20cannot%20handle%0Astretch%20and%20shear%20transforms%20essential%20for%20detailed%20deformations%20of%20geometry.%0ATo%20address%20this%2C%20we%20propose%20SurFhead%2C%20a%20novel%20method%20that%20reconstructs%20riggable%0Ahead%20geometry%20from%20RGB%20videos%20using%202D%20Gaussian%20surfels%2C%20which%20offer%0Awell-defined%20geometric%20properties%2C%20such%20as%20precise%20depth%20from%20fixed%20ray%0Aintersections%20and%20normals%20derived%20from%20their%20surface%20orientation%2C%20making%20them%0Aadvantageous%20over%203D%20counterparts.%20SurFhead%20ensures%20high-fidelity%20rendering%20of%0Aboth%20normals%20and%20images%2C%20even%20in%20extreme%20poses%2C%20by%20leveraging%20classical%0Amesh-based%20deformation%20transfer%20and%20affine%20transformation%20interpolation.%0ASurFhead%20introduces%20precise%20geometric%20deformation%20and%20blends%20surfels%20through%0Apolar%20decomposition%20of%20transformations%2C%20including%20those%20affecting%20normals.%20Our%0Akey%20contribution%20lies%20in%20bridging%20classical%20graphics%20techniques%2C%20such%20as%0Amesh-based%20deformation%2C%20with%20modern%20Gaussian%20primitives%2C%20achieving%0Astate-of-the-art%20geometry%20reconstruction%20and%20rendering%20quality.%20Unlike%20previous%0Aavatar%20rendering%20approaches%2C%20SurFhead%20enables%20efficient%20reconstruction%20driven%0Aby%20Gaussian%20primitives%20while%20preserving%20high-fidelity%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11682v1&entry.124074799=Read"},
{"title": "LoGS: Visual Localization via Gaussian Splatting with Fewer Training\n  Images", "author": "Yuzhou Cheng and Jianhao Jiao and Yue Wang and Dimitrios Kanoulas", "abstract": "  Visual localization involves estimating a query image's 6-DoF (degrees of\nfreedom) camera pose, which is a fundamental component in various computer\nvision and robotic tasks. This paper presents LoGS, a vision-based localization\npipeline utilizing the 3D Gaussian Splatting (GS) technique as scene\nrepresentation. This novel representation allows high-quality novel view\nsynthesis. During the mapping phase, structure-from-motion (SfM) is applied\nfirst, followed by the generation of a GS map. During localization, the initial\nposition is obtained through image retrieval, local feature matching coupled\nwith a PnP solver, and then a high-precision pose is achieved through the\nanalysis-by-synthesis manner on the GS map. Experimental results on four\nlarge-scale datasets demonstrate the proposed approach's SoTA accuracy in\nestimating camera poses and robustness under challenging few-shot conditions.\n", "link": "http://arxiv.org/abs/2410.11505v1", "date": "2024-10-15", "relevancy": 3.2938, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6973}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6928}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoGS%3A%20Visual%20Localization%20via%20Gaussian%20Splatting%20with%20Fewer%20Training%0A%20%20Images&body=Title%3A%20LoGS%3A%20Visual%20Localization%20via%20Gaussian%20Splatting%20with%20Fewer%20Training%0A%20%20Images%0AAuthor%3A%20Yuzhou%20Cheng%20and%20Jianhao%20Jiao%20and%20Yue%20Wang%20and%20Dimitrios%20Kanoulas%0AAbstract%3A%20%20%20Visual%20localization%20involves%20estimating%20a%20query%20image%27s%206-DoF%20%28degrees%20of%0Afreedom%29%20camera%20pose%2C%20which%20is%20a%20fundamental%20component%20in%20various%20computer%0Avision%20and%20robotic%20tasks.%20This%20paper%20presents%20LoGS%2C%20a%20vision-based%20localization%0Apipeline%20utilizing%20the%203D%20Gaussian%20Splatting%20%28GS%29%20technique%20as%20scene%0Arepresentation.%20This%20novel%20representation%20allows%20high-quality%20novel%20view%0Asynthesis.%20During%20the%20mapping%20phase%2C%20structure-from-motion%20%28SfM%29%20is%20applied%0Afirst%2C%20followed%20by%20the%20generation%20of%20a%20GS%20map.%20During%20localization%2C%20the%20initial%0Aposition%20is%20obtained%20through%20image%20retrieval%2C%20local%20feature%20matching%20coupled%0Awith%20a%20PnP%20solver%2C%20and%20then%20a%20high-precision%20pose%20is%20achieved%20through%20the%0Aanalysis-by-synthesis%20manner%20on%20the%20GS%20map.%20Experimental%20results%20on%20four%0Alarge-scale%20datasets%20demonstrate%20the%20proposed%20approach%27s%20SoTA%20accuracy%20in%0Aestimating%20camera%20poses%20and%20robustness%20under%20challenging%20few-shot%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoGS%253A%2520Visual%2520Localization%2520via%2520Gaussian%2520Splatting%2520with%2520Fewer%2520Training%250A%2520%2520Images%26entry.906535625%3DYuzhou%2520Cheng%2520and%2520Jianhao%2520Jiao%2520and%2520Yue%2520Wang%2520and%2520Dimitrios%2520Kanoulas%26entry.1292438233%3D%2520%2520Visual%2520localization%2520involves%2520estimating%2520a%2520query%2520image%2527s%25206-DoF%2520%2528degrees%2520of%250Afreedom%2529%2520camera%2520pose%252C%2520which%2520is%2520a%2520fundamental%2520component%2520in%2520various%2520computer%250Avision%2520and%2520robotic%2520tasks.%2520This%2520paper%2520presents%2520LoGS%252C%2520a%2520vision-based%2520localization%250Apipeline%2520utilizing%2520the%25203D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520technique%2520as%2520scene%250Arepresentation.%2520This%2520novel%2520representation%2520allows%2520high-quality%2520novel%2520view%250Asynthesis.%2520During%2520the%2520mapping%2520phase%252C%2520structure-from-motion%2520%2528SfM%2529%2520is%2520applied%250Afirst%252C%2520followed%2520by%2520the%2520generation%2520of%2520a%2520GS%2520map.%2520During%2520localization%252C%2520the%2520initial%250Aposition%2520is%2520obtained%2520through%2520image%2520retrieval%252C%2520local%2520feature%2520matching%2520coupled%250Awith%2520a%2520PnP%2520solver%252C%2520and%2520then%2520a%2520high-precision%2520pose%2520is%2520achieved%2520through%2520the%250Aanalysis-by-synthesis%2520manner%2520on%2520the%2520GS%2520map.%2520Experimental%2520results%2520on%2520four%250Alarge-scale%2520datasets%2520demonstrate%2520the%2520proposed%2520approach%2527s%2520SoTA%2520accuracy%2520in%250Aestimating%2520camera%2520poses%2520and%2520robustness%2520under%2520challenging%2520few-shot%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoGS%3A%20Visual%20Localization%20via%20Gaussian%20Splatting%20with%20Fewer%20Training%0A%20%20Images&entry.906535625=Yuzhou%20Cheng%20and%20Jianhao%20Jiao%20and%20Yue%20Wang%20and%20Dimitrios%20Kanoulas&entry.1292438233=%20%20Visual%20localization%20involves%20estimating%20a%20query%20image%27s%206-DoF%20%28degrees%20of%0Afreedom%29%20camera%20pose%2C%20which%20is%20a%20fundamental%20component%20in%20various%20computer%0Avision%20and%20robotic%20tasks.%20This%20paper%20presents%20LoGS%2C%20a%20vision-based%20localization%0Apipeline%20utilizing%20the%203D%20Gaussian%20Splatting%20%28GS%29%20technique%20as%20scene%0Arepresentation.%20This%20novel%20representation%20allows%20high-quality%20novel%20view%0Asynthesis.%20During%20the%20mapping%20phase%2C%20structure-from-motion%20%28SfM%29%20is%20applied%0Afirst%2C%20followed%20by%20the%20generation%20of%20a%20GS%20map.%20During%20localization%2C%20the%20initial%0Aposition%20is%20obtained%20through%20image%20retrieval%2C%20local%20feature%20matching%20coupled%0Awith%20a%20PnP%20solver%2C%20and%20then%20a%20high-precision%20pose%20is%20achieved%20through%20the%0Aanalysis-by-synthesis%20manner%20on%20the%20GS%20map.%20Experimental%20results%20on%20four%0Alarge-scale%20datasets%20demonstrate%20the%20proposed%20approach%27s%20SoTA%20accuracy%20in%0Aestimating%20camera%20poses%20and%20robustness%20under%20challenging%20few-shot%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11505v1&entry.124074799=Read"},
{"title": "PAVLM: Advancing Point Cloud based Affordance Understanding Via\n  Vision-Language Model", "author": "Shang-Ching Liu and Van Nhiem Tran and Wenkai Chen and Wei-Lun Cheng and Yen-Lin Huang and I-Bin Liao and Yung-Hui Li and Jianwei Zhang", "abstract": "  Affordance understanding, the task of identifying actionable regions on 3D\nobjects, plays a vital role in allowing robotic systems to engage with and\noperate within the physical world. Although Visual Language Models (VLMs) have\nexcelled in high-level reasoning and long-horizon planning for robotic\nmanipulation, they still fall short in grasping the nuanced physical properties\nrequired for effective human-robot interaction. In this paper, we introduce\nPAVLM (Point cloud Affordance Vision-Language Model), an innovative framework\nthat utilizes the extensive multimodal knowledge embedded in pre-trained\nlanguage models to enhance 3D affordance understanding of point cloud. PAVLM\nintegrates a geometric-guided propagation module with hidden embeddings from\nlarge language models (LLMs) to enrich visual semantics. On the language side,\nwe prompt Llama-3.1 models to generate refined context-aware text, augmenting\nthe instructional input with deeper semantic cues. Experimental results on the\n3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods\nfor both full and partial point clouds, particularly excelling in its\ngeneralization to novel open-world affordance tasks of 3D objects. For more\ninformation, visit our project site: pavlm-source.github.io.\n", "link": "http://arxiv.org/abs/2410.11564v1", "date": "2024-10-15", "relevancy": 3.1049, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6299}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAVLM%3A%20Advancing%20Point%20Cloud%20based%20Affordance%20Understanding%20Via%0A%20%20Vision-Language%20Model&body=Title%3A%20PAVLM%3A%20Advancing%20Point%20Cloud%20based%20Affordance%20Understanding%20Via%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Shang-Ching%20Liu%20and%20Van%20Nhiem%20Tran%20and%20Wenkai%20Chen%20and%20Wei-Lun%20Cheng%20and%20Yen-Lin%20Huang%20and%20I-Bin%20Liao%20and%20Yung-Hui%20Li%20and%20Jianwei%20Zhang%0AAbstract%3A%20%20%20Affordance%20understanding%2C%20the%20task%20of%20identifying%20actionable%20regions%20on%203D%0Aobjects%2C%20plays%20a%20vital%20role%20in%20allowing%20robotic%20systems%20to%20engage%20with%20and%0Aoperate%20within%20the%20physical%20world.%20Although%20Visual%20Language%20Models%20%28VLMs%29%20have%0Aexcelled%20in%20high-level%20reasoning%20and%20long-horizon%20planning%20for%20robotic%0Amanipulation%2C%20they%20still%20fall%20short%20in%20grasping%20the%20nuanced%20physical%20properties%0Arequired%20for%20effective%20human-robot%20interaction.%20In%20this%20paper%2C%20we%20introduce%0APAVLM%20%28Point%20cloud%20Affordance%20Vision-Language%20Model%29%2C%20an%20innovative%20framework%0Athat%20utilizes%20the%20extensive%20multimodal%20knowledge%20embedded%20in%20pre-trained%0Alanguage%20models%20to%20enhance%203D%20affordance%20understanding%20of%20point%20cloud.%20PAVLM%0Aintegrates%20a%20geometric-guided%20propagation%20module%20with%20hidden%20embeddings%20from%0Alarge%20language%20models%20%28LLMs%29%20to%20enrich%20visual%20semantics.%20On%20the%20language%20side%2C%0Awe%20prompt%20Llama-3.1%20models%20to%20generate%20refined%20context-aware%20text%2C%20augmenting%0Athe%20instructional%20input%20with%20deeper%20semantic%20cues.%20Experimental%20results%20on%20the%0A3D-AffordanceNet%20benchmark%20demonstrate%20that%20PAVLM%20outperforms%20baseline%20methods%0Afor%20both%20full%20and%20partial%20point%20clouds%2C%20particularly%20excelling%20in%20its%0Ageneralization%20to%20novel%20open-world%20affordance%20tasks%20of%203D%20objects.%20For%20more%0Ainformation%2C%20visit%20our%20project%20site%3A%20pavlm-source.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAVLM%253A%2520Advancing%2520Point%2520Cloud%2520based%2520Affordance%2520Understanding%2520Via%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DShang-Ching%2520Liu%2520and%2520Van%2520Nhiem%2520Tran%2520and%2520Wenkai%2520Chen%2520and%2520Wei-Lun%2520Cheng%2520and%2520Yen-Lin%2520Huang%2520and%2520I-Bin%2520Liao%2520and%2520Yung-Hui%2520Li%2520and%2520Jianwei%2520Zhang%26entry.1292438233%3D%2520%2520Affordance%2520understanding%252C%2520the%2520task%2520of%2520identifying%2520actionable%2520regions%2520on%25203D%250Aobjects%252C%2520plays%2520a%2520vital%2520role%2520in%2520allowing%2520robotic%2520systems%2520to%2520engage%2520with%2520and%250Aoperate%2520within%2520the%2520physical%2520world.%2520Although%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520have%250Aexcelled%2520in%2520high-level%2520reasoning%2520and%2520long-horizon%2520planning%2520for%2520robotic%250Amanipulation%252C%2520they%2520still%2520fall%2520short%2520in%2520grasping%2520the%2520nuanced%2520physical%2520properties%250Arequired%2520for%2520effective%2520human-robot%2520interaction.%2520In%2520this%2520paper%252C%2520we%2520introduce%250APAVLM%2520%2528Point%2520cloud%2520Affordance%2520Vision-Language%2520Model%2529%252C%2520an%2520innovative%2520framework%250Athat%2520utilizes%2520the%2520extensive%2520multimodal%2520knowledge%2520embedded%2520in%2520pre-trained%250Alanguage%2520models%2520to%2520enhance%25203D%2520affordance%2520understanding%2520of%2520point%2520cloud.%2520PAVLM%250Aintegrates%2520a%2520geometric-guided%2520propagation%2520module%2520with%2520hidden%2520embeddings%2520from%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520enrich%2520visual%2520semantics.%2520On%2520the%2520language%2520side%252C%250Awe%2520prompt%2520Llama-3.1%2520models%2520to%2520generate%2520refined%2520context-aware%2520text%252C%2520augmenting%250Athe%2520instructional%2520input%2520with%2520deeper%2520semantic%2520cues.%2520Experimental%2520results%2520on%2520the%250A3D-AffordanceNet%2520benchmark%2520demonstrate%2520that%2520PAVLM%2520outperforms%2520baseline%2520methods%250Afor%2520both%2520full%2520and%2520partial%2520point%2520clouds%252C%2520particularly%2520excelling%2520in%2520its%250Ageneralization%2520to%2520novel%2520open-world%2520affordance%2520tasks%2520of%25203D%2520objects.%2520For%2520more%250Ainformation%252C%2520visit%2520our%2520project%2520site%253A%2520pavlm-source.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAVLM%3A%20Advancing%20Point%20Cloud%20based%20Affordance%20Understanding%20Via%0A%20%20Vision-Language%20Model&entry.906535625=Shang-Ching%20Liu%20and%20Van%20Nhiem%20Tran%20and%20Wenkai%20Chen%20and%20Wei-Lun%20Cheng%20and%20Yen-Lin%20Huang%20and%20I-Bin%20Liao%20and%20Yung-Hui%20Li%20and%20Jianwei%20Zhang&entry.1292438233=%20%20Affordance%20understanding%2C%20the%20task%20of%20identifying%20actionable%20regions%20on%203D%0Aobjects%2C%20plays%20a%20vital%20role%20in%20allowing%20robotic%20systems%20to%20engage%20with%20and%0Aoperate%20within%20the%20physical%20world.%20Although%20Visual%20Language%20Models%20%28VLMs%29%20have%0Aexcelled%20in%20high-level%20reasoning%20and%20long-horizon%20planning%20for%20robotic%0Amanipulation%2C%20they%20still%20fall%20short%20in%20grasping%20the%20nuanced%20physical%20properties%0Arequired%20for%20effective%20human-robot%20interaction.%20In%20this%20paper%2C%20we%20introduce%0APAVLM%20%28Point%20cloud%20Affordance%20Vision-Language%20Model%29%2C%20an%20innovative%20framework%0Athat%20utilizes%20the%20extensive%20multimodal%20knowledge%20embedded%20in%20pre-trained%0Alanguage%20models%20to%20enhance%203D%20affordance%20understanding%20of%20point%20cloud.%20PAVLM%0Aintegrates%20a%20geometric-guided%20propagation%20module%20with%20hidden%20embeddings%20from%0Alarge%20language%20models%20%28LLMs%29%20to%20enrich%20visual%20semantics.%20On%20the%20language%20side%2C%0Awe%20prompt%20Llama-3.1%20models%20to%20generate%20refined%20context-aware%20text%2C%20augmenting%0Athe%20instructional%20input%20with%20deeper%20semantic%20cues.%20Experimental%20results%20on%20the%0A3D-AffordanceNet%20benchmark%20demonstrate%20that%20PAVLM%20outperforms%20baseline%20methods%0Afor%20both%20full%20and%20partial%20point%20clouds%2C%20particularly%20excelling%20in%20its%0Ageneralization%20to%20novel%20open-world%20affordance%20tasks%20of%203D%20objects.%20For%20more%0Ainformation%2C%20visit%20our%20project%20site%3A%20pavlm-source.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11564v1&entry.124074799=Read"},
{"title": "Shelf-Supervised Cross-Modal Pre-Training for 3D Object Detection", "author": "Mehar Khurana and Neehar Peri and James Hays and Deva Ramanan", "abstract": "  State-of-the-art 3D object detectors are often trained on massive labeled\ndatasets. However, annotating 3D bounding boxes remains prohibitively expensive\nand time-consuming, particularly for LiDAR. Instead, recent works demonstrate\nthat self-supervised pre-training with unlabeled data can improve detection\naccuracy with limited labels. Contemporary methods adapt best-practices for\nself-supervised learning from the image domain to point clouds (such as\ncontrastive learning). However, publicly available 3D datasets are considerably\nsmaller and less diverse than those used for image-based self-supervised\nlearning, limiting their effectiveness. We do note, however, that such 3D data\nis naturally collected in a multimodal fashion, often paired with images.\nRather than pre-training with only self-supervised objectives, we argue that it\nis better to bootstrap point cloud representations using image-based foundation\nmodels trained on internet-scale data. Specifically, we propose a\nshelf-supervised approach (e.g. supervised with off-the-shelf image foundation\nmodels) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR\ndata. Pre-training 3D detectors with such pseudo-labels yields significantly\nbetter semi-supervised detection accuracy than prior self-supervised pretext\ntasks. Importantly, we show that image-based shelf-supervision is helpful for\ntraining LiDAR-only, RGB-only and multi-modal (RGB + LiDAR) detectors. We\ndemonstrate the effectiveness of our approach on nuScenes and WOD,\nsignificantly improving over prior work in limited data settings. Our code is\navailable at https://github.com/meharkhurana03/cm3d\n", "link": "http://arxiv.org/abs/2406.10115v3", "date": "2024-10-15", "relevancy": 3.0586, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6179}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6091}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shelf-Supervised%20Cross-Modal%20Pre-Training%20for%203D%20Object%20Detection&body=Title%3A%20Shelf-Supervised%20Cross-Modal%20Pre-Training%20for%203D%20Object%20Detection%0AAuthor%3A%20Mehar%20Khurana%20and%20Neehar%20Peri%20and%20James%20Hays%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20State-of-the-art%203D%20object%20detectors%20are%20often%20trained%20on%20massive%20labeled%0Adatasets.%20However%2C%20annotating%203D%20bounding%20boxes%20remains%20prohibitively%20expensive%0Aand%20time-consuming%2C%20particularly%20for%20LiDAR.%20Instead%2C%20recent%20works%20demonstrate%0Athat%20self-supervised%20pre-training%20with%20unlabeled%20data%20can%20improve%20detection%0Aaccuracy%20with%20limited%20labels.%20Contemporary%20methods%20adapt%20best-practices%20for%0Aself-supervised%20learning%20from%20the%20image%20domain%20to%20point%20clouds%20%28such%20as%0Acontrastive%20learning%29.%20However%2C%20publicly%20available%203D%20datasets%20are%20considerably%0Asmaller%20and%20less%20diverse%20than%20those%20used%20for%20image-based%20self-supervised%0Alearning%2C%20limiting%20their%20effectiveness.%20We%20do%20note%2C%20however%2C%20that%20such%203D%20data%0Ais%20naturally%20collected%20in%20a%20multimodal%20fashion%2C%20often%20paired%20with%20images.%0ARather%20than%20pre-training%20with%20only%20self-supervised%20objectives%2C%20we%20argue%20that%20it%0Ais%20better%20to%20bootstrap%20point%20cloud%20representations%20using%20image-based%20foundation%0Amodels%20trained%20on%20internet-scale%20data.%20Specifically%2C%20we%20propose%20a%0Ashelf-supervised%20approach%20%28e.g.%20supervised%20with%20off-the-shelf%20image%20foundation%0Amodels%29%20for%20generating%20zero-shot%203D%20bounding%20boxes%20from%20paired%20RGB%20and%20LiDAR%0Adata.%20Pre-training%203D%20detectors%20with%20such%20pseudo-labels%20yields%20significantly%0Abetter%20semi-supervised%20detection%20accuracy%20than%20prior%20self-supervised%20pretext%0Atasks.%20Importantly%2C%20we%20show%20that%20image-based%20shelf-supervision%20is%20helpful%20for%0Atraining%20LiDAR-only%2C%20RGB-only%20and%20multi-modal%20%28RGB%20%2B%20LiDAR%29%20detectors.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20on%20nuScenes%20and%20WOD%2C%0Asignificantly%20improving%20over%20prior%20work%20in%20limited%20data%20settings.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/meharkhurana03/cm3d%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10115v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShelf-Supervised%2520Cross-Modal%2520Pre-Training%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DMehar%2520Khurana%2520and%2520Neehar%2520Peri%2520and%2520James%2520Hays%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520State-of-the-art%25203D%2520object%2520detectors%2520are%2520often%2520trained%2520on%2520massive%2520labeled%250Adatasets.%2520However%252C%2520annotating%25203D%2520bounding%2520boxes%2520remains%2520prohibitively%2520expensive%250Aand%2520time-consuming%252C%2520particularly%2520for%2520LiDAR.%2520Instead%252C%2520recent%2520works%2520demonstrate%250Athat%2520self-supervised%2520pre-training%2520with%2520unlabeled%2520data%2520can%2520improve%2520detection%250Aaccuracy%2520with%2520limited%2520labels.%2520Contemporary%2520methods%2520adapt%2520best-practices%2520for%250Aself-supervised%2520learning%2520from%2520the%2520image%2520domain%2520to%2520point%2520clouds%2520%2528such%2520as%250Acontrastive%2520learning%2529.%2520However%252C%2520publicly%2520available%25203D%2520datasets%2520are%2520considerably%250Asmaller%2520and%2520less%2520diverse%2520than%2520those%2520used%2520for%2520image-based%2520self-supervised%250Alearning%252C%2520limiting%2520their%2520effectiveness.%2520We%2520do%2520note%252C%2520however%252C%2520that%2520such%25203D%2520data%250Ais%2520naturally%2520collected%2520in%2520a%2520multimodal%2520fashion%252C%2520often%2520paired%2520with%2520images.%250ARather%2520than%2520pre-training%2520with%2520only%2520self-supervised%2520objectives%252C%2520we%2520argue%2520that%2520it%250Ais%2520better%2520to%2520bootstrap%2520point%2520cloud%2520representations%2520using%2520image-based%2520foundation%250Amodels%2520trained%2520on%2520internet-scale%2520data.%2520Specifically%252C%2520we%2520propose%2520a%250Ashelf-supervised%2520approach%2520%2528e.g.%2520supervised%2520with%2520off-the-shelf%2520image%2520foundation%250Amodels%2529%2520for%2520generating%2520zero-shot%25203D%2520bounding%2520boxes%2520from%2520paired%2520RGB%2520and%2520LiDAR%250Adata.%2520Pre-training%25203D%2520detectors%2520with%2520such%2520pseudo-labels%2520yields%2520significantly%250Abetter%2520semi-supervised%2520detection%2520accuracy%2520than%2520prior%2520self-supervised%2520pretext%250Atasks.%2520Importantly%252C%2520we%2520show%2520that%2520image-based%2520shelf-supervision%2520is%2520helpful%2520for%250Atraining%2520LiDAR-only%252C%2520RGB-only%2520and%2520multi-modal%2520%2528RGB%2520%252B%2520LiDAR%2529%2520detectors.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520nuScenes%2520and%2520WOD%252C%250Asignificantly%2520improving%2520over%2520prior%2520work%2520in%2520limited%2520data%2520settings.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/meharkhurana03/cm3d%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10115v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shelf-Supervised%20Cross-Modal%20Pre-Training%20for%203D%20Object%20Detection&entry.906535625=Mehar%20Khurana%20and%20Neehar%20Peri%20and%20James%20Hays%20and%20Deva%20Ramanan&entry.1292438233=%20%20State-of-the-art%203D%20object%20detectors%20are%20often%20trained%20on%20massive%20labeled%0Adatasets.%20However%2C%20annotating%203D%20bounding%20boxes%20remains%20prohibitively%20expensive%0Aand%20time-consuming%2C%20particularly%20for%20LiDAR.%20Instead%2C%20recent%20works%20demonstrate%0Athat%20self-supervised%20pre-training%20with%20unlabeled%20data%20can%20improve%20detection%0Aaccuracy%20with%20limited%20labels.%20Contemporary%20methods%20adapt%20best-practices%20for%0Aself-supervised%20learning%20from%20the%20image%20domain%20to%20point%20clouds%20%28such%20as%0Acontrastive%20learning%29.%20However%2C%20publicly%20available%203D%20datasets%20are%20considerably%0Asmaller%20and%20less%20diverse%20than%20those%20used%20for%20image-based%20self-supervised%0Alearning%2C%20limiting%20their%20effectiveness.%20We%20do%20note%2C%20however%2C%20that%20such%203D%20data%0Ais%20naturally%20collected%20in%20a%20multimodal%20fashion%2C%20often%20paired%20with%20images.%0ARather%20than%20pre-training%20with%20only%20self-supervised%20objectives%2C%20we%20argue%20that%20it%0Ais%20better%20to%20bootstrap%20point%20cloud%20representations%20using%20image-based%20foundation%0Amodels%20trained%20on%20internet-scale%20data.%20Specifically%2C%20we%20propose%20a%0Ashelf-supervised%20approach%20%28e.g.%20supervised%20with%20off-the-shelf%20image%20foundation%0Amodels%29%20for%20generating%20zero-shot%203D%20bounding%20boxes%20from%20paired%20RGB%20and%20LiDAR%0Adata.%20Pre-training%203D%20detectors%20with%20such%20pseudo-labels%20yields%20significantly%0Abetter%20semi-supervised%20detection%20accuracy%20than%20prior%20self-supervised%20pretext%0Atasks.%20Importantly%2C%20we%20show%20that%20image-based%20shelf-supervision%20is%20helpful%20for%0Atraining%20LiDAR-only%2C%20RGB-only%20and%20multi-modal%20%28RGB%20%2B%20LiDAR%29%20detectors.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20on%20nuScenes%20and%20WOD%2C%0Asignificantly%20improving%20over%20prior%20work%20in%20limited%20data%20settings.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/meharkhurana03/cm3d%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10115v3&entry.124074799=Read"},
{"title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for\n  Embodied AI", "author": "Sijie Cheng and Kechen Fang and Yangyang Yu and Sicheng Zhou and Bohao Li and Ye Tian and Tingguang Li and Lei Han and Yang Liu", "abstract": "  Recent advancements in Multi-modal Large Language Models (MLLMs) have opened\nnew avenues for applications in Embodied AI. Building on previous work,\nEgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating\negocentric video understanding capabilities. To bridge the gap between MLLMs\nand low-level control in Embodied AI, we design four key interrelated tasks:\nvideo question-answering, hierarchy planning, visual grounding and reward\nmodeling. To minimize manual annotation costs, we develop an automatic data\ngeneration pipeline based on the Ego4D dataset, leveraging the prior knowledge\nand multimodal capabilities of GPT-4o. Three human annotators then filter the\ngenerated data to ensure diversity and quality, resulting in the VidEgoThink\nbenchmark. We conduct extensive experiments with three types of models:\nAPI-based MLLMs, open-source image-based MLLMs, and open-source video-based\nMLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform\npoorly across all tasks related to egocentric video understanding. These\nfindings suggest that foundation models still require significant advancements\nto be effectively applied to first-person scenarios in Embodied AI. In\nconclusion, VidEgoThink reflects a research trend towards employing MLLMs for\negocentric vision, akin to human capabilities, enabling active observation and\ninteraction in the complex real-world environments.\n", "link": "http://arxiv.org/abs/2410.11623v1", "date": "2024-10-15", "relevancy": 3.0351, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6151}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidEgoThink%3A%20Assessing%20Egocentric%20Video%20Understanding%20Capabilities%20for%0A%20%20Embodied%20AI&body=Title%3A%20VidEgoThink%3A%20Assessing%20Egocentric%20Video%20Understanding%20Capabilities%20for%0A%20%20Embodied%20AI%0AAuthor%3A%20Sijie%20Cheng%20and%20Kechen%20Fang%20and%20Yangyang%20Yu%20and%20Sicheng%20Zhou%20and%20Bohao%20Li%20and%20Ye%20Tian%20and%20Tingguang%20Li%20and%20Lei%20Han%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20opened%0Anew%20avenues%20for%20applications%20in%20Embodied%20AI.%20Building%20on%20previous%20work%2C%0AEgoThink%2C%20we%20introduce%20VidEgoThink%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0Aegocentric%20video%20understanding%20capabilities.%20To%20bridge%20the%20gap%20between%20MLLMs%0Aand%20low-level%20control%20in%20Embodied%20AI%2C%20we%20design%20four%20key%20interrelated%20tasks%3A%0Avideo%20question-answering%2C%20hierarchy%20planning%2C%20visual%20grounding%20and%20reward%0Amodeling.%20To%20minimize%20manual%20annotation%20costs%2C%20we%20develop%20an%20automatic%20data%0Ageneration%20pipeline%20based%20on%20the%20Ego4D%20dataset%2C%20leveraging%20the%20prior%20knowledge%0Aand%20multimodal%20capabilities%20of%20GPT-4o.%20Three%20human%20annotators%20then%20filter%20the%0Agenerated%20data%20to%20ensure%20diversity%20and%20quality%2C%20resulting%20in%20the%20VidEgoThink%0Abenchmark.%20We%20conduct%20extensive%20experiments%20with%20three%20types%20of%20models%3A%0AAPI-based%20MLLMs%2C%20open-source%20image-based%20MLLMs%2C%20and%20open-source%20video-based%0AMLLMs.%20Experimental%20results%20indicate%20that%20all%20MLLMs%2C%20including%20GPT-4o%2C%20perform%0Apoorly%20across%20all%20tasks%20related%20to%20egocentric%20video%20understanding.%20These%0Afindings%20suggest%20that%20foundation%20models%20still%20require%20significant%20advancements%0Ato%20be%20effectively%20applied%20to%20first-person%20scenarios%20in%20Embodied%20AI.%20In%0Aconclusion%2C%20VidEgoThink%20reflects%20a%20research%20trend%20towards%20employing%20MLLMs%20for%0Aegocentric%20vision%2C%20akin%20to%20human%20capabilities%2C%20enabling%20active%20observation%20and%0Ainteraction%20in%20the%20complex%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidEgoThink%253A%2520Assessing%2520Egocentric%2520Video%2520Understanding%2520Capabilities%2520for%250A%2520%2520Embodied%2520AI%26entry.906535625%3DSijie%2520Cheng%2520and%2520Kechen%2520Fang%2520and%2520Yangyang%2520Yu%2520and%2520Sicheng%2520Zhou%2520and%2520Bohao%2520Li%2520and%2520Ye%2520Tian%2520and%2520Tingguang%2520Li%2520and%2520Lei%2520Han%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520opened%250Anew%2520avenues%2520for%2520applications%2520in%2520Embodied%2520AI.%2520Building%2520on%2520previous%2520work%252C%250AEgoThink%252C%2520we%2520introduce%2520VidEgoThink%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%250Aegocentric%2520video%2520understanding%2520capabilities.%2520To%2520bridge%2520the%2520gap%2520between%2520MLLMs%250Aand%2520low-level%2520control%2520in%2520Embodied%2520AI%252C%2520we%2520design%2520four%2520key%2520interrelated%2520tasks%253A%250Avideo%2520question-answering%252C%2520hierarchy%2520planning%252C%2520visual%2520grounding%2520and%2520reward%250Amodeling.%2520To%2520minimize%2520manual%2520annotation%2520costs%252C%2520we%2520develop%2520an%2520automatic%2520data%250Ageneration%2520pipeline%2520based%2520on%2520the%2520Ego4D%2520dataset%252C%2520leveraging%2520the%2520prior%2520knowledge%250Aand%2520multimodal%2520capabilities%2520of%2520GPT-4o.%2520Three%2520human%2520annotators%2520then%2520filter%2520the%250Agenerated%2520data%2520to%2520ensure%2520diversity%2520and%2520quality%252C%2520resulting%2520in%2520the%2520VidEgoThink%250Abenchmark.%2520We%2520conduct%2520extensive%2520experiments%2520with%2520three%2520types%2520of%2520models%253A%250AAPI-based%2520MLLMs%252C%2520open-source%2520image-based%2520MLLMs%252C%2520and%2520open-source%2520video-based%250AMLLMs.%2520Experimental%2520results%2520indicate%2520that%2520all%2520MLLMs%252C%2520including%2520GPT-4o%252C%2520perform%250Apoorly%2520across%2520all%2520tasks%2520related%2520to%2520egocentric%2520video%2520understanding.%2520These%250Afindings%2520suggest%2520that%2520foundation%2520models%2520still%2520require%2520significant%2520advancements%250Ato%2520be%2520effectively%2520applied%2520to%2520first-person%2520scenarios%2520in%2520Embodied%2520AI.%2520In%250Aconclusion%252C%2520VidEgoThink%2520reflects%2520a%2520research%2520trend%2520towards%2520employing%2520MLLMs%2520for%250Aegocentric%2520vision%252C%2520akin%2520to%2520human%2520capabilities%252C%2520enabling%2520active%2520observation%2520and%250Ainteraction%2520in%2520the%2520complex%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidEgoThink%3A%20Assessing%20Egocentric%20Video%20Understanding%20Capabilities%20for%0A%20%20Embodied%20AI&entry.906535625=Sijie%20Cheng%20and%20Kechen%20Fang%20and%20Yangyang%20Yu%20and%20Sicheng%20Zhou%20and%20Bohao%20Li%20and%20Ye%20Tian%20and%20Tingguang%20Li%20and%20Lei%20Han%20and%20Yang%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20opened%0Anew%20avenues%20for%20applications%20in%20Embodied%20AI.%20Building%20on%20previous%20work%2C%0AEgoThink%2C%20we%20introduce%20VidEgoThink%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0Aegocentric%20video%20understanding%20capabilities.%20To%20bridge%20the%20gap%20between%20MLLMs%0Aand%20low-level%20control%20in%20Embodied%20AI%2C%20we%20design%20four%20key%20interrelated%20tasks%3A%0Avideo%20question-answering%2C%20hierarchy%20planning%2C%20visual%20grounding%20and%20reward%0Amodeling.%20To%20minimize%20manual%20annotation%20costs%2C%20we%20develop%20an%20automatic%20data%0Ageneration%20pipeline%20based%20on%20the%20Ego4D%20dataset%2C%20leveraging%20the%20prior%20knowledge%0Aand%20multimodal%20capabilities%20of%20GPT-4o.%20Three%20human%20annotators%20then%20filter%20the%0Agenerated%20data%20to%20ensure%20diversity%20and%20quality%2C%20resulting%20in%20the%20VidEgoThink%0Abenchmark.%20We%20conduct%20extensive%20experiments%20with%20three%20types%20of%20models%3A%0AAPI-based%20MLLMs%2C%20open-source%20image-based%20MLLMs%2C%20and%20open-source%20video-based%0AMLLMs.%20Experimental%20results%20indicate%20that%20all%20MLLMs%2C%20including%20GPT-4o%2C%20perform%0Apoorly%20across%20all%20tasks%20related%20to%20egocentric%20video%20understanding.%20These%0Afindings%20suggest%20that%20foundation%20models%20still%20require%20significant%20advancements%0Ato%20be%20effectively%20applied%20to%20first-person%20scenarios%20in%20Embodied%20AI.%20In%0Aconclusion%2C%20VidEgoThink%20reflects%20a%20research%20trend%20towards%20employing%20MLLMs%20for%0Aegocentric%20vision%2C%20akin%20to%20human%20capabilities%2C%20enabling%20active%20observation%20and%0Ainteraction%20in%20the%20complex%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11623v1&entry.124074799=Read"},
{"title": "Unveiling the Mystery of Visual Attributes of Concrete and Abstract\n  Concepts: Variability, Nearest Neighbors, and Challenging Categories", "author": "Tarun Tater and Sabine Schulte im Walde and Diego Frassinelli", "abstract": "  The visual representation of a concept varies significantly depending on its\nmeaning and the context where it occurs; this poses multiple challenges both\nfor vision and multimodal models. Our study focuses on concreteness, a\nwell-researched lexical-semantic variable, using it as a case study to examine\nthe variability in visual representations. We rely on images associated with\napproximately 1,000 abstract and concrete concepts extracted from two different\ndatasets: Bing and YFCC. Our goals are: (i) evaluate whether visual diversity\nin the depiction of concepts can reliably distinguish between concrete and\nabstract concepts; (ii) analyze the variability of visual features across\nmultiple images of the same concept through a nearest neighbor analysis; and\n(iii) identify challenging factors contributing to this variability by\ncategorizing and annotating images. Our findings indicate that for classifying\nimages of abstract versus concrete concepts, a combination of basic visual\nfeatures such as color and texture is more effective than features extracted by\nmore complex models like Vision Transformer (ViT). However, ViTs show better\nperformances in the nearest neighbor analysis, emphasizing the need for a\ncareful selection of visual features when analyzing conceptual variables\nthrough modalities other than text.\n", "link": "http://arxiv.org/abs/2410.11657v1", "date": "2024-10-15", "relevancy": 2.9976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Mystery%20of%20Visual%20Attributes%20of%20Concrete%20and%20Abstract%0A%20%20Concepts%3A%20Variability%2C%20Nearest%20Neighbors%2C%20and%20Challenging%20Categories&body=Title%3A%20Unveiling%20the%20Mystery%20of%20Visual%20Attributes%20of%20Concrete%20and%20Abstract%0A%20%20Concepts%3A%20Variability%2C%20Nearest%20Neighbors%2C%20and%20Challenging%20Categories%0AAuthor%3A%20Tarun%20Tater%20and%20Sabine%20Schulte%20im%20Walde%20and%20Diego%20Frassinelli%0AAbstract%3A%20%20%20The%20visual%20representation%20of%20a%20concept%20varies%20significantly%20depending%20on%20its%0Ameaning%20and%20the%20context%20where%20it%20occurs%3B%20this%20poses%20multiple%20challenges%20both%0Afor%20vision%20and%20multimodal%20models.%20Our%20study%20focuses%20on%20concreteness%2C%20a%0Awell-researched%20lexical-semantic%20variable%2C%20using%20it%20as%20a%20case%20study%20to%20examine%0Athe%20variability%20in%20visual%20representations.%20We%20rely%20on%20images%20associated%20with%0Aapproximately%201%2C000%20abstract%20and%20concrete%20concepts%20extracted%20from%20two%20different%0Adatasets%3A%20Bing%20and%20YFCC.%20Our%20goals%20are%3A%20%28i%29%20evaluate%20whether%20visual%20diversity%0Ain%20the%20depiction%20of%20concepts%20can%20reliably%20distinguish%20between%20concrete%20and%0Aabstract%20concepts%3B%20%28ii%29%20analyze%20the%20variability%20of%20visual%20features%20across%0Amultiple%20images%20of%20the%20same%20concept%20through%20a%20nearest%20neighbor%20analysis%3B%20and%0A%28iii%29%20identify%20challenging%20factors%20contributing%20to%20this%20variability%20by%0Acategorizing%20and%20annotating%20images.%20Our%20findings%20indicate%20that%20for%20classifying%0Aimages%20of%20abstract%20versus%20concrete%20concepts%2C%20a%20combination%20of%20basic%20visual%0Afeatures%20such%20as%20color%20and%20texture%20is%20more%20effective%20than%20features%20extracted%20by%0Amore%20complex%20models%20like%20Vision%20Transformer%20%28ViT%29.%20However%2C%20ViTs%20show%20better%0Aperformances%20in%20the%20nearest%20neighbor%20analysis%2C%20emphasizing%20the%20need%20for%20a%0Acareful%20selection%20of%20visual%20features%20when%20analyzing%20conceptual%20variables%0Athrough%20modalities%20other%20than%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Mystery%2520of%2520Visual%2520Attributes%2520of%2520Concrete%2520and%2520Abstract%250A%2520%2520Concepts%253A%2520Variability%252C%2520Nearest%2520Neighbors%252C%2520and%2520Challenging%2520Categories%26entry.906535625%3DTarun%2520Tater%2520and%2520Sabine%2520Schulte%2520im%2520Walde%2520and%2520Diego%2520Frassinelli%26entry.1292438233%3D%2520%2520The%2520visual%2520representation%2520of%2520a%2520concept%2520varies%2520significantly%2520depending%2520on%2520its%250Ameaning%2520and%2520the%2520context%2520where%2520it%2520occurs%253B%2520this%2520poses%2520multiple%2520challenges%2520both%250Afor%2520vision%2520and%2520multimodal%2520models.%2520Our%2520study%2520focuses%2520on%2520concreteness%252C%2520a%250Awell-researched%2520lexical-semantic%2520variable%252C%2520using%2520it%2520as%2520a%2520case%2520study%2520to%2520examine%250Athe%2520variability%2520in%2520visual%2520representations.%2520We%2520rely%2520on%2520images%2520associated%2520with%250Aapproximately%25201%252C000%2520abstract%2520and%2520concrete%2520concepts%2520extracted%2520from%2520two%2520different%250Adatasets%253A%2520Bing%2520and%2520YFCC.%2520Our%2520goals%2520are%253A%2520%2528i%2529%2520evaluate%2520whether%2520visual%2520diversity%250Ain%2520the%2520depiction%2520of%2520concepts%2520can%2520reliably%2520distinguish%2520between%2520concrete%2520and%250Aabstract%2520concepts%253B%2520%2528ii%2529%2520analyze%2520the%2520variability%2520of%2520visual%2520features%2520across%250Amultiple%2520images%2520of%2520the%2520same%2520concept%2520through%2520a%2520nearest%2520neighbor%2520analysis%253B%2520and%250A%2528iii%2529%2520identify%2520challenging%2520factors%2520contributing%2520to%2520this%2520variability%2520by%250Acategorizing%2520and%2520annotating%2520images.%2520Our%2520findings%2520indicate%2520that%2520for%2520classifying%250Aimages%2520of%2520abstract%2520versus%2520concrete%2520concepts%252C%2520a%2520combination%2520of%2520basic%2520visual%250Afeatures%2520such%2520as%2520color%2520and%2520texture%2520is%2520more%2520effective%2520than%2520features%2520extracted%2520by%250Amore%2520complex%2520models%2520like%2520Vision%2520Transformer%2520%2528ViT%2529.%2520However%252C%2520ViTs%2520show%2520better%250Aperformances%2520in%2520the%2520nearest%2520neighbor%2520analysis%252C%2520emphasizing%2520the%2520need%2520for%2520a%250Acareful%2520selection%2520of%2520visual%2520features%2520when%2520analyzing%2520conceptual%2520variables%250Athrough%2520modalities%2520other%2520than%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Mystery%20of%20Visual%20Attributes%20of%20Concrete%20and%20Abstract%0A%20%20Concepts%3A%20Variability%2C%20Nearest%20Neighbors%2C%20and%20Challenging%20Categories&entry.906535625=Tarun%20Tater%20and%20Sabine%20Schulte%20im%20Walde%20and%20Diego%20Frassinelli&entry.1292438233=%20%20The%20visual%20representation%20of%20a%20concept%20varies%20significantly%20depending%20on%20its%0Ameaning%20and%20the%20context%20where%20it%20occurs%3B%20this%20poses%20multiple%20challenges%20both%0Afor%20vision%20and%20multimodal%20models.%20Our%20study%20focuses%20on%20concreteness%2C%20a%0Awell-researched%20lexical-semantic%20variable%2C%20using%20it%20as%20a%20case%20study%20to%20examine%0Athe%20variability%20in%20visual%20representations.%20We%20rely%20on%20images%20associated%20with%0Aapproximately%201%2C000%20abstract%20and%20concrete%20concepts%20extracted%20from%20two%20different%0Adatasets%3A%20Bing%20and%20YFCC.%20Our%20goals%20are%3A%20%28i%29%20evaluate%20whether%20visual%20diversity%0Ain%20the%20depiction%20of%20concepts%20can%20reliably%20distinguish%20between%20concrete%20and%0Aabstract%20concepts%3B%20%28ii%29%20analyze%20the%20variability%20of%20visual%20features%20across%0Amultiple%20images%20of%20the%20same%20concept%20through%20a%20nearest%20neighbor%20analysis%3B%20and%0A%28iii%29%20identify%20challenging%20factors%20contributing%20to%20this%20variability%20by%0Acategorizing%20and%20annotating%20images.%20Our%20findings%20indicate%20that%20for%20classifying%0Aimages%20of%20abstract%20versus%20concrete%20concepts%2C%20a%20combination%20of%20basic%20visual%0Afeatures%20such%20as%20color%20and%20texture%20is%20more%20effective%20than%20features%20extracted%20by%0Amore%20complex%20models%20like%20Vision%20Transformer%20%28ViT%29.%20However%2C%20ViTs%20show%20better%0Aperformances%20in%20the%20nearest%20neighbor%20analysis%2C%20emphasizing%20the%20need%20for%20a%0Acareful%20selection%20of%20visual%20features%20when%20analyzing%20conceptual%20variables%0Athrough%20modalities%20other%20than%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11657v1&entry.124074799=Read"},
{"title": "A3D: Does Diffusion Dream about 3D Alignment?", "author": "Savva Ignatyev and Nina Konovalova and Daniil Selikhanovych and Oleg Voynov and Nikolay Patakin and Ilya Olkov and Dmitry Senushkin and Alexey Artemov and Anton Konushin and Alexander Filippov and Peter Wonka and Evgeny Burnaev", "abstract": "  We tackle the problem of text-driven 3D generation from a geometry alignment\nperspective. Given a set of text prompts, we aim to generate a collection of\nobjects with semantically corresponding parts aligned across them. Recent\nmethods based on Score Distillation have succeeded in distilling the knowledge\nfrom 2D diffusion models to high-quality representations of the 3D objects.\nThese methods handle multiple text queries separately, and therefore the\nresulting objects have a high variability in object pose and structure.\nHowever, in some applications, such as 3D asset design, it may be desirable to\nobtain a set of objects aligned with each other. In order to achieve the\nalignment of the corresponding parts of the generated objects, we propose to\nembed these objects into a common latent space and optimize the continuous\ntransitions between these objects. We enforce two kinds of properties of these\ntransitions: smoothness of the transition and plausibility of the intermediate\nobjects along the transition. We demonstrate that both of these properties are\nessential for good alignment. We provide several practical scenarios that\nbenefit from alignment between the objects, including 3D editing and object\nhybridization, and experimentally demonstrate the effectiveness of our method.\n\\href{https://voyleg.github.io/a3d/}{voyleg.github.io/a3d}\n", "link": "http://arxiv.org/abs/2406.15020v2", "date": "2024-10-15", "relevancy": 2.9886, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A3D%3A%20Does%20Diffusion%20Dream%20about%203D%20Alignment%3F&body=Title%3A%20A3D%3A%20Does%20Diffusion%20Dream%20about%203D%20Alignment%3F%0AAuthor%3A%20Savva%20Ignatyev%20and%20Nina%20Konovalova%20and%20Daniil%20Selikhanovych%20and%20Oleg%20Voynov%20and%20Nikolay%20Patakin%20and%20Ilya%20Olkov%20and%20Dmitry%20Senushkin%20and%20Alexey%20Artemov%20and%20Anton%20Konushin%20and%20Alexander%20Filippov%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20text-driven%203D%20generation%20from%20a%20geometry%20alignment%0Aperspective.%20Given%20a%20set%20of%20text%20prompts%2C%20we%20aim%20to%20generate%20a%20collection%20of%0Aobjects%20with%20semantically%20corresponding%20parts%20aligned%20across%20them.%20Recent%0Amethods%20based%20on%20Score%20Distillation%20have%20succeeded%20in%20distilling%20the%20knowledge%0Afrom%202D%20diffusion%20models%20to%20high-quality%20representations%20of%20the%203D%20objects.%0AThese%20methods%20handle%20multiple%20text%20queries%20separately%2C%20and%20therefore%20the%0Aresulting%20objects%20have%20a%20high%20variability%20in%20object%20pose%20and%20structure.%0AHowever%2C%20in%20some%20applications%2C%20such%20as%203D%20asset%20design%2C%20it%20may%20be%20desirable%20to%0Aobtain%20a%20set%20of%20objects%20aligned%20with%20each%20other.%20In%20order%20to%20achieve%20the%0Aalignment%20of%20the%20corresponding%20parts%20of%20the%20generated%20objects%2C%20we%20propose%20to%0Aembed%20these%20objects%20into%20a%20common%20latent%20space%20and%20optimize%20the%20continuous%0Atransitions%20between%20these%20objects.%20We%20enforce%20two%20kinds%20of%20properties%20of%20these%0Atransitions%3A%20smoothness%20of%20the%20transition%20and%20plausibility%20of%20the%20intermediate%0Aobjects%20along%20the%20transition.%20We%20demonstrate%20that%20both%20of%20these%20properties%20are%0Aessential%20for%20good%20alignment.%20We%20provide%20several%20practical%20scenarios%20that%0Abenefit%20from%20alignment%20between%20the%20objects%2C%20including%203D%20editing%20and%20object%0Ahybridization%2C%20and%20experimentally%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A%5Chref%7Bhttps%3A//voyleg.github.io/a3d/%7D%7Bvoyleg.github.io/a3d%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA3D%253A%2520Does%2520Diffusion%2520Dream%2520about%25203D%2520Alignment%253F%26entry.906535625%3DSavva%2520Ignatyev%2520and%2520Nina%2520Konovalova%2520and%2520Daniil%2520Selikhanovych%2520and%2520Oleg%2520Voynov%2520and%2520Nikolay%2520Patakin%2520and%2520Ilya%2520Olkov%2520and%2520Dmitry%2520Senushkin%2520and%2520Alexey%2520Artemov%2520and%2520Anton%2520Konushin%2520and%2520Alexander%2520Filippov%2520and%2520Peter%2520Wonka%2520and%2520Evgeny%2520Burnaev%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520text-driven%25203D%2520generation%2520from%2520a%2520geometry%2520alignment%250Aperspective.%2520Given%2520a%2520set%2520of%2520text%2520prompts%252C%2520we%2520aim%2520to%2520generate%2520a%2520collection%2520of%250Aobjects%2520with%2520semantically%2520corresponding%2520parts%2520aligned%2520across%2520them.%2520Recent%250Amethods%2520based%2520on%2520Score%2520Distillation%2520have%2520succeeded%2520in%2520distilling%2520the%2520knowledge%250Afrom%25202D%2520diffusion%2520models%2520to%2520high-quality%2520representations%2520of%2520the%25203D%2520objects.%250AThese%2520methods%2520handle%2520multiple%2520text%2520queries%2520separately%252C%2520and%2520therefore%2520the%250Aresulting%2520objects%2520have%2520a%2520high%2520variability%2520in%2520object%2520pose%2520and%2520structure.%250AHowever%252C%2520in%2520some%2520applications%252C%2520such%2520as%25203D%2520asset%2520design%252C%2520it%2520may%2520be%2520desirable%2520to%250Aobtain%2520a%2520set%2520of%2520objects%2520aligned%2520with%2520each%2520other.%2520In%2520order%2520to%2520achieve%2520the%250Aalignment%2520of%2520the%2520corresponding%2520parts%2520of%2520the%2520generated%2520objects%252C%2520we%2520propose%2520to%250Aembed%2520these%2520objects%2520into%2520a%2520common%2520latent%2520space%2520and%2520optimize%2520the%2520continuous%250Atransitions%2520between%2520these%2520objects.%2520We%2520enforce%2520two%2520kinds%2520of%2520properties%2520of%2520these%250Atransitions%253A%2520smoothness%2520of%2520the%2520transition%2520and%2520plausibility%2520of%2520the%2520intermediate%250Aobjects%2520along%2520the%2520transition.%2520We%2520demonstrate%2520that%2520both%2520of%2520these%2520properties%2520are%250Aessential%2520for%2520good%2520alignment.%2520We%2520provide%2520several%2520practical%2520scenarios%2520that%250Abenefit%2520from%2520alignment%2520between%2520the%2520objects%252C%2520including%25203D%2520editing%2520and%2520object%250Ahybridization%252C%2520and%2520experimentally%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%250A%255Chref%257Bhttps%253A//voyleg.github.io/a3d/%257D%257Bvoyleg.github.io/a3d%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A3D%3A%20Does%20Diffusion%20Dream%20about%203D%20Alignment%3F&entry.906535625=Savva%20Ignatyev%20and%20Nina%20Konovalova%20and%20Daniil%20Selikhanovych%20and%20Oleg%20Voynov%20and%20Nikolay%20Patakin%20and%20Ilya%20Olkov%20and%20Dmitry%20Senushkin%20and%20Alexey%20Artemov%20and%20Anton%20Konushin%20and%20Alexander%20Filippov%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20text-driven%203D%20generation%20from%20a%20geometry%20alignment%0Aperspective.%20Given%20a%20set%20of%20text%20prompts%2C%20we%20aim%20to%20generate%20a%20collection%20of%0Aobjects%20with%20semantically%20corresponding%20parts%20aligned%20across%20them.%20Recent%0Amethods%20based%20on%20Score%20Distillation%20have%20succeeded%20in%20distilling%20the%20knowledge%0Afrom%202D%20diffusion%20models%20to%20high-quality%20representations%20of%20the%203D%20objects.%0AThese%20methods%20handle%20multiple%20text%20queries%20separately%2C%20and%20therefore%20the%0Aresulting%20objects%20have%20a%20high%20variability%20in%20object%20pose%20and%20structure.%0AHowever%2C%20in%20some%20applications%2C%20such%20as%203D%20asset%20design%2C%20it%20may%20be%20desirable%20to%0Aobtain%20a%20set%20of%20objects%20aligned%20with%20each%20other.%20In%20order%20to%20achieve%20the%0Aalignment%20of%20the%20corresponding%20parts%20of%20the%20generated%20objects%2C%20we%20propose%20to%0Aembed%20these%20objects%20into%20a%20common%20latent%20space%20and%20optimize%20the%20continuous%0Atransitions%20between%20these%20objects.%20We%20enforce%20two%20kinds%20of%20properties%20of%20these%0Atransitions%3A%20smoothness%20of%20the%20transition%20and%20plausibility%20of%20the%20intermediate%0Aobjects%20along%20the%20transition.%20We%20demonstrate%20that%20both%20of%20these%20properties%20are%0Aessential%20for%20good%20alignment.%20We%20provide%20several%20practical%20scenarios%20that%0Abenefit%20from%20alignment%20between%20the%20objects%2C%20including%203D%20editing%20and%20object%0Ahybridization%2C%20and%20experimentally%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A%5Chref%7Bhttps%3A//voyleg.github.io/a3d/%7D%7Bvoyleg.github.io/a3d%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15020v2&entry.124074799=Read"},
{"title": "A Survey of Low-shot Vision-Language Model Adaptation via Representer\n  Theorem", "author": "Kun Ding and Ying Wang and Gaofeng Meng and Shiming Xiang", "abstract": "  The advent of pre-trained vision-language foundation models has\nrevolutionized the field of zero/few-shot (i.e., low-shot) image recognition.\nThe key challenge to address under the condition of limited training data is\nhow to fine-tune pre-trained vision-language models in a parameter-efficient\nmanner. Previously, numerous approaches tackling this challenge have been\nproposed. Meantime, a few survey papers are also published to summarize these\nworks. However, there still lacks a unified computational framework to\nintegrate existing methods together, identify their nature and support in-depth\ncomparison. As such, this survey paper first proposes a unified computational\nframework from the perspective of Representer Theorem and then derives many of\nthe existing methods by specializing this framework. Thereafter, a comparative\nanalysis is conducted to uncover the differences and relationships between\nexisting methods. Based on the analyses, some possible variants to improve the\nexisting works are presented. As a demonstration, we extend existing methods by\nmodeling inter-class correlation between representers in reproducing kernel\nHilbert space (RKHS), which is implemented by exploiting the closed-form\nsolution of kernel ridge regression. Extensive experiments on 11 datasets are\nconducted to validate the effectiveness of this method. Toward the end of this\npaper, we discuss the limitations and provide further research directions.\n", "link": "http://arxiv.org/abs/2410.11686v1", "date": "2024-10-15", "relevancy": 2.9695, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6027}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Low-shot%20Vision-Language%20Model%20Adaptation%20via%20Representer%0A%20%20Theorem&body=Title%3A%20A%20Survey%20of%20Low-shot%20Vision-Language%20Model%20Adaptation%20via%20Representer%0A%20%20Theorem%0AAuthor%3A%20Kun%20Ding%20and%20Ying%20Wang%20and%20Gaofeng%20Meng%20and%20Shiming%20Xiang%0AAbstract%3A%20%20%20The%20advent%20of%20pre-trained%20vision-language%20foundation%20models%20has%0Arevolutionized%20the%20field%20of%20zero/few-shot%20%28i.e.%2C%20low-shot%29%20image%20recognition.%0AThe%20key%20challenge%20to%20address%20under%20the%20condition%20of%20limited%20training%20data%20is%0Ahow%20to%20fine-tune%20pre-trained%20vision-language%20models%20in%20a%20parameter-efficient%0Amanner.%20Previously%2C%20numerous%20approaches%20tackling%20this%20challenge%20have%20been%0Aproposed.%20Meantime%2C%20a%20few%20survey%20papers%20are%20also%20published%20to%20summarize%20these%0Aworks.%20However%2C%20there%20still%20lacks%20a%20unified%20computational%20framework%20to%0Aintegrate%20existing%20methods%20together%2C%20identify%20their%20nature%20and%20support%20in-depth%0Acomparison.%20As%20such%2C%20this%20survey%20paper%20first%20proposes%20a%20unified%20computational%0Aframework%20from%20the%20perspective%20of%20Representer%20Theorem%20and%20then%20derives%20many%20of%0Athe%20existing%20methods%20by%20specializing%20this%20framework.%20Thereafter%2C%20a%20comparative%0Aanalysis%20is%20conducted%20to%20uncover%20the%20differences%20and%20relationships%20between%0Aexisting%20methods.%20Based%20on%20the%20analyses%2C%20some%20possible%20variants%20to%20improve%20the%0Aexisting%20works%20are%20presented.%20As%20a%20demonstration%2C%20we%20extend%20existing%20methods%20by%0Amodeling%20inter-class%20correlation%20between%20representers%20in%20reproducing%20kernel%0AHilbert%20space%20%28RKHS%29%2C%20which%20is%20implemented%20by%20exploiting%20the%20closed-form%0Asolution%20of%20kernel%20ridge%20regression.%20Extensive%20experiments%20on%2011%20datasets%20are%0Aconducted%20to%20validate%20the%20effectiveness%20of%20this%20method.%20Toward%20the%20end%20of%20this%0Apaper%2C%20we%20discuss%20the%20limitations%20and%20provide%20further%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Low-shot%2520Vision-Language%2520Model%2520Adaptation%2520via%2520Representer%250A%2520%2520Theorem%26entry.906535625%3DKun%2520Ding%2520and%2520Ying%2520Wang%2520and%2520Gaofeng%2520Meng%2520and%2520Shiming%2520Xiang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520pre-trained%2520vision-language%2520foundation%2520models%2520has%250Arevolutionized%2520the%2520field%2520of%2520zero/few-shot%2520%2528i.e.%252C%2520low-shot%2529%2520image%2520recognition.%250AThe%2520key%2520challenge%2520to%2520address%2520under%2520the%2520condition%2520of%2520limited%2520training%2520data%2520is%250Ahow%2520to%2520fine-tune%2520pre-trained%2520vision-language%2520models%2520in%2520a%2520parameter-efficient%250Amanner.%2520Previously%252C%2520numerous%2520approaches%2520tackling%2520this%2520challenge%2520have%2520been%250Aproposed.%2520Meantime%252C%2520a%2520few%2520survey%2520papers%2520are%2520also%2520published%2520to%2520summarize%2520these%250Aworks.%2520However%252C%2520there%2520still%2520lacks%2520a%2520unified%2520computational%2520framework%2520to%250Aintegrate%2520existing%2520methods%2520together%252C%2520identify%2520their%2520nature%2520and%2520support%2520in-depth%250Acomparison.%2520As%2520such%252C%2520this%2520survey%2520paper%2520first%2520proposes%2520a%2520unified%2520computational%250Aframework%2520from%2520the%2520perspective%2520of%2520Representer%2520Theorem%2520and%2520then%2520derives%2520many%2520of%250Athe%2520existing%2520methods%2520by%2520specializing%2520this%2520framework.%2520Thereafter%252C%2520a%2520comparative%250Aanalysis%2520is%2520conducted%2520to%2520uncover%2520the%2520differences%2520and%2520relationships%2520between%250Aexisting%2520methods.%2520Based%2520on%2520the%2520analyses%252C%2520some%2520possible%2520variants%2520to%2520improve%2520the%250Aexisting%2520works%2520are%2520presented.%2520As%2520a%2520demonstration%252C%2520we%2520extend%2520existing%2520methods%2520by%250Amodeling%2520inter-class%2520correlation%2520between%2520representers%2520in%2520reproducing%2520kernel%250AHilbert%2520space%2520%2528RKHS%2529%252C%2520which%2520is%2520implemented%2520by%2520exploiting%2520the%2520closed-form%250Asolution%2520of%2520kernel%2520ridge%2520regression.%2520Extensive%2520experiments%2520on%252011%2520datasets%2520are%250Aconducted%2520to%2520validate%2520the%2520effectiveness%2520of%2520this%2520method.%2520Toward%2520the%2520end%2520of%2520this%250Apaper%252C%2520we%2520discuss%2520the%2520limitations%2520and%2520provide%2520further%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Low-shot%20Vision-Language%20Model%20Adaptation%20via%20Representer%0A%20%20Theorem&entry.906535625=Kun%20Ding%20and%20Ying%20Wang%20and%20Gaofeng%20Meng%20and%20Shiming%20Xiang&entry.1292438233=%20%20The%20advent%20of%20pre-trained%20vision-language%20foundation%20models%20has%0Arevolutionized%20the%20field%20of%20zero/few-shot%20%28i.e.%2C%20low-shot%29%20image%20recognition.%0AThe%20key%20challenge%20to%20address%20under%20the%20condition%20of%20limited%20training%20data%20is%0Ahow%20to%20fine-tune%20pre-trained%20vision-language%20models%20in%20a%20parameter-efficient%0Amanner.%20Previously%2C%20numerous%20approaches%20tackling%20this%20challenge%20have%20been%0Aproposed.%20Meantime%2C%20a%20few%20survey%20papers%20are%20also%20published%20to%20summarize%20these%0Aworks.%20However%2C%20there%20still%20lacks%20a%20unified%20computational%20framework%20to%0Aintegrate%20existing%20methods%20together%2C%20identify%20their%20nature%20and%20support%20in-depth%0Acomparison.%20As%20such%2C%20this%20survey%20paper%20first%20proposes%20a%20unified%20computational%0Aframework%20from%20the%20perspective%20of%20Representer%20Theorem%20and%20then%20derives%20many%20of%0Athe%20existing%20methods%20by%20specializing%20this%20framework.%20Thereafter%2C%20a%20comparative%0Aanalysis%20is%20conducted%20to%20uncover%20the%20differences%20and%20relationships%20between%0Aexisting%20methods.%20Based%20on%20the%20analyses%2C%20some%20possible%20variants%20to%20improve%20the%0Aexisting%20works%20are%20presented.%20As%20a%20demonstration%2C%20we%20extend%20existing%20methods%20by%0Amodeling%20inter-class%20correlation%20between%20representers%20in%20reproducing%20kernel%0AHilbert%20space%20%28RKHS%29%2C%20which%20is%20implemented%20by%20exploiting%20the%20closed-form%0Asolution%20of%20kernel%20ridge%20regression.%20Extensive%20experiments%20on%2011%20datasets%20are%0Aconducted%20to%20validate%20the%20effectiveness%20of%20this%20method.%20Toward%20the%20end%20of%20this%0Apaper%2C%20we%20discuss%20the%20limitations%20and%20provide%20further%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11686v1&entry.124074799=Read"},
{"title": "Degradation Oriented and Regularized Network for Real-World Depth\n  Super-Resolution", "author": "Zhengxue Wang and Zhiqiang Yan", "abstract": "  Recently, existing RGB-guided depth super-resolution methods achieve\nexcellent performance based on the assumption of fixed and known degradation\n(e.g., bicubic downsampling). However, in real-world scenarios, the captured\ndepth often suffers from unconventional and agnostic degradation due to sensor\nlimitations and the complexity of imaging environments (e.g., low reflective\nsurface, illumination). Their performance significantly declines when these\nreal degradation differ from their assumptions. To address these issues, we\npropose a Degradation Oriented and Regularized Network, DORNet, which pays more\nattention on learning degradation representation of low-resolution depth that\ncan provide targeted guidance for depth recovery. Specifically, we first design\na self-supervised Degradation Learning to model the discriminative degradation\nrepresentation of low-resolution depth using routing selection-based\nDegradation Regularization. Then, we present a Degradation Awareness that\nrecursively conducts multiple Degradation-Oriented Feature Transformations,\neach of which selectively embeds RGB information into the depth based on the\nlearned degradation representation. Extensive experimental results on both real\nand synthetic datasets demonstrate that our method achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2410.11666v1", "date": "2024-10-15", "relevancy": 2.9289, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5814}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degradation%20Oriented%20and%20Regularized%20Network%20for%20Real-World%20Depth%0A%20%20Super-Resolution&body=Title%3A%20Degradation%20Oriented%20and%20Regularized%20Network%20for%20Real-World%20Depth%0A%20%20Super-Resolution%0AAuthor%3A%20Zhengxue%20Wang%20and%20Zhiqiang%20Yan%0AAbstract%3A%20%20%20Recently%2C%20existing%20RGB-guided%20depth%20super-resolution%20methods%20achieve%0Aexcellent%20performance%20based%20on%20the%20assumption%20of%20fixed%20and%20known%20degradation%0A%28e.g.%2C%20bicubic%20downsampling%29.%20However%2C%20in%20real-world%20scenarios%2C%20the%20captured%0Adepth%20often%20suffers%20from%20unconventional%20and%20agnostic%20degradation%20due%20to%20sensor%0Alimitations%20and%20the%20complexity%20of%20imaging%20environments%20%28e.g.%2C%20low%20reflective%0Asurface%2C%20illumination%29.%20Their%20performance%20significantly%20declines%20when%20these%0Areal%20degradation%20differ%20from%20their%20assumptions.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Degradation%20Oriented%20and%20Regularized%20Network%2C%20DORNet%2C%20which%20pays%20more%0Aattention%20on%20learning%20degradation%20representation%20of%20low-resolution%20depth%20that%0Acan%20provide%20targeted%20guidance%20for%20depth%20recovery.%20Specifically%2C%20we%20first%20design%0Aa%20self-supervised%20Degradation%20Learning%20to%20model%20the%20discriminative%20degradation%0Arepresentation%20of%20low-resolution%20depth%20using%20routing%20selection-based%0ADegradation%20Regularization.%20Then%2C%20we%20present%20a%20Degradation%20Awareness%20that%0Arecursively%20conducts%20multiple%20Degradation-Oriented%20Feature%20Transformations%2C%0Aeach%20of%20which%20selectively%20embeds%20RGB%20information%20into%20the%20depth%20based%20on%20the%0Alearned%20degradation%20representation.%20Extensive%20experimental%20results%20on%20both%20real%0Aand%20synthetic%20datasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegradation%2520Oriented%2520and%2520Regularized%2520Network%2520for%2520Real-World%2520Depth%250A%2520%2520Super-Resolution%26entry.906535625%3DZhengxue%2520Wang%2520and%2520Zhiqiang%2520Yan%26entry.1292438233%3D%2520%2520Recently%252C%2520existing%2520RGB-guided%2520depth%2520super-resolution%2520methods%2520achieve%250Aexcellent%2520performance%2520based%2520on%2520the%2520assumption%2520of%2520fixed%2520and%2520known%2520degradation%250A%2528e.g.%252C%2520bicubic%2520downsampling%2529.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520the%2520captured%250Adepth%2520often%2520suffers%2520from%2520unconventional%2520and%2520agnostic%2520degradation%2520due%2520to%2520sensor%250Alimitations%2520and%2520the%2520complexity%2520of%2520imaging%2520environments%2520%2528e.g.%252C%2520low%2520reflective%250Asurface%252C%2520illumination%2529.%2520Their%2520performance%2520significantly%2520declines%2520when%2520these%250Areal%2520degradation%2520differ%2520from%2520their%2520assumptions.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520Degradation%2520Oriented%2520and%2520Regularized%2520Network%252C%2520DORNet%252C%2520which%2520pays%2520more%250Aattention%2520on%2520learning%2520degradation%2520representation%2520of%2520low-resolution%2520depth%2520that%250Acan%2520provide%2520targeted%2520guidance%2520for%2520depth%2520recovery.%2520Specifically%252C%2520we%2520first%2520design%250Aa%2520self-supervised%2520Degradation%2520Learning%2520to%2520model%2520the%2520discriminative%2520degradation%250Arepresentation%2520of%2520low-resolution%2520depth%2520using%2520routing%2520selection-based%250ADegradation%2520Regularization.%2520Then%252C%2520we%2520present%2520a%2520Degradation%2520Awareness%2520that%250Arecursively%2520conducts%2520multiple%2520Degradation-Oriented%2520Feature%2520Transformations%252C%250Aeach%2520of%2520which%2520selectively%2520embeds%2520RGB%2520information%2520into%2520the%2520depth%2520based%2520on%2520the%250Alearned%2520degradation%2520representation.%2520Extensive%2520experimental%2520results%2520on%2520both%2520real%250Aand%2520synthetic%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degradation%20Oriented%20and%20Regularized%20Network%20for%20Real-World%20Depth%0A%20%20Super-Resolution&entry.906535625=Zhengxue%20Wang%20and%20Zhiqiang%20Yan&entry.1292438233=%20%20Recently%2C%20existing%20RGB-guided%20depth%20super-resolution%20methods%20achieve%0Aexcellent%20performance%20based%20on%20the%20assumption%20of%20fixed%20and%20known%20degradation%0A%28e.g.%2C%20bicubic%20downsampling%29.%20However%2C%20in%20real-world%20scenarios%2C%20the%20captured%0Adepth%20often%20suffers%20from%20unconventional%20and%20agnostic%20degradation%20due%20to%20sensor%0Alimitations%20and%20the%20complexity%20of%20imaging%20environments%20%28e.g.%2C%20low%20reflective%0Asurface%2C%20illumination%29.%20Their%20performance%20significantly%20declines%20when%20these%0Areal%20degradation%20differ%20from%20their%20assumptions.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Degradation%20Oriented%20and%20Regularized%20Network%2C%20DORNet%2C%20which%20pays%20more%0Aattention%20on%20learning%20degradation%20representation%20of%20low-resolution%20depth%20that%0Acan%20provide%20targeted%20guidance%20for%20depth%20recovery.%20Specifically%2C%20we%20first%20design%0Aa%20self-supervised%20Degradation%20Learning%20to%20model%20the%20discriminative%20degradation%0Arepresentation%20of%20low-resolution%20depth%20using%20routing%20selection-based%0ADegradation%20Regularization.%20Then%2C%20we%20present%20a%20Degradation%20Awareness%20that%0Arecursively%20conducts%20multiple%20Degradation-Oriented%20Feature%20Transformations%2C%0Aeach%20of%20which%20selectively%20embeds%20RGB%20information%20into%20the%20depth%20based%20on%20the%0Alearned%20degradation%20representation.%20Extensive%20experimental%20results%20on%20both%20real%0Aand%20synthetic%20datasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11666v1&entry.124074799=Read"},
{"title": "Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of\n  Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery", "author": "Willow Liu and Shuxin Qiao and Kyle Gao and Hongjie He and Michael A. Chapman and Linlin Xu and Jonathan Li", "abstract": "  This research addresses the need for high-definition (HD) maps for autonomous\nvehicles (AVs), focusing on road lane information derived from aerial imagery.\nWhile Earth observation data offers valuable resources for map creation,\nspecialized models for road lane extraction are still underdeveloped in remote\nsensing. In this study, we perform an extensive comparison of twelve\nfoundational deep learning-based semantic segmentation models for road lane\nmarking extraction from high-definition remote sensing images, assessing their\nperformance under transfer learning with partially labeled datasets. These\nmodels were fine-tuned on the partially labeled Waterloo Urban Scene dataset,\nand pre-trained on the SkyScapes dataset, simulating a likely scenario of\nreal-life model deployment under partial labeling. We observed and assessed the\nfine-tuning performance and overall performance. Models showed significant\nperformance improvements after fine-tuning, with mean IoU scores ranging from\n33.56% to 76.11%, and recall ranging from 66.0% to 98.96%. Transformer-based\nmodels outperformed convolutional neural networks, emphasizing the importance\nof model pre-training and fine-tuning in enhancing HD map development for AV\nnavigation.\n", "link": "http://arxiv.org/abs/2410.05717v2", "date": "2024-10-15", "relevancy": 2.9221, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancements%20in%20Road%20Lane%20Mapping%3A%20Comparative%20Fine-Tuning%20Analysis%20of%0A%20%20Deep%20Learning-based%20Semantic%20Segmentation%20Methods%20Using%20Aerial%20Imagery&body=Title%3A%20Advancements%20in%20Road%20Lane%20Mapping%3A%20Comparative%20Fine-Tuning%20Analysis%20of%0A%20%20Deep%20Learning-based%20Semantic%20Segmentation%20Methods%20Using%20Aerial%20Imagery%0AAuthor%3A%20Willow%20Liu%20and%20Shuxin%20Qiao%20and%20Kyle%20Gao%20and%20Hongjie%20He%20and%20Michael%20A.%20Chapman%20and%20Linlin%20Xu%20and%20Jonathan%20Li%0AAbstract%3A%20%20%20This%20research%20addresses%20the%20need%20for%20high-definition%20%28HD%29%20maps%20for%20autonomous%0Avehicles%20%28AVs%29%2C%20focusing%20on%20road%20lane%20information%20derived%20from%20aerial%20imagery.%0AWhile%20Earth%20observation%20data%20offers%20valuable%20resources%20for%20map%20creation%2C%0Aspecialized%20models%20for%20road%20lane%20extraction%20are%20still%20underdeveloped%20in%20remote%0Asensing.%20In%20this%20study%2C%20we%20perform%20an%20extensive%20comparison%20of%20twelve%0Afoundational%20deep%20learning-based%20semantic%20segmentation%20models%20for%20road%20lane%0Amarking%20extraction%20from%20high-definition%20remote%20sensing%20images%2C%20assessing%20their%0Aperformance%20under%20transfer%20learning%20with%20partially%20labeled%20datasets.%20These%0Amodels%20were%20fine-tuned%20on%20the%20partially%20labeled%20Waterloo%20Urban%20Scene%20dataset%2C%0Aand%20pre-trained%20on%20the%20SkyScapes%20dataset%2C%20simulating%20a%20likely%20scenario%20of%0Areal-life%20model%20deployment%20under%20partial%20labeling.%20We%20observed%20and%20assessed%20the%0Afine-tuning%20performance%20and%20overall%20performance.%20Models%20showed%20significant%0Aperformance%20improvements%20after%20fine-tuning%2C%20with%20mean%20IoU%20scores%20ranging%20from%0A33.56%25%20to%2076.11%25%2C%20and%20recall%20ranging%20from%2066.0%25%20to%2098.96%25.%20Transformer-based%0Amodels%20outperformed%20convolutional%20neural%20networks%2C%20emphasizing%20the%20importance%0Aof%20model%20pre-training%20and%20fine-tuning%20in%20enhancing%20HD%20map%20development%20for%20AV%0Anavigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancements%2520in%2520Road%2520Lane%2520Mapping%253A%2520Comparative%2520Fine-Tuning%2520Analysis%2520of%250A%2520%2520Deep%2520Learning-based%2520Semantic%2520Segmentation%2520Methods%2520Using%2520Aerial%2520Imagery%26entry.906535625%3DWillow%2520Liu%2520and%2520Shuxin%2520Qiao%2520and%2520Kyle%2520Gao%2520and%2520Hongjie%2520He%2520and%2520Michael%2520A.%2520Chapman%2520and%2520Linlin%2520Xu%2520and%2520Jonathan%2520Li%26entry.1292438233%3D%2520%2520This%2520research%2520addresses%2520the%2520need%2520for%2520high-definition%2520%2528HD%2529%2520maps%2520for%2520autonomous%250Avehicles%2520%2528AVs%2529%252C%2520focusing%2520on%2520road%2520lane%2520information%2520derived%2520from%2520aerial%2520imagery.%250AWhile%2520Earth%2520observation%2520data%2520offers%2520valuable%2520resources%2520for%2520map%2520creation%252C%250Aspecialized%2520models%2520for%2520road%2520lane%2520extraction%2520are%2520still%2520underdeveloped%2520in%2520remote%250Asensing.%2520In%2520this%2520study%252C%2520we%2520perform%2520an%2520extensive%2520comparison%2520of%2520twelve%250Afoundational%2520deep%2520learning-based%2520semantic%2520segmentation%2520models%2520for%2520road%2520lane%250Amarking%2520extraction%2520from%2520high-definition%2520remote%2520sensing%2520images%252C%2520assessing%2520their%250Aperformance%2520under%2520transfer%2520learning%2520with%2520partially%2520labeled%2520datasets.%2520These%250Amodels%2520were%2520fine-tuned%2520on%2520the%2520partially%2520labeled%2520Waterloo%2520Urban%2520Scene%2520dataset%252C%250Aand%2520pre-trained%2520on%2520the%2520SkyScapes%2520dataset%252C%2520simulating%2520a%2520likely%2520scenario%2520of%250Areal-life%2520model%2520deployment%2520under%2520partial%2520labeling.%2520We%2520observed%2520and%2520assessed%2520the%250Afine-tuning%2520performance%2520and%2520overall%2520performance.%2520Models%2520showed%2520significant%250Aperformance%2520improvements%2520after%2520fine-tuning%252C%2520with%2520mean%2520IoU%2520scores%2520ranging%2520from%250A33.56%2525%2520to%252076.11%2525%252C%2520and%2520recall%2520ranging%2520from%252066.0%2525%2520to%252098.96%2525.%2520Transformer-based%250Amodels%2520outperformed%2520convolutional%2520neural%2520networks%252C%2520emphasizing%2520the%2520importance%250Aof%2520model%2520pre-training%2520and%2520fine-tuning%2520in%2520enhancing%2520HD%2520map%2520development%2520for%2520AV%250Anavigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancements%20in%20Road%20Lane%20Mapping%3A%20Comparative%20Fine-Tuning%20Analysis%20of%0A%20%20Deep%20Learning-based%20Semantic%20Segmentation%20Methods%20Using%20Aerial%20Imagery&entry.906535625=Willow%20Liu%20and%20Shuxin%20Qiao%20and%20Kyle%20Gao%20and%20Hongjie%20He%20and%20Michael%20A.%20Chapman%20and%20Linlin%20Xu%20and%20Jonathan%20Li&entry.1292438233=%20%20This%20research%20addresses%20the%20need%20for%20high-definition%20%28HD%29%20maps%20for%20autonomous%0Avehicles%20%28AVs%29%2C%20focusing%20on%20road%20lane%20information%20derived%20from%20aerial%20imagery.%0AWhile%20Earth%20observation%20data%20offers%20valuable%20resources%20for%20map%20creation%2C%0Aspecialized%20models%20for%20road%20lane%20extraction%20are%20still%20underdeveloped%20in%20remote%0Asensing.%20In%20this%20study%2C%20we%20perform%20an%20extensive%20comparison%20of%20twelve%0Afoundational%20deep%20learning-based%20semantic%20segmentation%20models%20for%20road%20lane%0Amarking%20extraction%20from%20high-definition%20remote%20sensing%20images%2C%20assessing%20their%0Aperformance%20under%20transfer%20learning%20with%20partially%20labeled%20datasets.%20These%0Amodels%20were%20fine-tuned%20on%20the%20partially%20labeled%20Waterloo%20Urban%20Scene%20dataset%2C%0Aand%20pre-trained%20on%20the%20SkyScapes%20dataset%2C%20simulating%20a%20likely%20scenario%20of%0Areal-life%20model%20deployment%20under%20partial%20labeling.%20We%20observed%20and%20assessed%20the%0Afine-tuning%20performance%20and%20overall%20performance.%20Models%20showed%20significant%0Aperformance%20improvements%20after%20fine-tuning%2C%20with%20mean%20IoU%20scores%20ranging%20from%0A33.56%25%20to%2076.11%25%2C%20and%20recall%20ranging%20from%2066.0%25%20to%2098.96%25.%20Transformer-based%0Amodels%20outperformed%20convolutional%20neural%20networks%2C%20emphasizing%20the%20importance%0Aof%20model%20pre-training%20and%20fine-tuning%20in%20enhancing%20HD%20map%20development%20for%20AV%0Anavigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05717v2&entry.124074799=Read"},
{"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained\n  Vision-Language Understanding", "author": "Yue Cao and Yangzhou Liu and Zhe Chen and Guangchen Shi and Wenhai Wang and Danhuai Zhao and Tong Lu", "abstract": "  Despite significant advancements in Multimodal Large Language Models (MLLMs)\nfor understanding complex human intentions through cross-modal interactions,\ncapturing intricate image details remains challenging. Previous methods\nintegrating multiple vision encoders to enhance visual detail introduce\nredundancy and computational overhead. We observe that most MLLMs utilize only\nthe last-layer feature map of the vision encoder for visual representation,\nneglecting the rich fine-grained information in shallow feature maps. To\naddress this issue, we propose \\modelname, a simple yet effective multi-layer\nfeature fuser that efficiently integrates deep and shallow features from Vision\nTransformers (ViTs). Specifically, it leverages semantically aligned deep\nfeatures as queries to dynamically extract missing details from shallow\nfeatures, thus preserving semantic alignment while enriching the representation\nwith fine-grained information. Applied to the LLaVA-1.5 model,\n\\modelname~achieves significant improvements in visual representation and\nbenchmark performance, providing a more flexible and lightweight solution\ncompared to multi-encoder ensemble methods. The code and model have been\nreleased at https://github.com/yuecao0119/MMFuser.\n", "link": "http://arxiv.org/abs/2410.11829v1", "date": "2024-10-15", "relevancy": 2.8553, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMFuser%3A%20Multimodal%20Multi-Layer%20Feature%20Fuser%20for%20Fine-Grained%0A%20%20Vision-Language%20Understanding&body=Title%3A%20MMFuser%3A%20Multimodal%20Multi-Layer%20Feature%20Fuser%20for%20Fine-Grained%0A%20%20Vision-Language%20Understanding%0AAuthor%3A%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhe%20Chen%20and%20Guangchen%20Shi%20and%20Wenhai%20Wang%20and%20Danhuai%20Zhao%20and%20Tong%20Lu%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Afor%20understanding%20complex%20human%20intentions%20through%20cross-modal%20interactions%2C%0Acapturing%20intricate%20image%20details%20remains%20challenging.%20Previous%20methods%0Aintegrating%20multiple%20vision%20encoders%20to%20enhance%20visual%20detail%20introduce%0Aredundancy%20and%20computational%20overhead.%20We%20observe%20that%20most%20MLLMs%20utilize%20only%0Athe%20last-layer%20feature%20map%20of%20the%20vision%20encoder%20for%20visual%20representation%2C%0Aneglecting%20the%20rich%20fine-grained%20information%20in%20shallow%20feature%20maps.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20%5Cmodelname%2C%20a%20simple%20yet%20effective%20multi-layer%0Afeature%20fuser%20that%20efficiently%20integrates%20deep%20and%20shallow%20features%20from%20Vision%0ATransformers%20%28ViTs%29.%20Specifically%2C%20it%20leverages%20semantically%20aligned%20deep%0Afeatures%20as%20queries%20to%20dynamically%20extract%20missing%20details%20from%20shallow%0Afeatures%2C%20thus%20preserving%20semantic%20alignment%20while%20enriching%20the%20representation%0Awith%20fine-grained%20information.%20Applied%20to%20the%20LLaVA-1.5%20model%2C%0A%5Cmodelname~achieves%20significant%20improvements%20in%20visual%20representation%20and%0Abenchmark%20performance%2C%20providing%20a%20more%20flexible%20and%20lightweight%20solution%0Acompared%20to%20multi-encoder%20ensemble%20methods.%20The%20code%20and%20model%20have%20been%0Areleased%20at%20https%3A//github.com/yuecao0119/MMFuser.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMFuser%253A%2520Multimodal%2520Multi-Layer%2520Feature%2520Fuser%2520for%2520Fine-Grained%250A%2520%2520Vision-Language%2520Understanding%26entry.906535625%3DYue%2520Cao%2520and%2520Yangzhou%2520Liu%2520and%2520Zhe%2520Chen%2520and%2520Guangchen%2520Shi%2520and%2520Wenhai%2520Wang%2520and%2520Danhuai%2520Zhao%2520and%2520Tong%2520Lu%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Afor%2520understanding%2520complex%2520human%2520intentions%2520through%2520cross-modal%2520interactions%252C%250Acapturing%2520intricate%2520image%2520details%2520remains%2520challenging.%2520Previous%2520methods%250Aintegrating%2520multiple%2520vision%2520encoders%2520to%2520enhance%2520visual%2520detail%2520introduce%250Aredundancy%2520and%2520computational%2520overhead.%2520We%2520observe%2520that%2520most%2520MLLMs%2520utilize%2520only%250Athe%2520last-layer%2520feature%2520map%2520of%2520the%2520vision%2520encoder%2520for%2520visual%2520representation%252C%250Aneglecting%2520the%2520rich%2520fine-grained%2520information%2520in%2520shallow%2520feature%2520maps.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520%255Cmodelname%252C%2520a%2520simple%2520yet%2520effective%2520multi-layer%250Afeature%2520fuser%2520that%2520efficiently%2520integrates%2520deep%2520and%2520shallow%2520features%2520from%2520Vision%250ATransformers%2520%2528ViTs%2529.%2520Specifically%252C%2520it%2520leverages%2520semantically%2520aligned%2520deep%250Afeatures%2520as%2520queries%2520to%2520dynamically%2520extract%2520missing%2520details%2520from%2520shallow%250Afeatures%252C%2520thus%2520preserving%2520semantic%2520alignment%2520while%2520enriching%2520the%2520representation%250Awith%2520fine-grained%2520information.%2520Applied%2520to%2520the%2520LLaVA-1.5%2520model%252C%250A%255Cmodelname~achieves%2520significant%2520improvements%2520in%2520visual%2520representation%2520and%250Abenchmark%2520performance%252C%2520providing%2520a%2520more%2520flexible%2520and%2520lightweight%2520solution%250Acompared%2520to%2520multi-encoder%2520ensemble%2520methods.%2520The%2520code%2520and%2520model%2520have%2520been%250Areleased%2520at%2520https%253A//github.com/yuecao0119/MMFuser.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMFuser%3A%20Multimodal%20Multi-Layer%20Feature%20Fuser%20for%20Fine-Grained%0A%20%20Vision-Language%20Understanding&entry.906535625=Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhe%20Chen%20and%20Guangchen%20Shi%20and%20Wenhai%20Wang%20and%20Danhuai%20Zhao%20and%20Tong%20Lu&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Afor%20understanding%20complex%20human%20intentions%20through%20cross-modal%20interactions%2C%0Acapturing%20intricate%20image%20details%20remains%20challenging.%20Previous%20methods%0Aintegrating%20multiple%20vision%20encoders%20to%20enhance%20visual%20detail%20introduce%0Aredundancy%20and%20computational%20overhead.%20We%20observe%20that%20most%20MLLMs%20utilize%20only%0Athe%20last-layer%20feature%20map%20of%20the%20vision%20encoder%20for%20visual%20representation%2C%0Aneglecting%20the%20rich%20fine-grained%20information%20in%20shallow%20feature%20maps.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20%5Cmodelname%2C%20a%20simple%20yet%20effective%20multi-layer%0Afeature%20fuser%20that%20efficiently%20integrates%20deep%20and%20shallow%20features%20from%20Vision%0ATransformers%20%28ViTs%29.%20Specifically%2C%20it%20leverages%20semantically%20aligned%20deep%0Afeatures%20as%20queries%20to%20dynamically%20extract%20missing%20details%20from%20shallow%0Afeatures%2C%20thus%20preserving%20semantic%20alignment%20while%20enriching%20the%20representation%0Awith%20fine-grained%20information.%20Applied%20to%20the%20LLaVA-1.5%20model%2C%0A%5Cmodelname~achieves%20significant%20improvements%20in%20visual%20representation%20and%0Abenchmark%20performance%2C%20providing%20a%20more%20flexible%20and%20lightweight%20solution%0Acompared%20to%20multi-encoder%20ensemble%20methods.%20The%20code%20and%20model%20have%20been%0Areleased%20at%20https%3A//github.com/yuecao0119/MMFuser.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11829v1&entry.124074799=Read"},
{"title": "MCTBench: Multimodal Cognition towards Text-Rich Visual Scenes Benchmark", "author": "Bin Shan and Xiang Fei and Wei Shi and An-Lan Wang and Guozhi Tang and Lei Liao and Jingqun Tang and Xiang Bai and Can Huang", "abstract": "  The comprehension of text-rich visual scenes has become a focal point for\nevaluating Multi-modal Large Language Models (MLLMs) due to their widespread\napplications. Current benchmarks tailored to the scenario emphasize perceptual\ncapabilities, while overlooking the assessment of cognitive abilities. To\naddress this limitation, we introduce a Multimodal benchmark towards Text-rich\nvisual scenes, to evaluate the Cognitive capabilities of MLLMs through visual\nreasoning and content-creation tasks (MCTBench). To mitigate potential\nevaluation bias from the varying distributions of datasets, MCTBench\nincorporates several perception tasks (e.g., scene text recognition) to ensure\na consistent comparison of both the cognitive and perceptual capabilities of\nMLLMs. To improve the efficiency and fairness of content-creation evaluation,\nwe conduct an automatic evaluation pipeline. Evaluations of various MLLMs on\nMCTBench reveal that, despite their impressive perceptual capabilities, their\ncognition abilities require enhancement. We hope MCTBench will offer the\ncommunity an efficient resource to explore and enhance cognitive capabilities\ntowards text-rich visual scenes.\n", "link": "http://arxiv.org/abs/2410.11538v1", "date": "2024-10-15", "relevancy": 2.8532, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCTBench%3A%20Multimodal%20Cognition%20towards%20Text-Rich%20Visual%20Scenes%20Benchmark&body=Title%3A%20MCTBench%3A%20Multimodal%20Cognition%20towards%20Text-Rich%20Visual%20Scenes%20Benchmark%0AAuthor%3A%20Bin%20Shan%20and%20Xiang%20Fei%20and%20Wei%20Shi%20and%20An-Lan%20Wang%20and%20Guozhi%20Tang%20and%20Lei%20Liao%20and%20Jingqun%20Tang%20and%20Xiang%20Bai%20and%20Can%20Huang%0AAbstract%3A%20%20%20The%20comprehension%20of%20text-rich%20visual%20scenes%20has%20become%20a%20focal%20point%20for%0Aevaluating%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20due%20to%20their%20widespread%0Aapplications.%20Current%20benchmarks%20tailored%20to%20the%20scenario%20emphasize%20perceptual%0Acapabilities%2C%20while%20overlooking%20the%20assessment%20of%20cognitive%20abilities.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20Multimodal%20benchmark%20towards%20Text-rich%0Avisual%20scenes%2C%20to%20evaluate%20the%20Cognitive%20capabilities%20of%20MLLMs%20through%20visual%0Areasoning%20and%20content-creation%20tasks%20%28MCTBench%29.%20To%20mitigate%20potential%0Aevaluation%20bias%20from%20the%20varying%20distributions%20of%20datasets%2C%20MCTBench%0Aincorporates%20several%20perception%20tasks%20%28e.g.%2C%20scene%20text%20recognition%29%20to%20ensure%0Aa%20consistent%20comparison%20of%20both%20the%20cognitive%20and%20perceptual%20capabilities%20of%0AMLLMs.%20To%20improve%20the%20efficiency%20and%20fairness%20of%20content-creation%20evaluation%2C%0Awe%20conduct%20an%20automatic%20evaluation%20pipeline.%20Evaluations%20of%20various%20MLLMs%20on%0AMCTBench%20reveal%20that%2C%20despite%20their%20impressive%20perceptual%20capabilities%2C%20their%0Acognition%20abilities%20require%20enhancement.%20We%20hope%20MCTBench%20will%20offer%20the%0Acommunity%20an%20efficient%20resource%20to%20explore%20and%20enhance%20cognitive%20capabilities%0Atowards%20text-rich%20visual%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCTBench%253A%2520Multimodal%2520Cognition%2520towards%2520Text-Rich%2520Visual%2520Scenes%2520Benchmark%26entry.906535625%3DBin%2520Shan%2520and%2520Xiang%2520Fei%2520and%2520Wei%2520Shi%2520and%2520An-Lan%2520Wang%2520and%2520Guozhi%2520Tang%2520and%2520Lei%2520Liao%2520and%2520Jingqun%2520Tang%2520and%2520Xiang%2520Bai%2520and%2520Can%2520Huang%26entry.1292438233%3D%2520%2520The%2520comprehension%2520of%2520text-rich%2520visual%2520scenes%2520has%2520become%2520a%2520focal%2520point%2520for%250Aevaluating%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520due%2520to%2520their%2520widespread%250Aapplications.%2520Current%2520benchmarks%2520tailored%2520to%2520the%2520scenario%2520emphasize%2520perceptual%250Acapabilities%252C%2520while%2520overlooking%2520the%2520assessment%2520of%2520cognitive%2520abilities.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520Multimodal%2520benchmark%2520towards%2520Text-rich%250Avisual%2520scenes%252C%2520to%2520evaluate%2520the%2520Cognitive%2520capabilities%2520of%2520MLLMs%2520through%2520visual%250Areasoning%2520and%2520content-creation%2520tasks%2520%2528MCTBench%2529.%2520To%2520mitigate%2520potential%250Aevaluation%2520bias%2520from%2520the%2520varying%2520distributions%2520of%2520datasets%252C%2520MCTBench%250Aincorporates%2520several%2520perception%2520tasks%2520%2528e.g.%252C%2520scene%2520text%2520recognition%2529%2520to%2520ensure%250Aa%2520consistent%2520comparison%2520of%2520both%2520the%2520cognitive%2520and%2520perceptual%2520capabilities%2520of%250AMLLMs.%2520To%2520improve%2520the%2520efficiency%2520and%2520fairness%2520of%2520content-creation%2520evaluation%252C%250Awe%2520conduct%2520an%2520automatic%2520evaluation%2520pipeline.%2520Evaluations%2520of%2520various%2520MLLMs%2520on%250AMCTBench%2520reveal%2520that%252C%2520despite%2520their%2520impressive%2520perceptual%2520capabilities%252C%2520their%250Acognition%2520abilities%2520require%2520enhancement.%2520We%2520hope%2520MCTBench%2520will%2520offer%2520the%250Acommunity%2520an%2520efficient%2520resource%2520to%2520explore%2520and%2520enhance%2520cognitive%2520capabilities%250Atowards%2520text-rich%2520visual%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCTBench%3A%20Multimodal%20Cognition%20towards%20Text-Rich%20Visual%20Scenes%20Benchmark&entry.906535625=Bin%20Shan%20and%20Xiang%20Fei%20and%20Wei%20Shi%20and%20An-Lan%20Wang%20and%20Guozhi%20Tang%20and%20Lei%20Liao%20and%20Jingqun%20Tang%20and%20Xiang%20Bai%20and%20Can%20Huang&entry.1292438233=%20%20The%20comprehension%20of%20text-rich%20visual%20scenes%20has%20become%20a%20focal%20point%20for%0Aevaluating%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20due%20to%20their%20widespread%0Aapplications.%20Current%20benchmarks%20tailored%20to%20the%20scenario%20emphasize%20perceptual%0Acapabilities%2C%20while%20overlooking%20the%20assessment%20of%20cognitive%20abilities.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20Multimodal%20benchmark%20towards%20Text-rich%0Avisual%20scenes%2C%20to%20evaluate%20the%20Cognitive%20capabilities%20of%20MLLMs%20through%20visual%0Areasoning%20and%20content-creation%20tasks%20%28MCTBench%29.%20To%20mitigate%20potential%0Aevaluation%20bias%20from%20the%20varying%20distributions%20of%20datasets%2C%20MCTBench%0Aincorporates%20several%20perception%20tasks%20%28e.g.%2C%20scene%20text%20recognition%29%20to%20ensure%0Aa%20consistent%20comparison%20of%20both%20the%20cognitive%20and%20perceptual%20capabilities%20of%0AMLLMs.%20To%20improve%20the%20efficiency%20and%20fairness%20of%20content-creation%20evaluation%2C%0Awe%20conduct%20an%20automatic%20evaluation%20pipeline.%20Evaluations%20of%20various%20MLLMs%20on%0AMCTBench%20reveal%20that%2C%20despite%20their%20impressive%20perceptual%20capabilities%2C%20their%0Acognition%20abilities%20require%20enhancement.%20We%20hope%20MCTBench%20will%20offer%20the%0Acommunity%20an%20efficient%20resource%20to%20explore%20and%20enhance%20cognitive%20capabilities%0Atowards%20text-rich%20visual%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11538v1&entry.124074799=Read"},
{"title": "PSVMA+: Exploring Multi-granularity Semantic-visual Adaption for\n  Generalized Zero-shot Learning", "author": "Man Liu and Huihui Bai and Feng Li and Chunjie Zhang and Yunchao Wei and Meng Wang and Tat-Seng Chua and Yao Zhao", "abstract": "  Generalized zero-shot learning (GZSL) endeavors to identify the unseen\ncategories using knowledge from the seen domain, necessitating the intrinsic\ninteractions between the visual features and attribute semantic features.\nHowever, GZSL suffers from insufficient visual-semantic correspondences due to\nthe attribute diversity and instance diversity. Attribute diversity refers to\nvarying semantic granularity in attribute descriptions, ranging from low-level\n(specific, directly observable) to high-level (abstract, highly generic)\ncharacteristics. This diversity challenges the collection of adequate visual\ncues for attributes under a uni-granularity. Additionally, diverse visual\ninstances corresponding to the same sharing attributes introduce semantic\nambiguity, leading to vague visual patterns. To tackle these problems, we\npropose a multi-granularity progressive semantic-visual mutual adaption\n(PSVMA+) network, where sufficient visual elements across granularity levels\ncan be gathered to remedy the granularity inconsistency. PSVMA+ explores\nsemantic-visual interactions at different granularity levels, enabling\nawareness of multi-granularity in both visual and semantic elements. At each\ngranularity level, the dual semantic-visual transformer module (DSVTM) recasts\nthe sharing attributes into instance-centric attributes and aggregates the\nsemantic-related visual regions, thereby learning unambiguous visual features\nto accommodate various instances. Given the diverse contributions of different\ngranularities, PSVMA+ employs selective cross-granularity learning to leverage\nknowledge from reliable granularities and adaptively fuses multi-granularity\nfeatures for comprehensive representations. Experimental results demonstrate\nthat PSVMA+ consistently outperforms state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2410.11560v1", "date": "2024-10-15", "relevancy": 2.8465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSVMA%2B%3A%20Exploring%20Multi-granularity%20Semantic-visual%20Adaption%20for%0A%20%20Generalized%20Zero-shot%20Learning&body=Title%3A%20PSVMA%2B%3A%20Exploring%20Multi-granularity%20Semantic-visual%20Adaption%20for%0A%20%20Generalized%20Zero-shot%20Learning%0AAuthor%3A%20Man%20Liu%20and%20Huihui%20Bai%20and%20Feng%20Li%20and%20Chunjie%20Zhang%20and%20Yunchao%20Wei%20and%20Meng%20Wang%20and%20Tat-Seng%20Chua%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Generalized%20zero-shot%20learning%20%28GZSL%29%20endeavors%20to%20identify%20the%20unseen%0Acategories%20using%20knowledge%20from%20the%20seen%20domain%2C%20necessitating%20the%20intrinsic%0Ainteractions%20between%20the%20visual%20features%20and%20attribute%20semantic%20features.%0AHowever%2C%20GZSL%20suffers%20from%20insufficient%20visual-semantic%20correspondences%20due%20to%0Athe%20attribute%20diversity%20and%20instance%20diversity.%20Attribute%20diversity%20refers%20to%0Avarying%20semantic%20granularity%20in%20attribute%20descriptions%2C%20ranging%20from%20low-level%0A%28specific%2C%20directly%20observable%29%20to%20high-level%20%28abstract%2C%20highly%20generic%29%0Acharacteristics.%20This%20diversity%20challenges%20the%20collection%20of%20adequate%20visual%0Acues%20for%20attributes%20under%20a%20uni-granularity.%20Additionally%2C%20diverse%20visual%0Ainstances%20corresponding%20to%20the%20same%20sharing%20attributes%20introduce%20semantic%0Aambiguity%2C%20leading%20to%20vague%20visual%20patterns.%20To%20tackle%20these%20problems%2C%20we%0Apropose%20a%20multi-granularity%20progressive%20semantic-visual%20mutual%20adaption%0A%28PSVMA%2B%29%20network%2C%20where%20sufficient%20visual%20elements%20across%20granularity%20levels%0Acan%20be%20gathered%20to%20remedy%20the%20granularity%20inconsistency.%20PSVMA%2B%20explores%0Asemantic-visual%20interactions%20at%20different%20granularity%20levels%2C%20enabling%0Aawareness%20of%20multi-granularity%20in%20both%20visual%20and%20semantic%20elements.%20At%20each%0Agranularity%20level%2C%20the%20dual%20semantic-visual%20transformer%20module%20%28DSVTM%29%20recasts%0Athe%20sharing%20attributes%20into%20instance-centric%20attributes%20and%20aggregates%20the%0Asemantic-related%20visual%20regions%2C%20thereby%20learning%20unambiguous%20visual%20features%0Ato%20accommodate%20various%20instances.%20Given%20the%20diverse%20contributions%20of%20different%0Agranularities%2C%20PSVMA%2B%20employs%20selective%20cross-granularity%20learning%20to%20leverage%0Aknowledge%20from%20reliable%20granularities%20and%20adaptively%20fuses%20multi-granularity%0Afeatures%20for%20comprehensive%20representations.%20Experimental%20results%20demonstrate%0Athat%20PSVMA%2B%20consistently%20outperforms%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSVMA%252B%253A%2520Exploring%2520Multi-granularity%2520Semantic-visual%2520Adaption%2520for%250A%2520%2520Generalized%2520Zero-shot%2520Learning%26entry.906535625%3DMan%2520Liu%2520and%2520Huihui%2520Bai%2520and%2520Feng%2520Li%2520and%2520Chunjie%2520Zhang%2520and%2520Yunchao%2520Wei%2520and%2520Meng%2520Wang%2520and%2520Tat-Seng%2520Chua%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520Generalized%2520zero-shot%2520learning%2520%2528GZSL%2529%2520endeavors%2520to%2520identify%2520the%2520unseen%250Acategories%2520using%2520knowledge%2520from%2520the%2520seen%2520domain%252C%2520necessitating%2520the%2520intrinsic%250Ainteractions%2520between%2520the%2520visual%2520features%2520and%2520attribute%2520semantic%2520features.%250AHowever%252C%2520GZSL%2520suffers%2520from%2520insufficient%2520visual-semantic%2520correspondences%2520due%2520to%250Athe%2520attribute%2520diversity%2520and%2520instance%2520diversity.%2520Attribute%2520diversity%2520refers%2520to%250Avarying%2520semantic%2520granularity%2520in%2520attribute%2520descriptions%252C%2520ranging%2520from%2520low-level%250A%2528specific%252C%2520directly%2520observable%2529%2520to%2520high-level%2520%2528abstract%252C%2520highly%2520generic%2529%250Acharacteristics.%2520This%2520diversity%2520challenges%2520the%2520collection%2520of%2520adequate%2520visual%250Acues%2520for%2520attributes%2520under%2520a%2520uni-granularity.%2520Additionally%252C%2520diverse%2520visual%250Ainstances%2520corresponding%2520to%2520the%2520same%2520sharing%2520attributes%2520introduce%2520semantic%250Aambiguity%252C%2520leading%2520to%2520vague%2520visual%2520patterns.%2520To%2520tackle%2520these%2520problems%252C%2520we%250Apropose%2520a%2520multi-granularity%2520progressive%2520semantic-visual%2520mutual%2520adaption%250A%2528PSVMA%252B%2529%2520network%252C%2520where%2520sufficient%2520visual%2520elements%2520across%2520granularity%2520levels%250Acan%2520be%2520gathered%2520to%2520remedy%2520the%2520granularity%2520inconsistency.%2520PSVMA%252B%2520explores%250Asemantic-visual%2520interactions%2520at%2520different%2520granularity%2520levels%252C%2520enabling%250Aawareness%2520of%2520multi-granularity%2520in%2520both%2520visual%2520and%2520semantic%2520elements.%2520At%2520each%250Agranularity%2520level%252C%2520the%2520dual%2520semantic-visual%2520transformer%2520module%2520%2528DSVTM%2529%2520recasts%250Athe%2520sharing%2520attributes%2520into%2520instance-centric%2520attributes%2520and%2520aggregates%2520the%250Asemantic-related%2520visual%2520regions%252C%2520thereby%2520learning%2520unambiguous%2520visual%2520features%250Ato%2520accommodate%2520various%2520instances.%2520Given%2520the%2520diverse%2520contributions%2520of%2520different%250Agranularities%252C%2520PSVMA%252B%2520employs%2520selective%2520cross-granularity%2520learning%2520to%2520leverage%250Aknowledge%2520from%2520reliable%2520granularities%2520and%2520adaptively%2520fuses%2520multi-granularity%250Afeatures%2520for%2520comprehensive%2520representations.%2520Experimental%2520results%2520demonstrate%250Athat%2520PSVMA%252B%2520consistently%2520outperforms%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSVMA%2B%3A%20Exploring%20Multi-granularity%20Semantic-visual%20Adaption%20for%0A%20%20Generalized%20Zero-shot%20Learning&entry.906535625=Man%20Liu%20and%20Huihui%20Bai%20and%20Feng%20Li%20and%20Chunjie%20Zhang%20and%20Yunchao%20Wei%20and%20Meng%20Wang%20and%20Tat-Seng%20Chua%20and%20Yao%20Zhao&entry.1292438233=%20%20Generalized%20zero-shot%20learning%20%28GZSL%29%20endeavors%20to%20identify%20the%20unseen%0Acategories%20using%20knowledge%20from%20the%20seen%20domain%2C%20necessitating%20the%20intrinsic%0Ainteractions%20between%20the%20visual%20features%20and%20attribute%20semantic%20features.%0AHowever%2C%20GZSL%20suffers%20from%20insufficient%20visual-semantic%20correspondences%20due%20to%0Athe%20attribute%20diversity%20and%20instance%20diversity.%20Attribute%20diversity%20refers%20to%0Avarying%20semantic%20granularity%20in%20attribute%20descriptions%2C%20ranging%20from%20low-level%0A%28specific%2C%20directly%20observable%29%20to%20high-level%20%28abstract%2C%20highly%20generic%29%0Acharacteristics.%20This%20diversity%20challenges%20the%20collection%20of%20adequate%20visual%0Acues%20for%20attributes%20under%20a%20uni-granularity.%20Additionally%2C%20diverse%20visual%0Ainstances%20corresponding%20to%20the%20same%20sharing%20attributes%20introduce%20semantic%0Aambiguity%2C%20leading%20to%20vague%20visual%20patterns.%20To%20tackle%20these%20problems%2C%20we%0Apropose%20a%20multi-granularity%20progressive%20semantic-visual%20mutual%20adaption%0A%28PSVMA%2B%29%20network%2C%20where%20sufficient%20visual%20elements%20across%20granularity%20levels%0Acan%20be%20gathered%20to%20remedy%20the%20granularity%20inconsistency.%20PSVMA%2B%20explores%0Asemantic-visual%20interactions%20at%20different%20granularity%20levels%2C%20enabling%0Aawareness%20of%20multi-granularity%20in%20both%20visual%20and%20semantic%20elements.%20At%20each%0Agranularity%20level%2C%20the%20dual%20semantic-visual%20transformer%20module%20%28DSVTM%29%20recasts%0Athe%20sharing%20attributes%20into%20instance-centric%20attributes%20and%20aggregates%20the%0Asemantic-related%20visual%20regions%2C%20thereby%20learning%20unambiguous%20visual%20features%0Ato%20accommodate%20various%20instances.%20Given%20the%20diverse%20contributions%20of%20different%0Agranularities%2C%20PSVMA%2B%20employs%20selective%20cross-granularity%20learning%20to%20leverage%0Aknowledge%20from%20reliable%20granularities%20and%20adaptively%20fuses%20multi-granularity%0Afeatures%20for%20comprehensive%20representations.%20Experimental%20results%20demonstrate%0Athat%20PSVMA%2B%20consistently%20outperforms%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11560v1&entry.124074799=Read"},
{"title": "Robotic Arm Platform for Multi-View Image Acquisition and 3D\n  Reconstruction in Minimally Invasive Surgery", "author": "Alexander Saikia and Chiara Di Vece and Sierra Bonilla and Chloe He and Morenike Magbagbeola and Laurent Mennillo and Tobias Czempiel and Sophia Bano and Danail Stoyanov", "abstract": "  Minimally invasive surgery (MIS) offers significant benefits such as reduced\nrecovery time and minimised patient trauma, but poses challenges in visibility\nand access, making accurate 3D reconstruction a significant tool in surgical\nplanning and navigation. This work introduces a robotic arm platform for\nefficient multi-view image acquisition and precise 3D reconstruction in MIS\nsettings. We adapted a laparoscope to a robotic arm and captured ex-vivo images\nof several ovine organs across varying lighting conditions (operating room and\nlaparoscopic) and trajectories (spherical and laparoscopic). We employed\nrecently released learning-based feature matchers combined with COLMAP to\nproduce our reconstructions. The reconstructions were evaluated against\nhigh-precision laser scans for quantitative evaluation. Our results show that\nwhilst reconstructions suffer most under realistic MIS lighting and trajectory,\nmany versions of our pipeline achieve close to sub-millimetre accuracy with an\naverage of 1.05 mm Root Mean Squared Error and 0.82 mm Chamfer distance. Our\nbest reconstruction results occur with operating room lighting and spherical\ntrajectories. Our robotic platform provides a tool for controlled, repeatable\nmulti-view data acquisition for 3D generation in MIS environments which we hope\nleads to new datasets for training learning-based models.\n", "link": "http://arxiv.org/abs/2410.11703v1", "date": "2024-10-15", "relevancy": 2.8216, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Arm%20Platform%20for%20Multi-View%20Image%20Acquisition%20and%203D%0A%20%20Reconstruction%20in%20Minimally%20Invasive%20Surgery&body=Title%3A%20Robotic%20Arm%20Platform%20for%20Multi-View%20Image%20Acquisition%20and%203D%0A%20%20Reconstruction%20in%20Minimally%20Invasive%20Surgery%0AAuthor%3A%20Alexander%20Saikia%20and%20Chiara%20Di%20Vece%20and%20Sierra%20Bonilla%20and%20Chloe%20He%20and%20Morenike%20Magbagbeola%20and%20Laurent%20Mennillo%20and%20Tobias%20Czempiel%20and%20Sophia%20Bano%20and%20Danail%20Stoyanov%0AAbstract%3A%20%20%20Minimally%20invasive%20surgery%20%28MIS%29%20offers%20significant%20benefits%20such%20as%20reduced%0Arecovery%20time%20and%20minimised%20patient%20trauma%2C%20but%20poses%20challenges%20in%20visibility%0Aand%20access%2C%20making%20accurate%203D%20reconstruction%20a%20significant%20tool%20in%20surgical%0Aplanning%20and%20navigation.%20This%20work%20introduces%20a%20robotic%20arm%20platform%20for%0Aefficient%20multi-view%20image%20acquisition%20and%20precise%203D%20reconstruction%20in%20MIS%0Asettings.%20We%20adapted%20a%20laparoscope%20to%20a%20robotic%20arm%20and%20captured%20ex-vivo%20images%0Aof%20several%20ovine%20organs%20across%20varying%20lighting%20conditions%20%28operating%20room%20and%0Alaparoscopic%29%20and%20trajectories%20%28spherical%20and%20laparoscopic%29.%20We%20employed%0Arecently%20released%20learning-based%20feature%20matchers%20combined%20with%20COLMAP%20to%0Aproduce%20our%20reconstructions.%20The%20reconstructions%20were%20evaluated%20against%0Ahigh-precision%20laser%20scans%20for%20quantitative%20evaluation.%20Our%20results%20show%20that%0Awhilst%20reconstructions%20suffer%20most%20under%20realistic%20MIS%20lighting%20and%20trajectory%2C%0Amany%20versions%20of%20our%20pipeline%20achieve%20close%20to%20sub-millimetre%20accuracy%20with%20an%0Aaverage%20of%201.05%20mm%20Root%20Mean%20Squared%20Error%20and%200.82%20mm%20Chamfer%20distance.%20Our%0Abest%20reconstruction%20results%20occur%20with%20operating%20room%20lighting%20and%20spherical%0Atrajectories.%20Our%20robotic%20platform%20provides%20a%20tool%20for%20controlled%2C%20repeatable%0Amulti-view%20data%20acquisition%20for%203D%20generation%20in%20MIS%20environments%20which%20we%20hope%0Aleads%20to%20new%20datasets%20for%20training%20learning-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Arm%2520Platform%2520for%2520Multi-View%2520Image%2520Acquisition%2520and%25203D%250A%2520%2520Reconstruction%2520in%2520Minimally%2520Invasive%2520Surgery%26entry.906535625%3DAlexander%2520Saikia%2520and%2520Chiara%2520Di%2520Vece%2520and%2520Sierra%2520Bonilla%2520and%2520Chloe%2520He%2520and%2520Morenike%2520Magbagbeola%2520and%2520Laurent%2520Mennillo%2520and%2520Tobias%2520Czempiel%2520and%2520Sophia%2520Bano%2520and%2520Danail%2520Stoyanov%26entry.1292438233%3D%2520%2520Minimally%2520invasive%2520surgery%2520%2528MIS%2529%2520offers%2520significant%2520benefits%2520such%2520as%2520reduced%250Arecovery%2520time%2520and%2520minimised%2520patient%2520trauma%252C%2520but%2520poses%2520challenges%2520in%2520visibility%250Aand%2520access%252C%2520making%2520accurate%25203D%2520reconstruction%2520a%2520significant%2520tool%2520in%2520surgical%250Aplanning%2520and%2520navigation.%2520This%2520work%2520introduces%2520a%2520robotic%2520arm%2520platform%2520for%250Aefficient%2520multi-view%2520image%2520acquisition%2520and%2520precise%25203D%2520reconstruction%2520in%2520MIS%250Asettings.%2520We%2520adapted%2520a%2520laparoscope%2520to%2520a%2520robotic%2520arm%2520and%2520captured%2520ex-vivo%2520images%250Aof%2520several%2520ovine%2520organs%2520across%2520varying%2520lighting%2520conditions%2520%2528operating%2520room%2520and%250Alaparoscopic%2529%2520and%2520trajectories%2520%2528spherical%2520and%2520laparoscopic%2529.%2520We%2520employed%250Arecently%2520released%2520learning-based%2520feature%2520matchers%2520combined%2520with%2520COLMAP%2520to%250Aproduce%2520our%2520reconstructions.%2520The%2520reconstructions%2520were%2520evaluated%2520against%250Ahigh-precision%2520laser%2520scans%2520for%2520quantitative%2520evaluation.%2520Our%2520results%2520show%2520that%250Awhilst%2520reconstructions%2520suffer%2520most%2520under%2520realistic%2520MIS%2520lighting%2520and%2520trajectory%252C%250Amany%2520versions%2520of%2520our%2520pipeline%2520achieve%2520close%2520to%2520sub-millimetre%2520accuracy%2520with%2520an%250Aaverage%2520of%25201.05%2520mm%2520Root%2520Mean%2520Squared%2520Error%2520and%25200.82%2520mm%2520Chamfer%2520distance.%2520Our%250Abest%2520reconstruction%2520results%2520occur%2520with%2520operating%2520room%2520lighting%2520and%2520spherical%250Atrajectories.%2520Our%2520robotic%2520platform%2520provides%2520a%2520tool%2520for%2520controlled%252C%2520repeatable%250Amulti-view%2520data%2520acquisition%2520for%25203D%2520generation%2520in%2520MIS%2520environments%2520which%2520we%2520hope%250Aleads%2520to%2520new%2520datasets%2520for%2520training%2520learning-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Arm%20Platform%20for%20Multi-View%20Image%20Acquisition%20and%203D%0A%20%20Reconstruction%20in%20Minimally%20Invasive%20Surgery&entry.906535625=Alexander%20Saikia%20and%20Chiara%20Di%20Vece%20and%20Sierra%20Bonilla%20and%20Chloe%20He%20and%20Morenike%20Magbagbeola%20and%20Laurent%20Mennillo%20and%20Tobias%20Czempiel%20and%20Sophia%20Bano%20and%20Danail%20Stoyanov&entry.1292438233=%20%20Minimally%20invasive%20surgery%20%28MIS%29%20offers%20significant%20benefits%20such%20as%20reduced%0Arecovery%20time%20and%20minimised%20patient%20trauma%2C%20but%20poses%20challenges%20in%20visibility%0Aand%20access%2C%20making%20accurate%203D%20reconstruction%20a%20significant%20tool%20in%20surgical%0Aplanning%20and%20navigation.%20This%20work%20introduces%20a%20robotic%20arm%20platform%20for%0Aefficient%20multi-view%20image%20acquisition%20and%20precise%203D%20reconstruction%20in%20MIS%0Asettings.%20We%20adapted%20a%20laparoscope%20to%20a%20robotic%20arm%20and%20captured%20ex-vivo%20images%0Aof%20several%20ovine%20organs%20across%20varying%20lighting%20conditions%20%28operating%20room%20and%0Alaparoscopic%29%20and%20trajectories%20%28spherical%20and%20laparoscopic%29.%20We%20employed%0Arecently%20released%20learning-based%20feature%20matchers%20combined%20with%20COLMAP%20to%0Aproduce%20our%20reconstructions.%20The%20reconstructions%20were%20evaluated%20against%0Ahigh-precision%20laser%20scans%20for%20quantitative%20evaluation.%20Our%20results%20show%20that%0Awhilst%20reconstructions%20suffer%20most%20under%20realistic%20MIS%20lighting%20and%20trajectory%2C%0Amany%20versions%20of%20our%20pipeline%20achieve%20close%20to%20sub-millimetre%20accuracy%20with%20an%0Aaverage%20of%201.05%20mm%20Root%20Mean%20Squared%20Error%20and%200.82%20mm%20Chamfer%20distance.%20Our%0Abest%20reconstruction%20results%20occur%20with%20operating%20room%20lighting%20and%20spherical%0Atrajectories.%20Our%20robotic%20platform%20provides%20a%20tool%20for%20controlled%2C%20repeatable%0Amulti-view%20data%20acquisition%20for%203D%20generation%20in%20MIS%20environments%20which%20we%20hope%0Aleads%20to%20new%20datasets%20for%20training%20learning-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11703v1&entry.124074799=Read"},
{"title": "Enhancing Agent Learning through World Dynamics Modeling", "author": "Zhiyuan Sun and Haochen Shi and Marc-Alexandre C\u00f4t\u00e9 and Glen Berseth and Xingdi Yuan and Bang Liu", "abstract": "  Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.\n", "link": "http://arxiv.org/abs/2407.17695v2", "date": "2024-10-15", "relevancy": 2.7986, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Agent%20Learning%20through%20World%20Dynamics%20Modeling&body=Title%3A%20Enhancing%20Agent%20Learning%20through%20World%20Dynamics%20Modeling%0AAuthor%3A%20Zhiyuan%20Sun%20and%20Haochen%20Shi%20and%20Marc-Alexandre%20C%C3%B4t%C3%A9%20and%20Glen%20Berseth%20and%20Xingdi%20Yuan%20and%20Bang%20Liu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20increasingly%20applied%20to%20tasks%20in%0Alanguage%20understanding%20and%20interactive%20decision-making%2C%20with%20their%20impressive%0Aperformance%20largely%20attributed%20to%20the%20extensive%20domain%20knowledge%20embedded%0Awithin%20them.%20However%2C%20the%20depth%20and%20breadth%20of%20this%20knowledge%20can%20vary%20across%0Adomains.%20Many%20existing%20approaches%20assume%20that%20LLMs%20possess%20a%20comprehensive%0Aunderstanding%20of%20their%20environment%2C%20often%20overlooking%20potential%20gaps%20in%20their%0Agrasp%20of%20actual%20world%20dynamics.%20To%20address%20this%2C%20we%20introduce%20Discover%2C%20Verify%2C%0Aand%20Evolve%20%28DiVE%29%2C%20a%20framework%20that%20discovers%20world%20dynamics%20from%20a%20small%0Anumber%20of%20demonstrations%2C%20verifies%20the%20accuracy%20of%20these%20dynamics%2C%20and%20evolves%0Anew%2C%20advanced%20dynamics%20tailored%20to%20the%20current%20situation.%20Through%20extensive%0Aevaluations%2C%20we%20assess%20the%20impact%20of%20each%20component%20on%20performance%20and%20compare%0Athe%20dynamics%20generated%20by%20DiVE%20to%20human-annotated%20dynamics.%20Our%20results%20show%0Athat%20LLMs%20guided%20by%20DiVE%20make%20more%20informed%20decisions%2C%20achieving%20rewards%0Acomparable%20to%20human%20players%20in%20the%20Crafter%20environment%20and%20surpassing%20methods%0Athat%20require%20prior%20task-specific%20training%20in%20the%20MiniHack%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Agent%2520Learning%2520through%2520World%2520Dynamics%2520Modeling%26entry.906535625%3DZhiyuan%2520Sun%2520and%2520Haochen%2520Shi%2520and%2520Marc-Alexandre%2520C%25C3%25B4t%25C3%25A9%2520and%2520Glen%2520Berseth%2520and%2520Xingdi%2520Yuan%2520and%2520Bang%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520increasingly%2520applied%2520to%2520tasks%2520in%250Alanguage%2520understanding%2520and%2520interactive%2520decision-making%252C%2520with%2520their%2520impressive%250Aperformance%2520largely%2520attributed%2520to%2520the%2520extensive%2520domain%2520knowledge%2520embedded%250Awithin%2520them.%2520However%252C%2520the%2520depth%2520and%2520breadth%2520of%2520this%2520knowledge%2520can%2520vary%2520across%250Adomains.%2520Many%2520existing%2520approaches%2520assume%2520that%2520LLMs%2520possess%2520a%2520comprehensive%250Aunderstanding%2520of%2520their%2520environment%252C%2520often%2520overlooking%2520potential%2520gaps%2520in%2520their%250Agrasp%2520of%2520actual%2520world%2520dynamics.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Discover%252C%2520Verify%252C%250Aand%2520Evolve%2520%2528DiVE%2529%252C%2520a%2520framework%2520that%2520discovers%2520world%2520dynamics%2520from%2520a%2520small%250Anumber%2520of%2520demonstrations%252C%2520verifies%2520the%2520accuracy%2520of%2520these%2520dynamics%252C%2520and%2520evolves%250Anew%252C%2520advanced%2520dynamics%2520tailored%2520to%2520the%2520current%2520situation.%2520Through%2520extensive%250Aevaluations%252C%2520we%2520assess%2520the%2520impact%2520of%2520each%2520component%2520on%2520performance%2520and%2520compare%250Athe%2520dynamics%2520generated%2520by%2520DiVE%2520to%2520human-annotated%2520dynamics.%2520Our%2520results%2520show%250Athat%2520LLMs%2520guided%2520by%2520DiVE%2520make%2520more%2520informed%2520decisions%252C%2520achieving%2520rewards%250Acomparable%2520to%2520human%2520players%2520in%2520the%2520Crafter%2520environment%2520and%2520surpassing%2520methods%250Athat%2520require%2520prior%2520task-specific%2520training%2520in%2520the%2520MiniHack%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Agent%20Learning%20through%20World%20Dynamics%20Modeling&entry.906535625=Zhiyuan%20Sun%20and%20Haochen%20Shi%20and%20Marc-Alexandre%20C%C3%B4t%C3%A9%20and%20Glen%20Berseth%20and%20Xingdi%20Yuan%20and%20Bang%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20increasingly%20applied%20to%20tasks%20in%0Alanguage%20understanding%20and%20interactive%20decision-making%2C%20with%20their%20impressive%0Aperformance%20largely%20attributed%20to%20the%20extensive%20domain%20knowledge%20embedded%0Awithin%20them.%20However%2C%20the%20depth%20and%20breadth%20of%20this%20knowledge%20can%20vary%20across%0Adomains.%20Many%20existing%20approaches%20assume%20that%20LLMs%20possess%20a%20comprehensive%0Aunderstanding%20of%20their%20environment%2C%20often%20overlooking%20potential%20gaps%20in%20their%0Agrasp%20of%20actual%20world%20dynamics.%20To%20address%20this%2C%20we%20introduce%20Discover%2C%20Verify%2C%0Aand%20Evolve%20%28DiVE%29%2C%20a%20framework%20that%20discovers%20world%20dynamics%20from%20a%20small%0Anumber%20of%20demonstrations%2C%20verifies%20the%20accuracy%20of%20these%20dynamics%2C%20and%20evolves%0Anew%2C%20advanced%20dynamics%20tailored%20to%20the%20current%20situation.%20Through%20extensive%0Aevaluations%2C%20we%20assess%20the%20impact%20of%20each%20component%20on%20performance%20and%20compare%0Athe%20dynamics%20generated%20by%20DiVE%20to%20human-annotated%20dynamics.%20Our%20results%20show%0Athat%20LLMs%20guided%20by%20DiVE%20make%20more%20informed%20decisions%2C%20achieving%20rewards%0Acomparable%20to%20human%20players%20in%20the%20Crafter%20environment%20and%20surpassing%20methods%0Athat%20require%20prior%20task-specific%20training%20in%20the%20MiniHack%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17695v2&entry.124074799=Read"},
{"title": "A Probabilistic Model Behind Self-Supervised Learning", "author": "Alice Bizeul and Bernhard Sch\u00f6lkopf and Carl Allen", "abstract": "  In self-supervised learning (SSL), representations are learned via an\nauxiliary task without annotated labels. A common task is to classify\naugmentations or different modalities of the data, which share semantic content\n(e.g. an object in an image) but differ in style (e.g. the object's location).\nMany approaches to self-supervised learning have been proposed, e.g. SimCLR,\nCLIP, and DINO, which have recently gained much attention for their\nrepresentations achieving downstream performance comparable to supervised\nlearning. However, a theoretical understanding of self-supervised methods\neludes. Addressing this, we present a generative latent variable model for\nself-supervised learning and show that several families of discriminative SSL,\nincluding contrastive methods, induce a comparable distribution over\nrepresentations, providing a unifying theoretical framework for these methods.\nThe proposed model also justifies connections drawn to mutual information and\nthe use of a ''projection head''. Learning representations by fitting the model\ngeneratively (termed SimVAE) improves performance over discriminative and other\nVAE-based methods on simple image benchmarks and significantly narrows the gap\nbetween generative and discriminative representation learning in more complex\nsettings. Importantly, as our analysis predicts, SimVAE outperforms\nself-supervised learning where style information is required, taking an\nimportant step toward understanding self-supervised methods and achieving\ntask-agnostic representations.\n", "link": "http://arxiv.org/abs/2402.01399v3", "date": "2024-10-15", "relevancy": 2.7822, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5918}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Probabilistic%20Model%20Behind%20Self-Supervised%20Learning&body=Title%3A%20A%20Probabilistic%20Model%20Behind%20Self-Supervised%20Learning%0AAuthor%3A%20Alice%20Bizeul%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Carl%20Allen%0AAbstract%3A%20%20%20In%20self-supervised%20learning%20%28SSL%29%2C%20representations%20are%20learned%20via%20an%0Aauxiliary%20task%20without%20annotated%20labels.%20A%20common%20task%20is%20to%20classify%0Aaugmentations%20or%20different%20modalities%20of%20the%20data%2C%20which%20share%20semantic%20content%0A%28e.g.%20an%20object%20in%20an%20image%29%20but%20differ%20in%20style%20%28e.g.%20the%20object%27s%20location%29.%0AMany%20approaches%20to%20self-supervised%20learning%20have%20been%20proposed%2C%20e.g.%20SimCLR%2C%0ACLIP%2C%20and%20DINO%2C%20which%20have%20recently%20gained%20much%20attention%20for%20their%0Arepresentations%20achieving%20downstream%20performance%20comparable%20to%20supervised%0Alearning.%20However%2C%20a%20theoretical%20understanding%20of%20self-supervised%20methods%0Aeludes.%20Addressing%20this%2C%20we%20present%20a%20generative%20latent%20variable%20model%20for%0Aself-supervised%20learning%20and%20show%20that%20several%20families%20of%20discriminative%20SSL%2C%0Aincluding%20contrastive%20methods%2C%20induce%20a%20comparable%20distribution%20over%0Arepresentations%2C%20providing%20a%20unifying%20theoretical%20framework%20for%20these%20methods.%0AThe%20proposed%20model%20also%20justifies%20connections%20drawn%20to%20mutual%20information%20and%0Athe%20use%20of%20a%20%27%27projection%20head%27%27.%20Learning%20representations%20by%20fitting%20the%20model%0Ageneratively%20%28termed%20SimVAE%29%20improves%20performance%20over%20discriminative%20and%20other%0AVAE-based%20methods%20on%20simple%20image%20benchmarks%20and%20significantly%20narrows%20the%20gap%0Abetween%20generative%20and%20discriminative%20representation%20learning%20in%20more%20complex%0Asettings.%20Importantly%2C%20as%20our%20analysis%20predicts%2C%20SimVAE%20outperforms%0Aself-supervised%20learning%20where%20style%20information%20is%20required%2C%20taking%20an%0Aimportant%20step%20toward%20understanding%20self-supervised%20methods%20and%20achieving%0Atask-agnostic%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01399v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Probabilistic%2520Model%2520Behind%2520Self-Supervised%2520Learning%26entry.906535625%3DAlice%2520Bizeul%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Carl%2520Allen%26entry.1292438233%3D%2520%2520In%2520self-supervised%2520learning%2520%2528SSL%2529%252C%2520representations%2520are%2520learned%2520via%2520an%250Aauxiliary%2520task%2520without%2520annotated%2520labels.%2520A%2520common%2520task%2520is%2520to%2520classify%250Aaugmentations%2520or%2520different%2520modalities%2520of%2520the%2520data%252C%2520which%2520share%2520semantic%2520content%250A%2528e.g.%2520an%2520object%2520in%2520an%2520image%2529%2520but%2520differ%2520in%2520style%2520%2528e.g.%2520the%2520object%2527s%2520location%2529.%250AMany%2520approaches%2520to%2520self-supervised%2520learning%2520have%2520been%2520proposed%252C%2520e.g.%2520SimCLR%252C%250ACLIP%252C%2520and%2520DINO%252C%2520which%2520have%2520recently%2520gained%2520much%2520attention%2520for%2520their%250Arepresentations%2520achieving%2520downstream%2520performance%2520comparable%2520to%2520supervised%250Alearning.%2520However%252C%2520a%2520theoretical%2520understanding%2520of%2520self-supervised%2520methods%250Aeludes.%2520Addressing%2520this%252C%2520we%2520present%2520a%2520generative%2520latent%2520variable%2520model%2520for%250Aself-supervised%2520learning%2520and%2520show%2520that%2520several%2520families%2520of%2520discriminative%2520SSL%252C%250Aincluding%2520contrastive%2520methods%252C%2520induce%2520a%2520comparable%2520distribution%2520over%250Arepresentations%252C%2520providing%2520a%2520unifying%2520theoretical%2520framework%2520for%2520these%2520methods.%250AThe%2520proposed%2520model%2520also%2520justifies%2520connections%2520drawn%2520to%2520mutual%2520information%2520and%250Athe%2520use%2520of%2520a%2520%2527%2527projection%2520head%2527%2527.%2520Learning%2520representations%2520by%2520fitting%2520the%2520model%250Ageneratively%2520%2528termed%2520SimVAE%2529%2520improves%2520performance%2520over%2520discriminative%2520and%2520other%250AVAE-based%2520methods%2520on%2520simple%2520image%2520benchmarks%2520and%2520significantly%2520narrows%2520the%2520gap%250Abetween%2520generative%2520and%2520discriminative%2520representation%2520learning%2520in%2520more%2520complex%250Asettings.%2520Importantly%252C%2520as%2520our%2520analysis%2520predicts%252C%2520SimVAE%2520outperforms%250Aself-supervised%2520learning%2520where%2520style%2520information%2520is%2520required%252C%2520taking%2520an%250Aimportant%2520step%2520toward%2520understanding%2520self-supervised%2520methods%2520and%2520achieving%250Atask-agnostic%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01399v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Probabilistic%20Model%20Behind%20Self-Supervised%20Learning&entry.906535625=Alice%20Bizeul%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Carl%20Allen&entry.1292438233=%20%20In%20self-supervised%20learning%20%28SSL%29%2C%20representations%20are%20learned%20via%20an%0Aauxiliary%20task%20without%20annotated%20labels.%20A%20common%20task%20is%20to%20classify%0Aaugmentations%20or%20different%20modalities%20of%20the%20data%2C%20which%20share%20semantic%20content%0A%28e.g.%20an%20object%20in%20an%20image%29%20but%20differ%20in%20style%20%28e.g.%20the%20object%27s%20location%29.%0AMany%20approaches%20to%20self-supervised%20learning%20have%20been%20proposed%2C%20e.g.%20SimCLR%2C%0ACLIP%2C%20and%20DINO%2C%20which%20have%20recently%20gained%20much%20attention%20for%20their%0Arepresentations%20achieving%20downstream%20performance%20comparable%20to%20supervised%0Alearning.%20However%2C%20a%20theoretical%20understanding%20of%20self-supervised%20methods%0Aeludes.%20Addressing%20this%2C%20we%20present%20a%20generative%20latent%20variable%20model%20for%0Aself-supervised%20learning%20and%20show%20that%20several%20families%20of%20discriminative%20SSL%2C%0Aincluding%20contrastive%20methods%2C%20induce%20a%20comparable%20distribution%20over%0Arepresentations%2C%20providing%20a%20unifying%20theoretical%20framework%20for%20these%20methods.%0AThe%20proposed%20model%20also%20justifies%20connections%20drawn%20to%20mutual%20information%20and%0Athe%20use%20of%20a%20%27%27projection%20head%27%27.%20Learning%20representations%20by%20fitting%20the%20model%0Ageneratively%20%28termed%20SimVAE%29%20improves%20performance%20over%20discriminative%20and%20other%0AVAE-based%20methods%20on%20simple%20image%20benchmarks%20and%20significantly%20narrows%20the%20gap%0Abetween%20generative%20and%20discriminative%20representation%20learning%20in%20more%20complex%0Asettings.%20Importantly%2C%20as%20our%20analysis%20predicts%2C%20SimVAE%20outperforms%0Aself-supervised%20learning%20where%20style%20information%20is%20required%2C%20taking%20an%0Aimportant%20step%20toward%20understanding%20self-supervised%20methods%20and%20achieving%0Atask-agnostic%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01399v3&entry.124074799=Read"},
{"title": "RS-MOCO: A deep learning-based topology-preserving image registration\n  method for cardiac T1 mapping", "author": "Chiyi Huang and Longwei Sun and Dong Liang and Haifeng Liang and Hongwu Zeng and Yanjie Zhu", "abstract": "  Cardiac T1 mapping can evaluate various clinical symptoms of myocardial\ntissue. However, there is currently a lack of effective, robust, and efficient\nmethods for motion correction in cardiac T1 mapping. In this paper, we propose\na deep learning-based and topology-preserving image registration framework for\nmotion correction in cardiac T1 mapping. Notably, our proposed implicit\nconsistency constraint dubbed BLOC, to some extent preserves the image topology\nin registration by bidirectional consistency constraint and local anti-folding\nconstraint. To address the contrast variation issue, we introduce a weighted\nimage similarity metric for multimodal registration of cardiac T1-weighted\nimages. Besides, a semi-supervised myocardium segmentation network and a\ndual-domain attention module are integrated into the framework to further\nimprove the performance of the registration. Numerous comparative experiments,\nas well as ablation studies, demonstrated the effectiveness and high robustness\nof our method. The results also indicate that the proposed weighted image\nsimilarity metric, specifically crafted for our network, contributes a lot to\nthe enhancement of the motion correction efficacy, while the bidirectional\nconsistency constraint combined with the local anti-folding constraint ensures\na more desirable topology-preserving registration mapping.\n", "link": "http://arxiv.org/abs/2410.11651v1", "date": "2024-10-15", "relevancy": 2.7799, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5767}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RS-MOCO%3A%20A%20deep%20learning-based%20topology-preserving%20image%20registration%0A%20%20method%20for%20cardiac%20T1%20mapping&body=Title%3A%20RS-MOCO%3A%20A%20deep%20learning-based%20topology-preserving%20image%20registration%0A%20%20method%20for%20cardiac%20T1%20mapping%0AAuthor%3A%20Chiyi%20Huang%20and%20Longwei%20Sun%20and%20Dong%20Liang%20and%20Haifeng%20Liang%20and%20Hongwu%20Zeng%20and%20Yanjie%20Zhu%0AAbstract%3A%20%20%20Cardiac%20T1%20mapping%20can%20evaluate%20various%20clinical%20symptoms%20of%20myocardial%0Atissue.%20However%2C%20there%20is%20currently%20a%20lack%20of%20effective%2C%20robust%2C%20and%20efficient%0Amethods%20for%20motion%20correction%20in%20cardiac%20T1%20mapping.%20In%20this%20paper%2C%20we%20propose%0Aa%20deep%20learning-based%20and%20topology-preserving%20image%20registration%20framework%20for%0Amotion%20correction%20in%20cardiac%20T1%20mapping.%20Notably%2C%20our%20proposed%20implicit%0Aconsistency%20constraint%20dubbed%20BLOC%2C%20to%20some%20extent%20preserves%20the%20image%20topology%0Ain%20registration%20by%20bidirectional%20consistency%20constraint%20and%20local%20anti-folding%0Aconstraint.%20To%20address%20the%20contrast%20variation%20issue%2C%20we%20introduce%20a%20weighted%0Aimage%20similarity%20metric%20for%20multimodal%20registration%20of%20cardiac%20T1-weighted%0Aimages.%20Besides%2C%20a%20semi-supervised%20myocardium%20segmentation%20network%20and%20a%0Adual-domain%20attention%20module%20are%20integrated%20into%20the%20framework%20to%20further%0Aimprove%20the%20performance%20of%20the%20registration.%20Numerous%20comparative%20experiments%2C%0Aas%20well%20as%20ablation%20studies%2C%20demonstrated%20the%20effectiveness%20and%20high%20robustness%0Aof%20our%20method.%20The%20results%20also%20indicate%20that%20the%20proposed%20weighted%20image%0Asimilarity%20metric%2C%20specifically%20crafted%20for%20our%20network%2C%20contributes%20a%20lot%20to%0Athe%20enhancement%20of%20the%20motion%20correction%20efficacy%2C%20while%20the%20bidirectional%0Aconsistency%20constraint%20combined%20with%20the%20local%20anti-folding%20constraint%20ensures%0Aa%20more%20desirable%20topology-preserving%20registration%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRS-MOCO%253A%2520A%2520deep%2520learning-based%2520topology-preserving%2520image%2520registration%250A%2520%2520method%2520for%2520cardiac%2520T1%2520mapping%26entry.906535625%3DChiyi%2520Huang%2520and%2520Longwei%2520Sun%2520and%2520Dong%2520Liang%2520and%2520Haifeng%2520Liang%2520and%2520Hongwu%2520Zeng%2520and%2520Yanjie%2520Zhu%26entry.1292438233%3D%2520%2520Cardiac%2520T1%2520mapping%2520can%2520evaluate%2520various%2520clinical%2520symptoms%2520of%2520myocardial%250Atissue.%2520However%252C%2520there%2520is%2520currently%2520a%2520lack%2520of%2520effective%252C%2520robust%252C%2520and%2520efficient%250Amethods%2520for%2520motion%2520correction%2520in%2520cardiac%2520T1%2520mapping.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520deep%2520learning-based%2520and%2520topology-preserving%2520image%2520registration%2520framework%2520for%250Amotion%2520correction%2520in%2520cardiac%2520T1%2520mapping.%2520Notably%252C%2520our%2520proposed%2520implicit%250Aconsistency%2520constraint%2520dubbed%2520BLOC%252C%2520to%2520some%2520extent%2520preserves%2520the%2520image%2520topology%250Ain%2520registration%2520by%2520bidirectional%2520consistency%2520constraint%2520and%2520local%2520anti-folding%250Aconstraint.%2520To%2520address%2520the%2520contrast%2520variation%2520issue%252C%2520we%2520introduce%2520a%2520weighted%250Aimage%2520similarity%2520metric%2520for%2520multimodal%2520registration%2520of%2520cardiac%2520T1-weighted%250Aimages.%2520Besides%252C%2520a%2520semi-supervised%2520myocardium%2520segmentation%2520network%2520and%2520a%250Adual-domain%2520attention%2520module%2520are%2520integrated%2520into%2520the%2520framework%2520to%2520further%250Aimprove%2520the%2520performance%2520of%2520the%2520registration.%2520Numerous%2520comparative%2520experiments%252C%250Aas%2520well%2520as%2520ablation%2520studies%252C%2520demonstrated%2520the%2520effectiveness%2520and%2520high%2520robustness%250Aof%2520our%2520method.%2520The%2520results%2520also%2520indicate%2520that%2520the%2520proposed%2520weighted%2520image%250Asimilarity%2520metric%252C%2520specifically%2520crafted%2520for%2520our%2520network%252C%2520contributes%2520a%2520lot%2520to%250Athe%2520enhancement%2520of%2520the%2520motion%2520correction%2520efficacy%252C%2520while%2520the%2520bidirectional%250Aconsistency%2520constraint%2520combined%2520with%2520the%2520local%2520anti-folding%2520constraint%2520ensures%250Aa%2520more%2520desirable%2520topology-preserving%2520registration%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RS-MOCO%3A%20A%20deep%20learning-based%20topology-preserving%20image%20registration%0A%20%20method%20for%20cardiac%20T1%20mapping&entry.906535625=Chiyi%20Huang%20and%20Longwei%20Sun%20and%20Dong%20Liang%20and%20Haifeng%20Liang%20and%20Hongwu%20Zeng%20and%20Yanjie%20Zhu&entry.1292438233=%20%20Cardiac%20T1%20mapping%20can%20evaluate%20various%20clinical%20symptoms%20of%20myocardial%0Atissue.%20However%2C%20there%20is%20currently%20a%20lack%20of%20effective%2C%20robust%2C%20and%20efficient%0Amethods%20for%20motion%20correction%20in%20cardiac%20T1%20mapping.%20In%20this%20paper%2C%20we%20propose%0Aa%20deep%20learning-based%20and%20topology-preserving%20image%20registration%20framework%20for%0Amotion%20correction%20in%20cardiac%20T1%20mapping.%20Notably%2C%20our%20proposed%20implicit%0Aconsistency%20constraint%20dubbed%20BLOC%2C%20to%20some%20extent%20preserves%20the%20image%20topology%0Ain%20registration%20by%20bidirectional%20consistency%20constraint%20and%20local%20anti-folding%0Aconstraint.%20To%20address%20the%20contrast%20variation%20issue%2C%20we%20introduce%20a%20weighted%0Aimage%20similarity%20metric%20for%20multimodal%20registration%20of%20cardiac%20T1-weighted%0Aimages.%20Besides%2C%20a%20semi-supervised%20myocardium%20segmentation%20network%20and%20a%0Adual-domain%20attention%20module%20are%20integrated%20into%20the%20framework%20to%20further%0Aimprove%20the%20performance%20of%20the%20registration.%20Numerous%20comparative%20experiments%2C%0Aas%20well%20as%20ablation%20studies%2C%20demonstrated%20the%20effectiveness%20and%20high%20robustness%0Aof%20our%20method.%20The%20results%20also%20indicate%20that%20the%20proposed%20weighted%20image%0Asimilarity%20metric%2C%20specifically%20crafted%20for%20our%20network%2C%20contributes%20a%20lot%20to%0Athe%20enhancement%20of%20the%20motion%20correction%20efficacy%2C%20while%20the%20bidirectional%0Aconsistency%20constraint%20combined%20with%20the%20local%20anti-folding%20constraint%20ensures%0Aa%20more%20desirable%20topology-preserving%20registration%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11651v1&entry.124074799=Read"},
{"title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension", "author": "Amaia Cardiel and Eloi Zablocki and Elias Ramzi and Oriane Sim\u00e9oni and Matthieu Cord", "abstract": "  Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific finetuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs, and it allows for\nthe adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple\ndatasets using different VLMs and LLMs, demonstrating significant performance\nimprovements and highlighting the versatility of our method. While LLM-wrapper\nis not meant to directly compete with standard white-box fine-tuning, it offers\na practical and effective alternative for black-box VLM adaptation. The code\nwill be open-sourced.\n", "link": "http://arxiv.org/abs/2409.11919v2", "date": "2024-10-15", "relevancy": 2.7771, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5619}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-wrapper%3A%20Black-Box%20Semantic-Aware%20Adaptation%20of%20Vision-Language%0A%20%20Models%20for%20Referring%20Expression%20Comprehension&body=Title%3A%20LLM-wrapper%3A%20Black-Box%20Semantic-Aware%20Adaptation%20of%20Vision-Language%0A%20%20Models%20for%20Referring%20Expression%20Comprehension%0AAuthor%3A%20Amaia%20Cardiel%20and%20Eloi%20Zablocki%20and%20Elias%20Ramzi%20and%20Oriane%20Sim%C3%A9oni%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Avarious%20open-vocabulary%20tasks%2C%20yet%20their%20zero-shot%20performance%20lags%20behind%0Atask-specific%20finetuned%20models%2C%20particularly%20in%20complex%20tasks%20like%20Referring%0AExpression%20Comprehension%20%28REC%29.%20Fine-tuning%20usually%20requires%20%27white-box%27%20access%0Ato%20the%20model%27s%20architecture%20and%20weights%2C%20which%20is%20not%20always%20feasible%20due%20to%0Aproprietary%20or%20privacy%20concerns.%20In%20this%20work%2C%20we%20propose%20LLM-wrapper%2C%20a%20method%0Afor%20%27black-box%27%20adaptation%20of%20VLMs%20for%20the%20REC%20task%20using%20Large%20Language%20Models%0A%28LLMs%29.%20LLM-wrapper%20capitalizes%20on%20the%20reasoning%20abilities%20of%20LLMs%2C%20improved%0Awith%20a%20light%20fine-tuning%2C%20to%20select%20the%20most%20relevant%20bounding%20box%20matching%20the%0Areferring%20expression%2C%20from%20candidates%20generated%20by%20a%20zero-shot%20black-box%20VLM.%0AOur%20approach%20offers%20several%20advantages%3A%20it%20enables%20the%20adaptation%20of%0Aclosed-source%20models%20without%20needing%20access%20to%20their%20internal%20workings%2C%20it%20is%0Aversatile%20as%20it%20works%20with%20any%20VLM%2C%20it%20transfers%20to%20new%20VLMs%2C%20and%20it%20allows%20for%0Athe%20adaptation%20of%20an%20ensemble%20of%20VLMs.%20We%20evaluate%20LLM-wrapper%20on%20multiple%0Adatasets%20using%20different%20VLMs%20and%20LLMs%2C%20demonstrating%20significant%20performance%0Aimprovements%20and%20highlighting%20the%20versatility%20of%20our%20method.%20While%20LLM-wrapper%0Ais%20not%20meant%20to%20directly%20compete%20with%20standard%20white-box%20fine-tuning%2C%20it%20offers%0Aa%20practical%20and%20effective%20alternative%20for%20black-box%20VLM%20adaptation.%20The%20code%0Awill%20be%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-wrapper%253A%2520Black-Box%2520Semantic-Aware%2520Adaptation%2520of%2520Vision-Language%250A%2520%2520Models%2520for%2520Referring%2520Expression%2520Comprehension%26entry.906535625%3DAmaia%2520Cardiel%2520and%2520Eloi%2520Zablocki%2520and%2520Elias%2520Ramzi%2520and%2520Oriane%2520Sim%25C3%25A9oni%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Avarious%2520open-vocabulary%2520tasks%252C%2520yet%2520their%2520zero-shot%2520performance%2520lags%2520behind%250Atask-specific%2520finetuned%2520models%252C%2520particularly%2520in%2520complex%2520tasks%2520like%2520Referring%250AExpression%2520Comprehension%2520%2528REC%2529.%2520Fine-tuning%2520usually%2520requires%2520%2527white-box%2527%2520access%250Ato%2520the%2520model%2527s%2520architecture%2520and%2520weights%252C%2520which%2520is%2520not%2520always%2520feasible%2520due%2520to%250Aproprietary%2520or%2520privacy%2520concerns.%2520In%2520this%2520work%252C%2520we%2520propose%2520LLM-wrapper%252C%2520a%2520method%250Afor%2520%2527black-box%2527%2520adaptation%2520of%2520VLMs%2520for%2520the%2520REC%2520task%2520using%2520Large%2520Language%2520Models%250A%2528LLMs%2529.%2520LLM-wrapper%2520capitalizes%2520on%2520the%2520reasoning%2520abilities%2520of%2520LLMs%252C%2520improved%250Awith%2520a%2520light%2520fine-tuning%252C%2520to%2520select%2520the%2520most%2520relevant%2520bounding%2520box%2520matching%2520the%250Areferring%2520expression%252C%2520from%2520candidates%2520generated%2520by%2520a%2520zero-shot%2520black-box%2520VLM.%250AOur%2520approach%2520offers%2520several%2520advantages%253A%2520it%2520enables%2520the%2520adaptation%2520of%250Aclosed-source%2520models%2520without%2520needing%2520access%2520to%2520their%2520internal%2520workings%252C%2520it%2520is%250Aversatile%2520as%2520it%2520works%2520with%2520any%2520VLM%252C%2520it%2520transfers%2520to%2520new%2520VLMs%252C%2520and%2520it%2520allows%2520for%250Athe%2520adaptation%2520of%2520an%2520ensemble%2520of%2520VLMs.%2520We%2520evaluate%2520LLM-wrapper%2520on%2520multiple%250Adatasets%2520using%2520different%2520VLMs%2520and%2520LLMs%252C%2520demonstrating%2520significant%2520performance%250Aimprovements%2520and%2520highlighting%2520the%2520versatility%2520of%2520our%2520method.%2520While%2520LLM-wrapper%250Ais%2520not%2520meant%2520to%2520directly%2520compete%2520with%2520standard%2520white-box%2520fine-tuning%252C%2520it%2520offers%250Aa%2520practical%2520and%2520effective%2520alternative%2520for%2520black-box%2520VLM%2520adaptation.%2520The%2520code%250Awill%2520be%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-wrapper%3A%20Black-Box%20Semantic-Aware%20Adaptation%20of%20Vision-Language%0A%20%20Models%20for%20Referring%20Expression%20Comprehension&entry.906535625=Amaia%20Cardiel%20and%20Eloi%20Zablocki%20and%20Elias%20Ramzi%20and%20Oriane%20Sim%C3%A9oni%20and%20Matthieu%20Cord&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Avarious%20open-vocabulary%20tasks%2C%20yet%20their%20zero-shot%20performance%20lags%20behind%0Atask-specific%20finetuned%20models%2C%20particularly%20in%20complex%20tasks%20like%20Referring%0AExpression%20Comprehension%20%28REC%29.%20Fine-tuning%20usually%20requires%20%27white-box%27%20access%0Ato%20the%20model%27s%20architecture%20and%20weights%2C%20which%20is%20not%20always%20feasible%20due%20to%0Aproprietary%20or%20privacy%20concerns.%20In%20this%20work%2C%20we%20propose%20LLM-wrapper%2C%20a%20method%0Afor%20%27black-box%27%20adaptation%20of%20VLMs%20for%20the%20REC%20task%20using%20Large%20Language%20Models%0A%28LLMs%29.%20LLM-wrapper%20capitalizes%20on%20the%20reasoning%20abilities%20of%20LLMs%2C%20improved%0Awith%20a%20light%20fine-tuning%2C%20to%20select%20the%20most%20relevant%20bounding%20box%20matching%20the%0Areferring%20expression%2C%20from%20candidates%20generated%20by%20a%20zero-shot%20black-box%20VLM.%0AOur%20approach%20offers%20several%20advantages%3A%20it%20enables%20the%20adaptation%20of%0Aclosed-source%20models%20without%20needing%20access%20to%20their%20internal%20workings%2C%20it%20is%0Aversatile%20as%20it%20works%20with%20any%20VLM%2C%20it%20transfers%20to%20new%20VLMs%2C%20and%20it%20allows%20for%0Athe%20adaptation%20of%20an%20ensemble%20of%20VLMs.%20We%20evaluate%20LLM-wrapper%20on%20multiple%0Adatasets%20using%20different%20VLMs%20and%20LLMs%2C%20demonstrating%20significant%20performance%0Aimprovements%20and%20highlighting%20the%20versatility%20of%20our%20method.%20While%20LLM-wrapper%0Ais%20not%20meant%20to%20directly%20compete%20with%20standard%20white-box%20fine-tuning%2C%20it%20offers%0Aa%20practical%20and%20effective%20alternative%20for%20black-box%20VLM%20adaptation.%20The%20code%0Awill%20be%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11919v2&entry.124074799=Read"},
{"title": "POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search", "author": "Chong-Yang Xiang and Jun-Yan He and Zhi-Qi Cheng and Xiao Wu and Xian-Sheng Hua", "abstract": "  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces the Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the fundamental limitations of traditional FLD methods. POPoS\nemploys three key innovations: (1) Pseudo-range multilateration is utilized to\ncorrect heatmap errors, enhancing the precision of landmark localization. By\nintegrating multiple anchor points, this approach minimizes the impact of\nindividual heatmap inaccuracies, leading to robust overall positioning. (2) To\nimprove the pseudo-range accuracy of selected anchor points, a new loss\nfunction, named multilateration anchor loss, is proposed. This loss function\neffectively enhances the accuracy of the distance map, mitigates the risk of\nlocal optima, and ensures optimal solutions. (3) A single-step parallel\ncomputation algorithm is introduced, significantly enhancing computational\nefficiency and reducing processing time. Comprehensive evaluations across five\nbenchmark datasets demonstrate that POPoS consistently outperforms existing\nmethods, particularly excelling in low-resolution scenarios with minimal\ncomputational overhead. These features establish POPoS as a highly efficient\nand accurate tool for FLD, with broad applicability in real-world scenarios.\nThe code is available at https://github.com/teslatasy/PoPoS\n", "link": "http://arxiv.org/abs/2410.09583v2", "date": "2024-10-15", "relevancy": 2.7105, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5888}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5273}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POPoS%3A%20Improving%20Efficient%20and%20Robust%20Facial%20Landmark%20Detection%20with%0A%20%20Parallel%20Optimal%20Position%20Search&body=Title%3A%20POPoS%3A%20Improving%20Efficient%20and%20Robust%20Facial%20Landmark%20Detection%20with%0A%20%20Parallel%20Optimal%20Position%20Search%0AAuthor%3A%20Chong-Yang%20Xiang%20and%20Jun-Yan%20He%20and%20Zhi-Qi%20Cheng%20and%20Xiao%20Wu%20and%20Xian-Sheng%20Hua%0AAbstract%3A%20%20%20Achieving%20a%20balance%20between%20accuracy%20and%20efficiency%20is%20a%20critical%20challenge%0Ain%20facial%20landmark%20detection%20%28FLD%29.%20This%20paper%20introduces%20the%20Parallel%20Optimal%0APosition%20Search%20%28POPoS%29%2C%20a%20high-precision%20encoding-decoding%20framework%20designed%0Ato%20address%20the%20fundamental%20limitations%20of%20traditional%20FLD%20methods.%20POPoS%0Aemploys%20three%20key%20innovations%3A%20%281%29%20Pseudo-range%20multilateration%20is%20utilized%20to%0Acorrect%20heatmap%20errors%2C%20enhancing%20the%20precision%20of%20landmark%20localization.%20By%0Aintegrating%20multiple%20anchor%20points%2C%20this%20approach%20minimizes%20the%20impact%20of%0Aindividual%20heatmap%20inaccuracies%2C%20leading%20to%20robust%20overall%20positioning.%20%282%29%20To%0Aimprove%20the%20pseudo-range%20accuracy%20of%20selected%20anchor%20points%2C%20a%20new%20loss%0Afunction%2C%20named%20multilateration%20anchor%20loss%2C%20is%20proposed.%20This%20loss%20function%0Aeffectively%20enhances%20the%20accuracy%20of%20the%20distance%20map%2C%20mitigates%20the%20risk%20of%0Alocal%20optima%2C%20and%20ensures%20optimal%20solutions.%20%283%29%20A%20single-step%20parallel%0Acomputation%20algorithm%20is%20introduced%2C%20significantly%20enhancing%20computational%0Aefficiency%20and%20reducing%20processing%20time.%20Comprehensive%20evaluations%20across%20five%0Abenchmark%20datasets%20demonstrate%20that%20POPoS%20consistently%20outperforms%20existing%0Amethods%2C%20particularly%20excelling%20in%20low-resolution%20scenarios%20with%20minimal%0Acomputational%20overhead.%20These%20features%20establish%20POPoS%20as%20a%20highly%20efficient%0Aand%20accurate%20tool%20for%20FLD%2C%20with%20broad%20applicability%20in%20real-world%20scenarios.%0AThe%20code%20is%20available%20at%20https%3A//github.com/teslatasy/PoPoS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09583v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOPoS%253A%2520Improving%2520Efficient%2520and%2520Robust%2520Facial%2520Landmark%2520Detection%2520with%250A%2520%2520Parallel%2520Optimal%2520Position%2520Search%26entry.906535625%3DChong-Yang%2520Xiang%2520and%2520Jun-Yan%2520He%2520and%2520Zhi-Qi%2520Cheng%2520and%2520Xiao%2520Wu%2520and%2520Xian-Sheng%2520Hua%26entry.1292438233%3D%2520%2520Achieving%2520a%2520balance%2520between%2520accuracy%2520and%2520efficiency%2520is%2520a%2520critical%2520challenge%250Ain%2520facial%2520landmark%2520detection%2520%2528FLD%2529.%2520This%2520paper%2520introduces%2520the%2520Parallel%2520Optimal%250APosition%2520Search%2520%2528POPoS%2529%252C%2520a%2520high-precision%2520encoding-decoding%2520framework%2520designed%250Ato%2520address%2520the%2520fundamental%2520limitations%2520of%2520traditional%2520FLD%2520methods.%2520POPoS%250Aemploys%2520three%2520key%2520innovations%253A%2520%25281%2529%2520Pseudo-range%2520multilateration%2520is%2520utilized%2520to%250Acorrect%2520heatmap%2520errors%252C%2520enhancing%2520the%2520precision%2520of%2520landmark%2520localization.%2520By%250Aintegrating%2520multiple%2520anchor%2520points%252C%2520this%2520approach%2520minimizes%2520the%2520impact%2520of%250Aindividual%2520heatmap%2520inaccuracies%252C%2520leading%2520to%2520robust%2520overall%2520positioning.%2520%25282%2529%2520To%250Aimprove%2520the%2520pseudo-range%2520accuracy%2520of%2520selected%2520anchor%2520points%252C%2520a%2520new%2520loss%250Afunction%252C%2520named%2520multilateration%2520anchor%2520loss%252C%2520is%2520proposed.%2520This%2520loss%2520function%250Aeffectively%2520enhances%2520the%2520accuracy%2520of%2520the%2520distance%2520map%252C%2520mitigates%2520the%2520risk%2520of%250Alocal%2520optima%252C%2520and%2520ensures%2520optimal%2520solutions.%2520%25283%2529%2520A%2520single-step%2520parallel%250Acomputation%2520algorithm%2520is%2520introduced%252C%2520significantly%2520enhancing%2520computational%250Aefficiency%2520and%2520reducing%2520processing%2520time.%2520Comprehensive%2520evaluations%2520across%2520five%250Abenchmark%2520datasets%2520demonstrate%2520that%2520POPoS%2520consistently%2520outperforms%2520existing%250Amethods%252C%2520particularly%2520excelling%2520in%2520low-resolution%2520scenarios%2520with%2520minimal%250Acomputational%2520overhead.%2520These%2520features%2520establish%2520POPoS%2520as%2520a%2520highly%2520efficient%250Aand%2520accurate%2520tool%2520for%2520FLD%252C%2520with%2520broad%2520applicability%2520in%2520real-world%2520scenarios.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/teslatasy/PoPoS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09583v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POPoS%3A%20Improving%20Efficient%20and%20Robust%20Facial%20Landmark%20Detection%20with%0A%20%20Parallel%20Optimal%20Position%20Search&entry.906535625=Chong-Yang%20Xiang%20and%20Jun-Yan%20He%20and%20Zhi-Qi%20Cheng%20and%20Xiao%20Wu%20and%20Xian-Sheng%20Hua&entry.1292438233=%20%20Achieving%20a%20balance%20between%20accuracy%20and%20efficiency%20is%20a%20critical%20challenge%0Ain%20facial%20landmark%20detection%20%28FLD%29.%20This%20paper%20introduces%20the%20Parallel%20Optimal%0APosition%20Search%20%28POPoS%29%2C%20a%20high-precision%20encoding-decoding%20framework%20designed%0Ato%20address%20the%20fundamental%20limitations%20of%20traditional%20FLD%20methods.%20POPoS%0Aemploys%20three%20key%20innovations%3A%20%281%29%20Pseudo-range%20multilateration%20is%20utilized%20to%0Acorrect%20heatmap%20errors%2C%20enhancing%20the%20precision%20of%20landmark%20localization.%20By%0Aintegrating%20multiple%20anchor%20points%2C%20this%20approach%20minimizes%20the%20impact%20of%0Aindividual%20heatmap%20inaccuracies%2C%20leading%20to%20robust%20overall%20positioning.%20%282%29%20To%0Aimprove%20the%20pseudo-range%20accuracy%20of%20selected%20anchor%20points%2C%20a%20new%20loss%0Afunction%2C%20named%20multilateration%20anchor%20loss%2C%20is%20proposed.%20This%20loss%20function%0Aeffectively%20enhances%20the%20accuracy%20of%20the%20distance%20map%2C%20mitigates%20the%20risk%20of%0Alocal%20optima%2C%20and%20ensures%20optimal%20solutions.%20%283%29%20A%20single-step%20parallel%0Acomputation%20algorithm%20is%20introduced%2C%20significantly%20enhancing%20computational%0Aefficiency%20and%20reducing%20processing%20time.%20Comprehensive%20evaluations%20across%20five%0Abenchmark%20datasets%20demonstrate%20that%20POPoS%20consistently%20outperforms%20existing%0Amethods%2C%20particularly%20excelling%20in%20low-resolution%20scenarios%20with%20minimal%0Acomputational%20overhead.%20These%20features%20establish%20POPoS%20as%20a%20highly%20efficient%0Aand%20accurate%20tool%20for%20FLD%2C%20with%20broad%20applicability%20in%20real-world%20scenarios.%0AThe%20code%20is%20available%20at%20https%3A//github.com/teslatasy/PoPoS%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09583v2&entry.124074799=Read"},
{"title": "KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual\n  Entities", "author": "Hsin-Ping Huang and Xinyi Wang and Yonatan Bitton and Hagai Taitelbaum and Gaurav Singh Tomar and Ming-Wei Chang and Xuhui Jia and Kelvin C. K. Chan and Hexiang Hu and Yu-Chuan Su and Ming-Hsuan Yang", "abstract": "  Recent advancements in text-to-image generation have significantly enhanced\nthe quality of synthesized images. Despite this progress, evaluations\npredominantly focus on aesthetic appeal or alignment with text prompts.\nConsequently, there is limited understanding of whether these models can\naccurately represent a wide variety of realistic visual entities - a task\nrequiring real-world knowledge. To address this gap, we propose a benchmark\nfocused on evaluating Knowledge-InTensive image generaTion on real-world\nENtities (i.e., KITTEN). Using KITTEN, we conduct a systematic study on the\nfidelity of entities in text-to-image generation models, focusing on their\nability to generate a wide range of real-world visual entities, such as\nlandmark buildings, aircraft, plants, and animals. We evaluate the latest\ntext-to-image models and retrieval-augmented customization models using both\nautomatic metrics and carefully-designed human evaluations, with an emphasis on\nthe fidelity of entities in the generated images. Our findings reveal that even\nthe most advanced text-to-image models often fail to generate entities with\naccurate visual details. Although retrieval-augmented models can enhance the\nfidelity of entity by incorporating reference images during testing, they often\nover-rely on these references and struggle to produce novel configurations of\nthe entity as requested in creative text prompts.\n", "link": "http://arxiv.org/abs/2410.11824v1", "date": "2024-10-15", "relevancy": 2.7028, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.58}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5219}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KITTEN%3A%20A%20Knowledge-Intensive%20Evaluation%20of%20Image%20Generation%20on%20Visual%0A%20%20Entities&body=Title%3A%20KITTEN%3A%20A%20Knowledge-Intensive%20Evaluation%20of%20Image%20Generation%20on%20Visual%0A%20%20Entities%0AAuthor%3A%20Hsin-Ping%20Huang%20and%20Xinyi%20Wang%20and%20Yonatan%20Bitton%20and%20Hagai%20Taitelbaum%20and%20Gaurav%20Singh%20Tomar%20and%20Ming-Wei%20Chang%20and%20Xuhui%20Jia%20and%20Kelvin%20C.%20K.%20Chan%20and%20Hexiang%20Hu%20and%20Yu-Chuan%20Su%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-image%20generation%20have%20significantly%20enhanced%0Athe%20quality%20of%20synthesized%20images.%20Despite%20this%20progress%2C%20evaluations%0Apredominantly%20focus%20on%20aesthetic%20appeal%20or%20alignment%20with%20text%20prompts.%0AConsequently%2C%20there%20is%20limited%20understanding%20of%20whether%20these%20models%20can%0Aaccurately%20represent%20a%20wide%20variety%20of%20realistic%20visual%20entities%20-%20a%20task%0Arequiring%20real-world%20knowledge.%20To%20address%20this%20gap%2C%20we%20propose%20a%20benchmark%0Afocused%20on%20evaluating%20Knowledge-InTensive%20image%20generaTion%20on%20real-world%0AENtities%20%28i.e.%2C%20KITTEN%29.%20Using%20KITTEN%2C%20we%20conduct%20a%20systematic%20study%20on%20the%0Afidelity%20of%20entities%20in%20text-to-image%20generation%20models%2C%20focusing%20on%20their%0Aability%20to%20generate%20a%20wide%20range%20of%20real-world%20visual%20entities%2C%20such%20as%0Alandmark%20buildings%2C%20aircraft%2C%20plants%2C%20and%20animals.%20We%20evaluate%20the%20latest%0Atext-to-image%20models%20and%20retrieval-augmented%20customization%20models%20using%20both%0Aautomatic%20metrics%20and%20carefully-designed%20human%20evaluations%2C%20with%20an%20emphasis%20on%0Athe%20fidelity%20of%20entities%20in%20the%20generated%20images.%20Our%20findings%20reveal%20that%20even%0Athe%20most%20advanced%20text-to-image%20models%20often%20fail%20to%20generate%20entities%20with%0Aaccurate%20visual%20details.%20Although%20retrieval-augmented%20models%20can%20enhance%20the%0Afidelity%20of%20entity%20by%20incorporating%20reference%20images%20during%20testing%2C%20they%20often%0Aover-rely%20on%20these%20references%20and%20struggle%20to%20produce%20novel%20configurations%20of%0Athe%20entity%20as%20requested%20in%20creative%20text%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKITTEN%253A%2520A%2520Knowledge-Intensive%2520Evaluation%2520of%2520Image%2520Generation%2520on%2520Visual%250A%2520%2520Entities%26entry.906535625%3DHsin-Ping%2520Huang%2520and%2520Xinyi%2520Wang%2520and%2520Yonatan%2520Bitton%2520and%2520Hagai%2520Taitelbaum%2520and%2520Gaurav%2520Singh%2520Tomar%2520and%2520Ming-Wei%2520Chang%2520and%2520Xuhui%2520Jia%2520and%2520Kelvin%2520C.%2520K.%2520Chan%2520and%2520Hexiang%2520Hu%2520and%2520Yu-Chuan%2520Su%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-image%2520generation%2520have%2520significantly%2520enhanced%250Athe%2520quality%2520of%2520synthesized%2520images.%2520Despite%2520this%2520progress%252C%2520evaluations%250Apredominantly%2520focus%2520on%2520aesthetic%2520appeal%2520or%2520alignment%2520with%2520text%2520prompts.%250AConsequently%252C%2520there%2520is%2520limited%2520understanding%2520of%2520whether%2520these%2520models%2520can%250Aaccurately%2520represent%2520a%2520wide%2520variety%2520of%2520realistic%2520visual%2520entities%2520-%2520a%2520task%250Arequiring%2520real-world%2520knowledge.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520benchmark%250Afocused%2520on%2520evaluating%2520Knowledge-InTensive%2520image%2520generaTion%2520on%2520real-world%250AENtities%2520%2528i.e.%252C%2520KITTEN%2529.%2520Using%2520KITTEN%252C%2520we%2520conduct%2520a%2520systematic%2520study%2520on%2520the%250Afidelity%2520of%2520entities%2520in%2520text-to-image%2520generation%2520models%252C%2520focusing%2520on%2520their%250Aability%2520to%2520generate%2520a%2520wide%2520range%2520of%2520real-world%2520visual%2520entities%252C%2520such%2520as%250Alandmark%2520buildings%252C%2520aircraft%252C%2520plants%252C%2520and%2520animals.%2520We%2520evaluate%2520the%2520latest%250Atext-to-image%2520models%2520and%2520retrieval-augmented%2520customization%2520models%2520using%2520both%250Aautomatic%2520metrics%2520and%2520carefully-designed%2520human%2520evaluations%252C%2520with%2520an%2520emphasis%2520on%250Athe%2520fidelity%2520of%2520entities%2520in%2520the%2520generated%2520images.%2520Our%2520findings%2520reveal%2520that%2520even%250Athe%2520most%2520advanced%2520text-to-image%2520models%2520often%2520fail%2520to%2520generate%2520entities%2520with%250Aaccurate%2520visual%2520details.%2520Although%2520retrieval-augmented%2520models%2520can%2520enhance%2520the%250Afidelity%2520of%2520entity%2520by%2520incorporating%2520reference%2520images%2520during%2520testing%252C%2520they%2520often%250Aover-rely%2520on%2520these%2520references%2520and%2520struggle%2520to%2520produce%2520novel%2520configurations%2520of%250Athe%2520entity%2520as%2520requested%2520in%2520creative%2520text%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KITTEN%3A%20A%20Knowledge-Intensive%20Evaluation%20of%20Image%20Generation%20on%20Visual%0A%20%20Entities&entry.906535625=Hsin-Ping%20Huang%20and%20Xinyi%20Wang%20and%20Yonatan%20Bitton%20and%20Hagai%20Taitelbaum%20and%20Gaurav%20Singh%20Tomar%20and%20Ming-Wei%20Chang%20and%20Xuhui%20Jia%20and%20Kelvin%20C.%20K.%20Chan%20and%20Hexiang%20Hu%20and%20Yu-Chuan%20Su%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-image%20generation%20have%20significantly%20enhanced%0Athe%20quality%20of%20synthesized%20images.%20Despite%20this%20progress%2C%20evaluations%0Apredominantly%20focus%20on%20aesthetic%20appeal%20or%20alignment%20with%20text%20prompts.%0AConsequently%2C%20there%20is%20limited%20understanding%20of%20whether%20these%20models%20can%0Aaccurately%20represent%20a%20wide%20variety%20of%20realistic%20visual%20entities%20-%20a%20task%0Arequiring%20real-world%20knowledge.%20To%20address%20this%20gap%2C%20we%20propose%20a%20benchmark%0Afocused%20on%20evaluating%20Knowledge-InTensive%20image%20generaTion%20on%20real-world%0AENtities%20%28i.e.%2C%20KITTEN%29.%20Using%20KITTEN%2C%20we%20conduct%20a%20systematic%20study%20on%20the%0Afidelity%20of%20entities%20in%20text-to-image%20generation%20models%2C%20focusing%20on%20their%0Aability%20to%20generate%20a%20wide%20range%20of%20real-world%20visual%20entities%2C%20such%20as%0Alandmark%20buildings%2C%20aircraft%2C%20plants%2C%20and%20animals.%20We%20evaluate%20the%20latest%0Atext-to-image%20models%20and%20retrieval-augmented%20customization%20models%20using%20both%0Aautomatic%20metrics%20and%20carefully-designed%20human%20evaluations%2C%20with%20an%20emphasis%20on%0Athe%20fidelity%20of%20entities%20in%20the%20generated%20images.%20Our%20findings%20reveal%20that%20even%0Athe%20most%20advanced%20text-to-image%20models%20often%20fail%20to%20generate%20entities%20with%0Aaccurate%20visual%20details.%20Although%20retrieval-augmented%20models%20can%20enhance%20the%0Afidelity%20of%20entity%20by%20incorporating%20reference%20images%20during%20testing%2C%20they%20often%0Aover-rely%20on%20these%20references%20and%20struggle%20to%20produce%20novel%20configurations%20of%0Athe%20entity%20as%20requested%20in%20creative%20text%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11824v1&entry.124074799=Read"},
{"title": "SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology\n  Image Understanding", "author": "Ying Chen and Guoan Wang and Yuanfeng Ji and Yanjun Li and Jin Ye and Tianbin Li and Bin Zhang and Nana Pei and Rongshan Yu and Yu Qiao and Junjun He", "abstract": "  Despite the progress made by multimodal large language models (MLLMs) in\ncomputational pathology, they remain limited by a predominant focus on\npatch-level analysis, missing essential contextual information at the\nwhole-slide level. The lack of large-scale instruction datasets and the\ngigapixel scale of whole slide images (WSIs) pose significant developmental\nchallenges. In this paper, we present SlideChat, the first vision-language\nassistant capable of understanding gigapixel whole-slide images, exhibiting\nexcellent multimodal conversational capability and response complex instruction\nacross diverse pathology scenarios. To support its development, we created\nSlideInstruction, the largest instruction-following dataset for WSIs consisting\nof 4.2K WSI captions and 176K VQA pairs with multiple categories. Furthermore,\nwe propose SlideBench, a multimodal benchmark that incorporates captioning and\nVQA tasks to assess SlideChat's capabilities in varied clinical settings such\nas microscopy, diagnosis. Compared to both general and specialized MLLMs,\nSlideChat exhibits exceptional capabilities achieving state-of-the-art\nperformance on 18 of 22 tasks. For example, it achieved an overall accuracy of\n81.17% on SlideBench-VQA (TCGA), and 54.15% on SlideBench-VQA (BCNB). We will\nfully release SlideChat, SlideInstruction and SlideBench as open-source\nresources to facilitate research and development in computational pathology.\n", "link": "http://arxiv.org/abs/2410.11761v1", "date": "2024-10-15", "relevancy": 2.7023, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlideChat%3A%20A%20Large%20Vision-Language%20Assistant%20for%20Whole-Slide%20Pathology%0A%20%20Image%20Understanding&body=Title%3A%20SlideChat%3A%20A%20Large%20Vision-Language%20Assistant%20for%20Whole-Slide%20Pathology%0A%20%20Image%20Understanding%0AAuthor%3A%20Ying%20Chen%20and%20Guoan%20Wang%20and%20Yuanfeng%20Ji%20and%20Yanjun%20Li%20and%20Jin%20Ye%20and%20Tianbin%20Li%20and%20Bin%20Zhang%20and%20Nana%20Pei%20and%20Rongshan%20Yu%20and%20Yu%20Qiao%20and%20Junjun%20He%0AAbstract%3A%20%20%20Despite%20the%20progress%20made%20by%20multimodal%20large%20language%20models%20%28MLLMs%29%20in%0Acomputational%20pathology%2C%20they%20remain%20limited%20by%20a%20predominant%20focus%20on%0Apatch-level%20analysis%2C%20missing%20essential%20contextual%20information%20at%20the%0Awhole-slide%20level.%20The%20lack%20of%20large-scale%20instruction%20datasets%20and%20the%0Agigapixel%20scale%20of%20whole%20slide%20images%20%28WSIs%29%20pose%20significant%20developmental%0Achallenges.%20In%20this%20paper%2C%20we%20present%20SlideChat%2C%20the%20first%20vision-language%0Aassistant%20capable%20of%20understanding%20gigapixel%20whole-slide%20images%2C%20exhibiting%0Aexcellent%20multimodal%20conversational%20capability%20and%20response%20complex%20instruction%0Aacross%20diverse%20pathology%20scenarios.%20To%20support%20its%20development%2C%20we%20created%0ASlideInstruction%2C%20the%20largest%20instruction-following%20dataset%20for%20WSIs%20consisting%0Aof%204.2K%20WSI%20captions%20and%20176K%20VQA%20pairs%20with%20multiple%20categories.%20Furthermore%2C%0Awe%20propose%20SlideBench%2C%20a%20multimodal%20benchmark%20that%20incorporates%20captioning%20and%0AVQA%20tasks%20to%20assess%20SlideChat%27s%20capabilities%20in%20varied%20clinical%20settings%20such%0Aas%20microscopy%2C%20diagnosis.%20Compared%20to%20both%20general%20and%20specialized%20MLLMs%2C%0ASlideChat%20exhibits%20exceptional%20capabilities%20achieving%20state-of-the-art%0Aperformance%20on%2018%20of%2022%20tasks.%20For%20example%2C%20it%20achieved%20an%20overall%20accuracy%20of%0A81.17%25%20on%20SlideBench-VQA%20%28TCGA%29%2C%20and%2054.15%25%20on%20SlideBench-VQA%20%28BCNB%29.%20We%20will%0Afully%20release%20SlideChat%2C%20SlideInstruction%20and%20SlideBench%20as%20open-source%0Aresources%20to%20facilitate%20research%20and%20development%20in%20computational%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlideChat%253A%2520A%2520Large%2520Vision-Language%2520Assistant%2520for%2520Whole-Slide%2520Pathology%250A%2520%2520Image%2520Understanding%26entry.906535625%3DYing%2520Chen%2520and%2520Guoan%2520Wang%2520and%2520Yuanfeng%2520Ji%2520and%2520Yanjun%2520Li%2520and%2520Jin%2520Ye%2520and%2520Tianbin%2520Li%2520and%2520Bin%2520Zhang%2520and%2520Nana%2520Pei%2520and%2520Rongshan%2520Yu%2520and%2520Yu%2520Qiao%2520and%2520Junjun%2520He%26entry.1292438233%3D%2520%2520Despite%2520the%2520progress%2520made%2520by%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520in%250Acomputational%2520pathology%252C%2520they%2520remain%2520limited%2520by%2520a%2520predominant%2520focus%2520on%250Apatch-level%2520analysis%252C%2520missing%2520essential%2520contextual%2520information%2520at%2520the%250Awhole-slide%2520level.%2520The%2520lack%2520of%2520large-scale%2520instruction%2520datasets%2520and%2520the%250Agigapixel%2520scale%2520of%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520pose%2520significant%2520developmental%250Achallenges.%2520In%2520this%2520paper%252C%2520we%2520present%2520SlideChat%252C%2520the%2520first%2520vision-language%250Aassistant%2520capable%2520of%2520understanding%2520gigapixel%2520whole-slide%2520images%252C%2520exhibiting%250Aexcellent%2520multimodal%2520conversational%2520capability%2520and%2520response%2520complex%2520instruction%250Aacross%2520diverse%2520pathology%2520scenarios.%2520To%2520support%2520its%2520development%252C%2520we%2520created%250ASlideInstruction%252C%2520the%2520largest%2520instruction-following%2520dataset%2520for%2520WSIs%2520consisting%250Aof%25204.2K%2520WSI%2520captions%2520and%2520176K%2520VQA%2520pairs%2520with%2520multiple%2520categories.%2520Furthermore%252C%250Awe%2520propose%2520SlideBench%252C%2520a%2520multimodal%2520benchmark%2520that%2520incorporates%2520captioning%2520and%250AVQA%2520tasks%2520to%2520assess%2520SlideChat%2527s%2520capabilities%2520in%2520varied%2520clinical%2520settings%2520such%250Aas%2520microscopy%252C%2520diagnosis.%2520Compared%2520to%2520both%2520general%2520and%2520specialized%2520MLLMs%252C%250ASlideChat%2520exhibits%2520exceptional%2520capabilities%2520achieving%2520state-of-the-art%250Aperformance%2520on%252018%2520of%252022%2520tasks.%2520For%2520example%252C%2520it%2520achieved%2520an%2520overall%2520accuracy%2520of%250A81.17%2525%2520on%2520SlideBench-VQA%2520%2528TCGA%2529%252C%2520and%252054.15%2525%2520on%2520SlideBench-VQA%2520%2528BCNB%2529.%2520We%2520will%250Afully%2520release%2520SlideChat%252C%2520SlideInstruction%2520and%2520SlideBench%2520as%2520open-source%250Aresources%2520to%2520facilitate%2520research%2520and%2520development%2520in%2520computational%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlideChat%3A%20A%20Large%20Vision-Language%20Assistant%20for%20Whole-Slide%20Pathology%0A%20%20Image%20Understanding&entry.906535625=Ying%20Chen%20and%20Guoan%20Wang%20and%20Yuanfeng%20Ji%20and%20Yanjun%20Li%20and%20Jin%20Ye%20and%20Tianbin%20Li%20and%20Bin%20Zhang%20and%20Nana%20Pei%20and%20Rongshan%20Yu%20and%20Yu%20Qiao%20and%20Junjun%20He&entry.1292438233=%20%20Despite%20the%20progress%20made%20by%20multimodal%20large%20language%20models%20%28MLLMs%29%20in%0Acomputational%20pathology%2C%20they%20remain%20limited%20by%20a%20predominant%20focus%20on%0Apatch-level%20analysis%2C%20missing%20essential%20contextual%20information%20at%20the%0Awhole-slide%20level.%20The%20lack%20of%20large-scale%20instruction%20datasets%20and%20the%0Agigapixel%20scale%20of%20whole%20slide%20images%20%28WSIs%29%20pose%20significant%20developmental%0Achallenges.%20In%20this%20paper%2C%20we%20present%20SlideChat%2C%20the%20first%20vision-language%0Aassistant%20capable%20of%20understanding%20gigapixel%20whole-slide%20images%2C%20exhibiting%0Aexcellent%20multimodal%20conversational%20capability%20and%20response%20complex%20instruction%0Aacross%20diverse%20pathology%20scenarios.%20To%20support%20its%20development%2C%20we%20created%0ASlideInstruction%2C%20the%20largest%20instruction-following%20dataset%20for%20WSIs%20consisting%0Aof%204.2K%20WSI%20captions%20and%20176K%20VQA%20pairs%20with%20multiple%20categories.%20Furthermore%2C%0Awe%20propose%20SlideBench%2C%20a%20multimodal%20benchmark%20that%20incorporates%20captioning%20and%0AVQA%20tasks%20to%20assess%20SlideChat%27s%20capabilities%20in%20varied%20clinical%20settings%20such%0Aas%20microscopy%2C%20diagnosis.%20Compared%20to%20both%20general%20and%20specialized%20MLLMs%2C%0ASlideChat%20exhibits%20exceptional%20capabilities%20achieving%20state-of-the-art%0Aperformance%20on%2018%20of%2022%20tasks.%20For%20example%2C%20it%20achieved%20an%20overall%20accuracy%20of%0A81.17%25%20on%20SlideBench-VQA%20%28TCGA%29%2C%20and%2054.15%25%20on%20SlideBench-VQA%20%28BCNB%29.%20We%20will%0Afully%20release%20SlideChat%2C%20SlideInstruction%20and%20SlideBench%20as%20open-source%0Aresources%20to%20facilitate%20research%20and%20development%20in%20computational%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11761v1&entry.124074799=Read"},
{"title": "Latent Action Pretraining from Videos", "author": "Seonghyeon Ye and Joel Jang and Byeongguk Jeon and Sejune Joo and Jianwei Yang and Baolin Peng and Ajay Mandlekar and Reuben Tan and Yu-Wei Chao and Bill Yuchen Lin and Lars Liden and Kimin Lee and Jianfeng Gao and Luke Zettlemoyer and Dieter Fox and Minjoon Seo", "abstract": "  We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.\n", "link": "http://arxiv.org/abs/2410.11758v1", "date": "2024-10-15", "relevancy": 2.6868, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5691}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Action%20Pretraining%20from%20Videos&body=Title%3A%20Latent%20Action%20Pretraining%20from%20Videos%0AAuthor%3A%20Seonghyeon%20Ye%20and%20Joel%20Jang%20and%20Byeongguk%20Jeon%20and%20Sejune%20Joo%20and%20Jianwei%20Yang%20and%20Baolin%20Peng%20and%20Ajay%20Mandlekar%20and%20Reuben%20Tan%20and%20Yu-Wei%20Chao%20and%20Bill%20Yuchen%20Lin%20and%20Lars%20Liden%20and%20Kimin%20Lee%20and%20Jianfeng%20Gao%20and%20Luke%20Zettlemoyer%20and%20Dieter%20Fox%20and%20Minjoon%20Seo%0AAbstract%3A%20%20%20We%20introduce%20Latent%20Action%20Pretraining%20for%20general%20Action%20models%20%28LAPA%29%2C%20an%0Aunsupervised%20method%20for%20pretraining%20Vision-Language-Action%20%28VLA%29%20models%20without%0Aground-truth%20robot%20action%20labels.%20Existing%20Vision-Language-Action%20models%0Arequire%20action%20labels%20typically%20collected%20by%20human%20teleoperators%20during%0Apretraining%2C%20which%20significantly%20limits%20possible%20data%20sources%20and%20scale.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20to%20learn%20from%20internet-scale%20videos%20that%20do%20not%0Ahave%20robot%20action%20labels.%20We%20first%20train%20an%20action%20quantization%20model%0Aleveraging%20VQ-VAE-based%20objective%20to%20learn%20discrete%20latent%20actions%20between%0Aimage%20frames%2C%20then%20pretrain%20a%20latent%20VLA%20model%20to%20predict%20these%20latent%20actions%0Afrom%20observations%20and%20task%20descriptions%2C%20and%20finally%20finetune%20the%20VLA%20on%0Asmall-scale%20robot%20manipulation%20data%20to%20map%20from%20latent%20to%20robot%20actions.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aexisting%20techniques%20that%20train%20robot%20manipulation%20policies%20from%20large-scale%0Avideos.%20Furthermore%2C%20it%20outperforms%20the%20state-of-the-art%20VLA%20model%20trained%20with%0Arobotic%20action%20labels%20on%20real-world%20manipulation%20tasks%20that%20require%20language%0Aconditioning%2C%20generalization%20to%20unseen%20objects%2C%20and%20semantic%20generalization%20to%0Aunseen%20instructions.%20Training%20only%20on%20human%20manipulation%20videos%20also%20shows%0Apositive%20transfer%2C%20opening%20up%20the%20potential%20for%20leveraging%20web-scale%20data%20for%0Arobotics%20foundation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Action%2520Pretraining%2520from%2520Videos%26entry.906535625%3DSeonghyeon%2520Ye%2520and%2520Joel%2520Jang%2520and%2520Byeongguk%2520Jeon%2520and%2520Sejune%2520Joo%2520and%2520Jianwei%2520Yang%2520and%2520Baolin%2520Peng%2520and%2520Ajay%2520Mandlekar%2520and%2520Reuben%2520Tan%2520and%2520Yu-Wei%2520Chao%2520and%2520Bill%2520Yuchen%2520Lin%2520and%2520Lars%2520Liden%2520and%2520Kimin%2520Lee%2520and%2520Jianfeng%2520Gao%2520and%2520Luke%2520Zettlemoyer%2520and%2520Dieter%2520Fox%2520and%2520Minjoon%2520Seo%26entry.1292438233%3D%2520%2520We%2520introduce%2520Latent%2520Action%2520Pretraining%2520for%2520general%2520Action%2520models%2520%2528LAPA%2529%252C%2520an%250Aunsupervised%2520method%2520for%2520pretraining%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520without%250Aground-truth%2520robot%2520action%2520labels.%2520Existing%2520Vision-Language-Action%2520models%250Arequire%2520action%2520labels%2520typically%2520collected%2520by%2520human%2520teleoperators%2520during%250Apretraining%252C%2520which%2520significantly%2520limits%2520possible%2520data%2520sources%2520and%2520scale.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520method%2520to%2520learn%2520from%2520internet-scale%2520videos%2520that%2520do%2520not%250Ahave%2520robot%2520action%2520labels.%2520We%2520first%2520train%2520an%2520action%2520quantization%2520model%250Aleveraging%2520VQ-VAE-based%2520objective%2520to%2520learn%2520discrete%2520latent%2520actions%2520between%250Aimage%2520frames%252C%2520then%2520pretrain%2520a%2520latent%2520VLA%2520model%2520to%2520predict%2520these%2520latent%2520actions%250Afrom%2520observations%2520and%2520task%2520descriptions%252C%2520and%2520finally%2520finetune%2520the%2520VLA%2520on%250Asmall-scale%2520robot%2520manipulation%2520data%2520to%2520map%2520from%2520latent%2520to%2520robot%2520actions.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Aexisting%2520techniques%2520that%2520train%2520robot%2520manipulation%2520policies%2520from%2520large-scale%250Avideos.%2520Furthermore%252C%2520it%2520outperforms%2520the%2520state-of-the-art%2520VLA%2520model%2520trained%2520with%250Arobotic%2520action%2520labels%2520on%2520real-world%2520manipulation%2520tasks%2520that%2520require%2520language%250Aconditioning%252C%2520generalization%2520to%2520unseen%2520objects%252C%2520and%2520semantic%2520generalization%2520to%250Aunseen%2520instructions.%2520Training%2520only%2520on%2520human%2520manipulation%2520videos%2520also%2520shows%250Apositive%2520transfer%252C%2520opening%2520up%2520the%2520potential%2520for%2520leveraging%2520web-scale%2520data%2520for%250Arobotics%2520foundation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Action%20Pretraining%20from%20Videos&entry.906535625=Seonghyeon%20Ye%20and%20Joel%20Jang%20and%20Byeongguk%20Jeon%20and%20Sejune%20Joo%20and%20Jianwei%20Yang%20and%20Baolin%20Peng%20and%20Ajay%20Mandlekar%20and%20Reuben%20Tan%20and%20Yu-Wei%20Chao%20and%20Bill%20Yuchen%20Lin%20and%20Lars%20Liden%20and%20Kimin%20Lee%20and%20Jianfeng%20Gao%20and%20Luke%20Zettlemoyer%20and%20Dieter%20Fox%20and%20Minjoon%20Seo&entry.1292438233=%20%20We%20introduce%20Latent%20Action%20Pretraining%20for%20general%20Action%20models%20%28LAPA%29%2C%20an%0Aunsupervised%20method%20for%20pretraining%20Vision-Language-Action%20%28VLA%29%20models%20without%0Aground-truth%20robot%20action%20labels.%20Existing%20Vision-Language-Action%20models%0Arequire%20action%20labels%20typically%20collected%20by%20human%20teleoperators%20during%0Apretraining%2C%20which%20significantly%20limits%20possible%20data%20sources%20and%20scale.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20to%20learn%20from%20internet-scale%20videos%20that%20do%20not%0Ahave%20robot%20action%20labels.%20We%20first%20train%20an%20action%20quantization%20model%0Aleveraging%20VQ-VAE-based%20objective%20to%20learn%20discrete%20latent%20actions%20between%0Aimage%20frames%2C%20then%20pretrain%20a%20latent%20VLA%20model%20to%20predict%20these%20latent%20actions%0Afrom%20observations%20and%20task%20descriptions%2C%20and%20finally%20finetune%20the%20VLA%20on%0Asmall-scale%20robot%20manipulation%20data%20to%20map%20from%20latent%20to%20robot%20actions.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aexisting%20techniques%20that%20train%20robot%20manipulation%20policies%20from%20large-scale%0Avideos.%20Furthermore%2C%20it%20outperforms%20the%20state-of-the-art%20VLA%20model%20trained%20with%0Arobotic%20action%20labels%20on%20real-world%20manipulation%20tasks%20that%20require%20language%0Aconditioning%2C%20generalization%20to%20unseen%20objects%2C%20and%20semantic%20generalization%20to%0Aunseen%20instructions.%20Training%20only%20on%20human%20manipulation%20videos%20also%20shows%0Apositive%20transfer%2C%20opening%20up%20the%20potential%20for%20leveraging%20web-scale%20data%20for%0Arobotics%20foundation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11758v1&entry.124074799=Read"},
{"title": "Analyzing (In)Abilities of SAEs via Formal Languages", "author": "Abhinav Menon and Manish Shrivastava and David Krueger and Ekdeep Singh Lubana", "abstract": "  Autoencoders have been used for finding interpretable and disentangled\nfeatures underlying neural network representations in both image and text\ndomains. While the efficacy and pitfalls of such methods are well-studied in\nvision, there is a lack of corresponding results, both qualitative and\nquantitative, for the text domain. We aim to address this gap by training\nsparse autoencoders (SAEs) on a synthetic testbed of formal languages.\nSpecifically, we train SAEs on the hidden representations of models trained on\nformal languages (Dyck-2, Expr, and English PCFG) under a wide variety of\nhyperparameter settings, finding interpretable latents often emerge in the\nfeatures learned by our SAEs. However, similar to vision, we find performance\nturns out to be highly sensitive to inductive biases of the training pipeline.\nMoreover, we show latents correlating to certain features of the input do not\nalways induce a causal impact on model's computation. We thus argue that\ncausality has to become a central target in SAE training: learning of causal\nfeatures should be incentivized from the ground-up. Motivated by this, we\npropose and perform preliminary investigations for an approach that promotes\nlearning of causally relevant features in our formal language setting.\n", "link": "http://arxiv.org/abs/2410.11767v1", "date": "2024-10-15", "relevancy": 2.6595, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20%28In%29Abilities%20of%20SAEs%20via%20Formal%20Languages&body=Title%3A%20Analyzing%20%28In%29Abilities%20of%20SAEs%20via%20Formal%20Languages%0AAuthor%3A%20Abhinav%20Menon%20and%20Manish%20Shrivastava%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana%0AAbstract%3A%20%20%20Autoencoders%20have%20been%20used%20for%20finding%20interpretable%20and%20disentangled%0Afeatures%20underlying%20neural%20network%20representations%20in%20both%20image%20and%20text%0Adomains.%20While%20the%20efficacy%20and%20pitfalls%20of%20such%20methods%20are%20well-studied%20in%0Avision%2C%20there%20is%20a%20lack%20of%20corresponding%20results%2C%20both%20qualitative%20and%0Aquantitative%2C%20for%20the%20text%20domain.%20We%20aim%20to%20address%20this%20gap%20by%20training%0Asparse%20autoencoders%20%28SAEs%29%20on%20a%20synthetic%20testbed%20of%20formal%20languages.%0ASpecifically%2C%20we%20train%20SAEs%20on%20the%20hidden%20representations%20of%20models%20trained%20on%0Aformal%20languages%20%28Dyck-2%2C%20Expr%2C%20and%20English%20PCFG%29%20under%20a%20wide%20variety%20of%0Ahyperparameter%20settings%2C%20finding%20interpretable%20latents%20often%20emerge%20in%20the%0Afeatures%20learned%20by%20our%20SAEs.%20However%2C%20similar%20to%20vision%2C%20we%20find%20performance%0Aturns%20out%20to%20be%20highly%20sensitive%20to%20inductive%20biases%20of%20the%20training%20pipeline.%0AMoreover%2C%20we%20show%20latents%20correlating%20to%20certain%20features%20of%20the%20input%20do%20not%0Aalways%20induce%20a%20causal%20impact%20on%20model%27s%20computation.%20We%20thus%20argue%20that%0Acausality%20has%20to%20become%20a%20central%20target%20in%20SAE%20training%3A%20learning%20of%20causal%0Afeatures%20should%20be%20incentivized%20from%20the%20ground-up.%20Motivated%20by%20this%2C%20we%0Apropose%20and%20perform%20preliminary%20investigations%20for%20an%20approach%20that%20promotes%0Alearning%20of%20causally%20relevant%20features%20in%20our%20formal%20language%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520%2528In%2529Abilities%2520of%2520SAEs%2520via%2520Formal%2520Languages%26entry.906535625%3DAbhinav%2520Menon%2520and%2520Manish%2520Shrivastava%2520and%2520David%2520Krueger%2520and%2520Ekdeep%2520Singh%2520Lubana%26entry.1292438233%3D%2520%2520Autoencoders%2520have%2520been%2520used%2520for%2520finding%2520interpretable%2520and%2520disentangled%250Afeatures%2520underlying%2520neural%2520network%2520representations%2520in%2520both%2520image%2520and%2520text%250Adomains.%2520While%2520the%2520efficacy%2520and%2520pitfalls%2520of%2520such%2520methods%2520are%2520well-studied%2520in%250Avision%252C%2520there%2520is%2520a%2520lack%2520of%2520corresponding%2520results%252C%2520both%2520qualitative%2520and%250Aquantitative%252C%2520for%2520the%2520text%2520domain.%2520We%2520aim%2520to%2520address%2520this%2520gap%2520by%2520training%250Asparse%2520autoencoders%2520%2528SAEs%2529%2520on%2520a%2520synthetic%2520testbed%2520of%2520formal%2520languages.%250ASpecifically%252C%2520we%2520train%2520SAEs%2520on%2520the%2520hidden%2520representations%2520of%2520models%2520trained%2520on%250Aformal%2520languages%2520%2528Dyck-2%252C%2520Expr%252C%2520and%2520English%2520PCFG%2529%2520under%2520a%2520wide%2520variety%2520of%250Ahyperparameter%2520settings%252C%2520finding%2520interpretable%2520latents%2520often%2520emerge%2520in%2520the%250Afeatures%2520learned%2520by%2520our%2520SAEs.%2520However%252C%2520similar%2520to%2520vision%252C%2520we%2520find%2520performance%250Aturns%2520out%2520to%2520be%2520highly%2520sensitive%2520to%2520inductive%2520biases%2520of%2520the%2520training%2520pipeline.%250AMoreover%252C%2520we%2520show%2520latents%2520correlating%2520to%2520certain%2520features%2520of%2520the%2520input%2520do%2520not%250Aalways%2520induce%2520a%2520causal%2520impact%2520on%2520model%2527s%2520computation.%2520We%2520thus%2520argue%2520that%250Acausality%2520has%2520to%2520become%2520a%2520central%2520target%2520in%2520SAE%2520training%253A%2520learning%2520of%2520causal%250Afeatures%2520should%2520be%2520incentivized%2520from%2520the%2520ground-up.%2520Motivated%2520by%2520this%252C%2520we%250Apropose%2520and%2520perform%2520preliminary%2520investigations%2520for%2520an%2520approach%2520that%2520promotes%250Alearning%2520of%2520causally%2520relevant%2520features%2520in%2520our%2520formal%2520language%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20%28In%29Abilities%20of%20SAEs%20via%20Formal%20Languages&entry.906535625=Abhinav%20Menon%20and%20Manish%20Shrivastava%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana&entry.1292438233=%20%20Autoencoders%20have%20been%20used%20for%20finding%20interpretable%20and%20disentangled%0Afeatures%20underlying%20neural%20network%20representations%20in%20both%20image%20and%20text%0Adomains.%20While%20the%20efficacy%20and%20pitfalls%20of%20such%20methods%20are%20well-studied%20in%0Avision%2C%20there%20is%20a%20lack%20of%20corresponding%20results%2C%20both%20qualitative%20and%0Aquantitative%2C%20for%20the%20text%20domain.%20We%20aim%20to%20address%20this%20gap%20by%20training%0Asparse%20autoencoders%20%28SAEs%29%20on%20a%20synthetic%20testbed%20of%20formal%20languages.%0ASpecifically%2C%20we%20train%20SAEs%20on%20the%20hidden%20representations%20of%20models%20trained%20on%0Aformal%20languages%20%28Dyck-2%2C%20Expr%2C%20and%20English%20PCFG%29%20under%20a%20wide%20variety%20of%0Ahyperparameter%20settings%2C%20finding%20interpretable%20latents%20often%20emerge%20in%20the%0Afeatures%20learned%20by%20our%20SAEs.%20However%2C%20similar%20to%20vision%2C%20we%20find%20performance%0Aturns%20out%20to%20be%20highly%20sensitive%20to%20inductive%20biases%20of%20the%20training%20pipeline.%0AMoreover%2C%20we%20show%20latents%20correlating%20to%20certain%20features%20of%20the%20input%20do%20not%0Aalways%20induce%20a%20causal%20impact%20on%20model%27s%20computation.%20We%20thus%20argue%20that%0Acausality%20has%20to%20become%20a%20central%20target%20in%20SAE%20training%3A%20learning%20of%20causal%0Afeatures%20should%20be%20incentivized%20from%20the%20ground-up.%20Motivated%20by%20this%2C%20we%0Apropose%20and%20perform%20preliminary%20investigations%20for%20an%20approach%20that%20promotes%0Alearning%20of%20causally%20relevant%20features%20in%20our%20formal%20language%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11767v1&entry.124074799=Read"},
{"title": "Fractal Calibration for long-tailed object detection", "author": "Konstantinos Panagiotis Alexandridis and Ismail Elezi and Jiankang Deng and Anh Nguyen and Shan Luo", "abstract": "  Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. The code will be released.\n", "link": "http://arxiv.org/abs/2410.11774v1", "date": "2024-10-15", "relevancy": 2.6398, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5736}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fractal%20Calibration%20for%20long-tailed%20object%20detection&body=Title%3A%20Fractal%20Calibration%20for%20long-tailed%20object%20detection%0AAuthor%3A%20Konstantinos%20Panagiotis%20Alexandridis%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%20and%20Anh%20Nguyen%20and%20Shan%20Luo%0AAbstract%3A%20%20%20Real-world%20datasets%20follow%20an%20imbalanced%20distribution%2C%20which%20poses%0Asignificant%20challenges%20in%20rare-category%20object%20detection.%20Recent%20studies%20tackle%0Athis%20problem%20by%20developing%20re-weighting%20and%20re-sampling%20methods%2C%20that%20utilise%0Athe%20class%20frequencies%20of%20the%20dataset.%20However%2C%20these%20techniques%20focus%20solely%20on%0Athe%20frequency%20statistics%20and%20ignore%20the%20distribution%20of%20the%20classes%20in%20image%0Aspace%2C%20missing%20important%20information.%20In%20contrast%20to%20them%2C%20we%20propose%20FRActal%0ACALibration%20%28FRACAL%29%3A%20a%20novel%20post-calibration%20method%20for%20long-tailed%20object%0Adetection.%20FRACAL%20devises%20a%20logit%20adjustment%20method%20that%20utilises%20the%20fractal%0Adimension%20to%20estimate%20how%20uniformly%20classes%20are%20distributed%20in%20image%20space.%0ADuring%20inference%2C%20it%20uses%20the%20fractal%20dimension%20to%20inversely%20downweight%20the%0Aprobabilities%20of%20uniformly%20spaced%20class%20predictions%20achieving%20balance%20in%20two%0Aaxes%3A%20between%20frequent%20and%20rare%20categories%2C%20and%20between%20uniformly%20spaced%20and%0Asparsely%20spaced%20classes.%20FRACAL%20is%20a%20post-processing%20method%20and%20it%20does%20not%0Arequire%20any%20training%2C%20also%20it%20can%20be%20combined%20with%20many%20off-the-shelf%20models%0Asuch%20as%20one-stage%20sigmoid%20detectors%20and%20two-stage%20instance%20segmentation%20models.%0AFRACAL%20boosts%20the%20rare%20class%20performance%20by%20up%20to%208.6%25%20and%20surpasses%20all%0Aprevious%20methods%20on%20LVIS%20dataset%2C%20while%20showing%20good%20generalisation%20to%20other%0Adatasets%20such%20as%20COCO%2C%20V3Det%20and%20OpenImages.%20The%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFractal%2520Calibration%2520for%2520long-tailed%2520object%2520detection%26entry.906535625%3DKonstantinos%2520Panagiotis%2520Alexandridis%2520and%2520Ismail%2520Elezi%2520and%2520Jiankang%2520Deng%2520and%2520Anh%2520Nguyen%2520and%2520Shan%2520Luo%26entry.1292438233%3D%2520%2520Real-world%2520datasets%2520follow%2520an%2520imbalanced%2520distribution%252C%2520which%2520poses%250Asignificant%2520challenges%2520in%2520rare-category%2520object%2520detection.%2520Recent%2520studies%2520tackle%250Athis%2520problem%2520by%2520developing%2520re-weighting%2520and%2520re-sampling%2520methods%252C%2520that%2520utilise%250Athe%2520class%2520frequencies%2520of%2520the%2520dataset.%2520However%252C%2520these%2520techniques%2520focus%2520solely%2520on%250Athe%2520frequency%2520statistics%2520and%2520ignore%2520the%2520distribution%2520of%2520the%2520classes%2520in%2520image%250Aspace%252C%2520missing%2520important%2520information.%2520In%2520contrast%2520to%2520them%252C%2520we%2520propose%2520FRActal%250ACALibration%2520%2528FRACAL%2529%253A%2520a%2520novel%2520post-calibration%2520method%2520for%2520long-tailed%2520object%250Adetection.%2520FRACAL%2520devises%2520a%2520logit%2520adjustment%2520method%2520that%2520utilises%2520the%2520fractal%250Adimension%2520to%2520estimate%2520how%2520uniformly%2520classes%2520are%2520distributed%2520in%2520image%2520space.%250ADuring%2520inference%252C%2520it%2520uses%2520the%2520fractal%2520dimension%2520to%2520inversely%2520downweight%2520the%250Aprobabilities%2520of%2520uniformly%2520spaced%2520class%2520predictions%2520achieving%2520balance%2520in%2520two%250Aaxes%253A%2520between%2520frequent%2520and%2520rare%2520categories%252C%2520and%2520between%2520uniformly%2520spaced%2520and%250Asparsely%2520spaced%2520classes.%2520FRACAL%2520is%2520a%2520post-processing%2520method%2520and%2520it%2520does%2520not%250Arequire%2520any%2520training%252C%2520also%2520it%2520can%2520be%2520combined%2520with%2520many%2520off-the-shelf%2520models%250Asuch%2520as%2520one-stage%2520sigmoid%2520detectors%2520and%2520two-stage%2520instance%2520segmentation%2520models.%250AFRACAL%2520boosts%2520the%2520rare%2520class%2520performance%2520by%2520up%2520to%25208.6%2525%2520and%2520surpasses%2520all%250Aprevious%2520methods%2520on%2520LVIS%2520dataset%252C%2520while%2520showing%2520good%2520generalisation%2520to%2520other%250Adatasets%2520such%2520as%2520COCO%252C%2520V3Det%2520and%2520OpenImages.%2520The%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fractal%20Calibration%20for%20long-tailed%20object%20detection&entry.906535625=Konstantinos%20Panagiotis%20Alexandridis%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%20and%20Anh%20Nguyen%20and%20Shan%20Luo&entry.1292438233=%20%20Real-world%20datasets%20follow%20an%20imbalanced%20distribution%2C%20which%20poses%0Asignificant%20challenges%20in%20rare-category%20object%20detection.%20Recent%20studies%20tackle%0Athis%20problem%20by%20developing%20re-weighting%20and%20re-sampling%20methods%2C%20that%20utilise%0Athe%20class%20frequencies%20of%20the%20dataset.%20However%2C%20these%20techniques%20focus%20solely%20on%0Athe%20frequency%20statistics%20and%20ignore%20the%20distribution%20of%20the%20classes%20in%20image%0Aspace%2C%20missing%20important%20information.%20In%20contrast%20to%20them%2C%20we%20propose%20FRActal%0ACALibration%20%28FRACAL%29%3A%20a%20novel%20post-calibration%20method%20for%20long-tailed%20object%0Adetection.%20FRACAL%20devises%20a%20logit%20adjustment%20method%20that%20utilises%20the%20fractal%0Adimension%20to%20estimate%20how%20uniformly%20classes%20are%20distributed%20in%20image%20space.%0ADuring%20inference%2C%20it%20uses%20the%20fractal%20dimension%20to%20inversely%20downweight%20the%0Aprobabilities%20of%20uniformly%20spaced%20class%20predictions%20achieving%20balance%20in%20two%0Aaxes%3A%20between%20frequent%20and%20rare%20categories%2C%20and%20between%20uniformly%20spaced%20and%0Asparsely%20spaced%20classes.%20FRACAL%20is%20a%20post-processing%20method%20and%20it%20does%20not%0Arequire%20any%20training%2C%20also%20it%20can%20be%20combined%20with%20many%20off-the-shelf%20models%0Asuch%20as%20one-stage%20sigmoid%20detectors%20and%20two-stage%20instance%20segmentation%20models.%0AFRACAL%20boosts%20the%20rare%20class%20performance%20by%20up%20to%208.6%25%20and%20surpasses%20all%0Aprevious%20methods%20on%20LVIS%20dataset%2C%20while%20showing%20good%20generalisation%20to%20other%0Adatasets%20such%20as%20COCO%2C%20V3Det%20and%20OpenImages.%20The%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11774v1&entry.124074799=Read"},
{"title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability", "author": "Tsz Ting Chung and Leyang Cui and Lemao Liu and Xinting Huang and Shuming Shi and Dit-Yan Yeung", "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.\n", "link": "http://arxiv.org/abs/2410.11786v1", "date": "2024-10-15", "relevancy": 2.5685, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5156}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selection-p%3A%20Self-Supervised%20Task-Agnostic%20Prompt%20Compression%20for%0A%20%20Faithfulness%20and%20Transferability&body=Title%3A%20Selection-p%3A%20Self-Supervised%20Task-Agnostic%20Prompt%20Compression%20for%0A%20%20Faithfulness%20and%20Transferability%0AAuthor%3A%20Tsz%20Ting%20Chung%20and%20Leyang%20Cui%20and%20Lemao%20Liu%20and%20Xinting%20Huang%20and%20Shuming%20Shi%20and%20Dit-Yan%20Yeung%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%20a%0Awide%20range%20of%20natural%20language%20processing%20tasks%20when%20leveraging%20in-context%0Alearning.%20To%20mitigate%20the%20additional%20computational%20and%20financial%20costs%0Aassociated%20with%20in-context%20learning%2C%20several%20prompt%20compression%20methods%20have%0Abeen%20proposed%20to%20compress%20the%20in-context%20learning%20prompts.%20Despite%20their%0Asuccess%2C%20these%20methods%20face%20challenges%20with%20transferability%20due%20to%0Amodel-specific%20compression%2C%20or%20rely%20on%20external%20training%20data%2C%20such%20as%20GPT-4.%0AIn%20this%20paper%2C%20we%20investigate%20the%20ability%20of%20LLMs%20to%20develop%20a%20unified%0Acompression%20method%20that%20discretizes%20uninformative%20tokens%2C%20utilizing%20a%0Aself-supervised%20pre-training%20technique.%20By%20introducing%20a%20small%20number%20of%0Aparameters%20during%20the%20continual%20pre-training%2C%20the%20proposed%20Selection-p%20produces%0Aa%20probability%20for%20each%20input%20token%2C%20indicating%20whether%20to%20preserve%20or%20discard%0Ait.%20Experiments%20show%20Selection-p%20achieves%20state-of-the-art%20performance%20across%0Anumerous%20classification%20tasks%2C%20achieving%20compression%20rates%20of%20up%20to%2010%20times%0Awhile%20experiencing%20only%20a%20marginal%200.8%25%20decrease%20in%20performance.%20Moreover%2C%20it%0Aexhibits%20superior%20transferability%20to%20different%20models%20compared%20to%20prior%20work.%0AAdditionally%2C%20we%20further%20analyze%20how%20Selection-p%20helps%20maintain%20performance%20on%0Ain-context%20learning%20with%20long%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelection-p%253A%2520Self-Supervised%2520Task-Agnostic%2520Prompt%2520Compression%2520for%250A%2520%2520Faithfulness%2520and%2520Transferability%26entry.906535625%3DTsz%2520Ting%2520Chung%2520and%2520Leyang%2520Cui%2520and%2520Lemao%2520Liu%2520and%2520Xinting%2520Huang%2520and%2520Shuming%2520Shi%2520and%2520Dit-Yan%2520Yeung%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520a%250Awide%2520range%2520of%2520natural%2520language%2520processing%2520tasks%2520when%2520leveraging%2520in-context%250Alearning.%2520To%2520mitigate%2520the%2520additional%2520computational%2520and%2520financial%2520costs%250Aassociated%2520with%2520in-context%2520learning%252C%2520several%2520prompt%2520compression%2520methods%2520have%250Abeen%2520proposed%2520to%2520compress%2520the%2520in-context%2520learning%2520prompts.%2520Despite%2520their%250Asuccess%252C%2520these%2520methods%2520face%2520challenges%2520with%2520transferability%2520due%2520to%250Amodel-specific%2520compression%252C%2520or%2520rely%2520on%2520external%2520training%2520data%252C%2520such%2520as%2520GPT-4.%250AIn%2520this%2520paper%252C%2520we%2520investigate%2520the%2520ability%2520of%2520LLMs%2520to%2520develop%2520a%2520unified%250Acompression%2520method%2520that%2520discretizes%2520uninformative%2520tokens%252C%2520utilizing%2520a%250Aself-supervised%2520pre-training%2520technique.%2520By%2520introducing%2520a%2520small%2520number%2520of%250Aparameters%2520during%2520the%2520continual%2520pre-training%252C%2520the%2520proposed%2520Selection-p%2520produces%250Aa%2520probability%2520for%2520each%2520input%2520token%252C%2520indicating%2520whether%2520to%2520preserve%2520or%2520discard%250Ait.%2520Experiments%2520show%2520Selection-p%2520achieves%2520state-of-the-art%2520performance%2520across%250Anumerous%2520classification%2520tasks%252C%2520achieving%2520compression%2520rates%2520of%2520up%2520to%252010%2520times%250Awhile%2520experiencing%2520only%2520a%2520marginal%25200.8%2525%2520decrease%2520in%2520performance.%2520Moreover%252C%2520it%250Aexhibits%2520superior%2520transferability%2520to%2520different%2520models%2520compared%2520to%2520prior%2520work.%250AAdditionally%252C%2520we%2520further%2520analyze%2520how%2520Selection-p%2520helps%2520maintain%2520performance%2520on%250Ain-context%2520learning%2520with%2520long%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selection-p%3A%20Self-Supervised%20Task-Agnostic%20Prompt%20Compression%20for%0A%20%20Faithfulness%20and%20Transferability&entry.906535625=Tsz%20Ting%20Chung%20and%20Leyang%20Cui%20and%20Lemao%20Liu%20and%20Xinting%20Huang%20and%20Shuming%20Shi%20and%20Dit-Yan%20Yeung&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%20a%0Awide%20range%20of%20natural%20language%20processing%20tasks%20when%20leveraging%20in-context%0Alearning.%20To%20mitigate%20the%20additional%20computational%20and%20financial%20costs%0Aassociated%20with%20in-context%20learning%2C%20several%20prompt%20compression%20methods%20have%0Abeen%20proposed%20to%20compress%20the%20in-context%20learning%20prompts.%20Despite%20their%0Asuccess%2C%20these%20methods%20face%20challenges%20with%20transferability%20due%20to%0Amodel-specific%20compression%2C%20or%20rely%20on%20external%20training%20data%2C%20such%20as%20GPT-4.%0AIn%20this%20paper%2C%20we%20investigate%20the%20ability%20of%20LLMs%20to%20develop%20a%20unified%0Acompression%20method%20that%20discretizes%20uninformative%20tokens%2C%20utilizing%20a%0Aself-supervised%20pre-training%20technique.%20By%20introducing%20a%20small%20number%20of%0Aparameters%20during%20the%20continual%20pre-training%2C%20the%20proposed%20Selection-p%20produces%0Aa%20probability%20for%20each%20input%20token%2C%20indicating%20whether%20to%20preserve%20or%20discard%0Ait.%20Experiments%20show%20Selection-p%20achieves%20state-of-the-art%20performance%20across%0Anumerous%20classification%20tasks%2C%20achieving%20compression%20rates%20of%20up%20to%2010%20times%0Awhile%20experiencing%20only%20a%20marginal%200.8%25%20decrease%20in%20performance.%20Moreover%2C%20it%0Aexhibits%20superior%20transferability%20to%20different%20models%20compared%20to%20prior%20work.%0AAdditionally%2C%20we%20further%20analyze%20how%20Selection-p%20helps%20maintain%20performance%20on%0Ain-context%20learning%20with%20long%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11786v1&entry.124074799=Read"},
{"title": "Generalized Simplicial Attention Neural Networks", "author": "Claudio Battiloro and Lucia Testa and Lorenzo Giusti and Stefania Sardellitti and Paolo Di Lorenzo and Sergio Barbarossa", "abstract": "  Graph machine learning methods excel at leveraging pairwise relations present\nin the data. However, graphs are unable to fully capture the multi-way\ninteractions inherent in many complex systems. An effective way to incorporate\nthem is to model the data on higher-order combinatorial topological spaces,\nsuch as Simplicial Complexes (SCs) or Cell Complexes. For this reason, we\nintroduce Generalized Simplicial Attention Neural Networks (GSANs), novel\nneural network architectures designed to process data living on simplicial\ncomplexes using masked self-attentional layers. Hinging on topological signal\nprocessing principles, we devise a series of principled self-attention\nmechanisms able to process data associated with simplices of various order,\nsuch as nodes, edges, triangles, and beyond. These schemes learn how to combine\ndata associated with neighbor simplices of consecutive order in a task-oriented\nfashion, leveraging on the simplicial Dirac operator and its Dirac\ndecomposition. We also prove that GSAN satisfies two fundamental properties:\npermutation equivariance and simplicial-awareness. Finally, we illustrate how\nour approach compares favorably with other simplicial and graph models when\napplied to several (inductive and transductive) tasks such as trajectory\nprediction, missing data imputation, graph classification, and simplex\nprediction.\n", "link": "http://arxiv.org/abs/2309.02138v2", "date": "2024-10-15", "relevancy": 2.5152, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5187}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4956}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Simplicial%20Attention%20Neural%20Networks&body=Title%3A%20Generalized%20Simplicial%20Attention%20Neural%20Networks%0AAuthor%3A%20Claudio%20Battiloro%20and%20Lucia%20Testa%20and%20Lorenzo%20Giusti%20and%20Stefania%20Sardellitti%20and%20Paolo%20Di%20Lorenzo%20and%20Sergio%20Barbarossa%0AAbstract%3A%20%20%20Graph%20machine%20learning%20methods%20excel%20at%20leveraging%20pairwise%20relations%20present%0Ain%20the%20data.%20However%2C%20graphs%20are%20unable%20to%20fully%20capture%20the%20multi-way%0Ainteractions%20inherent%20in%20many%20complex%20systems.%20An%20effective%20way%20to%20incorporate%0Athem%20is%20to%20model%20the%20data%20on%20higher-order%20combinatorial%20topological%20spaces%2C%0Asuch%20as%20Simplicial%20Complexes%20%28SCs%29%20or%20Cell%20Complexes.%20For%20this%20reason%2C%20we%0Aintroduce%20Generalized%20Simplicial%20Attention%20Neural%20Networks%20%28GSANs%29%2C%20novel%0Aneural%20network%20architectures%20designed%20to%20process%20data%20living%20on%20simplicial%0Acomplexes%20using%20masked%20self-attentional%20layers.%20Hinging%20on%20topological%20signal%0Aprocessing%20principles%2C%20we%20devise%20a%20series%20of%20principled%20self-attention%0Amechanisms%20able%20to%20process%20data%20associated%20with%20simplices%20of%20various%20order%2C%0Asuch%20as%20nodes%2C%20edges%2C%20triangles%2C%20and%20beyond.%20These%20schemes%20learn%20how%20to%20combine%0Adata%20associated%20with%20neighbor%20simplices%20of%20consecutive%20order%20in%20a%20task-oriented%0Afashion%2C%20leveraging%20on%20the%20simplicial%20Dirac%20operator%20and%20its%20Dirac%0Adecomposition.%20We%20also%20prove%20that%20GSAN%20satisfies%20two%20fundamental%20properties%3A%0Apermutation%20equivariance%20and%20simplicial-awareness.%20Finally%2C%20we%20illustrate%20how%0Aour%20approach%20compares%20favorably%20with%20other%20simplicial%20and%20graph%20models%20when%0Aapplied%20to%20several%20%28inductive%20and%20transductive%29%20tasks%20such%20as%20trajectory%0Aprediction%2C%20missing%20data%20imputation%2C%20graph%20classification%2C%20and%20simplex%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.02138v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Simplicial%2520Attention%2520Neural%2520Networks%26entry.906535625%3DClaudio%2520Battiloro%2520and%2520Lucia%2520Testa%2520and%2520Lorenzo%2520Giusti%2520and%2520Stefania%2520Sardellitti%2520and%2520Paolo%2520Di%2520Lorenzo%2520and%2520Sergio%2520Barbarossa%26entry.1292438233%3D%2520%2520Graph%2520machine%2520learning%2520methods%2520excel%2520at%2520leveraging%2520pairwise%2520relations%2520present%250Ain%2520the%2520data.%2520However%252C%2520graphs%2520are%2520unable%2520to%2520fully%2520capture%2520the%2520multi-way%250Ainteractions%2520inherent%2520in%2520many%2520complex%2520systems.%2520An%2520effective%2520way%2520to%2520incorporate%250Athem%2520is%2520to%2520model%2520the%2520data%2520on%2520higher-order%2520combinatorial%2520topological%2520spaces%252C%250Asuch%2520as%2520Simplicial%2520Complexes%2520%2528SCs%2529%2520or%2520Cell%2520Complexes.%2520For%2520this%2520reason%252C%2520we%250Aintroduce%2520Generalized%2520Simplicial%2520Attention%2520Neural%2520Networks%2520%2528GSANs%2529%252C%2520novel%250Aneural%2520network%2520architectures%2520designed%2520to%2520process%2520data%2520living%2520on%2520simplicial%250Acomplexes%2520using%2520masked%2520self-attentional%2520layers.%2520Hinging%2520on%2520topological%2520signal%250Aprocessing%2520principles%252C%2520we%2520devise%2520a%2520series%2520of%2520principled%2520self-attention%250Amechanisms%2520able%2520to%2520process%2520data%2520associated%2520with%2520simplices%2520of%2520various%2520order%252C%250Asuch%2520as%2520nodes%252C%2520edges%252C%2520triangles%252C%2520and%2520beyond.%2520These%2520schemes%2520learn%2520how%2520to%2520combine%250Adata%2520associated%2520with%2520neighbor%2520simplices%2520of%2520consecutive%2520order%2520in%2520a%2520task-oriented%250Afashion%252C%2520leveraging%2520on%2520the%2520simplicial%2520Dirac%2520operator%2520and%2520its%2520Dirac%250Adecomposition.%2520We%2520also%2520prove%2520that%2520GSAN%2520satisfies%2520two%2520fundamental%2520properties%253A%250Apermutation%2520equivariance%2520and%2520simplicial-awareness.%2520Finally%252C%2520we%2520illustrate%2520how%250Aour%2520approach%2520compares%2520favorably%2520with%2520other%2520simplicial%2520and%2520graph%2520models%2520when%250Aapplied%2520to%2520several%2520%2528inductive%2520and%2520transductive%2529%2520tasks%2520such%2520as%2520trajectory%250Aprediction%252C%2520missing%2520data%2520imputation%252C%2520graph%2520classification%252C%2520and%2520simplex%250Aprediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.02138v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Simplicial%20Attention%20Neural%20Networks&entry.906535625=Claudio%20Battiloro%20and%20Lucia%20Testa%20and%20Lorenzo%20Giusti%20and%20Stefania%20Sardellitti%20and%20Paolo%20Di%20Lorenzo%20and%20Sergio%20Barbarossa&entry.1292438233=%20%20Graph%20machine%20learning%20methods%20excel%20at%20leveraging%20pairwise%20relations%20present%0Ain%20the%20data.%20However%2C%20graphs%20are%20unable%20to%20fully%20capture%20the%20multi-way%0Ainteractions%20inherent%20in%20many%20complex%20systems.%20An%20effective%20way%20to%20incorporate%0Athem%20is%20to%20model%20the%20data%20on%20higher-order%20combinatorial%20topological%20spaces%2C%0Asuch%20as%20Simplicial%20Complexes%20%28SCs%29%20or%20Cell%20Complexes.%20For%20this%20reason%2C%20we%0Aintroduce%20Generalized%20Simplicial%20Attention%20Neural%20Networks%20%28GSANs%29%2C%20novel%0Aneural%20network%20architectures%20designed%20to%20process%20data%20living%20on%20simplicial%0Acomplexes%20using%20masked%20self-attentional%20layers.%20Hinging%20on%20topological%20signal%0Aprocessing%20principles%2C%20we%20devise%20a%20series%20of%20principled%20self-attention%0Amechanisms%20able%20to%20process%20data%20associated%20with%20simplices%20of%20various%20order%2C%0Asuch%20as%20nodes%2C%20edges%2C%20triangles%2C%20and%20beyond.%20These%20schemes%20learn%20how%20to%20combine%0Adata%20associated%20with%20neighbor%20simplices%20of%20consecutive%20order%20in%20a%20task-oriented%0Afashion%2C%20leveraging%20on%20the%20simplicial%20Dirac%20operator%20and%20its%20Dirac%0Adecomposition.%20We%20also%20prove%20that%20GSAN%20satisfies%20two%20fundamental%20properties%3A%0Apermutation%20equivariance%20and%20simplicial-awareness.%20Finally%2C%20we%20illustrate%20how%0Aour%20approach%20compares%20favorably%20with%20other%20simplicial%20and%20graph%20models%20when%0Aapplied%20to%20several%20%28inductive%20and%20transductive%29%20tasks%20such%20as%20trajectory%0Aprediction%2C%20missing%20data%20imputation%2C%20graph%20classification%2C%20and%20simplex%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.02138v2&entry.124074799=Read"},
{"title": "Simultaneous Diffusion Sampling for Conditional LiDAR Generation", "author": "Ryan Faulkner and Luke Haub and Simon Ratcliffe and Anh-Dzung Doan and Ian Reid and Tat-Jun Chin", "abstract": "  By enabling capturing of 3D point clouds that reflect the geometry of the\nimmediate environment, LiDAR has emerged as a primary sensor for autonomous\nsystems. If a LiDAR scan is too sparse, occluded by obstacles, or too small in\nrange, enhancing the point cloud scan by while respecting the geometry of the\nscene is useful for downstream tasks. Motivated by the explosive growth of\ninterest in generative methods in vision, conditional LiDAR generation is\nstarting to take off. This paper proposes a novel simultaneous diffusion\nsampling methodology to generate point clouds conditioned on the 3D structure\nof the scene as seen from multiple views. The key idea is to impose multi-view\ngeometric constraints on the generation process, exploiting mutual information\nfor enhanced results. Our method begins by recasting the input scan to multiple\nnew viewpoints around the scan, thus creating multiple synthetic LiDAR scans.\nThen, the synthetic and input LiDAR scans simultaneously undergo conditional\ngeneration according to our methodology. Results show that our method can\nproduce accurate and geometrically consistent enhancements to point cloud\nscans, allowing it to outperform existing methods by a large margin in a\nvariety of benchmarks.\n", "link": "http://arxiv.org/abs/2410.11628v1", "date": "2024-10-15", "relevancy": 2.5111, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6359}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6359}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Diffusion%20Sampling%20for%20Conditional%20LiDAR%20Generation&body=Title%3A%20Simultaneous%20Diffusion%20Sampling%20for%20Conditional%20LiDAR%20Generation%0AAuthor%3A%20Ryan%20Faulkner%20and%20Luke%20Haub%20and%20Simon%20Ratcliffe%20and%20Anh-Dzung%20Doan%20and%20Ian%20Reid%20and%20Tat-Jun%20Chin%0AAbstract%3A%20%20%20By%20enabling%20capturing%20of%203D%20point%20clouds%20that%20reflect%20the%20geometry%20of%20the%0Aimmediate%20environment%2C%20LiDAR%20has%20emerged%20as%20a%20primary%20sensor%20for%20autonomous%0Asystems.%20If%20a%20LiDAR%20scan%20is%20too%20sparse%2C%20occluded%20by%20obstacles%2C%20or%20too%20small%20in%0Arange%2C%20enhancing%20the%20point%20cloud%20scan%20by%20while%20respecting%20the%20geometry%20of%20the%0Ascene%20is%20useful%20for%20downstream%20tasks.%20Motivated%20by%20the%20explosive%20growth%20of%0Ainterest%20in%20generative%20methods%20in%20vision%2C%20conditional%20LiDAR%20generation%20is%0Astarting%20to%20take%20off.%20This%20paper%20proposes%20a%20novel%20simultaneous%20diffusion%0Asampling%20methodology%20to%20generate%20point%20clouds%20conditioned%20on%20the%203D%20structure%0Aof%20the%20scene%20as%20seen%20from%20multiple%20views.%20The%20key%20idea%20is%20to%20impose%20multi-view%0Ageometric%20constraints%20on%20the%20generation%20process%2C%20exploiting%20mutual%20information%0Afor%20enhanced%20results.%20Our%20method%20begins%20by%20recasting%20the%20input%20scan%20to%20multiple%0Anew%20viewpoints%20around%20the%20scan%2C%20thus%20creating%20multiple%20synthetic%20LiDAR%20scans.%0AThen%2C%20the%20synthetic%20and%20input%20LiDAR%20scans%20simultaneously%20undergo%20conditional%0Ageneration%20according%20to%20our%20methodology.%20Results%20show%20that%20our%20method%20can%0Aproduce%20accurate%20and%20geometrically%20consistent%20enhancements%20to%20point%20cloud%0Ascans%2C%20allowing%20it%20to%20outperform%20existing%20methods%20by%20a%20large%20margin%20in%20a%0Avariety%20of%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Diffusion%2520Sampling%2520for%2520Conditional%2520LiDAR%2520Generation%26entry.906535625%3DRyan%2520Faulkner%2520and%2520Luke%2520Haub%2520and%2520Simon%2520Ratcliffe%2520and%2520Anh-Dzung%2520Doan%2520and%2520Ian%2520Reid%2520and%2520Tat-Jun%2520Chin%26entry.1292438233%3D%2520%2520By%2520enabling%2520capturing%2520of%25203D%2520point%2520clouds%2520that%2520reflect%2520the%2520geometry%2520of%2520the%250Aimmediate%2520environment%252C%2520LiDAR%2520has%2520emerged%2520as%2520a%2520primary%2520sensor%2520for%2520autonomous%250Asystems.%2520If%2520a%2520LiDAR%2520scan%2520is%2520too%2520sparse%252C%2520occluded%2520by%2520obstacles%252C%2520or%2520too%2520small%2520in%250Arange%252C%2520enhancing%2520the%2520point%2520cloud%2520scan%2520by%2520while%2520respecting%2520the%2520geometry%2520of%2520the%250Ascene%2520is%2520useful%2520for%2520downstream%2520tasks.%2520Motivated%2520by%2520the%2520explosive%2520growth%2520of%250Ainterest%2520in%2520generative%2520methods%2520in%2520vision%252C%2520conditional%2520LiDAR%2520generation%2520is%250Astarting%2520to%2520take%2520off.%2520This%2520paper%2520proposes%2520a%2520novel%2520simultaneous%2520diffusion%250Asampling%2520methodology%2520to%2520generate%2520point%2520clouds%2520conditioned%2520on%2520the%25203D%2520structure%250Aof%2520the%2520scene%2520as%2520seen%2520from%2520multiple%2520views.%2520The%2520key%2520idea%2520is%2520to%2520impose%2520multi-view%250Ageometric%2520constraints%2520on%2520the%2520generation%2520process%252C%2520exploiting%2520mutual%2520information%250Afor%2520enhanced%2520results.%2520Our%2520method%2520begins%2520by%2520recasting%2520the%2520input%2520scan%2520to%2520multiple%250Anew%2520viewpoints%2520around%2520the%2520scan%252C%2520thus%2520creating%2520multiple%2520synthetic%2520LiDAR%2520scans.%250AThen%252C%2520the%2520synthetic%2520and%2520input%2520LiDAR%2520scans%2520simultaneously%2520undergo%2520conditional%250Ageneration%2520according%2520to%2520our%2520methodology.%2520Results%2520show%2520that%2520our%2520method%2520can%250Aproduce%2520accurate%2520and%2520geometrically%2520consistent%2520enhancements%2520to%2520point%2520cloud%250Ascans%252C%2520allowing%2520it%2520to%2520outperform%2520existing%2520methods%2520by%2520a%2520large%2520margin%2520in%2520a%250Avariety%2520of%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Diffusion%20Sampling%20for%20Conditional%20LiDAR%20Generation&entry.906535625=Ryan%20Faulkner%20and%20Luke%20Haub%20and%20Simon%20Ratcliffe%20and%20Anh-Dzung%20Doan%20and%20Ian%20Reid%20and%20Tat-Jun%20Chin&entry.1292438233=%20%20By%20enabling%20capturing%20of%203D%20point%20clouds%20that%20reflect%20the%20geometry%20of%20the%0Aimmediate%20environment%2C%20LiDAR%20has%20emerged%20as%20a%20primary%20sensor%20for%20autonomous%0Asystems.%20If%20a%20LiDAR%20scan%20is%20too%20sparse%2C%20occluded%20by%20obstacles%2C%20or%20too%20small%20in%0Arange%2C%20enhancing%20the%20point%20cloud%20scan%20by%20while%20respecting%20the%20geometry%20of%20the%0Ascene%20is%20useful%20for%20downstream%20tasks.%20Motivated%20by%20the%20explosive%20growth%20of%0Ainterest%20in%20generative%20methods%20in%20vision%2C%20conditional%20LiDAR%20generation%20is%0Astarting%20to%20take%20off.%20This%20paper%20proposes%20a%20novel%20simultaneous%20diffusion%0Asampling%20methodology%20to%20generate%20point%20clouds%20conditioned%20on%20the%203D%20structure%0Aof%20the%20scene%20as%20seen%20from%20multiple%20views.%20The%20key%20idea%20is%20to%20impose%20multi-view%0Ageometric%20constraints%20on%20the%20generation%20process%2C%20exploiting%20mutual%20information%0Afor%20enhanced%20results.%20Our%20method%20begins%20by%20recasting%20the%20input%20scan%20to%20multiple%0Anew%20viewpoints%20around%20the%20scan%2C%20thus%20creating%20multiple%20synthetic%20LiDAR%20scans.%0AThen%2C%20the%20synthetic%20and%20input%20LiDAR%20scans%20simultaneously%20undergo%20conditional%0Ageneration%20according%20to%20our%20methodology.%20Results%20show%20that%20our%20method%20can%0Aproduce%20accurate%20and%20geometrically%20consistent%20enhancements%20to%20point%20cloud%0Ascans%2C%20allowing%20it%20to%20outperform%20existing%20methods%20by%20a%20large%20margin%20in%20a%0Avariety%20of%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11628v1&entry.124074799=Read"},
{"title": "Nonlinear Gaussian process tomography with imposed non-negativity\n  constraints on physical quantities for plasma diagnostics", "author": "Kenji Ueda and Masaki Nishiura", "abstract": "  We propose a novel tomographic method, nonlinear Gaussian process tomography\n(nonlinear GPT) that employs the Laplace approximation to ensure the\nnon-negative physical quantity, such as the emissivity of plasma optical\ndiagnostics. This new method implements a logarithmic Gaussian process (log-GP)\nto model plasma distribution more naturally, thereby expanding the limitations\nof standard GPT, which are restricted to linear problems and may yield\nnon-physical negative values. The effectiveness of the proposed log-GP\ntomography is demonstrated through a case study using the Ring Trap 1 (RT-1)\ndevice, where log-GPT outperforms existing methods, standard GPT, and the\nMinimum Fisher Information (MFI) methods in terms of reconstruction accuracy.\nThe result highlights the effectiveness of nonlinear GPT for imposing physical\nconstraints in applications to an inverse problem.\n", "link": "http://arxiv.org/abs/2410.11454v1", "date": "2024-10-15", "relevancy": 2.5064, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5235}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4954}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20Gaussian%20process%20tomography%20with%20imposed%20non-negativity%0A%20%20constraints%20on%20physical%20quantities%20for%20plasma%20diagnostics&body=Title%3A%20Nonlinear%20Gaussian%20process%20tomography%20with%20imposed%20non-negativity%0A%20%20constraints%20on%20physical%20quantities%20for%20plasma%20diagnostics%0AAuthor%3A%20Kenji%20Ueda%20and%20Masaki%20Nishiura%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20tomographic%20method%2C%20nonlinear%20Gaussian%20process%20tomography%0A%28nonlinear%20GPT%29%20that%20employs%20the%20Laplace%20approximation%20to%20ensure%20the%0Anon-negative%20physical%20quantity%2C%20such%20as%20the%20emissivity%20of%20plasma%20optical%0Adiagnostics.%20This%20new%20method%20implements%20a%20logarithmic%20Gaussian%20process%20%28log-GP%29%0Ato%20model%20plasma%20distribution%20more%20naturally%2C%20thereby%20expanding%20the%20limitations%0Aof%20standard%20GPT%2C%20which%20are%20restricted%20to%20linear%20problems%20and%20may%20yield%0Anon-physical%20negative%20values.%20The%20effectiveness%20of%20the%20proposed%20log-GP%0Atomography%20is%20demonstrated%20through%20a%20case%20study%20using%20the%20Ring%20Trap%201%20%28RT-1%29%0Adevice%2C%20where%20log-GPT%20outperforms%20existing%20methods%2C%20standard%20GPT%2C%20and%20the%0AMinimum%20Fisher%20Information%20%28MFI%29%20methods%20in%20terms%20of%20reconstruction%20accuracy.%0AThe%20result%20highlights%20the%20effectiveness%20of%20nonlinear%20GPT%20for%20imposing%20physical%0Aconstraints%20in%20applications%20to%20an%20inverse%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520Gaussian%2520process%2520tomography%2520with%2520imposed%2520non-negativity%250A%2520%2520constraints%2520on%2520physical%2520quantities%2520for%2520plasma%2520diagnostics%26entry.906535625%3DKenji%2520Ueda%2520and%2520Masaki%2520Nishiura%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520tomographic%2520method%252C%2520nonlinear%2520Gaussian%2520process%2520tomography%250A%2528nonlinear%2520GPT%2529%2520that%2520employs%2520the%2520Laplace%2520approximation%2520to%2520ensure%2520the%250Anon-negative%2520physical%2520quantity%252C%2520such%2520as%2520the%2520emissivity%2520of%2520plasma%2520optical%250Adiagnostics.%2520This%2520new%2520method%2520implements%2520a%2520logarithmic%2520Gaussian%2520process%2520%2528log-GP%2529%250Ato%2520model%2520plasma%2520distribution%2520more%2520naturally%252C%2520thereby%2520expanding%2520the%2520limitations%250Aof%2520standard%2520GPT%252C%2520which%2520are%2520restricted%2520to%2520linear%2520problems%2520and%2520may%2520yield%250Anon-physical%2520negative%2520values.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520log-GP%250Atomography%2520is%2520demonstrated%2520through%2520a%2520case%2520study%2520using%2520the%2520Ring%2520Trap%25201%2520%2528RT-1%2529%250Adevice%252C%2520where%2520log-GPT%2520outperforms%2520existing%2520methods%252C%2520standard%2520GPT%252C%2520and%2520the%250AMinimum%2520Fisher%2520Information%2520%2528MFI%2529%2520methods%2520in%2520terms%2520of%2520reconstruction%2520accuracy.%250AThe%2520result%2520highlights%2520the%2520effectiveness%2520of%2520nonlinear%2520GPT%2520for%2520imposing%2520physical%250Aconstraints%2520in%2520applications%2520to%2520an%2520inverse%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20Gaussian%20process%20tomography%20with%20imposed%20non-negativity%0A%20%20constraints%20on%20physical%20quantities%20for%20plasma%20diagnostics&entry.906535625=Kenji%20Ueda%20and%20Masaki%20Nishiura&entry.1292438233=%20%20We%20propose%20a%20novel%20tomographic%20method%2C%20nonlinear%20Gaussian%20process%20tomography%0A%28nonlinear%20GPT%29%20that%20employs%20the%20Laplace%20approximation%20to%20ensure%20the%0Anon-negative%20physical%20quantity%2C%20such%20as%20the%20emissivity%20of%20plasma%20optical%0Adiagnostics.%20This%20new%20method%20implements%20a%20logarithmic%20Gaussian%20process%20%28log-GP%29%0Ato%20model%20plasma%20distribution%20more%20naturally%2C%20thereby%20expanding%20the%20limitations%0Aof%20standard%20GPT%2C%20which%20are%20restricted%20to%20linear%20problems%20and%20may%20yield%0Anon-physical%20negative%20values.%20The%20effectiveness%20of%20the%20proposed%20log-GP%0Atomography%20is%20demonstrated%20through%20a%20case%20study%20using%20the%20Ring%20Trap%201%20%28RT-1%29%0Adevice%2C%20where%20log-GPT%20outperforms%20existing%20methods%2C%20standard%20GPT%2C%20and%20the%0AMinimum%20Fisher%20Information%20%28MFI%29%20methods%20in%20terms%20of%20reconstruction%20accuracy.%0AThe%20result%20highlights%20the%20effectiveness%20of%20nonlinear%20GPT%20for%20imposing%20physical%0Aconstraints%20in%20applications%20to%20an%20inverse%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11454v1&entry.124074799=Read"},
{"title": "ECGN: A Cluster-Aware Approach to Graph Neural Networks for Imbalanced\n  Classification", "author": "Bishal Thapaliya and Anh Nguyen and Yao Lu and Tian Xie and Igor Grudetskyi and Fudong Lin and Antonios Valkanas and Jingyu Liu and Deepayan Chakraborty and Bilel Fehri", "abstract": "  Classifying nodes in a graph is a common problem. The ideal classifier must\nadapt to any imbalances in the class distribution. It must also use information\nin the clustering structure of real-world graphs. Existing Graph Neural\nNetworks (GNNs) have not addressed both problems together. We propose the\nEnhanced Cluster-aware Graph Network (ECGN), a novel method that addresses\nthese issues by integrating cluster-specific training with synthetic node\ngeneration. Unlike traditional GNNs that apply the same node update process for\nall nodes, ECGN learns different aggregations for different clusters. We also\nuse the clusters to generate new minority-class nodes in a way that helps\nclarify the inter-class decision boundary. By combining cluster-aware\nembeddings with a global integration step, ECGN enhances the quality of the\nresulting node embeddings. Our method works with any underlying GNN and any\ncluster generation technique. Experimental results show that ECGN consistently\noutperforms its closest competitors by up to 11% on some widely studied\nbenchmark datasets.\n", "link": "http://arxiv.org/abs/2410.11765v1", "date": "2024-10-15", "relevancy": 2.4965, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5455}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4867}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECGN%3A%20A%20Cluster-Aware%20Approach%20to%20Graph%20Neural%20Networks%20for%20Imbalanced%0A%20%20Classification&body=Title%3A%20ECGN%3A%20A%20Cluster-Aware%20Approach%20to%20Graph%20Neural%20Networks%20for%20Imbalanced%0A%20%20Classification%0AAuthor%3A%20Bishal%20Thapaliya%20and%20Anh%20Nguyen%20and%20Yao%20Lu%20and%20Tian%20Xie%20and%20Igor%20Grudetskyi%20and%20Fudong%20Lin%20and%20Antonios%20Valkanas%20and%20Jingyu%20Liu%20and%20Deepayan%20Chakraborty%20and%20Bilel%20Fehri%0AAbstract%3A%20%20%20Classifying%20nodes%20in%20a%20graph%20is%20a%20common%20problem.%20The%20ideal%20classifier%20must%0Aadapt%20to%20any%20imbalances%20in%20the%20class%20distribution.%20It%20must%20also%20use%20information%0Ain%20the%20clustering%20structure%20of%20real-world%20graphs.%20Existing%20Graph%20Neural%0ANetworks%20%28GNNs%29%20have%20not%20addressed%20both%20problems%20together.%20We%20propose%20the%0AEnhanced%20Cluster-aware%20Graph%20Network%20%28ECGN%29%2C%20a%20novel%20method%20that%20addresses%0Athese%20issues%20by%20integrating%20cluster-specific%20training%20with%20synthetic%20node%0Ageneration.%20Unlike%20traditional%20GNNs%20that%20apply%20the%20same%20node%20update%20process%20for%0Aall%20nodes%2C%20ECGN%20learns%20different%20aggregations%20for%20different%20clusters.%20We%20also%0Ause%20the%20clusters%20to%20generate%20new%20minority-class%20nodes%20in%20a%20way%20that%20helps%0Aclarify%20the%20inter-class%20decision%20boundary.%20By%20combining%20cluster-aware%0Aembeddings%20with%20a%20global%20integration%20step%2C%20ECGN%20enhances%20the%20quality%20of%20the%0Aresulting%20node%20embeddings.%20Our%20method%20works%20with%20any%20underlying%20GNN%20and%20any%0Acluster%20generation%20technique.%20Experimental%20results%20show%20that%20ECGN%20consistently%0Aoutperforms%20its%20closest%20competitors%20by%20up%20to%2011%25%20on%20some%20widely%20studied%0Abenchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECGN%253A%2520A%2520Cluster-Aware%2520Approach%2520to%2520Graph%2520Neural%2520Networks%2520for%2520Imbalanced%250A%2520%2520Classification%26entry.906535625%3DBishal%2520Thapaliya%2520and%2520Anh%2520Nguyen%2520and%2520Yao%2520Lu%2520and%2520Tian%2520Xie%2520and%2520Igor%2520Grudetskyi%2520and%2520Fudong%2520Lin%2520and%2520Antonios%2520Valkanas%2520and%2520Jingyu%2520Liu%2520and%2520Deepayan%2520Chakraborty%2520and%2520Bilel%2520Fehri%26entry.1292438233%3D%2520%2520Classifying%2520nodes%2520in%2520a%2520graph%2520is%2520a%2520common%2520problem.%2520The%2520ideal%2520classifier%2520must%250Aadapt%2520to%2520any%2520imbalances%2520in%2520the%2520class%2520distribution.%2520It%2520must%2520also%2520use%2520information%250Ain%2520the%2520clustering%2520structure%2520of%2520real-world%2520graphs.%2520Existing%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520have%2520not%2520addressed%2520both%2520problems%2520together.%2520We%2520propose%2520the%250AEnhanced%2520Cluster-aware%2520Graph%2520Network%2520%2528ECGN%2529%252C%2520a%2520novel%2520method%2520that%2520addresses%250Athese%2520issues%2520by%2520integrating%2520cluster-specific%2520training%2520with%2520synthetic%2520node%250Ageneration.%2520Unlike%2520traditional%2520GNNs%2520that%2520apply%2520the%2520same%2520node%2520update%2520process%2520for%250Aall%2520nodes%252C%2520ECGN%2520learns%2520different%2520aggregations%2520for%2520different%2520clusters.%2520We%2520also%250Ause%2520the%2520clusters%2520to%2520generate%2520new%2520minority-class%2520nodes%2520in%2520a%2520way%2520that%2520helps%250Aclarify%2520the%2520inter-class%2520decision%2520boundary.%2520By%2520combining%2520cluster-aware%250Aembeddings%2520with%2520a%2520global%2520integration%2520step%252C%2520ECGN%2520enhances%2520the%2520quality%2520of%2520the%250Aresulting%2520node%2520embeddings.%2520Our%2520method%2520works%2520with%2520any%2520underlying%2520GNN%2520and%2520any%250Acluster%2520generation%2520technique.%2520Experimental%2520results%2520show%2520that%2520ECGN%2520consistently%250Aoutperforms%2520its%2520closest%2520competitors%2520by%2520up%2520to%252011%2525%2520on%2520some%2520widely%2520studied%250Abenchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECGN%3A%20A%20Cluster-Aware%20Approach%20to%20Graph%20Neural%20Networks%20for%20Imbalanced%0A%20%20Classification&entry.906535625=Bishal%20Thapaliya%20and%20Anh%20Nguyen%20and%20Yao%20Lu%20and%20Tian%20Xie%20and%20Igor%20Grudetskyi%20and%20Fudong%20Lin%20and%20Antonios%20Valkanas%20and%20Jingyu%20Liu%20and%20Deepayan%20Chakraborty%20and%20Bilel%20Fehri&entry.1292438233=%20%20Classifying%20nodes%20in%20a%20graph%20is%20a%20common%20problem.%20The%20ideal%20classifier%20must%0Aadapt%20to%20any%20imbalances%20in%20the%20class%20distribution.%20It%20must%20also%20use%20information%0Ain%20the%20clustering%20structure%20of%20real-world%20graphs.%20Existing%20Graph%20Neural%0ANetworks%20%28GNNs%29%20have%20not%20addressed%20both%20problems%20together.%20We%20propose%20the%0AEnhanced%20Cluster-aware%20Graph%20Network%20%28ECGN%29%2C%20a%20novel%20method%20that%20addresses%0Athese%20issues%20by%20integrating%20cluster-specific%20training%20with%20synthetic%20node%0Ageneration.%20Unlike%20traditional%20GNNs%20that%20apply%20the%20same%20node%20update%20process%20for%0Aall%20nodes%2C%20ECGN%20learns%20different%20aggregations%20for%20different%20clusters.%20We%20also%0Ause%20the%20clusters%20to%20generate%20new%20minority-class%20nodes%20in%20a%20way%20that%20helps%0Aclarify%20the%20inter-class%20decision%20boundary.%20By%20combining%20cluster-aware%0Aembeddings%20with%20a%20global%20integration%20step%2C%20ECGN%20enhances%20the%20quality%20of%20the%0Aresulting%20node%20embeddings.%20Our%20method%20works%20with%20any%20underlying%20GNN%20and%20any%0Acluster%20generation%20technique.%20Experimental%20results%20show%20that%20ECGN%20consistently%0Aoutperforms%20its%20closest%20competitors%20by%20up%20to%2011%25%20on%20some%20widely%20studied%0Abenchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11765v1&entry.124074799=Read"},
{"title": "MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT\n  Images", "author": "Yanwu Xu and Li Sun and Wei Peng and Shuyue Jia and Katelyn Morrison and Adam Perer and Afrooz Zandifar and Shyam Visweswaran and Motahhare Eslami and Kayhan Batmanghelich", "abstract": "  This paper introduces an innovative methodology for producing high-quality 3D\nlung CT images guided by textual information. While diffusion-based generative\nmodels are increasingly used in medical imaging, current state-of-the-art\napproaches are limited to low-resolution outputs and underutilize radiology\nreports' abundant information. The radiology reports can enhance the generation\nprocess by providing additional guidance and offering fine-grained control over\nthe synthesis of images. Nevertheless, expanding text-guided generation to\nhigh-resolution 3D images poses significant memory and anatomical\ndetail-preserving challenges. Addressing the memory issue, we introduce a\nhierarchical scheme that uses a modified UNet architecture. We start by\nsynthesizing low-resolution images conditioned on the text, serving as a\nfoundation for subsequent generators for complete volumetric data. To ensure\nthe anatomical plausibility of the generated samples, we provide further\nguidance by generating vascular, airway, and lobular segmentation masks in\nconjunction with the CT images. The model demonstrates the capability to use\ntextual input and segmentation tasks to generate synthesized images. The\nresults of comparative assessments indicate that our approach exhibits superior\nperformance compared to the most advanced models based on GAN and diffusion\ntechniques, especially in accurately retaining crucial anatomical features such\nas fissure lines, airways, and vascular structures. This innovation introduces\nnovel possibilities. This study focuses on two main objectives: (1) the\ndevelopment of a method for creating images based on textual prompts and\nanatomical components, and (2) the capability to generate new images\nconditioning on anatomical elements. The advancements in image generation can\nbe applied to enhance numerous downstream tasks.\n", "link": "http://arxiv.org/abs/2310.03559v6", "date": "2024-10-15", "relevancy": 2.4937, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.633}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.633}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedSyn%3A%20Text-guided%20Anatomy-aware%20Synthesis%20of%20High-Fidelity%203D%20CT%0A%20%20Images&body=Title%3A%20MedSyn%3A%20Text-guided%20Anatomy-aware%20Synthesis%20of%20High-Fidelity%203D%20CT%0A%20%20Images%0AAuthor%3A%20Yanwu%20Xu%20and%20Li%20Sun%20and%20Wei%20Peng%20and%20Shuyue%20Jia%20and%20Katelyn%20Morrison%20and%20Adam%20Perer%20and%20Afrooz%20Zandifar%20and%20Shyam%20Visweswaran%20and%20Motahhare%20Eslami%20and%20Kayhan%20Batmanghelich%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20innovative%20methodology%20for%20producing%20high-quality%203D%0Alung%20CT%20images%20guided%20by%20textual%20information.%20While%20diffusion-based%20generative%0Amodels%20are%20increasingly%20used%20in%20medical%20imaging%2C%20current%20state-of-the-art%0Aapproaches%20are%20limited%20to%20low-resolution%20outputs%20and%20underutilize%20radiology%0Areports%27%20abundant%20information.%20The%20radiology%20reports%20can%20enhance%20the%20generation%0Aprocess%20by%20providing%20additional%20guidance%20and%20offering%20fine-grained%20control%20over%0Athe%20synthesis%20of%20images.%20Nevertheless%2C%20expanding%20text-guided%20generation%20to%0Ahigh-resolution%203D%20images%20poses%20significant%20memory%20and%20anatomical%0Adetail-preserving%20challenges.%20Addressing%20the%20memory%20issue%2C%20we%20introduce%20a%0Ahierarchical%20scheme%20that%20uses%20a%20modified%20UNet%20architecture.%20We%20start%20by%0Asynthesizing%20low-resolution%20images%20conditioned%20on%20the%20text%2C%20serving%20as%20a%0Afoundation%20for%20subsequent%20generators%20for%20complete%20volumetric%20data.%20To%20ensure%0Athe%20anatomical%20plausibility%20of%20the%20generated%20samples%2C%20we%20provide%20further%0Aguidance%20by%20generating%20vascular%2C%20airway%2C%20and%20lobular%20segmentation%20masks%20in%0Aconjunction%20with%20the%20CT%20images.%20The%20model%20demonstrates%20the%20capability%20to%20use%0Atextual%20input%20and%20segmentation%20tasks%20to%20generate%20synthesized%20images.%20The%0Aresults%20of%20comparative%20assessments%20indicate%20that%20our%20approach%20exhibits%20superior%0Aperformance%20compared%20to%20the%20most%20advanced%20models%20based%20on%20GAN%20and%20diffusion%0Atechniques%2C%20especially%20in%20accurately%20retaining%20crucial%20anatomical%20features%20such%0Aas%20fissure%20lines%2C%20airways%2C%20and%20vascular%20structures.%20This%20innovation%20introduces%0Anovel%20possibilities.%20This%20study%20focuses%20on%20two%20main%20objectives%3A%20%281%29%20the%0Adevelopment%20of%20a%20method%20for%20creating%20images%20based%20on%20textual%20prompts%20and%0Aanatomical%20components%2C%20and%20%282%29%20the%20capability%20to%20generate%20new%20images%0Aconditioning%20on%20anatomical%20elements.%20The%20advancements%20in%20image%20generation%20can%0Abe%20applied%20to%20enhance%20numerous%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03559v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedSyn%253A%2520Text-guided%2520Anatomy-aware%2520Synthesis%2520of%2520High-Fidelity%25203D%2520CT%250A%2520%2520Images%26entry.906535625%3DYanwu%2520Xu%2520and%2520Li%2520Sun%2520and%2520Wei%2520Peng%2520and%2520Shuyue%2520Jia%2520and%2520Katelyn%2520Morrison%2520and%2520Adam%2520Perer%2520and%2520Afrooz%2520Zandifar%2520and%2520Shyam%2520Visweswaran%2520and%2520Motahhare%2520Eslami%2520and%2520Kayhan%2520Batmanghelich%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520innovative%2520methodology%2520for%2520producing%2520high-quality%25203D%250Alung%2520CT%2520images%2520guided%2520by%2520textual%2520information.%2520While%2520diffusion-based%2520generative%250Amodels%2520are%2520increasingly%2520used%2520in%2520medical%2520imaging%252C%2520current%2520state-of-the-art%250Aapproaches%2520are%2520limited%2520to%2520low-resolution%2520outputs%2520and%2520underutilize%2520radiology%250Areports%2527%2520abundant%2520information.%2520The%2520radiology%2520reports%2520can%2520enhance%2520the%2520generation%250Aprocess%2520by%2520providing%2520additional%2520guidance%2520and%2520offering%2520fine-grained%2520control%2520over%250Athe%2520synthesis%2520of%2520images.%2520Nevertheless%252C%2520expanding%2520text-guided%2520generation%2520to%250Ahigh-resolution%25203D%2520images%2520poses%2520significant%2520memory%2520and%2520anatomical%250Adetail-preserving%2520challenges.%2520Addressing%2520the%2520memory%2520issue%252C%2520we%2520introduce%2520a%250Ahierarchical%2520scheme%2520that%2520uses%2520a%2520modified%2520UNet%2520architecture.%2520We%2520start%2520by%250Asynthesizing%2520low-resolution%2520images%2520conditioned%2520on%2520the%2520text%252C%2520serving%2520as%2520a%250Afoundation%2520for%2520subsequent%2520generators%2520for%2520complete%2520volumetric%2520data.%2520To%2520ensure%250Athe%2520anatomical%2520plausibility%2520of%2520the%2520generated%2520samples%252C%2520we%2520provide%2520further%250Aguidance%2520by%2520generating%2520vascular%252C%2520airway%252C%2520and%2520lobular%2520segmentation%2520masks%2520in%250Aconjunction%2520with%2520the%2520CT%2520images.%2520The%2520model%2520demonstrates%2520the%2520capability%2520to%2520use%250Atextual%2520input%2520and%2520segmentation%2520tasks%2520to%2520generate%2520synthesized%2520images.%2520The%250Aresults%2520of%2520comparative%2520assessments%2520indicate%2520that%2520our%2520approach%2520exhibits%2520superior%250Aperformance%2520compared%2520to%2520the%2520most%2520advanced%2520models%2520based%2520on%2520GAN%2520and%2520diffusion%250Atechniques%252C%2520especially%2520in%2520accurately%2520retaining%2520crucial%2520anatomical%2520features%2520such%250Aas%2520fissure%2520lines%252C%2520airways%252C%2520and%2520vascular%2520structures.%2520This%2520innovation%2520introduces%250Anovel%2520possibilities.%2520This%2520study%2520focuses%2520on%2520two%2520main%2520objectives%253A%2520%25281%2529%2520the%250Adevelopment%2520of%2520a%2520method%2520for%2520creating%2520images%2520based%2520on%2520textual%2520prompts%2520and%250Aanatomical%2520components%252C%2520and%2520%25282%2529%2520the%2520capability%2520to%2520generate%2520new%2520images%250Aconditioning%2520on%2520anatomical%2520elements.%2520The%2520advancements%2520in%2520image%2520generation%2520can%250Abe%2520applied%2520to%2520enhance%2520numerous%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03559v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedSyn%3A%20Text-guided%20Anatomy-aware%20Synthesis%20of%20High-Fidelity%203D%20CT%0A%20%20Images&entry.906535625=Yanwu%20Xu%20and%20Li%20Sun%20and%20Wei%20Peng%20and%20Shuyue%20Jia%20and%20Katelyn%20Morrison%20and%20Adam%20Perer%20and%20Afrooz%20Zandifar%20and%20Shyam%20Visweswaran%20and%20Motahhare%20Eslami%20and%20Kayhan%20Batmanghelich&entry.1292438233=%20%20This%20paper%20introduces%20an%20innovative%20methodology%20for%20producing%20high-quality%203D%0Alung%20CT%20images%20guided%20by%20textual%20information.%20While%20diffusion-based%20generative%0Amodels%20are%20increasingly%20used%20in%20medical%20imaging%2C%20current%20state-of-the-art%0Aapproaches%20are%20limited%20to%20low-resolution%20outputs%20and%20underutilize%20radiology%0Areports%27%20abundant%20information.%20The%20radiology%20reports%20can%20enhance%20the%20generation%0Aprocess%20by%20providing%20additional%20guidance%20and%20offering%20fine-grained%20control%20over%0Athe%20synthesis%20of%20images.%20Nevertheless%2C%20expanding%20text-guided%20generation%20to%0Ahigh-resolution%203D%20images%20poses%20significant%20memory%20and%20anatomical%0Adetail-preserving%20challenges.%20Addressing%20the%20memory%20issue%2C%20we%20introduce%20a%0Ahierarchical%20scheme%20that%20uses%20a%20modified%20UNet%20architecture.%20We%20start%20by%0Asynthesizing%20low-resolution%20images%20conditioned%20on%20the%20text%2C%20serving%20as%20a%0Afoundation%20for%20subsequent%20generators%20for%20complete%20volumetric%20data.%20To%20ensure%0Athe%20anatomical%20plausibility%20of%20the%20generated%20samples%2C%20we%20provide%20further%0Aguidance%20by%20generating%20vascular%2C%20airway%2C%20and%20lobular%20segmentation%20masks%20in%0Aconjunction%20with%20the%20CT%20images.%20The%20model%20demonstrates%20the%20capability%20to%20use%0Atextual%20input%20and%20segmentation%20tasks%20to%20generate%20synthesized%20images.%20The%0Aresults%20of%20comparative%20assessments%20indicate%20that%20our%20approach%20exhibits%20superior%0Aperformance%20compared%20to%20the%20most%20advanced%20models%20based%20on%20GAN%20and%20diffusion%0Atechniques%2C%20especially%20in%20accurately%20retaining%20crucial%20anatomical%20features%20such%0Aas%20fissure%20lines%2C%20airways%2C%20and%20vascular%20structures.%20This%20innovation%20introduces%0Anovel%20possibilities.%20This%20study%20focuses%20on%20two%20main%20objectives%3A%20%281%29%20the%0Adevelopment%20of%20a%20method%20for%20creating%20images%20based%20on%20textual%20prompts%20and%0Aanatomical%20components%2C%20and%20%282%29%20the%20capability%20to%20generate%20new%20images%0Aconditioning%20on%20anatomical%20elements.%20The%20advancements%20in%20image%20generation%20can%0Abe%20applied%20to%20enhance%20numerous%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03559v6&entry.124074799=Read"},
{"title": "Encoding architecture algebra", "author": "Stephane Bersier and Xinyi Chen-Lin", "abstract": "  Despite the wide variety of input types in machine learning, this diversity\nis often not fully reflected in their representations or model architectures,\nleading to inefficiencies throughout a model's lifecycle. This paper introduces\nan algebraic approach to constructing input-encoding architectures that\nproperly account for the data's structure, providing a step toward achieving\nmore typeful machine learning.\n", "link": "http://arxiv.org/abs/2410.11776v1", "date": "2024-10-15", "relevancy": 2.4892, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Encoding%20architecture%20algebra&body=Title%3A%20Encoding%20architecture%20algebra%0AAuthor%3A%20Stephane%20Bersier%20and%20Xinyi%20Chen-Lin%0AAbstract%3A%20%20%20Despite%20the%20wide%20variety%20of%20input%20types%20in%20machine%20learning%2C%20this%20diversity%0Ais%20often%20not%20fully%20reflected%20in%20their%20representations%20or%20model%20architectures%2C%0Aleading%20to%20inefficiencies%20throughout%20a%20model%27s%20lifecycle.%20This%20paper%20introduces%0Aan%20algebraic%20approach%20to%20constructing%20input-encoding%20architectures%20that%0Aproperly%20account%20for%20the%20data%27s%20structure%2C%20providing%20a%20step%20toward%20achieving%0Amore%20typeful%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEncoding%2520architecture%2520algebra%26entry.906535625%3DStephane%2520Bersier%2520and%2520Xinyi%2520Chen-Lin%26entry.1292438233%3D%2520%2520Despite%2520the%2520wide%2520variety%2520of%2520input%2520types%2520in%2520machine%2520learning%252C%2520this%2520diversity%250Ais%2520often%2520not%2520fully%2520reflected%2520in%2520their%2520representations%2520or%2520model%2520architectures%252C%250Aleading%2520to%2520inefficiencies%2520throughout%2520a%2520model%2527s%2520lifecycle.%2520This%2520paper%2520introduces%250Aan%2520algebraic%2520approach%2520to%2520constructing%2520input-encoding%2520architectures%2520that%250Aproperly%2520account%2520for%2520the%2520data%2527s%2520structure%252C%2520providing%2520a%2520step%2520toward%2520achieving%250Amore%2520typeful%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Encoding%20architecture%20algebra&entry.906535625=Stephane%20Bersier%20and%20Xinyi%20Chen-Lin&entry.1292438233=%20%20Despite%20the%20wide%20variety%20of%20input%20types%20in%20machine%20learning%2C%20this%20diversity%0Ais%20often%20not%20fully%20reflected%20in%20their%20representations%20or%20model%20architectures%2C%0Aleading%20to%20inefficiencies%20throughout%20a%20model%27s%20lifecycle.%20This%20paper%20introduces%0Aan%20algebraic%20approach%20to%20constructing%20input-encoding%20architectures%20that%0Aproperly%20account%20for%20the%20data%27s%20structure%2C%20providing%20a%20step%20toward%20achieving%0Amore%20typeful%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11776v1&entry.124074799=Read"},
{"title": "Leveraging Structure Knowledge and Deep Models for the Detection of\n  Abnormal Handwritten Text", "author": "Zi-Rui Wang", "abstract": "  Currently, the destruction of the sequence structure in handwritten text has\nbecome one of the main bottlenecks restricting the recognition task. The\ntypical situations include additional specific markers (the text swapping\nmodification) and the text overlap caused by character modifications like\ndeletion, replacement, and insertion. In this paper, we propose a two-stage\ndetection algorithm that combines structure knowledge and deep models for the\nabove mentioned text. Firstly, different structure prototypes are roughly\nlocated from handwritten text images. Based on the detection results of the\nfirst stage, in the second stage, we adopt different strategies. Specifically,\na shape regression network trained by a novel semi-supervised contrast training\nstrategy is introduced and the positional relationship between the characters\nis fully employed. Experiments on two handwritten text datasets show that the\nproposed method can greatly improve the detection performance. The new dataset\nis available at https://github.com/Wukong90.\n", "link": "http://arxiv.org/abs/2410.11670v1", "date": "2024-10-15", "relevancy": 2.4856, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5173}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Structure%20Knowledge%20and%20Deep%20Models%20for%20the%20Detection%20of%0A%20%20Abnormal%20Handwritten%20Text&body=Title%3A%20Leveraging%20Structure%20Knowledge%20and%20Deep%20Models%20for%20the%20Detection%20of%0A%20%20Abnormal%20Handwritten%20Text%0AAuthor%3A%20Zi-Rui%20Wang%0AAbstract%3A%20%20%20Currently%2C%20the%20destruction%20of%20the%20sequence%20structure%20in%20handwritten%20text%20has%0Abecome%20one%20of%20the%20main%20bottlenecks%20restricting%20the%20recognition%20task.%20The%0Atypical%20situations%20include%20additional%20specific%20markers%20%28the%20text%20swapping%0Amodification%29%20and%20the%20text%20overlap%20caused%20by%20character%20modifications%20like%0Adeletion%2C%20replacement%2C%20and%20insertion.%20In%20this%20paper%2C%20we%20propose%20a%20two-stage%0Adetection%20algorithm%20that%20combines%20structure%20knowledge%20and%20deep%20models%20for%20the%0Aabove%20mentioned%20text.%20Firstly%2C%20different%20structure%20prototypes%20are%20roughly%0Alocated%20from%20handwritten%20text%20images.%20Based%20on%20the%20detection%20results%20of%20the%0Afirst%20stage%2C%20in%20the%20second%20stage%2C%20we%20adopt%20different%20strategies.%20Specifically%2C%0Aa%20shape%20regression%20network%20trained%20by%20a%20novel%20semi-supervised%20contrast%20training%0Astrategy%20is%20introduced%20and%20the%20positional%20relationship%20between%20the%20characters%0Ais%20fully%20employed.%20Experiments%20on%20two%20handwritten%20text%20datasets%20show%20that%20the%0Aproposed%20method%20can%20greatly%20improve%20the%20detection%20performance.%20The%20new%20dataset%0Ais%20available%20at%20https%3A//github.com/Wukong90.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Structure%2520Knowledge%2520and%2520Deep%2520Models%2520for%2520the%2520Detection%2520of%250A%2520%2520Abnormal%2520Handwritten%2520Text%26entry.906535625%3DZi-Rui%2520Wang%26entry.1292438233%3D%2520%2520Currently%252C%2520the%2520destruction%2520of%2520the%2520sequence%2520structure%2520in%2520handwritten%2520text%2520has%250Abecome%2520one%2520of%2520the%2520main%2520bottlenecks%2520restricting%2520the%2520recognition%2520task.%2520The%250Atypical%2520situations%2520include%2520additional%2520specific%2520markers%2520%2528the%2520text%2520swapping%250Amodification%2529%2520and%2520the%2520text%2520overlap%2520caused%2520by%2520character%2520modifications%2520like%250Adeletion%252C%2520replacement%252C%2520and%2520insertion.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520two-stage%250Adetection%2520algorithm%2520that%2520combines%2520structure%2520knowledge%2520and%2520deep%2520models%2520for%2520the%250Aabove%2520mentioned%2520text.%2520Firstly%252C%2520different%2520structure%2520prototypes%2520are%2520roughly%250Alocated%2520from%2520handwritten%2520text%2520images.%2520Based%2520on%2520the%2520detection%2520results%2520of%2520the%250Afirst%2520stage%252C%2520in%2520the%2520second%2520stage%252C%2520we%2520adopt%2520different%2520strategies.%2520Specifically%252C%250Aa%2520shape%2520regression%2520network%2520trained%2520by%2520a%2520novel%2520semi-supervised%2520contrast%2520training%250Astrategy%2520is%2520introduced%2520and%2520the%2520positional%2520relationship%2520between%2520the%2520characters%250Ais%2520fully%2520employed.%2520Experiments%2520on%2520two%2520handwritten%2520text%2520datasets%2520show%2520that%2520the%250Aproposed%2520method%2520can%2520greatly%2520improve%2520the%2520detection%2520performance.%2520The%2520new%2520dataset%250Ais%2520available%2520at%2520https%253A//github.com/Wukong90.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Structure%20Knowledge%20and%20Deep%20Models%20for%20the%20Detection%20of%0A%20%20Abnormal%20Handwritten%20Text&entry.906535625=Zi-Rui%20Wang&entry.1292438233=%20%20Currently%2C%20the%20destruction%20of%20the%20sequence%20structure%20in%20handwritten%20text%20has%0Abecome%20one%20of%20the%20main%20bottlenecks%20restricting%20the%20recognition%20task.%20The%0Atypical%20situations%20include%20additional%20specific%20markers%20%28the%20text%20swapping%0Amodification%29%20and%20the%20text%20overlap%20caused%20by%20character%20modifications%20like%0Adeletion%2C%20replacement%2C%20and%20insertion.%20In%20this%20paper%2C%20we%20propose%20a%20two-stage%0Adetection%20algorithm%20that%20combines%20structure%20knowledge%20and%20deep%20models%20for%20the%0Aabove%20mentioned%20text.%20Firstly%2C%20different%20structure%20prototypes%20are%20roughly%0Alocated%20from%20handwritten%20text%20images.%20Based%20on%20the%20detection%20results%20of%20the%0Afirst%20stage%2C%20in%20the%20second%20stage%2C%20we%20adopt%20different%20strategies.%20Specifically%2C%0Aa%20shape%20regression%20network%20trained%20by%20a%20novel%20semi-supervised%20contrast%20training%0Astrategy%20is%20introduced%20and%20the%20positional%20relationship%20between%20the%20characters%0Ais%20fully%20employed.%20Experiments%20on%20two%20handwritten%20text%20datasets%20show%20that%20the%0Aproposed%20method%20can%20greatly%20improve%20the%20detection%20performance.%20The%20new%20dataset%0Ais%20available%20at%20https%3A//github.com/Wukong90.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11670v1&entry.124074799=Read"},
{"title": "Augmentation-aware Self-supervised Learning with Conditioned Projector", "author": "Marcin Przewi\u0119\u017alikowski and Mateusz Pyla and Bartosz Zieli\u0144ski and Bart\u0142omiej Twardowski and Jacek Tabor and Marek \u015amieja", "abstract": "  Self-supervised learning (SSL) is a powerful technique for learning from\nunlabeled data. By learning to remain invariant to applied data augmentations,\nmethods such as SimCLR and MoCo can reach quality on par with supervised\napproaches. However, this invariance may be detrimental for solving downstream\ntasks that depend on traits affected by augmentations used during pretraining,\nsuch as color. In this paper, we propose to foster sensitivity to such\ncharacteristics in the representation space by modifying the projector network,\na common component of self-supervised architectures. Specifically, we\nsupplement the projector with information about augmentations applied to\nimages. For the projector to take advantage of this auxiliary conditioning when\nsolving the SSL task, the feature extractor learns to preserve the augmentation\ninformation in its representations. Our approach, coined Conditional\nAugmentation-aware Self-supervised Learning (CASSLE), is directly applicable to\ntypical joint-embedding SSL methods regardless of their objective functions.\nMoreover, it does not require major changes in the network architecture or\nprior knowledge of downstream tasks. In addition to an analysis of sensitivity\ntowards different data augmentations, we conduct a series of experiments, which\nshow that CASSLE improves over various SSL methods, reaching state-of-the-art\nperformance in multiple downstream tasks.\n", "link": "http://arxiv.org/abs/2306.06082v3", "date": "2024-10-15", "relevancy": 2.476, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5148}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmentation-aware%20Self-supervised%20Learning%20with%20Conditioned%20Projector&body=Title%3A%20Augmentation-aware%20Self-supervised%20Learning%20with%20Conditioned%20Projector%0AAuthor%3A%20Marcin%20Przewi%C4%99%C5%BAlikowski%20and%20Mateusz%20Pyla%20and%20Bartosz%20Zieli%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%20and%20Jacek%20Tabor%20and%20Marek%20%C5%9Amieja%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20is%20a%20powerful%20technique%20for%20learning%20from%0Aunlabeled%20data.%20By%20learning%20to%20remain%20invariant%20to%20applied%20data%20augmentations%2C%0Amethods%20such%20as%20SimCLR%20and%20MoCo%20can%20reach%20quality%20on%20par%20with%20supervised%0Aapproaches.%20However%2C%20this%20invariance%20may%20be%20detrimental%20for%20solving%20downstream%0Atasks%20that%20depend%20on%20traits%20affected%20by%20augmentations%20used%20during%20pretraining%2C%0Asuch%20as%20color.%20In%20this%20paper%2C%20we%20propose%20to%20foster%20sensitivity%20to%20such%0Acharacteristics%20in%20the%20representation%20space%20by%20modifying%20the%20projector%20network%2C%0Aa%20common%20component%20of%20self-supervised%20architectures.%20Specifically%2C%20we%0Asupplement%20the%20projector%20with%20information%20about%20augmentations%20applied%20to%0Aimages.%20For%20the%20projector%20to%20take%20advantage%20of%20this%20auxiliary%20conditioning%20when%0Asolving%20the%20SSL%20task%2C%20the%20feature%20extractor%20learns%20to%20preserve%20the%20augmentation%0Ainformation%20in%20its%20representations.%20Our%20approach%2C%20coined%20Conditional%0AAugmentation-aware%20Self-supervised%20Learning%20%28CASSLE%29%2C%20is%20directly%20applicable%20to%0Atypical%20joint-embedding%20SSL%20methods%20regardless%20of%20their%20objective%20functions.%0AMoreover%2C%20it%20does%20not%20require%20major%20changes%20in%20the%20network%20architecture%20or%0Aprior%20knowledge%20of%20downstream%20tasks.%20In%20addition%20to%20an%20analysis%20of%20sensitivity%0Atowards%20different%20data%20augmentations%2C%20we%20conduct%20a%20series%20of%20experiments%2C%20which%0Ashow%20that%20CASSLE%20improves%20over%20various%20SSL%20methods%2C%20reaching%20state-of-the-art%0Aperformance%20in%20multiple%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06082v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmentation-aware%2520Self-supervised%2520Learning%2520with%2520Conditioned%2520Projector%26entry.906535625%3DMarcin%2520Przewi%25C4%2599%25C5%25BAlikowski%2520and%2520Mateusz%2520Pyla%2520and%2520Bartosz%2520Zieli%25C5%2584ski%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520Jacek%2520Tabor%2520and%2520Marek%2520%25C5%259Amieja%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520is%2520a%2520powerful%2520technique%2520for%2520learning%2520from%250Aunlabeled%2520data.%2520By%2520learning%2520to%2520remain%2520invariant%2520to%2520applied%2520data%2520augmentations%252C%250Amethods%2520such%2520as%2520SimCLR%2520and%2520MoCo%2520can%2520reach%2520quality%2520on%2520par%2520with%2520supervised%250Aapproaches.%2520However%252C%2520this%2520invariance%2520may%2520be%2520detrimental%2520for%2520solving%2520downstream%250Atasks%2520that%2520depend%2520on%2520traits%2520affected%2520by%2520augmentations%2520used%2520during%2520pretraining%252C%250Asuch%2520as%2520color.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520foster%2520sensitivity%2520to%2520such%250Acharacteristics%2520in%2520the%2520representation%2520space%2520by%2520modifying%2520the%2520projector%2520network%252C%250Aa%2520common%2520component%2520of%2520self-supervised%2520architectures.%2520Specifically%252C%2520we%250Asupplement%2520the%2520projector%2520with%2520information%2520about%2520augmentations%2520applied%2520to%250Aimages.%2520For%2520the%2520projector%2520to%2520take%2520advantage%2520of%2520this%2520auxiliary%2520conditioning%2520when%250Asolving%2520the%2520SSL%2520task%252C%2520the%2520feature%2520extractor%2520learns%2520to%2520preserve%2520the%2520augmentation%250Ainformation%2520in%2520its%2520representations.%2520Our%2520approach%252C%2520coined%2520Conditional%250AAugmentation-aware%2520Self-supervised%2520Learning%2520%2528CASSLE%2529%252C%2520is%2520directly%2520applicable%2520to%250Atypical%2520joint-embedding%2520SSL%2520methods%2520regardless%2520of%2520their%2520objective%2520functions.%250AMoreover%252C%2520it%2520does%2520not%2520require%2520major%2520changes%2520in%2520the%2520network%2520architecture%2520or%250Aprior%2520knowledge%2520of%2520downstream%2520tasks.%2520In%2520addition%2520to%2520an%2520analysis%2520of%2520sensitivity%250Atowards%2520different%2520data%2520augmentations%252C%2520we%2520conduct%2520a%2520series%2520of%2520experiments%252C%2520which%250Ashow%2520that%2520CASSLE%2520improves%2520over%2520various%2520SSL%2520methods%252C%2520reaching%2520state-of-the-art%250Aperformance%2520in%2520multiple%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06082v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmentation-aware%20Self-supervised%20Learning%20with%20Conditioned%20Projector&entry.906535625=Marcin%20Przewi%C4%99%C5%BAlikowski%20and%20Mateusz%20Pyla%20and%20Bartosz%20Zieli%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%20and%20Jacek%20Tabor%20and%20Marek%20%C5%9Amieja&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20is%20a%20powerful%20technique%20for%20learning%20from%0Aunlabeled%20data.%20By%20learning%20to%20remain%20invariant%20to%20applied%20data%20augmentations%2C%0Amethods%20such%20as%20SimCLR%20and%20MoCo%20can%20reach%20quality%20on%20par%20with%20supervised%0Aapproaches.%20However%2C%20this%20invariance%20may%20be%20detrimental%20for%20solving%20downstream%0Atasks%20that%20depend%20on%20traits%20affected%20by%20augmentations%20used%20during%20pretraining%2C%0Asuch%20as%20color.%20In%20this%20paper%2C%20we%20propose%20to%20foster%20sensitivity%20to%20such%0Acharacteristics%20in%20the%20representation%20space%20by%20modifying%20the%20projector%20network%2C%0Aa%20common%20component%20of%20self-supervised%20architectures.%20Specifically%2C%20we%0Asupplement%20the%20projector%20with%20information%20about%20augmentations%20applied%20to%0Aimages.%20For%20the%20projector%20to%20take%20advantage%20of%20this%20auxiliary%20conditioning%20when%0Asolving%20the%20SSL%20task%2C%20the%20feature%20extractor%20learns%20to%20preserve%20the%20augmentation%0Ainformation%20in%20its%20representations.%20Our%20approach%2C%20coined%20Conditional%0AAugmentation-aware%20Self-supervised%20Learning%20%28CASSLE%29%2C%20is%20directly%20applicable%20to%0Atypical%20joint-embedding%20SSL%20methods%20regardless%20of%20their%20objective%20functions.%0AMoreover%2C%20it%20does%20not%20require%20major%20changes%20in%20the%20network%20architecture%20or%0Aprior%20knowledge%20of%20downstream%20tasks.%20In%20addition%20to%20an%20analysis%20of%20sensitivity%0Atowards%20different%20data%20augmentations%2C%20we%20conduct%20a%20series%20of%20experiments%2C%20which%0Ashow%20that%20CASSLE%20improves%20over%20various%20SSL%20methods%2C%20reaching%20state-of-the-art%0Aperformance%20in%20multiple%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06082v3&entry.124074799=Read"},
{"title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models", "author": "Deniz Bayazit and Negar Foroutan and Zeming Chen and Gail Weiss and Antoine Bosselut", "abstract": "  Pretrained language models (LMs) encode implicit representations of knowledge\nin their parameters. However, localizing these representations and\ndisentangling them from each other remains an open problem. In this work, we\ninvestigate whether pretrained language models contain various\nknowledge-critical subnetworks: particular sparse computational subgraphs that\ncan, if removed, precisely suppress specific knowledge the model has memorized.\nWe propose a multi-objective differentiable masking scheme that can be applied\nto both weights and neurons to discover such subnetworks and show that we can\nuse them to precisely remove specific knowledge from models while minimizing\nadverse effects on the behavior of the original model. We demonstrate our\nmethod on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+\nsparsity) that are critical for expressing specific collections of relational\nknowledge. When these subnetworks are removed, the remaining network maintains\nmost of its initial abilities but struggles to represent the suppressed\nknowledge.\n", "link": "http://arxiv.org/abs/2310.03084v2", "date": "2024-10-15", "relevancy": 2.4752, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Knowledge-Critical%20Subnetworks%20in%20Pretrained%20Language%20Models&body=Title%3A%20Discovering%20Knowledge-Critical%20Subnetworks%20in%20Pretrained%20Language%20Models%0AAuthor%3A%20Deniz%20Bayazit%20and%20Negar%20Foroutan%20and%20Zeming%20Chen%20and%20Gail%20Weiss%20and%20Antoine%20Bosselut%0AAbstract%3A%20%20%20Pretrained%20language%20models%20%28LMs%29%20encode%20implicit%20representations%20of%20knowledge%0Ain%20their%20parameters.%20However%2C%20localizing%20these%20representations%20and%0Adisentangling%20them%20from%20each%20other%20remains%20an%20open%20problem.%20In%20this%20work%2C%20we%0Ainvestigate%20whether%20pretrained%20language%20models%20contain%20various%0Aknowledge-critical%20subnetworks%3A%20particular%20sparse%20computational%20subgraphs%20that%0Acan%2C%20if%20removed%2C%20precisely%20suppress%20specific%20knowledge%20the%20model%20has%20memorized.%0AWe%20propose%20a%20multi-objective%20differentiable%20masking%20scheme%20that%20can%20be%20applied%0Ato%20both%20weights%20and%20neurons%20to%20discover%20such%20subnetworks%20and%20show%20that%20we%20can%0Ause%20them%20to%20precisely%20remove%20specific%20knowledge%20from%20models%20while%20minimizing%0Aadverse%20effects%20on%20the%20behavior%20of%20the%20original%20model.%20We%20demonstrate%20our%0Amethod%20on%20multiple%20GPT2%20variants%2C%20uncovering%20highly%20sparse%20subnetworks%20%2898%25%2B%0Asparsity%29%20that%20are%20critical%20for%20expressing%20specific%20collections%20of%20relational%0Aknowledge.%20When%20these%20subnetworks%20are%20removed%2C%20the%20remaining%20network%20maintains%0Amost%20of%20its%20initial%20abilities%20but%20struggles%20to%20represent%20the%20suppressed%0Aknowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03084v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Knowledge-Critical%2520Subnetworks%2520in%2520Pretrained%2520Language%2520Models%26entry.906535625%3DDeniz%2520Bayazit%2520and%2520Negar%2520Foroutan%2520and%2520Zeming%2520Chen%2520and%2520Gail%2520Weiss%2520and%2520Antoine%2520Bosselut%26entry.1292438233%3D%2520%2520Pretrained%2520language%2520models%2520%2528LMs%2529%2520encode%2520implicit%2520representations%2520of%2520knowledge%250Ain%2520their%2520parameters.%2520However%252C%2520localizing%2520these%2520representations%2520and%250Adisentangling%2520them%2520from%2520each%2520other%2520remains%2520an%2520open%2520problem.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520whether%2520pretrained%2520language%2520models%2520contain%2520various%250Aknowledge-critical%2520subnetworks%253A%2520particular%2520sparse%2520computational%2520subgraphs%2520that%250Acan%252C%2520if%2520removed%252C%2520precisely%2520suppress%2520specific%2520knowledge%2520the%2520model%2520has%2520memorized.%250AWe%2520propose%2520a%2520multi-objective%2520differentiable%2520masking%2520scheme%2520that%2520can%2520be%2520applied%250Ato%2520both%2520weights%2520and%2520neurons%2520to%2520discover%2520such%2520subnetworks%2520and%2520show%2520that%2520we%2520can%250Ause%2520them%2520to%2520precisely%2520remove%2520specific%2520knowledge%2520from%2520models%2520while%2520minimizing%250Aadverse%2520effects%2520on%2520the%2520behavior%2520of%2520the%2520original%2520model.%2520We%2520demonstrate%2520our%250Amethod%2520on%2520multiple%2520GPT2%2520variants%252C%2520uncovering%2520highly%2520sparse%2520subnetworks%2520%252898%2525%252B%250Asparsity%2529%2520that%2520are%2520critical%2520for%2520expressing%2520specific%2520collections%2520of%2520relational%250Aknowledge.%2520When%2520these%2520subnetworks%2520are%2520removed%252C%2520the%2520remaining%2520network%2520maintains%250Amost%2520of%2520its%2520initial%2520abilities%2520but%2520struggles%2520to%2520represent%2520the%2520suppressed%250Aknowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03084v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Knowledge-Critical%20Subnetworks%20in%20Pretrained%20Language%20Models&entry.906535625=Deniz%20Bayazit%20and%20Negar%20Foroutan%20and%20Zeming%20Chen%20and%20Gail%20Weiss%20and%20Antoine%20Bosselut&entry.1292438233=%20%20Pretrained%20language%20models%20%28LMs%29%20encode%20implicit%20representations%20of%20knowledge%0Ain%20their%20parameters.%20However%2C%20localizing%20these%20representations%20and%0Adisentangling%20them%20from%20each%20other%20remains%20an%20open%20problem.%20In%20this%20work%2C%20we%0Ainvestigate%20whether%20pretrained%20language%20models%20contain%20various%0Aknowledge-critical%20subnetworks%3A%20particular%20sparse%20computational%20subgraphs%20that%0Acan%2C%20if%20removed%2C%20precisely%20suppress%20specific%20knowledge%20the%20model%20has%20memorized.%0AWe%20propose%20a%20multi-objective%20differentiable%20masking%20scheme%20that%20can%20be%20applied%0Ato%20both%20weights%20and%20neurons%20to%20discover%20such%20subnetworks%20and%20show%20that%20we%20can%0Ause%20them%20to%20precisely%20remove%20specific%20knowledge%20from%20models%20while%20minimizing%0Aadverse%20effects%20on%20the%20behavior%20of%20the%20original%20model.%20We%20demonstrate%20our%0Amethod%20on%20multiple%20GPT2%20variants%2C%20uncovering%20highly%20sparse%20subnetworks%20%2898%25%2B%0Asparsity%29%20that%20are%20critical%20for%20expressing%20specific%20collections%20of%20relational%0Aknowledge.%20When%20these%20subnetworks%20are%20removed%2C%20the%20remaining%20network%20maintains%0Amost%20of%20its%20initial%20abilities%20but%20struggles%20to%20represent%20the%20suppressed%0Aknowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03084v2&entry.124074799=Read"},
{"title": "Adaptive Hybrid Model Pruning in Federated Learning through Loss\n  Exploration", "author": "Christian Intern\u00f2 and Elena Raponi and Niki van Stein and Thomas B\u00e4ck and Markus Olhofer and Yaochu Jin and Barbara Hammer", "abstract": "  The rapid proliferation of smart devices coupled with the advent of 6G\nnetworks has profoundly reshaped the domain of collaborative machine learning.\nAlongside growing privacy-security concerns in sensitive fields, these\ndevelopments have positioned federated learning (FL) as a pivotal technology\nfor decentralized model training. Despite its vast potential, specially in the\nage of complex foundation models, FL encounters challenges such as elevated\ncommunication costs, computational constraints, and the complexities of non-IID\ndata distributions. We introduce AutoFLIP, an innovative approach that utilizes\na federated loss exploration phase to drive adaptive hybrid pruning, operating\nin a structured and unstructured way. This innovative mechanism automatically\nidentifies and prunes model substructure by distilling knowledge on model\ngradients behavior across different non-IID client losses topology, thereby\noptimizing computational efficiency and enhancing model performance on resource\nconstrained scenarios. Extensive experiments on various datasets and FL tasks\nreveal that AutoFLIP not only efficiently accelerates global convergence, but\nalso achieves superior accuracy and robustness compared to traditional methods.\nOn average, AutoFLIP reduces computational overhead by 48.8% and communication\ncosts by 35.5%, while improving global accuracy. By significantly reducing\nthese overheads, AutoFLIP offer the way for efficient FL deployment in\nreal-world applications for a scalable and broad applicability.\n", "link": "http://arxiv.org/abs/2405.10271v2", "date": "2024-10-15", "relevancy": 2.4699, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.495}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Hybrid%20Model%20Pruning%20in%20Federated%20Learning%20through%20Loss%0A%20%20Exploration&body=Title%3A%20Adaptive%20Hybrid%20Model%20Pruning%20in%20Federated%20Learning%20through%20Loss%0A%20%20Exploration%0AAuthor%3A%20Christian%20Intern%C3%B2%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck%20and%20Markus%20Olhofer%20and%20Yaochu%20Jin%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20The%20rapid%20proliferation%20of%20smart%20devices%20coupled%20with%20the%20advent%20of%206G%0Anetworks%20has%20profoundly%20reshaped%20the%20domain%20of%20collaborative%20machine%20learning.%0AAlongside%20growing%20privacy-security%20concerns%20in%20sensitive%20fields%2C%20these%0Adevelopments%20have%20positioned%20federated%20learning%20%28FL%29%20as%20a%20pivotal%20technology%0Afor%20decentralized%20model%20training.%20Despite%20its%20vast%20potential%2C%20specially%20in%20the%0Aage%20of%20complex%20foundation%20models%2C%20FL%20encounters%20challenges%20such%20as%20elevated%0Acommunication%20costs%2C%20computational%20constraints%2C%20and%20the%20complexities%20of%20non-IID%0Adata%20distributions.%20We%20introduce%20AutoFLIP%2C%20an%20innovative%20approach%20that%20utilizes%0Aa%20federated%20loss%20exploration%20phase%20to%20drive%20adaptive%20hybrid%20pruning%2C%20operating%0Ain%20a%20structured%20and%20unstructured%20way.%20This%20innovative%20mechanism%20automatically%0Aidentifies%20and%20prunes%20model%20substructure%20by%20distilling%20knowledge%20on%20model%0Agradients%20behavior%20across%20different%20non-IID%20client%20losses%20topology%2C%20thereby%0Aoptimizing%20computational%20efficiency%20and%20enhancing%20model%20performance%20on%20resource%0Aconstrained%20scenarios.%20Extensive%20experiments%20on%20various%20datasets%20and%20FL%20tasks%0Areveal%20that%20AutoFLIP%20not%20only%20efficiently%20accelerates%20global%20convergence%2C%20but%0Aalso%20achieves%20superior%20accuracy%20and%20robustness%20compared%20to%20traditional%20methods.%0AOn%20average%2C%20AutoFLIP%20reduces%20computational%20overhead%20by%2048.8%25%20and%20communication%0Acosts%20by%2035.5%25%2C%20while%20improving%20global%20accuracy.%20By%20significantly%20reducing%0Athese%20overheads%2C%20AutoFLIP%20offer%20the%20way%20for%20efficient%20FL%20deployment%20in%0Areal-world%20applications%20for%20a%20scalable%20and%20broad%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Hybrid%2520Model%2520Pruning%2520in%2520Federated%2520Learning%2520through%2520Loss%250A%2520%2520Exploration%26entry.906535625%3DChristian%2520Intern%25C3%25B2%2520and%2520Elena%2520Raponi%2520and%2520Niki%2520van%2520Stein%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Markus%2520Olhofer%2520and%2520Yaochu%2520Jin%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520The%2520rapid%2520proliferation%2520of%2520smart%2520devices%2520coupled%2520with%2520the%2520advent%2520of%25206G%250Anetworks%2520has%2520profoundly%2520reshaped%2520the%2520domain%2520of%2520collaborative%2520machine%2520learning.%250AAlongside%2520growing%2520privacy-security%2520concerns%2520in%2520sensitive%2520fields%252C%2520these%250Adevelopments%2520have%2520positioned%2520federated%2520learning%2520%2528FL%2529%2520as%2520a%2520pivotal%2520technology%250Afor%2520decentralized%2520model%2520training.%2520Despite%2520its%2520vast%2520potential%252C%2520specially%2520in%2520the%250Aage%2520of%2520complex%2520foundation%2520models%252C%2520FL%2520encounters%2520challenges%2520such%2520as%2520elevated%250Acommunication%2520costs%252C%2520computational%2520constraints%252C%2520and%2520the%2520complexities%2520of%2520non-IID%250Adata%2520distributions.%2520We%2520introduce%2520AutoFLIP%252C%2520an%2520innovative%2520approach%2520that%2520utilizes%250Aa%2520federated%2520loss%2520exploration%2520phase%2520to%2520drive%2520adaptive%2520hybrid%2520pruning%252C%2520operating%250Ain%2520a%2520structured%2520and%2520unstructured%2520way.%2520This%2520innovative%2520mechanism%2520automatically%250Aidentifies%2520and%2520prunes%2520model%2520substructure%2520by%2520distilling%2520knowledge%2520on%2520model%250Agradients%2520behavior%2520across%2520different%2520non-IID%2520client%2520losses%2520topology%252C%2520thereby%250Aoptimizing%2520computational%2520efficiency%2520and%2520enhancing%2520model%2520performance%2520on%2520resource%250Aconstrained%2520scenarios.%2520Extensive%2520experiments%2520on%2520various%2520datasets%2520and%2520FL%2520tasks%250Areveal%2520that%2520AutoFLIP%2520not%2520only%2520efficiently%2520accelerates%2520global%2520convergence%252C%2520but%250Aalso%2520achieves%2520superior%2520accuracy%2520and%2520robustness%2520compared%2520to%2520traditional%2520methods.%250AOn%2520average%252C%2520AutoFLIP%2520reduces%2520computational%2520overhead%2520by%252048.8%2525%2520and%2520communication%250Acosts%2520by%252035.5%2525%252C%2520while%2520improving%2520global%2520accuracy.%2520By%2520significantly%2520reducing%250Athese%2520overheads%252C%2520AutoFLIP%2520offer%2520the%2520way%2520for%2520efficient%2520FL%2520deployment%2520in%250Areal-world%2520applications%2520for%2520a%2520scalable%2520and%2520broad%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Hybrid%20Model%20Pruning%20in%20Federated%20Learning%20through%20Loss%0A%20%20Exploration&entry.906535625=Christian%20Intern%C3%B2%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck%20and%20Markus%20Olhofer%20and%20Yaochu%20Jin%20and%20Barbara%20Hammer&entry.1292438233=%20%20The%20rapid%20proliferation%20of%20smart%20devices%20coupled%20with%20the%20advent%20of%206G%0Anetworks%20has%20profoundly%20reshaped%20the%20domain%20of%20collaborative%20machine%20learning.%0AAlongside%20growing%20privacy-security%20concerns%20in%20sensitive%20fields%2C%20these%0Adevelopments%20have%20positioned%20federated%20learning%20%28FL%29%20as%20a%20pivotal%20technology%0Afor%20decentralized%20model%20training.%20Despite%20its%20vast%20potential%2C%20specially%20in%20the%0Aage%20of%20complex%20foundation%20models%2C%20FL%20encounters%20challenges%20such%20as%20elevated%0Acommunication%20costs%2C%20computational%20constraints%2C%20and%20the%20complexities%20of%20non-IID%0Adata%20distributions.%20We%20introduce%20AutoFLIP%2C%20an%20innovative%20approach%20that%20utilizes%0Aa%20federated%20loss%20exploration%20phase%20to%20drive%20adaptive%20hybrid%20pruning%2C%20operating%0Ain%20a%20structured%20and%20unstructured%20way.%20This%20innovative%20mechanism%20automatically%0Aidentifies%20and%20prunes%20model%20substructure%20by%20distilling%20knowledge%20on%20model%0Agradients%20behavior%20across%20different%20non-IID%20client%20losses%20topology%2C%20thereby%0Aoptimizing%20computational%20efficiency%20and%20enhancing%20model%20performance%20on%20resource%0Aconstrained%20scenarios.%20Extensive%20experiments%20on%20various%20datasets%20and%20FL%20tasks%0Areveal%20that%20AutoFLIP%20not%20only%20efficiently%20accelerates%20global%20convergence%2C%20but%0Aalso%20achieves%20superior%20accuracy%20and%20robustness%20compared%20to%20traditional%20methods.%0AOn%20average%2C%20AutoFLIP%20reduces%20computational%20overhead%20by%2048.8%25%20and%20communication%0Acosts%20by%2035.5%25%2C%20while%20improving%20global%20accuracy.%20By%20significantly%20reducing%0Athese%20overheads%2C%20AutoFLIP%20offer%20the%20way%20for%20efficient%20FL%20deployment%20in%0Areal-world%20applications%20for%20a%20scalable%20and%20broad%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10271v2&entry.124074799=Read"},
{"title": "Analysis and Benchmarking of Extending Blind Face Image Restoration to\n  Videos", "author": "Zhouxia Wang and Jiawei Zhang and Xintao Wang and Tianshui Chen and Ying Shan and Wenping Wang and Ping Luo", "abstract": "  Recent progress in blind face restoration has resulted in producing\nhigh-quality restored results for static images. However, efforts to extend\nthese advancements to video scenarios have been minimal, partly because of the\nabsence of benchmarks that allow for a comprehensive and fair comparison. In\nthis work, we first present a fair evaluation benchmark, in which we first\nintroduce a Real-world Low-Quality Face Video benchmark (RFV-LQ), evaluate\nseveral leading image-based face restoration algorithms, and conduct a thorough\nsystematical analysis of the benefits and challenges associated with extending\nblind face image restoration algorithms to degraded face videos. Our analysis\nidentifies several key issues, primarily categorized into two aspects:\nsignificant jitters in facial components and noise-shape flickering between\nframes. To address these issues, we propose a Temporal Consistency Network\n(TCN) cooperated with alignment smoothing to reduce jitters and flickers in\nrestored videos. TCN is a flexible component that can be seamlessly plugged\ninto the most advanced face image restoration algorithms, ensuring the quality\nof image-based restoration is maintained as closely as possible. Extensive\nexperiments have been conducted to evaluate the effectiveness and efficiency of\nour proposed TCN and alignment smoothing operation. Project page:\nhttps://wzhouxiff.github.io/projects/FIR2FVR/FIR2FVR.\n", "link": "http://arxiv.org/abs/2410.11828v1", "date": "2024-10-15", "relevancy": 2.4622, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6756}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5731}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20and%20Benchmarking%20of%20Extending%20Blind%20Face%20Image%20Restoration%20to%0A%20%20Videos&body=Title%3A%20Analysis%20and%20Benchmarking%20of%20Extending%20Blind%20Face%20Image%20Restoration%20to%0A%20%20Videos%0AAuthor%3A%20Zhouxia%20Wang%20and%20Jiawei%20Zhang%20and%20Xintao%20Wang%20and%20Tianshui%20Chen%20and%20Ying%20Shan%20and%20Wenping%20Wang%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Recent%20progress%20in%20blind%20face%20restoration%20has%20resulted%20in%20producing%0Ahigh-quality%20restored%20results%20for%20static%20images.%20However%2C%20efforts%20to%20extend%0Athese%20advancements%20to%20video%20scenarios%20have%20been%20minimal%2C%20partly%20because%20of%20the%0Aabsence%20of%20benchmarks%20that%20allow%20for%20a%20comprehensive%20and%20fair%20comparison.%20In%0Athis%20work%2C%20we%20first%20present%20a%20fair%20evaluation%20benchmark%2C%20in%20which%20we%20first%0Aintroduce%20a%20Real-world%20Low-Quality%20Face%20Video%20benchmark%20%28RFV-LQ%29%2C%20evaluate%0Aseveral%20leading%20image-based%20face%20restoration%20algorithms%2C%20and%20conduct%20a%20thorough%0Asystematical%20analysis%20of%20the%20benefits%20and%20challenges%20associated%20with%20extending%0Ablind%20face%20image%20restoration%20algorithms%20to%20degraded%20face%20videos.%20Our%20analysis%0Aidentifies%20several%20key%20issues%2C%20primarily%20categorized%20into%20two%20aspects%3A%0Asignificant%20jitters%20in%20facial%20components%20and%20noise-shape%20flickering%20between%0Aframes.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Temporal%20Consistency%20Network%0A%28TCN%29%20cooperated%20with%20alignment%20smoothing%20to%20reduce%20jitters%20and%20flickers%20in%0Arestored%20videos.%20TCN%20is%20a%20flexible%20component%20that%20can%20be%20seamlessly%20plugged%0Ainto%20the%20most%20advanced%20face%20image%20restoration%20algorithms%2C%20ensuring%20the%20quality%0Aof%20image-based%20restoration%20is%20maintained%20as%20closely%20as%20possible.%20Extensive%0Aexperiments%20have%20been%20conducted%20to%20evaluate%20the%20effectiveness%20and%20efficiency%20of%0Aour%20proposed%20TCN%20and%20alignment%20smoothing%20operation.%20Project%20page%3A%0Ahttps%3A//wzhouxiff.github.io/projects/FIR2FVR/FIR2FVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520and%2520Benchmarking%2520of%2520Extending%2520Blind%2520Face%2520Image%2520Restoration%2520to%250A%2520%2520Videos%26entry.906535625%3DZhouxia%2520Wang%2520and%2520Jiawei%2520Zhang%2520and%2520Xintao%2520Wang%2520and%2520Tianshui%2520Chen%2520and%2520Ying%2520Shan%2520and%2520Wenping%2520Wang%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520blind%2520face%2520restoration%2520has%2520resulted%2520in%2520producing%250Ahigh-quality%2520restored%2520results%2520for%2520static%2520images.%2520However%252C%2520efforts%2520to%2520extend%250Athese%2520advancements%2520to%2520video%2520scenarios%2520have%2520been%2520minimal%252C%2520partly%2520because%2520of%2520the%250Aabsence%2520of%2520benchmarks%2520that%2520allow%2520for%2520a%2520comprehensive%2520and%2520fair%2520comparison.%2520In%250Athis%2520work%252C%2520we%2520first%2520present%2520a%2520fair%2520evaluation%2520benchmark%252C%2520in%2520which%2520we%2520first%250Aintroduce%2520a%2520Real-world%2520Low-Quality%2520Face%2520Video%2520benchmark%2520%2528RFV-LQ%2529%252C%2520evaluate%250Aseveral%2520leading%2520image-based%2520face%2520restoration%2520algorithms%252C%2520and%2520conduct%2520a%2520thorough%250Asystematical%2520analysis%2520of%2520the%2520benefits%2520and%2520challenges%2520associated%2520with%2520extending%250Ablind%2520face%2520image%2520restoration%2520algorithms%2520to%2520degraded%2520face%2520videos.%2520Our%2520analysis%250Aidentifies%2520several%2520key%2520issues%252C%2520primarily%2520categorized%2520into%2520two%2520aspects%253A%250Asignificant%2520jitters%2520in%2520facial%2520components%2520and%2520noise-shape%2520flickering%2520between%250Aframes.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520Temporal%2520Consistency%2520Network%250A%2528TCN%2529%2520cooperated%2520with%2520alignment%2520smoothing%2520to%2520reduce%2520jitters%2520and%2520flickers%2520in%250Arestored%2520videos.%2520TCN%2520is%2520a%2520flexible%2520component%2520that%2520can%2520be%2520seamlessly%2520plugged%250Ainto%2520the%2520most%2520advanced%2520face%2520image%2520restoration%2520algorithms%252C%2520ensuring%2520the%2520quality%250Aof%2520image-based%2520restoration%2520is%2520maintained%2520as%2520closely%2520as%2520possible.%2520Extensive%250Aexperiments%2520have%2520been%2520conducted%2520to%2520evaluate%2520the%2520effectiveness%2520and%2520efficiency%2520of%250Aour%2520proposed%2520TCN%2520and%2520alignment%2520smoothing%2520operation.%2520Project%2520page%253A%250Ahttps%253A//wzhouxiff.github.io/projects/FIR2FVR/FIR2FVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20and%20Benchmarking%20of%20Extending%20Blind%20Face%20Image%20Restoration%20to%0A%20%20Videos&entry.906535625=Zhouxia%20Wang%20and%20Jiawei%20Zhang%20and%20Xintao%20Wang%20and%20Tianshui%20Chen%20and%20Ying%20Shan%20and%20Wenping%20Wang%20and%20Ping%20Luo&entry.1292438233=%20%20Recent%20progress%20in%20blind%20face%20restoration%20has%20resulted%20in%20producing%0Ahigh-quality%20restored%20results%20for%20static%20images.%20However%2C%20efforts%20to%20extend%0Athese%20advancements%20to%20video%20scenarios%20have%20been%20minimal%2C%20partly%20because%20of%20the%0Aabsence%20of%20benchmarks%20that%20allow%20for%20a%20comprehensive%20and%20fair%20comparison.%20In%0Athis%20work%2C%20we%20first%20present%20a%20fair%20evaluation%20benchmark%2C%20in%20which%20we%20first%0Aintroduce%20a%20Real-world%20Low-Quality%20Face%20Video%20benchmark%20%28RFV-LQ%29%2C%20evaluate%0Aseveral%20leading%20image-based%20face%20restoration%20algorithms%2C%20and%20conduct%20a%20thorough%0Asystematical%20analysis%20of%20the%20benefits%20and%20challenges%20associated%20with%20extending%0Ablind%20face%20image%20restoration%20algorithms%20to%20degraded%20face%20videos.%20Our%20analysis%0Aidentifies%20several%20key%20issues%2C%20primarily%20categorized%20into%20two%20aspects%3A%0Asignificant%20jitters%20in%20facial%20components%20and%20noise-shape%20flickering%20between%0Aframes.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Temporal%20Consistency%20Network%0A%28TCN%29%20cooperated%20with%20alignment%20smoothing%20to%20reduce%20jitters%20and%20flickers%20in%0Arestored%20videos.%20TCN%20is%20a%20flexible%20component%20that%20can%20be%20seamlessly%20plugged%0Ainto%20the%20most%20advanced%20face%20image%20restoration%20algorithms%2C%20ensuring%20the%20quality%0Aof%20image-based%20restoration%20is%20maintained%20as%20closely%20as%20possible.%20Extensive%0Aexperiments%20have%20been%20conducted%20to%20evaluate%20the%20effectiveness%20and%20efficiency%20of%0Aour%20proposed%20TCN%20and%20alignment%20smoothing%20operation.%20Project%20page%3A%0Ahttps%3A//wzhouxiff.github.io/projects/FIR2FVR/FIR2FVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11828v1&entry.124074799=Read"},
{"title": "Can sparse autoencoders make sense of latent representations?", "author": "Viktoria Schuster", "abstract": "  Sparse autoencoders (SAEs) have lately been used to uncover interpretable\nlatent features in large language models. Here, we explore their potential for\ndecomposing latent representations in complex and high-dimensional biological\ndata, where the underlying variables are often unknown. On simulated data we\nshow that generative hidden variables can be captured in learned\nrepresentations in the form of superpositions. The degree to which they are\nlearned depends on the completeness of the representations. Superpositions,\nhowever, are not identifiable if these generative variables are unknown. SAEs\ncan to some extent recover these variables, yielding interpretable features.\nApplied to single-cell multi-omics data, we show that an SAE can uncover key\nbiological processes such as carbon dioxide transport and ion homeostasis,\nwhich are crucial for red blood cell differentiation and immune function. Our\nfindings highlight how SAEs can be used in advancing interpretability in\nbiological and other scientific domains.\n", "link": "http://arxiv.org/abs/2410.11468v1", "date": "2024-10-15", "relevancy": 2.4601, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20sparse%20autoencoders%20make%20sense%20of%20latent%20representations%3F&body=Title%3A%20Can%20sparse%20autoencoders%20make%20sense%20of%20latent%20representations%3F%0AAuthor%3A%20Viktoria%20Schuster%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20lately%20been%20used%20to%20uncover%20interpretable%0Alatent%20features%20in%20large%20language%20models.%20Here%2C%20we%20explore%20their%20potential%20for%0Adecomposing%20latent%20representations%20in%20complex%20and%20high-dimensional%20biological%0Adata%2C%20where%20the%20underlying%20variables%20are%20often%20unknown.%20On%20simulated%20data%20we%0Ashow%20that%20generative%20hidden%20variables%20can%20be%20captured%20in%20learned%0Arepresentations%20in%20the%20form%20of%20superpositions.%20The%20degree%20to%20which%20they%20are%0Alearned%20depends%20on%20the%20completeness%20of%20the%20representations.%20Superpositions%2C%0Ahowever%2C%20are%20not%20identifiable%20if%20these%20generative%20variables%20are%20unknown.%20SAEs%0Acan%20to%20some%20extent%20recover%20these%20variables%2C%20yielding%20interpretable%20features.%0AApplied%20to%20single-cell%20multi-omics%20data%2C%20we%20show%20that%20an%20SAE%20can%20uncover%20key%0Abiological%20processes%20such%20as%20carbon%20dioxide%20transport%20and%20ion%20homeostasis%2C%0Awhich%20are%20crucial%20for%20red%20blood%20cell%20differentiation%20and%20immune%20function.%20Our%0Afindings%20highlight%20how%20SAEs%20can%20be%20used%20in%20advancing%20interpretability%20in%0Abiological%20and%20other%20scientific%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520sparse%2520autoencoders%2520make%2520sense%2520of%2520latent%2520representations%253F%26entry.906535625%3DViktoria%2520Schuster%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520lately%2520been%2520used%2520to%2520uncover%2520interpretable%250Alatent%2520features%2520in%2520large%2520language%2520models.%2520Here%252C%2520we%2520explore%2520their%2520potential%2520for%250Adecomposing%2520latent%2520representations%2520in%2520complex%2520and%2520high-dimensional%2520biological%250Adata%252C%2520where%2520the%2520underlying%2520variables%2520are%2520often%2520unknown.%2520On%2520simulated%2520data%2520we%250Ashow%2520that%2520generative%2520hidden%2520variables%2520can%2520be%2520captured%2520in%2520learned%250Arepresentations%2520in%2520the%2520form%2520of%2520superpositions.%2520The%2520degree%2520to%2520which%2520they%2520are%250Alearned%2520depends%2520on%2520the%2520completeness%2520of%2520the%2520representations.%2520Superpositions%252C%250Ahowever%252C%2520are%2520not%2520identifiable%2520if%2520these%2520generative%2520variables%2520are%2520unknown.%2520SAEs%250Acan%2520to%2520some%2520extent%2520recover%2520these%2520variables%252C%2520yielding%2520interpretable%2520features.%250AApplied%2520to%2520single-cell%2520multi-omics%2520data%252C%2520we%2520show%2520that%2520an%2520SAE%2520can%2520uncover%2520key%250Abiological%2520processes%2520such%2520as%2520carbon%2520dioxide%2520transport%2520and%2520ion%2520homeostasis%252C%250Awhich%2520are%2520crucial%2520for%2520red%2520blood%2520cell%2520differentiation%2520and%2520immune%2520function.%2520Our%250Afindings%2520highlight%2520how%2520SAEs%2520can%2520be%2520used%2520in%2520advancing%2520interpretability%2520in%250Abiological%2520and%2520other%2520scientific%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20sparse%20autoencoders%20make%20sense%20of%20latent%20representations%3F&entry.906535625=Viktoria%20Schuster&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20lately%20been%20used%20to%20uncover%20interpretable%0Alatent%20features%20in%20large%20language%20models.%20Here%2C%20we%20explore%20their%20potential%20for%0Adecomposing%20latent%20representations%20in%20complex%20and%20high-dimensional%20biological%0Adata%2C%20where%20the%20underlying%20variables%20are%20often%20unknown.%20On%20simulated%20data%20we%0Ashow%20that%20generative%20hidden%20variables%20can%20be%20captured%20in%20learned%0Arepresentations%20in%20the%20form%20of%20superpositions.%20The%20degree%20to%20which%20they%20are%0Alearned%20depends%20on%20the%20completeness%20of%20the%20representations.%20Superpositions%2C%0Ahowever%2C%20are%20not%20identifiable%20if%20these%20generative%20variables%20are%20unknown.%20SAEs%0Acan%20to%20some%20extent%20recover%20these%20variables%2C%20yielding%20interpretable%20features.%0AApplied%20to%20single-cell%20multi-omics%20data%2C%20we%20show%20that%20an%20SAE%20can%20uncover%20key%0Abiological%20processes%20such%20as%20carbon%20dioxide%20transport%20and%20ion%20homeostasis%2C%0Awhich%20are%20crucial%20for%20red%20blood%20cell%20differentiation%20and%20immune%20function.%20Our%0Afindings%20highlight%20how%20SAEs%20can%20be%20used%20in%20advancing%20interpretability%20in%0Abiological%20and%20other%20scientific%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11468v1&entry.124074799=Read"},
{"title": "A Novel Gaussian Min-Max Theorem and its Applications", "author": "Danil Akhtiamov and David Bosch and Reza Ghane and K Nithin Varma and Babak Hassibi", "abstract": "  A celebrated result by Gordon allows one to compare the min-max behavior of\ntwo Gaussian processes if certain inequality conditions are met. The\nconsequences of this result include the Gaussian min-max (GMT) and convex\nGaussian min-max (CGMT) theorems which have had far-reaching implications in\nhigh-dimensional statistics, machine learning, non-smooth optimization, and\nsignal processing. Both theorems rely on a pair of Gaussian processes, first\nidentified by Slepian, that satisfy Gordon's comparison inequalities. In this\npaper, we identify such a new pair. The resulting theorems extend the classical\nGMT and CGMT Theorems from the case where the underlying Gaussian matrix in the\nprimary process has iid rows to where it has independent but\nnon-identically-distributed ones. The new CGMT is applied to the problems of\nmulti-source Gaussian regression, as well as to binary classification of\ngeneral Gaussian mixture models.\n", "link": "http://arxiv.org/abs/2402.07356v3", "date": "2024-10-15", "relevancy": 2.4589, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4924}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4923}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Gaussian%20Min-Max%20Theorem%20and%20its%20Applications&body=Title%3A%20A%20Novel%20Gaussian%20Min-Max%20Theorem%20and%20its%20Applications%0AAuthor%3A%20Danil%20Akhtiamov%20and%20David%20Bosch%20and%20Reza%20Ghane%20and%20K%20Nithin%20Varma%20and%20Babak%20Hassibi%0AAbstract%3A%20%20%20A%20celebrated%20result%20by%20Gordon%20allows%20one%20to%20compare%20the%20min-max%20behavior%20of%0Atwo%20Gaussian%20processes%20if%20certain%20inequality%20conditions%20are%20met.%20The%0Aconsequences%20of%20this%20result%20include%20the%20Gaussian%20min-max%20%28GMT%29%20and%20convex%0AGaussian%20min-max%20%28CGMT%29%20theorems%20which%20have%20had%20far-reaching%20implications%20in%0Ahigh-dimensional%20statistics%2C%20machine%20learning%2C%20non-smooth%20optimization%2C%20and%0Asignal%20processing.%20Both%20theorems%20rely%20on%20a%20pair%20of%20Gaussian%20processes%2C%20first%0Aidentified%20by%20Slepian%2C%20that%20satisfy%20Gordon%27s%20comparison%20inequalities.%20In%20this%0Apaper%2C%20we%20identify%20such%20a%20new%20pair.%20The%20resulting%20theorems%20extend%20the%20classical%0AGMT%20and%20CGMT%20Theorems%20from%20the%20case%20where%20the%20underlying%20Gaussian%20matrix%20in%20the%0Aprimary%20process%20has%20iid%20rows%20to%20where%20it%20has%20independent%20but%0Anon-identically-distributed%20ones.%20The%20new%20CGMT%20is%20applied%20to%20the%20problems%20of%0Amulti-source%20Gaussian%20regression%2C%20as%20well%20as%20to%20binary%20classification%20of%0Ageneral%20Gaussian%20mixture%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07356v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Gaussian%2520Min-Max%2520Theorem%2520and%2520its%2520Applications%26entry.906535625%3DDanil%2520Akhtiamov%2520and%2520David%2520Bosch%2520and%2520Reza%2520Ghane%2520and%2520K%2520Nithin%2520Varma%2520and%2520Babak%2520Hassibi%26entry.1292438233%3D%2520%2520A%2520celebrated%2520result%2520by%2520Gordon%2520allows%2520one%2520to%2520compare%2520the%2520min-max%2520behavior%2520of%250Atwo%2520Gaussian%2520processes%2520if%2520certain%2520inequality%2520conditions%2520are%2520met.%2520The%250Aconsequences%2520of%2520this%2520result%2520include%2520the%2520Gaussian%2520min-max%2520%2528GMT%2529%2520and%2520convex%250AGaussian%2520min-max%2520%2528CGMT%2529%2520theorems%2520which%2520have%2520had%2520far-reaching%2520implications%2520in%250Ahigh-dimensional%2520statistics%252C%2520machine%2520learning%252C%2520non-smooth%2520optimization%252C%2520and%250Asignal%2520processing.%2520Both%2520theorems%2520rely%2520on%2520a%2520pair%2520of%2520Gaussian%2520processes%252C%2520first%250Aidentified%2520by%2520Slepian%252C%2520that%2520satisfy%2520Gordon%2527s%2520comparison%2520inequalities.%2520In%2520this%250Apaper%252C%2520we%2520identify%2520such%2520a%2520new%2520pair.%2520The%2520resulting%2520theorems%2520extend%2520the%2520classical%250AGMT%2520and%2520CGMT%2520Theorems%2520from%2520the%2520case%2520where%2520the%2520underlying%2520Gaussian%2520matrix%2520in%2520the%250Aprimary%2520process%2520has%2520iid%2520rows%2520to%2520where%2520it%2520has%2520independent%2520but%250Anon-identically-distributed%2520ones.%2520The%2520new%2520CGMT%2520is%2520applied%2520to%2520the%2520problems%2520of%250Amulti-source%2520Gaussian%2520regression%252C%2520as%2520well%2520as%2520to%2520binary%2520classification%2520of%250Ageneral%2520Gaussian%2520mixture%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07356v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Gaussian%20Min-Max%20Theorem%20and%20its%20Applications&entry.906535625=Danil%20Akhtiamov%20and%20David%20Bosch%20and%20Reza%20Ghane%20and%20K%20Nithin%20Varma%20and%20Babak%20Hassibi&entry.1292438233=%20%20A%20celebrated%20result%20by%20Gordon%20allows%20one%20to%20compare%20the%20min-max%20behavior%20of%0Atwo%20Gaussian%20processes%20if%20certain%20inequality%20conditions%20are%20met.%20The%0Aconsequences%20of%20this%20result%20include%20the%20Gaussian%20min-max%20%28GMT%29%20and%20convex%0AGaussian%20min-max%20%28CGMT%29%20theorems%20which%20have%20had%20far-reaching%20implications%20in%0Ahigh-dimensional%20statistics%2C%20machine%20learning%2C%20non-smooth%20optimization%2C%20and%0Asignal%20processing.%20Both%20theorems%20rely%20on%20a%20pair%20of%20Gaussian%20processes%2C%20first%0Aidentified%20by%20Slepian%2C%20that%20satisfy%20Gordon%27s%20comparison%20inequalities.%20In%20this%0Apaper%2C%20we%20identify%20such%20a%20new%20pair.%20The%20resulting%20theorems%20extend%20the%20classical%0AGMT%20and%20CGMT%20Theorems%20from%20the%20case%20where%20the%20underlying%20Gaussian%20matrix%20in%20the%0Aprimary%20process%20has%20iid%20rows%20to%20where%20it%20has%20independent%20but%0Anon-identically-distributed%20ones.%20The%20new%20CGMT%20is%20applied%20to%20the%20problems%20of%0Amulti-source%20Gaussian%20regression%2C%20as%20well%20as%20to%20binary%20classification%20of%0Ageneral%20Gaussian%20mixture%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07356v3&entry.124074799=Read"},
{"title": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs", "author": "Mehdi Ali and Michael Fromm and Klaudia Thellmann and Jan Ebert and Alexander Arno Weber and Richard Rutmann and Charvi Jain and Max L\u00fcbbering and Daniel Steinigen and Johannes Leveling and Katrin Klug and Jasper Schulze Buschhoff and Lena Jurkschat and Hammam Abdelwahab and Benny J\u00f6rg Stein and Karl-Heinz Sylla and Pavel Denisov and Nicolo' Brandizzi and Qasid Saleem and Anirban Bhowmick and Lennard Helmer and Chelsea John and Pedro Ortiz Suarez and Malte Ostendorff and Alex Jude and Lalith Manjunath and Samuel Weinbach and Carolin Penke and Oleg Filatov and Shima Asaadi and Fabio Barth and Rafet Sifa and Fabian K\u00fcch and Andreas Herten and Ren\u00e9 J\u00e4kel and Georg Rehm and Stefan Kesselheim and Joachim K\u00f6hler and Nicolas Flores-Herr", "abstract": "  We present two multilingual LLMs designed to embrace Europe's linguistic\ndiversity by supporting all 24 official languages of the European Union.\nTrained on a dataset comprising around 60% non-English data and utilizing a\ncustom multilingual tokenizer, our models address the limitations of existing\nLLMs that predominantly focus on English or a few high-resource languages. We\ndetail the models' development principles, i.e., data composition, tokenizer\noptimization, and training methodologies. The models demonstrate competitive\nperformance across multilingual benchmarks, as evidenced by their performance\non European versions of ARC, HellaSwag, MMLU, and TruthfulQA.\n", "link": "http://arxiv.org/abs/2410.03730v2", "date": "2024-10-15", "relevancy": 2.4477, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teuken-7B-Base%20%26%20Teuken-7B-Instruct%3A%20Towards%20European%20LLMs&body=Title%3A%20Teuken-7B-Base%20%26%20Teuken-7B-Instruct%3A%20Towards%20European%20LLMs%0AAuthor%3A%20Mehdi%20Ali%20and%20Michael%20Fromm%20and%20Klaudia%20Thellmann%20and%20Jan%20Ebert%20and%20Alexander%20Arno%20Weber%20and%20Richard%20Rutmann%20and%20Charvi%20Jain%20and%20Max%20L%C3%BCbbering%20and%20Daniel%20Steinigen%20and%20Johannes%20Leveling%20and%20Katrin%20Klug%20and%20Jasper%20Schulze%20Buschhoff%20and%20Lena%20Jurkschat%20and%20Hammam%20Abdelwahab%20and%20Benny%20J%C3%B6rg%20Stein%20and%20Karl-Heinz%20Sylla%20and%20Pavel%20Denisov%20and%20Nicolo%27%20Brandizzi%20and%20Qasid%20Saleem%20and%20Anirban%20Bhowmick%20and%20Lennard%20Helmer%20and%20Chelsea%20John%20and%20Pedro%20Ortiz%20Suarez%20and%20Malte%20Ostendorff%20and%20Alex%20Jude%20and%20Lalith%20Manjunath%20and%20Samuel%20Weinbach%20and%20Carolin%20Penke%20and%20Oleg%20Filatov%20and%20Shima%20Asaadi%20and%20Fabio%20Barth%20and%20Rafet%20Sifa%20and%20Fabian%20K%C3%BCch%20and%20Andreas%20Herten%20and%20Ren%C3%A9%20J%C3%A4kel%20and%20Georg%20Rehm%20and%20Stefan%20Kesselheim%20and%20Joachim%20K%C3%B6hler%20and%20Nicolas%20Flores-Herr%0AAbstract%3A%20%20%20We%20present%20two%20multilingual%20LLMs%20designed%20to%20embrace%20Europe%27s%20linguistic%0Adiversity%20by%20supporting%20all%2024%20official%20languages%20of%20the%20European%20Union.%0ATrained%20on%20a%20dataset%20comprising%20around%2060%25%20non-English%20data%20and%20utilizing%20a%0Acustom%20multilingual%20tokenizer%2C%20our%20models%20address%20the%20limitations%20of%20existing%0ALLMs%20that%20predominantly%20focus%20on%20English%20or%20a%20few%20high-resource%20languages.%20We%0Adetail%20the%20models%27%20development%20principles%2C%20i.e.%2C%20data%20composition%2C%20tokenizer%0Aoptimization%2C%20and%20training%20methodologies.%20The%20models%20demonstrate%20competitive%0Aperformance%20across%20multilingual%20benchmarks%2C%20as%20evidenced%20by%20their%20performance%0Aon%20European%20versions%20of%20ARC%2C%20HellaSwag%2C%20MMLU%2C%20and%20TruthfulQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeuken-7B-Base%2520%2526%2520Teuken-7B-Instruct%253A%2520Towards%2520European%2520LLMs%26entry.906535625%3DMehdi%2520Ali%2520and%2520Michael%2520Fromm%2520and%2520Klaudia%2520Thellmann%2520and%2520Jan%2520Ebert%2520and%2520Alexander%2520Arno%2520Weber%2520and%2520Richard%2520Rutmann%2520and%2520Charvi%2520Jain%2520and%2520Max%2520L%25C3%25BCbbering%2520and%2520Daniel%2520Steinigen%2520and%2520Johannes%2520Leveling%2520and%2520Katrin%2520Klug%2520and%2520Jasper%2520Schulze%2520Buschhoff%2520and%2520Lena%2520Jurkschat%2520and%2520Hammam%2520Abdelwahab%2520and%2520Benny%2520J%25C3%25B6rg%2520Stein%2520and%2520Karl-Heinz%2520Sylla%2520and%2520Pavel%2520Denisov%2520and%2520Nicolo%2527%2520Brandizzi%2520and%2520Qasid%2520Saleem%2520and%2520Anirban%2520Bhowmick%2520and%2520Lennard%2520Helmer%2520and%2520Chelsea%2520John%2520and%2520Pedro%2520Ortiz%2520Suarez%2520and%2520Malte%2520Ostendorff%2520and%2520Alex%2520Jude%2520and%2520Lalith%2520Manjunath%2520and%2520Samuel%2520Weinbach%2520and%2520Carolin%2520Penke%2520and%2520Oleg%2520Filatov%2520and%2520Shima%2520Asaadi%2520and%2520Fabio%2520Barth%2520and%2520Rafet%2520Sifa%2520and%2520Fabian%2520K%25C3%25BCch%2520and%2520Andreas%2520Herten%2520and%2520Ren%25C3%25A9%2520J%25C3%25A4kel%2520and%2520Georg%2520Rehm%2520and%2520Stefan%2520Kesselheim%2520and%2520Joachim%2520K%25C3%25B6hler%2520and%2520Nicolas%2520Flores-Herr%26entry.1292438233%3D%2520%2520We%2520present%2520two%2520multilingual%2520LLMs%2520designed%2520to%2520embrace%2520Europe%2527s%2520linguistic%250Adiversity%2520by%2520supporting%2520all%252024%2520official%2520languages%2520of%2520the%2520European%2520Union.%250ATrained%2520on%2520a%2520dataset%2520comprising%2520around%252060%2525%2520non-English%2520data%2520and%2520utilizing%2520a%250Acustom%2520multilingual%2520tokenizer%252C%2520our%2520models%2520address%2520the%2520limitations%2520of%2520existing%250ALLMs%2520that%2520predominantly%2520focus%2520on%2520English%2520or%2520a%2520few%2520high-resource%2520languages.%2520We%250Adetail%2520the%2520models%2527%2520development%2520principles%252C%2520i.e.%252C%2520data%2520composition%252C%2520tokenizer%250Aoptimization%252C%2520and%2520training%2520methodologies.%2520The%2520models%2520demonstrate%2520competitive%250Aperformance%2520across%2520multilingual%2520benchmarks%252C%2520as%2520evidenced%2520by%2520their%2520performance%250Aon%2520European%2520versions%2520of%2520ARC%252C%2520HellaSwag%252C%2520MMLU%252C%2520and%2520TruthfulQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teuken-7B-Base%20%26%20Teuken-7B-Instruct%3A%20Towards%20European%20LLMs&entry.906535625=Mehdi%20Ali%20and%20Michael%20Fromm%20and%20Klaudia%20Thellmann%20and%20Jan%20Ebert%20and%20Alexander%20Arno%20Weber%20and%20Richard%20Rutmann%20and%20Charvi%20Jain%20and%20Max%20L%C3%BCbbering%20and%20Daniel%20Steinigen%20and%20Johannes%20Leveling%20and%20Katrin%20Klug%20and%20Jasper%20Schulze%20Buschhoff%20and%20Lena%20Jurkschat%20and%20Hammam%20Abdelwahab%20and%20Benny%20J%C3%B6rg%20Stein%20and%20Karl-Heinz%20Sylla%20and%20Pavel%20Denisov%20and%20Nicolo%27%20Brandizzi%20and%20Qasid%20Saleem%20and%20Anirban%20Bhowmick%20and%20Lennard%20Helmer%20and%20Chelsea%20John%20and%20Pedro%20Ortiz%20Suarez%20and%20Malte%20Ostendorff%20and%20Alex%20Jude%20and%20Lalith%20Manjunath%20and%20Samuel%20Weinbach%20and%20Carolin%20Penke%20and%20Oleg%20Filatov%20and%20Shima%20Asaadi%20and%20Fabio%20Barth%20and%20Rafet%20Sifa%20and%20Fabian%20K%C3%BCch%20and%20Andreas%20Herten%20and%20Ren%C3%A9%20J%C3%A4kel%20and%20Georg%20Rehm%20and%20Stefan%20Kesselheim%20and%20Joachim%20K%C3%B6hler%20and%20Nicolas%20Flores-Herr&entry.1292438233=%20%20We%20present%20two%20multilingual%20LLMs%20designed%20to%20embrace%20Europe%27s%20linguistic%0Adiversity%20by%20supporting%20all%2024%20official%20languages%20of%20the%20European%20Union.%0ATrained%20on%20a%20dataset%20comprising%20around%2060%25%20non-English%20data%20and%20utilizing%20a%0Acustom%20multilingual%20tokenizer%2C%20our%20models%20address%20the%20limitations%20of%20existing%0ALLMs%20that%20predominantly%20focus%20on%20English%20or%20a%20few%20high-resource%20languages.%20We%0Adetail%20the%20models%27%20development%20principles%2C%20i.e.%2C%20data%20composition%2C%20tokenizer%0Aoptimization%2C%20and%20training%20methodologies.%20The%20models%20demonstrate%20competitive%0Aperformance%20across%20multilingual%20benchmarks%2C%20as%20evidenced%20by%20their%20performance%0Aon%20European%20versions%20of%20ARC%2C%20HellaSwag%2C%20MMLU%2C%20and%20TruthfulQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03730v2&entry.124074799=Read"},
{"title": "Why Go Full? Elevating Federated Learning Through Partial Network\n  Updates", "author": "Haolin Wang and Xuefeng Liu and Jianwei Niu and Wenkai Guo and Shaojie Tang", "abstract": "  Federated learning is a distributed machine learning paradigm designed to\nprotect user data privacy, which has been successfully implemented across\nvarious scenarios. In traditional federated learning, the entire parameter set\nof local models is updated and averaged in each training round. Although this\nfull network update method maximizes knowledge acquisition and sharing for each\nmodel layer, it prevents the layers of the global model from cooperating\neffectively to complete the tasks of each client, a challenge we refer to as\nlayer mismatch. This mismatch problem recurs after every parameter averaging,\nconsequently slowing down model convergence and degrading overall performance.\nTo address the layer mismatch issue, we introduce the FedPart method, which\nrestricts model updates to either a single layer or a few layers during each\ncommunication round. Furthermore, to maintain the efficiency of knowledge\nacquisition and sharing, we develop several strategies to select trainable\nlayers in each round, including sequential updating and multi-round cycle\ntraining. Through both theoretical analysis and experiments, our findings\ndemonstrate that the FedPart method significantly surpasses conventional full\nnetwork update strategies in terms of convergence speed and accuracy, while\nalso reducing communication and computational overheads.\n", "link": "http://arxiv.org/abs/2410.11559v1", "date": "2024-10-15", "relevancy": 2.4468, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.501}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4895}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Go%20Full%3F%20Elevating%20Federated%20Learning%20Through%20Partial%20Network%0A%20%20Updates&body=Title%3A%20Why%20Go%20Full%3F%20Elevating%20Federated%20Learning%20Through%20Partial%20Network%0A%20%20Updates%0AAuthor%3A%20Haolin%20Wang%20and%20Xuefeng%20Liu%20and%20Jianwei%20Niu%20and%20Wenkai%20Guo%20and%20Shaojie%20Tang%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20distributed%20machine%20learning%20paradigm%20designed%20to%0Aprotect%20user%20data%20privacy%2C%20which%20has%20been%20successfully%20implemented%20across%0Avarious%20scenarios.%20In%20traditional%20federated%20learning%2C%20the%20entire%20parameter%20set%0Aof%20local%20models%20is%20updated%20and%20averaged%20in%20each%20training%20round.%20Although%20this%0Afull%20network%20update%20method%20maximizes%20knowledge%20acquisition%20and%20sharing%20for%20each%0Amodel%20layer%2C%20it%20prevents%20the%20layers%20of%20the%20global%20model%20from%20cooperating%0Aeffectively%20to%20complete%20the%20tasks%20of%20each%20client%2C%20a%20challenge%20we%20refer%20to%20as%0Alayer%20mismatch.%20This%20mismatch%20problem%20recurs%20after%20every%20parameter%20averaging%2C%0Aconsequently%20slowing%20down%20model%20convergence%20and%20degrading%20overall%20performance.%0ATo%20address%20the%20layer%20mismatch%20issue%2C%20we%20introduce%20the%20FedPart%20method%2C%20which%0Arestricts%20model%20updates%20to%20either%20a%20single%20layer%20or%20a%20few%20layers%20during%20each%0Acommunication%20round.%20Furthermore%2C%20to%20maintain%20the%20efficiency%20of%20knowledge%0Aacquisition%20and%20sharing%2C%20we%20develop%20several%20strategies%20to%20select%20trainable%0Alayers%20in%20each%20round%2C%20including%20sequential%20updating%20and%20multi-round%20cycle%0Atraining.%20Through%20both%20theoretical%20analysis%20and%20experiments%2C%20our%20findings%0Ademonstrate%20that%20the%20FedPart%20method%20significantly%20surpasses%20conventional%20full%0Anetwork%20update%20strategies%20in%20terms%20of%20convergence%20speed%20and%20accuracy%2C%20while%0Aalso%20reducing%20communication%20and%20computational%20overheads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Go%2520Full%253F%2520Elevating%2520Federated%2520Learning%2520Through%2520Partial%2520Network%250A%2520%2520Updates%26entry.906535625%3DHaolin%2520Wang%2520and%2520Xuefeng%2520Liu%2520and%2520Jianwei%2520Niu%2520and%2520Wenkai%2520Guo%2520and%2520Shaojie%2520Tang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520distributed%2520machine%2520learning%2520paradigm%2520designed%2520to%250Aprotect%2520user%2520data%2520privacy%252C%2520which%2520has%2520been%2520successfully%2520implemented%2520across%250Avarious%2520scenarios.%2520In%2520traditional%2520federated%2520learning%252C%2520the%2520entire%2520parameter%2520set%250Aof%2520local%2520models%2520is%2520updated%2520and%2520averaged%2520in%2520each%2520training%2520round.%2520Although%2520this%250Afull%2520network%2520update%2520method%2520maximizes%2520knowledge%2520acquisition%2520and%2520sharing%2520for%2520each%250Amodel%2520layer%252C%2520it%2520prevents%2520the%2520layers%2520of%2520the%2520global%2520model%2520from%2520cooperating%250Aeffectively%2520to%2520complete%2520the%2520tasks%2520of%2520each%2520client%252C%2520a%2520challenge%2520we%2520refer%2520to%2520as%250Alayer%2520mismatch.%2520This%2520mismatch%2520problem%2520recurs%2520after%2520every%2520parameter%2520averaging%252C%250Aconsequently%2520slowing%2520down%2520model%2520convergence%2520and%2520degrading%2520overall%2520performance.%250ATo%2520address%2520the%2520layer%2520mismatch%2520issue%252C%2520we%2520introduce%2520the%2520FedPart%2520method%252C%2520which%250Arestricts%2520model%2520updates%2520to%2520either%2520a%2520single%2520layer%2520or%2520a%2520few%2520layers%2520during%2520each%250Acommunication%2520round.%2520Furthermore%252C%2520to%2520maintain%2520the%2520efficiency%2520of%2520knowledge%250Aacquisition%2520and%2520sharing%252C%2520we%2520develop%2520several%2520strategies%2520to%2520select%2520trainable%250Alayers%2520in%2520each%2520round%252C%2520including%2520sequential%2520updating%2520and%2520multi-round%2520cycle%250Atraining.%2520Through%2520both%2520theoretical%2520analysis%2520and%2520experiments%252C%2520our%2520findings%250Ademonstrate%2520that%2520the%2520FedPart%2520method%2520significantly%2520surpasses%2520conventional%2520full%250Anetwork%2520update%2520strategies%2520in%2520terms%2520of%2520convergence%2520speed%2520and%2520accuracy%252C%2520while%250Aalso%2520reducing%2520communication%2520and%2520computational%2520overheads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Go%20Full%3F%20Elevating%20Federated%20Learning%20Through%20Partial%20Network%0A%20%20Updates&entry.906535625=Haolin%20Wang%20and%20Xuefeng%20Liu%20and%20Jianwei%20Niu%20and%20Wenkai%20Guo%20and%20Shaojie%20Tang&entry.1292438233=%20%20Federated%20learning%20is%20a%20distributed%20machine%20learning%20paradigm%20designed%20to%0Aprotect%20user%20data%20privacy%2C%20which%20has%20been%20successfully%20implemented%20across%0Avarious%20scenarios.%20In%20traditional%20federated%20learning%2C%20the%20entire%20parameter%20set%0Aof%20local%20models%20is%20updated%20and%20averaged%20in%20each%20training%20round.%20Although%20this%0Afull%20network%20update%20method%20maximizes%20knowledge%20acquisition%20and%20sharing%20for%20each%0Amodel%20layer%2C%20it%20prevents%20the%20layers%20of%20the%20global%20model%20from%20cooperating%0Aeffectively%20to%20complete%20the%20tasks%20of%20each%20client%2C%20a%20challenge%20we%20refer%20to%20as%0Alayer%20mismatch.%20This%20mismatch%20problem%20recurs%20after%20every%20parameter%20averaging%2C%0Aconsequently%20slowing%20down%20model%20convergence%20and%20degrading%20overall%20performance.%0ATo%20address%20the%20layer%20mismatch%20issue%2C%20we%20introduce%20the%20FedPart%20method%2C%20which%0Arestricts%20model%20updates%20to%20either%20a%20single%20layer%20or%20a%20few%20layers%20during%20each%0Acommunication%20round.%20Furthermore%2C%20to%20maintain%20the%20efficiency%20of%20knowledge%0Aacquisition%20and%20sharing%2C%20we%20develop%20several%20strategies%20to%20select%20trainable%0Alayers%20in%20each%20round%2C%20including%20sequential%20updating%20and%20multi-round%20cycle%0Atraining.%20Through%20both%20theoretical%20analysis%20and%20experiments%2C%20our%20findings%0Ademonstrate%20that%20the%20FedPart%20method%20significantly%20surpasses%20conventional%20full%0Anetwork%20update%20strategies%20in%20terms%20of%20convergence%20speed%20and%20accuracy%2C%20while%0Aalso%20reducing%20communication%20and%20computational%20overheads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11559v1&entry.124074799=Read"},
{"title": "Latent BKI: Open-Dictionary Continuous Mapping in Visual-Language Latent\n  Spaces with Quantifiable Uncertainty", "author": "Joey Wilson and Ruihan Xu and Yile Sun and Parker Ewen and Minghan Zhu and Kira Barton and Maani Ghaffari", "abstract": "  This paper introduces a novel probabilistic mapping algorithm, Latent BKI,\nwhich enables open-vocabulary mapping with quantifiable uncertainty.\nTraditionally, semantic mapping algorithms focus on a fixed set of semantic\ncategories which limits their applicability for complex robotic tasks.\nVision-Language (VL) models have recently emerged as a technique to jointly\nmodel language and visual features in a latent space, enabling semantic\nrecognition beyond a predefined, fixed set of semantic classes. Latent BKI\nrecurrently incorporates neural embeddings from VL models into a voxel map with\nquantifiable uncertainty, leveraging the spatial correlations of nearby\nobservations through Bayesian Kernel Inference (BKI). Latent BKI is evaluated\nagainst similar explicit semantic mapping and VL mapping frameworks on the\npopular MatterPort-3D and Semantic KITTI data sets, demonstrating that Latent\nBKI maintains the probabilistic benefits of continuous mapping with the\nadditional benefit of open-dictionary queries. Real-world experiments\ndemonstrate applicability to challenging indoor environments.\n", "link": "http://arxiv.org/abs/2410.11783v1", "date": "2024-10-15", "relevancy": 2.4444, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6331}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20BKI%3A%20Open-Dictionary%20Continuous%20Mapping%20in%20Visual-Language%20Latent%0A%20%20Spaces%20with%20Quantifiable%20Uncertainty&body=Title%3A%20Latent%20BKI%3A%20Open-Dictionary%20Continuous%20Mapping%20in%20Visual-Language%20Latent%0A%20%20Spaces%20with%20Quantifiable%20Uncertainty%0AAuthor%3A%20Joey%20Wilson%20and%20Ruihan%20Xu%20and%20Yile%20Sun%20and%20Parker%20Ewen%20and%20Minghan%20Zhu%20and%20Kira%20Barton%20and%20Maani%20Ghaffari%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20probabilistic%20mapping%20algorithm%2C%20Latent%20BKI%2C%0Awhich%20enables%20open-vocabulary%20mapping%20with%20quantifiable%20uncertainty.%0ATraditionally%2C%20semantic%20mapping%20algorithms%20focus%20on%20a%20fixed%20set%20of%20semantic%0Acategories%20which%20limits%20their%20applicability%20for%20complex%20robotic%20tasks.%0AVision-Language%20%28VL%29%20models%20have%20recently%20emerged%20as%20a%20technique%20to%20jointly%0Amodel%20language%20and%20visual%20features%20in%20a%20latent%20space%2C%20enabling%20semantic%0Arecognition%20beyond%20a%20predefined%2C%20fixed%20set%20of%20semantic%20classes.%20Latent%20BKI%0Arecurrently%20incorporates%20neural%20embeddings%20from%20VL%20models%20into%20a%20voxel%20map%20with%0Aquantifiable%20uncertainty%2C%20leveraging%20the%20spatial%20correlations%20of%20nearby%0Aobservations%20through%20Bayesian%20Kernel%20Inference%20%28BKI%29.%20Latent%20BKI%20is%20evaluated%0Aagainst%20similar%20explicit%20semantic%20mapping%20and%20VL%20mapping%20frameworks%20on%20the%0Apopular%20MatterPort-3D%20and%20Semantic%20KITTI%20data%20sets%2C%20demonstrating%20that%20Latent%0ABKI%20maintains%20the%20probabilistic%20benefits%20of%20continuous%20mapping%20with%20the%0Aadditional%20benefit%20of%20open-dictionary%20queries.%20Real-world%20experiments%0Ademonstrate%20applicability%20to%20challenging%20indoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520BKI%253A%2520Open-Dictionary%2520Continuous%2520Mapping%2520in%2520Visual-Language%2520Latent%250A%2520%2520Spaces%2520with%2520Quantifiable%2520Uncertainty%26entry.906535625%3DJoey%2520Wilson%2520and%2520Ruihan%2520Xu%2520and%2520Yile%2520Sun%2520and%2520Parker%2520Ewen%2520and%2520Minghan%2520Zhu%2520and%2520Kira%2520Barton%2520and%2520Maani%2520Ghaffari%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520probabilistic%2520mapping%2520algorithm%252C%2520Latent%2520BKI%252C%250Awhich%2520enables%2520open-vocabulary%2520mapping%2520with%2520quantifiable%2520uncertainty.%250ATraditionally%252C%2520semantic%2520mapping%2520algorithms%2520focus%2520on%2520a%2520fixed%2520set%2520of%2520semantic%250Acategories%2520which%2520limits%2520their%2520applicability%2520for%2520complex%2520robotic%2520tasks.%250AVision-Language%2520%2528VL%2529%2520models%2520have%2520recently%2520emerged%2520as%2520a%2520technique%2520to%2520jointly%250Amodel%2520language%2520and%2520visual%2520features%2520in%2520a%2520latent%2520space%252C%2520enabling%2520semantic%250Arecognition%2520beyond%2520a%2520predefined%252C%2520fixed%2520set%2520of%2520semantic%2520classes.%2520Latent%2520BKI%250Arecurrently%2520incorporates%2520neural%2520embeddings%2520from%2520VL%2520models%2520into%2520a%2520voxel%2520map%2520with%250Aquantifiable%2520uncertainty%252C%2520leveraging%2520the%2520spatial%2520correlations%2520of%2520nearby%250Aobservations%2520through%2520Bayesian%2520Kernel%2520Inference%2520%2528BKI%2529.%2520Latent%2520BKI%2520is%2520evaluated%250Aagainst%2520similar%2520explicit%2520semantic%2520mapping%2520and%2520VL%2520mapping%2520frameworks%2520on%2520the%250Apopular%2520MatterPort-3D%2520and%2520Semantic%2520KITTI%2520data%2520sets%252C%2520demonstrating%2520that%2520Latent%250ABKI%2520maintains%2520the%2520probabilistic%2520benefits%2520of%2520continuous%2520mapping%2520with%2520the%250Aadditional%2520benefit%2520of%2520open-dictionary%2520queries.%2520Real-world%2520experiments%250Ademonstrate%2520applicability%2520to%2520challenging%2520indoor%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20BKI%3A%20Open-Dictionary%20Continuous%20Mapping%20in%20Visual-Language%20Latent%0A%20%20Spaces%20with%20Quantifiable%20Uncertainty&entry.906535625=Joey%20Wilson%20and%20Ruihan%20Xu%20and%20Yile%20Sun%20and%20Parker%20Ewen%20and%20Minghan%20Zhu%20and%20Kira%20Barton%20and%20Maani%20Ghaffari&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20probabilistic%20mapping%20algorithm%2C%20Latent%20BKI%2C%0Awhich%20enables%20open-vocabulary%20mapping%20with%20quantifiable%20uncertainty.%0ATraditionally%2C%20semantic%20mapping%20algorithms%20focus%20on%20a%20fixed%20set%20of%20semantic%0Acategories%20which%20limits%20their%20applicability%20for%20complex%20robotic%20tasks.%0AVision-Language%20%28VL%29%20models%20have%20recently%20emerged%20as%20a%20technique%20to%20jointly%0Amodel%20language%20and%20visual%20features%20in%20a%20latent%20space%2C%20enabling%20semantic%0Arecognition%20beyond%20a%20predefined%2C%20fixed%20set%20of%20semantic%20classes.%20Latent%20BKI%0Arecurrently%20incorporates%20neural%20embeddings%20from%20VL%20models%20into%20a%20voxel%20map%20with%0Aquantifiable%20uncertainty%2C%20leveraging%20the%20spatial%20correlations%20of%20nearby%0Aobservations%20through%20Bayesian%20Kernel%20Inference%20%28BKI%29.%20Latent%20BKI%20is%20evaluated%0Aagainst%20similar%20explicit%20semantic%20mapping%20and%20VL%20mapping%20frameworks%20on%20the%0Apopular%20MatterPort-3D%20and%20Semantic%20KITTI%20data%20sets%2C%20demonstrating%20that%20Latent%0ABKI%20maintains%20the%20probabilistic%20benefits%20of%20continuous%20mapping%20with%20the%0Aadditional%20benefit%20of%20open-dictionary%20queries.%20Real-world%20experiments%0Ademonstrate%20applicability%20to%20challenging%20indoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11783v1&entry.124074799=Read"},
{"title": "Are Large Language Models Strategic Decision Makers? A Study of\n  Performance and Bias in Two-Player Non-Zero-Sum Games", "author": "Nathan Herr and Fernando Acero and Roberta Raileanu and Mar\u00eda P\u00e9rez-Ortiz and Zhibin Li", "abstract": "  Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic decision-making abilities remain largely\nunexplored. To fully benefit from the potential of LLMs, it's essential to\nunderstand their ability to function in complex social scenarios. Game theory,\nwhich is already used to understand real-world interactions, provides a good\nframework for assessing these abilities. This work investigates the performance\nand merits of LLMs in canonical game-theoretic two-player non-zero-sum games,\nStag Hunt and Prisoner Dilemma. Our structured evaluation of GPT-3.5,\nGPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making\ndecisions in these games, are affected by at least one of the following\nsystematic biases: positional bias, payoff bias, or behavioural bias. This\nindicates that LLMs do not fully rely on logical reasoning when making these\nstrategic decisions. As a result, it was found that the LLMs' performance drops\nwhen the game configuration is misaligned with the affecting biases. When\nmisaligned, GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B show an average\nperformance drop of 32\\%, 25\\%, 34\\%, and 29\\% respectively in Stag Hunt, and\n28\\%, 16\\%, 34\\%, and 24\\% respectively in Prisoner's Dilemma. Surprisingly,\nGPT-4o (a top-performing LLM across standard benchmarks) suffers the most\nsubstantial performance drop, suggesting that newer models are not addressing\nthese issues. Interestingly, we found that a commonly used method of improving\nthe reasoning capabilities of LLMs, chain-of-thought (CoT) prompting, reduces\nthe biases in GPT-3.5, GPT-4o, and Llama-3-8B but increases the effect of the\nbias in GPT-4-Turbo, indicating that CoT alone cannot fully serve as a robust\nsolution to this problem. We perform several additional experiments, which\nprovide further insight into these observed behaviours.\n", "link": "http://arxiv.org/abs/2407.04467v3", "date": "2024-10-15", "relevancy": 2.4319, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20Strategic%20Decision%20Makers%3F%20A%20Study%20of%0A%20%20Performance%20and%20Bias%20in%20Two-Player%20Non-Zero-Sum%20Games&body=Title%3A%20Are%20Large%20Language%20Models%20Strategic%20Decision%20Makers%3F%20A%20Study%20of%0A%20%20Performance%20and%20Bias%20in%20Two-Player%20Non-Zero-Sum%20Games%0AAuthor%3A%20Nathan%20Herr%20and%20Fernando%20Acero%20and%20Roberta%20Raileanu%20and%20Mar%C3%ADa%20P%C3%A9rez-Ortiz%20and%20Zhibin%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20increasingly%20used%20in%20real-world%0Asettings%2C%20yet%20their%20strategic%20decision-making%20abilities%20remain%20largely%0Aunexplored.%20To%20fully%20benefit%20from%20the%20potential%20of%20LLMs%2C%20it%27s%20essential%20to%0Aunderstand%20their%20ability%20to%20function%20in%20complex%20social%20scenarios.%20Game%20theory%2C%0Awhich%20is%20already%20used%20to%20understand%20real-world%20interactions%2C%20provides%20a%20good%0Aframework%20for%20assessing%20these%20abilities.%20This%20work%20investigates%20the%20performance%0Aand%20merits%20of%20LLMs%20in%20canonical%20game-theoretic%20two-player%20non-zero-sum%20games%2C%0AStag%20Hunt%20and%20Prisoner%20Dilemma.%20Our%20structured%20evaluation%20of%20GPT-3.5%2C%0AGPT-4-Turbo%2C%20GPT-4o%2C%20and%20Llama-3-8B%20shows%20that%20these%20models%2C%20when%20making%0Adecisions%20in%20these%20games%2C%20are%20affected%20by%20at%20least%20one%20of%20the%20following%0Asystematic%20biases%3A%20positional%20bias%2C%20payoff%20bias%2C%20or%20behavioural%20bias.%20This%0Aindicates%20that%20LLMs%20do%20not%20fully%20rely%20on%20logical%20reasoning%20when%20making%20these%0Astrategic%20decisions.%20As%20a%20result%2C%20it%20was%20found%20that%20the%20LLMs%27%20performance%20drops%0Awhen%20the%20game%20configuration%20is%20misaligned%20with%20the%20affecting%20biases.%20When%0Amisaligned%2C%20GPT-3.5%2C%20GPT-4-Turbo%2C%20GPT-4o%2C%20and%20Llama-3-8B%20show%20an%20average%0Aperformance%20drop%20of%2032%5C%25%2C%2025%5C%25%2C%2034%5C%25%2C%20and%2029%5C%25%20respectively%20in%20Stag%20Hunt%2C%20and%0A28%5C%25%2C%2016%5C%25%2C%2034%5C%25%2C%20and%2024%5C%25%20respectively%20in%20Prisoner%27s%20Dilemma.%20Surprisingly%2C%0AGPT-4o%20%28a%20top-performing%20LLM%20across%20standard%20benchmarks%29%20suffers%20the%20most%0Asubstantial%20performance%20drop%2C%20suggesting%20that%20newer%20models%20are%20not%20addressing%0Athese%20issues.%20Interestingly%2C%20we%20found%20that%20a%20commonly%20used%20method%20of%20improving%0Athe%20reasoning%20capabilities%20of%20LLMs%2C%20chain-of-thought%20%28CoT%29%20prompting%2C%20reduces%0Athe%20biases%20in%20GPT-3.5%2C%20GPT-4o%2C%20and%20Llama-3-8B%20but%20increases%20the%20effect%20of%20the%0Abias%20in%20GPT-4-Turbo%2C%20indicating%20that%20CoT%20alone%20cannot%20fully%20serve%20as%20a%20robust%0Asolution%20to%20this%20problem.%20We%20perform%20several%20additional%20experiments%2C%20which%0Aprovide%20further%20insight%20into%20these%20observed%20behaviours.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04467v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Language%2520Models%2520Strategic%2520Decision%2520Makers%253F%2520A%2520Study%2520of%250A%2520%2520Performance%2520and%2520Bias%2520in%2520Two-Player%2520Non-Zero-Sum%2520Games%26entry.906535625%3DNathan%2520Herr%2520and%2520Fernando%2520Acero%2520and%2520Roberta%2520Raileanu%2520and%2520Mar%25C3%25ADa%2520P%25C3%25A9rez-Ortiz%2520and%2520Zhibin%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520increasingly%2520used%2520in%2520real-world%250Asettings%252C%2520yet%2520their%2520strategic%2520decision-making%2520abilities%2520remain%2520largely%250Aunexplored.%2520To%2520fully%2520benefit%2520from%2520the%2520potential%2520of%2520LLMs%252C%2520it%2527s%2520essential%2520to%250Aunderstand%2520their%2520ability%2520to%2520function%2520in%2520complex%2520social%2520scenarios.%2520Game%2520theory%252C%250Awhich%2520is%2520already%2520used%2520to%2520understand%2520real-world%2520interactions%252C%2520provides%2520a%2520good%250Aframework%2520for%2520assessing%2520these%2520abilities.%2520This%2520work%2520investigates%2520the%2520performance%250Aand%2520merits%2520of%2520LLMs%2520in%2520canonical%2520game-theoretic%2520two-player%2520non-zero-sum%2520games%252C%250AStag%2520Hunt%2520and%2520Prisoner%2520Dilemma.%2520Our%2520structured%2520evaluation%2520of%2520GPT-3.5%252C%250AGPT-4-Turbo%252C%2520GPT-4o%252C%2520and%2520Llama-3-8B%2520shows%2520that%2520these%2520models%252C%2520when%2520making%250Adecisions%2520in%2520these%2520games%252C%2520are%2520affected%2520by%2520at%2520least%2520one%2520of%2520the%2520following%250Asystematic%2520biases%253A%2520positional%2520bias%252C%2520payoff%2520bias%252C%2520or%2520behavioural%2520bias.%2520This%250Aindicates%2520that%2520LLMs%2520do%2520not%2520fully%2520rely%2520on%2520logical%2520reasoning%2520when%2520making%2520these%250Astrategic%2520decisions.%2520As%2520a%2520result%252C%2520it%2520was%2520found%2520that%2520the%2520LLMs%2527%2520performance%2520drops%250Awhen%2520the%2520game%2520configuration%2520is%2520misaligned%2520with%2520the%2520affecting%2520biases.%2520When%250Amisaligned%252C%2520GPT-3.5%252C%2520GPT-4-Turbo%252C%2520GPT-4o%252C%2520and%2520Llama-3-8B%2520show%2520an%2520average%250Aperformance%2520drop%2520of%252032%255C%2525%252C%252025%255C%2525%252C%252034%255C%2525%252C%2520and%252029%255C%2525%2520respectively%2520in%2520Stag%2520Hunt%252C%2520and%250A28%255C%2525%252C%252016%255C%2525%252C%252034%255C%2525%252C%2520and%252024%255C%2525%2520respectively%2520in%2520Prisoner%2527s%2520Dilemma.%2520Surprisingly%252C%250AGPT-4o%2520%2528a%2520top-performing%2520LLM%2520across%2520standard%2520benchmarks%2529%2520suffers%2520the%2520most%250Asubstantial%2520performance%2520drop%252C%2520suggesting%2520that%2520newer%2520models%2520are%2520not%2520addressing%250Athese%2520issues.%2520Interestingly%252C%2520we%2520found%2520that%2520a%2520commonly%2520used%2520method%2520of%2520improving%250Athe%2520reasoning%2520capabilities%2520of%2520LLMs%252C%2520chain-of-thought%2520%2528CoT%2529%2520prompting%252C%2520reduces%250Athe%2520biases%2520in%2520GPT-3.5%252C%2520GPT-4o%252C%2520and%2520Llama-3-8B%2520but%2520increases%2520the%2520effect%2520of%2520the%250Abias%2520in%2520GPT-4-Turbo%252C%2520indicating%2520that%2520CoT%2520alone%2520cannot%2520fully%2520serve%2520as%2520a%2520robust%250Asolution%2520to%2520this%2520problem.%2520We%2520perform%2520several%2520additional%2520experiments%252C%2520which%250Aprovide%2520further%2520insight%2520into%2520these%2520observed%2520behaviours.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04467v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20Strategic%20Decision%20Makers%3F%20A%20Study%20of%0A%20%20Performance%20and%20Bias%20in%20Two-Player%20Non-Zero-Sum%20Games&entry.906535625=Nathan%20Herr%20and%20Fernando%20Acero%20and%20Roberta%20Raileanu%20and%20Mar%C3%ADa%20P%C3%A9rez-Ortiz%20and%20Zhibin%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20increasingly%20used%20in%20real-world%0Asettings%2C%20yet%20their%20strategic%20decision-making%20abilities%20remain%20largely%0Aunexplored.%20To%20fully%20benefit%20from%20the%20potential%20of%20LLMs%2C%20it%27s%20essential%20to%0Aunderstand%20their%20ability%20to%20function%20in%20complex%20social%20scenarios.%20Game%20theory%2C%0Awhich%20is%20already%20used%20to%20understand%20real-world%20interactions%2C%20provides%20a%20good%0Aframework%20for%20assessing%20these%20abilities.%20This%20work%20investigates%20the%20performance%0Aand%20merits%20of%20LLMs%20in%20canonical%20game-theoretic%20two-player%20non-zero-sum%20games%2C%0AStag%20Hunt%20and%20Prisoner%20Dilemma.%20Our%20structured%20evaluation%20of%20GPT-3.5%2C%0AGPT-4-Turbo%2C%20GPT-4o%2C%20and%20Llama-3-8B%20shows%20that%20these%20models%2C%20when%20making%0Adecisions%20in%20these%20games%2C%20are%20affected%20by%20at%20least%20one%20of%20the%20following%0Asystematic%20biases%3A%20positional%20bias%2C%20payoff%20bias%2C%20or%20behavioural%20bias.%20This%0Aindicates%20that%20LLMs%20do%20not%20fully%20rely%20on%20logical%20reasoning%20when%20making%20these%0Astrategic%20decisions.%20As%20a%20result%2C%20it%20was%20found%20that%20the%20LLMs%27%20performance%20drops%0Awhen%20the%20game%20configuration%20is%20misaligned%20with%20the%20affecting%20biases.%20When%0Amisaligned%2C%20GPT-3.5%2C%20GPT-4-Turbo%2C%20GPT-4o%2C%20and%20Llama-3-8B%20show%20an%20average%0Aperformance%20drop%20of%2032%5C%25%2C%2025%5C%25%2C%2034%5C%25%2C%20and%2029%5C%25%20respectively%20in%20Stag%20Hunt%2C%20and%0A28%5C%25%2C%2016%5C%25%2C%2034%5C%25%2C%20and%2024%5C%25%20respectively%20in%20Prisoner%27s%20Dilemma.%20Surprisingly%2C%0AGPT-4o%20%28a%20top-performing%20LLM%20across%20standard%20benchmarks%29%20suffers%20the%20most%0Asubstantial%20performance%20drop%2C%20suggesting%20that%20newer%20models%20are%20not%20addressing%0Athese%20issues.%20Interestingly%2C%20we%20found%20that%20a%20commonly%20used%20method%20of%20improving%0Athe%20reasoning%20capabilities%20of%20LLMs%2C%20chain-of-thought%20%28CoT%29%20prompting%2C%20reduces%0Athe%20biases%20in%20GPT-3.5%2C%20GPT-4o%2C%20and%20Llama-3-8B%20but%20increases%20the%20effect%20of%20the%0Abias%20in%20GPT-4-Turbo%2C%20indicating%20that%20CoT%20alone%20cannot%20fully%20serve%20as%20a%20robust%0Asolution%20to%20this%20problem.%20We%20perform%20several%20additional%20experiments%2C%20which%0Aprovide%20further%20insight%20into%20these%20observed%20behaviours.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04467v3&entry.124074799=Read"},
{"title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?", "author": "Zhengbo Wang and Jian Liang and Ran He and Zilei Wang and Tieniu Tan", "abstract": "  Low-rank adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning of foundation models. Despite its\ncomputational efficiency, LoRA still yields inferior performance compared to\nfull fine-tuning. In this paper, we first uncover a fundamental connection\nbetween the optimization processes of LoRA and full fine-tuning: using LoRA for\noptimization is mathematically equivalent to full fine-tuning using a low-rank\ngradient for parameter updates. And this low-rank gradient can be expressed in\nterms of the gradients of the two low-rank matrices in LoRA. Leveraging this\ninsight, we introduce LoRA-Pro, a method that enhances LoRA's performance by\nstrategically adjusting the gradients of these low-rank matrices. This\nadjustment allows the low-rank gradient to more accurately approximate the full\nfine-tuning gradient, thereby narrowing the performance gap between LoRA and\nfull fine-tuning. Furthermore, we theoretically derive the optimal solutions\nfor adjusting the gradients of the low-rank matrices, applying them during\nfine-tuning in LoRA-Pro. We conduct extensive experiments across natural\nlanguage understanding, dialogue generation, mathematical reasoning, code\ngeneration, and image classification tasks, demonstrating that LoRA-Pro\nsubstantially improves LoRA's performance, effectively narrowing the gap with\nfull fine-tuning. Code is publicly available at\n\\url{https://github.com/mrflogs/LoRA-Pro}.\n", "link": "http://arxiv.org/abs/2407.18242v2", "date": "2024-10-15", "relevancy": 2.427, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-Pro%3A%20Are%20Low-Rank%20Adapters%20Properly%20Optimized%3F&body=Title%3A%20LoRA-Pro%3A%20Are%20Low-Rank%20Adapters%20Properly%20Optimized%3F%0AAuthor%3A%20Zhengbo%20Wang%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Low-rank%20adaptation%2C%20also%20known%20as%20LoRA%2C%20has%20emerged%20as%20a%20prominent%20method%0Afor%20parameter-efficient%20fine-tuning%20of%20foundation%20models.%20Despite%20its%0Acomputational%20efficiency%2C%20LoRA%20still%20yields%20inferior%20performance%20compared%20to%0Afull%20fine-tuning.%20In%20this%20paper%2C%20we%20first%20uncover%20a%20fundamental%20connection%0Abetween%20the%20optimization%20processes%20of%20LoRA%20and%20full%20fine-tuning%3A%20using%20LoRA%20for%0Aoptimization%20is%20mathematically%20equivalent%20to%20full%20fine-tuning%20using%20a%20low-rank%0Agradient%20for%20parameter%20updates.%20And%20this%20low-rank%20gradient%20can%20be%20expressed%20in%0Aterms%20of%20the%20gradients%20of%20the%20two%20low-rank%20matrices%20in%20LoRA.%20Leveraging%20this%0Ainsight%2C%20we%20introduce%20LoRA-Pro%2C%20a%20method%20that%20enhances%20LoRA%27s%20performance%20by%0Astrategically%20adjusting%20the%20gradients%20of%20these%20low-rank%20matrices.%20This%0Aadjustment%20allows%20the%20low-rank%20gradient%20to%20more%20accurately%20approximate%20the%20full%0Afine-tuning%20gradient%2C%20thereby%20narrowing%20the%20performance%20gap%20between%20LoRA%20and%0Afull%20fine-tuning.%20Furthermore%2C%20we%20theoretically%20derive%20the%20optimal%20solutions%0Afor%20adjusting%20the%20gradients%20of%20the%20low-rank%20matrices%2C%20applying%20them%20during%0Afine-tuning%20in%20LoRA-Pro.%20We%20conduct%20extensive%20experiments%20across%20natural%0Alanguage%20understanding%2C%20dialogue%20generation%2C%20mathematical%20reasoning%2C%20code%0Ageneration%2C%20and%20image%20classification%20tasks%2C%20demonstrating%20that%20LoRA-Pro%0Asubstantially%20improves%20LoRA%27s%20performance%2C%20effectively%20narrowing%20the%20gap%20with%0Afull%20fine-tuning.%20Code%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mrflogs/LoRA-Pro%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-Pro%253A%2520Are%2520Low-Rank%2520Adapters%2520Properly%2520Optimized%253F%26entry.906535625%3DZhengbo%2520Wang%2520and%2520Jian%2520Liang%2520and%2520Ran%2520He%2520and%2520Zilei%2520Wang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%252C%2520also%2520known%2520as%2520LoRA%252C%2520has%2520emerged%2520as%2520a%2520prominent%2520method%250Afor%2520parameter-efficient%2520fine-tuning%2520of%2520foundation%2520models.%2520Despite%2520its%250Acomputational%2520efficiency%252C%2520LoRA%2520still%2520yields%2520inferior%2520performance%2520compared%2520to%250Afull%2520fine-tuning.%2520In%2520this%2520paper%252C%2520we%2520first%2520uncover%2520a%2520fundamental%2520connection%250Abetween%2520the%2520optimization%2520processes%2520of%2520LoRA%2520and%2520full%2520fine-tuning%253A%2520using%2520LoRA%2520for%250Aoptimization%2520is%2520mathematically%2520equivalent%2520to%2520full%2520fine-tuning%2520using%2520a%2520low-rank%250Agradient%2520for%2520parameter%2520updates.%2520And%2520this%2520low-rank%2520gradient%2520can%2520be%2520expressed%2520in%250Aterms%2520of%2520the%2520gradients%2520of%2520the%2520two%2520low-rank%2520matrices%2520in%2520LoRA.%2520Leveraging%2520this%250Ainsight%252C%2520we%2520introduce%2520LoRA-Pro%252C%2520a%2520method%2520that%2520enhances%2520LoRA%2527s%2520performance%2520by%250Astrategically%2520adjusting%2520the%2520gradients%2520of%2520these%2520low-rank%2520matrices.%2520This%250Aadjustment%2520allows%2520the%2520low-rank%2520gradient%2520to%2520more%2520accurately%2520approximate%2520the%2520full%250Afine-tuning%2520gradient%252C%2520thereby%2520narrowing%2520the%2520performance%2520gap%2520between%2520LoRA%2520and%250Afull%2520fine-tuning.%2520Furthermore%252C%2520we%2520theoretically%2520derive%2520the%2520optimal%2520solutions%250Afor%2520adjusting%2520the%2520gradients%2520of%2520the%2520low-rank%2520matrices%252C%2520applying%2520them%2520during%250Afine-tuning%2520in%2520LoRA-Pro.%2520We%2520conduct%2520extensive%2520experiments%2520across%2520natural%250Alanguage%2520understanding%252C%2520dialogue%2520generation%252C%2520mathematical%2520reasoning%252C%2520code%250Ageneration%252C%2520and%2520image%2520classification%2520tasks%252C%2520demonstrating%2520that%2520LoRA-Pro%250Asubstantially%2520improves%2520LoRA%2527s%2520performance%252C%2520effectively%2520narrowing%2520the%2520gap%2520with%250Afull%2520fine-tuning.%2520Code%2520is%2520publicly%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/mrflogs/LoRA-Pro%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-Pro%3A%20Are%20Low-Rank%20Adapters%20Properly%20Optimized%3F&entry.906535625=Zhengbo%20Wang%20and%20Jian%20Liang%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan&entry.1292438233=%20%20Low-rank%20adaptation%2C%20also%20known%20as%20LoRA%2C%20has%20emerged%20as%20a%20prominent%20method%0Afor%20parameter-efficient%20fine-tuning%20of%20foundation%20models.%20Despite%20its%0Acomputational%20efficiency%2C%20LoRA%20still%20yields%20inferior%20performance%20compared%20to%0Afull%20fine-tuning.%20In%20this%20paper%2C%20we%20first%20uncover%20a%20fundamental%20connection%0Abetween%20the%20optimization%20processes%20of%20LoRA%20and%20full%20fine-tuning%3A%20using%20LoRA%20for%0Aoptimization%20is%20mathematically%20equivalent%20to%20full%20fine-tuning%20using%20a%20low-rank%0Agradient%20for%20parameter%20updates.%20And%20this%20low-rank%20gradient%20can%20be%20expressed%20in%0Aterms%20of%20the%20gradients%20of%20the%20two%20low-rank%20matrices%20in%20LoRA.%20Leveraging%20this%0Ainsight%2C%20we%20introduce%20LoRA-Pro%2C%20a%20method%20that%20enhances%20LoRA%27s%20performance%20by%0Astrategically%20adjusting%20the%20gradients%20of%20these%20low-rank%20matrices.%20This%0Aadjustment%20allows%20the%20low-rank%20gradient%20to%20more%20accurately%20approximate%20the%20full%0Afine-tuning%20gradient%2C%20thereby%20narrowing%20the%20performance%20gap%20between%20LoRA%20and%0Afull%20fine-tuning.%20Furthermore%2C%20we%20theoretically%20derive%20the%20optimal%20solutions%0Afor%20adjusting%20the%20gradients%20of%20the%20low-rank%20matrices%2C%20applying%20them%20during%0Afine-tuning%20in%20LoRA-Pro.%20We%20conduct%20extensive%20experiments%20across%20natural%0Alanguage%20understanding%2C%20dialogue%20generation%2C%20mathematical%20reasoning%2C%20code%0Ageneration%2C%20and%20image%20classification%20tasks%2C%20demonstrating%20that%20LoRA-Pro%0Asubstantially%20improves%20LoRA%27s%20performance%2C%20effectively%20narrowing%20the%20gap%20with%0Afull%20fine-tuning.%20Code%20is%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mrflogs/LoRA-Pro%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18242v2&entry.124074799=Read"},
{"title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications", "author": "Sathya Krishnan Suresh and Wu Mengjun and Tushar Pranav and Eng Siong Chng", "abstract": "  The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.\n", "link": "http://arxiv.org/abs/2409.19020v2", "date": "2024-10-15", "relevancy": 2.4251, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiaSynth%3A%20Synthetic%20Dialogue%20Generation%20Framework%20for%20Low%20Resource%0A%20%20Dialogue%20Applications&body=Title%3A%20DiaSynth%3A%20Synthetic%20Dialogue%20Generation%20Framework%20for%20Low%20Resource%0A%20%20Dialogue%20Applications%0AAuthor%3A%20Sathya%20Krishnan%20Suresh%20and%20Wu%20Mengjun%20and%20Tushar%20Pranav%20and%20Eng%20Siong%20Chng%0AAbstract%3A%20%20%20The%20scarcity%20of%20domain-specific%20dialogue%20datasets%20limits%20the%20development%20of%0Adialogue%20systems%20across%20applications.%20Existing%20research%20is%20constrained%20by%0Ageneral%20or%20niche%20datasets%20that%20lack%20sufficient%20scale%20for%20training%20dialogue%0Asystems.%20To%20address%20this%20gap%2C%20we%20introduce%20DiaSynth%20-%20a%20synthetic%20dialogue%0Ageneration%20framework%20capable%20of%20generating%20high-quality%2C%20contextually%20rich%0Adialogues%20across%20a%20wide%20range%20of%20domains.%20Unlike%20existing%20frameworks%2C%20DiaSynth%0Auses%20Large%20Language%20Models%20%28LLMs%29%20and%20Chain%20of%20Thought%20%28CoT%29%20reasoning%20to%0Agenerate%20dynamic%2C%20domain-specific%20dialogues%20with%20simulated%20personas%20and%20diverse%0Aconversational%20features.%20We%20perform%20our%20experiments%20by%20generating%20synthetic%0Adata%20using%20different%20LLMs%20and%20few-shot%20examples%20from%20DialogSum%20and%20SAMSum.%20The%0Apretrained%20language%20models%20fine-tuned%20on%20the%20synthetic%20data%20outperform%20the%20base%0Amodels%20by%2016.47%25%20on%20dialogue%20summarization%2C%20while%20the%20comparison%20between%20models%0Afine-tuned%20on%20in-domain%20data%20and%20synthetic%20data%20shows%20that%20the%20synthetic%20data%0Ais%20able%20to%20capture%2090.48%25%20of%20the%20performance%20distribution%20of%20the%20in-domain%20data%0Aon%20dialogue%20summarization.%20The%20quality%20of%20the%20data%20generated%20also%20increases%20as%0Awe%20increase%20the%20size%20of%20LLM%20from%203B%20to%208B.%20These%20results%20validate%20DiaSynth%27s%0Apotential%20as%20a%20robust%20alternative%20to%20traditional%20data%20collection%20methods.%20We%0Aopen%20source%20the%20code%20and%20data%20generated%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiaSynth%253A%2520Synthetic%2520Dialogue%2520Generation%2520Framework%2520for%2520Low%2520Resource%250A%2520%2520Dialogue%2520Applications%26entry.906535625%3DSathya%2520Krishnan%2520Suresh%2520and%2520Wu%2520Mengjun%2520and%2520Tushar%2520Pranav%2520and%2520Eng%2520Siong%2520Chng%26entry.1292438233%3D%2520%2520The%2520scarcity%2520of%2520domain-specific%2520dialogue%2520datasets%2520limits%2520the%2520development%2520of%250Adialogue%2520systems%2520across%2520applications.%2520Existing%2520research%2520is%2520constrained%2520by%250Ageneral%2520or%2520niche%2520datasets%2520that%2520lack%2520sufficient%2520scale%2520for%2520training%2520dialogue%250Asystems.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520DiaSynth%2520-%2520a%2520synthetic%2520dialogue%250Ageneration%2520framework%2520capable%2520of%2520generating%2520high-quality%252C%2520contextually%2520rich%250Adialogues%2520across%2520a%2520wide%2520range%2520of%2520domains.%2520Unlike%2520existing%2520frameworks%252C%2520DiaSynth%250Auses%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520reasoning%2520to%250Agenerate%2520dynamic%252C%2520domain-specific%2520dialogues%2520with%2520simulated%2520personas%2520and%2520diverse%250Aconversational%2520features.%2520We%2520perform%2520our%2520experiments%2520by%2520generating%2520synthetic%250Adata%2520using%2520different%2520LLMs%2520and%2520few-shot%2520examples%2520from%2520DialogSum%2520and%2520SAMSum.%2520The%250Apretrained%2520language%2520models%2520fine-tuned%2520on%2520the%2520synthetic%2520data%2520outperform%2520the%2520base%250Amodels%2520by%252016.47%2525%2520on%2520dialogue%2520summarization%252C%2520while%2520the%2520comparison%2520between%2520models%250Afine-tuned%2520on%2520in-domain%2520data%2520and%2520synthetic%2520data%2520shows%2520that%2520the%2520synthetic%2520data%250Ais%2520able%2520to%2520capture%252090.48%2525%2520of%2520the%2520performance%2520distribution%2520of%2520the%2520in-domain%2520data%250Aon%2520dialogue%2520summarization.%2520The%2520quality%2520of%2520the%2520data%2520generated%2520also%2520increases%2520as%250Awe%2520increase%2520the%2520size%2520of%2520LLM%2520from%25203B%2520to%25208B.%2520These%2520results%2520validate%2520DiaSynth%2527s%250Apotential%2520as%2520a%2520robust%2520alternative%2520to%2520traditional%2520data%2520collection%2520methods.%2520We%250Aopen%2520source%2520the%2520code%2520and%2520data%2520generated%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiaSynth%3A%20Synthetic%20Dialogue%20Generation%20Framework%20for%20Low%20Resource%0A%20%20Dialogue%20Applications&entry.906535625=Sathya%20Krishnan%20Suresh%20and%20Wu%20Mengjun%20and%20Tushar%20Pranav%20and%20Eng%20Siong%20Chng&entry.1292438233=%20%20The%20scarcity%20of%20domain-specific%20dialogue%20datasets%20limits%20the%20development%20of%0Adialogue%20systems%20across%20applications.%20Existing%20research%20is%20constrained%20by%0Ageneral%20or%20niche%20datasets%20that%20lack%20sufficient%20scale%20for%20training%20dialogue%0Asystems.%20To%20address%20this%20gap%2C%20we%20introduce%20DiaSynth%20-%20a%20synthetic%20dialogue%0Ageneration%20framework%20capable%20of%20generating%20high-quality%2C%20contextually%20rich%0Adialogues%20across%20a%20wide%20range%20of%20domains.%20Unlike%20existing%20frameworks%2C%20DiaSynth%0Auses%20Large%20Language%20Models%20%28LLMs%29%20and%20Chain%20of%20Thought%20%28CoT%29%20reasoning%20to%0Agenerate%20dynamic%2C%20domain-specific%20dialogues%20with%20simulated%20personas%20and%20diverse%0Aconversational%20features.%20We%20perform%20our%20experiments%20by%20generating%20synthetic%0Adata%20using%20different%20LLMs%20and%20few-shot%20examples%20from%20DialogSum%20and%20SAMSum.%20The%0Apretrained%20language%20models%20fine-tuned%20on%20the%20synthetic%20data%20outperform%20the%20base%0Amodels%20by%2016.47%25%20on%20dialogue%20summarization%2C%20while%20the%20comparison%20between%20models%0Afine-tuned%20on%20in-domain%20data%20and%20synthetic%20data%20shows%20that%20the%20synthetic%20data%0Ais%20able%20to%20capture%2090.48%25%20of%20the%20performance%20distribution%20of%20the%20in-domain%20data%0Aon%20dialogue%20summarization.%20The%20quality%20of%20the%20data%20generated%20also%20increases%20as%0Awe%20increase%20the%20size%20of%20LLM%20from%203B%20to%208B.%20These%20results%20validate%20DiaSynth%27s%0Apotential%20as%20a%20robust%20alternative%20to%20traditional%20data%20collection%20methods.%20We%0Aopen%20source%20the%20code%20and%20data%20generated%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19020v2&entry.124074799=Read"},
{"title": "LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under\n  Low-Resource Scenarios", "author": "Wen Wuzhenghong and Zhang Yongpan and Pan Su and Sun Yuwei and Lu Pengwei and Ding Cheng", "abstract": "  Large language models revolutionize Text2SQL through supervised fine-tuning,\nyet a crucial limitation is overlooked: the complexity of databases leads to an\nincreased context length, consequently resulting in higher GPU memory demands\nfor model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL\ncomprises two supervised fine-tuning models: the schema\\_link model and the\nSQL\\_generation model, with the schema\\_link model serving as the focal point\nfor streamlining the overall process. During the fine-tuning of the\nschema\\_link model, LR-SQL breaks down the complete database into flexible\ncombinations of tables with adjustable quantities, enabling the model to learn\nthe relationships within the entire database from these dispersed slices.\nFurthermore, to enhance the model's ability to perceive the relationships among\nvarious discrete slices during inference, LR-SQL trains the model's\nChain-of-Thought capability for this task. Experimental results demonstrate\nthat LR-SQL can reduce the total GPU memory usage by 40\\% compared to existing\nfine-tuning methods, while only losing 2\\% of table prediction accuracy in\nschema\\_link task. For the overall Text2SQL task, the Execution Accuracy\ndecrease by 0.6\\%.Our project is now available on\nhttps://github.com/hongWin/LR-SQL\n", "link": "http://arxiv.org/abs/2410.11457v1", "date": "2024-10-15", "relevancy": 2.4068, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LR-SQL%3A%20A%20Supervised%20Fine-Tuning%20Method%20for%20Text2SQL%20Tasks%20under%0A%20%20Low-Resource%20Scenarios&body=Title%3A%20LR-SQL%3A%20A%20Supervised%20Fine-Tuning%20Method%20for%20Text2SQL%20Tasks%20under%0A%20%20Low-Resource%20Scenarios%0AAuthor%3A%20Wen%20Wuzhenghong%20and%20Zhang%20Yongpan%20and%20Pan%20Su%20and%20Sun%20Yuwei%20and%20Lu%20Pengwei%20and%20Ding%20Cheng%0AAbstract%3A%20%20%20Large%20language%20models%20revolutionize%20Text2SQL%20through%20supervised%20fine-tuning%2C%0Ayet%20a%20crucial%20limitation%20is%20overlooked%3A%20the%20complexity%20of%20databases%20leads%20to%20an%0Aincreased%20context%20length%2C%20consequently%20resulting%20in%20higher%20GPU%20memory%20demands%0Afor%20model%20fine-tuning.%20To%20address%20this%20issue%2C%20we%20propose%20LR-SQL.%20LR-SQL%0Acomprises%20two%20supervised%20fine-tuning%20models%3A%20the%20schema%5C_link%20model%20and%20the%0ASQL%5C_generation%20model%2C%20with%20the%20schema%5C_link%20model%20serving%20as%20the%20focal%20point%0Afor%20streamlining%20the%20overall%20process.%20During%20the%20fine-tuning%20of%20the%0Aschema%5C_link%20model%2C%20LR-SQL%20breaks%20down%20the%20complete%20database%20into%20flexible%0Acombinations%20of%20tables%20with%20adjustable%20quantities%2C%20enabling%20the%20model%20to%20learn%0Athe%20relationships%20within%20the%20entire%20database%20from%20these%20dispersed%20slices.%0AFurthermore%2C%20to%20enhance%20the%20model%27s%20ability%20to%20perceive%20the%20relationships%20among%0Avarious%20discrete%20slices%20during%20inference%2C%20LR-SQL%20trains%20the%20model%27s%0AChain-of-Thought%20capability%20for%20this%20task.%20Experimental%20results%20demonstrate%0Athat%20LR-SQL%20can%20reduce%20the%20total%20GPU%20memory%20usage%20by%2040%5C%25%20compared%20to%20existing%0Afine-tuning%20methods%2C%20while%20only%20losing%202%5C%25%20of%20table%20prediction%20accuracy%20in%0Aschema%5C_link%20task.%20For%20the%20overall%20Text2SQL%20task%2C%20the%20Execution%20Accuracy%0Adecrease%20by%200.6%5C%25.Our%20project%20is%20now%20available%20on%0Ahttps%3A//github.com/hongWin/LR-SQL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLR-SQL%253A%2520A%2520Supervised%2520Fine-Tuning%2520Method%2520for%2520Text2SQL%2520Tasks%2520under%250A%2520%2520Low-Resource%2520Scenarios%26entry.906535625%3DWen%2520Wuzhenghong%2520and%2520Zhang%2520Yongpan%2520and%2520Pan%2520Su%2520and%2520Sun%2520Yuwei%2520and%2520Lu%2520Pengwei%2520and%2520Ding%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520revolutionize%2520Text2SQL%2520through%2520supervised%2520fine-tuning%252C%250Ayet%2520a%2520crucial%2520limitation%2520is%2520overlooked%253A%2520the%2520complexity%2520of%2520databases%2520leads%2520to%2520an%250Aincreased%2520context%2520length%252C%2520consequently%2520resulting%2520in%2520higher%2520GPU%2520memory%2520demands%250Afor%2520model%2520fine-tuning.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520LR-SQL.%2520LR-SQL%250Acomprises%2520two%2520supervised%2520fine-tuning%2520models%253A%2520the%2520schema%255C_link%2520model%2520and%2520the%250ASQL%255C_generation%2520model%252C%2520with%2520the%2520schema%255C_link%2520model%2520serving%2520as%2520the%2520focal%2520point%250Afor%2520streamlining%2520the%2520overall%2520process.%2520During%2520the%2520fine-tuning%2520of%2520the%250Aschema%255C_link%2520model%252C%2520LR-SQL%2520breaks%2520down%2520the%2520complete%2520database%2520into%2520flexible%250Acombinations%2520of%2520tables%2520with%2520adjustable%2520quantities%252C%2520enabling%2520the%2520model%2520to%2520learn%250Athe%2520relationships%2520within%2520the%2520entire%2520database%2520from%2520these%2520dispersed%2520slices.%250AFurthermore%252C%2520to%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520perceive%2520the%2520relationships%2520among%250Avarious%2520discrete%2520slices%2520during%2520inference%252C%2520LR-SQL%2520trains%2520the%2520model%2527s%250AChain-of-Thought%2520capability%2520for%2520this%2520task.%2520Experimental%2520results%2520demonstrate%250Athat%2520LR-SQL%2520can%2520reduce%2520the%2520total%2520GPU%2520memory%2520usage%2520by%252040%255C%2525%2520compared%2520to%2520existing%250Afine-tuning%2520methods%252C%2520while%2520only%2520losing%25202%255C%2525%2520of%2520table%2520prediction%2520accuracy%2520in%250Aschema%255C_link%2520task.%2520For%2520the%2520overall%2520Text2SQL%2520task%252C%2520the%2520Execution%2520Accuracy%250Adecrease%2520by%25200.6%255C%2525.Our%2520project%2520is%2520now%2520available%2520on%250Ahttps%253A//github.com/hongWin/LR-SQL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LR-SQL%3A%20A%20Supervised%20Fine-Tuning%20Method%20for%20Text2SQL%20Tasks%20under%0A%20%20Low-Resource%20Scenarios&entry.906535625=Wen%20Wuzhenghong%20and%20Zhang%20Yongpan%20and%20Pan%20Su%20and%20Sun%20Yuwei%20and%20Lu%20Pengwei%20and%20Ding%20Cheng&entry.1292438233=%20%20Large%20language%20models%20revolutionize%20Text2SQL%20through%20supervised%20fine-tuning%2C%0Ayet%20a%20crucial%20limitation%20is%20overlooked%3A%20the%20complexity%20of%20databases%20leads%20to%20an%0Aincreased%20context%20length%2C%20consequently%20resulting%20in%20higher%20GPU%20memory%20demands%0Afor%20model%20fine-tuning.%20To%20address%20this%20issue%2C%20we%20propose%20LR-SQL.%20LR-SQL%0Acomprises%20two%20supervised%20fine-tuning%20models%3A%20the%20schema%5C_link%20model%20and%20the%0ASQL%5C_generation%20model%2C%20with%20the%20schema%5C_link%20model%20serving%20as%20the%20focal%20point%0Afor%20streamlining%20the%20overall%20process.%20During%20the%20fine-tuning%20of%20the%0Aschema%5C_link%20model%2C%20LR-SQL%20breaks%20down%20the%20complete%20database%20into%20flexible%0Acombinations%20of%20tables%20with%20adjustable%20quantities%2C%20enabling%20the%20model%20to%20learn%0Athe%20relationships%20within%20the%20entire%20database%20from%20these%20dispersed%20slices.%0AFurthermore%2C%20to%20enhance%20the%20model%27s%20ability%20to%20perceive%20the%20relationships%20among%0Avarious%20discrete%20slices%20during%20inference%2C%20LR-SQL%20trains%20the%20model%27s%0AChain-of-Thought%20capability%20for%20this%20task.%20Experimental%20results%20demonstrate%0Athat%20LR-SQL%20can%20reduce%20the%20total%20GPU%20memory%20usage%20by%2040%5C%25%20compared%20to%20existing%0Afine-tuning%20methods%2C%20while%20only%20losing%202%5C%25%20of%20table%20prediction%20accuracy%20in%0Aschema%5C_link%20task.%20For%20the%20overall%20Text2SQL%20task%2C%20the%20Execution%20Accuracy%0Adecrease%20by%200.6%5C%25.Our%20project%20is%20now%20available%20on%0Ahttps%3A//github.com/hongWin/LR-SQL%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11457v1&entry.124074799=Read"},
{"title": "Language Models Encode Numbers Using Digit Representations in Base 10", "author": "Amit Arnold Levy and Mor Geva", "abstract": "  Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across \\textit{the digits} of the answer rather than\nnormally around \\textit{its numeric value}. Through a series of probing\nexperiments and causal interventions, we show that LLMs internally represent\nnumbers with individual circular representations per-digit in base 10. This\ndigit-wise representation, as opposed to a value representation, sheds light on\nthe error patterns of models on tasks involving numerical reasoning and could\nserve as a basis for future studies on analyzing numerical mechanisms in LLMs.\n", "link": "http://arxiv.org/abs/2410.11781v1", "date": "2024-10-15", "relevancy": 2.4064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Encode%20Numbers%20Using%20Digit%20Representations%20in%20Base%2010&body=Title%3A%20Language%20Models%20Encode%20Numbers%20Using%20Digit%20Representations%20in%20Base%2010%0AAuthor%3A%20Amit%20Arnold%20Levy%20and%20Mor%20Geva%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20frequently%20make%20errors%20when%20handling%20even%20simple%0Anumerical%20problems%2C%20such%20as%20comparing%20two%20small%20numbers.%20A%20natural%20hypothesis%0Ais%20that%20these%20errors%20stem%20from%20how%20LLMs%20represent%20numbers%2C%20and%20specifically%2C%0Awhether%20their%20representations%20of%20numbers%20capture%20their%20numeric%20values.%20We%0Atackle%20this%20question%20from%20the%20observation%20that%20LLM%20errors%20on%20numerical%20tasks%0Aare%20often%20distributed%20across%20%5Ctextit%7Bthe%20digits%7D%20of%20the%20answer%20rather%20than%0Anormally%20around%20%5Ctextit%7Bits%20numeric%20value%7D.%20Through%20a%20series%20of%20probing%0Aexperiments%20and%20causal%20interventions%2C%20we%20show%20that%20LLMs%20internally%20represent%0Anumbers%20with%20individual%20circular%20representations%20per-digit%20in%20base%2010.%20This%0Adigit-wise%20representation%2C%20as%20opposed%20to%20a%20value%20representation%2C%20sheds%20light%20on%0Athe%20error%20patterns%20of%20models%20on%20tasks%20involving%20numerical%20reasoning%20and%20could%0Aserve%20as%20a%20basis%20for%20future%20studies%20on%20analyzing%20numerical%20mechanisms%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Encode%2520Numbers%2520Using%2520Digit%2520Representations%2520in%2520Base%252010%26entry.906535625%3DAmit%2520Arnold%2520Levy%2520and%2520Mor%2520Geva%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520frequently%2520make%2520errors%2520when%2520handling%2520even%2520simple%250Anumerical%2520problems%252C%2520such%2520as%2520comparing%2520two%2520small%2520numbers.%2520A%2520natural%2520hypothesis%250Ais%2520that%2520these%2520errors%2520stem%2520from%2520how%2520LLMs%2520represent%2520numbers%252C%2520and%2520specifically%252C%250Awhether%2520their%2520representations%2520of%2520numbers%2520capture%2520their%2520numeric%2520values.%2520We%250Atackle%2520this%2520question%2520from%2520the%2520observation%2520that%2520LLM%2520errors%2520on%2520numerical%2520tasks%250Aare%2520often%2520distributed%2520across%2520%255Ctextit%257Bthe%2520digits%257D%2520of%2520the%2520answer%2520rather%2520than%250Anormally%2520around%2520%255Ctextit%257Bits%2520numeric%2520value%257D.%2520Through%2520a%2520series%2520of%2520probing%250Aexperiments%2520and%2520causal%2520interventions%252C%2520we%2520show%2520that%2520LLMs%2520internally%2520represent%250Anumbers%2520with%2520individual%2520circular%2520representations%2520per-digit%2520in%2520base%252010.%2520This%250Adigit-wise%2520representation%252C%2520as%2520opposed%2520to%2520a%2520value%2520representation%252C%2520sheds%2520light%2520on%250Athe%2520error%2520patterns%2520of%2520models%2520on%2520tasks%2520involving%2520numerical%2520reasoning%2520and%2520could%250Aserve%2520as%2520a%2520basis%2520for%2520future%2520studies%2520on%2520analyzing%2520numerical%2520mechanisms%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Encode%20Numbers%20Using%20Digit%20Representations%20in%20Base%2010&entry.906535625=Amit%20Arnold%20Levy%20and%20Mor%20Geva&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20frequently%20make%20errors%20when%20handling%20even%20simple%0Anumerical%20problems%2C%20such%20as%20comparing%20two%20small%20numbers.%20A%20natural%20hypothesis%0Ais%20that%20these%20errors%20stem%20from%20how%20LLMs%20represent%20numbers%2C%20and%20specifically%2C%0Awhether%20their%20representations%20of%20numbers%20capture%20their%20numeric%20values.%20We%0Atackle%20this%20question%20from%20the%20observation%20that%20LLM%20errors%20on%20numerical%20tasks%0Aare%20often%20distributed%20across%20%5Ctextit%7Bthe%20digits%7D%20of%20the%20answer%20rather%20than%0Anormally%20around%20%5Ctextit%7Bits%20numeric%20value%7D.%20Through%20a%20series%20of%20probing%0Aexperiments%20and%20causal%20interventions%2C%20we%20show%20that%20LLMs%20internally%20represent%0Anumbers%20with%20individual%20circular%20representations%20per-digit%20in%20base%2010.%20This%0Adigit-wise%20representation%2C%20as%20opposed%20to%20a%20value%20representation%2C%20sheds%20light%20on%0Athe%20error%20patterns%20of%20models%20on%20tasks%20involving%20numerical%20reasoning%20and%20could%0Aserve%20as%20a%20basis%20for%20future%20studies%20on%20analyzing%20numerical%20mechanisms%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11781v1&entry.124074799=Read"},
{"title": "Leaving the barn door open for Clever Hans: Simple features predict LLM\n  benchmark answers", "author": "Lorenzo Pacchiardi and Marko Tesic and Lucy G. Cheke and Jos\u00e9 Hern\u00e1ndez-Orallo", "abstract": "  The integrity of AI benchmarks is fundamental to accurately assess the\ncapabilities of AI systems. The internal validity of these benchmarks - i.e.,\nmaking sure they are free from confounding factors - is crucial for ensuring\nthat they are measuring what they are designed to measure. In this paper, we\nexplore a key issue related to internal validity: the possibility that AI\nsystems can solve benchmarks in unintended ways, bypassing the capability being\ntested. This phenomenon, widely known in human and animal experiments, is often\nreferred to as the 'Clever Hans' effect, where tasks are solved using spurious\ncues, often involving much simpler processes than those putatively assessed.\nPrevious research suggests that language models can exhibit this behaviour as\nwell. In several older Natural Language Processing (NLP) benchmarks, individual\n$n$-grams like \"not\" have been found to be highly predictive of the correct\nlabels, and supervised NLP models have been shown to exploit these patterns. In\nthis work, we investigate the extent to which simple $n$-grams extracted from\nbenchmark instances can be combined to predict labels in modern multiple-choice\nbenchmarks designed for LLMs, and whether LLMs might be using such $n$-gram\npatterns to solve these benchmarks. We show how simple classifiers trained on\nthese $n$-grams can achieve high scores on several benchmarks, despite lacking\nthe capabilities being tested. Additionally, we provide evidence that modern\nLLMs might be using these superficial patterns to solve benchmarks. This\nsuggests that the internal validity of these benchmarks may be compromised and\ncaution should be exercised when interpreting LLM performance results on them.\n", "link": "http://arxiv.org/abs/2410.11672v1", "date": "2024-10-15", "relevancy": 2.3926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leaving%20the%20barn%20door%20open%20for%20Clever%20Hans%3A%20Simple%20features%20predict%20LLM%0A%20%20benchmark%20answers&body=Title%3A%20Leaving%20the%20barn%20door%20open%20for%20Clever%20Hans%3A%20Simple%20features%20predict%20LLM%0A%20%20benchmark%20answers%0AAuthor%3A%20Lorenzo%20Pacchiardi%20and%20Marko%20Tesic%20and%20Lucy%20G.%20Cheke%20and%20Jos%C3%A9%20Hern%C3%A1ndez-Orallo%0AAbstract%3A%20%20%20The%20integrity%20of%20AI%20benchmarks%20is%20fundamental%20to%20accurately%20assess%20the%0Acapabilities%20of%20AI%20systems.%20The%20internal%20validity%20of%20these%20benchmarks%20-%20i.e.%2C%0Amaking%20sure%20they%20are%20free%20from%20confounding%20factors%20-%20is%20crucial%20for%20ensuring%0Athat%20they%20are%20measuring%20what%20they%20are%20designed%20to%20measure.%20In%20this%20paper%2C%20we%0Aexplore%20a%20key%20issue%20related%20to%20internal%20validity%3A%20the%20possibility%20that%20AI%0Asystems%20can%20solve%20benchmarks%20in%20unintended%20ways%2C%20bypassing%20the%20capability%20being%0Atested.%20This%20phenomenon%2C%20widely%20known%20in%20human%20and%20animal%20experiments%2C%20is%20often%0Areferred%20to%20as%20the%20%27Clever%20Hans%27%20effect%2C%20where%20tasks%20are%20solved%20using%20spurious%0Acues%2C%20often%20involving%20much%20simpler%20processes%20than%20those%20putatively%20assessed.%0APrevious%20research%20suggests%20that%20language%20models%20can%20exhibit%20this%20behaviour%20as%0Awell.%20In%20several%20older%20Natural%20Language%20Processing%20%28NLP%29%20benchmarks%2C%20individual%0A%24n%24-grams%20like%20%22not%22%20have%20been%20found%20to%20be%20highly%20predictive%20of%20the%20correct%0Alabels%2C%20and%20supervised%20NLP%20models%20have%20been%20shown%20to%20exploit%20these%20patterns.%20In%0Athis%20work%2C%20we%20investigate%20the%20extent%20to%20which%20simple%20%24n%24-grams%20extracted%20from%0Abenchmark%20instances%20can%20be%20combined%20to%20predict%20labels%20in%20modern%20multiple-choice%0Abenchmarks%20designed%20for%20LLMs%2C%20and%20whether%20LLMs%20might%20be%20using%20such%20%24n%24-gram%0Apatterns%20to%20solve%20these%20benchmarks.%20We%20show%20how%20simple%20classifiers%20trained%20on%0Athese%20%24n%24-grams%20can%20achieve%20high%20scores%20on%20several%20benchmarks%2C%20despite%20lacking%0Athe%20capabilities%20being%20tested.%20Additionally%2C%20we%20provide%20evidence%20that%20modern%0ALLMs%20might%20be%20using%20these%20superficial%20patterns%20to%20solve%20benchmarks.%20This%0Asuggests%20that%20the%20internal%20validity%20of%20these%20benchmarks%20may%20be%20compromised%20and%0Acaution%20should%20be%20exercised%20when%20interpreting%20LLM%20performance%20results%20on%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeaving%2520the%2520barn%2520door%2520open%2520for%2520Clever%2520Hans%253A%2520Simple%2520features%2520predict%2520LLM%250A%2520%2520benchmark%2520answers%26entry.906535625%3DLorenzo%2520Pacchiardi%2520and%2520Marko%2520Tesic%2520and%2520Lucy%2520G.%2520Cheke%2520and%2520Jos%25C3%25A9%2520Hern%25C3%25A1ndez-Orallo%26entry.1292438233%3D%2520%2520The%2520integrity%2520of%2520AI%2520benchmarks%2520is%2520fundamental%2520to%2520accurately%2520assess%2520the%250Acapabilities%2520of%2520AI%2520systems.%2520The%2520internal%2520validity%2520of%2520these%2520benchmarks%2520-%2520i.e.%252C%250Amaking%2520sure%2520they%2520are%2520free%2520from%2520confounding%2520factors%2520-%2520is%2520crucial%2520for%2520ensuring%250Athat%2520they%2520are%2520measuring%2520what%2520they%2520are%2520designed%2520to%2520measure.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520a%2520key%2520issue%2520related%2520to%2520internal%2520validity%253A%2520the%2520possibility%2520that%2520AI%250Asystems%2520can%2520solve%2520benchmarks%2520in%2520unintended%2520ways%252C%2520bypassing%2520the%2520capability%2520being%250Atested.%2520This%2520phenomenon%252C%2520widely%2520known%2520in%2520human%2520and%2520animal%2520experiments%252C%2520is%2520often%250Areferred%2520to%2520as%2520the%2520%2527Clever%2520Hans%2527%2520effect%252C%2520where%2520tasks%2520are%2520solved%2520using%2520spurious%250Acues%252C%2520often%2520involving%2520much%2520simpler%2520processes%2520than%2520those%2520putatively%2520assessed.%250APrevious%2520research%2520suggests%2520that%2520language%2520models%2520can%2520exhibit%2520this%2520behaviour%2520as%250Awell.%2520In%2520several%2520older%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520benchmarks%252C%2520individual%250A%2524n%2524-grams%2520like%2520%2522not%2522%2520have%2520been%2520found%2520to%2520be%2520highly%2520predictive%2520of%2520the%2520correct%250Alabels%252C%2520and%2520supervised%2520NLP%2520models%2520have%2520been%2520shown%2520to%2520exploit%2520these%2520patterns.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520simple%2520%2524n%2524-grams%2520extracted%2520from%250Abenchmark%2520instances%2520can%2520be%2520combined%2520to%2520predict%2520labels%2520in%2520modern%2520multiple-choice%250Abenchmarks%2520designed%2520for%2520LLMs%252C%2520and%2520whether%2520LLMs%2520might%2520be%2520using%2520such%2520%2524n%2524-gram%250Apatterns%2520to%2520solve%2520these%2520benchmarks.%2520We%2520show%2520how%2520simple%2520classifiers%2520trained%2520on%250Athese%2520%2524n%2524-grams%2520can%2520achieve%2520high%2520scores%2520on%2520several%2520benchmarks%252C%2520despite%2520lacking%250Athe%2520capabilities%2520being%2520tested.%2520Additionally%252C%2520we%2520provide%2520evidence%2520that%2520modern%250ALLMs%2520might%2520be%2520using%2520these%2520superficial%2520patterns%2520to%2520solve%2520benchmarks.%2520This%250Asuggests%2520that%2520the%2520internal%2520validity%2520of%2520these%2520benchmarks%2520may%2520be%2520compromised%2520and%250Acaution%2520should%2520be%2520exercised%2520when%2520interpreting%2520LLM%2520performance%2520results%2520on%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leaving%20the%20barn%20door%20open%20for%20Clever%20Hans%3A%20Simple%20features%20predict%20LLM%0A%20%20benchmark%20answers&entry.906535625=Lorenzo%20Pacchiardi%20and%20Marko%20Tesic%20and%20Lucy%20G.%20Cheke%20and%20Jos%C3%A9%20Hern%C3%A1ndez-Orallo&entry.1292438233=%20%20The%20integrity%20of%20AI%20benchmarks%20is%20fundamental%20to%20accurately%20assess%20the%0Acapabilities%20of%20AI%20systems.%20The%20internal%20validity%20of%20these%20benchmarks%20-%20i.e.%2C%0Amaking%20sure%20they%20are%20free%20from%20confounding%20factors%20-%20is%20crucial%20for%20ensuring%0Athat%20they%20are%20measuring%20what%20they%20are%20designed%20to%20measure.%20In%20this%20paper%2C%20we%0Aexplore%20a%20key%20issue%20related%20to%20internal%20validity%3A%20the%20possibility%20that%20AI%0Asystems%20can%20solve%20benchmarks%20in%20unintended%20ways%2C%20bypassing%20the%20capability%20being%0Atested.%20This%20phenomenon%2C%20widely%20known%20in%20human%20and%20animal%20experiments%2C%20is%20often%0Areferred%20to%20as%20the%20%27Clever%20Hans%27%20effect%2C%20where%20tasks%20are%20solved%20using%20spurious%0Acues%2C%20often%20involving%20much%20simpler%20processes%20than%20those%20putatively%20assessed.%0APrevious%20research%20suggests%20that%20language%20models%20can%20exhibit%20this%20behaviour%20as%0Awell.%20In%20several%20older%20Natural%20Language%20Processing%20%28NLP%29%20benchmarks%2C%20individual%0A%24n%24-grams%20like%20%22not%22%20have%20been%20found%20to%20be%20highly%20predictive%20of%20the%20correct%0Alabels%2C%20and%20supervised%20NLP%20models%20have%20been%20shown%20to%20exploit%20these%20patterns.%20In%0Athis%20work%2C%20we%20investigate%20the%20extent%20to%20which%20simple%20%24n%24-grams%20extracted%20from%0Abenchmark%20instances%20can%20be%20combined%20to%20predict%20labels%20in%20modern%20multiple-choice%0Abenchmarks%20designed%20for%20LLMs%2C%20and%20whether%20LLMs%20might%20be%20using%20such%20%24n%24-gram%0Apatterns%20to%20solve%20these%20benchmarks.%20We%20show%20how%20simple%20classifiers%20trained%20on%0Athese%20%24n%24-grams%20can%20achieve%20high%20scores%20on%20several%20benchmarks%2C%20despite%20lacking%0Athe%20capabilities%20being%20tested.%20Additionally%2C%20we%20provide%20evidence%20that%20modern%0ALLMs%20might%20be%20using%20these%20superficial%20patterns%20to%20solve%20benchmarks.%20This%0Asuggests%20that%20the%20internal%20validity%20of%20these%20benchmarks%20may%20be%20compromised%20and%0Acaution%20should%20be%20exercised%20when%20interpreting%20LLM%20performance%20results%20on%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11672v1&entry.124074799=Read"},
{"title": "LoSAM: Local Search in Additive Noise Models with Unmeasured\n  Confounders, a Top-Down Global Discovery Approach", "author": "Sujai Hiremath and Kyra Gan and Promit Ghosal", "abstract": "  We address the challenge of causal discovery in structural equation models\nwith additive noise without imposing additional assumptions on the underlying\ndata-generating process. We introduce local search in additive noise model\n(LoSAM), which generalizes an existing nonlinear method that leverages local\ncausal substructures to the general additive noise setting, allowing for both\nlinear and nonlinear causal mechanisms. We show that LoSAM achieves polynomial\nruntime, and improves runtime and efficiency by exploiting new substructures to\nminimize the conditioning set at each step. Further, we introduce a variant of\nLoSAM, LoSAM-UC, that is robust to unmeasured confounding among roots, a\nproperty that is often not satisfied by functional-causal-model-based methods.\nWe numerically demonstrate the utility of LoSAM, showing that it outperforms\nexisting benchmarks.\n", "link": "http://arxiv.org/abs/2410.11759v1", "date": "2024-10-15", "relevancy": 2.3924, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoSAM%3A%20Local%20Search%20in%20Additive%20Noise%20Models%20with%20Unmeasured%0A%20%20Confounders%2C%20a%20Top-Down%20Global%20Discovery%20Approach&body=Title%3A%20LoSAM%3A%20Local%20Search%20in%20Additive%20Noise%20Models%20with%20Unmeasured%0A%20%20Confounders%2C%20a%20Top-Down%20Global%20Discovery%20Approach%0AAuthor%3A%20Sujai%20Hiremath%20and%20Kyra%20Gan%20and%20Promit%20Ghosal%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20causal%20discovery%20in%20structural%20equation%20models%0Awith%20additive%20noise%20without%20imposing%20additional%20assumptions%20on%20the%20underlying%0Adata-generating%20process.%20We%20introduce%20local%20search%20in%20additive%20noise%20model%0A%28LoSAM%29%2C%20which%20generalizes%20an%20existing%20nonlinear%20method%20that%20leverages%20local%0Acausal%20substructures%20to%20the%20general%20additive%20noise%20setting%2C%20allowing%20for%20both%0Alinear%20and%20nonlinear%20causal%20mechanisms.%20We%20show%20that%20LoSAM%20achieves%20polynomial%0Aruntime%2C%20and%20improves%20runtime%20and%20efficiency%20by%20exploiting%20new%20substructures%20to%0Aminimize%20the%20conditioning%20set%20at%20each%20step.%20Further%2C%20we%20introduce%20a%20variant%20of%0ALoSAM%2C%20LoSAM-UC%2C%20that%20is%20robust%20to%20unmeasured%20confounding%20among%20roots%2C%20a%0Aproperty%20that%20is%20often%20not%20satisfied%20by%20functional-causal-model-based%20methods.%0AWe%20numerically%20demonstrate%20the%20utility%20of%20LoSAM%2C%20showing%20that%20it%20outperforms%0Aexisting%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoSAM%253A%2520Local%2520Search%2520in%2520Additive%2520Noise%2520Models%2520with%2520Unmeasured%250A%2520%2520Confounders%252C%2520a%2520Top-Down%2520Global%2520Discovery%2520Approach%26entry.906535625%3DSujai%2520Hiremath%2520and%2520Kyra%2520Gan%2520and%2520Promit%2520Ghosal%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520causal%2520discovery%2520in%2520structural%2520equation%2520models%250Awith%2520additive%2520noise%2520without%2520imposing%2520additional%2520assumptions%2520on%2520the%2520underlying%250Adata-generating%2520process.%2520We%2520introduce%2520local%2520search%2520in%2520additive%2520noise%2520model%250A%2528LoSAM%2529%252C%2520which%2520generalizes%2520an%2520existing%2520nonlinear%2520method%2520that%2520leverages%2520local%250Acausal%2520substructures%2520to%2520the%2520general%2520additive%2520noise%2520setting%252C%2520allowing%2520for%2520both%250Alinear%2520and%2520nonlinear%2520causal%2520mechanisms.%2520We%2520show%2520that%2520LoSAM%2520achieves%2520polynomial%250Aruntime%252C%2520and%2520improves%2520runtime%2520and%2520efficiency%2520by%2520exploiting%2520new%2520substructures%2520to%250Aminimize%2520the%2520conditioning%2520set%2520at%2520each%2520step.%2520Further%252C%2520we%2520introduce%2520a%2520variant%2520of%250ALoSAM%252C%2520LoSAM-UC%252C%2520that%2520is%2520robust%2520to%2520unmeasured%2520confounding%2520among%2520roots%252C%2520a%250Aproperty%2520that%2520is%2520often%2520not%2520satisfied%2520by%2520functional-causal-model-based%2520methods.%250AWe%2520numerically%2520demonstrate%2520the%2520utility%2520of%2520LoSAM%252C%2520showing%2520that%2520it%2520outperforms%250Aexisting%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoSAM%3A%20Local%20Search%20in%20Additive%20Noise%20Models%20with%20Unmeasured%0A%20%20Confounders%2C%20a%20Top-Down%20Global%20Discovery%20Approach&entry.906535625=Sujai%20Hiremath%20and%20Kyra%20Gan%20and%20Promit%20Ghosal&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20causal%20discovery%20in%20structural%20equation%20models%0Awith%20additive%20noise%20without%20imposing%20additional%20assumptions%20on%20the%20underlying%0Adata-generating%20process.%20We%20introduce%20local%20search%20in%20additive%20noise%20model%0A%28LoSAM%29%2C%20which%20generalizes%20an%20existing%20nonlinear%20method%20that%20leverages%20local%0Acausal%20substructures%20to%20the%20general%20additive%20noise%20setting%2C%20allowing%20for%20both%0Alinear%20and%20nonlinear%20causal%20mechanisms.%20We%20show%20that%20LoSAM%20achieves%20polynomial%0Aruntime%2C%20and%20improves%20runtime%20and%20efficiency%20by%20exploiting%20new%20substructures%20to%0Aminimize%20the%20conditioning%20set%20at%20each%20step.%20Further%2C%20we%20introduce%20a%20variant%20of%0ALoSAM%2C%20LoSAM-UC%2C%20that%20is%20robust%20to%20unmeasured%20confounding%20among%20roots%2C%20a%0Aproperty%20that%20is%20often%20not%20satisfied%20by%20functional-causal-model-based%20methods.%0AWe%20numerically%20demonstrate%20the%20utility%20of%20LoSAM%2C%20showing%20that%20it%20outperforms%0Aexisting%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11759v1&entry.124074799=Read"},
{"title": "Federated Continual Learning Goes Online: Uncertainty-Aware Memory\n  Management for Vision Tasks and Beyond", "author": "Giuseppe Serra and Florian Buettner", "abstract": "  Given the ability to model more realistic and dynamic problems, Federated\nContinual Learning (FCL) has been increasingly investigated recently. A\nwell-known problem encountered in this setting is the so-called catastrophic\nforgetting, for which the learning model is inclined to focus on more recent\ntasks while forgetting the previously learned knowledge. The majority of the\ncurrent approaches in FCL propose generative-based solutions to solve said\nproblem. However, this setting requires multiple training epochs over the data,\nimplying an offline setting where datasets are stored locally and remain\nunchanged over time. Furthermore, the proposed solutions are tailored for\nvision tasks solely. To overcome these limitations, we propose a new approach\nto deal with different modalities in the online scenario where new data arrive\nin streams of mini-batches that can only be processed once. To solve\ncatastrophic forgetting, we propose an uncertainty-aware memory-based approach.\nSpecifically, we suggest using an estimator based on the Bregman Information\n(BI) to compute the model's variance at the sample level. Through measures of\npredictive uncertainty, we retrieve samples with specific characteristics, and\n- by retraining the model on such samples - we demonstrate the potential of\nthis approach to reduce the forgetting effect in realistic settings while\nmaintaining data confidentiality and competitive communication efficiency\ncompared to state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2405.18925v3", "date": "2024-10-15", "relevancy": 2.388, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6113}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5916}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Continual%20Learning%20Goes%20Online%3A%20Uncertainty-Aware%20Memory%0A%20%20Management%20for%20Vision%20Tasks%20and%20Beyond&body=Title%3A%20Federated%20Continual%20Learning%20Goes%20Online%3A%20Uncertainty-Aware%20Memory%0A%20%20Management%20for%20Vision%20Tasks%20and%20Beyond%0AAuthor%3A%20Giuseppe%20Serra%20and%20Florian%20Buettner%0AAbstract%3A%20%20%20Given%20the%20ability%20to%20model%20more%20realistic%20and%20dynamic%20problems%2C%20Federated%0AContinual%20Learning%20%28FCL%29%20has%20been%20increasingly%20investigated%20recently.%20A%0Awell-known%20problem%20encountered%20in%20this%20setting%20is%20the%20so-called%20catastrophic%0Aforgetting%2C%20for%20which%20the%20learning%20model%20is%20inclined%20to%20focus%20on%20more%20recent%0Atasks%20while%20forgetting%20the%20previously%20learned%20knowledge.%20The%20majority%20of%20the%0Acurrent%20approaches%20in%20FCL%20propose%20generative-based%20solutions%20to%20solve%20said%0Aproblem.%20However%2C%20this%20setting%20requires%20multiple%20training%20epochs%20over%20the%20data%2C%0Aimplying%20an%20offline%20setting%20where%20datasets%20are%20stored%20locally%20and%20remain%0Aunchanged%20over%20time.%20Furthermore%2C%20the%20proposed%20solutions%20are%20tailored%20for%0Avision%20tasks%20solely.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20new%20approach%0Ato%20deal%20with%20different%20modalities%20in%20the%20online%20scenario%20where%20new%20data%20arrive%0Ain%20streams%20of%20mini-batches%20that%20can%20only%20be%20processed%20once.%20To%20solve%0Acatastrophic%20forgetting%2C%20we%20propose%20an%20uncertainty-aware%20memory-based%20approach.%0ASpecifically%2C%20we%20suggest%20using%20an%20estimator%20based%20on%20the%20Bregman%20Information%0A%28BI%29%20to%20compute%20the%20model%27s%20variance%20at%20the%20sample%20level.%20Through%20measures%20of%0Apredictive%20uncertainty%2C%20we%20retrieve%20samples%20with%20specific%20characteristics%2C%20and%0A-%20by%20retraining%20the%20model%20on%20such%20samples%20-%20we%20demonstrate%20the%20potential%20of%0Athis%20approach%20to%20reduce%20the%20forgetting%20effect%20in%20realistic%20settings%20while%0Amaintaining%20data%20confidentiality%20and%20competitive%20communication%20efficiency%0Acompared%20to%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18925v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Continual%2520Learning%2520Goes%2520Online%253A%2520Uncertainty-Aware%2520Memory%250A%2520%2520Management%2520for%2520Vision%2520Tasks%2520and%2520Beyond%26entry.906535625%3DGiuseppe%2520Serra%2520and%2520Florian%2520Buettner%26entry.1292438233%3D%2520%2520Given%2520the%2520ability%2520to%2520model%2520more%2520realistic%2520and%2520dynamic%2520problems%252C%2520Federated%250AContinual%2520Learning%2520%2528FCL%2529%2520has%2520been%2520increasingly%2520investigated%2520recently.%2520A%250Awell-known%2520problem%2520encountered%2520in%2520this%2520setting%2520is%2520the%2520so-called%2520catastrophic%250Aforgetting%252C%2520for%2520which%2520the%2520learning%2520model%2520is%2520inclined%2520to%2520focus%2520on%2520more%2520recent%250Atasks%2520while%2520forgetting%2520the%2520previously%2520learned%2520knowledge.%2520The%2520majority%2520of%2520the%250Acurrent%2520approaches%2520in%2520FCL%2520propose%2520generative-based%2520solutions%2520to%2520solve%2520said%250Aproblem.%2520However%252C%2520this%2520setting%2520requires%2520multiple%2520training%2520epochs%2520over%2520the%2520data%252C%250Aimplying%2520an%2520offline%2520setting%2520where%2520datasets%2520are%2520stored%2520locally%2520and%2520remain%250Aunchanged%2520over%2520time.%2520Furthermore%252C%2520the%2520proposed%2520solutions%2520are%2520tailored%2520for%250Avision%2520tasks%2520solely.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520new%2520approach%250Ato%2520deal%2520with%2520different%2520modalities%2520in%2520the%2520online%2520scenario%2520where%2520new%2520data%2520arrive%250Ain%2520streams%2520of%2520mini-batches%2520that%2520can%2520only%2520be%2520processed%2520once.%2520To%2520solve%250Acatastrophic%2520forgetting%252C%2520we%2520propose%2520an%2520uncertainty-aware%2520memory-based%2520approach.%250ASpecifically%252C%2520we%2520suggest%2520using%2520an%2520estimator%2520based%2520on%2520the%2520Bregman%2520Information%250A%2528BI%2529%2520to%2520compute%2520the%2520model%2527s%2520variance%2520at%2520the%2520sample%2520level.%2520Through%2520measures%2520of%250Apredictive%2520uncertainty%252C%2520we%2520retrieve%2520samples%2520with%2520specific%2520characteristics%252C%2520and%250A-%2520by%2520retraining%2520the%2520model%2520on%2520such%2520samples%2520-%2520we%2520demonstrate%2520the%2520potential%2520of%250Athis%2520approach%2520to%2520reduce%2520the%2520forgetting%2520effect%2520in%2520realistic%2520settings%2520while%250Amaintaining%2520data%2520confidentiality%2520and%2520competitive%2520communication%2520efficiency%250Acompared%2520to%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18925v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Continual%20Learning%20Goes%20Online%3A%20Uncertainty-Aware%20Memory%0A%20%20Management%20for%20Vision%20Tasks%20and%20Beyond&entry.906535625=Giuseppe%20Serra%20and%20Florian%20Buettner&entry.1292438233=%20%20Given%20the%20ability%20to%20model%20more%20realistic%20and%20dynamic%20problems%2C%20Federated%0AContinual%20Learning%20%28FCL%29%20has%20been%20increasingly%20investigated%20recently.%20A%0Awell-known%20problem%20encountered%20in%20this%20setting%20is%20the%20so-called%20catastrophic%0Aforgetting%2C%20for%20which%20the%20learning%20model%20is%20inclined%20to%20focus%20on%20more%20recent%0Atasks%20while%20forgetting%20the%20previously%20learned%20knowledge.%20The%20majority%20of%20the%0Acurrent%20approaches%20in%20FCL%20propose%20generative-based%20solutions%20to%20solve%20said%0Aproblem.%20However%2C%20this%20setting%20requires%20multiple%20training%20epochs%20over%20the%20data%2C%0Aimplying%20an%20offline%20setting%20where%20datasets%20are%20stored%20locally%20and%20remain%0Aunchanged%20over%20time.%20Furthermore%2C%20the%20proposed%20solutions%20are%20tailored%20for%0Avision%20tasks%20solely.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20new%20approach%0Ato%20deal%20with%20different%20modalities%20in%20the%20online%20scenario%20where%20new%20data%20arrive%0Ain%20streams%20of%20mini-batches%20that%20can%20only%20be%20processed%20once.%20To%20solve%0Acatastrophic%20forgetting%2C%20we%20propose%20an%20uncertainty-aware%20memory-based%20approach.%0ASpecifically%2C%20we%20suggest%20using%20an%20estimator%20based%20on%20the%20Bregman%20Information%0A%28BI%29%20to%20compute%20the%20model%27s%20variance%20at%20the%20sample%20level.%20Through%20measures%20of%0Apredictive%20uncertainty%2C%20we%20retrieve%20samples%20with%20specific%20characteristics%2C%20and%0A-%20by%20retraining%20the%20model%20on%20such%20samples%20-%20we%20demonstrate%20the%20potential%20of%0Athis%20approach%20to%20reduce%20the%20forgetting%20effect%20in%20realistic%20settings%20while%0Amaintaining%20data%20confidentiality%20and%20competitive%20communication%20efficiency%0Acompared%20to%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18925v3&entry.124074799=Read"},
{"title": "MimicTalk: Mimicking a personalized and expressive 3D talking face in\n  minutes", "author": "Zhenhui Ye and Tianyun Zhong and Yi Ren and Ziyue Jiang and Jiawei Huang and Rongjie Huang and Jinglin Liu and Jinzheng He and Chen Zhang and Zehan Wang and Xize Chen and Xiang Yin and Zhou Zhao", "abstract": "  Talking face generation (TFG) aims to animate a target identity's face to\ncreate realistic talking videos. Personalized TFG is a variant that emphasizes\nthe perceptual identity similarity of the synthesized result (from the\nperspective of appearance and talking style). While previous works typically\nsolve this problem by learning an individual neural radiance field (NeRF) for\neach identity to implicitly store its static and dynamic information, we find\nit inefficient and non-generalized due to the per-identity-per-training\nframework and the limited training data. To this end, we propose MimicTalk, the\nfirst attempt that exploits the rich knowledge from a NeRF-based\nperson-agnostic generic model for improving the efficiency and robustness of\npersonalized TFG. To be specific, (1) we first come up with a person-agnostic\n3D TFG model as the base model and propose to adapt it into a specific\nidentity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help\nthe model learn the personalized static appearance and facial dynamic features;\n(3) To generate the facial motion of the personalized talking style, we propose\nan in-context stylized audio-to-motion model that mimics the implicit talking\nstyle provided in the reference video without information loss by an explicit\nstyle representation. The adaptation process to an unseen identity can be\nperformed in 15 minutes, which is 47 times faster than previous\nperson-dependent methods. Experiments show that our MimicTalk surpasses\nprevious baselines regarding video quality, efficiency, and expressiveness.\nSource code and video samples are available at https://mimictalk.github.io .\n", "link": "http://arxiv.org/abs/2410.06734v2", "date": "2024-10-15", "relevancy": 2.3668, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6203}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6114}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MimicTalk%3A%20Mimicking%20a%20personalized%20and%20expressive%203D%20talking%20face%20in%0A%20%20minutes&body=Title%3A%20MimicTalk%3A%20Mimicking%20a%20personalized%20and%20expressive%203D%20talking%20face%20in%0A%20%20minutes%0AAuthor%3A%20Zhenhui%20Ye%20and%20Tianyun%20Zhong%20and%20Yi%20Ren%20and%20Ziyue%20Jiang%20and%20Jiawei%20Huang%20and%20Rongjie%20Huang%20and%20Jinglin%20Liu%20and%20Jinzheng%20He%20and%20Chen%20Zhang%20and%20Zehan%20Wang%20and%20Xize%20Chen%20and%20Xiang%20Yin%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Talking%20face%20generation%20%28TFG%29%20aims%20to%20animate%20a%20target%20identity%27s%20face%20to%0Acreate%20realistic%20talking%20videos.%20Personalized%20TFG%20is%20a%20variant%20that%20emphasizes%0Athe%20perceptual%20identity%20similarity%20of%20the%20synthesized%20result%20%28from%20the%0Aperspective%20of%20appearance%20and%20talking%20style%29.%20While%20previous%20works%20typically%0Asolve%20this%20problem%20by%20learning%20an%20individual%20neural%20radiance%20field%20%28NeRF%29%20for%0Aeach%20identity%20to%20implicitly%20store%20its%20static%20and%20dynamic%20information%2C%20we%20find%0Ait%20inefficient%20and%20non-generalized%20due%20to%20the%20per-identity-per-training%0Aframework%20and%20the%20limited%20training%20data.%20To%20this%20end%2C%20we%20propose%20MimicTalk%2C%20the%0Afirst%20attempt%20that%20exploits%20the%20rich%20knowledge%20from%20a%20NeRF-based%0Aperson-agnostic%20generic%20model%20for%20improving%20the%20efficiency%20and%20robustness%20of%0Apersonalized%20TFG.%20To%20be%20specific%2C%20%281%29%20we%20first%20come%20up%20with%20a%20person-agnostic%0A3D%20TFG%20model%20as%20the%20base%20model%20and%20propose%20to%20adapt%20it%20into%20a%20specific%0Aidentity%3B%20%282%29%20we%20propose%20a%20static-dynamic-hybrid%20adaptation%20pipeline%20to%20help%0Athe%20model%20learn%20the%20personalized%20static%20appearance%20and%20facial%20dynamic%20features%3B%0A%283%29%20To%20generate%20the%20facial%20motion%20of%20the%20personalized%20talking%20style%2C%20we%20propose%0Aan%20in-context%20stylized%20audio-to-motion%20model%20that%20mimics%20the%20implicit%20talking%0Astyle%20provided%20in%20the%20reference%20video%20without%20information%20loss%20by%20an%20explicit%0Astyle%20representation.%20The%20adaptation%20process%20to%20an%20unseen%20identity%20can%20be%0Aperformed%20in%2015%20minutes%2C%20which%20is%2047%20times%20faster%20than%20previous%0Aperson-dependent%20methods.%20Experiments%20show%20that%20our%20MimicTalk%20surpasses%0Aprevious%20baselines%20regarding%20video%20quality%2C%20efficiency%2C%20and%20expressiveness.%0ASource%20code%20and%20video%20samples%20are%20available%20at%20https%3A//mimictalk.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimicTalk%253A%2520Mimicking%2520a%2520personalized%2520and%2520expressive%25203D%2520talking%2520face%2520in%250A%2520%2520minutes%26entry.906535625%3DZhenhui%2520Ye%2520and%2520Tianyun%2520Zhong%2520and%2520Yi%2520Ren%2520and%2520Ziyue%2520Jiang%2520and%2520Jiawei%2520Huang%2520and%2520Rongjie%2520Huang%2520and%2520Jinglin%2520Liu%2520and%2520Jinzheng%2520He%2520and%2520Chen%2520Zhang%2520and%2520Zehan%2520Wang%2520and%2520Xize%2520Chen%2520and%2520Xiang%2520Yin%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Talking%2520face%2520generation%2520%2528TFG%2529%2520aims%2520to%2520animate%2520a%2520target%2520identity%2527s%2520face%2520to%250Acreate%2520realistic%2520talking%2520videos.%2520Personalized%2520TFG%2520is%2520a%2520variant%2520that%2520emphasizes%250Athe%2520perceptual%2520identity%2520similarity%2520of%2520the%2520synthesized%2520result%2520%2528from%2520the%250Aperspective%2520of%2520appearance%2520and%2520talking%2520style%2529.%2520While%2520previous%2520works%2520typically%250Asolve%2520this%2520problem%2520by%2520learning%2520an%2520individual%2520neural%2520radiance%2520field%2520%2528NeRF%2529%2520for%250Aeach%2520identity%2520to%2520implicitly%2520store%2520its%2520static%2520and%2520dynamic%2520information%252C%2520we%2520find%250Ait%2520inefficient%2520and%2520non-generalized%2520due%2520to%2520the%2520per-identity-per-training%250Aframework%2520and%2520the%2520limited%2520training%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520MimicTalk%252C%2520the%250Afirst%2520attempt%2520that%2520exploits%2520the%2520rich%2520knowledge%2520from%2520a%2520NeRF-based%250Aperson-agnostic%2520generic%2520model%2520for%2520improving%2520the%2520efficiency%2520and%2520robustness%2520of%250Apersonalized%2520TFG.%2520To%2520be%2520specific%252C%2520%25281%2529%2520we%2520first%2520come%2520up%2520with%2520a%2520person-agnostic%250A3D%2520TFG%2520model%2520as%2520the%2520base%2520model%2520and%2520propose%2520to%2520adapt%2520it%2520into%2520a%2520specific%250Aidentity%253B%2520%25282%2529%2520we%2520propose%2520a%2520static-dynamic-hybrid%2520adaptation%2520pipeline%2520to%2520help%250Athe%2520model%2520learn%2520the%2520personalized%2520static%2520appearance%2520and%2520facial%2520dynamic%2520features%253B%250A%25283%2529%2520To%2520generate%2520the%2520facial%2520motion%2520of%2520the%2520personalized%2520talking%2520style%252C%2520we%2520propose%250Aan%2520in-context%2520stylized%2520audio-to-motion%2520model%2520that%2520mimics%2520the%2520implicit%2520talking%250Astyle%2520provided%2520in%2520the%2520reference%2520video%2520without%2520information%2520loss%2520by%2520an%2520explicit%250Astyle%2520representation.%2520The%2520adaptation%2520process%2520to%2520an%2520unseen%2520identity%2520can%2520be%250Aperformed%2520in%252015%2520minutes%252C%2520which%2520is%252047%2520times%2520faster%2520than%2520previous%250Aperson-dependent%2520methods.%2520Experiments%2520show%2520that%2520our%2520MimicTalk%2520surpasses%250Aprevious%2520baselines%2520regarding%2520video%2520quality%252C%2520efficiency%252C%2520and%2520expressiveness.%250ASource%2520code%2520and%2520video%2520samples%2520are%2520available%2520at%2520https%253A//mimictalk.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MimicTalk%3A%20Mimicking%20a%20personalized%20and%20expressive%203D%20talking%20face%20in%0A%20%20minutes&entry.906535625=Zhenhui%20Ye%20and%20Tianyun%20Zhong%20and%20Yi%20Ren%20and%20Ziyue%20Jiang%20and%20Jiawei%20Huang%20and%20Rongjie%20Huang%20and%20Jinglin%20Liu%20and%20Jinzheng%20He%20and%20Chen%20Zhang%20and%20Zehan%20Wang%20and%20Xize%20Chen%20and%20Xiang%20Yin%20and%20Zhou%20Zhao&entry.1292438233=%20%20Talking%20face%20generation%20%28TFG%29%20aims%20to%20animate%20a%20target%20identity%27s%20face%20to%0Acreate%20realistic%20talking%20videos.%20Personalized%20TFG%20is%20a%20variant%20that%20emphasizes%0Athe%20perceptual%20identity%20similarity%20of%20the%20synthesized%20result%20%28from%20the%0Aperspective%20of%20appearance%20and%20talking%20style%29.%20While%20previous%20works%20typically%0Asolve%20this%20problem%20by%20learning%20an%20individual%20neural%20radiance%20field%20%28NeRF%29%20for%0Aeach%20identity%20to%20implicitly%20store%20its%20static%20and%20dynamic%20information%2C%20we%20find%0Ait%20inefficient%20and%20non-generalized%20due%20to%20the%20per-identity-per-training%0Aframework%20and%20the%20limited%20training%20data.%20To%20this%20end%2C%20we%20propose%20MimicTalk%2C%20the%0Afirst%20attempt%20that%20exploits%20the%20rich%20knowledge%20from%20a%20NeRF-based%0Aperson-agnostic%20generic%20model%20for%20improving%20the%20efficiency%20and%20robustness%20of%0Apersonalized%20TFG.%20To%20be%20specific%2C%20%281%29%20we%20first%20come%20up%20with%20a%20person-agnostic%0A3D%20TFG%20model%20as%20the%20base%20model%20and%20propose%20to%20adapt%20it%20into%20a%20specific%0Aidentity%3B%20%282%29%20we%20propose%20a%20static-dynamic-hybrid%20adaptation%20pipeline%20to%20help%0Athe%20model%20learn%20the%20personalized%20static%20appearance%20and%20facial%20dynamic%20features%3B%0A%283%29%20To%20generate%20the%20facial%20motion%20of%20the%20personalized%20talking%20style%2C%20we%20propose%0Aan%20in-context%20stylized%20audio-to-motion%20model%20that%20mimics%20the%20implicit%20talking%0Astyle%20provided%20in%20the%20reference%20video%20without%20information%20loss%20by%20an%20explicit%0Astyle%20representation.%20The%20adaptation%20process%20to%20an%20unseen%20identity%20can%20be%0Aperformed%20in%2015%20minutes%2C%20which%20is%2047%20times%20faster%20than%20previous%0Aperson-dependent%20methods.%20Experiments%20show%20that%20our%20MimicTalk%20surpasses%0Aprevious%20baselines%20regarding%20video%20quality%2C%20efficiency%2C%20and%20expressiveness.%0ASource%20code%20and%20video%20samples%20are%20available%20at%20https%3A//mimictalk.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06734v2&entry.124074799=Read"},
{"title": "VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual\n  Language Models", "author": "Zihang Li and Haowen Hou", "abstract": "  Accurately understanding complex visual information is crucial for visual\nlanguage models (VLMs). Enhancing image resolution can improve visual\nperception capabilities, not only reducing hallucinations but also boosting\nperformance in tasks that demand high resolution, such as text-rich or document\nanalysis. In this paper, we present VisualRWKV-HD and VisualRWKV-UHD, two\nadvancements in the VisualRWKV model family, specifically designed to process\nhigh-resolution visual inputs. For VisualRWKV-HD, we developed a lossless\ndownsampling method to effectively integrate a high-resolution vision encoder\nwith low-resolution encoders, without extending the input sequence length. For\nthe VisualRWKV-UHD model, we enhanced image representation by dividing the\nimage into four segments, which are then recombined with the original image.\nThis technique allows the model to incorporate both high-resolution and\nlow-resolution features, effectively balancing coarse and fine-grained\ninformation. As a result, the model supports resolutions up to 4096 x 4096\npixels, offering a more detailed and comprehensive visual processing\ncapability. Both VisualRWKV-HD and VisualRWKV-UHD not only achieve strong\nresults on VLM benchmarks but also show marked improvements in performance for\ntext-rich tasks.\n", "link": "http://arxiv.org/abs/2410.11665v1", "date": "2024-10-15", "relevancy": 2.3668, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualRWKV-HD%20and%20UHD%3A%20Advancing%20High-Resolution%20Processing%20for%20Visual%0A%20%20Language%20Models&body=Title%3A%20VisualRWKV-HD%20and%20UHD%3A%20Advancing%20High-Resolution%20Processing%20for%20Visual%0A%20%20Language%20Models%0AAuthor%3A%20Zihang%20Li%20and%20Haowen%20Hou%0AAbstract%3A%20%20%20Accurately%20understanding%20complex%20visual%20information%20is%20crucial%20for%20visual%0Alanguage%20models%20%28VLMs%29.%20Enhancing%20image%20resolution%20can%20improve%20visual%0Aperception%20capabilities%2C%20not%20only%20reducing%20hallucinations%20but%20also%20boosting%0Aperformance%20in%20tasks%20that%20demand%20high%20resolution%2C%20such%20as%20text-rich%20or%20document%0Aanalysis.%20In%20this%20paper%2C%20we%20present%20VisualRWKV-HD%20and%20VisualRWKV-UHD%2C%20two%0Aadvancements%20in%20the%20VisualRWKV%20model%20family%2C%20specifically%20designed%20to%20process%0Ahigh-resolution%20visual%20inputs.%20For%20VisualRWKV-HD%2C%20we%20developed%20a%20lossless%0Adownsampling%20method%20to%20effectively%20integrate%20a%20high-resolution%20vision%20encoder%0Awith%20low-resolution%20encoders%2C%20without%20extending%20the%20input%20sequence%20length.%20For%0Athe%20VisualRWKV-UHD%20model%2C%20we%20enhanced%20image%20representation%20by%20dividing%20the%0Aimage%20into%20four%20segments%2C%20which%20are%20then%20recombined%20with%20the%20original%20image.%0AThis%20technique%20allows%20the%20model%20to%20incorporate%20both%20high-resolution%20and%0Alow-resolution%20features%2C%20effectively%20balancing%20coarse%20and%20fine-grained%0Ainformation.%20As%20a%20result%2C%20the%20model%20supports%20resolutions%20up%20to%204096%20x%204096%0Apixels%2C%20offering%20a%20more%20detailed%20and%20comprehensive%20visual%20processing%0Acapability.%20Both%20VisualRWKV-HD%20and%20VisualRWKV-UHD%20not%20only%20achieve%20strong%0Aresults%20on%20VLM%20benchmarks%20but%20also%20show%20marked%20improvements%20in%20performance%20for%0Atext-rich%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualRWKV-HD%2520and%2520UHD%253A%2520Advancing%2520High-Resolution%2520Processing%2520for%2520Visual%250A%2520%2520Language%2520Models%26entry.906535625%3DZihang%2520Li%2520and%2520Haowen%2520Hou%26entry.1292438233%3D%2520%2520Accurately%2520understanding%2520complex%2520visual%2520information%2520is%2520crucial%2520for%2520visual%250Alanguage%2520models%2520%2528VLMs%2529.%2520Enhancing%2520image%2520resolution%2520can%2520improve%2520visual%250Aperception%2520capabilities%252C%2520not%2520only%2520reducing%2520hallucinations%2520but%2520also%2520boosting%250Aperformance%2520in%2520tasks%2520that%2520demand%2520high%2520resolution%252C%2520such%2520as%2520text-rich%2520or%2520document%250Aanalysis.%2520In%2520this%2520paper%252C%2520we%2520present%2520VisualRWKV-HD%2520and%2520VisualRWKV-UHD%252C%2520two%250Aadvancements%2520in%2520the%2520VisualRWKV%2520model%2520family%252C%2520specifically%2520designed%2520to%2520process%250Ahigh-resolution%2520visual%2520inputs.%2520For%2520VisualRWKV-HD%252C%2520we%2520developed%2520a%2520lossless%250Adownsampling%2520method%2520to%2520effectively%2520integrate%2520a%2520high-resolution%2520vision%2520encoder%250Awith%2520low-resolution%2520encoders%252C%2520without%2520extending%2520the%2520input%2520sequence%2520length.%2520For%250Athe%2520VisualRWKV-UHD%2520model%252C%2520we%2520enhanced%2520image%2520representation%2520by%2520dividing%2520the%250Aimage%2520into%2520four%2520segments%252C%2520which%2520are%2520then%2520recombined%2520with%2520the%2520original%2520image.%250AThis%2520technique%2520allows%2520the%2520model%2520to%2520incorporate%2520both%2520high-resolution%2520and%250Alow-resolution%2520features%252C%2520effectively%2520balancing%2520coarse%2520and%2520fine-grained%250Ainformation.%2520As%2520a%2520result%252C%2520the%2520model%2520supports%2520resolutions%2520up%2520to%25204096%2520x%25204096%250Apixels%252C%2520offering%2520a%2520more%2520detailed%2520and%2520comprehensive%2520visual%2520processing%250Acapability.%2520Both%2520VisualRWKV-HD%2520and%2520VisualRWKV-UHD%2520not%2520only%2520achieve%2520strong%250Aresults%2520on%2520VLM%2520benchmarks%2520but%2520also%2520show%2520marked%2520improvements%2520in%2520performance%2520for%250Atext-rich%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualRWKV-HD%20and%20UHD%3A%20Advancing%20High-Resolution%20Processing%20for%20Visual%0A%20%20Language%20Models&entry.906535625=Zihang%20Li%20and%20Haowen%20Hou&entry.1292438233=%20%20Accurately%20understanding%20complex%20visual%20information%20is%20crucial%20for%20visual%0Alanguage%20models%20%28VLMs%29.%20Enhancing%20image%20resolution%20can%20improve%20visual%0Aperception%20capabilities%2C%20not%20only%20reducing%20hallucinations%20but%20also%20boosting%0Aperformance%20in%20tasks%20that%20demand%20high%20resolution%2C%20such%20as%20text-rich%20or%20document%0Aanalysis.%20In%20this%20paper%2C%20we%20present%20VisualRWKV-HD%20and%20VisualRWKV-UHD%2C%20two%0Aadvancements%20in%20the%20VisualRWKV%20model%20family%2C%20specifically%20designed%20to%20process%0Ahigh-resolution%20visual%20inputs.%20For%20VisualRWKV-HD%2C%20we%20developed%20a%20lossless%0Adownsampling%20method%20to%20effectively%20integrate%20a%20high-resolution%20vision%20encoder%0Awith%20low-resolution%20encoders%2C%20without%20extending%20the%20input%20sequence%20length.%20For%0Athe%20VisualRWKV-UHD%20model%2C%20we%20enhanced%20image%20representation%20by%20dividing%20the%0Aimage%20into%20four%20segments%2C%20which%20are%20then%20recombined%20with%20the%20original%20image.%0AThis%20technique%20allows%20the%20model%20to%20incorporate%20both%20high-resolution%20and%0Alow-resolution%20features%2C%20effectively%20balancing%20coarse%20and%20fine-grained%0Ainformation.%20As%20a%20result%2C%20the%20model%20supports%20resolutions%20up%20to%204096%20x%204096%0Apixels%2C%20offering%20a%20more%20detailed%20and%20comprehensive%20visual%20processing%0Acapability.%20Both%20VisualRWKV-HD%20and%20VisualRWKV-UHD%20not%20only%20achieve%20strong%0Aresults%20on%20VLM%20benchmarks%20but%20also%20show%20marked%20improvements%20in%20performance%20for%0Atext-rich%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11665v1&entry.124074799=Read"},
{"title": "NavTopo: Leveraging Topological Maps For Autonomous Navigation Of a\n  Mobile Robot", "author": "Kirill Muravyev and Konstantin Yakovlev", "abstract": "  Autonomous navigation of a mobile robot is a challenging task which requires\nability of mapping, localization, path planning and path following.\nConventional mapping methods build a dense metric map like an occupancy grid,\nwhich is affected by odometry error accumulation and consumes a lot of memory\nand computations in large environments. Another approach to mapping is the\nusage of topological properties, e.g. adjacency of locations in the\nenvironment. Topological maps are less prone to odometry error accumulation and\nhigh resources consumption, and also enable fast path planning because of the\ngraph sparsity. Based on this idea, we proposed NavTopo - a full navigation\npipeline based on topological map and two-level path planning. The pipeline\nlocalizes in the graph by matching neural network descriptors and 2D\nprojections of the input point clouds, which significantly reduces memory\nconsumption compared to metric and topological point cloud-based approaches. We\ntest our approach in a large indoor photo-relaistic simulated environment and\ncompare it to a metric map-based approach based on popular metric mapping\nmethod RTAB-MAP. The experimental results show that our topological approach\nsignificantly outperforms the metric one in terms of performance, keeping\nproper navigational efficiency.\n", "link": "http://arxiv.org/abs/2410.11492v1", "date": "2024-10-15", "relevancy": 2.3655, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6278}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5723}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NavTopo%3A%20Leveraging%20Topological%20Maps%20For%20Autonomous%20Navigation%20Of%20a%0A%20%20Mobile%20Robot&body=Title%3A%20NavTopo%3A%20Leveraging%20Topological%20Maps%20For%20Autonomous%20Navigation%20Of%20a%0A%20%20Mobile%20Robot%0AAuthor%3A%20Kirill%20Muravyev%20and%20Konstantin%20Yakovlev%0AAbstract%3A%20%20%20Autonomous%20navigation%20of%20a%20mobile%20robot%20is%20a%20challenging%20task%20which%20requires%0Aability%20of%20mapping%2C%20localization%2C%20path%20planning%20and%20path%20following.%0AConventional%20mapping%20methods%20build%20a%20dense%20metric%20map%20like%20an%20occupancy%20grid%2C%0Awhich%20is%20affected%20by%20odometry%20error%20accumulation%20and%20consumes%20a%20lot%20of%20memory%0Aand%20computations%20in%20large%20environments.%20Another%20approach%20to%20mapping%20is%20the%0Ausage%20of%20topological%20properties%2C%20e.g.%20adjacency%20of%20locations%20in%20the%0Aenvironment.%20Topological%20maps%20are%20less%20prone%20to%20odometry%20error%20accumulation%20and%0Ahigh%20resources%20consumption%2C%20and%20also%20enable%20fast%20path%20planning%20because%20of%20the%0Agraph%20sparsity.%20Based%20on%20this%20idea%2C%20we%20proposed%20NavTopo%20-%20a%20full%20navigation%0Apipeline%20based%20on%20topological%20map%20and%20two-level%20path%20planning.%20The%20pipeline%0Alocalizes%20in%20the%20graph%20by%20matching%20neural%20network%20descriptors%20and%202D%0Aprojections%20of%20the%20input%20point%20clouds%2C%20which%20significantly%20reduces%20memory%0Aconsumption%20compared%20to%20metric%20and%20topological%20point%20cloud-based%20approaches.%20We%0Atest%20our%20approach%20in%20a%20large%20indoor%20photo-relaistic%20simulated%20environment%20and%0Acompare%20it%20to%20a%20metric%20map-based%20approach%20based%20on%20popular%20metric%20mapping%0Amethod%20RTAB-MAP.%20The%20experimental%20results%20show%20that%20our%20topological%20approach%0Asignificantly%20outperforms%20the%20metric%20one%20in%20terms%20of%20performance%2C%20keeping%0Aproper%20navigational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavTopo%253A%2520Leveraging%2520Topological%2520Maps%2520For%2520Autonomous%2520Navigation%2520Of%2520a%250A%2520%2520Mobile%2520Robot%26entry.906535625%3DKirill%2520Muravyev%2520and%2520Konstantin%2520Yakovlev%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520of%2520a%2520mobile%2520robot%2520is%2520a%2520challenging%2520task%2520which%2520requires%250Aability%2520of%2520mapping%252C%2520localization%252C%2520path%2520planning%2520and%2520path%2520following.%250AConventional%2520mapping%2520methods%2520build%2520a%2520dense%2520metric%2520map%2520like%2520an%2520occupancy%2520grid%252C%250Awhich%2520is%2520affected%2520by%2520odometry%2520error%2520accumulation%2520and%2520consumes%2520a%2520lot%2520of%2520memory%250Aand%2520computations%2520in%2520large%2520environments.%2520Another%2520approach%2520to%2520mapping%2520is%2520the%250Ausage%2520of%2520topological%2520properties%252C%2520e.g.%2520adjacency%2520of%2520locations%2520in%2520the%250Aenvironment.%2520Topological%2520maps%2520are%2520less%2520prone%2520to%2520odometry%2520error%2520accumulation%2520and%250Ahigh%2520resources%2520consumption%252C%2520and%2520also%2520enable%2520fast%2520path%2520planning%2520because%2520of%2520the%250Agraph%2520sparsity.%2520Based%2520on%2520this%2520idea%252C%2520we%2520proposed%2520NavTopo%2520-%2520a%2520full%2520navigation%250Apipeline%2520based%2520on%2520topological%2520map%2520and%2520two-level%2520path%2520planning.%2520The%2520pipeline%250Alocalizes%2520in%2520the%2520graph%2520by%2520matching%2520neural%2520network%2520descriptors%2520and%25202D%250Aprojections%2520of%2520the%2520input%2520point%2520clouds%252C%2520which%2520significantly%2520reduces%2520memory%250Aconsumption%2520compared%2520to%2520metric%2520and%2520topological%2520point%2520cloud-based%2520approaches.%2520We%250Atest%2520our%2520approach%2520in%2520a%2520large%2520indoor%2520photo-relaistic%2520simulated%2520environment%2520and%250Acompare%2520it%2520to%2520a%2520metric%2520map-based%2520approach%2520based%2520on%2520popular%2520metric%2520mapping%250Amethod%2520RTAB-MAP.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520topological%2520approach%250Asignificantly%2520outperforms%2520the%2520metric%2520one%2520in%2520terms%2520of%2520performance%252C%2520keeping%250Aproper%2520navigational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NavTopo%3A%20Leveraging%20Topological%20Maps%20For%20Autonomous%20Navigation%20Of%20a%0A%20%20Mobile%20Robot&entry.906535625=Kirill%20Muravyev%20and%20Konstantin%20Yakovlev&entry.1292438233=%20%20Autonomous%20navigation%20of%20a%20mobile%20robot%20is%20a%20challenging%20task%20which%20requires%0Aability%20of%20mapping%2C%20localization%2C%20path%20planning%20and%20path%20following.%0AConventional%20mapping%20methods%20build%20a%20dense%20metric%20map%20like%20an%20occupancy%20grid%2C%0Awhich%20is%20affected%20by%20odometry%20error%20accumulation%20and%20consumes%20a%20lot%20of%20memory%0Aand%20computations%20in%20large%20environments.%20Another%20approach%20to%20mapping%20is%20the%0Ausage%20of%20topological%20properties%2C%20e.g.%20adjacency%20of%20locations%20in%20the%0Aenvironment.%20Topological%20maps%20are%20less%20prone%20to%20odometry%20error%20accumulation%20and%0Ahigh%20resources%20consumption%2C%20and%20also%20enable%20fast%20path%20planning%20because%20of%20the%0Agraph%20sparsity.%20Based%20on%20this%20idea%2C%20we%20proposed%20NavTopo%20-%20a%20full%20navigation%0Apipeline%20based%20on%20topological%20map%20and%20two-level%20path%20planning.%20The%20pipeline%0Alocalizes%20in%20the%20graph%20by%20matching%20neural%20network%20descriptors%20and%202D%0Aprojections%20of%20the%20input%20point%20clouds%2C%20which%20significantly%20reduces%20memory%0Aconsumption%20compared%20to%20metric%20and%20topological%20point%20cloud-based%20approaches.%20We%0Atest%20our%20approach%20in%20a%20large%20indoor%20photo-relaistic%20simulated%20environment%20and%0Acompare%20it%20to%20a%20metric%20map-based%20approach%20based%20on%20popular%20metric%20mapping%0Amethod%20RTAB-MAP.%20The%20experimental%20results%20show%20that%20our%20topological%20approach%0Asignificantly%20outperforms%20the%20metric%20one%20in%20terms%20of%20performance%2C%20keeping%0Aproper%20navigational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11492v1&entry.124074799=Read"},
{"title": "Depth Estimation From Monocular Images With Enhanced Encoder-Decoder\n  Architecture", "author": "Dabbrata Das and Argho Deb Das and Farhan Sadaf", "abstract": "  Estimating depth from a single 2D image is a challenging task because of the\nneed for stereo or multi-view data, which normally provides depth information.\nThis paper deals with this challenge by introducing a novel deep learning-based\napproach using an encoder-decoder architecture, where the Inception-ResNet-v2\nmodel is utilized as the encoder. According to the available literature, this\nis the first instance of using Inception-ResNet-v2 as an encoder for monocular\ndepth estimation, illustrating better performance than previous models. The use\nof Inception-ResNet-v2 enables our model to capture complex objects and\nfine-grained details effectively that are generally difficult to predict.\nBesides, our model incorporates multi-scale feature extraction to enhance depth\nprediction accuracy across different kinds of object sizes and distances. We\npropose a composite loss function consisting of depth loss, gradient edge loss,\nand SSIM loss, where the weights are fine-tuned to optimize the weighted sum,\nensuring better balance across different aspects of depth estimation.\nExperimental results on the NYU Depth V2 dataset show that our model achieves\nstate-of-the-art performance, with an ARE of 0.064, RMSE of 0.228, and accuracy\n($\\delta$ $<1.25$) of 89.3%. These metrics demonstrate that our model\neffectively predicts depth, even in challenging circumstances, providing a\nscalable solution for real-world applications in robotics, 3D reconstruction,\nand augmented reality.\n", "link": "http://arxiv.org/abs/2410.11610v1", "date": "2024-10-15", "relevancy": 2.3641, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Estimation%20From%20Monocular%20Images%20With%20Enhanced%20Encoder-Decoder%0A%20%20Architecture&body=Title%3A%20Depth%20Estimation%20From%20Monocular%20Images%20With%20Enhanced%20Encoder-Decoder%0A%20%20Architecture%0AAuthor%3A%20Dabbrata%20Das%20and%20Argho%20Deb%20Das%20and%20Farhan%20Sadaf%0AAbstract%3A%20%20%20Estimating%20depth%20from%20a%20single%202D%20image%20is%20a%20challenging%20task%20because%20of%20the%0Aneed%20for%20stereo%20or%20multi-view%20data%2C%20which%20normally%20provides%20depth%20information.%0AThis%20paper%20deals%20with%20this%20challenge%20by%20introducing%20a%20novel%20deep%20learning-based%0Aapproach%20using%20an%20encoder-decoder%20architecture%2C%20where%20the%20Inception-ResNet-v2%0Amodel%20is%20utilized%20as%20the%20encoder.%20According%20to%20the%20available%20literature%2C%20this%0Ais%20the%20first%20instance%20of%20using%20Inception-ResNet-v2%20as%20an%20encoder%20for%20monocular%0Adepth%20estimation%2C%20illustrating%20better%20performance%20than%20previous%20models.%20The%20use%0Aof%20Inception-ResNet-v2%20enables%20our%20model%20to%20capture%20complex%20objects%20and%0Afine-grained%20details%20effectively%20that%20are%20generally%20difficult%20to%20predict.%0ABesides%2C%20our%20model%20incorporates%20multi-scale%20feature%20extraction%20to%20enhance%20depth%0Aprediction%20accuracy%20across%20different%20kinds%20of%20object%20sizes%20and%20distances.%20We%0Apropose%20a%20composite%20loss%20function%20consisting%20of%20depth%20loss%2C%20gradient%20edge%20loss%2C%0Aand%20SSIM%20loss%2C%20where%20the%20weights%20are%20fine-tuned%20to%20optimize%20the%20weighted%20sum%2C%0Aensuring%20better%20balance%20across%20different%20aspects%20of%20depth%20estimation.%0AExperimental%20results%20on%20the%20NYU%20Depth%20V2%20dataset%20show%20that%20our%20model%20achieves%0Astate-of-the-art%20performance%2C%20with%20an%20ARE%20of%200.064%2C%20RMSE%20of%200.228%2C%20and%20accuracy%0A%28%24%5Cdelta%24%20%24%3C1.25%24%29%20of%2089.3%25.%20These%20metrics%20demonstrate%20that%20our%20model%0Aeffectively%20predicts%20depth%2C%20even%20in%20challenging%20circumstances%2C%20providing%20a%0Ascalable%20solution%20for%20real-world%20applications%20in%20robotics%2C%203D%20reconstruction%2C%0Aand%20augmented%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Estimation%2520From%2520Monocular%2520Images%2520With%2520Enhanced%2520Encoder-Decoder%250A%2520%2520Architecture%26entry.906535625%3DDabbrata%2520Das%2520and%2520Argho%2520Deb%2520Das%2520and%2520Farhan%2520Sadaf%26entry.1292438233%3D%2520%2520Estimating%2520depth%2520from%2520a%2520single%25202D%2520image%2520is%2520a%2520challenging%2520task%2520because%2520of%2520the%250Aneed%2520for%2520stereo%2520or%2520multi-view%2520data%252C%2520which%2520normally%2520provides%2520depth%2520information.%250AThis%2520paper%2520deals%2520with%2520this%2520challenge%2520by%2520introducing%2520a%2520novel%2520deep%2520learning-based%250Aapproach%2520using%2520an%2520encoder-decoder%2520architecture%252C%2520where%2520the%2520Inception-ResNet-v2%250Amodel%2520is%2520utilized%2520as%2520the%2520encoder.%2520According%2520to%2520the%2520available%2520literature%252C%2520this%250Ais%2520the%2520first%2520instance%2520of%2520using%2520Inception-ResNet-v2%2520as%2520an%2520encoder%2520for%2520monocular%250Adepth%2520estimation%252C%2520illustrating%2520better%2520performance%2520than%2520previous%2520models.%2520The%2520use%250Aof%2520Inception-ResNet-v2%2520enables%2520our%2520model%2520to%2520capture%2520complex%2520objects%2520and%250Afine-grained%2520details%2520effectively%2520that%2520are%2520generally%2520difficult%2520to%2520predict.%250ABesides%252C%2520our%2520model%2520incorporates%2520multi-scale%2520feature%2520extraction%2520to%2520enhance%2520depth%250Aprediction%2520accuracy%2520across%2520different%2520kinds%2520of%2520object%2520sizes%2520and%2520distances.%2520We%250Apropose%2520a%2520composite%2520loss%2520function%2520consisting%2520of%2520depth%2520loss%252C%2520gradient%2520edge%2520loss%252C%250Aand%2520SSIM%2520loss%252C%2520where%2520the%2520weights%2520are%2520fine-tuned%2520to%2520optimize%2520the%2520weighted%2520sum%252C%250Aensuring%2520better%2520balance%2520across%2520different%2520aspects%2520of%2520depth%2520estimation.%250AExperimental%2520results%2520on%2520the%2520NYU%2520Depth%2520V2%2520dataset%2520show%2520that%2520our%2520model%2520achieves%250Astate-of-the-art%2520performance%252C%2520with%2520an%2520ARE%2520of%25200.064%252C%2520RMSE%2520of%25200.228%252C%2520and%2520accuracy%250A%2528%2524%255Cdelta%2524%2520%2524%253C1.25%2524%2529%2520of%252089.3%2525.%2520These%2520metrics%2520demonstrate%2520that%2520our%2520model%250Aeffectively%2520predicts%2520depth%252C%2520even%2520in%2520challenging%2520circumstances%252C%2520providing%2520a%250Ascalable%2520solution%2520for%2520real-world%2520applications%2520in%2520robotics%252C%25203D%2520reconstruction%252C%250Aand%2520augmented%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Estimation%20From%20Monocular%20Images%20With%20Enhanced%20Encoder-Decoder%0A%20%20Architecture&entry.906535625=Dabbrata%20Das%20and%20Argho%20Deb%20Das%20and%20Farhan%20Sadaf&entry.1292438233=%20%20Estimating%20depth%20from%20a%20single%202D%20image%20is%20a%20challenging%20task%20because%20of%20the%0Aneed%20for%20stereo%20or%20multi-view%20data%2C%20which%20normally%20provides%20depth%20information.%0AThis%20paper%20deals%20with%20this%20challenge%20by%20introducing%20a%20novel%20deep%20learning-based%0Aapproach%20using%20an%20encoder-decoder%20architecture%2C%20where%20the%20Inception-ResNet-v2%0Amodel%20is%20utilized%20as%20the%20encoder.%20According%20to%20the%20available%20literature%2C%20this%0Ais%20the%20first%20instance%20of%20using%20Inception-ResNet-v2%20as%20an%20encoder%20for%20monocular%0Adepth%20estimation%2C%20illustrating%20better%20performance%20than%20previous%20models.%20The%20use%0Aof%20Inception-ResNet-v2%20enables%20our%20model%20to%20capture%20complex%20objects%20and%0Afine-grained%20details%20effectively%20that%20are%20generally%20difficult%20to%20predict.%0ABesides%2C%20our%20model%20incorporates%20multi-scale%20feature%20extraction%20to%20enhance%20depth%0Aprediction%20accuracy%20across%20different%20kinds%20of%20object%20sizes%20and%20distances.%20We%0Apropose%20a%20composite%20loss%20function%20consisting%20of%20depth%20loss%2C%20gradient%20edge%20loss%2C%0Aand%20SSIM%20loss%2C%20where%20the%20weights%20are%20fine-tuned%20to%20optimize%20the%20weighted%20sum%2C%0Aensuring%20better%20balance%20across%20different%20aspects%20of%20depth%20estimation.%0AExperimental%20results%20on%20the%20NYU%20Depth%20V2%20dataset%20show%20that%20our%20model%20achieves%0Astate-of-the-art%20performance%2C%20with%20an%20ARE%20of%200.064%2C%20RMSE%20of%200.228%2C%20and%20accuracy%0A%28%24%5Cdelta%24%20%24%3C1.25%24%29%20of%2089.3%25.%20These%20metrics%20demonstrate%20that%20our%20model%0Aeffectively%20predicts%20depth%2C%20even%20in%20challenging%20circumstances%2C%20providing%20a%0Ascalable%20solution%20for%20real-world%20applications%20in%20robotics%2C%203D%20reconstruction%2C%0Aand%20augmented%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11610v1&entry.124074799=Read"},
{"title": "Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor", "author": "Shaokui Wei and Hongyuan Zha and Baoyuan Wu", "abstract": "  Data-poisoning backdoor attacks are serious security threats to machine\nlearning models, where an adversary can manipulate the training dataset to\ninject backdoors into models. In this paper, we focus on in-training backdoor\ndefense, aiming to train a clean model even when the dataset may be potentially\npoisoned. Unlike most existing methods that primarily detect and remove/unlearn\nsuspicious samples to mitigate malicious backdoor attacks, we propose a novel\ndefense approach called PDB (Proactive Defensive Backdoor). Specifically, PDB\nleverages the home-field advantage of defenders by proactively injecting a\ndefensive backdoor into the model during training. Taking advantage of\ncontrolling the training process, the defensive backdoor is designed to\nsuppress the malicious backdoor effectively while remaining secret to\nattackers. In addition, we introduce a reversible mapping to determine the\ndefensive target label. During inference, PDB embeds a defensive trigger in the\ninputs and reverses the model's prediction, suppressing malicious backdoor and\nensuring the model's utility on the original task. Experimental results across\nvarious datasets and models demonstrate that our approach achieves\nstate-of-the-art defense performance against a wide range of backdoor attacks.\nThe code is available at\nhttps://github.com/shawkui/Proactive_Defensive_Backdoor.\n", "link": "http://arxiv.org/abs/2405.16112v2", "date": "2024-10-15", "relevancy": 2.36, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4856}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4664}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Backdoor%20Attack%20by%20Injecting%20Proactive%20Defensive%20Backdoor&body=Title%3A%20Mitigating%20Backdoor%20Attack%20by%20Injecting%20Proactive%20Defensive%20Backdoor%0AAuthor%3A%20Shaokui%20Wei%20and%20Hongyuan%20Zha%20and%20Baoyuan%20Wu%0AAbstract%3A%20%20%20Data-poisoning%20backdoor%20attacks%20are%20serious%20security%20threats%20to%20machine%0Alearning%20models%2C%20where%20an%20adversary%20can%20manipulate%20the%20training%20dataset%20to%0Ainject%20backdoors%20into%20models.%20In%20this%20paper%2C%20we%20focus%20on%20in-training%20backdoor%0Adefense%2C%20aiming%20to%20train%20a%20clean%20model%20even%20when%20the%20dataset%20may%20be%20potentially%0Apoisoned.%20Unlike%20most%20existing%20methods%20that%20primarily%20detect%20and%20remove/unlearn%0Asuspicious%20samples%20to%20mitigate%20malicious%20backdoor%20attacks%2C%20we%20propose%20a%20novel%0Adefense%20approach%20called%20PDB%20%28Proactive%20Defensive%20Backdoor%29.%20Specifically%2C%20PDB%0Aleverages%20the%20home-field%20advantage%20of%20defenders%20by%20proactively%20injecting%20a%0Adefensive%20backdoor%20into%20the%20model%20during%20training.%20Taking%20advantage%20of%0Acontrolling%20the%20training%20process%2C%20the%20defensive%20backdoor%20is%20designed%20to%0Asuppress%20the%20malicious%20backdoor%20effectively%20while%20remaining%20secret%20to%0Aattackers.%20In%20addition%2C%20we%20introduce%20a%20reversible%20mapping%20to%20determine%20the%0Adefensive%20target%20label.%20During%20inference%2C%20PDB%20embeds%20a%20defensive%20trigger%20in%20the%0Ainputs%20and%20reverses%20the%20model%27s%20prediction%2C%20suppressing%20malicious%20backdoor%20and%0Aensuring%20the%20model%27s%20utility%20on%20the%20original%20task.%20Experimental%20results%20across%0Avarious%20datasets%20and%20models%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20defense%20performance%20against%20a%20wide%20range%20of%20backdoor%20attacks.%0AThe%20code%20is%20available%20at%0Ahttps%3A//github.com/shawkui/Proactive_Defensive_Backdoor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Backdoor%2520Attack%2520by%2520Injecting%2520Proactive%2520Defensive%2520Backdoor%26entry.906535625%3DShaokui%2520Wei%2520and%2520Hongyuan%2520Zha%2520and%2520Baoyuan%2520Wu%26entry.1292438233%3D%2520%2520Data-poisoning%2520backdoor%2520attacks%2520are%2520serious%2520security%2520threats%2520to%2520machine%250Alearning%2520models%252C%2520where%2520an%2520adversary%2520can%2520manipulate%2520the%2520training%2520dataset%2520to%250Ainject%2520backdoors%2520into%2520models.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520in-training%2520backdoor%250Adefense%252C%2520aiming%2520to%2520train%2520a%2520clean%2520model%2520even%2520when%2520the%2520dataset%2520may%2520be%2520potentially%250Apoisoned.%2520Unlike%2520most%2520existing%2520methods%2520that%2520primarily%2520detect%2520and%2520remove/unlearn%250Asuspicious%2520samples%2520to%2520mitigate%2520malicious%2520backdoor%2520attacks%252C%2520we%2520propose%2520a%2520novel%250Adefense%2520approach%2520called%2520PDB%2520%2528Proactive%2520Defensive%2520Backdoor%2529.%2520Specifically%252C%2520PDB%250Aleverages%2520the%2520home-field%2520advantage%2520of%2520defenders%2520by%2520proactively%2520injecting%2520a%250Adefensive%2520backdoor%2520into%2520the%2520model%2520during%2520training.%2520Taking%2520advantage%2520of%250Acontrolling%2520the%2520training%2520process%252C%2520the%2520defensive%2520backdoor%2520is%2520designed%2520to%250Asuppress%2520the%2520malicious%2520backdoor%2520effectively%2520while%2520remaining%2520secret%2520to%250Aattackers.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520reversible%2520mapping%2520to%2520determine%2520the%250Adefensive%2520target%2520label.%2520During%2520inference%252C%2520PDB%2520embeds%2520a%2520defensive%2520trigger%2520in%2520the%250Ainputs%2520and%2520reverses%2520the%2520model%2527s%2520prediction%252C%2520suppressing%2520malicious%2520backdoor%2520and%250Aensuring%2520the%2520model%2527s%2520utility%2520on%2520the%2520original%2520task.%2520Experimental%2520results%2520across%250Avarious%2520datasets%2520and%2520models%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Astate-of-the-art%2520defense%2520performance%2520against%2520a%2520wide%2520range%2520of%2520backdoor%2520attacks.%250AThe%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shawkui/Proactive_Defensive_Backdoor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Backdoor%20Attack%20by%20Injecting%20Proactive%20Defensive%20Backdoor&entry.906535625=Shaokui%20Wei%20and%20Hongyuan%20Zha%20and%20Baoyuan%20Wu&entry.1292438233=%20%20Data-poisoning%20backdoor%20attacks%20are%20serious%20security%20threats%20to%20machine%0Alearning%20models%2C%20where%20an%20adversary%20can%20manipulate%20the%20training%20dataset%20to%0Ainject%20backdoors%20into%20models.%20In%20this%20paper%2C%20we%20focus%20on%20in-training%20backdoor%0Adefense%2C%20aiming%20to%20train%20a%20clean%20model%20even%20when%20the%20dataset%20may%20be%20potentially%0Apoisoned.%20Unlike%20most%20existing%20methods%20that%20primarily%20detect%20and%20remove/unlearn%0Asuspicious%20samples%20to%20mitigate%20malicious%20backdoor%20attacks%2C%20we%20propose%20a%20novel%0Adefense%20approach%20called%20PDB%20%28Proactive%20Defensive%20Backdoor%29.%20Specifically%2C%20PDB%0Aleverages%20the%20home-field%20advantage%20of%20defenders%20by%20proactively%20injecting%20a%0Adefensive%20backdoor%20into%20the%20model%20during%20training.%20Taking%20advantage%20of%0Acontrolling%20the%20training%20process%2C%20the%20defensive%20backdoor%20is%20designed%20to%0Asuppress%20the%20malicious%20backdoor%20effectively%20while%20remaining%20secret%20to%0Aattackers.%20In%20addition%2C%20we%20introduce%20a%20reversible%20mapping%20to%20determine%20the%0Adefensive%20target%20label.%20During%20inference%2C%20PDB%20embeds%20a%20defensive%20trigger%20in%20the%0Ainputs%20and%20reverses%20the%20model%27s%20prediction%2C%20suppressing%20malicious%20backdoor%20and%0Aensuring%20the%20model%27s%20utility%20on%20the%20original%20task.%20Experimental%20results%20across%0Avarious%20datasets%20and%20models%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20defense%20performance%20against%20a%20wide%20range%20of%20backdoor%20attacks.%0AThe%20code%20is%20available%20at%0Ahttps%3A//github.com/shawkui/Proactive_Defensive_Backdoor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16112v2&entry.124074799=Read"},
{"title": "What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and\n  the Ship of Language Models", "author": "Shengqi Zhu and Jeffrey M. Rzeszotarski", "abstract": "  The term Language Models (LMs), as a time-specific collection of models of\ninterest, is constantly reinvented, with its referents updated much like the\n$\\textit{Ship of Theseus}$ replaces its parts but remains the same ship in\nessence. In this paper, we investigate this $\\textit{Ship of Language Models}$\nproblem, wherein scientific evolution takes the form of continuous, implicit\nretrofits of key existing terms. We seek to initiate a novel perspective of\nscientific progress, in addition to the more well-studied emergence of new\nterms. To this end, we construct the data infrastructure based on recent NLP\npublications. Then, we perform a series of text-based analyses toward a\ndetailed, quantitative understanding of the use of Language Models as a term of\nart. Our work highlights how systems and theories influence each other in\nscientific discourse, and we call for attention to the transformation of this\nShip that we all are contributing to.\n", "link": "http://arxiv.org/abs/2407.01929v2", "date": "2024-10-15", "relevancy": 2.3571, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20We%20Talk%20About%20When%20We%20Talk%20About%20LMs%3A%20Implicit%20Paradigm%20Shifts%20and%0A%20%20the%20Ship%20of%20Language%20Models&body=Title%3A%20What%20We%20Talk%20About%20When%20We%20Talk%20About%20LMs%3A%20Implicit%20Paradigm%20Shifts%20and%0A%20%20the%20Ship%20of%20Language%20Models%0AAuthor%3A%20Shengqi%20Zhu%20and%20Jeffrey%20M.%20Rzeszotarski%0AAbstract%3A%20%20%20The%20term%20Language%20Models%20%28LMs%29%2C%20as%20a%20time-specific%20collection%20of%20models%20of%0Ainterest%2C%20is%20constantly%20reinvented%2C%20with%20its%20referents%20updated%20much%20like%20the%0A%24%5Ctextit%7BShip%20of%20Theseus%7D%24%20replaces%20its%20parts%20but%20remains%20the%20same%20ship%20in%0Aessence.%20In%20this%20paper%2C%20we%20investigate%20this%20%24%5Ctextit%7BShip%20of%20Language%20Models%7D%24%0Aproblem%2C%20wherein%20scientific%20evolution%20takes%20the%20form%20of%20continuous%2C%20implicit%0Aretrofits%20of%20key%20existing%20terms.%20We%20seek%20to%20initiate%20a%20novel%20perspective%20of%0Ascientific%20progress%2C%20in%20addition%20to%20the%20more%20well-studied%20emergence%20of%20new%0Aterms.%20To%20this%20end%2C%20we%20construct%20the%20data%20infrastructure%20based%20on%20recent%20NLP%0Apublications.%20Then%2C%20we%20perform%20a%20series%20of%20text-based%20analyses%20toward%20a%0Adetailed%2C%20quantitative%20understanding%20of%20the%20use%20of%20Language%20Models%20as%20a%20term%20of%0Aart.%20Our%20work%20highlights%20how%20systems%20and%20theories%20influence%20each%20other%20in%0Ascientific%20discourse%2C%20and%20we%20call%20for%20attention%20to%20the%20transformation%20of%20this%0AShip%20that%20we%20all%20are%20contributing%20to.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520We%2520Talk%2520About%2520When%2520We%2520Talk%2520About%2520LMs%253A%2520Implicit%2520Paradigm%2520Shifts%2520and%250A%2520%2520the%2520Ship%2520of%2520Language%2520Models%26entry.906535625%3DShengqi%2520Zhu%2520and%2520Jeffrey%2520M.%2520Rzeszotarski%26entry.1292438233%3D%2520%2520The%2520term%2520Language%2520Models%2520%2528LMs%2529%252C%2520as%2520a%2520time-specific%2520collection%2520of%2520models%2520of%250Ainterest%252C%2520is%2520constantly%2520reinvented%252C%2520with%2520its%2520referents%2520updated%2520much%2520like%2520the%250A%2524%255Ctextit%257BShip%2520of%2520Theseus%257D%2524%2520replaces%2520its%2520parts%2520but%2520remains%2520the%2520same%2520ship%2520in%250Aessence.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520this%2520%2524%255Ctextit%257BShip%2520of%2520Language%2520Models%257D%2524%250Aproblem%252C%2520wherein%2520scientific%2520evolution%2520takes%2520the%2520form%2520of%2520continuous%252C%2520implicit%250Aretrofits%2520of%2520key%2520existing%2520terms.%2520We%2520seek%2520to%2520initiate%2520a%2520novel%2520perspective%2520of%250Ascientific%2520progress%252C%2520in%2520addition%2520to%2520the%2520more%2520well-studied%2520emergence%2520of%2520new%250Aterms.%2520To%2520this%2520end%252C%2520we%2520construct%2520the%2520data%2520infrastructure%2520based%2520on%2520recent%2520NLP%250Apublications.%2520Then%252C%2520we%2520perform%2520a%2520series%2520of%2520text-based%2520analyses%2520toward%2520a%250Adetailed%252C%2520quantitative%2520understanding%2520of%2520the%2520use%2520of%2520Language%2520Models%2520as%2520a%2520term%2520of%250Aart.%2520Our%2520work%2520highlights%2520how%2520systems%2520and%2520theories%2520influence%2520each%2520other%2520in%250Ascientific%2520discourse%252C%2520and%2520we%2520call%2520for%2520attention%2520to%2520the%2520transformation%2520of%2520this%250AShip%2520that%2520we%2520all%2520are%2520contributing%2520to.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20We%20Talk%20About%20When%20We%20Talk%20About%20LMs%3A%20Implicit%20Paradigm%20Shifts%20and%0A%20%20the%20Ship%20of%20Language%20Models&entry.906535625=Shengqi%20Zhu%20and%20Jeffrey%20M.%20Rzeszotarski&entry.1292438233=%20%20The%20term%20Language%20Models%20%28LMs%29%2C%20as%20a%20time-specific%20collection%20of%20models%20of%0Ainterest%2C%20is%20constantly%20reinvented%2C%20with%20its%20referents%20updated%20much%20like%20the%0A%24%5Ctextit%7BShip%20of%20Theseus%7D%24%20replaces%20its%20parts%20but%20remains%20the%20same%20ship%20in%0Aessence.%20In%20this%20paper%2C%20we%20investigate%20this%20%24%5Ctextit%7BShip%20of%20Language%20Models%7D%24%0Aproblem%2C%20wherein%20scientific%20evolution%20takes%20the%20form%20of%20continuous%2C%20implicit%0Aretrofits%20of%20key%20existing%20terms.%20We%20seek%20to%20initiate%20a%20novel%20perspective%20of%0Ascientific%20progress%2C%20in%20addition%20to%20the%20more%20well-studied%20emergence%20of%20new%0Aterms.%20To%20this%20end%2C%20we%20construct%20the%20data%20infrastructure%20based%20on%20recent%20NLP%0Apublications.%20Then%2C%20we%20perform%20a%20series%20of%20text-based%20analyses%20toward%20a%0Adetailed%2C%20quantitative%20understanding%20of%20the%20use%20of%20Language%20Models%20as%20a%20term%20of%0Aart.%20Our%20work%20highlights%20how%20systems%20and%20theories%20influence%20each%20other%20in%0Ascientific%20discourse%2C%20and%20we%20call%20for%20attention%20to%20the%20transformation%20of%20this%0AShip%20that%20we%20all%20are%20contributing%20to.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01929v2&entry.124074799=Read"},
{"title": "Towards Fair Graph Representation Learning in Social Networks", "author": "Guixian Zhang and Guan Yuan and Debo Cheng and Lin Liu and Jiuyong Li and Shichao Zhang", "abstract": "  With the widespread use of Graph Neural Networks (GNNs) for representation\nlearning from network data, the fairness of GNN models has raised great\nattention lately. Fair GNNs aim to ensure that node representations can be\naccurately classified, but not easily associated with a specific group.\nExisting advanced approaches essentially enhance the generalisation of node\nrepresentation in combination with data augmentation strategy, and do not\ndirectly impose constraints on the fairness of GNNs. In this work, we identify\nthat a fundamental reason for the unfairness of GNNs in social network learning\nis the phenomenon of social homophily, i.e., users in the same group are more\ninclined to congregate. The message-passing mechanism of GNNs can cause users\nin the same group to have similar representations due to social homophily,\nleading model predictions to establish spurious correlations with sensitive\nattributes. Inspired by this reason, we propose a method called Equity-Aware\nGNN (EAGNN) towards fair graph representation learning. Specifically, to ensure\nthat model predictions are independent of sensitive attributes while\nmaintaining prediction performance, we introduce constraints for fair\nrepresentation learning based on three principles: sufficiency, independence,\nand separation. We theoretically demonstrate that our EAGNN method can\neffectively achieve group fairness. Extensive experiments on three datasets\nwith varying levels of social homophily illustrate that our EAGNN method\nachieves the state-of-the-art performance across two fairness metrics and\noffers competitive effectiveness.\n", "link": "http://arxiv.org/abs/2410.11493v1", "date": "2024-10-15", "relevancy": 2.3521, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4892}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4788}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fair%20Graph%20Representation%20Learning%20in%20Social%20Networks&body=Title%3A%20Towards%20Fair%20Graph%20Representation%20Learning%20in%20Social%20Networks%0AAuthor%3A%20Guixian%20Zhang%20and%20Guan%20Yuan%20and%20Debo%20Cheng%20and%20Lin%20Liu%20and%20Jiuyong%20Li%20and%20Shichao%20Zhang%0AAbstract%3A%20%20%20With%20the%20widespread%20use%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20representation%0Alearning%20from%20network%20data%2C%20the%20fairness%20of%20GNN%20models%20has%20raised%20great%0Aattention%20lately.%20Fair%20GNNs%20aim%20to%20ensure%20that%20node%20representations%20can%20be%0Aaccurately%20classified%2C%20but%20not%20easily%20associated%20with%20a%20specific%20group.%0AExisting%20advanced%20approaches%20essentially%20enhance%20the%20generalisation%20of%20node%0Arepresentation%20in%20combination%20with%20data%20augmentation%20strategy%2C%20and%20do%20not%0Adirectly%20impose%20constraints%20on%20the%20fairness%20of%20GNNs.%20In%20this%20work%2C%20we%20identify%0Athat%20a%20fundamental%20reason%20for%20the%20unfairness%20of%20GNNs%20in%20social%20network%20learning%0Ais%20the%20phenomenon%20of%20social%20homophily%2C%20i.e.%2C%20users%20in%20the%20same%20group%20are%20more%0Ainclined%20to%20congregate.%20The%20message-passing%20mechanism%20of%20GNNs%20can%20cause%20users%0Ain%20the%20same%20group%20to%20have%20similar%20representations%20due%20to%20social%20homophily%2C%0Aleading%20model%20predictions%20to%20establish%20spurious%20correlations%20with%20sensitive%0Aattributes.%20Inspired%20by%20this%20reason%2C%20we%20propose%20a%20method%20called%20Equity-Aware%0AGNN%20%28EAGNN%29%20towards%20fair%20graph%20representation%20learning.%20Specifically%2C%20to%20ensure%0Athat%20model%20predictions%20are%20independent%20of%20sensitive%20attributes%20while%0Amaintaining%20prediction%20performance%2C%20we%20introduce%20constraints%20for%20fair%0Arepresentation%20learning%20based%20on%20three%20principles%3A%20sufficiency%2C%20independence%2C%0Aand%20separation.%20We%20theoretically%20demonstrate%20that%20our%20EAGNN%20method%20can%0Aeffectively%20achieve%20group%20fairness.%20Extensive%20experiments%20on%20three%20datasets%0Awith%20varying%20levels%20of%20social%20homophily%20illustrate%20that%20our%20EAGNN%20method%0Aachieves%20the%20state-of-the-art%20performance%20across%20two%20fairness%20metrics%20and%0Aoffers%20competitive%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fair%2520Graph%2520Representation%2520Learning%2520in%2520Social%2520Networks%26entry.906535625%3DGuixian%2520Zhang%2520and%2520Guan%2520Yuan%2520and%2520Debo%2520Cheng%2520and%2520Lin%2520Liu%2520and%2520Jiuyong%2520Li%2520and%2520Shichao%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520use%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520for%2520representation%250Alearning%2520from%2520network%2520data%252C%2520the%2520fairness%2520of%2520GNN%2520models%2520has%2520raised%2520great%250Aattention%2520lately.%2520Fair%2520GNNs%2520aim%2520to%2520ensure%2520that%2520node%2520representations%2520can%2520be%250Aaccurately%2520classified%252C%2520but%2520not%2520easily%2520associated%2520with%2520a%2520specific%2520group.%250AExisting%2520advanced%2520approaches%2520essentially%2520enhance%2520the%2520generalisation%2520of%2520node%250Arepresentation%2520in%2520combination%2520with%2520data%2520augmentation%2520strategy%252C%2520and%2520do%2520not%250Adirectly%2520impose%2520constraints%2520on%2520the%2520fairness%2520of%2520GNNs.%2520In%2520this%2520work%252C%2520we%2520identify%250Athat%2520a%2520fundamental%2520reason%2520for%2520the%2520unfairness%2520of%2520GNNs%2520in%2520social%2520network%2520learning%250Ais%2520the%2520phenomenon%2520of%2520social%2520homophily%252C%2520i.e.%252C%2520users%2520in%2520the%2520same%2520group%2520are%2520more%250Ainclined%2520to%2520congregate.%2520The%2520message-passing%2520mechanism%2520of%2520GNNs%2520can%2520cause%2520users%250Ain%2520the%2520same%2520group%2520to%2520have%2520similar%2520representations%2520due%2520to%2520social%2520homophily%252C%250Aleading%2520model%2520predictions%2520to%2520establish%2520spurious%2520correlations%2520with%2520sensitive%250Aattributes.%2520Inspired%2520by%2520this%2520reason%252C%2520we%2520propose%2520a%2520method%2520called%2520Equity-Aware%250AGNN%2520%2528EAGNN%2529%2520towards%2520fair%2520graph%2520representation%2520learning.%2520Specifically%252C%2520to%2520ensure%250Athat%2520model%2520predictions%2520are%2520independent%2520of%2520sensitive%2520attributes%2520while%250Amaintaining%2520prediction%2520performance%252C%2520we%2520introduce%2520constraints%2520for%2520fair%250Arepresentation%2520learning%2520based%2520on%2520three%2520principles%253A%2520sufficiency%252C%2520independence%252C%250Aand%2520separation.%2520We%2520theoretically%2520demonstrate%2520that%2520our%2520EAGNN%2520method%2520can%250Aeffectively%2520achieve%2520group%2520fairness.%2520Extensive%2520experiments%2520on%2520three%2520datasets%250Awith%2520varying%2520levels%2520of%2520social%2520homophily%2520illustrate%2520that%2520our%2520EAGNN%2520method%250Aachieves%2520the%2520state-of-the-art%2520performance%2520across%2520two%2520fairness%2520metrics%2520and%250Aoffers%2520competitive%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fair%20Graph%20Representation%20Learning%20in%20Social%20Networks&entry.906535625=Guixian%20Zhang%20and%20Guan%20Yuan%20and%20Debo%20Cheng%20and%20Lin%20Liu%20and%20Jiuyong%20Li%20and%20Shichao%20Zhang&entry.1292438233=%20%20With%20the%20widespread%20use%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20representation%0Alearning%20from%20network%20data%2C%20the%20fairness%20of%20GNN%20models%20has%20raised%20great%0Aattention%20lately.%20Fair%20GNNs%20aim%20to%20ensure%20that%20node%20representations%20can%20be%0Aaccurately%20classified%2C%20but%20not%20easily%20associated%20with%20a%20specific%20group.%0AExisting%20advanced%20approaches%20essentially%20enhance%20the%20generalisation%20of%20node%0Arepresentation%20in%20combination%20with%20data%20augmentation%20strategy%2C%20and%20do%20not%0Adirectly%20impose%20constraints%20on%20the%20fairness%20of%20GNNs.%20In%20this%20work%2C%20we%20identify%0Athat%20a%20fundamental%20reason%20for%20the%20unfairness%20of%20GNNs%20in%20social%20network%20learning%0Ais%20the%20phenomenon%20of%20social%20homophily%2C%20i.e.%2C%20users%20in%20the%20same%20group%20are%20more%0Ainclined%20to%20congregate.%20The%20message-passing%20mechanism%20of%20GNNs%20can%20cause%20users%0Ain%20the%20same%20group%20to%20have%20similar%20representations%20due%20to%20social%20homophily%2C%0Aleading%20model%20predictions%20to%20establish%20spurious%20correlations%20with%20sensitive%0Aattributes.%20Inspired%20by%20this%20reason%2C%20we%20propose%20a%20method%20called%20Equity-Aware%0AGNN%20%28EAGNN%29%20towards%20fair%20graph%20representation%20learning.%20Specifically%2C%20to%20ensure%0Athat%20model%20predictions%20are%20independent%20of%20sensitive%20attributes%20while%0Amaintaining%20prediction%20performance%2C%20we%20introduce%20constraints%20for%20fair%0Arepresentation%20learning%20based%20on%20three%20principles%3A%20sufficiency%2C%20independence%2C%0Aand%20separation.%20We%20theoretically%20demonstrate%20that%20our%20EAGNN%20method%20can%0Aeffectively%20achieve%20group%20fairness.%20Extensive%20experiments%20on%20three%20datasets%0Awith%20varying%20levels%20of%20social%20homophily%20illustrate%20that%20our%20EAGNN%20method%0Aachieves%20the%20state-of-the-art%20performance%20across%20two%20fairness%20metrics%20and%0Aoffers%20competitive%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11493v1&entry.124074799=Read"},
{"title": "Phantom: General Trigger Attacks on Retrieval Augmented Language\n  Generation", "author": "Harsh Chaudhari and Giorgio Severi and John Abascal and Matthew Jagielski and Christopher A. Choquette-Choo and Milad Nasr and Cristina Nita-Rotaru and Alina Oprea", "abstract": "  Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\nnew attack vectors that allow an adversary to inject a single malicious\ndocument into a RAG system's knowledge base, and mount a backdoor poisoning\nattack. We design Phantom, a general two-stage optimization framework against\nRAG systems, that crafts a malicious poisoned document leading to an integrity\nviolation in the model's output. First, the document is constructed to be\nretrieved only when a specific trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and\nGPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's\nblack-box production RAG system, \"Chat with RTX\".\n", "link": "http://arxiv.org/abs/2405.20485v2", "date": "2024-10-15", "relevancy": 2.3481, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4837}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4626}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phantom%3A%20General%20Trigger%20Attacks%20on%20Retrieval%20Augmented%20Language%0A%20%20Generation&body=Title%3A%20Phantom%3A%20General%20Trigger%20Attacks%20on%20Retrieval%20Augmented%20Language%0A%20%20Generation%0AAuthor%3A%20Harsh%20Chaudhari%20and%20Giorgio%20Severi%20and%20John%20Abascal%20and%20Matthew%20Jagielski%20and%20Christopher%20A.%20Choquette-Choo%20and%20Milad%20Nasr%20and%20Cristina%20Nita-Rotaru%20and%20Alina%20Oprea%0AAbstract%3A%20%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20expands%20the%20capabilities%20of%20modern%20large%0Alanguage%20models%20%28LLMs%29%2C%20by%20anchoring%2C%20adapting%2C%20and%20personalizing%20their%0Aresponses%20to%20the%20most%20relevant%20knowledge%20sources.%20It%20is%20particularly%20useful%20in%0Achatbot%20applications%2C%20allowing%20developers%20to%20customize%20LLM%20output%20without%0Aexpensive%20retraining.%20Despite%20their%20significant%20utility%20in%20various%0Aapplications%2C%20RAG%20systems%20present%20new%20security%20risks.%20In%20this%20work%2C%20we%20propose%0Anew%20attack%20vectors%20that%20allow%20an%20adversary%20to%20inject%20a%20single%20malicious%0Adocument%20into%20a%20RAG%20system%27s%20knowledge%20base%2C%20and%20mount%20a%20backdoor%20poisoning%0Aattack.%20We%20design%20Phantom%2C%20a%20general%20two-stage%20optimization%20framework%20against%0ARAG%20systems%2C%20that%20crafts%20a%20malicious%20poisoned%20document%20leading%20to%20an%20integrity%0Aviolation%20in%20the%20model%27s%20output.%20First%2C%20the%20document%20is%20constructed%20to%20be%0Aretrieved%20only%20when%20a%20specific%20trigger%20sequence%20of%20tokens%20appears%20in%20the%0Avictim%27s%20queries.%20Second%2C%20the%20document%20is%20further%20optimized%20with%20crafted%0Aadversarial%20text%20that%20induces%20various%20adversarial%20objectives%20on%20the%20LLM%20output%2C%0Aincluding%20refusal%20to%20answer%2C%20reputation%20damage%2C%20privacy%20violations%2C%20and%20harmful%0Abehaviors.%20We%20demonstrate%20our%20attacks%20on%20multiple%20LLM%20architectures%2C%20including%0AGemma%2C%20Vicuna%2C%20and%20Llama%2C%20and%20show%20that%20they%20transfer%20to%20GPT-3.5%20Turbo%20and%0AGPT-4.%20Finally%2C%20we%20successfully%20conducted%20a%20Phantom%20attack%20on%20NVIDIA%27s%0Ablack-box%20production%20RAG%20system%2C%20%22Chat%20with%20RTX%22.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhantom%253A%2520General%2520Trigger%2520Attacks%2520on%2520Retrieval%2520Augmented%2520Language%250A%2520%2520Generation%26entry.906535625%3DHarsh%2520Chaudhari%2520and%2520Giorgio%2520Severi%2520and%2520John%2520Abascal%2520and%2520Matthew%2520Jagielski%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Milad%2520Nasr%2520and%2520Cristina%2520Nita-Rotaru%2520and%2520Alina%2520Oprea%26entry.1292438233%3D%2520%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520expands%2520the%2520capabilities%2520of%2520modern%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520by%2520anchoring%252C%2520adapting%252C%2520and%2520personalizing%2520their%250Aresponses%2520to%2520the%2520most%2520relevant%2520knowledge%2520sources.%2520It%2520is%2520particularly%2520useful%2520in%250Achatbot%2520applications%252C%2520allowing%2520developers%2520to%2520customize%2520LLM%2520output%2520without%250Aexpensive%2520retraining.%2520Despite%2520their%2520significant%2520utility%2520in%2520various%250Aapplications%252C%2520RAG%2520systems%2520present%2520new%2520security%2520risks.%2520In%2520this%2520work%252C%2520we%2520propose%250Anew%2520attack%2520vectors%2520that%2520allow%2520an%2520adversary%2520to%2520inject%2520a%2520single%2520malicious%250Adocument%2520into%2520a%2520RAG%2520system%2527s%2520knowledge%2520base%252C%2520and%2520mount%2520a%2520backdoor%2520poisoning%250Aattack.%2520We%2520design%2520Phantom%252C%2520a%2520general%2520two-stage%2520optimization%2520framework%2520against%250ARAG%2520systems%252C%2520that%2520crafts%2520a%2520malicious%2520poisoned%2520document%2520leading%2520to%2520an%2520integrity%250Aviolation%2520in%2520the%2520model%2527s%2520output.%2520First%252C%2520the%2520document%2520is%2520constructed%2520to%2520be%250Aretrieved%2520only%2520when%2520a%2520specific%2520trigger%2520sequence%2520of%2520tokens%2520appears%2520in%2520the%250Avictim%2527s%2520queries.%2520Second%252C%2520the%2520document%2520is%2520further%2520optimized%2520with%2520crafted%250Aadversarial%2520text%2520that%2520induces%2520various%2520adversarial%2520objectives%2520on%2520the%2520LLM%2520output%252C%250Aincluding%2520refusal%2520to%2520answer%252C%2520reputation%2520damage%252C%2520privacy%2520violations%252C%2520and%2520harmful%250Abehaviors.%2520We%2520demonstrate%2520our%2520attacks%2520on%2520multiple%2520LLM%2520architectures%252C%2520including%250AGemma%252C%2520Vicuna%252C%2520and%2520Llama%252C%2520and%2520show%2520that%2520they%2520transfer%2520to%2520GPT-3.5%2520Turbo%2520and%250AGPT-4.%2520Finally%252C%2520we%2520successfully%2520conducted%2520a%2520Phantom%2520attack%2520on%2520NVIDIA%2527s%250Ablack-box%2520production%2520RAG%2520system%252C%2520%2522Chat%2520with%2520RTX%2522.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phantom%3A%20General%20Trigger%20Attacks%20on%20Retrieval%20Augmented%20Language%0A%20%20Generation&entry.906535625=Harsh%20Chaudhari%20and%20Giorgio%20Severi%20and%20John%20Abascal%20and%20Matthew%20Jagielski%20and%20Christopher%20A.%20Choquette-Choo%20and%20Milad%20Nasr%20and%20Cristina%20Nita-Rotaru%20and%20Alina%20Oprea&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20expands%20the%20capabilities%20of%20modern%20large%0Alanguage%20models%20%28LLMs%29%2C%20by%20anchoring%2C%20adapting%2C%20and%20personalizing%20their%0Aresponses%20to%20the%20most%20relevant%20knowledge%20sources.%20It%20is%20particularly%20useful%20in%0Achatbot%20applications%2C%20allowing%20developers%20to%20customize%20LLM%20output%20without%0Aexpensive%20retraining.%20Despite%20their%20significant%20utility%20in%20various%0Aapplications%2C%20RAG%20systems%20present%20new%20security%20risks.%20In%20this%20work%2C%20we%20propose%0Anew%20attack%20vectors%20that%20allow%20an%20adversary%20to%20inject%20a%20single%20malicious%0Adocument%20into%20a%20RAG%20system%27s%20knowledge%20base%2C%20and%20mount%20a%20backdoor%20poisoning%0Aattack.%20We%20design%20Phantom%2C%20a%20general%20two-stage%20optimization%20framework%20against%0ARAG%20systems%2C%20that%20crafts%20a%20malicious%20poisoned%20document%20leading%20to%20an%20integrity%0Aviolation%20in%20the%20model%27s%20output.%20First%2C%20the%20document%20is%20constructed%20to%20be%0Aretrieved%20only%20when%20a%20specific%20trigger%20sequence%20of%20tokens%20appears%20in%20the%0Avictim%27s%20queries.%20Second%2C%20the%20document%20is%20further%20optimized%20with%20crafted%0Aadversarial%20text%20that%20induces%20various%20adversarial%20objectives%20on%20the%20LLM%20output%2C%0Aincluding%20refusal%20to%20answer%2C%20reputation%20damage%2C%20privacy%20violations%2C%20and%20harmful%0Abehaviors.%20We%20demonstrate%20our%20attacks%20on%20multiple%20LLM%20architectures%2C%20including%0AGemma%2C%20Vicuna%2C%20and%20Llama%2C%20and%20show%20that%20they%20transfer%20to%20GPT-3.5%20Turbo%20and%0AGPT-4.%20Finally%2C%20we%20successfully%20conducted%20a%20Phantom%20attack%20on%20NVIDIA%27s%0Ablack-box%20production%20RAG%20system%2C%20%22Chat%20with%20RTX%22.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20485v2&entry.124074799=Read"},
{"title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video\n  Retrieval", "author": "Reno Kriz and Kate Sanders and David Etter and Kenton Murray and Cameron Carpenter and Kelly Van Ochten and Hannah Recknor and Jimena Guallar-Blasco and Alexander Martin and Ronald Colaianni and Nolan King and Eugene Yang and Benjamin Van Durme", "abstract": "  Efficiently retrieving and synthesizing information from large-scale\nmultimodal collections has become a critical challenge. However, existing video\nretrieval datasets suffer from scope limitations, primarily focusing on\nmatching descriptive but vague queries with small collections of professionally\nedited, English-centric videos. To address this gap, we introduce\n$\\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric video\nretrieval benchmark featuring a collection of more than 218,000 news videos and\n3,906 queries targeting specific world events. These queries specifically\ntarget information found in the visual content, audio, embedded text, and text\nmetadata of the videos, requiring systems leverage all these sources to succeed\nat the task. Preliminary results show that state-of-the-art vision-language\nmodels struggle significantly with this task, and while alternative approaches\nshow promise, they are still insufficient to adequately address this problem.\nThese findings underscore the need for more robust multimodal retrieval\nsystems, as effective video retrieval is a crucial step towards multimodal\ncontent understanding and generation tasks.\n", "link": "http://arxiv.org/abs/2410.11619v1", "date": "2024-10-15", "relevancy": 2.3385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiVENT%202.0%3A%20A%20Massive%20Multilingual%20Benchmark%20for%20Event-Centric%20Video%0A%20%20Retrieval&body=Title%3A%20MultiVENT%202.0%3A%20A%20Massive%20Multilingual%20Benchmark%20for%20Event-Centric%20Video%0A%20%20Retrieval%0AAuthor%3A%20Reno%20Kriz%20and%20Kate%20Sanders%20and%20David%20Etter%20and%20Kenton%20Murray%20and%20Cameron%20Carpenter%20and%20Kelly%20Van%20Ochten%20and%20Hannah%20Recknor%20and%20Jimena%20Guallar-Blasco%20and%20Alexander%20Martin%20and%20Ronald%20Colaianni%20and%20Nolan%20King%20and%20Eugene%20Yang%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20Efficiently%20retrieving%20and%20synthesizing%20information%20from%20large-scale%0Amultimodal%20collections%20has%20become%20a%20critical%20challenge.%20However%2C%20existing%20video%0Aretrieval%20datasets%20suffer%20from%20scope%20limitations%2C%20primarily%20focusing%20on%0Amatching%20descriptive%20but%20vague%20queries%20with%20small%20collections%20of%20professionally%0Aedited%2C%20English-centric%20videos.%20To%20address%20this%20gap%2C%20we%20introduce%0A%24%5Ctextbf%7BMultiVENT%202.0%7D%24%2C%20a%20large-scale%2C%20multilingual%20event-centric%20video%0Aretrieval%20benchmark%20featuring%20a%20collection%20of%20more%20than%20218%2C000%20news%20videos%20and%0A3%2C906%20queries%20targeting%20specific%20world%20events.%20These%20queries%20specifically%0Atarget%20information%20found%20in%20the%20visual%20content%2C%20audio%2C%20embedded%20text%2C%20and%20text%0Ametadata%20of%20the%20videos%2C%20requiring%20systems%20leverage%20all%20these%20sources%20to%20succeed%0Aat%20the%20task.%20Preliminary%20results%20show%20that%20state-of-the-art%20vision-language%0Amodels%20struggle%20significantly%20with%20this%20task%2C%20and%20while%20alternative%20approaches%0Ashow%20promise%2C%20they%20are%20still%20insufficient%20to%20adequately%20address%20this%20problem.%0AThese%20findings%20underscore%20the%20need%20for%20more%20robust%20multimodal%20retrieval%0Asystems%2C%20as%20effective%20video%20retrieval%20is%20a%20crucial%20step%20towards%20multimodal%0Acontent%20understanding%20and%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiVENT%25202.0%253A%2520A%2520Massive%2520Multilingual%2520Benchmark%2520for%2520Event-Centric%2520Video%250A%2520%2520Retrieval%26entry.906535625%3DReno%2520Kriz%2520and%2520Kate%2520Sanders%2520and%2520David%2520Etter%2520and%2520Kenton%2520Murray%2520and%2520Cameron%2520Carpenter%2520and%2520Kelly%2520Van%2520Ochten%2520and%2520Hannah%2520Recknor%2520and%2520Jimena%2520Guallar-Blasco%2520and%2520Alexander%2520Martin%2520and%2520Ronald%2520Colaianni%2520and%2520Nolan%2520King%2520and%2520Eugene%2520Yang%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520Efficiently%2520retrieving%2520and%2520synthesizing%2520information%2520from%2520large-scale%250Amultimodal%2520collections%2520has%2520become%2520a%2520critical%2520challenge.%2520However%252C%2520existing%2520video%250Aretrieval%2520datasets%2520suffer%2520from%2520scope%2520limitations%252C%2520primarily%2520focusing%2520on%250Amatching%2520descriptive%2520but%2520vague%2520queries%2520with%2520small%2520collections%2520of%2520professionally%250Aedited%252C%2520English-centric%2520videos.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250A%2524%255Ctextbf%257BMultiVENT%25202.0%257D%2524%252C%2520a%2520large-scale%252C%2520multilingual%2520event-centric%2520video%250Aretrieval%2520benchmark%2520featuring%2520a%2520collection%2520of%2520more%2520than%2520218%252C000%2520news%2520videos%2520and%250A3%252C906%2520queries%2520targeting%2520specific%2520world%2520events.%2520These%2520queries%2520specifically%250Atarget%2520information%2520found%2520in%2520the%2520visual%2520content%252C%2520audio%252C%2520embedded%2520text%252C%2520and%2520text%250Ametadata%2520of%2520the%2520videos%252C%2520requiring%2520systems%2520leverage%2520all%2520these%2520sources%2520to%2520succeed%250Aat%2520the%2520task.%2520Preliminary%2520results%2520show%2520that%2520state-of-the-art%2520vision-language%250Amodels%2520struggle%2520significantly%2520with%2520this%2520task%252C%2520and%2520while%2520alternative%2520approaches%250Ashow%2520promise%252C%2520they%2520are%2520still%2520insufficient%2520to%2520adequately%2520address%2520this%2520problem.%250AThese%2520findings%2520underscore%2520the%2520need%2520for%2520more%2520robust%2520multimodal%2520retrieval%250Asystems%252C%2520as%2520effective%2520video%2520retrieval%2520is%2520a%2520crucial%2520step%2520towards%2520multimodal%250Acontent%2520understanding%2520and%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiVENT%202.0%3A%20A%20Massive%20Multilingual%20Benchmark%20for%20Event-Centric%20Video%0A%20%20Retrieval&entry.906535625=Reno%20Kriz%20and%20Kate%20Sanders%20and%20David%20Etter%20and%20Kenton%20Murray%20and%20Cameron%20Carpenter%20and%20Kelly%20Van%20Ochten%20and%20Hannah%20Recknor%20and%20Jimena%20Guallar-Blasco%20and%20Alexander%20Martin%20and%20Ronald%20Colaianni%20and%20Nolan%20King%20and%20Eugene%20Yang%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20Efficiently%20retrieving%20and%20synthesizing%20information%20from%20large-scale%0Amultimodal%20collections%20has%20become%20a%20critical%20challenge.%20However%2C%20existing%20video%0Aretrieval%20datasets%20suffer%20from%20scope%20limitations%2C%20primarily%20focusing%20on%0Amatching%20descriptive%20but%20vague%20queries%20with%20small%20collections%20of%20professionally%0Aedited%2C%20English-centric%20videos.%20To%20address%20this%20gap%2C%20we%20introduce%0A%24%5Ctextbf%7BMultiVENT%202.0%7D%24%2C%20a%20large-scale%2C%20multilingual%20event-centric%20video%0Aretrieval%20benchmark%20featuring%20a%20collection%20of%20more%20than%20218%2C000%20news%20videos%20and%0A3%2C906%20queries%20targeting%20specific%20world%20events.%20These%20queries%20specifically%0Atarget%20information%20found%20in%20the%20visual%20content%2C%20audio%2C%20embedded%20text%2C%20and%20text%0Ametadata%20of%20the%20videos%2C%20requiring%20systems%20leverage%20all%20these%20sources%20to%20succeed%0Aat%20the%20task.%20Preliminary%20results%20show%20that%20state-of-the-art%20vision-language%0Amodels%20struggle%20significantly%20with%20this%20task%2C%20and%20while%20alternative%20approaches%0Ashow%20promise%2C%20they%20are%20still%20insufficient%20to%20adequately%20address%20this%20problem.%0AThese%20findings%20underscore%20the%20need%20for%20more%20robust%20multimodal%20retrieval%0Asystems%2C%20as%20effective%20video%20retrieval%20is%20a%20crucial%20step%20towards%20multimodal%0Acontent%20understanding%20and%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11619v1&entry.124074799=Read"},
{"title": "ReinDiffuse: Crafting Physically Plausible Motions with Reinforced\n  Diffusion Model", "author": "Gaoge Han and Mingjiang Liang and Jinglei Tang and Yongkang Cheng and Wei Liu and Shaoli Huang", "abstract": "  Generating human motion from textual descriptions is a challenging task.\nExisting methods either struggle with physical credibility or are limited by\nthe complexities of physics simulations. In this paper, we present\n\\emph{ReinDiffuse} that combines reinforcement learning with motion diffusion\nmodel to generate physically credible human motions that align with textual\ndescriptions. Our method adapts Motion Diffusion Model to output a\nparameterized distribution of actions, making them compatible with\nreinforcement learning paradigms. We employ reinforcement learning with the\nobjective of maximizing physically plausible rewards to optimize motion\ngeneration for physical fidelity. Our approach outperforms existing\nstate-of-the-art models on two major datasets, HumanML3D and KIT-ML, achieving\nsignificant improvements in physical plausibility and motion quality. Project:\nhttps://reindiffuse.github.io/\n", "link": "http://arxiv.org/abs/2410.07296v2", "date": "2024-10-15", "relevancy": 2.3163, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5993}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5778}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReinDiffuse%3A%20Crafting%20Physically%20Plausible%20Motions%20with%20Reinforced%0A%20%20Diffusion%20Model&body=Title%3A%20ReinDiffuse%3A%20Crafting%20Physically%20Plausible%20Motions%20with%20Reinforced%0A%20%20Diffusion%20Model%0AAuthor%3A%20Gaoge%20Han%20and%20Mingjiang%20Liang%20and%20Jinglei%20Tang%20and%20Yongkang%20Cheng%20and%20Wei%20Liu%20and%20Shaoli%20Huang%0AAbstract%3A%20%20%20Generating%20human%20motion%20from%20textual%20descriptions%20is%20a%20challenging%20task.%0AExisting%20methods%20either%20struggle%20with%20physical%20credibility%20or%20are%20limited%20by%0Athe%20complexities%20of%20physics%20simulations.%20In%20this%20paper%2C%20we%20present%0A%5Cemph%7BReinDiffuse%7D%20that%20combines%20reinforcement%20learning%20with%20motion%20diffusion%0Amodel%20to%20generate%20physically%20credible%20human%20motions%20that%20align%20with%20textual%0Adescriptions.%20Our%20method%20adapts%20Motion%20Diffusion%20Model%20to%20output%20a%0Aparameterized%20distribution%20of%20actions%2C%20making%20them%20compatible%20with%0Areinforcement%20learning%20paradigms.%20We%20employ%20reinforcement%20learning%20with%20the%0Aobjective%20of%20maximizing%20physically%20plausible%20rewards%20to%20optimize%20motion%0Ageneration%20for%20physical%20fidelity.%20Our%20approach%20outperforms%20existing%0Astate-of-the-art%20models%20on%20two%20major%20datasets%2C%20HumanML3D%20and%20KIT-ML%2C%20achieving%0Asignificant%20improvements%20in%20physical%20plausibility%20and%20motion%20quality.%20Project%3A%0Ahttps%3A//reindiffuse.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07296v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinDiffuse%253A%2520Crafting%2520Physically%2520Plausible%2520Motions%2520with%2520Reinforced%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DGaoge%2520Han%2520and%2520Mingjiang%2520Liang%2520and%2520Jinglei%2520Tang%2520and%2520Yongkang%2520Cheng%2520and%2520Wei%2520Liu%2520and%2520Shaoli%2520Huang%26entry.1292438233%3D%2520%2520Generating%2520human%2520motion%2520from%2520textual%2520descriptions%2520is%2520a%2520challenging%2520task.%250AExisting%2520methods%2520either%2520struggle%2520with%2520physical%2520credibility%2520or%2520are%2520limited%2520by%250Athe%2520complexities%2520of%2520physics%2520simulations.%2520In%2520this%2520paper%252C%2520we%2520present%250A%255Cemph%257BReinDiffuse%257D%2520that%2520combines%2520reinforcement%2520learning%2520with%2520motion%2520diffusion%250Amodel%2520to%2520generate%2520physically%2520credible%2520human%2520motions%2520that%2520align%2520with%2520textual%250Adescriptions.%2520Our%2520method%2520adapts%2520Motion%2520Diffusion%2520Model%2520to%2520output%2520a%250Aparameterized%2520distribution%2520of%2520actions%252C%2520making%2520them%2520compatible%2520with%250Areinforcement%2520learning%2520paradigms.%2520We%2520employ%2520reinforcement%2520learning%2520with%2520the%250Aobjective%2520of%2520maximizing%2520physically%2520plausible%2520rewards%2520to%2520optimize%2520motion%250Ageneration%2520for%2520physical%2520fidelity.%2520Our%2520approach%2520outperforms%2520existing%250Astate-of-the-art%2520models%2520on%2520two%2520major%2520datasets%252C%2520HumanML3D%2520and%2520KIT-ML%252C%2520achieving%250Asignificant%2520improvements%2520in%2520physical%2520plausibility%2520and%2520motion%2520quality.%2520Project%253A%250Ahttps%253A//reindiffuse.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07296v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReinDiffuse%3A%20Crafting%20Physically%20Plausible%20Motions%20with%20Reinforced%0A%20%20Diffusion%20Model&entry.906535625=Gaoge%20Han%20and%20Mingjiang%20Liang%20and%20Jinglei%20Tang%20and%20Yongkang%20Cheng%20and%20Wei%20Liu%20and%20Shaoli%20Huang&entry.1292438233=%20%20Generating%20human%20motion%20from%20textual%20descriptions%20is%20a%20challenging%20task.%0AExisting%20methods%20either%20struggle%20with%20physical%20credibility%20or%20are%20limited%20by%0Athe%20complexities%20of%20physics%20simulations.%20In%20this%20paper%2C%20we%20present%0A%5Cemph%7BReinDiffuse%7D%20that%20combines%20reinforcement%20learning%20with%20motion%20diffusion%0Amodel%20to%20generate%20physically%20credible%20human%20motions%20that%20align%20with%20textual%0Adescriptions.%20Our%20method%20adapts%20Motion%20Diffusion%20Model%20to%20output%20a%0Aparameterized%20distribution%20of%20actions%2C%20making%20them%20compatible%20with%0Areinforcement%20learning%20paradigms.%20We%20employ%20reinforcement%20learning%20with%20the%0Aobjective%20of%20maximizing%20physically%20plausible%20rewards%20to%20optimize%20motion%0Ageneration%20for%20physical%20fidelity.%20Our%20approach%20outperforms%20existing%0Astate-of-the-art%20models%20on%20two%20major%20datasets%2C%20HumanML3D%20and%20KIT-ML%2C%20achieving%0Asignificant%20improvements%20in%20physical%20plausibility%20and%20motion%20quality.%20Project%3A%0Ahttps%3A//reindiffuse.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07296v2&entry.124074799=Read"},
{"title": "LoKO: Low-Rank Kalman Optimizer for Online Fine-Tuning of Large Models", "author": "Hossein Abdi and Mingfei Sun and Andi Zhang and Samuel Kaski and Wei Pan", "abstract": "  Training large models with millions or even billions of parameters from\nscratch incurs substantial computational costs. Parameter Efficient Fine-Tuning\n(PEFT) methods, particularly Low-Rank Adaptation (LoRA), address this challenge\nby adapting only a reduced number of parameters to specific tasks with\ngradient-based optimizers. In this paper, we cast PEFT as an optimal\nfiltering/state estimation problem and present Low-Rank Kalman Optimizer (LoKO)\nto estimate the optimal trainable parameters in an online manner. We leverage\nthe low-rank decomposition in LoRA to significantly reduce matrix sizes in\nKalman iterations and further capitalize on a diagonal approximation of the\ncovariance matrix to effectively decrease computational complexity from\nquadratic to linear in the number of trainable parameters. Moreover, we\ndiscovered that the initialization of the covariance matrix within the Kalman\nalgorithm and the accurate estimation of the observation noise covariance are\nthe keys in this formulation, and we propose robust approaches that work well\nacross a vast range of well-established computer vision and language models.\nOur results show that LoKO converges with fewer iterations and yields better\nperformance models compared to commonly used optimizers with LoRA in both image\nclassifications and language tasks. Our study opens up the possibility of\nleveraging the Kalman filter as an effective optimizer for the online\nfine-tuning of large models.\n", "link": "http://arxiv.org/abs/2410.11551v1", "date": "2024-10-15", "relevancy": 2.3087, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4589}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoKO%3A%20Low-Rank%20Kalman%20Optimizer%20for%20Online%20Fine-Tuning%20of%20Large%20Models&body=Title%3A%20LoKO%3A%20Low-Rank%20Kalman%20Optimizer%20for%20Online%20Fine-Tuning%20of%20Large%20Models%0AAuthor%3A%20Hossein%20Abdi%20and%20Mingfei%20Sun%20and%20Andi%20Zhang%20and%20Samuel%20Kaski%20and%20Wei%20Pan%0AAbstract%3A%20%20%20Training%20large%20models%20with%20millions%20or%20even%20billions%20of%20parameters%20from%0Ascratch%20incurs%20substantial%20computational%20costs.%20Parameter%20Efficient%20Fine-Tuning%0A%28PEFT%29%20methods%2C%20particularly%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20address%20this%20challenge%0Aby%20adapting%20only%20a%20reduced%20number%20of%20parameters%20to%20specific%20tasks%20with%0Agradient-based%20optimizers.%20In%20this%20paper%2C%20we%20cast%20PEFT%20as%20an%20optimal%0Afiltering/state%20estimation%20problem%20and%20present%20Low-Rank%20Kalman%20Optimizer%20%28LoKO%29%0Ato%20estimate%20the%20optimal%20trainable%20parameters%20in%20an%20online%20manner.%20We%20leverage%0Athe%20low-rank%20decomposition%20in%20LoRA%20to%20significantly%20reduce%20matrix%20sizes%20in%0AKalman%20iterations%20and%20further%20capitalize%20on%20a%20diagonal%20approximation%20of%20the%0Acovariance%20matrix%20to%20effectively%20decrease%20computational%20complexity%20from%0Aquadratic%20to%20linear%20in%20the%20number%20of%20trainable%20parameters.%20Moreover%2C%20we%0Adiscovered%20that%20the%20initialization%20of%20the%20covariance%20matrix%20within%20the%20Kalman%0Aalgorithm%20and%20the%20accurate%20estimation%20of%20the%20observation%20noise%20covariance%20are%0Athe%20keys%20in%20this%20formulation%2C%20and%20we%20propose%20robust%20approaches%20that%20work%20well%0Aacross%20a%20vast%20range%20of%20well-established%20computer%20vision%20and%20language%20models.%0AOur%20results%20show%20that%20LoKO%20converges%20with%20fewer%20iterations%20and%20yields%20better%0Aperformance%20models%20compared%20to%20commonly%20used%20optimizers%20with%20LoRA%20in%20both%20image%0Aclassifications%20and%20language%20tasks.%20Our%20study%20opens%20up%20the%20possibility%20of%0Aleveraging%20the%20Kalman%20filter%20as%20an%20effective%20optimizer%20for%20the%20online%0Afine-tuning%20of%20large%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoKO%253A%2520Low-Rank%2520Kalman%2520Optimizer%2520for%2520Online%2520Fine-Tuning%2520of%2520Large%2520Models%26entry.906535625%3DHossein%2520Abdi%2520and%2520Mingfei%2520Sun%2520and%2520Andi%2520Zhang%2520and%2520Samuel%2520Kaski%2520and%2520Wei%2520Pan%26entry.1292438233%3D%2520%2520Training%2520large%2520models%2520with%2520millions%2520or%2520even%2520billions%2520of%2520parameters%2520from%250Ascratch%2520incurs%2520substantial%2520computational%2520costs.%2520Parameter%2520Efficient%2520Fine-Tuning%250A%2528PEFT%2529%2520methods%252C%2520particularly%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520address%2520this%2520challenge%250Aby%2520adapting%2520only%2520a%2520reduced%2520number%2520of%2520parameters%2520to%2520specific%2520tasks%2520with%250Agradient-based%2520optimizers.%2520In%2520this%2520paper%252C%2520we%2520cast%2520PEFT%2520as%2520an%2520optimal%250Afiltering/state%2520estimation%2520problem%2520and%2520present%2520Low-Rank%2520Kalman%2520Optimizer%2520%2528LoKO%2529%250Ato%2520estimate%2520the%2520optimal%2520trainable%2520parameters%2520in%2520an%2520online%2520manner.%2520We%2520leverage%250Athe%2520low-rank%2520decomposition%2520in%2520LoRA%2520to%2520significantly%2520reduce%2520matrix%2520sizes%2520in%250AKalman%2520iterations%2520and%2520further%2520capitalize%2520on%2520a%2520diagonal%2520approximation%2520of%2520the%250Acovariance%2520matrix%2520to%2520effectively%2520decrease%2520computational%2520complexity%2520from%250Aquadratic%2520to%2520linear%2520in%2520the%2520number%2520of%2520trainable%2520parameters.%2520Moreover%252C%2520we%250Adiscovered%2520that%2520the%2520initialization%2520of%2520the%2520covariance%2520matrix%2520within%2520the%2520Kalman%250Aalgorithm%2520and%2520the%2520accurate%2520estimation%2520of%2520the%2520observation%2520noise%2520covariance%2520are%250Athe%2520keys%2520in%2520this%2520formulation%252C%2520and%2520we%2520propose%2520robust%2520approaches%2520that%2520work%2520well%250Aacross%2520a%2520vast%2520range%2520of%2520well-established%2520computer%2520vision%2520and%2520language%2520models.%250AOur%2520results%2520show%2520that%2520LoKO%2520converges%2520with%2520fewer%2520iterations%2520and%2520yields%2520better%250Aperformance%2520models%2520compared%2520to%2520commonly%2520used%2520optimizers%2520with%2520LoRA%2520in%2520both%2520image%250Aclassifications%2520and%2520language%2520tasks.%2520Our%2520study%2520opens%2520up%2520the%2520possibility%2520of%250Aleveraging%2520the%2520Kalman%2520filter%2520as%2520an%2520effective%2520optimizer%2520for%2520the%2520online%250Afine-tuning%2520of%2520large%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoKO%3A%20Low-Rank%20Kalman%20Optimizer%20for%20Online%20Fine-Tuning%20of%20Large%20Models&entry.906535625=Hossein%20Abdi%20and%20Mingfei%20Sun%20and%20Andi%20Zhang%20and%20Samuel%20Kaski%20and%20Wei%20Pan&entry.1292438233=%20%20Training%20large%20models%20with%20millions%20or%20even%20billions%20of%20parameters%20from%0Ascratch%20incurs%20substantial%20computational%20costs.%20Parameter%20Efficient%20Fine-Tuning%0A%28PEFT%29%20methods%2C%20particularly%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20address%20this%20challenge%0Aby%20adapting%20only%20a%20reduced%20number%20of%20parameters%20to%20specific%20tasks%20with%0Agradient-based%20optimizers.%20In%20this%20paper%2C%20we%20cast%20PEFT%20as%20an%20optimal%0Afiltering/state%20estimation%20problem%20and%20present%20Low-Rank%20Kalman%20Optimizer%20%28LoKO%29%0Ato%20estimate%20the%20optimal%20trainable%20parameters%20in%20an%20online%20manner.%20We%20leverage%0Athe%20low-rank%20decomposition%20in%20LoRA%20to%20significantly%20reduce%20matrix%20sizes%20in%0AKalman%20iterations%20and%20further%20capitalize%20on%20a%20diagonal%20approximation%20of%20the%0Acovariance%20matrix%20to%20effectively%20decrease%20computational%20complexity%20from%0Aquadratic%20to%20linear%20in%20the%20number%20of%20trainable%20parameters.%20Moreover%2C%20we%0Adiscovered%20that%20the%20initialization%20of%20the%20covariance%20matrix%20within%20the%20Kalman%0Aalgorithm%20and%20the%20accurate%20estimation%20of%20the%20observation%20noise%20covariance%20are%0Athe%20keys%20in%20this%20formulation%2C%20and%20we%20propose%20robust%20approaches%20that%20work%20well%0Aacross%20a%20vast%20range%20of%20well-established%20computer%20vision%20and%20language%20models.%0AOur%20results%20show%20that%20LoKO%20converges%20with%20fewer%20iterations%20and%20yields%20better%0Aperformance%20models%20compared%20to%20commonly%20used%20optimizers%20with%20LoRA%20in%20both%20image%0Aclassifications%20and%20language%20tasks.%20Our%20study%20opens%20up%20the%20possibility%20of%0Aleveraging%20the%20Kalman%20filter%20as%20an%20effective%20optimizer%20for%20the%20online%0Afine-tuning%20of%20large%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11551v1&entry.124074799=Read"},
{"title": "VIA: Unified Spatiotemporal Video Adaptation Framework for Global and\n  Local Video Editing", "author": "Jing Gu and Yuwei Fang and Ivan Skorokhodov and Peter Wonka and Xinya Du and Sergey Tulyakov and Xin Eric Wang", "abstract": "  Video editing is a cornerstone of digital media, from entertainment and\neducation to professional communication. However, previous methods often\noverlook the necessity of comprehensively understanding both global and local\ncontexts, leading to inaccurate and inconsistent edits in the spatiotemporal\ndimension, especially for long videos. In this paper, we introduce VIA, a\nunified spatiotemporal Video Adaptation framework for global and local video\nediting, pushing the limits of consistently editing minute-long videos. First,\nto ensure local consistency within individual frames, we designed test-time\nediting adaptation to adapt a pre-trained image editing model for improving\nconsistency between potential editing directions and the text instruction, and\nadapt masked latent variables for precise local control. Furthermore, to\nmaintain global consistency over the video sequence, we introduce\nspatiotemporal adaptation that recursively gather consistent attention\nvariables in key frames and strategically applies them across the whole\nsequence to realize the editing effects. Extensive experiments demonstrate\nthat, compared to baseline methods, our VIA approach produces edits that are\nmore faithful to the source videos, more coherent in the spatiotemporal\ncontext, and more precise in local control. More importantly, we show that VIA\ncan achieve consistent long video editing in minutes, unlocking the potential\nfor advanced video editing tasks over long video sequences.\n", "link": "http://arxiv.org/abs/2406.12831v2", "date": "2024-10-15", "relevancy": 2.3071, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6349}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5858}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIA%3A%20Unified%20Spatiotemporal%20Video%20Adaptation%20Framework%20for%20Global%20and%0A%20%20Local%20Video%20Editing&body=Title%3A%20VIA%3A%20Unified%20Spatiotemporal%20Video%20Adaptation%20Framework%20for%20Global%20and%0A%20%20Local%20Video%20Editing%0AAuthor%3A%20Jing%20Gu%20and%20Yuwei%20Fang%20and%20Ivan%20Skorokhodov%20and%20Peter%20Wonka%20and%20Xinya%20Du%20and%20Sergey%20Tulyakov%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20Video%20editing%20is%20a%20cornerstone%20of%20digital%20media%2C%20from%20entertainment%20and%0Aeducation%20to%20professional%20communication.%20However%2C%20previous%20methods%20often%0Aoverlook%20the%20necessity%20of%20comprehensively%20understanding%20both%20global%20and%20local%0Acontexts%2C%20leading%20to%20inaccurate%20and%20inconsistent%20edits%20in%20the%20spatiotemporal%0Adimension%2C%20especially%20for%20long%20videos.%20In%20this%20paper%2C%20we%20introduce%20VIA%2C%20a%0Aunified%20spatiotemporal%20Video%20Adaptation%20framework%20for%20global%20and%20local%20video%0Aediting%2C%20pushing%20the%20limits%20of%20consistently%20editing%20minute-long%20videos.%20First%2C%0Ato%20ensure%20local%20consistency%20within%20individual%20frames%2C%20we%20designed%20test-time%0Aediting%20adaptation%20to%20adapt%20a%20pre-trained%20image%20editing%20model%20for%20improving%0Aconsistency%20between%20potential%20editing%20directions%20and%20the%20text%20instruction%2C%20and%0Aadapt%20masked%20latent%20variables%20for%20precise%20local%20control.%20Furthermore%2C%20to%0Amaintain%20global%20consistency%20over%20the%20video%20sequence%2C%20we%20introduce%0Aspatiotemporal%20adaptation%20that%20recursively%20gather%20consistent%20attention%0Avariables%20in%20key%20frames%20and%20strategically%20applies%20them%20across%20the%20whole%0Asequence%20to%20realize%20the%20editing%20effects.%20Extensive%20experiments%20demonstrate%0Athat%2C%20compared%20to%20baseline%20methods%2C%20our%20VIA%20approach%20produces%20edits%20that%20are%0Amore%20faithful%20to%20the%20source%20videos%2C%20more%20coherent%20in%20the%20spatiotemporal%0Acontext%2C%20and%20more%20precise%20in%20local%20control.%20More%20importantly%2C%20we%20show%20that%20VIA%0Acan%20achieve%20consistent%20long%20video%20editing%20in%20minutes%2C%20unlocking%20the%20potential%0Afor%20advanced%20video%20editing%20tasks%20over%20long%20video%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIA%253A%2520Unified%2520Spatiotemporal%2520Video%2520Adaptation%2520Framework%2520for%2520Global%2520and%250A%2520%2520Local%2520Video%2520Editing%26entry.906535625%3DJing%2520Gu%2520and%2520Yuwei%2520Fang%2520and%2520Ivan%2520Skorokhodov%2520and%2520Peter%2520Wonka%2520and%2520Xinya%2520Du%2520and%2520Sergey%2520Tulyakov%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520Video%2520editing%2520is%2520a%2520cornerstone%2520of%2520digital%2520media%252C%2520from%2520entertainment%2520and%250Aeducation%2520to%2520professional%2520communication.%2520However%252C%2520previous%2520methods%2520often%250Aoverlook%2520the%2520necessity%2520of%2520comprehensively%2520understanding%2520both%2520global%2520and%2520local%250Acontexts%252C%2520leading%2520to%2520inaccurate%2520and%2520inconsistent%2520edits%2520in%2520the%2520spatiotemporal%250Adimension%252C%2520especially%2520for%2520long%2520videos.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VIA%252C%2520a%250Aunified%2520spatiotemporal%2520Video%2520Adaptation%2520framework%2520for%2520global%2520and%2520local%2520video%250Aediting%252C%2520pushing%2520the%2520limits%2520of%2520consistently%2520editing%2520minute-long%2520videos.%2520First%252C%250Ato%2520ensure%2520local%2520consistency%2520within%2520individual%2520frames%252C%2520we%2520designed%2520test-time%250Aediting%2520adaptation%2520to%2520adapt%2520a%2520pre-trained%2520image%2520editing%2520model%2520for%2520improving%250Aconsistency%2520between%2520potential%2520editing%2520directions%2520and%2520the%2520text%2520instruction%252C%2520and%250Aadapt%2520masked%2520latent%2520variables%2520for%2520precise%2520local%2520control.%2520Furthermore%252C%2520to%250Amaintain%2520global%2520consistency%2520over%2520the%2520video%2520sequence%252C%2520we%2520introduce%250Aspatiotemporal%2520adaptation%2520that%2520recursively%2520gather%2520consistent%2520attention%250Avariables%2520in%2520key%2520frames%2520and%2520strategically%2520applies%2520them%2520across%2520the%2520whole%250Asequence%2520to%2520realize%2520the%2520editing%2520effects.%2520Extensive%2520experiments%2520demonstrate%250Athat%252C%2520compared%2520to%2520baseline%2520methods%252C%2520our%2520VIA%2520approach%2520produces%2520edits%2520that%2520are%250Amore%2520faithful%2520to%2520the%2520source%2520videos%252C%2520more%2520coherent%2520in%2520the%2520spatiotemporal%250Acontext%252C%2520and%2520more%2520precise%2520in%2520local%2520control.%2520More%2520importantly%252C%2520we%2520show%2520that%2520VIA%250Acan%2520achieve%2520consistent%2520long%2520video%2520editing%2520in%2520minutes%252C%2520unlocking%2520the%2520potential%250Afor%2520advanced%2520video%2520editing%2520tasks%2520over%2520long%2520video%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIA%3A%20Unified%20Spatiotemporal%20Video%20Adaptation%20Framework%20for%20Global%20and%0A%20%20Local%20Video%20Editing&entry.906535625=Jing%20Gu%20and%20Yuwei%20Fang%20and%20Ivan%20Skorokhodov%20and%20Peter%20Wonka%20and%20Xinya%20Du%20and%20Sergey%20Tulyakov%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20Video%20editing%20is%20a%20cornerstone%20of%20digital%20media%2C%20from%20entertainment%20and%0Aeducation%20to%20professional%20communication.%20However%2C%20previous%20methods%20often%0Aoverlook%20the%20necessity%20of%20comprehensively%20understanding%20both%20global%20and%20local%0Acontexts%2C%20leading%20to%20inaccurate%20and%20inconsistent%20edits%20in%20the%20spatiotemporal%0Adimension%2C%20especially%20for%20long%20videos.%20In%20this%20paper%2C%20we%20introduce%20VIA%2C%20a%0Aunified%20spatiotemporal%20Video%20Adaptation%20framework%20for%20global%20and%20local%20video%0Aediting%2C%20pushing%20the%20limits%20of%20consistently%20editing%20minute-long%20videos.%20First%2C%0Ato%20ensure%20local%20consistency%20within%20individual%20frames%2C%20we%20designed%20test-time%0Aediting%20adaptation%20to%20adapt%20a%20pre-trained%20image%20editing%20model%20for%20improving%0Aconsistency%20between%20potential%20editing%20directions%20and%20the%20text%20instruction%2C%20and%0Aadapt%20masked%20latent%20variables%20for%20precise%20local%20control.%20Furthermore%2C%20to%0Amaintain%20global%20consistency%20over%20the%20video%20sequence%2C%20we%20introduce%0Aspatiotemporal%20adaptation%20that%20recursively%20gather%20consistent%20attention%0Avariables%20in%20key%20frames%20and%20strategically%20applies%20them%20across%20the%20whole%0Asequence%20to%20realize%20the%20editing%20effects.%20Extensive%20experiments%20demonstrate%0Athat%2C%20compared%20to%20baseline%20methods%2C%20our%20VIA%20approach%20produces%20edits%20that%20are%0Amore%20faithful%20to%20the%20source%20videos%2C%20more%20coherent%20in%20the%20spatiotemporal%0Acontext%2C%20and%20more%20precise%20in%20local%20control.%20More%20importantly%2C%20we%20show%20that%20VIA%0Acan%20achieve%20consistent%20long%20video%20editing%20in%20minutes%2C%20unlocking%20the%20potential%0Afor%20advanced%20video%20editing%20tasks%20over%20long%20video%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12831v2&entry.124074799=Read"},
{"title": "Multi-round jailbreak attack on large language models", "author": "Yihua Zhou and Xiaochuan Shi", "abstract": "  Ensuring the safety and alignment of large language models (LLMs) with human\nvalues is crucial for generating responses that are beneficial to humanity.\nWhile LLMs have the capability to identify and avoid harmful queries, they\nremain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can\ninduce the generation of toxic content. Traditional single-round jailbreak\nattacks, such as GCG and AutoDAN, do not alter the sensitive words in the\ndangerous prompts. Although they can temporarily bypass the model's safeguards\nthrough prompt engineering, their success rate drops significantly as the LLM\nis further fine-tuned, and they cannot effectively circumvent static rule-based\nfilters that remove the hazardous vocabulary.\n  In this study, to better understand jailbreak attacks, we introduce a\nmulti-round jailbreak approach. This method can rewrite the dangerous prompts,\ndecomposing them into a series of less harmful sub-questions to bypass the\nLLM's safety checks. We first use the LLM to perform a decomposition task,\nbreaking down a set of natural language questions into a sequence of\nprogressive sub-questions, which are then used to fine-tune the Llama3-8B\nmodel, enabling it to decompose hazardous prompts. The fine-tuned model is then\nused to break down the problematic prompt, and the resulting sub-questions are\nsequentially asked to the victim model. If the victim model rejects a\nsub-question, a new decomposition is generated, and the process is repeated\nuntil the final objective is achieved. Our experimental results show a 94\\%\nsuccess rate on the llama2-7B and demonstrate the effectiveness of this\napproach in circumventing static rule-based filters.\n", "link": "http://arxiv.org/abs/2410.11533v1", "date": "2024-10-15", "relevancy": 2.2837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-round%20jailbreak%20attack%20on%20large%20language%20models&body=Title%3A%20Multi-round%20jailbreak%20attack%20on%20large%20language%20models%0AAuthor%3A%20Yihua%20Zhou%20and%20Xiaochuan%20Shi%0AAbstract%3A%20%20%20Ensuring%20the%20safety%20and%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%0Avalues%20is%20crucial%20for%20generating%20responses%20that%20are%20beneficial%20to%20humanity.%0AWhile%20LLMs%20have%20the%20capability%20to%20identify%20and%20avoid%20harmful%20queries%2C%20they%0Aremain%20vulnerable%20to%20%22jailbreak%22%20attacks%2C%20where%20carefully%20crafted%20prompts%20can%0Ainduce%20the%20generation%20of%20toxic%20content.%20Traditional%20single-round%20jailbreak%0Aattacks%2C%20such%20as%20GCG%20and%20AutoDAN%2C%20do%20not%20alter%20the%20sensitive%20words%20in%20the%0Adangerous%20prompts.%20Although%20they%20can%20temporarily%20bypass%20the%20model%27s%20safeguards%0Athrough%20prompt%20engineering%2C%20their%20success%20rate%20drops%20significantly%20as%20the%20LLM%0Ais%20further%20fine-tuned%2C%20and%20they%20cannot%20effectively%20circumvent%20static%20rule-based%0Afilters%20that%20remove%20the%20hazardous%20vocabulary.%0A%20%20In%20this%20study%2C%20to%20better%20understand%20jailbreak%20attacks%2C%20we%20introduce%20a%0Amulti-round%20jailbreak%20approach.%20This%20method%20can%20rewrite%20the%20dangerous%20prompts%2C%0Adecomposing%20them%20into%20a%20series%20of%20less%20harmful%20sub-questions%20to%20bypass%20the%0ALLM%27s%20safety%20checks.%20We%20first%20use%20the%20LLM%20to%20perform%20a%20decomposition%20task%2C%0Abreaking%20down%20a%20set%20of%20natural%20language%20questions%20into%20a%20sequence%20of%0Aprogressive%20sub-questions%2C%20which%20are%20then%20used%20to%20fine-tune%20the%20Llama3-8B%0Amodel%2C%20enabling%20it%20to%20decompose%20hazardous%20prompts.%20The%20fine-tuned%20model%20is%20then%0Aused%20to%20break%20down%20the%20problematic%20prompt%2C%20and%20the%20resulting%20sub-questions%20are%0Asequentially%20asked%20to%20the%20victim%20model.%20If%20the%20victim%20model%20rejects%20a%0Asub-question%2C%20a%20new%20decomposition%20is%20generated%2C%20and%20the%20process%20is%20repeated%0Auntil%20the%20final%20objective%20is%20achieved.%20Our%20experimental%20results%20show%20a%2094%5C%25%0Asuccess%20rate%20on%20the%20llama2-7B%20and%20demonstrate%20the%20effectiveness%20of%20this%0Aapproach%20in%20circumventing%20static%20rule-based%20filters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-round%2520jailbreak%2520attack%2520on%2520large%2520language%2520models%26entry.906535625%3DYihua%2520Zhou%2520and%2520Xiaochuan%2520Shi%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520safety%2520and%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%250Avalues%2520is%2520crucial%2520for%2520generating%2520responses%2520that%2520are%2520beneficial%2520to%2520humanity.%250AWhile%2520LLMs%2520have%2520the%2520capability%2520to%2520identify%2520and%2520avoid%2520harmful%2520queries%252C%2520they%250Aremain%2520vulnerable%2520to%2520%2522jailbreak%2522%2520attacks%252C%2520where%2520carefully%2520crafted%2520prompts%2520can%250Ainduce%2520the%2520generation%2520of%2520toxic%2520content.%2520Traditional%2520single-round%2520jailbreak%250Aattacks%252C%2520such%2520as%2520GCG%2520and%2520AutoDAN%252C%2520do%2520not%2520alter%2520the%2520sensitive%2520words%2520in%2520the%250Adangerous%2520prompts.%2520Although%2520they%2520can%2520temporarily%2520bypass%2520the%2520model%2527s%2520safeguards%250Athrough%2520prompt%2520engineering%252C%2520their%2520success%2520rate%2520drops%2520significantly%2520as%2520the%2520LLM%250Ais%2520further%2520fine-tuned%252C%2520and%2520they%2520cannot%2520effectively%2520circumvent%2520static%2520rule-based%250Afilters%2520that%2520remove%2520the%2520hazardous%2520vocabulary.%250A%2520%2520In%2520this%2520study%252C%2520to%2520better%2520understand%2520jailbreak%2520attacks%252C%2520we%2520introduce%2520a%250Amulti-round%2520jailbreak%2520approach.%2520This%2520method%2520can%2520rewrite%2520the%2520dangerous%2520prompts%252C%250Adecomposing%2520them%2520into%2520a%2520series%2520of%2520less%2520harmful%2520sub-questions%2520to%2520bypass%2520the%250ALLM%2527s%2520safety%2520checks.%2520We%2520first%2520use%2520the%2520LLM%2520to%2520perform%2520a%2520decomposition%2520task%252C%250Abreaking%2520down%2520a%2520set%2520of%2520natural%2520language%2520questions%2520into%2520a%2520sequence%2520of%250Aprogressive%2520sub-questions%252C%2520which%2520are%2520then%2520used%2520to%2520fine-tune%2520the%2520Llama3-8B%250Amodel%252C%2520enabling%2520it%2520to%2520decompose%2520hazardous%2520prompts.%2520The%2520fine-tuned%2520model%2520is%2520then%250Aused%2520to%2520break%2520down%2520the%2520problematic%2520prompt%252C%2520and%2520the%2520resulting%2520sub-questions%2520are%250Asequentially%2520asked%2520to%2520the%2520victim%2520model.%2520If%2520the%2520victim%2520model%2520rejects%2520a%250Asub-question%252C%2520a%2520new%2520decomposition%2520is%2520generated%252C%2520and%2520the%2520process%2520is%2520repeated%250Auntil%2520the%2520final%2520objective%2520is%2520achieved.%2520Our%2520experimental%2520results%2520show%2520a%252094%255C%2525%250Asuccess%2520rate%2520on%2520the%2520llama2-7B%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520this%250Aapproach%2520in%2520circumventing%2520static%2520rule-based%2520filters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-round%20jailbreak%20attack%20on%20large%20language%20models&entry.906535625=Yihua%20Zhou%20and%20Xiaochuan%20Shi&entry.1292438233=%20%20Ensuring%20the%20safety%20and%20alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%0Avalues%20is%20crucial%20for%20generating%20responses%20that%20are%20beneficial%20to%20humanity.%0AWhile%20LLMs%20have%20the%20capability%20to%20identify%20and%20avoid%20harmful%20queries%2C%20they%0Aremain%20vulnerable%20to%20%22jailbreak%22%20attacks%2C%20where%20carefully%20crafted%20prompts%20can%0Ainduce%20the%20generation%20of%20toxic%20content.%20Traditional%20single-round%20jailbreak%0Aattacks%2C%20such%20as%20GCG%20and%20AutoDAN%2C%20do%20not%20alter%20the%20sensitive%20words%20in%20the%0Adangerous%20prompts.%20Although%20they%20can%20temporarily%20bypass%20the%20model%27s%20safeguards%0Athrough%20prompt%20engineering%2C%20their%20success%20rate%20drops%20significantly%20as%20the%20LLM%0Ais%20further%20fine-tuned%2C%20and%20they%20cannot%20effectively%20circumvent%20static%20rule-based%0Afilters%20that%20remove%20the%20hazardous%20vocabulary.%0A%20%20In%20this%20study%2C%20to%20better%20understand%20jailbreak%20attacks%2C%20we%20introduce%20a%0Amulti-round%20jailbreak%20approach.%20This%20method%20can%20rewrite%20the%20dangerous%20prompts%2C%0Adecomposing%20them%20into%20a%20series%20of%20less%20harmful%20sub-questions%20to%20bypass%20the%0ALLM%27s%20safety%20checks.%20We%20first%20use%20the%20LLM%20to%20perform%20a%20decomposition%20task%2C%0Abreaking%20down%20a%20set%20of%20natural%20language%20questions%20into%20a%20sequence%20of%0Aprogressive%20sub-questions%2C%20which%20are%20then%20used%20to%20fine-tune%20the%20Llama3-8B%0Amodel%2C%20enabling%20it%20to%20decompose%20hazardous%20prompts.%20The%20fine-tuned%20model%20is%20then%0Aused%20to%20break%20down%20the%20problematic%20prompt%2C%20and%20the%20resulting%20sub-questions%20are%0Asequentially%20asked%20to%20the%20victim%20model.%20If%20the%20victim%20model%20rejects%20a%0Asub-question%2C%20a%20new%20decomposition%20is%20generated%2C%20and%20the%20process%20is%20repeated%0Auntil%20the%20final%20objective%20is%20achieved.%20Our%20experimental%20results%20show%20a%2094%5C%25%0Asuccess%20rate%20on%20the%20llama2-7B%20and%20demonstrate%20the%20effectiveness%20of%20this%0Aapproach%20in%20circumventing%20static%20rule-based%20filters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11533v1&entry.124074799=Read"},
{"title": "Look Ma, no markers: holistic performance capture without the hassle", "author": "Charlie Hewitt and Fatemeh Saleh and Sadegh Aliakbarian and Lohit Petikam and Shideh Rezaeifar and Louis Florentin and Zafiirah Hosenie and Thomas J Cashman and Julien Valentin and Darren Cosker and Tadas Baltrusaitis", "abstract": "  We tackle the problem of highly-accurate, holistic performance capture for\nthe face, body and hands simultaneously. Motion-capture technologies used in\nfilm and game production typically focus only on face, body or hand capture\nindependently, involve complex and expensive hardware and a high degree of\nmanual intervention from skilled operators. While machine-learning-based\napproaches exist to overcome these problems, they usually only support a single\ncamera, often operate on a single part of the body, do not produce precise\nworld-space results, and rarely generalize outside specific contexts. In this\nwork, we introduce the first technique for marker-free, high-quality\nreconstruction of the complete human body, including eyes and tongue, without\nrequiring any calibration, manual intervention or custom hardware. Our approach\nproduces stable world-space results from arbitrary camera rigs as well as\nsupporting varied capture environments and clothing. We achieve this through a\nhybrid approach that leverages machine learning models trained exclusively on\nsynthetic data and powerful parametric models of human shape and motion. We\nevaluate our method on a number of body, face and hand reconstruction\nbenchmarks and demonstrate state-of-the-art results that generalize on diverse\ndatasets.\n", "link": "http://arxiv.org/abs/2410.11520v1", "date": "2024-10-15", "relevancy": 2.2826, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5781}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5676}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Ma%2C%20no%20markers%3A%20holistic%20performance%20capture%20without%20the%20hassle&body=Title%3A%20Look%20Ma%2C%20no%20markers%3A%20holistic%20performance%20capture%20without%20the%20hassle%0AAuthor%3A%20Charlie%20Hewitt%20and%20Fatemeh%20Saleh%20and%20Sadegh%20Aliakbarian%20and%20Lohit%20Petikam%20and%20Shideh%20Rezaeifar%20and%20Louis%20Florentin%20and%20Zafiirah%20Hosenie%20and%20Thomas%20J%20Cashman%20and%20Julien%20Valentin%20and%20Darren%20Cosker%20and%20Tadas%20Baltrusaitis%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20highly-accurate%2C%20holistic%20performance%20capture%20for%0Athe%20face%2C%20body%20and%20hands%20simultaneously.%20Motion-capture%20technologies%20used%20in%0Afilm%20and%20game%20production%20typically%20focus%20only%20on%20face%2C%20body%20or%20hand%20capture%0Aindependently%2C%20involve%20complex%20and%20expensive%20hardware%20and%20a%20high%20degree%20of%0Amanual%20intervention%20from%20skilled%20operators.%20While%20machine-learning-based%0Aapproaches%20exist%20to%20overcome%20these%20problems%2C%20they%20usually%20only%20support%20a%20single%0Acamera%2C%20often%20operate%20on%20a%20single%20part%20of%20the%20body%2C%20do%20not%20produce%20precise%0Aworld-space%20results%2C%20and%20rarely%20generalize%20outside%20specific%20contexts.%20In%20this%0Awork%2C%20we%20introduce%20the%20first%20technique%20for%20marker-free%2C%20high-quality%0Areconstruction%20of%20the%20complete%20human%20body%2C%20including%20eyes%20and%20tongue%2C%20without%0Arequiring%20any%20calibration%2C%20manual%20intervention%20or%20custom%20hardware.%20Our%20approach%0Aproduces%20stable%20world-space%20results%20from%20arbitrary%20camera%20rigs%20as%20well%20as%0Asupporting%20varied%20capture%20environments%20and%20clothing.%20We%20achieve%20this%20through%20a%0Ahybrid%20approach%20that%20leverages%20machine%20learning%20models%20trained%20exclusively%20on%0Asynthetic%20data%20and%20powerful%20parametric%20models%20of%20human%20shape%20and%20motion.%20We%0Aevaluate%20our%20method%20on%20a%20number%20of%20body%2C%20face%20and%20hand%20reconstruction%0Abenchmarks%20and%20demonstrate%20state-of-the-art%20results%20that%20generalize%20on%20diverse%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Ma%252C%2520no%2520markers%253A%2520holistic%2520performance%2520capture%2520without%2520the%2520hassle%26entry.906535625%3DCharlie%2520Hewitt%2520and%2520Fatemeh%2520Saleh%2520and%2520Sadegh%2520Aliakbarian%2520and%2520Lohit%2520Petikam%2520and%2520Shideh%2520Rezaeifar%2520and%2520Louis%2520Florentin%2520and%2520Zafiirah%2520Hosenie%2520and%2520Thomas%2520J%2520Cashman%2520and%2520Julien%2520Valentin%2520and%2520Darren%2520Cosker%2520and%2520Tadas%2520Baltrusaitis%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520highly-accurate%252C%2520holistic%2520performance%2520capture%2520for%250Athe%2520face%252C%2520body%2520and%2520hands%2520simultaneously.%2520Motion-capture%2520technologies%2520used%2520in%250Afilm%2520and%2520game%2520production%2520typically%2520focus%2520only%2520on%2520face%252C%2520body%2520or%2520hand%2520capture%250Aindependently%252C%2520involve%2520complex%2520and%2520expensive%2520hardware%2520and%2520a%2520high%2520degree%2520of%250Amanual%2520intervention%2520from%2520skilled%2520operators.%2520While%2520machine-learning-based%250Aapproaches%2520exist%2520to%2520overcome%2520these%2520problems%252C%2520they%2520usually%2520only%2520support%2520a%2520single%250Acamera%252C%2520often%2520operate%2520on%2520a%2520single%2520part%2520of%2520the%2520body%252C%2520do%2520not%2520produce%2520precise%250Aworld-space%2520results%252C%2520and%2520rarely%2520generalize%2520outside%2520specific%2520contexts.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520the%2520first%2520technique%2520for%2520marker-free%252C%2520high-quality%250Areconstruction%2520of%2520the%2520complete%2520human%2520body%252C%2520including%2520eyes%2520and%2520tongue%252C%2520without%250Arequiring%2520any%2520calibration%252C%2520manual%2520intervention%2520or%2520custom%2520hardware.%2520Our%2520approach%250Aproduces%2520stable%2520world-space%2520results%2520from%2520arbitrary%2520camera%2520rigs%2520as%2520well%2520as%250Asupporting%2520varied%2520capture%2520environments%2520and%2520clothing.%2520We%2520achieve%2520this%2520through%2520a%250Ahybrid%2520approach%2520that%2520leverages%2520machine%2520learning%2520models%2520trained%2520exclusively%2520on%250Asynthetic%2520data%2520and%2520powerful%2520parametric%2520models%2520of%2520human%2520shape%2520and%2520motion.%2520We%250Aevaluate%2520our%2520method%2520on%2520a%2520number%2520of%2520body%252C%2520face%2520and%2520hand%2520reconstruction%250Abenchmarks%2520and%2520demonstrate%2520state-of-the-art%2520results%2520that%2520generalize%2520on%2520diverse%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Ma%2C%20no%20markers%3A%20holistic%20performance%20capture%20without%20the%20hassle&entry.906535625=Charlie%20Hewitt%20and%20Fatemeh%20Saleh%20and%20Sadegh%20Aliakbarian%20and%20Lohit%20Petikam%20and%20Shideh%20Rezaeifar%20and%20Louis%20Florentin%20and%20Zafiirah%20Hosenie%20and%20Thomas%20J%20Cashman%20and%20Julien%20Valentin%20and%20Darren%20Cosker%20and%20Tadas%20Baltrusaitis&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20highly-accurate%2C%20holistic%20performance%20capture%20for%0Athe%20face%2C%20body%20and%20hands%20simultaneously.%20Motion-capture%20technologies%20used%20in%0Afilm%20and%20game%20production%20typically%20focus%20only%20on%20face%2C%20body%20or%20hand%20capture%0Aindependently%2C%20involve%20complex%20and%20expensive%20hardware%20and%20a%20high%20degree%20of%0Amanual%20intervention%20from%20skilled%20operators.%20While%20machine-learning-based%0Aapproaches%20exist%20to%20overcome%20these%20problems%2C%20they%20usually%20only%20support%20a%20single%0Acamera%2C%20often%20operate%20on%20a%20single%20part%20of%20the%20body%2C%20do%20not%20produce%20precise%0Aworld-space%20results%2C%20and%20rarely%20generalize%20outside%20specific%20contexts.%20In%20this%0Awork%2C%20we%20introduce%20the%20first%20technique%20for%20marker-free%2C%20high-quality%0Areconstruction%20of%20the%20complete%20human%20body%2C%20including%20eyes%20and%20tongue%2C%20without%0Arequiring%20any%20calibration%2C%20manual%20intervention%20or%20custom%20hardware.%20Our%20approach%0Aproduces%20stable%20world-space%20results%20from%20arbitrary%20camera%20rigs%20as%20well%20as%0Asupporting%20varied%20capture%20environments%20and%20clothing.%20We%20achieve%20this%20through%20a%0Ahybrid%20approach%20that%20leverages%20machine%20learning%20models%20trained%20exclusively%20on%0Asynthetic%20data%20and%20powerful%20parametric%20models%20of%20human%20shape%20and%20motion.%20We%0Aevaluate%20our%20method%20on%20a%20number%20of%20body%2C%20face%20and%20hand%20reconstruction%0Abenchmarks%20and%20demonstrate%20state-of-the-art%20results%20that%20generalize%20on%20diverse%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11520v1&entry.124074799=Read"},
{"title": "Fine-Grained Spatiotemporal Motion Alignment for Contrastive Video\n  Representation Learning", "author": "Minghao Zhu and Xiao Lin and Ronghao Dang and Chengju Liu and Qijun Chen", "abstract": "  As the most essential property in a video, motion information is critical to\na robust and generalized video representation. To inject motion dynamics,\nrecent works have adopted frame difference as the source of motion information\nin video contrastive learning, considering the trade-off between quality and\ncost. However, existing works align motion features at the instance level,\nwhich suffers from spatial and temporal weak alignment across modalities. In\nthis paper, we present a \\textbf{Fi}ne-grained \\textbf{M}otion\n\\textbf{A}lignment (FIMA) framework, capable of introducing well-aligned and\nsignificant motion information. Specifically, we first develop a dense\ncontrastive learning framework in the spatiotemporal domain to generate\npixel-level motion supervision. Then, we design a motion decoder and a\nforeground sampling strategy to eliminate the weak alignments in terms of time\nand space. Moreover, a frame-level motion contrastive loss is presented to\nimprove the temporal diversity of the motion features. Extensive experiments\ndemonstrate that the representations learned by FIMA possess great\nmotion-awareness capabilities and achieve state-of-the-art or competitive\nresults on downstream tasks across UCF101, HMDB51, and Diving48 datasets. Code\nis available at \\url{https://github.com/ZMHH-H/FIMA}.\n", "link": "http://arxiv.org/abs/2309.00297v2", "date": "2024-10-15", "relevancy": 2.2789, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6176}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5715}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Spatiotemporal%20Motion%20Alignment%20for%20Contrastive%20Video%0A%20%20Representation%20Learning&body=Title%3A%20Fine-Grained%20Spatiotemporal%20Motion%20Alignment%20for%20Contrastive%20Video%0A%20%20Representation%20Learning%0AAuthor%3A%20Minghao%20Zhu%20and%20Xiao%20Lin%20and%20Ronghao%20Dang%20and%20Chengju%20Liu%20and%20Qijun%20Chen%0AAbstract%3A%20%20%20As%20the%20most%20essential%20property%20in%20a%20video%2C%20motion%20information%20is%20critical%20to%0Aa%20robust%20and%20generalized%20video%20representation.%20To%20inject%20motion%20dynamics%2C%0Arecent%20works%20have%20adopted%20frame%20difference%20as%20the%20source%20of%20motion%20information%0Ain%20video%20contrastive%20learning%2C%20considering%20the%20trade-off%20between%20quality%20and%0Acost.%20However%2C%20existing%20works%20align%20motion%20features%20at%20the%20instance%20level%2C%0Awhich%20suffers%20from%20spatial%20and%20temporal%20weak%20alignment%20across%20modalities.%20In%0Athis%20paper%2C%20we%20present%20a%20%5Ctextbf%7BFi%7Dne-grained%20%5Ctextbf%7BM%7Dotion%0A%5Ctextbf%7BA%7Dlignment%20%28FIMA%29%20framework%2C%20capable%20of%20introducing%20well-aligned%20and%0Asignificant%20motion%20information.%20Specifically%2C%20we%20first%20develop%20a%20dense%0Acontrastive%20learning%20framework%20in%20the%20spatiotemporal%20domain%20to%20generate%0Apixel-level%20motion%20supervision.%20Then%2C%20we%20design%20a%20motion%20decoder%20and%20a%0Aforeground%20sampling%20strategy%20to%20eliminate%20the%20weak%20alignments%20in%20terms%20of%20time%0Aand%20space.%20Moreover%2C%20a%20frame-level%20motion%20contrastive%20loss%20is%20presented%20to%0Aimprove%20the%20temporal%20diversity%20of%20the%20motion%20features.%20Extensive%20experiments%0Ademonstrate%20that%20the%20representations%20learned%20by%20FIMA%20possess%20great%0Amotion-awareness%20capabilities%20and%20achieve%20state-of-the-art%20or%20competitive%0Aresults%20on%20downstream%20tasks%20across%20UCF101%2C%20HMDB51%2C%20and%20Diving48%20datasets.%20Code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/ZMHH-H/FIMA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00297v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Spatiotemporal%2520Motion%2520Alignment%2520for%2520Contrastive%2520Video%250A%2520%2520Representation%2520Learning%26entry.906535625%3DMinghao%2520Zhu%2520and%2520Xiao%2520Lin%2520and%2520Ronghao%2520Dang%2520and%2520Chengju%2520Liu%2520and%2520Qijun%2520Chen%26entry.1292438233%3D%2520%2520As%2520the%2520most%2520essential%2520property%2520in%2520a%2520video%252C%2520motion%2520information%2520is%2520critical%2520to%250Aa%2520robust%2520and%2520generalized%2520video%2520representation.%2520To%2520inject%2520motion%2520dynamics%252C%250Arecent%2520works%2520have%2520adopted%2520frame%2520difference%2520as%2520the%2520source%2520of%2520motion%2520information%250Ain%2520video%2520contrastive%2520learning%252C%2520considering%2520the%2520trade-off%2520between%2520quality%2520and%250Acost.%2520However%252C%2520existing%2520works%2520align%2520motion%2520features%2520at%2520the%2520instance%2520level%252C%250Awhich%2520suffers%2520from%2520spatial%2520and%2520temporal%2520weak%2520alignment%2520across%2520modalities.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520%255Ctextbf%257BFi%257Dne-grained%2520%255Ctextbf%257BM%257Dotion%250A%255Ctextbf%257BA%257Dlignment%2520%2528FIMA%2529%2520framework%252C%2520capable%2520of%2520introducing%2520well-aligned%2520and%250Asignificant%2520motion%2520information.%2520Specifically%252C%2520we%2520first%2520develop%2520a%2520dense%250Acontrastive%2520learning%2520framework%2520in%2520the%2520spatiotemporal%2520domain%2520to%2520generate%250Apixel-level%2520motion%2520supervision.%2520Then%252C%2520we%2520design%2520a%2520motion%2520decoder%2520and%2520a%250Aforeground%2520sampling%2520strategy%2520to%2520eliminate%2520the%2520weak%2520alignments%2520in%2520terms%2520of%2520time%250Aand%2520space.%2520Moreover%252C%2520a%2520frame-level%2520motion%2520contrastive%2520loss%2520is%2520presented%2520to%250Aimprove%2520the%2520temporal%2520diversity%2520of%2520the%2520motion%2520features.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520the%2520representations%2520learned%2520by%2520FIMA%2520possess%2520great%250Amotion-awareness%2520capabilities%2520and%2520achieve%2520state-of-the-art%2520or%2520competitive%250Aresults%2520on%2520downstream%2520tasks%2520across%2520UCF101%252C%2520HMDB51%252C%2520and%2520Diving48%2520datasets.%2520Code%250Ais%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/ZMHH-H/FIMA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00297v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Spatiotemporal%20Motion%20Alignment%20for%20Contrastive%20Video%0A%20%20Representation%20Learning&entry.906535625=Minghao%20Zhu%20and%20Xiao%20Lin%20and%20Ronghao%20Dang%20and%20Chengju%20Liu%20and%20Qijun%20Chen&entry.1292438233=%20%20As%20the%20most%20essential%20property%20in%20a%20video%2C%20motion%20information%20is%20critical%20to%0Aa%20robust%20and%20generalized%20video%20representation.%20To%20inject%20motion%20dynamics%2C%0Arecent%20works%20have%20adopted%20frame%20difference%20as%20the%20source%20of%20motion%20information%0Ain%20video%20contrastive%20learning%2C%20considering%20the%20trade-off%20between%20quality%20and%0Acost.%20However%2C%20existing%20works%20align%20motion%20features%20at%20the%20instance%20level%2C%0Awhich%20suffers%20from%20spatial%20and%20temporal%20weak%20alignment%20across%20modalities.%20In%0Athis%20paper%2C%20we%20present%20a%20%5Ctextbf%7BFi%7Dne-grained%20%5Ctextbf%7BM%7Dotion%0A%5Ctextbf%7BA%7Dlignment%20%28FIMA%29%20framework%2C%20capable%20of%20introducing%20well-aligned%20and%0Asignificant%20motion%20information.%20Specifically%2C%20we%20first%20develop%20a%20dense%0Acontrastive%20learning%20framework%20in%20the%20spatiotemporal%20domain%20to%20generate%0Apixel-level%20motion%20supervision.%20Then%2C%20we%20design%20a%20motion%20decoder%20and%20a%0Aforeground%20sampling%20strategy%20to%20eliminate%20the%20weak%20alignments%20in%20terms%20of%20time%0Aand%20space.%20Moreover%2C%20a%20frame-level%20motion%20contrastive%20loss%20is%20presented%20to%0Aimprove%20the%20temporal%20diversity%20of%20the%20motion%20features.%20Extensive%20experiments%0Ademonstrate%20that%20the%20representations%20learned%20by%20FIMA%20possess%20great%0Amotion-awareness%20capabilities%20and%20achieve%20state-of-the-art%20or%20competitive%0Aresults%20on%20downstream%20tasks%20across%20UCF101%2C%20HMDB51%2C%20and%20Diving48%20datasets.%20Code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/ZMHH-H/FIMA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00297v2&entry.124074799=Read"},
{"title": "ImageFolder: Autoregressive Image Generation with Folded Tokens", "author": "Xiang Li and Kai Qiu and Hao Chen and Jason Kuen and Jiuxiang Gu and Bhiksha Raj and Zhe Lin", "abstract": "  Image tokenizers are crucial for visual generative models, e.g., diffusion\nmodels (DMs) and autoregressive (AR) models, as they construct the latent\nrepresentation for modeling. Increasing token length is a common approach to\nimprove the image reconstruction quality. However, tokenizers with longer token\nlengths are not guaranteed to achieve better generation quality. There exists a\ntrade-off between reconstruction and generation quality regarding token length.\nIn this paper, we investigate the impact of token length on both image\nreconstruction and generation and provide a flexible solution to the tradeoff.\nWe propose ImageFolder, a semantic tokenizer that provides spatially aligned\nimage tokens that can be folded during autoregressive modeling to improve both\ngeneration efficiency and quality. To enhance the representative capability\nwithout increasing token length, we leverage dual-branch product quantization\nto capture different contexts of images. Specifically, semantic regularization\nis introduced in one branch to encourage compacted semantic information while\nanother branch is designed to capture the remaining pixel-level details.\nExtensive experiments demonstrate the superior quality of image generation and\nshorter token length with ImageFolder tokenizer.\n", "link": "http://arxiv.org/abs/2410.01756v2", "date": "2024-10-15", "relevancy": 2.2617, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6369}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImageFolder%3A%20Autoregressive%20Image%20Generation%20with%20Folded%20Tokens&body=Title%3A%20ImageFolder%3A%20Autoregressive%20Image%20Generation%20with%20Folded%20Tokens%0AAuthor%3A%20Xiang%20Li%20and%20Kai%20Qiu%20and%20Hao%20Chen%20and%20Jason%20Kuen%20and%20Jiuxiang%20Gu%20and%20Bhiksha%20Raj%20and%20Zhe%20Lin%0AAbstract%3A%20%20%20Image%20tokenizers%20are%20crucial%20for%20visual%20generative%20models%2C%20e.g.%2C%20diffusion%0Amodels%20%28DMs%29%20and%20autoregressive%20%28AR%29%20models%2C%20as%20they%20construct%20the%20latent%0Arepresentation%20for%20modeling.%20Increasing%20token%20length%20is%20a%20common%20approach%20to%0Aimprove%20the%20image%20reconstruction%20quality.%20However%2C%20tokenizers%20with%20longer%20token%0Alengths%20are%20not%20guaranteed%20to%20achieve%20better%20generation%20quality.%20There%20exists%20a%0Atrade-off%20between%20reconstruction%20and%20generation%20quality%20regarding%20token%20length.%0AIn%20this%20paper%2C%20we%20investigate%20the%20impact%20of%20token%20length%20on%20both%20image%0Areconstruction%20and%20generation%20and%20provide%20a%20flexible%20solution%20to%20the%20tradeoff.%0AWe%20propose%20ImageFolder%2C%20a%20semantic%20tokenizer%20that%20provides%20spatially%20aligned%0Aimage%20tokens%20that%20can%20be%20folded%20during%20autoregressive%20modeling%20to%20improve%20both%0Ageneration%20efficiency%20and%20quality.%20To%20enhance%20the%20representative%20capability%0Awithout%20increasing%20token%20length%2C%20we%20leverage%20dual-branch%20product%20quantization%0Ato%20capture%20different%20contexts%20of%20images.%20Specifically%2C%20semantic%20regularization%0Ais%20introduced%20in%20one%20branch%20to%20encourage%20compacted%20semantic%20information%20while%0Aanother%20branch%20is%20designed%20to%20capture%20the%20remaining%20pixel-level%20details.%0AExtensive%20experiments%20demonstrate%20the%20superior%20quality%20of%20image%20generation%20and%0Ashorter%20token%20length%20with%20ImageFolder%20tokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImageFolder%253A%2520Autoregressive%2520Image%2520Generation%2520with%2520Folded%2520Tokens%26entry.906535625%3DXiang%2520Li%2520and%2520Kai%2520Qiu%2520and%2520Hao%2520Chen%2520and%2520Jason%2520Kuen%2520and%2520Jiuxiang%2520Gu%2520and%2520Bhiksha%2520Raj%2520and%2520Zhe%2520Lin%26entry.1292438233%3D%2520%2520Image%2520tokenizers%2520are%2520crucial%2520for%2520visual%2520generative%2520models%252C%2520e.g.%252C%2520diffusion%250Amodels%2520%2528DMs%2529%2520and%2520autoregressive%2520%2528AR%2529%2520models%252C%2520as%2520they%2520construct%2520the%2520latent%250Arepresentation%2520for%2520modeling.%2520Increasing%2520token%2520length%2520is%2520a%2520common%2520approach%2520to%250Aimprove%2520the%2520image%2520reconstruction%2520quality.%2520However%252C%2520tokenizers%2520with%2520longer%2520token%250Alengths%2520are%2520not%2520guaranteed%2520to%2520achieve%2520better%2520generation%2520quality.%2520There%2520exists%2520a%250Atrade-off%2520between%2520reconstruction%2520and%2520generation%2520quality%2520regarding%2520token%2520length.%250AIn%2520this%2520paper%252C%2520we%2520investigate%2520the%2520impact%2520of%2520token%2520length%2520on%2520both%2520image%250Areconstruction%2520and%2520generation%2520and%2520provide%2520a%2520flexible%2520solution%2520to%2520the%2520tradeoff.%250AWe%2520propose%2520ImageFolder%252C%2520a%2520semantic%2520tokenizer%2520that%2520provides%2520spatially%2520aligned%250Aimage%2520tokens%2520that%2520can%2520be%2520folded%2520during%2520autoregressive%2520modeling%2520to%2520improve%2520both%250Ageneration%2520efficiency%2520and%2520quality.%2520To%2520enhance%2520the%2520representative%2520capability%250Awithout%2520increasing%2520token%2520length%252C%2520we%2520leverage%2520dual-branch%2520product%2520quantization%250Ato%2520capture%2520different%2520contexts%2520of%2520images.%2520Specifically%252C%2520semantic%2520regularization%250Ais%2520introduced%2520in%2520one%2520branch%2520to%2520encourage%2520compacted%2520semantic%2520information%2520while%250Aanother%2520branch%2520is%2520designed%2520to%2520capture%2520the%2520remaining%2520pixel-level%2520details.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superior%2520quality%2520of%2520image%2520generation%2520and%250Ashorter%2520token%2520length%2520with%2520ImageFolder%2520tokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageFolder%3A%20Autoregressive%20Image%20Generation%20with%20Folded%20Tokens&entry.906535625=Xiang%20Li%20and%20Kai%20Qiu%20and%20Hao%20Chen%20and%20Jason%20Kuen%20and%20Jiuxiang%20Gu%20and%20Bhiksha%20Raj%20and%20Zhe%20Lin&entry.1292438233=%20%20Image%20tokenizers%20are%20crucial%20for%20visual%20generative%20models%2C%20e.g.%2C%20diffusion%0Amodels%20%28DMs%29%20and%20autoregressive%20%28AR%29%20models%2C%20as%20they%20construct%20the%20latent%0Arepresentation%20for%20modeling.%20Increasing%20token%20length%20is%20a%20common%20approach%20to%0Aimprove%20the%20image%20reconstruction%20quality.%20However%2C%20tokenizers%20with%20longer%20token%0Alengths%20are%20not%20guaranteed%20to%20achieve%20better%20generation%20quality.%20There%20exists%20a%0Atrade-off%20between%20reconstruction%20and%20generation%20quality%20regarding%20token%20length.%0AIn%20this%20paper%2C%20we%20investigate%20the%20impact%20of%20token%20length%20on%20both%20image%0Areconstruction%20and%20generation%20and%20provide%20a%20flexible%20solution%20to%20the%20tradeoff.%0AWe%20propose%20ImageFolder%2C%20a%20semantic%20tokenizer%20that%20provides%20spatially%20aligned%0Aimage%20tokens%20that%20can%20be%20folded%20during%20autoregressive%20modeling%20to%20improve%20both%0Ageneration%20efficiency%20and%20quality.%20To%20enhance%20the%20representative%20capability%0Awithout%20increasing%20token%20length%2C%20we%20leverage%20dual-branch%20product%20quantization%0Ato%20capture%20different%20contexts%20of%20images.%20Specifically%2C%20semantic%20regularization%0Ais%20introduced%20in%20one%20branch%20to%20encourage%20compacted%20semantic%20information%20while%0Aanother%20branch%20is%20designed%20to%20capture%20the%20remaining%20pixel-level%20details.%0AExtensive%20experiments%20demonstrate%20the%20superior%20quality%20of%20image%20generation%20and%0Ashorter%20token%20length%20with%20ImageFolder%20tokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01756v2&entry.124074799=Read"},
{"title": "Improving semantic understanding in speech language models via\n  brain-tuning", "author": "Omer Moussa and Dietrich Klakow and Mariya Toneva", "abstract": "  Speech language models align with human brain responses to natural language\nto an impressive degree. However, current models rely heavily on low-level\nspeech features, indicating they lack brain-relevant semantics which limits\ntheir utility as model organisms of semantic processing in the brain. In this\nwork, we address this limitation by inducing brain-relevant bias directly into\nthe models via fine-tuning with fMRI recordings of people listening to natural\nstories, a process we name brain-tuning. After testing it on 3 different\npretrained model families, we show that brain-tuning not only improves overall\nalignment with new brain recordings in semantic language regions, but also\nreduces the reliance on low-level speech features for this alignment.\nExcitingly, we further show that brain-tuning leads to 1) consistent\nimprovements in performance on a range of downstream tasks and 2) a\nrepresentational space with increased semantic preference. Our results provide\nconverging evidence, for the first time, that incorporating brain signals into\nthe training of language models improves the models' semantic understanding.\n", "link": "http://arxiv.org/abs/2410.09230v2", "date": "2024-10-15", "relevancy": 2.2519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20semantic%20understanding%20in%20speech%20language%20models%20via%0A%20%20brain-tuning&body=Title%3A%20Improving%20semantic%20understanding%20in%20speech%20language%20models%20via%0A%20%20brain-tuning%0AAuthor%3A%20Omer%20Moussa%20and%20Dietrich%20Klakow%20and%20Mariya%20Toneva%0AAbstract%3A%20%20%20Speech%20language%20models%20align%20with%20human%20brain%20responses%20to%20natural%20language%0Ato%20an%20impressive%20degree.%20However%2C%20current%20models%20rely%20heavily%20on%20low-level%0Aspeech%20features%2C%20indicating%20they%20lack%20brain-relevant%20semantics%20which%20limits%0Atheir%20utility%20as%20model%20organisms%20of%20semantic%20processing%20in%20the%20brain.%20In%20this%0Awork%2C%20we%20address%20this%20limitation%20by%20inducing%20brain-relevant%20bias%20directly%20into%0Athe%20models%20via%20fine-tuning%20with%20fMRI%20recordings%20of%20people%20listening%20to%20natural%0Astories%2C%20a%20process%20we%20name%20brain-tuning.%20After%20testing%20it%20on%203%20different%0Apretrained%20model%20families%2C%20we%20show%20that%20brain-tuning%20not%20only%20improves%20overall%0Aalignment%20with%20new%20brain%20recordings%20in%20semantic%20language%20regions%2C%20but%20also%0Areduces%20the%20reliance%20on%20low-level%20speech%20features%20for%20this%20alignment.%0AExcitingly%2C%20we%20further%20show%20that%20brain-tuning%20leads%20to%201%29%20consistent%0Aimprovements%20in%20performance%20on%20a%20range%20of%20downstream%20tasks%20and%202%29%20a%0Arepresentational%20space%20with%20increased%20semantic%20preference.%20Our%20results%20provide%0Aconverging%20evidence%2C%20for%20the%20first%20time%2C%20that%20incorporating%20brain%20signals%20into%0Athe%20training%20of%20language%20models%20improves%20the%20models%27%20semantic%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520semantic%2520understanding%2520in%2520speech%2520language%2520models%2520via%250A%2520%2520brain-tuning%26entry.906535625%3DOmer%2520Moussa%2520and%2520Dietrich%2520Klakow%2520and%2520Mariya%2520Toneva%26entry.1292438233%3D%2520%2520Speech%2520language%2520models%2520align%2520with%2520human%2520brain%2520responses%2520to%2520natural%2520language%250Ato%2520an%2520impressive%2520degree.%2520However%252C%2520current%2520models%2520rely%2520heavily%2520on%2520low-level%250Aspeech%2520features%252C%2520indicating%2520they%2520lack%2520brain-relevant%2520semantics%2520which%2520limits%250Atheir%2520utility%2520as%2520model%2520organisms%2520of%2520semantic%2520processing%2520in%2520the%2520brain.%2520In%2520this%250Awork%252C%2520we%2520address%2520this%2520limitation%2520by%2520inducing%2520brain-relevant%2520bias%2520directly%2520into%250Athe%2520models%2520via%2520fine-tuning%2520with%2520fMRI%2520recordings%2520of%2520people%2520listening%2520to%2520natural%250Astories%252C%2520a%2520process%2520we%2520name%2520brain-tuning.%2520After%2520testing%2520it%2520on%25203%2520different%250Apretrained%2520model%2520families%252C%2520we%2520show%2520that%2520brain-tuning%2520not%2520only%2520improves%2520overall%250Aalignment%2520with%2520new%2520brain%2520recordings%2520in%2520semantic%2520language%2520regions%252C%2520but%2520also%250Areduces%2520the%2520reliance%2520on%2520low-level%2520speech%2520features%2520for%2520this%2520alignment.%250AExcitingly%252C%2520we%2520further%2520show%2520that%2520brain-tuning%2520leads%2520to%25201%2529%2520consistent%250Aimprovements%2520in%2520performance%2520on%2520a%2520range%2520of%2520downstream%2520tasks%2520and%25202%2529%2520a%250Arepresentational%2520space%2520with%2520increased%2520semantic%2520preference.%2520Our%2520results%2520provide%250Aconverging%2520evidence%252C%2520for%2520the%2520first%2520time%252C%2520that%2520incorporating%2520brain%2520signals%2520into%250Athe%2520training%2520of%2520language%2520models%2520improves%2520the%2520models%2527%2520semantic%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20semantic%20understanding%20in%20speech%20language%20models%20via%0A%20%20brain-tuning&entry.906535625=Omer%20Moussa%20and%20Dietrich%20Klakow%20and%20Mariya%20Toneva&entry.1292438233=%20%20Speech%20language%20models%20align%20with%20human%20brain%20responses%20to%20natural%20language%0Ato%20an%20impressive%20degree.%20However%2C%20current%20models%20rely%20heavily%20on%20low-level%0Aspeech%20features%2C%20indicating%20they%20lack%20brain-relevant%20semantics%20which%20limits%0Atheir%20utility%20as%20model%20organisms%20of%20semantic%20processing%20in%20the%20brain.%20In%20this%0Awork%2C%20we%20address%20this%20limitation%20by%20inducing%20brain-relevant%20bias%20directly%20into%0Athe%20models%20via%20fine-tuning%20with%20fMRI%20recordings%20of%20people%20listening%20to%20natural%0Astories%2C%20a%20process%20we%20name%20brain-tuning.%20After%20testing%20it%20on%203%20different%0Apretrained%20model%20families%2C%20we%20show%20that%20brain-tuning%20not%20only%20improves%20overall%0Aalignment%20with%20new%20brain%20recordings%20in%20semantic%20language%20regions%2C%20but%20also%0Areduces%20the%20reliance%20on%20low-level%20speech%20features%20for%20this%20alignment.%0AExcitingly%2C%20we%20further%20show%20that%20brain-tuning%20leads%20to%201%29%20consistent%0Aimprovements%20in%20performance%20on%20a%20range%20of%20downstream%20tasks%20and%202%29%20a%0Arepresentational%20space%20with%20increased%20semantic%20preference.%20Our%20results%20provide%0Aconverging%20evidence%2C%20for%20the%20first%20time%2C%20that%20incorporating%20brain%20signals%20into%0Athe%20training%20of%20language%20models%20improves%20the%20models%27%20semantic%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09230v2&entry.124074799=Read"},
{"title": "Fast Local Neural Regression for Low-Cost, Path Traced Lambertian Global\n  Illumination", "author": "Arturo Salmi and Szabolcs Cs\u00e9falvay and James Imber", "abstract": "  Despite recent advances in hardware acceleration of ray tracing, real-time\nray budgets remain stubbornly limited at a handful of samples per pixel (spp)\non commodity hardware, placing the onus on denoising algorithms to achieve high\nvisual quality for path traced global illumination. Neural network-based\nsolutions give excellent result quality at the cost of increased execution time\nrelative to hand-engineered methods, making them less suitable for deployment\non resource-constrained systems. We therefore propose incorporating a neural\nnetwork into a computationally-efficient local linear model-based denoiser, and\ndemonstrate faithful single-frame reconstruction of global illumination for\nLambertian scenes at very low sample counts (1spp) and for low computational\ncost. Other contributions include improving the quality and performance of\nlocal linear model-based denoising through a simplified mathematical treatment,\nand demonstration of the surprising usefulness of ambient occlusion as a guide\nchannel. We also show how our technique is straightforwardly extensible to\njoint denoising and upsampling of path traced renders with reference to\nlow-cost, rasterized guide channels.\n", "link": "http://arxiv.org/abs/2410.11625v1", "date": "2024-10-15", "relevancy": 2.2516, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5865}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5482}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Local%20Neural%20Regression%20for%20Low-Cost%2C%20Path%20Traced%20Lambertian%20Global%0A%20%20Illumination&body=Title%3A%20Fast%20Local%20Neural%20Regression%20for%20Low-Cost%2C%20Path%20Traced%20Lambertian%20Global%0A%20%20Illumination%0AAuthor%3A%20Arturo%20Salmi%20and%20Szabolcs%20Cs%C3%A9falvay%20and%20James%20Imber%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20hardware%20acceleration%20of%20ray%20tracing%2C%20real-time%0Aray%20budgets%20remain%20stubbornly%20limited%20at%20a%20handful%20of%20samples%20per%20pixel%20%28spp%29%0Aon%20commodity%20hardware%2C%20placing%20the%20onus%20on%20denoising%20algorithms%20to%20achieve%20high%0Avisual%20quality%20for%20path%20traced%20global%20illumination.%20Neural%20network-based%0Asolutions%20give%20excellent%20result%20quality%20at%20the%20cost%20of%20increased%20execution%20time%0Arelative%20to%20hand-engineered%20methods%2C%20making%20them%20less%20suitable%20for%20deployment%0Aon%20resource-constrained%20systems.%20We%20therefore%20propose%20incorporating%20a%20neural%0Anetwork%20into%20a%20computationally-efficient%20local%20linear%20model-based%20denoiser%2C%20and%0Ademonstrate%20faithful%20single-frame%20reconstruction%20of%20global%20illumination%20for%0ALambertian%20scenes%20at%20very%20low%20sample%20counts%20%281spp%29%20and%20for%20low%20computational%0Acost.%20Other%20contributions%20include%20improving%20the%20quality%20and%20performance%20of%0Alocal%20linear%20model-based%20denoising%20through%20a%20simplified%20mathematical%20treatment%2C%0Aand%20demonstration%20of%20the%20surprising%20usefulness%20of%20ambient%20occlusion%20as%20a%20guide%0Achannel.%20We%20also%20show%20how%20our%20technique%20is%20straightforwardly%20extensible%20to%0Ajoint%20denoising%20and%20upsampling%20of%20path%20traced%20renders%20with%20reference%20to%0Alow-cost%2C%20rasterized%20guide%20channels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Local%2520Neural%2520Regression%2520for%2520Low-Cost%252C%2520Path%2520Traced%2520Lambertian%2520Global%250A%2520%2520Illumination%26entry.906535625%3DArturo%2520Salmi%2520and%2520Szabolcs%2520Cs%25C3%25A9falvay%2520and%2520James%2520Imber%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520hardware%2520acceleration%2520of%2520ray%2520tracing%252C%2520real-time%250Aray%2520budgets%2520remain%2520stubbornly%2520limited%2520at%2520a%2520handful%2520of%2520samples%2520per%2520pixel%2520%2528spp%2529%250Aon%2520commodity%2520hardware%252C%2520placing%2520the%2520onus%2520on%2520denoising%2520algorithms%2520to%2520achieve%2520high%250Avisual%2520quality%2520for%2520path%2520traced%2520global%2520illumination.%2520Neural%2520network-based%250Asolutions%2520give%2520excellent%2520result%2520quality%2520at%2520the%2520cost%2520of%2520increased%2520execution%2520time%250Arelative%2520to%2520hand-engineered%2520methods%252C%2520making%2520them%2520less%2520suitable%2520for%2520deployment%250Aon%2520resource-constrained%2520systems.%2520We%2520therefore%2520propose%2520incorporating%2520a%2520neural%250Anetwork%2520into%2520a%2520computationally-efficient%2520local%2520linear%2520model-based%2520denoiser%252C%2520and%250Ademonstrate%2520faithful%2520single-frame%2520reconstruction%2520of%2520global%2520illumination%2520for%250ALambertian%2520scenes%2520at%2520very%2520low%2520sample%2520counts%2520%25281spp%2529%2520and%2520for%2520low%2520computational%250Acost.%2520Other%2520contributions%2520include%2520improving%2520the%2520quality%2520and%2520performance%2520of%250Alocal%2520linear%2520model-based%2520denoising%2520through%2520a%2520simplified%2520mathematical%2520treatment%252C%250Aand%2520demonstration%2520of%2520the%2520surprising%2520usefulness%2520of%2520ambient%2520occlusion%2520as%2520a%2520guide%250Achannel.%2520We%2520also%2520show%2520how%2520our%2520technique%2520is%2520straightforwardly%2520extensible%2520to%250Ajoint%2520denoising%2520and%2520upsampling%2520of%2520path%2520traced%2520renders%2520with%2520reference%2520to%250Alow-cost%252C%2520rasterized%2520guide%2520channels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Local%20Neural%20Regression%20for%20Low-Cost%2C%20Path%20Traced%20Lambertian%20Global%0A%20%20Illumination&entry.906535625=Arturo%20Salmi%20and%20Szabolcs%20Cs%C3%A9falvay%20and%20James%20Imber&entry.1292438233=%20%20Despite%20recent%20advances%20in%20hardware%20acceleration%20of%20ray%20tracing%2C%20real-time%0Aray%20budgets%20remain%20stubbornly%20limited%20at%20a%20handful%20of%20samples%20per%20pixel%20%28spp%29%0Aon%20commodity%20hardware%2C%20placing%20the%20onus%20on%20denoising%20algorithms%20to%20achieve%20high%0Avisual%20quality%20for%20path%20traced%20global%20illumination.%20Neural%20network-based%0Asolutions%20give%20excellent%20result%20quality%20at%20the%20cost%20of%20increased%20execution%20time%0Arelative%20to%20hand-engineered%20methods%2C%20making%20them%20less%20suitable%20for%20deployment%0Aon%20resource-constrained%20systems.%20We%20therefore%20propose%20incorporating%20a%20neural%0Anetwork%20into%20a%20computationally-efficient%20local%20linear%20model-based%20denoiser%2C%20and%0Ademonstrate%20faithful%20single-frame%20reconstruction%20of%20global%20illumination%20for%0ALambertian%20scenes%20at%20very%20low%20sample%20counts%20%281spp%29%20and%20for%20low%20computational%0Acost.%20Other%20contributions%20include%20improving%20the%20quality%20and%20performance%20of%0Alocal%20linear%20model-based%20denoising%20through%20a%20simplified%20mathematical%20treatment%2C%0Aand%20demonstration%20of%20the%20surprising%20usefulness%20of%20ambient%20occlusion%20as%20a%20guide%0Achannel.%20We%20also%20show%20how%20our%20technique%20is%20straightforwardly%20extensible%20to%0Ajoint%20denoising%20and%20upsampling%20of%20path%20traced%20renders%20with%20reference%20to%0Alow-cost%2C%20rasterized%20guide%20channels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11625v1&entry.124074799=Read"},
{"title": "Overcoming Domain Limitations in Open-vocabulary Segmentation", "author": "Dongjun Hwang and Seong Joon Oh and Junsuk Choe", "abstract": "  Open-vocabulary segmentation (OVS) has gained attention for its ability to\nrecognize a broader range of classes. However, OVS models show significant\nperformance drops when applied to unseen domains beyond the previous training\ndataset. Fine-tuning these models on new datasets can improve performance, but\noften leads to the catastrophic forgetting of previously learned knowledge. To\naddress this issue, we propose a method that allows OVS models to learn\ninformation from new domains while preserving prior knowledge. Our approach\nbegins by evaluating the input sample's proximity to multiple domains, using\nprecomputed multivariate normal distributions for each domain. Based on this\nprediction, we dynamically interpolate between the weights of the pre-trained\ndecoder and the fine-tuned decoders. Extensive experiments demonstrate that\nthis approach allows OVS models to adapt to new domains while maintaining\nperformance on the previous training dataset. The source code is available at\nhttps://github.com/dongjunhwang/dwi.\n", "link": "http://arxiv.org/abs/2410.11536v1", "date": "2024-10-15", "relevancy": 2.2485, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Domain%20Limitations%20in%20Open-vocabulary%20Segmentation&body=Title%3A%20Overcoming%20Domain%20Limitations%20in%20Open-vocabulary%20Segmentation%0AAuthor%3A%20Dongjun%20Hwang%20and%20Seong%20Joon%20Oh%20and%20Junsuk%20Choe%0AAbstract%3A%20%20%20Open-vocabulary%20segmentation%20%28OVS%29%20has%20gained%20attention%20for%20its%20ability%20to%0Arecognize%20a%20broader%20range%20of%20classes.%20However%2C%20OVS%20models%20show%20significant%0Aperformance%20drops%20when%20applied%20to%20unseen%20domains%20beyond%20the%20previous%20training%0Adataset.%20Fine-tuning%20these%20models%20on%20new%20datasets%20can%20improve%20performance%2C%20but%0Aoften%20leads%20to%20the%20catastrophic%20forgetting%20of%20previously%20learned%20knowledge.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20method%20that%20allows%20OVS%20models%20to%20learn%0Ainformation%20from%20new%20domains%20while%20preserving%20prior%20knowledge.%20Our%20approach%0Abegins%20by%20evaluating%20the%20input%20sample%27s%20proximity%20to%20multiple%20domains%2C%20using%0Aprecomputed%20multivariate%20normal%20distributions%20for%20each%20domain.%20Based%20on%20this%0Aprediction%2C%20we%20dynamically%20interpolate%20between%20the%20weights%20of%20the%20pre-trained%0Adecoder%20and%20the%20fine-tuned%20decoders.%20Extensive%20experiments%20demonstrate%20that%0Athis%20approach%20allows%20OVS%20models%20to%20adapt%20to%20new%20domains%20while%20maintaining%0Aperformance%20on%20the%20previous%20training%20dataset.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/dongjunhwang/dwi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Domain%2520Limitations%2520in%2520Open-vocabulary%2520Segmentation%26entry.906535625%3DDongjun%2520Hwang%2520and%2520Seong%2520Joon%2520Oh%2520and%2520Junsuk%2520Choe%26entry.1292438233%3D%2520%2520Open-vocabulary%2520segmentation%2520%2528OVS%2529%2520has%2520gained%2520attention%2520for%2520its%2520ability%2520to%250Arecognize%2520a%2520broader%2520range%2520of%2520classes.%2520However%252C%2520OVS%2520models%2520show%2520significant%250Aperformance%2520drops%2520when%2520applied%2520to%2520unseen%2520domains%2520beyond%2520the%2520previous%2520training%250Adataset.%2520Fine-tuning%2520these%2520models%2520on%2520new%2520datasets%2520can%2520improve%2520performance%252C%2520but%250Aoften%2520leads%2520to%2520the%2520catastrophic%2520forgetting%2520of%2520previously%2520learned%2520knowledge.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520method%2520that%2520allows%2520OVS%2520models%2520to%2520learn%250Ainformation%2520from%2520new%2520domains%2520while%2520preserving%2520prior%2520knowledge.%2520Our%2520approach%250Abegins%2520by%2520evaluating%2520the%2520input%2520sample%2527s%2520proximity%2520to%2520multiple%2520domains%252C%2520using%250Aprecomputed%2520multivariate%2520normal%2520distributions%2520for%2520each%2520domain.%2520Based%2520on%2520this%250Aprediction%252C%2520we%2520dynamically%2520interpolate%2520between%2520the%2520weights%2520of%2520the%2520pre-trained%250Adecoder%2520and%2520the%2520fine-tuned%2520decoders.%2520Extensive%2520experiments%2520demonstrate%2520that%250Athis%2520approach%2520allows%2520OVS%2520models%2520to%2520adapt%2520to%2520new%2520domains%2520while%2520maintaining%250Aperformance%2520on%2520the%2520previous%2520training%2520dataset.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/dongjunhwang/dwi.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Domain%20Limitations%20in%20Open-vocabulary%20Segmentation&entry.906535625=Dongjun%20Hwang%20and%20Seong%20Joon%20Oh%20and%20Junsuk%20Choe&entry.1292438233=%20%20Open-vocabulary%20segmentation%20%28OVS%29%20has%20gained%20attention%20for%20its%20ability%20to%0Arecognize%20a%20broader%20range%20of%20classes.%20However%2C%20OVS%20models%20show%20significant%0Aperformance%20drops%20when%20applied%20to%20unseen%20domains%20beyond%20the%20previous%20training%0Adataset.%20Fine-tuning%20these%20models%20on%20new%20datasets%20can%20improve%20performance%2C%20but%0Aoften%20leads%20to%20the%20catastrophic%20forgetting%20of%20previously%20learned%20knowledge.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20method%20that%20allows%20OVS%20models%20to%20learn%0Ainformation%20from%20new%20domains%20while%20preserving%20prior%20knowledge.%20Our%20approach%0Abegins%20by%20evaluating%20the%20input%20sample%27s%20proximity%20to%20multiple%20domains%2C%20using%0Aprecomputed%20multivariate%20normal%20distributions%20for%20each%20domain.%20Based%20on%20this%0Aprediction%2C%20we%20dynamically%20interpolate%20between%20the%20weights%20of%20the%20pre-trained%0Adecoder%20and%20the%20fine-tuned%20decoders.%20Extensive%20experiments%20demonstrate%20that%0Athis%20approach%20allows%20OVS%20models%20to%20adapt%20to%20new%20domains%20while%20maintaining%0Aperformance%20on%20the%20previous%20training%20dataset.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/dongjunhwang/dwi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11536v1&entry.124074799=Read"},
{"title": "LeOCLR: Leveraging Original Images for Contrastive Learning of Visual\n  Representations", "author": "Mohammad Alkhalefi and Georgios Leontidis and Mingjun Zhong", "abstract": "  Contrastive instance discrimination methods outperform supervised learning in\ndownstream tasks such as image classification and object detection. However,\nthese methods rely heavily on data augmentation during representation learning,\nwhich can lead to suboptimal results if not implemented carefully. A common\naugmentation technique in contrastive learning is random cropping followed by\nresizing. This can degrade the quality of representation learning when the two\nrandom crops contain distinct semantic content. To tackle this issue, we\nintroduce LeOCLR (Leveraging Original Images for Contrastive Learning of Visual\nRepresentations), a framework that employs a novel instance discrimination\napproach and an adapted loss function. This method prevents the loss of\nimportant semantic features caused by mapping different object parts during\nrepresentation learning. Our experiments demonstrate that LeOCLR consistently\nimproves representation learning across various datasets, outperforming\nbaseline models. For instance, LeOCLR surpasses MoCo-v2 by 5.1% on ImageNet-1K\nin linear evaluation and outperforms several other methods on transfer learning\nand object detection tasks.\n", "link": "http://arxiv.org/abs/2403.06813v3", "date": "2024-10-15", "relevancy": 2.2167, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations&body=Title%3A%20LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations%0AAuthor%3A%20Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong%0AAbstract%3A%20%20%20Contrastive%20instance%20discrimination%20methods%20outperform%20supervised%20learning%20in%0Adownstream%20tasks%20such%20as%20image%20classification%20and%20object%20detection.%20However%2C%0Athese%20methods%20rely%20heavily%20on%20data%20augmentation%20during%20representation%20learning%2C%0Awhich%20can%20lead%20to%20suboptimal%20results%20if%20not%20implemented%20carefully.%20A%20common%0Aaugmentation%20technique%20in%20contrastive%20learning%20is%20random%20cropping%20followed%20by%0Aresizing.%20This%20can%20degrade%20the%20quality%20of%20representation%20learning%20when%20the%20two%0Arandom%20crops%20contain%20distinct%20semantic%20content.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20LeOCLR%20%28Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0ARepresentations%29%2C%20a%20framework%20that%20employs%20a%20novel%20instance%20discrimination%0Aapproach%20and%20an%20adapted%20loss%20function.%20This%20method%20prevents%20the%20loss%20of%0Aimportant%20semantic%20features%20caused%20by%20mapping%20different%20object%20parts%20during%0Arepresentation%20learning.%20Our%20experiments%20demonstrate%20that%20LeOCLR%20consistently%0Aimproves%20representation%20learning%20across%20various%20datasets%2C%20outperforming%0Abaseline%20models.%20For%20instance%2C%20LeOCLR%20surpasses%20MoCo-v2%20by%205.1%25%20on%20ImageNet-1K%0Ain%20linear%20evaluation%20and%20outperforms%20several%20other%20methods%20on%20transfer%20learning%0Aand%20object%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06813v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeOCLR%253A%2520Leveraging%2520Original%2520Images%2520for%2520Contrastive%2520Learning%2520of%2520Visual%250A%2520%2520Representations%26entry.906535625%3DMohammad%2520Alkhalefi%2520and%2520Georgios%2520Leontidis%2520and%2520Mingjun%2520Zhong%26entry.1292438233%3D%2520%2520Contrastive%2520instance%2520discrimination%2520methods%2520outperform%2520supervised%2520learning%2520in%250Adownstream%2520tasks%2520such%2520as%2520image%2520classification%2520and%2520object%2520detection.%2520However%252C%250Athese%2520methods%2520rely%2520heavily%2520on%2520data%2520augmentation%2520during%2520representation%2520learning%252C%250Awhich%2520can%2520lead%2520to%2520suboptimal%2520results%2520if%2520not%2520implemented%2520carefully.%2520A%2520common%250Aaugmentation%2520technique%2520in%2520contrastive%2520learning%2520is%2520random%2520cropping%2520followed%2520by%250Aresizing.%2520This%2520can%2520degrade%2520the%2520quality%2520of%2520representation%2520learning%2520when%2520the%2520two%250Arandom%2520crops%2520contain%2520distinct%2520semantic%2520content.%2520To%2520tackle%2520this%2520issue%252C%2520we%250Aintroduce%2520LeOCLR%2520%2528Leveraging%2520Original%2520Images%2520for%2520Contrastive%2520Learning%2520of%2520Visual%250ARepresentations%2529%252C%2520a%2520framework%2520that%2520employs%2520a%2520novel%2520instance%2520discrimination%250Aapproach%2520and%2520an%2520adapted%2520loss%2520function.%2520This%2520method%2520prevents%2520the%2520loss%2520of%250Aimportant%2520semantic%2520features%2520caused%2520by%2520mapping%2520different%2520object%2520parts%2520during%250Arepresentation%2520learning.%2520Our%2520experiments%2520demonstrate%2520that%2520LeOCLR%2520consistently%250Aimproves%2520representation%2520learning%2520across%2520various%2520datasets%252C%2520outperforming%250Abaseline%2520models.%2520For%2520instance%252C%2520LeOCLR%2520surpasses%2520MoCo-v2%2520by%25205.1%2525%2520on%2520ImageNet-1K%250Ain%2520linear%2520evaluation%2520and%2520outperforms%2520several%2520other%2520methods%2520on%2520transfer%2520learning%250Aand%2520object%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06813v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations&entry.906535625=Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong&entry.1292438233=%20%20Contrastive%20instance%20discrimination%20methods%20outperform%20supervised%20learning%20in%0Adownstream%20tasks%20such%20as%20image%20classification%20and%20object%20detection.%20However%2C%0Athese%20methods%20rely%20heavily%20on%20data%20augmentation%20during%20representation%20learning%2C%0Awhich%20can%20lead%20to%20suboptimal%20results%20if%20not%20implemented%20carefully.%20A%20common%0Aaugmentation%20technique%20in%20contrastive%20learning%20is%20random%20cropping%20followed%20by%0Aresizing.%20This%20can%20degrade%20the%20quality%20of%20representation%20learning%20when%20the%20two%0Arandom%20crops%20contain%20distinct%20semantic%20content.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20LeOCLR%20%28Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0ARepresentations%29%2C%20a%20framework%20that%20employs%20a%20novel%20instance%20discrimination%0Aapproach%20and%20an%20adapted%20loss%20function.%20This%20method%20prevents%20the%20loss%20of%0Aimportant%20semantic%20features%20caused%20by%20mapping%20different%20object%20parts%20during%0Arepresentation%20learning.%20Our%20experiments%20demonstrate%20that%20LeOCLR%20consistently%0Aimproves%20representation%20learning%20across%20various%20datasets%2C%20outperforming%0Abaseline%20models.%20For%20instance%2C%20LeOCLR%20surpasses%20MoCo-v2%20by%205.1%25%20on%20ImageNet-1K%0Ain%20linear%20evaluation%20and%20outperforms%20several%20other%20methods%20on%20transfer%20learning%0Aand%20object%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06813v3&entry.124074799=Read"},
{"title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models", "author": "Mu Cai and Reuben Tan and Jianrui Zhang and Bocheng Zou and Kai Zhang and Feng Yao and Fangrui Zhu and Jing Gu and Yiwu Zhong and Yuzhang Shang and Yao Dou and Jaden Park and Jianfeng Gao and Yong Jae Lee and Jianwei Yang", "abstract": "  Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.\n", "link": "http://arxiv.org/abs/2410.10818v2", "date": "2024-10-15", "relevancy": 2.2058, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5723}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TemporalBench%3A%20Benchmarking%20Fine-grained%20Temporal%20Understanding%20for%0A%20%20Multimodal%20Video%20Models&body=Title%3A%20TemporalBench%3A%20Benchmarking%20Fine-grained%20Temporal%20Understanding%20for%0A%20%20Multimodal%20Video%20Models%0AAuthor%3A%20Mu%20Cai%20and%20Reuben%20Tan%20and%20Jianrui%20Zhang%20and%20Bocheng%20Zou%20and%20Kai%20Zhang%20and%20Feng%20Yao%20and%20Fangrui%20Zhu%20and%20Jing%20Gu%20and%20Yiwu%20Zhong%20and%20Yuzhang%20Shang%20and%20Yao%20Dou%20and%20Jaden%20Park%20and%20Jianfeng%20Gao%20and%20Yong%20Jae%20Lee%20and%20Jianwei%20Yang%0AAbstract%3A%20%20%20Understanding%20fine-grained%20temporal%20dynamics%20is%20crucial%20for%20multimodal%20video%0Acomprehension%20and%20generation.%20Due%20to%20the%20lack%20of%20fine-grained%20temporal%0Aannotations%2C%20existing%20video%20benchmarks%20mostly%20resemble%20static%20image%20benchmarks%0Aand%20are%20incompetent%20at%20evaluating%20models%20for%20temporal%20understanding.%20In%20this%0Apaper%2C%20we%20introduce%20TemporalBench%2C%20a%20new%20benchmark%20dedicated%20to%20evaluating%0Afine-grained%20temporal%20understanding%20in%20videos.%20TemporalBench%20consists%20of%20~10K%0Avideo%20question-answer%20pairs%2C%20derived%20from%20~2K%20high-quality%20human%20annotations%0Adetailing%20the%20temporal%20dynamics%20in%20video%20clips.%20As%20a%20result%2C%20our%20benchmark%0Aprovides%20a%20unique%20testbed%20for%20evaluating%20various%20temporal%20understanding%20and%0Areasoning%20abilities%20such%20as%20action%20frequency%2C%20motion%20magnitude%2C%20event%20order%2C%0Aetc.%20Moreover%2C%20it%20enables%20evaluations%20on%20various%20tasks%20like%20both%20video%20question%0Aanswering%20and%20captioning%2C%20both%20short%20and%20long%20video%20understanding%2C%20as%20well%20as%0Adifferent%20models%20such%20as%20multimodal%20video%20embedding%20models%20and%20text%20generation%0Amodels.%20Results%20show%20that%20state-of-the-art%20models%20like%20GPT-4o%20achieve%20only%0A38.5%25%20question%20answering%20accuracy%20on%20TemporalBench%2C%20demonstrating%20a%20significant%0Agap%20%28~30%25%29%20between%20humans%20and%20AI%20in%20temporal%20understanding.%20Furthermore%2C%20we%0Anotice%20a%20critical%20pitfall%20for%20multi-choice%20QA%20where%20LLMs%20can%20detect%20the%20subtle%0Achanges%20in%20negative%20captions%20and%20find%20a%20centralized%20description%20as%20a%20cue%20for%0Aits%20prediction%2C%20where%20we%20propose%20Multiple%20Binary%20Accuracy%20%28MBA%29%20to%20correct%20such%0Abias.%20We%20hope%20that%20TemporalBench%20can%20foster%20research%20on%20improving%20models%27%0Atemporal%20reasoning%20capabilities.%20Both%20dataset%20and%20evaluation%20code%20will%20be%20made%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10818v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporalBench%253A%2520Benchmarking%2520Fine-grained%2520Temporal%2520Understanding%2520for%250A%2520%2520Multimodal%2520Video%2520Models%26entry.906535625%3DMu%2520Cai%2520and%2520Reuben%2520Tan%2520and%2520Jianrui%2520Zhang%2520and%2520Bocheng%2520Zou%2520and%2520Kai%2520Zhang%2520and%2520Feng%2520Yao%2520and%2520Fangrui%2520Zhu%2520and%2520Jing%2520Gu%2520and%2520Yiwu%2520Zhong%2520and%2520Yuzhang%2520Shang%2520and%2520Yao%2520Dou%2520and%2520Jaden%2520Park%2520and%2520Jianfeng%2520Gao%2520and%2520Yong%2520Jae%2520Lee%2520and%2520Jianwei%2520Yang%26entry.1292438233%3D%2520%2520Understanding%2520fine-grained%2520temporal%2520dynamics%2520is%2520crucial%2520for%2520multimodal%2520video%250Acomprehension%2520and%2520generation.%2520Due%2520to%2520the%2520lack%2520of%2520fine-grained%2520temporal%250Aannotations%252C%2520existing%2520video%2520benchmarks%2520mostly%2520resemble%2520static%2520image%2520benchmarks%250Aand%2520are%2520incompetent%2520at%2520evaluating%2520models%2520for%2520temporal%2520understanding.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520TemporalBench%252C%2520a%2520new%2520benchmark%2520dedicated%2520to%2520evaluating%250Afine-grained%2520temporal%2520understanding%2520in%2520videos.%2520TemporalBench%2520consists%2520of%2520~10K%250Avideo%2520question-answer%2520pairs%252C%2520derived%2520from%2520~2K%2520high-quality%2520human%2520annotations%250Adetailing%2520the%2520temporal%2520dynamics%2520in%2520video%2520clips.%2520As%2520a%2520result%252C%2520our%2520benchmark%250Aprovides%2520a%2520unique%2520testbed%2520for%2520evaluating%2520various%2520temporal%2520understanding%2520and%250Areasoning%2520abilities%2520such%2520as%2520action%2520frequency%252C%2520motion%2520magnitude%252C%2520event%2520order%252C%250Aetc.%2520Moreover%252C%2520it%2520enables%2520evaluations%2520on%2520various%2520tasks%2520like%2520both%2520video%2520question%250Aanswering%2520and%2520captioning%252C%2520both%2520short%2520and%2520long%2520video%2520understanding%252C%2520as%2520well%2520as%250Adifferent%2520models%2520such%2520as%2520multimodal%2520video%2520embedding%2520models%2520and%2520text%2520generation%250Amodels.%2520Results%2520show%2520that%2520state-of-the-art%2520models%2520like%2520GPT-4o%2520achieve%2520only%250A38.5%2525%2520question%2520answering%2520accuracy%2520on%2520TemporalBench%252C%2520demonstrating%2520a%2520significant%250Agap%2520%2528~30%2525%2529%2520between%2520humans%2520and%2520AI%2520in%2520temporal%2520understanding.%2520Furthermore%252C%2520we%250Anotice%2520a%2520critical%2520pitfall%2520for%2520multi-choice%2520QA%2520where%2520LLMs%2520can%2520detect%2520the%2520subtle%250Achanges%2520in%2520negative%2520captions%2520and%2520find%2520a%2520centralized%2520description%2520as%2520a%2520cue%2520for%250Aits%2520prediction%252C%2520where%2520we%2520propose%2520Multiple%2520Binary%2520Accuracy%2520%2528MBA%2529%2520to%2520correct%2520such%250Abias.%2520We%2520hope%2520that%2520TemporalBench%2520can%2520foster%2520research%2520on%2520improving%2520models%2527%250Atemporal%2520reasoning%2520capabilities.%2520Both%2520dataset%2520and%2520evaluation%2520code%2520will%2520be%2520made%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10818v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TemporalBench%3A%20Benchmarking%20Fine-grained%20Temporal%20Understanding%20for%0A%20%20Multimodal%20Video%20Models&entry.906535625=Mu%20Cai%20and%20Reuben%20Tan%20and%20Jianrui%20Zhang%20and%20Bocheng%20Zou%20and%20Kai%20Zhang%20and%20Feng%20Yao%20and%20Fangrui%20Zhu%20and%20Jing%20Gu%20and%20Yiwu%20Zhong%20and%20Yuzhang%20Shang%20and%20Yao%20Dou%20and%20Jaden%20Park%20and%20Jianfeng%20Gao%20and%20Yong%20Jae%20Lee%20and%20Jianwei%20Yang&entry.1292438233=%20%20Understanding%20fine-grained%20temporal%20dynamics%20is%20crucial%20for%20multimodal%20video%0Acomprehension%20and%20generation.%20Due%20to%20the%20lack%20of%20fine-grained%20temporal%0Aannotations%2C%20existing%20video%20benchmarks%20mostly%20resemble%20static%20image%20benchmarks%0Aand%20are%20incompetent%20at%20evaluating%20models%20for%20temporal%20understanding.%20In%20this%0Apaper%2C%20we%20introduce%20TemporalBench%2C%20a%20new%20benchmark%20dedicated%20to%20evaluating%0Afine-grained%20temporal%20understanding%20in%20videos.%20TemporalBench%20consists%20of%20~10K%0Avideo%20question-answer%20pairs%2C%20derived%20from%20~2K%20high-quality%20human%20annotations%0Adetailing%20the%20temporal%20dynamics%20in%20video%20clips.%20As%20a%20result%2C%20our%20benchmark%0Aprovides%20a%20unique%20testbed%20for%20evaluating%20various%20temporal%20understanding%20and%0Areasoning%20abilities%20such%20as%20action%20frequency%2C%20motion%20magnitude%2C%20event%20order%2C%0Aetc.%20Moreover%2C%20it%20enables%20evaluations%20on%20various%20tasks%20like%20both%20video%20question%0Aanswering%20and%20captioning%2C%20both%20short%20and%20long%20video%20understanding%2C%20as%20well%20as%0Adifferent%20models%20such%20as%20multimodal%20video%20embedding%20models%20and%20text%20generation%0Amodels.%20Results%20show%20that%20state-of-the-art%20models%20like%20GPT-4o%20achieve%20only%0A38.5%25%20question%20answering%20accuracy%20on%20TemporalBench%2C%20demonstrating%20a%20significant%0Agap%20%28~30%25%29%20between%20humans%20and%20AI%20in%20temporal%20understanding.%20Furthermore%2C%20we%0Anotice%20a%20critical%20pitfall%20for%20multi-choice%20QA%20where%20LLMs%20can%20detect%20the%20subtle%0Achanges%20in%20negative%20captions%20and%20find%20a%20centralized%20description%20as%20a%20cue%20for%0Aits%20prediction%2C%20where%20we%20propose%20Multiple%20Binary%20Accuracy%20%28MBA%29%20to%20correct%20such%0Abias.%20We%20hope%20that%20TemporalBench%20can%20foster%20research%20on%20improving%20models%27%0Atemporal%20reasoning%20capabilities.%20Both%20dataset%20and%20evaluation%20code%20will%20be%20made%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10818v2&entry.124074799=Read"},
{"title": "It's Just Another Day: Unique Video Captioning by Discriminative\n  Prompting", "author": "Toby Perrett and Tengda Han and Dima Damen and Andrew Zisserman", "abstract": "  Long videos contain many repeating actions, events and shots. These\nrepetitions are frequently given identical captions, which makes it difficult\nto retrieve the exact desired clip using a text search. In this paper, we\nformulate the problem of unique captioning: Given multiple clips with the same\ncaption, we generate a new caption for each clip that uniquely identifies it.\nWe propose Captioning by Discriminative Prompting (CDP), which predicts a\nproperty that can separate identically captioned clips, and use it to generate\nunique captions. We introduce two benchmarks for unique captioning, based on\negocentric footage and timeloop movies - where repeating actions are common. We\ndemonstrate that captions generated by CDP improve text-to-video R@1 by 15% for\negocentric videos and 10% in timeloop movies.\n", "link": "http://arxiv.org/abs/2410.11702v1", "date": "2024-10-15", "relevancy": 2.2037, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5989}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20It%27s%20Just%20Another%20Day%3A%20Unique%20Video%20Captioning%20by%20Discriminative%0A%20%20Prompting&body=Title%3A%20It%27s%20Just%20Another%20Day%3A%20Unique%20Video%20Captioning%20by%20Discriminative%0A%20%20Prompting%0AAuthor%3A%20Toby%20Perrett%20and%20Tengda%20Han%20and%20Dima%20Damen%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Long%20videos%20contain%20many%20repeating%20actions%2C%20events%20and%20shots.%20These%0Arepetitions%20are%20frequently%20given%20identical%20captions%2C%20which%20makes%20it%20difficult%0Ato%20retrieve%20the%20exact%20desired%20clip%20using%20a%20text%20search.%20In%20this%20paper%2C%20we%0Aformulate%20the%20problem%20of%20unique%20captioning%3A%20Given%20multiple%20clips%20with%20the%20same%0Acaption%2C%20we%20generate%20a%20new%20caption%20for%20each%20clip%20that%20uniquely%20identifies%20it.%0AWe%20propose%20Captioning%20by%20Discriminative%20Prompting%20%28CDP%29%2C%20which%20predicts%20a%0Aproperty%20that%20can%20separate%20identically%20captioned%20clips%2C%20and%20use%20it%20to%20generate%0Aunique%20captions.%20We%20introduce%20two%20benchmarks%20for%20unique%20captioning%2C%20based%20on%0Aegocentric%20footage%20and%20timeloop%20movies%20-%20where%20repeating%20actions%20are%20common.%20We%0Ademonstrate%20that%20captions%20generated%20by%20CDP%20improve%20text-to-video%20R%401%20by%2015%25%20for%0Aegocentric%20videos%20and%2010%25%20in%20timeloop%20movies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIt%2527s%2520Just%2520Another%2520Day%253A%2520Unique%2520Video%2520Captioning%2520by%2520Discriminative%250A%2520%2520Prompting%26entry.906535625%3DToby%2520Perrett%2520and%2520Tengda%2520Han%2520and%2520Dima%2520Damen%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Long%2520videos%2520contain%2520many%2520repeating%2520actions%252C%2520events%2520and%2520shots.%2520These%250Arepetitions%2520are%2520frequently%2520given%2520identical%2520captions%252C%2520which%2520makes%2520it%2520difficult%250Ato%2520retrieve%2520the%2520exact%2520desired%2520clip%2520using%2520a%2520text%2520search.%2520In%2520this%2520paper%252C%2520we%250Aformulate%2520the%2520problem%2520of%2520unique%2520captioning%253A%2520Given%2520multiple%2520clips%2520with%2520the%2520same%250Acaption%252C%2520we%2520generate%2520a%2520new%2520caption%2520for%2520each%2520clip%2520that%2520uniquely%2520identifies%2520it.%250AWe%2520propose%2520Captioning%2520by%2520Discriminative%2520Prompting%2520%2528CDP%2529%252C%2520which%2520predicts%2520a%250Aproperty%2520that%2520can%2520separate%2520identically%2520captioned%2520clips%252C%2520and%2520use%2520it%2520to%2520generate%250Aunique%2520captions.%2520We%2520introduce%2520two%2520benchmarks%2520for%2520unique%2520captioning%252C%2520based%2520on%250Aegocentric%2520footage%2520and%2520timeloop%2520movies%2520-%2520where%2520repeating%2520actions%2520are%2520common.%2520We%250Ademonstrate%2520that%2520captions%2520generated%2520by%2520CDP%2520improve%2520text-to-video%2520R%25401%2520by%252015%2525%2520for%250Aegocentric%2520videos%2520and%252010%2525%2520in%2520timeloop%2520movies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=It%27s%20Just%20Another%20Day%3A%20Unique%20Video%20Captioning%20by%20Discriminative%0A%20%20Prompting&entry.906535625=Toby%20Perrett%20and%20Tengda%20Han%20and%20Dima%20Damen%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Long%20videos%20contain%20many%20repeating%20actions%2C%20events%20and%20shots.%20These%0Arepetitions%20are%20frequently%20given%20identical%20captions%2C%20which%20makes%20it%20difficult%0Ato%20retrieve%20the%20exact%20desired%20clip%20using%20a%20text%20search.%20In%20this%20paper%2C%20we%0Aformulate%20the%20problem%20of%20unique%20captioning%3A%20Given%20multiple%20clips%20with%20the%20same%0Acaption%2C%20we%20generate%20a%20new%20caption%20for%20each%20clip%20that%20uniquely%20identifies%20it.%0AWe%20propose%20Captioning%20by%20Discriminative%20Prompting%20%28CDP%29%2C%20which%20predicts%20a%0Aproperty%20that%20can%20separate%20identically%20captioned%20clips%2C%20and%20use%20it%20to%20generate%0Aunique%20captions.%20We%20introduce%20two%20benchmarks%20for%20unique%20captioning%2C%20based%20on%0Aegocentric%20footage%20and%20timeloop%20movies%20-%20where%20repeating%20actions%20are%20common.%20We%0Ademonstrate%20that%20captions%20generated%20by%20CDP%20improve%20text-to-video%20R%401%20by%2015%25%20for%0Aegocentric%20videos%20and%2010%25%20in%20timeloop%20movies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11702v1&entry.124074799=Read"},
{"title": "Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution", "author": "Hongyu An and Xinfeng Zhang and Li Zhang and Ruiqin Xiong", "abstract": "  Omnidirectional video (ODV) can provide an immersive experience and is widely\nutilized in the field of virtual reality and augmented reality. However, the\nrestricted capturing devices and transmission bandwidth lead to the low\nresolution of ODVs. Video super-resolution (VSR) methods are proposed to\nenhance the resolution of videos, but ODV projection distortions in the\napplication are not well addressed directly applying such methods. To achieve\nbetter super-resolution reconstruction quality, we propose a novel\nSpatio-Temporal Distortion Aware Network (STDAN) oriented to ODV\ncharacteristics. Specifically, a spatio-temporal distortion modulation module\nis introduced to improve spatial ODV projection distortions and exploit the\ntemporal correlation according to intra and inter alignments. Next, we design a\nmulti-frame reconstruction and fusion mechanism to refine the consistency of\nreconstructed ODV frames. Furthermore, we incorporate latitude-saliency\nadaptive maps in the loss function to concentrate on important viewpoint\nregions with higher texture complexity and human-watching interest. In\naddition, we collect a new ODV-SR dataset with various scenarios. Extensive\nexperimental results demonstrate that the proposed STDAN achieves superior\nsuper-resolution performance on ODVs and outperforms state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2410.11506v1", "date": "2024-10-15", "relevancy": 2.1956, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5608}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5424}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Distortion%20Aware%20Omnidirectional%20Video%20Super-Resolution&body=Title%3A%20Spatio-Temporal%20Distortion%20Aware%20Omnidirectional%20Video%20Super-Resolution%0AAuthor%3A%20Hongyu%20An%20and%20Xinfeng%20Zhang%20and%20Li%20Zhang%20and%20Ruiqin%20Xiong%0AAbstract%3A%20%20%20Omnidirectional%20video%20%28ODV%29%20can%20provide%20an%20immersive%20experience%20and%20is%20widely%0Autilized%20in%20the%20field%20of%20virtual%20reality%20and%20augmented%20reality.%20However%2C%20the%0Arestricted%20capturing%20devices%20and%20transmission%20bandwidth%20lead%20to%20the%20low%0Aresolution%20of%20ODVs.%20Video%20super-resolution%20%28VSR%29%20methods%20are%20proposed%20to%0Aenhance%20the%20resolution%20of%20videos%2C%20but%20ODV%20projection%20distortions%20in%20the%0Aapplication%20are%20not%20well%20addressed%20directly%20applying%20such%20methods.%20To%20achieve%0Abetter%20super-resolution%20reconstruction%20quality%2C%20we%20propose%20a%20novel%0ASpatio-Temporal%20Distortion%20Aware%20Network%20%28STDAN%29%20oriented%20to%20ODV%0Acharacteristics.%20Specifically%2C%20a%20spatio-temporal%20distortion%20modulation%20module%0Ais%20introduced%20to%20improve%20spatial%20ODV%20projection%20distortions%20and%20exploit%20the%0Atemporal%20correlation%20according%20to%20intra%20and%20inter%20alignments.%20Next%2C%20we%20design%20a%0Amulti-frame%20reconstruction%20and%20fusion%20mechanism%20to%20refine%20the%20consistency%20of%0Areconstructed%20ODV%20frames.%20Furthermore%2C%20we%20incorporate%20latitude-saliency%0Aadaptive%20maps%20in%20the%20loss%20function%20to%20concentrate%20on%20important%20viewpoint%0Aregions%20with%20higher%20texture%20complexity%20and%20human-watching%20interest.%20In%0Aaddition%2C%20we%20collect%20a%20new%20ODV-SR%20dataset%20with%20various%20scenarios.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20STDAN%20achieves%20superior%0Asuper-resolution%20performance%20on%20ODVs%20and%20outperforms%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520Distortion%2520Aware%2520Omnidirectional%2520Video%2520Super-Resolution%26entry.906535625%3DHongyu%2520An%2520and%2520Xinfeng%2520Zhang%2520and%2520Li%2520Zhang%2520and%2520Ruiqin%2520Xiong%26entry.1292438233%3D%2520%2520Omnidirectional%2520video%2520%2528ODV%2529%2520can%2520provide%2520an%2520immersive%2520experience%2520and%2520is%2520widely%250Autilized%2520in%2520the%2520field%2520of%2520virtual%2520reality%2520and%2520augmented%2520reality.%2520However%252C%2520the%250Arestricted%2520capturing%2520devices%2520and%2520transmission%2520bandwidth%2520lead%2520to%2520the%2520low%250Aresolution%2520of%2520ODVs.%2520Video%2520super-resolution%2520%2528VSR%2529%2520methods%2520are%2520proposed%2520to%250Aenhance%2520the%2520resolution%2520of%2520videos%252C%2520but%2520ODV%2520projection%2520distortions%2520in%2520the%250Aapplication%2520are%2520not%2520well%2520addressed%2520directly%2520applying%2520such%2520methods.%2520To%2520achieve%250Abetter%2520super-resolution%2520reconstruction%2520quality%252C%2520we%2520propose%2520a%2520novel%250ASpatio-Temporal%2520Distortion%2520Aware%2520Network%2520%2528STDAN%2529%2520oriented%2520to%2520ODV%250Acharacteristics.%2520Specifically%252C%2520a%2520spatio-temporal%2520distortion%2520modulation%2520module%250Ais%2520introduced%2520to%2520improve%2520spatial%2520ODV%2520projection%2520distortions%2520and%2520exploit%2520the%250Atemporal%2520correlation%2520according%2520to%2520intra%2520and%2520inter%2520alignments.%2520Next%252C%2520we%2520design%2520a%250Amulti-frame%2520reconstruction%2520and%2520fusion%2520mechanism%2520to%2520refine%2520the%2520consistency%2520of%250Areconstructed%2520ODV%2520frames.%2520Furthermore%252C%2520we%2520incorporate%2520latitude-saliency%250Aadaptive%2520maps%2520in%2520the%2520loss%2520function%2520to%2520concentrate%2520on%2520important%2520viewpoint%250Aregions%2520with%2520higher%2520texture%2520complexity%2520and%2520human-watching%2520interest.%2520In%250Aaddition%252C%2520we%2520collect%2520a%2520new%2520ODV-SR%2520dataset%2520with%2520various%2520scenarios.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520STDAN%2520achieves%2520superior%250Asuper-resolution%2520performance%2520on%2520ODVs%2520and%2520outperforms%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Distortion%20Aware%20Omnidirectional%20Video%20Super-Resolution&entry.906535625=Hongyu%20An%20and%20Xinfeng%20Zhang%20and%20Li%20Zhang%20and%20Ruiqin%20Xiong&entry.1292438233=%20%20Omnidirectional%20video%20%28ODV%29%20can%20provide%20an%20immersive%20experience%20and%20is%20widely%0Autilized%20in%20the%20field%20of%20virtual%20reality%20and%20augmented%20reality.%20However%2C%20the%0Arestricted%20capturing%20devices%20and%20transmission%20bandwidth%20lead%20to%20the%20low%0Aresolution%20of%20ODVs.%20Video%20super-resolution%20%28VSR%29%20methods%20are%20proposed%20to%0Aenhance%20the%20resolution%20of%20videos%2C%20but%20ODV%20projection%20distortions%20in%20the%0Aapplication%20are%20not%20well%20addressed%20directly%20applying%20such%20methods.%20To%20achieve%0Abetter%20super-resolution%20reconstruction%20quality%2C%20we%20propose%20a%20novel%0ASpatio-Temporal%20Distortion%20Aware%20Network%20%28STDAN%29%20oriented%20to%20ODV%0Acharacteristics.%20Specifically%2C%20a%20spatio-temporal%20distortion%20modulation%20module%0Ais%20introduced%20to%20improve%20spatial%20ODV%20projection%20distortions%20and%20exploit%20the%0Atemporal%20correlation%20according%20to%20intra%20and%20inter%20alignments.%20Next%2C%20we%20design%20a%0Amulti-frame%20reconstruction%20and%20fusion%20mechanism%20to%20refine%20the%20consistency%20of%0Areconstructed%20ODV%20frames.%20Furthermore%2C%20we%20incorporate%20latitude-saliency%0Aadaptive%20maps%20in%20the%20loss%20function%20to%20concentrate%20on%20important%20viewpoint%0Aregions%20with%20higher%20texture%20complexity%20and%20human-watching%20interest.%20In%0Aaddition%2C%20we%20collect%20a%20new%20ODV-SR%20dataset%20with%20various%20scenarios.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20STDAN%20achieves%20superior%0Asuper-resolution%20performance%20on%20ODVs%20and%20outperforms%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11506v1&entry.124074799=Read"},
{"title": "AuToMATo: An Out-Of-The-Box Persistence-Based Clustering Algorithm", "author": "Marius Huber and Sara Kalisnik and Patrick Schnider", "abstract": "  We present AuToMATo, a novel clustering algorithm based on persistent\nhomology. While AuToMATo is not parameter-free per se, we provide default\nchoices for its parameters that make it into an out-of-the-box clustering\nalgorithm that performs well across the board. AuToMATo combines the existing\nToMATo clustering algorithm with a bootstrapping procedure in order to separate\nsignificant peaks of an estimated density function from non-significant ones.\nWe perform a thorough comparison of AuToMATo (with its parameters fixed to\ntheir defaults) against many other state-of-the-art clustering algorithms. We\nfind not only that AuToMATo compares favorably against parameter-free\nclustering algorithms, but in many instances also significantly outperforms\neven the best selection of parameters for other algorithms. AuToMATo is\nmotivated by applications in topological data analysis, in particular the\nMapper algorithm, where it is desirable to work with a clustering algorithm\nthat does not need tuning of its parameters. Indeed, we provide evidence that\nAuToMATo performs well when used with Mapper. Finally, we provide an\nopen-source implementation of AuToMATo in Python that is fully compatible with\nthe standard scikit-learn architecture.\n", "link": "http://arxiv.org/abs/2408.06958v2", "date": "2024-10-15", "relevancy": 2.1951, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4345}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuToMATo%3A%20An%20Out-Of-The-Box%20Persistence-Based%20Clustering%20Algorithm&body=Title%3A%20AuToMATo%3A%20An%20Out-Of-The-Box%20Persistence-Based%20Clustering%20Algorithm%0AAuthor%3A%20Marius%20Huber%20and%20Sara%20Kalisnik%20and%20Patrick%20Schnider%0AAbstract%3A%20%20%20We%20present%20AuToMATo%2C%20a%20novel%20clustering%20algorithm%20based%20on%20persistent%0Ahomology.%20While%20AuToMATo%20is%20not%20parameter-free%20per%20se%2C%20we%20provide%20default%0Achoices%20for%20its%20parameters%20that%20make%20it%20into%20an%20out-of-the-box%20clustering%0Aalgorithm%20that%20performs%20well%20across%20the%20board.%20AuToMATo%20combines%20the%20existing%0AToMATo%20clustering%20algorithm%20with%20a%20bootstrapping%20procedure%20in%20order%20to%20separate%0Asignificant%20peaks%20of%20an%20estimated%20density%20function%20from%20non-significant%20ones.%0AWe%20perform%20a%20thorough%20comparison%20of%20AuToMATo%20%28with%20its%20parameters%20fixed%20to%0Atheir%20defaults%29%20against%20many%20other%20state-of-the-art%20clustering%20algorithms.%20We%0Afind%20not%20only%20that%20AuToMATo%20compares%20favorably%20against%20parameter-free%0Aclustering%20algorithms%2C%20but%20in%20many%20instances%20also%20significantly%20outperforms%0Aeven%20the%20best%20selection%20of%20parameters%20for%20other%20algorithms.%20AuToMATo%20is%0Amotivated%20by%20applications%20in%20topological%20data%20analysis%2C%20in%20particular%20the%0AMapper%20algorithm%2C%20where%20it%20is%20desirable%20to%20work%20with%20a%20clustering%20algorithm%0Athat%20does%20not%20need%20tuning%20of%20its%20parameters.%20Indeed%2C%20we%20provide%20evidence%20that%0AAuToMATo%20performs%20well%20when%20used%20with%20Mapper.%20Finally%2C%20we%20provide%20an%0Aopen-source%20implementation%20of%20AuToMATo%20in%20Python%20that%20is%20fully%20compatible%20with%0Athe%20standard%20scikit-learn%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuToMATo%253A%2520An%2520Out-Of-The-Box%2520Persistence-Based%2520Clustering%2520Algorithm%26entry.906535625%3DMarius%2520Huber%2520and%2520Sara%2520Kalisnik%2520and%2520Patrick%2520Schnider%26entry.1292438233%3D%2520%2520We%2520present%2520AuToMATo%252C%2520a%2520novel%2520clustering%2520algorithm%2520based%2520on%2520persistent%250Ahomology.%2520While%2520AuToMATo%2520is%2520not%2520parameter-free%2520per%2520se%252C%2520we%2520provide%2520default%250Achoices%2520for%2520its%2520parameters%2520that%2520make%2520it%2520into%2520an%2520out-of-the-box%2520clustering%250Aalgorithm%2520that%2520performs%2520well%2520across%2520the%2520board.%2520AuToMATo%2520combines%2520the%2520existing%250AToMATo%2520clustering%2520algorithm%2520with%2520a%2520bootstrapping%2520procedure%2520in%2520order%2520to%2520separate%250Asignificant%2520peaks%2520of%2520an%2520estimated%2520density%2520function%2520from%2520non-significant%2520ones.%250AWe%2520perform%2520a%2520thorough%2520comparison%2520of%2520AuToMATo%2520%2528with%2520its%2520parameters%2520fixed%2520to%250Atheir%2520defaults%2529%2520against%2520many%2520other%2520state-of-the-art%2520clustering%2520algorithms.%2520We%250Afind%2520not%2520only%2520that%2520AuToMATo%2520compares%2520favorably%2520against%2520parameter-free%250Aclustering%2520algorithms%252C%2520but%2520in%2520many%2520instances%2520also%2520significantly%2520outperforms%250Aeven%2520the%2520best%2520selection%2520of%2520parameters%2520for%2520other%2520algorithms.%2520AuToMATo%2520is%250Amotivated%2520by%2520applications%2520in%2520topological%2520data%2520analysis%252C%2520in%2520particular%2520the%250AMapper%2520algorithm%252C%2520where%2520it%2520is%2520desirable%2520to%2520work%2520with%2520a%2520clustering%2520algorithm%250Athat%2520does%2520not%2520need%2520tuning%2520of%2520its%2520parameters.%2520Indeed%252C%2520we%2520provide%2520evidence%2520that%250AAuToMATo%2520performs%2520well%2520when%2520used%2520with%2520Mapper.%2520Finally%252C%2520we%2520provide%2520an%250Aopen-source%2520implementation%2520of%2520AuToMATo%2520in%2520Python%2520that%2520is%2520fully%2520compatible%2520with%250Athe%2520standard%2520scikit-learn%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuToMATo%3A%20An%20Out-Of-The-Box%20Persistence-Based%20Clustering%20Algorithm&entry.906535625=Marius%20Huber%20and%20Sara%20Kalisnik%20and%20Patrick%20Schnider&entry.1292438233=%20%20We%20present%20AuToMATo%2C%20a%20novel%20clustering%20algorithm%20based%20on%20persistent%0Ahomology.%20While%20AuToMATo%20is%20not%20parameter-free%20per%20se%2C%20we%20provide%20default%0Achoices%20for%20its%20parameters%20that%20make%20it%20into%20an%20out-of-the-box%20clustering%0Aalgorithm%20that%20performs%20well%20across%20the%20board.%20AuToMATo%20combines%20the%20existing%0AToMATo%20clustering%20algorithm%20with%20a%20bootstrapping%20procedure%20in%20order%20to%20separate%0Asignificant%20peaks%20of%20an%20estimated%20density%20function%20from%20non-significant%20ones.%0AWe%20perform%20a%20thorough%20comparison%20of%20AuToMATo%20%28with%20its%20parameters%20fixed%20to%0Atheir%20defaults%29%20against%20many%20other%20state-of-the-art%20clustering%20algorithms.%20We%0Afind%20not%20only%20that%20AuToMATo%20compares%20favorably%20against%20parameter-free%0Aclustering%20algorithms%2C%20but%20in%20many%20instances%20also%20significantly%20outperforms%0Aeven%20the%20best%20selection%20of%20parameters%20for%20other%20algorithms.%20AuToMATo%20is%0Amotivated%20by%20applications%20in%20topological%20data%20analysis%2C%20in%20particular%20the%0AMapper%20algorithm%2C%20where%20it%20is%20desirable%20to%20work%20with%20a%20clustering%20algorithm%0Athat%20does%20not%20need%20tuning%20of%20its%20parameters.%20Indeed%2C%20we%20provide%20evidence%20that%0AAuToMATo%20performs%20well%20when%20used%20with%20Mapper.%20Finally%2C%20we%20provide%20an%0Aopen-source%20implementation%20of%20AuToMATo%20in%20Python%20that%20is%20fully%20compatible%20with%0Athe%20standard%20scikit-learn%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06958v2&entry.124074799=Read"},
{"title": "Contrastive Touch-to-Touch Pretraining", "author": "Samanta Rodriguez and Yiming Dou and William van den Bogert and Miquel Oller and Kevin So and Andrew Owens and Nima Fazeli", "abstract": "  Today's tactile sensors have a variety of different designs, making it\nchallenging to develop general-purpose methods for processing touch signals. In\nthis paper, we learn a unified representation that captures the shared\ninformation between different tactile sensors. Unlike current approaches that\nfocus on reconstruction or task-specific supervision, we leverage contrastive\nlearning to integrate tactile signals from two different sensors into a shared\nembedding space, using a dataset in which the same objects are probed with\nmultiple sensors. We apply this approach to paired touch signals from GelSlim\nand Soft Bubble sensors. We show that our learned features provide strong\npretraining for downstream pose estimation and classification tasks. We also\nshow that our embedding enables models trained using one touch sensor to be\ndeployed using another without additional training. Project details can be\nfound at https://www.mmintlab.com/research/cttp/.\n", "link": "http://arxiv.org/abs/2410.11834v1", "date": "2024-10-15", "relevancy": 2.1936, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5719}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5377}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Touch-to-Touch%20Pretraining&body=Title%3A%20Contrastive%20Touch-to-Touch%20Pretraining%0AAuthor%3A%20Samanta%20Rodriguez%20and%20Yiming%20Dou%20and%20William%20van%20den%20Bogert%20and%20Miquel%20Oller%20and%20Kevin%20So%20and%20Andrew%20Owens%20and%20Nima%20Fazeli%0AAbstract%3A%20%20%20Today%27s%20tactile%20sensors%20have%20a%20variety%20of%20different%20designs%2C%20making%20it%0Achallenging%20to%20develop%20general-purpose%20methods%20for%20processing%20touch%20signals.%20In%0Athis%20paper%2C%20we%20learn%20a%20unified%20representation%20that%20captures%20the%20shared%0Ainformation%20between%20different%20tactile%20sensors.%20Unlike%20current%20approaches%20that%0Afocus%20on%20reconstruction%20or%20task-specific%20supervision%2C%20we%20leverage%20contrastive%0Alearning%20to%20integrate%20tactile%20signals%20from%20two%20different%20sensors%20into%20a%20shared%0Aembedding%20space%2C%20using%20a%20dataset%20in%20which%20the%20same%20objects%20are%20probed%20with%0Amultiple%20sensors.%20We%20apply%20this%20approach%20to%20paired%20touch%20signals%20from%20GelSlim%0Aand%20Soft%20Bubble%20sensors.%20We%20show%20that%20our%20learned%20features%20provide%20strong%0Apretraining%20for%20downstream%20pose%20estimation%20and%20classification%20tasks.%20We%20also%0Ashow%20that%20our%20embedding%20enables%20models%20trained%20using%20one%20touch%20sensor%20to%20be%0Adeployed%20using%20another%20without%20additional%20training.%20Project%20details%20can%20be%0Afound%20at%20https%3A//www.mmintlab.com/research/cttp/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Touch-to-Touch%2520Pretraining%26entry.906535625%3DSamanta%2520Rodriguez%2520and%2520Yiming%2520Dou%2520and%2520William%2520van%2520den%2520Bogert%2520and%2520Miquel%2520Oller%2520and%2520Kevin%2520So%2520and%2520Andrew%2520Owens%2520and%2520Nima%2520Fazeli%26entry.1292438233%3D%2520%2520Today%2527s%2520tactile%2520sensors%2520have%2520a%2520variety%2520of%2520different%2520designs%252C%2520making%2520it%250Achallenging%2520to%2520develop%2520general-purpose%2520methods%2520for%2520processing%2520touch%2520signals.%2520In%250Athis%2520paper%252C%2520we%2520learn%2520a%2520unified%2520representation%2520that%2520captures%2520the%2520shared%250Ainformation%2520between%2520different%2520tactile%2520sensors.%2520Unlike%2520current%2520approaches%2520that%250Afocus%2520on%2520reconstruction%2520or%2520task-specific%2520supervision%252C%2520we%2520leverage%2520contrastive%250Alearning%2520to%2520integrate%2520tactile%2520signals%2520from%2520two%2520different%2520sensors%2520into%2520a%2520shared%250Aembedding%2520space%252C%2520using%2520a%2520dataset%2520in%2520which%2520the%2520same%2520objects%2520are%2520probed%2520with%250Amultiple%2520sensors.%2520We%2520apply%2520this%2520approach%2520to%2520paired%2520touch%2520signals%2520from%2520GelSlim%250Aand%2520Soft%2520Bubble%2520sensors.%2520We%2520show%2520that%2520our%2520learned%2520features%2520provide%2520strong%250Apretraining%2520for%2520downstream%2520pose%2520estimation%2520and%2520classification%2520tasks.%2520We%2520also%250Ashow%2520that%2520our%2520embedding%2520enables%2520models%2520trained%2520using%2520one%2520touch%2520sensor%2520to%2520be%250Adeployed%2520using%2520another%2520without%2520additional%2520training.%2520Project%2520details%2520can%2520be%250Afound%2520at%2520https%253A//www.mmintlab.com/research/cttp/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Touch-to-Touch%20Pretraining&entry.906535625=Samanta%20Rodriguez%20and%20Yiming%20Dou%20and%20William%20van%20den%20Bogert%20and%20Miquel%20Oller%20and%20Kevin%20So%20and%20Andrew%20Owens%20and%20Nima%20Fazeli&entry.1292438233=%20%20Today%27s%20tactile%20sensors%20have%20a%20variety%20of%20different%20designs%2C%20making%20it%0Achallenging%20to%20develop%20general-purpose%20methods%20for%20processing%20touch%20signals.%20In%0Athis%20paper%2C%20we%20learn%20a%20unified%20representation%20that%20captures%20the%20shared%0Ainformation%20between%20different%20tactile%20sensors.%20Unlike%20current%20approaches%20that%0Afocus%20on%20reconstruction%20or%20task-specific%20supervision%2C%20we%20leverage%20contrastive%0Alearning%20to%20integrate%20tactile%20signals%20from%20two%20different%20sensors%20into%20a%20shared%0Aembedding%20space%2C%20using%20a%20dataset%20in%20which%20the%20same%20objects%20are%20probed%20with%0Amultiple%20sensors.%20We%20apply%20this%20approach%20to%20paired%20touch%20signals%20from%20GelSlim%0Aand%20Soft%20Bubble%20sensors.%20We%20show%20that%20our%20learned%20features%20provide%20strong%0Apretraining%20for%20downstream%20pose%20estimation%20and%20classification%20tasks.%20We%20also%0Ashow%20that%20our%20embedding%20enables%20models%20trained%20using%20one%20touch%20sensor%20to%20be%0Adeployed%20using%20another%20without%20additional%20training.%20Project%20details%20can%20be%0Afound%20at%20https%3A//www.mmintlab.com/research/cttp/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11834v1&entry.124074799=Read"},
{"title": "Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge\n  Augmentation in Vision Language Model", "author": "Diwei Wang and Kun Yuan and Candice Muller and Fr\u00e9d\u00e9ric Blanc and Nicolas Padoy and Hyewon Seo", "abstract": "  We present a knowledge augmentation strategy for assessing the diagnostic\ngroups and gait impairment from monocular gait videos. Based on a large-scale\npre-trained Vision Language Model (VLM), our model learns and improves visual,\ntextual, and numerical representations of patient gait videos, through a\ncollective learning across three distinct modalities: gait videos,\nclass-specific descriptions, and numerical gait parameters. Our specific\ncontributions are two-fold: First, we adopt a knowledge-aware prompt tuning\nstrategy to utilize the class-specific medical description in guiding the text\nprompt learning. Second, we integrate the paired gait parameters in the form of\nnumerical texts to enhance the numeracy of the textual representation. Results\ndemonstrate that our model not only significantly outperforms state-of-the-art\nmethods in video-based classification tasks but also adeptly decodes the\nlearned class-specific text features into natural language descriptions using\nthe vocabulary of quantitative gait parameters. The code and the model will be\nmade available at our project page:\nhttps://lisqzqng.github.io/GaitAnalysisVLM/.\n", "link": "http://arxiv.org/abs/2403.13756v2", "date": "2024-10-15", "relevancy": 2.193, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Gait%20Video%20Analysis%20in%20Neurodegenerative%20Diseases%20by%20Knowledge%0A%20%20Augmentation%20in%20Vision%20Language%20Model&body=Title%3A%20Enhancing%20Gait%20Video%20Analysis%20in%20Neurodegenerative%20Diseases%20by%20Knowledge%0A%20%20Augmentation%20in%20Vision%20Language%20Model%0AAuthor%3A%20Diwei%20Wang%20and%20Kun%20Yuan%20and%20Candice%20Muller%20and%20Fr%C3%A9d%C3%A9ric%20Blanc%20and%20Nicolas%20Padoy%20and%20Hyewon%20Seo%0AAbstract%3A%20%20%20We%20present%20a%20knowledge%20augmentation%20strategy%20for%20assessing%20the%20diagnostic%0Agroups%20and%20gait%20impairment%20from%20monocular%20gait%20videos.%20Based%20on%20a%20large-scale%0Apre-trained%20Vision%20Language%20Model%20%28VLM%29%2C%20our%20model%20learns%20and%20improves%20visual%2C%0Atextual%2C%20and%20numerical%20representations%20of%20patient%20gait%20videos%2C%20through%20a%0Acollective%20learning%20across%20three%20distinct%20modalities%3A%20gait%20videos%2C%0Aclass-specific%20descriptions%2C%20and%20numerical%20gait%20parameters.%20Our%20specific%0Acontributions%20are%20two-fold%3A%20First%2C%20we%20adopt%20a%20knowledge-aware%20prompt%20tuning%0Astrategy%20to%20utilize%20the%20class-specific%20medical%20description%20in%20guiding%20the%20text%0Aprompt%20learning.%20Second%2C%20we%20integrate%20the%20paired%20gait%20parameters%20in%20the%20form%20of%0Anumerical%20texts%20to%20enhance%20the%20numeracy%20of%20the%20textual%20representation.%20Results%0Ademonstrate%20that%20our%20model%20not%20only%20significantly%20outperforms%20state-of-the-art%0Amethods%20in%20video-based%20classification%20tasks%20but%20also%20adeptly%20decodes%20the%0Alearned%20class-specific%20text%20features%20into%20natural%20language%20descriptions%20using%0Athe%20vocabulary%20of%20quantitative%20gait%20parameters.%20The%20code%20and%20the%20model%20will%20be%0Amade%20available%20at%20our%20project%20page%3A%0Ahttps%3A//lisqzqng.github.io/GaitAnalysisVLM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Gait%2520Video%2520Analysis%2520in%2520Neurodegenerative%2520Diseases%2520by%2520Knowledge%250A%2520%2520Augmentation%2520in%2520Vision%2520Language%2520Model%26entry.906535625%3DDiwei%2520Wang%2520and%2520Kun%2520Yuan%2520and%2520Candice%2520Muller%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Blanc%2520and%2520Nicolas%2520Padoy%2520and%2520Hyewon%2520Seo%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520knowledge%2520augmentation%2520strategy%2520for%2520assessing%2520the%2520diagnostic%250Agroups%2520and%2520gait%2520impairment%2520from%2520monocular%2520gait%2520videos.%2520Based%2520on%2520a%2520large-scale%250Apre-trained%2520Vision%2520Language%2520Model%2520%2528VLM%2529%252C%2520our%2520model%2520learns%2520and%2520improves%2520visual%252C%250Atextual%252C%2520and%2520numerical%2520representations%2520of%2520patient%2520gait%2520videos%252C%2520through%2520a%250Acollective%2520learning%2520across%2520three%2520distinct%2520modalities%253A%2520gait%2520videos%252C%250Aclass-specific%2520descriptions%252C%2520and%2520numerical%2520gait%2520parameters.%2520Our%2520specific%250Acontributions%2520are%2520two-fold%253A%2520First%252C%2520we%2520adopt%2520a%2520knowledge-aware%2520prompt%2520tuning%250Astrategy%2520to%2520utilize%2520the%2520class-specific%2520medical%2520description%2520in%2520guiding%2520the%2520text%250Aprompt%2520learning.%2520Second%252C%2520we%2520integrate%2520the%2520paired%2520gait%2520parameters%2520in%2520the%2520form%2520of%250Anumerical%2520texts%2520to%2520enhance%2520the%2520numeracy%2520of%2520the%2520textual%2520representation.%2520Results%250Ademonstrate%2520that%2520our%2520model%2520not%2520only%2520significantly%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520video-based%2520classification%2520tasks%2520but%2520also%2520adeptly%2520decodes%2520the%250Alearned%2520class-specific%2520text%2520features%2520into%2520natural%2520language%2520descriptions%2520using%250Athe%2520vocabulary%2520of%2520quantitative%2520gait%2520parameters.%2520The%2520code%2520and%2520the%2520model%2520will%2520be%250Amade%2520available%2520at%2520our%2520project%2520page%253A%250Ahttps%253A//lisqzqng.github.io/GaitAnalysisVLM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Gait%20Video%20Analysis%20in%20Neurodegenerative%20Diseases%20by%20Knowledge%0A%20%20Augmentation%20in%20Vision%20Language%20Model&entry.906535625=Diwei%20Wang%20and%20Kun%20Yuan%20and%20Candice%20Muller%20and%20Fr%C3%A9d%C3%A9ric%20Blanc%20and%20Nicolas%20Padoy%20and%20Hyewon%20Seo&entry.1292438233=%20%20We%20present%20a%20knowledge%20augmentation%20strategy%20for%20assessing%20the%20diagnostic%0Agroups%20and%20gait%20impairment%20from%20monocular%20gait%20videos.%20Based%20on%20a%20large-scale%0Apre-trained%20Vision%20Language%20Model%20%28VLM%29%2C%20our%20model%20learns%20and%20improves%20visual%2C%0Atextual%2C%20and%20numerical%20representations%20of%20patient%20gait%20videos%2C%20through%20a%0Acollective%20learning%20across%20three%20distinct%20modalities%3A%20gait%20videos%2C%0Aclass-specific%20descriptions%2C%20and%20numerical%20gait%20parameters.%20Our%20specific%0Acontributions%20are%20two-fold%3A%20First%2C%20we%20adopt%20a%20knowledge-aware%20prompt%20tuning%0Astrategy%20to%20utilize%20the%20class-specific%20medical%20description%20in%20guiding%20the%20text%0Aprompt%20learning.%20Second%2C%20we%20integrate%20the%20paired%20gait%20parameters%20in%20the%20form%20of%0Anumerical%20texts%20to%20enhance%20the%20numeracy%20of%20the%20textual%20representation.%20Results%0Ademonstrate%20that%20our%20model%20not%20only%20significantly%20outperforms%20state-of-the-art%0Amethods%20in%20video-based%20classification%20tasks%20but%20also%20adeptly%20decodes%20the%0Alearned%20class-specific%20text%20features%20into%20natural%20language%20descriptions%20using%0Athe%20vocabulary%20of%20quantitative%20gait%20parameters.%20The%20code%20and%20the%20model%20will%20be%0Amade%20available%20at%20our%20project%20page%3A%0Ahttps%3A//lisqzqng.github.io/GaitAnalysisVLM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13756v2&entry.124074799=Read"},
{"title": "Unraveling the Mechanics of Learning-Based Demonstration Selection for\n  In-Context Learning", "author": "Hui Liu and Wenya Wang and Hao Sun and Chris Xing Tian and Chenqi Kong and Xin Dong and Haoliang Li", "abstract": "  Large Language Models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities from few-shot demonstration exemplars. While recent\nlearning-based demonstration selection methods have proven beneficial to ICL by\nchoosing more useful exemplars, their underlying mechanisms are opaque,\nhindering efforts to address limitations such as high training costs and poor\ngeneralization across tasks. These methods generally assume the selection\nprocess captures similarities between the exemplar and the target instance,\nhowever, it remains unknown what kinds of similarities are captured and vital\nto performing ICL. To dive into this question, we analyze the working\nmechanisms of the learning-based demonstration selection methods and\nempirically identify two important factors related to similarity measurement:\n1) The ability to integrate different levels of task-agnostic text similarities\nbetween the input of exemplars and test cases enhances generalization power\nacross different tasks. 2) Incorporating task-specific labels when measuring\nthe similarities significantly improves the performance on each specific task.\nWe validate these two findings through extensive quantitative and qualitative\nanalyses across ten datasets and various LLMs. Based on our findings, we\nintroduce two effective yet simplified exemplar selection methods catering to\ntask-agnostic and task-specific demands, eliminating the costly LLM inference\noverhead.\n", "link": "http://arxiv.org/abs/2406.11890v2", "date": "2024-10-15", "relevancy": 2.1857, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20Mechanics%20of%20Learning-Based%20Demonstration%20Selection%20for%0A%20%20In-Context%20Learning&body=Title%3A%20Unraveling%20the%20Mechanics%20of%20Learning-Based%20Demonstration%20Selection%20for%0A%20%20In-Context%20Learning%0AAuthor%3A%20Hui%20Liu%20and%20Wenya%20Wang%20and%20Hao%20Sun%20and%20Chris%20Xing%20Tian%20and%20Chenqi%20Kong%20and%20Xin%20Dong%20and%20Haoliang%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20in-context%20learning%0A%28ICL%29%20capabilities%20from%20few-shot%20demonstration%20exemplars.%20While%20recent%0Alearning-based%20demonstration%20selection%20methods%20have%20proven%20beneficial%20to%20ICL%20by%0Achoosing%20more%20useful%20exemplars%2C%20their%20underlying%20mechanisms%20are%20opaque%2C%0Ahindering%20efforts%20to%20address%20limitations%20such%20as%20high%20training%20costs%20and%20poor%0Ageneralization%20across%20tasks.%20These%20methods%20generally%20assume%20the%20selection%0Aprocess%20captures%20similarities%20between%20the%20exemplar%20and%20the%20target%20instance%2C%0Ahowever%2C%20it%20remains%20unknown%20what%20kinds%20of%20similarities%20are%20captured%20and%20vital%0Ato%20performing%20ICL.%20To%20dive%20into%20this%20question%2C%20we%20analyze%20the%20working%0Amechanisms%20of%20the%20learning-based%20demonstration%20selection%20methods%20and%0Aempirically%20identify%20two%20important%20factors%20related%20to%20similarity%20measurement%3A%0A1%29%20The%20ability%20to%20integrate%20different%20levels%20of%20task-agnostic%20text%20similarities%0Abetween%20the%20input%20of%20exemplars%20and%20test%20cases%20enhances%20generalization%20power%0Aacross%20different%20tasks.%202%29%20Incorporating%20task-specific%20labels%20when%20measuring%0Athe%20similarities%20significantly%20improves%20the%20performance%20on%20each%20specific%20task.%0AWe%20validate%20these%20two%20findings%20through%20extensive%20quantitative%20and%20qualitative%0Aanalyses%20across%20ten%20datasets%20and%20various%20LLMs.%20Based%20on%20our%20findings%2C%20we%0Aintroduce%20two%20effective%20yet%20simplified%20exemplar%20selection%20methods%20catering%20to%0Atask-agnostic%20and%20task-specific%20demands%2C%20eliminating%20the%20costly%20LLM%20inference%0Aoverhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520the%2520Mechanics%2520of%2520Learning-Based%2520Demonstration%2520Selection%2520for%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DHui%2520Liu%2520and%2520Wenya%2520Wang%2520and%2520Hao%2520Sun%2520and%2520Chris%2520Xing%2520Tian%2520and%2520Chenqi%2520Kong%2520and%2520Xin%2520Dong%2520and%2520Haoliang%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520in-context%2520learning%250A%2528ICL%2529%2520capabilities%2520from%2520few-shot%2520demonstration%2520exemplars.%2520While%2520recent%250Alearning-based%2520demonstration%2520selection%2520methods%2520have%2520proven%2520beneficial%2520to%2520ICL%2520by%250Achoosing%2520more%2520useful%2520exemplars%252C%2520their%2520underlying%2520mechanisms%2520are%2520opaque%252C%250Ahindering%2520efforts%2520to%2520address%2520limitations%2520such%2520as%2520high%2520training%2520costs%2520and%2520poor%250Ageneralization%2520across%2520tasks.%2520These%2520methods%2520generally%2520assume%2520the%2520selection%250Aprocess%2520captures%2520similarities%2520between%2520the%2520exemplar%2520and%2520the%2520target%2520instance%252C%250Ahowever%252C%2520it%2520remains%2520unknown%2520what%2520kinds%2520of%2520similarities%2520are%2520captured%2520and%2520vital%250Ato%2520performing%2520ICL.%2520To%2520dive%2520into%2520this%2520question%252C%2520we%2520analyze%2520the%2520working%250Amechanisms%2520of%2520the%2520learning-based%2520demonstration%2520selection%2520methods%2520and%250Aempirically%2520identify%2520two%2520important%2520factors%2520related%2520to%2520similarity%2520measurement%253A%250A1%2529%2520The%2520ability%2520to%2520integrate%2520different%2520levels%2520of%2520task-agnostic%2520text%2520similarities%250Abetween%2520the%2520input%2520of%2520exemplars%2520and%2520test%2520cases%2520enhances%2520generalization%2520power%250Aacross%2520different%2520tasks.%25202%2529%2520Incorporating%2520task-specific%2520labels%2520when%2520measuring%250Athe%2520similarities%2520significantly%2520improves%2520the%2520performance%2520on%2520each%2520specific%2520task.%250AWe%2520validate%2520these%2520two%2520findings%2520through%2520extensive%2520quantitative%2520and%2520qualitative%250Aanalyses%2520across%2520ten%2520datasets%2520and%2520various%2520LLMs.%2520Based%2520on%2520our%2520findings%252C%2520we%250Aintroduce%2520two%2520effective%2520yet%2520simplified%2520exemplar%2520selection%2520methods%2520catering%2520to%250Atask-agnostic%2520and%2520task-specific%2520demands%252C%2520eliminating%2520the%2520costly%2520LLM%2520inference%250Aoverhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20Mechanics%20of%20Learning-Based%20Demonstration%20Selection%20for%0A%20%20In-Context%20Learning&entry.906535625=Hui%20Liu%20and%20Wenya%20Wang%20and%20Hao%20Sun%20and%20Chris%20Xing%20Tian%20and%20Chenqi%20Kong%20and%20Xin%20Dong%20and%20Haoliang%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20in-context%20learning%0A%28ICL%29%20capabilities%20from%20few-shot%20demonstration%20exemplars.%20While%20recent%0Alearning-based%20demonstration%20selection%20methods%20have%20proven%20beneficial%20to%20ICL%20by%0Achoosing%20more%20useful%20exemplars%2C%20their%20underlying%20mechanisms%20are%20opaque%2C%0Ahindering%20efforts%20to%20address%20limitations%20such%20as%20high%20training%20costs%20and%20poor%0Ageneralization%20across%20tasks.%20These%20methods%20generally%20assume%20the%20selection%0Aprocess%20captures%20similarities%20between%20the%20exemplar%20and%20the%20target%20instance%2C%0Ahowever%2C%20it%20remains%20unknown%20what%20kinds%20of%20similarities%20are%20captured%20and%20vital%0Ato%20performing%20ICL.%20To%20dive%20into%20this%20question%2C%20we%20analyze%20the%20working%0Amechanisms%20of%20the%20learning-based%20demonstration%20selection%20methods%20and%0Aempirically%20identify%20two%20important%20factors%20related%20to%20similarity%20measurement%3A%0A1%29%20The%20ability%20to%20integrate%20different%20levels%20of%20task-agnostic%20text%20similarities%0Abetween%20the%20input%20of%20exemplars%20and%20test%20cases%20enhances%20generalization%20power%0Aacross%20different%20tasks.%202%29%20Incorporating%20task-specific%20labels%20when%20measuring%0Athe%20similarities%20significantly%20improves%20the%20performance%20on%20each%20specific%20task.%0AWe%20validate%20these%20two%20findings%20through%20extensive%20quantitative%20and%20qualitative%0Aanalyses%20across%20ten%20datasets%20and%20various%20LLMs.%20Based%20on%20our%20findings%2C%20we%0Aintroduce%20two%20effective%20yet%20simplified%20exemplar%20selection%20methods%20catering%20to%0Atask-agnostic%20and%20task-specific%20demands%2C%20eliminating%20the%20costly%20LLM%20inference%0Aoverhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11890v2&entry.124074799=Read"},
{"title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws", "author": "Yiding Jiang and Allan Zhou and Zhili Feng and Sadhika Malladi and J. Zico Kolter", "abstract": "  The composition of pretraining data is a key determinant of foundation\nmodels' performance, but there is no standard guideline for allocating a\nlimited computational budget across different data sources. Most current\napproaches either rely on extensive experiments with smaller models or dynamic\ndata adjustments that also require proxy models, both of which significantly\nincrease the workflow complexity and computational overhead. In this paper, we\nintroduce Adaptive Data Optimization (ADO), an algorithm that optimizes data\ndistributions in an online fashion, concurrent with model training. Unlike\nexisting techniques, ADO does not require external knowledge, proxy models, or\nmodifications to the model update. Instead, ADO uses per-domain scaling laws to\nestimate the learning potential of each domain during training and adjusts the\ndata mixture accordingly, making it more scalable and easier to integrate.\nExperiments demonstrate that ADO can achieve comparable or better performance\nthan prior methods while maintaining computational efficiency across different\ncomputation scales, offering a practical solution for dynamically adjusting\ndata distribution without sacrificing flexibility or increasing costs. Beyond\nits practical benefits, ADO also provides a new perspective on data collection\nstrategies via scaling laws.\n", "link": "http://arxiv.org/abs/2410.11820v1", "date": "2024-10-15", "relevancy": 2.1841, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5542}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5446}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Data%20Optimization%3A%20Dynamic%20Sample%20Selection%20with%20Scaling%20Laws&body=Title%3A%20Adaptive%20Data%20Optimization%3A%20Dynamic%20Sample%20Selection%20with%20Scaling%20Laws%0AAuthor%3A%20Yiding%20Jiang%20and%20Allan%20Zhou%20and%20Zhili%20Feng%20and%20Sadhika%20Malladi%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20The%20composition%20of%20pretraining%20data%20is%20a%20key%20determinant%20of%20foundation%0Amodels%27%20performance%2C%20but%20there%20is%20no%20standard%20guideline%20for%20allocating%20a%0Alimited%20computational%20budget%20across%20different%20data%20sources.%20Most%20current%0Aapproaches%20either%20rely%20on%20extensive%20experiments%20with%20smaller%20models%20or%20dynamic%0Adata%20adjustments%20that%20also%20require%20proxy%20models%2C%20both%20of%20which%20significantly%0Aincrease%20the%20workflow%20complexity%20and%20computational%20overhead.%20In%20this%20paper%2C%20we%0Aintroduce%20Adaptive%20Data%20Optimization%20%28ADO%29%2C%20an%20algorithm%20that%20optimizes%20data%0Adistributions%20in%20an%20online%20fashion%2C%20concurrent%20with%20model%20training.%20Unlike%0Aexisting%20techniques%2C%20ADO%20does%20not%20require%20external%20knowledge%2C%20proxy%20models%2C%20or%0Amodifications%20to%20the%20model%20update.%20Instead%2C%20ADO%20uses%20per-domain%20scaling%20laws%20to%0Aestimate%20the%20learning%20potential%20of%20each%20domain%20during%20training%20and%20adjusts%20the%0Adata%20mixture%20accordingly%2C%20making%20it%20more%20scalable%20and%20easier%20to%20integrate.%0AExperiments%20demonstrate%20that%20ADO%20can%20achieve%20comparable%20or%20better%20performance%0Athan%20prior%20methods%20while%20maintaining%20computational%20efficiency%20across%20different%0Acomputation%20scales%2C%20offering%20a%20practical%20solution%20for%20dynamically%20adjusting%0Adata%20distribution%20without%20sacrificing%20flexibility%20or%20increasing%20costs.%20Beyond%0Aits%20practical%20benefits%2C%20ADO%20also%20provides%20a%20new%20perspective%20on%20data%20collection%0Astrategies%20via%20scaling%20laws.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Data%2520Optimization%253A%2520Dynamic%2520Sample%2520Selection%2520with%2520Scaling%2520Laws%26entry.906535625%3DYiding%2520Jiang%2520and%2520Allan%2520Zhou%2520and%2520Zhili%2520Feng%2520and%2520Sadhika%2520Malladi%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520The%2520composition%2520of%2520pretraining%2520data%2520is%2520a%2520key%2520determinant%2520of%2520foundation%250Amodels%2527%2520performance%252C%2520but%2520there%2520is%2520no%2520standard%2520guideline%2520for%2520allocating%2520a%250Alimited%2520computational%2520budget%2520across%2520different%2520data%2520sources.%2520Most%2520current%250Aapproaches%2520either%2520rely%2520on%2520extensive%2520experiments%2520with%2520smaller%2520models%2520or%2520dynamic%250Adata%2520adjustments%2520that%2520also%2520require%2520proxy%2520models%252C%2520both%2520of%2520which%2520significantly%250Aincrease%2520the%2520workflow%2520complexity%2520and%2520computational%2520overhead.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Adaptive%2520Data%2520Optimization%2520%2528ADO%2529%252C%2520an%2520algorithm%2520that%2520optimizes%2520data%250Adistributions%2520in%2520an%2520online%2520fashion%252C%2520concurrent%2520with%2520model%2520training.%2520Unlike%250Aexisting%2520techniques%252C%2520ADO%2520does%2520not%2520require%2520external%2520knowledge%252C%2520proxy%2520models%252C%2520or%250Amodifications%2520to%2520the%2520model%2520update.%2520Instead%252C%2520ADO%2520uses%2520per-domain%2520scaling%2520laws%2520to%250Aestimate%2520the%2520learning%2520potential%2520of%2520each%2520domain%2520during%2520training%2520and%2520adjusts%2520the%250Adata%2520mixture%2520accordingly%252C%2520making%2520it%2520more%2520scalable%2520and%2520easier%2520to%2520integrate.%250AExperiments%2520demonstrate%2520that%2520ADO%2520can%2520achieve%2520comparable%2520or%2520better%2520performance%250Athan%2520prior%2520methods%2520while%2520maintaining%2520computational%2520efficiency%2520across%2520different%250Acomputation%2520scales%252C%2520offering%2520a%2520practical%2520solution%2520for%2520dynamically%2520adjusting%250Adata%2520distribution%2520without%2520sacrificing%2520flexibility%2520or%2520increasing%2520costs.%2520Beyond%250Aits%2520practical%2520benefits%252C%2520ADO%2520also%2520provides%2520a%2520new%2520perspective%2520on%2520data%2520collection%250Astrategies%2520via%2520scaling%2520laws.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Data%20Optimization%3A%20Dynamic%20Sample%20Selection%20with%20Scaling%20Laws&entry.906535625=Yiding%20Jiang%20and%20Allan%20Zhou%20and%20Zhili%20Feng%20and%20Sadhika%20Malladi%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20The%20composition%20of%20pretraining%20data%20is%20a%20key%20determinant%20of%20foundation%0Amodels%27%20performance%2C%20but%20there%20is%20no%20standard%20guideline%20for%20allocating%20a%0Alimited%20computational%20budget%20across%20different%20data%20sources.%20Most%20current%0Aapproaches%20either%20rely%20on%20extensive%20experiments%20with%20smaller%20models%20or%20dynamic%0Adata%20adjustments%20that%20also%20require%20proxy%20models%2C%20both%20of%20which%20significantly%0Aincrease%20the%20workflow%20complexity%20and%20computational%20overhead.%20In%20this%20paper%2C%20we%0Aintroduce%20Adaptive%20Data%20Optimization%20%28ADO%29%2C%20an%20algorithm%20that%20optimizes%20data%0Adistributions%20in%20an%20online%20fashion%2C%20concurrent%20with%20model%20training.%20Unlike%0Aexisting%20techniques%2C%20ADO%20does%20not%20require%20external%20knowledge%2C%20proxy%20models%2C%20or%0Amodifications%20to%20the%20model%20update.%20Instead%2C%20ADO%20uses%20per-domain%20scaling%20laws%20to%0Aestimate%20the%20learning%20potential%20of%20each%20domain%20during%20training%20and%20adjusts%20the%0Adata%20mixture%20accordingly%2C%20making%20it%20more%20scalable%20and%20easier%20to%20integrate.%0AExperiments%20demonstrate%20that%20ADO%20can%20achieve%20comparable%20or%20better%20performance%0Athan%20prior%20methods%20while%20maintaining%20computational%20efficiency%20across%20different%0Acomputation%20scales%2C%20offering%20a%20practical%20solution%20for%20dynamically%20adjusting%0Adata%20distribution%20without%20sacrificing%20flexibility%20or%20increasing%20costs.%20Beyond%0Aits%20practical%20benefits%2C%20ADO%20also%20provides%20a%20new%20perspective%20on%20data%20collection%0Astrategies%20via%20scaling%20laws.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11820v1&entry.124074799=Read"},
{"title": "Dual-Teacher Ensemble Models with Double-Copy-Paste for 3D\n  Semi-Supervised Medical Image Segmentation", "author": "Zhan Fa and Shumeng Li and Jian Zhang and Lei Qi and Qian Yu and Yinghuan Shi", "abstract": "  Semi-supervised learning (SSL) techniques address the high labeling costs in\n3D medical image segmentation, with the teacher-student model being a common\napproach. However, using an exponential moving average (EMA) in single-teacher\nmodels may cause coupling issues, where the weights of the student and teacher\nmodels become similar, limiting the teacher's ability to provide additional\nknowledge for the student. Dual-teacher models were introduced to address this\nproblem but often neglected the importance of maintaining teacher model\ndiversity, leading to coupling issues among teachers. To address the coupling\nissue, we incorporate a double-copy-paste (DCP) technique to enhance the\ndiversity among the teachers. Additionally, we introduce the Staged Selective\nEnsemble (SSE) module, which selects different ensemble methods based on the\ncharacteristics of the samples and enables more accurate segmentation of label\nboundaries, thereby improving the quality of pseudo-labels. Experimental\nresults demonstrate the effectiveness of our proposed method in 3D medical\nimage segmentation tasks. Here is the code link:\nhttps://github.com/Fazhan-cs/DCP.\n", "link": "http://arxiv.org/abs/2410.11509v1", "date": "2024-10-15", "relevancy": 2.1838, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5558}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5484}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Teacher%20Ensemble%20Models%20with%20Double-Copy-Paste%20for%203D%0A%20%20Semi-Supervised%20Medical%20Image%20Segmentation&body=Title%3A%20Dual-Teacher%20Ensemble%20Models%20with%20Double-Copy-Paste%20for%203D%0A%20%20Semi-Supervised%20Medical%20Image%20Segmentation%0AAuthor%3A%20Zhan%20Fa%20and%20Shumeng%20Li%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Qian%20Yu%20and%20Yinghuan%20Shi%0AAbstract%3A%20%20%20Semi-supervised%20learning%20%28SSL%29%20techniques%20address%20the%20high%20labeling%20costs%20in%0A3D%20medical%20image%20segmentation%2C%20with%20the%20teacher-student%20model%20being%20a%20common%0Aapproach.%20However%2C%20using%20an%20exponential%20moving%20average%20%28EMA%29%20in%20single-teacher%0Amodels%20may%20cause%20coupling%20issues%2C%20where%20the%20weights%20of%20the%20student%20and%20teacher%0Amodels%20become%20similar%2C%20limiting%20the%20teacher%27s%20ability%20to%20provide%20additional%0Aknowledge%20for%20the%20student.%20Dual-teacher%20models%20were%20introduced%20to%20address%20this%0Aproblem%20but%20often%20neglected%20the%20importance%20of%20maintaining%20teacher%20model%0Adiversity%2C%20leading%20to%20coupling%20issues%20among%20teachers.%20To%20address%20the%20coupling%0Aissue%2C%20we%20incorporate%20a%20double-copy-paste%20%28DCP%29%20technique%20to%20enhance%20the%0Adiversity%20among%20the%20teachers.%20Additionally%2C%20we%20introduce%20the%20Staged%20Selective%0AEnsemble%20%28SSE%29%20module%2C%20which%20selects%20different%20ensemble%20methods%20based%20on%20the%0Acharacteristics%20of%20the%20samples%20and%20enables%20more%20accurate%20segmentation%20of%20label%0Aboundaries%2C%20thereby%20improving%20the%20quality%20of%20pseudo-labels.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20in%203D%20medical%0Aimage%20segmentation%20tasks.%20Here%20is%20the%20code%20link%3A%0Ahttps%3A//github.com/Fazhan-cs/DCP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Teacher%2520Ensemble%2520Models%2520with%2520Double-Copy-Paste%2520for%25203D%250A%2520%2520Semi-Supervised%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DZhan%2520Fa%2520and%2520Shumeng%2520Li%2520and%2520Jian%2520Zhang%2520and%2520Lei%2520Qi%2520and%2520Qian%2520Yu%2520and%2520Yinghuan%2520Shi%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520%2528SSL%2529%2520techniques%2520address%2520the%2520high%2520labeling%2520costs%2520in%250A3D%2520medical%2520image%2520segmentation%252C%2520with%2520the%2520teacher-student%2520model%2520being%2520a%2520common%250Aapproach.%2520However%252C%2520using%2520an%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520in%2520single-teacher%250Amodels%2520may%2520cause%2520coupling%2520issues%252C%2520where%2520the%2520weights%2520of%2520the%2520student%2520and%2520teacher%250Amodels%2520become%2520similar%252C%2520limiting%2520the%2520teacher%2527s%2520ability%2520to%2520provide%2520additional%250Aknowledge%2520for%2520the%2520student.%2520Dual-teacher%2520models%2520were%2520introduced%2520to%2520address%2520this%250Aproblem%2520but%2520often%2520neglected%2520the%2520importance%2520of%2520maintaining%2520teacher%2520model%250Adiversity%252C%2520leading%2520to%2520coupling%2520issues%2520among%2520teachers.%2520To%2520address%2520the%2520coupling%250Aissue%252C%2520we%2520incorporate%2520a%2520double-copy-paste%2520%2528DCP%2529%2520technique%2520to%2520enhance%2520the%250Adiversity%2520among%2520the%2520teachers.%2520Additionally%252C%2520we%2520introduce%2520the%2520Staged%2520Selective%250AEnsemble%2520%2528SSE%2529%2520module%252C%2520which%2520selects%2520different%2520ensemble%2520methods%2520based%2520on%2520the%250Acharacteristics%2520of%2520the%2520samples%2520and%2520enables%2520more%2520accurate%2520segmentation%2520of%2520label%250Aboundaries%252C%2520thereby%2520improving%2520the%2520quality%2520of%2520pseudo-labels.%2520Experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%2520in%25203D%2520medical%250Aimage%2520segmentation%2520tasks.%2520Here%2520is%2520the%2520code%2520link%253A%250Ahttps%253A//github.com/Fazhan-cs/DCP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Teacher%20Ensemble%20Models%20with%20Double-Copy-Paste%20for%203D%0A%20%20Semi-Supervised%20Medical%20Image%20Segmentation&entry.906535625=Zhan%20Fa%20and%20Shumeng%20Li%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Qian%20Yu%20and%20Yinghuan%20Shi&entry.1292438233=%20%20Semi-supervised%20learning%20%28SSL%29%20techniques%20address%20the%20high%20labeling%20costs%20in%0A3D%20medical%20image%20segmentation%2C%20with%20the%20teacher-student%20model%20being%20a%20common%0Aapproach.%20However%2C%20using%20an%20exponential%20moving%20average%20%28EMA%29%20in%20single-teacher%0Amodels%20may%20cause%20coupling%20issues%2C%20where%20the%20weights%20of%20the%20student%20and%20teacher%0Amodels%20become%20similar%2C%20limiting%20the%20teacher%27s%20ability%20to%20provide%20additional%0Aknowledge%20for%20the%20student.%20Dual-teacher%20models%20were%20introduced%20to%20address%20this%0Aproblem%20but%20often%20neglected%20the%20importance%20of%20maintaining%20teacher%20model%0Adiversity%2C%20leading%20to%20coupling%20issues%20among%20teachers.%20To%20address%20the%20coupling%0Aissue%2C%20we%20incorporate%20a%20double-copy-paste%20%28DCP%29%20technique%20to%20enhance%20the%0Adiversity%20among%20the%20teachers.%20Additionally%2C%20we%20introduce%20the%20Staged%20Selective%0AEnsemble%20%28SSE%29%20module%2C%20which%20selects%20different%20ensemble%20methods%20based%20on%20the%0Acharacteristics%20of%20the%20samples%20and%20enables%20more%20accurate%20segmentation%20of%20label%0Aboundaries%2C%20thereby%20improving%20the%20quality%20of%20pseudo-labels.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20in%203D%20medical%0Aimage%20segmentation%20tasks.%20Here%20is%20the%20code%20link%3A%0Ahttps%3A//github.com/Fazhan-cs/DCP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11509v1&entry.124074799=Read"},
{"title": "Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World\n  Model Disentanglement", "author": "Zhi Wang and Li Zhang and Wenhao Wu and Yuanheng Zhu and Dongbin Zhao and Chunlin Chen", "abstract": "  A longstanding goal of artificial general intelligence is highly capable\ngeneralists that can learn from diverse experiences and generalize to unseen\ntasks. The language and vision communities have seen remarkable progress toward\nthis trend by scaling up transformer-based models trained on massive datasets,\nwhile reinforcement learning (RL) agents still suffer from poor generalization\ncapacity under such paradigms. To tackle this challenge, we propose Meta\nDecision Transformer (Meta-DT), which leverages the sequential modeling ability\nof the transformer architecture and robust task representation learning via\nworld model disentanglement to achieve efficient generalization in offline\nmeta-RL. We pretrain a context-aware world model to learn a compact task\nrepresentation, and inject it as a contextual condition to the causal\ntransformer to guide task-oriented sequence generation. Then, we subtly utilize\nhistory trajectories generated by the meta-policy as a self-guided prompt to\nexploit the architectural inductive bias. We select the trajectory segment that\nyields the largest prediction error on the pretrained world model to construct\nthe prompt, aiming to encode task-specific information complementary to the\nworld model maximally. Notably, the proposed framework eliminates the\nrequirement of any expert demonstration or domain knowledge at test time.\nExperimental results on MuJoCo and Meta-World benchmarks across various dataset\ntypes show that Meta-DT exhibits superior few and zero-shot generalization\ncapacity compared to strong baselines while being more practical with fewer\nprerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT.\n", "link": "http://arxiv.org/abs/2410.11448v1", "date": "2024-10-15", "relevancy": 2.1642, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5613}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5375}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-DT%3A%20Offline%20Meta-RL%20as%20Conditional%20Sequence%20Modeling%20with%20World%0A%20%20Model%20Disentanglement&body=Title%3A%20Meta-DT%3A%20Offline%20Meta-RL%20as%20Conditional%20Sequence%20Modeling%20with%20World%0A%20%20Model%20Disentanglement%0AAuthor%3A%20Zhi%20Wang%20and%20Li%20Zhang%20and%20Wenhao%20Wu%20and%20Yuanheng%20Zhu%20and%20Dongbin%20Zhao%20and%20Chunlin%20Chen%0AAbstract%3A%20%20%20A%20longstanding%20goal%20of%20artificial%20general%20intelligence%20is%20highly%20capable%0Ageneralists%20that%20can%20learn%20from%20diverse%20experiences%20and%20generalize%20to%20unseen%0Atasks.%20The%20language%20and%20vision%20communities%20have%20seen%20remarkable%20progress%20toward%0Athis%20trend%20by%20scaling%20up%20transformer-based%20models%20trained%20on%20massive%20datasets%2C%0Awhile%20reinforcement%20learning%20%28RL%29%20agents%20still%20suffer%20from%20poor%20generalization%0Acapacity%20under%20such%20paradigms.%20To%20tackle%20this%20challenge%2C%20we%20propose%20Meta%0ADecision%20Transformer%20%28Meta-DT%29%2C%20which%20leverages%20the%20sequential%20modeling%20ability%0Aof%20the%20transformer%20architecture%20and%20robust%20task%20representation%20learning%20via%0Aworld%20model%20disentanglement%20to%20achieve%20efficient%20generalization%20in%20offline%0Ameta-RL.%20We%20pretrain%20a%20context-aware%20world%20model%20to%20learn%20a%20compact%20task%0Arepresentation%2C%20and%20inject%20it%20as%20a%20contextual%20condition%20to%20the%20causal%0Atransformer%20to%20guide%20task-oriented%20sequence%20generation.%20Then%2C%20we%20subtly%20utilize%0Ahistory%20trajectories%20generated%20by%20the%20meta-policy%20as%20a%20self-guided%20prompt%20to%0Aexploit%20the%20architectural%20inductive%20bias.%20We%20select%20the%20trajectory%20segment%20that%0Ayields%20the%20largest%20prediction%20error%20on%20the%20pretrained%20world%20model%20to%20construct%0Athe%20prompt%2C%20aiming%20to%20encode%20task-specific%20information%20complementary%20to%20the%0Aworld%20model%20maximally.%20Notably%2C%20the%20proposed%20framework%20eliminates%20the%0Arequirement%20of%20any%20expert%20demonstration%20or%20domain%20knowledge%20at%20test%20time.%0AExperimental%20results%20on%20MuJoCo%20and%20Meta-World%20benchmarks%20across%20various%20dataset%0Atypes%20show%20that%20Meta-DT%20exhibits%20superior%20few%20and%20zero-shot%20generalization%0Acapacity%20compared%20to%20strong%20baselines%20while%20being%20more%20practical%20with%20fewer%0Aprerequisites.%20Our%20code%20is%20available%20at%20https%3A//github.com/NJU-RL/Meta-DT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-DT%253A%2520Offline%2520Meta-RL%2520as%2520Conditional%2520Sequence%2520Modeling%2520with%2520World%250A%2520%2520Model%2520Disentanglement%26entry.906535625%3DZhi%2520Wang%2520and%2520Li%2520Zhang%2520and%2520Wenhao%2520Wu%2520and%2520Yuanheng%2520Zhu%2520and%2520Dongbin%2520Zhao%2520and%2520Chunlin%2520Chen%26entry.1292438233%3D%2520%2520A%2520longstanding%2520goal%2520of%2520artificial%2520general%2520intelligence%2520is%2520highly%2520capable%250Ageneralists%2520that%2520can%2520learn%2520from%2520diverse%2520experiences%2520and%2520generalize%2520to%2520unseen%250Atasks.%2520The%2520language%2520and%2520vision%2520communities%2520have%2520seen%2520remarkable%2520progress%2520toward%250Athis%2520trend%2520by%2520scaling%2520up%2520transformer-based%2520models%2520trained%2520on%2520massive%2520datasets%252C%250Awhile%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520still%2520suffer%2520from%2520poor%2520generalization%250Acapacity%2520under%2520such%2520paradigms.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520Meta%250ADecision%2520Transformer%2520%2528Meta-DT%2529%252C%2520which%2520leverages%2520the%2520sequential%2520modeling%2520ability%250Aof%2520the%2520transformer%2520architecture%2520and%2520robust%2520task%2520representation%2520learning%2520via%250Aworld%2520model%2520disentanglement%2520to%2520achieve%2520efficient%2520generalization%2520in%2520offline%250Ameta-RL.%2520We%2520pretrain%2520a%2520context-aware%2520world%2520model%2520to%2520learn%2520a%2520compact%2520task%250Arepresentation%252C%2520and%2520inject%2520it%2520as%2520a%2520contextual%2520condition%2520to%2520the%2520causal%250Atransformer%2520to%2520guide%2520task-oriented%2520sequence%2520generation.%2520Then%252C%2520we%2520subtly%2520utilize%250Ahistory%2520trajectories%2520generated%2520by%2520the%2520meta-policy%2520as%2520a%2520self-guided%2520prompt%2520to%250Aexploit%2520the%2520architectural%2520inductive%2520bias.%2520We%2520select%2520the%2520trajectory%2520segment%2520that%250Ayields%2520the%2520largest%2520prediction%2520error%2520on%2520the%2520pretrained%2520world%2520model%2520to%2520construct%250Athe%2520prompt%252C%2520aiming%2520to%2520encode%2520task-specific%2520information%2520complementary%2520to%2520the%250Aworld%2520model%2520maximally.%2520Notably%252C%2520the%2520proposed%2520framework%2520eliminates%2520the%250Arequirement%2520of%2520any%2520expert%2520demonstration%2520or%2520domain%2520knowledge%2520at%2520test%2520time.%250AExperimental%2520results%2520on%2520MuJoCo%2520and%2520Meta-World%2520benchmarks%2520across%2520various%2520dataset%250Atypes%2520show%2520that%2520Meta-DT%2520exhibits%2520superior%2520few%2520and%2520zero-shot%2520generalization%250Acapacity%2520compared%2520to%2520strong%2520baselines%2520while%2520being%2520more%2520practical%2520with%2520fewer%250Aprerequisites.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/NJU-RL/Meta-DT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-DT%3A%20Offline%20Meta-RL%20as%20Conditional%20Sequence%20Modeling%20with%20World%0A%20%20Model%20Disentanglement&entry.906535625=Zhi%20Wang%20and%20Li%20Zhang%20and%20Wenhao%20Wu%20and%20Yuanheng%20Zhu%20and%20Dongbin%20Zhao%20and%20Chunlin%20Chen&entry.1292438233=%20%20A%20longstanding%20goal%20of%20artificial%20general%20intelligence%20is%20highly%20capable%0Ageneralists%20that%20can%20learn%20from%20diverse%20experiences%20and%20generalize%20to%20unseen%0Atasks.%20The%20language%20and%20vision%20communities%20have%20seen%20remarkable%20progress%20toward%0Athis%20trend%20by%20scaling%20up%20transformer-based%20models%20trained%20on%20massive%20datasets%2C%0Awhile%20reinforcement%20learning%20%28RL%29%20agents%20still%20suffer%20from%20poor%20generalization%0Acapacity%20under%20such%20paradigms.%20To%20tackle%20this%20challenge%2C%20we%20propose%20Meta%0ADecision%20Transformer%20%28Meta-DT%29%2C%20which%20leverages%20the%20sequential%20modeling%20ability%0Aof%20the%20transformer%20architecture%20and%20robust%20task%20representation%20learning%20via%0Aworld%20model%20disentanglement%20to%20achieve%20efficient%20generalization%20in%20offline%0Ameta-RL.%20We%20pretrain%20a%20context-aware%20world%20model%20to%20learn%20a%20compact%20task%0Arepresentation%2C%20and%20inject%20it%20as%20a%20contextual%20condition%20to%20the%20causal%0Atransformer%20to%20guide%20task-oriented%20sequence%20generation.%20Then%2C%20we%20subtly%20utilize%0Ahistory%20trajectories%20generated%20by%20the%20meta-policy%20as%20a%20self-guided%20prompt%20to%0Aexploit%20the%20architectural%20inductive%20bias.%20We%20select%20the%20trajectory%20segment%20that%0Ayields%20the%20largest%20prediction%20error%20on%20the%20pretrained%20world%20model%20to%20construct%0Athe%20prompt%2C%20aiming%20to%20encode%20task-specific%20information%20complementary%20to%20the%0Aworld%20model%20maximally.%20Notably%2C%20the%20proposed%20framework%20eliminates%20the%0Arequirement%20of%20any%20expert%20demonstration%20or%20domain%20knowledge%20at%20test%20time.%0AExperimental%20results%20on%20MuJoCo%20and%20Meta-World%20benchmarks%20across%20various%20dataset%0Atypes%20show%20that%20Meta-DT%20exhibits%20superior%20few%20and%20zero-shot%20generalization%0Acapacity%20compared%20to%20strong%20baselines%20while%20being%20more%20practical%20with%20fewer%0Aprerequisites.%20Our%20code%20is%20available%20at%20https%3A//github.com/NJU-RL/Meta-DT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11448v1&entry.124074799=Read"},
{"title": "The $\u03bc\\mathcal{G}$ Language for Programming Graph Neural Networks", "author": "Matteo Belenchia and Flavio Corradini and Michela Quadrini and Michele Loreti", "abstract": "  Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.\n", "link": "http://arxiv.org/abs/2407.09441v4", "date": "2024-10-15", "relevancy": 2.1549, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4373}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4285}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&body=Title%3A%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks%0AAuthor%3A%20Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti%0AAbstract%3A%20%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09441v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520%2524%25CE%25BC%255Cmathcal%257BG%257D%2524%2520Language%2520for%2520Programming%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMatteo%2520Belenchia%2520and%2520Flavio%2520Corradini%2520and%2520Michela%2520Quadrini%2520and%2520Michele%2520Loreti%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520form%2520a%2520class%2520of%2520deep%2520learning%2520architectures%250Aspecifically%2520designed%2520to%2520work%2520with%2520graph-structured%2520data.%2520As%2520such%252C%2520they%2520share%250Athe%2520inherent%2520limitations%2520and%2520problems%2520of%2520deep%2520learning%252C%2520especially%2520regarding%250Athe%2520issues%2520of%2520explainability%2520and%2520trustworthiness.%2520We%2520propose%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%252C%250Aan%2520original%2520domain-specific%2520language%2520for%2520the%2520specification%2520of%2520graph%2520neural%250Anetworks%2520that%2520aims%2520to%2520overcome%2520these%2520issues.%2520The%2520language%2527s%2520syntax%2520is%250Aintroduced%252C%2520and%2520its%2520meaning%2520is%2520rigorously%2520defined%2520by%2520a%2520denotational%2520semantics.%250AAn%2520equivalent%2520characterization%2520in%2520the%2520form%2520of%2520an%2520operational%2520semantics%2520is%2520also%250Aprovided%2520and%252C%2520together%2520with%2520a%2520type%2520system%252C%2520is%2520used%2520to%2520prove%2520the%2520type%2520soundness%250Aof%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524.%2520We%2520show%2520how%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%2520programs%2520can%2520be%2520represented%250Ain%2520a%2520more%2520user-friendly%2520graphical%2520visualization%252C%2520and%2520provide%2520examples%2520of%2520its%250Agenerality%2520by%2520showing%2520how%2520it%2520can%2520be%2520used%2520to%2520define%2520some%2520of%2520the%2520most%2520popular%250Agraph%2520neural%2520network%2520models%252C%2520or%2520to%2520develop%2520any%2520custom%2520graph%2520processing%250Aapplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09441v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&entry.906535625=Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti&entry.1292438233=%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09441v4&entry.124074799=Read"},
{"title": "Breaking Modality Gap in RGBT Tracking: Coupled Knowledge Distillation", "author": "Andong Lu and Jiacong Zhao and Chenglong Li and Yun Xiao and Bin Luo", "abstract": "  Modality gap between RGB and thermal infrared (TIR) images is a crucial issue\nbut often overlooked in existing RGBT tracking methods. It can be observed that\nmodality gap mainly lies in the image style difference. In this work, we\npropose a novel Coupled Knowledge Distillation framework called CKD, which\npursues common styles of different modalities to break modality gap, for high\nperformance RGBT tracking. In particular, we introduce two student networks and\nemploy the style distillation loss to make their style features consistent as\nmuch as possible. Through alleviating the style difference of two student\nnetworks, we can break modality gap of different modalities well. However, the\ndistillation of style features might harm to the content representations of two\nmodalities in student networks. To handle this issue, we take original RGB and\nTIR networks as the teachers, and distill their content knowledge into two\nstudent networks respectively by the style-content orthogonal feature\ndecoupling scheme. We couple the above two distillation processes in an online\noptimization framework to form new feature representations of RGB and thermal\nmodalities without modality gap. In addition, we design a masked modeling\nstrategy and a multi-modal candidate token elimination strategy into CKD to\nimprove tracking robustness and efficiency respectively. Extensive experiments\non five standard RGBT tracking datasets validate the effectiveness of the\nproposed method against state-of-the-art methods while achieving the fastest\ntracking speed of 96.4 FPS. Code available at\nhttps://github.com/Multi-Modality-Tracking/CKD.\n", "link": "http://arxiv.org/abs/2410.11586v1", "date": "2024-10-15", "relevancy": 2.1527, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.55}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Modality%20Gap%20in%20RGBT%20Tracking%3A%20Coupled%20Knowledge%20Distillation&body=Title%3A%20Breaking%20Modality%20Gap%20in%20RGBT%20Tracking%3A%20Coupled%20Knowledge%20Distillation%0AAuthor%3A%20Andong%20Lu%20and%20Jiacong%20Zhao%20and%20Chenglong%20Li%20and%20Yun%20Xiao%20and%20Bin%20Luo%0AAbstract%3A%20%20%20Modality%20gap%20between%20RGB%20and%20thermal%20infrared%20%28TIR%29%20images%20is%20a%20crucial%20issue%0Abut%20often%20overlooked%20in%20existing%20RGBT%20tracking%20methods.%20It%20can%20be%20observed%20that%0Amodality%20gap%20mainly%20lies%20in%20the%20image%20style%20difference.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20Coupled%20Knowledge%20Distillation%20framework%20called%20CKD%2C%20which%0Apursues%20common%20styles%20of%20different%20modalities%20to%20break%20modality%20gap%2C%20for%20high%0Aperformance%20RGBT%20tracking.%20In%20particular%2C%20we%20introduce%20two%20student%20networks%20and%0Aemploy%20the%20style%20distillation%20loss%20to%20make%20their%20style%20features%20consistent%20as%0Amuch%20as%20possible.%20Through%20alleviating%20the%20style%20difference%20of%20two%20student%0Anetworks%2C%20we%20can%20break%20modality%20gap%20of%20different%20modalities%20well.%20However%2C%20the%0Adistillation%20of%20style%20features%20might%20harm%20to%20the%20content%20representations%20of%20two%0Amodalities%20in%20student%20networks.%20To%20handle%20this%20issue%2C%20we%20take%20original%20RGB%20and%0ATIR%20networks%20as%20the%20teachers%2C%20and%20distill%20their%20content%20knowledge%20into%20two%0Astudent%20networks%20respectively%20by%20the%20style-content%20orthogonal%20feature%0Adecoupling%20scheme.%20We%20couple%20the%20above%20two%20distillation%20processes%20in%20an%20online%0Aoptimization%20framework%20to%20form%20new%20feature%20representations%20of%20RGB%20and%20thermal%0Amodalities%20without%20modality%20gap.%20In%20addition%2C%20we%20design%20a%20masked%20modeling%0Astrategy%20and%20a%20multi-modal%20candidate%20token%20elimination%20strategy%20into%20CKD%20to%0Aimprove%20tracking%20robustness%20and%20efficiency%20respectively.%20Extensive%20experiments%0Aon%20five%20standard%20RGBT%20tracking%20datasets%20validate%20the%20effectiveness%20of%20the%0Aproposed%20method%20against%20state-of-the-art%20methods%20while%20achieving%20the%20fastest%0Atracking%20speed%20of%2096.4%20FPS.%20Code%20available%20at%0Ahttps%3A//github.com/Multi-Modality-Tracking/CKD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Modality%2520Gap%2520in%2520RGBT%2520Tracking%253A%2520Coupled%2520Knowledge%2520Distillation%26entry.906535625%3DAndong%2520Lu%2520and%2520Jiacong%2520Zhao%2520and%2520Chenglong%2520Li%2520and%2520Yun%2520Xiao%2520and%2520Bin%2520Luo%26entry.1292438233%3D%2520%2520Modality%2520gap%2520between%2520RGB%2520and%2520thermal%2520infrared%2520%2528TIR%2529%2520images%2520is%2520a%2520crucial%2520issue%250Abut%2520often%2520overlooked%2520in%2520existing%2520RGBT%2520tracking%2520methods.%2520It%2520can%2520be%2520observed%2520that%250Amodality%2520gap%2520mainly%2520lies%2520in%2520the%2520image%2520style%2520difference.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520Coupled%2520Knowledge%2520Distillation%2520framework%2520called%2520CKD%252C%2520which%250Apursues%2520common%2520styles%2520of%2520different%2520modalities%2520to%2520break%2520modality%2520gap%252C%2520for%2520high%250Aperformance%2520RGBT%2520tracking.%2520In%2520particular%252C%2520we%2520introduce%2520two%2520student%2520networks%2520and%250Aemploy%2520the%2520style%2520distillation%2520loss%2520to%2520make%2520their%2520style%2520features%2520consistent%2520as%250Amuch%2520as%2520possible.%2520Through%2520alleviating%2520the%2520style%2520difference%2520of%2520two%2520student%250Anetworks%252C%2520we%2520can%2520break%2520modality%2520gap%2520of%2520different%2520modalities%2520well.%2520However%252C%2520the%250Adistillation%2520of%2520style%2520features%2520might%2520harm%2520to%2520the%2520content%2520representations%2520of%2520two%250Amodalities%2520in%2520student%2520networks.%2520To%2520handle%2520this%2520issue%252C%2520we%2520take%2520original%2520RGB%2520and%250ATIR%2520networks%2520as%2520the%2520teachers%252C%2520and%2520distill%2520their%2520content%2520knowledge%2520into%2520two%250Astudent%2520networks%2520respectively%2520by%2520the%2520style-content%2520orthogonal%2520feature%250Adecoupling%2520scheme.%2520We%2520couple%2520the%2520above%2520two%2520distillation%2520processes%2520in%2520an%2520online%250Aoptimization%2520framework%2520to%2520form%2520new%2520feature%2520representations%2520of%2520RGB%2520and%2520thermal%250Amodalities%2520without%2520modality%2520gap.%2520In%2520addition%252C%2520we%2520design%2520a%2520masked%2520modeling%250Astrategy%2520and%2520a%2520multi-modal%2520candidate%2520token%2520elimination%2520strategy%2520into%2520CKD%2520to%250Aimprove%2520tracking%2520robustness%2520and%2520efficiency%2520respectively.%2520Extensive%2520experiments%250Aon%2520five%2520standard%2520RGBT%2520tracking%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%2520against%2520state-of-the-art%2520methods%2520while%2520achieving%2520the%2520fastest%250Atracking%2520speed%2520of%252096.4%2520FPS.%2520Code%2520available%2520at%250Ahttps%253A//github.com/Multi-Modality-Tracking/CKD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Modality%20Gap%20in%20RGBT%20Tracking%3A%20Coupled%20Knowledge%20Distillation&entry.906535625=Andong%20Lu%20and%20Jiacong%20Zhao%20and%20Chenglong%20Li%20and%20Yun%20Xiao%20and%20Bin%20Luo&entry.1292438233=%20%20Modality%20gap%20between%20RGB%20and%20thermal%20infrared%20%28TIR%29%20images%20is%20a%20crucial%20issue%0Abut%20often%20overlooked%20in%20existing%20RGBT%20tracking%20methods.%20It%20can%20be%20observed%20that%0Amodality%20gap%20mainly%20lies%20in%20the%20image%20style%20difference.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20Coupled%20Knowledge%20Distillation%20framework%20called%20CKD%2C%20which%0Apursues%20common%20styles%20of%20different%20modalities%20to%20break%20modality%20gap%2C%20for%20high%0Aperformance%20RGBT%20tracking.%20In%20particular%2C%20we%20introduce%20two%20student%20networks%20and%0Aemploy%20the%20style%20distillation%20loss%20to%20make%20their%20style%20features%20consistent%20as%0Amuch%20as%20possible.%20Through%20alleviating%20the%20style%20difference%20of%20two%20student%0Anetworks%2C%20we%20can%20break%20modality%20gap%20of%20different%20modalities%20well.%20However%2C%20the%0Adistillation%20of%20style%20features%20might%20harm%20to%20the%20content%20representations%20of%20two%0Amodalities%20in%20student%20networks.%20To%20handle%20this%20issue%2C%20we%20take%20original%20RGB%20and%0ATIR%20networks%20as%20the%20teachers%2C%20and%20distill%20their%20content%20knowledge%20into%20two%0Astudent%20networks%20respectively%20by%20the%20style-content%20orthogonal%20feature%0Adecoupling%20scheme.%20We%20couple%20the%20above%20two%20distillation%20processes%20in%20an%20online%0Aoptimization%20framework%20to%20form%20new%20feature%20representations%20of%20RGB%20and%20thermal%0Amodalities%20without%20modality%20gap.%20In%20addition%2C%20we%20design%20a%20masked%20modeling%0Astrategy%20and%20a%20multi-modal%20candidate%20token%20elimination%20strategy%20into%20CKD%20to%0Aimprove%20tracking%20robustness%20and%20efficiency%20respectively.%20Extensive%20experiments%0Aon%20five%20standard%20RGBT%20tracking%20datasets%20validate%20the%20effectiveness%20of%20the%0Aproposed%20method%20against%20state-of-the-art%20methods%20while%20achieving%20the%20fastest%0Atracking%20speed%20of%2096.4%20FPS.%20Code%20available%20at%0Ahttps%3A//github.com/Multi-Modality-Tracking/CKD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11586v1&entry.124074799=Read"},
{"title": "ODES: Domain Adaptation with Expert Guidance for Online Medical Image\n  Segmentation", "author": "Md Shazid Islam and Sayak Nag and Arindam Dutta and Miraj Ahmed and Fahim Faisal Niloy and Amit K. Roy-Chowdhury", "abstract": "  Unsupervised domain adaptive segmentation typically relies on self-training\nusing pseudo labels predicted by a pre-trained network on an unlabeled target\ndataset. However, the noisy nature of such pseudo-labels presents a major\nbottleneck in adapting a network to the distribution shift between source and\ntarget datasets. This challenge is exaggerated when the network encounters an\nincoming data stream in online fashion, where the network is constrained to\nadapt to incoming streams of target domain data in exactly one round of forward\nand backward passes. In this scenario, relying solely on inaccurate\npseudo-labels can lead to low-quality segmentation, which is detrimental to\nmedical image analysis where accuracy and precision are of utmost priority. We\nhypothesize that a small amount of pixel-level annotation obtained from an\nexpert can address this problem, thereby enhancing the performance of domain\nadaptation of online streaming data, even in the absence of dedicated training\ndata. We call our method ODES: Domain Adaptation with Expert Guidance for\nOnline Medical Image Segmentation that adapts to each incoming data batch in an\nonline setup, incorporating feedback from an expert through active learning.\nThrough active learning, the most informative pixels in each image can be\nselected for expert annotation. However, the acquisition of pixel-level\nannotations across all images in a batch often leads to redundant information\nwhile increasing temporal overhead in online learning. To reduce the annotation\nacquisition time and make the adaptation process more online-friendly, we\nfurther propose a novel image-pruning strategy that selects the most useful\nsubset of images from the current batch for active learning. Our proposed\napproach outperforms existing online adaptation approaches and produces\ncompetitive results compared to offline domain adaptive active learning\nmethods.\n", "link": "http://arxiv.org/abs/2312.05407v2", "date": "2024-10-15", "relevancy": 2.1511, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%20Online%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%20Online%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Md%20Shazid%20Islam%20and%20Sayak%20Nag%20and%20Arindam%20Dutta%20and%20Miraj%20Ahmed%20and%20Fahim%20Faisal%20Niloy%20and%20Amit%20K.%20Roy-Chowdhury%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptive%20segmentation%20typically%20relies%20on%20self-training%0Ausing%20pseudo%20labels%20predicted%20by%20a%20pre-trained%20network%20on%20an%20unlabeled%20target%0Adataset.%20However%2C%20the%20noisy%20nature%20of%20such%20pseudo-labels%20presents%20a%20major%0Abottleneck%20in%20adapting%20a%20network%20to%20the%20distribution%20shift%20between%20source%20and%0Atarget%20datasets.%20This%20challenge%20is%20exaggerated%20when%20the%20network%20encounters%20an%0Aincoming%20data%20stream%20in%20online%20fashion%2C%20where%20the%20network%20is%20constrained%20to%0Aadapt%20to%20incoming%20streams%20of%20target%20domain%20data%20in%20exactly%20one%20round%20of%20forward%0Aand%20backward%20passes.%20In%20this%20scenario%2C%20relying%20solely%20on%20inaccurate%0Apseudo-labels%20can%20lead%20to%20low-quality%20segmentation%2C%20which%20is%20detrimental%20to%0Amedical%20image%20analysis%20where%20accuracy%20and%20precision%20are%20of%20utmost%20priority.%20We%0Ahypothesize%20that%20a%20small%20amount%20of%20pixel-level%20annotation%20obtained%20from%20an%0Aexpert%20can%20address%20this%20problem%2C%20thereby%20enhancing%20the%20performance%20of%20domain%0Aadaptation%20of%20online%20streaming%20data%2C%20even%20in%20the%20absence%20of%20dedicated%20training%0Adata.%20We%20call%20our%20method%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%0AOnline%20Medical%20Image%20Segmentation%20that%20adapts%20to%20each%20incoming%20data%20batch%20in%20an%0Aonline%20setup%2C%20incorporating%20feedback%20from%20an%20expert%20through%20active%20learning.%0AThrough%20active%20learning%2C%20the%20most%20informative%20pixels%20in%20each%20image%20can%20be%0Aselected%20for%20expert%20annotation.%20However%2C%20the%20acquisition%20of%20pixel-level%0Aannotations%20across%20all%20images%20in%20a%20batch%20often%20leads%20to%20redundant%20information%0Awhile%20increasing%20temporal%20overhead%20in%20online%20learning.%20To%20reduce%20the%20annotation%0Aacquisition%20time%20and%20make%20the%20adaptation%20process%20more%20online-friendly%2C%20we%0Afurther%20propose%20a%20novel%20image-pruning%20strategy%20that%20selects%20the%20most%20useful%0Asubset%20of%20images%20from%20the%20current%20batch%20for%20active%20learning.%20Our%20proposed%0Aapproach%20outperforms%20existing%20online%20adaptation%20approaches%20and%20produces%0Acompetitive%20results%20compared%20to%20offline%20domain%20adaptive%20active%20learning%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODES%253A%2520Domain%2520Adaptation%2520with%2520Expert%2520Guidance%2520for%2520Online%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DMd%2520Shazid%2520Islam%2520and%2520Sayak%2520Nag%2520and%2520Arindam%2520Dutta%2520and%2520Miraj%2520Ahmed%2520and%2520Fahim%2520Faisal%2520Niloy%2520and%2520Amit%2520K.%2520Roy-Chowdhury%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptive%2520segmentation%2520typically%2520relies%2520on%2520self-training%250Ausing%2520pseudo%2520labels%2520predicted%2520by%2520a%2520pre-trained%2520network%2520on%2520an%2520unlabeled%2520target%250Adataset.%2520However%252C%2520the%2520noisy%2520nature%2520of%2520such%2520pseudo-labels%2520presents%2520a%2520major%250Abottleneck%2520in%2520adapting%2520a%2520network%2520to%2520the%2520distribution%2520shift%2520between%2520source%2520and%250Atarget%2520datasets.%2520This%2520challenge%2520is%2520exaggerated%2520when%2520the%2520network%2520encounters%2520an%250Aincoming%2520data%2520stream%2520in%2520online%2520fashion%252C%2520where%2520the%2520network%2520is%2520constrained%2520to%250Aadapt%2520to%2520incoming%2520streams%2520of%2520target%2520domain%2520data%2520in%2520exactly%2520one%2520round%2520of%2520forward%250Aand%2520backward%2520passes.%2520In%2520this%2520scenario%252C%2520relying%2520solely%2520on%2520inaccurate%250Apseudo-labels%2520can%2520lead%2520to%2520low-quality%2520segmentation%252C%2520which%2520is%2520detrimental%2520to%250Amedical%2520image%2520analysis%2520where%2520accuracy%2520and%2520precision%2520are%2520of%2520utmost%2520priority.%2520We%250Ahypothesize%2520that%2520a%2520small%2520amount%2520of%2520pixel-level%2520annotation%2520obtained%2520from%2520an%250Aexpert%2520can%2520address%2520this%2520problem%252C%2520thereby%2520enhancing%2520the%2520performance%2520of%2520domain%250Aadaptation%2520of%2520online%2520streaming%2520data%252C%2520even%2520in%2520the%2520absence%2520of%2520dedicated%2520training%250Adata.%2520We%2520call%2520our%2520method%2520ODES%253A%2520Domain%2520Adaptation%2520with%2520Expert%2520Guidance%2520for%250AOnline%2520Medical%2520Image%2520Segmentation%2520that%2520adapts%2520to%2520each%2520incoming%2520data%2520batch%2520in%2520an%250Aonline%2520setup%252C%2520incorporating%2520feedback%2520from%2520an%2520expert%2520through%2520active%2520learning.%250AThrough%2520active%2520learning%252C%2520the%2520most%2520informative%2520pixels%2520in%2520each%2520image%2520can%2520be%250Aselected%2520for%2520expert%2520annotation.%2520However%252C%2520the%2520acquisition%2520of%2520pixel-level%250Aannotations%2520across%2520all%2520images%2520in%2520a%2520batch%2520often%2520leads%2520to%2520redundant%2520information%250Awhile%2520increasing%2520temporal%2520overhead%2520in%2520online%2520learning.%2520To%2520reduce%2520the%2520annotation%250Aacquisition%2520time%2520and%2520make%2520the%2520adaptation%2520process%2520more%2520online-friendly%252C%2520we%250Afurther%2520propose%2520a%2520novel%2520image-pruning%2520strategy%2520that%2520selects%2520the%2520most%2520useful%250Asubset%2520of%2520images%2520from%2520the%2520current%2520batch%2520for%2520active%2520learning.%2520Our%2520proposed%250Aapproach%2520outperforms%2520existing%2520online%2520adaptation%2520approaches%2520and%2520produces%250Acompetitive%2520results%2520compared%2520to%2520offline%2520domain%2520adaptive%2520active%2520learning%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%20Online%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Md%20Shazid%20Islam%20and%20Sayak%20Nag%20and%20Arindam%20Dutta%20and%20Miraj%20Ahmed%20and%20Fahim%20Faisal%20Niloy%20and%20Amit%20K.%20Roy-Chowdhury&entry.1292438233=%20%20Unsupervised%20domain%20adaptive%20segmentation%20typically%20relies%20on%20self-training%0Ausing%20pseudo%20labels%20predicted%20by%20a%20pre-trained%20network%20on%20an%20unlabeled%20target%0Adataset.%20However%2C%20the%20noisy%20nature%20of%20such%20pseudo-labels%20presents%20a%20major%0Abottleneck%20in%20adapting%20a%20network%20to%20the%20distribution%20shift%20between%20source%20and%0Atarget%20datasets.%20This%20challenge%20is%20exaggerated%20when%20the%20network%20encounters%20an%0Aincoming%20data%20stream%20in%20online%20fashion%2C%20where%20the%20network%20is%20constrained%20to%0Aadapt%20to%20incoming%20streams%20of%20target%20domain%20data%20in%20exactly%20one%20round%20of%20forward%0Aand%20backward%20passes.%20In%20this%20scenario%2C%20relying%20solely%20on%20inaccurate%0Apseudo-labels%20can%20lead%20to%20low-quality%20segmentation%2C%20which%20is%20detrimental%20to%0Amedical%20image%20analysis%20where%20accuracy%20and%20precision%20are%20of%20utmost%20priority.%20We%0Ahypothesize%20that%20a%20small%20amount%20of%20pixel-level%20annotation%20obtained%20from%20an%0Aexpert%20can%20address%20this%20problem%2C%20thereby%20enhancing%20the%20performance%20of%20domain%0Aadaptation%20of%20online%20streaming%20data%2C%20even%20in%20the%20absence%20of%20dedicated%20training%0Adata.%20We%20call%20our%20method%20ODES%3A%20Domain%20Adaptation%20with%20Expert%20Guidance%20for%0AOnline%20Medical%20Image%20Segmentation%20that%20adapts%20to%20each%20incoming%20data%20batch%20in%20an%0Aonline%20setup%2C%20incorporating%20feedback%20from%20an%20expert%20through%20active%20learning.%0AThrough%20active%20learning%2C%20the%20most%20informative%20pixels%20in%20each%20image%20can%20be%0Aselected%20for%20expert%20annotation.%20However%2C%20the%20acquisition%20of%20pixel-level%0Aannotations%20across%20all%20images%20in%20a%20batch%20often%20leads%20to%20redundant%20information%0Awhile%20increasing%20temporal%20overhead%20in%20online%20learning.%20To%20reduce%20the%20annotation%0Aacquisition%20time%20and%20make%20the%20adaptation%20process%20more%20online-friendly%2C%20we%0Afurther%20propose%20a%20novel%20image-pruning%20strategy%20that%20selects%20the%20most%20useful%0Asubset%20of%20images%20from%20the%20current%20batch%20for%20active%20learning.%20Our%20proposed%0Aapproach%20outperforms%20existing%20online%20adaptation%20approaches%20and%20produces%0Acompetitive%20results%20compared%20to%20offline%20domain%20adaptive%20active%20learning%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05407v2&entry.124074799=Read"},
{"title": "Self-Data Distillation for Recovering Quality in Pruned Large Language\n  Models", "author": "Vithursan Thangarasa and Ganesh Venkatesh and Nish Sinnadurai and Sean Lie", "abstract": "  Large language models have driven significant progress in natural language\nprocessing, but their deployment requires substantial compute and memory\nresources. As models scale, compression techniques become essential for\nbalancing model quality with computational efficiency. Structured pruning,\nwhich removes less critical components of the model, is a promising strategy\nfor reducing complexity. However, one-shot pruning often results in significant\nquality degradation, particularly in tasks requiring multi-step reasoning. To\nrecover lost quality, supervised fine-tuning (SFT) is commonly applied, but it\ncan lead to catastrophic forgetting by shifting the model's learned data\ndistribution. Therefore, addressing the degradation from both pruning and SFT\nis essential to preserve the original model's quality. In this work, we propose\nself-data distilled fine-tuning to address these challenges. Our approach\nleverages the original, unpruned model to generate a distilled dataset that\npreserves semantic richness and mitigates catastrophic forgetting by\nmaintaining alignment with the base model's knowledge. Empirically, we\ndemonstrate that self-data distillation consistently outperforms standard SFT,\nimproving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard\nv1. Specifically, when pruning 6 decoder blocks on Llama3.1-8B Instruct (i.e.,\n32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our\nmethod retains 91.2% of the original model's accuracy compared to 81.7% with\nSFT, while reducing real-world FLOPs by 16.30%. Furthermore, our approach\nscales effectively across datasets, with the quality improving as the dataset\nsize increases.\n", "link": "http://arxiv.org/abs/2410.09982v2", "date": "2024-10-15", "relevancy": 2.1491, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5593}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5457}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Data%20Distillation%20for%20Recovering%20Quality%20in%20Pruned%20Large%20Language%0A%20%20Models&body=Title%3A%20Self-Data%20Distillation%20for%20Recovering%20Quality%20in%20Pruned%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Vithursan%20Thangarasa%20and%20Ganesh%20Venkatesh%20and%20Nish%20Sinnadurai%20and%20Sean%20Lie%0AAbstract%3A%20%20%20Large%20language%20models%20have%20driven%20significant%20progress%20in%20natural%20language%0Aprocessing%2C%20but%20their%20deployment%20requires%20substantial%20compute%20and%20memory%0Aresources.%20As%20models%20scale%2C%20compression%20techniques%20become%20essential%20for%0Abalancing%20model%20quality%20with%20computational%20efficiency.%20Structured%20pruning%2C%0Awhich%20removes%20less%20critical%20components%20of%20the%20model%2C%20is%20a%20promising%20strategy%0Afor%20reducing%20complexity.%20However%2C%20one-shot%20pruning%20often%20results%20in%20significant%0Aquality%20degradation%2C%20particularly%20in%20tasks%20requiring%20multi-step%20reasoning.%20To%0Arecover%20lost%20quality%2C%20supervised%20fine-tuning%20%28SFT%29%20is%20commonly%20applied%2C%20but%20it%0Acan%20lead%20to%20catastrophic%20forgetting%20by%20shifting%20the%20model%27s%20learned%20data%0Adistribution.%20Therefore%2C%20addressing%20the%20degradation%20from%20both%20pruning%20and%20SFT%0Ais%20essential%20to%20preserve%20the%20original%20model%27s%20quality.%20In%20this%20work%2C%20we%20propose%0Aself-data%20distilled%20fine-tuning%20to%20address%20these%20challenges.%20Our%20approach%0Aleverages%20the%20original%2C%20unpruned%20model%20to%20generate%20a%20distilled%20dataset%20that%0Apreserves%20semantic%20richness%20and%20mitigates%20catastrophic%20forgetting%20by%0Amaintaining%20alignment%20with%20the%20base%20model%27s%20knowledge.%20Empirically%2C%20we%0Ademonstrate%20that%20self-data%20distillation%20consistently%20outperforms%20standard%20SFT%2C%0Aimproving%20average%20accuracy%20by%20up%20to%208%25%20on%20the%20HuggingFace%20OpenLLM%20Leaderboard%0Av1.%20Specifically%2C%20when%20pruning%206%20decoder%20blocks%20on%20Llama3.1-8B%20Instruct%20%28i.e.%2C%0A32%20to%2026%20layers%2C%20reducing%20the%20model%20size%20from%208.03B%20to%206.72B%20parameters%29%2C%20our%0Amethod%20retains%2091.2%25%20of%20the%20original%20model%27s%20accuracy%20compared%20to%2081.7%25%20with%0ASFT%2C%20while%20reducing%20real-world%20FLOPs%20by%2016.30%25.%20Furthermore%2C%20our%20approach%0Ascales%20effectively%20across%20datasets%2C%20with%20the%20quality%20improving%20as%20the%20dataset%0Asize%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Data%2520Distillation%2520for%2520Recovering%2520Quality%2520in%2520Pruned%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DVithursan%2520Thangarasa%2520and%2520Ganesh%2520Venkatesh%2520and%2520Nish%2520Sinnadurai%2520and%2520Sean%2520Lie%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520driven%2520significant%2520progress%2520in%2520natural%2520language%250Aprocessing%252C%2520but%2520their%2520deployment%2520requires%2520substantial%2520compute%2520and%2520memory%250Aresources.%2520As%2520models%2520scale%252C%2520compression%2520techniques%2520become%2520essential%2520for%250Abalancing%2520model%2520quality%2520with%2520computational%2520efficiency.%2520Structured%2520pruning%252C%250Awhich%2520removes%2520less%2520critical%2520components%2520of%2520the%2520model%252C%2520is%2520a%2520promising%2520strategy%250Afor%2520reducing%2520complexity.%2520However%252C%2520one-shot%2520pruning%2520often%2520results%2520in%2520significant%250Aquality%2520degradation%252C%2520particularly%2520in%2520tasks%2520requiring%2520multi-step%2520reasoning.%2520To%250Arecover%2520lost%2520quality%252C%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520is%2520commonly%2520applied%252C%2520but%2520it%250Acan%2520lead%2520to%2520catastrophic%2520forgetting%2520by%2520shifting%2520the%2520model%2527s%2520learned%2520data%250Adistribution.%2520Therefore%252C%2520addressing%2520the%2520degradation%2520from%2520both%2520pruning%2520and%2520SFT%250Ais%2520essential%2520to%2520preserve%2520the%2520original%2520model%2527s%2520quality.%2520In%2520this%2520work%252C%2520we%2520propose%250Aself-data%2520distilled%2520fine-tuning%2520to%2520address%2520these%2520challenges.%2520Our%2520approach%250Aleverages%2520the%2520original%252C%2520unpruned%2520model%2520to%2520generate%2520a%2520distilled%2520dataset%2520that%250Apreserves%2520semantic%2520richness%2520and%2520mitigates%2520catastrophic%2520forgetting%2520by%250Amaintaining%2520alignment%2520with%2520the%2520base%2520model%2527s%2520knowledge.%2520Empirically%252C%2520we%250Ademonstrate%2520that%2520self-data%2520distillation%2520consistently%2520outperforms%2520standard%2520SFT%252C%250Aimproving%2520average%2520accuracy%2520by%2520up%2520to%25208%2525%2520on%2520the%2520HuggingFace%2520OpenLLM%2520Leaderboard%250Av1.%2520Specifically%252C%2520when%2520pruning%25206%2520decoder%2520blocks%2520on%2520Llama3.1-8B%2520Instruct%2520%2528i.e.%252C%250A32%2520to%252026%2520layers%252C%2520reducing%2520the%2520model%2520size%2520from%25208.03B%2520to%25206.72B%2520parameters%2529%252C%2520our%250Amethod%2520retains%252091.2%2525%2520of%2520the%2520original%2520model%2527s%2520accuracy%2520compared%2520to%252081.7%2525%2520with%250ASFT%252C%2520while%2520reducing%2520real-world%2520FLOPs%2520by%252016.30%2525.%2520Furthermore%252C%2520our%2520approach%250Ascales%2520effectively%2520across%2520datasets%252C%2520with%2520the%2520quality%2520improving%2520as%2520the%2520dataset%250Asize%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Data%20Distillation%20for%20Recovering%20Quality%20in%20Pruned%20Large%20Language%0A%20%20Models&entry.906535625=Vithursan%20Thangarasa%20and%20Ganesh%20Venkatesh%20and%20Nish%20Sinnadurai%20and%20Sean%20Lie&entry.1292438233=%20%20Large%20language%20models%20have%20driven%20significant%20progress%20in%20natural%20language%0Aprocessing%2C%20but%20their%20deployment%20requires%20substantial%20compute%20and%20memory%0Aresources.%20As%20models%20scale%2C%20compression%20techniques%20become%20essential%20for%0Abalancing%20model%20quality%20with%20computational%20efficiency.%20Structured%20pruning%2C%0Awhich%20removes%20less%20critical%20components%20of%20the%20model%2C%20is%20a%20promising%20strategy%0Afor%20reducing%20complexity.%20However%2C%20one-shot%20pruning%20often%20results%20in%20significant%0Aquality%20degradation%2C%20particularly%20in%20tasks%20requiring%20multi-step%20reasoning.%20To%0Arecover%20lost%20quality%2C%20supervised%20fine-tuning%20%28SFT%29%20is%20commonly%20applied%2C%20but%20it%0Acan%20lead%20to%20catastrophic%20forgetting%20by%20shifting%20the%20model%27s%20learned%20data%0Adistribution.%20Therefore%2C%20addressing%20the%20degradation%20from%20both%20pruning%20and%20SFT%0Ais%20essential%20to%20preserve%20the%20original%20model%27s%20quality.%20In%20this%20work%2C%20we%20propose%0Aself-data%20distilled%20fine-tuning%20to%20address%20these%20challenges.%20Our%20approach%0Aleverages%20the%20original%2C%20unpruned%20model%20to%20generate%20a%20distilled%20dataset%20that%0Apreserves%20semantic%20richness%20and%20mitigates%20catastrophic%20forgetting%20by%0Amaintaining%20alignment%20with%20the%20base%20model%27s%20knowledge.%20Empirically%2C%20we%0Ademonstrate%20that%20self-data%20distillation%20consistently%20outperforms%20standard%20SFT%2C%0Aimproving%20average%20accuracy%20by%20up%20to%208%25%20on%20the%20HuggingFace%20OpenLLM%20Leaderboard%0Av1.%20Specifically%2C%20when%20pruning%206%20decoder%20blocks%20on%20Llama3.1-8B%20Instruct%20%28i.e.%2C%0A32%20to%2026%20layers%2C%20reducing%20the%20model%20size%20from%208.03B%20to%206.72B%20parameters%29%2C%20our%0Amethod%20retains%2091.2%25%20of%20the%20original%20model%27s%20accuracy%20compared%20to%2081.7%25%20with%0ASFT%2C%20while%20reducing%20real-world%20FLOPs%20by%2016.30%25.%20Furthermore%2C%20our%20approach%0Ascales%20effectively%20across%20datasets%2C%20with%20the%20quality%20improving%20as%20the%20dataset%0Asize%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09982v2&entry.124074799=Read"},
{"title": "Learning Truncated Causal History Model for Video Restoration", "author": "Amirhosein Ghasemabadi and Muhammad Kamran Janjua and Mohammad Salameh and Di Niu", "abstract": "  One key challenge to video restoration is to model the transition dynamics of\nvideo frames governed by motion. In this work, we propose TURTLE to learn the\ntruncated causal history model for efficient and high-performing video\nrestoration. Unlike traditional methods that process a range of contextual\nframes in parallel, TURTLE enhances efficiency by storing and summarizing a\ntruncated history of the input frame latent representation into an evolving\nhistorical state. This is achieved through a sophisticated similarity-based\nretrieval mechanism that implicitly accounts for inter-frame motion and\nalignment. The causal design in TURTLE enables recurrence in inference through\nstate-memorized historical features while allowing parallel training by\nsampling truncated video clips. We report new state-of-the-art results on a\nmultitude of video restoration benchmark tasks, including video desnowing,\nnighttime video deraining, video raindrops and rain streak removal, video\nsuper-resolution, real-world and synthetic video deblurring, and blind video\ndenoising while reducing the computational cost compared to existing best\ncontextual methods on all these tasks.\n", "link": "http://arxiv.org/abs/2410.03936v2", "date": "2024-10-15", "relevancy": 2.1453, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5801}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5579}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Truncated%20Causal%20History%20Model%20for%20Video%20Restoration&body=Title%3A%20Learning%20Truncated%20Causal%20History%20Model%20for%20Video%20Restoration%0AAuthor%3A%20Amirhosein%20Ghasemabadi%20and%20Muhammad%20Kamran%20Janjua%20and%20Mohammad%20Salameh%20and%20Di%20Niu%0AAbstract%3A%20%20%20One%20key%20challenge%20to%20video%20restoration%20is%20to%20model%20the%20transition%20dynamics%20of%0Avideo%20frames%20governed%20by%20motion.%20In%20this%20work%2C%20we%20propose%20TURTLE%20to%20learn%20the%0Atruncated%20causal%20history%20model%20for%20efficient%20and%20high-performing%20video%0Arestoration.%20Unlike%20traditional%20methods%20that%20process%20a%20range%20of%20contextual%0Aframes%20in%20parallel%2C%20TURTLE%20enhances%20efficiency%20by%20storing%20and%20summarizing%20a%0Atruncated%20history%20of%20the%20input%20frame%20latent%20representation%20into%20an%20evolving%0Ahistorical%20state.%20This%20is%20achieved%20through%20a%20sophisticated%20similarity-based%0Aretrieval%20mechanism%20that%20implicitly%20accounts%20for%20inter-frame%20motion%20and%0Aalignment.%20The%20causal%20design%20in%20TURTLE%20enables%20recurrence%20in%20inference%20through%0Astate-memorized%20historical%20features%20while%20allowing%20parallel%20training%20by%0Asampling%20truncated%20video%20clips.%20We%20report%20new%20state-of-the-art%20results%20on%20a%0Amultitude%20of%20video%20restoration%20benchmark%20tasks%2C%20including%20video%20desnowing%2C%0Anighttime%20video%20deraining%2C%20video%20raindrops%20and%20rain%20streak%20removal%2C%20video%0Asuper-resolution%2C%20real-world%20and%20synthetic%20video%20deblurring%2C%20and%20blind%20video%0Adenoising%20while%20reducing%20the%20computational%20cost%20compared%20to%20existing%20best%0Acontextual%20methods%20on%20all%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Truncated%2520Causal%2520History%2520Model%2520for%2520Video%2520Restoration%26entry.906535625%3DAmirhosein%2520Ghasemabadi%2520and%2520Muhammad%2520Kamran%2520Janjua%2520and%2520Mohammad%2520Salameh%2520and%2520Di%2520Niu%26entry.1292438233%3D%2520%2520One%2520key%2520challenge%2520to%2520video%2520restoration%2520is%2520to%2520model%2520the%2520transition%2520dynamics%2520of%250Avideo%2520frames%2520governed%2520by%2520motion.%2520In%2520this%2520work%252C%2520we%2520propose%2520TURTLE%2520to%2520learn%2520the%250Atruncated%2520causal%2520history%2520model%2520for%2520efficient%2520and%2520high-performing%2520video%250Arestoration.%2520Unlike%2520traditional%2520methods%2520that%2520process%2520a%2520range%2520of%2520contextual%250Aframes%2520in%2520parallel%252C%2520TURTLE%2520enhances%2520efficiency%2520by%2520storing%2520and%2520summarizing%2520a%250Atruncated%2520history%2520of%2520the%2520input%2520frame%2520latent%2520representation%2520into%2520an%2520evolving%250Ahistorical%2520state.%2520This%2520is%2520achieved%2520through%2520a%2520sophisticated%2520similarity-based%250Aretrieval%2520mechanism%2520that%2520implicitly%2520accounts%2520for%2520inter-frame%2520motion%2520and%250Aalignment.%2520The%2520causal%2520design%2520in%2520TURTLE%2520enables%2520recurrence%2520in%2520inference%2520through%250Astate-memorized%2520historical%2520features%2520while%2520allowing%2520parallel%2520training%2520by%250Asampling%2520truncated%2520video%2520clips.%2520We%2520report%2520new%2520state-of-the-art%2520results%2520on%2520a%250Amultitude%2520of%2520video%2520restoration%2520benchmark%2520tasks%252C%2520including%2520video%2520desnowing%252C%250Anighttime%2520video%2520deraining%252C%2520video%2520raindrops%2520and%2520rain%2520streak%2520removal%252C%2520video%250Asuper-resolution%252C%2520real-world%2520and%2520synthetic%2520video%2520deblurring%252C%2520and%2520blind%2520video%250Adenoising%2520while%2520reducing%2520the%2520computational%2520cost%2520compared%2520to%2520existing%2520best%250Acontextual%2520methods%2520on%2520all%2520these%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Truncated%20Causal%20History%20Model%20for%20Video%20Restoration&entry.906535625=Amirhosein%20Ghasemabadi%20and%20Muhammad%20Kamran%20Janjua%20and%20Mohammad%20Salameh%20and%20Di%20Niu&entry.1292438233=%20%20One%20key%20challenge%20to%20video%20restoration%20is%20to%20model%20the%20transition%20dynamics%20of%0Avideo%20frames%20governed%20by%20motion.%20In%20this%20work%2C%20we%20propose%20TURTLE%20to%20learn%20the%0Atruncated%20causal%20history%20model%20for%20efficient%20and%20high-performing%20video%0Arestoration.%20Unlike%20traditional%20methods%20that%20process%20a%20range%20of%20contextual%0Aframes%20in%20parallel%2C%20TURTLE%20enhances%20efficiency%20by%20storing%20and%20summarizing%20a%0Atruncated%20history%20of%20the%20input%20frame%20latent%20representation%20into%20an%20evolving%0Ahistorical%20state.%20This%20is%20achieved%20through%20a%20sophisticated%20similarity-based%0Aretrieval%20mechanism%20that%20implicitly%20accounts%20for%20inter-frame%20motion%20and%0Aalignment.%20The%20causal%20design%20in%20TURTLE%20enables%20recurrence%20in%20inference%20through%0Astate-memorized%20historical%20features%20while%20allowing%20parallel%20training%20by%0Asampling%20truncated%20video%20clips.%20We%20report%20new%20state-of-the-art%20results%20on%20a%0Amultitude%20of%20video%20restoration%20benchmark%20tasks%2C%20including%20video%20desnowing%2C%0Anighttime%20video%20deraining%2C%20video%20raindrops%20and%20rain%20streak%20removal%2C%20video%0Asuper-resolution%2C%20real-world%20and%20synthetic%20video%20deblurring%2C%20and%20blind%20video%0Adenoising%20while%20reducing%20the%20computational%20cost%20compared%20to%20existing%20best%0Acontextual%20methods%20on%20all%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03936v2&entry.124074799=Read"},
{"title": "AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement\n  Learning", "author": "Subhojeet Pramanik and Esraa Elelimy and Marlos C. Machado and Adam White", "abstract": "  In this paper we investigate transformer architectures designed for partially\nobservable online reinforcement learning. The self-attention mechanism in the\ntransformer architecture is capable of capturing long-range dependencies and it\nis the main reason behind its effectiveness in processing sequential data.\nNevertheless, despite their success, transformers have two significant\ndrawbacks that still limit their applicability in online reinforcement\nlearning: (1) in order to remember all past information, the self-attention\nmechanism requires access to the whole history to be provided as context. (2)\nThe inference cost in transformers is expensive. In this paper, we introduce\nrecurrent alternatives to the transformer self-attention mechanism that offer\ncontext-independent inference cost, leverage long-range dependencies\neffectively, and performs well in online reinforcement learning task. We\nquantify the impact of the different components of our architecture in a\ndiagnostic environment and assess performance gains in 2D and 3D pixel-based\npartially-observable environments (e.g. T-Maze, Mystery Path, Craftax, and\nMemory Maze). Compared with a state-of-the-art architecture, GTrXL, inference\nin our approach is at least 40% cheaper while reducing memory use more than\n50%. Our approach either performs similarly or better than GTrXL, improving\nmore than 37% upon GTrXL performance in harder tasks.\n", "link": "http://arxiv.org/abs/2310.15719v2", "date": "2024-10-15", "relevancy": 2.1412, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5753}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5281}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGaLiTe%3A%20Approximate%20Gated%20Linear%20Transformers%20for%20Online%20Reinforcement%0A%20%20Learning&body=Title%3A%20AGaLiTe%3A%20Approximate%20Gated%20Linear%20Transformers%20for%20Online%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Subhojeet%20Pramanik%20and%20Esraa%20Elelimy%20and%20Marlos%20C.%20Machado%20and%20Adam%20White%0AAbstract%3A%20%20%20In%20this%20paper%20we%20investigate%20transformer%20architectures%20designed%20for%20partially%0Aobservable%20online%20reinforcement%20learning.%20The%20self-attention%20mechanism%20in%20the%0Atransformer%20architecture%20is%20capable%20of%20capturing%20long-range%20dependencies%20and%20it%0Ais%20the%20main%20reason%20behind%20its%20effectiveness%20in%20processing%20sequential%20data.%0ANevertheless%2C%20despite%20their%20success%2C%20transformers%20have%20two%20significant%0Adrawbacks%20that%20still%20limit%20their%20applicability%20in%20online%20reinforcement%0Alearning%3A%20%281%29%20in%20order%20to%20remember%20all%20past%20information%2C%20the%20self-attention%0Amechanism%20requires%20access%20to%20the%20whole%20history%20to%20be%20provided%20as%20context.%20%282%29%0AThe%20inference%20cost%20in%20transformers%20is%20expensive.%20In%20this%20paper%2C%20we%20introduce%0Arecurrent%20alternatives%20to%20the%20transformer%20self-attention%20mechanism%20that%20offer%0Acontext-independent%20inference%20cost%2C%20leverage%20long-range%20dependencies%0Aeffectively%2C%20and%20performs%20well%20in%20online%20reinforcement%20learning%20task.%20We%0Aquantify%20the%20impact%20of%20the%20different%20components%20of%20our%20architecture%20in%20a%0Adiagnostic%20environment%20and%20assess%20performance%20gains%20in%202D%20and%203D%20pixel-based%0Apartially-observable%20environments%20%28e.g.%20T-Maze%2C%20Mystery%20Path%2C%20Craftax%2C%20and%0AMemory%20Maze%29.%20Compared%20with%20a%20state-of-the-art%20architecture%2C%20GTrXL%2C%20inference%0Ain%20our%20approach%20is%20at%20least%2040%25%20cheaper%20while%20reducing%20memory%20use%20more%20than%0A50%25.%20Our%20approach%20either%20performs%20similarly%20or%20better%20than%20GTrXL%2C%20improving%0Amore%20than%2037%25%20upon%20GTrXL%20performance%20in%20harder%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.15719v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGaLiTe%253A%2520Approximate%2520Gated%2520Linear%2520Transformers%2520for%2520Online%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DSubhojeet%2520Pramanik%2520and%2520Esraa%2520Elelimy%2520and%2520Marlos%2520C.%2520Machado%2520and%2520Adam%2520White%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520investigate%2520transformer%2520architectures%2520designed%2520for%2520partially%250Aobservable%2520online%2520reinforcement%2520learning.%2520The%2520self-attention%2520mechanism%2520in%2520the%250Atransformer%2520architecture%2520is%2520capable%2520of%2520capturing%2520long-range%2520dependencies%2520and%2520it%250Ais%2520the%2520main%2520reason%2520behind%2520its%2520effectiveness%2520in%2520processing%2520sequential%2520data.%250ANevertheless%252C%2520despite%2520their%2520success%252C%2520transformers%2520have%2520two%2520significant%250Adrawbacks%2520that%2520still%2520limit%2520their%2520applicability%2520in%2520online%2520reinforcement%250Alearning%253A%2520%25281%2529%2520in%2520order%2520to%2520remember%2520all%2520past%2520information%252C%2520the%2520self-attention%250Amechanism%2520requires%2520access%2520to%2520the%2520whole%2520history%2520to%2520be%2520provided%2520as%2520context.%2520%25282%2529%250AThe%2520inference%2520cost%2520in%2520transformers%2520is%2520expensive.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Arecurrent%2520alternatives%2520to%2520the%2520transformer%2520self-attention%2520mechanism%2520that%2520offer%250Acontext-independent%2520inference%2520cost%252C%2520leverage%2520long-range%2520dependencies%250Aeffectively%252C%2520and%2520performs%2520well%2520in%2520online%2520reinforcement%2520learning%2520task.%2520We%250Aquantify%2520the%2520impact%2520of%2520the%2520different%2520components%2520of%2520our%2520architecture%2520in%2520a%250Adiagnostic%2520environment%2520and%2520assess%2520performance%2520gains%2520in%25202D%2520and%25203D%2520pixel-based%250Apartially-observable%2520environments%2520%2528e.g.%2520T-Maze%252C%2520Mystery%2520Path%252C%2520Craftax%252C%2520and%250AMemory%2520Maze%2529.%2520Compared%2520with%2520a%2520state-of-the-art%2520architecture%252C%2520GTrXL%252C%2520inference%250Ain%2520our%2520approach%2520is%2520at%2520least%252040%2525%2520cheaper%2520while%2520reducing%2520memory%2520use%2520more%2520than%250A50%2525.%2520Our%2520approach%2520either%2520performs%2520similarly%2520or%2520better%2520than%2520GTrXL%252C%2520improving%250Amore%2520than%252037%2525%2520upon%2520GTrXL%2520performance%2520in%2520harder%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.15719v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGaLiTe%3A%20Approximate%20Gated%20Linear%20Transformers%20for%20Online%20Reinforcement%0A%20%20Learning&entry.906535625=Subhojeet%20Pramanik%20and%20Esraa%20Elelimy%20and%20Marlos%20C.%20Machado%20and%20Adam%20White&entry.1292438233=%20%20In%20this%20paper%20we%20investigate%20transformer%20architectures%20designed%20for%20partially%0Aobservable%20online%20reinforcement%20learning.%20The%20self-attention%20mechanism%20in%20the%0Atransformer%20architecture%20is%20capable%20of%20capturing%20long-range%20dependencies%20and%20it%0Ais%20the%20main%20reason%20behind%20its%20effectiveness%20in%20processing%20sequential%20data.%0ANevertheless%2C%20despite%20their%20success%2C%20transformers%20have%20two%20significant%0Adrawbacks%20that%20still%20limit%20their%20applicability%20in%20online%20reinforcement%0Alearning%3A%20%281%29%20in%20order%20to%20remember%20all%20past%20information%2C%20the%20self-attention%0Amechanism%20requires%20access%20to%20the%20whole%20history%20to%20be%20provided%20as%20context.%20%282%29%0AThe%20inference%20cost%20in%20transformers%20is%20expensive.%20In%20this%20paper%2C%20we%20introduce%0Arecurrent%20alternatives%20to%20the%20transformer%20self-attention%20mechanism%20that%20offer%0Acontext-independent%20inference%20cost%2C%20leverage%20long-range%20dependencies%0Aeffectively%2C%20and%20performs%20well%20in%20online%20reinforcement%20learning%20task.%20We%0Aquantify%20the%20impact%20of%20the%20different%20components%20of%20our%20architecture%20in%20a%0Adiagnostic%20environment%20and%20assess%20performance%20gains%20in%202D%20and%203D%20pixel-based%0Apartially-observable%20environments%20%28e.g.%20T-Maze%2C%20Mystery%20Path%2C%20Craftax%2C%20and%0AMemory%20Maze%29.%20Compared%20with%20a%20state-of-the-art%20architecture%2C%20GTrXL%2C%20inference%0Ain%20our%20approach%20is%20at%20least%2040%25%20cheaper%20while%20reducing%20memory%20use%20more%20than%0A50%25.%20Our%20approach%20either%20performs%20similarly%20or%20better%20than%20GTrXL%2C%20improving%0Amore%20than%2037%25%20upon%20GTrXL%20performance%20in%20harder%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.15719v2&entry.124074799=Read"},
{"title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge", "author": "Nico Wagner and Michael Desmond and Rahul Nair and Zahra Ashktorab and Elizabeth M. Daly and Qian Pan and Mart\u00edn Santill\u00e1n Cooper and James M. Johnson and Werner Geyer", "abstract": "  LLM-as-a-Judge is a widely used method for evaluating the performance of\nLarge Language Models (LLMs) across various tasks. We address the challenge of\nquantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty\nquantification has been well-studied in other domains, applying it effectively\nto LLMs poses unique challenges due to their complex decision-making\ncapabilities and computational demands. In this paper, we introduce a novel\nmethod for quantifying uncertainty designed to enhance the trustworthiness of\nLLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the\nrelationships between generated assessments and possible ratings. By\ncross-evaluating these relationships and constructing a confusion matrix based\non token probabilities, the method derives labels of high or low uncertainty.\nWe evaluate our method across multiple benchmarks, demonstrating a strong\ncorrelation between the accuracy of LLM evaluations and the derived uncertainty\nscores. Our findings suggest that this method can significantly improve the\nreliability and consistency of LLM-as-a-Judge evaluations.\n", "link": "http://arxiv.org/abs/2410.11594v1", "date": "2024-10-15", "relevancy": 2.1313, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5997}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Black-box%20Uncertainty%20Quantification%20Method%20for%20LLM-as-a-Judge&body=Title%3A%20Black-box%20Uncertainty%20Quantification%20Method%20for%20LLM-as-a-Judge%0AAuthor%3A%20Nico%20Wagner%20and%20Michael%20Desmond%20and%20Rahul%20Nair%20and%20Zahra%20Ashktorab%20and%20Elizabeth%20M.%20Daly%20and%20Qian%20Pan%20and%20Mart%C3%ADn%20Santill%C3%A1n%20Cooper%20and%20James%20M.%20Johnson%20and%20Werner%20Geyer%0AAbstract%3A%20%20%20LLM-as-a-Judge%20is%20a%20widely%20used%20method%20for%20evaluating%20the%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%20across%20various%20tasks.%20We%20address%20the%20challenge%20of%0Aquantifying%20the%20uncertainty%20of%20LLM-as-a-Judge%20evaluations.%20While%20uncertainty%0Aquantification%20has%20been%20well-studied%20in%20other%20domains%2C%20applying%20it%20effectively%0Ato%20LLMs%20poses%20unique%20challenges%20due%20to%20their%20complex%20decision-making%0Acapabilities%20and%20computational%20demands.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amethod%20for%20quantifying%20uncertainty%20designed%20to%20enhance%20the%20trustworthiness%20of%0ALLM-as-a-Judge%20evaluations.%20The%20method%20quantifies%20uncertainty%20by%20analyzing%20the%0Arelationships%20between%20generated%20assessments%20and%20possible%20ratings.%20By%0Across-evaluating%20these%20relationships%20and%20constructing%20a%20confusion%20matrix%20based%0Aon%20token%20probabilities%2C%20the%20method%20derives%20labels%20of%20high%20or%20low%20uncertainty.%0AWe%20evaluate%20our%20method%20across%20multiple%20benchmarks%2C%20demonstrating%20a%20strong%0Acorrelation%20between%20the%20accuracy%20of%20LLM%20evaluations%20and%20the%20derived%20uncertainty%0Ascores.%20Our%20findings%20suggest%20that%20this%20method%20can%20significantly%20improve%20the%0Areliability%20and%20consistency%20of%20LLM-as-a-Judge%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlack-box%2520Uncertainty%2520Quantification%2520Method%2520for%2520LLM-as-a-Judge%26entry.906535625%3DNico%2520Wagner%2520and%2520Michael%2520Desmond%2520and%2520Rahul%2520Nair%2520and%2520Zahra%2520Ashktorab%2520and%2520Elizabeth%2520M.%2520Daly%2520and%2520Qian%2520Pan%2520and%2520Mart%25C3%25ADn%2520Santill%25C3%25A1n%2520Cooper%2520and%2520James%2520M.%2520Johnson%2520and%2520Werner%2520Geyer%26entry.1292438233%3D%2520%2520LLM-as-a-Judge%2520is%2520a%2520widely%2520used%2520method%2520for%2520evaluating%2520the%2520performance%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520various%2520tasks.%2520We%2520address%2520the%2520challenge%2520of%250Aquantifying%2520the%2520uncertainty%2520of%2520LLM-as-a-Judge%2520evaluations.%2520While%2520uncertainty%250Aquantification%2520has%2520been%2520well-studied%2520in%2520other%2520domains%252C%2520applying%2520it%2520effectively%250Ato%2520LLMs%2520poses%2520unique%2520challenges%2520due%2520to%2520their%2520complex%2520decision-making%250Acapabilities%2520and%2520computational%2520demands.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Amethod%2520for%2520quantifying%2520uncertainty%2520designed%2520to%2520enhance%2520the%2520trustworthiness%2520of%250ALLM-as-a-Judge%2520evaluations.%2520The%2520method%2520quantifies%2520uncertainty%2520by%2520analyzing%2520the%250Arelationships%2520between%2520generated%2520assessments%2520and%2520possible%2520ratings.%2520By%250Across-evaluating%2520these%2520relationships%2520and%2520constructing%2520a%2520confusion%2520matrix%2520based%250Aon%2520token%2520probabilities%252C%2520the%2520method%2520derives%2520labels%2520of%2520high%2520or%2520low%2520uncertainty.%250AWe%2520evaluate%2520our%2520method%2520across%2520multiple%2520benchmarks%252C%2520demonstrating%2520a%2520strong%250Acorrelation%2520between%2520the%2520accuracy%2520of%2520LLM%2520evaluations%2520and%2520the%2520derived%2520uncertainty%250Ascores.%2520Our%2520findings%2520suggest%2520that%2520this%2520method%2520can%2520significantly%2520improve%2520the%250Areliability%2520and%2520consistency%2520of%2520LLM-as-a-Judge%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Black-box%20Uncertainty%20Quantification%20Method%20for%20LLM-as-a-Judge&entry.906535625=Nico%20Wagner%20and%20Michael%20Desmond%20and%20Rahul%20Nair%20and%20Zahra%20Ashktorab%20and%20Elizabeth%20M.%20Daly%20and%20Qian%20Pan%20and%20Mart%C3%ADn%20Santill%C3%A1n%20Cooper%20and%20James%20M.%20Johnson%20and%20Werner%20Geyer&entry.1292438233=%20%20LLM-as-a-Judge%20is%20a%20widely%20used%20method%20for%20evaluating%20the%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%20across%20various%20tasks.%20We%20address%20the%20challenge%20of%0Aquantifying%20the%20uncertainty%20of%20LLM-as-a-Judge%20evaluations.%20While%20uncertainty%0Aquantification%20has%20been%20well-studied%20in%20other%20domains%2C%20applying%20it%20effectively%0Ato%20LLMs%20poses%20unique%20challenges%20due%20to%20their%20complex%20decision-making%0Acapabilities%20and%20computational%20demands.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amethod%20for%20quantifying%20uncertainty%20designed%20to%20enhance%20the%20trustworthiness%20of%0ALLM-as-a-Judge%20evaluations.%20The%20method%20quantifies%20uncertainty%20by%20analyzing%20the%0Arelationships%20between%20generated%20assessments%20and%20possible%20ratings.%20By%0Across-evaluating%20these%20relationships%20and%20constructing%20a%20confusion%20matrix%20based%0Aon%20token%20probabilities%2C%20the%20method%20derives%20labels%20of%20high%20or%20low%20uncertainty.%0AWe%20evaluate%20our%20method%20across%20multiple%20benchmarks%2C%20demonstrating%20a%20strong%0Acorrelation%20between%20the%20accuracy%20of%20LLM%20evaluations%20and%20the%20derived%20uncertainty%0Ascores.%20Our%20findings%20suggest%20that%20this%20method%20can%20significantly%20improve%20the%0Areliability%20and%20consistency%20of%20LLM-as-a-Judge%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11594v1&entry.124074799=Read"},
{"title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation", "author": "Chenxi Wang and Xiang Chen and Ningyu Zhang and Bozhong Tian and Haoming Xu and Shumin Deng and Huajun Chen", "abstract": "  Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs (DeCo), which adaptively selects\nthe appropriate preceding layers and proportionally integrates knowledge into\nthe final layer to adjust the output logits. Note that DeCo is model agnostic\nand can be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.\n", "link": "http://arxiv.org/abs/2410.11779v1", "date": "2024-10-15", "relevancy": 2.1306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLLM%20can%20see%3F%20Dynamic%20Correction%20Decoding%20for%20Hallucination%20Mitigation&body=Title%3A%20MLLM%20can%20see%3F%20Dynamic%20Correction%20Decoding%20for%20Hallucination%20Mitigation%0AAuthor%3A%20Chenxi%20Wang%20and%20Xiang%20Chen%20and%20Ningyu%20Zhang%20and%20Bozhong%20Tian%20and%20Haoming%20Xu%20and%20Shumin%20Deng%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20frequently%20exhibit%20hallucination%0Aphenomena%2C%20but%20the%20underlying%20reasons%20remain%20poorly%20understood.%20In%20this%20paper%2C%0Awe%20present%20an%20empirical%20analysis%20and%20find%20that%2C%20although%20MLLMs%20incorrectly%0Agenerate%20the%20objects%20in%20the%20final%20output%2C%20they%20are%20actually%20able%20to%20recognize%0Avisual%20objects%20in%20the%20preceding%20layers.%20We%20speculate%20that%20this%20may%20be%20due%20to%0Athe%20strong%20knowledge%20priors%20of%20the%20language%20model%20suppressing%20the%20visual%0Ainformation%2C%20leading%20to%20hallucinations.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%0Adynamic%20correction%20decoding%20method%20for%20MLLMs%20%28DeCo%29%2C%20which%20adaptively%20selects%0Athe%20appropriate%20preceding%20layers%20and%20proportionally%20integrates%20knowledge%20into%0Athe%20final%20layer%20to%20adjust%20the%20output%20logits.%20Note%20that%20DeCo%20is%20model%20agnostic%0Aand%20can%20be%20seamlessly%20incorporated%20with%20various%20classic%20decoding%20strategies%20and%0Aapplied%20to%20different%20MLLMs.%20We%20evaluate%20DeCo%20on%20widely-used%20benchmarks%2C%0Ademonstrating%20that%20it%20can%20reduce%20hallucination%20rates%20by%20a%20large%20margin%20compared%0Ato%20baselines%2C%20highlighting%20its%20potential%20to%20mitigate%20hallucinations.%20Code%20is%0Aavailable%20at%20https%3A//github.com/zjunlp/DeCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLLM%2520can%2520see%253F%2520Dynamic%2520Correction%2520Decoding%2520for%2520Hallucination%2520Mitigation%26entry.906535625%3DChenxi%2520Wang%2520and%2520Xiang%2520Chen%2520and%2520Ningyu%2520Zhang%2520and%2520Bozhong%2520Tian%2520and%2520Haoming%2520Xu%2520and%2520Shumin%2520Deng%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520frequently%2520exhibit%2520hallucination%250Aphenomena%252C%2520but%2520the%2520underlying%2520reasons%2520remain%2520poorly%2520understood.%2520In%2520this%2520paper%252C%250Awe%2520present%2520an%2520empirical%2520analysis%2520and%2520find%2520that%252C%2520although%2520MLLMs%2520incorrectly%250Agenerate%2520the%2520objects%2520in%2520the%2520final%2520output%252C%2520they%2520are%2520actually%2520able%2520to%2520recognize%250Avisual%2520objects%2520in%2520the%2520preceding%2520layers.%2520We%2520speculate%2520that%2520this%2520may%2520be%2520due%2520to%250Athe%2520strong%2520knowledge%2520priors%2520of%2520the%2520language%2520model%2520suppressing%2520the%2520visual%250Ainformation%252C%2520leading%2520to%2520hallucinations.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520novel%250Adynamic%2520correction%2520decoding%2520method%2520for%2520MLLMs%2520%2528DeCo%2529%252C%2520which%2520adaptively%2520selects%250Athe%2520appropriate%2520preceding%2520layers%2520and%2520proportionally%2520integrates%2520knowledge%2520into%250Athe%2520final%2520layer%2520to%2520adjust%2520the%2520output%2520logits.%2520Note%2520that%2520DeCo%2520is%2520model%2520agnostic%250Aand%2520can%2520be%2520seamlessly%2520incorporated%2520with%2520various%2520classic%2520decoding%2520strategies%2520and%250Aapplied%2520to%2520different%2520MLLMs.%2520We%2520evaluate%2520DeCo%2520on%2520widely-used%2520benchmarks%252C%250Ademonstrating%2520that%2520it%2520can%2520reduce%2520hallucination%2520rates%2520by%2520a%2520large%2520margin%2520compared%250Ato%2520baselines%252C%2520highlighting%2520its%2520potential%2520to%2520mitigate%2520hallucinations.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/zjunlp/DeCo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLLM%20can%20see%3F%20Dynamic%20Correction%20Decoding%20for%20Hallucination%20Mitigation&entry.906535625=Chenxi%20Wang%20and%20Xiang%20Chen%20and%20Ningyu%20Zhang%20and%20Bozhong%20Tian%20and%20Haoming%20Xu%20and%20Shumin%20Deng%20and%20Huajun%20Chen&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20frequently%20exhibit%20hallucination%0Aphenomena%2C%20but%20the%20underlying%20reasons%20remain%20poorly%20understood.%20In%20this%20paper%2C%0Awe%20present%20an%20empirical%20analysis%20and%20find%20that%2C%20although%20MLLMs%20incorrectly%0Agenerate%20the%20objects%20in%20the%20final%20output%2C%20they%20are%20actually%20able%20to%20recognize%0Avisual%20objects%20in%20the%20preceding%20layers.%20We%20speculate%20that%20this%20may%20be%20due%20to%0Athe%20strong%20knowledge%20priors%20of%20the%20language%20model%20suppressing%20the%20visual%0Ainformation%2C%20leading%20to%20hallucinations.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%0Adynamic%20correction%20decoding%20method%20for%20MLLMs%20%28DeCo%29%2C%20which%20adaptively%20selects%0Athe%20appropriate%20preceding%20layers%20and%20proportionally%20integrates%20knowledge%20into%0Athe%20final%20layer%20to%20adjust%20the%20output%20logits.%20Note%20that%20DeCo%20is%20model%20agnostic%0Aand%20can%20be%20seamlessly%20incorporated%20with%20various%20classic%20decoding%20strategies%20and%0Aapplied%20to%20different%20MLLMs.%20We%20evaluate%20DeCo%20on%20widely-used%20benchmarks%2C%0Ademonstrating%20that%20it%20can%20reduce%20hallucination%20rates%20by%20a%20large%20margin%20compared%0Ato%20baselines%2C%20highlighting%20its%20potential%20to%20mitigate%20hallucinations.%20Code%20is%0Aavailable%20at%20https%3A//github.com/zjunlp/DeCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11779v1&entry.124074799=Read"},
{"title": "Feature-guided score diffusion for sampling conditional densities", "author": "Zahra Kadkhodaie and St\u00e9phane Mallat and Eero P. Simoncelli", "abstract": "  Score diffusion methods can learn probability densities from samples. The\nscore of the noise-corrupted density is estimated using a deep neural network,\nwhich is then used to iteratively transport a Gaussian white noise density to a\ntarget density. Variants for conditional densities have been developed, but\ncorrect estimation of the corresponding scores is difficult. We avoid these\ndifficulties by introducing an algorithm that guides the diffusion with a\nprojected score. The projection pushes the image feature vector towards the\nfeature vector centroid of the target class. The projected score and the\nfeature vectors are learned by the same network. Specifically, the image\nfeature vector is defined as the spatial averages of the channels activations\nin select layers of the network. Optimizing the projected score for denoising\nloss encourages image feature vectors of each class to cluster around their\ncentroids. It also leads to the separations of the centroids. We show that\nthese centroids provide a low-dimensional Euclidean embedding of the class\nconditional densities. We demonstrate that the algorithm can generate high\nquality and diverse samples from the conditioning class. Conditional generation\ncan be performed using feature vectors interpolated between those of the\ntraining set, demonstrating out-of-distribution generalization.\n", "link": "http://arxiv.org/abs/2410.11646v1", "date": "2024-10-15", "relevancy": 2.1172, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5784}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5261}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-guided%20score%20diffusion%20for%20sampling%20conditional%20densities&body=Title%3A%20Feature-guided%20score%20diffusion%20for%20sampling%20conditional%20densities%0AAuthor%3A%20Zahra%20Kadkhodaie%20and%20St%C3%A9phane%20Mallat%20and%20Eero%20P.%20Simoncelli%0AAbstract%3A%20%20%20Score%20diffusion%20methods%20can%20learn%20probability%20densities%20from%20samples.%20The%0Ascore%20of%20the%20noise-corrupted%20density%20is%20estimated%20using%20a%20deep%20neural%20network%2C%0Awhich%20is%20then%20used%20to%20iteratively%20transport%20a%20Gaussian%20white%20noise%20density%20to%20a%0Atarget%20density.%20Variants%20for%20conditional%20densities%20have%20been%20developed%2C%20but%0Acorrect%20estimation%20of%20the%20corresponding%20scores%20is%20difficult.%20We%20avoid%20these%0Adifficulties%20by%20introducing%20an%20algorithm%20that%20guides%20the%20diffusion%20with%20a%0Aprojected%20score.%20The%20projection%20pushes%20the%20image%20feature%20vector%20towards%20the%0Afeature%20vector%20centroid%20of%20the%20target%20class.%20The%20projected%20score%20and%20the%0Afeature%20vectors%20are%20learned%20by%20the%20same%20network.%20Specifically%2C%20the%20image%0Afeature%20vector%20is%20defined%20as%20the%20spatial%20averages%20of%20the%20channels%20activations%0Ain%20select%20layers%20of%20the%20network.%20Optimizing%20the%20projected%20score%20for%20denoising%0Aloss%20encourages%20image%20feature%20vectors%20of%20each%20class%20to%20cluster%20around%20their%0Acentroids.%20It%20also%20leads%20to%20the%20separations%20of%20the%20centroids.%20We%20show%20that%0Athese%20centroids%20provide%20a%20low-dimensional%20Euclidean%20embedding%20of%20the%20class%0Aconditional%20densities.%20We%20demonstrate%20that%20the%20algorithm%20can%20generate%20high%0Aquality%20and%20diverse%20samples%20from%20the%20conditioning%20class.%20Conditional%20generation%0Acan%20be%20performed%20using%20feature%20vectors%20interpolated%20between%20those%20of%20the%0Atraining%20set%2C%20demonstrating%20out-of-distribution%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-guided%2520score%2520diffusion%2520for%2520sampling%2520conditional%2520densities%26entry.906535625%3DZahra%2520Kadkhodaie%2520and%2520St%25C3%25A9phane%2520Mallat%2520and%2520Eero%2520P.%2520Simoncelli%26entry.1292438233%3D%2520%2520Score%2520diffusion%2520methods%2520can%2520learn%2520probability%2520densities%2520from%2520samples.%2520The%250Ascore%2520of%2520the%2520noise-corrupted%2520density%2520is%2520estimated%2520using%2520a%2520deep%2520neural%2520network%252C%250Awhich%2520is%2520then%2520used%2520to%2520iteratively%2520transport%2520a%2520Gaussian%2520white%2520noise%2520density%2520to%2520a%250Atarget%2520density.%2520Variants%2520for%2520conditional%2520densities%2520have%2520been%2520developed%252C%2520but%250Acorrect%2520estimation%2520of%2520the%2520corresponding%2520scores%2520is%2520difficult.%2520We%2520avoid%2520these%250Adifficulties%2520by%2520introducing%2520an%2520algorithm%2520that%2520guides%2520the%2520diffusion%2520with%2520a%250Aprojected%2520score.%2520The%2520projection%2520pushes%2520the%2520image%2520feature%2520vector%2520towards%2520the%250Afeature%2520vector%2520centroid%2520of%2520the%2520target%2520class.%2520The%2520projected%2520score%2520and%2520the%250Afeature%2520vectors%2520are%2520learned%2520by%2520the%2520same%2520network.%2520Specifically%252C%2520the%2520image%250Afeature%2520vector%2520is%2520defined%2520as%2520the%2520spatial%2520averages%2520of%2520the%2520channels%2520activations%250Ain%2520select%2520layers%2520of%2520the%2520network.%2520Optimizing%2520the%2520projected%2520score%2520for%2520denoising%250Aloss%2520encourages%2520image%2520feature%2520vectors%2520of%2520each%2520class%2520to%2520cluster%2520around%2520their%250Acentroids.%2520It%2520also%2520leads%2520to%2520the%2520separations%2520of%2520the%2520centroids.%2520We%2520show%2520that%250Athese%2520centroids%2520provide%2520a%2520low-dimensional%2520Euclidean%2520embedding%2520of%2520the%2520class%250Aconditional%2520densities.%2520We%2520demonstrate%2520that%2520the%2520algorithm%2520can%2520generate%2520high%250Aquality%2520and%2520diverse%2520samples%2520from%2520the%2520conditioning%2520class.%2520Conditional%2520generation%250Acan%2520be%2520performed%2520using%2520feature%2520vectors%2520interpolated%2520between%2520those%2520of%2520the%250Atraining%2520set%252C%2520demonstrating%2520out-of-distribution%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-guided%20score%20diffusion%20for%20sampling%20conditional%20densities&entry.906535625=Zahra%20Kadkhodaie%20and%20St%C3%A9phane%20Mallat%20and%20Eero%20P.%20Simoncelli&entry.1292438233=%20%20Score%20diffusion%20methods%20can%20learn%20probability%20densities%20from%20samples.%20The%0Ascore%20of%20the%20noise-corrupted%20density%20is%20estimated%20using%20a%20deep%20neural%20network%2C%0Awhich%20is%20then%20used%20to%20iteratively%20transport%20a%20Gaussian%20white%20noise%20density%20to%20a%0Atarget%20density.%20Variants%20for%20conditional%20densities%20have%20been%20developed%2C%20but%0Acorrect%20estimation%20of%20the%20corresponding%20scores%20is%20difficult.%20We%20avoid%20these%0Adifficulties%20by%20introducing%20an%20algorithm%20that%20guides%20the%20diffusion%20with%20a%0Aprojected%20score.%20The%20projection%20pushes%20the%20image%20feature%20vector%20towards%20the%0Afeature%20vector%20centroid%20of%20the%20target%20class.%20The%20projected%20score%20and%20the%0Afeature%20vectors%20are%20learned%20by%20the%20same%20network.%20Specifically%2C%20the%20image%0Afeature%20vector%20is%20defined%20as%20the%20spatial%20averages%20of%20the%20channels%20activations%0Ain%20select%20layers%20of%20the%20network.%20Optimizing%20the%20projected%20score%20for%20denoising%0Aloss%20encourages%20image%20feature%20vectors%20of%20each%20class%20to%20cluster%20around%20their%0Acentroids.%20It%20also%20leads%20to%20the%20separations%20of%20the%20centroids.%20We%20show%20that%0Athese%20centroids%20provide%20a%20low-dimensional%20Euclidean%20embedding%20of%20the%20class%0Aconditional%20densities.%20We%20demonstrate%20that%20the%20algorithm%20can%20generate%20high%0Aquality%20and%20diverse%20samples%20from%20the%20conditioning%20class.%20Conditional%20generation%0Acan%20be%20performed%20using%20feature%20vectors%20interpolated%20between%20those%20of%20the%0Atraining%20set%2C%20demonstrating%20out-of-distribution%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11646v1&entry.124074799=Read"},
{"title": "Estimating the distribution of numerosity and non-numerical visual\n  magnitudes in natural scenes using computer vision", "author": "Kuinan Hou and Marco Zorzi and Alberto Testolin", "abstract": "  Humans share with many animal species the ability to perceive and\napproximately represent the number of objects in visual scenes. This ability\nimproves throughout childhood, suggesting that learning and development play a\nkey role in shaping our number sense. This hypothesis is further supported by\ncomputational investigations based on deep learning, which have shown that\nnumerosity perception can spontaneously emerge in neural networks that learn\nthe statistical structure of images with a varying number of items. However,\nneural network models are usually trained using synthetic datasets that might\nnot faithfully reflect the statistical structure of natural environments, and\nthere is also growing interest in using more ecological visual stimuli to\ninvestigate numerosity perception in humans. In this work, we exploit recent\nadvances in computer vision algorithms to design and implement an original\npipeline that can be used to estimate the distribution of numerosity and\nnon-numerical magnitudes in large-scale datasets containing thousands of real\nimages depicting objects in daily life situations. We show that in natural\nvisual scenes the frequency of appearance of different numerosities follows a\npower law distribution. Moreover, we show that the correlational structure for\nnumerosity and continuous magnitudes is stable across datasets and scene types\n(homogeneous vs. heterogeneous object sets). We suggest that considering such\n\"ecological\" pattern of covariance is important to understand the influence of\nnon-numerical visual cues on numerosity judgements.\n", "link": "http://arxiv.org/abs/2409.11028v2", "date": "2024-10-15", "relevancy": 2.1117, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20the%20distribution%20of%20numerosity%20and%20non-numerical%20visual%0A%20%20magnitudes%20in%20natural%20scenes%20using%20computer%20vision&body=Title%3A%20Estimating%20the%20distribution%20of%20numerosity%20and%20non-numerical%20visual%0A%20%20magnitudes%20in%20natural%20scenes%20using%20computer%20vision%0AAuthor%3A%20Kuinan%20Hou%20and%20Marco%20Zorzi%20and%20Alberto%20Testolin%0AAbstract%3A%20%20%20Humans%20share%20with%20many%20animal%20species%20the%20ability%20to%20perceive%20and%0Aapproximately%20represent%20the%20number%20of%20objects%20in%20visual%20scenes.%20This%20ability%0Aimproves%20throughout%20childhood%2C%20suggesting%20that%20learning%20and%20development%20play%20a%0Akey%20role%20in%20shaping%20our%20number%20sense.%20This%20hypothesis%20is%20further%20supported%20by%0Acomputational%20investigations%20based%20on%20deep%20learning%2C%20which%20have%20shown%20that%0Anumerosity%20perception%20can%20spontaneously%20emerge%20in%20neural%20networks%20that%20learn%0Athe%20statistical%20structure%20of%20images%20with%20a%20varying%20number%20of%20items.%20However%2C%0Aneural%20network%20models%20are%20usually%20trained%20using%20synthetic%20datasets%20that%20might%0Anot%20faithfully%20reflect%20the%20statistical%20structure%20of%20natural%20environments%2C%20and%0Athere%20is%20also%20growing%20interest%20in%20using%20more%20ecological%20visual%20stimuli%20to%0Ainvestigate%20numerosity%20perception%20in%20humans.%20In%20this%20work%2C%20we%20exploit%20recent%0Aadvances%20in%20computer%20vision%20algorithms%20to%20design%20and%20implement%20an%20original%0Apipeline%20that%20can%20be%20used%20to%20estimate%20the%20distribution%20of%20numerosity%20and%0Anon-numerical%20magnitudes%20in%20large-scale%20datasets%20containing%20thousands%20of%20real%0Aimages%20depicting%20objects%20in%20daily%20life%20situations.%20We%20show%20that%20in%20natural%0Avisual%20scenes%20the%20frequency%20of%20appearance%20of%20different%20numerosities%20follows%20a%0Apower%20law%20distribution.%20Moreover%2C%20we%20show%20that%20the%20correlational%20structure%20for%0Anumerosity%20and%20continuous%20magnitudes%20is%20stable%20across%20datasets%20and%20scene%20types%0A%28homogeneous%20vs.%20heterogeneous%20object%20sets%29.%20We%20suggest%20that%20considering%20such%0A%22ecological%22%20pattern%20of%20covariance%20is%20important%20to%20understand%20the%20influence%20of%0Anon-numerical%20visual%20cues%20on%20numerosity%20judgements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520the%2520distribution%2520of%2520numerosity%2520and%2520non-numerical%2520visual%250A%2520%2520magnitudes%2520in%2520natural%2520scenes%2520using%2520computer%2520vision%26entry.906535625%3DKuinan%2520Hou%2520and%2520Marco%2520Zorzi%2520and%2520Alberto%2520Testolin%26entry.1292438233%3D%2520%2520Humans%2520share%2520with%2520many%2520animal%2520species%2520the%2520ability%2520to%2520perceive%2520and%250Aapproximately%2520represent%2520the%2520number%2520of%2520objects%2520in%2520visual%2520scenes.%2520This%2520ability%250Aimproves%2520throughout%2520childhood%252C%2520suggesting%2520that%2520learning%2520and%2520development%2520play%2520a%250Akey%2520role%2520in%2520shaping%2520our%2520number%2520sense.%2520This%2520hypothesis%2520is%2520further%2520supported%2520by%250Acomputational%2520investigations%2520based%2520on%2520deep%2520learning%252C%2520which%2520have%2520shown%2520that%250Anumerosity%2520perception%2520can%2520spontaneously%2520emerge%2520in%2520neural%2520networks%2520that%2520learn%250Athe%2520statistical%2520structure%2520of%2520images%2520with%2520a%2520varying%2520number%2520of%2520items.%2520However%252C%250Aneural%2520network%2520models%2520are%2520usually%2520trained%2520using%2520synthetic%2520datasets%2520that%2520might%250Anot%2520faithfully%2520reflect%2520the%2520statistical%2520structure%2520of%2520natural%2520environments%252C%2520and%250Athere%2520is%2520also%2520growing%2520interest%2520in%2520using%2520more%2520ecological%2520visual%2520stimuli%2520to%250Ainvestigate%2520numerosity%2520perception%2520in%2520humans.%2520In%2520this%2520work%252C%2520we%2520exploit%2520recent%250Aadvances%2520in%2520computer%2520vision%2520algorithms%2520to%2520design%2520and%2520implement%2520an%2520original%250Apipeline%2520that%2520can%2520be%2520used%2520to%2520estimate%2520the%2520distribution%2520of%2520numerosity%2520and%250Anon-numerical%2520magnitudes%2520in%2520large-scale%2520datasets%2520containing%2520thousands%2520of%2520real%250Aimages%2520depicting%2520objects%2520in%2520daily%2520life%2520situations.%2520We%2520show%2520that%2520in%2520natural%250Avisual%2520scenes%2520the%2520frequency%2520of%2520appearance%2520of%2520different%2520numerosities%2520follows%2520a%250Apower%2520law%2520distribution.%2520Moreover%252C%2520we%2520show%2520that%2520the%2520correlational%2520structure%2520for%250Anumerosity%2520and%2520continuous%2520magnitudes%2520is%2520stable%2520across%2520datasets%2520and%2520scene%2520types%250A%2528homogeneous%2520vs.%2520heterogeneous%2520object%2520sets%2529.%2520We%2520suggest%2520that%2520considering%2520such%250A%2522ecological%2522%2520pattern%2520of%2520covariance%2520is%2520important%2520to%2520understand%2520the%2520influence%2520of%250Anon-numerical%2520visual%2520cues%2520on%2520numerosity%2520judgements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20the%20distribution%20of%20numerosity%20and%20non-numerical%20visual%0A%20%20magnitudes%20in%20natural%20scenes%20using%20computer%20vision&entry.906535625=Kuinan%20Hou%20and%20Marco%20Zorzi%20and%20Alberto%20Testolin&entry.1292438233=%20%20Humans%20share%20with%20many%20animal%20species%20the%20ability%20to%20perceive%20and%0Aapproximately%20represent%20the%20number%20of%20objects%20in%20visual%20scenes.%20This%20ability%0Aimproves%20throughout%20childhood%2C%20suggesting%20that%20learning%20and%20development%20play%20a%0Akey%20role%20in%20shaping%20our%20number%20sense.%20This%20hypothesis%20is%20further%20supported%20by%0Acomputational%20investigations%20based%20on%20deep%20learning%2C%20which%20have%20shown%20that%0Anumerosity%20perception%20can%20spontaneously%20emerge%20in%20neural%20networks%20that%20learn%0Athe%20statistical%20structure%20of%20images%20with%20a%20varying%20number%20of%20items.%20However%2C%0Aneural%20network%20models%20are%20usually%20trained%20using%20synthetic%20datasets%20that%20might%0Anot%20faithfully%20reflect%20the%20statistical%20structure%20of%20natural%20environments%2C%20and%0Athere%20is%20also%20growing%20interest%20in%20using%20more%20ecological%20visual%20stimuli%20to%0Ainvestigate%20numerosity%20perception%20in%20humans.%20In%20this%20work%2C%20we%20exploit%20recent%0Aadvances%20in%20computer%20vision%20algorithms%20to%20design%20and%20implement%20an%20original%0Apipeline%20that%20can%20be%20used%20to%20estimate%20the%20distribution%20of%20numerosity%20and%0Anon-numerical%20magnitudes%20in%20large-scale%20datasets%20containing%20thousands%20of%20real%0Aimages%20depicting%20objects%20in%20daily%20life%20situations.%20We%20show%20that%20in%20natural%0Avisual%20scenes%20the%20frequency%20of%20appearance%20of%20different%20numerosities%20follows%20a%0Apower%20law%20distribution.%20Moreover%2C%20we%20show%20that%20the%20correlational%20structure%20for%0Anumerosity%20and%20continuous%20magnitudes%20is%20stable%20across%20datasets%20and%20scene%20types%0A%28homogeneous%20vs.%20heterogeneous%20object%20sets%29.%20We%20suggest%20that%20considering%20such%0A%22ecological%22%20pattern%20of%20covariance%20is%20important%20to%20understand%20the%20influence%20of%0Anon-numerical%20visual%20cues%20on%20numerosity%20judgements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11028v2&entry.124074799=Read"},
{"title": "Jigsaw++: Imagining Complete Shape Priors for Object Reassembly", "author": "Jiaxin Lu and Gang Hua and Qixing Huang", "abstract": "  The automatic assembly problem has attracted increasing interest due to its\ncomplex challenges that involve 3D representation. This paper introduces\nJigsaw++, a novel generative method designed to tackle the multifaceted\nchallenges of reconstruction for the reassembly problem. Existing approach\nfocusing primarily on piecewise information for both part and fracture\nassembly, often overlooking the integration of complete object prior. Jigsaw++\ndistinguishes itself by learning a category-agnostic shape prior of complete\nobjects. It employs the proposed \"retargeting\" strategy that effectively\nleverages the output of any existing assembly method to generate complete shape\nreconstructions. This capability allows it to function orthogonally to the\ncurrent methods. Through extensive evaluations on Breaking Bad dataset and\nPartNet, Jigsaw++ has demonstrated its effectiveness, reducing reconstruction\nerrors and enhancing the precision of shape reconstruction, which sets a new\ndirection for future reassembly model developments.\n", "link": "http://arxiv.org/abs/2410.11816v1", "date": "2024-10-15", "relevancy": 2.1109, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5423}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5274}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jigsaw%2B%2B%3A%20Imagining%20Complete%20Shape%20Priors%20for%20Object%20Reassembly&body=Title%3A%20Jigsaw%2B%2B%3A%20Imagining%20Complete%20Shape%20Priors%20for%20Object%20Reassembly%0AAuthor%3A%20Jiaxin%20Lu%20and%20Gang%20Hua%20and%20Qixing%20Huang%0AAbstract%3A%20%20%20The%20automatic%20assembly%20problem%20has%20attracted%20increasing%20interest%20due%20to%20its%0Acomplex%20challenges%20that%20involve%203D%20representation.%20This%20paper%20introduces%0AJigsaw%2B%2B%2C%20a%20novel%20generative%20method%20designed%20to%20tackle%20the%20multifaceted%0Achallenges%20of%20reconstruction%20for%20the%20reassembly%20problem.%20Existing%20approach%0Afocusing%20primarily%20on%20piecewise%20information%20for%20both%20part%20and%20fracture%0Aassembly%2C%20often%20overlooking%20the%20integration%20of%20complete%20object%20prior.%20Jigsaw%2B%2B%0Adistinguishes%20itself%20by%20learning%20a%20category-agnostic%20shape%20prior%20of%20complete%0Aobjects.%20It%20employs%20the%20proposed%20%22retargeting%22%20strategy%20that%20effectively%0Aleverages%20the%20output%20of%20any%20existing%20assembly%20method%20to%20generate%20complete%20shape%0Areconstructions.%20This%20capability%20allows%20it%20to%20function%20orthogonally%20to%20the%0Acurrent%20methods.%20Through%20extensive%20evaluations%20on%20Breaking%20Bad%20dataset%20and%0APartNet%2C%20Jigsaw%2B%2B%20has%20demonstrated%20its%20effectiveness%2C%20reducing%20reconstruction%0Aerrors%20and%20enhancing%20the%20precision%20of%20shape%20reconstruction%2C%20which%20sets%20a%20new%0Adirection%20for%20future%20reassembly%20model%20developments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJigsaw%252B%252B%253A%2520Imagining%2520Complete%2520Shape%2520Priors%2520for%2520Object%2520Reassembly%26entry.906535625%3DJiaxin%2520Lu%2520and%2520Gang%2520Hua%2520and%2520Qixing%2520Huang%26entry.1292438233%3D%2520%2520The%2520automatic%2520assembly%2520problem%2520has%2520attracted%2520increasing%2520interest%2520due%2520to%2520its%250Acomplex%2520challenges%2520that%2520involve%25203D%2520representation.%2520This%2520paper%2520introduces%250AJigsaw%252B%252B%252C%2520a%2520novel%2520generative%2520method%2520designed%2520to%2520tackle%2520the%2520multifaceted%250Achallenges%2520of%2520reconstruction%2520for%2520the%2520reassembly%2520problem.%2520Existing%2520approach%250Afocusing%2520primarily%2520on%2520piecewise%2520information%2520for%2520both%2520part%2520and%2520fracture%250Aassembly%252C%2520often%2520overlooking%2520the%2520integration%2520of%2520complete%2520object%2520prior.%2520Jigsaw%252B%252B%250Adistinguishes%2520itself%2520by%2520learning%2520a%2520category-agnostic%2520shape%2520prior%2520of%2520complete%250Aobjects.%2520It%2520employs%2520the%2520proposed%2520%2522retargeting%2522%2520strategy%2520that%2520effectively%250Aleverages%2520the%2520output%2520of%2520any%2520existing%2520assembly%2520method%2520to%2520generate%2520complete%2520shape%250Areconstructions.%2520This%2520capability%2520allows%2520it%2520to%2520function%2520orthogonally%2520to%2520the%250Acurrent%2520methods.%2520Through%2520extensive%2520evaluations%2520on%2520Breaking%2520Bad%2520dataset%2520and%250APartNet%252C%2520Jigsaw%252B%252B%2520has%2520demonstrated%2520its%2520effectiveness%252C%2520reducing%2520reconstruction%250Aerrors%2520and%2520enhancing%2520the%2520precision%2520of%2520shape%2520reconstruction%252C%2520which%2520sets%2520a%2520new%250Adirection%2520for%2520future%2520reassembly%2520model%2520developments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jigsaw%2B%2B%3A%20Imagining%20Complete%20Shape%20Priors%20for%20Object%20Reassembly&entry.906535625=Jiaxin%20Lu%20and%20Gang%20Hua%20and%20Qixing%20Huang&entry.1292438233=%20%20The%20automatic%20assembly%20problem%20has%20attracted%20increasing%20interest%20due%20to%20its%0Acomplex%20challenges%20that%20involve%203D%20representation.%20This%20paper%20introduces%0AJigsaw%2B%2B%2C%20a%20novel%20generative%20method%20designed%20to%20tackle%20the%20multifaceted%0Achallenges%20of%20reconstruction%20for%20the%20reassembly%20problem.%20Existing%20approach%0Afocusing%20primarily%20on%20piecewise%20information%20for%20both%20part%20and%20fracture%0Aassembly%2C%20often%20overlooking%20the%20integration%20of%20complete%20object%20prior.%20Jigsaw%2B%2B%0Adistinguishes%20itself%20by%20learning%20a%20category-agnostic%20shape%20prior%20of%20complete%0Aobjects.%20It%20employs%20the%20proposed%20%22retargeting%22%20strategy%20that%20effectively%0Aleverages%20the%20output%20of%20any%20existing%20assembly%20method%20to%20generate%20complete%20shape%0Areconstructions.%20This%20capability%20allows%20it%20to%20function%20orthogonally%20to%20the%0Acurrent%20methods.%20Through%20extensive%20evaluations%20on%20Breaking%20Bad%20dataset%20and%0APartNet%2C%20Jigsaw%2B%2B%20has%20demonstrated%20its%20effectiveness%2C%20reducing%20reconstruction%0Aerrors%20and%20enhancing%20the%20precision%20of%20shape%20reconstruction%2C%20which%20sets%20a%20new%0Adirection%20for%20future%20reassembly%20model%20developments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11816v1&entry.124074799=Read"},
{"title": "Uncovering mesa-optimization algorithms in Transformers", "author": "Johannes von Oswald and Maximilian Schlegel and Alexander Meulemans and Seijin Kobayashi and Eyvind Niklasson and Nicolas Zucchet and Nino Scherrer and Nolan Miller and Mark Sandler and Blaise Ag\u00fcera y Arcas and Max Vladymyrov and Razvan Pascanu and Jo\u00e3o Sacramento", "abstract": "  Some autoregressive models exhibit in-context learning capabilities: being\nable to learn as an input sequence is processed, without undergoing any\nparameter changes, and without being explicitly trained to do so. The origins\nof this phenomenon are still poorly understood. Here we analyze a series of\nTransformer models trained to perform synthetic sequence prediction tasks, and\ndiscover that standard next-token prediction error minimization gives rise to a\nsubsidiary learning algorithm that adjusts the model as new inputs are\nrevealed. We show that this process corresponds to gradient-based optimization\nof a principled objective function, which leads to strong generalization\nperformance on unseen sequences. Our findings explain in-context learning as a\nproduct of autoregressive loss minimization and inform the design of new\noptimization-based Transformer layers.\n", "link": "http://arxiv.org/abs/2309.05858v2", "date": "2024-10-15", "relevancy": 2.0413, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.575}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5005}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20mesa-optimization%20algorithms%20in%20Transformers&body=Title%3A%20Uncovering%20mesa-optimization%20algorithms%20in%20Transformers%0AAuthor%3A%20Johannes%20von%20Oswald%20and%20Maximilian%20Schlegel%20and%20Alexander%20Meulemans%20and%20Seijin%20Kobayashi%20and%20Eyvind%20Niklasson%20and%20Nicolas%20Zucchet%20and%20Nino%20Scherrer%20and%20Nolan%20Miller%20and%20Mark%20Sandler%20and%20Blaise%20Ag%C3%BCera%20y%20Arcas%20and%20Max%20Vladymyrov%20and%20Razvan%20Pascanu%20and%20Jo%C3%A3o%20Sacramento%0AAbstract%3A%20%20%20Some%20autoregressive%20models%20exhibit%20in-context%20learning%20capabilities%3A%20being%0Aable%20to%20learn%20as%20an%20input%20sequence%20is%20processed%2C%20without%20undergoing%20any%0Aparameter%20changes%2C%20and%20without%20being%20explicitly%20trained%20to%20do%20so.%20The%20origins%0Aof%20this%20phenomenon%20are%20still%20poorly%20understood.%20Here%20we%20analyze%20a%20series%20of%0ATransformer%20models%20trained%20to%20perform%20synthetic%20sequence%20prediction%20tasks%2C%20and%0Adiscover%20that%20standard%20next-token%20prediction%20error%20minimization%20gives%20rise%20to%20a%0Asubsidiary%20learning%20algorithm%20that%20adjusts%20the%20model%20as%20new%20inputs%20are%0Arevealed.%20We%20show%20that%20this%20process%20corresponds%20to%20gradient-based%20optimization%0Aof%20a%20principled%20objective%20function%2C%20which%20leads%20to%20strong%20generalization%0Aperformance%20on%20unseen%20sequences.%20Our%20findings%20explain%20in-context%20learning%20as%20a%0Aproduct%20of%20autoregressive%20loss%20minimization%20and%20inform%20the%20design%20of%20new%0Aoptimization-based%20Transformer%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520mesa-optimization%2520algorithms%2520in%2520Transformers%26entry.906535625%3DJohannes%2520von%2520Oswald%2520and%2520Maximilian%2520Schlegel%2520and%2520Alexander%2520Meulemans%2520and%2520Seijin%2520Kobayashi%2520and%2520Eyvind%2520Niklasson%2520and%2520Nicolas%2520Zucchet%2520and%2520Nino%2520Scherrer%2520and%2520Nolan%2520Miller%2520and%2520Mark%2520Sandler%2520and%2520Blaise%2520Ag%25C3%25BCera%2520y%2520Arcas%2520and%2520Max%2520Vladymyrov%2520and%2520Razvan%2520Pascanu%2520and%2520Jo%25C3%25A3o%2520Sacramento%26entry.1292438233%3D%2520%2520Some%2520autoregressive%2520models%2520exhibit%2520in-context%2520learning%2520capabilities%253A%2520being%250Aable%2520to%2520learn%2520as%2520an%2520input%2520sequence%2520is%2520processed%252C%2520without%2520undergoing%2520any%250Aparameter%2520changes%252C%2520and%2520without%2520being%2520explicitly%2520trained%2520to%2520do%2520so.%2520The%2520origins%250Aof%2520this%2520phenomenon%2520are%2520still%2520poorly%2520understood.%2520Here%2520we%2520analyze%2520a%2520series%2520of%250ATransformer%2520models%2520trained%2520to%2520perform%2520synthetic%2520sequence%2520prediction%2520tasks%252C%2520and%250Adiscover%2520that%2520standard%2520next-token%2520prediction%2520error%2520minimization%2520gives%2520rise%2520to%2520a%250Asubsidiary%2520learning%2520algorithm%2520that%2520adjusts%2520the%2520model%2520as%2520new%2520inputs%2520are%250Arevealed.%2520We%2520show%2520that%2520this%2520process%2520corresponds%2520to%2520gradient-based%2520optimization%250Aof%2520a%2520principled%2520objective%2520function%252C%2520which%2520leads%2520to%2520strong%2520generalization%250Aperformance%2520on%2520unseen%2520sequences.%2520Our%2520findings%2520explain%2520in-context%2520learning%2520as%2520a%250Aproduct%2520of%2520autoregressive%2520loss%2520minimization%2520and%2520inform%2520the%2520design%2520of%2520new%250Aoptimization-based%2520Transformer%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20mesa-optimization%20algorithms%20in%20Transformers&entry.906535625=Johannes%20von%20Oswald%20and%20Maximilian%20Schlegel%20and%20Alexander%20Meulemans%20and%20Seijin%20Kobayashi%20and%20Eyvind%20Niklasson%20and%20Nicolas%20Zucchet%20and%20Nino%20Scherrer%20and%20Nolan%20Miller%20and%20Mark%20Sandler%20and%20Blaise%20Ag%C3%BCera%20y%20Arcas%20and%20Max%20Vladymyrov%20and%20Razvan%20Pascanu%20and%20Jo%C3%A3o%20Sacramento&entry.1292438233=%20%20Some%20autoregressive%20models%20exhibit%20in-context%20learning%20capabilities%3A%20being%0Aable%20to%20learn%20as%20an%20input%20sequence%20is%20processed%2C%20without%20undergoing%20any%0Aparameter%20changes%2C%20and%20without%20being%20explicitly%20trained%20to%20do%20so.%20The%20origins%0Aof%20this%20phenomenon%20are%20still%20poorly%20understood.%20Here%20we%20analyze%20a%20series%20of%0ATransformer%20models%20trained%20to%20perform%20synthetic%20sequence%20prediction%20tasks%2C%20and%0Adiscover%20that%20standard%20next-token%20prediction%20error%20minimization%20gives%20rise%20to%20a%0Asubsidiary%20learning%20algorithm%20that%20adjusts%20the%20model%20as%20new%20inputs%20are%0Arevealed.%20We%20show%20that%20this%20process%20corresponds%20to%20gradient-based%20optimization%0Aof%20a%20principled%20objective%20function%2C%20which%20leads%20to%20strong%20generalization%0Aperformance%20on%20unseen%20sequences.%20Our%20findings%20explain%20in-context%20learning%20as%20a%0Aproduct%20of%20autoregressive%20loss%20minimization%20and%20inform%20the%20design%20of%20new%0Aoptimization-based%20Transformer%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05858v2&entry.124074799=Read"},
{"title": "Active Label Refinement for Robust Training of Imbalanced Medical Image\n  Classification Tasks in the Presence of High Label Noise", "author": "Bidur Khanal and Tianhong Dai and Binod Bhattarai and Cristian Linte", "abstract": "  The robustness of supervised deep learning-based medical image classification\nis significantly undermined by label noise. Although several methods have been\nproposed to enhance classification performance in the presence of noisy labels,\nthey face some challenges: 1) a struggle with class-imbalanced datasets,\nleading to the frequent overlooking of minority classes as noisy samples; 2) a\nsingular focus on maximizing performance using noisy datasets, without\nincorporating experts-in-the-loop for actively cleaning the noisy labels. To\nmitigate these challenges, we propose a two-phase approach that combines\nLearning with Noisy Labels (LNL) and active learning. This approach not only\nimproves the robustness of medical image classification in the presence of\nnoisy labels, but also iteratively improves the quality of the dataset by\nrelabeling the important incorrect labels, under a limited annotation budget.\nFurthermore, we introduce a novel Variance of Gradients approach in LNL phase,\nwhich complements the loss-based sample selection by also sampling\nunder-represented samples. Using two imbalanced noisy medical classification\ndatasets, we demonstrate that that our proposed technique is superior to its\npredecessors at handling class imbalance by not misidentifying clean samples\nfrom minority classes as mostly noisy samples.\n", "link": "http://arxiv.org/abs/2407.05973v2", "date": "2024-10-15", "relevancy": 2.0973, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5618}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5223}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Label%20Refinement%20for%20Robust%20Training%20of%20Imbalanced%20Medical%20Image%0A%20%20Classification%20Tasks%20in%20the%20Presence%20of%20High%20Label%20Noise&body=Title%3A%20Active%20Label%20Refinement%20for%20Robust%20Training%20of%20Imbalanced%20Medical%20Image%0A%20%20Classification%20Tasks%20in%20the%20Presence%20of%20High%20Label%20Noise%0AAuthor%3A%20Bidur%20Khanal%20and%20Tianhong%20Dai%20and%20Binod%20Bhattarai%20and%20Cristian%20Linte%0AAbstract%3A%20%20%20The%20robustness%20of%20supervised%20deep%20learning-based%20medical%20image%20classification%0Ais%20significantly%20undermined%20by%20label%20noise.%20Although%20several%20methods%20have%20been%0Aproposed%20to%20enhance%20classification%20performance%20in%20the%20presence%20of%20noisy%20labels%2C%0Athey%20face%20some%20challenges%3A%201%29%20a%20struggle%20with%20class-imbalanced%20datasets%2C%0Aleading%20to%20the%20frequent%20overlooking%20of%20minority%20classes%20as%20noisy%20samples%3B%202%29%20a%0Asingular%20focus%20on%20maximizing%20performance%20using%20noisy%20datasets%2C%20without%0Aincorporating%20experts-in-the-loop%20for%20actively%20cleaning%20the%20noisy%20labels.%20To%0Amitigate%20these%20challenges%2C%20we%20propose%20a%20two-phase%20approach%20that%20combines%0ALearning%20with%20Noisy%20Labels%20%28LNL%29%20and%20active%20learning.%20This%20approach%20not%20only%0Aimproves%20the%20robustness%20of%20medical%20image%20classification%20in%20the%20presence%20of%0Anoisy%20labels%2C%20but%20also%20iteratively%20improves%20the%20quality%20of%20the%20dataset%20by%0Arelabeling%20the%20important%20incorrect%20labels%2C%20under%20a%20limited%20annotation%20budget.%0AFurthermore%2C%20we%20introduce%20a%20novel%20Variance%20of%20Gradients%20approach%20in%20LNL%20phase%2C%0Awhich%20complements%20the%20loss-based%20sample%20selection%20by%20also%20sampling%0Aunder-represented%20samples.%20Using%20two%20imbalanced%20noisy%20medical%20classification%0Adatasets%2C%20we%20demonstrate%20that%20that%20our%20proposed%20technique%20is%20superior%20to%20its%0Apredecessors%20at%20handling%20class%20imbalance%20by%20not%20misidentifying%20clean%20samples%0Afrom%20minority%20classes%20as%20mostly%20noisy%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05973v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Label%2520Refinement%2520for%2520Robust%2520Training%2520of%2520Imbalanced%2520Medical%2520Image%250A%2520%2520Classification%2520Tasks%2520in%2520the%2520Presence%2520of%2520High%2520Label%2520Noise%26entry.906535625%3DBidur%2520Khanal%2520and%2520Tianhong%2520Dai%2520and%2520Binod%2520Bhattarai%2520and%2520Cristian%2520Linte%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520supervised%2520deep%2520learning-based%2520medical%2520image%2520classification%250Ais%2520significantly%2520undermined%2520by%2520label%2520noise.%2520Although%2520several%2520methods%2520have%2520been%250Aproposed%2520to%2520enhance%2520classification%2520performance%2520in%2520the%2520presence%2520of%2520noisy%2520labels%252C%250Athey%2520face%2520some%2520challenges%253A%25201%2529%2520a%2520struggle%2520with%2520class-imbalanced%2520datasets%252C%250Aleading%2520to%2520the%2520frequent%2520overlooking%2520of%2520minority%2520classes%2520as%2520noisy%2520samples%253B%25202%2529%2520a%250Asingular%2520focus%2520on%2520maximizing%2520performance%2520using%2520noisy%2520datasets%252C%2520without%250Aincorporating%2520experts-in-the-loop%2520for%2520actively%2520cleaning%2520the%2520noisy%2520labels.%2520To%250Amitigate%2520these%2520challenges%252C%2520we%2520propose%2520a%2520two-phase%2520approach%2520that%2520combines%250ALearning%2520with%2520Noisy%2520Labels%2520%2528LNL%2529%2520and%2520active%2520learning.%2520This%2520approach%2520not%2520only%250Aimproves%2520the%2520robustness%2520of%2520medical%2520image%2520classification%2520in%2520the%2520presence%2520of%250Anoisy%2520labels%252C%2520but%2520also%2520iteratively%2520improves%2520the%2520quality%2520of%2520the%2520dataset%2520by%250Arelabeling%2520the%2520important%2520incorrect%2520labels%252C%2520under%2520a%2520limited%2520annotation%2520budget.%250AFurthermore%252C%2520we%2520introduce%2520a%2520novel%2520Variance%2520of%2520Gradients%2520approach%2520in%2520LNL%2520phase%252C%250Awhich%2520complements%2520the%2520loss-based%2520sample%2520selection%2520by%2520also%2520sampling%250Aunder-represented%2520samples.%2520Using%2520two%2520imbalanced%2520noisy%2520medical%2520classification%250Adatasets%252C%2520we%2520demonstrate%2520that%2520that%2520our%2520proposed%2520technique%2520is%2520superior%2520to%2520its%250Apredecessors%2520at%2520handling%2520class%2520imbalance%2520by%2520not%2520misidentifying%2520clean%2520samples%250Afrom%2520minority%2520classes%2520as%2520mostly%2520noisy%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05973v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Label%20Refinement%20for%20Robust%20Training%20of%20Imbalanced%20Medical%20Image%0A%20%20Classification%20Tasks%20in%20the%20Presence%20of%20High%20Label%20Noise&entry.906535625=Bidur%20Khanal%20and%20Tianhong%20Dai%20and%20Binod%20Bhattarai%20and%20Cristian%20Linte&entry.1292438233=%20%20The%20robustness%20of%20supervised%20deep%20learning-based%20medical%20image%20classification%0Ais%20significantly%20undermined%20by%20label%20noise.%20Although%20several%20methods%20have%20been%0Aproposed%20to%20enhance%20classification%20performance%20in%20the%20presence%20of%20noisy%20labels%2C%0Athey%20face%20some%20challenges%3A%201%29%20a%20struggle%20with%20class-imbalanced%20datasets%2C%0Aleading%20to%20the%20frequent%20overlooking%20of%20minority%20classes%20as%20noisy%20samples%3B%202%29%20a%0Asingular%20focus%20on%20maximizing%20performance%20using%20noisy%20datasets%2C%20without%0Aincorporating%20experts-in-the-loop%20for%20actively%20cleaning%20the%20noisy%20labels.%20To%0Amitigate%20these%20challenges%2C%20we%20propose%20a%20two-phase%20approach%20that%20combines%0ALearning%20with%20Noisy%20Labels%20%28LNL%29%20and%20active%20learning.%20This%20approach%20not%20only%0Aimproves%20the%20robustness%20of%20medical%20image%20classification%20in%20the%20presence%20of%0Anoisy%20labels%2C%20but%20also%20iteratively%20improves%20the%20quality%20of%20the%20dataset%20by%0Arelabeling%20the%20important%20incorrect%20labels%2C%20under%20a%20limited%20annotation%20budget.%0AFurthermore%2C%20we%20introduce%20a%20novel%20Variance%20of%20Gradients%20approach%20in%20LNL%20phase%2C%0Awhich%20complements%20the%20loss-based%20sample%20selection%20by%20also%20sampling%0Aunder-represented%20samples.%20Using%20two%20imbalanced%20noisy%20medical%20classification%0Adatasets%2C%20we%20demonstrate%20that%20that%20our%20proposed%20technique%20is%20superior%20to%20its%0Apredecessors%20at%20handling%20class%20imbalance%20by%20not%20misidentifying%20clean%20samples%0Afrom%20minority%20classes%20as%20mostly%20noisy%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05973v2&entry.124074799=Read"},
{"title": "Zero-shot Model-based Reinforcement Learning using Large Language Models", "author": "Abdelhakim Benechehab and Youssef Attia El Hili and Ambroise Odonnat and Oussama Zekri and Albert Thomas and Giuseppe Paolo and Maurizio Filippone and Ievgen Redko and Bal\u00e1zs K\u00e9gl", "abstract": "  The emerging zero-shot capabilities of Large Language Models (LLMs) have led\nto their applications in areas extending well beyond natural language\nprocessing tasks. In reinforcement learning, while LLMs have been extensively\nused in text-based environments, their integration with continuous state spaces\nremains understudied. In this paper, we investigate how pre-trained LLMs can be\nleveraged to predict in context the dynamics of continuous Markov decision\nprocesses. We identify handling multivariate data and incorporating the control\nsignal as key challenges that limit the potential of LLMs' deployment in this\nsetup and propose Disentangled In-Context Learning (DICL) to address them. We\npresent proof-of-concept applications in two reinforcement learning settings:\nmodel-based policy evaluation and data-augmented off-policy reinforcement\nlearning, supported by theoretical analysis of the proposed methods. Our\nexperiments further demonstrate that our approach produces well-calibrated\nuncertainty estimates. We release the code at\nhttps://github.com/abenechehab/dicl.\n", "link": "http://arxiv.org/abs/2410.11711v1", "date": "2024-10-15", "relevancy": 2.1085, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Model-based%20Reinforcement%20Learning%20using%20Large%20Language%20Models&body=Title%3A%20Zero-shot%20Model-based%20Reinforcement%20Learning%20using%20Large%20Language%20Models%0AAuthor%3A%20Abdelhakim%20Benechehab%20and%20Youssef%20Attia%20El%20Hili%20and%20Ambroise%20Odonnat%20and%20Oussama%20Zekri%20and%20Albert%20Thomas%20and%20Giuseppe%20Paolo%20and%20Maurizio%20Filippone%20and%20Ievgen%20Redko%20and%20Bal%C3%A1zs%20K%C3%A9gl%0AAbstract%3A%20%20%20The%20emerging%20zero-shot%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%20led%0Ato%20their%20applications%20in%20areas%20extending%20well%20beyond%20natural%20language%0Aprocessing%20tasks.%20In%20reinforcement%20learning%2C%20while%20LLMs%20have%20been%20extensively%0Aused%20in%20text-based%20environments%2C%20their%20integration%20with%20continuous%20state%20spaces%0Aremains%20understudied.%20In%20this%20paper%2C%20we%20investigate%20how%20pre-trained%20LLMs%20can%20be%0Aleveraged%20to%20predict%20in%20context%20the%20dynamics%20of%20continuous%20Markov%20decision%0Aprocesses.%20We%20identify%20handling%20multivariate%20data%20and%20incorporating%20the%20control%0Asignal%20as%20key%20challenges%20that%20limit%20the%20potential%20of%20LLMs%27%20deployment%20in%20this%0Asetup%20and%20propose%20Disentangled%20In-Context%20Learning%20%28DICL%29%20to%20address%20them.%20We%0Apresent%20proof-of-concept%20applications%20in%20two%20reinforcement%20learning%20settings%3A%0Amodel-based%20policy%20evaluation%20and%20data-augmented%20off-policy%20reinforcement%0Alearning%2C%20supported%20by%20theoretical%20analysis%20of%20the%20proposed%20methods.%20Our%0Aexperiments%20further%20demonstrate%20that%20our%20approach%20produces%20well-calibrated%0Auncertainty%20estimates.%20We%20release%20the%20code%20at%0Ahttps%3A//github.com/abenechehab/dicl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Model-based%2520Reinforcement%2520Learning%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DAbdelhakim%2520Benechehab%2520and%2520Youssef%2520Attia%2520El%2520Hili%2520and%2520Ambroise%2520Odonnat%2520and%2520Oussama%2520Zekri%2520and%2520Albert%2520Thomas%2520and%2520Giuseppe%2520Paolo%2520and%2520Maurizio%2520Filippone%2520and%2520Ievgen%2520Redko%2520and%2520Bal%25C3%25A1zs%2520K%25C3%25A9gl%26entry.1292438233%3D%2520%2520The%2520emerging%2520zero-shot%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520led%250Ato%2520their%2520applications%2520in%2520areas%2520extending%2520well%2520beyond%2520natural%2520language%250Aprocessing%2520tasks.%2520In%2520reinforcement%2520learning%252C%2520while%2520LLMs%2520have%2520been%2520extensively%250Aused%2520in%2520text-based%2520environments%252C%2520their%2520integration%2520with%2520continuous%2520state%2520spaces%250Aremains%2520understudied.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520pre-trained%2520LLMs%2520can%2520be%250Aleveraged%2520to%2520predict%2520in%2520context%2520the%2520dynamics%2520of%2520continuous%2520Markov%2520decision%250Aprocesses.%2520We%2520identify%2520handling%2520multivariate%2520data%2520and%2520incorporating%2520the%2520control%250Asignal%2520as%2520key%2520challenges%2520that%2520limit%2520the%2520potential%2520of%2520LLMs%2527%2520deployment%2520in%2520this%250Asetup%2520and%2520propose%2520Disentangled%2520In-Context%2520Learning%2520%2528DICL%2529%2520to%2520address%2520them.%2520We%250Apresent%2520proof-of-concept%2520applications%2520in%2520two%2520reinforcement%2520learning%2520settings%253A%250Amodel-based%2520policy%2520evaluation%2520and%2520data-augmented%2520off-policy%2520reinforcement%250Alearning%252C%2520supported%2520by%2520theoretical%2520analysis%2520of%2520the%2520proposed%2520methods.%2520Our%250Aexperiments%2520further%2520demonstrate%2520that%2520our%2520approach%2520produces%2520well-calibrated%250Auncertainty%2520estimates.%2520We%2520release%2520the%2520code%2520at%250Ahttps%253A//github.com/abenechehab/dicl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Model-based%20Reinforcement%20Learning%20using%20Large%20Language%20Models&entry.906535625=Abdelhakim%20Benechehab%20and%20Youssef%20Attia%20El%20Hili%20and%20Ambroise%20Odonnat%20and%20Oussama%20Zekri%20and%20Albert%20Thomas%20and%20Giuseppe%20Paolo%20and%20Maurizio%20Filippone%20and%20Ievgen%20Redko%20and%20Bal%C3%A1zs%20K%C3%A9gl&entry.1292438233=%20%20The%20emerging%20zero-shot%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%20led%0Ato%20their%20applications%20in%20areas%20extending%20well%20beyond%20natural%20language%0Aprocessing%20tasks.%20In%20reinforcement%20learning%2C%20while%20LLMs%20have%20been%20extensively%0Aused%20in%20text-based%20environments%2C%20their%20integration%20with%20continuous%20state%20spaces%0Aremains%20understudied.%20In%20this%20paper%2C%20we%20investigate%20how%20pre-trained%20LLMs%20can%20be%0Aleveraged%20to%20predict%20in%20context%20the%20dynamics%20of%20continuous%20Markov%20decision%0Aprocesses.%20We%20identify%20handling%20multivariate%20data%20and%20incorporating%20the%20control%0Asignal%20as%20key%20challenges%20that%20limit%20the%20potential%20of%20LLMs%27%20deployment%20in%20this%0Asetup%20and%20propose%20Disentangled%20In-Context%20Learning%20%28DICL%29%20to%20address%20them.%20We%0Apresent%20proof-of-concept%20applications%20in%20two%20reinforcement%20learning%20settings%3A%0Amodel-based%20policy%20evaluation%20and%20data-augmented%20off-policy%20reinforcement%0Alearning%2C%20supported%20by%20theoretical%20analysis%20of%20the%20proposed%20methods.%20Our%0Aexperiments%20further%20demonstrate%20that%20our%20approach%20produces%20well-calibrated%0Auncertainty%20estimates.%20We%20release%20the%20code%20at%0Ahttps%3A//github.com/abenechehab/dicl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11711v1&entry.124074799=Read"},
{"title": "Equivariant Diffusion Policy", "author": "Dian Wang and Stephen Hart and David Surovik and Tarik Kelestemur and Haojie Huang and Haibo Zhao and Mark Yeatman and Jiuguang Wang and Robin Walters and Robert Platt", "abstract": "  Recent work has shown diffusion models are an effective approach to learning\nthe multimodal distributions arising from demonstration data in behavior\ncloning. However, a drawback of this approach is the need to learn a denoising\nfunction, which is significantly more complex than learning an explicit policy.\nIn this work, we propose Equivariant Diffusion Policy, a novel diffusion policy\nlearning method that leverages domain symmetries to obtain better sample\nefficiency and generalization in the denoising function. We theoretically\nanalyze the $\\mathrm{SO}(2)$ symmetry of full 6-DoF control and characterize\nwhen a diffusion model is $\\mathrm{SO}(2)$-equivariant. We furthermore evaluate\nthe method empirically on a set of 12 simulation tasks in MimicGen, and show\nthat it obtains a success rate that is, on average, 21.9% higher than the\nbaseline Diffusion Policy. We also evaluate the method on a real-world system\nto show that effective policies can be learned with relatively few training\nsamples, whereas the baseline Diffusion Policy cannot.\n", "link": "http://arxiv.org/abs/2407.01812v3", "date": "2024-10-15", "relevancy": 1.6137, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5739}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5276}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Diffusion%20Policy&body=Title%3A%20Equivariant%20Diffusion%20Policy%0AAuthor%3A%20Dian%20Wang%20and%20Stephen%20Hart%20and%20David%20Surovik%20and%20Tarik%20Kelestemur%20and%20Haojie%20Huang%20and%20Haibo%20Zhao%20and%20Mark%20Yeatman%20and%20Jiuguang%20Wang%20and%20Robin%20Walters%20and%20Robert%20Platt%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20diffusion%20models%20are%20an%20effective%20approach%20to%20learning%0Athe%20multimodal%20distributions%20arising%20from%20demonstration%20data%20in%20behavior%0Acloning.%20However%2C%20a%20drawback%20of%20this%20approach%20is%20the%20need%20to%20learn%20a%20denoising%0Afunction%2C%20which%20is%20significantly%20more%20complex%20than%20learning%20an%20explicit%20policy.%0AIn%20this%20work%2C%20we%20propose%20Equivariant%20Diffusion%20Policy%2C%20a%20novel%20diffusion%20policy%0Alearning%20method%20that%20leverages%20domain%20symmetries%20to%20obtain%20better%20sample%0Aefficiency%20and%20generalization%20in%20the%20denoising%20function.%20We%20theoretically%0Aanalyze%20the%20%24%5Cmathrm%7BSO%7D%282%29%24%20symmetry%20of%20full%206-DoF%20control%20and%20characterize%0Awhen%20a%20diffusion%20model%20is%20%24%5Cmathrm%7BSO%7D%282%29%24-equivariant.%20We%20furthermore%20evaluate%0Athe%20method%20empirically%20on%20a%20set%20of%2012%20simulation%20tasks%20in%20MimicGen%2C%20and%20show%0Athat%20it%20obtains%20a%20success%20rate%20that%20is%2C%20on%20average%2C%2021.9%25%20higher%20than%20the%0Abaseline%20Diffusion%20Policy.%20We%20also%20evaluate%20the%20method%20on%20a%20real-world%20system%0Ato%20show%20that%20effective%20policies%20can%20be%20learned%20with%20relatively%20few%20training%0Asamples%2C%20whereas%20the%20baseline%20Diffusion%20Policy%20cannot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01812v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Diffusion%2520Policy%26entry.906535625%3DDian%2520Wang%2520and%2520Stephen%2520Hart%2520and%2520David%2520Surovik%2520and%2520Tarik%2520Kelestemur%2520and%2520Haojie%2520Huang%2520and%2520Haibo%2520Zhao%2520and%2520Mark%2520Yeatman%2520and%2520Jiuguang%2520Wang%2520and%2520Robin%2520Walters%2520and%2520Robert%2520Platt%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520diffusion%2520models%2520are%2520an%2520effective%2520approach%2520to%2520learning%250Athe%2520multimodal%2520distributions%2520arising%2520from%2520demonstration%2520data%2520in%2520behavior%250Acloning.%2520However%252C%2520a%2520drawback%2520of%2520this%2520approach%2520is%2520the%2520need%2520to%2520learn%2520a%2520denoising%250Afunction%252C%2520which%2520is%2520significantly%2520more%2520complex%2520than%2520learning%2520an%2520explicit%2520policy.%250AIn%2520this%2520work%252C%2520we%2520propose%2520Equivariant%2520Diffusion%2520Policy%252C%2520a%2520novel%2520diffusion%2520policy%250Alearning%2520method%2520that%2520leverages%2520domain%2520symmetries%2520to%2520obtain%2520better%2520sample%250Aefficiency%2520and%2520generalization%2520in%2520the%2520denoising%2520function.%2520We%2520theoretically%250Aanalyze%2520the%2520%2524%255Cmathrm%257BSO%257D%25282%2529%2524%2520symmetry%2520of%2520full%25206-DoF%2520control%2520and%2520characterize%250Awhen%2520a%2520diffusion%2520model%2520is%2520%2524%255Cmathrm%257BSO%257D%25282%2529%2524-equivariant.%2520We%2520furthermore%2520evaluate%250Athe%2520method%2520empirically%2520on%2520a%2520set%2520of%252012%2520simulation%2520tasks%2520in%2520MimicGen%252C%2520and%2520show%250Athat%2520it%2520obtains%2520a%2520success%2520rate%2520that%2520is%252C%2520on%2520average%252C%252021.9%2525%2520higher%2520than%2520the%250Abaseline%2520Diffusion%2520Policy.%2520We%2520also%2520evaluate%2520the%2520method%2520on%2520a%2520real-world%2520system%250Ato%2520show%2520that%2520effective%2520policies%2520can%2520be%2520learned%2520with%2520relatively%2520few%2520training%250Asamples%252C%2520whereas%2520the%2520baseline%2520Diffusion%2520Policy%2520cannot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01812v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Diffusion%20Policy&entry.906535625=Dian%20Wang%20and%20Stephen%20Hart%20and%20David%20Surovik%20and%20Tarik%20Kelestemur%20and%20Haojie%20Huang%20and%20Haibo%20Zhao%20and%20Mark%20Yeatman%20and%20Jiuguang%20Wang%20and%20Robin%20Walters%20and%20Robert%20Platt&entry.1292438233=%20%20Recent%20work%20has%20shown%20diffusion%20models%20are%20an%20effective%20approach%20to%20learning%0Athe%20multimodal%20distributions%20arising%20from%20demonstration%20data%20in%20behavior%0Acloning.%20However%2C%20a%20drawback%20of%20this%20approach%20is%20the%20need%20to%20learn%20a%20denoising%0Afunction%2C%20which%20is%20significantly%20more%20complex%20than%20learning%20an%20explicit%20policy.%0AIn%20this%20work%2C%20we%20propose%20Equivariant%20Diffusion%20Policy%2C%20a%20novel%20diffusion%20policy%0Alearning%20method%20that%20leverages%20domain%20symmetries%20to%20obtain%20better%20sample%0Aefficiency%20and%20generalization%20in%20the%20denoising%20function.%20We%20theoretically%0Aanalyze%20the%20%24%5Cmathrm%7BSO%7D%282%29%24%20symmetry%20of%20full%206-DoF%20control%20and%20characterize%0Awhen%20a%20diffusion%20model%20is%20%24%5Cmathrm%7BSO%7D%282%29%24-equivariant.%20We%20furthermore%20evaluate%0Athe%20method%20empirically%20on%20a%20set%20of%2012%20simulation%20tasks%20in%20MimicGen%2C%20and%20show%0Athat%20it%20obtains%20a%20success%20rate%20that%20is%2C%20on%20average%2C%2021.9%25%20higher%20than%20the%0Abaseline%20Diffusion%20Policy.%20We%20also%20evaluate%20the%20method%20on%20a%20real-world%20system%0Ato%20show%20that%20effective%20policies%20can%20be%20learned%20with%20relatively%20few%20training%0Asamples%2C%20whereas%20the%20baseline%20Diffusion%20Policy%20cannot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01812v3&entry.124074799=Read"},
{"title": "Prompt a Robot to Walk with Large Language Models", "author": "Yen-Jen Wang and Bike Zhang and Jianyu Chen and Koushil Sreenath", "abstract": "  Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .\n", "link": "http://arxiv.org/abs/2309.09969v3", "date": "2024-10-15", "relevancy": 1.6762, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5938}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5864}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20a%20Robot%20to%20Walk%20with%20Large%20Language%20Models&body=Title%3A%20Prompt%20a%20Robot%20to%20Walk%20with%20Large%20Language%20Models%0AAuthor%3A%20Yen-Jen%20Wang%20and%20Bike%20Zhang%20and%20Jianyu%20Chen%20and%20Koushil%20Sreenath%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20pre-trained%20on%20vast%20internet-scale%20data%20have%0Ashowcased%20remarkable%20capabilities%20across%20diverse%20domains.%20Recently%2C%20there%20has%0Abeen%20escalating%20interest%20in%20deploying%20LLMs%20for%20robotics%2C%20aiming%20to%20harness%20the%0Apower%20of%20foundation%20models%20in%20real-world%20settings.%20However%2C%20this%20approach%20faces%0Asignificant%20challenges%2C%20particularly%20in%20grounding%20these%20models%20in%20the%20physical%0Aworld%20and%20in%20generating%20dynamic%20robot%20motions.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20paradigm%20in%20which%20we%20use%20few-shot%20prompts%20collected%20from%20the%0Aphysical%20environment%2C%20enabling%20the%20LLM%20to%20autoregressively%20generate%20low-level%0Acontrol%20commands%20for%20robots%20without%20task-specific%20fine-tuning.%20Experiments%0Aacross%20various%20robots%20and%20environments%20validate%20that%20our%20method%20can%20effectively%0Aprompt%20a%20robot%20to%20walk.%20We%20thus%20illustrate%20how%20LLMs%20can%20proficiently%20function%0Aas%20low-level%20feedback%20controllers%20for%20dynamic%20motion%20control%20even%20in%0Ahigh-dimensional%20robotic%20systems.%20The%20project%20website%20and%20source%20code%20can%20be%0Afound%20at%3A%20https%3A//prompt2walk.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09969v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520a%2520Robot%2520to%2520Walk%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DYen-Jen%2520Wang%2520and%2520Bike%2520Zhang%2520and%2520Jianyu%2520Chen%2520and%2520Koushil%2520Sreenath%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520pre-trained%2520on%2520vast%2520internet-scale%2520data%2520have%250Ashowcased%2520remarkable%2520capabilities%2520across%2520diverse%2520domains.%2520Recently%252C%2520there%2520has%250Abeen%2520escalating%2520interest%2520in%2520deploying%2520LLMs%2520for%2520robotics%252C%2520aiming%2520to%2520harness%2520the%250Apower%2520of%2520foundation%2520models%2520in%2520real-world%2520settings.%2520However%252C%2520this%2520approach%2520faces%250Asignificant%2520challenges%252C%2520particularly%2520in%2520grounding%2520these%2520models%2520in%2520the%2520physical%250Aworld%2520and%2520in%2520generating%2520dynamic%2520robot%2520motions.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520novel%2520paradigm%2520in%2520which%2520we%2520use%2520few-shot%2520prompts%2520collected%2520from%2520the%250Aphysical%2520environment%252C%2520enabling%2520the%2520LLM%2520to%2520autoregressively%2520generate%2520low-level%250Acontrol%2520commands%2520for%2520robots%2520without%2520task-specific%2520fine-tuning.%2520Experiments%250Aacross%2520various%2520robots%2520and%2520environments%2520validate%2520that%2520our%2520method%2520can%2520effectively%250Aprompt%2520a%2520robot%2520to%2520walk.%2520We%2520thus%2520illustrate%2520how%2520LLMs%2520can%2520proficiently%2520function%250Aas%2520low-level%2520feedback%2520controllers%2520for%2520dynamic%2520motion%2520control%2520even%2520in%250Ahigh-dimensional%2520robotic%2520systems.%2520The%2520project%2520website%2520and%2520source%2520code%2520can%2520be%250Afound%2520at%253A%2520https%253A//prompt2walk.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09969v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20a%20Robot%20to%20Walk%20with%20Large%20Language%20Models&entry.906535625=Yen-Jen%20Wang%20and%20Bike%20Zhang%20and%20Jianyu%20Chen%20and%20Koushil%20Sreenath&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20pre-trained%20on%20vast%20internet-scale%20data%20have%0Ashowcased%20remarkable%20capabilities%20across%20diverse%20domains.%20Recently%2C%20there%20has%0Abeen%20escalating%20interest%20in%20deploying%20LLMs%20for%20robotics%2C%20aiming%20to%20harness%20the%0Apower%20of%20foundation%20models%20in%20real-world%20settings.%20However%2C%20this%20approach%20faces%0Asignificant%20challenges%2C%20particularly%20in%20grounding%20these%20models%20in%20the%20physical%0Aworld%20and%20in%20generating%20dynamic%20robot%20motions.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20paradigm%20in%20which%20we%20use%20few-shot%20prompts%20collected%20from%20the%0Aphysical%20environment%2C%20enabling%20the%20LLM%20to%20autoregressively%20generate%20low-level%0Acontrol%20commands%20for%20robots%20without%20task-specific%20fine-tuning.%20Experiments%0Aacross%20various%20robots%20and%20environments%20validate%20that%20our%20method%20can%20effectively%0Aprompt%20a%20robot%20to%20walk.%20We%20thus%20illustrate%20how%20LLMs%20can%20proficiently%20function%0Aas%20low-level%20feedback%20controllers%20for%20dynamic%20motion%20control%20even%20in%0Ahigh-dimensional%20robotic%20systems.%20The%20project%20website%20and%20source%20code%20can%20be%0Afound%20at%3A%20https%3A//prompt2walk.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09969v3&entry.124074799=Read"},
{"title": "TADA: Temporal Adversarial Data Augmentation for Time Series Data", "author": "Byeong Tak Lee and Joon-myoung Kwon and Yong-Yeon Jo", "abstract": "  Domain generalization aim to train models to effectively perform on samples\nthat are unseen and outside of the distribution. Adversarial data augmentation\n(ADA) is a widely used technique in domain generalization. It enhances the\nmodel robustness by including synthetic samples designed to simulate potential\nunseen scenarios into the training datasets, which is then used to train the\nmodel. However, in time series data, traditional ADA approaches often fail to\naddress distribution shifts related to temporal characteristics. To address\nthis limitation, we propose Temporal Adversarial Data Augmentation (TADA) for\ntime series data, which incorporate time warping into ADA. Although time\nwarping is inherently non-differentiable, ADA relies on generating samples\nthrough backpropagation. We resolve this issue by leveraging the duality\nbetween phase shifts in the frequency domain and time shifts in the time\ndomain, thereby making the process differentiable. Our evaluations across\nvarious time series datasets demonstrate that TADA outperforms existing methods\nfor domain generalization. In addition, using distribution visualization, we\nconfirmed that the distribution shifts induced by TADA are clearly different\nfrom those induced by ADA, and together, they effectively simulate real-world\ndistribution shifts.\n", "link": "http://arxiv.org/abs/2407.15174v2", "date": "2024-10-15", "relevancy": 2.0415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5145}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TADA%3A%20Temporal%20Adversarial%20Data%20Augmentation%20for%20Time%20Series%20Data&body=Title%3A%20TADA%3A%20Temporal%20Adversarial%20Data%20Augmentation%20for%20Time%20Series%20Data%0AAuthor%3A%20Byeong%20Tak%20Lee%20and%20Joon-myoung%20Kwon%20and%20Yong-Yeon%20Jo%0AAbstract%3A%20%20%20Domain%20generalization%20aim%20to%20train%20models%20to%20effectively%20perform%20on%20samples%0Athat%20are%20unseen%20and%20outside%20of%20the%20distribution.%20Adversarial%20data%20augmentation%0A%28ADA%29%20is%20a%20widely%20used%20technique%20in%20domain%20generalization.%20It%20enhances%20the%0Amodel%20robustness%20by%20including%20synthetic%20samples%20designed%20to%20simulate%20potential%0Aunseen%20scenarios%20into%20the%20training%20datasets%2C%20which%20is%20then%20used%20to%20train%20the%0Amodel.%20However%2C%20in%20time%20series%20data%2C%20traditional%20ADA%20approaches%20often%20fail%20to%0Aaddress%20distribution%20shifts%20related%20to%20temporal%20characteristics.%20To%20address%0Athis%20limitation%2C%20we%20propose%20Temporal%20Adversarial%20Data%20Augmentation%20%28TADA%29%20for%0Atime%20series%20data%2C%20which%20incorporate%20time%20warping%20into%20ADA.%20Although%20time%0Awarping%20is%20inherently%20non-differentiable%2C%20ADA%20relies%20on%20generating%20samples%0Athrough%20backpropagation.%20We%20resolve%20this%20issue%20by%20leveraging%20the%20duality%0Abetween%20phase%20shifts%20in%20the%20frequency%20domain%20and%20time%20shifts%20in%20the%20time%0Adomain%2C%20thereby%20making%20the%20process%20differentiable.%20Our%20evaluations%20across%0Avarious%20time%20series%20datasets%20demonstrate%20that%20TADA%20outperforms%20existing%20methods%0Afor%20domain%20generalization.%20In%20addition%2C%20using%20distribution%20visualization%2C%20we%0Aconfirmed%20that%20the%20distribution%20shifts%20induced%20by%20TADA%20are%20clearly%20different%0Afrom%20those%20induced%20by%20ADA%2C%20and%20together%2C%20they%20effectively%20simulate%20real-world%0Adistribution%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTADA%253A%2520Temporal%2520Adversarial%2520Data%2520Augmentation%2520for%2520Time%2520Series%2520Data%26entry.906535625%3DByeong%2520Tak%2520Lee%2520and%2520Joon-myoung%2520Kwon%2520and%2520Yong-Yeon%2520Jo%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520aim%2520to%2520train%2520models%2520to%2520effectively%2520perform%2520on%2520samples%250Athat%2520are%2520unseen%2520and%2520outside%2520of%2520the%2520distribution.%2520Adversarial%2520data%2520augmentation%250A%2528ADA%2529%2520is%2520a%2520widely%2520used%2520technique%2520in%2520domain%2520generalization.%2520It%2520enhances%2520the%250Amodel%2520robustness%2520by%2520including%2520synthetic%2520samples%2520designed%2520to%2520simulate%2520potential%250Aunseen%2520scenarios%2520into%2520the%2520training%2520datasets%252C%2520which%2520is%2520then%2520used%2520to%2520train%2520the%250Amodel.%2520However%252C%2520in%2520time%2520series%2520data%252C%2520traditional%2520ADA%2520approaches%2520often%2520fail%2520to%250Aaddress%2520distribution%2520shifts%2520related%2520to%2520temporal%2520characteristics.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520propose%2520Temporal%2520Adversarial%2520Data%2520Augmentation%2520%2528TADA%2529%2520for%250Atime%2520series%2520data%252C%2520which%2520incorporate%2520time%2520warping%2520into%2520ADA.%2520Although%2520time%250Awarping%2520is%2520inherently%2520non-differentiable%252C%2520ADA%2520relies%2520on%2520generating%2520samples%250Athrough%2520backpropagation.%2520We%2520resolve%2520this%2520issue%2520by%2520leveraging%2520the%2520duality%250Abetween%2520phase%2520shifts%2520in%2520the%2520frequency%2520domain%2520and%2520time%2520shifts%2520in%2520the%2520time%250Adomain%252C%2520thereby%2520making%2520the%2520process%2520differentiable.%2520Our%2520evaluations%2520across%250Avarious%2520time%2520series%2520datasets%2520demonstrate%2520that%2520TADA%2520outperforms%2520existing%2520methods%250Afor%2520domain%2520generalization.%2520In%2520addition%252C%2520using%2520distribution%2520visualization%252C%2520we%250Aconfirmed%2520that%2520the%2520distribution%2520shifts%2520induced%2520by%2520TADA%2520are%2520clearly%2520different%250Afrom%2520those%2520induced%2520by%2520ADA%252C%2520and%2520together%252C%2520they%2520effectively%2520simulate%2520real-world%250Adistribution%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TADA%3A%20Temporal%20Adversarial%20Data%20Augmentation%20for%20Time%20Series%20Data&entry.906535625=Byeong%20Tak%20Lee%20and%20Joon-myoung%20Kwon%20and%20Yong-Yeon%20Jo&entry.1292438233=%20%20Domain%20generalization%20aim%20to%20train%20models%20to%20effectively%20perform%20on%20samples%0Athat%20are%20unseen%20and%20outside%20of%20the%20distribution.%20Adversarial%20data%20augmentation%0A%28ADA%29%20is%20a%20widely%20used%20technique%20in%20domain%20generalization.%20It%20enhances%20the%0Amodel%20robustness%20by%20including%20synthetic%20samples%20designed%20to%20simulate%20potential%0Aunseen%20scenarios%20into%20the%20training%20datasets%2C%20which%20is%20then%20used%20to%20train%20the%0Amodel.%20However%2C%20in%20time%20series%20data%2C%20traditional%20ADA%20approaches%20often%20fail%20to%0Aaddress%20distribution%20shifts%20related%20to%20temporal%20characteristics.%20To%20address%0Athis%20limitation%2C%20we%20propose%20Temporal%20Adversarial%20Data%20Augmentation%20%28TADA%29%20for%0Atime%20series%20data%2C%20which%20incorporate%20time%20warping%20into%20ADA.%20Although%20time%0Awarping%20is%20inherently%20non-differentiable%2C%20ADA%20relies%20on%20generating%20samples%0Athrough%20backpropagation.%20We%20resolve%20this%20issue%20by%20leveraging%20the%20duality%0Abetween%20phase%20shifts%20in%20the%20frequency%20domain%20and%20time%20shifts%20in%20the%20time%0Adomain%2C%20thereby%20making%20the%20process%20differentiable.%20Our%20evaluations%20across%0Avarious%20time%20series%20datasets%20demonstrate%20that%20TADA%20outperforms%20existing%20methods%0Afor%20domain%20generalization.%20In%20addition%2C%20using%20distribution%20visualization%2C%20we%0Aconfirmed%20that%20the%20distribution%20shifts%20induced%20by%20TADA%20are%20clearly%20different%0Afrom%20those%20induced%20by%20ADA%2C%20and%20together%2C%20they%20effectively%20simulate%20real-world%0Adistribution%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15174v2&entry.124074799=Read"},
{"title": "MoH: Multi-Head Attention as Mixture-of-Head Attention", "author": "Peng Jin and Bo Zhu and Li Yuan and Shuicheng Yan", "abstract": "  In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.\n", "link": "http://arxiv.org/abs/2410.11842v1", "date": "2024-10-15", "relevancy": 2.0848, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5437}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.52}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoH%3A%20Multi-Head%20Attention%20as%20Mixture-of-Head%20Attention&body=Title%3A%20MoH%3A%20Multi-Head%20Attention%20as%20Mixture-of-Head%20Attention%0AAuthor%3A%20Peng%20Jin%20and%20Bo%20Zhu%20and%20Li%20Yuan%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20upgrade%20the%20multi-head%20attention%20mechanism%2C%20the%20core%20of%20the%0ATransformer%20model%2C%20to%20improve%20efficiency%20while%20maintaining%20or%20surpassing%20the%0Aprevious%20accuracy%20level.%20We%20show%20that%20multi-head%20attention%20can%20be%20expressed%20in%0Athe%20summation%20form.%20Drawing%20on%20the%20insight%20that%20not%20all%20attention%20heads%20hold%0Aequal%20significance%2C%20we%20propose%20Mixture-of-Head%20attention%20%28MoH%29%2C%20a%20new%0Aarchitecture%20that%20treats%20attention%20heads%20as%20experts%20in%20the%20Mixture-of-Experts%0A%28MoE%29%20mechanism.%20MoH%20has%20two%20significant%20advantages%3A%20First%2C%20MoH%20enables%20each%0Atoken%20to%20select%20the%20appropriate%20attention%20heads%2C%20enhancing%20inference%20efficiency%0Awithout%20compromising%20accuracy%20or%20increasing%20the%20number%20of%20parameters.%20Second%2C%0AMoH%20replaces%20the%20standard%20summation%20in%20multi-head%20attention%20with%20a%20weighted%0Asummation%2C%20introducing%20flexibility%20to%20the%20attention%20mechanism%20and%20unlocking%0Aextra%20performance%20potential.%20Extensive%20experiments%20on%20ViT%2C%20DiT%2C%20and%20LLMs%0Ademonstrate%20that%20MoH%20outperforms%20multi-head%20attention%20by%20using%20only%2050%25-90%25%20of%0Athe%20attention%20heads.%20Moreover%2C%20we%20demonstrate%20that%20pre-trained%20multi-head%0Aattention%20models%2C%20such%20as%20LLaMA3-8B%2C%20can%20be%20further%20continue-tuned%20into%20our%20MoH%0Amodels.%20Notably%2C%20MoH-LLaMA3-8B%20achieves%20an%20average%20accuracy%20of%2064.0%25%20across%2014%0Abenchmarks%2C%20outperforming%20LLaMA3-8B%20by%202.4%25%20by%20utilizing%20only%2075%25%20of%20the%0Aattention%20heads.%20We%20believe%20the%20proposed%20MoH%20is%20a%20promising%20alternative%20to%0Amulti-head%20attention%20and%20provides%20a%20strong%20foundation%20for%20developing%20advanced%0Aand%20efficient%20attention-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoH%253A%2520Multi-Head%2520Attention%2520as%2520Mixture-of-Head%2520Attention%26entry.906535625%3DPeng%2520Jin%2520and%2520Bo%2520Zhu%2520and%2520Li%2520Yuan%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520upgrade%2520the%2520multi-head%2520attention%2520mechanism%252C%2520the%2520core%2520of%2520the%250ATransformer%2520model%252C%2520to%2520improve%2520efficiency%2520while%2520maintaining%2520or%2520surpassing%2520the%250Aprevious%2520accuracy%2520level.%2520We%2520show%2520that%2520multi-head%2520attention%2520can%2520be%2520expressed%2520in%250Athe%2520summation%2520form.%2520Drawing%2520on%2520the%2520insight%2520that%2520not%2520all%2520attention%2520heads%2520hold%250Aequal%2520significance%252C%2520we%2520propose%2520Mixture-of-Head%2520attention%2520%2528MoH%2529%252C%2520a%2520new%250Aarchitecture%2520that%2520treats%2520attention%2520heads%2520as%2520experts%2520in%2520the%2520Mixture-of-Experts%250A%2528MoE%2529%2520mechanism.%2520MoH%2520has%2520two%2520significant%2520advantages%253A%2520First%252C%2520MoH%2520enables%2520each%250Atoken%2520to%2520select%2520the%2520appropriate%2520attention%2520heads%252C%2520enhancing%2520inference%2520efficiency%250Awithout%2520compromising%2520accuracy%2520or%2520increasing%2520the%2520number%2520of%2520parameters.%2520Second%252C%250AMoH%2520replaces%2520the%2520standard%2520summation%2520in%2520multi-head%2520attention%2520with%2520a%2520weighted%250Asummation%252C%2520introducing%2520flexibility%2520to%2520the%2520attention%2520mechanism%2520and%2520unlocking%250Aextra%2520performance%2520potential.%2520Extensive%2520experiments%2520on%2520ViT%252C%2520DiT%252C%2520and%2520LLMs%250Ademonstrate%2520that%2520MoH%2520outperforms%2520multi-head%2520attention%2520by%2520using%2520only%252050%2525-90%2525%2520of%250Athe%2520attention%2520heads.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520pre-trained%2520multi-head%250Aattention%2520models%252C%2520such%2520as%2520LLaMA3-8B%252C%2520can%2520be%2520further%2520continue-tuned%2520into%2520our%2520MoH%250Amodels.%2520Notably%252C%2520MoH-LLaMA3-8B%2520achieves%2520an%2520average%2520accuracy%2520of%252064.0%2525%2520across%252014%250Abenchmarks%252C%2520outperforming%2520LLaMA3-8B%2520by%25202.4%2525%2520by%2520utilizing%2520only%252075%2525%2520of%2520the%250Aattention%2520heads.%2520We%2520believe%2520the%2520proposed%2520MoH%2520is%2520a%2520promising%2520alternative%2520to%250Amulti-head%2520attention%2520and%2520provides%2520a%2520strong%2520foundation%2520for%2520developing%2520advanced%250Aand%2520efficient%2520attention-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoH%3A%20Multi-Head%20Attention%20as%20Mixture-of-Head%20Attention&entry.906535625=Peng%20Jin%20and%20Bo%20Zhu%20and%20Li%20Yuan%20and%20Shuicheng%20Yan&entry.1292438233=%20%20In%20this%20work%2C%20we%20upgrade%20the%20multi-head%20attention%20mechanism%2C%20the%20core%20of%20the%0ATransformer%20model%2C%20to%20improve%20efficiency%20while%20maintaining%20or%20surpassing%20the%0Aprevious%20accuracy%20level.%20We%20show%20that%20multi-head%20attention%20can%20be%20expressed%20in%0Athe%20summation%20form.%20Drawing%20on%20the%20insight%20that%20not%20all%20attention%20heads%20hold%0Aequal%20significance%2C%20we%20propose%20Mixture-of-Head%20attention%20%28MoH%29%2C%20a%20new%0Aarchitecture%20that%20treats%20attention%20heads%20as%20experts%20in%20the%20Mixture-of-Experts%0A%28MoE%29%20mechanism.%20MoH%20has%20two%20significant%20advantages%3A%20First%2C%20MoH%20enables%20each%0Atoken%20to%20select%20the%20appropriate%20attention%20heads%2C%20enhancing%20inference%20efficiency%0Awithout%20compromising%20accuracy%20or%20increasing%20the%20number%20of%20parameters.%20Second%2C%0AMoH%20replaces%20the%20standard%20summation%20in%20multi-head%20attention%20with%20a%20weighted%0Asummation%2C%20introducing%20flexibility%20to%20the%20attention%20mechanism%20and%20unlocking%0Aextra%20performance%20potential.%20Extensive%20experiments%20on%20ViT%2C%20DiT%2C%20and%20LLMs%0Ademonstrate%20that%20MoH%20outperforms%20multi-head%20attention%20by%20using%20only%2050%25-90%25%20of%0Athe%20attention%20heads.%20Moreover%2C%20we%20demonstrate%20that%20pre-trained%20multi-head%0Aattention%20models%2C%20such%20as%20LLaMA3-8B%2C%20can%20be%20further%20continue-tuned%20into%20our%20MoH%0Amodels.%20Notably%2C%20MoH-LLaMA3-8B%20achieves%20an%20average%20accuracy%20of%2064.0%25%20across%2014%0Abenchmarks%2C%20outperforming%20LLaMA3-8B%20by%202.4%25%20by%20utilizing%20only%2075%25%20of%20the%0Aattention%20heads.%20We%20believe%20the%20proposed%20MoH%20is%20a%20promising%20alternative%20to%0Amulti-head%20attention%20and%20provides%20a%20strong%20foundation%20for%20developing%20advanced%0Aand%20efficient%20attention-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11842v1&entry.124074799=Read"},
{"title": "Teaching AI Agents to Search with Reflective-MCTS and Exploratory\n  Learning", "author": "Xiao Yu and Baolin Peng and Vineeth Vajipey and Hao Cheng and Michel Galley and Jianfeng Gao and Zhou Yu", "abstract": "  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon\nplanning tasks. To address these limitations, we present Reflective Monte Carlo\nTree Search (R-MCTS) and Exploratory Learning to build o1-like models for\nagentic applications. We first introduce R-MCTS, a novel test-time algorithm\ndesigned to enhance the ability of AI agents to explore decision space on the\nfly. R-MCTS extends traditional MCTS by 1) incorporating contrastive\nreflection, allowing agents to learn from past interactions and dynamically\nimprove their search efficiency; and 2) using multi-agent debate to provide\nreliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the experience gained from test-time search can be effectively transferred\nback to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1)\ndemonstrates the ability to explore the environment, evaluate a state, and\nbacktrack to viable ones when it detects that the current state cannot lead to\nsuccess, and 2) matches 87% of R-MCTS's performance while using significantly\nless compute. Notably, our work demonstrates the compute scaling properties in\nboth training - data collection with R-MCTS - and testing time. These results\nsuggest a promising research direction to enhance VLMs' reasoning and planning\ncapabilities for agentic applications via test-time search and self-learning.\n", "link": "http://arxiv.org/abs/2410.02052v2", "date": "2024-10-15", "relevancy": 1.6587, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6115}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5403}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20AI%20Agents%20to%20Search%20with%20Reflective-MCTS%20and%20Exploratory%0A%20%20Learning&body=Title%3A%20Teaching%20AI%20Agents%20to%20Search%20with%20Reflective-MCTS%20and%20Exploratory%0A%20%20Learning%0AAuthor%3A%20Xiao%20Yu%20and%20Baolin%20Peng%20and%20Vineeth%20Vajipey%20and%20Hao%20Cheng%20and%20Michel%20Galley%20and%20Jianfeng%20Gao%20and%20Zhou%20Yu%0AAbstract%3A%20%20%20Autonomous%20agents%20have%20demonstrated%20significant%20potential%20in%20automating%0Acomplex%20multistep%20decision-making%20tasks.%20However%2C%20even%20state-of-the-art%0Avision-language%20models%20%28VLMs%29%2C%20such%20as%20GPT-4o%2C%20still%20fall%20short%20of%20human-level%0Aperformance%2C%20particularly%20in%20intricate%20web%20environments%20and%20long-horizon%0Aplanning%20tasks.%20To%20address%20these%20limitations%2C%20we%20present%20Reflective%20Monte%20Carlo%0ATree%20Search%20%28R-MCTS%29%20and%20Exploratory%20Learning%20to%20build%20o1-like%20models%20for%0Aagentic%20applications.%20We%20first%20introduce%20R-MCTS%2C%20a%20novel%20test-time%20algorithm%0Adesigned%20to%20enhance%20the%20ability%20of%20AI%20agents%20to%20explore%20decision%20space%20on%20the%0Afly.%20R-MCTS%20extends%20traditional%20MCTS%20by%201%29%20incorporating%20contrastive%0Areflection%2C%20allowing%20agents%20to%20learn%20from%20past%20interactions%20and%20dynamically%0Aimprove%20their%20search%20efficiency%3B%20and%202%29%20using%20multi-agent%20debate%20to%20provide%0Areliable%20state%20evaluation.%20Next%2C%20we%20introduce%20Exploratory%20Learning%2C%20a%20novel%0Alearning%20strategy%20to%20teach%20agents%20to%20search%20at%20inference%20time%20without%20relying%0Aon%20any%20external%20search%20algorithms.%20On%20the%20challenging%20VisualWebArena%20benchmark%2C%0Aour%20GPT-4o-based%20R-MCTS%20agent%20achieves%20a%206%25%20to%2030%25%20relative%20improvement%20across%0Avarious%20tasks%20compared%20to%20the%20previous%20state-of-the-art.%20Additionally%2C%20we%20show%0Athat%20the%20experience%20gained%20from%20test-time%20search%20can%20be%20effectively%20transferred%0Aback%20to%20GPT-4o%20via%20fine-tuning.%20After%20Exploratory%20Learning%2C%20GPT-4o%201%29%0Ademonstrates%20the%20ability%20to%20explore%20the%20environment%2C%20evaluate%20a%20state%2C%20and%0Abacktrack%20to%20viable%20ones%20when%20it%20detects%20that%20the%20current%20state%20cannot%20lead%20to%0Asuccess%2C%20and%202%29%20matches%2087%25%20of%20R-MCTS%27s%20performance%20while%20using%20significantly%0Aless%20compute.%20Notably%2C%20our%20work%20demonstrates%20the%20compute%20scaling%20properties%20in%0Aboth%20training%20-%20data%20collection%20with%20R-MCTS%20-%20and%20testing%20time.%20These%20results%0Asuggest%20a%20promising%20research%20direction%20to%20enhance%20VLMs%27%20reasoning%20and%20planning%0Acapabilities%20for%20agentic%20applications%20via%20test-time%20search%20and%20self-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520AI%2520Agents%2520to%2520Search%2520with%2520Reflective-MCTS%2520and%2520Exploratory%250A%2520%2520Learning%26entry.906535625%3DXiao%2520Yu%2520and%2520Baolin%2520Peng%2520and%2520Vineeth%2520Vajipey%2520and%2520Hao%2520Cheng%2520and%2520Michel%2520Galley%2520and%2520Jianfeng%2520Gao%2520and%2520Zhou%2520Yu%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520have%2520demonstrated%2520significant%2520potential%2520in%2520automating%250Acomplex%2520multistep%2520decision-making%2520tasks.%2520However%252C%2520even%2520state-of-the-art%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520GPT-4o%252C%2520still%2520fall%2520short%2520of%2520human-level%250Aperformance%252C%2520particularly%2520in%2520intricate%2520web%2520environments%2520and%2520long-horizon%250Aplanning%2520tasks.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520Reflective%2520Monte%2520Carlo%250ATree%2520Search%2520%2528R-MCTS%2529%2520and%2520Exploratory%2520Learning%2520to%2520build%2520o1-like%2520models%2520for%250Aagentic%2520applications.%2520We%2520first%2520introduce%2520R-MCTS%252C%2520a%2520novel%2520test-time%2520algorithm%250Adesigned%2520to%2520enhance%2520the%2520ability%2520of%2520AI%2520agents%2520to%2520explore%2520decision%2520space%2520on%2520the%250Afly.%2520R-MCTS%2520extends%2520traditional%2520MCTS%2520by%25201%2529%2520incorporating%2520contrastive%250Areflection%252C%2520allowing%2520agents%2520to%2520learn%2520from%2520past%2520interactions%2520and%2520dynamically%250Aimprove%2520their%2520search%2520efficiency%253B%2520and%25202%2529%2520using%2520multi-agent%2520debate%2520to%2520provide%250Areliable%2520state%2520evaluation.%2520Next%252C%2520we%2520introduce%2520Exploratory%2520Learning%252C%2520a%2520novel%250Alearning%2520strategy%2520to%2520teach%2520agents%2520to%2520search%2520at%2520inference%2520time%2520without%2520relying%250Aon%2520any%2520external%2520search%2520algorithms.%2520On%2520the%2520challenging%2520VisualWebArena%2520benchmark%252C%250Aour%2520GPT-4o-based%2520R-MCTS%2520agent%2520achieves%2520a%25206%2525%2520to%252030%2525%2520relative%2520improvement%2520across%250Avarious%2520tasks%2520compared%2520to%2520the%2520previous%2520state-of-the-art.%2520Additionally%252C%2520we%2520show%250Athat%2520the%2520experience%2520gained%2520from%2520test-time%2520search%2520can%2520be%2520effectively%2520transferred%250Aback%2520to%2520GPT-4o%2520via%2520fine-tuning.%2520After%2520Exploratory%2520Learning%252C%2520GPT-4o%25201%2529%250Ademonstrates%2520the%2520ability%2520to%2520explore%2520the%2520environment%252C%2520evaluate%2520a%2520state%252C%2520and%250Abacktrack%2520to%2520viable%2520ones%2520when%2520it%2520detects%2520that%2520the%2520current%2520state%2520cannot%2520lead%2520to%250Asuccess%252C%2520and%25202%2529%2520matches%252087%2525%2520of%2520R-MCTS%2527s%2520performance%2520while%2520using%2520significantly%250Aless%2520compute.%2520Notably%252C%2520our%2520work%2520demonstrates%2520the%2520compute%2520scaling%2520properties%2520in%250Aboth%2520training%2520-%2520data%2520collection%2520with%2520R-MCTS%2520-%2520and%2520testing%2520time.%2520These%2520results%250Asuggest%2520a%2520promising%2520research%2520direction%2520to%2520enhance%2520VLMs%2527%2520reasoning%2520and%2520planning%250Acapabilities%2520for%2520agentic%2520applications%2520via%2520test-time%2520search%2520and%2520self-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20AI%20Agents%20to%20Search%20with%20Reflective-MCTS%20and%20Exploratory%0A%20%20Learning&entry.906535625=Xiao%20Yu%20and%20Baolin%20Peng%20and%20Vineeth%20Vajipey%20and%20Hao%20Cheng%20and%20Michel%20Galley%20and%20Jianfeng%20Gao%20and%20Zhou%20Yu&entry.1292438233=%20%20Autonomous%20agents%20have%20demonstrated%20significant%20potential%20in%20automating%0Acomplex%20multistep%20decision-making%20tasks.%20However%2C%20even%20state-of-the-art%0Avision-language%20models%20%28VLMs%29%2C%20such%20as%20GPT-4o%2C%20still%20fall%20short%20of%20human-level%0Aperformance%2C%20particularly%20in%20intricate%20web%20environments%20and%20long-horizon%0Aplanning%20tasks.%20To%20address%20these%20limitations%2C%20we%20present%20Reflective%20Monte%20Carlo%0ATree%20Search%20%28R-MCTS%29%20and%20Exploratory%20Learning%20to%20build%20o1-like%20models%20for%0Aagentic%20applications.%20We%20first%20introduce%20R-MCTS%2C%20a%20novel%20test-time%20algorithm%0Adesigned%20to%20enhance%20the%20ability%20of%20AI%20agents%20to%20explore%20decision%20space%20on%20the%0Afly.%20R-MCTS%20extends%20traditional%20MCTS%20by%201%29%20incorporating%20contrastive%0Areflection%2C%20allowing%20agents%20to%20learn%20from%20past%20interactions%20and%20dynamically%0Aimprove%20their%20search%20efficiency%3B%20and%202%29%20using%20multi-agent%20debate%20to%20provide%0Areliable%20state%20evaluation.%20Next%2C%20we%20introduce%20Exploratory%20Learning%2C%20a%20novel%0Alearning%20strategy%20to%20teach%20agents%20to%20search%20at%20inference%20time%20without%20relying%0Aon%20any%20external%20search%20algorithms.%20On%20the%20challenging%20VisualWebArena%20benchmark%2C%0Aour%20GPT-4o-based%20R-MCTS%20agent%20achieves%20a%206%25%20to%2030%25%20relative%20improvement%20across%0Avarious%20tasks%20compared%20to%20the%20previous%20state-of-the-art.%20Additionally%2C%20we%20show%0Athat%20the%20experience%20gained%20from%20test-time%20search%20can%20be%20effectively%20transferred%0Aback%20to%20GPT-4o%20via%20fine-tuning.%20After%20Exploratory%20Learning%2C%20GPT-4o%201%29%0Ademonstrates%20the%20ability%20to%20explore%20the%20environment%2C%20evaluate%20a%20state%2C%20and%0Abacktrack%20to%20viable%20ones%20when%20it%20detects%20that%20the%20current%20state%20cannot%20lead%20to%0Asuccess%2C%20and%202%29%20matches%2087%25%20of%20R-MCTS%27s%20performance%20while%20using%20significantly%0Aless%20compute.%20Notably%2C%20our%20work%20demonstrates%20the%20compute%20scaling%20properties%20in%0Aboth%20training%20-%20data%20collection%20with%20R-MCTS%20-%20and%20testing%20time.%20These%20results%0Asuggest%20a%20promising%20research%20direction%20to%20enhance%20VLMs%27%20reasoning%20and%20planning%0Acapabilities%20for%20agentic%20applications%20via%20test-time%20search%20and%20self-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02052v2&entry.124074799=Read"},
{"title": "Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering", "author": "Youngsun Lim and Hojun Choi and Hyunjung Shim", "abstract": "  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing text-to-image models can\ncorrectly respond to these questions. The I-HallA v1.0 dataset comprises 1.2K\ndiverse image-text pairs across nine categories with 1,000 rigorously curated\nquestions covering various compositional challenges. We evaluate five\ntext-to-image models using I-HallA and reveal that these state-of-the-art\nmodels often fail to accurately convey factual information. Moreover, we\nvalidate the reliability of our metric by demonstrating a strong Spearman\ncorrelation (rho=0.95) with human judgments. We believe our benchmark dataset\nand metric can serve as a foundation for developing factually accurate\ntext-to-image generation models.\n", "link": "http://arxiv.org/abs/2409.12784v4", "date": "2024-10-15", "relevancy": 2.1031, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5317}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5314}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering&body=Title%3A%20Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering%0AAuthor%3A%20Youngsun%20Lim%20and%20Hojun%20Choi%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Despite%20the%20impressive%20success%20of%20text-to-image%20%28TTI%29%20generation%20models%2C%0Aexisting%20studies%20overlook%20the%20issue%20of%20whether%20these%20models%20accurately%20convey%0Afactual%20information.%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20image%0Ahallucination%2C%20where%20images%20created%20by%20generation%20models%20fail%20to%20faithfully%0Adepict%20factual%20content.%20To%20address%20this%2C%20we%20introduce%20I-HallA%20%28Image%0AHallucination%20evaluation%20with%20Question%20Answering%29%2C%20a%20novel%20automated%20evaluation%0Ametric%20that%20measures%20the%20factuality%20of%20generated%20images%20through%20visual%20question%0Aanswering%20%28VQA%29.%20We%20also%20introduce%20I-HallA%20v1.0%2C%20a%20curated%20benchmark%20dataset%0Afor%20this%20purpose.%20As%20part%20of%20this%20process%2C%20we%20develop%20a%20pipeline%20that%20generates%0Ahigh-quality%20question-answer%20pairs%20using%20multiple%20GPT-4%20Omni-based%20agents%2C%20with%0Ahuman%20judgments%20to%20ensure%20accuracy.%20Our%20evaluation%20protocols%20measure%20image%0Ahallucination%20by%20testing%20if%20images%20from%20existing%20text-to-image%20models%20can%0Acorrectly%20respond%20to%20these%20questions.%20The%20I-HallA%20v1.0%20dataset%20comprises%201.2K%0Adiverse%20image-text%20pairs%20across%20nine%20categories%20with%201%2C000%20rigorously%20curated%0Aquestions%20covering%20various%20compositional%20challenges.%20We%20evaluate%20five%0Atext-to-image%20models%20using%20I-HallA%20and%20reveal%20that%20these%20state-of-the-art%0Amodels%20often%20fail%20to%20accurately%20convey%20factual%20information.%20Moreover%2C%20we%0Avalidate%20the%20reliability%20of%20our%20metric%20by%20demonstrating%20a%20strong%20Spearman%0Acorrelation%20%28rho%3D0.95%29%20with%20human%20judgments.%20We%20believe%20our%20benchmark%20dataset%0Aand%20metric%20can%20serve%20as%20a%20foundation%20for%20developing%20factually%20accurate%0Atext-to-image%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12784v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Image%2520Hallucination%2520in%2520Text-to-Image%2520Generation%2520with%250A%2520%2520Question-Answering%26entry.906535625%3DYoungsun%2520Lim%2520and%2520Hojun%2520Choi%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520success%2520of%2520text-to-image%2520%2528TTI%2529%2520generation%2520models%252C%250Aexisting%2520studies%2520overlook%2520the%2520issue%2520of%2520whether%2520these%2520models%2520accurately%2520convey%250Afactual%2520information.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520problem%2520of%2520image%250Ahallucination%252C%2520where%2520images%2520created%2520by%2520generation%2520models%2520fail%2520to%2520faithfully%250Adepict%2520factual%2520content.%2520To%2520address%2520this%252C%2520we%2520introduce%2520I-HallA%2520%2528Image%250AHallucination%2520evaluation%2520with%2520Question%2520Answering%2529%252C%2520a%2520novel%2520automated%2520evaluation%250Ametric%2520that%2520measures%2520the%2520factuality%2520of%2520generated%2520images%2520through%2520visual%2520question%250Aanswering%2520%2528VQA%2529.%2520We%2520also%2520introduce%2520I-HallA%2520v1.0%252C%2520a%2520curated%2520benchmark%2520dataset%250Afor%2520this%2520purpose.%2520As%2520part%2520of%2520this%2520process%252C%2520we%2520develop%2520a%2520pipeline%2520that%2520generates%250Ahigh-quality%2520question-answer%2520pairs%2520using%2520multiple%2520GPT-4%2520Omni-based%2520agents%252C%2520with%250Ahuman%2520judgments%2520to%2520ensure%2520accuracy.%2520Our%2520evaluation%2520protocols%2520measure%2520image%250Ahallucination%2520by%2520testing%2520if%2520images%2520from%2520existing%2520text-to-image%2520models%2520can%250Acorrectly%2520respond%2520to%2520these%2520questions.%2520The%2520I-HallA%2520v1.0%2520dataset%2520comprises%25201.2K%250Adiverse%2520image-text%2520pairs%2520across%2520nine%2520categories%2520with%25201%252C000%2520rigorously%2520curated%250Aquestions%2520covering%2520various%2520compositional%2520challenges.%2520We%2520evaluate%2520five%250Atext-to-image%2520models%2520using%2520I-HallA%2520and%2520reveal%2520that%2520these%2520state-of-the-art%250Amodels%2520often%2520fail%2520to%2520accurately%2520convey%2520factual%2520information.%2520Moreover%252C%2520we%250Avalidate%2520the%2520reliability%2520of%2520our%2520metric%2520by%2520demonstrating%2520a%2520strong%2520Spearman%250Acorrelation%2520%2528rho%253D0.95%2529%2520with%2520human%2520judgments.%2520We%2520believe%2520our%2520benchmark%2520dataset%250Aand%2520metric%2520can%2520serve%2520as%2520a%2520foundation%2520for%2520developing%2520factually%2520accurate%250Atext-to-image%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12784v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Image%20Hallucination%20in%20Text-to-Image%20Generation%20with%0A%20%20Question-Answering&entry.906535625=Youngsun%20Lim%20and%20Hojun%20Choi%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Despite%20the%20impressive%20success%20of%20text-to-image%20%28TTI%29%20generation%20models%2C%0Aexisting%20studies%20overlook%20the%20issue%20of%20whether%20these%20models%20accurately%20convey%0Afactual%20information.%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20image%0Ahallucination%2C%20where%20images%20created%20by%20generation%20models%20fail%20to%20faithfully%0Adepict%20factual%20content.%20To%20address%20this%2C%20we%20introduce%20I-HallA%20%28Image%0AHallucination%20evaluation%20with%20Question%20Answering%29%2C%20a%20novel%20automated%20evaluation%0Ametric%20that%20measures%20the%20factuality%20of%20generated%20images%20through%20visual%20question%0Aanswering%20%28VQA%29.%20We%20also%20introduce%20I-HallA%20v1.0%2C%20a%20curated%20benchmark%20dataset%0Afor%20this%20purpose.%20As%20part%20of%20this%20process%2C%20we%20develop%20a%20pipeline%20that%20generates%0Ahigh-quality%20question-answer%20pairs%20using%20multiple%20GPT-4%20Omni-based%20agents%2C%20with%0Ahuman%20judgments%20to%20ensure%20accuracy.%20Our%20evaluation%20protocols%20measure%20image%0Ahallucination%20by%20testing%20if%20images%20from%20existing%20text-to-image%20models%20can%0Acorrectly%20respond%20to%20these%20questions.%20The%20I-HallA%20v1.0%20dataset%20comprises%201.2K%0Adiverse%20image-text%20pairs%20across%20nine%20categories%20with%201%2C000%20rigorously%20curated%0Aquestions%20covering%20various%20compositional%20challenges.%20We%20evaluate%20five%0Atext-to-image%20models%20using%20I-HallA%20and%20reveal%20that%20these%20state-of-the-art%0Amodels%20often%20fail%20to%20accurately%20convey%20factual%20information.%20Moreover%2C%20we%0Avalidate%20the%20reliability%20of%20our%20metric%20by%20demonstrating%20a%20strong%20Spearman%0Acorrelation%20%28rho%3D0.95%29%20with%20human%20judgments.%20We%20believe%20our%20benchmark%20dataset%0Aand%20metric%20can%20serve%20as%20a%20foundation%20for%20developing%20factually%20accurate%0Atext-to-image%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12784v4&entry.124074799=Read"},
{"title": "Towards a Healthy AI Tradition: Lessons from Biology and Biomedical\n  Science", "author": "Simon Kasif", "abstract": "  AI is a magnificent field that directly and profoundly touches on numerous\ndisciplines ranging from philosophy, computer science, engineering,\nmathematics, decision and data science and economics, to cognitive science,\nneuroscience and more. The number of applications and impact of AI is second to\nnone and the potential of AI to broadly impact future science developments is\nparticularly thrilling. While attempts to understand knowledge, reasoning,\ncognition and learning go back centuries, AI remains a relatively new field. In\npart due to the fact it has so many wide-ranging overlaps with other disparate\nfields it appears to have trouble developing a robust identity and culture.\nHere we suggest that contrasting the fast-moving AI culture to biological and\nbiomedical sciences is both insightful and useful way to inaugurate a healthy\ntradition needed to envision and manage our ascent to AGI and beyond\n(independent of the AI Platforms used). The co-evolution of AI and Biomedical\nScience offers many benefits to both fields. In a previous perspective, we\nsuggested that biomedical laboratories or centers can usefully embrace logistic\ntraditions in AI labs that will allow them to be highly collaborative, improve\nthe reproducibility of research, reduce risk aversion and produce faster\nmentorship pathways for PhDs and fellows. This perspective focuses on the\nbenefits to AI by adapting features of biomedical science at higher, primarily\ncultural levels.\n", "link": "http://arxiv.org/abs/2410.11590v1", "date": "2024-10-15", "relevancy": 1.607, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4106}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4041}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Healthy%20AI%20Tradition%3A%20Lessons%20from%20Biology%20and%20Biomedical%0A%20%20Science&body=Title%3A%20Towards%20a%20Healthy%20AI%20Tradition%3A%20Lessons%20from%20Biology%20and%20Biomedical%0A%20%20Science%0AAuthor%3A%20Simon%20Kasif%0AAbstract%3A%20%20%20AI%20is%20a%20magnificent%20field%20that%20directly%20and%20profoundly%20touches%20on%20numerous%0Adisciplines%20ranging%20from%20philosophy%2C%20computer%20science%2C%20engineering%2C%0Amathematics%2C%20decision%20and%20data%20science%20and%20economics%2C%20to%20cognitive%20science%2C%0Aneuroscience%20and%20more.%20The%20number%20of%20applications%20and%20impact%20of%20AI%20is%20second%20to%0Anone%20and%20the%20potential%20of%20AI%20to%20broadly%20impact%20future%20science%20developments%20is%0Aparticularly%20thrilling.%20While%20attempts%20to%20understand%20knowledge%2C%20reasoning%2C%0Acognition%20and%20learning%20go%20back%20centuries%2C%20AI%20remains%20a%20relatively%20new%20field.%20In%0Apart%20due%20to%20the%20fact%20it%20has%20so%20many%20wide-ranging%20overlaps%20with%20other%20disparate%0Afields%20it%20appears%20to%20have%20trouble%20developing%20a%20robust%20identity%20and%20culture.%0AHere%20we%20suggest%20that%20contrasting%20the%20fast-moving%20AI%20culture%20to%20biological%20and%0Abiomedical%20sciences%20is%20both%20insightful%20and%20useful%20way%20to%20inaugurate%20a%20healthy%0Atradition%20needed%20to%20envision%20and%20manage%20our%20ascent%20to%20AGI%20and%20beyond%0A%28independent%20of%20the%20AI%20Platforms%20used%29.%20The%20co-evolution%20of%20AI%20and%20Biomedical%0AScience%20offers%20many%20benefits%20to%20both%20fields.%20In%20a%20previous%20perspective%2C%20we%0Asuggested%20that%20biomedical%20laboratories%20or%20centers%20can%20usefully%20embrace%20logistic%0Atraditions%20in%20AI%20labs%20that%20will%20allow%20them%20to%20be%20highly%20collaborative%2C%20improve%0Athe%20reproducibility%20of%20research%2C%20reduce%20risk%20aversion%20and%20produce%20faster%0Amentorship%20pathways%20for%20PhDs%20and%20fellows.%20This%20perspective%20focuses%20on%20the%0Abenefits%20to%20AI%20by%20adapting%20features%20of%20biomedical%20science%20at%20higher%2C%20primarily%0Acultural%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Healthy%2520AI%2520Tradition%253A%2520Lessons%2520from%2520Biology%2520and%2520Biomedical%250A%2520%2520Science%26entry.906535625%3DSimon%2520Kasif%26entry.1292438233%3D%2520%2520AI%2520is%2520a%2520magnificent%2520field%2520that%2520directly%2520and%2520profoundly%2520touches%2520on%2520numerous%250Adisciplines%2520ranging%2520from%2520philosophy%252C%2520computer%2520science%252C%2520engineering%252C%250Amathematics%252C%2520decision%2520and%2520data%2520science%2520and%2520economics%252C%2520to%2520cognitive%2520science%252C%250Aneuroscience%2520and%2520more.%2520The%2520number%2520of%2520applications%2520and%2520impact%2520of%2520AI%2520is%2520second%2520to%250Anone%2520and%2520the%2520potential%2520of%2520AI%2520to%2520broadly%2520impact%2520future%2520science%2520developments%2520is%250Aparticularly%2520thrilling.%2520While%2520attempts%2520to%2520understand%2520knowledge%252C%2520reasoning%252C%250Acognition%2520and%2520learning%2520go%2520back%2520centuries%252C%2520AI%2520remains%2520a%2520relatively%2520new%2520field.%2520In%250Apart%2520due%2520to%2520the%2520fact%2520it%2520has%2520so%2520many%2520wide-ranging%2520overlaps%2520with%2520other%2520disparate%250Afields%2520it%2520appears%2520to%2520have%2520trouble%2520developing%2520a%2520robust%2520identity%2520and%2520culture.%250AHere%2520we%2520suggest%2520that%2520contrasting%2520the%2520fast-moving%2520AI%2520culture%2520to%2520biological%2520and%250Abiomedical%2520sciences%2520is%2520both%2520insightful%2520and%2520useful%2520way%2520to%2520inaugurate%2520a%2520healthy%250Atradition%2520needed%2520to%2520envision%2520and%2520manage%2520our%2520ascent%2520to%2520AGI%2520and%2520beyond%250A%2528independent%2520of%2520the%2520AI%2520Platforms%2520used%2529.%2520The%2520co-evolution%2520of%2520AI%2520and%2520Biomedical%250AScience%2520offers%2520many%2520benefits%2520to%2520both%2520fields.%2520In%2520a%2520previous%2520perspective%252C%2520we%250Asuggested%2520that%2520biomedical%2520laboratories%2520or%2520centers%2520can%2520usefully%2520embrace%2520logistic%250Atraditions%2520in%2520AI%2520labs%2520that%2520will%2520allow%2520them%2520to%2520be%2520highly%2520collaborative%252C%2520improve%250Athe%2520reproducibility%2520of%2520research%252C%2520reduce%2520risk%2520aversion%2520and%2520produce%2520faster%250Amentorship%2520pathways%2520for%2520PhDs%2520and%2520fellows.%2520This%2520perspective%2520focuses%2520on%2520the%250Abenefits%2520to%2520AI%2520by%2520adapting%2520features%2520of%2520biomedical%2520science%2520at%2520higher%252C%2520primarily%250Acultural%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Healthy%20AI%20Tradition%3A%20Lessons%20from%20Biology%20and%20Biomedical%0A%20%20Science&entry.906535625=Simon%20Kasif&entry.1292438233=%20%20AI%20is%20a%20magnificent%20field%20that%20directly%20and%20profoundly%20touches%20on%20numerous%0Adisciplines%20ranging%20from%20philosophy%2C%20computer%20science%2C%20engineering%2C%0Amathematics%2C%20decision%20and%20data%20science%20and%20economics%2C%20to%20cognitive%20science%2C%0Aneuroscience%20and%20more.%20The%20number%20of%20applications%20and%20impact%20of%20AI%20is%20second%20to%0Anone%20and%20the%20potential%20of%20AI%20to%20broadly%20impact%20future%20science%20developments%20is%0Aparticularly%20thrilling.%20While%20attempts%20to%20understand%20knowledge%2C%20reasoning%2C%0Acognition%20and%20learning%20go%20back%20centuries%2C%20AI%20remains%20a%20relatively%20new%20field.%20In%0Apart%20due%20to%20the%20fact%20it%20has%20so%20many%20wide-ranging%20overlaps%20with%20other%20disparate%0Afields%20it%20appears%20to%20have%20trouble%20developing%20a%20robust%20identity%20and%20culture.%0AHere%20we%20suggest%20that%20contrasting%20the%20fast-moving%20AI%20culture%20to%20biological%20and%0Abiomedical%20sciences%20is%20both%20insightful%20and%20useful%20way%20to%20inaugurate%20a%20healthy%0Atradition%20needed%20to%20envision%20and%20manage%20our%20ascent%20to%20AGI%20and%20beyond%0A%28independent%20of%20the%20AI%20Platforms%20used%29.%20The%20co-evolution%20of%20AI%20and%20Biomedical%0AScience%20offers%20many%20benefits%20to%20both%20fields.%20In%20a%20previous%20perspective%2C%20we%0Asuggested%20that%20biomedical%20laboratories%20or%20centers%20can%20usefully%20embrace%20logistic%0Atraditions%20in%20AI%20labs%20that%20will%20allow%20them%20to%20be%20highly%20collaborative%2C%20improve%0Athe%20reproducibility%20of%20research%2C%20reduce%20risk%20aversion%20and%20produce%20faster%0Amentorship%20pathways%20for%20PhDs%20and%20fellows.%20This%20perspective%20focuses%20on%20the%0Abenefits%20to%20AI%20by%20adapting%20features%20of%20biomedical%20science%20at%20higher%2C%20primarily%0Acultural%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11590v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


