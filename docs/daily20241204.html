<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241203.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent\n  Gaussian Reconstruction", "author": "Lingteng Qiu and Shenhao Zhu and Qi Zuo and Xiaodong Gu and Yuan Dong and Junfei Zhang and Chao Xu and Zhe Li and Weihao Yuan and Liefeng Bo and Guanying Chen and Zilong Dong", "abstract": "  Generating animatable human avatars from a single image is essential for\nvarious digital human modeling applications. Existing 3D reconstruction methods\noften struggle to capture fine details in animatable models, while generative\napproaches for controllable animation, though avoiding explicit 3D modeling,\nsuffer from viewpoint inconsistencies in extreme poses and computational\ninefficiencies. In this paper, we address these challenges by leveraging the\npower of generative models to produce detailed multi-view canonical pose\nimages, which help resolve ambiguities in animatable human reconstruction. We\nthen propose a robust method for 3D reconstruction of inconsistent images,\nenabling real-time rendering during inference. Specifically, we adapt a\ntransformer-based video generation model to generate multi-view canonical pose\nimages and normal maps, pretraining on a large-scale video dataset to improve\ngeneralization. To handle view inconsistencies, we recast the reconstruction\nproblem as a 4D task and introduce an efficient 3D modeling approach using 4D\nGaussian Splatting. Experiments demonstrate that our method achieves\nphotorealistic, real-time animation of 3D human avatars from in-the-wild\nimages, showcasing its effectiveness and generalization capability.\n", "link": "http://arxiv.org/abs/2412.02684v1", "date": "2024-12-03", "relevancy": 3.5706, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.729}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7067}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AniGS%3A%20Animatable%20Gaussian%20Avatar%20from%20a%20Single%20Image%20with%20Inconsistent%0A%20%20Gaussian%20Reconstruction&body=Title%3A%20AniGS%3A%20Animatable%20Gaussian%20Avatar%20from%20a%20Single%20Image%20with%20Inconsistent%0A%20%20Gaussian%20Reconstruction%0AAuthor%3A%20Lingteng%20Qiu%20and%20Shenhao%20Zhu%20and%20Qi%20Zuo%20and%20Xiaodong%20Gu%20and%20Yuan%20Dong%20and%20Junfei%20Zhang%20and%20Chao%20Xu%20and%20Zhe%20Li%20and%20Weihao%20Yuan%20and%20Liefeng%20Bo%20and%20Guanying%20Chen%20and%20Zilong%20Dong%0AAbstract%3A%20%20%20Generating%20animatable%20human%20avatars%20from%20a%20single%20image%20is%20essential%20for%0Avarious%20digital%20human%20modeling%20applications.%20Existing%203D%20reconstruction%20methods%0Aoften%20struggle%20to%20capture%20fine%20details%20in%20animatable%20models%2C%20while%20generative%0Aapproaches%20for%20controllable%20animation%2C%20though%20avoiding%20explicit%203D%20modeling%2C%0Asuffer%20from%20viewpoint%20inconsistencies%20in%20extreme%20poses%20and%20computational%0Ainefficiencies.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20leveraging%20the%0Apower%20of%20generative%20models%20to%20produce%20detailed%20multi-view%20canonical%20pose%0Aimages%2C%20which%20help%20resolve%20ambiguities%20in%20animatable%20human%20reconstruction.%20We%0Athen%20propose%20a%20robust%20method%20for%203D%20reconstruction%20of%20inconsistent%20images%2C%0Aenabling%20real-time%20rendering%20during%20inference.%20Specifically%2C%20we%20adapt%20a%0Atransformer-based%20video%20generation%20model%20to%20generate%20multi-view%20canonical%20pose%0Aimages%20and%20normal%20maps%2C%20pretraining%20on%20a%20large-scale%20video%20dataset%20to%20improve%0Ageneralization.%20To%20handle%20view%20inconsistencies%2C%20we%20recast%20the%20reconstruction%0Aproblem%20as%20a%204D%20task%20and%20introduce%20an%20efficient%203D%20modeling%20approach%20using%204D%0AGaussian%20Splatting.%20Experiments%20demonstrate%20that%20our%20method%20achieves%0Aphotorealistic%2C%20real-time%20animation%20of%203D%20human%20avatars%20from%20in-the-wild%0Aimages%2C%20showcasing%20its%20effectiveness%20and%20generalization%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAniGS%253A%2520Animatable%2520Gaussian%2520Avatar%2520from%2520a%2520Single%2520Image%2520with%2520Inconsistent%250A%2520%2520Gaussian%2520Reconstruction%26entry.906535625%3DLingteng%2520Qiu%2520and%2520Shenhao%2520Zhu%2520and%2520Qi%2520Zuo%2520and%2520Xiaodong%2520Gu%2520and%2520Yuan%2520Dong%2520and%2520Junfei%2520Zhang%2520and%2520Chao%2520Xu%2520and%2520Zhe%2520Li%2520and%2520Weihao%2520Yuan%2520and%2520Liefeng%2520Bo%2520and%2520Guanying%2520Chen%2520and%2520Zilong%2520Dong%26entry.1292438233%3D%2520%2520Generating%2520animatable%2520human%2520avatars%2520from%2520a%2520single%2520image%2520is%2520essential%2520for%250Avarious%2520digital%2520human%2520modeling%2520applications.%2520Existing%25203D%2520reconstruction%2520methods%250Aoften%2520struggle%2520to%2520capture%2520fine%2520details%2520in%2520animatable%2520models%252C%2520while%2520generative%250Aapproaches%2520for%2520controllable%2520animation%252C%2520though%2520avoiding%2520explicit%25203D%2520modeling%252C%250Asuffer%2520from%2520viewpoint%2520inconsistencies%2520in%2520extreme%2520poses%2520and%2520computational%250Ainefficiencies.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520challenges%2520by%2520leveraging%2520the%250Apower%2520of%2520generative%2520models%2520to%2520produce%2520detailed%2520multi-view%2520canonical%2520pose%250Aimages%252C%2520which%2520help%2520resolve%2520ambiguities%2520in%2520animatable%2520human%2520reconstruction.%2520We%250Athen%2520propose%2520a%2520robust%2520method%2520for%25203D%2520reconstruction%2520of%2520inconsistent%2520images%252C%250Aenabling%2520real-time%2520rendering%2520during%2520inference.%2520Specifically%252C%2520we%2520adapt%2520a%250Atransformer-based%2520video%2520generation%2520model%2520to%2520generate%2520multi-view%2520canonical%2520pose%250Aimages%2520and%2520normal%2520maps%252C%2520pretraining%2520on%2520a%2520large-scale%2520video%2520dataset%2520to%2520improve%250Ageneralization.%2520To%2520handle%2520view%2520inconsistencies%252C%2520we%2520recast%2520the%2520reconstruction%250Aproblem%2520as%2520a%25204D%2520task%2520and%2520introduce%2520an%2520efficient%25203D%2520modeling%2520approach%2520using%25204D%250AGaussian%2520Splatting.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Aphotorealistic%252C%2520real-time%2520animation%2520of%25203D%2520human%2520avatars%2520from%2520in-the-wild%250Aimages%252C%2520showcasing%2520its%2520effectiveness%2520and%2520generalization%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AniGS%3A%20Animatable%20Gaussian%20Avatar%20from%20a%20Single%20Image%20with%20Inconsistent%0A%20%20Gaussian%20Reconstruction&entry.906535625=Lingteng%20Qiu%20and%20Shenhao%20Zhu%20and%20Qi%20Zuo%20and%20Xiaodong%20Gu%20and%20Yuan%20Dong%20and%20Junfei%20Zhang%20and%20Chao%20Xu%20and%20Zhe%20Li%20and%20Weihao%20Yuan%20and%20Liefeng%20Bo%20and%20Guanying%20Chen%20and%20Zilong%20Dong&entry.1292438233=%20%20Generating%20animatable%20human%20avatars%20from%20a%20single%20image%20is%20essential%20for%0Avarious%20digital%20human%20modeling%20applications.%20Existing%203D%20reconstruction%20methods%0Aoften%20struggle%20to%20capture%20fine%20details%20in%20animatable%20models%2C%20while%20generative%0Aapproaches%20for%20controllable%20animation%2C%20though%20avoiding%20explicit%203D%20modeling%2C%0Asuffer%20from%20viewpoint%20inconsistencies%20in%20extreme%20poses%20and%20computational%0Ainefficiencies.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20leveraging%20the%0Apower%20of%20generative%20models%20to%20produce%20detailed%20multi-view%20canonical%20pose%0Aimages%2C%20which%20help%20resolve%20ambiguities%20in%20animatable%20human%20reconstruction.%20We%0Athen%20propose%20a%20robust%20method%20for%203D%20reconstruction%20of%20inconsistent%20images%2C%0Aenabling%20real-time%20rendering%20during%20inference.%20Specifically%2C%20we%20adapt%20a%0Atransformer-based%20video%20generation%20model%20to%20generate%20multi-view%20canonical%20pose%0Aimages%20and%20normal%20maps%2C%20pretraining%20on%20a%20large-scale%20video%20dataset%20to%20improve%0Ageneralization.%20To%20handle%20view%20inconsistencies%2C%20we%20recast%20the%20reconstruction%0Aproblem%20as%20a%204D%20task%20and%20introduce%20an%20efficient%203D%20modeling%20approach%20using%204D%0AGaussian%20Splatting.%20Experiments%20demonstrate%20that%20our%20method%20achieves%0Aphotorealistic%2C%20real-time%20animation%20of%203D%20human%20avatars%20from%20in-the-wild%0Aimages%2C%20showcasing%20its%20effectiveness%20and%20generalization%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02684v1&entry.124074799=Read"},
{"title": "Sharp-It: A Multi-view to Multi-view Diffusion Model for 3D Synthesis\n  and Manipulation", "author": "Yiftach Edelstein and Or Patashnik and Dana Cohen-Bar and Lihi Zelnik-Manor", "abstract": "  Advancements in text-to-image diffusion models have led to significant\nprogress in fast 3D content creation. One common approach is to generate a set\nof multi-view images of an object, and then reconstruct it into a 3D model.\nHowever, this approach bypasses the use of a native 3D representation of the\nobject and is hence prone to geometric artifacts and limited in controllability\nand manipulation capabilities. An alternative approach involves native 3D\ngenerative models that directly produce 3D representations. These models,\nhowever, are typically limited in their resolution, resulting in lower quality\n3D objects. In this work, we bridge the quality gap between methods that\ndirectly generate 3D representations and ones that reconstruct 3D objects from\nmulti-view images. We introduce a multi-view to multi-view diffusion model\ncalled Sharp-It, which takes a 3D consistent set of multi-view images rendered\nfrom a low-quality object and enriches its geometric details and texture. The\ndiffusion model operates on the multi-view set in parallel, in the sense that\nit shares features across the generated views. A high-quality 3D model can then\nbe reconstructed from the enriched multi-view set. By leveraging the advantages\nof both 2D and 3D approaches, our method offers an efficient and controllable\nmethod for high-quality 3D content creation. We demonstrate that Sharp-It\nenables various 3D applications, such as fast synthesis, editing, and\ncontrolled generation, while attaining high-quality assets.\n", "link": "http://arxiv.org/abs/2412.02631v1", "date": "2024-12-03", "relevancy": 3.546, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7466}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7466}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp-It%3A%20A%20Multi-view%20to%20Multi-view%20Diffusion%20Model%20for%203D%20Synthesis%0A%20%20and%20Manipulation&body=Title%3A%20Sharp-It%3A%20A%20Multi-view%20to%20Multi-view%20Diffusion%20Model%20for%203D%20Synthesis%0A%20%20and%20Manipulation%0AAuthor%3A%20Yiftach%20Edelstein%20and%20Or%20Patashnik%20and%20Dana%20Cohen-Bar%20and%20Lihi%20Zelnik-Manor%0AAbstract%3A%20%20%20Advancements%20in%20text-to-image%20diffusion%20models%20have%20led%20to%20significant%0Aprogress%20in%20fast%203D%20content%20creation.%20One%20common%20approach%20is%20to%20generate%20a%20set%0Aof%20multi-view%20images%20of%20an%20object%2C%20and%20then%20reconstruct%20it%20into%20a%203D%20model.%0AHowever%2C%20this%20approach%20bypasses%20the%20use%20of%20a%20native%203D%20representation%20of%20the%0Aobject%20and%20is%20hence%20prone%20to%20geometric%20artifacts%20and%20limited%20in%20controllability%0Aand%20manipulation%20capabilities.%20An%20alternative%20approach%20involves%20native%203D%0Agenerative%20models%20that%20directly%20produce%203D%20representations.%20These%20models%2C%0Ahowever%2C%20are%20typically%20limited%20in%20their%20resolution%2C%20resulting%20in%20lower%20quality%0A3D%20objects.%20In%20this%20work%2C%20we%20bridge%20the%20quality%20gap%20between%20methods%20that%0Adirectly%20generate%203D%20representations%20and%20ones%20that%20reconstruct%203D%20objects%20from%0Amulti-view%20images.%20We%20introduce%20a%20multi-view%20to%20multi-view%20diffusion%20model%0Acalled%20Sharp-It%2C%20which%20takes%20a%203D%20consistent%20set%20of%20multi-view%20images%20rendered%0Afrom%20a%20low-quality%20object%20and%20enriches%20its%20geometric%20details%20and%20texture.%20The%0Adiffusion%20model%20operates%20on%20the%20multi-view%20set%20in%20parallel%2C%20in%20the%20sense%20that%0Ait%20shares%20features%20across%20the%20generated%20views.%20A%20high-quality%203D%20model%20can%20then%0Abe%20reconstructed%20from%20the%20enriched%20multi-view%20set.%20By%20leveraging%20the%20advantages%0Aof%20both%202D%20and%203D%20approaches%2C%20our%20method%20offers%20an%20efficient%20and%20controllable%0Amethod%20for%20high-quality%203D%20content%20creation.%20We%20demonstrate%20that%20Sharp-It%0Aenables%20various%203D%20applications%2C%20such%20as%20fast%20synthesis%2C%20editing%2C%20and%0Acontrolled%20generation%2C%20while%20attaining%20high-quality%20assets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp-It%253A%2520A%2520Multi-view%2520to%2520Multi-view%2520Diffusion%2520Model%2520for%25203D%2520Synthesis%250A%2520%2520and%2520Manipulation%26entry.906535625%3DYiftach%2520Edelstein%2520and%2520Or%2520Patashnik%2520and%2520Dana%2520Cohen-Bar%2520and%2520Lihi%2520Zelnik-Manor%26entry.1292438233%3D%2520%2520Advancements%2520in%2520text-to-image%2520diffusion%2520models%2520have%2520led%2520to%2520significant%250Aprogress%2520in%2520fast%25203D%2520content%2520creation.%2520One%2520common%2520approach%2520is%2520to%2520generate%2520a%2520set%250Aof%2520multi-view%2520images%2520of%2520an%2520object%252C%2520and%2520then%2520reconstruct%2520it%2520into%2520a%25203D%2520model.%250AHowever%252C%2520this%2520approach%2520bypasses%2520the%2520use%2520of%2520a%2520native%25203D%2520representation%2520of%2520the%250Aobject%2520and%2520is%2520hence%2520prone%2520to%2520geometric%2520artifacts%2520and%2520limited%2520in%2520controllability%250Aand%2520manipulation%2520capabilities.%2520An%2520alternative%2520approach%2520involves%2520native%25203D%250Agenerative%2520models%2520that%2520directly%2520produce%25203D%2520representations.%2520These%2520models%252C%250Ahowever%252C%2520are%2520typically%2520limited%2520in%2520their%2520resolution%252C%2520resulting%2520in%2520lower%2520quality%250A3D%2520objects.%2520In%2520this%2520work%252C%2520we%2520bridge%2520the%2520quality%2520gap%2520between%2520methods%2520that%250Adirectly%2520generate%25203D%2520representations%2520and%2520ones%2520that%2520reconstruct%25203D%2520objects%2520from%250Amulti-view%2520images.%2520We%2520introduce%2520a%2520multi-view%2520to%2520multi-view%2520diffusion%2520model%250Acalled%2520Sharp-It%252C%2520which%2520takes%2520a%25203D%2520consistent%2520set%2520of%2520multi-view%2520images%2520rendered%250Afrom%2520a%2520low-quality%2520object%2520and%2520enriches%2520its%2520geometric%2520details%2520and%2520texture.%2520The%250Adiffusion%2520model%2520operates%2520on%2520the%2520multi-view%2520set%2520in%2520parallel%252C%2520in%2520the%2520sense%2520that%250Ait%2520shares%2520features%2520across%2520the%2520generated%2520views.%2520A%2520high-quality%25203D%2520model%2520can%2520then%250Abe%2520reconstructed%2520from%2520the%2520enriched%2520multi-view%2520set.%2520By%2520leveraging%2520the%2520advantages%250Aof%2520both%25202D%2520and%25203D%2520approaches%252C%2520our%2520method%2520offers%2520an%2520efficient%2520and%2520controllable%250Amethod%2520for%2520high-quality%25203D%2520content%2520creation.%2520We%2520demonstrate%2520that%2520Sharp-It%250Aenables%2520various%25203D%2520applications%252C%2520such%2520as%2520fast%2520synthesis%252C%2520editing%252C%2520and%250Acontrolled%2520generation%252C%2520while%2520attaining%2520high-quality%2520assets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp-It%3A%20A%20Multi-view%20to%20Multi-view%20Diffusion%20Model%20for%203D%20Synthesis%0A%20%20and%20Manipulation&entry.906535625=Yiftach%20Edelstein%20and%20Or%20Patashnik%20and%20Dana%20Cohen-Bar%20and%20Lihi%20Zelnik-Manor&entry.1292438233=%20%20Advancements%20in%20text-to-image%20diffusion%20models%20have%20led%20to%20significant%0Aprogress%20in%20fast%203D%20content%20creation.%20One%20common%20approach%20is%20to%20generate%20a%20set%0Aof%20multi-view%20images%20of%20an%20object%2C%20and%20then%20reconstruct%20it%20into%20a%203D%20model.%0AHowever%2C%20this%20approach%20bypasses%20the%20use%20of%20a%20native%203D%20representation%20of%20the%0Aobject%20and%20is%20hence%20prone%20to%20geometric%20artifacts%20and%20limited%20in%20controllability%0Aand%20manipulation%20capabilities.%20An%20alternative%20approach%20involves%20native%203D%0Agenerative%20models%20that%20directly%20produce%203D%20representations.%20These%20models%2C%0Ahowever%2C%20are%20typically%20limited%20in%20their%20resolution%2C%20resulting%20in%20lower%20quality%0A3D%20objects.%20In%20this%20work%2C%20we%20bridge%20the%20quality%20gap%20between%20methods%20that%0Adirectly%20generate%203D%20representations%20and%20ones%20that%20reconstruct%203D%20objects%20from%0Amulti-view%20images.%20We%20introduce%20a%20multi-view%20to%20multi-view%20diffusion%20model%0Acalled%20Sharp-It%2C%20which%20takes%20a%203D%20consistent%20set%20of%20multi-view%20images%20rendered%0Afrom%20a%20low-quality%20object%20and%20enriches%20its%20geometric%20details%20and%20texture.%20The%0Adiffusion%20model%20operates%20on%20the%20multi-view%20set%20in%20parallel%2C%20in%20the%20sense%20that%0Ait%20shares%20features%20across%20the%20generated%20views.%20A%20high-quality%203D%20model%20can%20then%0Abe%20reconstructed%20from%20the%20enriched%20multi-view%20set.%20By%20leveraging%20the%20advantages%0Aof%20both%202D%20and%203D%20approaches%2C%20our%20method%20offers%20an%20efficient%20and%20controllable%0Amethod%20for%20high-quality%203D%20content%20creation.%20We%20demonstrate%20that%20Sharp-It%0Aenables%20various%203D%20applications%2C%20such%20as%20fast%20synthesis%2C%20editing%2C%20and%0Acontrolled%20generation%2C%20while%20attaining%20high-quality%20assets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02631v1&entry.124074799=Read"},
{"title": "Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction\n  via Spiking Neuron-based Gaussian Splatting", "author": "Weixing Zhang and Zongrui Li and De Ma and Huajin Tang and Xudong Jiang and Qian Zheng and Gang Pan", "abstract": "  3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes.\nDespite recent advances in improving surface reconstruction accuracy, the\nreconstructed results still exhibit bias and suffer from inefficiency in\nstorage and training. This paper provides a different observation on the cause\nof the inefficiency and the reconstruction bias, which is attributed to the\nintegration of the low-opacity parts (LOPs) of the generated Gaussians. We show\nthat LOPs consist of Gaussians with overall low-opacity (LOGs) and the\nlow-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two\ntypes of LOPs by integrating spiking neurons into the Gaussian Splatting\npipeline. Specifically, we introduce global and local full-precision\nintegrate-and-fire spiking neurons to the opacity and representation function\nof flattened 3D Gaussians, respectively. Furthermore, we enhance the density\ncontrol strategy with spiking neurons' thresholds and a new criterion on the\nscale of Gaussians. Our method can represent more accurate reconstructed\nsurfaces at a lower cost. The supplementary material and code are available at\nhttps://github.com/zju-bmi-lab/SpikingGS.\n", "link": "http://arxiv.org/abs/2410.07266v5", "date": "2024-12-03", "relevancy": 3.4204, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7251}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6752}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20GS%3A%20Towards%20High-Accuracy%20and%20Low-Cost%20Surface%20Reconstruction%0A%20%20via%20Spiking%20Neuron-based%20Gaussian%20Splatting&body=Title%3A%20Spiking%20GS%3A%20Towards%20High-Accuracy%20and%20Low-Cost%20Surface%20Reconstruction%0A%20%20via%20Spiking%20Neuron-based%20Gaussian%20Splatting%0AAuthor%3A%20Weixing%20Zhang%20and%20Zongrui%20Li%20and%20De%20Ma%20and%20Huajin%20Tang%20and%20Xudong%20Jiang%20and%20Qian%20Zheng%20and%20Gang%20Pan%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20is%20capable%20of%20reconstructing%203D%20scenes%20in%20minutes.%0ADespite%20recent%20advances%20in%20improving%20surface%20reconstruction%20accuracy%2C%20the%0Areconstructed%20results%20still%20exhibit%20bias%20and%20suffer%20from%20inefficiency%20in%0Astorage%20and%20training.%20This%20paper%20provides%20a%20different%20observation%20on%20the%20cause%0Aof%20the%20inefficiency%20and%20the%20reconstruction%20bias%2C%20which%20is%20attributed%20to%20the%0Aintegration%20of%20the%20low-opacity%20parts%20%28LOPs%29%20of%20the%20generated%20Gaussians.%20We%20show%0Athat%20LOPs%20consist%20of%20Gaussians%20with%20overall%20low-opacity%20%28LOGs%29%20and%20the%0Alow-opacity%20tails%20%28LOTs%29%20of%20Gaussians.%20We%20propose%20Spiking%20GS%20to%20reduce%20such%20two%0Atypes%20of%20LOPs%20by%20integrating%20spiking%20neurons%20into%20the%20Gaussian%20Splatting%0Apipeline.%20Specifically%2C%20we%20introduce%20global%20and%20local%20full-precision%0Aintegrate-and-fire%20spiking%20neurons%20to%20the%20opacity%20and%20representation%20function%0Aof%20flattened%203D%20Gaussians%2C%20respectively.%20Furthermore%2C%20we%20enhance%20the%20density%0Acontrol%20strategy%20with%20spiking%20neurons%27%20thresholds%20and%20a%20new%20criterion%20on%20the%0Ascale%20of%20Gaussians.%20Our%20method%20can%20represent%20more%20accurate%20reconstructed%0Asurfaces%20at%20a%20lower%20cost.%20The%20supplementary%20material%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/zju-bmi-lab/SpikingGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07266v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520GS%253A%2520Towards%2520High-Accuracy%2520and%2520Low-Cost%2520Surface%2520Reconstruction%250A%2520%2520via%2520Spiking%2520Neuron-based%2520Gaussian%2520Splatting%26entry.906535625%3DWeixing%2520Zhang%2520and%2520Zongrui%2520Li%2520and%2520De%2520Ma%2520and%2520Huajin%2520Tang%2520and%2520Xudong%2520Jiang%2520and%2520Qian%2520Zheng%2520and%2520Gang%2520Pan%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520is%2520capable%2520of%2520reconstructing%25203D%2520scenes%2520in%2520minutes.%250ADespite%2520recent%2520advances%2520in%2520improving%2520surface%2520reconstruction%2520accuracy%252C%2520the%250Areconstructed%2520results%2520still%2520exhibit%2520bias%2520and%2520suffer%2520from%2520inefficiency%2520in%250Astorage%2520and%2520training.%2520This%2520paper%2520provides%2520a%2520different%2520observation%2520on%2520the%2520cause%250Aof%2520the%2520inefficiency%2520and%2520the%2520reconstruction%2520bias%252C%2520which%2520is%2520attributed%2520to%2520the%250Aintegration%2520of%2520the%2520low-opacity%2520parts%2520%2528LOPs%2529%2520of%2520the%2520generated%2520Gaussians.%2520We%2520show%250Athat%2520LOPs%2520consist%2520of%2520Gaussians%2520with%2520overall%2520low-opacity%2520%2528LOGs%2529%2520and%2520the%250Alow-opacity%2520tails%2520%2528LOTs%2529%2520of%2520Gaussians.%2520We%2520propose%2520Spiking%2520GS%2520to%2520reduce%2520such%2520two%250Atypes%2520of%2520LOPs%2520by%2520integrating%2520spiking%2520neurons%2520into%2520the%2520Gaussian%2520Splatting%250Apipeline.%2520Specifically%252C%2520we%2520introduce%2520global%2520and%2520local%2520full-precision%250Aintegrate-and-fire%2520spiking%2520neurons%2520to%2520the%2520opacity%2520and%2520representation%2520function%250Aof%2520flattened%25203D%2520Gaussians%252C%2520respectively.%2520Furthermore%252C%2520we%2520enhance%2520the%2520density%250Acontrol%2520strategy%2520with%2520spiking%2520neurons%2527%2520thresholds%2520and%2520a%2520new%2520criterion%2520on%2520the%250Ascale%2520of%2520Gaussians.%2520Our%2520method%2520can%2520represent%2520more%2520accurate%2520reconstructed%250Asurfaces%2520at%2520a%2520lower%2520cost.%2520The%2520supplementary%2520material%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/zju-bmi-lab/SpikingGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07266v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20GS%3A%20Towards%20High-Accuracy%20and%20Low-Cost%20Surface%20Reconstruction%0A%20%20via%20Spiking%20Neuron-based%20Gaussian%20Splatting&entry.906535625=Weixing%20Zhang%20and%20Zongrui%20Li%20and%20De%20Ma%20and%20Huajin%20Tang%20and%20Xudong%20Jiang%20and%20Qian%20Zheng%20and%20Gang%20Pan&entry.1292438233=%20%203D%20Gaussian%20Splatting%20is%20capable%20of%20reconstructing%203D%20scenes%20in%20minutes.%0ADespite%20recent%20advances%20in%20improving%20surface%20reconstruction%20accuracy%2C%20the%0Areconstructed%20results%20still%20exhibit%20bias%20and%20suffer%20from%20inefficiency%20in%0Astorage%20and%20training.%20This%20paper%20provides%20a%20different%20observation%20on%20the%20cause%0Aof%20the%20inefficiency%20and%20the%20reconstruction%20bias%2C%20which%20is%20attributed%20to%20the%0Aintegration%20of%20the%20low-opacity%20parts%20%28LOPs%29%20of%20the%20generated%20Gaussians.%20We%20show%0Athat%20LOPs%20consist%20of%20Gaussians%20with%20overall%20low-opacity%20%28LOGs%29%20and%20the%0Alow-opacity%20tails%20%28LOTs%29%20of%20Gaussians.%20We%20propose%20Spiking%20GS%20to%20reduce%20such%20two%0Atypes%20of%20LOPs%20by%20integrating%20spiking%20neurons%20into%20the%20Gaussian%20Splatting%0Apipeline.%20Specifically%2C%20we%20introduce%20global%20and%20local%20full-precision%0Aintegrate-and-fire%20spiking%20neurons%20to%20the%20opacity%20and%20representation%20function%0Aof%20flattened%203D%20Gaussians%2C%20respectively.%20Furthermore%2C%20we%20enhance%20the%20density%0Acontrol%20strategy%20with%20spiking%20neurons%27%20thresholds%20and%20a%20new%20criterion%20on%20the%0Ascale%20of%20Gaussians.%20Our%20method%20can%20represent%20more%20accurate%20reconstructed%0Asurfaces%20at%20a%20lower%20cost.%20The%20supplementary%20material%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/zju-bmi-lab/SpikingGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07266v5&entry.124074799=Read"},
{"title": "RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex\n  Motions via Relay Gaussians", "author": "Qiankun Gao and Yanmin Wu and Chengxiang Wen and Jiarui Meng and Luyang Tang and Jie Chen and Ronggang Wang and Jian Zhang", "abstract": "  Reconstructing dynamic scenes with large-scale and complex motions remains a\nsignificant challenge. Recent techniques like Neural Radiance Fields and 3D\nGaussian Splatting (3DGS) have shown promise but still struggle with scenes\ninvolving substantial movement. This paper proposes RelayGS, a novel method\nbased on 3DGS, specifically designed to represent and reconstruct highly\ndynamic scenes. Our RelayGS learns a complete 4D representation with canonical\n3D Gaussians and a compact motion field, consisting of three stages. First, we\nlearn a fundamental 3DGS from all frames, ignoring temporal scene variations,\nand use a learnable mask to separate the highly dynamic foreground from the\nminimally moving background. Second, we replicate multiple copies of the\ndecoupled foreground Gaussians from the first stage, each corresponding to a\ntemporal segment, and optimize them using pseudo-views constructed from\nmultiple frames within each segment. These Gaussians, termed Relay Gaussians,\nact as explicit relay nodes, simplifying and breaking down large-scale motion\ntrajectories into smaller, manageable segments. Finally, we jointly learn the\nscene's temporal motion and refine the canonical Gaussians learned from the\nfirst two stages. We conduct thorough experiments on two dynamic scene datasets\nfeaturing large and complex motions, where our RelayGS outperforms\nstate-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs\nreal-world basketball game scenes in a much more complete and coherent manner,\nwhereas previous methods usually struggle to capture the complex motion of\nplayers. Code will be publicly available at https://github.com/gqk/RelayGS\n", "link": "http://arxiv.org/abs/2412.02493v1", "date": "2024-12-03", "relevancy": 3.3596, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6859}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6822}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelayGS%3A%20Reconstructing%20Dynamic%20Scenes%20with%20Large-Scale%20and%20Complex%0A%20%20Motions%20via%20Relay%20Gaussians&body=Title%3A%20RelayGS%3A%20Reconstructing%20Dynamic%20Scenes%20with%20Large-Scale%20and%20Complex%0A%20%20Motions%20via%20Relay%20Gaussians%0AAuthor%3A%20Qiankun%20Gao%20and%20Yanmin%20Wu%20and%20Chengxiang%20Wen%20and%20Jiarui%20Meng%20and%20Luyang%20Tang%20and%20Jie%20Chen%20and%20Ronggang%20Wang%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20Reconstructing%20dynamic%20scenes%20with%20large-scale%20and%20complex%20motions%20remains%20a%0Asignificant%20challenge.%20Recent%20techniques%20like%20Neural%20Radiance%20Fields%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20shown%20promise%20but%20still%20struggle%20with%20scenes%0Ainvolving%20substantial%20movement.%20This%20paper%20proposes%20RelayGS%2C%20a%20novel%20method%0Abased%20on%203DGS%2C%20specifically%20designed%20to%20represent%20and%20reconstruct%20highly%0Adynamic%20scenes.%20Our%20RelayGS%20learns%20a%20complete%204D%20representation%20with%20canonical%0A3D%20Gaussians%20and%20a%20compact%20motion%20field%2C%20consisting%20of%20three%20stages.%20First%2C%20we%0Alearn%20a%20fundamental%203DGS%20from%20all%20frames%2C%20ignoring%20temporal%20scene%20variations%2C%0Aand%20use%20a%20learnable%20mask%20to%20separate%20the%20highly%20dynamic%20foreground%20from%20the%0Aminimally%20moving%20background.%20Second%2C%20we%20replicate%20multiple%20copies%20of%20the%0Adecoupled%20foreground%20Gaussians%20from%20the%20first%20stage%2C%20each%20corresponding%20to%20a%0Atemporal%20segment%2C%20and%20optimize%20them%20using%20pseudo-views%20constructed%20from%0Amultiple%20frames%20within%20each%20segment.%20These%20Gaussians%2C%20termed%20Relay%20Gaussians%2C%0Aact%20as%20explicit%20relay%20nodes%2C%20simplifying%20and%20breaking%20down%20large-scale%20motion%0Atrajectories%20into%20smaller%2C%20manageable%20segments.%20Finally%2C%20we%20jointly%20learn%20the%0Ascene%27s%20temporal%20motion%20and%20refine%20the%20canonical%20Gaussians%20learned%20from%20the%0Afirst%20two%20stages.%20We%20conduct%20thorough%20experiments%20on%20two%20dynamic%20scene%20datasets%0Afeaturing%20large%20and%20complex%20motions%2C%20where%20our%20RelayGS%20outperforms%0Astate-of-the-arts%20by%20more%20than%201%20dB%20in%20PSNR%2C%20and%20successfully%20reconstructs%0Areal-world%20basketball%20game%20scenes%20in%20a%20much%20more%20complete%20and%20coherent%20manner%2C%0Awhereas%20previous%20methods%20usually%20struggle%20to%20capture%20the%20complex%20motion%20of%0Aplayers.%20Code%20will%20be%20publicly%20available%20at%20https%3A//github.com/gqk/RelayGS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelayGS%253A%2520Reconstructing%2520Dynamic%2520Scenes%2520with%2520Large-Scale%2520and%2520Complex%250A%2520%2520Motions%2520via%2520Relay%2520Gaussians%26entry.906535625%3DQiankun%2520Gao%2520and%2520Yanmin%2520Wu%2520and%2520Chengxiang%2520Wen%2520and%2520Jiarui%2520Meng%2520and%2520Luyang%2520Tang%2520and%2520Jie%2520Chen%2520and%2520Ronggang%2520Wang%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%2520scenes%2520with%2520large-scale%2520and%2520complex%2520motions%2520remains%2520a%250Asignificant%2520challenge.%2520Recent%2520techniques%2520like%2520Neural%2520Radiance%2520Fields%2520and%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520have%2520shown%2520promise%2520but%2520still%2520struggle%2520with%2520scenes%250Ainvolving%2520substantial%2520movement.%2520This%2520paper%2520proposes%2520RelayGS%252C%2520a%2520novel%2520method%250Abased%2520on%25203DGS%252C%2520specifically%2520designed%2520to%2520represent%2520and%2520reconstruct%2520highly%250Adynamic%2520scenes.%2520Our%2520RelayGS%2520learns%2520a%2520complete%25204D%2520representation%2520with%2520canonical%250A3D%2520Gaussians%2520and%2520a%2520compact%2520motion%2520field%252C%2520consisting%2520of%2520three%2520stages.%2520First%252C%2520we%250Alearn%2520a%2520fundamental%25203DGS%2520from%2520all%2520frames%252C%2520ignoring%2520temporal%2520scene%2520variations%252C%250Aand%2520use%2520a%2520learnable%2520mask%2520to%2520separate%2520the%2520highly%2520dynamic%2520foreground%2520from%2520the%250Aminimally%2520moving%2520background.%2520Second%252C%2520we%2520replicate%2520multiple%2520copies%2520of%2520the%250Adecoupled%2520foreground%2520Gaussians%2520from%2520the%2520first%2520stage%252C%2520each%2520corresponding%2520to%2520a%250Atemporal%2520segment%252C%2520and%2520optimize%2520them%2520using%2520pseudo-views%2520constructed%2520from%250Amultiple%2520frames%2520within%2520each%2520segment.%2520These%2520Gaussians%252C%2520termed%2520Relay%2520Gaussians%252C%250Aact%2520as%2520explicit%2520relay%2520nodes%252C%2520simplifying%2520and%2520breaking%2520down%2520large-scale%2520motion%250Atrajectories%2520into%2520smaller%252C%2520manageable%2520segments.%2520Finally%252C%2520we%2520jointly%2520learn%2520the%250Ascene%2527s%2520temporal%2520motion%2520and%2520refine%2520the%2520canonical%2520Gaussians%2520learned%2520from%2520the%250Afirst%2520two%2520stages.%2520We%2520conduct%2520thorough%2520experiments%2520on%2520two%2520dynamic%2520scene%2520datasets%250Afeaturing%2520large%2520and%2520complex%2520motions%252C%2520where%2520our%2520RelayGS%2520outperforms%250Astate-of-the-arts%2520by%2520more%2520than%25201%2520dB%2520in%2520PSNR%252C%2520and%2520successfully%2520reconstructs%250Areal-world%2520basketball%2520game%2520scenes%2520in%2520a%2520much%2520more%2520complete%2520and%2520coherent%2520manner%252C%250Awhereas%2520previous%2520methods%2520usually%2520struggle%2520to%2520capture%2520the%2520complex%2520motion%2520of%250Aplayers.%2520Code%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/gqk/RelayGS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelayGS%3A%20Reconstructing%20Dynamic%20Scenes%20with%20Large-Scale%20and%20Complex%0A%20%20Motions%20via%20Relay%20Gaussians&entry.906535625=Qiankun%20Gao%20and%20Yanmin%20Wu%20and%20Chengxiang%20Wen%20and%20Jiarui%20Meng%20and%20Luyang%20Tang%20and%20Jie%20Chen%20and%20Ronggang%20Wang%20and%20Jian%20Zhang&entry.1292438233=%20%20Reconstructing%20dynamic%20scenes%20with%20large-scale%20and%20complex%20motions%20remains%20a%0Asignificant%20challenge.%20Recent%20techniques%20like%20Neural%20Radiance%20Fields%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20shown%20promise%20but%20still%20struggle%20with%20scenes%0Ainvolving%20substantial%20movement.%20This%20paper%20proposes%20RelayGS%2C%20a%20novel%20method%0Abased%20on%203DGS%2C%20specifically%20designed%20to%20represent%20and%20reconstruct%20highly%0Adynamic%20scenes.%20Our%20RelayGS%20learns%20a%20complete%204D%20representation%20with%20canonical%0A3D%20Gaussians%20and%20a%20compact%20motion%20field%2C%20consisting%20of%20three%20stages.%20First%2C%20we%0Alearn%20a%20fundamental%203DGS%20from%20all%20frames%2C%20ignoring%20temporal%20scene%20variations%2C%0Aand%20use%20a%20learnable%20mask%20to%20separate%20the%20highly%20dynamic%20foreground%20from%20the%0Aminimally%20moving%20background.%20Second%2C%20we%20replicate%20multiple%20copies%20of%20the%0Adecoupled%20foreground%20Gaussians%20from%20the%20first%20stage%2C%20each%20corresponding%20to%20a%0Atemporal%20segment%2C%20and%20optimize%20them%20using%20pseudo-views%20constructed%20from%0Amultiple%20frames%20within%20each%20segment.%20These%20Gaussians%2C%20termed%20Relay%20Gaussians%2C%0Aact%20as%20explicit%20relay%20nodes%2C%20simplifying%20and%20breaking%20down%20large-scale%20motion%0Atrajectories%20into%20smaller%2C%20manageable%20segments.%20Finally%2C%20we%20jointly%20learn%20the%0Ascene%27s%20temporal%20motion%20and%20refine%20the%20canonical%20Gaussians%20learned%20from%20the%0Afirst%20two%20stages.%20We%20conduct%20thorough%20experiments%20on%20two%20dynamic%20scene%20datasets%0Afeaturing%20large%20and%20complex%20motions%2C%20where%20our%20RelayGS%20outperforms%0Astate-of-the-arts%20by%20more%20than%201%20dB%20in%20PSNR%2C%20and%20successfully%20reconstructs%0Areal-world%20basketball%20game%20scenes%20in%20a%20much%20more%20complete%20and%20coherent%20manner%2C%0Awhereas%20previous%20methods%20usually%20struggle%20to%20capture%20the%20complex%20motion%20of%0Aplayers.%20Code%20will%20be%20publicly%20available%20at%20https%3A//github.com/gqk/RelayGS%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02493v1&entry.124074799=Read"},
{"title": "RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex\n  Motions via Relay Gaussians", "author": "Qiankun Gao and Yanmin Wu and Chengxiang Wen and Jiarui Meng and Luyang Tang and Jie Chen and Ronggang Wang and Jian Zhang", "abstract": "  Reconstructing dynamic scenes with large-scale and complex motions remains a\nsignificant challenge. Recent techniques like Neural Radiance Fields and 3D\nGaussian Splatting (3DGS) have shown promise but still struggle with scenes\ninvolving substantial movement. This paper proposes RelayGS, a novel method\nbased on 3DGS, specifically designed to represent and reconstruct highly\ndynamic scenes. Our RelayGS learns a complete 4D representation with canonical\n3D Gaussians and a compact motion field, consisting of three stages. First, we\nlearn a fundamental 3DGS from all frames, ignoring temporal scene variations,\nand use a learnable mask to separate the highly dynamic foreground from the\nminimally moving background. Second, we replicate multiple copies of the\ndecoupled foreground Gaussians from the first stage, each corresponding to a\ntemporal segment, and optimize them using pseudo-views constructed from\nmultiple frames within each segment. These Gaussians, termed Relay Gaussians,\nact as explicit relay nodes, simplifying and breaking down large-scale motion\ntrajectories into smaller, manageable segments. Finally, we jointly learn the\nscene's temporal motion and refine the canonical Gaussians learned from the\nfirst two stages. We conduct thorough experiments on two dynamic scene datasets\nfeaturing large and complex motions, where our RelayGS outperforms\nstate-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs\nreal-world basketball game scenes in a much more complete and coherent manner,\nwhereas previous methods usually struggle to capture the complex motion of\nplayers. Code will be publicly available at https://github.com/gqk/RelayGS\n", "link": "http://arxiv.org/abs/2412.02493v1", "date": "2024-12-03", "relevancy": 3.3596, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6859}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6822}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelayGS%3A%20Reconstructing%20Dynamic%20Scenes%20with%20Large-Scale%20and%20Complex%0A%20%20Motions%20via%20Relay%20Gaussians&body=Title%3A%20RelayGS%3A%20Reconstructing%20Dynamic%20Scenes%20with%20Large-Scale%20and%20Complex%0A%20%20Motions%20via%20Relay%20Gaussians%0AAuthor%3A%20Qiankun%20Gao%20and%20Yanmin%20Wu%20and%20Chengxiang%20Wen%20and%20Jiarui%20Meng%20and%20Luyang%20Tang%20and%20Jie%20Chen%20and%20Ronggang%20Wang%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20Reconstructing%20dynamic%20scenes%20with%20large-scale%20and%20complex%20motions%20remains%20a%0Asignificant%20challenge.%20Recent%20techniques%20like%20Neural%20Radiance%20Fields%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20shown%20promise%20but%20still%20struggle%20with%20scenes%0Ainvolving%20substantial%20movement.%20This%20paper%20proposes%20RelayGS%2C%20a%20novel%20method%0Abased%20on%203DGS%2C%20specifically%20designed%20to%20represent%20and%20reconstruct%20highly%0Adynamic%20scenes.%20Our%20RelayGS%20learns%20a%20complete%204D%20representation%20with%20canonical%0A3D%20Gaussians%20and%20a%20compact%20motion%20field%2C%20consisting%20of%20three%20stages.%20First%2C%20we%0Alearn%20a%20fundamental%203DGS%20from%20all%20frames%2C%20ignoring%20temporal%20scene%20variations%2C%0Aand%20use%20a%20learnable%20mask%20to%20separate%20the%20highly%20dynamic%20foreground%20from%20the%0Aminimally%20moving%20background.%20Second%2C%20we%20replicate%20multiple%20copies%20of%20the%0Adecoupled%20foreground%20Gaussians%20from%20the%20first%20stage%2C%20each%20corresponding%20to%20a%0Atemporal%20segment%2C%20and%20optimize%20them%20using%20pseudo-views%20constructed%20from%0Amultiple%20frames%20within%20each%20segment.%20These%20Gaussians%2C%20termed%20Relay%20Gaussians%2C%0Aact%20as%20explicit%20relay%20nodes%2C%20simplifying%20and%20breaking%20down%20large-scale%20motion%0Atrajectories%20into%20smaller%2C%20manageable%20segments.%20Finally%2C%20we%20jointly%20learn%20the%0Ascene%27s%20temporal%20motion%20and%20refine%20the%20canonical%20Gaussians%20learned%20from%20the%0Afirst%20two%20stages.%20We%20conduct%20thorough%20experiments%20on%20two%20dynamic%20scene%20datasets%0Afeaturing%20large%20and%20complex%20motions%2C%20where%20our%20RelayGS%20outperforms%0Astate-of-the-arts%20by%20more%20than%201%20dB%20in%20PSNR%2C%20and%20successfully%20reconstructs%0Areal-world%20basketball%20game%20scenes%20in%20a%20much%20more%20complete%20and%20coherent%20manner%2C%0Awhereas%20previous%20methods%20usually%20struggle%20to%20capture%20the%20complex%20motion%20of%0Aplayers.%20Code%20will%20be%20publicly%20available%20at%20https%3A//github.com/gqk/RelayGS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelayGS%253A%2520Reconstructing%2520Dynamic%2520Scenes%2520with%2520Large-Scale%2520and%2520Complex%250A%2520%2520Motions%2520via%2520Relay%2520Gaussians%26entry.906535625%3DQiankun%2520Gao%2520and%2520Yanmin%2520Wu%2520and%2520Chengxiang%2520Wen%2520and%2520Jiarui%2520Meng%2520and%2520Luyang%2520Tang%2520and%2520Jie%2520Chen%2520and%2520Ronggang%2520Wang%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%2520scenes%2520with%2520large-scale%2520and%2520complex%2520motions%2520remains%2520a%250Asignificant%2520challenge.%2520Recent%2520techniques%2520like%2520Neural%2520Radiance%2520Fields%2520and%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520have%2520shown%2520promise%2520but%2520still%2520struggle%2520with%2520scenes%250Ainvolving%2520substantial%2520movement.%2520This%2520paper%2520proposes%2520RelayGS%252C%2520a%2520novel%2520method%250Abased%2520on%25203DGS%252C%2520specifically%2520designed%2520to%2520represent%2520and%2520reconstruct%2520highly%250Adynamic%2520scenes.%2520Our%2520RelayGS%2520learns%2520a%2520complete%25204D%2520representation%2520with%2520canonical%250A3D%2520Gaussians%2520and%2520a%2520compact%2520motion%2520field%252C%2520consisting%2520of%2520three%2520stages.%2520First%252C%2520we%250Alearn%2520a%2520fundamental%25203DGS%2520from%2520all%2520frames%252C%2520ignoring%2520temporal%2520scene%2520variations%252C%250Aand%2520use%2520a%2520learnable%2520mask%2520to%2520separate%2520the%2520highly%2520dynamic%2520foreground%2520from%2520the%250Aminimally%2520moving%2520background.%2520Second%252C%2520we%2520replicate%2520multiple%2520copies%2520of%2520the%250Adecoupled%2520foreground%2520Gaussians%2520from%2520the%2520first%2520stage%252C%2520each%2520corresponding%2520to%2520a%250Atemporal%2520segment%252C%2520and%2520optimize%2520them%2520using%2520pseudo-views%2520constructed%2520from%250Amultiple%2520frames%2520within%2520each%2520segment.%2520These%2520Gaussians%252C%2520termed%2520Relay%2520Gaussians%252C%250Aact%2520as%2520explicit%2520relay%2520nodes%252C%2520simplifying%2520and%2520breaking%2520down%2520large-scale%2520motion%250Atrajectories%2520into%2520smaller%252C%2520manageable%2520segments.%2520Finally%252C%2520we%2520jointly%2520learn%2520the%250Ascene%2527s%2520temporal%2520motion%2520and%2520refine%2520the%2520canonical%2520Gaussians%2520learned%2520from%2520the%250Afirst%2520two%2520stages.%2520We%2520conduct%2520thorough%2520experiments%2520on%2520two%2520dynamic%2520scene%2520datasets%250Afeaturing%2520large%2520and%2520complex%2520motions%252C%2520where%2520our%2520RelayGS%2520outperforms%250Astate-of-the-arts%2520by%2520more%2520than%25201%2520dB%2520in%2520PSNR%252C%2520and%2520successfully%2520reconstructs%250Areal-world%2520basketball%2520game%2520scenes%2520in%2520a%2520much%2520more%2520complete%2520and%2520coherent%2520manner%252C%250Awhereas%2520previous%2520methods%2520usually%2520struggle%2520to%2520capture%2520the%2520complex%2520motion%2520of%250Aplayers.%2520Code%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/gqk/RelayGS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelayGS%3A%20Reconstructing%20Dynamic%20Scenes%20with%20Large-Scale%20and%20Complex%0A%20%20Motions%20via%20Relay%20Gaussians&entry.906535625=Qiankun%20Gao%20and%20Yanmin%20Wu%20and%20Chengxiang%20Wen%20and%20Jiarui%20Meng%20and%20Luyang%20Tang%20and%20Jie%20Chen%20and%20Ronggang%20Wang%20and%20Jian%20Zhang&entry.1292438233=%20%20Reconstructing%20dynamic%20scenes%20with%20large-scale%20and%20complex%20motions%20remains%20a%0Asignificant%20challenge.%20Recent%20techniques%20like%20Neural%20Radiance%20Fields%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20shown%20promise%20but%20still%20struggle%20with%20scenes%0Ainvolving%20substantial%20movement.%20This%20paper%20proposes%20RelayGS%2C%20a%20novel%20method%0Abased%20on%203DGS%2C%20specifically%20designed%20to%20represent%20and%20reconstruct%20highly%0Adynamic%20scenes.%20Our%20RelayGS%20learns%20a%20complete%204D%20representation%20with%20canonical%0A3D%20Gaussians%20and%20a%20compact%20motion%20field%2C%20consisting%20of%20three%20stages.%20First%2C%20we%0Alearn%20a%20fundamental%203DGS%20from%20all%20frames%2C%20ignoring%20temporal%20scene%20variations%2C%0Aand%20use%20a%20learnable%20mask%20to%20separate%20the%20highly%20dynamic%20foreground%20from%20the%0Aminimally%20moving%20background.%20Second%2C%20we%20replicate%20multiple%20copies%20of%20the%0Adecoupled%20foreground%20Gaussians%20from%20the%20first%20stage%2C%20each%20corresponding%20to%20a%0Atemporal%20segment%2C%20and%20optimize%20them%20using%20pseudo-views%20constructed%20from%0Amultiple%20frames%20within%20each%20segment.%20These%20Gaussians%2C%20termed%20Relay%20Gaussians%2C%0Aact%20as%20explicit%20relay%20nodes%2C%20simplifying%20and%20breaking%20down%20large-scale%20motion%0Atrajectories%20into%20smaller%2C%20manageable%20segments.%20Finally%2C%20we%20jointly%20learn%20the%0Ascene%27s%20temporal%20motion%20and%20refine%20the%20canonical%20Gaussians%20learned%20from%20the%0Afirst%20two%20stages.%20We%20conduct%20thorough%20experiments%20on%20two%20dynamic%20scene%20datasets%0Afeaturing%20large%20and%20complex%20motions%2C%20where%20our%20RelayGS%20outperforms%0Astate-of-the-arts%20by%20more%20than%201%20dB%20in%20PSNR%2C%20and%20successfully%20reconstructs%0Areal-world%20basketball%20game%20scenes%20in%20a%20much%20more%20complete%20and%20coherent%20manner%2C%0Awhereas%20previous%20methods%20usually%20struggle%20to%20capture%20the%20complex%20motion%20of%0Aplayers.%20Code%20will%20be%20publicly%20available%20at%20https%3A//github.com/gqk/RelayGS%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02493v1&entry.124074799=Read"},
{"title": "D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup", "author": "Joanna Waczy\u0144ska and Piotr Borycki and Joanna Kaleta and S\u0142awomir Tadeja and Przemys\u0142aw Spurek", "abstract": "  Over the past years, we have observed an abundance of approaches for modeling\ndynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS to\nrepresent the scene's structure and the neural network to model dynamics. Such\napproaches allow fast rendering and extracting each element of such a dynamic\nscene. However, modifying such objects over time is challenging. SC-GS (Sparse\nControlled Gaussian Splatting) enhanced with Deformed Control Points partially\nsolves this issue. However, this approach necessitates selecting elements that\nneed to be kept fixed, as well as centroids that should be adjusted throughout\nediting. Moreover, this task poses additional difficulties regarding the\nre-productivity of such editing. To address this, we propose Dynamic\nMulti-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired\nrepresentation of dynamic GS. Additionally, we propose a strategy of linking\nparameterized Gaussian splats, forming a Triangle Soup with the estimated mesh.\nConsequently, we can separately construct new trajectories for the 3D objects\ncomposing the scene. Thus, we can make the scene's dynamic editable over time\nor while maintaining partial dynamics.\n", "link": "http://arxiv.org/abs/2405.14276v3", "date": "2024-12-03", "relevancy": 3.3302, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7163}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6421}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-MiSo%3A%20Editing%20Dynamic%203D%20Scenes%20using%20Multi-Gaussians%20Soup&body=Title%3A%20D-MiSo%3A%20Editing%20Dynamic%203D%20Scenes%20using%20Multi-Gaussians%20Soup%0AAuthor%3A%20Joanna%20Waczy%C5%84ska%20and%20Piotr%20Borycki%20and%20Joanna%20Kaleta%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Over%20the%20past%20years%2C%20we%20have%20observed%20an%20abundance%20of%20approaches%20for%20modeling%0Adynamic%203D%20scenes%20using%20Gaussian%20Splatting%20%28GS%29.%20Such%20solutions%20use%20GS%20to%0Arepresent%20the%20scene%27s%20structure%20and%20the%20neural%20network%20to%20model%20dynamics.%20Such%0Aapproaches%20allow%20fast%20rendering%20and%20extracting%20each%20element%20of%20such%20a%20dynamic%0Ascene.%20However%2C%20modifying%20such%20objects%20over%20time%20is%20challenging.%20SC-GS%20%28Sparse%0AControlled%20Gaussian%20Splatting%29%20enhanced%20with%20Deformed%20Control%20Points%20partially%0Asolves%20this%20issue.%20However%2C%20this%20approach%20necessitates%20selecting%20elements%20that%0Aneed%20to%20be%20kept%20fixed%2C%20as%20well%20as%20centroids%20that%20should%20be%20adjusted%20throughout%0Aediting.%20Moreover%2C%20this%20task%20poses%20additional%20difficulties%20regarding%20the%0Are-productivity%20of%20such%20editing.%20To%20address%20this%2C%20we%20propose%20Dynamic%0AMulti-Gaussian%20Soup%20%28D-MiSo%29%2C%20which%20allows%20us%20to%20model%20the%20mesh-inspired%0Arepresentation%20of%20dynamic%20GS.%20Additionally%2C%20we%20propose%20a%20strategy%20of%20linking%0Aparameterized%20Gaussian%20splats%2C%20forming%20a%20Triangle%20Soup%20with%20the%20estimated%20mesh.%0AConsequently%2C%20we%20can%20separately%20construct%20new%20trajectories%20for%20the%203D%20objects%0Acomposing%20the%20scene.%20Thus%2C%20we%20can%20make%20the%20scene%27s%20dynamic%20editable%20over%20time%0Aor%20while%20maintaining%20partial%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-MiSo%253A%2520Editing%2520Dynamic%25203D%2520Scenes%2520using%2520Multi-Gaussians%2520Soup%26entry.906535625%3DJoanna%2520Waczy%25C5%2584ska%2520and%2520Piotr%2520Borycki%2520and%2520Joanna%2520Kaleta%2520and%2520S%25C5%2582awomir%2520Tadeja%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520years%252C%2520we%2520have%2520observed%2520an%2520abundance%2520of%2520approaches%2520for%2520modeling%250Adynamic%25203D%2520scenes%2520using%2520Gaussian%2520Splatting%2520%2528GS%2529.%2520Such%2520solutions%2520use%2520GS%2520to%250Arepresent%2520the%2520scene%2527s%2520structure%2520and%2520the%2520neural%2520network%2520to%2520model%2520dynamics.%2520Such%250Aapproaches%2520allow%2520fast%2520rendering%2520and%2520extracting%2520each%2520element%2520of%2520such%2520a%2520dynamic%250Ascene.%2520However%252C%2520modifying%2520such%2520objects%2520over%2520time%2520is%2520challenging.%2520SC-GS%2520%2528Sparse%250AControlled%2520Gaussian%2520Splatting%2529%2520enhanced%2520with%2520Deformed%2520Control%2520Points%2520partially%250Asolves%2520this%2520issue.%2520However%252C%2520this%2520approach%2520necessitates%2520selecting%2520elements%2520that%250Aneed%2520to%2520be%2520kept%2520fixed%252C%2520as%2520well%2520as%2520centroids%2520that%2520should%2520be%2520adjusted%2520throughout%250Aediting.%2520Moreover%252C%2520this%2520task%2520poses%2520additional%2520difficulties%2520regarding%2520the%250Are-productivity%2520of%2520such%2520editing.%2520To%2520address%2520this%252C%2520we%2520propose%2520Dynamic%250AMulti-Gaussian%2520Soup%2520%2528D-MiSo%2529%252C%2520which%2520allows%2520us%2520to%2520model%2520the%2520mesh-inspired%250Arepresentation%2520of%2520dynamic%2520GS.%2520Additionally%252C%2520we%2520propose%2520a%2520strategy%2520of%2520linking%250Aparameterized%2520Gaussian%2520splats%252C%2520forming%2520a%2520Triangle%2520Soup%2520with%2520the%2520estimated%2520mesh.%250AConsequently%252C%2520we%2520can%2520separately%2520construct%2520new%2520trajectories%2520for%2520the%25203D%2520objects%250Acomposing%2520the%2520scene.%2520Thus%252C%2520we%2520can%2520make%2520the%2520scene%2527s%2520dynamic%2520editable%2520over%2520time%250Aor%2520while%2520maintaining%2520partial%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-MiSo%3A%20Editing%20Dynamic%203D%20Scenes%20using%20Multi-Gaussians%20Soup&entry.906535625=Joanna%20Waczy%C5%84ska%20and%20Piotr%20Borycki%20and%20Joanna%20Kaleta%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Over%20the%20past%20years%2C%20we%20have%20observed%20an%20abundance%20of%20approaches%20for%20modeling%0Adynamic%203D%20scenes%20using%20Gaussian%20Splatting%20%28GS%29.%20Such%20solutions%20use%20GS%20to%0Arepresent%20the%20scene%27s%20structure%20and%20the%20neural%20network%20to%20model%20dynamics.%20Such%0Aapproaches%20allow%20fast%20rendering%20and%20extracting%20each%20element%20of%20such%20a%20dynamic%0Ascene.%20However%2C%20modifying%20such%20objects%20over%20time%20is%20challenging.%20SC-GS%20%28Sparse%0AControlled%20Gaussian%20Splatting%29%20enhanced%20with%20Deformed%20Control%20Points%20partially%0Asolves%20this%20issue.%20However%2C%20this%20approach%20necessitates%20selecting%20elements%20that%0Aneed%20to%20be%20kept%20fixed%2C%20as%20well%20as%20centroids%20that%20should%20be%20adjusted%20throughout%0Aediting.%20Moreover%2C%20this%20task%20poses%20additional%20difficulties%20regarding%20the%0Are-productivity%20of%20such%20editing.%20To%20address%20this%2C%20we%20propose%20Dynamic%0AMulti-Gaussian%20Soup%20%28D-MiSo%29%2C%20which%20allows%20us%20to%20model%20the%20mesh-inspired%0Arepresentation%20of%20dynamic%20GS.%20Additionally%2C%20we%20propose%20a%20strategy%20of%20linking%0Aparameterized%20Gaussian%20splats%2C%20forming%20a%20Triangle%20Soup%20with%20the%20estimated%20mesh.%0AConsequently%2C%20we%20can%20separately%20construct%20new%20trajectories%20for%20the%203D%20objects%0Acomposing%20the%20scene.%20Thus%2C%20we%20can%20make%20the%20scene%27s%20dynamic%20editable%20over%20time%0Aor%20while%20maintaining%20partial%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14276v3&entry.124074799=Read"},
{"title": "Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation\n  Benchmark", "author": "Haidong Xu and Meishan Zhang and Hao Ju and Zhedong Zheng and Hongyuan Zhu and Erik Cambria and Min Zhang and Hao Fei", "abstract": "  Producing emotionally dynamic 3D facial avatars with text derived from spoken\nwords (Emo3D) has been a pivotal research topic in 3D avatar generation. While\nprogress has been made in general-purpose 3D avatar generation, the exploration\nof generating emotional 3D avatars remains scarce, primarily due to the\ncomplexities of identifying and rendering rich emotions from spoken words. This\npaper reexamines Emo3D generation and draws inspiration from human processes,\nbreaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping\n(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in\ndetermining the quality of Emo3D generation and encompasses three key\nchallenges: Expression Diversity, Emotion-Content Consistency, and Expression\nFluidity. To address these challenges, we introduce a novel benchmark to\nadvance research in Emo3D generation. First, we present EmoAva, a large-scale,\nhigh-quality dataset for T3DEM, comprising 15,000 text-to-3D expression\nmappings that characterize the aforementioned three challenges in Emo3D\ngeneration. Furthermore, we develop various metrics to effectively evaluate\nmodels against these identified challenges. Next, to effectively model the\nconsistency, diversity, and fluidity of human expressions in the T3DEM step, we\npropose the Continuous Text-to-Expression Generator, which employs an\nautoregressive Conditional Variational Autoencoder for expression code\ngeneration, enhanced with Latent Temporal Attention and Expression-wise\nAttention mechanisms. Finally, to further enhance the 3DAR step on rendering\nhigher-quality subtle expressions, we present the Globally-informed Gaussian\nAvatar (GiGA) model. GiGA incorporates a global information mechanism into 3D\nGaussian representations, enabling the capture of subtle micro-expressions and\nseamless transitions between emotional states.\n", "link": "http://arxiv.org/abs/2412.02508v1", "date": "2024-12-03", "relevancy": 3.0621, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6362}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6362}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Rich%20Emotions%20in%203D%20Avatars%3A%20A%20Text-to-3D%20Avatar%20Generation%0A%20%20Benchmark&body=Title%3A%20Towards%20Rich%20Emotions%20in%203D%20Avatars%3A%20A%20Text-to-3D%20Avatar%20Generation%0A%20%20Benchmark%0AAuthor%3A%20Haidong%20Xu%20and%20Meishan%20Zhang%20and%20Hao%20Ju%20and%20Zhedong%20Zheng%20and%20Hongyuan%20Zhu%20and%20Erik%20Cambria%20and%20Min%20Zhang%20and%20Hao%20Fei%0AAbstract%3A%20%20%20Producing%20emotionally%20dynamic%203D%20facial%20avatars%20with%20text%20derived%20from%20spoken%0Awords%20%28Emo3D%29%20has%20been%20a%20pivotal%20research%20topic%20in%203D%20avatar%20generation.%20While%0Aprogress%20has%20been%20made%20in%20general-purpose%203D%20avatar%20generation%2C%20the%20exploration%0Aof%20generating%20emotional%203D%20avatars%20remains%20scarce%2C%20primarily%20due%20to%20the%0Acomplexities%20of%20identifying%20and%20rendering%20rich%20emotions%20from%20spoken%20words.%20This%0Apaper%20reexamines%20Emo3D%20generation%20and%20draws%20inspiration%20from%20human%20processes%2C%0Abreaking%20down%20Emo3D%20into%20two%20cascading%20steps%3A%20Text-to-3D%20Expression%20Mapping%0A%28T3DEM%29%20and%203D%20Avatar%20Rendering%20%283DAR%29.%20T3DEM%20is%20the%20most%20crucial%20step%20in%0Adetermining%20the%20quality%20of%20Emo3D%20generation%20and%20encompasses%20three%20key%0Achallenges%3A%20Expression%20Diversity%2C%20Emotion-Content%20Consistency%2C%20and%20Expression%0AFluidity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20benchmark%20to%0Aadvance%20research%20in%20Emo3D%20generation.%20First%2C%20we%20present%20EmoAva%2C%20a%20large-scale%2C%0Ahigh-quality%20dataset%20for%20T3DEM%2C%20comprising%2015%2C000%20text-to-3D%20expression%0Amappings%20that%20characterize%20the%20aforementioned%20three%20challenges%20in%20Emo3D%0Ageneration.%20Furthermore%2C%20we%20develop%20various%20metrics%20to%20effectively%20evaluate%0Amodels%20against%20these%20identified%20challenges.%20Next%2C%20to%20effectively%20model%20the%0Aconsistency%2C%20diversity%2C%20and%20fluidity%20of%20human%20expressions%20in%20the%20T3DEM%20step%2C%20we%0Apropose%20the%20Continuous%20Text-to-Expression%20Generator%2C%20which%20employs%20an%0Aautoregressive%20Conditional%20Variational%20Autoencoder%20for%20expression%20code%0Ageneration%2C%20enhanced%20with%20Latent%20Temporal%20Attention%20and%20Expression-wise%0AAttention%20mechanisms.%20Finally%2C%20to%20further%20enhance%20the%203DAR%20step%20on%20rendering%0Ahigher-quality%20subtle%20expressions%2C%20we%20present%20the%20Globally-informed%20Gaussian%0AAvatar%20%28GiGA%29%20model.%20GiGA%20incorporates%20a%20global%20information%20mechanism%20into%203D%0AGaussian%20representations%2C%20enabling%20the%20capture%20of%20subtle%20micro-expressions%20and%0Aseamless%20transitions%20between%20emotional%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Rich%2520Emotions%2520in%25203D%2520Avatars%253A%2520A%2520Text-to-3D%2520Avatar%2520Generation%250A%2520%2520Benchmark%26entry.906535625%3DHaidong%2520Xu%2520and%2520Meishan%2520Zhang%2520and%2520Hao%2520Ju%2520and%2520Zhedong%2520Zheng%2520and%2520Hongyuan%2520Zhu%2520and%2520Erik%2520Cambria%2520and%2520Min%2520Zhang%2520and%2520Hao%2520Fei%26entry.1292438233%3D%2520%2520Producing%2520emotionally%2520dynamic%25203D%2520facial%2520avatars%2520with%2520text%2520derived%2520from%2520spoken%250Awords%2520%2528Emo3D%2529%2520has%2520been%2520a%2520pivotal%2520research%2520topic%2520in%25203D%2520avatar%2520generation.%2520While%250Aprogress%2520has%2520been%2520made%2520in%2520general-purpose%25203D%2520avatar%2520generation%252C%2520the%2520exploration%250Aof%2520generating%2520emotional%25203D%2520avatars%2520remains%2520scarce%252C%2520primarily%2520due%2520to%2520the%250Acomplexities%2520of%2520identifying%2520and%2520rendering%2520rich%2520emotions%2520from%2520spoken%2520words.%2520This%250Apaper%2520reexamines%2520Emo3D%2520generation%2520and%2520draws%2520inspiration%2520from%2520human%2520processes%252C%250Abreaking%2520down%2520Emo3D%2520into%2520two%2520cascading%2520steps%253A%2520Text-to-3D%2520Expression%2520Mapping%250A%2528T3DEM%2529%2520and%25203D%2520Avatar%2520Rendering%2520%25283DAR%2529.%2520T3DEM%2520is%2520the%2520most%2520crucial%2520step%2520in%250Adetermining%2520the%2520quality%2520of%2520Emo3D%2520generation%2520and%2520encompasses%2520three%2520key%250Achallenges%253A%2520Expression%2520Diversity%252C%2520Emotion-Content%2520Consistency%252C%2520and%2520Expression%250AFluidity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%2520to%250Aadvance%2520research%2520in%2520Emo3D%2520generation.%2520First%252C%2520we%2520present%2520EmoAva%252C%2520a%2520large-scale%252C%250Ahigh-quality%2520dataset%2520for%2520T3DEM%252C%2520comprising%252015%252C000%2520text-to-3D%2520expression%250Amappings%2520that%2520characterize%2520the%2520aforementioned%2520three%2520challenges%2520in%2520Emo3D%250Ageneration.%2520Furthermore%252C%2520we%2520develop%2520various%2520metrics%2520to%2520effectively%2520evaluate%250Amodels%2520against%2520these%2520identified%2520challenges.%2520Next%252C%2520to%2520effectively%2520model%2520the%250Aconsistency%252C%2520diversity%252C%2520and%2520fluidity%2520of%2520human%2520expressions%2520in%2520the%2520T3DEM%2520step%252C%2520we%250Apropose%2520the%2520Continuous%2520Text-to-Expression%2520Generator%252C%2520which%2520employs%2520an%250Aautoregressive%2520Conditional%2520Variational%2520Autoencoder%2520for%2520expression%2520code%250Ageneration%252C%2520enhanced%2520with%2520Latent%2520Temporal%2520Attention%2520and%2520Expression-wise%250AAttention%2520mechanisms.%2520Finally%252C%2520to%2520further%2520enhance%2520the%25203DAR%2520step%2520on%2520rendering%250Ahigher-quality%2520subtle%2520expressions%252C%2520we%2520present%2520the%2520Globally-informed%2520Gaussian%250AAvatar%2520%2528GiGA%2529%2520model.%2520GiGA%2520incorporates%2520a%2520global%2520information%2520mechanism%2520into%25203D%250AGaussian%2520representations%252C%2520enabling%2520the%2520capture%2520of%2520subtle%2520micro-expressions%2520and%250Aseamless%2520transitions%2520between%2520emotional%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Rich%20Emotions%20in%203D%20Avatars%3A%20A%20Text-to-3D%20Avatar%20Generation%0A%20%20Benchmark&entry.906535625=Haidong%20Xu%20and%20Meishan%20Zhang%20and%20Hao%20Ju%20and%20Zhedong%20Zheng%20and%20Hongyuan%20Zhu%20and%20Erik%20Cambria%20and%20Min%20Zhang%20and%20Hao%20Fei&entry.1292438233=%20%20Producing%20emotionally%20dynamic%203D%20facial%20avatars%20with%20text%20derived%20from%20spoken%0Awords%20%28Emo3D%29%20has%20been%20a%20pivotal%20research%20topic%20in%203D%20avatar%20generation.%20While%0Aprogress%20has%20been%20made%20in%20general-purpose%203D%20avatar%20generation%2C%20the%20exploration%0Aof%20generating%20emotional%203D%20avatars%20remains%20scarce%2C%20primarily%20due%20to%20the%0Acomplexities%20of%20identifying%20and%20rendering%20rich%20emotions%20from%20spoken%20words.%20This%0Apaper%20reexamines%20Emo3D%20generation%20and%20draws%20inspiration%20from%20human%20processes%2C%0Abreaking%20down%20Emo3D%20into%20two%20cascading%20steps%3A%20Text-to-3D%20Expression%20Mapping%0A%28T3DEM%29%20and%203D%20Avatar%20Rendering%20%283DAR%29.%20T3DEM%20is%20the%20most%20crucial%20step%20in%0Adetermining%20the%20quality%20of%20Emo3D%20generation%20and%20encompasses%20three%20key%0Achallenges%3A%20Expression%20Diversity%2C%20Emotion-Content%20Consistency%2C%20and%20Expression%0AFluidity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20benchmark%20to%0Aadvance%20research%20in%20Emo3D%20generation.%20First%2C%20we%20present%20EmoAva%2C%20a%20large-scale%2C%0Ahigh-quality%20dataset%20for%20T3DEM%2C%20comprising%2015%2C000%20text-to-3D%20expression%0Amappings%20that%20characterize%20the%20aforementioned%20three%20challenges%20in%20Emo3D%0Ageneration.%20Furthermore%2C%20we%20develop%20various%20metrics%20to%20effectively%20evaluate%0Amodels%20against%20these%20identified%20challenges.%20Next%2C%20to%20effectively%20model%20the%0Aconsistency%2C%20diversity%2C%20and%20fluidity%20of%20human%20expressions%20in%20the%20T3DEM%20step%2C%20we%0Apropose%20the%20Continuous%20Text-to-Expression%20Generator%2C%20which%20employs%20an%0Aautoregressive%20Conditional%20Variational%20Autoencoder%20for%20expression%20code%0Ageneration%2C%20enhanced%20with%20Latent%20Temporal%20Attention%20and%20Expression-wise%0AAttention%20mechanisms.%20Finally%2C%20to%20further%20enhance%20the%203DAR%20step%20on%20rendering%0Ahigher-quality%20subtle%20expressions%2C%20we%20present%20the%20Globally-informed%20Gaussian%0AAvatar%20%28GiGA%29%20model.%20GiGA%20incorporates%20a%20global%20information%20mechanism%20into%203D%0AGaussian%20representations%2C%20enabling%20the%20capture%20of%20subtle%20micro-expressions%20and%0Aseamless%20transitions%20between%20emotional%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02508v1&entry.124074799=Read"},
{"title": "Realistic Surgical Simulation from Monocular Videos", "author": "Kailing Wang and Chen Yang and Keyang Zhao and Xiaokang Yang and Wei Shen", "abstract": "  This paper tackles the challenge of automatically performing realistic\nsurgical simulations from readily available surgical videos. Recent efforts\nhave successfully integrated physically grounded dynamics within 3D Gaussians\nto perform high-fidelity simulations in well-reconstructed simulation\nenvironments from static scenes. However, they struggle with the geometric\ninconsistency in reconstructing simulation environments and unrealistic\nphysical deformations in simulations of soft tissues when it comes to dynamic\nand complex surgical processes. In this paper, we propose SurgiSim, a novel\nautomatic simulation system to overcome these limitations. To build a surgical\nsimulation environment, we maintain a canonical 3D scene composed of 3D\nGaussians coupled with a deformation field to represent a dynamic surgical\nscene. This process involves a multi-stage optimization with trajectory and\nanisotropic regularization, enhancing the geometry consistency of the canonical\nscene, which serves as the simulation environment. To achieve realistic\nphysical simulations in this environment, we implement a Visco-Elastic\ndeformation model based on the Maxwell model, effectively restoring the complex\ndeformations of tissues. Additionally, we infer the physical parameters of\ntissues by minimizing the discrepancies between the input video and simulation\nresults guided by estimated tissue motion, ensuring realistic simulation\noutcomes. Experiments on various surgical scenarios and interactions\ndemonstrate SurgiSim's ability to perform realistic simulation of soft tissues\namong surgical procedures, showing its enormous potential for enhancing\nsurgical training, planning, and robotic surgery systems. The project page is\nat https://namaenashibot.github.io/SurgiSim/.\n", "link": "http://arxiv.org/abs/2412.02359v1", "date": "2024-12-03", "relevancy": 2.9815, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6138}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Surgical%20Simulation%20from%20Monocular%20Videos&body=Title%3A%20Realistic%20Surgical%20Simulation%20from%20Monocular%20Videos%0AAuthor%3A%20Kailing%20Wang%20and%20Chen%20Yang%20and%20Keyang%20Zhao%20and%20Xiaokang%20Yang%20and%20Wei%20Shen%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20challenge%20of%20automatically%20performing%20realistic%0Asurgical%20simulations%20from%20readily%20available%20surgical%20videos.%20Recent%20efforts%0Ahave%20successfully%20integrated%20physically%20grounded%20dynamics%20within%203D%20Gaussians%0Ato%20perform%20high-fidelity%20simulations%20in%20well-reconstructed%20simulation%0Aenvironments%20from%20static%20scenes.%20However%2C%20they%20struggle%20with%20the%20geometric%0Ainconsistency%20in%20reconstructing%20simulation%20environments%20and%20unrealistic%0Aphysical%20deformations%20in%20simulations%20of%20soft%20tissues%20when%20it%20comes%20to%20dynamic%0Aand%20complex%20surgical%20processes.%20In%20this%20paper%2C%20we%20propose%20SurgiSim%2C%20a%20novel%0Aautomatic%20simulation%20system%20to%20overcome%20these%20limitations.%20To%20build%20a%20surgical%0Asimulation%20environment%2C%20we%20maintain%20a%20canonical%203D%20scene%20composed%20of%203D%0AGaussians%20coupled%20with%20a%20deformation%20field%20to%20represent%20a%20dynamic%20surgical%0Ascene.%20This%20process%20involves%20a%20multi-stage%20optimization%20with%20trajectory%20and%0Aanisotropic%20regularization%2C%20enhancing%20the%20geometry%20consistency%20of%20the%20canonical%0Ascene%2C%20which%20serves%20as%20the%20simulation%20environment.%20To%20achieve%20realistic%0Aphysical%20simulations%20in%20this%20environment%2C%20we%20implement%20a%20Visco-Elastic%0Adeformation%20model%20based%20on%20the%20Maxwell%20model%2C%20effectively%20restoring%20the%20complex%0Adeformations%20of%20tissues.%20Additionally%2C%20we%20infer%20the%20physical%20parameters%20of%0Atissues%20by%20minimizing%20the%20discrepancies%20between%20the%20input%20video%20and%20simulation%0Aresults%20guided%20by%20estimated%20tissue%20motion%2C%20ensuring%20realistic%20simulation%0Aoutcomes.%20Experiments%20on%20various%20surgical%20scenarios%20and%20interactions%0Ademonstrate%20SurgiSim%27s%20ability%20to%20perform%20realistic%20simulation%20of%20soft%20tissues%0Aamong%20surgical%20procedures%2C%20showing%20its%20enormous%20potential%20for%20enhancing%0Asurgical%20training%2C%20planning%2C%20and%20robotic%20surgery%20systems.%20The%20project%20page%20is%0Aat%20https%3A//namaenashibot.github.io/SurgiSim/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Surgical%2520Simulation%2520from%2520Monocular%2520Videos%26entry.906535625%3DKailing%2520Wang%2520and%2520Chen%2520Yang%2520and%2520Keyang%2520Zhao%2520and%2520Xiaokang%2520Yang%2520and%2520Wei%2520Shen%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520challenge%2520of%2520automatically%2520performing%2520realistic%250Asurgical%2520simulations%2520from%2520readily%2520available%2520surgical%2520videos.%2520Recent%2520efforts%250Ahave%2520successfully%2520integrated%2520physically%2520grounded%2520dynamics%2520within%25203D%2520Gaussians%250Ato%2520perform%2520high-fidelity%2520simulations%2520in%2520well-reconstructed%2520simulation%250Aenvironments%2520from%2520static%2520scenes.%2520However%252C%2520they%2520struggle%2520with%2520the%2520geometric%250Ainconsistency%2520in%2520reconstructing%2520simulation%2520environments%2520and%2520unrealistic%250Aphysical%2520deformations%2520in%2520simulations%2520of%2520soft%2520tissues%2520when%2520it%2520comes%2520to%2520dynamic%250Aand%2520complex%2520surgical%2520processes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SurgiSim%252C%2520a%2520novel%250Aautomatic%2520simulation%2520system%2520to%2520overcome%2520these%2520limitations.%2520To%2520build%2520a%2520surgical%250Asimulation%2520environment%252C%2520we%2520maintain%2520a%2520canonical%25203D%2520scene%2520composed%2520of%25203D%250AGaussians%2520coupled%2520with%2520a%2520deformation%2520field%2520to%2520represent%2520a%2520dynamic%2520surgical%250Ascene.%2520This%2520process%2520involves%2520a%2520multi-stage%2520optimization%2520with%2520trajectory%2520and%250Aanisotropic%2520regularization%252C%2520enhancing%2520the%2520geometry%2520consistency%2520of%2520the%2520canonical%250Ascene%252C%2520which%2520serves%2520as%2520the%2520simulation%2520environment.%2520To%2520achieve%2520realistic%250Aphysical%2520simulations%2520in%2520this%2520environment%252C%2520we%2520implement%2520a%2520Visco-Elastic%250Adeformation%2520model%2520based%2520on%2520the%2520Maxwell%2520model%252C%2520effectively%2520restoring%2520the%2520complex%250Adeformations%2520of%2520tissues.%2520Additionally%252C%2520we%2520infer%2520the%2520physical%2520parameters%2520of%250Atissues%2520by%2520minimizing%2520the%2520discrepancies%2520between%2520the%2520input%2520video%2520and%2520simulation%250Aresults%2520guided%2520by%2520estimated%2520tissue%2520motion%252C%2520ensuring%2520realistic%2520simulation%250Aoutcomes.%2520Experiments%2520on%2520various%2520surgical%2520scenarios%2520and%2520interactions%250Ademonstrate%2520SurgiSim%2527s%2520ability%2520to%2520perform%2520realistic%2520simulation%2520of%2520soft%2520tissues%250Aamong%2520surgical%2520procedures%252C%2520showing%2520its%2520enormous%2520potential%2520for%2520enhancing%250Asurgical%2520training%252C%2520planning%252C%2520and%2520robotic%2520surgery%2520systems.%2520The%2520project%2520page%2520is%250Aat%2520https%253A//namaenashibot.github.io/SurgiSim/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Surgical%20Simulation%20from%20Monocular%20Videos&entry.906535625=Kailing%20Wang%20and%20Chen%20Yang%20and%20Keyang%20Zhao%20and%20Xiaokang%20Yang%20and%20Wei%20Shen&entry.1292438233=%20%20This%20paper%20tackles%20the%20challenge%20of%20automatically%20performing%20realistic%0Asurgical%20simulations%20from%20readily%20available%20surgical%20videos.%20Recent%20efforts%0Ahave%20successfully%20integrated%20physically%20grounded%20dynamics%20within%203D%20Gaussians%0Ato%20perform%20high-fidelity%20simulations%20in%20well-reconstructed%20simulation%0Aenvironments%20from%20static%20scenes.%20However%2C%20they%20struggle%20with%20the%20geometric%0Ainconsistency%20in%20reconstructing%20simulation%20environments%20and%20unrealistic%0Aphysical%20deformations%20in%20simulations%20of%20soft%20tissues%20when%20it%20comes%20to%20dynamic%0Aand%20complex%20surgical%20processes.%20In%20this%20paper%2C%20we%20propose%20SurgiSim%2C%20a%20novel%0Aautomatic%20simulation%20system%20to%20overcome%20these%20limitations.%20To%20build%20a%20surgical%0Asimulation%20environment%2C%20we%20maintain%20a%20canonical%203D%20scene%20composed%20of%203D%0AGaussians%20coupled%20with%20a%20deformation%20field%20to%20represent%20a%20dynamic%20surgical%0Ascene.%20This%20process%20involves%20a%20multi-stage%20optimization%20with%20trajectory%20and%0Aanisotropic%20regularization%2C%20enhancing%20the%20geometry%20consistency%20of%20the%20canonical%0Ascene%2C%20which%20serves%20as%20the%20simulation%20environment.%20To%20achieve%20realistic%0Aphysical%20simulations%20in%20this%20environment%2C%20we%20implement%20a%20Visco-Elastic%0Adeformation%20model%20based%20on%20the%20Maxwell%20model%2C%20effectively%20restoring%20the%20complex%0Adeformations%20of%20tissues.%20Additionally%2C%20we%20infer%20the%20physical%20parameters%20of%0Atissues%20by%20minimizing%20the%20discrepancies%20between%20the%20input%20video%20and%20simulation%0Aresults%20guided%20by%20estimated%20tissue%20motion%2C%20ensuring%20realistic%20simulation%0Aoutcomes.%20Experiments%20on%20various%20surgical%20scenarios%20and%20interactions%0Ademonstrate%20SurgiSim%27s%20ability%20to%20perform%20realistic%20simulation%20of%20soft%20tissues%0Aamong%20surgical%20procedures%2C%20showing%20its%20enormous%20potential%20for%20enhancing%0Asurgical%20training%2C%20planning%2C%20and%20robotic%20surgery%20systems.%20The%20project%20page%20is%0Aat%20https%3A//namaenashibot.github.io/SurgiSim/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02359v1&entry.124074799=Read"},
{"title": "Realistic Surgical Simulation from Monocular Videos", "author": "Kailing Wang and Chen Yang and Keyang Zhao and Xiaokang Yang and Wei Shen", "abstract": "  This paper tackles the challenge of automatically performing realistic\nsurgical simulations from readily available surgical videos. Recent efforts\nhave successfully integrated physically grounded dynamics within 3D Gaussians\nto perform high-fidelity simulations in well-reconstructed simulation\nenvironments from static scenes. However, they struggle with the geometric\ninconsistency in reconstructing simulation environments and unrealistic\nphysical deformations in simulations of soft tissues when it comes to dynamic\nand complex surgical processes. In this paper, we propose SurgiSim, a novel\nautomatic simulation system to overcome these limitations. To build a surgical\nsimulation environment, we maintain a canonical 3D scene composed of 3D\nGaussians coupled with a deformation field to represent a dynamic surgical\nscene. This process involves a multi-stage optimization with trajectory and\nanisotropic regularization, enhancing the geometry consistency of the canonical\nscene, which serves as the simulation environment. To achieve realistic\nphysical simulations in this environment, we implement a Visco-Elastic\ndeformation model based on the Maxwell model, effectively restoring the complex\ndeformations of tissues. Additionally, we infer the physical parameters of\ntissues by minimizing the discrepancies between the input video and simulation\nresults guided by estimated tissue motion, ensuring realistic simulation\noutcomes. Experiments on various surgical scenarios and interactions\ndemonstrate SurgiSim's ability to perform realistic simulation of soft tissues\namong surgical procedures, showing its enormous potential for enhancing\nsurgical training, planning, and robotic surgery systems. The project page is\nat https://namaenashibot.github.io/SurgiSim/.\n", "link": "http://arxiv.org/abs/2412.02359v1", "date": "2024-12-03", "relevancy": 2.9815, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6138}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Surgical%20Simulation%20from%20Monocular%20Videos&body=Title%3A%20Realistic%20Surgical%20Simulation%20from%20Monocular%20Videos%0AAuthor%3A%20Kailing%20Wang%20and%20Chen%20Yang%20and%20Keyang%20Zhao%20and%20Xiaokang%20Yang%20and%20Wei%20Shen%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20challenge%20of%20automatically%20performing%20realistic%0Asurgical%20simulations%20from%20readily%20available%20surgical%20videos.%20Recent%20efforts%0Ahave%20successfully%20integrated%20physically%20grounded%20dynamics%20within%203D%20Gaussians%0Ato%20perform%20high-fidelity%20simulations%20in%20well-reconstructed%20simulation%0Aenvironments%20from%20static%20scenes.%20However%2C%20they%20struggle%20with%20the%20geometric%0Ainconsistency%20in%20reconstructing%20simulation%20environments%20and%20unrealistic%0Aphysical%20deformations%20in%20simulations%20of%20soft%20tissues%20when%20it%20comes%20to%20dynamic%0Aand%20complex%20surgical%20processes.%20In%20this%20paper%2C%20we%20propose%20SurgiSim%2C%20a%20novel%0Aautomatic%20simulation%20system%20to%20overcome%20these%20limitations.%20To%20build%20a%20surgical%0Asimulation%20environment%2C%20we%20maintain%20a%20canonical%203D%20scene%20composed%20of%203D%0AGaussians%20coupled%20with%20a%20deformation%20field%20to%20represent%20a%20dynamic%20surgical%0Ascene.%20This%20process%20involves%20a%20multi-stage%20optimization%20with%20trajectory%20and%0Aanisotropic%20regularization%2C%20enhancing%20the%20geometry%20consistency%20of%20the%20canonical%0Ascene%2C%20which%20serves%20as%20the%20simulation%20environment.%20To%20achieve%20realistic%0Aphysical%20simulations%20in%20this%20environment%2C%20we%20implement%20a%20Visco-Elastic%0Adeformation%20model%20based%20on%20the%20Maxwell%20model%2C%20effectively%20restoring%20the%20complex%0Adeformations%20of%20tissues.%20Additionally%2C%20we%20infer%20the%20physical%20parameters%20of%0Atissues%20by%20minimizing%20the%20discrepancies%20between%20the%20input%20video%20and%20simulation%0Aresults%20guided%20by%20estimated%20tissue%20motion%2C%20ensuring%20realistic%20simulation%0Aoutcomes.%20Experiments%20on%20various%20surgical%20scenarios%20and%20interactions%0Ademonstrate%20SurgiSim%27s%20ability%20to%20perform%20realistic%20simulation%20of%20soft%20tissues%0Aamong%20surgical%20procedures%2C%20showing%20its%20enormous%20potential%20for%20enhancing%0Asurgical%20training%2C%20planning%2C%20and%20robotic%20surgery%20systems.%20The%20project%20page%20is%0Aat%20https%3A//namaenashibot.github.io/SurgiSim/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Surgical%2520Simulation%2520from%2520Monocular%2520Videos%26entry.906535625%3DKailing%2520Wang%2520and%2520Chen%2520Yang%2520and%2520Keyang%2520Zhao%2520and%2520Xiaokang%2520Yang%2520and%2520Wei%2520Shen%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520challenge%2520of%2520automatically%2520performing%2520realistic%250Asurgical%2520simulations%2520from%2520readily%2520available%2520surgical%2520videos.%2520Recent%2520efforts%250Ahave%2520successfully%2520integrated%2520physically%2520grounded%2520dynamics%2520within%25203D%2520Gaussians%250Ato%2520perform%2520high-fidelity%2520simulations%2520in%2520well-reconstructed%2520simulation%250Aenvironments%2520from%2520static%2520scenes.%2520However%252C%2520they%2520struggle%2520with%2520the%2520geometric%250Ainconsistency%2520in%2520reconstructing%2520simulation%2520environments%2520and%2520unrealistic%250Aphysical%2520deformations%2520in%2520simulations%2520of%2520soft%2520tissues%2520when%2520it%2520comes%2520to%2520dynamic%250Aand%2520complex%2520surgical%2520processes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SurgiSim%252C%2520a%2520novel%250Aautomatic%2520simulation%2520system%2520to%2520overcome%2520these%2520limitations.%2520To%2520build%2520a%2520surgical%250Asimulation%2520environment%252C%2520we%2520maintain%2520a%2520canonical%25203D%2520scene%2520composed%2520of%25203D%250AGaussians%2520coupled%2520with%2520a%2520deformation%2520field%2520to%2520represent%2520a%2520dynamic%2520surgical%250Ascene.%2520This%2520process%2520involves%2520a%2520multi-stage%2520optimization%2520with%2520trajectory%2520and%250Aanisotropic%2520regularization%252C%2520enhancing%2520the%2520geometry%2520consistency%2520of%2520the%2520canonical%250Ascene%252C%2520which%2520serves%2520as%2520the%2520simulation%2520environment.%2520To%2520achieve%2520realistic%250Aphysical%2520simulations%2520in%2520this%2520environment%252C%2520we%2520implement%2520a%2520Visco-Elastic%250Adeformation%2520model%2520based%2520on%2520the%2520Maxwell%2520model%252C%2520effectively%2520restoring%2520the%2520complex%250Adeformations%2520of%2520tissues.%2520Additionally%252C%2520we%2520infer%2520the%2520physical%2520parameters%2520of%250Atissues%2520by%2520minimizing%2520the%2520discrepancies%2520between%2520the%2520input%2520video%2520and%2520simulation%250Aresults%2520guided%2520by%2520estimated%2520tissue%2520motion%252C%2520ensuring%2520realistic%2520simulation%250Aoutcomes.%2520Experiments%2520on%2520various%2520surgical%2520scenarios%2520and%2520interactions%250Ademonstrate%2520SurgiSim%2527s%2520ability%2520to%2520perform%2520realistic%2520simulation%2520of%2520soft%2520tissues%250Aamong%2520surgical%2520procedures%252C%2520showing%2520its%2520enormous%2520potential%2520for%2520enhancing%250Asurgical%2520training%252C%2520planning%252C%2520and%2520robotic%2520surgery%2520systems.%2520The%2520project%2520page%2520is%250Aat%2520https%253A//namaenashibot.github.io/SurgiSim/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Surgical%20Simulation%20from%20Monocular%20Videos&entry.906535625=Kailing%20Wang%20and%20Chen%20Yang%20and%20Keyang%20Zhao%20and%20Xiaokang%20Yang%20and%20Wei%20Shen&entry.1292438233=%20%20This%20paper%20tackles%20the%20challenge%20of%20automatically%20performing%20realistic%0Asurgical%20simulations%20from%20readily%20available%20surgical%20videos.%20Recent%20efforts%0Ahave%20successfully%20integrated%20physically%20grounded%20dynamics%20within%203D%20Gaussians%0Ato%20perform%20high-fidelity%20simulations%20in%20well-reconstructed%20simulation%0Aenvironments%20from%20static%20scenes.%20However%2C%20they%20struggle%20with%20the%20geometric%0Ainconsistency%20in%20reconstructing%20simulation%20environments%20and%20unrealistic%0Aphysical%20deformations%20in%20simulations%20of%20soft%20tissues%20when%20it%20comes%20to%20dynamic%0Aand%20complex%20surgical%20processes.%20In%20this%20paper%2C%20we%20propose%20SurgiSim%2C%20a%20novel%0Aautomatic%20simulation%20system%20to%20overcome%20these%20limitations.%20To%20build%20a%20surgical%0Asimulation%20environment%2C%20we%20maintain%20a%20canonical%203D%20scene%20composed%20of%203D%0AGaussians%20coupled%20with%20a%20deformation%20field%20to%20represent%20a%20dynamic%20surgical%0Ascene.%20This%20process%20involves%20a%20multi-stage%20optimization%20with%20trajectory%20and%0Aanisotropic%20regularization%2C%20enhancing%20the%20geometry%20consistency%20of%20the%20canonical%0Ascene%2C%20which%20serves%20as%20the%20simulation%20environment.%20To%20achieve%20realistic%0Aphysical%20simulations%20in%20this%20environment%2C%20we%20implement%20a%20Visco-Elastic%0Adeformation%20model%20based%20on%20the%20Maxwell%20model%2C%20effectively%20restoring%20the%20complex%0Adeformations%20of%20tissues.%20Additionally%2C%20we%20infer%20the%20physical%20parameters%20of%0Atissues%20by%20minimizing%20the%20discrepancies%20between%20the%20input%20video%20and%20simulation%0Aresults%20guided%20by%20estimated%20tissue%20motion%2C%20ensuring%20realistic%20simulation%0Aoutcomes.%20Experiments%20on%20various%20surgical%20scenarios%20and%20interactions%0Ademonstrate%20SurgiSim%27s%20ability%20to%20perform%20realistic%20simulation%20of%20soft%20tissues%0Aamong%20surgical%20procedures%2C%20showing%20its%20enormous%20potential%20for%20enhancing%0Asurgical%20training%2C%20planning%2C%20and%20robotic%20surgery%20systems.%20The%20project%20page%20is%0Aat%20https%3A//namaenashibot.github.io/SurgiSim/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02359v1&entry.124074799=Read"},
{"title": "LiDAR-based Registration against Georeferenced Models for Globally\n  Consistent Allocentric Maps", "author": "Jan Quenzel and Linus T. Mallwitz and Benedikt T. Arnold and Sven Behnke", "abstract": "  Modern unmanned aerial vehicles (UAVs) are irreplaceable in search and rescue\n(SAR) missions to obtain a situational overview or provide closeups without\nendangering personnel. However, UAVs heavily rely on global navigation\nsatellite system (GNSS) for localization which works well in open spaces, but\nthe precision drastically degrades in the vicinity of buildings. These\ninaccuracies hinder aggregation of diverse data from multiple sources in a\nunified georeferenced frame for SAR operators. In contrast, CityGML models\nprovide approximate building shapes with accurate georeferenced poses. Besides,\nLiDAR works best in the vicinity of 3D structures. Hence, we refine coarse GNSS\nmeasurements by registering LiDAR maps against CityGML and digital elevation\nmap (DEM) models as a prior for allocentric mapping. An intuitive plausibility\nscore selects the best hypothesis based on occupancy using a 2D height map.\nAfterwards, we integrate the registration results in a continuous-time\nspline-based pose graph optimizer with LiDAR odometry and further sensing\nmodalities to obtain globally consistent, georeferenced trajectories and maps.\nWe evaluate the viability of our approach on multiple flights captured at two\ndistinct testing sites. Our method successfully reduced GNSS offset errors from\nup-to 16 m to below 0.5 m on multiple flights. Furthermore, we obtain globally\nconsistent maps w.r.t. prior 3D geospatial models.\n", "link": "http://arxiv.org/abs/2412.02533v1", "date": "2024-12-03", "relevancy": 2.9689, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5729}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-based%20Registration%20against%20Georeferenced%20Models%20for%20Globally%0A%20%20Consistent%20Allocentric%20Maps&body=Title%3A%20LiDAR-based%20Registration%20against%20Georeferenced%20Models%20for%20Globally%0A%20%20Consistent%20Allocentric%20Maps%0AAuthor%3A%20Jan%20Quenzel%20and%20Linus%20T.%20Mallwitz%20and%20Benedikt%20T.%20Arnold%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Modern%20unmanned%20aerial%20vehicles%20%28UAVs%29%20are%20irreplaceable%20in%20search%20and%20rescue%0A%28SAR%29%20missions%20to%20obtain%20a%20situational%20overview%20or%20provide%20closeups%20without%0Aendangering%20personnel.%20However%2C%20UAVs%20heavily%20rely%20on%20global%20navigation%0Asatellite%20system%20%28GNSS%29%20for%20localization%20which%20works%20well%20in%20open%20spaces%2C%20but%0Athe%20precision%20drastically%20degrades%20in%20the%20vicinity%20of%20buildings.%20These%0Ainaccuracies%20hinder%20aggregation%20of%20diverse%20data%20from%20multiple%20sources%20in%20a%0Aunified%20georeferenced%20frame%20for%20SAR%20operators.%20In%20contrast%2C%20CityGML%20models%0Aprovide%20approximate%20building%20shapes%20with%20accurate%20georeferenced%20poses.%20Besides%2C%0ALiDAR%20works%20best%20in%20the%20vicinity%20of%203D%20structures.%20Hence%2C%20we%20refine%20coarse%20GNSS%0Ameasurements%20by%20registering%20LiDAR%20maps%20against%20CityGML%20and%20digital%20elevation%0Amap%20%28DEM%29%20models%20as%20a%20prior%20for%20allocentric%20mapping.%20An%20intuitive%20plausibility%0Ascore%20selects%20the%20best%20hypothesis%20based%20on%20occupancy%20using%20a%202D%20height%20map.%0AAfterwards%2C%20we%20integrate%20the%20registration%20results%20in%20a%20continuous-time%0Aspline-based%20pose%20graph%20optimizer%20with%20LiDAR%20odometry%20and%20further%20sensing%0Amodalities%20to%20obtain%20globally%20consistent%2C%20georeferenced%20trajectories%20and%20maps.%0AWe%20evaluate%20the%20viability%20of%20our%20approach%20on%20multiple%20flights%20captured%20at%20two%0Adistinct%20testing%20sites.%20Our%20method%20successfully%20reduced%20GNSS%20offset%20errors%20from%0Aup-to%2016%20m%20to%20below%200.5%20m%20on%20multiple%20flights.%20Furthermore%2C%20we%20obtain%20globally%0Aconsistent%20maps%20w.r.t.%20prior%203D%20geospatial%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-based%2520Registration%2520against%2520Georeferenced%2520Models%2520for%2520Globally%250A%2520%2520Consistent%2520Allocentric%2520Maps%26entry.906535625%3DJan%2520Quenzel%2520and%2520Linus%2520T.%2520Mallwitz%2520and%2520Benedikt%2520T.%2520Arnold%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Modern%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520are%2520irreplaceable%2520in%2520search%2520and%2520rescue%250A%2528SAR%2529%2520missions%2520to%2520obtain%2520a%2520situational%2520overview%2520or%2520provide%2520closeups%2520without%250Aendangering%2520personnel.%2520However%252C%2520UAVs%2520heavily%2520rely%2520on%2520global%2520navigation%250Asatellite%2520system%2520%2528GNSS%2529%2520for%2520localization%2520which%2520works%2520well%2520in%2520open%2520spaces%252C%2520but%250Athe%2520precision%2520drastically%2520degrades%2520in%2520the%2520vicinity%2520of%2520buildings.%2520These%250Ainaccuracies%2520hinder%2520aggregation%2520of%2520diverse%2520data%2520from%2520multiple%2520sources%2520in%2520a%250Aunified%2520georeferenced%2520frame%2520for%2520SAR%2520operators.%2520In%2520contrast%252C%2520CityGML%2520models%250Aprovide%2520approximate%2520building%2520shapes%2520with%2520accurate%2520georeferenced%2520poses.%2520Besides%252C%250ALiDAR%2520works%2520best%2520in%2520the%2520vicinity%2520of%25203D%2520structures.%2520Hence%252C%2520we%2520refine%2520coarse%2520GNSS%250Ameasurements%2520by%2520registering%2520LiDAR%2520maps%2520against%2520CityGML%2520and%2520digital%2520elevation%250Amap%2520%2528DEM%2529%2520models%2520as%2520a%2520prior%2520for%2520allocentric%2520mapping.%2520An%2520intuitive%2520plausibility%250Ascore%2520selects%2520the%2520best%2520hypothesis%2520based%2520on%2520occupancy%2520using%2520a%25202D%2520height%2520map.%250AAfterwards%252C%2520we%2520integrate%2520the%2520registration%2520results%2520in%2520a%2520continuous-time%250Aspline-based%2520pose%2520graph%2520optimizer%2520with%2520LiDAR%2520odometry%2520and%2520further%2520sensing%250Amodalities%2520to%2520obtain%2520globally%2520consistent%252C%2520georeferenced%2520trajectories%2520and%2520maps.%250AWe%2520evaluate%2520the%2520viability%2520of%2520our%2520approach%2520on%2520multiple%2520flights%2520captured%2520at%2520two%250Adistinct%2520testing%2520sites.%2520Our%2520method%2520successfully%2520reduced%2520GNSS%2520offset%2520errors%2520from%250Aup-to%252016%2520m%2520to%2520below%25200.5%2520m%2520on%2520multiple%2520flights.%2520Furthermore%252C%2520we%2520obtain%2520globally%250Aconsistent%2520maps%2520w.r.t.%2520prior%25203D%2520geospatial%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-based%20Registration%20against%20Georeferenced%20Models%20for%20Globally%0A%20%20Consistent%20Allocentric%20Maps&entry.906535625=Jan%20Quenzel%20and%20Linus%20T.%20Mallwitz%20and%20Benedikt%20T.%20Arnold%20and%20Sven%20Behnke&entry.1292438233=%20%20Modern%20unmanned%20aerial%20vehicles%20%28UAVs%29%20are%20irreplaceable%20in%20search%20and%20rescue%0A%28SAR%29%20missions%20to%20obtain%20a%20situational%20overview%20or%20provide%20closeups%20without%0Aendangering%20personnel.%20However%2C%20UAVs%20heavily%20rely%20on%20global%20navigation%0Asatellite%20system%20%28GNSS%29%20for%20localization%20which%20works%20well%20in%20open%20spaces%2C%20but%0Athe%20precision%20drastically%20degrades%20in%20the%20vicinity%20of%20buildings.%20These%0Ainaccuracies%20hinder%20aggregation%20of%20diverse%20data%20from%20multiple%20sources%20in%20a%0Aunified%20georeferenced%20frame%20for%20SAR%20operators.%20In%20contrast%2C%20CityGML%20models%0Aprovide%20approximate%20building%20shapes%20with%20accurate%20georeferenced%20poses.%20Besides%2C%0ALiDAR%20works%20best%20in%20the%20vicinity%20of%203D%20structures.%20Hence%2C%20we%20refine%20coarse%20GNSS%0Ameasurements%20by%20registering%20LiDAR%20maps%20against%20CityGML%20and%20digital%20elevation%0Amap%20%28DEM%29%20models%20as%20a%20prior%20for%20allocentric%20mapping.%20An%20intuitive%20plausibility%0Ascore%20selects%20the%20best%20hypothesis%20based%20on%20occupancy%20using%20a%202D%20height%20map.%0AAfterwards%2C%20we%20integrate%20the%20registration%20results%20in%20a%20continuous-time%0Aspline-based%20pose%20graph%20optimizer%20with%20LiDAR%20odometry%20and%20further%20sensing%0Amodalities%20to%20obtain%20globally%20consistent%2C%20georeferenced%20trajectories%20and%20maps.%0AWe%20evaluate%20the%20viability%20of%20our%20approach%20on%20multiple%20flights%20captured%20at%20two%0Adistinct%20testing%20sites.%20Our%20method%20successfully%20reduced%20GNSS%20offset%20errors%20from%0Aup-to%2016%20m%20to%20below%200.5%20m%20on%20multiple%20flights.%20Furthermore%2C%20we%20obtain%20globally%0Aconsistent%20maps%20w.r.t.%20prior%203D%20geospatial%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02533v1&entry.124074799=Read"},
{"title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand\n  Audio-Visual Information?", "author": "Kaixiong Gong and Kaituo Feng and Bohao Li and Yibing Wang and Mofan Cheng and Shijia Yang and Jiaming Han and Benyou Wang and Yutong Bai and Zhuoran Yang and Xiangyu Yue", "abstract": "  Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini\n1.5 Pro, and Reka Core, have expanded their capabilities to include vision and\naudio modalities. While these models demonstrate impressive performance across\na wide range of audio-visual applications, our proposed DeafTest reveals that\nMLLMs often struggle with simple tasks humans find trivial: 1) determining\nwhich of two sounds is louder, and 2) determining which of two sounds has a\nhigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a\ncomprehensive audio-visual benchmark designed to assess whether those MLLMs can\ntruly understand the audio-visual information. This benchmark encompasses 4,555\ncarefully crafted problems, each incorporating text, visual, and audio\ncomponents. To successfully infer answers, models must effectively leverage\nclues from both visual and audio inputs. To ensure precise and objective\nevaluation of MLLM responses, we have structured the questions as\nmultiple-choice, eliminating the need for human evaluation or LLM-assisted\nassessment. We benchmark a series of closed-source and open-source models and\nsummarize the observations. By revealing the limitations of current models, we\naim to provide useful insight for future dataset collection and model\ndevelopment.\n", "link": "http://arxiv.org/abs/2412.02611v1", "date": "2024-12-03", "relevancy": 2.9532, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AV-Odyssey%20Bench%3A%20Can%20Your%20Multimodal%20LLMs%20Really%20Understand%0A%20%20Audio-Visual%20Information%3F&body=Title%3A%20AV-Odyssey%20Bench%3A%20Can%20Your%20Multimodal%20LLMs%20Really%20Understand%0A%20%20Audio-Visual%20Information%3F%0AAuthor%3A%20Kaixiong%20Gong%20and%20Kaituo%20Feng%20and%20Bohao%20Li%20and%20Yibing%20Wang%20and%20Mofan%20Cheng%20and%20Shijia%20Yang%20and%20Jiaming%20Han%20and%20Benyou%20Wang%20and%20Yutong%20Bai%20and%20Zhuoran%20Yang%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%20GPT-4o%2C%20Gemini%0A1.5%20Pro%2C%20and%20Reka%20Core%2C%20have%20expanded%20their%20capabilities%20to%20include%20vision%20and%0Aaudio%20modalities.%20While%20these%20models%20demonstrate%20impressive%20performance%20across%0Aa%20wide%20range%20of%20audio-visual%20applications%2C%20our%20proposed%20DeafTest%20reveals%20that%0AMLLMs%20often%20struggle%20with%20simple%20tasks%20humans%20find%20trivial%3A%201%29%20determining%0Awhich%20of%20two%20sounds%20is%20louder%2C%20and%202%29%20determining%20which%20of%20two%20sounds%20has%20a%0Ahigher%20pitch.%20Motivated%20by%20these%20observations%2C%20we%20introduce%20AV-Odyssey%20Bench%2C%20a%0Acomprehensive%20audio-visual%20benchmark%20designed%20to%20assess%20whether%20those%20MLLMs%20can%0Atruly%20understand%20the%20audio-visual%20information.%20This%20benchmark%20encompasses%204%2C555%0Acarefully%20crafted%20problems%2C%20each%20incorporating%20text%2C%20visual%2C%20and%20audio%0Acomponents.%20To%20successfully%20infer%20answers%2C%20models%20must%20effectively%20leverage%0Aclues%20from%20both%20visual%20and%20audio%20inputs.%20To%20ensure%20precise%20and%20objective%0Aevaluation%20of%20MLLM%20responses%2C%20we%20have%20structured%20the%20questions%20as%0Amultiple-choice%2C%20eliminating%20the%20need%20for%20human%20evaluation%20or%20LLM-assisted%0Aassessment.%20We%20benchmark%20a%20series%20of%20closed-source%20and%20open-source%20models%20and%0Asummarize%20the%20observations.%20By%20revealing%20the%20limitations%20of%20current%20models%2C%20we%0Aaim%20to%20provide%20useful%20insight%20for%20future%20dataset%20collection%20and%20model%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAV-Odyssey%2520Bench%253A%2520Can%2520Your%2520Multimodal%2520LLMs%2520Really%2520Understand%250A%2520%2520Audio-Visual%2520Information%253F%26entry.906535625%3DKaixiong%2520Gong%2520and%2520Kaituo%2520Feng%2520and%2520Bohao%2520Li%2520and%2520Yibing%2520Wang%2520and%2520Mofan%2520Cheng%2520and%2520Shijia%2520Yang%2520and%2520Jiaming%2520Han%2520and%2520Benyou%2520Wang%2520and%2520Yutong%2520Bai%2520and%2520Zhuoran%2520Yang%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Recently%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520such%2520as%2520GPT-4o%252C%2520Gemini%250A1.5%2520Pro%252C%2520and%2520Reka%2520Core%252C%2520have%2520expanded%2520their%2520capabilities%2520to%2520include%2520vision%2520and%250Aaudio%2520modalities.%2520While%2520these%2520models%2520demonstrate%2520impressive%2520performance%2520across%250Aa%2520wide%2520range%2520of%2520audio-visual%2520applications%252C%2520our%2520proposed%2520DeafTest%2520reveals%2520that%250AMLLMs%2520often%2520struggle%2520with%2520simple%2520tasks%2520humans%2520find%2520trivial%253A%25201%2529%2520determining%250Awhich%2520of%2520two%2520sounds%2520is%2520louder%252C%2520and%25202%2529%2520determining%2520which%2520of%2520two%2520sounds%2520has%2520a%250Ahigher%2520pitch.%2520Motivated%2520by%2520these%2520observations%252C%2520we%2520introduce%2520AV-Odyssey%2520Bench%252C%2520a%250Acomprehensive%2520audio-visual%2520benchmark%2520designed%2520to%2520assess%2520whether%2520those%2520MLLMs%2520can%250Atruly%2520understand%2520the%2520audio-visual%2520information.%2520This%2520benchmark%2520encompasses%25204%252C555%250Acarefully%2520crafted%2520problems%252C%2520each%2520incorporating%2520text%252C%2520visual%252C%2520and%2520audio%250Acomponents.%2520To%2520successfully%2520infer%2520answers%252C%2520models%2520must%2520effectively%2520leverage%250Aclues%2520from%2520both%2520visual%2520and%2520audio%2520inputs.%2520To%2520ensure%2520precise%2520and%2520objective%250Aevaluation%2520of%2520MLLM%2520responses%252C%2520we%2520have%2520structured%2520the%2520questions%2520as%250Amultiple-choice%252C%2520eliminating%2520the%2520need%2520for%2520human%2520evaluation%2520or%2520LLM-assisted%250Aassessment.%2520We%2520benchmark%2520a%2520series%2520of%2520closed-source%2520and%2520open-source%2520models%2520and%250Asummarize%2520the%2520observations.%2520By%2520revealing%2520the%2520limitations%2520of%2520current%2520models%252C%2520we%250Aaim%2520to%2520provide%2520useful%2520insight%2520for%2520future%2520dataset%2520collection%2520and%2520model%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AV-Odyssey%20Bench%3A%20Can%20Your%20Multimodal%20LLMs%20Really%20Understand%0A%20%20Audio-Visual%20Information%3F&entry.906535625=Kaixiong%20Gong%20and%20Kaituo%20Feng%20and%20Bohao%20Li%20and%20Yibing%20Wang%20and%20Mofan%20Cheng%20and%20Shijia%20Yang%20and%20Jiaming%20Han%20and%20Benyou%20Wang%20and%20Yutong%20Bai%20and%20Zhuoran%20Yang%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%20GPT-4o%2C%20Gemini%0A1.5%20Pro%2C%20and%20Reka%20Core%2C%20have%20expanded%20their%20capabilities%20to%20include%20vision%20and%0Aaudio%20modalities.%20While%20these%20models%20demonstrate%20impressive%20performance%20across%0Aa%20wide%20range%20of%20audio-visual%20applications%2C%20our%20proposed%20DeafTest%20reveals%20that%0AMLLMs%20often%20struggle%20with%20simple%20tasks%20humans%20find%20trivial%3A%201%29%20determining%0Awhich%20of%20two%20sounds%20is%20louder%2C%20and%202%29%20determining%20which%20of%20two%20sounds%20has%20a%0Ahigher%20pitch.%20Motivated%20by%20these%20observations%2C%20we%20introduce%20AV-Odyssey%20Bench%2C%20a%0Acomprehensive%20audio-visual%20benchmark%20designed%20to%20assess%20whether%20those%20MLLMs%20can%0Atruly%20understand%20the%20audio-visual%20information.%20This%20benchmark%20encompasses%204%2C555%0Acarefully%20crafted%20problems%2C%20each%20incorporating%20text%2C%20visual%2C%20and%20audio%0Acomponents.%20To%20successfully%20infer%20answers%2C%20models%20must%20effectively%20leverage%0Aclues%20from%20both%20visual%20and%20audio%20inputs.%20To%20ensure%20precise%20and%20objective%0Aevaluation%20of%20MLLM%20responses%2C%20we%20have%20structured%20the%20questions%20as%0Amultiple-choice%2C%20eliminating%20the%20need%20for%20human%20evaluation%20or%20LLM-assisted%0Aassessment.%20We%20benchmark%20a%20series%20of%20closed-source%20and%20open-source%20models%20and%0Asummarize%20the%20observations.%20By%20revealing%20the%20limitations%20of%20current%20models%2C%20we%0Aaim%20to%20provide%20useful%20insight%20for%20future%20dataset%20collection%20and%20model%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02611v1&entry.124074799=Read"},
{"title": "3D Face Reconstruction From Radar Images", "author": "Valentin Braeutigam and Vanessa Wirth and Ingrid Ullmann and Christian Sch\u00fc\u00dfler and Martin Vossiek and Matthias Berking and Bernhard Egger", "abstract": "  The 3D reconstruction of faces gains wide attention in computer vision and is\nused in many fields of application, for example, animation, virtual reality,\nand even forensics. This work is motivated by monitoring patients in sleep\nlaboratories. Due to their unique characteristics, sensors from the radar\ndomain have advantages compared to optical sensors, namely penetration of\nelectrically non-conductive materials and independence of light. These\nadvantages of radar signals unlock new applications and require adaptation of\n3D reconstruction frameworks. We propose a novel model-based method for 3D\nreconstruction from radar images. We generate a dataset of synthetic radar\nimages with a physics-based but non-differentiable radar renderer. This dataset\nis used to train a CNN-based encoder to estimate the parameters of a 3D\nmorphable face model. Whilst the encoder alone already leads to strong\nreconstructions of synthetic data, we extend our reconstruction in an\nAnalysis-by-Synthesis fashion to a model-based autoencoder. This is enabled by\nlearning the rendering process in the decoder, which acts as an object-specific\ndifferentiable radar renderer. Subsequently, the combination of both network\nparts is trained to minimize both, the loss of the parameters and the loss of\nthe resulting reconstructed radar image. This leads to the additional benefit,\nthat at test time the parameters can be further optimized by finetuning the\nautoencoder unsupervised on the image loss. We evaluated our framework on\ngenerated synthetic face images as well as on real radar images with 3D ground\ntruth of four individuals.\n", "link": "http://arxiv.org/abs/2412.02403v1", "date": "2024-12-03", "relevancy": 2.9482, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5961}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5864}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Face%20Reconstruction%20From%20Radar%20Images&body=Title%3A%203D%20Face%20Reconstruction%20From%20Radar%20Images%0AAuthor%3A%20Valentin%20Braeutigam%20and%20Vanessa%20Wirth%20and%20Ingrid%20Ullmann%20and%20Christian%20Sch%C3%BC%C3%9Fler%20and%20Martin%20Vossiek%20and%20Matthias%20Berking%20and%20Bernhard%20Egger%0AAbstract%3A%20%20%20The%203D%20reconstruction%20of%20faces%20gains%20wide%20attention%20in%20computer%20vision%20and%20is%0Aused%20in%20many%20fields%20of%20application%2C%20for%20example%2C%20animation%2C%20virtual%20reality%2C%0Aand%20even%20forensics.%20This%20work%20is%20motivated%20by%20monitoring%20patients%20in%20sleep%0Alaboratories.%20Due%20to%20their%20unique%20characteristics%2C%20sensors%20from%20the%20radar%0Adomain%20have%20advantages%20compared%20to%20optical%20sensors%2C%20namely%20penetration%20of%0Aelectrically%20non-conductive%20materials%20and%20independence%20of%20light.%20These%0Aadvantages%20of%20radar%20signals%20unlock%20new%20applications%20and%20require%20adaptation%20of%0A3D%20reconstruction%20frameworks.%20We%20propose%20a%20novel%20model-based%20method%20for%203D%0Areconstruction%20from%20radar%20images.%20We%20generate%20a%20dataset%20of%20synthetic%20radar%0Aimages%20with%20a%20physics-based%20but%20non-differentiable%20radar%20renderer.%20This%20dataset%0Ais%20used%20to%20train%20a%20CNN-based%20encoder%20to%20estimate%20the%20parameters%20of%20a%203D%0Amorphable%20face%20model.%20Whilst%20the%20encoder%20alone%20already%20leads%20to%20strong%0Areconstructions%20of%20synthetic%20data%2C%20we%20extend%20our%20reconstruction%20in%20an%0AAnalysis-by-Synthesis%20fashion%20to%20a%20model-based%20autoencoder.%20This%20is%20enabled%20by%0Alearning%20the%20rendering%20process%20in%20the%20decoder%2C%20which%20acts%20as%20an%20object-specific%0Adifferentiable%20radar%20renderer.%20Subsequently%2C%20the%20combination%20of%20both%20network%0Aparts%20is%20trained%20to%20minimize%20both%2C%20the%20loss%20of%20the%20parameters%20and%20the%20loss%20of%0Athe%20resulting%20reconstructed%20radar%20image.%20This%20leads%20to%20the%20additional%20benefit%2C%0Athat%20at%20test%20time%20the%20parameters%20can%20be%20further%20optimized%20by%20finetuning%20the%0Aautoencoder%20unsupervised%20on%20the%20image%20loss.%20We%20evaluated%20our%20framework%20on%0Agenerated%20synthetic%20face%20images%20as%20well%20as%20on%20real%20radar%20images%20with%203D%20ground%0Atruth%20of%20four%20individuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Face%2520Reconstruction%2520From%2520Radar%2520Images%26entry.906535625%3DValentin%2520Braeutigam%2520and%2520Vanessa%2520Wirth%2520and%2520Ingrid%2520Ullmann%2520and%2520Christian%2520Sch%25C3%25BC%25C3%259Fler%2520and%2520Martin%2520Vossiek%2520and%2520Matthias%2520Berking%2520and%2520Bernhard%2520Egger%26entry.1292438233%3D%2520%2520The%25203D%2520reconstruction%2520of%2520faces%2520gains%2520wide%2520attention%2520in%2520computer%2520vision%2520and%2520is%250Aused%2520in%2520many%2520fields%2520of%2520application%252C%2520for%2520example%252C%2520animation%252C%2520virtual%2520reality%252C%250Aand%2520even%2520forensics.%2520This%2520work%2520is%2520motivated%2520by%2520monitoring%2520patients%2520in%2520sleep%250Alaboratories.%2520Due%2520to%2520their%2520unique%2520characteristics%252C%2520sensors%2520from%2520the%2520radar%250Adomain%2520have%2520advantages%2520compared%2520to%2520optical%2520sensors%252C%2520namely%2520penetration%2520of%250Aelectrically%2520non-conductive%2520materials%2520and%2520independence%2520of%2520light.%2520These%250Aadvantages%2520of%2520radar%2520signals%2520unlock%2520new%2520applications%2520and%2520require%2520adaptation%2520of%250A3D%2520reconstruction%2520frameworks.%2520We%2520propose%2520a%2520novel%2520model-based%2520method%2520for%25203D%250Areconstruction%2520from%2520radar%2520images.%2520We%2520generate%2520a%2520dataset%2520of%2520synthetic%2520radar%250Aimages%2520with%2520a%2520physics-based%2520but%2520non-differentiable%2520radar%2520renderer.%2520This%2520dataset%250Ais%2520used%2520to%2520train%2520a%2520CNN-based%2520encoder%2520to%2520estimate%2520the%2520parameters%2520of%2520a%25203D%250Amorphable%2520face%2520model.%2520Whilst%2520the%2520encoder%2520alone%2520already%2520leads%2520to%2520strong%250Areconstructions%2520of%2520synthetic%2520data%252C%2520we%2520extend%2520our%2520reconstruction%2520in%2520an%250AAnalysis-by-Synthesis%2520fashion%2520to%2520a%2520model-based%2520autoencoder.%2520This%2520is%2520enabled%2520by%250Alearning%2520the%2520rendering%2520process%2520in%2520the%2520decoder%252C%2520which%2520acts%2520as%2520an%2520object-specific%250Adifferentiable%2520radar%2520renderer.%2520Subsequently%252C%2520the%2520combination%2520of%2520both%2520network%250Aparts%2520is%2520trained%2520to%2520minimize%2520both%252C%2520the%2520loss%2520of%2520the%2520parameters%2520and%2520the%2520loss%2520of%250Athe%2520resulting%2520reconstructed%2520radar%2520image.%2520This%2520leads%2520to%2520the%2520additional%2520benefit%252C%250Athat%2520at%2520test%2520time%2520the%2520parameters%2520can%2520be%2520further%2520optimized%2520by%2520finetuning%2520the%250Aautoencoder%2520unsupervised%2520on%2520the%2520image%2520loss.%2520We%2520evaluated%2520our%2520framework%2520on%250Agenerated%2520synthetic%2520face%2520images%2520as%2520well%2520as%2520on%2520real%2520radar%2520images%2520with%25203D%2520ground%250Atruth%2520of%2520four%2520individuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Face%20Reconstruction%20From%20Radar%20Images&entry.906535625=Valentin%20Braeutigam%20and%20Vanessa%20Wirth%20and%20Ingrid%20Ullmann%20and%20Christian%20Sch%C3%BC%C3%9Fler%20and%20Martin%20Vossiek%20and%20Matthias%20Berking%20and%20Bernhard%20Egger&entry.1292438233=%20%20The%203D%20reconstruction%20of%20faces%20gains%20wide%20attention%20in%20computer%20vision%20and%20is%0Aused%20in%20many%20fields%20of%20application%2C%20for%20example%2C%20animation%2C%20virtual%20reality%2C%0Aand%20even%20forensics.%20This%20work%20is%20motivated%20by%20monitoring%20patients%20in%20sleep%0Alaboratories.%20Due%20to%20their%20unique%20characteristics%2C%20sensors%20from%20the%20radar%0Adomain%20have%20advantages%20compared%20to%20optical%20sensors%2C%20namely%20penetration%20of%0Aelectrically%20non-conductive%20materials%20and%20independence%20of%20light.%20These%0Aadvantages%20of%20radar%20signals%20unlock%20new%20applications%20and%20require%20adaptation%20of%0A3D%20reconstruction%20frameworks.%20We%20propose%20a%20novel%20model-based%20method%20for%203D%0Areconstruction%20from%20radar%20images.%20We%20generate%20a%20dataset%20of%20synthetic%20radar%0Aimages%20with%20a%20physics-based%20but%20non-differentiable%20radar%20renderer.%20This%20dataset%0Ais%20used%20to%20train%20a%20CNN-based%20encoder%20to%20estimate%20the%20parameters%20of%20a%203D%0Amorphable%20face%20model.%20Whilst%20the%20encoder%20alone%20already%20leads%20to%20strong%0Areconstructions%20of%20synthetic%20data%2C%20we%20extend%20our%20reconstruction%20in%20an%0AAnalysis-by-Synthesis%20fashion%20to%20a%20model-based%20autoencoder.%20This%20is%20enabled%20by%0Alearning%20the%20rendering%20process%20in%20the%20decoder%2C%20which%20acts%20as%20an%20object-specific%0Adifferentiable%20radar%20renderer.%20Subsequently%2C%20the%20combination%20of%20both%20network%0Aparts%20is%20trained%20to%20minimize%20both%2C%20the%20loss%20of%20the%20parameters%20and%20the%20loss%20of%0Athe%20resulting%20reconstructed%20radar%20image.%20This%20leads%20to%20the%20additional%20benefit%2C%0Athat%20at%20test%20time%20the%20parameters%20can%20be%20further%20optimized%20by%20finetuning%20the%0Aautoencoder%20unsupervised%20on%20the%20image%20loss.%20We%20evaluated%20our%20framework%20on%0Agenerated%20synthetic%20face%20images%20as%20well%20as%20on%20real%20radar%20images%20with%203D%20ground%0Atruth%20of%20four%20individuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02403v1&entry.124074799=Read"},
{"title": "RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D\n  Referring Expression Segmentation", "author": "Changli Wu and Qi Chen and Jiayi Ji and Haowei Wang and Yiwei Ma and You Huang and Gen Luo and Hao Fei and Xiaoshuai Sun and Rongrong Ji", "abstract": "  3D Referring Expression Segmentation (3D-RES) aims to segment 3D objects by\ncorrelating referring expressions with point clouds. However, traditional\napproaches frequently encounter issues like over-segmentation or\nmis-segmentation, due to insufficient emphasis on spatial information of\ninstances. In this paper, we introduce a Rule-Guided Spatial Awareness Network\n(RG-SAN) by utilizing solely the spatial information of the target instance for\nsupervision. This approach enables the network to accurately depict the spatial\nrelationships among all entities described in the text, thus enhancing the\nreasoning capabilities. The RG-SAN consists of the Text-driven Localization\nModule (TLM) and the Rule-guided Weak Supervision (RWS) strategy. The TLM\ninitially locates all mentioned instances and iteratively refines their\npositional information. The RWS strategy, acknowledging that only target\nobjects have supervised positional information, employs dependency tree rules\nto precisely guide the core instance's positioning. Extensive testing on the\nScanRefer benchmark has shown that RG-SAN not only establishes new performance\nbenchmarks, with an mIoU increase of 5.1 points, but also exhibits significant\nimprovements in robustness when processing descriptions with spatial ambiguity.\nAll codes are available at https://github.com/sosppxo/RG-SAN.\n", "link": "http://arxiv.org/abs/2412.02402v1", "date": "2024-12-03", "relevancy": 2.8986, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RG-SAN%3A%20Rule-Guided%20Spatial%20Awareness%20Network%20for%20End-to-End%203D%0A%20%20Referring%20Expression%20Segmentation&body=Title%3A%20RG-SAN%3A%20Rule-Guided%20Spatial%20Awareness%20Network%20for%20End-to-End%203D%0A%20%20Referring%20Expression%20Segmentation%0AAuthor%3A%20Changli%20Wu%20and%20Qi%20Chen%20and%20Jiayi%20Ji%20and%20Haowei%20Wang%20and%20Yiwei%20Ma%20and%20You%20Huang%20and%20Gen%20Luo%20and%20Hao%20Fei%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%203D%20Referring%20Expression%20Segmentation%20%283D-RES%29%20aims%20to%20segment%203D%20objects%20by%0Acorrelating%20referring%20expressions%20with%20point%20clouds.%20However%2C%20traditional%0Aapproaches%20frequently%20encounter%20issues%20like%20over-segmentation%20or%0Amis-segmentation%2C%20due%20to%20insufficient%20emphasis%20on%20spatial%20information%20of%0Ainstances.%20In%20this%20paper%2C%20we%20introduce%20a%20Rule-Guided%20Spatial%20Awareness%20Network%0A%28RG-SAN%29%20by%20utilizing%20solely%20the%20spatial%20information%20of%20the%20target%20instance%20for%0Asupervision.%20This%20approach%20enables%20the%20network%20to%20accurately%20depict%20the%20spatial%0Arelationships%20among%20all%20entities%20described%20in%20the%20text%2C%20thus%20enhancing%20the%0Areasoning%20capabilities.%20The%20RG-SAN%20consists%20of%20the%20Text-driven%20Localization%0AModule%20%28TLM%29%20and%20the%20Rule-guided%20Weak%20Supervision%20%28RWS%29%20strategy.%20The%20TLM%0Ainitially%20locates%20all%20mentioned%20instances%20and%20iteratively%20refines%20their%0Apositional%20information.%20The%20RWS%20strategy%2C%20acknowledging%20that%20only%20target%0Aobjects%20have%20supervised%20positional%20information%2C%20employs%20dependency%20tree%20rules%0Ato%20precisely%20guide%20the%20core%20instance%27s%20positioning.%20Extensive%20testing%20on%20the%0AScanRefer%20benchmark%20has%20shown%20that%20RG-SAN%20not%20only%20establishes%20new%20performance%0Abenchmarks%2C%20with%20an%20mIoU%20increase%20of%205.1%20points%2C%20but%20also%20exhibits%20significant%0Aimprovements%20in%20robustness%20when%20processing%20descriptions%20with%20spatial%20ambiguity.%0AAll%20codes%20are%20available%20at%20https%3A//github.com/sosppxo/RG-SAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRG-SAN%253A%2520Rule-Guided%2520Spatial%2520Awareness%2520Network%2520for%2520End-to-End%25203D%250A%2520%2520Referring%2520Expression%2520Segmentation%26entry.906535625%3DChangli%2520Wu%2520and%2520Qi%2520Chen%2520and%2520Jiayi%2520Ji%2520and%2520Haowei%2520Wang%2520and%2520Yiwei%2520Ma%2520and%2520You%2520Huang%2520and%2520Gen%2520Luo%2520and%2520Hao%2520Fei%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%25203D%2520Referring%2520Expression%2520Segmentation%2520%25283D-RES%2529%2520aims%2520to%2520segment%25203D%2520objects%2520by%250Acorrelating%2520referring%2520expressions%2520with%2520point%2520clouds.%2520However%252C%2520traditional%250Aapproaches%2520frequently%2520encounter%2520issues%2520like%2520over-segmentation%2520or%250Amis-segmentation%252C%2520due%2520to%2520insufficient%2520emphasis%2520on%2520spatial%2520information%2520of%250Ainstances.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520Rule-Guided%2520Spatial%2520Awareness%2520Network%250A%2528RG-SAN%2529%2520by%2520utilizing%2520solely%2520the%2520spatial%2520information%2520of%2520the%2520target%2520instance%2520for%250Asupervision.%2520This%2520approach%2520enables%2520the%2520network%2520to%2520accurately%2520depict%2520the%2520spatial%250Arelationships%2520among%2520all%2520entities%2520described%2520in%2520the%2520text%252C%2520thus%2520enhancing%2520the%250Areasoning%2520capabilities.%2520The%2520RG-SAN%2520consists%2520of%2520the%2520Text-driven%2520Localization%250AModule%2520%2528TLM%2529%2520and%2520the%2520Rule-guided%2520Weak%2520Supervision%2520%2528RWS%2529%2520strategy.%2520The%2520TLM%250Ainitially%2520locates%2520all%2520mentioned%2520instances%2520and%2520iteratively%2520refines%2520their%250Apositional%2520information.%2520The%2520RWS%2520strategy%252C%2520acknowledging%2520that%2520only%2520target%250Aobjects%2520have%2520supervised%2520positional%2520information%252C%2520employs%2520dependency%2520tree%2520rules%250Ato%2520precisely%2520guide%2520the%2520core%2520instance%2527s%2520positioning.%2520Extensive%2520testing%2520on%2520the%250AScanRefer%2520benchmark%2520has%2520shown%2520that%2520RG-SAN%2520not%2520only%2520establishes%2520new%2520performance%250Abenchmarks%252C%2520with%2520an%2520mIoU%2520increase%2520of%25205.1%2520points%252C%2520but%2520also%2520exhibits%2520significant%250Aimprovements%2520in%2520robustness%2520when%2520processing%2520descriptions%2520with%2520spatial%2520ambiguity.%250AAll%2520codes%2520are%2520available%2520at%2520https%253A//github.com/sosppxo/RG-SAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RG-SAN%3A%20Rule-Guided%20Spatial%20Awareness%20Network%20for%20End-to-End%203D%0A%20%20Referring%20Expression%20Segmentation&entry.906535625=Changli%20Wu%20and%20Qi%20Chen%20and%20Jiayi%20Ji%20and%20Haowei%20Wang%20and%20Yiwei%20Ma%20and%20You%20Huang%20and%20Gen%20Luo%20and%20Hao%20Fei%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%203D%20Referring%20Expression%20Segmentation%20%283D-RES%29%20aims%20to%20segment%203D%20objects%20by%0Acorrelating%20referring%20expressions%20with%20point%20clouds.%20However%2C%20traditional%0Aapproaches%20frequently%20encounter%20issues%20like%20over-segmentation%20or%0Amis-segmentation%2C%20due%20to%20insufficient%20emphasis%20on%20spatial%20information%20of%0Ainstances.%20In%20this%20paper%2C%20we%20introduce%20a%20Rule-Guided%20Spatial%20Awareness%20Network%0A%28RG-SAN%29%20by%20utilizing%20solely%20the%20spatial%20information%20of%20the%20target%20instance%20for%0Asupervision.%20This%20approach%20enables%20the%20network%20to%20accurately%20depict%20the%20spatial%0Arelationships%20among%20all%20entities%20described%20in%20the%20text%2C%20thus%20enhancing%20the%0Areasoning%20capabilities.%20The%20RG-SAN%20consists%20of%20the%20Text-driven%20Localization%0AModule%20%28TLM%29%20and%20the%20Rule-guided%20Weak%20Supervision%20%28RWS%29%20strategy.%20The%20TLM%0Ainitially%20locates%20all%20mentioned%20instances%20and%20iteratively%20refines%20their%0Apositional%20information.%20The%20RWS%20strategy%2C%20acknowledging%20that%20only%20target%0Aobjects%20have%20supervised%20positional%20information%2C%20employs%20dependency%20tree%20rules%0Ato%20precisely%20guide%20the%20core%20instance%27s%20positioning.%20Extensive%20testing%20on%20the%0AScanRefer%20benchmark%20has%20shown%20that%20RG-SAN%20not%20only%20establishes%20new%20performance%0Abenchmarks%2C%20with%20an%20mIoU%20increase%20of%205.1%20points%2C%20but%20also%20exhibits%20significant%0Aimprovements%20in%20robustness%20when%20processing%20descriptions%20with%20spatial%20ambiguity.%0AAll%20codes%20are%20available%20at%20https%3A//github.com/sosppxo/RG-SAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02402v1&entry.124074799=Read"},
{"title": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View", "author": "Zichen Yu and Quanli Liu and Wei Wang and Liyong Zhang and Xiaoguang Zhao", "abstract": "  Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git.(This work has been submitted to the\nIEEE for possible publication. Copyright may be transferred without notice,\nafter which this version may no longer be accessible)\n", "link": "http://arxiv.org/abs/2408.16200v3", "date": "2024-12-04", "relevancy": 2.8834, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5785}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolarBEVDet%3A%20Exploring%20Polar%20Representation%20for%20Multi-View%203D%20Object%0A%20%20Detection%20in%20Bird%27s-Eye-View&body=Title%3A%20PolarBEVDet%3A%20Exploring%20Polar%20Representation%20for%20Multi-View%203D%20Object%0A%20%20Detection%20in%20Bird%27s-Eye-View%0AAuthor%3A%20Zichen%20Yu%20and%20Quanli%20Liu%20and%20Wei%20Wang%20and%20Liyong%20Zhang%20and%20Xiaoguang%20Zhao%0AAbstract%3A%20%20%20Recently%2C%20LSS-based%20multi-view%203D%20object%20detection%20provides%20an%20economical%20and%0Adeployment-friendly%20solution%20for%20autonomous%20driving.%20However%2C%20all%20the%20existing%0ALSS-based%20methods%20transform%20multi-view%20image%20features%20into%20a%20Cartesian%0ABird%27s-Eye-View%28BEV%29%20representation%2C%20which%20does%20not%20take%20into%20account%20the%0Anon-uniform%20image%20information%20distribution%20and%20hardly%20exploits%20the%20view%0Asymmetry.%20In%20this%20paper%2C%20in%20order%20to%20adapt%20the%20image%20information%20distribution%0Aand%20preserve%20the%20view%20symmetry%20by%20regular%20convolution%2C%20we%20propose%20to%20employ%20the%0Apolar%20BEV%20representation%20to%20substitute%20the%20Cartesian%20BEV%20representation.%20To%0Aachieve%20this%2C%20we%20elaborately%20tailor%20three%20modules%3A%20a%20polar%20view%20transformer%20to%0Agenerate%20the%20polar%20BEV%20representation%2C%20a%20polar%20temporal%20fusion%20module%20for%0Afusing%20historical%20polar%20BEV%20features%20and%20a%20polar%20detection%20head%20to%20predict%20the%0Apolar-parameterized%20representation%20of%20the%20object.%20In%20addition%2C%20we%20design%20a%202D%0Aauxiliary%20detection%20head%20and%20a%20spatial%20attention%20enhancement%20module%20to%20improve%0Athe%20quality%20of%20feature%20extraction%20in%20perspective%20view%20and%20BEV%2C%20respectively.%0AFinally%2C%20we%20integrate%20the%20above%20improvements%20into%20a%20novel%20multi-view%203D%20object%0Adetector%2C%20PolarBEVDet.%20Experiments%20on%20nuScenes%20show%20that%20PolarBEVDet%20achieves%0Athe%20superior%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Yzichen/PolarBEVDet.git.%28This%20work%20has%20been%20submitted%20to%20the%0AIEEE%20for%20possible%20publication.%20Copyright%20may%20be%20transferred%20without%20notice%2C%0Aafter%20which%20this%20version%20may%20no%20longer%20be%20accessible%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16200v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolarBEVDet%253A%2520Exploring%2520Polar%2520Representation%2520for%2520Multi-View%25203D%2520Object%250A%2520%2520Detection%2520in%2520Bird%2527s-Eye-View%26entry.906535625%3DZichen%2520Yu%2520and%2520Quanli%2520Liu%2520and%2520Wei%2520Wang%2520and%2520Liyong%2520Zhang%2520and%2520Xiaoguang%2520Zhao%26entry.1292438233%3D%2520%2520Recently%252C%2520LSS-based%2520multi-view%25203D%2520object%2520detection%2520provides%2520an%2520economical%2520and%250Adeployment-friendly%2520solution%2520for%2520autonomous%2520driving.%2520However%252C%2520all%2520the%2520existing%250ALSS-based%2520methods%2520transform%2520multi-view%2520image%2520features%2520into%2520a%2520Cartesian%250ABird%2527s-Eye-View%2528BEV%2529%2520representation%252C%2520which%2520does%2520not%2520take%2520into%2520account%2520the%250Anon-uniform%2520image%2520information%2520distribution%2520and%2520hardly%2520exploits%2520the%2520view%250Asymmetry.%2520In%2520this%2520paper%252C%2520in%2520order%2520to%2520adapt%2520the%2520image%2520information%2520distribution%250Aand%2520preserve%2520the%2520view%2520symmetry%2520by%2520regular%2520convolution%252C%2520we%2520propose%2520to%2520employ%2520the%250Apolar%2520BEV%2520representation%2520to%2520substitute%2520the%2520Cartesian%2520BEV%2520representation.%2520To%250Aachieve%2520this%252C%2520we%2520elaborately%2520tailor%2520three%2520modules%253A%2520a%2520polar%2520view%2520transformer%2520to%250Agenerate%2520the%2520polar%2520BEV%2520representation%252C%2520a%2520polar%2520temporal%2520fusion%2520module%2520for%250Afusing%2520historical%2520polar%2520BEV%2520features%2520and%2520a%2520polar%2520detection%2520head%2520to%2520predict%2520the%250Apolar-parameterized%2520representation%2520of%2520the%2520object.%2520In%2520addition%252C%2520we%2520design%2520a%25202D%250Aauxiliary%2520detection%2520head%2520and%2520a%2520spatial%2520attention%2520enhancement%2520module%2520to%2520improve%250Athe%2520quality%2520of%2520feature%2520extraction%2520in%2520perspective%2520view%2520and%2520BEV%252C%2520respectively.%250AFinally%252C%2520we%2520integrate%2520the%2520above%2520improvements%2520into%2520a%2520novel%2520multi-view%25203D%2520object%250Adetector%252C%2520PolarBEVDet.%2520Experiments%2520on%2520nuScenes%2520show%2520that%2520PolarBEVDet%2520achieves%250Athe%2520superior%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Yzichen/PolarBEVDet.git.%2528This%2520work%2520has%2520been%2520submitted%2520to%2520the%250AIEEE%2520for%2520possible%2520publication.%2520Copyright%2520may%2520be%2520transferred%2520without%2520notice%252C%250Aafter%2520which%2520this%2520version%2520may%2520no%2520longer%2520be%2520accessible%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16200v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolarBEVDet%3A%20Exploring%20Polar%20Representation%20for%20Multi-View%203D%20Object%0A%20%20Detection%20in%20Bird%27s-Eye-View&entry.906535625=Zichen%20Yu%20and%20Quanli%20Liu%20and%20Wei%20Wang%20and%20Liyong%20Zhang%20and%20Xiaoguang%20Zhao&entry.1292438233=%20%20Recently%2C%20LSS-based%20multi-view%203D%20object%20detection%20provides%20an%20economical%20and%0Adeployment-friendly%20solution%20for%20autonomous%20driving.%20However%2C%20all%20the%20existing%0ALSS-based%20methods%20transform%20multi-view%20image%20features%20into%20a%20Cartesian%0ABird%27s-Eye-View%28BEV%29%20representation%2C%20which%20does%20not%20take%20into%20account%20the%0Anon-uniform%20image%20information%20distribution%20and%20hardly%20exploits%20the%20view%0Asymmetry.%20In%20this%20paper%2C%20in%20order%20to%20adapt%20the%20image%20information%20distribution%0Aand%20preserve%20the%20view%20symmetry%20by%20regular%20convolution%2C%20we%20propose%20to%20employ%20the%0Apolar%20BEV%20representation%20to%20substitute%20the%20Cartesian%20BEV%20representation.%20To%0Aachieve%20this%2C%20we%20elaborately%20tailor%20three%20modules%3A%20a%20polar%20view%20transformer%20to%0Agenerate%20the%20polar%20BEV%20representation%2C%20a%20polar%20temporal%20fusion%20module%20for%0Afusing%20historical%20polar%20BEV%20features%20and%20a%20polar%20detection%20head%20to%20predict%20the%0Apolar-parameterized%20representation%20of%20the%20object.%20In%20addition%2C%20we%20design%20a%202D%0Aauxiliary%20detection%20head%20and%20a%20spatial%20attention%20enhancement%20module%20to%20improve%0Athe%20quality%20of%20feature%20extraction%20in%20perspective%20view%20and%20BEV%2C%20respectively.%0AFinally%2C%20we%20integrate%20the%20above%20improvements%20into%20a%20novel%20multi-view%203D%20object%0Adetector%2C%20PolarBEVDet.%20Experiments%20on%20nuScenes%20show%20that%20PolarBEVDet%20achieves%0Athe%20superior%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Yzichen/PolarBEVDet.git.%28This%20work%20has%20been%20submitted%20to%20the%0AIEEE%20for%20possible%20publication.%20Copyright%20may%20be%20transferred%20without%20notice%2C%0Aafter%20which%20this%20version%20may%20no%20longer%20be%20accessible%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16200v3&entry.124074799=Read"},
{"title": "ScImage: How Good Are Multimodal Large Language Models at Scientific\n  Text-to-Image Generation?", "author": "Leixin Zhang and Steffen Eger and Yinjie Cheng and Weihe Zhai and Jonas Belouadi and Christoph Leiter and Simone Paolo Ponzetto and Fahimeh Moafian and Zhixue Zhao", "abstract": "  Multimodal large language models (LLMs) have demonstrated impressive\ncapabilities in generating high-quality images from textual instructions.\nHowever, their performance in generating scientific images--a critical\napplication for accelerating scientific progress--remains underexplored. In\nthis work, we address this gap by introducing ScImage, a benchmark designed to\nevaluate the multimodal capabilities of LLMs in generating scientific images\nfrom textual descriptions. ScImage assesses three key dimensions of\nunderstanding: spatial, numeric, and attribute comprehension, as well as their\ncombinations, focusing on the relationships between scientific objects (e.g.,\nsquares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E,\nand StableDiffusion, using two modes of output generation: code-based outputs\n(Python, TikZ) and direct raster image generation. Additionally, we examine\nfour different input languages: English, German, Farsi, and Chinese. Our\nevaluation, conducted with 11 scientists across three criteria (correctness,\nrelevance, and scientific accuracy), reveals that while GPT-4o produces outputs\nof decent quality for simpler prompts involving individual dimensions such as\nspatial, numeric, or attribute understanding in isolation, all models face\nchallenges in this task, especially for more complex prompts.\n", "link": "http://arxiv.org/abs/2412.02368v1", "date": "2024-12-03", "relevancy": 2.8617, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.577}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScImage%3A%20How%20Good%20Are%20Multimodal%20Large%20Language%20Models%20at%20Scientific%0A%20%20Text-to-Image%20Generation%3F&body=Title%3A%20ScImage%3A%20How%20Good%20Are%20Multimodal%20Large%20Language%20Models%20at%20Scientific%0A%20%20Text-to-Image%20Generation%3F%0AAuthor%3A%20Leixin%20Zhang%20and%20Steffen%20Eger%20and%20Yinjie%20Cheng%20and%20Weihe%20Zhai%20and%20Jonas%20Belouadi%20and%20Christoph%20Leiter%20and%20Simone%20Paolo%20Ponzetto%20and%20Fahimeh%20Moafian%20and%20Zhixue%20Zhao%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20in%20generating%20high-quality%20images%20from%20textual%20instructions.%0AHowever%2C%20their%20performance%20in%20generating%20scientific%20images--a%20critical%0Aapplication%20for%20accelerating%20scientific%20progress--remains%20underexplored.%20In%0Athis%20work%2C%20we%20address%20this%20gap%20by%20introducing%20ScImage%2C%20a%20benchmark%20designed%20to%0Aevaluate%20the%20multimodal%20capabilities%20of%20LLMs%20in%20generating%20scientific%20images%0Afrom%20textual%20descriptions.%20ScImage%20assesses%20three%20key%20dimensions%20of%0Aunderstanding%3A%20spatial%2C%20numeric%2C%20and%20attribute%20comprehension%2C%20as%20well%20as%20their%0Acombinations%2C%20focusing%20on%20the%20relationships%20between%20scientific%20objects%20%28e.g.%2C%0Asquares%2C%20circles%29.%20We%20evaluate%20five%20models%2C%20GPT-4o%2C%20Llama%2C%20AutomaTikZ%2C%20Dall-E%2C%0Aand%20StableDiffusion%2C%20using%20two%20modes%20of%20output%20generation%3A%20code-based%20outputs%0A%28Python%2C%20TikZ%29%20and%20direct%20raster%20image%20generation.%20Additionally%2C%20we%20examine%0Afour%20different%20input%20languages%3A%20English%2C%20German%2C%20Farsi%2C%20and%20Chinese.%20Our%0Aevaluation%2C%20conducted%20with%2011%20scientists%20across%20three%20criteria%20%28correctness%2C%0Arelevance%2C%20and%20scientific%20accuracy%29%2C%20reveals%20that%20while%20GPT-4o%20produces%20outputs%0Aof%20decent%20quality%20for%20simpler%20prompts%20involving%20individual%20dimensions%20such%20as%0Aspatial%2C%20numeric%2C%20or%20attribute%20understanding%20in%20isolation%2C%20all%20models%20face%0Achallenges%20in%20this%20task%2C%20especially%20for%20more%20complex%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScImage%253A%2520How%2520Good%2520Are%2520Multimodal%2520Large%2520Language%2520Models%2520at%2520Scientific%250A%2520%2520Text-to-Image%2520Generation%253F%26entry.906535625%3DLeixin%2520Zhang%2520and%2520Steffen%2520Eger%2520and%2520Yinjie%2520Cheng%2520and%2520Weihe%2520Zhai%2520and%2520Jonas%2520Belouadi%2520and%2520Christoph%2520Leiter%2520and%2520Simone%2520Paolo%2520Ponzetto%2520and%2520Fahimeh%2520Moafian%2520and%2520Zhixue%2520Zhao%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%250Acapabilities%2520in%2520generating%2520high-quality%2520images%2520from%2520textual%2520instructions.%250AHowever%252C%2520their%2520performance%2520in%2520generating%2520scientific%2520images--a%2520critical%250Aapplication%2520for%2520accelerating%2520scientific%2520progress--remains%2520underexplored.%2520In%250Athis%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520ScImage%252C%2520a%2520benchmark%2520designed%2520to%250Aevaluate%2520the%2520multimodal%2520capabilities%2520of%2520LLMs%2520in%2520generating%2520scientific%2520images%250Afrom%2520textual%2520descriptions.%2520ScImage%2520assesses%2520three%2520key%2520dimensions%2520of%250Aunderstanding%253A%2520spatial%252C%2520numeric%252C%2520and%2520attribute%2520comprehension%252C%2520as%2520well%2520as%2520their%250Acombinations%252C%2520focusing%2520on%2520the%2520relationships%2520between%2520scientific%2520objects%2520%2528e.g.%252C%250Asquares%252C%2520circles%2529.%2520We%2520evaluate%2520five%2520models%252C%2520GPT-4o%252C%2520Llama%252C%2520AutomaTikZ%252C%2520Dall-E%252C%250Aand%2520StableDiffusion%252C%2520using%2520two%2520modes%2520of%2520output%2520generation%253A%2520code-based%2520outputs%250A%2528Python%252C%2520TikZ%2529%2520and%2520direct%2520raster%2520image%2520generation.%2520Additionally%252C%2520we%2520examine%250Afour%2520different%2520input%2520languages%253A%2520English%252C%2520German%252C%2520Farsi%252C%2520and%2520Chinese.%2520Our%250Aevaluation%252C%2520conducted%2520with%252011%2520scientists%2520across%2520three%2520criteria%2520%2528correctness%252C%250Arelevance%252C%2520and%2520scientific%2520accuracy%2529%252C%2520reveals%2520that%2520while%2520GPT-4o%2520produces%2520outputs%250Aof%2520decent%2520quality%2520for%2520simpler%2520prompts%2520involving%2520individual%2520dimensions%2520such%2520as%250Aspatial%252C%2520numeric%252C%2520or%2520attribute%2520understanding%2520in%2520isolation%252C%2520all%2520models%2520face%250Achallenges%2520in%2520this%2520task%252C%2520especially%2520for%2520more%2520complex%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScImage%3A%20How%20Good%20Are%20Multimodal%20Large%20Language%20Models%20at%20Scientific%0A%20%20Text-to-Image%20Generation%3F&entry.906535625=Leixin%20Zhang%20and%20Steffen%20Eger%20and%20Yinjie%20Cheng%20and%20Weihe%20Zhai%20and%20Jonas%20Belouadi%20and%20Christoph%20Leiter%20and%20Simone%20Paolo%20Ponzetto%20and%20Fahimeh%20Moafian%20and%20Zhixue%20Zhao&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20in%20generating%20high-quality%20images%20from%20textual%20instructions.%0AHowever%2C%20their%20performance%20in%20generating%20scientific%20images--a%20critical%0Aapplication%20for%20accelerating%20scientific%20progress--remains%20underexplored.%20In%0Athis%20work%2C%20we%20address%20this%20gap%20by%20introducing%20ScImage%2C%20a%20benchmark%20designed%20to%0Aevaluate%20the%20multimodal%20capabilities%20of%20LLMs%20in%20generating%20scientific%20images%0Afrom%20textual%20descriptions.%20ScImage%20assesses%20three%20key%20dimensions%20of%0Aunderstanding%3A%20spatial%2C%20numeric%2C%20and%20attribute%20comprehension%2C%20as%20well%20as%20their%0Acombinations%2C%20focusing%20on%20the%20relationships%20between%20scientific%20objects%20%28e.g.%2C%0Asquares%2C%20circles%29.%20We%20evaluate%20five%20models%2C%20GPT-4o%2C%20Llama%2C%20AutomaTikZ%2C%20Dall-E%2C%0Aand%20StableDiffusion%2C%20using%20two%20modes%20of%20output%20generation%3A%20code-based%20outputs%0A%28Python%2C%20TikZ%29%20and%20direct%20raster%20image%20generation.%20Additionally%2C%20we%20examine%0Afour%20different%20input%20languages%3A%20English%2C%20German%2C%20Farsi%2C%20and%20Chinese.%20Our%0Aevaluation%2C%20conducted%20with%2011%20scientists%20across%20three%20criteria%20%28correctness%2C%0Arelevance%2C%20and%20scientific%20accuracy%29%2C%20reveals%20that%20while%20GPT-4o%20produces%20outputs%0Aof%20decent%20quality%20for%20simpler%20prompts%20involving%20individual%20dimensions%20such%20as%0Aspatial%2C%20numeric%2C%20or%20attribute%20understanding%20in%20isolation%2C%20all%20models%20face%0Achallenges%20in%20this%20task%2C%20especially%20for%20more%20complex%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02368v1&entry.124074799=Read"},
{"title": "MedTet: An Online Motion Model for 4D Heart Reconstruction", "author": "Yihong Chen and Jiancheng Yang and Deniz Sayin Mercadier and Hieu Le and Pascal Fua", "abstract": "  We present a novel approach to reconstruction of 3D cardiac motion from\nsparse intraoperative data. While existing methods can accurately reconstruct\n3D organ geometries from full 3D volumetric imaging, they cannot be used during\nsurgical interventions where usually limited observed data, such as a few 2D\nframes or 1D signals, is available in real-time. We propose a versatile\nframework for reconstructing 3D motion from such partial data. It discretizes\nthe 3D space into a deformable tetrahedral grid with signed distance values,\nproviding implicit unlimited resolution while maintaining explicit control over\nmotion dynamics. Given an initial 3D model reconstructed from pre-operative\nfull volumetric data, our system, equipped with an universal observation\nencoder, can reconstruct coherent 3D cardiac motion from full 3D volumes, a few\n2D MRI slices or even 1D signals. Extensive experiments on cardiac intervention\nscenarios demonstrate our ability to generate plausible and anatomically\nconsistent 3D motion reconstructions from various sparse real-time\nobservations, highlighting its potential for multimodal cardiac imaging. Our\ncode and model will be made available at https://github.com/Scalsol/MedTet.\n", "link": "http://arxiv.org/abs/2412.02589v1", "date": "2024-12-03", "relevancy": 2.8418, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction&body=Title%3A%20MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction%0AAuthor%3A%20Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20reconstruction%20of%203D%20cardiac%20motion%20from%0Asparse%20intraoperative%20data.%20While%20existing%20methods%20can%20accurately%20reconstruct%0A3D%20organ%20geometries%20from%20full%203D%20volumetric%20imaging%2C%20they%20cannot%20be%20used%20during%0Asurgical%20interventions%20where%20usually%20limited%20observed%20data%2C%20such%20as%20a%20few%202D%0Aframes%20or%201D%20signals%2C%20is%20available%20in%20real-time.%20We%20propose%20a%20versatile%0Aframework%20for%20reconstructing%203D%20motion%20from%20such%20partial%20data.%20It%20discretizes%0Athe%203D%20space%20into%20a%20deformable%20tetrahedral%20grid%20with%20signed%20distance%20values%2C%0Aproviding%20implicit%20unlimited%20resolution%20while%20maintaining%20explicit%20control%20over%0Amotion%20dynamics.%20Given%20an%20initial%203D%20model%20reconstructed%20from%20pre-operative%0Afull%20volumetric%20data%2C%20our%20system%2C%20equipped%20with%20an%20universal%20observation%0Aencoder%2C%20can%20reconstruct%20coherent%203D%20cardiac%20motion%20from%20full%203D%20volumes%2C%20a%20few%0A2D%20MRI%20slices%20or%20even%201D%20signals.%20Extensive%20experiments%20on%20cardiac%20intervention%0Ascenarios%20demonstrate%20our%20ability%20to%20generate%20plausible%20and%20anatomically%0Aconsistent%203D%20motion%20reconstructions%20from%20various%20sparse%20real-time%0Aobservations%2C%20highlighting%20its%20potential%20for%20multimodal%20cardiac%20imaging.%20Our%0Acode%20and%20model%20will%20be%20made%20available%20at%20https%3A//github.com/Scalsol/MedTet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedTet%253A%2520An%2520Online%2520Motion%2520Model%2520for%25204D%2520Heart%2520Reconstruction%26entry.906535625%3DYihong%2520Chen%2520and%2520Jiancheng%2520Yang%2520and%2520Deniz%2520Sayin%2520Mercadier%2520and%2520Hieu%2520Le%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520reconstruction%2520of%25203D%2520cardiac%2520motion%2520from%250Asparse%2520intraoperative%2520data.%2520While%2520existing%2520methods%2520can%2520accurately%2520reconstruct%250A3D%2520organ%2520geometries%2520from%2520full%25203D%2520volumetric%2520imaging%252C%2520they%2520cannot%2520be%2520used%2520during%250Asurgical%2520interventions%2520where%2520usually%2520limited%2520observed%2520data%252C%2520such%2520as%2520a%2520few%25202D%250Aframes%2520or%25201D%2520signals%252C%2520is%2520available%2520in%2520real-time.%2520We%2520propose%2520a%2520versatile%250Aframework%2520for%2520reconstructing%25203D%2520motion%2520from%2520such%2520partial%2520data.%2520It%2520discretizes%250Athe%25203D%2520space%2520into%2520a%2520deformable%2520tetrahedral%2520grid%2520with%2520signed%2520distance%2520values%252C%250Aproviding%2520implicit%2520unlimited%2520resolution%2520while%2520maintaining%2520explicit%2520control%2520over%250Amotion%2520dynamics.%2520Given%2520an%2520initial%25203D%2520model%2520reconstructed%2520from%2520pre-operative%250Afull%2520volumetric%2520data%252C%2520our%2520system%252C%2520equipped%2520with%2520an%2520universal%2520observation%250Aencoder%252C%2520can%2520reconstruct%2520coherent%25203D%2520cardiac%2520motion%2520from%2520full%25203D%2520volumes%252C%2520a%2520few%250A2D%2520MRI%2520slices%2520or%2520even%25201D%2520signals.%2520Extensive%2520experiments%2520on%2520cardiac%2520intervention%250Ascenarios%2520demonstrate%2520our%2520ability%2520to%2520generate%2520plausible%2520and%2520anatomically%250Aconsistent%25203D%2520motion%2520reconstructions%2520from%2520various%2520sparse%2520real-time%250Aobservations%252C%2520highlighting%2520its%2520potential%2520for%2520multimodal%2520cardiac%2520imaging.%2520Our%250Acode%2520and%2520model%2520will%2520be%2520made%2520available%2520at%2520https%253A//github.com/Scalsol/MedTet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction&entry.906535625=Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Pascal%20Fua&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20reconstruction%20of%203D%20cardiac%20motion%20from%0Asparse%20intraoperative%20data.%20While%20existing%20methods%20can%20accurately%20reconstruct%0A3D%20organ%20geometries%20from%20full%203D%20volumetric%20imaging%2C%20they%20cannot%20be%20used%20during%0Asurgical%20interventions%20where%20usually%20limited%20observed%20data%2C%20such%20as%20a%20few%202D%0Aframes%20or%201D%20signals%2C%20is%20available%20in%20real-time.%20We%20propose%20a%20versatile%0Aframework%20for%20reconstructing%203D%20motion%20from%20such%20partial%20data.%20It%20discretizes%0Athe%203D%20space%20into%20a%20deformable%20tetrahedral%20grid%20with%20signed%20distance%20values%2C%0Aproviding%20implicit%20unlimited%20resolution%20while%20maintaining%20explicit%20control%20over%0Amotion%20dynamics.%20Given%20an%20initial%203D%20model%20reconstructed%20from%20pre-operative%0Afull%20volumetric%20data%2C%20our%20system%2C%20equipped%20with%20an%20universal%20observation%0Aencoder%2C%20can%20reconstruct%20coherent%203D%20cardiac%20motion%20from%20full%203D%20volumes%2C%20a%20few%0A2D%20MRI%20slices%20or%20even%201D%20signals.%20Extensive%20experiments%20on%20cardiac%20intervention%0Ascenarios%20demonstrate%20our%20ability%20to%20generate%20plausible%20and%20anatomically%0Aconsistent%203D%20motion%20reconstructions%20from%20various%20sparse%20real-time%0Aobservations%2C%20highlighting%20its%20potential%20for%20multimodal%20cardiac%20imaging.%20Our%0Acode%20and%20model%20will%20be%20made%20available%20at%20https%3A//github.com/Scalsol/MedTet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02589v1&entry.124074799=Read"},
{"title": "MedTet: An Online Motion Model for 4D Heart Reconstruction", "author": "Yihong Chen and Jiancheng Yang and Deniz Sayin Mercadier and Hieu Le and Pascal Fua", "abstract": "  We present a novel approach to reconstruction of 3D cardiac motion from\nsparse intraoperative data. While existing methods can accurately reconstruct\n3D organ geometries from full 3D volumetric imaging, they cannot be used during\nsurgical interventions where usually limited observed data, such as a few 2D\nframes or 1D signals, is available in real-time. We propose a versatile\nframework for reconstructing 3D motion from such partial data. It discretizes\nthe 3D space into a deformable tetrahedral grid with signed distance values,\nproviding implicit unlimited resolution while maintaining explicit control over\nmotion dynamics. Given an initial 3D model reconstructed from pre-operative\nfull volumetric data, our system, equipped with an universal observation\nencoder, can reconstruct coherent 3D cardiac motion from full 3D volumes, a few\n2D MRI slices or even 1D signals. Extensive experiments on cardiac intervention\nscenarios demonstrate our ability to generate plausible and anatomically\nconsistent 3D motion reconstructions from various sparse real-time\nobservations, highlighting its potential for multimodal cardiac imaging. Our\ncode and model will be made available at https://github.com/Scalsol/MedTet.\n", "link": "http://arxiv.org/abs/2412.02589v1", "date": "2024-12-03", "relevancy": 2.8418, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction&body=Title%3A%20MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction%0AAuthor%3A%20Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20reconstruction%20of%203D%20cardiac%20motion%20from%0Asparse%20intraoperative%20data.%20While%20existing%20methods%20can%20accurately%20reconstruct%0A3D%20organ%20geometries%20from%20full%203D%20volumetric%20imaging%2C%20they%20cannot%20be%20used%20during%0Asurgical%20interventions%20where%20usually%20limited%20observed%20data%2C%20such%20as%20a%20few%202D%0Aframes%20or%201D%20signals%2C%20is%20available%20in%20real-time.%20We%20propose%20a%20versatile%0Aframework%20for%20reconstructing%203D%20motion%20from%20such%20partial%20data.%20It%20discretizes%0Athe%203D%20space%20into%20a%20deformable%20tetrahedral%20grid%20with%20signed%20distance%20values%2C%0Aproviding%20implicit%20unlimited%20resolution%20while%20maintaining%20explicit%20control%20over%0Amotion%20dynamics.%20Given%20an%20initial%203D%20model%20reconstructed%20from%20pre-operative%0Afull%20volumetric%20data%2C%20our%20system%2C%20equipped%20with%20an%20universal%20observation%0Aencoder%2C%20can%20reconstruct%20coherent%203D%20cardiac%20motion%20from%20full%203D%20volumes%2C%20a%20few%0A2D%20MRI%20slices%20or%20even%201D%20signals.%20Extensive%20experiments%20on%20cardiac%20intervention%0Ascenarios%20demonstrate%20our%20ability%20to%20generate%20plausible%20and%20anatomically%0Aconsistent%203D%20motion%20reconstructions%20from%20various%20sparse%20real-time%0Aobservations%2C%20highlighting%20its%20potential%20for%20multimodal%20cardiac%20imaging.%20Our%0Acode%20and%20model%20will%20be%20made%20available%20at%20https%3A//github.com/Scalsol/MedTet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedTet%253A%2520An%2520Online%2520Motion%2520Model%2520for%25204D%2520Heart%2520Reconstruction%26entry.906535625%3DYihong%2520Chen%2520and%2520Jiancheng%2520Yang%2520and%2520Deniz%2520Sayin%2520Mercadier%2520and%2520Hieu%2520Le%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520reconstruction%2520of%25203D%2520cardiac%2520motion%2520from%250Asparse%2520intraoperative%2520data.%2520While%2520existing%2520methods%2520can%2520accurately%2520reconstruct%250A3D%2520organ%2520geometries%2520from%2520full%25203D%2520volumetric%2520imaging%252C%2520they%2520cannot%2520be%2520used%2520during%250Asurgical%2520interventions%2520where%2520usually%2520limited%2520observed%2520data%252C%2520such%2520as%2520a%2520few%25202D%250Aframes%2520or%25201D%2520signals%252C%2520is%2520available%2520in%2520real-time.%2520We%2520propose%2520a%2520versatile%250Aframework%2520for%2520reconstructing%25203D%2520motion%2520from%2520such%2520partial%2520data.%2520It%2520discretizes%250Athe%25203D%2520space%2520into%2520a%2520deformable%2520tetrahedral%2520grid%2520with%2520signed%2520distance%2520values%252C%250Aproviding%2520implicit%2520unlimited%2520resolution%2520while%2520maintaining%2520explicit%2520control%2520over%250Amotion%2520dynamics.%2520Given%2520an%2520initial%25203D%2520model%2520reconstructed%2520from%2520pre-operative%250Afull%2520volumetric%2520data%252C%2520our%2520system%252C%2520equipped%2520with%2520an%2520universal%2520observation%250Aencoder%252C%2520can%2520reconstruct%2520coherent%25203D%2520cardiac%2520motion%2520from%2520full%25203D%2520volumes%252C%2520a%2520few%250A2D%2520MRI%2520slices%2520or%2520even%25201D%2520signals.%2520Extensive%2520experiments%2520on%2520cardiac%2520intervention%250Ascenarios%2520demonstrate%2520our%2520ability%2520to%2520generate%2520plausible%2520and%2520anatomically%250Aconsistent%25203D%2520motion%2520reconstructions%2520from%2520various%2520sparse%2520real-time%250Aobservations%252C%2520highlighting%2520its%2520potential%2520for%2520multimodal%2520cardiac%2520imaging.%2520Our%250Acode%2520and%2520model%2520will%2520be%2520made%2520available%2520at%2520https%253A//github.com/Scalsol/MedTet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction&entry.906535625=Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Pascal%20Fua&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20reconstruction%20of%203D%20cardiac%20motion%20from%0Asparse%20intraoperative%20data.%20While%20existing%20methods%20can%20accurately%20reconstruct%0A3D%20organ%20geometries%20from%20full%203D%20volumetric%20imaging%2C%20they%20cannot%20be%20used%20during%0Asurgical%20interventions%20where%20usually%20limited%20observed%20data%2C%20such%20as%20a%20few%202D%0Aframes%20or%201D%20signals%2C%20is%20available%20in%20real-time.%20We%20propose%20a%20versatile%0Aframework%20for%20reconstructing%203D%20motion%20from%20such%20partial%20data.%20It%20discretizes%0Athe%203D%20space%20into%20a%20deformable%20tetrahedral%20grid%20with%20signed%20distance%20values%2C%0Aproviding%20implicit%20unlimited%20resolution%20while%20maintaining%20explicit%20control%20over%0Amotion%20dynamics.%20Given%20an%20initial%203D%20model%20reconstructed%20from%20pre-operative%0Afull%20volumetric%20data%2C%20our%20system%2C%20equipped%20with%20an%20universal%20observation%0Aencoder%2C%20can%20reconstruct%20coherent%203D%20cardiac%20motion%20from%20full%203D%20volumes%2C%20a%20few%0A2D%20MRI%20slices%20or%20even%201D%20signals.%20Extensive%20experiments%20on%20cardiac%20intervention%0Ascenarios%20demonstrate%20our%20ability%20to%20generate%20plausible%20and%20anatomically%0Aconsistent%203D%20motion%20reconstructions%20from%20various%20sparse%20real-time%0Aobservations%2C%20highlighting%20its%20potential%20for%20multimodal%20cardiac%20imaging.%20Our%0Acode%20and%20model%20will%20be%20made%20available%20at%20https%3A//github.com/Scalsol/MedTet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02589v1&entry.124074799=Read"},
{"title": "MedTet: An Online Motion Model for 4D Heart Reconstruction", "author": "Yihong Chen and Jiancheng Yang and Deniz Sayin Mercadier and Hieu Le and Pascal Fua", "abstract": "  We present a novel approach to reconstruction of 3D cardiac motion from\nsparse intraoperative data. While existing methods can accurately reconstruct\n3D organ geometries from full 3D volumetric imaging, they cannot be used during\nsurgical interventions where usually limited observed data, such as a few 2D\nframes or 1D signals, is available in real-time. We propose a versatile\nframework for reconstructing 3D motion from such partial data. It discretizes\nthe 3D space into a deformable tetrahedral grid with signed distance values,\nproviding implicit unlimited resolution while maintaining explicit control over\nmotion dynamics. Given an initial 3D model reconstructed from pre-operative\nfull volumetric data, our system, equipped with an universal observation\nencoder, can reconstruct coherent 3D cardiac motion from full 3D volumes, a few\n2D MRI slices or even 1D signals. Extensive experiments on cardiac intervention\nscenarios demonstrate our ability to generate plausible and anatomically\nconsistent 3D motion reconstructions from various sparse real-time\nobservations, highlighting its potential for multimodal cardiac imaging. Our\ncode and model will be made available at https://github.com/Scalsol/MedTet.\n", "link": "http://arxiv.org/abs/2412.02589v1", "date": "2024-12-03", "relevancy": 2.8418, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction&body=Title%3A%20MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction%0AAuthor%3A%20Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20reconstruction%20of%203D%20cardiac%20motion%20from%0Asparse%20intraoperative%20data.%20While%20existing%20methods%20can%20accurately%20reconstruct%0A3D%20organ%20geometries%20from%20full%203D%20volumetric%20imaging%2C%20they%20cannot%20be%20used%20during%0Asurgical%20interventions%20where%20usually%20limited%20observed%20data%2C%20such%20as%20a%20few%202D%0Aframes%20or%201D%20signals%2C%20is%20available%20in%20real-time.%20We%20propose%20a%20versatile%0Aframework%20for%20reconstructing%203D%20motion%20from%20such%20partial%20data.%20It%20discretizes%0Athe%203D%20space%20into%20a%20deformable%20tetrahedral%20grid%20with%20signed%20distance%20values%2C%0Aproviding%20implicit%20unlimited%20resolution%20while%20maintaining%20explicit%20control%20over%0Amotion%20dynamics.%20Given%20an%20initial%203D%20model%20reconstructed%20from%20pre-operative%0Afull%20volumetric%20data%2C%20our%20system%2C%20equipped%20with%20an%20universal%20observation%0Aencoder%2C%20can%20reconstruct%20coherent%203D%20cardiac%20motion%20from%20full%203D%20volumes%2C%20a%20few%0A2D%20MRI%20slices%20or%20even%201D%20signals.%20Extensive%20experiments%20on%20cardiac%20intervention%0Ascenarios%20demonstrate%20our%20ability%20to%20generate%20plausible%20and%20anatomically%0Aconsistent%203D%20motion%20reconstructions%20from%20various%20sparse%20real-time%0Aobservations%2C%20highlighting%20its%20potential%20for%20multimodal%20cardiac%20imaging.%20Our%0Acode%20and%20model%20will%20be%20made%20available%20at%20https%3A//github.com/Scalsol/MedTet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedTet%253A%2520An%2520Online%2520Motion%2520Model%2520for%25204D%2520Heart%2520Reconstruction%26entry.906535625%3DYihong%2520Chen%2520and%2520Jiancheng%2520Yang%2520and%2520Deniz%2520Sayin%2520Mercadier%2520and%2520Hieu%2520Le%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520reconstruction%2520of%25203D%2520cardiac%2520motion%2520from%250Asparse%2520intraoperative%2520data.%2520While%2520existing%2520methods%2520can%2520accurately%2520reconstruct%250A3D%2520organ%2520geometries%2520from%2520full%25203D%2520volumetric%2520imaging%252C%2520they%2520cannot%2520be%2520used%2520during%250Asurgical%2520interventions%2520where%2520usually%2520limited%2520observed%2520data%252C%2520such%2520as%2520a%2520few%25202D%250Aframes%2520or%25201D%2520signals%252C%2520is%2520available%2520in%2520real-time.%2520We%2520propose%2520a%2520versatile%250Aframework%2520for%2520reconstructing%25203D%2520motion%2520from%2520such%2520partial%2520data.%2520It%2520discretizes%250Athe%25203D%2520space%2520into%2520a%2520deformable%2520tetrahedral%2520grid%2520with%2520signed%2520distance%2520values%252C%250Aproviding%2520implicit%2520unlimited%2520resolution%2520while%2520maintaining%2520explicit%2520control%2520over%250Amotion%2520dynamics.%2520Given%2520an%2520initial%25203D%2520model%2520reconstructed%2520from%2520pre-operative%250Afull%2520volumetric%2520data%252C%2520our%2520system%252C%2520equipped%2520with%2520an%2520universal%2520observation%250Aencoder%252C%2520can%2520reconstruct%2520coherent%25203D%2520cardiac%2520motion%2520from%2520full%25203D%2520volumes%252C%2520a%2520few%250A2D%2520MRI%2520slices%2520or%2520even%25201D%2520signals.%2520Extensive%2520experiments%2520on%2520cardiac%2520intervention%250Ascenarios%2520demonstrate%2520our%2520ability%2520to%2520generate%2520plausible%2520and%2520anatomically%250Aconsistent%25203D%2520motion%2520reconstructions%2520from%2520various%2520sparse%2520real-time%250Aobservations%252C%2520highlighting%2520its%2520potential%2520for%2520multimodal%2520cardiac%2520imaging.%2520Our%250Acode%2520and%2520model%2520will%2520be%2520made%2520available%2520at%2520https%253A//github.com/Scalsol/MedTet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedTet%3A%20An%20Online%20Motion%20Model%20for%204D%20Heart%20Reconstruction&entry.906535625=Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Pascal%20Fua&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20reconstruction%20of%203D%20cardiac%20motion%20from%0Asparse%20intraoperative%20data.%20While%20existing%20methods%20can%20accurately%20reconstruct%0A3D%20organ%20geometries%20from%20full%203D%20volumetric%20imaging%2C%20they%20cannot%20be%20used%20during%0Asurgical%20interventions%20where%20usually%20limited%20observed%20data%2C%20such%20as%20a%20few%202D%0Aframes%20or%201D%20signals%2C%20is%20available%20in%20real-time.%20We%20propose%20a%20versatile%0Aframework%20for%20reconstructing%203D%20motion%20from%20such%20partial%20data.%20It%20discretizes%0Athe%203D%20space%20into%20a%20deformable%20tetrahedral%20grid%20with%20signed%20distance%20values%2C%0Aproviding%20implicit%20unlimited%20resolution%20while%20maintaining%20explicit%20control%20over%0Amotion%20dynamics.%20Given%20an%20initial%203D%20model%20reconstructed%20from%20pre-operative%0Afull%20volumetric%20data%2C%20our%20system%2C%20equipped%20with%20an%20universal%20observation%0Aencoder%2C%20can%20reconstruct%20coherent%203D%20cardiac%20motion%20from%20full%203D%20volumes%2C%20a%20few%0A2D%20MRI%20slices%20or%20even%201D%20signals.%20Extensive%20experiments%20on%20cardiac%20intervention%0Ascenarios%20demonstrate%20our%20ability%20to%20generate%20plausible%20and%20anatomically%0Aconsistent%203D%20motion%20reconstructions%20from%20various%20sparse%20real-time%0Aobservations%2C%20highlighting%20its%20potential%20for%20multimodal%20cardiac%20imaging.%20Our%0Acode%20and%20model%20will%20be%20made%20available%20at%20https%3A//github.com/Scalsol/MedTet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02589v1&entry.124074799=Read"},
{"title": "Dual Exposure Stereo for Extended Dynamic Range 3D Imaging", "author": "Juhyung Choi and Jinnyeong Kim and Seokjun Choi and Jinwoo Lee and Samuel Brucker and Mario Bijelic and Felix Heide and Seung-Hwan Baek", "abstract": "  Achieving robust stereo 3D imaging under diverse illumination conditions is\nan important however challenging task, due to the limited dynamic ranges (DRs)\nof cameras, which are significantly smaller than real world DR. As a result,\nthe accuracy of existing stereo depth estimation methods is often compromised\nby under- or over-exposed images. Here, we introduce dual-exposure stereo for\nextended dynamic range 3D imaging. We develop automatic dual-exposure control\nmethod that adjusts the dual exposures, diverging them when the scene DR\nexceeds the camera DR, thereby providing information about broader DR. From the\ncaptured dual-exposure stereo images, we estimate depth using motion-aware\ndual-exposure stereo network. To validate our method, we develop a robot-vision\nsystem, collect stereo video datasets, and generate a synthetic dataset. Our\nmethod outperforms other exposure control methods.\n", "link": "http://arxiv.org/abs/2412.02351v1", "date": "2024-12-03", "relevancy": 2.8085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5619}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5619}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Exposure%20Stereo%20for%20Extended%20Dynamic%20Range%203D%20Imaging&body=Title%3A%20Dual%20Exposure%20Stereo%20for%20Extended%20Dynamic%20Range%203D%20Imaging%0AAuthor%3A%20Juhyung%20Choi%20and%20Jinnyeong%20Kim%20and%20Seokjun%20Choi%20and%20Jinwoo%20Lee%20and%20Samuel%20Brucker%20and%20Mario%20Bijelic%20and%20Felix%20Heide%20and%20Seung-Hwan%20Baek%0AAbstract%3A%20%20%20Achieving%20robust%20stereo%203D%20imaging%20under%20diverse%20illumination%20conditions%20is%0Aan%20important%20however%20challenging%20task%2C%20due%20to%20the%20limited%20dynamic%20ranges%20%28DRs%29%0Aof%20cameras%2C%20which%20are%20significantly%20smaller%20than%20real%20world%20DR.%20As%20a%20result%2C%0Athe%20accuracy%20of%20existing%20stereo%20depth%20estimation%20methods%20is%20often%20compromised%0Aby%20under-%20or%20over-exposed%20images.%20Here%2C%20we%20introduce%20dual-exposure%20stereo%20for%0Aextended%20dynamic%20range%203D%20imaging.%20We%20develop%20automatic%20dual-exposure%20control%0Amethod%20that%20adjusts%20the%20dual%20exposures%2C%20diverging%20them%20when%20the%20scene%20DR%0Aexceeds%20the%20camera%20DR%2C%20thereby%20providing%20information%20about%20broader%20DR.%20From%20the%0Acaptured%20dual-exposure%20stereo%20images%2C%20we%20estimate%20depth%20using%20motion-aware%0Adual-exposure%20stereo%20network.%20To%20validate%20our%20method%2C%20we%20develop%20a%20robot-vision%0Asystem%2C%20collect%20stereo%20video%20datasets%2C%20and%20generate%20a%20synthetic%20dataset.%20Our%0Amethod%20outperforms%20other%20exposure%20control%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Exposure%2520Stereo%2520for%2520Extended%2520Dynamic%2520Range%25203D%2520Imaging%26entry.906535625%3DJuhyung%2520Choi%2520and%2520Jinnyeong%2520Kim%2520and%2520Seokjun%2520Choi%2520and%2520Jinwoo%2520Lee%2520and%2520Samuel%2520Brucker%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%2520and%2520Seung-Hwan%2520Baek%26entry.1292438233%3D%2520%2520Achieving%2520robust%2520stereo%25203D%2520imaging%2520under%2520diverse%2520illumination%2520conditions%2520is%250Aan%2520important%2520however%2520challenging%2520task%252C%2520due%2520to%2520the%2520limited%2520dynamic%2520ranges%2520%2528DRs%2529%250Aof%2520cameras%252C%2520which%2520are%2520significantly%2520smaller%2520than%2520real%2520world%2520DR.%2520As%2520a%2520result%252C%250Athe%2520accuracy%2520of%2520existing%2520stereo%2520depth%2520estimation%2520methods%2520is%2520often%2520compromised%250Aby%2520under-%2520or%2520over-exposed%2520images.%2520Here%252C%2520we%2520introduce%2520dual-exposure%2520stereo%2520for%250Aextended%2520dynamic%2520range%25203D%2520imaging.%2520We%2520develop%2520automatic%2520dual-exposure%2520control%250Amethod%2520that%2520adjusts%2520the%2520dual%2520exposures%252C%2520diverging%2520them%2520when%2520the%2520scene%2520DR%250Aexceeds%2520the%2520camera%2520DR%252C%2520thereby%2520providing%2520information%2520about%2520broader%2520DR.%2520From%2520the%250Acaptured%2520dual-exposure%2520stereo%2520images%252C%2520we%2520estimate%2520depth%2520using%2520motion-aware%250Adual-exposure%2520stereo%2520network.%2520To%2520validate%2520our%2520method%252C%2520we%2520develop%2520a%2520robot-vision%250Asystem%252C%2520collect%2520stereo%2520video%2520datasets%252C%2520and%2520generate%2520a%2520synthetic%2520dataset.%2520Our%250Amethod%2520outperforms%2520other%2520exposure%2520control%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Exposure%20Stereo%20for%20Extended%20Dynamic%20Range%203D%20Imaging&entry.906535625=Juhyung%20Choi%20and%20Jinnyeong%20Kim%20and%20Seokjun%20Choi%20and%20Jinwoo%20Lee%20and%20Samuel%20Brucker%20and%20Mario%20Bijelic%20and%20Felix%20Heide%20and%20Seung-Hwan%20Baek&entry.1292438233=%20%20Achieving%20robust%20stereo%203D%20imaging%20under%20diverse%20illumination%20conditions%20is%0Aan%20important%20however%20challenging%20task%2C%20due%20to%20the%20limited%20dynamic%20ranges%20%28DRs%29%0Aof%20cameras%2C%20which%20are%20significantly%20smaller%20than%20real%20world%20DR.%20As%20a%20result%2C%0Athe%20accuracy%20of%20existing%20stereo%20depth%20estimation%20methods%20is%20often%20compromised%0Aby%20under-%20or%20over-exposed%20images.%20Here%2C%20we%20introduce%20dual-exposure%20stereo%20for%0Aextended%20dynamic%20range%203D%20imaging.%20We%20develop%20automatic%20dual-exposure%20control%0Amethod%20that%20adjusts%20the%20dual%20exposures%2C%20diverging%20them%20when%20the%20scene%20DR%0Aexceeds%20the%20camera%20DR%2C%20thereby%20providing%20information%20about%20broader%20DR.%20From%20the%0Acaptured%20dual-exposure%20stereo%20images%2C%20we%20estimate%20depth%20using%20motion-aware%0Adual-exposure%20stereo%20network.%20To%20validate%20our%20method%2C%20we%20develop%20a%20robot-vision%0Asystem%2C%20collect%20stereo%20video%20datasets%2C%20and%20generate%20a%20synthetic%20dataset.%20Our%0Amethod%20outperforms%20other%20exposure%20control%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02351v1&entry.124074799=Read"},
{"title": "DPE-Net: Dual-Parallel Encoder Based Network for Semantic Segmentation\n  of Polyps", "author": "Malik Abdul Manan and Feng Jinchao and Shahzad Ahmed and Abdul Raheem", "abstract": "  In medical imaging, efficient segmentation of colon polyps plays a pivotal\nrole in minimally invasive solutions for colorectal cancer. This study\nintroduces a novel approach employing two parallel encoder branches within a\nnetwork for polyp segmentation. One branch of the encoder incorporates the dual\nconvolution blocks that have the capability to maintain feature information\nover increased depths, and the other block embraces the single convolution\nblock with the addition of the previous layer's feature, offering diversity in\nfeature extraction within the encoder, combining them before transpose layers\nwith a depth-wise concatenation operation. Our model demonstrated superior\nperformance, surpassing several established deep-learning architectures on the\nKvasir and CVC-ClinicDB datasets, achieved a Dice score of 0.919, a mIoU of\n0.866 for the Kvasir dataset, and a Dice score of 0.931 and a mIoU of 0.891 for\nthe CVC-ClinicDB. The visual and quantitative results highlight the efficacy of\nour model, potentially setting a new model in medical image segmentation.\n", "link": "http://arxiv.org/abs/2412.00888v2", "date": "2024-12-03", "relevancy": 2.744, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPE-Net%3A%20Dual-Parallel%20Encoder%20Based%20Network%20for%20Semantic%20Segmentation%0A%20%20of%20Polyps&body=Title%3A%20DPE-Net%3A%20Dual-Parallel%20Encoder%20Based%20Network%20for%20Semantic%20Segmentation%0A%20%20of%20Polyps%0AAuthor%3A%20Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Shahzad%20Ahmed%20and%20Abdul%20Raheem%0AAbstract%3A%20%20%20In%20medical%20imaging%2C%20efficient%20segmentation%20of%20colon%20polyps%20plays%20a%20pivotal%0Arole%20in%20minimally%20invasive%20solutions%20for%20colorectal%20cancer.%20This%20study%0Aintroduces%20a%20novel%20approach%20employing%20two%20parallel%20encoder%20branches%20within%20a%0Anetwork%20for%20polyp%20segmentation.%20One%20branch%20of%20the%20encoder%20incorporates%20the%20dual%0Aconvolution%20blocks%20that%20have%20the%20capability%20to%20maintain%20feature%20information%0Aover%20increased%20depths%2C%20and%20the%20other%20block%20embraces%20the%20single%20convolution%0Ablock%20with%20the%20addition%20of%20the%20previous%20layer%27s%20feature%2C%20offering%20diversity%20in%0Afeature%20extraction%20within%20the%20encoder%2C%20combining%20them%20before%20transpose%20layers%0Awith%20a%20depth-wise%20concatenation%20operation.%20Our%20model%20demonstrated%20superior%0Aperformance%2C%20surpassing%20several%20established%20deep-learning%20architectures%20on%20the%0AKvasir%20and%20CVC-ClinicDB%20datasets%2C%20achieved%20a%20Dice%20score%20of%200.919%2C%20a%20mIoU%20of%0A0.866%20for%20the%20Kvasir%20dataset%2C%20and%20a%20Dice%20score%20of%200.931%20and%20a%20mIoU%20of%200.891%20for%0Athe%20CVC-ClinicDB.%20The%20visual%20and%20quantitative%20results%20highlight%20the%20efficacy%20of%0Aour%20model%2C%20potentially%20setting%20a%20new%20model%20in%20medical%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPE-Net%253A%2520Dual-Parallel%2520Encoder%2520Based%2520Network%2520for%2520Semantic%2520Segmentation%250A%2520%2520of%2520Polyps%26entry.906535625%3DMalik%2520Abdul%2520Manan%2520and%2520Feng%2520Jinchao%2520and%2520Shahzad%2520Ahmed%2520and%2520Abdul%2520Raheem%26entry.1292438233%3D%2520%2520In%2520medical%2520imaging%252C%2520efficient%2520segmentation%2520of%2520colon%2520polyps%2520plays%2520a%2520pivotal%250Arole%2520in%2520minimally%2520invasive%2520solutions%2520for%2520colorectal%2520cancer.%2520This%2520study%250Aintroduces%2520a%2520novel%2520approach%2520employing%2520two%2520parallel%2520encoder%2520branches%2520within%2520a%250Anetwork%2520for%2520polyp%2520segmentation.%2520One%2520branch%2520of%2520the%2520encoder%2520incorporates%2520the%2520dual%250Aconvolution%2520blocks%2520that%2520have%2520the%2520capability%2520to%2520maintain%2520feature%2520information%250Aover%2520increased%2520depths%252C%2520and%2520the%2520other%2520block%2520embraces%2520the%2520single%2520convolution%250Ablock%2520with%2520the%2520addition%2520of%2520the%2520previous%2520layer%2527s%2520feature%252C%2520offering%2520diversity%2520in%250Afeature%2520extraction%2520within%2520the%2520encoder%252C%2520combining%2520them%2520before%2520transpose%2520layers%250Awith%2520a%2520depth-wise%2520concatenation%2520operation.%2520Our%2520model%2520demonstrated%2520superior%250Aperformance%252C%2520surpassing%2520several%2520established%2520deep-learning%2520architectures%2520on%2520the%250AKvasir%2520and%2520CVC-ClinicDB%2520datasets%252C%2520achieved%2520a%2520Dice%2520score%2520of%25200.919%252C%2520a%2520mIoU%2520of%250A0.866%2520for%2520the%2520Kvasir%2520dataset%252C%2520and%2520a%2520Dice%2520score%2520of%25200.931%2520and%2520a%2520mIoU%2520of%25200.891%2520for%250Athe%2520CVC-ClinicDB.%2520The%2520visual%2520and%2520quantitative%2520results%2520highlight%2520the%2520efficacy%2520of%250Aour%2520model%252C%2520potentially%2520setting%2520a%2520new%2520model%2520in%2520medical%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPE-Net%3A%20Dual-Parallel%20Encoder%20Based%20Network%20for%20Semantic%20Segmentation%0A%20%20of%20Polyps&entry.906535625=Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Shahzad%20Ahmed%20and%20Abdul%20Raheem&entry.1292438233=%20%20In%20medical%20imaging%2C%20efficient%20segmentation%20of%20colon%20polyps%20plays%20a%20pivotal%0Arole%20in%20minimally%20invasive%20solutions%20for%20colorectal%20cancer.%20This%20study%0Aintroduces%20a%20novel%20approach%20employing%20two%20parallel%20encoder%20branches%20within%20a%0Anetwork%20for%20polyp%20segmentation.%20One%20branch%20of%20the%20encoder%20incorporates%20the%20dual%0Aconvolution%20blocks%20that%20have%20the%20capability%20to%20maintain%20feature%20information%0Aover%20increased%20depths%2C%20and%20the%20other%20block%20embraces%20the%20single%20convolution%0Ablock%20with%20the%20addition%20of%20the%20previous%20layer%27s%20feature%2C%20offering%20diversity%20in%0Afeature%20extraction%20within%20the%20encoder%2C%20combining%20them%20before%20transpose%20layers%0Awith%20a%20depth-wise%20concatenation%20operation.%20Our%20model%20demonstrated%20superior%0Aperformance%2C%20surpassing%20several%20established%20deep-learning%20architectures%20on%20the%0AKvasir%20and%20CVC-ClinicDB%20datasets%2C%20achieved%20a%20Dice%20score%20of%200.919%2C%20a%20mIoU%20of%0A0.866%20for%20the%20Kvasir%20dataset%2C%20and%20a%20Dice%20score%20of%200.931%20and%20a%20mIoU%20of%200.891%20for%0Athe%20CVC-ClinicDB.%20The%20visual%20and%20quantitative%20results%20highlight%20the%20efficacy%20of%0Aour%20model%2C%20potentially%20setting%20a%20new%20model%20in%20medical%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00888v2&entry.124074799=Read"},
{"title": "DPE-Net: Dual-Parallel Encoder Based Network for Semantic Segmentation\n  of Polyps", "author": "Malik Abdul Manan and Feng Jinchao and Shahzad Ahmed and Abdul Raheem", "abstract": "  In medical imaging, efficient segmentation of colon polyps plays a pivotal\nrole in minimally invasive solutions for colorectal cancer. This study\nintroduces a novel approach employing two parallel encoder branches within a\nnetwork for polyp segmentation. One branch of the encoder incorporates the dual\nconvolution blocks that have the capability to maintain feature information\nover increased depths, and the other block embraces the single convolution\nblock with the addition of the previous layer's feature, offering diversity in\nfeature extraction within the encoder, combining them before transpose layers\nwith a depth-wise concatenation operation. Our model demonstrated superior\nperformance, surpassing several established deep-learning architectures on the\nKvasir and CVC-ClinicDB datasets, achieved a Dice score of 0.919, a mIoU of\n0.866 for the Kvasir dataset, and a Dice score of 0.931 and a mIoU of 0.891 for\nthe CVC-ClinicDB. The visual and quantitative results highlight the efficacy of\nour model, potentially setting a new model in medical image segmentation.\n", "link": "http://arxiv.org/abs/2412.00888v2", "date": "2024-12-03", "relevancy": 2.744, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPE-Net%3A%20Dual-Parallel%20Encoder%20Based%20Network%20for%20Semantic%20Segmentation%0A%20%20of%20Polyps&body=Title%3A%20DPE-Net%3A%20Dual-Parallel%20Encoder%20Based%20Network%20for%20Semantic%20Segmentation%0A%20%20of%20Polyps%0AAuthor%3A%20Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Shahzad%20Ahmed%20and%20Abdul%20Raheem%0AAbstract%3A%20%20%20In%20medical%20imaging%2C%20efficient%20segmentation%20of%20colon%20polyps%20plays%20a%20pivotal%0Arole%20in%20minimally%20invasive%20solutions%20for%20colorectal%20cancer.%20This%20study%0Aintroduces%20a%20novel%20approach%20employing%20two%20parallel%20encoder%20branches%20within%20a%0Anetwork%20for%20polyp%20segmentation.%20One%20branch%20of%20the%20encoder%20incorporates%20the%20dual%0Aconvolution%20blocks%20that%20have%20the%20capability%20to%20maintain%20feature%20information%0Aover%20increased%20depths%2C%20and%20the%20other%20block%20embraces%20the%20single%20convolution%0Ablock%20with%20the%20addition%20of%20the%20previous%20layer%27s%20feature%2C%20offering%20diversity%20in%0Afeature%20extraction%20within%20the%20encoder%2C%20combining%20them%20before%20transpose%20layers%0Awith%20a%20depth-wise%20concatenation%20operation.%20Our%20model%20demonstrated%20superior%0Aperformance%2C%20surpassing%20several%20established%20deep-learning%20architectures%20on%20the%0AKvasir%20and%20CVC-ClinicDB%20datasets%2C%20achieved%20a%20Dice%20score%20of%200.919%2C%20a%20mIoU%20of%0A0.866%20for%20the%20Kvasir%20dataset%2C%20and%20a%20Dice%20score%20of%200.931%20and%20a%20mIoU%20of%200.891%20for%0Athe%20CVC-ClinicDB.%20The%20visual%20and%20quantitative%20results%20highlight%20the%20efficacy%20of%0Aour%20model%2C%20potentially%20setting%20a%20new%20model%20in%20medical%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPE-Net%253A%2520Dual-Parallel%2520Encoder%2520Based%2520Network%2520for%2520Semantic%2520Segmentation%250A%2520%2520of%2520Polyps%26entry.906535625%3DMalik%2520Abdul%2520Manan%2520and%2520Feng%2520Jinchao%2520and%2520Shahzad%2520Ahmed%2520and%2520Abdul%2520Raheem%26entry.1292438233%3D%2520%2520In%2520medical%2520imaging%252C%2520efficient%2520segmentation%2520of%2520colon%2520polyps%2520plays%2520a%2520pivotal%250Arole%2520in%2520minimally%2520invasive%2520solutions%2520for%2520colorectal%2520cancer.%2520This%2520study%250Aintroduces%2520a%2520novel%2520approach%2520employing%2520two%2520parallel%2520encoder%2520branches%2520within%2520a%250Anetwork%2520for%2520polyp%2520segmentation.%2520One%2520branch%2520of%2520the%2520encoder%2520incorporates%2520the%2520dual%250Aconvolution%2520blocks%2520that%2520have%2520the%2520capability%2520to%2520maintain%2520feature%2520information%250Aover%2520increased%2520depths%252C%2520and%2520the%2520other%2520block%2520embraces%2520the%2520single%2520convolution%250Ablock%2520with%2520the%2520addition%2520of%2520the%2520previous%2520layer%2527s%2520feature%252C%2520offering%2520diversity%2520in%250Afeature%2520extraction%2520within%2520the%2520encoder%252C%2520combining%2520them%2520before%2520transpose%2520layers%250Awith%2520a%2520depth-wise%2520concatenation%2520operation.%2520Our%2520model%2520demonstrated%2520superior%250Aperformance%252C%2520surpassing%2520several%2520established%2520deep-learning%2520architectures%2520on%2520the%250AKvasir%2520and%2520CVC-ClinicDB%2520datasets%252C%2520achieved%2520a%2520Dice%2520score%2520of%25200.919%252C%2520a%2520mIoU%2520of%250A0.866%2520for%2520the%2520Kvasir%2520dataset%252C%2520and%2520a%2520Dice%2520score%2520of%25200.931%2520and%2520a%2520mIoU%2520of%25200.891%2520for%250Athe%2520CVC-ClinicDB.%2520The%2520visual%2520and%2520quantitative%2520results%2520highlight%2520the%2520efficacy%2520of%250Aour%2520model%252C%2520potentially%2520setting%2520a%2520new%2520model%2520in%2520medical%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPE-Net%3A%20Dual-Parallel%20Encoder%20Based%20Network%20for%20Semantic%20Segmentation%0A%20%20of%20Polyps&entry.906535625=Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Shahzad%20Ahmed%20and%20Abdul%20Raheem&entry.1292438233=%20%20In%20medical%20imaging%2C%20efficient%20segmentation%20of%20colon%20polyps%20plays%20a%20pivotal%0Arole%20in%20minimally%20invasive%20solutions%20for%20colorectal%20cancer.%20This%20study%0Aintroduces%20a%20novel%20approach%20employing%20two%20parallel%20encoder%20branches%20within%20a%0Anetwork%20for%20polyp%20segmentation.%20One%20branch%20of%20the%20encoder%20incorporates%20the%20dual%0Aconvolution%20blocks%20that%20have%20the%20capability%20to%20maintain%20feature%20information%0Aover%20increased%20depths%2C%20and%20the%20other%20block%20embraces%20the%20single%20convolution%0Ablock%20with%20the%20addition%20of%20the%20previous%20layer%27s%20feature%2C%20offering%20diversity%20in%0Afeature%20extraction%20within%20the%20encoder%2C%20combining%20them%20before%20transpose%20layers%0Awith%20a%20depth-wise%20concatenation%20operation.%20Our%20model%20demonstrated%20superior%0Aperformance%2C%20surpassing%20several%20established%20deep-learning%20architectures%20on%20the%0AKvasir%20and%20CVC-ClinicDB%20datasets%2C%20achieved%20a%20Dice%20score%20of%200.919%2C%20a%20mIoU%20of%0A0.866%20for%20the%20Kvasir%20dataset%2C%20and%20a%20Dice%20score%20of%200.931%20and%20a%20mIoU%20of%200.891%20for%0Athe%20CVC-ClinicDB.%20The%20visual%20and%20quantitative%20results%20highlight%20the%20efficacy%20of%0Aour%20model%2C%20potentially%20setting%20a%20new%20model%20in%20medical%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00888v2&entry.124074799=Read"},
{"title": "Improved Localized Machine Unlearning Through the Lens of Memorization", "author": "Reihaneh Torkzadehmahani and Reza Nasirigerdeh and Georgios Kaissis and Daniel Rueckert and Gintare Karolina Dziugaite and Eleni Triantafillou", "abstract": "  Machine unlearning refers to removing the influence of a specified subset of\ntraining data from a machine learning model, efficiently, after it has already\nbeen trained. This is important for key applications, including making the\nmodel more accurate by removing outdated, mislabeled, or poisoned data. In this\nwork, we study localized unlearning, where the unlearning algorithm operates on\na (small) identified subset of parameters. Drawing inspiration from the\nmemorization literature, we propose an improved localization strategy that\nyields strong results when paired with existing unlearning algorithms. We also\npropose a new unlearning algorithm, Deletion by Example Localization (DEL),\nthat resets the parameters deemed-to-be most critical according to our\nlocalization strategy, and then finetunes them. Our extensive experiments on\ndifferent datasets, forget sets and metrics reveal that DEL sets a new\nstate-of-the-art for unlearning metrics, against both localized and\nfull-parameter methods, while modifying a small subset of parameters, and\noutperforms the state-of-the-art localized unlearning in terms of test accuracy\ntoo.\n", "link": "http://arxiv.org/abs/2412.02432v1", "date": "2024-12-03", "relevancy": 2.6866, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5674}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.531}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Localized%20Machine%20Unlearning%20Through%20the%20Lens%20of%20Memorization&body=Title%3A%20Improved%20Localized%20Machine%20Unlearning%20Through%20the%20Lens%20of%20Memorization%0AAuthor%3A%20Reihaneh%20Torkzadehmahani%20and%20Reza%20Nasirigerdeh%20and%20Georgios%20Kaissis%20and%20Daniel%20Rueckert%20and%20Gintare%20Karolina%20Dziugaite%20and%20Eleni%20Triantafillou%0AAbstract%3A%20%20%20Machine%20unlearning%20refers%20to%20removing%20the%20influence%20of%20a%20specified%20subset%20of%0Atraining%20data%20from%20a%20machine%20learning%20model%2C%20efficiently%2C%20after%20it%20has%20already%0Abeen%20trained.%20This%20is%20important%20for%20key%20applications%2C%20including%20making%20the%0Amodel%20more%20accurate%20by%20removing%20outdated%2C%20mislabeled%2C%20or%20poisoned%20data.%20In%20this%0Awork%2C%20we%20study%20localized%20unlearning%2C%20where%20the%20unlearning%20algorithm%20operates%20on%0Aa%20%28small%29%20identified%20subset%20of%20parameters.%20Drawing%20inspiration%20from%20the%0Amemorization%20literature%2C%20we%20propose%20an%20improved%20localization%20strategy%20that%0Ayields%20strong%20results%20when%20paired%20with%20existing%20unlearning%20algorithms.%20We%20also%0Apropose%20a%20new%20unlearning%20algorithm%2C%20Deletion%20by%20Example%20Localization%20%28DEL%29%2C%0Athat%20resets%20the%20parameters%20deemed-to-be%20most%20critical%20according%20to%20our%0Alocalization%20strategy%2C%20and%20then%20finetunes%20them.%20Our%20extensive%20experiments%20on%0Adifferent%20datasets%2C%20forget%20sets%20and%20metrics%20reveal%20that%20DEL%20sets%20a%20new%0Astate-of-the-art%20for%20unlearning%20metrics%2C%20against%20both%20localized%20and%0Afull-parameter%20methods%2C%20while%20modifying%20a%20small%20subset%20of%20parameters%2C%20and%0Aoutperforms%20the%20state-of-the-art%20localized%20unlearning%20in%20terms%20of%20test%20accuracy%0Atoo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Localized%2520Machine%2520Unlearning%2520Through%2520the%2520Lens%2520of%2520Memorization%26entry.906535625%3DReihaneh%2520Torkzadehmahani%2520and%2520Reza%2520Nasirigerdeh%2520and%2520Georgios%2520Kaissis%2520and%2520Daniel%2520Rueckert%2520and%2520Gintare%2520Karolina%2520Dziugaite%2520and%2520Eleni%2520Triantafillou%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520refers%2520to%2520removing%2520the%2520influence%2520of%2520a%2520specified%2520subset%2520of%250Atraining%2520data%2520from%2520a%2520machine%2520learning%2520model%252C%2520efficiently%252C%2520after%2520it%2520has%2520already%250Abeen%2520trained.%2520This%2520is%2520important%2520for%2520key%2520applications%252C%2520including%2520making%2520the%250Amodel%2520more%2520accurate%2520by%2520removing%2520outdated%252C%2520mislabeled%252C%2520or%2520poisoned%2520data.%2520In%2520this%250Awork%252C%2520we%2520study%2520localized%2520unlearning%252C%2520where%2520the%2520unlearning%2520algorithm%2520operates%2520on%250Aa%2520%2528small%2529%2520identified%2520subset%2520of%2520parameters.%2520Drawing%2520inspiration%2520from%2520the%250Amemorization%2520literature%252C%2520we%2520propose%2520an%2520improved%2520localization%2520strategy%2520that%250Ayields%2520strong%2520results%2520when%2520paired%2520with%2520existing%2520unlearning%2520algorithms.%2520We%2520also%250Apropose%2520a%2520new%2520unlearning%2520algorithm%252C%2520Deletion%2520by%2520Example%2520Localization%2520%2528DEL%2529%252C%250Athat%2520resets%2520the%2520parameters%2520deemed-to-be%2520most%2520critical%2520according%2520to%2520our%250Alocalization%2520strategy%252C%2520and%2520then%2520finetunes%2520them.%2520Our%2520extensive%2520experiments%2520on%250Adifferent%2520datasets%252C%2520forget%2520sets%2520and%2520metrics%2520reveal%2520that%2520DEL%2520sets%2520a%2520new%250Astate-of-the-art%2520for%2520unlearning%2520metrics%252C%2520against%2520both%2520localized%2520and%250Afull-parameter%2520methods%252C%2520while%2520modifying%2520a%2520small%2520subset%2520of%2520parameters%252C%2520and%250Aoutperforms%2520the%2520state-of-the-art%2520localized%2520unlearning%2520in%2520terms%2520of%2520test%2520accuracy%250Atoo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Localized%20Machine%20Unlearning%20Through%20the%20Lens%20of%20Memorization&entry.906535625=Reihaneh%20Torkzadehmahani%20and%20Reza%20Nasirigerdeh%20and%20Georgios%20Kaissis%20and%20Daniel%20Rueckert%20and%20Gintare%20Karolina%20Dziugaite%20and%20Eleni%20Triantafillou&entry.1292438233=%20%20Machine%20unlearning%20refers%20to%20removing%20the%20influence%20of%20a%20specified%20subset%20of%0Atraining%20data%20from%20a%20machine%20learning%20model%2C%20efficiently%2C%20after%20it%20has%20already%0Abeen%20trained.%20This%20is%20important%20for%20key%20applications%2C%20including%20making%20the%0Amodel%20more%20accurate%20by%20removing%20outdated%2C%20mislabeled%2C%20or%20poisoned%20data.%20In%20this%0Awork%2C%20we%20study%20localized%20unlearning%2C%20where%20the%20unlearning%20algorithm%20operates%20on%0Aa%20%28small%29%20identified%20subset%20of%20parameters.%20Drawing%20inspiration%20from%20the%0Amemorization%20literature%2C%20we%20propose%20an%20improved%20localization%20strategy%20that%0Ayields%20strong%20results%20when%20paired%20with%20existing%20unlearning%20algorithms.%20We%20also%0Apropose%20a%20new%20unlearning%20algorithm%2C%20Deletion%20by%20Example%20Localization%20%28DEL%29%2C%0Athat%20resets%20the%20parameters%20deemed-to-be%20most%20critical%20according%20to%20our%0Alocalization%20strategy%2C%20and%20then%20finetunes%20them.%20Our%20extensive%20experiments%20on%0Adifferent%20datasets%2C%20forget%20sets%20and%20metrics%20reveal%20that%20DEL%20sets%20a%20new%0Astate-of-the-art%20for%20unlearning%20metrics%2C%20against%20both%20localized%20and%0Afull-parameter%20methods%2C%20while%20modifying%20a%20small%20subset%20of%20parameters%2C%20and%0Aoutperforms%20the%20state-of-the-art%20localized%20unlearning%20in%20terms%20of%20test%20accuracy%0Atoo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02432v1&entry.124074799=Read"},
{"title": "Improved Localized Machine Unlearning Through the Lens of Memorization", "author": "Reihaneh Torkzadehmahani and Reza Nasirigerdeh and Georgios Kaissis and Daniel Rueckert and Gintare Karolina Dziugaite and Eleni Triantafillou", "abstract": "  Machine unlearning refers to removing the influence of a specified subset of\ntraining data from a machine learning model, efficiently, after it has already\nbeen trained. This is important for key applications, including making the\nmodel more accurate by removing outdated, mislabeled, or poisoned data. In this\nwork, we study localized unlearning, where the unlearning algorithm operates on\na (small) identified subset of parameters. Drawing inspiration from the\nmemorization literature, we propose an improved localization strategy that\nyields strong results when paired with existing unlearning algorithms. We also\npropose a new unlearning algorithm, Deletion by Example Localization (DEL),\nthat resets the parameters deemed-to-be most critical according to our\nlocalization strategy, and then finetunes them. Our extensive experiments on\ndifferent datasets, forget sets and metrics reveal that DEL sets a new\nstate-of-the-art for unlearning metrics, against both localized and\nfull-parameter methods, while modifying a small subset of parameters, and\noutperforms the state-of-the-art localized unlearning in terms of test accuracy\ntoo.\n", "link": "http://arxiv.org/abs/2412.02432v1", "date": "2024-12-03", "relevancy": 2.6866, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5674}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.531}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Localized%20Machine%20Unlearning%20Through%20the%20Lens%20of%20Memorization&body=Title%3A%20Improved%20Localized%20Machine%20Unlearning%20Through%20the%20Lens%20of%20Memorization%0AAuthor%3A%20Reihaneh%20Torkzadehmahani%20and%20Reza%20Nasirigerdeh%20and%20Georgios%20Kaissis%20and%20Daniel%20Rueckert%20and%20Gintare%20Karolina%20Dziugaite%20and%20Eleni%20Triantafillou%0AAbstract%3A%20%20%20Machine%20unlearning%20refers%20to%20removing%20the%20influence%20of%20a%20specified%20subset%20of%0Atraining%20data%20from%20a%20machine%20learning%20model%2C%20efficiently%2C%20after%20it%20has%20already%0Abeen%20trained.%20This%20is%20important%20for%20key%20applications%2C%20including%20making%20the%0Amodel%20more%20accurate%20by%20removing%20outdated%2C%20mislabeled%2C%20or%20poisoned%20data.%20In%20this%0Awork%2C%20we%20study%20localized%20unlearning%2C%20where%20the%20unlearning%20algorithm%20operates%20on%0Aa%20%28small%29%20identified%20subset%20of%20parameters.%20Drawing%20inspiration%20from%20the%0Amemorization%20literature%2C%20we%20propose%20an%20improved%20localization%20strategy%20that%0Ayields%20strong%20results%20when%20paired%20with%20existing%20unlearning%20algorithms.%20We%20also%0Apropose%20a%20new%20unlearning%20algorithm%2C%20Deletion%20by%20Example%20Localization%20%28DEL%29%2C%0Athat%20resets%20the%20parameters%20deemed-to-be%20most%20critical%20according%20to%20our%0Alocalization%20strategy%2C%20and%20then%20finetunes%20them.%20Our%20extensive%20experiments%20on%0Adifferent%20datasets%2C%20forget%20sets%20and%20metrics%20reveal%20that%20DEL%20sets%20a%20new%0Astate-of-the-art%20for%20unlearning%20metrics%2C%20against%20both%20localized%20and%0Afull-parameter%20methods%2C%20while%20modifying%20a%20small%20subset%20of%20parameters%2C%20and%0Aoutperforms%20the%20state-of-the-art%20localized%20unlearning%20in%20terms%20of%20test%20accuracy%0Atoo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Localized%2520Machine%2520Unlearning%2520Through%2520the%2520Lens%2520of%2520Memorization%26entry.906535625%3DReihaneh%2520Torkzadehmahani%2520and%2520Reza%2520Nasirigerdeh%2520and%2520Georgios%2520Kaissis%2520and%2520Daniel%2520Rueckert%2520and%2520Gintare%2520Karolina%2520Dziugaite%2520and%2520Eleni%2520Triantafillou%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520refers%2520to%2520removing%2520the%2520influence%2520of%2520a%2520specified%2520subset%2520of%250Atraining%2520data%2520from%2520a%2520machine%2520learning%2520model%252C%2520efficiently%252C%2520after%2520it%2520has%2520already%250Abeen%2520trained.%2520This%2520is%2520important%2520for%2520key%2520applications%252C%2520including%2520making%2520the%250Amodel%2520more%2520accurate%2520by%2520removing%2520outdated%252C%2520mislabeled%252C%2520or%2520poisoned%2520data.%2520In%2520this%250Awork%252C%2520we%2520study%2520localized%2520unlearning%252C%2520where%2520the%2520unlearning%2520algorithm%2520operates%2520on%250Aa%2520%2528small%2529%2520identified%2520subset%2520of%2520parameters.%2520Drawing%2520inspiration%2520from%2520the%250Amemorization%2520literature%252C%2520we%2520propose%2520an%2520improved%2520localization%2520strategy%2520that%250Ayields%2520strong%2520results%2520when%2520paired%2520with%2520existing%2520unlearning%2520algorithms.%2520We%2520also%250Apropose%2520a%2520new%2520unlearning%2520algorithm%252C%2520Deletion%2520by%2520Example%2520Localization%2520%2528DEL%2529%252C%250Athat%2520resets%2520the%2520parameters%2520deemed-to-be%2520most%2520critical%2520according%2520to%2520our%250Alocalization%2520strategy%252C%2520and%2520then%2520finetunes%2520them.%2520Our%2520extensive%2520experiments%2520on%250Adifferent%2520datasets%252C%2520forget%2520sets%2520and%2520metrics%2520reveal%2520that%2520DEL%2520sets%2520a%2520new%250Astate-of-the-art%2520for%2520unlearning%2520metrics%252C%2520against%2520both%2520localized%2520and%250Afull-parameter%2520methods%252C%2520while%2520modifying%2520a%2520small%2520subset%2520of%2520parameters%252C%2520and%250Aoutperforms%2520the%2520state-of-the-art%2520localized%2520unlearning%2520in%2520terms%2520of%2520test%2520accuracy%250Atoo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Localized%20Machine%20Unlearning%20Through%20the%20Lens%20of%20Memorization&entry.906535625=Reihaneh%20Torkzadehmahani%20and%20Reza%20Nasirigerdeh%20and%20Georgios%20Kaissis%20and%20Daniel%20Rueckert%20and%20Gintare%20Karolina%20Dziugaite%20and%20Eleni%20Triantafillou&entry.1292438233=%20%20Machine%20unlearning%20refers%20to%20removing%20the%20influence%20of%20a%20specified%20subset%20of%0Atraining%20data%20from%20a%20machine%20learning%20model%2C%20efficiently%2C%20after%20it%20has%20already%0Abeen%20trained.%20This%20is%20important%20for%20key%20applications%2C%20including%20making%20the%0Amodel%20more%20accurate%20by%20removing%20outdated%2C%20mislabeled%2C%20or%20poisoned%20data.%20In%20this%0Awork%2C%20we%20study%20localized%20unlearning%2C%20where%20the%20unlearning%20algorithm%20operates%20on%0Aa%20%28small%29%20identified%20subset%20of%20parameters.%20Drawing%20inspiration%20from%20the%0Amemorization%20literature%2C%20we%20propose%20an%20improved%20localization%20strategy%20that%0Ayields%20strong%20results%20when%20paired%20with%20existing%20unlearning%20algorithms.%20We%20also%0Apropose%20a%20new%20unlearning%20algorithm%2C%20Deletion%20by%20Example%20Localization%20%28DEL%29%2C%0Athat%20resets%20the%20parameters%20deemed-to-be%20most%20critical%20according%20to%20our%0Alocalization%20strategy%2C%20and%20then%20finetunes%20them.%20Our%20extensive%20experiments%20on%0Adifferent%20datasets%2C%20forget%20sets%20and%20metrics%20reveal%20that%20DEL%20sets%20a%20new%0Astate-of-the-art%20for%20unlearning%20metrics%2C%20against%20both%20localized%20and%0Afull-parameter%20methods%2C%20while%20modifying%20a%20small%20subset%20of%20parameters%2C%20and%0Aoutperforms%20the%20state-of-the-art%20localized%20unlearning%20in%20terms%20of%20test%20accuracy%0Atoo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02432v1&entry.124074799=Read"},
{"title": "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs", "author": "Alireza Rezazadeh and Zichao Li and Wei Wei and Yujia Bao", "abstract": "  Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management.\n", "link": "http://arxiv.org/abs/2410.14052v2", "date": "2024-12-03", "relevancy": 2.6781, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Isolated%20Conversations%20to%20Hierarchical%20Schemas%3A%20Dynamic%20Tree%20Memory%0A%20%20Representation%20for%20LLMs&body=Title%3A%20From%20Isolated%20Conversations%20to%20Hierarchical%20Schemas%3A%20Dynamic%20Tree%20Memory%0A%20%20Representation%20for%20LLMs%0AAuthor%3A%20Alireza%20Rezazadeh%20and%20Zichao%20Li%20and%20Wei%20Wei%20and%20Yujia%20Bao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20have%20significantly%20improved%0Atheir%20context%20windows%2C%20yet%20challenges%20in%20effective%20long-term%20memory%20management%0Aremain.%20We%20introduce%20MemTree%2C%20an%20algorithm%20that%20leverages%20a%20dynamic%2C%0Atree-structured%20memory%20representation%20to%20optimize%20the%20organization%2C%20retrieval%2C%0Aand%20integration%20of%20information%2C%20akin%20to%20human%20cognitive%20schemas.%20MemTree%0Aorganizes%20memory%20hierarchically%2C%20with%20each%20node%20encapsulating%20aggregated%0Atextual%20content%2C%20corresponding%20semantic%20embeddings%2C%20and%20varying%20abstraction%0Alevels%20across%20the%20tree%27s%20depths.%20Our%20algorithm%20dynamically%20adapts%20this%20memory%0Astructure%20by%20computing%20and%20comparing%20semantic%20embeddings%20of%20new%20and%20existing%0Ainformation%20to%20enrich%20the%20model%27s%20context-awareness.%20This%20approach%20allows%0AMemTree%20to%20handle%20complex%20reasoning%20and%20extended%20interactions%20more%20effectively%0Athan%20traditional%20memory%20augmentation%20methods%2C%20which%20often%20rely%20on%20flat%20lookup%0Atables.%20Evaluations%20on%20benchmarks%20for%20multi-turn%20dialogue%20understanding%20and%0Adocument%20question%20answering%20show%20that%20MemTree%20significantly%20enhances%0Aperformance%20in%20scenarios%20that%20demand%20structured%20memory%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Isolated%2520Conversations%2520to%2520Hierarchical%2520Schemas%253A%2520Dynamic%2520Tree%2520Memory%250A%2520%2520Representation%2520for%2520LLMs%26entry.906535625%3DAlireza%2520Rezazadeh%2520and%2520Zichao%2520Li%2520and%2520Wei%2520Wei%2520and%2520Yujia%2520Bao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520have%2520significantly%2520improved%250Atheir%2520context%2520windows%252C%2520yet%2520challenges%2520in%2520effective%2520long-term%2520memory%2520management%250Aremain.%2520We%2520introduce%2520MemTree%252C%2520an%2520algorithm%2520that%2520leverages%2520a%2520dynamic%252C%250Atree-structured%2520memory%2520representation%2520to%2520optimize%2520the%2520organization%252C%2520retrieval%252C%250Aand%2520integration%2520of%2520information%252C%2520akin%2520to%2520human%2520cognitive%2520schemas.%2520MemTree%250Aorganizes%2520memory%2520hierarchically%252C%2520with%2520each%2520node%2520encapsulating%2520aggregated%250Atextual%2520content%252C%2520corresponding%2520semantic%2520embeddings%252C%2520and%2520varying%2520abstraction%250Alevels%2520across%2520the%2520tree%2527s%2520depths.%2520Our%2520algorithm%2520dynamically%2520adapts%2520this%2520memory%250Astructure%2520by%2520computing%2520and%2520comparing%2520semantic%2520embeddings%2520of%2520new%2520and%2520existing%250Ainformation%2520to%2520enrich%2520the%2520model%2527s%2520context-awareness.%2520This%2520approach%2520allows%250AMemTree%2520to%2520handle%2520complex%2520reasoning%2520and%2520extended%2520interactions%2520more%2520effectively%250Athan%2520traditional%2520memory%2520augmentation%2520methods%252C%2520which%2520often%2520rely%2520on%2520flat%2520lookup%250Atables.%2520Evaluations%2520on%2520benchmarks%2520for%2520multi-turn%2520dialogue%2520understanding%2520and%250Adocument%2520question%2520answering%2520show%2520that%2520MemTree%2520significantly%2520enhances%250Aperformance%2520in%2520scenarios%2520that%2520demand%2520structured%2520memory%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Isolated%20Conversations%20to%20Hierarchical%20Schemas%3A%20Dynamic%20Tree%20Memory%0A%20%20Representation%20for%20LLMs&entry.906535625=Alireza%20Rezazadeh%20and%20Zichao%20Li%20and%20Wei%20Wei%20and%20Yujia%20Bao&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20have%20significantly%20improved%0Atheir%20context%20windows%2C%20yet%20challenges%20in%20effective%20long-term%20memory%20management%0Aremain.%20We%20introduce%20MemTree%2C%20an%20algorithm%20that%20leverages%20a%20dynamic%2C%0Atree-structured%20memory%20representation%20to%20optimize%20the%20organization%2C%20retrieval%2C%0Aand%20integration%20of%20information%2C%20akin%20to%20human%20cognitive%20schemas.%20MemTree%0Aorganizes%20memory%20hierarchically%2C%20with%20each%20node%20encapsulating%20aggregated%0Atextual%20content%2C%20corresponding%20semantic%20embeddings%2C%20and%20varying%20abstraction%0Alevels%20across%20the%20tree%27s%20depths.%20Our%20algorithm%20dynamically%20adapts%20this%20memory%0Astructure%20by%20computing%20and%20comparing%20semantic%20embeddings%20of%20new%20and%20existing%0Ainformation%20to%20enrich%20the%20model%27s%20context-awareness.%20This%20approach%20allows%0AMemTree%20to%20handle%20complex%20reasoning%20and%20extended%20interactions%20more%20effectively%0Athan%20traditional%20memory%20augmentation%20methods%2C%20which%20often%20rely%20on%20flat%20lookup%0Atables.%20Evaluations%20on%20benchmarks%20for%20multi-turn%20dialogue%20understanding%20and%0Adocument%20question%20answering%20show%20that%20MemTree%20significantly%20enhances%0Aperformance%20in%20scenarios%20that%20demand%20structured%20memory%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14052v2&entry.124074799=Read"},
{"title": "OMENN: One Matrix to Explain Neural Networks", "author": "Adam Wr\u00f3bel and Miko\u0142aj Janusz and Bartosz Zieli\u0144ski and Dawid Rymarczyk", "abstract": "  Deep Learning (DL) models are often black boxes, making their decision-making\nprocesses difficult to interpret. This lack of transparency has driven\nadvancements in eXplainable Artificial Intelligence (XAI), a field dedicated to\nclarifying the reasoning behind DL model predictions. Among these,\nattribution-based methods such as LRP and GradCAM are widely used, though they\nrely on approximations that can be imprecise.\n  To address these limitations, we introduce One Matrix to Explain Neural\nNetworks (OMENN), a novel post-hoc method that represents a neural network as a\nsingle, interpretable matrix for each specific input. This matrix is\nconstructed through a series of linear transformations that represent the\nprocessing of the input by each successive layer in the neural network. As a\nresult, OMENN provides locally precise, attribution-based explanations of the\ninput across various modern models, including ViTs and CNNs. We present a\ntheoretical analysis of OMENN based on dynamic linearity property and validate\nits effectiveness with extensive tests on two XAI benchmarks, demonstrating\nthat OMENN is competitive with state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2412.02399v1", "date": "2024-12-03", "relevancy": 2.6463, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMENN%3A%20One%20Matrix%20to%20Explain%20Neural%20Networks&body=Title%3A%20OMENN%3A%20One%20Matrix%20to%20Explain%20Neural%20Networks%0AAuthor%3A%20Adam%20Wr%C3%B3bel%20and%20Miko%C5%82aj%20Janusz%20and%20Bartosz%20Zieli%C5%84ski%20and%20Dawid%20Rymarczyk%0AAbstract%3A%20%20%20Deep%20Learning%20%28DL%29%20models%20are%20often%20black%20boxes%2C%20making%20their%20decision-making%0Aprocesses%20difficult%20to%20interpret.%20This%20lack%20of%20transparency%20has%20driven%0Aadvancements%20in%20eXplainable%20Artificial%20Intelligence%20%28XAI%29%2C%20a%20field%20dedicated%20to%0Aclarifying%20the%20reasoning%20behind%20DL%20model%20predictions.%20Among%20these%2C%0Aattribution-based%20methods%20such%20as%20LRP%20and%20GradCAM%20are%20widely%20used%2C%20though%20they%0Arely%20on%20approximations%20that%20can%20be%20imprecise.%0A%20%20To%20address%20these%20limitations%2C%20we%20introduce%20One%20Matrix%20to%20Explain%20Neural%0ANetworks%20%28OMENN%29%2C%20a%20novel%20post-hoc%20method%20that%20represents%20a%20neural%20network%20as%20a%0Asingle%2C%20interpretable%20matrix%20for%20each%20specific%20input.%20This%20matrix%20is%0Aconstructed%20through%20a%20series%20of%20linear%20transformations%20that%20represent%20the%0Aprocessing%20of%20the%20input%20by%20each%20successive%20layer%20in%20the%20neural%20network.%20As%20a%0Aresult%2C%20OMENN%20provides%20locally%20precise%2C%20attribution-based%20explanations%20of%20the%0Ainput%20across%20various%20modern%20models%2C%20including%20ViTs%20and%20CNNs.%20We%20present%20a%0Atheoretical%20analysis%20of%20OMENN%20based%20on%20dynamic%20linearity%20property%20and%20validate%0Aits%20effectiveness%20with%20extensive%20tests%20on%20two%20XAI%20benchmarks%2C%20demonstrating%0Athat%20OMENN%20is%20competitive%20with%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMENN%253A%2520One%2520Matrix%2520to%2520Explain%2520Neural%2520Networks%26entry.906535625%3DAdam%2520Wr%25C3%25B3bel%2520and%2520Miko%25C5%2582aj%2520Janusz%2520and%2520Bartosz%2520Zieli%25C5%2584ski%2520and%2520Dawid%2520Rymarczyk%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520%2528DL%2529%2520models%2520are%2520often%2520black%2520boxes%252C%2520making%2520their%2520decision-making%250Aprocesses%2520difficult%2520to%2520interpret.%2520This%2520lack%2520of%2520transparency%2520has%2520driven%250Aadvancements%2520in%2520eXplainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%252C%2520a%2520field%2520dedicated%2520to%250Aclarifying%2520the%2520reasoning%2520behind%2520DL%2520model%2520predictions.%2520Among%2520these%252C%250Aattribution-based%2520methods%2520such%2520as%2520LRP%2520and%2520GradCAM%2520are%2520widely%2520used%252C%2520though%2520they%250Arely%2520on%2520approximations%2520that%2520can%2520be%2520imprecise.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520One%2520Matrix%2520to%2520Explain%2520Neural%250ANetworks%2520%2528OMENN%2529%252C%2520a%2520novel%2520post-hoc%2520method%2520that%2520represents%2520a%2520neural%2520network%2520as%2520a%250Asingle%252C%2520interpretable%2520matrix%2520for%2520each%2520specific%2520input.%2520This%2520matrix%2520is%250Aconstructed%2520through%2520a%2520series%2520of%2520linear%2520transformations%2520that%2520represent%2520the%250Aprocessing%2520of%2520the%2520input%2520by%2520each%2520successive%2520layer%2520in%2520the%2520neural%2520network.%2520As%2520a%250Aresult%252C%2520OMENN%2520provides%2520locally%2520precise%252C%2520attribution-based%2520explanations%2520of%2520the%250Ainput%2520across%2520various%2520modern%2520models%252C%2520including%2520ViTs%2520and%2520CNNs.%2520We%2520present%2520a%250Atheoretical%2520analysis%2520of%2520OMENN%2520based%2520on%2520dynamic%2520linearity%2520property%2520and%2520validate%250Aits%2520effectiveness%2520with%2520extensive%2520tests%2520on%2520two%2520XAI%2520benchmarks%252C%2520demonstrating%250Athat%2520OMENN%2520is%2520competitive%2520with%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMENN%3A%20One%20Matrix%20to%20Explain%20Neural%20Networks&entry.906535625=Adam%20Wr%C3%B3bel%20and%20Miko%C5%82aj%20Janusz%20and%20Bartosz%20Zieli%C5%84ski%20and%20Dawid%20Rymarczyk&entry.1292438233=%20%20Deep%20Learning%20%28DL%29%20models%20are%20often%20black%20boxes%2C%20making%20their%20decision-making%0Aprocesses%20difficult%20to%20interpret.%20This%20lack%20of%20transparency%20has%20driven%0Aadvancements%20in%20eXplainable%20Artificial%20Intelligence%20%28XAI%29%2C%20a%20field%20dedicated%20to%0Aclarifying%20the%20reasoning%20behind%20DL%20model%20predictions.%20Among%20these%2C%0Aattribution-based%20methods%20such%20as%20LRP%20and%20GradCAM%20are%20widely%20used%2C%20though%20they%0Arely%20on%20approximations%20that%20can%20be%20imprecise.%0A%20%20To%20address%20these%20limitations%2C%20we%20introduce%20One%20Matrix%20to%20Explain%20Neural%0ANetworks%20%28OMENN%29%2C%20a%20novel%20post-hoc%20method%20that%20represents%20a%20neural%20network%20as%20a%0Asingle%2C%20interpretable%20matrix%20for%20each%20specific%20input.%20This%20matrix%20is%0Aconstructed%20through%20a%20series%20of%20linear%20transformations%20that%20represent%20the%0Aprocessing%20of%20the%20input%20by%20each%20successive%20layer%20in%20the%20neural%20network.%20As%20a%0Aresult%2C%20OMENN%20provides%20locally%20precise%2C%20attribution-based%20explanations%20of%20the%0Ainput%20across%20various%20modern%20models%2C%20including%20ViTs%20and%20CNNs.%20We%20present%20a%0Atheoretical%20analysis%20of%20OMENN%20based%20on%20dynamic%20linearity%20property%20and%20validate%0Aits%20effectiveness%20with%20extensive%20tests%20on%20two%20XAI%20benchmarks%2C%20demonstrating%0Athat%20OMENN%20is%20competitive%20with%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02399v1&entry.124074799=Read"},
{"title": "FCL-ViT: Task-Aware Attention Tuning for Continual Learning", "author": "Anestis Kaimakamidis and Ioannis Pitas", "abstract": "  Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN)\nknowledge to new tasks, without forgetting the old ones. However, modern CL\ntechniques focus on provisioning memory capabilities to existing DNN models\nrather than designing new ones that are able to adapt according to the task at\nhand. This paper presents the novel Feedback Continual Learning Vision\nTransformer (FCL-ViT) that uses a feedback mechanism to generate real-time\ndynamic attention features tailored to the current task. The FCL-ViT operates\nin two Phases. In phase 1, the generic image features are produced and\ndetermine where the Transformer should attend on the current image. In phase 2,\ntask-specific image features are generated that leverage dynamic attention. To\nthis end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs)\nare introduced that operate in both phases and are responsible for tuning the\nTABs attention, respectively. The FCL-ViT surpasses state-of-the-art\nperformance on Continual Learning compared to benchmark methods, while\nretaining a small number of trainable DNN parameters.\n", "link": "http://arxiv.org/abs/2412.02509v1", "date": "2024-12-03", "relevancy": 2.64, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5284}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&body=Title%3A%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning%0AAuthor%3A%20Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFCL-ViT%253A%2520Task-Aware%2520Attention%2520Tuning%2520for%2520Continual%2520Learning%26entry.906535625%3DAnestis%2520Kaimakamidis%2520and%2520Ioannis%2520Pitas%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520involves%2520adapting%2520the%2520prior%2520Deep%2520Neural%2520Network%2520%2528DNN%2529%250Aknowledge%2520to%2520new%2520tasks%252C%2520without%2520forgetting%2520the%2520old%2520ones.%2520However%252C%2520modern%2520CL%250Atechniques%2520focus%2520on%2520provisioning%2520memory%2520capabilities%2520to%2520existing%2520DNN%2520models%250Arather%2520than%2520designing%2520new%2520ones%2520that%2520are%2520able%2520to%2520adapt%2520according%2520to%2520the%2520task%2520at%250Ahand.%2520This%2520paper%2520presents%2520the%2520novel%2520Feedback%2520Continual%2520Learning%2520Vision%250ATransformer%2520%2528FCL-ViT%2529%2520that%2520uses%2520a%2520feedback%2520mechanism%2520to%2520generate%2520real-time%250Adynamic%2520attention%2520features%2520tailored%2520to%2520the%2520current%2520task.%2520The%2520FCL-ViT%2520operates%250Ain%2520two%2520Phases.%2520In%2520phase%25201%252C%2520the%2520generic%2520image%2520features%2520are%2520produced%2520and%250Adetermine%2520where%2520the%2520Transformer%2520should%2520attend%2520on%2520the%2520current%2520image.%2520In%2520phase%25202%252C%250Atask-specific%2520image%2520features%2520are%2520generated%2520that%2520leverage%2520dynamic%2520attention.%2520To%250Athis%2520end%252C%2520Tunable%2520self-Attention%2520Blocks%2520%2528TABs%2529%2520and%2520Task%2520Specific%2520Blocks%2520%2528TSBs%2529%250Aare%2520introduced%2520that%2520operate%2520in%2520both%2520phases%2520and%2520are%2520responsible%2520for%2520tuning%2520the%250ATABs%2520attention%252C%2520respectively.%2520The%2520FCL-ViT%2520surpasses%2520state-of-the-art%250Aperformance%2520on%2520Continual%2520Learning%2520compared%2520to%2520benchmark%2520methods%252C%2520while%250Aretaining%2520a%2520small%2520number%2520of%2520trainable%2520DNN%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&entry.906535625=Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02509v1&entry.124074799=Read"},
{"title": "FCL-ViT: Task-Aware Attention Tuning for Continual Learning", "author": "Anestis Kaimakamidis and Ioannis Pitas", "abstract": "  Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN)\nknowledge to new tasks, without forgetting the old ones. However, modern CL\ntechniques focus on provisioning memory capabilities to existing DNN models\nrather than designing new ones that are able to adapt according to the task at\nhand. This paper presents the novel Feedback Continual Learning Vision\nTransformer (FCL-ViT) that uses a feedback mechanism to generate real-time\ndynamic attention features tailored to the current task. The FCL-ViT operates\nin two Phases. In phase 1, the generic image features are produced and\ndetermine where the Transformer should attend on the current image. In phase 2,\ntask-specific image features are generated that leverage dynamic attention. To\nthis end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs)\nare introduced that operate in both phases and are responsible for tuning the\nTABs attention, respectively. The FCL-ViT surpasses state-of-the-art\nperformance on Continual Learning compared to benchmark methods, while\nretaining a small number of trainable DNN parameters.\n", "link": "http://arxiv.org/abs/2412.02509v1", "date": "2024-12-03", "relevancy": 2.64, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5284}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&body=Title%3A%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning%0AAuthor%3A%20Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFCL-ViT%253A%2520Task-Aware%2520Attention%2520Tuning%2520for%2520Continual%2520Learning%26entry.906535625%3DAnestis%2520Kaimakamidis%2520and%2520Ioannis%2520Pitas%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520involves%2520adapting%2520the%2520prior%2520Deep%2520Neural%2520Network%2520%2528DNN%2529%250Aknowledge%2520to%2520new%2520tasks%252C%2520without%2520forgetting%2520the%2520old%2520ones.%2520However%252C%2520modern%2520CL%250Atechniques%2520focus%2520on%2520provisioning%2520memory%2520capabilities%2520to%2520existing%2520DNN%2520models%250Arather%2520than%2520designing%2520new%2520ones%2520that%2520are%2520able%2520to%2520adapt%2520according%2520to%2520the%2520task%2520at%250Ahand.%2520This%2520paper%2520presents%2520the%2520novel%2520Feedback%2520Continual%2520Learning%2520Vision%250ATransformer%2520%2528FCL-ViT%2529%2520that%2520uses%2520a%2520feedback%2520mechanism%2520to%2520generate%2520real-time%250Adynamic%2520attention%2520features%2520tailored%2520to%2520the%2520current%2520task.%2520The%2520FCL-ViT%2520operates%250Ain%2520two%2520Phases.%2520In%2520phase%25201%252C%2520the%2520generic%2520image%2520features%2520are%2520produced%2520and%250Adetermine%2520where%2520the%2520Transformer%2520should%2520attend%2520on%2520the%2520current%2520image.%2520In%2520phase%25202%252C%250Atask-specific%2520image%2520features%2520are%2520generated%2520that%2520leverage%2520dynamic%2520attention.%2520To%250Athis%2520end%252C%2520Tunable%2520self-Attention%2520Blocks%2520%2528TABs%2529%2520and%2520Task%2520Specific%2520Blocks%2520%2528TSBs%2529%250Aare%2520introduced%2520that%2520operate%2520in%2520both%2520phases%2520and%2520are%2520responsible%2520for%2520tuning%2520the%250ATABs%2520attention%252C%2520respectively.%2520The%2520FCL-ViT%2520surpasses%2520state-of-the-art%250Aperformance%2520on%2520Continual%2520Learning%2520compared%2520to%2520benchmark%2520methods%252C%2520while%250Aretaining%2520a%2520small%2520number%2520of%2520trainable%2520DNN%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&entry.906535625=Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02509v1&entry.124074799=Read"},
{"title": "FCL-ViT: Task-Aware Attention Tuning for Continual Learning", "author": "Anestis Kaimakamidis and Ioannis Pitas", "abstract": "  Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN)\nknowledge to new tasks, without forgetting the old ones. However, modern CL\ntechniques focus on provisioning memory capabilities to existing DNN models\nrather than designing new ones that are able to adapt according to the task at\nhand. This paper presents the novel Feedback Continual Learning Vision\nTransformer (FCL-ViT) that uses a feedback mechanism to generate real-time\ndynamic attention features tailored to the current task. The FCL-ViT operates\nin two Phases. In phase 1, the generic image features are produced and\ndetermine where the Transformer should attend on the current image. In phase 2,\ntask-specific image features are generated that leverage dynamic attention. To\nthis end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs)\nare introduced that operate in both phases and are responsible for tuning the\nTABs attention, respectively. The FCL-ViT surpasses state-of-the-art\nperformance on Continual Learning compared to benchmark methods, while\nretaining a small number of trainable DNN parameters.\n", "link": "http://arxiv.org/abs/2412.02509v1", "date": "2024-12-03", "relevancy": 2.64, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5284}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&body=Title%3A%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning%0AAuthor%3A%20Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFCL-ViT%253A%2520Task-Aware%2520Attention%2520Tuning%2520for%2520Continual%2520Learning%26entry.906535625%3DAnestis%2520Kaimakamidis%2520and%2520Ioannis%2520Pitas%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520involves%2520adapting%2520the%2520prior%2520Deep%2520Neural%2520Network%2520%2528DNN%2529%250Aknowledge%2520to%2520new%2520tasks%252C%2520without%2520forgetting%2520the%2520old%2520ones.%2520However%252C%2520modern%2520CL%250Atechniques%2520focus%2520on%2520provisioning%2520memory%2520capabilities%2520to%2520existing%2520DNN%2520models%250Arather%2520than%2520designing%2520new%2520ones%2520that%2520are%2520able%2520to%2520adapt%2520according%2520to%2520the%2520task%2520at%250Ahand.%2520This%2520paper%2520presents%2520the%2520novel%2520Feedback%2520Continual%2520Learning%2520Vision%250ATransformer%2520%2528FCL-ViT%2529%2520that%2520uses%2520a%2520feedback%2520mechanism%2520to%2520generate%2520real-time%250Adynamic%2520attention%2520features%2520tailored%2520to%2520the%2520current%2520task.%2520The%2520FCL-ViT%2520operates%250Ain%2520two%2520Phases.%2520In%2520phase%25201%252C%2520the%2520generic%2520image%2520features%2520are%2520produced%2520and%250Adetermine%2520where%2520the%2520Transformer%2520should%2520attend%2520on%2520the%2520current%2520image.%2520In%2520phase%25202%252C%250Atask-specific%2520image%2520features%2520are%2520generated%2520that%2520leverage%2520dynamic%2520attention.%2520To%250Athis%2520end%252C%2520Tunable%2520self-Attention%2520Blocks%2520%2528TABs%2529%2520and%2520Task%2520Specific%2520Blocks%2520%2528TSBs%2529%250Aare%2520introduced%2520that%2520operate%2520in%2520both%2520phases%2520and%2520are%2520responsible%2520for%2520tuning%2520the%250ATABs%2520attention%252C%2520respectively.%2520The%2520FCL-ViT%2520surpasses%2520state-of-the-art%250Aperformance%2520on%2520Continual%2520Learning%2520compared%2520to%2520benchmark%2520methods%252C%2520while%250Aretaining%2520a%2520small%2520number%2520of%2520trainable%2520DNN%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&entry.906535625=Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02509v1&entry.124074799=Read"},
{"title": "BYE: Build Your Encoder with One Sequence of Exploration Data for\n  Long-Term Dynamic Scene Understanding", "author": "Chenguang Huang and Shengchao Yan and Wolfram Burgard", "abstract": "  Dynamic scene understanding remains a persistent challenge in robotic\napplications. Early dynamic mapping methods focused on mitigating the negative\ninfluence of short-term dynamic objects on camera motion estimation by masking\nor tracking specific categories, which often fall short in adapting to\nlong-term scene changes. Recent efforts address object association in long-term\ndynamic environments using neural networks trained on synthetic datasets, but\nthey still rely on predefined object shapes and categories. Other methods\nincorporate visual, geometric, or semantic heuristics for the association but\noften lack robustness. In this work, we introduce BYE, a class-agnostic,\nper-scene point cloud encoder that removes the need for predefined categories,\nshape priors, or extensive association datasets. Trained on only a single\nsequence of exploration data, BYE can efficiently perform object association in\ndynamically changing scenes. We further propose an ensembling scheme combining\nthe semantic strengths of Vision Language Models (VLMs) with the scene-specific\nexpertise of BYE, achieving a 7% improvement and a 95% success rate in object\nassociation tasks. Code and dataset are available at\nhttps://byencoder.github.io.\n", "link": "http://arxiv.org/abs/2412.02449v1", "date": "2024-12-03", "relevancy": 2.6121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6608}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BYE%3A%20Build%20Your%20Encoder%20with%20One%20Sequence%20of%20Exploration%20Data%20for%0A%20%20Long-Term%20Dynamic%20Scene%20Understanding&body=Title%3A%20BYE%3A%20Build%20Your%20Encoder%20with%20One%20Sequence%20of%20Exploration%20Data%20for%0A%20%20Long-Term%20Dynamic%20Scene%20Understanding%0AAuthor%3A%20Chenguang%20Huang%20and%20Shengchao%20Yan%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Dynamic%20scene%20understanding%20remains%20a%20persistent%20challenge%20in%20robotic%0Aapplications.%20Early%20dynamic%20mapping%20methods%20focused%20on%20mitigating%20the%20negative%0Ainfluence%20of%20short-term%20dynamic%20objects%20on%20camera%20motion%20estimation%20by%20masking%0Aor%20tracking%20specific%20categories%2C%20which%20often%20fall%20short%20in%20adapting%20to%0Along-term%20scene%20changes.%20Recent%20efforts%20address%20object%20association%20in%20long-term%0Adynamic%20environments%20using%20neural%20networks%20trained%20on%20synthetic%20datasets%2C%20but%0Athey%20still%20rely%20on%20predefined%20object%20shapes%20and%20categories.%20Other%20methods%0Aincorporate%20visual%2C%20geometric%2C%20or%20semantic%20heuristics%20for%20the%20association%20but%0Aoften%20lack%20robustness.%20In%20this%20work%2C%20we%20introduce%20BYE%2C%20a%20class-agnostic%2C%0Aper-scene%20point%20cloud%20encoder%20that%20removes%20the%20need%20for%20predefined%20categories%2C%0Ashape%20priors%2C%20or%20extensive%20association%20datasets.%20Trained%20on%20only%20a%20single%0Asequence%20of%20exploration%20data%2C%20BYE%20can%20efficiently%20perform%20object%20association%20in%0Adynamically%20changing%20scenes.%20We%20further%20propose%20an%20ensembling%20scheme%20combining%0Athe%20semantic%20strengths%20of%20Vision%20Language%20Models%20%28VLMs%29%20with%20the%20scene-specific%0Aexpertise%20of%20BYE%2C%20achieving%20a%207%25%20improvement%20and%20a%2095%25%20success%20rate%20in%20object%0Aassociation%20tasks.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//byencoder.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBYE%253A%2520Build%2520Your%2520Encoder%2520with%2520One%2520Sequence%2520of%2520Exploration%2520Data%2520for%250A%2520%2520Long-Term%2520Dynamic%2520Scene%2520Understanding%26entry.906535625%3DChenguang%2520Huang%2520and%2520Shengchao%2520Yan%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520Dynamic%2520scene%2520understanding%2520remains%2520a%2520persistent%2520challenge%2520in%2520robotic%250Aapplications.%2520Early%2520dynamic%2520mapping%2520methods%2520focused%2520on%2520mitigating%2520the%2520negative%250Ainfluence%2520of%2520short-term%2520dynamic%2520objects%2520on%2520camera%2520motion%2520estimation%2520by%2520masking%250Aor%2520tracking%2520specific%2520categories%252C%2520which%2520often%2520fall%2520short%2520in%2520adapting%2520to%250Along-term%2520scene%2520changes.%2520Recent%2520efforts%2520address%2520object%2520association%2520in%2520long-term%250Adynamic%2520environments%2520using%2520neural%2520networks%2520trained%2520on%2520synthetic%2520datasets%252C%2520but%250Athey%2520still%2520rely%2520on%2520predefined%2520object%2520shapes%2520and%2520categories.%2520Other%2520methods%250Aincorporate%2520visual%252C%2520geometric%252C%2520or%2520semantic%2520heuristics%2520for%2520the%2520association%2520but%250Aoften%2520lack%2520robustness.%2520In%2520this%2520work%252C%2520we%2520introduce%2520BYE%252C%2520a%2520class-agnostic%252C%250Aper-scene%2520point%2520cloud%2520encoder%2520that%2520removes%2520the%2520need%2520for%2520predefined%2520categories%252C%250Ashape%2520priors%252C%2520or%2520extensive%2520association%2520datasets.%2520Trained%2520on%2520only%2520a%2520single%250Asequence%2520of%2520exploration%2520data%252C%2520BYE%2520can%2520efficiently%2520perform%2520object%2520association%2520in%250Adynamically%2520changing%2520scenes.%2520We%2520further%2520propose%2520an%2520ensembling%2520scheme%2520combining%250Athe%2520semantic%2520strengths%2520of%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520with%2520the%2520scene-specific%250Aexpertise%2520of%2520BYE%252C%2520achieving%2520a%25207%2525%2520improvement%2520and%2520a%252095%2525%2520success%2520rate%2520in%2520object%250Aassociation%2520tasks.%2520Code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//byencoder.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BYE%3A%20Build%20Your%20Encoder%20with%20One%20Sequence%20of%20Exploration%20Data%20for%0A%20%20Long-Term%20Dynamic%20Scene%20Understanding&entry.906535625=Chenguang%20Huang%20and%20Shengchao%20Yan%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Dynamic%20scene%20understanding%20remains%20a%20persistent%20challenge%20in%20robotic%0Aapplications.%20Early%20dynamic%20mapping%20methods%20focused%20on%20mitigating%20the%20negative%0Ainfluence%20of%20short-term%20dynamic%20objects%20on%20camera%20motion%20estimation%20by%20masking%0Aor%20tracking%20specific%20categories%2C%20which%20often%20fall%20short%20in%20adapting%20to%0Along-term%20scene%20changes.%20Recent%20efforts%20address%20object%20association%20in%20long-term%0Adynamic%20environments%20using%20neural%20networks%20trained%20on%20synthetic%20datasets%2C%20but%0Athey%20still%20rely%20on%20predefined%20object%20shapes%20and%20categories.%20Other%20methods%0Aincorporate%20visual%2C%20geometric%2C%20or%20semantic%20heuristics%20for%20the%20association%20but%0Aoften%20lack%20robustness.%20In%20this%20work%2C%20we%20introduce%20BYE%2C%20a%20class-agnostic%2C%0Aper-scene%20point%20cloud%20encoder%20that%20removes%20the%20need%20for%20predefined%20categories%2C%0Ashape%20priors%2C%20or%20extensive%20association%20datasets.%20Trained%20on%20only%20a%20single%0Asequence%20of%20exploration%20data%2C%20BYE%20can%20efficiently%20perform%20object%20association%20in%0Adynamically%20changing%20scenes.%20We%20further%20propose%20an%20ensembling%20scheme%20combining%0Athe%20semantic%20strengths%20of%20Vision%20Language%20Models%20%28VLMs%29%20with%20the%20scene-specific%0Aexpertise%20of%20BYE%2C%20achieving%20a%207%25%20improvement%20and%20a%2095%25%20success%20rate%20in%20object%0Aassociation%20tasks.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//byencoder.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02449v1&entry.124074799=Read"},
{"title": "BYE: Build Your Encoder with One Sequence of Exploration Data for\n  Long-Term Dynamic Scene Understanding", "author": "Chenguang Huang and Shengchao Yan and Wolfram Burgard", "abstract": "  Dynamic scene understanding remains a persistent challenge in robotic\napplications. Early dynamic mapping methods focused on mitigating the negative\ninfluence of short-term dynamic objects on camera motion estimation by masking\nor tracking specific categories, which often fall short in adapting to\nlong-term scene changes. Recent efforts address object association in long-term\ndynamic environments using neural networks trained on synthetic datasets, but\nthey still rely on predefined object shapes and categories. Other methods\nincorporate visual, geometric, or semantic heuristics for the association but\noften lack robustness. In this work, we introduce BYE, a class-agnostic,\nper-scene point cloud encoder that removes the need for predefined categories,\nshape priors, or extensive association datasets. Trained on only a single\nsequence of exploration data, BYE can efficiently perform object association in\ndynamically changing scenes. We further propose an ensembling scheme combining\nthe semantic strengths of Vision Language Models (VLMs) with the scene-specific\nexpertise of BYE, achieving a 7% improvement and a 95% success rate in object\nassociation tasks. Code and dataset are available at\nhttps://byencoder.github.io.\n", "link": "http://arxiv.org/abs/2412.02449v1", "date": "2024-12-03", "relevancy": 2.6121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6608}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BYE%3A%20Build%20Your%20Encoder%20with%20One%20Sequence%20of%20Exploration%20Data%20for%0A%20%20Long-Term%20Dynamic%20Scene%20Understanding&body=Title%3A%20BYE%3A%20Build%20Your%20Encoder%20with%20One%20Sequence%20of%20Exploration%20Data%20for%0A%20%20Long-Term%20Dynamic%20Scene%20Understanding%0AAuthor%3A%20Chenguang%20Huang%20and%20Shengchao%20Yan%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Dynamic%20scene%20understanding%20remains%20a%20persistent%20challenge%20in%20robotic%0Aapplications.%20Early%20dynamic%20mapping%20methods%20focused%20on%20mitigating%20the%20negative%0Ainfluence%20of%20short-term%20dynamic%20objects%20on%20camera%20motion%20estimation%20by%20masking%0Aor%20tracking%20specific%20categories%2C%20which%20often%20fall%20short%20in%20adapting%20to%0Along-term%20scene%20changes.%20Recent%20efforts%20address%20object%20association%20in%20long-term%0Adynamic%20environments%20using%20neural%20networks%20trained%20on%20synthetic%20datasets%2C%20but%0Athey%20still%20rely%20on%20predefined%20object%20shapes%20and%20categories.%20Other%20methods%0Aincorporate%20visual%2C%20geometric%2C%20or%20semantic%20heuristics%20for%20the%20association%20but%0Aoften%20lack%20robustness.%20In%20this%20work%2C%20we%20introduce%20BYE%2C%20a%20class-agnostic%2C%0Aper-scene%20point%20cloud%20encoder%20that%20removes%20the%20need%20for%20predefined%20categories%2C%0Ashape%20priors%2C%20or%20extensive%20association%20datasets.%20Trained%20on%20only%20a%20single%0Asequence%20of%20exploration%20data%2C%20BYE%20can%20efficiently%20perform%20object%20association%20in%0Adynamically%20changing%20scenes.%20We%20further%20propose%20an%20ensembling%20scheme%20combining%0Athe%20semantic%20strengths%20of%20Vision%20Language%20Models%20%28VLMs%29%20with%20the%20scene-specific%0Aexpertise%20of%20BYE%2C%20achieving%20a%207%25%20improvement%20and%20a%2095%25%20success%20rate%20in%20object%0Aassociation%20tasks.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//byencoder.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBYE%253A%2520Build%2520Your%2520Encoder%2520with%2520One%2520Sequence%2520of%2520Exploration%2520Data%2520for%250A%2520%2520Long-Term%2520Dynamic%2520Scene%2520Understanding%26entry.906535625%3DChenguang%2520Huang%2520and%2520Shengchao%2520Yan%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520Dynamic%2520scene%2520understanding%2520remains%2520a%2520persistent%2520challenge%2520in%2520robotic%250Aapplications.%2520Early%2520dynamic%2520mapping%2520methods%2520focused%2520on%2520mitigating%2520the%2520negative%250Ainfluence%2520of%2520short-term%2520dynamic%2520objects%2520on%2520camera%2520motion%2520estimation%2520by%2520masking%250Aor%2520tracking%2520specific%2520categories%252C%2520which%2520often%2520fall%2520short%2520in%2520adapting%2520to%250Along-term%2520scene%2520changes.%2520Recent%2520efforts%2520address%2520object%2520association%2520in%2520long-term%250Adynamic%2520environments%2520using%2520neural%2520networks%2520trained%2520on%2520synthetic%2520datasets%252C%2520but%250Athey%2520still%2520rely%2520on%2520predefined%2520object%2520shapes%2520and%2520categories.%2520Other%2520methods%250Aincorporate%2520visual%252C%2520geometric%252C%2520or%2520semantic%2520heuristics%2520for%2520the%2520association%2520but%250Aoften%2520lack%2520robustness.%2520In%2520this%2520work%252C%2520we%2520introduce%2520BYE%252C%2520a%2520class-agnostic%252C%250Aper-scene%2520point%2520cloud%2520encoder%2520that%2520removes%2520the%2520need%2520for%2520predefined%2520categories%252C%250Ashape%2520priors%252C%2520or%2520extensive%2520association%2520datasets.%2520Trained%2520on%2520only%2520a%2520single%250Asequence%2520of%2520exploration%2520data%252C%2520BYE%2520can%2520efficiently%2520perform%2520object%2520association%2520in%250Adynamically%2520changing%2520scenes.%2520We%2520further%2520propose%2520an%2520ensembling%2520scheme%2520combining%250Athe%2520semantic%2520strengths%2520of%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520with%2520the%2520scene-specific%250Aexpertise%2520of%2520BYE%252C%2520achieving%2520a%25207%2525%2520improvement%2520and%2520a%252095%2525%2520success%2520rate%2520in%2520object%250Aassociation%2520tasks.%2520Code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//byencoder.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BYE%3A%20Build%20Your%20Encoder%20with%20One%20Sequence%20of%20Exploration%20Data%20for%0A%20%20Long-Term%20Dynamic%20Scene%20Understanding&entry.906535625=Chenguang%20Huang%20and%20Shengchao%20Yan%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Dynamic%20scene%20understanding%20remains%20a%20persistent%20challenge%20in%20robotic%0Aapplications.%20Early%20dynamic%20mapping%20methods%20focused%20on%20mitigating%20the%20negative%0Ainfluence%20of%20short-term%20dynamic%20objects%20on%20camera%20motion%20estimation%20by%20masking%0Aor%20tracking%20specific%20categories%2C%20which%20often%20fall%20short%20in%20adapting%20to%0Along-term%20scene%20changes.%20Recent%20efforts%20address%20object%20association%20in%20long-term%0Adynamic%20environments%20using%20neural%20networks%20trained%20on%20synthetic%20datasets%2C%20but%0Athey%20still%20rely%20on%20predefined%20object%20shapes%20and%20categories.%20Other%20methods%0Aincorporate%20visual%2C%20geometric%2C%20or%20semantic%20heuristics%20for%20the%20association%20but%0Aoften%20lack%20robustness.%20In%20this%20work%2C%20we%20introduce%20BYE%2C%20a%20class-agnostic%2C%0Aper-scene%20point%20cloud%20encoder%20that%20removes%20the%20need%20for%20predefined%20categories%2C%0Ashape%20priors%2C%20or%20extensive%20association%20datasets.%20Trained%20on%20only%20a%20single%0Asequence%20of%20exploration%20data%2C%20BYE%20can%20efficiently%20perform%20object%20association%20in%0Adynamically%20changing%20scenes.%20We%20further%20propose%20an%20ensembling%20scheme%20combining%0Athe%20semantic%20strengths%20of%20Vision%20Language%20Models%20%28VLMs%29%20with%20the%20scene-specific%0Aexpertise%20of%20BYE%2C%20achieving%20a%207%25%20improvement%20and%20a%2095%25%20success%20rate%20in%20object%0Aassociation%20tasks.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//byencoder.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02449v1&entry.124074799=Read"},
{"title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment", "author": "Shaolei Zhang and Kehao Zhang and Qingkai Fang and Shoutao Guo and Yan Zhou and Xiaodong Liu and Yang Feng", "abstract": "  Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.\n", "link": "http://arxiv.org/abs/2411.16300v2", "date": "2024-12-03", "relevancy": 2.5794, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&body=Title%3A%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment%0AAuthor%3A%20Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-2-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayLing%25202%253A%2520A%2520Multilingual%2520Large%2520Language%2520Model%2520with%2520Efficient%2520Language%250A%2520%2520Alignment%26entry.906535625%3DShaolei%2520Zhang%2520and%2520Kehao%2520Zhang%2520and%2520Qingkai%2520Fang%2520and%2520Shoutao%2520Guo%2520and%2520Yan%2520Zhou%2520and%2520Xiaodong%2520Liu%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%2520their%2520powerful%2520generative%2520capabilities%2520and%250Avast%2520knowledge%252C%2520empower%2520various%2520tasks%2520in%2520everyday%2520life.%2520However%252C%2520these%250Aabilities%2520are%2520primarily%2520concentrated%2520in%2520high-resource%2520languages%252C%2520leaving%250Alow-resource%2520languages%2520with%2520weaker%2520generative%2520capabilities%2520and%2520relatively%250Alimited%2520knowledge.%2520Enhancing%2520the%2520multilingual%2520capabilities%2520of%2520LLMs%2520is%2520therefore%250Acrucial%2520for%2520serving%2520over%2520100%2520linguistic%2520communities%2520worldwide.%2520An%2520intuitive%250Aapproach%2520to%2520enhance%2520the%2520multilingual%2520capabilities%2520would%2520be%2520to%2520construct%250Ainstruction%2520data%2520for%2520various%2520languages%252C%2520but%2520constructing%2520instruction%2520data%2520for%250Aover%2520100%2520languages%2520is%2520prohibitively%2520costly.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520BayLing%250A2%252C%2520which%2520efficiently%2520transfers%2520generative%2520capabilities%2520and%2520knowledge%2520from%250Ahigh-resource%2520languages%2520to%2520low-resource%2520languages%2520through%2520language%2520alignment.%250ATo%2520achieve%2520this%252C%2520we%2520constructed%2520a%2520dataset%2520of%25203.2%2520million%2520instructions%252C%250Acomprising%2520high-resource%2520language%2520instructions%2520%2528Chinese%2520and%2520English%2529%2520and%250Across-lingual%2520instructions%2520for%2520100%252B%2520languages%2520and%2520performed%2520instruction%2520tuning%250Abased%2520on%2520the%2520dataset%2520to%2520facilitate%2520the%2520capability%2520transfer%2520between%2520languages.%250AUsing%2520Llama%2520as%2520the%2520foundation%2520model%252C%2520we%2520developed%2520BayLing-2-7B%252C%2520BayLing-2-13B%252C%250Aand%2520BayLing-2-8B%252C%2520and%2520conducted%2520a%2520comprehensive%2520evaluation%2520of%2520BayLing.%2520For%250Amultilingual%2520translation%2520across%2520100%252B%2520languages%252C%2520BayLing%2520shows%2520superior%250Aperformance%2520compared%2520to%2520open-source%2520models%2520of%2520similar%2520scale.%2520For%2520multilingual%250Aknowledge%2520and%2520understanding%2520benchmarks%252C%2520BayLing%2520achieves%2520significant%250Aimprovements%2520across%2520over%252020%2520low-resource%2520languages%252C%2520demonstrating%2520its%250Acapability%2520of%2520effective%2520knowledge%2520transfer%2520from%2520high-resource%2520to%2520low-resource%250Alanguages.%2520Furthermore%252C%2520results%2520on%2520English%2520benchmarks%2520indicate%2520that%2520BayLing%250Amaintains%2520high%2520performance%2520in%2520highresource%2520languages%2520while%2520enhancing%2520the%250Aperformance%2520in%2520low-resource%2520languages.%2520Demo%252C%2520homepage%252C%2520code%2520and%2520models%2520of%250ABayLing%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&entry.906535625=Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-2-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16300v2&entry.124074799=Read"},
{"title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment", "author": "Shaolei Zhang and Kehao Zhang and Qingkai Fang and Shoutao Guo and Yan Zhou and Xiaodong Liu and Yang Feng", "abstract": "  Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.\n", "link": "http://arxiv.org/abs/2411.16300v2", "date": "2024-12-03", "relevancy": 2.5794, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&body=Title%3A%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment%0AAuthor%3A%20Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-2-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayLing%25202%253A%2520A%2520Multilingual%2520Large%2520Language%2520Model%2520with%2520Efficient%2520Language%250A%2520%2520Alignment%26entry.906535625%3DShaolei%2520Zhang%2520and%2520Kehao%2520Zhang%2520and%2520Qingkai%2520Fang%2520and%2520Shoutao%2520Guo%2520and%2520Yan%2520Zhou%2520and%2520Xiaodong%2520Liu%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%2520their%2520powerful%2520generative%2520capabilities%2520and%250Avast%2520knowledge%252C%2520empower%2520various%2520tasks%2520in%2520everyday%2520life.%2520However%252C%2520these%250Aabilities%2520are%2520primarily%2520concentrated%2520in%2520high-resource%2520languages%252C%2520leaving%250Alow-resource%2520languages%2520with%2520weaker%2520generative%2520capabilities%2520and%2520relatively%250Alimited%2520knowledge.%2520Enhancing%2520the%2520multilingual%2520capabilities%2520of%2520LLMs%2520is%2520therefore%250Acrucial%2520for%2520serving%2520over%2520100%2520linguistic%2520communities%2520worldwide.%2520An%2520intuitive%250Aapproach%2520to%2520enhance%2520the%2520multilingual%2520capabilities%2520would%2520be%2520to%2520construct%250Ainstruction%2520data%2520for%2520various%2520languages%252C%2520but%2520constructing%2520instruction%2520data%2520for%250Aover%2520100%2520languages%2520is%2520prohibitively%2520costly.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520BayLing%250A2%252C%2520which%2520efficiently%2520transfers%2520generative%2520capabilities%2520and%2520knowledge%2520from%250Ahigh-resource%2520languages%2520to%2520low-resource%2520languages%2520through%2520language%2520alignment.%250ATo%2520achieve%2520this%252C%2520we%2520constructed%2520a%2520dataset%2520of%25203.2%2520million%2520instructions%252C%250Acomprising%2520high-resource%2520language%2520instructions%2520%2528Chinese%2520and%2520English%2529%2520and%250Across-lingual%2520instructions%2520for%2520100%252B%2520languages%2520and%2520performed%2520instruction%2520tuning%250Abased%2520on%2520the%2520dataset%2520to%2520facilitate%2520the%2520capability%2520transfer%2520between%2520languages.%250AUsing%2520Llama%2520as%2520the%2520foundation%2520model%252C%2520we%2520developed%2520BayLing-2-7B%252C%2520BayLing-2-13B%252C%250Aand%2520BayLing-2-8B%252C%2520and%2520conducted%2520a%2520comprehensive%2520evaluation%2520of%2520BayLing.%2520For%250Amultilingual%2520translation%2520across%2520100%252B%2520languages%252C%2520BayLing%2520shows%2520superior%250Aperformance%2520compared%2520to%2520open-source%2520models%2520of%2520similar%2520scale.%2520For%2520multilingual%250Aknowledge%2520and%2520understanding%2520benchmarks%252C%2520BayLing%2520achieves%2520significant%250Aimprovements%2520across%2520over%252020%2520low-resource%2520languages%252C%2520demonstrating%2520its%250Acapability%2520of%2520effective%2520knowledge%2520transfer%2520from%2520high-resource%2520to%2520low-resource%250Alanguages.%2520Furthermore%252C%2520results%2520on%2520English%2520benchmarks%2520indicate%2520that%2520BayLing%250Amaintains%2520high%2520performance%2520in%2520highresource%2520languages%2520while%2520enhancing%2520the%250Aperformance%2520in%2520low-resource%2520languages.%2520Demo%252C%2520homepage%252C%2520code%2520and%2520models%2520of%250ABayLing%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&entry.906535625=Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-2-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16300v2&entry.124074799=Read"},
{"title": "Multi-Class Abnormality Classification Task in Video Capsule Endoscopy", "author": "Dev Rishi Verma and Vibhor Saxena and Dhruv Sharma and Arpan Gupta", "abstract": "  In this work for Capsule Vision Challenge 2024, we addressed the challenge of\nmulticlass anomaly classification in video capsule Endoscopy (VCE)[1] with a\nvariety of deep learning models, ranging from custom CNNs to advanced\ntransformer architectures. The purpose is to correctly classify diverse\ngastrointestinal disorders, which is critical for increasing diagnostic\nefficiency in clinical settings. We started with a baseline CNN model and\nimproved performance with ResNet[2] for better feature extraction, followed by\nVision Transformer (ViT)[3] to capture global dependencies. We further improve\nthe results by using Multiscale Vision Transformer (MViT)[4] for improved\nhierarchical feature extraction, while Dual Attention Vision Transformer\n(DaViT) [5] delivered best results by combining spatial and channel attention\nmethods. Our best balanced accuracy on validation set [6] was 0.8592 and Mean\nAUC was 0.9932. This methodology enabled us to improve model accuracy across a\nwide range of criteria, greatly surpassing all other methods.Additionally, our\nteam capsule commandos achieved 7th place ranking with a test set[7]\nperformance of Mean AUC: 0.7314 and balanced accuracy: 0.3235\n", "link": "http://arxiv.org/abs/2410.19973v3", "date": "2024-12-03", "relevancy": 2.5358, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5117}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Class%20Abnormality%20Classification%20Task%20in%20Video%20Capsule%20Endoscopy&body=Title%3A%20Multi-Class%20Abnormality%20Classification%20Task%20in%20Video%20Capsule%20Endoscopy%0AAuthor%3A%20Dev%20Rishi%20Verma%20and%20Vibhor%20Saxena%20and%20Dhruv%20Sharma%20and%20Arpan%20Gupta%0AAbstract%3A%20%20%20In%20this%20work%20for%20Capsule%20Vision%20Challenge%202024%2C%20we%20addressed%20the%20challenge%20of%0Amulticlass%20anomaly%20classification%20in%20video%20capsule%20Endoscopy%20%28VCE%29%5B1%5D%20with%20a%0Avariety%20of%20deep%20learning%20models%2C%20ranging%20from%20custom%20CNNs%20to%20advanced%0Atransformer%20architectures.%20The%20purpose%20is%20to%20correctly%20classify%20diverse%0Agastrointestinal%20disorders%2C%20which%20is%20critical%20for%20increasing%20diagnostic%0Aefficiency%20in%20clinical%20settings.%20We%20started%20with%20a%20baseline%20CNN%20model%20and%0Aimproved%20performance%20with%20ResNet%5B2%5D%20for%20better%20feature%20extraction%2C%20followed%20by%0AVision%20Transformer%20%28ViT%29%5B3%5D%20to%20capture%20global%20dependencies.%20We%20further%20improve%0Athe%20results%20by%20using%20Multiscale%20Vision%20Transformer%20%28MViT%29%5B4%5D%20for%20improved%0Ahierarchical%20feature%20extraction%2C%20while%20Dual%20Attention%20Vision%20Transformer%0A%28DaViT%29%20%5B5%5D%20delivered%20best%20results%20by%20combining%20spatial%20and%20channel%20attention%0Amethods.%20Our%20best%20balanced%20accuracy%20on%20validation%20set%20%5B6%5D%20was%200.8592%20and%20Mean%0AAUC%20was%200.9932.%20This%20methodology%20enabled%20us%20to%20improve%20model%20accuracy%20across%20a%0Awide%20range%20of%20criteria%2C%20greatly%20surpassing%20all%20other%20methods.Additionally%2C%20our%0Ateam%20capsule%20commandos%20achieved%207th%20place%20ranking%20with%20a%20test%20set%5B7%5D%0Aperformance%20of%20Mean%20AUC%3A%200.7314%20and%20balanced%20accuracy%3A%200.3235%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19973v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Class%2520Abnormality%2520Classification%2520Task%2520in%2520Video%2520Capsule%2520Endoscopy%26entry.906535625%3DDev%2520Rishi%2520Verma%2520and%2520Vibhor%2520Saxena%2520and%2520Dhruv%2520Sharma%2520and%2520Arpan%2520Gupta%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520for%2520Capsule%2520Vision%2520Challenge%25202024%252C%2520we%2520addressed%2520the%2520challenge%2520of%250Amulticlass%2520anomaly%2520classification%2520in%2520video%2520capsule%2520Endoscopy%2520%2528VCE%2529%255B1%255D%2520with%2520a%250Avariety%2520of%2520deep%2520learning%2520models%252C%2520ranging%2520from%2520custom%2520CNNs%2520to%2520advanced%250Atransformer%2520architectures.%2520The%2520purpose%2520is%2520to%2520correctly%2520classify%2520diverse%250Agastrointestinal%2520disorders%252C%2520which%2520is%2520critical%2520for%2520increasing%2520diagnostic%250Aefficiency%2520in%2520clinical%2520settings.%2520We%2520started%2520with%2520a%2520baseline%2520CNN%2520model%2520and%250Aimproved%2520performance%2520with%2520ResNet%255B2%255D%2520for%2520better%2520feature%2520extraction%252C%2520followed%2520by%250AVision%2520Transformer%2520%2528ViT%2529%255B3%255D%2520to%2520capture%2520global%2520dependencies.%2520We%2520further%2520improve%250Athe%2520results%2520by%2520using%2520Multiscale%2520Vision%2520Transformer%2520%2528MViT%2529%255B4%255D%2520for%2520improved%250Ahierarchical%2520feature%2520extraction%252C%2520while%2520Dual%2520Attention%2520Vision%2520Transformer%250A%2528DaViT%2529%2520%255B5%255D%2520delivered%2520best%2520results%2520by%2520combining%2520spatial%2520and%2520channel%2520attention%250Amethods.%2520Our%2520best%2520balanced%2520accuracy%2520on%2520validation%2520set%2520%255B6%255D%2520was%25200.8592%2520and%2520Mean%250AAUC%2520was%25200.9932.%2520This%2520methodology%2520enabled%2520us%2520to%2520improve%2520model%2520accuracy%2520across%2520a%250Awide%2520range%2520of%2520criteria%252C%2520greatly%2520surpassing%2520all%2520other%2520methods.Additionally%252C%2520our%250Ateam%2520capsule%2520commandos%2520achieved%25207th%2520place%2520ranking%2520with%2520a%2520test%2520set%255B7%255D%250Aperformance%2520of%2520Mean%2520AUC%253A%25200.7314%2520and%2520balanced%2520accuracy%253A%25200.3235%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19973v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Class%20Abnormality%20Classification%20Task%20in%20Video%20Capsule%20Endoscopy&entry.906535625=Dev%20Rishi%20Verma%20and%20Vibhor%20Saxena%20and%20Dhruv%20Sharma%20and%20Arpan%20Gupta&entry.1292438233=%20%20In%20this%20work%20for%20Capsule%20Vision%20Challenge%202024%2C%20we%20addressed%20the%20challenge%20of%0Amulticlass%20anomaly%20classification%20in%20video%20capsule%20Endoscopy%20%28VCE%29%5B1%5D%20with%20a%0Avariety%20of%20deep%20learning%20models%2C%20ranging%20from%20custom%20CNNs%20to%20advanced%0Atransformer%20architectures.%20The%20purpose%20is%20to%20correctly%20classify%20diverse%0Agastrointestinal%20disorders%2C%20which%20is%20critical%20for%20increasing%20diagnostic%0Aefficiency%20in%20clinical%20settings.%20We%20started%20with%20a%20baseline%20CNN%20model%20and%0Aimproved%20performance%20with%20ResNet%5B2%5D%20for%20better%20feature%20extraction%2C%20followed%20by%0AVision%20Transformer%20%28ViT%29%5B3%5D%20to%20capture%20global%20dependencies.%20We%20further%20improve%0Athe%20results%20by%20using%20Multiscale%20Vision%20Transformer%20%28MViT%29%5B4%5D%20for%20improved%0Ahierarchical%20feature%20extraction%2C%20while%20Dual%20Attention%20Vision%20Transformer%0A%28DaViT%29%20%5B5%5D%20delivered%20best%20results%20by%20combining%20spatial%20and%20channel%20attention%0Amethods.%20Our%20best%20balanced%20accuracy%20on%20validation%20set%20%5B6%5D%20was%200.8592%20and%20Mean%0AAUC%20was%200.9932.%20This%20methodology%20enabled%20us%20to%20improve%20model%20accuracy%20across%20a%0Awide%20range%20of%20criteria%2C%20greatly%20surpassing%20all%20other%20methods.Additionally%2C%20our%0Ateam%20capsule%20commandos%20achieved%207th%20place%20ranking%20with%20a%20test%20set%5B7%5D%0Aperformance%20of%20Mean%20AUC%3A%200.7314%20and%20balanced%20accuracy%3A%200.3235%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19973v3&entry.124074799=Read"},
{"title": "Tomographic SAR Reconstruction for Forest Height Estimation", "author": "Grace Colverd and Jumpei Takami and Laura Schade and Karol Bot and Joseph A. Gallego-Mejia", "abstract": "  Tree height estimation serves as an important proxy for biomass estimation in\necological and forestry applications. While traditional methods such as\nphotogrammetry and Light Detection and Ranging (LiDAR) offer accurate height\nmeasurements, their application on a global scale is often cost-prohibitive and\nlogistically challenging. In contrast, remote sensing techniques, particularly\n3D tomographic reconstruction from Synthetic Aperture Radar (SAR) imagery,\nprovide a scalable solution for global height estimation. SAR images have been\nused in earth observation contexts due to their ability to work in all\nweathers, unobscured by clouds. In this study, we use deep learning to estimate\nforest canopy height directly from 2D Single Look Complex (SLC) images, a\nderivative of SAR. Our method attempts to bypass traditional tomographic signal\nprocessing, potentially reducing latency from SAR capture to end product. We\nalso quantify the impact of varying numbers of SLC images on height estimation\naccuracy, aiming to inform future satellite operations and optimize data\ncollection strategies. Compared to full tomographic processing combined with\ndeep learning, our minimal method (partial processing + deep learning) falls\nshort, with an error 16-21\\% higher, highlighting the continuing relevance of\ngeometric signal processing.\n", "link": "http://arxiv.org/abs/2412.00903v2", "date": "2024-12-03", "relevancy": 2.506, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5482}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4861}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tomographic%20SAR%20Reconstruction%20for%20Forest%20Height%20Estimation&body=Title%3A%20Tomographic%20SAR%20Reconstruction%20for%20Forest%20Height%20Estimation%0AAuthor%3A%20Grace%20Colverd%20and%20Jumpei%20Takami%20and%20Laura%20Schade%20and%20Karol%20Bot%20and%20Joseph%20A.%20Gallego-Mejia%0AAbstract%3A%20%20%20Tree%20height%20estimation%20serves%20as%20an%20important%20proxy%20for%20biomass%20estimation%20in%0Aecological%20and%20forestry%20applications.%20While%20traditional%20methods%20such%20as%0Aphotogrammetry%20and%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20offer%20accurate%20height%0Ameasurements%2C%20their%20application%20on%20a%20global%20scale%20is%20often%20cost-prohibitive%20and%0Alogistically%20challenging.%20In%20contrast%2C%20remote%20sensing%20techniques%2C%20particularly%0A3D%20tomographic%20reconstruction%20from%20Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%2C%0Aprovide%20a%20scalable%20solution%20for%20global%20height%20estimation.%20SAR%20images%20have%20been%0Aused%20in%20earth%20observation%20contexts%20due%20to%20their%20ability%20to%20work%20in%20all%0Aweathers%2C%20unobscured%20by%20clouds.%20In%20this%20study%2C%20we%20use%20deep%20learning%20to%20estimate%0Aforest%20canopy%20height%20directly%20from%202D%20Single%20Look%20Complex%20%28SLC%29%20images%2C%20a%0Aderivative%20of%20SAR.%20Our%20method%20attempts%20to%20bypass%20traditional%20tomographic%20signal%0Aprocessing%2C%20potentially%20reducing%20latency%20from%20SAR%20capture%20to%20end%20product.%20We%0Aalso%20quantify%20the%20impact%20of%20varying%20numbers%20of%20SLC%20images%20on%20height%20estimation%0Aaccuracy%2C%20aiming%20to%20inform%20future%20satellite%20operations%20and%20optimize%20data%0Acollection%20strategies.%20Compared%20to%20full%20tomographic%20processing%20combined%20with%0Adeep%20learning%2C%20our%20minimal%20method%20%28partial%20processing%20%2B%20deep%20learning%29%20falls%0Ashort%2C%20with%20an%20error%2016-21%5C%25%20higher%2C%20highlighting%20the%20continuing%20relevance%20of%0Ageometric%20signal%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00903v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTomographic%2520SAR%2520Reconstruction%2520for%2520Forest%2520Height%2520Estimation%26entry.906535625%3DGrace%2520Colverd%2520and%2520Jumpei%2520Takami%2520and%2520Laura%2520Schade%2520and%2520Karol%2520Bot%2520and%2520Joseph%2520A.%2520Gallego-Mejia%26entry.1292438233%3D%2520%2520Tree%2520height%2520estimation%2520serves%2520as%2520an%2520important%2520proxy%2520for%2520biomass%2520estimation%2520in%250Aecological%2520and%2520forestry%2520applications.%2520While%2520traditional%2520methods%2520such%2520as%250Aphotogrammetry%2520and%2520Light%2520Detection%2520and%2520Ranging%2520%2528LiDAR%2529%2520offer%2520accurate%2520height%250Ameasurements%252C%2520their%2520application%2520on%2520a%2520global%2520scale%2520is%2520often%2520cost-prohibitive%2520and%250Alogistically%2520challenging.%2520In%2520contrast%252C%2520remote%2520sensing%2520techniques%252C%2520particularly%250A3D%2520tomographic%2520reconstruction%2520from%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520imagery%252C%250Aprovide%2520a%2520scalable%2520solution%2520for%2520global%2520height%2520estimation.%2520SAR%2520images%2520have%2520been%250Aused%2520in%2520earth%2520observation%2520contexts%2520due%2520to%2520their%2520ability%2520to%2520work%2520in%2520all%250Aweathers%252C%2520unobscured%2520by%2520clouds.%2520In%2520this%2520study%252C%2520we%2520use%2520deep%2520learning%2520to%2520estimate%250Aforest%2520canopy%2520height%2520directly%2520from%25202D%2520Single%2520Look%2520Complex%2520%2528SLC%2529%2520images%252C%2520a%250Aderivative%2520of%2520SAR.%2520Our%2520method%2520attempts%2520to%2520bypass%2520traditional%2520tomographic%2520signal%250Aprocessing%252C%2520potentially%2520reducing%2520latency%2520from%2520SAR%2520capture%2520to%2520end%2520product.%2520We%250Aalso%2520quantify%2520the%2520impact%2520of%2520varying%2520numbers%2520of%2520SLC%2520images%2520on%2520height%2520estimation%250Aaccuracy%252C%2520aiming%2520to%2520inform%2520future%2520satellite%2520operations%2520and%2520optimize%2520data%250Acollection%2520strategies.%2520Compared%2520to%2520full%2520tomographic%2520processing%2520combined%2520with%250Adeep%2520learning%252C%2520our%2520minimal%2520method%2520%2528partial%2520processing%2520%252B%2520deep%2520learning%2529%2520falls%250Ashort%252C%2520with%2520an%2520error%252016-21%255C%2525%2520higher%252C%2520highlighting%2520the%2520continuing%2520relevance%2520of%250Ageometric%2520signal%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00903v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tomographic%20SAR%20Reconstruction%20for%20Forest%20Height%20Estimation&entry.906535625=Grace%20Colverd%20and%20Jumpei%20Takami%20and%20Laura%20Schade%20and%20Karol%20Bot%20and%20Joseph%20A.%20Gallego-Mejia&entry.1292438233=%20%20Tree%20height%20estimation%20serves%20as%20an%20important%20proxy%20for%20biomass%20estimation%20in%0Aecological%20and%20forestry%20applications.%20While%20traditional%20methods%20such%20as%0Aphotogrammetry%20and%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20offer%20accurate%20height%0Ameasurements%2C%20their%20application%20on%20a%20global%20scale%20is%20often%20cost-prohibitive%20and%0Alogistically%20challenging.%20In%20contrast%2C%20remote%20sensing%20techniques%2C%20particularly%0A3D%20tomographic%20reconstruction%20from%20Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%2C%0Aprovide%20a%20scalable%20solution%20for%20global%20height%20estimation.%20SAR%20images%20have%20been%0Aused%20in%20earth%20observation%20contexts%20due%20to%20their%20ability%20to%20work%20in%20all%0Aweathers%2C%20unobscured%20by%20clouds.%20In%20this%20study%2C%20we%20use%20deep%20learning%20to%20estimate%0Aforest%20canopy%20height%20directly%20from%202D%20Single%20Look%20Complex%20%28SLC%29%20images%2C%20a%0Aderivative%20of%20SAR.%20Our%20method%20attempts%20to%20bypass%20traditional%20tomographic%20signal%0Aprocessing%2C%20potentially%20reducing%20latency%20from%20SAR%20capture%20to%20end%20product.%20We%0Aalso%20quantify%20the%20impact%20of%20varying%20numbers%20of%20SLC%20images%20on%20height%20estimation%0Aaccuracy%2C%20aiming%20to%20inform%20future%20satellite%20operations%20and%20optimize%20data%0Acollection%20strategies.%20Compared%20to%20full%20tomographic%20processing%20combined%20with%0Adeep%20learning%2C%20our%20minimal%20method%20%28partial%20processing%20%2B%20deep%20learning%29%20falls%0Ashort%2C%20with%20an%20error%2016-21%5C%25%20higher%2C%20highlighting%20the%20continuing%20relevance%20of%0Ageometric%20signal%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00903v2&entry.124074799=Read"},
{"title": "SceneFactor: Factored Latent 3D Diffusion for Controllable 3D Scene\n  Generation", "author": "Alexey Bokhovkin and Quan Meng and Shubham Tulsiani and Angela Dai", "abstract": "  We present SceneFactor, a diffusion-based approach for large-scale 3D scene\ngeneration that enables controllable generation and effortless editing.\nSceneFactor enables text-guided 3D scene synthesis through our factored\ndiffusion formulation, leveraging latent semantic and geometric manifolds for\ngeneration of arbitrary-sized 3D scenes. While text input enables easy,\ncontrollable generation, text guidance remains imprecise for intuitive,\nlocalized editing and manipulation of the generated 3D scenes. Our factored\nsemantic diffusion generates a proxy semantic space composed of semantic 3D\nboxes that enables controllable editing of generated scenes by adding,\nremoving, changing the size of the semantic 3D proxy boxes that guides\nhigh-fidelity, consistent 3D geometric editing. Extensive experiments\ndemonstrate that our approach enables high-fidelity 3D scene synthesis with\neffective controllable editing through our factored diffusion approach.\n", "link": "http://arxiv.org/abs/2412.01801v2", "date": "2024-12-03", "relevancy": 2.5035, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6347}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6347}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneFactor%3A%20Factored%20Latent%203D%20Diffusion%20for%20Controllable%203D%20Scene%0A%20%20Generation&body=Title%3A%20SceneFactor%3A%20Factored%20Latent%203D%20Diffusion%20for%20Controllable%203D%20Scene%0A%20%20Generation%0AAuthor%3A%20Alexey%20Bokhovkin%20and%20Quan%20Meng%20and%20Shubham%20Tulsiani%20and%20Angela%20Dai%0AAbstract%3A%20%20%20We%20present%20SceneFactor%2C%20a%20diffusion-based%20approach%20for%20large-scale%203D%20scene%0Ageneration%20that%20enables%20controllable%20generation%20and%20effortless%20editing.%0ASceneFactor%20enables%20text-guided%203D%20scene%20synthesis%20through%20our%20factored%0Adiffusion%20formulation%2C%20leveraging%20latent%20semantic%20and%20geometric%20manifolds%20for%0Ageneration%20of%20arbitrary-sized%203D%20scenes.%20While%20text%20input%20enables%20easy%2C%0Acontrollable%20generation%2C%20text%20guidance%20remains%20imprecise%20for%20intuitive%2C%0Alocalized%20editing%20and%20manipulation%20of%20the%20generated%203D%20scenes.%20Our%20factored%0Asemantic%20diffusion%20generates%20a%20proxy%20semantic%20space%20composed%20of%20semantic%203D%0Aboxes%20that%20enables%20controllable%20editing%20of%20generated%20scenes%20by%20adding%2C%0Aremoving%2C%20changing%20the%20size%20of%20the%20semantic%203D%20proxy%20boxes%20that%20guides%0Ahigh-fidelity%2C%20consistent%203D%20geometric%20editing.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20enables%20high-fidelity%203D%20scene%20synthesis%20with%0Aeffective%20controllable%20editing%20through%20our%20factored%20diffusion%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneFactor%253A%2520Factored%2520Latent%25203D%2520Diffusion%2520for%2520Controllable%25203D%2520Scene%250A%2520%2520Generation%26entry.906535625%3DAlexey%2520Bokhovkin%2520and%2520Quan%2520Meng%2520and%2520Shubham%2520Tulsiani%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520We%2520present%2520SceneFactor%252C%2520a%2520diffusion-based%2520approach%2520for%2520large-scale%25203D%2520scene%250Ageneration%2520that%2520enables%2520controllable%2520generation%2520and%2520effortless%2520editing.%250ASceneFactor%2520enables%2520text-guided%25203D%2520scene%2520synthesis%2520through%2520our%2520factored%250Adiffusion%2520formulation%252C%2520leveraging%2520latent%2520semantic%2520and%2520geometric%2520manifolds%2520for%250Ageneration%2520of%2520arbitrary-sized%25203D%2520scenes.%2520While%2520text%2520input%2520enables%2520easy%252C%250Acontrollable%2520generation%252C%2520text%2520guidance%2520remains%2520imprecise%2520for%2520intuitive%252C%250Alocalized%2520editing%2520and%2520manipulation%2520of%2520the%2520generated%25203D%2520scenes.%2520Our%2520factored%250Asemantic%2520diffusion%2520generates%2520a%2520proxy%2520semantic%2520space%2520composed%2520of%2520semantic%25203D%250Aboxes%2520that%2520enables%2520controllable%2520editing%2520of%2520generated%2520scenes%2520by%2520adding%252C%250Aremoving%252C%2520changing%2520the%2520size%2520of%2520the%2520semantic%25203D%2520proxy%2520boxes%2520that%2520guides%250Ahigh-fidelity%252C%2520consistent%25203D%2520geometric%2520editing.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520approach%2520enables%2520high-fidelity%25203D%2520scene%2520synthesis%2520with%250Aeffective%2520controllable%2520editing%2520through%2520our%2520factored%2520diffusion%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneFactor%3A%20Factored%20Latent%203D%20Diffusion%20for%20Controllable%203D%20Scene%0A%20%20Generation&entry.906535625=Alexey%20Bokhovkin%20and%20Quan%20Meng%20and%20Shubham%20Tulsiani%20and%20Angela%20Dai&entry.1292438233=%20%20We%20present%20SceneFactor%2C%20a%20diffusion-based%20approach%20for%20large-scale%203D%20scene%0Ageneration%20that%20enables%20controllable%20generation%20and%20effortless%20editing.%0ASceneFactor%20enables%20text-guided%203D%20scene%20synthesis%20through%20our%20factored%0Adiffusion%20formulation%2C%20leveraging%20latent%20semantic%20and%20geometric%20manifolds%20for%0Ageneration%20of%20arbitrary-sized%203D%20scenes.%20While%20text%20input%20enables%20easy%2C%0Acontrollable%20generation%2C%20text%20guidance%20remains%20imprecise%20for%20intuitive%2C%0Alocalized%20editing%20and%20manipulation%20of%20the%20generated%203D%20scenes.%20Our%20factored%0Asemantic%20diffusion%20generates%20a%20proxy%20semantic%20space%20composed%20of%20semantic%203D%0Aboxes%20that%20enables%20controllable%20editing%20of%20generated%20scenes%20by%20adding%2C%0Aremoving%2C%20changing%20the%20size%20of%20the%20semantic%203D%20proxy%20boxes%20that%20guides%0Ahigh-fidelity%2C%20consistent%203D%20geometric%20editing.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20enables%20high-fidelity%203D%20scene%20synthesis%20with%0Aeffective%20controllable%20editing%20through%20our%20factored%20diffusion%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01801v2&entry.124074799=Read"},
{"title": "SceneFactor: Factored Latent 3D Diffusion for Controllable 3D Scene\n  Generation", "author": "Alexey Bokhovkin and Quan Meng and Shubham Tulsiani and Angela Dai", "abstract": "  We present SceneFactor, a diffusion-based approach for large-scale 3D scene\ngeneration that enables controllable generation and effortless editing.\nSceneFactor enables text-guided 3D scene synthesis through our factored\ndiffusion formulation, leveraging latent semantic and geometric manifolds for\ngeneration of arbitrary-sized 3D scenes. While text input enables easy,\ncontrollable generation, text guidance remains imprecise for intuitive,\nlocalized editing and manipulation of the generated 3D scenes. Our factored\nsemantic diffusion generates a proxy semantic space composed of semantic 3D\nboxes that enables controllable editing of generated scenes by adding,\nremoving, changing the size of the semantic 3D proxy boxes that guides\nhigh-fidelity, consistent 3D geometric editing. Extensive experiments\ndemonstrate that our approach enables high-fidelity 3D scene synthesis with\neffective controllable editing through our factored diffusion approach.\n", "link": "http://arxiv.org/abs/2412.01801v2", "date": "2024-12-03", "relevancy": 2.5035, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6347}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6347}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneFactor%3A%20Factored%20Latent%203D%20Diffusion%20for%20Controllable%203D%20Scene%0A%20%20Generation&body=Title%3A%20SceneFactor%3A%20Factored%20Latent%203D%20Diffusion%20for%20Controllable%203D%20Scene%0A%20%20Generation%0AAuthor%3A%20Alexey%20Bokhovkin%20and%20Quan%20Meng%20and%20Shubham%20Tulsiani%20and%20Angela%20Dai%0AAbstract%3A%20%20%20We%20present%20SceneFactor%2C%20a%20diffusion-based%20approach%20for%20large-scale%203D%20scene%0Ageneration%20that%20enables%20controllable%20generation%20and%20effortless%20editing.%0ASceneFactor%20enables%20text-guided%203D%20scene%20synthesis%20through%20our%20factored%0Adiffusion%20formulation%2C%20leveraging%20latent%20semantic%20and%20geometric%20manifolds%20for%0Ageneration%20of%20arbitrary-sized%203D%20scenes.%20While%20text%20input%20enables%20easy%2C%0Acontrollable%20generation%2C%20text%20guidance%20remains%20imprecise%20for%20intuitive%2C%0Alocalized%20editing%20and%20manipulation%20of%20the%20generated%203D%20scenes.%20Our%20factored%0Asemantic%20diffusion%20generates%20a%20proxy%20semantic%20space%20composed%20of%20semantic%203D%0Aboxes%20that%20enables%20controllable%20editing%20of%20generated%20scenes%20by%20adding%2C%0Aremoving%2C%20changing%20the%20size%20of%20the%20semantic%203D%20proxy%20boxes%20that%20guides%0Ahigh-fidelity%2C%20consistent%203D%20geometric%20editing.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20enables%20high-fidelity%203D%20scene%20synthesis%20with%0Aeffective%20controllable%20editing%20through%20our%20factored%20diffusion%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneFactor%253A%2520Factored%2520Latent%25203D%2520Diffusion%2520for%2520Controllable%25203D%2520Scene%250A%2520%2520Generation%26entry.906535625%3DAlexey%2520Bokhovkin%2520and%2520Quan%2520Meng%2520and%2520Shubham%2520Tulsiani%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520We%2520present%2520SceneFactor%252C%2520a%2520diffusion-based%2520approach%2520for%2520large-scale%25203D%2520scene%250Ageneration%2520that%2520enables%2520controllable%2520generation%2520and%2520effortless%2520editing.%250ASceneFactor%2520enables%2520text-guided%25203D%2520scene%2520synthesis%2520through%2520our%2520factored%250Adiffusion%2520formulation%252C%2520leveraging%2520latent%2520semantic%2520and%2520geometric%2520manifolds%2520for%250Ageneration%2520of%2520arbitrary-sized%25203D%2520scenes.%2520While%2520text%2520input%2520enables%2520easy%252C%250Acontrollable%2520generation%252C%2520text%2520guidance%2520remains%2520imprecise%2520for%2520intuitive%252C%250Alocalized%2520editing%2520and%2520manipulation%2520of%2520the%2520generated%25203D%2520scenes.%2520Our%2520factored%250Asemantic%2520diffusion%2520generates%2520a%2520proxy%2520semantic%2520space%2520composed%2520of%2520semantic%25203D%250Aboxes%2520that%2520enables%2520controllable%2520editing%2520of%2520generated%2520scenes%2520by%2520adding%252C%250Aremoving%252C%2520changing%2520the%2520size%2520of%2520the%2520semantic%25203D%2520proxy%2520boxes%2520that%2520guides%250Ahigh-fidelity%252C%2520consistent%25203D%2520geometric%2520editing.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520approach%2520enables%2520high-fidelity%25203D%2520scene%2520synthesis%2520with%250Aeffective%2520controllable%2520editing%2520through%2520our%2520factored%2520diffusion%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneFactor%3A%20Factored%20Latent%203D%20Diffusion%20for%20Controllable%203D%20Scene%0A%20%20Generation&entry.906535625=Alexey%20Bokhovkin%20and%20Quan%20Meng%20and%20Shubham%20Tulsiani%20and%20Angela%20Dai&entry.1292438233=%20%20We%20present%20SceneFactor%2C%20a%20diffusion-based%20approach%20for%20large-scale%203D%20scene%0Ageneration%20that%20enables%20controllable%20generation%20and%20effortless%20editing.%0ASceneFactor%20enables%20text-guided%203D%20scene%20synthesis%20through%20our%20factored%0Adiffusion%20formulation%2C%20leveraging%20latent%20semantic%20and%20geometric%20manifolds%20for%0Ageneration%20of%20arbitrary-sized%203D%20scenes.%20While%20text%20input%20enables%20easy%2C%0Acontrollable%20generation%2C%20text%20guidance%20remains%20imprecise%20for%20intuitive%2C%0Alocalized%20editing%20and%20manipulation%20of%20the%20generated%203D%20scenes.%20Our%20factored%0Asemantic%20diffusion%20generates%20a%20proxy%20semantic%20space%20composed%20of%20semantic%203D%0Aboxes%20that%20enables%20controllable%20editing%20of%20generated%20scenes%20by%20adding%2C%0Aremoving%2C%20changing%20the%20size%20of%20the%20semantic%203D%20proxy%20boxes%20that%20guides%0Ahigh-fidelity%2C%20consistent%203D%20geometric%20editing.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20enables%20high-fidelity%203D%20scene%20synthesis%20with%0Aeffective%20controllable%20editing%20through%20our%20factored%20diffusion%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01801v2&entry.124074799=Read"},
{"title": "Diffusion Models with Anisotropic Gaussian Splatting for Image\n  Inpainting", "author": "Jacob Fein-Ashley and Benjamin Fein-Ashley", "abstract": "  Image inpainting is a fundamental task in computer vision, aiming to restore\nmissing or corrupted regions in images realistically. While recent deep\nlearning approaches have significantly advanced the state-of-the-art,\nchallenges remain in maintaining structural continuity and generating coherent\ntextures, particularly in large missing areas. Diffusion models have shown\npromise in generating high-fidelity images but often lack the structural\nguidance necessary for realistic inpainting. We propose a novel inpainting\nmethod that combines diffusion models with anisotropic Gaussian splatting to\ncapture both local structures and global context effectively. By modeling\nmissing regions using anisotropic Gaussian functions that adapt to local image\ngradients, our approach provides structural guidance to the diffusion-based\ninpainting network. The Gaussian splat maps are integrated into the diffusion\nprocess, enhancing the model's ability to generate high-fidelity and\nstructurally coherent inpainting results. Extensive experiments demonstrate\nthat our method outperforms state-of-the-art techniques, producing visually\nplausible results with enhanced structural integrity and texture realism.\n", "link": "http://arxiv.org/abs/2412.01682v2", "date": "2024-12-03", "relevancy": 2.5034, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.653}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6145}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Models%20with%20Anisotropic%20Gaussian%20Splatting%20for%20Image%0A%20%20Inpainting&body=Title%3A%20Diffusion%20Models%20with%20Anisotropic%20Gaussian%20Splatting%20for%20Image%0A%20%20Inpainting%0AAuthor%3A%20Jacob%20Fein-Ashley%20and%20Benjamin%20Fein-Ashley%0AAbstract%3A%20%20%20Image%20inpainting%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20aiming%20to%20restore%0Amissing%20or%20corrupted%20regions%20in%20images%20realistically.%20While%20recent%20deep%0Alearning%20approaches%20have%20significantly%20advanced%20the%20state-of-the-art%2C%0Achallenges%20remain%20in%20maintaining%20structural%20continuity%20and%20generating%20coherent%0Atextures%2C%20particularly%20in%20large%20missing%20areas.%20Diffusion%20models%20have%20shown%0Apromise%20in%20generating%20high-fidelity%20images%20but%20often%20lack%20the%20structural%0Aguidance%20necessary%20for%20realistic%20inpainting.%20We%20propose%20a%20novel%20inpainting%0Amethod%20that%20combines%20diffusion%20models%20with%20anisotropic%20Gaussian%20splatting%20to%0Acapture%20both%20local%20structures%20and%20global%20context%20effectively.%20By%20modeling%0Amissing%20regions%20using%20anisotropic%20Gaussian%20functions%20that%20adapt%20to%20local%20image%0Agradients%2C%20our%20approach%20provides%20structural%20guidance%20to%20the%20diffusion-based%0Ainpainting%20network.%20The%20Gaussian%20splat%20maps%20are%20integrated%20into%20the%20diffusion%0Aprocess%2C%20enhancing%20the%20model%27s%20ability%20to%20generate%20high-fidelity%20and%0Astructurally%20coherent%20inpainting%20results.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20outperforms%20state-of-the-art%20techniques%2C%20producing%20visually%0Aplausible%20results%20with%20enhanced%20structural%20integrity%20and%20texture%20realism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Models%2520with%2520Anisotropic%2520Gaussian%2520Splatting%2520for%2520Image%250A%2520%2520Inpainting%26entry.906535625%3DJacob%2520Fein-Ashley%2520and%2520Benjamin%2520Fein-Ashley%26entry.1292438233%3D%2520%2520Image%2520inpainting%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520aiming%2520to%2520restore%250Amissing%2520or%2520corrupted%2520regions%2520in%2520images%2520realistically.%2520While%2520recent%2520deep%250Alearning%2520approaches%2520have%2520significantly%2520advanced%2520the%2520state-of-the-art%252C%250Achallenges%2520remain%2520in%2520maintaining%2520structural%2520continuity%2520and%2520generating%2520coherent%250Atextures%252C%2520particularly%2520in%2520large%2520missing%2520areas.%2520Diffusion%2520models%2520have%2520shown%250Apromise%2520in%2520generating%2520high-fidelity%2520images%2520but%2520often%2520lack%2520the%2520structural%250Aguidance%2520necessary%2520for%2520realistic%2520inpainting.%2520We%2520propose%2520a%2520novel%2520inpainting%250Amethod%2520that%2520combines%2520diffusion%2520models%2520with%2520anisotropic%2520Gaussian%2520splatting%2520to%250Acapture%2520both%2520local%2520structures%2520and%2520global%2520context%2520effectively.%2520By%2520modeling%250Amissing%2520regions%2520using%2520anisotropic%2520Gaussian%2520functions%2520that%2520adapt%2520to%2520local%2520image%250Agradients%252C%2520our%2520approach%2520provides%2520structural%2520guidance%2520to%2520the%2520diffusion-based%250Ainpainting%2520network.%2520The%2520Gaussian%2520splat%2520maps%2520are%2520integrated%2520into%2520the%2520diffusion%250Aprocess%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520generate%2520high-fidelity%2520and%250Astructurally%2520coherent%2520inpainting%2520results.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520state-of-the-art%2520techniques%252C%2520producing%2520visually%250Aplausible%2520results%2520with%2520enhanced%2520structural%2520integrity%2520and%2520texture%2520realism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Models%20with%20Anisotropic%20Gaussian%20Splatting%20for%20Image%0A%20%20Inpainting&entry.906535625=Jacob%20Fein-Ashley%20and%20Benjamin%20Fein-Ashley&entry.1292438233=%20%20Image%20inpainting%20is%20a%20fundamental%20task%20in%20computer%20vision%2C%20aiming%20to%20restore%0Amissing%20or%20corrupted%20regions%20in%20images%20realistically.%20While%20recent%20deep%0Alearning%20approaches%20have%20significantly%20advanced%20the%20state-of-the-art%2C%0Achallenges%20remain%20in%20maintaining%20structural%20continuity%20and%20generating%20coherent%0Atextures%2C%20particularly%20in%20large%20missing%20areas.%20Diffusion%20models%20have%20shown%0Apromise%20in%20generating%20high-fidelity%20images%20but%20often%20lack%20the%20structural%0Aguidance%20necessary%20for%20realistic%20inpainting.%20We%20propose%20a%20novel%20inpainting%0Amethod%20that%20combines%20diffusion%20models%20with%20anisotropic%20Gaussian%20splatting%20to%0Acapture%20both%20local%20structures%20and%20global%20context%20effectively.%20By%20modeling%0Amissing%20regions%20using%20anisotropic%20Gaussian%20functions%20that%20adapt%20to%20local%20image%0Agradients%2C%20our%20approach%20provides%20structural%20guidance%20to%20the%20diffusion-based%0Ainpainting%20network.%20The%20Gaussian%20splat%20maps%20are%20integrated%20into%20the%20diffusion%0Aprocess%2C%20enhancing%20the%20model%27s%20ability%20to%20generate%20high-fidelity%20and%0Astructurally%20coherent%20inpainting%20results.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20outperforms%20state-of-the-art%20techniques%2C%20producing%20visually%0Aplausible%20results%20with%20enhanced%20structural%20integrity%20and%20texture%20realism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01682v2&entry.124074799=Read"},
{"title": "Motion Prompting: Controlling Video Generation with Motion Trajectories", "author": "Daniel Geng and Charles Herrmann and Junhwa Hur and Forrester Cole and Serena Zhang and Tobias Pfaff and Tatiana Lopez-Guevara and Carl Doersch and Yusuf Aytar and Michael Rubinstein and Chen Sun and Oliver Wang and Andrew Owens and Deqing Sun", "abstract": "  Motion control is crucial for generating expressive and compelling video\ncontent; however, most existing video generation models rely mainly on text\nprompts for control, which struggle to capture the nuances of dynamic actions\nand temporal compositions. To this end, we train a video generation model\nconditioned on spatio-temporally sparse or dense motion trajectories. In\ncontrast to prior motion conditioning work, this flexible representation can\nencode any number of trajectories, object-specific or global scene motion, and\ntemporally sparse motion; due to its flexibility we refer to this conditioning\nas motion prompts. While users may directly specify sparse trajectories, we\nalso show how to translate high-level user requests into detailed, semi-dense\nmotion prompts, a process we term motion prompt expansion. We demonstrate the\nversatility of our approach through various applications, including camera and\nobject motion control, \"interacting\" with an image, motion transfer, and image\nediting. Our results showcase emergent behaviors, such as realistic physics,\nsuggesting the potential of motion prompts for probing video models and\ninteracting with future generative world models. Finally, we evaluate\nquantitatively, conduct a human study, and demonstrate strong performance.\nVideo results are available on our webpage: https://motion-prompting.github.io/\n", "link": "http://arxiv.org/abs/2412.02700v1", "date": "2024-12-03", "relevancy": 2.4778, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6586}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6461}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Prompting%3A%20Controlling%20Video%20Generation%20with%20Motion%20Trajectories&body=Title%3A%20Motion%20Prompting%3A%20Controlling%20Video%20Generation%20with%20Motion%20Trajectories%0AAuthor%3A%20Daniel%20Geng%20and%20Charles%20Herrmann%20and%20Junhwa%20Hur%20and%20Forrester%20Cole%20and%20Serena%20Zhang%20and%20Tobias%20Pfaff%20and%20Tatiana%20Lopez-Guevara%20and%20Carl%20Doersch%20and%20Yusuf%20Aytar%20and%20Michael%20Rubinstein%20and%20Chen%20Sun%20and%20Oliver%20Wang%20and%20Andrew%20Owens%20and%20Deqing%20Sun%0AAbstract%3A%20%20%20Motion%20control%20is%20crucial%20for%20generating%20expressive%20and%20compelling%20video%0Acontent%3B%20however%2C%20most%20existing%20video%20generation%20models%20rely%20mainly%20on%20text%0Aprompts%20for%20control%2C%20which%20struggle%20to%20capture%20the%20nuances%20of%20dynamic%20actions%0Aand%20temporal%20compositions.%20To%20this%20end%2C%20we%20train%20a%20video%20generation%20model%0Aconditioned%20on%20spatio-temporally%20sparse%20or%20dense%20motion%20trajectories.%20In%0Acontrast%20to%20prior%20motion%20conditioning%20work%2C%20this%20flexible%20representation%20can%0Aencode%20any%20number%20of%20trajectories%2C%20object-specific%20or%20global%20scene%20motion%2C%20and%0Atemporally%20sparse%20motion%3B%20due%20to%20its%20flexibility%20we%20refer%20to%20this%20conditioning%0Aas%20motion%20prompts.%20While%20users%20may%20directly%20specify%20sparse%20trajectories%2C%20we%0Aalso%20show%20how%20to%20translate%20high-level%20user%20requests%20into%20detailed%2C%20semi-dense%0Amotion%20prompts%2C%20a%20process%20we%20term%20motion%20prompt%20expansion.%20We%20demonstrate%20the%0Aversatility%20of%20our%20approach%20through%20various%20applications%2C%20including%20camera%20and%0Aobject%20motion%20control%2C%20%22interacting%22%20with%20an%20image%2C%20motion%20transfer%2C%20and%20image%0Aediting.%20Our%20results%20showcase%20emergent%20behaviors%2C%20such%20as%20realistic%20physics%2C%0Asuggesting%20the%20potential%20of%20motion%20prompts%20for%20probing%20video%20models%20and%0Ainteracting%20with%20future%20generative%20world%20models.%20Finally%2C%20we%20evaluate%0Aquantitatively%2C%20conduct%20a%20human%20study%2C%20and%20demonstrate%20strong%20performance.%0AVideo%20results%20are%20available%20on%20our%20webpage%3A%20https%3A//motion-prompting.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Prompting%253A%2520Controlling%2520Video%2520Generation%2520with%2520Motion%2520Trajectories%26entry.906535625%3DDaniel%2520Geng%2520and%2520Charles%2520Herrmann%2520and%2520Junhwa%2520Hur%2520and%2520Forrester%2520Cole%2520and%2520Serena%2520Zhang%2520and%2520Tobias%2520Pfaff%2520and%2520Tatiana%2520Lopez-Guevara%2520and%2520Carl%2520Doersch%2520and%2520Yusuf%2520Aytar%2520and%2520Michael%2520Rubinstein%2520and%2520Chen%2520Sun%2520and%2520Oliver%2520Wang%2520and%2520Andrew%2520Owens%2520and%2520Deqing%2520Sun%26entry.1292438233%3D%2520%2520Motion%2520control%2520is%2520crucial%2520for%2520generating%2520expressive%2520and%2520compelling%2520video%250Acontent%253B%2520however%252C%2520most%2520existing%2520video%2520generation%2520models%2520rely%2520mainly%2520on%2520text%250Aprompts%2520for%2520control%252C%2520which%2520struggle%2520to%2520capture%2520the%2520nuances%2520of%2520dynamic%2520actions%250Aand%2520temporal%2520compositions.%2520To%2520this%2520end%252C%2520we%2520train%2520a%2520video%2520generation%2520model%250Aconditioned%2520on%2520spatio-temporally%2520sparse%2520or%2520dense%2520motion%2520trajectories.%2520In%250Acontrast%2520to%2520prior%2520motion%2520conditioning%2520work%252C%2520this%2520flexible%2520representation%2520can%250Aencode%2520any%2520number%2520of%2520trajectories%252C%2520object-specific%2520or%2520global%2520scene%2520motion%252C%2520and%250Atemporally%2520sparse%2520motion%253B%2520due%2520to%2520its%2520flexibility%2520we%2520refer%2520to%2520this%2520conditioning%250Aas%2520motion%2520prompts.%2520While%2520users%2520may%2520directly%2520specify%2520sparse%2520trajectories%252C%2520we%250Aalso%2520show%2520how%2520to%2520translate%2520high-level%2520user%2520requests%2520into%2520detailed%252C%2520semi-dense%250Amotion%2520prompts%252C%2520a%2520process%2520we%2520term%2520motion%2520prompt%2520expansion.%2520We%2520demonstrate%2520the%250Aversatility%2520of%2520our%2520approach%2520through%2520various%2520applications%252C%2520including%2520camera%2520and%250Aobject%2520motion%2520control%252C%2520%2522interacting%2522%2520with%2520an%2520image%252C%2520motion%2520transfer%252C%2520and%2520image%250Aediting.%2520Our%2520results%2520showcase%2520emergent%2520behaviors%252C%2520such%2520as%2520realistic%2520physics%252C%250Asuggesting%2520the%2520potential%2520of%2520motion%2520prompts%2520for%2520probing%2520video%2520models%2520and%250Ainteracting%2520with%2520future%2520generative%2520world%2520models.%2520Finally%252C%2520we%2520evaluate%250Aquantitatively%252C%2520conduct%2520a%2520human%2520study%252C%2520and%2520demonstrate%2520strong%2520performance.%250AVideo%2520results%2520are%2520available%2520on%2520our%2520webpage%253A%2520https%253A//motion-prompting.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Prompting%3A%20Controlling%20Video%20Generation%20with%20Motion%20Trajectories&entry.906535625=Daniel%20Geng%20and%20Charles%20Herrmann%20and%20Junhwa%20Hur%20and%20Forrester%20Cole%20and%20Serena%20Zhang%20and%20Tobias%20Pfaff%20and%20Tatiana%20Lopez-Guevara%20and%20Carl%20Doersch%20and%20Yusuf%20Aytar%20and%20Michael%20Rubinstein%20and%20Chen%20Sun%20and%20Oliver%20Wang%20and%20Andrew%20Owens%20and%20Deqing%20Sun&entry.1292438233=%20%20Motion%20control%20is%20crucial%20for%20generating%20expressive%20and%20compelling%20video%0Acontent%3B%20however%2C%20most%20existing%20video%20generation%20models%20rely%20mainly%20on%20text%0Aprompts%20for%20control%2C%20which%20struggle%20to%20capture%20the%20nuances%20of%20dynamic%20actions%0Aand%20temporal%20compositions.%20To%20this%20end%2C%20we%20train%20a%20video%20generation%20model%0Aconditioned%20on%20spatio-temporally%20sparse%20or%20dense%20motion%20trajectories.%20In%0Acontrast%20to%20prior%20motion%20conditioning%20work%2C%20this%20flexible%20representation%20can%0Aencode%20any%20number%20of%20trajectories%2C%20object-specific%20or%20global%20scene%20motion%2C%20and%0Atemporally%20sparse%20motion%3B%20due%20to%20its%20flexibility%20we%20refer%20to%20this%20conditioning%0Aas%20motion%20prompts.%20While%20users%20may%20directly%20specify%20sparse%20trajectories%2C%20we%0Aalso%20show%20how%20to%20translate%20high-level%20user%20requests%20into%20detailed%2C%20semi-dense%0Amotion%20prompts%2C%20a%20process%20we%20term%20motion%20prompt%20expansion.%20We%20demonstrate%20the%0Aversatility%20of%20our%20approach%20through%20various%20applications%2C%20including%20camera%20and%0Aobject%20motion%20control%2C%20%22interacting%22%20with%20an%20image%2C%20motion%20transfer%2C%20and%20image%0Aediting.%20Our%20results%20showcase%20emergent%20behaviors%2C%20such%20as%20realistic%20physics%2C%0Asuggesting%20the%20potential%20of%20motion%20prompts%20for%20probing%20video%20models%20and%0Ainteracting%20with%20future%20generative%20world%20models.%20Finally%2C%20we%20evaluate%0Aquantitatively%2C%20conduct%20a%20human%20study%2C%20and%20demonstrate%20strong%20performance.%0AVideo%20results%20are%20available%20on%20our%20webpage%3A%20https%3A//motion-prompting.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02700v1&entry.124074799=Read"},
{"title": "Towards Neuro-Symbolic Video Understanding", "author": "Minkyu Choi and Harsh Goel and Mohammad Omama and Yunhao Yang and Sahil Shah and Sandeep Chinchali", "abstract": "  The unprecedented surge in video data production in recent years necessitates\nefficient tools to extract meaningful frames from videos for downstream tasks.\nLong-term temporal reasoning is a key desideratum for frame retrieval systems.\nWhile state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are\nproficient in short-term semantic understanding, they surprisingly fail at\nlong-term reasoning across frames. A key reason for this failure is that they\nintertwine per-frame perception and temporal reasoning into a single deep\nnetwork. Hence, decoupling but co-designing semantic understanding and temporal\nreasoning is essential for efficient scene identification. We propose a system\nthat leverages vision-language models for semantic understanding of individual\nframes but effectively reasons about the long-term evolution of events using\nstate machines and temporal logic (TL) formulae that inherently capture memory.\nOur TL-based reasoning improves the F1 score of complex event identification by\n9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-art\nself-driving datasets such as Waymo and NuScenes.\n", "link": "http://arxiv.org/abs/2403.11021v3", "date": "2024-12-03", "relevancy": 2.4516, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.625}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Neuro-Symbolic%20Video%20Understanding&body=Title%3A%20Towards%20Neuro-Symbolic%20Video%20Understanding%0AAuthor%3A%20Minkyu%20Choi%20and%20Harsh%20Goel%20and%20Mohammad%20Omama%20and%20Yunhao%20Yang%20and%20Sahil%20Shah%20and%20Sandeep%20Chinchali%0AAbstract%3A%20%20%20The%20unprecedented%20surge%20in%20video%20data%20production%20in%20recent%20years%20necessitates%0Aefficient%20tools%20to%20extract%20meaningful%20frames%20from%20videos%20for%20downstream%20tasks.%0ALong-term%20temporal%20reasoning%20is%20a%20key%20desideratum%20for%20frame%20retrieval%20systems.%0AWhile%20state-of-the-art%20foundation%20models%2C%20like%20VideoLLaMA%20and%20ViCLIP%2C%20are%0Aproficient%20in%20short-term%20semantic%20understanding%2C%20they%20surprisingly%20fail%20at%0Along-term%20reasoning%20across%20frames.%20A%20key%20reason%20for%20this%20failure%20is%20that%20they%0Aintertwine%20per-frame%20perception%20and%20temporal%20reasoning%20into%20a%20single%20deep%0Anetwork.%20Hence%2C%20decoupling%20but%20co-designing%20semantic%20understanding%20and%20temporal%0Areasoning%20is%20essential%20for%20efficient%20scene%20identification.%20We%20propose%20a%20system%0Athat%20leverages%20vision-language%20models%20for%20semantic%20understanding%20of%20individual%0Aframes%20but%20effectively%20reasons%20about%20the%20long-term%20evolution%20of%20events%20using%0Astate%20machines%20and%20temporal%20logic%20%28TL%29%20formulae%20that%20inherently%20capture%20memory.%0AOur%20TL-based%20reasoning%20improves%20the%20F1%20score%20of%20complex%20event%20identification%20by%0A9-15%25%20compared%20to%20benchmarks%20that%20use%20GPT4%20for%20reasoning%20on%20state-of-the-art%0Aself-driving%20datasets%20such%20as%20Waymo%20and%20NuScenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11021v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Neuro-Symbolic%2520Video%2520Understanding%26entry.906535625%3DMinkyu%2520Choi%2520and%2520Harsh%2520Goel%2520and%2520Mohammad%2520Omama%2520and%2520Yunhao%2520Yang%2520and%2520Sahil%2520Shah%2520and%2520Sandeep%2520Chinchali%26entry.1292438233%3D%2520%2520The%2520unprecedented%2520surge%2520in%2520video%2520data%2520production%2520in%2520recent%2520years%2520necessitates%250Aefficient%2520tools%2520to%2520extract%2520meaningful%2520frames%2520from%2520videos%2520for%2520downstream%2520tasks.%250ALong-term%2520temporal%2520reasoning%2520is%2520a%2520key%2520desideratum%2520for%2520frame%2520retrieval%2520systems.%250AWhile%2520state-of-the-art%2520foundation%2520models%252C%2520like%2520VideoLLaMA%2520and%2520ViCLIP%252C%2520are%250Aproficient%2520in%2520short-term%2520semantic%2520understanding%252C%2520they%2520surprisingly%2520fail%2520at%250Along-term%2520reasoning%2520across%2520frames.%2520A%2520key%2520reason%2520for%2520this%2520failure%2520is%2520that%2520they%250Aintertwine%2520per-frame%2520perception%2520and%2520temporal%2520reasoning%2520into%2520a%2520single%2520deep%250Anetwork.%2520Hence%252C%2520decoupling%2520but%2520co-designing%2520semantic%2520understanding%2520and%2520temporal%250Areasoning%2520is%2520essential%2520for%2520efficient%2520scene%2520identification.%2520We%2520propose%2520a%2520system%250Athat%2520leverages%2520vision-language%2520models%2520for%2520semantic%2520understanding%2520of%2520individual%250Aframes%2520but%2520effectively%2520reasons%2520about%2520the%2520long-term%2520evolution%2520of%2520events%2520using%250Astate%2520machines%2520and%2520temporal%2520logic%2520%2528TL%2529%2520formulae%2520that%2520inherently%2520capture%2520memory.%250AOur%2520TL-based%2520reasoning%2520improves%2520the%2520F1%2520score%2520of%2520complex%2520event%2520identification%2520by%250A9-15%2525%2520compared%2520to%2520benchmarks%2520that%2520use%2520GPT4%2520for%2520reasoning%2520on%2520state-of-the-art%250Aself-driving%2520datasets%2520such%2520as%2520Waymo%2520and%2520NuScenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11021v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Neuro-Symbolic%20Video%20Understanding&entry.906535625=Minkyu%20Choi%20and%20Harsh%20Goel%20and%20Mohammad%20Omama%20and%20Yunhao%20Yang%20and%20Sahil%20Shah%20and%20Sandeep%20Chinchali&entry.1292438233=%20%20The%20unprecedented%20surge%20in%20video%20data%20production%20in%20recent%20years%20necessitates%0Aefficient%20tools%20to%20extract%20meaningful%20frames%20from%20videos%20for%20downstream%20tasks.%0ALong-term%20temporal%20reasoning%20is%20a%20key%20desideratum%20for%20frame%20retrieval%20systems.%0AWhile%20state-of-the-art%20foundation%20models%2C%20like%20VideoLLaMA%20and%20ViCLIP%2C%20are%0Aproficient%20in%20short-term%20semantic%20understanding%2C%20they%20surprisingly%20fail%20at%0Along-term%20reasoning%20across%20frames.%20A%20key%20reason%20for%20this%20failure%20is%20that%20they%0Aintertwine%20per-frame%20perception%20and%20temporal%20reasoning%20into%20a%20single%20deep%0Anetwork.%20Hence%2C%20decoupling%20but%20co-designing%20semantic%20understanding%20and%20temporal%0Areasoning%20is%20essential%20for%20efficient%20scene%20identification.%20We%20propose%20a%20system%0Athat%20leverages%20vision-language%20models%20for%20semantic%20understanding%20of%20individual%0Aframes%20but%20effectively%20reasons%20about%20the%20long-term%20evolution%20of%20events%20using%0Astate%20machines%20and%20temporal%20logic%20%28TL%29%20formulae%20that%20inherently%20capture%20memory.%0AOur%20TL-based%20reasoning%20improves%20the%20F1%20score%20of%20complex%20event%20identification%20by%0A9-15%25%20compared%20to%20benchmarks%20that%20use%20GPT4%20for%20reasoning%20on%20state-of-the-art%0Aself-driving%20datasets%20such%20as%20Waymo%20and%20NuScenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11021v3&entry.124074799=Read"},
{"title": "MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression\n  Prediction from Whole Slide Histopathology Images", "author": "Aniruddha Ganguly and Debolina Chatterjee and Wentao Huang and Jie Zhang and Alisa Yurovsky and Travis Steele Johnson and Chao Chen", "abstract": "  Recent advances in Spatial Transcriptomics (ST) pair histology images with\nspatially resolved gene expression profiles, enabling predictions of gene\nexpression across different tissue locations based on image patches. This opens\nup new possibilities for enhancing whole slide image (WSI) prediction tasks\nwith localized gene expression. However, existing methods fail to fully\nleverage the interactions between different tissue locations, which are crucial\nfor accurate joint prediction. To address this, we introduce MERGE\n(Multi-faceted hiErarchical gRaph for Gene Expressions), which combines a\nmulti-faceted hierarchical graph construction strategy with graph neural\nnetworks (GNN) to improve gene expression predictions from WSIs. By clustering\ntissue image patches based on both spatial and morphological features, and\nincorporating intra- and inter-cluster edges, our approach fosters interactions\nbetween distant tissue locations during GNN learning. As an additional\ncontribution, we evaluate different data smoothing techniques that are\nnecessary to mitigate artifacts in ST data, often caused by technical\nimperfections. We advocate for adopting gene-aware smoothing methods that are\nmore biologically justified. Experimental results on gene expression prediction\nshow that our GNN method outperforms state-of-the-art techniques across\nmultiple metrics.\n", "link": "http://arxiv.org/abs/2412.02601v1", "date": "2024-12-03", "relevancy": 2.3777, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4759}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MERGE%3A%20Multi-faceted%20Hierarchical%20Graph-based%20GNN%20for%20Gene%20Expression%0A%20%20Prediction%20from%20Whole%20Slide%20Histopathology%20Images&body=Title%3A%20MERGE%3A%20Multi-faceted%20Hierarchical%20Graph-based%20GNN%20for%20Gene%20Expression%0A%20%20Prediction%20from%20Whole%20Slide%20Histopathology%20Images%0AAuthor%3A%20Aniruddha%20Ganguly%20and%20Debolina%20Chatterjee%20and%20Wentao%20Huang%20and%20Jie%20Zhang%20and%20Alisa%20Yurovsky%20and%20Travis%20Steele%20Johnson%20and%20Chao%20Chen%0AAbstract%3A%20%20%20Recent%20advances%20in%20Spatial%20Transcriptomics%20%28ST%29%20pair%20histology%20images%20with%0Aspatially%20resolved%20gene%20expression%20profiles%2C%20enabling%20predictions%20of%20gene%0Aexpression%20across%20different%20tissue%20locations%20based%20on%20image%20patches.%20This%20opens%0Aup%20new%20possibilities%20for%20enhancing%20whole%20slide%20image%20%28WSI%29%20prediction%20tasks%0Awith%20localized%20gene%20expression.%20However%2C%20existing%20methods%20fail%20to%20fully%0Aleverage%20the%20interactions%20between%20different%20tissue%20locations%2C%20which%20are%20crucial%0Afor%20accurate%20joint%20prediction.%20To%20address%20this%2C%20we%20introduce%20MERGE%0A%28Multi-faceted%20hiErarchical%20gRaph%20for%20Gene%20Expressions%29%2C%20which%20combines%20a%0Amulti-faceted%20hierarchical%20graph%20construction%20strategy%20with%20graph%20neural%0Anetworks%20%28GNN%29%20to%20improve%20gene%20expression%20predictions%20from%20WSIs.%20By%20clustering%0Atissue%20image%20patches%20based%20on%20both%20spatial%20and%20morphological%20features%2C%20and%0Aincorporating%20intra-%20and%20inter-cluster%20edges%2C%20our%20approach%20fosters%20interactions%0Abetween%20distant%20tissue%20locations%20during%20GNN%20learning.%20As%20an%20additional%0Acontribution%2C%20we%20evaluate%20different%20data%20smoothing%20techniques%20that%20are%0Anecessary%20to%20mitigate%20artifacts%20in%20ST%20data%2C%20often%20caused%20by%20technical%0Aimperfections.%20We%20advocate%20for%20adopting%20gene-aware%20smoothing%20methods%20that%20are%0Amore%20biologically%20justified.%20Experimental%20results%20on%20gene%20expression%20prediction%0Ashow%20that%20our%20GNN%20method%20outperforms%20state-of-the-art%20techniques%20across%0Amultiple%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMERGE%253A%2520Multi-faceted%2520Hierarchical%2520Graph-based%2520GNN%2520for%2520Gene%2520Expression%250A%2520%2520Prediction%2520from%2520Whole%2520Slide%2520Histopathology%2520Images%26entry.906535625%3DAniruddha%2520Ganguly%2520and%2520Debolina%2520Chatterjee%2520and%2520Wentao%2520Huang%2520and%2520Jie%2520Zhang%2520and%2520Alisa%2520Yurovsky%2520and%2520Travis%2520Steele%2520Johnson%2520and%2520Chao%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Spatial%2520Transcriptomics%2520%2528ST%2529%2520pair%2520histology%2520images%2520with%250Aspatially%2520resolved%2520gene%2520expression%2520profiles%252C%2520enabling%2520predictions%2520of%2520gene%250Aexpression%2520across%2520different%2520tissue%2520locations%2520based%2520on%2520image%2520patches.%2520This%2520opens%250Aup%2520new%2520possibilities%2520for%2520enhancing%2520whole%2520slide%2520image%2520%2528WSI%2529%2520prediction%2520tasks%250Awith%2520localized%2520gene%2520expression.%2520However%252C%2520existing%2520methods%2520fail%2520to%2520fully%250Aleverage%2520the%2520interactions%2520between%2520different%2520tissue%2520locations%252C%2520which%2520are%2520crucial%250Afor%2520accurate%2520joint%2520prediction.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MERGE%250A%2528Multi-faceted%2520hiErarchical%2520gRaph%2520for%2520Gene%2520Expressions%2529%252C%2520which%2520combines%2520a%250Amulti-faceted%2520hierarchical%2520graph%2520construction%2520strategy%2520with%2520graph%2520neural%250Anetworks%2520%2528GNN%2529%2520to%2520improve%2520gene%2520expression%2520predictions%2520from%2520WSIs.%2520By%2520clustering%250Atissue%2520image%2520patches%2520based%2520on%2520both%2520spatial%2520and%2520morphological%2520features%252C%2520and%250Aincorporating%2520intra-%2520and%2520inter-cluster%2520edges%252C%2520our%2520approach%2520fosters%2520interactions%250Abetween%2520distant%2520tissue%2520locations%2520during%2520GNN%2520learning.%2520As%2520an%2520additional%250Acontribution%252C%2520we%2520evaluate%2520different%2520data%2520smoothing%2520techniques%2520that%2520are%250Anecessary%2520to%2520mitigate%2520artifacts%2520in%2520ST%2520data%252C%2520often%2520caused%2520by%2520technical%250Aimperfections.%2520We%2520advocate%2520for%2520adopting%2520gene-aware%2520smoothing%2520methods%2520that%2520are%250Amore%2520biologically%2520justified.%2520Experimental%2520results%2520on%2520gene%2520expression%2520prediction%250Ashow%2520that%2520our%2520GNN%2520method%2520outperforms%2520state-of-the-art%2520techniques%2520across%250Amultiple%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MERGE%3A%20Multi-faceted%20Hierarchical%20Graph-based%20GNN%20for%20Gene%20Expression%0A%20%20Prediction%20from%20Whole%20Slide%20Histopathology%20Images&entry.906535625=Aniruddha%20Ganguly%20and%20Debolina%20Chatterjee%20and%20Wentao%20Huang%20and%20Jie%20Zhang%20and%20Alisa%20Yurovsky%20and%20Travis%20Steele%20Johnson%20and%20Chao%20Chen&entry.1292438233=%20%20Recent%20advances%20in%20Spatial%20Transcriptomics%20%28ST%29%20pair%20histology%20images%20with%0Aspatially%20resolved%20gene%20expression%20profiles%2C%20enabling%20predictions%20of%20gene%0Aexpression%20across%20different%20tissue%20locations%20based%20on%20image%20patches.%20This%20opens%0Aup%20new%20possibilities%20for%20enhancing%20whole%20slide%20image%20%28WSI%29%20prediction%20tasks%0Awith%20localized%20gene%20expression.%20However%2C%20existing%20methods%20fail%20to%20fully%0Aleverage%20the%20interactions%20between%20different%20tissue%20locations%2C%20which%20are%20crucial%0Afor%20accurate%20joint%20prediction.%20To%20address%20this%2C%20we%20introduce%20MERGE%0A%28Multi-faceted%20hiErarchical%20gRaph%20for%20Gene%20Expressions%29%2C%20which%20combines%20a%0Amulti-faceted%20hierarchical%20graph%20construction%20strategy%20with%20graph%20neural%0Anetworks%20%28GNN%29%20to%20improve%20gene%20expression%20predictions%20from%20WSIs.%20By%20clustering%0Atissue%20image%20patches%20based%20on%20both%20spatial%20and%20morphological%20features%2C%20and%0Aincorporating%20intra-%20and%20inter-cluster%20edges%2C%20our%20approach%20fosters%20interactions%0Abetween%20distant%20tissue%20locations%20during%20GNN%20learning.%20As%20an%20additional%0Acontribution%2C%20we%20evaluate%20different%20data%20smoothing%20techniques%20that%20are%0Anecessary%20to%20mitigate%20artifacts%20in%20ST%20data%2C%20often%20caused%20by%20technical%0Aimperfections.%20We%20advocate%20for%20adopting%20gene-aware%20smoothing%20methods%20that%20are%0Amore%20biologically%20justified.%20Experimental%20results%20on%20gene%20expression%20prediction%0Ashow%20that%20our%20GNN%20method%20outperforms%20state-of-the-art%20techniques%20across%0Amultiple%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02601v1&entry.124074799=Read"},
{"title": "MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression\n  Prediction from Whole Slide Histopathology Images", "author": "Aniruddha Ganguly and Debolina Chatterjee and Wentao Huang and Jie Zhang and Alisa Yurovsky and Travis Steele Johnson and Chao Chen", "abstract": "  Recent advances in Spatial Transcriptomics (ST) pair histology images with\nspatially resolved gene expression profiles, enabling predictions of gene\nexpression across different tissue locations based on image patches. This opens\nup new possibilities for enhancing whole slide image (WSI) prediction tasks\nwith localized gene expression. However, existing methods fail to fully\nleverage the interactions between different tissue locations, which are crucial\nfor accurate joint prediction. To address this, we introduce MERGE\n(Multi-faceted hiErarchical gRaph for Gene Expressions), which combines a\nmulti-faceted hierarchical graph construction strategy with graph neural\nnetworks (GNN) to improve gene expression predictions from WSIs. By clustering\ntissue image patches based on both spatial and morphological features, and\nincorporating intra- and inter-cluster edges, our approach fosters interactions\nbetween distant tissue locations during GNN learning. As an additional\ncontribution, we evaluate different data smoothing techniques that are\nnecessary to mitigate artifacts in ST data, often caused by technical\nimperfections. We advocate for adopting gene-aware smoothing methods that are\nmore biologically justified. Experimental results on gene expression prediction\nshow that our GNN method outperforms state-of-the-art techniques across\nmultiple metrics.\n", "link": "http://arxiv.org/abs/2412.02601v1", "date": "2024-12-03", "relevancy": 2.3777, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4759}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MERGE%3A%20Multi-faceted%20Hierarchical%20Graph-based%20GNN%20for%20Gene%20Expression%0A%20%20Prediction%20from%20Whole%20Slide%20Histopathology%20Images&body=Title%3A%20MERGE%3A%20Multi-faceted%20Hierarchical%20Graph-based%20GNN%20for%20Gene%20Expression%0A%20%20Prediction%20from%20Whole%20Slide%20Histopathology%20Images%0AAuthor%3A%20Aniruddha%20Ganguly%20and%20Debolina%20Chatterjee%20and%20Wentao%20Huang%20and%20Jie%20Zhang%20and%20Alisa%20Yurovsky%20and%20Travis%20Steele%20Johnson%20and%20Chao%20Chen%0AAbstract%3A%20%20%20Recent%20advances%20in%20Spatial%20Transcriptomics%20%28ST%29%20pair%20histology%20images%20with%0Aspatially%20resolved%20gene%20expression%20profiles%2C%20enabling%20predictions%20of%20gene%0Aexpression%20across%20different%20tissue%20locations%20based%20on%20image%20patches.%20This%20opens%0Aup%20new%20possibilities%20for%20enhancing%20whole%20slide%20image%20%28WSI%29%20prediction%20tasks%0Awith%20localized%20gene%20expression.%20However%2C%20existing%20methods%20fail%20to%20fully%0Aleverage%20the%20interactions%20between%20different%20tissue%20locations%2C%20which%20are%20crucial%0Afor%20accurate%20joint%20prediction.%20To%20address%20this%2C%20we%20introduce%20MERGE%0A%28Multi-faceted%20hiErarchical%20gRaph%20for%20Gene%20Expressions%29%2C%20which%20combines%20a%0Amulti-faceted%20hierarchical%20graph%20construction%20strategy%20with%20graph%20neural%0Anetworks%20%28GNN%29%20to%20improve%20gene%20expression%20predictions%20from%20WSIs.%20By%20clustering%0Atissue%20image%20patches%20based%20on%20both%20spatial%20and%20morphological%20features%2C%20and%0Aincorporating%20intra-%20and%20inter-cluster%20edges%2C%20our%20approach%20fosters%20interactions%0Abetween%20distant%20tissue%20locations%20during%20GNN%20learning.%20As%20an%20additional%0Acontribution%2C%20we%20evaluate%20different%20data%20smoothing%20techniques%20that%20are%0Anecessary%20to%20mitigate%20artifacts%20in%20ST%20data%2C%20often%20caused%20by%20technical%0Aimperfections.%20We%20advocate%20for%20adopting%20gene-aware%20smoothing%20methods%20that%20are%0Amore%20biologically%20justified.%20Experimental%20results%20on%20gene%20expression%20prediction%0Ashow%20that%20our%20GNN%20method%20outperforms%20state-of-the-art%20techniques%20across%0Amultiple%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMERGE%253A%2520Multi-faceted%2520Hierarchical%2520Graph-based%2520GNN%2520for%2520Gene%2520Expression%250A%2520%2520Prediction%2520from%2520Whole%2520Slide%2520Histopathology%2520Images%26entry.906535625%3DAniruddha%2520Ganguly%2520and%2520Debolina%2520Chatterjee%2520and%2520Wentao%2520Huang%2520and%2520Jie%2520Zhang%2520and%2520Alisa%2520Yurovsky%2520and%2520Travis%2520Steele%2520Johnson%2520and%2520Chao%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Spatial%2520Transcriptomics%2520%2528ST%2529%2520pair%2520histology%2520images%2520with%250Aspatially%2520resolved%2520gene%2520expression%2520profiles%252C%2520enabling%2520predictions%2520of%2520gene%250Aexpression%2520across%2520different%2520tissue%2520locations%2520based%2520on%2520image%2520patches.%2520This%2520opens%250Aup%2520new%2520possibilities%2520for%2520enhancing%2520whole%2520slide%2520image%2520%2528WSI%2529%2520prediction%2520tasks%250Awith%2520localized%2520gene%2520expression.%2520However%252C%2520existing%2520methods%2520fail%2520to%2520fully%250Aleverage%2520the%2520interactions%2520between%2520different%2520tissue%2520locations%252C%2520which%2520are%2520crucial%250Afor%2520accurate%2520joint%2520prediction.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MERGE%250A%2528Multi-faceted%2520hiErarchical%2520gRaph%2520for%2520Gene%2520Expressions%2529%252C%2520which%2520combines%2520a%250Amulti-faceted%2520hierarchical%2520graph%2520construction%2520strategy%2520with%2520graph%2520neural%250Anetworks%2520%2528GNN%2529%2520to%2520improve%2520gene%2520expression%2520predictions%2520from%2520WSIs.%2520By%2520clustering%250Atissue%2520image%2520patches%2520based%2520on%2520both%2520spatial%2520and%2520morphological%2520features%252C%2520and%250Aincorporating%2520intra-%2520and%2520inter-cluster%2520edges%252C%2520our%2520approach%2520fosters%2520interactions%250Abetween%2520distant%2520tissue%2520locations%2520during%2520GNN%2520learning.%2520As%2520an%2520additional%250Acontribution%252C%2520we%2520evaluate%2520different%2520data%2520smoothing%2520techniques%2520that%2520are%250Anecessary%2520to%2520mitigate%2520artifacts%2520in%2520ST%2520data%252C%2520often%2520caused%2520by%2520technical%250Aimperfections.%2520We%2520advocate%2520for%2520adopting%2520gene-aware%2520smoothing%2520methods%2520that%2520are%250Amore%2520biologically%2520justified.%2520Experimental%2520results%2520on%2520gene%2520expression%2520prediction%250Ashow%2520that%2520our%2520GNN%2520method%2520outperforms%2520state-of-the-art%2520techniques%2520across%250Amultiple%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MERGE%3A%20Multi-faceted%20Hierarchical%20Graph-based%20GNN%20for%20Gene%20Expression%0A%20%20Prediction%20from%20Whole%20Slide%20Histopathology%20Images&entry.906535625=Aniruddha%20Ganguly%20and%20Debolina%20Chatterjee%20and%20Wentao%20Huang%20and%20Jie%20Zhang%20and%20Alisa%20Yurovsky%20and%20Travis%20Steele%20Johnson%20and%20Chao%20Chen&entry.1292438233=%20%20Recent%20advances%20in%20Spatial%20Transcriptomics%20%28ST%29%20pair%20histology%20images%20with%0Aspatially%20resolved%20gene%20expression%20profiles%2C%20enabling%20predictions%20of%20gene%0Aexpression%20across%20different%20tissue%20locations%20based%20on%20image%20patches.%20This%20opens%0Aup%20new%20possibilities%20for%20enhancing%20whole%20slide%20image%20%28WSI%29%20prediction%20tasks%0Awith%20localized%20gene%20expression.%20However%2C%20existing%20methods%20fail%20to%20fully%0Aleverage%20the%20interactions%20between%20different%20tissue%20locations%2C%20which%20are%20crucial%0Afor%20accurate%20joint%20prediction.%20To%20address%20this%2C%20we%20introduce%20MERGE%0A%28Multi-faceted%20hiErarchical%20gRaph%20for%20Gene%20Expressions%29%2C%20which%20combines%20a%0Amulti-faceted%20hierarchical%20graph%20construction%20strategy%20with%20graph%20neural%0Anetworks%20%28GNN%29%20to%20improve%20gene%20expression%20predictions%20from%20WSIs.%20By%20clustering%0Atissue%20image%20patches%20based%20on%20both%20spatial%20and%20morphological%20features%2C%20and%0Aincorporating%20intra-%20and%20inter-cluster%20edges%2C%20our%20approach%20fosters%20interactions%0Abetween%20distant%20tissue%20locations%20during%20GNN%20learning.%20As%20an%20additional%0Acontribution%2C%20we%20evaluate%20different%20data%20smoothing%20techniques%20that%20are%0Anecessary%20to%20mitigate%20artifacts%20in%20ST%20data%2C%20often%20caused%20by%20technical%0Aimperfections.%20We%20advocate%20for%20adopting%20gene-aware%20smoothing%20methods%20that%20are%0Amore%20biologically%20justified.%20Experimental%20results%20on%20gene%20expression%20prediction%0Ashow%20that%20our%20GNN%20method%20outperforms%20state-of-the-art%20techniques%20across%0Amultiple%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02601v1&entry.124074799=Read"},
{"title": "A Good Foundation is Worth Many Labels: Label-Efficient Panoptic\n  Segmentation", "author": "Niclas V\u00f6disch and K\u00fcrsat Petek and Markus K\u00e4ppeler and Abhinav Valada and Wolfram Burgard", "abstract": "  A key challenge for the widespread application of learning-based models for\nrobotic perception is to significantly reduce the required amount of annotated\ntraining data while achieving accurate predictions. This is essential not only\nto decrease operating costs but also to speed up deployment time. In this work,\nwe address this challenge for PAnoptic SegmenTation with fEw Labels (PASTEL) by\nexploiting the groundwork paved by visual foundation models. We leverage\ndescriptive image features from such a model to train two lightweight network\nheads for semantic segmentation and object boundary detection, using very few\nannotated training samples. We then merge their predictions via a novel fusion\nmodule that yields panoptic maps based on normalized cut. To further enhance\nthe performance, we utilize self-training on unlabeled images selected by a\nfeature-driven similarity scheme. We underline the relevance of our approach by\nemploying PASTEL to important robot perception use cases from autonomous\ndriving and agricultural robotics. In extensive experiments, we demonstrate\nthat PASTEL significantly outperforms previous methods for label-efficient\nsegmentation even when using fewer annotations. The code of our work is\npublicly available at http://pastel.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2405.19035v2", "date": "2024-12-03", "relevancy": 2.3582, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6091}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5884}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Good%20Foundation%20is%20Worth%20Many%20Labels%3A%20Label-Efficient%20Panoptic%0A%20%20Segmentation&body=Title%3A%20A%20Good%20Foundation%20is%20Worth%20Many%20Labels%3A%20Label-Efficient%20Panoptic%0A%20%20Segmentation%0AAuthor%3A%20Niclas%20V%C3%B6disch%20and%20K%C3%BCrsat%20Petek%20and%20Markus%20K%C3%A4ppeler%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20A%20key%20challenge%20for%20the%20widespread%20application%20of%20learning-based%20models%20for%0Arobotic%20perception%20is%20to%20significantly%20reduce%20the%20required%20amount%20of%20annotated%0Atraining%20data%20while%20achieving%20accurate%20predictions.%20This%20is%20essential%20not%20only%0Ato%20decrease%20operating%20costs%20but%20also%20to%20speed%20up%20deployment%20time.%20In%20this%20work%2C%0Awe%20address%20this%20challenge%20for%20PAnoptic%20SegmenTation%20with%20fEw%20Labels%20%28PASTEL%29%20by%0Aexploiting%20the%20groundwork%20paved%20by%20visual%20foundation%20models.%20We%20leverage%0Adescriptive%20image%20features%20from%20such%20a%20model%20to%20train%20two%20lightweight%20network%0Aheads%20for%20semantic%20segmentation%20and%20object%20boundary%20detection%2C%20using%20very%20few%0Aannotated%20training%20samples.%20We%20then%20merge%20their%20predictions%20via%20a%20novel%20fusion%0Amodule%20that%20yields%20panoptic%20maps%20based%20on%20normalized%20cut.%20To%20further%20enhance%0Athe%20performance%2C%20we%20utilize%20self-training%20on%20unlabeled%20images%20selected%20by%20a%0Afeature-driven%20similarity%20scheme.%20We%20underline%20the%20relevance%20of%20our%20approach%20by%0Aemploying%20PASTEL%20to%20important%20robot%20perception%20use%20cases%20from%20autonomous%0Adriving%20and%20agricultural%20robotics.%20In%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20PASTEL%20significantly%20outperforms%20previous%20methods%20for%20label-efficient%0Asegmentation%20even%20when%20using%20fewer%20annotations.%20The%20code%20of%20our%20work%20is%0Apublicly%20available%20at%20http%3A//pastel.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Good%2520Foundation%2520is%2520Worth%2520Many%2520Labels%253A%2520Label-Efficient%2520Panoptic%250A%2520%2520Segmentation%26entry.906535625%3DNiclas%2520V%25C3%25B6disch%2520and%2520K%25C3%25BCrsat%2520Petek%2520and%2520Markus%2520K%25C3%25A4ppeler%2520and%2520Abhinav%2520Valada%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520A%2520key%2520challenge%2520for%2520the%2520widespread%2520application%2520of%2520learning-based%2520models%2520for%250Arobotic%2520perception%2520is%2520to%2520significantly%2520reduce%2520the%2520required%2520amount%2520of%2520annotated%250Atraining%2520data%2520while%2520achieving%2520accurate%2520predictions.%2520This%2520is%2520essential%2520not%2520only%250Ato%2520decrease%2520operating%2520costs%2520but%2520also%2520to%2520speed%2520up%2520deployment%2520time.%2520In%2520this%2520work%252C%250Awe%2520address%2520this%2520challenge%2520for%2520PAnoptic%2520SegmenTation%2520with%2520fEw%2520Labels%2520%2528PASTEL%2529%2520by%250Aexploiting%2520the%2520groundwork%2520paved%2520by%2520visual%2520foundation%2520models.%2520We%2520leverage%250Adescriptive%2520image%2520features%2520from%2520such%2520a%2520model%2520to%2520train%2520two%2520lightweight%2520network%250Aheads%2520for%2520semantic%2520segmentation%2520and%2520object%2520boundary%2520detection%252C%2520using%2520very%2520few%250Aannotated%2520training%2520samples.%2520We%2520then%2520merge%2520their%2520predictions%2520via%2520a%2520novel%2520fusion%250Amodule%2520that%2520yields%2520panoptic%2520maps%2520based%2520on%2520normalized%2520cut.%2520To%2520further%2520enhance%250Athe%2520performance%252C%2520we%2520utilize%2520self-training%2520on%2520unlabeled%2520images%2520selected%2520by%2520a%250Afeature-driven%2520similarity%2520scheme.%2520We%2520underline%2520the%2520relevance%2520of%2520our%2520approach%2520by%250Aemploying%2520PASTEL%2520to%2520important%2520robot%2520perception%2520use%2520cases%2520from%2520autonomous%250Adriving%2520and%2520agricultural%2520robotics.%2520In%2520extensive%2520experiments%252C%2520we%2520demonstrate%250Athat%2520PASTEL%2520significantly%2520outperforms%2520previous%2520methods%2520for%2520label-efficient%250Asegmentation%2520even%2520when%2520using%2520fewer%2520annotations.%2520The%2520code%2520of%2520our%2520work%2520is%250Apublicly%2520available%2520at%2520http%253A//pastel.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Good%20Foundation%20is%20Worth%20Many%20Labels%3A%20Label-Efficient%20Panoptic%0A%20%20Segmentation&entry.906535625=Niclas%20V%C3%B6disch%20and%20K%C3%BCrsat%20Petek%20and%20Markus%20K%C3%A4ppeler%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard&entry.1292438233=%20%20A%20key%20challenge%20for%20the%20widespread%20application%20of%20learning-based%20models%20for%0Arobotic%20perception%20is%20to%20significantly%20reduce%20the%20required%20amount%20of%20annotated%0Atraining%20data%20while%20achieving%20accurate%20predictions.%20This%20is%20essential%20not%20only%0Ato%20decrease%20operating%20costs%20but%20also%20to%20speed%20up%20deployment%20time.%20In%20this%20work%2C%0Awe%20address%20this%20challenge%20for%20PAnoptic%20SegmenTation%20with%20fEw%20Labels%20%28PASTEL%29%20by%0Aexploiting%20the%20groundwork%20paved%20by%20visual%20foundation%20models.%20We%20leverage%0Adescriptive%20image%20features%20from%20such%20a%20model%20to%20train%20two%20lightweight%20network%0Aheads%20for%20semantic%20segmentation%20and%20object%20boundary%20detection%2C%20using%20very%20few%0Aannotated%20training%20samples.%20We%20then%20merge%20their%20predictions%20via%20a%20novel%20fusion%0Amodule%20that%20yields%20panoptic%20maps%20based%20on%20normalized%20cut.%20To%20further%20enhance%0Athe%20performance%2C%20we%20utilize%20self-training%20on%20unlabeled%20images%20selected%20by%20a%0Afeature-driven%20similarity%20scheme.%20We%20underline%20the%20relevance%20of%20our%20approach%20by%0Aemploying%20PASTEL%20to%20important%20robot%20perception%20use%20cases%20from%20autonomous%0Adriving%20and%20agricultural%20robotics.%20In%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20PASTEL%20significantly%20outperforms%20previous%20methods%20for%20label-efficient%0Asegmentation%20even%20when%20using%20fewer%20annotations.%20The%20code%20of%20our%20work%20is%0Apublicly%20available%20at%20http%3A//pastel.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19035v2&entry.124074799=Read"},
{"title": "LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation\n  with Instructional Inputs", "author": "Pranav Doma and Aliasghar Arab and Xuesu Xiao", "abstract": "  Autonomous navigation guided by natural language instructions is essential\nfor improving human-robot interaction and enabling complex operations in\ndynamic environments. While large language models (LLMs) are not inherently\ndesigned for planning, they can significantly enhance planning efficiency by\nproviding guidance and informing constraints to ensure safety. This paper\nintroduces a planning framework that integrates LLMs with 2D occupancy grid\nmaps and natural language commands to improve spatial reasoning and task\nexecution in resource-limited settings. By decomposing high-level commands and\nreal-time environmental data, the system generates structured navigation plans\nfor pick-and-place tasks, including obstacle avoidance, goal prioritization,\nand adaptive behaviors. The framework dynamically recalculates paths to address\nenvironmental changes and aligns with implicit social norms for seamless\nhuman-robot interaction. Our results demonstrates the potential of LLMs to\ndesign context-aware system to enhance navigation efficiency and safety in\nindustrial and dynamic environments.\n", "link": "http://arxiv.org/abs/2412.02655v1", "date": "2024-12-03", "relevancy": 2.3447, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5959}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5937}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Enhanced%20Path%20Planning%3A%20Safe%20and%20Efficient%20Autonomous%20Navigation%0A%20%20with%20Instructional%20Inputs&body=Title%3A%20LLM-Enhanced%20Path%20Planning%3A%20Safe%20and%20Efficient%20Autonomous%20Navigation%0A%20%20with%20Instructional%20Inputs%0AAuthor%3A%20Pranav%20Doma%20and%20Aliasghar%20Arab%20and%20Xuesu%20Xiao%0AAbstract%3A%20%20%20Autonomous%20navigation%20guided%20by%20natural%20language%20instructions%20is%20essential%0Afor%20improving%20human-robot%20interaction%20and%20enabling%20complex%20operations%20in%0Adynamic%20environments.%20While%20large%20language%20models%20%28LLMs%29%20are%20not%20inherently%0Adesigned%20for%20planning%2C%20they%20can%20significantly%20enhance%20planning%20efficiency%20by%0Aproviding%20guidance%20and%20informing%20constraints%20to%20ensure%20safety.%20This%20paper%0Aintroduces%20a%20planning%20framework%20that%20integrates%20LLMs%20with%202D%20occupancy%20grid%0Amaps%20and%20natural%20language%20commands%20to%20improve%20spatial%20reasoning%20and%20task%0Aexecution%20in%20resource-limited%20settings.%20By%20decomposing%20high-level%20commands%20and%0Areal-time%20environmental%20data%2C%20the%20system%20generates%20structured%20navigation%20plans%0Afor%20pick-and-place%20tasks%2C%20including%20obstacle%20avoidance%2C%20goal%20prioritization%2C%0Aand%20adaptive%20behaviors.%20The%20framework%20dynamically%20recalculates%20paths%20to%20address%0Aenvironmental%20changes%20and%20aligns%20with%20implicit%20social%20norms%20for%20seamless%0Ahuman-robot%20interaction.%20Our%20results%20demonstrates%20the%20potential%20of%20LLMs%20to%0Adesign%20context-aware%20system%20to%20enhance%20navigation%20efficiency%20and%20safety%20in%0Aindustrial%20and%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Enhanced%2520Path%2520Planning%253A%2520Safe%2520and%2520Efficient%2520Autonomous%2520Navigation%250A%2520%2520with%2520Instructional%2520Inputs%26entry.906535625%3DPranav%2520Doma%2520and%2520Aliasghar%2520Arab%2520and%2520Xuesu%2520Xiao%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520guided%2520by%2520natural%2520language%2520instructions%2520is%2520essential%250Afor%2520improving%2520human-robot%2520interaction%2520and%2520enabling%2520complex%2520operations%2520in%250Adynamic%2520environments.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520not%2520inherently%250Adesigned%2520for%2520planning%252C%2520they%2520can%2520significantly%2520enhance%2520planning%2520efficiency%2520by%250Aproviding%2520guidance%2520and%2520informing%2520constraints%2520to%2520ensure%2520safety.%2520This%2520paper%250Aintroduces%2520a%2520planning%2520framework%2520that%2520integrates%2520LLMs%2520with%25202D%2520occupancy%2520grid%250Amaps%2520and%2520natural%2520language%2520commands%2520to%2520improve%2520spatial%2520reasoning%2520and%2520task%250Aexecution%2520in%2520resource-limited%2520settings.%2520By%2520decomposing%2520high-level%2520commands%2520and%250Areal-time%2520environmental%2520data%252C%2520the%2520system%2520generates%2520structured%2520navigation%2520plans%250Afor%2520pick-and-place%2520tasks%252C%2520including%2520obstacle%2520avoidance%252C%2520goal%2520prioritization%252C%250Aand%2520adaptive%2520behaviors.%2520The%2520framework%2520dynamically%2520recalculates%2520paths%2520to%2520address%250Aenvironmental%2520changes%2520and%2520aligns%2520with%2520implicit%2520social%2520norms%2520for%2520seamless%250Ahuman-robot%2520interaction.%2520Our%2520results%2520demonstrates%2520the%2520potential%2520of%2520LLMs%2520to%250Adesign%2520context-aware%2520system%2520to%2520enhance%2520navigation%2520efficiency%2520and%2520safety%2520in%250Aindustrial%2520and%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Enhanced%20Path%20Planning%3A%20Safe%20and%20Efficient%20Autonomous%20Navigation%0A%20%20with%20Instructional%20Inputs&entry.906535625=Pranav%20Doma%20and%20Aliasghar%20Arab%20and%20Xuesu%20Xiao&entry.1292438233=%20%20Autonomous%20navigation%20guided%20by%20natural%20language%20instructions%20is%20essential%0Afor%20improving%20human-robot%20interaction%20and%20enabling%20complex%20operations%20in%0Adynamic%20environments.%20While%20large%20language%20models%20%28LLMs%29%20are%20not%20inherently%0Adesigned%20for%20planning%2C%20they%20can%20significantly%20enhance%20planning%20efficiency%20by%0Aproviding%20guidance%20and%20informing%20constraints%20to%20ensure%20safety.%20This%20paper%0Aintroduces%20a%20planning%20framework%20that%20integrates%20LLMs%20with%202D%20occupancy%20grid%0Amaps%20and%20natural%20language%20commands%20to%20improve%20spatial%20reasoning%20and%20task%0Aexecution%20in%20resource-limited%20settings.%20By%20decomposing%20high-level%20commands%20and%0Areal-time%20environmental%20data%2C%20the%20system%20generates%20structured%20navigation%20plans%0Afor%20pick-and-place%20tasks%2C%20including%20obstacle%20avoidance%2C%20goal%20prioritization%2C%0Aand%20adaptive%20behaviors.%20The%20framework%20dynamically%20recalculates%20paths%20to%20address%0Aenvironmental%20changes%20and%20aligns%20with%20implicit%20social%20norms%20for%20seamless%0Ahuman-robot%20interaction.%20Our%20results%20demonstrates%20the%20potential%20of%20LLMs%20to%0Adesign%20context-aware%20system%20to%20enhance%20navigation%20efficiency%20and%20safety%20in%0Aindustrial%20and%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02655v1&entry.124074799=Read"},
{"title": "HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for\n  Autonomous Driving", "author": "Zehuan Wu and Jingcheng Ni and Xiaodong Wang and Yuxin Guo and Rui Chen and Lewei Lu and Jifeng Dai and Yuwen Xiong", "abstract": "  Generative models have significantly improved the generation and prediction\nquality on either camera images or LiDAR point clouds for autonomous driving.\nHowever, a real-world autonomous driving system uses multiple kinds of input\nmodality, usually cameras and LiDARs, where they contain complementary\ninformation for generation, while existing generation methods ignore this\ncrucial feature, resulting in the generated results only covering separate 2D\nor 3D information. In order to fill the gap in 2D-3D multi-modal joint\ngeneration for autonomous driving, in this paper, we propose our framework,\n\\emph{HoloDrive}, to jointly generate the camera images and LiDAR point clouds.\nWe employ BEV-to-Camera and Camera-to-BEV transform modules between\nheterogeneous generative models, and introduce a depth prediction branch in the\n2D generative model to disambiguate the un-projecting from image space to BEV\nspace, then extend the method to predict the future by adding temporal\nstructure and carefully designed progressive training. Further, we conduct\nexperiments on single frame generation and world model benchmarks, and\ndemonstrate our method leads to significant performance gains over SOTA methods\nin terms of generation metrics.\n", "link": "http://arxiv.org/abs/2412.01407v2", "date": "2024-12-03", "relevancy": 2.3383, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.613}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5789}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloDrive%3A%20Holistic%202D-3D%20Multi-Modal%20Street%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20HoloDrive%3A%20Holistic%202D-3D%20Multi-Modal%20Street%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Zehuan%20Wu%20and%20Jingcheng%20Ni%20and%20Xiaodong%20Wang%20and%20Yuxin%20Guo%20and%20Rui%20Chen%20and%20Lewei%20Lu%20and%20Jifeng%20Dai%20and%20Yuwen%20Xiong%0AAbstract%3A%20%20%20Generative%20models%20have%20significantly%20improved%20the%20generation%20and%20prediction%0Aquality%20on%20either%20camera%20images%20or%20LiDAR%20point%20clouds%20for%20autonomous%20driving.%0AHowever%2C%20a%20real-world%20autonomous%20driving%20system%20uses%20multiple%20kinds%20of%20input%0Amodality%2C%20usually%20cameras%20and%20LiDARs%2C%20where%20they%20contain%20complementary%0Ainformation%20for%20generation%2C%20while%20existing%20generation%20methods%20ignore%20this%0Acrucial%20feature%2C%20resulting%20in%20the%20generated%20results%20only%20covering%20separate%202D%0Aor%203D%20information.%20In%20order%20to%20fill%20the%20gap%20in%202D-3D%20multi-modal%20joint%0Ageneration%20for%20autonomous%20driving%2C%20in%20this%20paper%2C%20we%20propose%20our%20framework%2C%0A%5Cemph%7BHoloDrive%7D%2C%20to%20jointly%20generate%20the%20camera%20images%20and%20LiDAR%20point%20clouds.%0AWe%20employ%20BEV-to-Camera%20and%20Camera-to-BEV%20transform%20modules%20between%0Aheterogeneous%20generative%20models%2C%20and%20introduce%20a%20depth%20prediction%20branch%20in%20the%0A2D%20generative%20model%20to%20disambiguate%20the%20un-projecting%20from%20image%20space%20to%20BEV%0Aspace%2C%20then%20extend%20the%20method%20to%20predict%20the%20future%20by%20adding%20temporal%0Astructure%20and%20carefully%20designed%20progressive%20training.%20Further%2C%20we%20conduct%0Aexperiments%20on%20single%20frame%20generation%20and%20world%20model%20benchmarks%2C%20and%0Ademonstrate%20our%20method%20leads%20to%20significant%20performance%20gains%20over%20SOTA%20methods%0Ain%20terms%20of%20generation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloDrive%253A%2520Holistic%25202D-3D%2520Multi-Modal%2520Street%2520Scene%2520Generation%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DZehuan%2520Wu%2520and%2520Jingcheng%2520Ni%2520and%2520Xiaodong%2520Wang%2520and%2520Yuxin%2520Guo%2520and%2520Rui%2520Chen%2520and%2520Lewei%2520Lu%2520and%2520Jifeng%2520Dai%2520and%2520Yuwen%2520Xiong%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520significantly%2520improved%2520the%2520generation%2520and%2520prediction%250Aquality%2520on%2520either%2520camera%2520images%2520or%2520LiDAR%2520point%2520clouds%2520for%2520autonomous%2520driving.%250AHowever%252C%2520a%2520real-world%2520autonomous%2520driving%2520system%2520uses%2520multiple%2520kinds%2520of%2520input%250Amodality%252C%2520usually%2520cameras%2520and%2520LiDARs%252C%2520where%2520they%2520contain%2520complementary%250Ainformation%2520for%2520generation%252C%2520while%2520existing%2520generation%2520methods%2520ignore%2520this%250Acrucial%2520feature%252C%2520resulting%2520in%2520the%2520generated%2520results%2520only%2520covering%2520separate%25202D%250Aor%25203D%2520information.%2520In%2520order%2520to%2520fill%2520the%2520gap%2520in%25202D-3D%2520multi-modal%2520joint%250Ageneration%2520for%2520autonomous%2520driving%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520our%2520framework%252C%250A%255Cemph%257BHoloDrive%257D%252C%2520to%2520jointly%2520generate%2520the%2520camera%2520images%2520and%2520LiDAR%2520point%2520clouds.%250AWe%2520employ%2520BEV-to-Camera%2520and%2520Camera-to-BEV%2520transform%2520modules%2520between%250Aheterogeneous%2520generative%2520models%252C%2520and%2520introduce%2520a%2520depth%2520prediction%2520branch%2520in%2520the%250A2D%2520generative%2520model%2520to%2520disambiguate%2520the%2520un-projecting%2520from%2520image%2520space%2520to%2520BEV%250Aspace%252C%2520then%2520extend%2520the%2520method%2520to%2520predict%2520the%2520future%2520by%2520adding%2520temporal%250Astructure%2520and%2520carefully%2520designed%2520progressive%2520training.%2520Further%252C%2520we%2520conduct%250Aexperiments%2520on%2520single%2520frame%2520generation%2520and%2520world%2520model%2520benchmarks%252C%2520and%250Ademonstrate%2520our%2520method%2520leads%2520to%2520significant%2520performance%2520gains%2520over%2520SOTA%2520methods%250Ain%2520terms%2520of%2520generation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloDrive%3A%20Holistic%202D-3D%20Multi-Modal%20Street%20Scene%20Generation%20for%0A%20%20Autonomous%20Driving&entry.906535625=Zehuan%20Wu%20and%20Jingcheng%20Ni%20and%20Xiaodong%20Wang%20and%20Yuxin%20Guo%20and%20Rui%20Chen%20and%20Lewei%20Lu%20and%20Jifeng%20Dai%20and%20Yuwen%20Xiong&entry.1292438233=%20%20Generative%20models%20have%20significantly%20improved%20the%20generation%20and%20prediction%0Aquality%20on%20either%20camera%20images%20or%20LiDAR%20point%20clouds%20for%20autonomous%20driving.%0AHowever%2C%20a%20real-world%20autonomous%20driving%20system%20uses%20multiple%20kinds%20of%20input%0Amodality%2C%20usually%20cameras%20and%20LiDARs%2C%20where%20they%20contain%20complementary%0Ainformation%20for%20generation%2C%20while%20existing%20generation%20methods%20ignore%20this%0Acrucial%20feature%2C%20resulting%20in%20the%20generated%20results%20only%20covering%20separate%202D%0Aor%203D%20information.%20In%20order%20to%20fill%20the%20gap%20in%202D-3D%20multi-modal%20joint%0Ageneration%20for%20autonomous%20driving%2C%20in%20this%20paper%2C%20we%20propose%20our%20framework%2C%0A%5Cemph%7BHoloDrive%7D%2C%20to%20jointly%20generate%20the%20camera%20images%20and%20LiDAR%20point%20clouds.%0AWe%20employ%20BEV-to-Camera%20and%20Camera-to-BEV%20transform%20modules%20between%0Aheterogeneous%20generative%20models%2C%20and%20introduce%20a%20depth%20prediction%20branch%20in%20the%0A2D%20generative%20model%20to%20disambiguate%20the%20un-projecting%20from%20image%20space%20to%20BEV%0Aspace%2C%20then%20extend%20the%20method%20to%20predict%20the%20future%20by%20adding%20temporal%0Astructure%20and%20carefully%20designed%20progressive%20training.%20Further%2C%20we%20conduct%0Aexperiments%20on%20single%20frame%20generation%20and%20world%20model%20benchmarks%2C%20and%0Ademonstrate%20our%20method%20leads%20to%20significant%20performance%20gains%20over%20SOTA%20methods%0Ain%20terms%20of%20generation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01407v2&entry.124074799=Read"},
{"title": "Interpretable Generalized Additive Models for Datasets with Missing\n  Values", "author": "Hayden McTavish and Jon Donnelly and Margo Seltzer and Cynthia Rudin", "abstract": "  Many important datasets contain samples that are missing one or more feature\nvalues. Maintaining the interpretability of machine learning models in the\npresence of such missing data is challenging. Singly or multiply imputing\nmissing values complicates the model's mapping from features to labels. On the\nother hand, reasoning on indicator variables that represent missingness\nintroduces a potentially large number of additional terms, sacrificing\nsparsity. We solve these problems with M-GAM, a sparse, generalized, additive\nmodeling approach that incorporates missingness indicators and their\ninteraction terms while maintaining sparsity through l0 regularization. We show\nthat M-GAM provides similar or superior accuracy to prior methods while\nsignificantly improving sparsity relative to either imputation or naive\ninclusion of indicator variables.\n", "link": "http://arxiv.org/abs/2412.02646v1", "date": "2024-12-03", "relevancy": 2.3195, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4679}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Generalized%20Additive%20Models%20for%20Datasets%20with%20Missing%0A%20%20Values&body=Title%3A%20Interpretable%20Generalized%20Additive%20Models%20for%20Datasets%20with%20Missing%0A%20%20Values%0AAuthor%3A%20Hayden%20McTavish%20and%20Jon%20Donnelly%20and%20Margo%20Seltzer%20and%20Cynthia%20Rudin%0AAbstract%3A%20%20%20Many%20important%20datasets%20contain%20samples%20that%20are%20missing%20one%20or%20more%20feature%0Avalues.%20Maintaining%20the%20interpretability%20of%20machine%20learning%20models%20in%20the%0Apresence%20of%20such%20missing%20data%20is%20challenging.%20Singly%20or%20multiply%20imputing%0Amissing%20values%20complicates%20the%20model%27s%20mapping%20from%20features%20to%20labels.%20On%20the%0Aother%20hand%2C%20reasoning%20on%20indicator%20variables%20that%20represent%20missingness%0Aintroduces%20a%20potentially%20large%20number%20of%20additional%20terms%2C%20sacrificing%0Asparsity.%20We%20solve%20these%20problems%20with%20M-GAM%2C%20a%20sparse%2C%20generalized%2C%20additive%0Amodeling%20approach%20that%20incorporates%20missingness%20indicators%20and%20their%0Ainteraction%20terms%20while%20maintaining%20sparsity%20through%20l0%20regularization.%20We%20show%0Athat%20M-GAM%20provides%20similar%20or%20superior%20accuracy%20to%20prior%20methods%20while%0Asignificantly%20improving%20sparsity%20relative%20to%20either%20imputation%20or%20naive%0Ainclusion%20of%20indicator%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Generalized%2520Additive%2520Models%2520for%2520Datasets%2520with%2520Missing%250A%2520%2520Values%26entry.906535625%3DHayden%2520McTavish%2520and%2520Jon%2520Donnelly%2520and%2520Margo%2520Seltzer%2520and%2520Cynthia%2520Rudin%26entry.1292438233%3D%2520%2520Many%2520important%2520datasets%2520contain%2520samples%2520that%2520are%2520missing%2520one%2520or%2520more%2520feature%250Avalues.%2520Maintaining%2520the%2520interpretability%2520of%2520machine%2520learning%2520models%2520in%2520the%250Apresence%2520of%2520such%2520missing%2520data%2520is%2520challenging.%2520Singly%2520or%2520multiply%2520imputing%250Amissing%2520values%2520complicates%2520the%2520model%2527s%2520mapping%2520from%2520features%2520to%2520labels.%2520On%2520the%250Aother%2520hand%252C%2520reasoning%2520on%2520indicator%2520variables%2520that%2520represent%2520missingness%250Aintroduces%2520a%2520potentially%2520large%2520number%2520of%2520additional%2520terms%252C%2520sacrificing%250Asparsity.%2520We%2520solve%2520these%2520problems%2520with%2520M-GAM%252C%2520a%2520sparse%252C%2520generalized%252C%2520additive%250Amodeling%2520approach%2520that%2520incorporates%2520missingness%2520indicators%2520and%2520their%250Ainteraction%2520terms%2520while%2520maintaining%2520sparsity%2520through%2520l0%2520regularization.%2520We%2520show%250Athat%2520M-GAM%2520provides%2520similar%2520or%2520superior%2520accuracy%2520to%2520prior%2520methods%2520while%250Asignificantly%2520improving%2520sparsity%2520relative%2520to%2520either%2520imputation%2520or%2520naive%250Ainclusion%2520of%2520indicator%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Generalized%20Additive%20Models%20for%20Datasets%20with%20Missing%0A%20%20Values&entry.906535625=Hayden%20McTavish%20and%20Jon%20Donnelly%20and%20Margo%20Seltzer%20and%20Cynthia%20Rudin&entry.1292438233=%20%20Many%20important%20datasets%20contain%20samples%20that%20are%20missing%20one%20or%20more%20feature%0Avalues.%20Maintaining%20the%20interpretability%20of%20machine%20learning%20models%20in%20the%0Apresence%20of%20such%20missing%20data%20is%20challenging.%20Singly%20or%20multiply%20imputing%0Amissing%20values%20complicates%20the%20model%27s%20mapping%20from%20features%20to%20labels.%20On%20the%0Aother%20hand%2C%20reasoning%20on%20indicator%20variables%20that%20represent%20missingness%0Aintroduces%20a%20potentially%20large%20number%20of%20additional%20terms%2C%20sacrificing%0Asparsity.%20We%20solve%20these%20problems%20with%20M-GAM%2C%20a%20sparse%2C%20generalized%2C%20additive%0Amodeling%20approach%20that%20incorporates%20missingness%20indicators%20and%20their%0Ainteraction%20terms%20while%20maintaining%20sparsity%20through%20l0%20regularization.%20We%20show%0Athat%20M-GAM%20provides%20similar%20or%20superior%20accuracy%20to%20prior%20methods%20while%0Asignificantly%20improving%20sparsity%20relative%20to%20either%20imputation%20or%20naive%0Ainclusion%20of%20indicator%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02646v1&entry.124074799=Read"},
{"title": "Interpretable Generalized Additive Models for Datasets with Missing\n  Values", "author": "Hayden McTavish and Jon Donnelly and Margo Seltzer and Cynthia Rudin", "abstract": "  Many important datasets contain samples that are missing one or more feature\nvalues. Maintaining the interpretability of machine learning models in the\npresence of such missing data is challenging. Singly or multiply imputing\nmissing values complicates the model's mapping from features to labels. On the\nother hand, reasoning on indicator variables that represent missingness\nintroduces a potentially large number of additional terms, sacrificing\nsparsity. We solve these problems with M-GAM, a sparse, generalized, additive\nmodeling approach that incorporates missingness indicators and their\ninteraction terms while maintaining sparsity through l0 regularization. We show\nthat M-GAM provides similar or superior accuracy to prior methods while\nsignificantly improving sparsity relative to either imputation or naive\ninclusion of indicator variables.\n", "link": "http://arxiv.org/abs/2412.02646v1", "date": "2024-12-03", "relevancy": 2.3195, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4679}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Generalized%20Additive%20Models%20for%20Datasets%20with%20Missing%0A%20%20Values&body=Title%3A%20Interpretable%20Generalized%20Additive%20Models%20for%20Datasets%20with%20Missing%0A%20%20Values%0AAuthor%3A%20Hayden%20McTavish%20and%20Jon%20Donnelly%20and%20Margo%20Seltzer%20and%20Cynthia%20Rudin%0AAbstract%3A%20%20%20Many%20important%20datasets%20contain%20samples%20that%20are%20missing%20one%20or%20more%20feature%0Avalues.%20Maintaining%20the%20interpretability%20of%20machine%20learning%20models%20in%20the%0Apresence%20of%20such%20missing%20data%20is%20challenging.%20Singly%20or%20multiply%20imputing%0Amissing%20values%20complicates%20the%20model%27s%20mapping%20from%20features%20to%20labels.%20On%20the%0Aother%20hand%2C%20reasoning%20on%20indicator%20variables%20that%20represent%20missingness%0Aintroduces%20a%20potentially%20large%20number%20of%20additional%20terms%2C%20sacrificing%0Asparsity.%20We%20solve%20these%20problems%20with%20M-GAM%2C%20a%20sparse%2C%20generalized%2C%20additive%0Amodeling%20approach%20that%20incorporates%20missingness%20indicators%20and%20their%0Ainteraction%20terms%20while%20maintaining%20sparsity%20through%20l0%20regularization.%20We%20show%0Athat%20M-GAM%20provides%20similar%20or%20superior%20accuracy%20to%20prior%20methods%20while%0Asignificantly%20improving%20sparsity%20relative%20to%20either%20imputation%20or%20naive%0Ainclusion%20of%20indicator%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Generalized%2520Additive%2520Models%2520for%2520Datasets%2520with%2520Missing%250A%2520%2520Values%26entry.906535625%3DHayden%2520McTavish%2520and%2520Jon%2520Donnelly%2520and%2520Margo%2520Seltzer%2520and%2520Cynthia%2520Rudin%26entry.1292438233%3D%2520%2520Many%2520important%2520datasets%2520contain%2520samples%2520that%2520are%2520missing%2520one%2520or%2520more%2520feature%250Avalues.%2520Maintaining%2520the%2520interpretability%2520of%2520machine%2520learning%2520models%2520in%2520the%250Apresence%2520of%2520such%2520missing%2520data%2520is%2520challenging.%2520Singly%2520or%2520multiply%2520imputing%250Amissing%2520values%2520complicates%2520the%2520model%2527s%2520mapping%2520from%2520features%2520to%2520labels.%2520On%2520the%250Aother%2520hand%252C%2520reasoning%2520on%2520indicator%2520variables%2520that%2520represent%2520missingness%250Aintroduces%2520a%2520potentially%2520large%2520number%2520of%2520additional%2520terms%252C%2520sacrificing%250Asparsity.%2520We%2520solve%2520these%2520problems%2520with%2520M-GAM%252C%2520a%2520sparse%252C%2520generalized%252C%2520additive%250Amodeling%2520approach%2520that%2520incorporates%2520missingness%2520indicators%2520and%2520their%250Ainteraction%2520terms%2520while%2520maintaining%2520sparsity%2520through%2520l0%2520regularization.%2520We%2520show%250Athat%2520M-GAM%2520provides%2520similar%2520or%2520superior%2520accuracy%2520to%2520prior%2520methods%2520while%250Asignificantly%2520improving%2520sparsity%2520relative%2520to%2520either%2520imputation%2520or%2520naive%250Ainclusion%2520of%2520indicator%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Generalized%20Additive%20Models%20for%20Datasets%20with%20Missing%0A%20%20Values&entry.906535625=Hayden%20McTavish%20and%20Jon%20Donnelly%20and%20Margo%20Seltzer%20and%20Cynthia%20Rudin&entry.1292438233=%20%20Many%20important%20datasets%20contain%20samples%20that%20are%20missing%20one%20or%20more%20feature%0Avalues.%20Maintaining%20the%20interpretability%20of%20machine%20learning%20models%20in%20the%0Apresence%20of%20such%20missing%20data%20is%20challenging.%20Singly%20or%20multiply%20imputing%0Amissing%20values%20complicates%20the%20model%27s%20mapping%20from%20features%20to%20labels.%20On%20the%0Aother%20hand%2C%20reasoning%20on%20indicator%20variables%20that%20represent%20missingness%0Aintroduces%20a%20potentially%20large%20number%20of%20additional%20terms%2C%20sacrificing%0Asparsity.%20We%20solve%20these%20problems%20with%20M-GAM%2C%20a%20sparse%2C%20generalized%2C%20additive%0Amodeling%20approach%20that%20incorporates%20missingness%20indicators%20and%20their%0Ainteraction%20terms%20while%20maintaining%20sparsity%20through%20l0%20regularization.%20We%20show%0Athat%20M-GAM%20provides%20similar%20or%20superior%20accuracy%20to%20prior%20methods%20while%0Asignificantly%20improving%20sparsity%20relative%20to%20either%20imputation%20or%20naive%0Ainclusion%20of%20indicator%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02646v1&entry.124074799=Read"},
{"title": "Scaling Image Tokenizers with Grouped Spherical Quantization", "author": "Jiangtao Wang and Zhen Qin and Yifan Zhang and Vincent Tao Hu and Bj\u00f6rn Ommer and Rania Briq and Stefan Kesselheim", "abstract": "  Vision tokenizers have gained a lot of attraction due to their scalability\nand compactness; previous works depend on old-school GAN-based hyperparameters,\nbiased comparisons, and a lack of comprehensive analysis of the scaling\nbehaviours. To tackle those issues, we introduce Grouped Spherical Quantization\n(GSQ), featuring spherical codebook initialization and lookup regularization to\nconstrain codebook latent to a spherical surface. Our empirical analysis of\nimage tokenizer training strategies demonstrates that GSQ-GAN achieves superior\nreconstruction quality over state-of-the-art methods with fewer training\niterations, providing a solid foundation for scaling studies. Building on this,\nwe systematically examine the scaling behaviours of GSQ, specifically in latent\ndimensionality, codebook size, and compression ratios, and their impact on\nmodel performance. Our findings reveal distinct behaviours at high and low\nspatial compression levels, underscoring challenges in representing\nhigh-dimensional latent spaces. We show that GSQ can restructure\nhigh-dimensional latent into compact, low-dimensional spaces, thus enabling\nefficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x\ndown-sampling with a reconstruction FID (rFID) of 0.50.\n", "link": "http://arxiv.org/abs/2412.02632v1", "date": "2024-12-03", "relevancy": 2.3144, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6161}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5772}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Image%20Tokenizers%20with%20Grouped%20Spherical%20Quantization&body=Title%3A%20Scaling%20Image%20Tokenizers%20with%20Grouped%20Spherical%20Quantization%0AAuthor%3A%20Jiangtao%20Wang%20and%20Zhen%20Qin%20and%20Yifan%20Zhang%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%20and%20Rania%20Briq%20and%20Stefan%20Kesselheim%0AAbstract%3A%20%20%20Vision%20tokenizers%20have%20gained%20a%20lot%20of%20attraction%20due%20to%20their%20scalability%0Aand%20compactness%3B%20previous%20works%20depend%20on%20old-school%20GAN-based%20hyperparameters%2C%0Abiased%20comparisons%2C%20and%20a%20lack%20of%20comprehensive%20analysis%20of%20the%20scaling%0Abehaviours.%20To%20tackle%20those%20issues%2C%20we%20introduce%20Grouped%20Spherical%20Quantization%0A%28GSQ%29%2C%20featuring%20spherical%20codebook%20initialization%20and%20lookup%20regularization%20to%0Aconstrain%20codebook%20latent%20to%20a%20spherical%20surface.%20Our%20empirical%20analysis%20of%0Aimage%20tokenizer%20training%20strategies%20demonstrates%20that%20GSQ-GAN%20achieves%20superior%0Areconstruction%20quality%20over%20state-of-the-art%20methods%20with%20fewer%20training%0Aiterations%2C%20providing%20a%20solid%20foundation%20for%20scaling%20studies.%20Building%20on%20this%2C%0Awe%20systematically%20examine%20the%20scaling%20behaviours%20of%20GSQ%2C%20specifically%20in%20latent%0Adimensionality%2C%20codebook%20size%2C%20and%20compression%20ratios%2C%20and%20their%20impact%20on%0Amodel%20performance.%20Our%20findings%20reveal%20distinct%20behaviours%20at%20high%20and%20low%0Aspatial%20compression%20levels%2C%20underscoring%20challenges%20in%20representing%0Ahigh-dimensional%20latent%20spaces.%20We%20show%20that%20GSQ%20can%20restructure%0Ahigh-dimensional%20latent%20into%20compact%2C%20low-dimensional%20spaces%2C%20thus%20enabling%0Aefficient%20scaling%20with%20improved%20quality.%20As%20a%20result%2C%20GSQ-GAN%20achieves%20a%2016x%0Adown-sampling%20with%20a%20reconstruction%20FID%20%28rFID%29%20of%200.50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Image%2520Tokenizers%2520with%2520Grouped%2520Spherical%2520Quantization%26entry.906535625%3DJiangtao%2520Wang%2520and%2520Zhen%2520Qin%2520and%2520Yifan%2520Zhang%2520and%2520Vincent%2520Tao%2520Hu%2520and%2520Bj%25C3%25B6rn%2520Ommer%2520and%2520Rania%2520Briq%2520and%2520Stefan%2520Kesselheim%26entry.1292438233%3D%2520%2520Vision%2520tokenizers%2520have%2520gained%2520a%2520lot%2520of%2520attraction%2520due%2520to%2520their%2520scalability%250Aand%2520compactness%253B%2520previous%2520works%2520depend%2520on%2520old-school%2520GAN-based%2520hyperparameters%252C%250Abiased%2520comparisons%252C%2520and%2520a%2520lack%2520of%2520comprehensive%2520analysis%2520of%2520the%2520scaling%250Abehaviours.%2520To%2520tackle%2520those%2520issues%252C%2520we%2520introduce%2520Grouped%2520Spherical%2520Quantization%250A%2528GSQ%2529%252C%2520featuring%2520spherical%2520codebook%2520initialization%2520and%2520lookup%2520regularization%2520to%250Aconstrain%2520codebook%2520latent%2520to%2520a%2520spherical%2520surface.%2520Our%2520empirical%2520analysis%2520of%250Aimage%2520tokenizer%2520training%2520strategies%2520demonstrates%2520that%2520GSQ-GAN%2520achieves%2520superior%250Areconstruction%2520quality%2520over%2520state-of-the-art%2520methods%2520with%2520fewer%2520training%250Aiterations%252C%2520providing%2520a%2520solid%2520foundation%2520for%2520scaling%2520studies.%2520Building%2520on%2520this%252C%250Awe%2520systematically%2520examine%2520the%2520scaling%2520behaviours%2520of%2520GSQ%252C%2520specifically%2520in%2520latent%250Adimensionality%252C%2520codebook%2520size%252C%2520and%2520compression%2520ratios%252C%2520and%2520their%2520impact%2520on%250Amodel%2520performance.%2520Our%2520findings%2520reveal%2520distinct%2520behaviours%2520at%2520high%2520and%2520low%250Aspatial%2520compression%2520levels%252C%2520underscoring%2520challenges%2520in%2520representing%250Ahigh-dimensional%2520latent%2520spaces.%2520We%2520show%2520that%2520GSQ%2520can%2520restructure%250Ahigh-dimensional%2520latent%2520into%2520compact%252C%2520low-dimensional%2520spaces%252C%2520thus%2520enabling%250Aefficient%2520scaling%2520with%2520improved%2520quality.%2520As%2520a%2520result%252C%2520GSQ-GAN%2520achieves%2520a%252016x%250Adown-sampling%2520with%2520a%2520reconstruction%2520FID%2520%2528rFID%2529%2520of%25200.50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Image%20Tokenizers%20with%20Grouped%20Spherical%20Quantization&entry.906535625=Jiangtao%20Wang%20and%20Zhen%20Qin%20and%20Yifan%20Zhang%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%20and%20Rania%20Briq%20and%20Stefan%20Kesselheim&entry.1292438233=%20%20Vision%20tokenizers%20have%20gained%20a%20lot%20of%20attraction%20due%20to%20their%20scalability%0Aand%20compactness%3B%20previous%20works%20depend%20on%20old-school%20GAN-based%20hyperparameters%2C%0Abiased%20comparisons%2C%20and%20a%20lack%20of%20comprehensive%20analysis%20of%20the%20scaling%0Abehaviours.%20To%20tackle%20those%20issues%2C%20we%20introduce%20Grouped%20Spherical%20Quantization%0A%28GSQ%29%2C%20featuring%20spherical%20codebook%20initialization%20and%20lookup%20regularization%20to%0Aconstrain%20codebook%20latent%20to%20a%20spherical%20surface.%20Our%20empirical%20analysis%20of%0Aimage%20tokenizer%20training%20strategies%20demonstrates%20that%20GSQ-GAN%20achieves%20superior%0Areconstruction%20quality%20over%20state-of-the-art%20methods%20with%20fewer%20training%0Aiterations%2C%20providing%20a%20solid%20foundation%20for%20scaling%20studies.%20Building%20on%20this%2C%0Awe%20systematically%20examine%20the%20scaling%20behaviours%20of%20GSQ%2C%20specifically%20in%20latent%0Adimensionality%2C%20codebook%20size%2C%20and%20compression%20ratios%2C%20and%20their%20impact%20on%0Amodel%20performance.%20Our%20findings%20reveal%20distinct%20behaviours%20at%20high%20and%20low%0Aspatial%20compression%20levels%2C%20underscoring%20challenges%20in%20representing%0Ahigh-dimensional%20latent%20spaces.%20We%20show%20that%20GSQ%20can%20restructure%0Ahigh-dimensional%20latent%20into%20compact%2C%20low-dimensional%20spaces%2C%20thus%20enabling%0Aefficient%20scaling%20with%20improved%20quality.%20As%20a%20result%2C%20GSQ-GAN%20achieves%20a%2016x%0Adown-sampling%20with%20a%20reconstruction%20FID%20%28rFID%29%20of%200.50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02632v1&entry.124074799=Read"},
{"title": "Scaling Image Tokenizers with Grouped Spherical Quantization", "author": "Jiangtao Wang and Zhen Qin and Yifan Zhang and Vincent Tao Hu and Bj\u00f6rn Ommer and Rania Briq and Stefan Kesselheim", "abstract": "  Vision tokenizers have gained a lot of attraction due to their scalability\nand compactness; previous works depend on old-school GAN-based hyperparameters,\nbiased comparisons, and a lack of comprehensive analysis of the scaling\nbehaviours. To tackle those issues, we introduce Grouped Spherical Quantization\n(GSQ), featuring spherical codebook initialization and lookup regularization to\nconstrain codebook latent to a spherical surface. Our empirical analysis of\nimage tokenizer training strategies demonstrates that GSQ-GAN achieves superior\nreconstruction quality over state-of-the-art methods with fewer training\niterations, providing a solid foundation for scaling studies. Building on this,\nwe systematically examine the scaling behaviours of GSQ, specifically in latent\ndimensionality, codebook size, and compression ratios, and their impact on\nmodel performance. Our findings reveal distinct behaviours at high and low\nspatial compression levels, underscoring challenges in representing\nhigh-dimensional latent spaces. We show that GSQ can restructure\nhigh-dimensional latent into compact, low-dimensional spaces, thus enabling\nefficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x\ndown-sampling with a reconstruction FID (rFID) of 0.50.\n", "link": "http://arxiv.org/abs/2412.02632v1", "date": "2024-12-03", "relevancy": 2.3144, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6161}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5772}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Image%20Tokenizers%20with%20Grouped%20Spherical%20Quantization&body=Title%3A%20Scaling%20Image%20Tokenizers%20with%20Grouped%20Spherical%20Quantization%0AAuthor%3A%20Jiangtao%20Wang%20and%20Zhen%20Qin%20and%20Yifan%20Zhang%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%20and%20Rania%20Briq%20and%20Stefan%20Kesselheim%0AAbstract%3A%20%20%20Vision%20tokenizers%20have%20gained%20a%20lot%20of%20attraction%20due%20to%20their%20scalability%0Aand%20compactness%3B%20previous%20works%20depend%20on%20old-school%20GAN-based%20hyperparameters%2C%0Abiased%20comparisons%2C%20and%20a%20lack%20of%20comprehensive%20analysis%20of%20the%20scaling%0Abehaviours.%20To%20tackle%20those%20issues%2C%20we%20introduce%20Grouped%20Spherical%20Quantization%0A%28GSQ%29%2C%20featuring%20spherical%20codebook%20initialization%20and%20lookup%20regularization%20to%0Aconstrain%20codebook%20latent%20to%20a%20spherical%20surface.%20Our%20empirical%20analysis%20of%0Aimage%20tokenizer%20training%20strategies%20demonstrates%20that%20GSQ-GAN%20achieves%20superior%0Areconstruction%20quality%20over%20state-of-the-art%20methods%20with%20fewer%20training%0Aiterations%2C%20providing%20a%20solid%20foundation%20for%20scaling%20studies.%20Building%20on%20this%2C%0Awe%20systematically%20examine%20the%20scaling%20behaviours%20of%20GSQ%2C%20specifically%20in%20latent%0Adimensionality%2C%20codebook%20size%2C%20and%20compression%20ratios%2C%20and%20their%20impact%20on%0Amodel%20performance.%20Our%20findings%20reveal%20distinct%20behaviours%20at%20high%20and%20low%0Aspatial%20compression%20levels%2C%20underscoring%20challenges%20in%20representing%0Ahigh-dimensional%20latent%20spaces.%20We%20show%20that%20GSQ%20can%20restructure%0Ahigh-dimensional%20latent%20into%20compact%2C%20low-dimensional%20spaces%2C%20thus%20enabling%0Aefficient%20scaling%20with%20improved%20quality.%20As%20a%20result%2C%20GSQ-GAN%20achieves%20a%2016x%0Adown-sampling%20with%20a%20reconstruction%20FID%20%28rFID%29%20of%200.50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Image%2520Tokenizers%2520with%2520Grouped%2520Spherical%2520Quantization%26entry.906535625%3DJiangtao%2520Wang%2520and%2520Zhen%2520Qin%2520and%2520Yifan%2520Zhang%2520and%2520Vincent%2520Tao%2520Hu%2520and%2520Bj%25C3%25B6rn%2520Ommer%2520and%2520Rania%2520Briq%2520and%2520Stefan%2520Kesselheim%26entry.1292438233%3D%2520%2520Vision%2520tokenizers%2520have%2520gained%2520a%2520lot%2520of%2520attraction%2520due%2520to%2520their%2520scalability%250Aand%2520compactness%253B%2520previous%2520works%2520depend%2520on%2520old-school%2520GAN-based%2520hyperparameters%252C%250Abiased%2520comparisons%252C%2520and%2520a%2520lack%2520of%2520comprehensive%2520analysis%2520of%2520the%2520scaling%250Abehaviours.%2520To%2520tackle%2520those%2520issues%252C%2520we%2520introduce%2520Grouped%2520Spherical%2520Quantization%250A%2528GSQ%2529%252C%2520featuring%2520spherical%2520codebook%2520initialization%2520and%2520lookup%2520regularization%2520to%250Aconstrain%2520codebook%2520latent%2520to%2520a%2520spherical%2520surface.%2520Our%2520empirical%2520analysis%2520of%250Aimage%2520tokenizer%2520training%2520strategies%2520demonstrates%2520that%2520GSQ-GAN%2520achieves%2520superior%250Areconstruction%2520quality%2520over%2520state-of-the-art%2520methods%2520with%2520fewer%2520training%250Aiterations%252C%2520providing%2520a%2520solid%2520foundation%2520for%2520scaling%2520studies.%2520Building%2520on%2520this%252C%250Awe%2520systematically%2520examine%2520the%2520scaling%2520behaviours%2520of%2520GSQ%252C%2520specifically%2520in%2520latent%250Adimensionality%252C%2520codebook%2520size%252C%2520and%2520compression%2520ratios%252C%2520and%2520their%2520impact%2520on%250Amodel%2520performance.%2520Our%2520findings%2520reveal%2520distinct%2520behaviours%2520at%2520high%2520and%2520low%250Aspatial%2520compression%2520levels%252C%2520underscoring%2520challenges%2520in%2520representing%250Ahigh-dimensional%2520latent%2520spaces.%2520We%2520show%2520that%2520GSQ%2520can%2520restructure%250Ahigh-dimensional%2520latent%2520into%2520compact%252C%2520low-dimensional%2520spaces%252C%2520thus%2520enabling%250Aefficient%2520scaling%2520with%2520improved%2520quality.%2520As%2520a%2520result%252C%2520GSQ-GAN%2520achieves%2520a%252016x%250Adown-sampling%2520with%2520a%2520reconstruction%2520FID%2520%2528rFID%2529%2520of%25200.50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Image%20Tokenizers%20with%20Grouped%20Spherical%20Quantization&entry.906535625=Jiangtao%20Wang%20and%20Zhen%20Qin%20and%20Yifan%20Zhang%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%20and%20Rania%20Briq%20and%20Stefan%20Kesselheim&entry.1292438233=%20%20Vision%20tokenizers%20have%20gained%20a%20lot%20of%20attraction%20due%20to%20their%20scalability%0Aand%20compactness%3B%20previous%20works%20depend%20on%20old-school%20GAN-based%20hyperparameters%2C%0Abiased%20comparisons%2C%20and%20a%20lack%20of%20comprehensive%20analysis%20of%20the%20scaling%0Abehaviours.%20To%20tackle%20those%20issues%2C%20we%20introduce%20Grouped%20Spherical%20Quantization%0A%28GSQ%29%2C%20featuring%20spherical%20codebook%20initialization%20and%20lookup%20regularization%20to%0Aconstrain%20codebook%20latent%20to%20a%20spherical%20surface.%20Our%20empirical%20analysis%20of%0Aimage%20tokenizer%20training%20strategies%20demonstrates%20that%20GSQ-GAN%20achieves%20superior%0Areconstruction%20quality%20over%20state-of-the-art%20methods%20with%20fewer%20training%0Aiterations%2C%20providing%20a%20solid%20foundation%20for%20scaling%20studies.%20Building%20on%20this%2C%0Awe%20systematically%20examine%20the%20scaling%20behaviours%20of%20GSQ%2C%20specifically%20in%20latent%0Adimensionality%2C%20codebook%20size%2C%20and%20compression%20ratios%2C%20and%20their%20impact%20on%0Amodel%20performance.%20Our%20findings%20reveal%20distinct%20behaviours%20at%20high%20and%20low%0Aspatial%20compression%20levels%2C%20underscoring%20challenges%20in%20representing%0Ahigh-dimensional%20latent%20spaces.%20We%20show%20that%20GSQ%20can%20restructure%0Ahigh-dimensional%20latent%20into%20compact%2C%20low-dimensional%20spaces%2C%20thus%20enabling%0Aefficient%20scaling%20with%20improved%20quality.%20As%20a%20result%2C%20GSQ-GAN%20achieves%20a%2016x%0Adown-sampling%20with%20a%20reconstruction%20FID%20%28rFID%29%20of%200.50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02632v1&entry.124074799=Read"},
{"title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification", "author": "Wenxuan Huang and Zijie Zhai and Yunhang Shen and Shaoshen Cao and Fei Zhao and Xiangfeng Xu and Zheyu Ye and Shaohui Lin", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n", "link": "http://arxiv.org/abs/2412.00876v2", "date": "2024-12-03", "relevancy": 2.2971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification&body=Title%3A%20Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification%0AAuthor%3A%20Wenxuan%20Huang%20and%20Zijie%20Zhai%20and%20Yunhang%20Shen%20and%20Shaoshen%20Cao%20and%20Fei%20Zhao%20and%20Xiangfeng%20Xu%20and%20Zheyu%20Ye%20and%20Shaohui%20Lin%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Avision%20understanding%2C%20reasoning%2C%20and%20interaction.%20However%2C%20the%20inference%0Acomputation%20and%20memory%20increase%20progressively%20with%20the%20generation%20of%20output%0Atokens%20during%20decoding%2C%20directly%20affecting%20the%20efficacy%20of%20MLLMs.%20Existing%0Amethods%20attempt%20to%20reduce%20the%20vision%20context%20redundancy%20to%20achieve%20efficient%0AMLLMs.%20Unfortunately%2C%20the%20efficiency%20benefits%20of%20the%20vision%20context%20reduction%0Ain%20the%20prefill%20stage%20gradually%20diminish%20during%20the%20decoding%20stage.%20To%20address%0Athis%20problem%2C%20we%20proposed%20a%20dynamic%20vision-language%20context%20sparsification%0Aframework%20Dynamic-LLaVA%2C%20which%20dynamically%20reduces%20the%20redundancy%20of%20vision%0Acontext%20in%20the%20prefill%20stage%20and%20decreases%20the%20memory%20and%20computation%20overhead%0Aof%20the%20generated%20language%20context%20during%20decoding.%20Dynamic-LLaVA%20designs%20a%0Atailored%20sparsification%20inference%20scheme%20for%20different%20inference%20modes%2C%20i.e.%2C%0Aprefill%2C%20decoding%20with%20and%20without%20KV%20cache%2C%20to%20achieve%20efficient%20inference%20of%0AMLLMs.%20In%20practice%2C%20Dynamic-LLaVA%20can%20reduce%20computation%20consumption%20by%0A%24%5Csim%2475%5C%25%20in%20the%20prefill%20stage.%20Meanwhile%2C%20throughout%20the%20entire%20generation%0Aprocess%20of%20MLLMs%2C%20Dynamic-LLaVA%20reduces%20the%20%24%5Csim%2450%5C%25%20computation%20consumption%0Aunder%20decoding%20without%20KV%20cache%2C%20while%20saving%20%24%5Csim%2450%5C%25%20GPU%20memory%20overhead%0Awhen%20decoding%20with%20KV%20cache%2C%20due%20to%20the%20vision-language%20context%20sparsification.%0AExtensive%20experiments%20also%20demonstrate%20that%20Dynamic-LLaVA%20achieves%20efficient%0Ainference%20for%20MLLMs%20with%20negligible%20understanding%20and%20generation%20ability%0Adegradation%20or%20even%20performance%20gains%20compared%20to%20the%20full-context%20inference%0Abaselines.%20Code%20is%20available%20at%20https%3A//github.com/Osilly/dynamic_llava%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic-LLaVA%253A%2520Efficient%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Dynamic%250A%2520%2520Vision-language%2520Context%2520Sparsification%26entry.906535625%3DWenxuan%2520Huang%2520and%2520Zijie%2520Zhai%2520and%2520Yunhang%2520Shen%2520and%2520Shaoshen%2520Cao%2520and%2520Fei%2520Zhao%2520and%2520Xiangfeng%2520Xu%2520and%2520Zheyu%2520Ye%2520and%2520Shaohui%2520Lin%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%250Avision%2520understanding%252C%2520reasoning%252C%2520and%2520interaction.%2520However%252C%2520the%2520inference%250Acomputation%2520and%2520memory%2520increase%2520progressively%2520with%2520the%2520generation%2520of%2520output%250Atokens%2520during%2520decoding%252C%2520directly%2520affecting%2520the%2520efficacy%2520of%2520MLLMs.%2520Existing%250Amethods%2520attempt%2520to%2520reduce%2520the%2520vision%2520context%2520redundancy%2520to%2520achieve%2520efficient%250AMLLMs.%2520Unfortunately%252C%2520the%2520efficiency%2520benefits%2520of%2520the%2520vision%2520context%2520reduction%250Ain%2520the%2520prefill%2520stage%2520gradually%2520diminish%2520during%2520the%2520decoding%2520stage.%2520To%2520address%250Athis%2520problem%252C%2520we%2520proposed%2520a%2520dynamic%2520vision-language%2520context%2520sparsification%250Aframework%2520Dynamic-LLaVA%252C%2520which%2520dynamically%2520reduces%2520the%2520redundancy%2520of%2520vision%250Acontext%2520in%2520the%2520prefill%2520stage%2520and%2520decreases%2520the%2520memory%2520and%2520computation%2520overhead%250Aof%2520the%2520generated%2520language%2520context%2520during%2520decoding.%2520Dynamic-LLaVA%2520designs%2520a%250Atailored%2520sparsification%2520inference%2520scheme%2520for%2520different%2520inference%2520modes%252C%2520i.e.%252C%250Aprefill%252C%2520decoding%2520with%2520and%2520without%2520KV%2520cache%252C%2520to%2520achieve%2520efficient%2520inference%2520of%250AMLLMs.%2520In%2520practice%252C%2520Dynamic-LLaVA%2520can%2520reduce%2520computation%2520consumption%2520by%250A%2524%255Csim%252475%255C%2525%2520in%2520the%2520prefill%2520stage.%2520Meanwhile%252C%2520throughout%2520the%2520entire%2520generation%250Aprocess%2520of%2520MLLMs%252C%2520Dynamic-LLaVA%2520reduces%2520the%2520%2524%255Csim%252450%255C%2525%2520computation%2520consumption%250Aunder%2520decoding%2520without%2520KV%2520cache%252C%2520while%2520saving%2520%2524%255Csim%252450%255C%2525%2520GPU%2520memory%2520overhead%250Awhen%2520decoding%2520with%2520KV%2520cache%252C%2520due%2520to%2520the%2520vision-language%2520context%2520sparsification.%250AExtensive%2520experiments%2520also%2520demonstrate%2520that%2520Dynamic-LLaVA%2520achieves%2520efficient%250Ainference%2520for%2520MLLMs%2520with%2520negligible%2520understanding%2520and%2520generation%2520ability%250Adegradation%2520or%2520even%2520performance%2520gains%2520compared%2520to%2520the%2520full-context%2520inference%250Abaselines.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Osilly/dynamic_llava%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification&entry.906535625=Wenxuan%20Huang%20and%20Zijie%20Zhai%20and%20Yunhang%20Shen%20and%20Shaoshen%20Cao%20and%20Fei%20Zhao%20and%20Xiangfeng%20Xu%20and%20Zheyu%20Ye%20and%20Shaohui%20Lin&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Avision%20understanding%2C%20reasoning%2C%20and%20interaction.%20However%2C%20the%20inference%0Acomputation%20and%20memory%20increase%20progressively%20with%20the%20generation%20of%20output%0Atokens%20during%20decoding%2C%20directly%20affecting%20the%20efficacy%20of%20MLLMs.%20Existing%0Amethods%20attempt%20to%20reduce%20the%20vision%20context%20redundancy%20to%20achieve%20efficient%0AMLLMs.%20Unfortunately%2C%20the%20efficiency%20benefits%20of%20the%20vision%20context%20reduction%0Ain%20the%20prefill%20stage%20gradually%20diminish%20during%20the%20decoding%20stage.%20To%20address%0Athis%20problem%2C%20we%20proposed%20a%20dynamic%20vision-language%20context%20sparsification%0Aframework%20Dynamic-LLaVA%2C%20which%20dynamically%20reduces%20the%20redundancy%20of%20vision%0Acontext%20in%20the%20prefill%20stage%20and%20decreases%20the%20memory%20and%20computation%20overhead%0Aof%20the%20generated%20language%20context%20during%20decoding.%20Dynamic-LLaVA%20designs%20a%0Atailored%20sparsification%20inference%20scheme%20for%20different%20inference%20modes%2C%20i.e.%2C%0Aprefill%2C%20decoding%20with%20and%20without%20KV%20cache%2C%20to%20achieve%20efficient%20inference%20of%0AMLLMs.%20In%20practice%2C%20Dynamic-LLaVA%20can%20reduce%20computation%20consumption%20by%0A%24%5Csim%2475%5C%25%20in%20the%20prefill%20stage.%20Meanwhile%2C%20throughout%20the%20entire%20generation%0Aprocess%20of%20MLLMs%2C%20Dynamic-LLaVA%20reduces%20the%20%24%5Csim%2450%5C%25%20computation%20consumption%0Aunder%20decoding%20without%20KV%20cache%2C%20while%20saving%20%24%5Csim%2450%5C%25%20GPU%20memory%20overhead%0Awhen%20decoding%20with%20KV%20cache%2C%20due%20to%20the%20vision-language%20context%20sparsification.%0AExtensive%20experiments%20also%20demonstrate%20that%20Dynamic-LLaVA%20achieves%20efficient%0Ainference%20for%20MLLMs%20with%20negligible%20understanding%20and%20generation%20ability%0Adegradation%20or%20even%20performance%20gains%20compared%20to%20the%20full-context%20inference%0Abaselines.%20Code%20is%20available%20at%20https%3A//github.com/Osilly/dynamic_llava%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00876v2&entry.124074799=Read"},
{"title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification", "author": "Wenxuan Huang and Zijie Zhai and Yunhang Shen and Shaoshen Cao and Fei Zhao and Xiangfeng Xu and Zheyu Ye and Shaohui Lin", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n", "link": "http://arxiv.org/abs/2412.00876v2", "date": "2024-12-03", "relevancy": 2.2971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification&body=Title%3A%20Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification%0AAuthor%3A%20Wenxuan%20Huang%20and%20Zijie%20Zhai%20and%20Yunhang%20Shen%20and%20Shaoshen%20Cao%20and%20Fei%20Zhao%20and%20Xiangfeng%20Xu%20and%20Zheyu%20Ye%20and%20Shaohui%20Lin%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Avision%20understanding%2C%20reasoning%2C%20and%20interaction.%20However%2C%20the%20inference%0Acomputation%20and%20memory%20increase%20progressively%20with%20the%20generation%20of%20output%0Atokens%20during%20decoding%2C%20directly%20affecting%20the%20efficacy%20of%20MLLMs.%20Existing%0Amethods%20attempt%20to%20reduce%20the%20vision%20context%20redundancy%20to%20achieve%20efficient%0AMLLMs.%20Unfortunately%2C%20the%20efficiency%20benefits%20of%20the%20vision%20context%20reduction%0Ain%20the%20prefill%20stage%20gradually%20diminish%20during%20the%20decoding%20stage.%20To%20address%0Athis%20problem%2C%20we%20proposed%20a%20dynamic%20vision-language%20context%20sparsification%0Aframework%20Dynamic-LLaVA%2C%20which%20dynamically%20reduces%20the%20redundancy%20of%20vision%0Acontext%20in%20the%20prefill%20stage%20and%20decreases%20the%20memory%20and%20computation%20overhead%0Aof%20the%20generated%20language%20context%20during%20decoding.%20Dynamic-LLaVA%20designs%20a%0Atailored%20sparsification%20inference%20scheme%20for%20different%20inference%20modes%2C%20i.e.%2C%0Aprefill%2C%20decoding%20with%20and%20without%20KV%20cache%2C%20to%20achieve%20efficient%20inference%20of%0AMLLMs.%20In%20practice%2C%20Dynamic-LLaVA%20can%20reduce%20computation%20consumption%20by%0A%24%5Csim%2475%5C%25%20in%20the%20prefill%20stage.%20Meanwhile%2C%20throughout%20the%20entire%20generation%0Aprocess%20of%20MLLMs%2C%20Dynamic-LLaVA%20reduces%20the%20%24%5Csim%2450%5C%25%20computation%20consumption%0Aunder%20decoding%20without%20KV%20cache%2C%20while%20saving%20%24%5Csim%2450%5C%25%20GPU%20memory%20overhead%0Awhen%20decoding%20with%20KV%20cache%2C%20due%20to%20the%20vision-language%20context%20sparsification.%0AExtensive%20experiments%20also%20demonstrate%20that%20Dynamic-LLaVA%20achieves%20efficient%0Ainference%20for%20MLLMs%20with%20negligible%20understanding%20and%20generation%20ability%0Adegradation%20or%20even%20performance%20gains%20compared%20to%20the%20full-context%20inference%0Abaselines.%20Code%20is%20available%20at%20https%3A//github.com/Osilly/dynamic_llava%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic-LLaVA%253A%2520Efficient%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Dynamic%250A%2520%2520Vision-language%2520Context%2520Sparsification%26entry.906535625%3DWenxuan%2520Huang%2520and%2520Zijie%2520Zhai%2520and%2520Yunhang%2520Shen%2520and%2520Shaoshen%2520Cao%2520and%2520Fei%2520Zhao%2520and%2520Xiangfeng%2520Xu%2520and%2520Zheyu%2520Ye%2520and%2520Shaohui%2520Lin%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%250Avision%2520understanding%252C%2520reasoning%252C%2520and%2520interaction.%2520However%252C%2520the%2520inference%250Acomputation%2520and%2520memory%2520increase%2520progressively%2520with%2520the%2520generation%2520of%2520output%250Atokens%2520during%2520decoding%252C%2520directly%2520affecting%2520the%2520efficacy%2520of%2520MLLMs.%2520Existing%250Amethods%2520attempt%2520to%2520reduce%2520the%2520vision%2520context%2520redundancy%2520to%2520achieve%2520efficient%250AMLLMs.%2520Unfortunately%252C%2520the%2520efficiency%2520benefits%2520of%2520the%2520vision%2520context%2520reduction%250Ain%2520the%2520prefill%2520stage%2520gradually%2520diminish%2520during%2520the%2520decoding%2520stage.%2520To%2520address%250Athis%2520problem%252C%2520we%2520proposed%2520a%2520dynamic%2520vision-language%2520context%2520sparsification%250Aframework%2520Dynamic-LLaVA%252C%2520which%2520dynamically%2520reduces%2520the%2520redundancy%2520of%2520vision%250Acontext%2520in%2520the%2520prefill%2520stage%2520and%2520decreases%2520the%2520memory%2520and%2520computation%2520overhead%250Aof%2520the%2520generated%2520language%2520context%2520during%2520decoding.%2520Dynamic-LLaVA%2520designs%2520a%250Atailored%2520sparsification%2520inference%2520scheme%2520for%2520different%2520inference%2520modes%252C%2520i.e.%252C%250Aprefill%252C%2520decoding%2520with%2520and%2520without%2520KV%2520cache%252C%2520to%2520achieve%2520efficient%2520inference%2520of%250AMLLMs.%2520In%2520practice%252C%2520Dynamic-LLaVA%2520can%2520reduce%2520computation%2520consumption%2520by%250A%2524%255Csim%252475%255C%2525%2520in%2520the%2520prefill%2520stage.%2520Meanwhile%252C%2520throughout%2520the%2520entire%2520generation%250Aprocess%2520of%2520MLLMs%252C%2520Dynamic-LLaVA%2520reduces%2520the%2520%2524%255Csim%252450%255C%2525%2520computation%2520consumption%250Aunder%2520decoding%2520without%2520KV%2520cache%252C%2520while%2520saving%2520%2524%255Csim%252450%255C%2525%2520GPU%2520memory%2520overhead%250Awhen%2520decoding%2520with%2520KV%2520cache%252C%2520due%2520to%2520the%2520vision-language%2520context%2520sparsification.%250AExtensive%2520experiments%2520also%2520demonstrate%2520that%2520Dynamic-LLaVA%2520achieves%2520efficient%250Ainference%2520for%2520MLLMs%2520with%2520negligible%2520understanding%2520and%2520generation%2520ability%250Adegradation%2520or%2520even%2520performance%2520gains%2520compared%2520to%2520the%2520full-context%2520inference%250Abaselines.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Osilly/dynamic_llava%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification&entry.906535625=Wenxuan%20Huang%20and%20Zijie%20Zhai%20and%20Yunhang%20Shen%20and%20Shaoshen%20Cao%20and%20Fei%20Zhao%20and%20Xiangfeng%20Xu%20and%20Zheyu%20Ye%20and%20Shaohui%20Lin&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Avision%20understanding%2C%20reasoning%2C%20and%20interaction.%20However%2C%20the%20inference%0Acomputation%20and%20memory%20increase%20progressively%20with%20the%20generation%20of%20output%0Atokens%20during%20decoding%2C%20directly%20affecting%20the%20efficacy%20of%20MLLMs.%20Existing%0Amethods%20attempt%20to%20reduce%20the%20vision%20context%20redundancy%20to%20achieve%20efficient%0AMLLMs.%20Unfortunately%2C%20the%20efficiency%20benefits%20of%20the%20vision%20context%20reduction%0Ain%20the%20prefill%20stage%20gradually%20diminish%20during%20the%20decoding%20stage.%20To%20address%0Athis%20problem%2C%20we%20proposed%20a%20dynamic%20vision-language%20context%20sparsification%0Aframework%20Dynamic-LLaVA%2C%20which%20dynamically%20reduces%20the%20redundancy%20of%20vision%0Acontext%20in%20the%20prefill%20stage%20and%20decreases%20the%20memory%20and%20computation%20overhead%0Aof%20the%20generated%20language%20context%20during%20decoding.%20Dynamic-LLaVA%20designs%20a%0Atailored%20sparsification%20inference%20scheme%20for%20different%20inference%20modes%2C%20i.e.%2C%0Aprefill%2C%20decoding%20with%20and%20without%20KV%20cache%2C%20to%20achieve%20efficient%20inference%20of%0AMLLMs.%20In%20practice%2C%20Dynamic-LLaVA%20can%20reduce%20computation%20consumption%20by%0A%24%5Csim%2475%5C%25%20in%20the%20prefill%20stage.%20Meanwhile%2C%20throughout%20the%20entire%20generation%0Aprocess%20of%20MLLMs%2C%20Dynamic-LLaVA%20reduces%20the%20%24%5Csim%2450%5C%25%20computation%20consumption%0Aunder%20decoding%20without%20KV%20cache%2C%20while%20saving%20%24%5Csim%2450%5C%25%20GPU%20memory%20overhead%0Awhen%20decoding%20with%20KV%20cache%2C%20due%20to%20the%20vision-language%20context%20sparsification.%0AExtensive%20experiments%20also%20demonstrate%20that%20Dynamic-LLaVA%20achieves%20efficient%0Ainference%20for%20MLLMs%20with%20negligible%20understanding%20and%20generation%20ability%0Adegradation%20or%20even%20performance%20gains%20compared%20to%20the%20full-context%20inference%0Abaselines.%20Code%20is%20available%20at%20https%3A//github.com/Osilly/dynamic_llava%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00876v2&entry.124074799=Read"},
{"title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification", "author": "Wenxuan Huang and Zijie Zhai and Yunhang Shen and Shaoshen Cao and Fei Zhao and Xiangfeng Xu and Zheyu Ye and Shaohui Lin", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n", "link": "http://arxiv.org/abs/2412.00876v2", "date": "2024-12-03", "relevancy": 2.2971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification&body=Title%3A%20Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification%0AAuthor%3A%20Wenxuan%20Huang%20and%20Zijie%20Zhai%20and%20Yunhang%20Shen%20and%20Shaoshen%20Cao%20and%20Fei%20Zhao%20and%20Xiangfeng%20Xu%20and%20Zheyu%20Ye%20and%20Shaohui%20Lin%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Avision%20understanding%2C%20reasoning%2C%20and%20interaction.%20However%2C%20the%20inference%0Acomputation%20and%20memory%20increase%20progressively%20with%20the%20generation%20of%20output%0Atokens%20during%20decoding%2C%20directly%20affecting%20the%20efficacy%20of%20MLLMs.%20Existing%0Amethods%20attempt%20to%20reduce%20the%20vision%20context%20redundancy%20to%20achieve%20efficient%0AMLLMs.%20Unfortunately%2C%20the%20efficiency%20benefits%20of%20the%20vision%20context%20reduction%0Ain%20the%20prefill%20stage%20gradually%20diminish%20during%20the%20decoding%20stage.%20To%20address%0Athis%20problem%2C%20we%20proposed%20a%20dynamic%20vision-language%20context%20sparsification%0Aframework%20Dynamic-LLaVA%2C%20which%20dynamically%20reduces%20the%20redundancy%20of%20vision%0Acontext%20in%20the%20prefill%20stage%20and%20decreases%20the%20memory%20and%20computation%20overhead%0Aof%20the%20generated%20language%20context%20during%20decoding.%20Dynamic-LLaVA%20designs%20a%0Atailored%20sparsification%20inference%20scheme%20for%20different%20inference%20modes%2C%20i.e.%2C%0Aprefill%2C%20decoding%20with%20and%20without%20KV%20cache%2C%20to%20achieve%20efficient%20inference%20of%0AMLLMs.%20In%20practice%2C%20Dynamic-LLaVA%20can%20reduce%20computation%20consumption%20by%0A%24%5Csim%2475%5C%25%20in%20the%20prefill%20stage.%20Meanwhile%2C%20throughout%20the%20entire%20generation%0Aprocess%20of%20MLLMs%2C%20Dynamic-LLaVA%20reduces%20the%20%24%5Csim%2450%5C%25%20computation%20consumption%0Aunder%20decoding%20without%20KV%20cache%2C%20while%20saving%20%24%5Csim%2450%5C%25%20GPU%20memory%20overhead%0Awhen%20decoding%20with%20KV%20cache%2C%20due%20to%20the%20vision-language%20context%20sparsification.%0AExtensive%20experiments%20also%20demonstrate%20that%20Dynamic-LLaVA%20achieves%20efficient%0Ainference%20for%20MLLMs%20with%20negligible%20understanding%20and%20generation%20ability%0Adegradation%20or%20even%20performance%20gains%20compared%20to%20the%20full-context%20inference%0Abaselines.%20Code%20is%20available%20at%20https%3A//github.com/Osilly/dynamic_llava%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic-LLaVA%253A%2520Efficient%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Dynamic%250A%2520%2520Vision-language%2520Context%2520Sparsification%26entry.906535625%3DWenxuan%2520Huang%2520and%2520Zijie%2520Zhai%2520and%2520Yunhang%2520Shen%2520and%2520Shaoshen%2520Cao%2520and%2520Fei%2520Zhao%2520and%2520Xiangfeng%2520Xu%2520and%2520Zheyu%2520Ye%2520and%2520Shaohui%2520Lin%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%250Avision%2520understanding%252C%2520reasoning%252C%2520and%2520interaction.%2520However%252C%2520the%2520inference%250Acomputation%2520and%2520memory%2520increase%2520progressively%2520with%2520the%2520generation%2520of%2520output%250Atokens%2520during%2520decoding%252C%2520directly%2520affecting%2520the%2520efficacy%2520of%2520MLLMs.%2520Existing%250Amethods%2520attempt%2520to%2520reduce%2520the%2520vision%2520context%2520redundancy%2520to%2520achieve%2520efficient%250AMLLMs.%2520Unfortunately%252C%2520the%2520efficiency%2520benefits%2520of%2520the%2520vision%2520context%2520reduction%250Ain%2520the%2520prefill%2520stage%2520gradually%2520diminish%2520during%2520the%2520decoding%2520stage.%2520To%2520address%250Athis%2520problem%252C%2520we%2520proposed%2520a%2520dynamic%2520vision-language%2520context%2520sparsification%250Aframework%2520Dynamic-LLaVA%252C%2520which%2520dynamically%2520reduces%2520the%2520redundancy%2520of%2520vision%250Acontext%2520in%2520the%2520prefill%2520stage%2520and%2520decreases%2520the%2520memory%2520and%2520computation%2520overhead%250Aof%2520the%2520generated%2520language%2520context%2520during%2520decoding.%2520Dynamic-LLaVA%2520designs%2520a%250Atailored%2520sparsification%2520inference%2520scheme%2520for%2520different%2520inference%2520modes%252C%2520i.e.%252C%250Aprefill%252C%2520decoding%2520with%2520and%2520without%2520KV%2520cache%252C%2520to%2520achieve%2520efficient%2520inference%2520of%250AMLLMs.%2520In%2520practice%252C%2520Dynamic-LLaVA%2520can%2520reduce%2520computation%2520consumption%2520by%250A%2524%255Csim%252475%255C%2525%2520in%2520the%2520prefill%2520stage.%2520Meanwhile%252C%2520throughout%2520the%2520entire%2520generation%250Aprocess%2520of%2520MLLMs%252C%2520Dynamic-LLaVA%2520reduces%2520the%2520%2524%255Csim%252450%255C%2525%2520computation%2520consumption%250Aunder%2520decoding%2520without%2520KV%2520cache%252C%2520while%2520saving%2520%2524%255Csim%252450%255C%2525%2520GPU%2520memory%2520overhead%250Awhen%2520decoding%2520with%2520KV%2520cache%252C%2520due%2520to%2520the%2520vision-language%2520context%2520sparsification.%250AExtensive%2520experiments%2520also%2520demonstrate%2520that%2520Dynamic-LLaVA%2520achieves%2520efficient%250Ainference%2520for%2520MLLMs%2520with%2520negligible%2520understanding%2520and%2520generation%2520ability%250Adegradation%2520or%2520even%2520performance%2520gains%2520compared%2520to%2520the%2520full-context%2520inference%250Abaselines.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Osilly/dynamic_llava%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic-LLaVA%3A%20Efficient%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%0A%20%20Vision-language%20Context%20Sparsification&entry.906535625=Wenxuan%20Huang%20and%20Zijie%20Zhai%20and%20Yunhang%20Shen%20and%20Shaoshen%20Cao%20and%20Fei%20Zhao%20and%20Xiangfeng%20Xu%20and%20Zheyu%20Ye%20and%20Shaohui%20Lin&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Avision%20understanding%2C%20reasoning%2C%20and%20interaction.%20However%2C%20the%20inference%0Acomputation%20and%20memory%20increase%20progressively%20with%20the%20generation%20of%20output%0Atokens%20during%20decoding%2C%20directly%20affecting%20the%20efficacy%20of%20MLLMs.%20Existing%0Amethods%20attempt%20to%20reduce%20the%20vision%20context%20redundancy%20to%20achieve%20efficient%0AMLLMs.%20Unfortunately%2C%20the%20efficiency%20benefits%20of%20the%20vision%20context%20reduction%0Ain%20the%20prefill%20stage%20gradually%20diminish%20during%20the%20decoding%20stage.%20To%20address%0Athis%20problem%2C%20we%20proposed%20a%20dynamic%20vision-language%20context%20sparsification%0Aframework%20Dynamic-LLaVA%2C%20which%20dynamically%20reduces%20the%20redundancy%20of%20vision%0Acontext%20in%20the%20prefill%20stage%20and%20decreases%20the%20memory%20and%20computation%20overhead%0Aof%20the%20generated%20language%20context%20during%20decoding.%20Dynamic-LLaVA%20designs%20a%0Atailored%20sparsification%20inference%20scheme%20for%20different%20inference%20modes%2C%20i.e.%2C%0Aprefill%2C%20decoding%20with%20and%20without%20KV%20cache%2C%20to%20achieve%20efficient%20inference%20of%0AMLLMs.%20In%20practice%2C%20Dynamic-LLaVA%20can%20reduce%20computation%20consumption%20by%0A%24%5Csim%2475%5C%25%20in%20the%20prefill%20stage.%20Meanwhile%2C%20throughout%20the%20entire%20generation%0Aprocess%20of%20MLLMs%2C%20Dynamic-LLaVA%20reduces%20the%20%24%5Csim%2450%5C%25%20computation%20consumption%0Aunder%20decoding%20without%20KV%20cache%2C%20while%20saving%20%24%5Csim%2450%5C%25%20GPU%20memory%20overhead%0Awhen%20decoding%20with%20KV%20cache%2C%20due%20to%20the%20vision-language%20context%20sparsification.%0AExtensive%20experiments%20also%20demonstrate%20that%20Dynamic-LLaVA%20achieves%20efficient%0Ainference%20for%20MLLMs%20with%20negligible%20understanding%20and%20generation%20ability%0Adegradation%20or%20even%20performance%20gains%20compared%20to%20the%20full-context%20inference%0Abaselines.%20Code%20is%20available%20at%20https%3A//github.com/Osilly/dynamic_llava%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00876v2&entry.124074799=Read"},
{"title": "GerPS-Compare: Comparing NER methods for legal norm analysis", "author": "Sarah T. Bachinger and Christoph Unger and Robin Erd and Leila Feddoul and Clara Lachenmaier and Sina Zarrie\u00df and Birgitta K\u00f6nig-Ries", "abstract": "  We apply NER to a particular sub-genre of legal texts in German: the genre of\nlegal norms regulating administrative processes in public service\nadministration. The analysis of such texts involves identifying stretches of\ntext that instantiate one of ten classes identified by public service\nadministration professionals. We investigate and compare three methods for\nperforming Named Entity Recognition (NER) to detect these classes: a Rule-based\nsystem, deep discriminative models, and a deep generative model. Our results\nshow that Deep Discriminative models outperform both the Rule-based system as\nwell as the Deep Generative model, the latter two roughly performing equally\nwell, outperforming each other in different classes. The main cause for this\nsomewhat surprising result is arguably the fact that the classes used in the\nanalysis are semantically and syntactically heterogeneous, in contrast to the\nclasses used in more standard NER tasks. Deep Discriminative models appear to\nbe better equipped for dealing with this heterogenerity than both generic LLMs\nand human linguists designing rule-based NER systems.\n", "link": "http://arxiv.org/abs/2412.02427v1", "date": "2024-12-03", "relevancy": 2.2616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GerPS-Compare%3A%20Comparing%20NER%20methods%20for%20legal%20norm%20analysis&body=Title%3A%20GerPS-Compare%3A%20Comparing%20NER%20methods%20for%20legal%20norm%20analysis%0AAuthor%3A%20Sarah%20T.%20Bachinger%20and%20Christoph%20Unger%20and%20Robin%20Erd%20and%20Leila%20Feddoul%20and%20Clara%20Lachenmaier%20and%20Sina%20Zarrie%C3%9F%20and%20Birgitta%20K%C3%B6nig-Ries%0AAbstract%3A%20%20%20We%20apply%20NER%20to%20a%20particular%20sub-genre%20of%20legal%20texts%20in%20German%3A%20the%20genre%20of%0Alegal%20norms%20regulating%20administrative%20processes%20in%20public%20service%0Aadministration.%20The%20analysis%20of%20such%20texts%20involves%20identifying%20stretches%20of%0Atext%20that%20instantiate%20one%20of%20ten%20classes%20identified%20by%20public%20service%0Aadministration%20professionals.%20We%20investigate%20and%20compare%20three%20methods%20for%0Aperforming%20Named%20Entity%20Recognition%20%28NER%29%20to%20detect%20these%20classes%3A%20a%20Rule-based%0Asystem%2C%20deep%20discriminative%20models%2C%20and%20a%20deep%20generative%20model.%20Our%20results%0Ashow%20that%20Deep%20Discriminative%20models%20outperform%20both%20the%20Rule-based%20system%20as%0Awell%20as%20the%20Deep%20Generative%20model%2C%20the%20latter%20two%20roughly%20performing%20equally%0Awell%2C%20outperforming%20each%20other%20in%20different%20classes.%20The%20main%20cause%20for%20this%0Asomewhat%20surprising%20result%20is%20arguably%20the%20fact%20that%20the%20classes%20used%20in%20the%0Aanalysis%20are%20semantically%20and%20syntactically%20heterogeneous%2C%20in%20contrast%20to%20the%0Aclasses%20used%20in%20more%20standard%20NER%20tasks.%20Deep%20Discriminative%20models%20appear%20to%0Abe%20better%20equipped%20for%20dealing%20with%20this%20heterogenerity%20than%20both%20generic%20LLMs%0Aand%20human%20linguists%20designing%20rule-based%20NER%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGerPS-Compare%253A%2520Comparing%2520NER%2520methods%2520for%2520legal%2520norm%2520analysis%26entry.906535625%3DSarah%2520T.%2520Bachinger%2520and%2520Christoph%2520Unger%2520and%2520Robin%2520Erd%2520and%2520Leila%2520Feddoul%2520and%2520Clara%2520Lachenmaier%2520and%2520Sina%2520Zarrie%25C3%259F%2520and%2520Birgitta%2520K%25C3%25B6nig-Ries%26entry.1292438233%3D%2520%2520We%2520apply%2520NER%2520to%2520a%2520particular%2520sub-genre%2520of%2520legal%2520texts%2520in%2520German%253A%2520the%2520genre%2520of%250Alegal%2520norms%2520regulating%2520administrative%2520processes%2520in%2520public%2520service%250Aadministration.%2520The%2520analysis%2520of%2520such%2520texts%2520involves%2520identifying%2520stretches%2520of%250Atext%2520that%2520instantiate%2520one%2520of%2520ten%2520classes%2520identified%2520by%2520public%2520service%250Aadministration%2520professionals.%2520We%2520investigate%2520and%2520compare%2520three%2520methods%2520for%250Aperforming%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520to%2520detect%2520these%2520classes%253A%2520a%2520Rule-based%250Asystem%252C%2520deep%2520discriminative%2520models%252C%2520and%2520a%2520deep%2520generative%2520model.%2520Our%2520results%250Ashow%2520that%2520Deep%2520Discriminative%2520models%2520outperform%2520both%2520the%2520Rule-based%2520system%2520as%250Awell%2520as%2520the%2520Deep%2520Generative%2520model%252C%2520the%2520latter%2520two%2520roughly%2520performing%2520equally%250Awell%252C%2520outperforming%2520each%2520other%2520in%2520different%2520classes.%2520The%2520main%2520cause%2520for%2520this%250Asomewhat%2520surprising%2520result%2520is%2520arguably%2520the%2520fact%2520that%2520the%2520classes%2520used%2520in%2520the%250Aanalysis%2520are%2520semantically%2520and%2520syntactically%2520heterogeneous%252C%2520in%2520contrast%2520to%2520the%250Aclasses%2520used%2520in%2520more%2520standard%2520NER%2520tasks.%2520Deep%2520Discriminative%2520models%2520appear%2520to%250Abe%2520better%2520equipped%2520for%2520dealing%2520with%2520this%2520heterogenerity%2520than%2520both%2520generic%2520LLMs%250Aand%2520human%2520linguists%2520designing%2520rule-based%2520NER%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GerPS-Compare%3A%20Comparing%20NER%20methods%20for%20legal%20norm%20analysis&entry.906535625=Sarah%20T.%20Bachinger%20and%20Christoph%20Unger%20and%20Robin%20Erd%20and%20Leila%20Feddoul%20and%20Clara%20Lachenmaier%20and%20Sina%20Zarrie%C3%9F%20and%20Birgitta%20K%C3%B6nig-Ries&entry.1292438233=%20%20We%20apply%20NER%20to%20a%20particular%20sub-genre%20of%20legal%20texts%20in%20German%3A%20the%20genre%20of%0Alegal%20norms%20regulating%20administrative%20processes%20in%20public%20service%0Aadministration.%20The%20analysis%20of%20such%20texts%20involves%20identifying%20stretches%20of%0Atext%20that%20instantiate%20one%20of%20ten%20classes%20identified%20by%20public%20service%0Aadministration%20professionals.%20We%20investigate%20and%20compare%20three%20methods%20for%0Aperforming%20Named%20Entity%20Recognition%20%28NER%29%20to%20detect%20these%20classes%3A%20a%20Rule-based%0Asystem%2C%20deep%20discriminative%20models%2C%20and%20a%20deep%20generative%20model.%20Our%20results%0Ashow%20that%20Deep%20Discriminative%20models%20outperform%20both%20the%20Rule-based%20system%20as%0Awell%20as%20the%20Deep%20Generative%20model%2C%20the%20latter%20two%20roughly%20performing%20equally%0Awell%2C%20outperforming%20each%20other%20in%20different%20classes.%20The%20main%20cause%20for%20this%0Asomewhat%20surprising%20result%20is%20arguably%20the%20fact%20that%20the%20classes%20used%20in%20the%0Aanalysis%20are%20semantically%20and%20syntactically%20heterogeneous%2C%20in%20contrast%20to%20the%0Aclasses%20used%20in%20more%20standard%20NER%20tasks.%20Deep%20Discriminative%20models%20appear%20to%0Abe%20better%20equipped%20for%20dealing%20with%20this%20heterogenerity%20than%20both%20generic%20LLMs%0Aand%20human%20linguists%20designing%20rule-based%20NER%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02427v1&entry.124074799=Read"},
{"title": "GerPS-Compare: Comparing NER methods for legal norm analysis", "author": "Sarah T. Bachinger and Christoph Unger and Robin Erd and Leila Feddoul and Clara Lachenmaier and Sina Zarrie\u00df and Birgitta K\u00f6nig-Ries", "abstract": "  We apply NER to a particular sub-genre of legal texts in German: the genre of\nlegal norms regulating administrative processes in public service\nadministration. The analysis of such texts involves identifying stretches of\ntext that instantiate one of ten classes identified by public service\nadministration professionals. We investigate and compare three methods for\nperforming Named Entity Recognition (NER) to detect these classes: a Rule-based\nsystem, deep discriminative models, and a deep generative model. Our results\nshow that Deep Discriminative models outperform both the Rule-based system as\nwell as the Deep Generative model, the latter two roughly performing equally\nwell, outperforming each other in different classes. The main cause for this\nsomewhat surprising result is arguably the fact that the classes used in the\nanalysis are semantically and syntactically heterogeneous, in contrast to the\nclasses used in more standard NER tasks. Deep Discriminative models appear to\nbe better equipped for dealing with this heterogenerity than both generic LLMs\nand human linguists designing rule-based NER systems.\n", "link": "http://arxiv.org/abs/2412.02427v1", "date": "2024-12-03", "relevancy": 2.2616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GerPS-Compare%3A%20Comparing%20NER%20methods%20for%20legal%20norm%20analysis&body=Title%3A%20GerPS-Compare%3A%20Comparing%20NER%20methods%20for%20legal%20norm%20analysis%0AAuthor%3A%20Sarah%20T.%20Bachinger%20and%20Christoph%20Unger%20and%20Robin%20Erd%20and%20Leila%20Feddoul%20and%20Clara%20Lachenmaier%20and%20Sina%20Zarrie%C3%9F%20and%20Birgitta%20K%C3%B6nig-Ries%0AAbstract%3A%20%20%20We%20apply%20NER%20to%20a%20particular%20sub-genre%20of%20legal%20texts%20in%20German%3A%20the%20genre%20of%0Alegal%20norms%20regulating%20administrative%20processes%20in%20public%20service%0Aadministration.%20The%20analysis%20of%20such%20texts%20involves%20identifying%20stretches%20of%0Atext%20that%20instantiate%20one%20of%20ten%20classes%20identified%20by%20public%20service%0Aadministration%20professionals.%20We%20investigate%20and%20compare%20three%20methods%20for%0Aperforming%20Named%20Entity%20Recognition%20%28NER%29%20to%20detect%20these%20classes%3A%20a%20Rule-based%0Asystem%2C%20deep%20discriminative%20models%2C%20and%20a%20deep%20generative%20model.%20Our%20results%0Ashow%20that%20Deep%20Discriminative%20models%20outperform%20both%20the%20Rule-based%20system%20as%0Awell%20as%20the%20Deep%20Generative%20model%2C%20the%20latter%20two%20roughly%20performing%20equally%0Awell%2C%20outperforming%20each%20other%20in%20different%20classes.%20The%20main%20cause%20for%20this%0Asomewhat%20surprising%20result%20is%20arguably%20the%20fact%20that%20the%20classes%20used%20in%20the%0Aanalysis%20are%20semantically%20and%20syntactically%20heterogeneous%2C%20in%20contrast%20to%20the%0Aclasses%20used%20in%20more%20standard%20NER%20tasks.%20Deep%20Discriminative%20models%20appear%20to%0Abe%20better%20equipped%20for%20dealing%20with%20this%20heterogenerity%20than%20both%20generic%20LLMs%0Aand%20human%20linguists%20designing%20rule-based%20NER%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGerPS-Compare%253A%2520Comparing%2520NER%2520methods%2520for%2520legal%2520norm%2520analysis%26entry.906535625%3DSarah%2520T.%2520Bachinger%2520and%2520Christoph%2520Unger%2520and%2520Robin%2520Erd%2520and%2520Leila%2520Feddoul%2520and%2520Clara%2520Lachenmaier%2520and%2520Sina%2520Zarrie%25C3%259F%2520and%2520Birgitta%2520K%25C3%25B6nig-Ries%26entry.1292438233%3D%2520%2520We%2520apply%2520NER%2520to%2520a%2520particular%2520sub-genre%2520of%2520legal%2520texts%2520in%2520German%253A%2520the%2520genre%2520of%250Alegal%2520norms%2520regulating%2520administrative%2520processes%2520in%2520public%2520service%250Aadministration.%2520The%2520analysis%2520of%2520such%2520texts%2520involves%2520identifying%2520stretches%2520of%250Atext%2520that%2520instantiate%2520one%2520of%2520ten%2520classes%2520identified%2520by%2520public%2520service%250Aadministration%2520professionals.%2520We%2520investigate%2520and%2520compare%2520three%2520methods%2520for%250Aperforming%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520to%2520detect%2520these%2520classes%253A%2520a%2520Rule-based%250Asystem%252C%2520deep%2520discriminative%2520models%252C%2520and%2520a%2520deep%2520generative%2520model.%2520Our%2520results%250Ashow%2520that%2520Deep%2520Discriminative%2520models%2520outperform%2520both%2520the%2520Rule-based%2520system%2520as%250Awell%2520as%2520the%2520Deep%2520Generative%2520model%252C%2520the%2520latter%2520two%2520roughly%2520performing%2520equally%250Awell%252C%2520outperforming%2520each%2520other%2520in%2520different%2520classes.%2520The%2520main%2520cause%2520for%2520this%250Asomewhat%2520surprising%2520result%2520is%2520arguably%2520the%2520fact%2520that%2520the%2520classes%2520used%2520in%2520the%250Aanalysis%2520are%2520semantically%2520and%2520syntactically%2520heterogeneous%252C%2520in%2520contrast%2520to%2520the%250Aclasses%2520used%2520in%2520more%2520standard%2520NER%2520tasks.%2520Deep%2520Discriminative%2520models%2520appear%2520to%250Abe%2520better%2520equipped%2520for%2520dealing%2520with%2520this%2520heterogenerity%2520than%2520both%2520generic%2520LLMs%250Aand%2520human%2520linguists%2520designing%2520rule-based%2520NER%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GerPS-Compare%3A%20Comparing%20NER%20methods%20for%20legal%20norm%20analysis&entry.906535625=Sarah%20T.%20Bachinger%20and%20Christoph%20Unger%20and%20Robin%20Erd%20and%20Leila%20Feddoul%20and%20Clara%20Lachenmaier%20and%20Sina%20Zarrie%C3%9F%20and%20Birgitta%20K%C3%B6nig-Ries&entry.1292438233=%20%20We%20apply%20NER%20to%20a%20particular%20sub-genre%20of%20legal%20texts%20in%20German%3A%20the%20genre%20of%0Alegal%20norms%20regulating%20administrative%20processes%20in%20public%20service%0Aadministration.%20The%20analysis%20of%20such%20texts%20involves%20identifying%20stretches%20of%0Atext%20that%20instantiate%20one%20of%20ten%20classes%20identified%20by%20public%20service%0Aadministration%20professionals.%20We%20investigate%20and%20compare%20three%20methods%20for%0Aperforming%20Named%20Entity%20Recognition%20%28NER%29%20to%20detect%20these%20classes%3A%20a%20Rule-based%0Asystem%2C%20deep%20discriminative%20models%2C%20and%20a%20deep%20generative%20model.%20Our%20results%0Ashow%20that%20Deep%20Discriminative%20models%20outperform%20both%20the%20Rule-based%20system%20as%0Awell%20as%20the%20Deep%20Generative%20model%2C%20the%20latter%20two%20roughly%20performing%20equally%0Awell%2C%20outperforming%20each%20other%20in%20different%20classes.%20The%20main%20cause%20for%20this%0Asomewhat%20surprising%20result%20is%20arguably%20the%20fact%20that%20the%20classes%20used%20in%20the%0Aanalysis%20are%20semantically%20and%20syntactically%20heterogeneous%2C%20in%20contrast%20to%20the%0Aclasses%20used%20in%20more%20standard%20NER%20tasks.%20Deep%20Discriminative%20models%20appear%20to%0Abe%20better%20equipped%20for%20dealing%20with%20this%20heterogenerity%20than%20both%20generic%20LLMs%0Aand%20human%20linguists%20designing%20rule-based%20NER%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02427v1&entry.124074799=Read"},
{"title": "Bio-inspired visual relative localization for large swarms of UAVs", "author": "Martin K\u0159\u00ed\u017eek and Matou\u0161 Vrba and Antonella Bari\u0161i\u0107 Kula\u0161 and Stjepan Bogdan and Martin Saska", "abstract": "  We propose a new approach to visual perception for relative localization of\nagents within large-scale swarms of UAVs. Inspired by biological perception\nutilized by schools of sardines, swarms of bees, and other large groups of\nanimals capable of moving in a decentralized yet coherent manner, our method\ndoes not rely on detecting individual neighbors by each agent and estimating\ntheir relative position, but rather we propose to regress a neighbor density\nover distance. This allows for a more accurate distance estimation as well as\nbetter scalability with respect to the number of neighbors. Additionally, a\nnovel swarm control algorithm is proposed to make it compatible with the new\nrelative localization method. We provide a thorough evaluation of the presented\nmethods and demonstrate that the regressing approach to distance estimation is\nmore robust to varying relative pose of the targets and that it is suitable to\nbe used as the main source of relative localization for swarm stabilization.\n", "link": "http://arxiv.org/abs/2412.02393v1", "date": "2024-12-03", "relevancy": 2.2608, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6105}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5505}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bio-inspired%20visual%20relative%20localization%20for%20large%20swarms%20of%20UAVs&body=Title%3A%20Bio-inspired%20visual%20relative%20localization%20for%20large%20swarms%20of%20UAVs%0AAuthor%3A%20Martin%20K%C5%99%C3%AD%C5%BEek%20and%20Matou%C5%A1%20Vrba%20and%20Antonella%20Bari%C5%A1i%C4%87%20Kula%C5%A1%20and%20Stjepan%20Bogdan%20and%20Martin%20Saska%0AAbstract%3A%20%20%20We%20propose%20a%20new%20approach%20to%20visual%20perception%20for%20relative%20localization%20of%0Aagents%20within%20large-scale%20swarms%20of%20UAVs.%20Inspired%20by%20biological%20perception%0Autilized%20by%20schools%20of%20sardines%2C%20swarms%20of%20bees%2C%20and%20other%20large%20groups%20of%0Aanimals%20capable%20of%20moving%20in%20a%20decentralized%20yet%20coherent%20manner%2C%20our%20method%0Adoes%20not%20rely%20on%20detecting%20individual%20neighbors%20by%20each%20agent%20and%20estimating%0Atheir%20relative%20position%2C%20but%20rather%20we%20propose%20to%20regress%20a%20neighbor%20density%0Aover%20distance.%20This%20allows%20for%20a%20more%20accurate%20distance%20estimation%20as%20well%20as%0Abetter%20scalability%20with%20respect%20to%20the%20number%20of%20neighbors.%20Additionally%2C%20a%0Anovel%20swarm%20control%20algorithm%20is%20proposed%20to%20make%20it%20compatible%20with%20the%20new%0Arelative%20localization%20method.%20We%20provide%20a%20thorough%20evaluation%20of%20the%20presented%0Amethods%20and%20demonstrate%20that%20the%20regressing%20approach%20to%20distance%20estimation%20is%0Amore%20robust%20to%20varying%20relative%20pose%20of%20the%20targets%20and%20that%20it%20is%20suitable%20to%0Abe%20used%20as%20the%20main%20source%20of%20relative%20localization%20for%20swarm%20stabilization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBio-inspired%2520visual%2520relative%2520localization%2520for%2520large%2520swarms%2520of%2520UAVs%26entry.906535625%3DMartin%2520K%25C5%2599%25C3%25AD%25C5%25BEek%2520and%2520Matou%25C5%25A1%2520Vrba%2520and%2520Antonella%2520Bari%25C5%25A1i%25C4%2587%2520Kula%25C5%25A1%2520and%2520Stjepan%2520Bogdan%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520approach%2520to%2520visual%2520perception%2520for%2520relative%2520localization%2520of%250Aagents%2520within%2520large-scale%2520swarms%2520of%2520UAVs.%2520Inspired%2520by%2520biological%2520perception%250Autilized%2520by%2520schools%2520of%2520sardines%252C%2520swarms%2520of%2520bees%252C%2520and%2520other%2520large%2520groups%2520of%250Aanimals%2520capable%2520of%2520moving%2520in%2520a%2520decentralized%2520yet%2520coherent%2520manner%252C%2520our%2520method%250Adoes%2520not%2520rely%2520on%2520detecting%2520individual%2520neighbors%2520by%2520each%2520agent%2520and%2520estimating%250Atheir%2520relative%2520position%252C%2520but%2520rather%2520we%2520propose%2520to%2520regress%2520a%2520neighbor%2520density%250Aover%2520distance.%2520This%2520allows%2520for%2520a%2520more%2520accurate%2520distance%2520estimation%2520as%2520well%2520as%250Abetter%2520scalability%2520with%2520respect%2520to%2520the%2520number%2520of%2520neighbors.%2520Additionally%252C%2520a%250Anovel%2520swarm%2520control%2520algorithm%2520is%2520proposed%2520to%2520make%2520it%2520compatible%2520with%2520the%2520new%250Arelative%2520localization%2520method.%2520We%2520provide%2520a%2520thorough%2520evaluation%2520of%2520the%2520presented%250Amethods%2520and%2520demonstrate%2520that%2520the%2520regressing%2520approach%2520to%2520distance%2520estimation%2520is%250Amore%2520robust%2520to%2520varying%2520relative%2520pose%2520of%2520the%2520targets%2520and%2520that%2520it%2520is%2520suitable%2520to%250Abe%2520used%2520as%2520the%2520main%2520source%2520of%2520relative%2520localization%2520for%2520swarm%2520stabilization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bio-inspired%20visual%20relative%20localization%20for%20large%20swarms%20of%20UAVs&entry.906535625=Martin%20K%C5%99%C3%AD%C5%BEek%20and%20Matou%C5%A1%20Vrba%20and%20Antonella%20Bari%C5%A1i%C4%87%20Kula%C5%A1%20and%20Stjepan%20Bogdan%20and%20Martin%20Saska&entry.1292438233=%20%20We%20propose%20a%20new%20approach%20to%20visual%20perception%20for%20relative%20localization%20of%0Aagents%20within%20large-scale%20swarms%20of%20UAVs.%20Inspired%20by%20biological%20perception%0Autilized%20by%20schools%20of%20sardines%2C%20swarms%20of%20bees%2C%20and%20other%20large%20groups%20of%0Aanimals%20capable%20of%20moving%20in%20a%20decentralized%20yet%20coherent%20manner%2C%20our%20method%0Adoes%20not%20rely%20on%20detecting%20individual%20neighbors%20by%20each%20agent%20and%20estimating%0Atheir%20relative%20position%2C%20but%20rather%20we%20propose%20to%20regress%20a%20neighbor%20density%0Aover%20distance.%20This%20allows%20for%20a%20more%20accurate%20distance%20estimation%20as%20well%20as%0Abetter%20scalability%20with%20respect%20to%20the%20number%20of%20neighbors.%20Additionally%2C%20a%0Anovel%20swarm%20control%20algorithm%20is%20proposed%20to%20make%20it%20compatible%20with%20the%20new%0Arelative%20localization%20method.%20We%20provide%20a%20thorough%20evaluation%20of%20the%20presented%0Amethods%20and%20demonstrate%20that%20the%20regressing%20approach%20to%20distance%20estimation%20is%0Amore%20robust%20to%20varying%20relative%20pose%20of%20the%20targets%20and%20that%20it%20is%20suitable%20to%0Abe%20used%20as%20the%20main%20source%20of%20relative%20localization%20for%20swarm%20stabilization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02393v1&entry.124074799=Read"},
{"title": "Enhancing joint automatic chest X-ray diagnosis and clinical visual\n  attention prediction with multi-stage cooperative learning", "author": "Zirui Qiu and Hassan Rivaz and Yiming Xiao", "abstract": "  Purpose: As visual inspection is an inherent process during radiological\nscreening, the associated eye gaze data can provide valuable insights into\nrelevant clinical decisions. As deep learning has become the state-of-the-art\nfor computer-assisted diagnosis, integrating human behavior, such as eye gaze\ndata, into these systems is instrumental to help align machine predictions with\nclinical diagnostic criteria, thus enhancing the quality of automatic\nradiological diagnosis. Methods: We propose a novel deep learning framework for\njoint disease diagnosis and prediction of corresponding clinical visual\nattention maps for chest X-ray scans. Specifically, we introduce a new\ndual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a\nResidual and Squeeze-and-Excitation block-based encoder to extract diverse\nfeatures for visual attention map prediction, and a multi-scale feature-fusion\nclassifier to perform disease classification. To tackle the issue of\nasynchronous training schedules of individual tasks in multi-task learning, we\nproposed a multi-stage cooperative learning strategy, with contrastive learning\nfor feature encoder pretraining to boost performance. Results: Our proposed\nmethod is shown to significantly outperform existing techniques for chest X-ray\ndiagnosis (AUC=0.93) and the quality of visual attention map prediction\n(Correlation coefficient=0.58). Conclusion: Benefiting from the proposed\nmulti-task multi-stage cooperative learning, our technique demonstrates the\nbenefit of integrating clinicians' eye gaze into clinical AI systems to boost\nperformance and potentially explainability.\n", "link": "http://arxiv.org/abs/2403.16970v3", "date": "2024-12-03", "relevancy": 2.2109, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5598}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20joint%20automatic%20chest%20X-ray%20diagnosis%20and%20clinical%20visual%0A%20%20attention%20prediction%20with%20multi-stage%20cooperative%20learning&body=Title%3A%20Enhancing%20joint%20automatic%20chest%20X-ray%20diagnosis%20and%20clinical%20visual%0A%20%20attention%20prediction%20with%20multi-stage%20cooperative%20learning%0AAuthor%3A%20Zirui%20Qiu%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao%0AAbstract%3A%20%20%20Purpose%3A%20As%20visual%20inspection%20is%20an%20inherent%20process%20during%20radiological%0Ascreening%2C%20the%20associated%20eye%20gaze%20data%20can%20provide%20valuable%20insights%20into%0Arelevant%20clinical%20decisions.%20As%20deep%20learning%20has%20become%20the%20state-of-the-art%0Afor%20computer-assisted%20diagnosis%2C%20integrating%20human%20behavior%2C%20such%20as%20eye%20gaze%0Adata%2C%20into%20these%20systems%20is%20instrumental%20to%20help%20align%20machine%20predictions%20with%0Aclinical%20diagnostic%20criteria%2C%20thus%20enhancing%20the%20quality%20of%20automatic%0Aradiological%20diagnosis.%20Methods%3A%20We%20propose%20a%20novel%20deep%20learning%20framework%20for%0Ajoint%20disease%20diagnosis%20and%20prediction%20of%20corresponding%20clinical%20visual%0Aattention%20maps%20for%20chest%20X-ray%20scans.%20Specifically%2C%20we%20introduce%20a%20new%0Adual-encoder%20multi-task%20UNet%2C%20which%20leverages%20both%20a%20DenseNet201%20backbone%20and%20a%0AResidual%20and%20Squeeze-and-Excitation%20block-based%20encoder%20to%20extract%20diverse%0Afeatures%20for%20visual%20attention%20map%20prediction%2C%20and%20a%20multi-scale%20feature-fusion%0Aclassifier%20to%20perform%20disease%20classification.%20To%20tackle%20the%20issue%20of%0Aasynchronous%20training%20schedules%20of%20individual%20tasks%20in%20multi-task%20learning%2C%20we%0Aproposed%20a%20multi-stage%20cooperative%20learning%20strategy%2C%20with%20contrastive%20learning%0Afor%20feature%20encoder%20pretraining%20to%20boost%20performance.%20Results%3A%20Our%20proposed%0Amethod%20is%20shown%20to%20significantly%20outperform%20existing%20techniques%20for%20chest%20X-ray%0Adiagnosis%20%28AUC%3D0.93%29%20and%20the%20quality%20of%20visual%20attention%20map%20prediction%0A%28Correlation%20coefficient%3D0.58%29.%20Conclusion%3A%20Benefiting%20from%20the%20proposed%0Amulti-task%20multi-stage%20cooperative%20learning%2C%20our%20technique%20demonstrates%20the%0Abenefit%20of%20integrating%20clinicians%27%20eye%20gaze%20into%20clinical%20AI%20systems%20to%20boost%0Aperformance%20and%20potentially%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16970v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520joint%2520automatic%2520chest%2520X-ray%2520diagnosis%2520and%2520clinical%2520visual%250A%2520%2520attention%2520prediction%2520with%2520multi-stage%2520cooperative%2520learning%26entry.906535625%3DZirui%2520Qiu%2520and%2520Hassan%2520Rivaz%2520and%2520Yiming%2520Xiao%26entry.1292438233%3D%2520%2520Purpose%253A%2520As%2520visual%2520inspection%2520is%2520an%2520inherent%2520process%2520during%2520radiological%250Ascreening%252C%2520the%2520associated%2520eye%2520gaze%2520data%2520can%2520provide%2520valuable%2520insights%2520into%250Arelevant%2520clinical%2520decisions.%2520As%2520deep%2520learning%2520has%2520become%2520the%2520state-of-the-art%250Afor%2520computer-assisted%2520diagnosis%252C%2520integrating%2520human%2520behavior%252C%2520such%2520as%2520eye%2520gaze%250Adata%252C%2520into%2520these%2520systems%2520is%2520instrumental%2520to%2520help%2520align%2520machine%2520predictions%2520with%250Aclinical%2520diagnostic%2520criteria%252C%2520thus%2520enhancing%2520the%2520quality%2520of%2520automatic%250Aradiological%2520diagnosis.%2520Methods%253A%2520We%2520propose%2520a%2520novel%2520deep%2520learning%2520framework%2520for%250Ajoint%2520disease%2520diagnosis%2520and%2520prediction%2520of%2520corresponding%2520clinical%2520visual%250Aattention%2520maps%2520for%2520chest%2520X-ray%2520scans.%2520Specifically%252C%2520we%2520introduce%2520a%2520new%250Adual-encoder%2520multi-task%2520UNet%252C%2520which%2520leverages%2520both%2520a%2520DenseNet201%2520backbone%2520and%2520a%250AResidual%2520and%2520Squeeze-and-Excitation%2520block-based%2520encoder%2520to%2520extract%2520diverse%250Afeatures%2520for%2520visual%2520attention%2520map%2520prediction%252C%2520and%2520a%2520multi-scale%2520feature-fusion%250Aclassifier%2520to%2520perform%2520disease%2520classification.%2520To%2520tackle%2520the%2520issue%2520of%250Aasynchronous%2520training%2520schedules%2520of%2520individual%2520tasks%2520in%2520multi-task%2520learning%252C%2520we%250Aproposed%2520a%2520multi-stage%2520cooperative%2520learning%2520strategy%252C%2520with%2520contrastive%2520learning%250Afor%2520feature%2520encoder%2520pretraining%2520to%2520boost%2520performance.%2520Results%253A%2520Our%2520proposed%250Amethod%2520is%2520shown%2520to%2520significantly%2520outperform%2520existing%2520techniques%2520for%2520chest%2520X-ray%250Adiagnosis%2520%2528AUC%253D0.93%2529%2520and%2520the%2520quality%2520of%2520visual%2520attention%2520map%2520prediction%250A%2528Correlation%2520coefficient%253D0.58%2529.%2520Conclusion%253A%2520Benefiting%2520from%2520the%2520proposed%250Amulti-task%2520multi-stage%2520cooperative%2520learning%252C%2520our%2520technique%2520demonstrates%2520the%250Abenefit%2520of%2520integrating%2520clinicians%2527%2520eye%2520gaze%2520into%2520clinical%2520AI%2520systems%2520to%2520boost%250Aperformance%2520and%2520potentially%2520explainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16970v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20joint%20automatic%20chest%20X-ray%20diagnosis%20and%20clinical%20visual%0A%20%20attention%20prediction%20with%20multi-stage%20cooperative%20learning&entry.906535625=Zirui%20Qiu%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao&entry.1292438233=%20%20Purpose%3A%20As%20visual%20inspection%20is%20an%20inherent%20process%20during%20radiological%0Ascreening%2C%20the%20associated%20eye%20gaze%20data%20can%20provide%20valuable%20insights%20into%0Arelevant%20clinical%20decisions.%20As%20deep%20learning%20has%20become%20the%20state-of-the-art%0Afor%20computer-assisted%20diagnosis%2C%20integrating%20human%20behavior%2C%20such%20as%20eye%20gaze%0Adata%2C%20into%20these%20systems%20is%20instrumental%20to%20help%20align%20machine%20predictions%20with%0Aclinical%20diagnostic%20criteria%2C%20thus%20enhancing%20the%20quality%20of%20automatic%0Aradiological%20diagnosis.%20Methods%3A%20We%20propose%20a%20novel%20deep%20learning%20framework%20for%0Ajoint%20disease%20diagnosis%20and%20prediction%20of%20corresponding%20clinical%20visual%0Aattention%20maps%20for%20chest%20X-ray%20scans.%20Specifically%2C%20we%20introduce%20a%20new%0Adual-encoder%20multi-task%20UNet%2C%20which%20leverages%20both%20a%20DenseNet201%20backbone%20and%20a%0AResidual%20and%20Squeeze-and-Excitation%20block-based%20encoder%20to%20extract%20diverse%0Afeatures%20for%20visual%20attention%20map%20prediction%2C%20and%20a%20multi-scale%20feature-fusion%0Aclassifier%20to%20perform%20disease%20classification.%20To%20tackle%20the%20issue%20of%0Aasynchronous%20training%20schedules%20of%20individual%20tasks%20in%20multi-task%20learning%2C%20we%0Aproposed%20a%20multi-stage%20cooperative%20learning%20strategy%2C%20with%20contrastive%20learning%0Afor%20feature%20encoder%20pretraining%20to%20boost%20performance.%20Results%3A%20Our%20proposed%0Amethod%20is%20shown%20to%20significantly%20outperform%20existing%20techniques%20for%20chest%20X-ray%0Adiagnosis%20%28AUC%3D0.93%29%20and%20the%20quality%20of%20visual%20attention%20map%20prediction%0A%28Correlation%20coefficient%3D0.58%29.%20Conclusion%3A%20Benefiting%20from%20the%20proposed%0Amulti-task%20multi-stage%20cooperative%20learning%2C%20our%20technique%20demonstrates%20the%0Abenefit%20of%20integrating%20clinicians%27%20eye%20gaze%20into%20clinical%20AI%20systems%20to%20boost%0Aperformance%20and%20potentially%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16970v3&entry.124074799=Read"},
{"title": "Enhancing joint automatic chest X-ray diagnosis and clinical visual\n  attention prediction with multi-stage cooperative learning", "author": "Zirui Qiu and Hassan Rivaz and Yiming Xiao", "abstract": "  Purpose: As visual inspection is an inherent process during radiological\nscreening, the associated eye gaze data can provide valuable insights into\nrelevant clinical decisions. As deep learning has become the state-of-the-art\nfor computer-assisted diagnosis, integrating human behavior, such as eye gaze\ndata, into these systems is instrumental to help align machine predictions with\nclinical diagnostic criteria, thus enhancing the quality of automatic\nradiological diagnosis. Methods: We propose a novel deep learning framework for\njoint disease diagnosis and prediction of corresponding clinical visual\nattention maps for chest X-ray scans. Specifically, we introduce a new\ndual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a\nResidual and Squeeze-and-Excitation block-based encoder to extract diverse\nfeatures for visual attention map prediction, and a multi-scale feature-fusion\nclassifier to perform disease classification. To tackle the issue of\nasynchronous training schedules of individual tasks in multi-task learning, we\nproposed a multi-stage cooperative learning strategy, with contrastive learning\nfor feature encoder pretraining to boost performance. Results: Our proposed\nmethod is shown to significantly outperform existing techniques for chest X-ray\ndiagnosis (AUC=0.93) and the quality of visual attention map prediction\n(Correlation coefficient=0.58). Conclusion: Benefiting from the proposed\nmulti-task multi-stage cooperative learning, our technique demonstrates the\nbenefit of integrating clinicians' eye gaze into clinical AI systems to boost\nperformance and potentially explainability.\n", "link": "http://arxiv.org/abs/2403.16970v3", "date": "2024-12-03", "relevancy": 2.2109, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5598}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20joint%20automatic%20chest%20X-ray%20diagnosis%20and%20clinical%20visual%0A%20%20attention%20prediction%20with%20multi-stage%20cooperative%20learning&body=Title%3A%20Enhancing%20joint%20automatic%20chest%20X-ray%20diagnosis%20and%20clinical%20visual%0A%20%20attention%20prediction%20with%20multi-stage%20cooperative%20learning%0AAuthor%3A%20Zirui%20Qiu%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao%0AAbstract%3A%20%20%20Purpose%3A%20As%20visual%20inspection%20is%20an%20inherent%20process%20during%20radiological%0Ascreening%2C%20the%20associated%20eye%20gaze%20data%20can%20provide%20valuable%20insights%20into%0Arelevant%20clinical%20decisions.%20As%20deep%20learning%20has%20become%20the%20state-of-the-art%0Afor%20computer-assisted%20diagnosis%2C%20integrating%20human%20behavior%2C%20such%20as%20eye%20gaze%0Adata%2C%20into%20these%20systems%20is%20instrumental%20to%20help%20align%20machine%20predictions%20with%0Aclinical%20diagnostic%20criteria%2C%20thus%20enhancing%20the%20quality%20of%20automatic%0Aradiological%20diagnosis.%20Methods%3A%20We%20propose%20a%20novel%20deep%20learning%20framework%20for%0Ajoint%20disease%20diagnosis%20and%20prediction%20of%20corresponding%20clinical%20visual%0Aattention%20maps%20for%20chest%20X-ray%20scans.%20Specifically%2C%20we%20introduce%20a%20new%0Adual-encoder%20multi-task%20UNet%2C%20which%20leverages%20both%20a%20DenseNet201%20backbone%20and%20a%0AResidual%20and%20Squeeze-and-Excitation%20block-based%20encoder%20to%20extract%20diverse%0Afeatures%20for%20visual%20attention%20map%20prediction%2C%20and%20a%20multi-scale%20feature-fusion%0Aclassifier%20to%20perform%20disease%20classification.%20To%20tackle%20the%20issue%20of%0Aasynchronous%20training%20schedules%20of%20individual%20tasks%20in%20multi-task%20learning%2C%20we%0Aproposed%20a%20multi-stage%20cooperative%20learning%20strategy%2C%20with%20contrastive%20learning%0Afor%20feature%20encoder%20pretraining%20to%20boost%20performance.%20Results%3A%20Our%20proposed%0Amethod%20is%20shown%20to%20significantly%20outperform%20existing%20techniques%20for%20chest%20X-ray%0Adiagnosis%20%28AUC%3D0.93%29%20and%20the%20quality%20of%20visual%20attention%20map%20prediction%0A%28Correlation%20coefficient%3D0.58%29.%20Conclusion%3A%20Benefiting%20from%20the%20proposed%0Amulti-task%20multi-stage%20cooperative%20learning%2C%20our%20technique%20demonstrates%20the%0Abenefit%20of%20integrating%20clinicians%27%20eye%20gaze%20into%20clinical%20AI%20systems%20to%20boost%0Aperformance%20and%20potentially%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16970v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520joint%2520automatic%2520chest%2520X-ray%2520diagnosis%2520and%2520clinical%2520visual%250A%2520%2520attention%2520prediction%2520with%2520multi-stage%2520cooperative%2520learning%26entry.906535625%3DZirui%2520Qiu%2520and%2520Hassan%2520Rivaz%2520and%2520Yiming%2520Xiao%26entry.1292438233%3D%2520%2520Purpose%253A%2520As%2520visual%2520inspection%2520is%2520an%2520inherent%2520process%2520during%2520radiological%250Ascreening%252C%2520the%2520associated%2520eye%2520gaze%2520data%2520can%2520provide%2520valuable%2520insights%2520into%250Arelevant%2520clinical%2520decisions.%2520As%2520deep%2520learning%2520has%2520become%2520the%2520state-of-the-art%250Afor%2520computer-assisted%2520diagnosis%252C%2520integrating%2520human%2520behavior%252C%2520such%2520as%2520eye%2520gaze%250Adata%252C%2520into%2520these%2520systems%2520is%2520instrumental%2520to%2520help%2520align%2520machine%2520predictions%2520with%250Aclinical%2520diagnostic%2520criteria%252C%2520thus%2520enhancing%2520the%2520quality%2520of%2520automatic%250Aradiological%2520diagnosis.%2520Methods%253A%2520We%2520propose%2520a%2520novel%2520deep%2520learning%2520framework%2520for%250Ajoint%2520disease%2520diagnosis%2520and%2520prediction%2520of%2520corresponding%2520clinical%2520visual%250Aattention%2520maps%2520for%2520chest%2520X-ray%2520scans.%2520Specifically%252C%2520we%2520introduce%2520a%2520new%250Adual-encoder%2520multi-task%2520UNet%252C%2520which%2520leverages%2520both%2520a%2520DenseNet201%2520backbone%2520and%2520a%250AResidual%2520and%2520Squeeze-and-Excitation%2520block-based%2520encoder%2520to%2520extract%2520diverse%250Afeatures%2520for%2520visual%2520attention%2520map%2520prediction%252C%2520and%2520a%2520multi-scale%2520feature-fusion%250Aclassifier%2520to%2520perform%2520disease%2520classification.%2520To%2520tackle%2520the%2520issue%2520of%250Aasynchronous%2520training%2520schedules%2520of%2520individual%2520tasks%2520in%2520multi-task%2520learning%252C%2520we%250Aproposed%2520a%2520multi-stage%2520cooperative%2520learning%2520strategy%252C%2520with%2520contrastive%2520learning%250Afor%2520feature%2520encoder%2520pretraining%2520to%2520boost%2520performance.%2520Results%253A%2520Our%2520proposed%250Amethod%2520is%2520shown%2520to%2520significantly%2520outperform%2520existing%2520techniques%2520for%2520chest%2520X-ray%250Adiagnosis%2520%2528AUC%253D0.93%2529%2520and%2520the%2520quality%2520of%2520visual%2520attention%2520map%2520prediction%250A%2528Correlation%2520coefficient%253D0.58%2529.%2520Conclusion%253A%2520Benefiting%2520from%2520the%2520proposed%250Amulti-task%2520multi-stage%2520cooperative%2520learning%252C%2520our%2520technique%2520demonstrates%2520the%250Abenefit%2520of%2520integrating%2520clinicians%2527%2520eye%2520gaze%2520into%2520clinical%2520AI%2520systems%2520to%2520boost%250Aperformance%2520and%2520potentially%2520explainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16970v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20joint%20automatic%20chest%20X-ray%20diagnosis%20and%20clinical%20visual%0A%20%20attention%20prediction%20with%20multi-stage%20cooperative%20learning&entry.906535625=Zirui%20Qiu%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao&entry.1292438233=%20%20Purpose%3A%20As%20visual%20inspection%20is%20an%20inherent%20process%20during%20radiological%0Ascreening%2C%20the%20associated%20eye%20gaze%20data%20can%20provide%20valuable%20insights%20into%0Arelevant%20clinical%20decisions.%20As%20deep%20learning%20has%20become%20the%20state-of-the-art%0Afor%20computer-assisted%20diagnosis%2C%20integrating%20human%20behavior%2C%20such%20as%20eye%20gaze%0Adata%2C%20into%20these%20systems%20is%20instrumental%20to%20help%20align%20machine%20predictions%20with%0Aclinical%20diagnostic%20criteria%2C%20thus%20enhancing%20the%20quality%20of%20automatic%0Aradiological%20diagnosis.%20Methods%3A%20We%20propose%20a%20novel%20deep%20learning%20framework%20for%0Ajoint%20disease%20diagnosis%20and%20prediction%20of%20corresponding%20clinical%20visual%0Aattention%20maps%20for%20chest%20X-ray%20scans.%20Specifically%2C%20we%20introduce%20a%20new%0Adual-encoder%20multi-task%20UNet%2C%20which%20leverages%20both%20a%20DenseNet201%20backbone%20and%20a%0AResidual%20and%20Squeeze-and-Excitation%20block-based%20encoder%20to%20extract%20diverse%0Afeatures%20for%20visual%20attention%20map%20prediction%2C%20and%20a%20multi-scale%20feature-fusion%0Aclassifier%20to%20perform%20disease%20classification.%20To%20tackle%20the%20issue%20of%0Aasynchronous%20training%20schedules%20of%20individual%20tasks%20in%20multi-task%20learning%2C%20we%0Aproposed%20a%20multi-stage%20cooperative%20learning%20strategy%2C%20with%20contrastive%20learning%0Afor%20feature%20encoder%20pretraining%20to%20boost%20performance.%20Results%3A%20Our%20proposed%0Amethod%20is%20shown%20to%20significantly%20outperform%20existing%20techniques%20for%20chest%20X-ray%0Adiagnosis%20%28AUC%3D0.93%29%20and%20the%20quality%20of%20visual%20attention%20map%20prediction%0A%28Correlation%20coefficient%3D0.58%29.%20Conclusion%3A%20Benefiting%20from%20the%20proposed%0Amulti-task%20multi-stage%20cooperative%20learning%2C%20our%20technique%20demonstrates%20the%0Abenefit%20of%20integrating%20clinicians%27%20eye%20gaze%20into%20clinical%20AI%20systems%20to%20boost%0Aperformance%20and%20potentially%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16970v3&entry.124074799=Read"},
{"title": "Single-Shot Metric Depth from Focused Plenoptic Cameras", "author": "Blanca Lasheras-Hernandez and Klaus H. Strobl and Sergio Izquierdo and Tim Bodenm\u00fcller and Rudolph Triebel and Javier Civera", "abstract": "  Metric depth estimation from visual sensors is crucial for robots to\nperceive, navigate, and interact with their environment. Traditional range\nimaging setups, such as stereo or structured light cameras, face hassles\nincluding calibration, occlusions, and hardware demands, with accuracy limited\nby the baseline between cameras. Single- and multi-view monocular depth offers\na more compact alternative, but is constrained by the unobservability of the\nmetric scale. Light field imaging provides a promising solution for estimating\nmetric depth by using a unique lens configuration through a single device.\nHowever, its application to single-view dense metric depth is under-addressed\nmainly due to the technology's high cost, the lack of public benchmarks, and\nproprietary geometrical models and software.\n  Our work explores the potential of focused plenoptic cameras for dense metric\ndepth. We propose a novel pipeline that predicts metric depth from a single\nplenoptic camera shot by first generating a sparse metric point cloud using\nmachine learning, which is then used to scale and align a dense relative depth\nmap regressed by a foundation depth model, resulting in dense metric depth. To\nvalidate it, we curated the Light Field & Stereo Image Dataset (LFS) of\nreal-world light field images with stereo depth labels, filling a current gap\nin existing resources. Experimental results show that our pipeline produces\naccurate metric depth predictions, laying a solid groundwork for future\nresearch in this field.\n", "link": "http://arxiv.org/abs/2412.02386v1", "date": "2024-12-03", "relevancy": 2.2024, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5626}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5422}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Shot%20Metric%20Depth%20from%20Focused%20Plenoptic%20Cameras&body=Title%3A%20Single-Shot%20Metric%20Depth%20from%20Focused%20Plenoptic%20Cameras%0AAuthor%3A%20Blanca%20Lasheras-Hernandez%20and%20Klaus%20H.%20Strobl%20and%20Sergio%20Izquierdo%20and%20Tim%20Bodenm%C3%BCller%20and%20Rudolph%20Triebel%20and%20Javier%20Civera%0AAbstract%3A%20%20%20Metric%20depth%20estimation%20from%20visual%20sensors%20is%20crucial%20for%20robots%20to%0Aperceive%2C%20navigate%2C%20and%20interact%20with%20their%20environment.%20Traditional%20range%0Aimaging%20setups%2C%20such%20as%20stereo%20or%20structured%20light%20cameras%2C%20face%20hassles%0Aincluding%20calibration%2C%20occlusions%2C%20and%20hardware%20demands%2C%20with%20accuracy%20limited%0Aby%20the%20baseline%20between%20cameras.%20Single-%20and%20multi-view%20monocular%20depth%20offers%0Aa%20more%20compact%20alternative%2C%20but%20is%20constrained%20by%20the%20unobservability%20of%20the%0Ametric%20scale.%20Light%20field%20imaging%20provides%20a%20promising%20solution%20for%20estimating%0Ametric%20depth%20by%20using%20a%20unique%20lens%20configuration%20through%20a%20single%20device.%0AHowever%2C%20its%20application%20to%20single-view%20dense%20metric%20depth%20is%20under-addressed%0Amainly%20due%20to%20the%20technology%27s%20high%20cost%2C%20the%20lack%20of%20public%20benchmarks%2C%20and%0Aproprietary%20geometrical%20models%20and%20software.%0A%20%20Our%20work%20explores%20the%20potential%20of%20focused%20plenoptic%20cameras%20for%20dense%20metric%0Adepth.%20We%20propose%20a%20novel%20pipeline%20that%20predicts%20metric%20depth%20from%20a%20single%0Aplenoptic%20camera%20shot%20by%20first%20generating%20a%20sparse%20metric%20point%20cloud%20using%0Amachine%20learning%2C%20which%20is%20then%20used%20to%20scale%20and%20align%20a%20dense%20relative%20depth%0Amap%20regressed%20by%20a%20foundation%20depth%20model%2C%20resulting%20in%20dense%20metric%20depth.%20To%0Avalidate%20it%2C%20we%20curated%20the%20Light%20Field%20%26%20Stereo%20Image%20Dataset%20%28LFS%29%20of%0Areal-world%20light%20field%20images%20with%20stereo%20depth%20labels%2C%20filling%20a%20current%20gap%0Ain%20existing%20resources.%20Experimental%20results%20show%20that%20our%20pipeline%20produces%0Aaccurate%20metric%20depth%20predictions%2C%20laying%20a%20solid%20groundwork%20for%20future%0Aresearch%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Shot%2520Metric%2520Depth%2520from%2520Focused%2520Plenoptic%2520Cameras%26entry.906535625%3DBlanca%2520Lasheras-Hernandez%2520and%2520Klaus%2520H.%2520Strobl%2520and%2520Sergio%2520Izquierdo%2520and%2520Tim%2520Bodenm%25C3%25BCller%2520and%2520Rudolph%2520Triebel%2520and%2520Javier%2520Civera%26entry.1292438233%3D%2520%2520Metric%2520depth%2520estimation%2520from%2520visual%2520sensors%2520is%2520crucial%2520for%2520robots%2520to%250Aperceive%252C%2520navigate%252C%2520and%2520interact%2520with%2520their%2520environment.%2520Traditional%2520range%250Aimaging%2520setups%252C%2520such%2520as%2520stereo%2520or%2520structured%2520light%2520cameras%252C%2520face%2520hassles%250Aincluding%2520calibration%252C%2520occlusions%252C%2520and%2520hardware%2520demands%252C%2520with%2520accuracy%2520limited%250Aby%2520the%2520baseline%2520between%2520cameras.%2520Single-%2520and%2520multi-view%2520monocular%2520depth%2520offers%250Aa%2520more%2520compact%2520alternative%252C%2520but%2520is%2520constrained%2520by%2520the%2520unobservability%2520of%2520the%250Ametric%2520scale.%2520Light%2520field%2520imaging%2520provides%2520a%2520promising%2520solution%2520for%2520estimating%250Ametric%2520depth%2520by%2520using%2520a%2520unique%2520lens%2520configuration%2520through%2520a%2520single%2520device.%250AHowever%252C%2520its%2520application%2520to%2520single-view%2520dense%2520metric%2520depth%2520is%2520under-addressed%250Amainly%2520due%2520to%2520the%2520technology%2527s%2520high%2520cost%252C%2520the%2520lack%2520of%2520public%2520benchmarks%252C%2520and%250Aproprietary%2520geometrical%2520models%2520and%2520software.%250A%2520%2520Our%2520work%2520explores%2520the%2520potential%2520of%2520focused%2520plenoptic%2520cameras%2520for%2520dense%2520metric%250Adepth.%2520We%2520propose%2520a%2520novel%2520pipeline%2520that%2520predicts%2520metric%2520depth%2520from%2520a%2520single%250Aplenoptic%2520camera%2520shot%2520by%2520first%2520generating%2520a%2520sparse%2520metric%2520point%2520cloud%2520using%250Amachine%2520learning%252C%2520which%2520is%2520then%2520used%2520to%2520scale%2520and%2520align%2520a%2520dense%2520relative%2520depth%250Amap%2520regressed%2520by%2520a%2520foundation%2520depth%2520model%252C%2520resulting%2520in%2520dense%2520metric%2520depth.%2520To%250Avalidate%2520it%252C%2520we%2520curated%2520the%2520Light%2520Field%2520%2526%2520Stereo%2520Image%2520Dataset%2520%2528LFS%2529%2520of%250Areal-world%2520light%2520field%2520images%2520with%2520stereo%2520depth%2520labels%252C%2520filling%2520a%2520current%2520gap%250Ain%2520existing%2520resources.%2520Experimental%2520results%2520show%2520that%2520our%2520pipeline%2520produces%250Aaccurate%2520metric%2520depth%2520predictions%252C%2520laying%2520a%2520solid%2520groundwork%2520for%2520future%250Aresearch%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Shot%20Metric%20Depth%20from%20Focused%20Plenoptic%20Cameras&entry.906535625=Blanca%20Lasheras-Hernandez%20and%20Klaus%20H.%20Strobl%20and%20Sergio%20Izquierdo%20and%20Tim%20Bodenm%C3%BCller%20and%20Rudolph%20Triebel%20and%20Javier%20Civera&entry.1292438233=%20%20Metric%20depth%20estimation%20from%20visual%20sensors%20is%20crucial%20for%20robots%20to%0Aperceive%2C%20navigate%2C%20and%20interact%20with%20their%20environment.%20Traditional%20range%0Aimaging%20setups%2C%20such%20as%20stereo%20or%20structured%20light%20cameras%2C%20face%20hassles%0Aincluding%20calibration%2C%20occlusions%2C%20and%20hardware%20demands%2C%20with%20accuracy%20limited%0Aby%20the%20baseline%20between%20cameras.%20Single-%20and%20multi-view%20monocular%20depth%20offers%0Aa%20more%20compact%20alternative%2C%20but%20is%20constrained%20by%20the%20unobservability%20of%20the%0Ametric%20scale.%20Light%20field%20imaging%20provides%20a%20promising%20solution%20for%20estimating%0Ametric%20depth%20by%20using%20a%20unique%20lens%20configuration%20through%20a%20single%20device.%0AHowever%2C%20its%20application%20to%20single-view%20dense%20metric%20depth%20is%20under-addressed%0Amainly%20due%20to%20the%20technology%27s%20high%20cost%2C%20the%20lack%20of%20public%20benchmarks%2C%20and%0Aproprietary%20geometrical%20models%20and%20software.%0A%20%20Our%20work%20explores%20the%20potential%20of%20focused%20plenoptic%20cameras%20for%20dense%20metric%0Adepth.%20We%20propose%20a%20novel%20pipeline%20that%20predicts%20metric%20depth%20from%20a%20single%0Aplenoptic%20camera%20shot%20by%20first%20generating%20a%20sparse%20metric%20point%20cloud%20using%0Amachine%20learning%2C%20which%20is%20then%20used%20to%20scale%20and%20align%20a%20dense%20relative%20depth%0Amap%20regressed%20by%20a%20foundation%20depth%20model%2C%20resulting%20in%20dense%20metric%20depth.%20To%0Avalidate%20it%2C%20we%20curated%20the%20Light%20Field%20%26%20Stereo%20Image%20Dataset%20%28LFS%29%20of%0Areal-world%20light%20field%20images%20with%20stereo%20depth%20labels%2C%20filling%20a%20current%20gap%0Ain%20existing%20resources.%20Experimental%20results%20show%20that%20our%20pipeline%20produces%0Aaccurate%20metric%20depth%20predictions%2C%20laying%20a%20solid%20groundwork%20for%20future%0Aresearch%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02386v1&entry.124074799=Read"},
{"title": "Single-Shot Metric Depth from Focused Plenoptic Cameras", "author": "Blanca Lasheras-Hernandez and Klaus H. Strobl and Sergio Izquierdo and Tim Bodenm\u00fcller and Rudolph Triebel and Javier Civera", "abstract": "  Metric depth estimation from visual sensors is crucial for robots to\nperceive, navigate, and interact with their environment. Traditional range\nimaging setups, such as stereo or structured light cameras, face hassles\nincluding calibration, occlusions, and hardware demands, with accuracy limited\nby the baseline between cameras. Single- and multi-view monocular depth offers\na more compact alternative, but is constrained by the unobservability of the\nmetric scale. Light field imaging provides a promising solution for estimating\nmetric depth by using a unique lens configuration through a single device.\nHowever, its application to single-view dense metric depth is under-addressed\nmainly due to the technology's high cost, the lack of public benchmarks, and\nproprietary geometrical models and software.\n  Our work explores the potential of focused plenoptic cameras for dense metric\ndepth. We propose a novel pipeline that predicts metric depth from a single\nplenoptic camera shot by first generating a sparse metric point cloud using\nmachine learning, which is then used to scale and align a dense relative depth\nmap regressed by a foundation depth model, resulting in dense metric depth. To\nvalidate it, we curated the Light Field & Stereo Image Dataset (LFS) of\nreal-world light field images with stereo depth labels, filling a current gap\nin existing resources. Experimental results show that our pipeline produces\naccurate metric depth predictions, laying a solid groundwork for future\nresearch in this field.\n", "link": "http://arxiv.org/abs/2412.02386v1", "date": "2024-12-03", "relevancy": 2.2024, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5626}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5422}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Shot%20Metric%20Depth%20from%20Focused%20Plenoptic%20Cameras&body=Title%3A%20Single-Shot%20Metric%20Depth%20from%20Focused%20Plenoptic%20Cameras%0AAuthor%3A%20Blanca%20Lasheras-Hernandez%20and%20Klaus%20H.%20Strobl%20and%20Sergio%20Izquierdo%20and%20Tim%20Bodenm%C3%BCller%20and%20Rudolph%20Triebel%20and%20Javier%20Civera%0AAbstract%3A%20%20%20Metric%20depth%20estimation%20from%20visual%20sensors%20is%20crucial%20for%20robots%20to%0Aperceive%2C%20navigate%2C%20and%20interact%20with%20their%20environment.%20Traditional%20range%0Aimaging%20setups%2C%20such%20as%20stereo%20or%20structured%20light%20cameras%2C%20face%20hassles%0Aincluding%20calibration%2C%20occlusions%2C%20and%20hardware%20demands%2C%20with%20accuracy%20limited%0Aby%20the%20baseline%20between%20cameras.%20Single-%20and%20multi-view%20monocular%20depth%20offers%0Aa%20more%20compact%20alternative%2C%20but%20is%20constrained%20by%20the%20unobservability%20of%20the%0Ametric%20scale.%20Light%20field%20imaging%20provides%20a%20promising%20solution%20for%20estimating%0Ametric%20depth%20by%20using%20a%20unique%20lens%20configuration%20through%20a%20single%20device.%0AHowever%2C%20its%20application%20to%20single-view%20dense%20metric%20depth%20is%20under-addressed%0Amainly%20due%20to%20the%20technology%27s%20high%20cost%2C%20the%20lack%20of%20public%20benchmarks%2C%20and%0Aproprietary%20geometrical%20models%20and%20software.%0A%20%20Our%20work%20explores%20the%20potential%20of%20focused%20plenoptic%20cameras%20for%20dense%20metric%0Adepth.%20We%20propose%20a%20novel%20pipeline%20that%20predicts%20metric%20depth%20from%20a%20single%0Aplenoptic%20camera%20shot%20by%20first%20generating%20a%20sparse%20metric%20point%20cloud%20using%0Amachine%20learning%2C%20which%20is%20then%20used%20to%20scale%20and%20align%20a%20dense%20relative%20depth%0Amap%20regressed%20by%20a%20foundation%20depth%20model%2C%20resulting%20in%20dense%20metric%20depth.%20To%0Avalidate%20it%2C%20we%20curated%20the%20Light%20Field%20%26%20Stereo%20Image%20Dataset%20%28LFS%29%20of%0Areal-world%20light%20field%20images%20with%20stereo%20depth%20labels%2C%20filling%20a%20current%20gap%0Ain%20existing%20resources.%20Experimental%20results%20show%20that%20our%20pipeline%20produces%0Aaccurate%20metric%20depth%20predictions%2C%20laying%20a%20solid%20groundwork%20for%20future%0Aresearch%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Shot%2520Metric%2520Depth%2520from%2520Focused%2520Plenoptic%2520Cameras%26entry.906535625%3DBlanca%2520Lasheras-Hernandez%2520and%2520Klaus%2520H.%2520Strobl%2520and%2520Sergio%2520Izquierdo%2520and%2520Tim%2520Bodenm%25C3%25BCller%2520and%2520Rudolph%2520Triebel%2520and%2520Javier%2520Civera%26entry.1292438233%3D%2520%2520Metric%2520depth%2520estimation%2520from%2520visual%2520sensors%2520is%2520crucial%2520for%2520robots%2520to%250Aperceive%252C%2520navigate%252C%2520and%2520interact%2520with%2520their%2520environment.%2520Traditional%2520range%250Aimaging%2520setups%252C%2520such%2520as%2520stereo%2520or%2520structured%2520light%2520cameras%252C%2520face%2520hassles%250Aincluding%2520calibration%252C%2520occlusions%252C%2520and%2520hardware%2520demands%252C%2520with%2520accuracy%2520limited%250Aby%2520the%2520baseline%2520between%2520cameras.%2520Single-%2520and%2520multi-view%2520monocular%2520depth%2520offers%250Aa%2520more%2520compact%2520alternative%252C%2520but%2520is%2520constrained%2520by%2520the%2520unobservability%2520of%2520the%250Ametric%2520scale.%2520Light%2520field%2520imaging%2520provides%2520a%2520promising%2520solution%2520for%2520estimating%250Ametric%2520depth%2520by%2520using%2520a%2520unique%2520lens%2520configuration%2520through%2520a%2520single%2520device.%250AHowever%252C%2520its%2520application%2520to%2520single-view%2520dense%2520metric%2520depth%2520is%2520under-addressed%250Amainly%2520due%2520to%2520the%2520technology%2527s%2520high%2520cost%252C%2520the%2520lack%2520of%2520public%2520benchmarks%252C%2520and%250Aproprietary%2520geometrical%2520models%2520and%2520software.%250A%2520%2520Our%2520work%2520explores%2520the%2520potential%2520of%2520focused%2520plenoptic%2520cameras%2520for%2520dense%2520metric%250Adepth.%2520We%2520propose%2520a%2520novel%2520pipeline%2520that%2520predicts%2520metric%2520depth%2520from%2520a%2520single%250Aplenoptic%2520camera%2520shot%2520by%2520first%2520generating%2520a%2520sparse%2520metric%2520point%2520cloud%2520using%250Amachine%2520learning%252C%2520which%2520is%2520then%2520used%2520to%2520scale%2520and%2520align%2520a%2520dense%2520relative%2520depth%250Amap%2520regressed%2520by%2520a%2520foundation%2520depth%2520model%252C%2520resulting%2520in%2520dense%2520metric%2520depth.%2520To%250Avalidate%2520it%252C%2520we%2520curated%2520the%2520Light%2520Field%2520%2526%2520Stereo%2520Image%2520Dataset%2520%2528LFS%2529%2520of%250Areal-world%2520light%2520field%2520images%2520with%2520stereo%2520depth%2520labels%252C%2520filling%2520a%2520current%2520gap%250Ain%2520existing%2520resources.%2520Experimental%2520results%2520show%2520that%2520our%2520pipeline%2520produces%250Aaccurate%2520metric%2520depth%2520predictions%252C%2520laying%2520a%2520solid%2520groundwork%2520for%2520future%250Aresearch%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Shot%20Metric%20Depth%20from%20Focused%20Plenoptic%20Cameras&entry.906535625=Blanca%20Lasheras-Hernandez%20and%20Klaus%20H.%20Strobl%20and%20Sergio%20Izquierdo%20and%20Tim%20Bodenm%C3%BCller%20and%20Rudolph%20Triebel%20and%20Javier%20Civera&entry.1292438233=%20%20Metric%20depth%20estimation%20from%20visual%20sensors%20is%20crucial%20for%20robots%20to%0Aperceive%2C%20navigate%2C%20and%20interact%20with%20their%20environment.%20Traditional%20range%0Aimaging%20setups%2C%20such%20as%20stereo%20or%20structured%20light%20cameras%2C%20face%20hassles%0Aincluding%20calibration%2C%20occlusions%2C%20and%20hardware%20demands%2C%20with%20accuracy%20limited%0Aby%20the%20baseline%20between%20cameras.%20Single-%20and%20multi-view%20monocular%20depth%20offers%0Aa%20more%20compact%20alternative%2C%20but%20is%20constrained%20by%20the%20unobservability%20of%20the%0Ametric%20scale.%20Light%20field%20imaging%20provides%20a%20promising%20solution%20for%20estimating%0Ametric%20depth%20by%20using%20a%20unique%20lens%20configuration%20through%20a%20single%20device.%0AHowever%2C%20its%20application%20to%20single-view%20dense%20metric%20depth%20is%20under-addressed%0Amainly%20due%20to%20the%20technology%27s%20high%20cost%2C%20the%20lack%20of%20public%20benchmarks%2C%20and%0Aproprietary%20geometrical%20models%20and%20software.%0A%20%20Our%20work%20explores%20the%20potential%20of%20focused%20plenoptic%20cameras%20for%20dense%20metric%0Adepth.%20We%20propose%20a%20novel%20pipeline%20that%20predicts%20metric%20depth%20from%20a%20single%0Aplenoptic%20camera%20shot%20by%20first%20generating%20a%20sparse%20metric%20point%20cloud%20using%0Amachine%20learning%2C%20which%20is%20then%20used%20to%20scale%20and%20align%20a%20dense%20relative%20depth%0Amap%20regressed%20by%20a%20foundation%20depth%20model%2C%20resulting%20in%20dense%20metric%20depth.%20To%0Avalidate%20it%2C%20we%20curated%20the%20Light%20Field%20%26%20Stereo%20Image%20Dataset%20%28LFS%29%20of%0Areal-world%20light%20field%20images%20with%20stereo%20depth%20labels%2C%20filling%20a%20current%20gap%0Ain%20existing%20resources.%20Experimental%20results%20show%20that%20our%20pipeline%20produces%0Aaccurate%20metric%20depth%20predictions%2C%20laying%20a%20solid%20groundwork%20for%20future%0Aresearch%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02386v1&entry.124074799=Read"},
{"title": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification", "author": "Arthur Thuy and Dries F. Benoit", "abstract": "  Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage.\n", "link": "http://arxiv.org/abs/2403.10182v2", "date": "2024-12-03", "relevancy": 2.1898, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20reliable%20uncertainty%20quantification%20with%20neural%20network%0A%20%20ensembles%20for%20industrial%20image%20classification&body=Title%3A%20Fast%20and%20reliable%20uncertainty%20quantification%20with%20neural%20network%0A%20%20ensembles%20for%20industrial%20image%20classification%0AAuthor%3A%20Arthur%20Thuy%20and%20Dries%20F.%20Benoit%0AAbstract%3A%20%20%20Image%20classification%20with%20neural%20networks%20%28NNs%29%20is%20widely%20used%20in%20industrial%0Aprocesses%2C%20situations%20where%20the%20model%20likely%20encounters%20unknown%20objects%20during%0Adeployment%2C%20i.e.%2C%20out-of-distribution%20%28OOD%29%20data.%20Worryingly%2C%20NNs%20tend%20to%20make%0Aconfident%20yet%20incorrect%20predictions%20when%20confronted%20with%20OOD%20data.%20To%20increase%0Athe%20models%27%20reliability%2C%20they%20should%20quantify%20the%20uncertainty%20in%20their%20own%0Apredictions%2C%20communicating%20when%20the%20output%20should%20%28not%29%20be%20trusted.%20Deep%0Aensembles%2C%20composed%20of%20multiple%20independent%20NNs%2C%20have%20been%20shown%20to%20perform%0Astrongly%20but%20are%20computationally%20expensive.%20Recent%20research%20has%20proposed%20more%0Aefficient%20NN%20ensembles%2C%20namely%20the%20snapshot%2C%20batch%2C%20and%20multi-input%0Amulti-output%20ensemble.%20This%20study%20investigates%20the%20predictive%20and%20uncertainty%0Aperformance%20of%20efficient%20NN%20ensembles%20in%20the%20context%20of%20image%20classification%0Afor%20industrial%20processes.%20It%20is%20the%20first%20to%20provide%20a%20comprehensive%20comparison%0Aand%20it%20proposes%20a%20novel%20Diversity%20Quality%20metric%20to%20quantify%20the%20ensembles%27%0Aperformance%20on%20the%20in-distribution%20and%20OOD%20sets%20in%20one%20single%20metric.%20The%0Aresults%20highlight%20the%20batch%20ensemble%20as%20a%20cost-effective%20and%20competitive%0Aalternative%20to%20the%20deep%20ensemble.%20It%20matches%20the%20deep%20ensemble%20in%20both%0Auncertainty%20and%20accuracy%20while%20exhibiting%20considerable%20savings%20in%20training%0Atime%2C%20test%20time%2C%20and%20memory%20storage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520reliable%2520uncertainty%2520quantification%2520with%2520neural%2520network%250A%2520%2520ensembles%2520for%2520industrial%2520image%2520classification%26entry.906535625%3DArthur%2520Thuy%2520and%2520Dries%2520F.%2520Benoit%26entry.1292438233%3D%2520%2520Image%2520classification%2520with%2520neural%2520networks%2520%2528NNs%2529%2520is%2520widely%2520used%2520in%2520industrial%250Aprocesses%252C%2520situations%2520where%2520the%2520model%2520likely%2520encounters%2520unknown%2520objects%2520during%250Adeployment%252C%2520i.e.%252C%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520Worryingly%252C%2520NNs%2520tend%2520to%2520make%250Aconfident%2520yet%2520incorrect%2520predictions%2520when%2520confronted%2520with%2520OOD%2520data.%2520To%2520increase%250Athe%2520models%2527%2520reliability%252C%2520they%2520should%2520quantify%2520the%2520uncertainty%2520in%2520their%2520own%250Apredictions%252C%2520communicating%2520when%2520the%2520output%2520should%2520%2528not%2529%2520be%2520trusted.%2520Deep%250Aensembles%252C%2520composed%2520of%2520multiple%2520independent%2520NNs%252C%2520have%2520been%2520shown%2520to%2520perform%250Astrongly%2520but%2520are%2520computationally%2520expensive.%2520Recent%2520research%2520has%2520proposed%2520more%250Aefficient%2520NN%2520ensembles%252C%2520namely%2520the%2520snapshot%252C%2520batch%252C%2520and%2520multi-input%250Amulti-output%2520ensemble.%2520This%2520study%2520investigates%2520the%2520predictive%2520and%2520uncertainty%250Aperformance%2520of%2520efficient%2520NN%2520ensembles%2520in%2520the%2520context%2520of%2520image%2520classification%250Afor%2520industrial%2520processes.%2520It%2520is%2520the%2520first%2520to%2520provide%2520a%2520comprehensive%2520comparison%250Aand%2520it%2520proposes%2520a%2520novel%2520Diversity%2520Quality%2520metric%2520to%2520quantify%2520the%2520ensembles%2527%250Aperformance%2520on%2520the%2520in-distribution%2520and%2520OOD%2520sets%2520in%2520one%2520single%2520metric.%2520The%250Aresults%2520highlight%2520the%2520batch%2520ensemble%2520as%2520a%2520cost-effective%2520and%2520competitive%250Aalternative%2520to%2520the%2520deep%2520ensemble.%2520It%2520matches%2520the%2520deep%2520ensemble%2520in%2520both%250Auncertainty%2520and%2520accuracy%2520while%2520exhibiting%2520considerable%2520savings%2520in%2520training%250Atime%252C%2520test%2520time%252C%2520and%2520memory%2520storage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20reliable%20uncertainty%20quantification%20with%20neural%20network%0A%20%20ensembles%20for%20industrial%20image%20classification&entry.906535625=Arthur%20Thuy%20and%20Dries%20F.%20Benoit&entry.1292438233=%20%20Image%20classification%20with%20neural%20networks%20%28NNs%29%20is%20widely%20used%20in%20industrial%0Aprocesses%2C%20situations%20where%20the%20model%20likely%20encounters%20unknown%20objects%20during%0Adeployment%2C%20i.e.%2C%20out-of-distribution%20%28OOD%29%20data.%20Worryingly%2C%20NNs%20tend%20to%20make%0Aconfident%20yet%20incorrect%20predictions%20when%20confronted%20with%20OOD%20data.%20To%20increase%0Athe%20models%27%20reliability%2C%20they%20should%20quantify%20the%20uncertainty%20in%20their%20own%0Apredictions%2C%20communicating%20when%20the%20output%20should%20%28not%29%20be%20trusted.%20Deep%0Aensembles%2C%20composed%20of%20multiple%20independent%20NNs%2C%20have%20been%20shown%20to%20perform%0Astrongly%20but%20are%20computationally%20expensive.%20Recent%20research%20has%20proposed%20more%0Aefficient%20NN%20ensembles%2C%20namely%20the%20snapshot%2C%20batch%2C%20and%20multi-input%0Amulti-output%20ensemble.%20This%20study%20investigates%20the%20predictive%20and%20uncertainty%0Aperformance%20of%20efficient%20NN%20ensembles%20in%20the%20context%20of%20image%20classification%0Afor%20industrial%20processes.%20It%20is%20the%20first%20to%20provide%20a%20comprehensive%20comparison%0Aand%20it%20proposes%20a%20novel%20Diversity%20Quality%20metric%20to%20quantify%20the%20ensembles%27%0Aperformance%20on%20the%20in-distribution%20and%20OOD%20sets%20in%20one%20single%20metric.%20The%0Aresults%20highlight%20the%20batch%20ensemble%20as%20a%20cost-effective%20and%20competitive%0Aalternative%20to%20the%20deep%20ensemble.%20It%20matches%20the%20deep%20ensemble%20in%20both%0Auncertainty%20and%20accuracy%20while%20exhibiting%20considerable%20savings%20in%20training%0Atime%2C%20test%20time%2C%20and%20memory%20storage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10182v2&entry.124074799=Read"},
{"title": "Trajectory-based Road Autolabeling with Lidar-Camera Fusion in Winter\n  Conditions", "author": "Eerik Alamikkotervo and Henrik Toikka and Kari Tammi and Risto Ojala", "abstract": "  Robust road segmentation in all road conditions is required for safe\nautonomous driving and advanced driver assistance systems. Supervised deep\nlearning methods provide accurate road segmentation in the domain of their\ntraining data but cannot be trusted in out-of-distribution scenarios. Including\nthe whole distribution in the trainset is challenging as each sample must be\nlabeled by hand. Trajectory-based self-supervised methods offer a potential\nsolution as they can learn from the traversed route without manual labels.\nHowever, existing trajectory-based methods use learning schemes that rely only\non the camera or only on the lidar. In this paper, trajectory-based learning is\nimplemented jointly with lidar and camera for increased performance. Our method\noutperforms recent standalone camera- and lidar-based methods when evaluated\nwith a challenging winter driving dataset including countryside and suburb\ndriving scenes. The source code is available at\nhttps://github.com/eerik98/lidar-camera-road-autolabeling.git\n", "link": "http://arxiv.org/abs/2412.02370v1", "date": "2024-12-03", "relevancy": 2.1715, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5438}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory-based%20Road%20Autolabeling%20with%20Lidar-Camera%20Fusion%20in%20Winter%0A%20%20Conditions&body=Title%3A%20Trajectory-based%20Road%20Autolabeling%20with%20Lidar-Camera%20Fusion%20in%20Winter%0A%20%20Conditions%0AAuthor%3A%20Eerik%20Alamikkotervo%20and%20Henrik%20Toikka%20and%20Kari%20Tammi%20and%20Risto%20Ojala%0AAbstract%3A%20%20%20Robust%20road%20segmentation%20in%20all%20road%20conditions%20is%20required%20for%20safe%0Aautonomous%20driving%20and%20advanced%20driver%20assistance%20systems.%20Supervised%20deep%0Alearning%20methods%20provide%20accurate%20road%20segmentation%20in%20the%20domain%20of%20their%0Atraining%20data%20but%20cannot%20be%20trusted%20in%20out-of-distribution%20scenarios.%20Including%0Athe%20whole%20distribution%20in%20the%20trainset%20is%20challenging%20as%20each%20sample%20must%20be%0Alabeled%20by%20hand.%20Trajectory-based%20self-supervised%20methods%20offer%20a%20potential%0Asolution%20as%20they%20can%20learn%20from%20the%20traversed%20route%20without%20manual%20labels.%0AHowever%2C%20existing%20trajectory-based%20methods%20use%20learning%20schemes%20that%20rely%20only%0Aon%20the%20camera%20or%20only%20on%20the%20lidar.%20In%20this%20paper%2C%20trajectory-based%20learning%20is%0Aimplemented%20jointly%20with%20lidar%20and%20camera%20for%20increased%20performance.%20Our%20method%0Aoutperforms%20recent%20standalone%20camera-%20and%20lidar-based%20methods%20when%20evaluated%0Awith%20a%20challenging%20winter%20driving%20dataset%20including%20countryside%20and%20suburb%0Adriving%20scenes.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/eerik98/lidar-camera-road-autolabeling.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory-based%2520Road%2520Autolabeling%2520with%2520Lidar-Camera%2520Fusion%2520in%2520Winter%250A%2520%2520Conditions%26entry.906535625%3DEerik%2520Alamikkotervo%2520and%2520Henrik%2520Toikka%2520and%2520Kari%2520Tammi%2520and%2520Risto%2520Ojala%26entry.1292438233%3D%2520%2520Robust%2520road%2520segmentation%2520in%2520all%2520road%2520conditions%2520is%2520required%2520for%2520safe%250Aautonomous%2520driving%2520and%2520advanced%2520driver%2520assistance%2520systems.%2520Supervised%2520deep%250Alearning%2520methods%2520provide%2520accurate%2520road%2520segmentation%2520in%2520the%2520domain%2520of%2520their%250Atraining%2520data%2520but%2520cannot%2520be%2520trusted%2520in%2520out-of-distribution%2520scenarios.%2520Including%250Athe%2520whole%2520distribution%2520in%2520the%2520trainset%2520is%2520challenging%2520as%2520each%2520sample%2520must%2520be%250Alabeled%2520by%2520hand.%2520Trajectory-based%2520self-supervised%2520methods%2520offer%2520a%2520potential%250Asolution%2520as%2520they%2520can%2520learn%2520from%2520the%2520traversed%2520route%2520without%2520manual%2520labels.%250AHowever%252C%2520existing%2520trajectory-based%2520methods%2520use%2520learning%2520schemes%2520that%2520rely%2520only%250Aon%2520the%2520camera%2520or%2520only%2520on%2520the%2520lidar.%2520In%2520this%2520paper%252C%2520trajectory-based%2520learning%2520is%250Aimplemented%2520jointly%2520with%2520lidar%2520and%2520camera%2520for%2520increased%2520performance.%2520Our%2520method%250Aoutperforms%2520recent%2520standalone%2520camera-%2520and%2520lidar-based%2520methods%2520when%2520evaluated%250Awith%2520a%2520challenging%2520winter%2520driving%2520dataset%2520including%2520countryside%2520and%2520suburb%250Adriving%2520scenes.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/eerik98/lidar-camera-road-autolabeling.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory-based%20Road%20Autolabeling%20with%20Lidar-Camera%20Fusion%20in%20Winter%0A%20%20Conditions&entry.906535625=Eerik%20Alamikkotervo%20and%20Henrik%20Toikka%20and%20Kari%20Tammi%20and%20Risto%20Ojala&entry.1292438233=%20%20Robust%20road%20segmentation%20in%20all%20road%20conditions%20is%20required%20for%20safe%0Aautonomous%20driving%20and%20advanced%20driver%20assistance%20systems.%20Supervised%20deep%0Alearning%20methods%20provide%20accurate%20road%20segmentation%20in%20the%20domain%20of%20their%0Atraining%20data%20but%20cannot%20be%20trusted%20in%20out-of-distribution%20scenarios.%20Including%0Athe%20whole%20distribution%20in%20the%20trainset%20is%20challenging%20as%20each%20sample%20must%20be%0Alabeled%20by%20hand.%20Trajectory-based%20self-supervised%20methods%20offer%20a%20potential%0Asolution%20as%20they%20can%20learn%20from%20the%20traversed%20route%20without%20manual%20labels.%0AHowever%2C%20existing%20trajectory-based%20methods%20use%20learning%20schemes%20that%20rely%20only%0Aon%20the%20camera%20or%20only%20on%20the%20lidar.%20In%20this%20paper%2C%20trajectory-based%20learning%20is%0Aimplemented%20jointly%20with%20lidar%20and%20camera%20for%20increased%20performance.%20Our%20method%0Aoutperforms%20recent%20standalone%20camera-%20and%20lidar-based%20methods%20when%20evaluated%0Awith%20a%20challenging%20winter%20driving%20dataset%20including%20countryside%20and%20suburb%0Adriving%20scenes.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/eerik98/lidar-camera-road-autolabeling.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02370v1&entry.124074799=Read"},
{"title": "Trajectory-based Road Autolabeling with Lidar-Camera Fusion in Winter\n  Conditions", "author": "Eerik Alamikkotervo and Henrik Toikka and Kari Tammi and Risto Ojala", "abstract": "  Robust road segmentation in all road conditions is required for safe\nautonomous driving and advanced driver assistance systems. Supervised deep\nlearning methods provide accurate road segmentation in the domain of their\ntraining data but cannot be trusted in out-of-distribution scenarios. Including\nthe whole distribution in the trainset is challenging as each sample must be\nlabeled by hand. Trajectory-based self-supervised methods offer a potential\nsolution as they can learn from the traversed route without manual labels.\nHowever, existing trajectory-based methods use learning schemes that rely only\non the camera or only on the lidar. In this paper, trajectory-based learning is\nimplemented jointly with lidar and camera for increased performance. Our method\noutperforms recent standalone camera- and lidar-based methods when evaluated\nwith a challenging winter driving dataset including countryside and suburb\ndriving scenes. The source code is available at\nhttps://github.com/eerik98/lidar-camera-road-autolabeling.git\n", "link": "http://arxiv.org/abs/2412.02370v1", "date": "2024-12-03", "relevancy": 2.1715, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5438}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory-based%20Road%20Autolabeling%20with%20Lidar-Camera%20Fusion%20in%20Winter%0A%20%20Conditions&body=Title%3A%20Trajectory-based%20Road%20Autolabeling%20with%20Lidar-Camera%20Fusion%20in%20Winter%0A%20%20Conditions%0AAuthor%3A%20Eerik%20Alamikkotervo%20and%20Henrik%20Toikka%20and%20Kari%20Tammi%20and%20Risto%20Ojala%0AAbstract%3A%20%20%20Robust%20road%20segmentation%20in%20all%20road%20conditions%20is%20required%20for%20safe%0Aautonomous%20driving%20and%20advanced%20driver%20assistance%20systems.%20Supervised%20deep%0Alearning%20methods%20provide%20accurate%20road%20segmentation%20in%20the%20domain%20of%20their%0Atraining%20data%20but%20cannot%20be%20trusted%20in%20out-of-distribution%20scenarios.%20Including%0Athe%20whole%20distribution%20in%20the%20trainset%20is%20challenging%20as%20each%20sample%20must%20be%0Alabeled%20by%20hand.%20Trajectory-based%20self-supervised%20methods%20offer%20a%20potential%0Asolution%20as%20they%20can%20learn%20from%20the%20traversed%20route%20without%20manual%20labels.%0AHowever%2C%20existing%20trajectory-based%20methods%20use%20learning%20schemes%20that%20rely%20only%0Aon%20the%20camera%20or%20only%20on%20the%20lidar.%20In%20this%20paper%2C%20trajectory-based%20learning%20is%0Aimplemented%20jointly%20with%20lidar%20and%20camera%20for%20increased%20performance.%20Our%20method%0Aoutperforms%20recent%20standalone%20camera-%20and%20lidar-based%20methods%20when%20evaluated%0Awith%20a%20challenging%20winter%20driving%20dataset%20including%20countryside%20and%20suburb%0Adriving%20scenes.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/eerik98/lidar-camera-road-autolabeling.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory-based%2520Road%2520Autolabeling%2520with%2520Lidar-Camera%2520Fusion%2520in%2520Winter%250A%2520%2520Conditions%26entry.906535625%3DEerik%2520Alamikkotervo%2520and%2520Henrik%2520Toikka%2520and%2520Kari%2520Tammi%2520and%2520Risto%2520Ojala%26entry.1292438233%3D%2520%2520Robust%2520road%2520segmentation%2520in%2520all%2520road%2520conditions%2520is%2520required%2520for%2520safe%250Aautonomous%2520driving%2520and%2520advanced%2520driver%2520assistance%2520systems.%2520Supervised%2520deep%250Alearning%2520methods%2520provide%2520accurate%2520road%2520segmentation%2520in%2520the%2520domain%2520of%2520their%250Atraining%2520data%2520but%2520cannot%2520be%2520trusted%2520in%2520out-of-distribution%2520scenarios.%2520Including%250Athe%2520whole%2520distribution%2520in%2520the%2520trainset%2520is%2520challenging%2520as%2520each%2520sample%2520must%2520be%250Alabeled%2520by%2520hand.%2520Trajectory-based%2520self-supervised%2520methods%2520offer%2520a%2520potential%250Asolution%2520as%2520they%2520can%2520learn%2520from%2520the%2520traversed%2520route%2520without%2520manual%2520labels.%250AHowever%252C%2520existing%2520trajectory-based%2520methods%2520use%2520learning%2520schemes%2520that%2520rely%2520only%250Aon%2520the%2520camera%2520or%2520only%2520on%2520the%2520lidar.%2520In%2520this%2520paper%252C%2520trajectory-based%2520learning%2520is%250Aimplemented%2520jointly%2520with%2520lidar%2520and%2520camera%2520for%2520increased%2520performance.%2520Our%2520method%250Aoutperforms%2520recent%2520standalone%2520camera-%2520and%2520lidar-based%2520methods%2520when%2520evaluated%250Awith%2520a%2520challenging%2520winter%2520driving%2520dataset%2520including%2520countryside%2520and%2520suburb%250Adriving%2520scenes.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/eerik98/lidar-camera-road-autolabeling.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory-based%20Road%20Autolabeling%20with%20Lidar-Camera%20Fusion%20in%20Winter%0A%20%20Conditions&entry.906535625=Eerik%20Alamikkotervo%20and%20Henrik%20Toikka%20and%20Kari%20Tammi%20and%20Risto%20Ojala&entry.1292438233=%20%20Robust%20road%20segmentation%20in%20all%20road%20conditions%20is%20required%20for%20safe%0Aautonomous%20driving%20and%20advanced%20driver%20assistance%20systems.%20Supervised%20deep%0Alearning%20methods%20provide%20accurate%20road%20segmentation%20in%20the%20domain%20of%20their%0Atraining%20data%20but%20cannot%20be%20trusted%20in%20out-of-distribution%20scenarios.%20Including%0Athe%20whole%20distribution%20in%20the%20trainset%20is%20challenging%20as%20each%20sample%20must%20be%0Alabeled%20by%20hand.%20Trajectory-based%20self-supervised%20methods%20offer%20a%20potential%0Asolution%20as%20they%20can%20learn%20from%20the%20traversed%20route%20without%20manual%20labels.%0AHowever%2C%20existing%20trajectory-based%20methods%20use%20learning%20schemes%20that%20rely%20only%0Aon%20the%20camera%20or%20only%20on%20the%20lidar.%20In%20this%20paper%2C%20trajectory-based%20learning%20is%0Aimplemented%20jointly%20with%20lidar%20and%20camera%20for%20increased%20performance.%20Our%20method%0Aoutperforms%20recent%20standalone%20camera-%20and%20lidar-based%20methods%20when%20evaluated%0Awith%20a%20challenging%20winter%20driving%20dataset%20including%20countryside%20and%20suburb%0Adriving%20scenes.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/eerik98/lidar-camera-road-autolabeling.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02370v1&entry.124074799=Read"},
{"title": "Enabling DBSCAN for Very Large-Scale High-Dimensional Spaces", "author": "Yongyu Wang", "abstract": "  DBSCAN is one of the most important non-parametric unsupervised data analysis\ntools. By applying DBSCAN to a dataset, two key analytical results can be\nobtained: (1) clustering data points based on density distribution and (2)\nidentifying outliers in the dataset. However, the time complexity of the DBSCAN\nalgorithm is $O(n^2 \\beta)$, where $n$ is the number of data points and $\\beta\n= O(D)$, with $D$ representing the dimensionality of the data space. As a\nresult, DBSCAN becomes computationally infeasible when both $n$ and $D$ are\nlarge. In this paper, we propose a DBSCAN method based on spectral data\ncompression, capable of efficiently processing datasets with a large number of\ndata points ($n$) and high dimensionality ($D$). By preserving only the most\ncritical structural information during the compression process, our method\neffectively removes substantial redundancy and noise. Consequently, the\nsolution quality of DBSCAN is significantly improved, enabling more accurate\nand reliable results.\n", "link": "http://arxiv.org/abs/2411.11421v3", "date": "2024-12-03", "relevancy": 2.161, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.442}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20DBSCAN%20for%20Very%20Large-Scale%20High-Dimensional%20Spaces&body=Title%3A%20Enabling%20DBSCAN%20for%20Very%20Large-Scale%20High-Dimensional%20Spaces%0AAuthor%3A%20Yongyu%20Wang%0AAbstract%3A%20%20%20DBSCAN%20is%20one%20of%20the%20most%20important%20non-parametric%20unsupervised%20data%20analysis%0Atools.%20By%20applying%20DBSCAN%20to%20a%20dataset%2C%20two%20key%20analytical%20results%20can%20be%0Aobtained%3A%20%281%29%20clustering%20data%20points%20based%20on%20density%20distribution%20and%20%282%29%0Aidentifying%20outliers%20in%20the%20dataset.%20However%2C%20the%20time%20complexity%20of%20the%20DBSCAN%0Aalgorithm%20is%20%24O%28n%5E2%20%5Cbeta%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20data%20points%20and%20%24%5Cbeta%0A%3D%20O%28D%29%24%2C%20with%20%24D%24%20representing%20the%20dimensionality%20of%20the%20data%20space.%20As%20a%0Aresult%2C%20DBSCAN%20becomes%20computationally%20infeasible%20when%20both%20%24n%24%20and%20%24D%24%20are%0Alarge.%20In%20this%20paper%2C%20we%20propose%20a%20DBSCAN%20method%20based%20on%20spectral%20data%0Acompression%2C%20capable%20of%20efficiently%20processing%20datasets%20with%20a%20large%20number%20of%0Adata%20points%20%28%24n%24%29%20and%20high%20dimensionality%20%28%24D%24%29.%20By%20preserving%20only%20the%20most%0Acritical%20structural%20information%20during%20the%20compression%20process%2C%20our%20method%0Aeffectively%20removes%20substantial%20redundancy%20and%20noise.%20Consequently%2C%20the%0Asolution%20quality%20of%20DBSCAN%20is%20significantly%20improved%2C%20enabling%20more%20accurate%0Aand%20reliable%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11421v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520DBSCAN%2520for%2520Very%2520Large-Scale%2520High-Dimensional%2520Spaces%26entry.906535625%3DYongyu%2520Wang%26entry.1292438233%3D%2520%2520DBSCAN%2520is%2520one%2520of%2520the%2520most%2520important%2520non-parametric%2520unsupervised%2520data%2520analysis%250Atools.%2520By%2520applying%2520DBSCAN%2520to%2520a%2520dataset%252C%2520two%2520key%2520analytical%2520results%2520can%2520be%250Aobtained%253A%2520%25281%2529%2520clustering%2520data%2520points%2520based%2520on%2520density%2520distribution%2520and%2520%25282%2529%250Aidentifying%2520outliers%2520in%2520the%2520dataset.%2520However%252C%2520the%2520time%2520complexity%2520of%2520the%2520DBSCAN%250Aalgorithm%2520is%2520%2524O%2528n%255E2%2520%255Cbeta%2529%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520data%2520points%2520and%2520%2524%255Cbeta%250A%253D%2520O%2528D%2529%2524%252C%2520with%2520%2524D%2524%2520representing%2520the%2520dimensionality%2520of%2520the%2520data%2520space.%2520As%2520a%250Aresult%252C%2520DBSCAN%2520becomes%2520computationally%2520infeasible%2520when%2520both%2520%2524n%2524%2520and%2520%2524D%2524%2520are%250Alarge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520DBSCAN%2520method%2520based%2520on%2520spectral%2520data%250Acompression%252C%2520capable%2520of%2520efficiently%2520processing%2520datasets%2520with%2520a%2520large%2520number%2520of%250Adata%2520points%2520%2528%2524n%2524%2529%2520and%2520high%2520dimensionality%2520%2528%2524D%2524%2529.%2520By%2520preserving%2520only%2520the%2520most%250Acritical%2520structural%2520information%2520during%2520the%2520compression%2520process%252C%2520our%2520method%250Aeffectively%2520removes%2520substantial%2520redundancy%2520and%2520noise.%2520Consequently%252C%2520the%250Asolution%2520quality%2520of%2520DBSCAN%2520is%2520significantly%2520improved%252C%2520enabling%2520more%2520accurate%250Aand%2520reliable%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11421v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20DBSCAN%20for%20Very%20Large-Scale%20High-Dimensional%20Spaces&entry.906535625=Yongyu%20Wang&entry.1292438233=%20%20DBSCAN%20is%20one%20of%20the%20most%20important%20non-parametric%20unsupervised%20data%20analysis%0Atools.%20By%20applying%20DBSCAN%20to%20a%20dataset%2C%20two%20key%20analytical%20results%20can%20be%0Aobtained%3A%20%281%29%20clustering%20data%20points%20based%20on%20density%20distribution%20and%20%282%29%0Aidentifying%20outliers%20in%20the%20dataset.%20However%2C%20the%20time%20complexity%20of%20the%20DBSCAN%0Aalgorithm%20is%20%24O%28n%5E2%20%5Cbeta%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20data%20points%20and%20%24%5Cbeta%0A%3D%20O%28D%29%24%2C%20with%20%24D%24%20representing%20the%20dimensionality%20of%20the%20data%20space.%20As%20a%0Aresult%2C%20DBSCAN%20becomes%20computationally%20infeasible%20when%20both%20%24n%24%20and%20%24D%24%20are%0Alarge.%20In%20this%20paper%2C%20we%20propose%20a%20DBSCAN%20method%20based%20on%20spectral%20data%0Acompression%2C%20capable%20of%20efficiently%20processing%20datasets%20with%20a%20large%20number%20of%0Adata%20points%20%28%24n%24%29%20and%20high%20dimensionality%20%28%24D%24%29.%20By%20preserving%20only%20the%20most%0Acritical%20structural%20information%20during%20the%20compression%20process%2C%20our%20method%0Aeffectively%20removes%20substantial%20redundancy%20and%20noise.%20Consequently%2C%20the%0Asolution%20quality%20of%20DBSCAN%20is%20significantly%20improved%2C%20enabling%20more%20accurate%0Aand%20reliable%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11421v3&entry.124074799=Read"},
{"title": "HERO: Hint-Based Efficient and Reliable Query Optimizer", "author": "Sergey Zinchenko and Sergey Iazov", "abstract": "  We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production.\n", "link": "http://arxiv.org/abs/2412.02372v1", "date": "2024-12-03", "relevancy": 2.1468, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4373}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HERO%3A%20Hint-Based%20Efficient%20and%20Reliable%20Query%20Optimizer&body=Title%3A%20HERO%3A%20Hint-Based%20Efficient%20and%20Reliable%20Query%20Optimizer%0AAuthor%3A%20Sergey%20Zinchenko%20and%20Sergey%20Iazov%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20model%20for%20learned%20query%20optimization%20which%20provides%20query%0Ahints%20leading%20to%20better%20execution%20plans.%20The%20model%20addresses%20the%20three%20key%0Achallenges%20in%20learned%20hint-based%20query%20optimization%3A%20reliable%20hint%0Arecommendation%20%28ensuring%20non-degradation%20of%20query%20latency%29%2C%20efficient%20hint%0Aexploration%2C%20and%20fast%20inference.%20We%20provide%20an%20in-depth%20analysis%20of%20existing%0ANN-based%20approaches%20to%20hint-based%20optimization%20and%20experimentally%20confirm%20the%0Anamed%20challenges%20for%20them.%20Our%20alternative%20solution%20consists%20of%20a%20new%20inference%0Aschema%20based%20on%20an%20ensemble%20of%20context-aware%20models%20and%20a%20graph%20storage%20for%0Areliable%20hint%20suggestion%20and%20fast%20inference%2C%20and%20a%20budget-controlled%20training%0Aprocedure%20with%20a%20local%20search%20algorithm%20that%20solves%20the%20issue%20of%20exponential%0Asearch%20space%20exploration.%20In%20experiments%20on%20standard%20benchmarks%2C%20our%20model%0Ademonstrates%20optimization%20capability%20close%20to%20the%20best%20achievable%20with%0Acoarse-grained%20hints.%20Controlling%20the%20degree%20of%20parallelism%20%28query%20dop%29%20in%0Aaddition%20to%20operator-related%20hints%20enables%20our%20model%20to%20achieve%203x%20latency%0Aimprovement%20on%20JOB%20benchmark%20which%20sets%20a%20new%20standard%20for%20optimization.%20Our%0Amodel%20is%20interpretable%20and%20easy%20to%20debug%2C%20which%20is%20particularly%20important%20for%0Adeployment%20in%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHERO%253A%2520Hint-Based%2520Efficient%2520and%2520Reliable%2520Query%2520Optimizer%26entry.906535625%3DSergey%2520Zinchenko%2520and%2520Sergey%2520Iazov%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520model%2520for%2520learned%2520query%2520optimization%2520which%2520provides%2520query%250Ahints%2520leading%2520to%2520better%2520execution%2520plans.%2520The%2520model%2520addresses%2520the%2520three%2520key%250Achallenges%2520in%2520learned%2520hint-based%2520query%2520optimization%253A%2520reliable%2520hint%250Arecommendation%2520%2528ensuring%2520non-degradation%2520of%2520query%2520latency%2529%252C%2520efficient%2520hint%250Aexploration%252C%2520and%2520fast%2520inference.%2520We%2520provide%2520an%2520in-depth%2520analysis%2520of%2520existing%250ANN-based%2520approaches%2520to%2520hint-based%2520optimization%2520and%2520experimentally%2520confirm%2520the%250Anamed%2520challenges%2520for%2520them.%2520Our%2520alternative%2520solution%2520consists%2520of%2520a%2520new%2520inference%250Aschema%2520based%2520on%2520an%2520ensemble%2520of%2520context-aware%2520models%2520and%2520a%2520graph%2520storage%2520for%250Areliable%2520hint%2520suggestion%2520and%2520fast%2520inference%252C%2520and%2520a%2520budget-controlled%2520training%250Aprocedure%2520with%2520a%2520local%2520search%2520algorithm%2520that%2520solves%2520the%2520issue%2520of%2520exponential%250Asearch%2520space%2520exploration.%2520In%2520experiments%2520on%2520standard%2520benchmarks%252C%2520our%2520model%250Ademonstrates%2520optimization%2520capability%2520close%2520to%2520the%2520best%2520achievable%2520with%250Acoarse-grained%2520hints.%2520Controlling%2520the%2520degree%2520of%2520parallelism%2520%2528query%2520dop%2529%2520in%250Aaddition%2520to%2520operator-related%2520hints%2520enables%2520our%2520model%2520to%2520achieve%25203x%2520latency%250Aimprovement%2520on%2520JOB%2520benchmark%2520which%2520sets%2520a%2520new%2520standard%2520for%2520optimization.%2520Our%250Amodel%2520is%2520interpretable%2520and%2520easy%2520to%2520debug%252C%2520which%2520is%2520particularly%2520important%2520for%250Adeployment%2520in%2520production.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HERO%3A%20Hint-Based%20Efficient%20and%20Reliable%20Query%20Optimizer&entry.906535625=Sergey%20Zinchenko%20and%20Sergey%20Iazov&entry.1292438233=%20%20We%20propose%20a%20novel%20model%20for%20learned%20query%20optimization%20which%20provides%20query%0Ahints%20leading%20to%20better%20execution%20plans.%20The%20model%20addresses%20the%20three%20key%0Achallenges%20in%20learned%20hint-based%20query%20optimization%3A%20reliable%20hint%0Arecommendation%20%28ensuring%20non-degradation%20of%20query%20latency%29%2C%20efficient%20hint%0Aexploration%2C%20and%20fast%20inference.%20We%20provide%20an%20in-depth%20analysis%20of%20existing%0ANN-based%20approaches%20to%20hint-based%20optimization%20and%20experimentally%20confirm%20the%0Anamed%20challenges%20for%20them.%20Our%20alternative%20solution%20consists%20of%20a%20new%20inference%0Aschema%20based%20on%20an%20ensemble%20of%20context-aware%20models%20and%20a%20graph%20storage%20for%0Areliable%20hint%20suggestion%20and%20fast%20inference%2C%20and%20a%20budget-controlled%20training%0Aprocedure%20with%20a%20local%20search%20algorithm%20that%20solves%20the%20issue%20of%20exponential%0Asearch%20space%20exploration.%20In%20experiments%20on%20standard%20benchmarks%2C%20our%20model%0Ademonstrates%20optimization%20capability%20close%20to%20the%20best%20achievable%20with%0Acoarse-grained%20hints.%20Controlling%20the%20degree%20of%20parallelism%20%28query%20dop%29%20in%0Aaddition%20to%20operator-related%20hints%20enables%20our%20model%20to%20achieve%203x%20latency%0Aimprovement%20on%20JOB%20benchmark%20which%20sets%20a%20new%20standard%20for%20optimization.%20Our%0Amodel%20is%20interpretable%20and%20easy%20to%20debug%2C%20which%20is%20particularly%20important%20for%0Adeployment%20in%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02372v1&entry.124074799=Read"},
{"title": "HERO: Hint-Based Efficient and Reliable Query Optimizer", "author": "Sergey Zinchenko and Sergey Iazov", "abstract": "  We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production.\n", "link": "http://arxiv.org/abs/2412.02372v1", "date": "2024-12-03", "relevancy": 2.1468, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4373}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HERO%3A%20Hint-Based%20Efficient%20and%20Reliable%20Query%20Optimizer&body=Title%3A%20HERO%3A%20Hint-Based%20Efficient%20and%20Reliable%20Query%20Optimizer%0AAuthor%3A%20Sergey%20Zinchenko%20and%20Sergey%20Iazov%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20model%20for%20learned%20query%20optimization%20which%20provides%20query%0Ahints%20leading%20to%20better%20execution%20plans.%20The%20model%20addresses%20the%20three%20key%0Achallenges%20in%20learned%20hint-based%20query%20optimization%3A%20reliable%20hint%0Arecommendation%20%28ensuring%20non-degradation%20of%20query%20latency%29%2C%20efficient%20hint%0Aexploration%2C%20and%20fast%20inference.%20We%20provide%20an%20in-depth%20analysis%20of%20existing%0ANN-based%20approaches%20to%20hint-based%20optimization%20and%20experimentally%20confirm%20the%0Anamed%20challenges%20for%20them.%20Our%20alternative%20solution%20consists%20of%20a%20new%20inference%0Aschema%20based%20on%20an%20ensemble%20of%20context-aware%20models%20and%20a%20graph%20storage%20for%0Areliable%20hint%20suggestion%20and%20fast%20inference%2C%20and%20a%20budget-controlled%20training%0Aprocedure%20with%20a%20local%20search%20algorithm%20that%20solves%20the%20issue%20of%20exponential%0Asearch%20space%20exploration.%20In%20experiments%20on%20standard%20benchmarks%2C%20our%20model%0Ademonstrates%20optimization%20capability%20close%20to%20the%20best%20achievable%20with%0Acoarse-grained%20hints.%20Controlling%20the%20degree%20of%20parallelism%20%28query%20dop%29%20in%0Aaddition%20to%20operator-related%20hints%20enables%20our%20model%20to%20achieve%203x%20latency%0Aimprovement%20on%20JOB%20benchmark%20which%20sets%20a%20new%20standard%20for%20optimization.%20Our%0Amodel%20is%20interpretable%20and%20easy%20to%20debug%2C%20which%20is%20particularly%20important%20for%0Adeployment%20in%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHERO%253A%2520Hint-Based%2520Efficient%2520and%2520Reliable%2520Query%2520Optimizer%26entry.906535625%3DSergey%2520Zinchenko%2520and%2520Sergey%2520Iazov%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520model%2520for%2520learned%2520query%2520optimization%2520which%2520provides%2520query%250Ahints%2520leading%2520to%2520better%2520execution%2520plans.%2520The%2520model%2520addresses%2520the%2520three%2520key%250Achallenges%2520in%2520learned%2520hint-based%2520query%2520optimization%253A%2520reliable%2520hint%250Arecommendation%2520%2528ensuring%2520non-degradation%2520of%2520query%2520latency%2529%252C%2520efficient%2520hint%250Aexploration%252C%2520and%2520fast%2520inference.%2520We%2520provide%2520an%2520in-depth%2520analysis%2520of%2520existing%250ANN-based%2520approaches%2520to%2520hint-based%2520optimization%2520and%2520experimentally%2520confirm%2520the%250Anamed%2520challenges%2520for%2520them.%2520Our%2520alternative%2520solution%2520consists%2520of%2520a%2520new%2520inference%250Aschema%2520based%2520on%2520an%2520ensemble%2520of%2520context-aware%2520models%2520and%2520a%2520graph%2520storage%2520for%250Areliable%2520hint%2520suggestion%2520and%2520fast%2520inference%252C%2520and%2520a%2520budget-controlled%2520training%250Aprocedure%2520with%2520a%2520local%2520search%2520algorithm%2520that%2520solves%2520the%2520issue%2520of%2520exponential%250Asearch%2520space%2520exploration.%2520In%2520experiments%2520on%2520standard%2520benchmarks%252C%2520our%2520model%250Ademonstrates%2520optimization%2520capability%2520close%2520to%2520the%2520best%2520achievable%2520with%250Acoarse-grained%2520hints.%2520Controlling%2520the%2520degree%2520of%2520parallelism%2520%2528query%2520dop%2529%2520in%250Aaddition%2520to%2520operator-related%2520hints%2520enables%2520our%2520model%2520to%2520achieve%25203x%2520latency%250Aimprovement%2520on%2520JOB%2520benchmark%2520which%2520sets%2520a%2520new%2520standard%2520for%2520optimization.%2520Our%250Amodel%2520is%2520interpretable%2520and%2520easy%2520to%2520debug%252C%2520which%2520is%2520particularly%2520important%2520for%250Adeployment%2520in%2520production.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HERO%3A%20Hint-Based%20Efficient%20and%20Reliable%20Query%20Optimizer&entry.906535625=Sergey%20Zinchenko%20and%20Sergey%20Iazov&entry.1292438233=%20%20We%20propose%20a%20novel%20model%20for%20learned%20query%20optimization%20which%20provides%20query%0Ahints%20leading%20to%20better%20execution%20plans.%20The%20model%20addresses%20the%20three%20key%0Achallenges%20in%20learned%20hint-based%20query%20optimization%3A%20reliable%20hint%0Arecommendation%20%28ensuring%20non-degradation%20of%20query%20latency%29%2C%20efficient%20hint%0Aexploration%2C%20and%20fast%20inference.%20We%20provide%20an%20in-depth%20analysis%20of%20existing%0ANN-based%20approaches%20to%20hint-based%20optimization%20and%20experimentally%20confirm%20the%0Anamed%20challenges%20for%20them.%20Our%20alternative%20solution%20consists%20of%20a%20new%20inference%0Aschema%20based%20on%20an%20ensemble%20of%20context-aware%20models%20and%20a%20graph%20storage%20for%0Areliable%20hint%20suggestion%20and%20fast%20inference%2C%20and%20a%20budget-controlled%20training%0Aprocedure%20with%20a%20local%20search%20algorithm%20that%20solves%20the%20issue%20of%20exponential%0Asearch%20space%20exploration.%20In%20experiments%20on%20standard%20benchmarks%2C%20our%20model%0Ademonstrates%20optimization%20capability%20close%20to%20the%20best%20achievable%20with%0Acoarse-grained%20hints.%20Controlling%20the%20degree%20of%20parallelism%20%28query%20dop%29%20in%0Aaddition%20to%20operator-related%20hints%20enables%20our%20model%20to%20achieve%203x%20latency%0Aimprovement%20on%20JOB%20benchmark%20which%20sets%20a%20new%20standard%20for%20optimization.%20Our%0Amodel%20is%20interpretable%20and%20easy%20to%20debug%2C%20which%20is%20particularly%20important%20for%0Adeployment%20in%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02372v1&entry.124074799=Read"},
{"title": "WEM-GAN: Wavelet transform based facial expression manipulation", "author": "Dongya Sun and Yunfei Hu and Xianzhe Zhang and Yingsong Hu", "abstract": "  Facial expression manipulation aims to change human facial expressions\nwithout affecting face recognition. In order to transform the facial\nexpressions to target expressions, previous methods relied on expression labels\nto guide the manipulation process. However, these methods failed to preserve\nthe details of facial features, which causes the weakening or the loss of\nidentity information in the output image. In our work, we propose WEM-GAN, in\nshort for wavelet-based expression manipulation GAN, which puts more efforts on\npreserving the details of the original image in the editing process. Firstly,\nwe take advantage of the wavelet transform technique and combine it with our\ngenerator with a U-net autoencoder backbone, in order to improve the\ngenerator's ability to preserve more details of facial features. Secondly, we\nalso implement the high-frequency component discriminator, and use\nhigh-frequency domain adversarial loss to further constrain the optimization of\nour model, providing the generated face image with more abundant details.\nAdditionally, in order to narrow the gap between generated facial expressions\nand target expressions, we use residual connections between encoder and\ndecoder, while also using relative action units (AUs) several times. Extensive\nqualitative and quantitative experiments have demonstrated that our model\nperforms better in preserving identity features, editing capability, and image\ngeneration quality on the AffectNet dataset. It also shows superior performance\nin metrics such as Average Content Distance (ACD) and Expression Distance (ED).\n", "link": "http://arxiv.org/abs/2412.02530v1", "date": "2024-12-03", "relevancy": 2.1441, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WEM-GAN%3A%20Wavelet%20transform%20based%20facial%20expression%20manipulation&body=Title%3A%20WEM-GAN%3A%20Wavelet%20transform%20based%20facial%20expression%20manipulation%0AAuthor%3A%20Dongya%20Sun%20and%20Yunfei%20Hu%20and%20Xianzhe%20Zhang%20and%20Yingsong%20Hu%0AAbstract%3A%20%20%20Facial%20expression%20manipulation%20aims%20to%20change%20human%20facial%20expressions%0Awithout%20affecting%20face%20recognition.%20In%20order%20to%20transform%20the%20facial%0Aexpressions%20to%20target%20expressions%2C%20previous%20methods%20relied%20on%20expression%20labels%0Ato%20guide%20the%20manipulation%20process.%20However%2C%20these%20methods%20failed%20to%20preserve%0Athe%20details%20of%20facial%20features%2C%20which%20causes%20the%20weakening%20or%20the%20loss%20of%0Aidentity%20information%20in%20the%20output%20image.%20In%20our%20work%2C%20we%20propose%20WEM-GAN%2C%20in%0Ashort%20for%20wavelet-based%20expression%20manipulation%20GAN%2C%20which%20puts%20more%20efforts%20on%0Apreserving%20the%20details%20of%20the%20original%20image%20in%20the%20editing%20process.%20Firstly%2C%0Awe%20take%20advantage%20of%20the%20wavelet%20transform%20technique%20and%20combine%20it%20with%20our%0Agenerator%20with%20a%20U-net%20autoencoder%20backbone%2C%20in%20order%20to%20improve%20the%0Agenerator%27s%20ability%20to%20preserve%20more%20details%20of%20facial%20features.%20Secondly%2C%20we%0Aalso%20implement%20the%20high-frequency%20component%20discriminator%2C%20and%20use%0Ahigh-frequency%20domain%20adversarial%20loss%20to%20further%20constrain%20the%20optimization%20of%0Aour%20model%2C%20providing%20the%20generated%20face%20image%20with%20more%20abundant%20details.%0AAdditionally%2C%20in%20order%20to%20narrow%20the%20gap%20between%20generated%20facial%20expressions%0Aand%20target%20expressions%2C%20we%20use%20residual%20connections%20between%20encoder%20and%0Adecoder%2C%20while%20also%20using%20relative%20action%20units%20%28AUs%29%20several%20times.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20have%20demonstrated%20that%20our%20model%0Aperforms%20better%20in%20preserving%20identity%20features%2C%20editing%20capability%2C%20and%20image%0Ageneration%20quality%20on%20the%20AffectNet%20dataset.%20It%20also%20shows%20superior%20performance%0Ain%20metrics%20such%20as%20Average%20Content%20Distance%20%28ACD%29%20and%20Expression%20Distance%20%28ED%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWEM-GAN%253A%2520Wavelet%2520transform%2520based%2520facial%2520expression%2520manipulation%26entry.906535625%3DDongya%2520Sun%2520and%2520Yunfei%2520Hu%2520and%2520Xianzhe%2520Zhang%2520and%2520Yingsong%2520Hu%26entry.1292438233%3D%2520%2520Facial%2520expression%2520manipulation%2520aims%2520to%2520change%2520human%2520facial%2520expressions%250Awithout%2520affecting%2520face%2520recognition.%2520In%2520order%2520to%2520transform%2520the%2520facial%250Aexpressions%2520to%2520target%2520expressions%252C%2520previous%2520methods%2520relied%2520on%2520expression%2520labels%250Ato%2520guide%2520the%2520manipulation%2520process.%2520However%252C%2520these%2520methods%2520failed%2520to%2520preserve%250Athe%2520details%2520of%2520facial%2520features%252C%2520which%2520causes%2520the%2520weakening%2520or%2520the%2520loss%2520of%250Aidentity%2520information%2520in%2520the%2520output%2520image.%2520In%2520our%2520work%252C%2520we%2520propose%2520WEM-GAN%252C%2520in%250Ashort%2520for%2520wavelet-based%2520expression%2520manipulation%2520GAN%252C%2520which%2520puts%2520more%2520efforts%2520on%250Apreserving%2520the%2520details%2520of%2520the%2520original%2520image%2520in%2520the%2520editing%2520process.%2520Firstly%252C%250Awe%2520take%2520advantage%2520of%2520the%2520wavelet%2520transform%2520technique%2520and%2520combine%2520it%2520with%2520our%250Agenerator%2520with%2520a%2520U-net%2520autoencoder%2520backbone%252C%2520in%2520order%2520to%2520improve%2520the%250Agenerator%2527s%2520ability%2520to%2520preserve%2520more%2520details%2520of%2520facial%2520features.%2520Secondly%252C%2520we%250Aalso%2520implement%2520the%2520high-frequency%2520component%2520discriminator%252C%2520and%2520use%250Ahigh-frequency%2520domain%2520adversarial%2520loss%2520to%2520further%2520constrain%2520the%2520optimization%2520of%250Aour%2520model%252C%2520providing%2520the%2520generated%2520face%2520image%2520with%2520more%2520abundant%2520details.%250AAdditionally%252C%2520in%2520order%2520to%2520narrow%2520the%2520gap%2520between%2520generated%2520facial%2520expressions%250Aand%2520target%2520expressions%252C%2520we%2520use%2520residual%2520connections%2520between%2520encoder%2520and%250Adecoder%252C%2520while%2520also%2520using%2520relative%2520action%2520units%2520%2528AUs%2529%2520several%2520times.%2520Extensive%250Aqualitative%2520and%2520quantitative%2520experiments%2520have%2520demonstrated%2520that%2520our%2520model%250Aperforms%2520better%2520in%2520preserving%2520identity%2520features%252C%2520editing%2520capability%252C%2520and%2520image%250Ageneration%2520quality%2520on%2520the%2520AffectNet%2520dataset.%2520It%2520also%2520shows%2520superior%2520performance%250Ain%2520metrics%2520such%2520as%2520Average%2520Content%2520Distance%2520%2528ACD%2529%2520and%2520Expression%2520Distance%2520%2528ED%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEM-GAN%3A%20Wavelet%20transform%20based%20facial%20expression%20manipulation&entry.906535625=Dongya%20Sun%20and%20Yunfei%20Hu%20and%20Xianzhe%20Zhang%20and%20Yingsong%20Hu&entry.1292438233=%20%20Facial%20expression%20manipulation%20aims%20to%20change%20human%20facial%20expressions%0Awithout%20affecting%20face%20recognition.%20In%20order%20to%20transform%20the%20facial%0Aexpressions%20to%20target%20expressions%2C%20previous%20methods%20relied%20on%20expression%20labels%0Ato%20guide%20the%20manipulation%20process.%20However%2C%20these%20methods%20failed%20to%20preserve%0Athe%20details%20of%20facial%20features%2C%20which%20causes%20the%20weakening%20or%20the%20loss%20of%0Aidentity%20information%20in%20the%20output%20image.%20In%20our%20work%2C%20we%20propose%20WEM-GAN%2C%20in%0Ashort%20for%20wavelet-based%20expression%20manipulation%20GAN%2C%20which%20puts%20more%20efforts%20on%0Apreserving%20the%20details%20of%20the%20original%20image%20in%20the%20editing%20process.%20Firstly%2C%0Awe%20take%20advantage%20of%20the%20wavelet%20transform%20technique%20and%20combine%20it%20with%20our%0Agenerator%20with%20a%20U-net%20autoencoder%20backbone%2C%20in%20order%20to%20improve%20the%0Agenerator%27s%20ability%20to%20preserve%20more%20details%20of%20facial%20features.%20Secondly%2C%20we%0Aalso%20implement%20the%20high-frequency%20component%20discriminator%2C%20and%20use%0Ahigh-frequency%20domain%20adversarial%20loss%20to%20further%20constrain%20the%20optimization%20of%0Aour%20model%2C%20providing%20the%20generated%20face%20image%20with%20more%20abundant%20details.%0AAdditionally%2C%20in%20order%20to%20narrow%20the%20gap%20between%20generated%20facial%20expressions%0Aand%20target%20expressions%2C%20we%20use%20residual%20connections%20between%20encoder%20and%0Adecoder%2C%20while%20also%20using%20relative%20action%20units%20%28AUs%29%20several%20times.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20have%20demonstrated%20that%20our%20model%0Aperforms%20better%20in%20preserving%20identity%20features%2C%20editing%20capability%2C%20and%20image%0Ageneration%20quality%20on%20the%20AffectNet%20dataset.%20It%20also%20shows%20superior%20performance%0Ain%20metrics%20such%20as%20Average%20Content%20Distance%20%28ACD%29%20and%20Expression%20Distance%20%28ED%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02530v1&entry.124074799=Read"},
{"title": "WEM-GAN: Wavelet transform based facial expression manipulation", "author": "Dongya Sun and Yunfei Hu and Xianzhe Zhang and Yingsong Hu", "abstract": "  Facial expression manipulation aims to change human facial expressions\nwithout affecting face recognition. In order to transform the facial\nexpressions to target expressions, previous methods relied on expression labels\nto guide the manipulation process. However, these methods failed to preserve\nthe details of facial features, which causes the weakening or the loss of\nidentity information in the output image. In our work, we propose WEM-GAN, in\nshort for wavelet-based expression manipulation GAN, which puts more efforts on\npreserving the details of the original image in the editing process. Firstly,\nwe take advantage of the wavelet transform technique and combine it with our\ngenerator with a U-net autoencoder backbone, in order to improve the\ngenerator's ability to preserve more details of facial features. Secondly, we\nalso implement the high-frequency component discriminator, and use\nhigh-frequency domain adversarial loss to further constrain the optimization of\nour model, providing the generated face image with more abundant details.\nAdditionally, in order to narrow the gap between generated facial expressions\nand target expressions, we use residual connections between encoder and\ndecoder, while also using relative action units (AUs) several times. Extensive\nqualitative and quantitative experiments have demonstrated that our model\nperforms better in preserving identity features, editing capability, and image\ngeneration quality on the AffectNet dataset. It also shows superior performance\nin metrics such as Average Content Distance (ACD) and Expression Distance (ED).\n", "link": "http://arxiv.org/abs/2412.02530v1", "date": "2024-12-03", "relevancy": 2.1441, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WEM-GAN%3A%20Wavelet%20transform%20based%20facial%20expression%20manipulation&body=Title%3A%20WEM-GAN%3A%20Wavelet%20transform%20based%20facial%20expression%20manipulation%0AAuthor%3A%20Dongya%20Sun%20and%20Yunfei%20Hu%20and%20Xianzhe%20Zhang%20and%20Yingsong%20Hu%0AAbstract%3A%20%20%20Facial%20expression%20manipulation%20aims%20to%20change%20human%20facial%20expressions%0Awithout%20affecting%20face%20recognition.%20In%20order%20to%20transform%20the%20facial%0Aexpressions%20to%20target%20expressions%2C%20previous%20methods%20relied%20on%20expression%20labels%0Ato%20guide%20the%20manipulation%20process.%20However%2C%20these%20methods%20failed%20to%20preserve%0Athe%20details%20of%20facial%20features%2C%20which%20causes%20the%20weakening%20or%20the%20loss%20of%0Aidentity%20information%20in%20the%20output%20image.%20In%20our%20work%2C%20we%20propose%20WEM-GAN%2C%20in%0Ashort%20for%20wavelet-based%20expression%20manipulation%20GAN%2C%20which%20puts%20more%20efforts%20on%0Apreserving%20the%20details%20of%20the%20original%20image%20in%20the%20editing%20process.%20Firstly%2C%0Awe%20take%20advantage%20of%20the%20wavelet%20transform%20technique%20and%20combine%20it%20with%20our%0Agenerator%20with%20a%20U-net%20autoencoder%20backbone%2C%20in%20order%20to%20improve%20the%0Agenerator%27s%20ability%20to%20preserve%20more%20details%20of%20facial%20features.%20Secondly%2C%20we%0Aalso%20implement%20the%20high-frequency%20component%20discriminator%2C%20and%20use%0Ahigh-frequency%20domain%20adversarial%20loss%20to%20further%20constrain%20the%20optimization%20of%0Aour%20model%2C%20providing%20the%20generated%20face%20image%20with%20more%20abundant%20details.%0AAdditionally%2C%20in%20order%20to%20narrow%20the%20gap%20between%20generated%20facial%20expressions%0Aand%20target%20expressions%2C%20we%20use%20residual%20connections%20between%20encoder%20and%0Adecoder%2C%20while%20also%20using%20relative%20action%20units%20%28AUs%29%20several%20times.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20have%20demonstrated%20that%20our%20model%0Aperforms%20better%20in%20preserving%20identity%20features%2C%20editing%20capability%2C%20and%20image%0Ageneration%20quality%20on%20the%20AffectNet%20dataset.%20It%20also%20shows%20superior%20performance%0Ain%20metrics%20such%20as%20Average%20Content%20Distance%20%28ACD%29%20and%20Expression%20Distance%20%28ED%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWEM-GAN%253A%2520Wavelet%2520transform%2520based%2520facial%2520expression%2520manipulation%26entry.906535625%3DDongya%2520Sun%2520and%2520Yunfei%2520Hu%2520and%2520Xianzhe%2520Zhang%2520and%2520Yingsong%2520Hu%26entry.1292438233%3D%2520%2520Facial%2520expression%2520manipulation%2520aims%2520to%2520change%2520human%2520facial%2520expressions%250Awithout%2520affecting%2520face%2520recognition.%2520In%2520order%2520to%2520transform%2520the%2520facial%250Aexpressions%2520to%2520target%2520expressions%252C%2520previous%2520methods%2520relied%2520on%2520expression%2520labels%250Ato%2520guide%2520the%2520manipulation%2520process.%2520However%252C%2520these%2520methods%2520failed%2520to%2520preserve%250Athe%2520details%2520of%2520facial%2520features%252C%2520which%2520causes%2520the%2520weakening%2520or%2520the%2520loss%2520of%250Aidentity%2520information%2520in%2520the%2520output%2520image.%2520In%2520our%2520work%252C%2520we%2520propose%2520WEM-GAN%252C%2520in%250Ashort%2520for%2520wavelet-based%2520expression%2520manipulation%2520GAN%252C%2520which%2520puts%2520more%2520efforts%2520on%250Apreserving%2520the%2520details%2520of%2520the%2520original%2520image%2520in%2520the%2520editing%2520process.%2520Firstly%252C%250Awe%2520take%2520advantage%2520of%2520the%2520wavelet%2520transform%2520technique%2520and%2520combine%2520it%2520with%2520our%250Agenerator%2520with%2520a%2520U-net%2520autoencoder%2520backbone%252C%2520in%2520order%2520to%2520improve%2520the%250Agenerator%2527s%2520ability%2520to%2520preserve%2520more%2520details%2520of%2520facial%2520features.%2520Secondly%252C%2520we%250Aalso%2520implement%2520the%2520high-frequency%2520component%2520discriminator%252C%2520and%2520use%250Ahigh-frequency%2520domain%2520adversarial%2520loss%2520to%2520further%2520constrain%2520the%2520optimization%2520of%250Aour%2520model%252C%2520providing%2520the%2520generated%2520face%2520image%2520with%2520more%2520abundant%2520details.%250AAdditionally%252C%2520in%2520order%2520to%2520narrow%2520the%2520gap%2520between%2520generated%2520facial%2520expressions%250Aand%2520target%2520expressions%252C%2520we%2520use%2520residual%2520connections%2520between%2520encoder%2520and%250Adecoder%252C%2520while%2520also%2520using%2520relative%2520action%2520units%2520%2528AUs%2529%2520several%2520times.%2520Extensive%250Aqualitative%2520and%2520quantitative%2520experiments%2520have%2520demonstrated%2520that%2520our%2520model%250Aperforms%2520better%2520in%2520preserving%2520identity%2520features%252C%2520editing%2520capability%252C%2520and%2520image%250Ageneration%2520quality%2520on%2520the%2520AffectNet%2520dataset.%2520It%2520also%2520shows%2520superior%2520performance%250Ain%2520metrics%2520such%2520as%2520Average%2520Content%2520Distance%2520%2528ACD%2529%2520and%2520Expression%2520Distance%2520%2528ED%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEM-GAN%3A%20Wavelet%20transform%20based%20facial%20expression%20manipulation&entry.906535625=Dongya%20Sun%20and%20Yunfei%20Hu%20and%20Xianzhe%20Zhang%20and%20Yingsong%20Hu&entry.1292438233=%20%20Facial%20expression%20manipulation%20aims%20to%20change%20human%20facial%20expressions%0Awithout%20affecting%20face%20recognition.%20In%20order%20to%20transform%20the%20facial%0Aexpressions%20to%20target%20expressions%2C%20previous%20methods%20relied%20on%20expression%20labels%0Ato%20guide%20the%20manipulation%20process.%20However%2C%20these%20methods%20failed%20to%20preserve%0Athe%20details%20of%20facial%20features%2C%20which%20causes%20the%20weakening%20or%20the%20loss%20of%0Aidentity%20information%20in%20the%20output%20image.%20In%20our%20work%2C%20we%20propose%20WEM-GAN%2C%20in%0Ashort%20for%20wavelet-based%20expression%20manipulation%20GAN%2C%20which%20puts%20more%20efforts%20on%0Apreserving%20the%20details%20of%20the%20original%20image%20in%20the%20editing%20process.%20Firstly%2C%0Awe%20take%20advantage%20of%20the%20wavelet%20transform%20technique%20and%20combine%20it%20with%20our%0Agenerator%20with%20a%20U-net%20autoencoder%20backbone%2C%20in%20order%20to%20improve%20the%0Agenerator%27s%20ability%20to%20preserve%20more%20details%20of%20facial%20features.%20Secondly%2C%20we%0Aalso%20implement%20the%20high-frequency%20component%20discriminator%2C%20and%20use%0Ahigh-frequency%20domain%20adversarial%20loss%20to%20further%20constrain%20the%20optimization%20of%0Aour%20model%2C%20providing%20the%20generated%20face%20image%20with%20more%20abundant%20details.%0AAdditionally%2C%20in%20order%20to%20narrow%20the%20gap%20between%20generated%20facial%20expressions%0Aand%20target%20expressions%2C%20we%20use%20residual%20connections%20between%20encoder%20and%0Adecoder%2C%20while%20also%20using%20relative%20action%20units%20%28AUs%29%20several%20times.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20have%20demonstrated%20that%20our%20model%0Aperforms%20better%20in%20preserving%20identity%20features%2C%20editing%20capability%2C%20and%20image%0Ageneration%20quality%20on%20the%20AffectNet%20dataset.%20It%20also%20shows%20superior%20performance%0Ain%20metrics%20such%20as%20Average%20Content%20Distance%20%28ACD%29%20and%20Expression%20Distance%20%28ED%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02530v1&entry.124074799=Read"},
{"title": "Who Walks With You Matters: Perceiving Social Interactions with Groups\n  for Pedestrian Trajectory Prediction", "author": "Ziqian Zou and Conghao Wong and Beihao Xia and Qinmu Peng and Xinge You", "abstract": "  Understanding and anticipating human movement has become more critical and\nchallenging in diverse applications such as autonomous driving and\nsurveillance. The complex interactions brought by different relations between\nagents are a crucial reason that poses challenges to this task. Researchers\nhave put much effort into designing a system using rule-based or data-based\nmodels to extract and validate the patterns between pedestrian trajectories and\nthese interactions, which has not been adequately addressed yet. Inspired by\nhow humans perceive social interactions with different level of relations to\nthemself, this work proposes the GrouP ConCeption (short for GPCC) model\ncomposed of the Group method, which categorizes nearby agents into either group\nmembers or non-group members based on a long-term distance kernel function, and\nthe Conception module, which perceives both visual and acoustic information\nsurrounding the target agent. Evaluated across multiple datasets, the GPCC\nmodel demonstrates significant improvements in trajectory prediction accuracy,\nvalidating its effectiveness in modeling both social and individual dynamics.\nThe qualitative analysis also indicates that the GPCC framework successfully\nleverages grouping and perception cues human-like intuitively to validate the\nproposed model's explainability in pedestrian trajectory forecasting.\n", "link": "http://arxiv.org/abs/2412.02395v1", "date": "2024-12-03", "relevancy": 2.1269, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5949}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20Walks%20With%20You%20Matters%3A%20Perceiving%20Social%20Interactions%20with%20Groups%0A%20%20for%20Pedestrian%20Trajectory%20Prediction&body=Title%3A%20Who%20Walks%20With%20You%20Matters%3A%20Perceiving%20Social%20Interactions%20with%20Groups%0A%20%20for%20Pedestrian%20Trajectory%20Prediction%0AAuthor%3A%20Ziqian%20Zou%20and%20Conghao%20Wong%20and%20Beihao%20Xia%20and%20Qinmu%20Peng%20and%20Xinge%20You%0AAbstract%3A%20%20%20Understanding%20and%20anticipating%20human%20movement%20has%20become%20more%20critical%20and%0Achallenging%20in%20diverse%20applications%20such%20as%20autonomous%20driving%20and%0Asurveillance.%20The%20complex%20interactions%20brought%20by%20different%20relations%20between%0Aagents%20are%20a%20crucial%20reason%20that%20poses%20challenges%20to%20this%20task.%20Researchers%0Ahave%20put%20much%20effort%20into%20designing%20a%20system%20using%20rule-based%20or%20data-based%0Amodels%20to%20extract%20and%20validate%20the%20patterns%20between%20pedestrian%20trajectories%20and%0Athese%20interactions%2C%20which%20has%20not%20been%20adequately%20addressed%20yet.%20Inspired%20by%0Ahow%20humans%20perceive%20social%20interactions%20with%20different%20level%20of%20relations%20to%0Athemself%2C%20this%20work%20proposes%20the%20GrouP%20ConCeption%20%28short%20for%20GPCC%29%20model%0Acomposed%20of%20the%20Group%20method%2C%20which%20categorizes%20nearby%20agents%20into%20either%20group%0Amembers%20or%20non-group%20members%20based%20on%20a%20long-term%20distance%20kernel%20function%2C%20and%0Athe%20Conception%20module%2C%20which%20perceives%20both%20visual%20and%20acoustic%20information%0Asurrounding%20the%20target%20agent.%20Evaluated%20across%20multiple%20datasets%2C%20the%20GPCC%0Amodel%20demonstrates%20significant%20improvements%20in%20trajectory%20prediction%20accuracy%2C%0Avalidating%20its%20effectiveness%20in%20modeling%20both%20social%20and%20individual%20dynamics.%0AThe%20qualitative%20analysis%20also%20indicates%20that%20the%20GPCC%20framework%20successfully%0Aleverages%20grouping%20and%20perception%20cues%20human-like%20intuitively%20to%20validate%20the%0Aproposed%20model%27s%20explainability%20in%20pedestrian%20trajectory%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520Walks%2520With%2520You%2520Matters%253A%2520Perceiving%2520Social%2520Interactions%2520with%2520Groups%250A%2520%2520for%2520Pedestrian%2520Trajectory%2520Prediction%26entry.906535625%3DZiqian%2520Zou%2520and%2520Conghao%2520Wong%2520and%2520Beihao%2520Xia%2520and%2520Qinmu%2520Peng%2520and%2520Xinge%2520You%26entry.1292438233%3D%2520%2520Understanding%2520and%2520anticipating%2520human%2520movement%2520has%2520become%2520more%2520critical%2520and%250Achallenging%2520in%2520diverse%2520applications%2520such%2520as%2520autonomous%2520driving%2520and%250Asurveillance.%2520The%2520complex%2520interactions%2520brought%2520by%2520different%2520relations%2520between%250Aagents%2520are%2520a%2520crucial%2520reason%2520that%2520poses%2520challenges%2520to%2520this%2520task.%2520Researchers%250Ahave%2520put%2520much%2520effort%2520into%2520designing%2520a%2520system%2520using%2520rule-based%2520or%2520data-based%250Amodels%2520to%2520extract%2520and%2520validate%2520the%2520patterns%2520between%2520pedestrian%2520trajectories%2520and%250Athese%2520interactions%252C%2520which%2520has%2520not%2520been%2520adequately%2520addressed%2520yet.%2520Inspired%2520by%250Ahow%2520humans%2520perceive%2520social%2520interactions%2520with%2520different%2520level%2520of%2520relations%2520to%250Athemself%252C%2520this%2520work%2520proposes%2520the%2520GrouP%2520ConCeption%2520%2528short%2520for%2520GPCC%2529%2520model%250Acomposed%2520of%2520the%2520Group%2520method%252C%2520which%2520categorizes%2520nearby%2520agents%2520into%2520either%2520group%250Amembers%2520or%2520non-group%2520members%2520based%2520on%2520a%2520long-term%2520distance%2520kernel%2520function%252C%2520and%250Athe%2520Conception%2520module%252C%2520which%2520perceives%2520both%2520visual%2520and%2520acoustic%2520information%250Asurrounding%2520the%2520target%2520agent.%2520Evaluated%2520across%2520multiple%2520datasets%252C%2520the%2520GPCC%250Amodel%2520demonstrates%2520significant%2520improvements%2520in%2520trajectory%2520prediction%2520accuracy%252C%250Avalidating%2520its%2520effectiveness%2520in%2520modeling%2520both%2520social%2520and%2520individual%2520dynamics.%250AThe%2520qualitative%2520analysis%2520also%2520indicates%2520that%2520the%2520GPCC%2520framework%2520successfully%250Aleverages%2520grouping%2520and%2520perception%2520cues%2520human-like%2520intuitively%2520to%2520validate%2520the%250Aproposed%2520model%2527s%2520explainability%2520in%2520pedestrian%2520trajectory%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Walks%20With%20You%20Matters%3A%20Perceiving%20Social%20Interactions%20with%20Groups%0A%20%20for%20Pedestrian%20Trajectory%20Prediction&entry.906535625=Ziqian%20Zou%20and%20Conghao%20Wong%20and%20Beihao%20Xia%20and%20Qinmu%20Peng%20and%20Xinge%20You&entry.1292438233=%20%20Understanding%20and%20anticipating%20human%20movement%20has%20become%20more%20critical%20and%0Achallenging%20in%20diverse%20applications%20such%20as%20autonomous%20driving%20and%0Asurveillance.%20The%20complex%20interactions%20brought%20by%20different%20relations%20between%0Aagents%20are%20a%20crucial%20reason%20that%20poses%20challenges%20to%20this%20task.%20Researchers%0Ahave%20put%20much%20effort%20into%20designing%20a%20system%20using%20rule-based%20or%20data-based%0Amodels%20to%20extract%20and%20validate%20the%20patterns%20between%20pedestrian%20trajectories%20and%0Athese%20interactions%2C%20which%20has%20not%20been%20adequately%20addressed%20yet.%20Inspired%20by%0Ahow%20humans%20perceive%20social%20interactions%20with%20different%20level%20of%20relations%20to%0Athemself%2C%20this%20work%20proposes%20the%20GrouP%20ConCeption%20%28short%20for%20GPCC%29%20model%0Acomposed%20of%20the%20Group%20method%2C%20which%20categorizes%20nearby%20agents%20into%20either%20group%0Amembers%20or%20non-group%20members%20based%20on%20a%20long-term%20distance%20kernel%20function%2C%20and%0Athe%20Conception%20module%2C%20which%20perceives%20both%20visual%20and%20acoustic%20information%0Asurrounding%20the%20target%20agent.%20Evaluated%20across%20multiple%20datasets%2C%20the%20GPCC%0Amodel%20demonstrates%20significant%20improvements%20in%20trajectory%20prediction%20accuracy%2C%0Avalidating%20its%20effectiveness%20in%20modeling%20both%20social%20and%20individual%20dynamics.%0AThe%20qualitative%20analysis%20also%20indicates%20that%20the%20GPCC%20framework%20successfully%0Aleverages%20grouping%20and%20perception%20cues%20human-like%20intuitively%20to%20validate%20the%0Aproposed%20model%27s%20explainability%20in%20pedestrian%20trajectory%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02395v1&entry.124074799=Read"},
{"title": "Who Walks With You Matters: Perceiving Social Interactions with Groups\n  for Pedestrian Trajectory Prediction", "author": "Ziqian Zou and Conghao Wong and Beihao Xia and Qinmu Peng and Xinge You", "abstract": "  Understanding and anticipating human movement has become more critical and\nchallenging in diverse applications such as autonomous driving and\nsurveillance. The complex interactions brought by different relations between\nagents are a crucial reason that poses challenges to this task. Researchers\nhave put much effort into designing a system using rule-based or data-based\nmodels to extract and validate the patterns between pedestrian trajectories and\nthese interactions, which has not been adequately addressed yet. Inspired by\nhow humans perceive social interactions with different level of relations to\nthemself, this work proposes the GrouP ConCeption (short for GPCC) model\ncomposed of the Group method, which categorizes nearby agents into either group\nmembers or non-group members based on a long-term distance kernel function, and\nthe Conception module, which perceives both visual and acoustic information\nsurrounding the target agent. Evaluated across multiple datasets, the GPCC\nmodel demonstrates significant improvements in trajectory prediction accuracy,\nvalidating its effectiveness in modeling both social and individual dynamics.\nThe qualitative analysis also indicates that the GPCC framework successfully\nleverages grouping and perception cues human-like intuitively to validate the\nproposed model's explainability in pedestrian trajectory forecasting.\n", "link": "http://arxiv.org/abs/2412.02395v1", "date": "2024-12-03", "relevancy": 2.1269, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5949}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20Walks%20With%20You%20Matters%3A%20Perceiving%20Social%20Interactions%20with%20Groups%0A%20%20for%20Pedestrian%20Trajectory%20Prediction&body=Title%3A%20Who%20Walks%20With%20You%20Matters%3A%20Perceiving%20Social%20Interactions%20with%20Groups%0A%20%20for%20Pedestrian%20Trajectory%20Prediction%0AAuthor%3A%20Ziqian%20Zou%20and%20Conghao%20Wong%20and%20Beihao%20Xia%20and%20Qinmu%20Peng%20and%20Xinge%20You%0AAbstract%3A%20%20%20Understanding%20and%20anticipating%20human%20movement%20has%20become%20more%20critical%20and%0Achallenging%20in%20diverse%20applications%20such%20as%20autonomous%20driving%20and%0Asurveillance.%20The%20complex%20interactions%20brought%20by%20different%20relations%20between%0Aagents%20are%20a%20crucial%20reason%20that%20poses%20challenges%20to%20this%20task.%20Researchers%0Ahave%20put%20much%20effort%20into%20designing%20a%20system%20using%20rule-based%20or%20data-based%0Amodels%20to%20extract%20and%20validate%20the%20patterns%20between%20pedestrian%20trajectories%20and%0Athese%20interactions%2C%20which%20has%20not%20been%20adequately%20addressed%20yet.%20Inspired%20by%0Ahow%20humans%20perceive%20social%20interactions%20with%20different%20level%20of%20relations%20to%0Athemself%2C%20this%20work%20proposes%20the%20GrouP%20ConCeption%20%28short%20for%20GPCC%29%20model%0Acomposed%20of%20the%20Group%20method%2C%20which%20categorizes%20nearby%20agents%20into%20either%20group%0Amembers%20or%20non-group%20members%20based%20on%20a%20long-term%20distance%20kernel%20function%2C%20and%0Athe%20Conception%20module%2C%20which%20perceives%20both%20visual%20and%20acoustic%20information%0Asurrounding%20the%20target%20agent.%20Evaluated%20across%20multiple%20datasets%2C%20the%20GPCC%0Amodel%20demonstrates%20significant%20improvements%20in%20trajectory%20prediction%20accuracy%2C%0Avalidating%20its%20effectiveness%20in%20modeling%20both%20social%20and%20individual%20dynamics.%0AThe%20qualitative%20analysis%20also%20indicates%20that%20the%20GPCC%20framework%20successfully%0Aleverages%20grouping%20and%20perception%20cues%20human-like%20intuitively%20to%20validate%20the%0Aproposed%20model%27s%20explainability%20in%20pedestrian%20trajectory%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520Walks%2520With%2520You%2520Matters%253A%2520Perceiving%2520Social%2520Interactions%2520with%2520Groups%250A%2520%2520for%2520Pedestrian%2520Trajectory%2520Prediction%26entry.906535625%3DZiqian%2520Zou%2520and%2520Conghao%2520Wong%2520and%2520Beihao%2520Xia%2520and%2520Qinmu%2520Peng%2520and%2520Xinge%2520You%26entry.1292438233%3D%2520%2520Understanding%2520and%2520anticipating%2520human%2520movement%2520has%2520become%2520more%2520critical%2520and%250Achallenging%2520in%2520diverse%2520applications%2520such%2520as%2520autonomous%2520driving%2520and%250Asurveillance.%2520The%2520complex%2520interactions%2520brought%2520by%2520different%2520relations%2520between%250Aagents%2520are%2520a%2520crucial%2520reason%2520that%2520poses%2520challenges%2520to%2520this%2520task.%2520Researchers%250Ahave%2520put%2520much%2520effort%2520into%2520designing%2520a%2520system%2520using%2520rule-based%2520or%2520data-based%250Amodels%2520to%2520extract%2520and%2520validate%2520the%2520patterns%2520between%2520pedestrian%2520trajectories%2520and%250Athese%2520interactions%252C%2520which%2520has%2520not%2520been%2520adequately%2520addressed%2520yet.%2520Inspired%2520by%250Ahow%2520humans%2520perceive%2520social%2520interactions%2520with%2520different%2520level%2520of%2520relations%2520to%250Athemself%252C%2520this%2520work%2520proposes%2520the%2520GrouP%2520ConCeption%2520%2528short%2520for%2520GPCC%2529%2520model%250Acomposed%2520of%2520the%2520Group%2520method%252C%2520which%2520categorizes%2520nearby%2520agents%2520into%2520either%2520group%250Amembers%2520or%2520non-group%2520members%2520based%2520on%2520a%2520long-term%2520distance%2520kernel%2520function%252C%2520and%250Athe%2520Conception%2520module%252C%2520which%2520perceives%2520both%2520visual%2520and%2520acoustic%2520information%250Asurrounding%2520the%2520target%2520agent.%2520Evaluated%2520across%2520multiple%2520datasets%252C%2520the%2520GPCC%250Amodel%2520demonstrates%2520significant%2520improvements%2520in%2520trajectory%2520prediction%2520accuracy%252C%250Avalidating%2520its%2520effectiveness%2520in%2520modeling%2520both%2520social%2520and%2520individual%2520dynamics.%250AThe%2520qualitative%2520analysis%2520also%2520indicates%2520that%2520the%2520GPCC%2520framework%2520successfully%250Aleverages%2520grouping%2520and%2520perception%2520cues%2520human-like%2520intuitively%2520to%2520validate%2520the%250Aproposed%2520model%2527s%2520explainability%2520in%2520pedestrian%2520trajectory%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Walks%20With%20You%20Matters%3A%20Perceiving%20Social%20Interactions%20with%20Groups%0A%20%20for%20Pedestrian%20Trajectory%20Prediction&entry.906535625=Ziqian%20Zou%20and%20Conghao%20Wong%20and%20Beihao%20Xia%20and%20Qinmu%20Peng%20and%20Xinge%20You&entry.1292438233=%20%20Understanding%20and%20anticipating%20human%20movement%20has%20become%20more%20critical%20and%0Achallenging%20in%20diverse%20applications%20such%20as%20autonomous%20driving%20and%0Asurveillance.%20The%20complex%20interactions%20brought%20by%20different%20relations%20between%0Aagents%20are%20a%20crucial%20reason%20that%20poses%20challenges%20to%20this%20task.%20Researchers%0Ahave%20put%20much%20effort%20into%20designing%20a%20system%20using%20rule-based%20or%20data-based%0Amodels%20to%20extract%20and%20validate%20the%20patterns%20between%20pedestrian%20trajectories%20and%0Athese%20interactions%2C%20which%20has%20not%20been%20adequately%20addressed%20yet.%20Inspired%20by%0Ahow%20humans%20perceive%20social%20interactions%20with%20different%20level%20of%20relations%20to%0Athemself%2C%20this%20work%20proposes%20the%20GrouP%20ConCeption%20%28short%20for%20GPCC%29%20model%0Acomposed%20of%20the%20Group%20method%2C%20which%20categorizes%20nearby%20agents%20into%20either%20group%0Amembers%20or%20non-group%20members%20based%20on%20a%20long-term%20distance%20kernel%20function%2C%20and%0Athe%20Conception%20module%2C%20which%20perceives%20both%20visual%20and%20acoustic%20information%0Asurrounding%20the%20target%20agent.%20Evaluated%20across%20multiple%20datasets%2C%20the%20GPCC%0Amodel%20demonstrates%20significant%20improvements%20in%20trajectory%20prediction%20accuracy%2C%0Avalidating%20its%20effectiveness%20in%20modeling%20both%20social%20and%20individual%20dynamics.%0AThe%20qualitative%20analysis%20also%20indicates%20that%20the%20GPCC%20framework%20successfully%0Aleverages%20grouping%20and%20perception%20cues%20human-like%20intuitively%20to%20validate%20the%0Aproposed%20model%27s%20explainability%20in%20pedestrian%20trajectory%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02395v1&entry.124074799=Read"},
{"title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving", "author": "Teng Wang and Wing-Yin Yu and Zhenqi He and Zehua Liu and Xiongwei Han and Hailei Gong and Han Wu and Wei Shi and Ruifeng She and Fangzhou Zhu and Tao Zhong", "abstract": "  LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, a algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions.\n", "link": "http://arxiv.org/abs/2411.17404v2", "date": "2024-12-03", "relevancy": 2.1177, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BPP-Search%3A%20Enhancing%20Tree%20of%20Thought%20Reasoning%20for%20Mathematical%0A%20%20Modeling%20Problem%20Solving&body=Title%3A%20BPP-Search%3A%20Enhancing%20Tree%20of%20Thought%20Reasoning%20for%20Mathematical%0A%20%20Modeling%20Problem%20Solving%0AAuthor%3A%20Teng%20Wang%20and%20Wing-Yin%20Yu%20and%20Zhenqi%20He%20and%20Zehua%20Liu%20and%20Xiongwei%20Han%20and%20Hailei%20Gong%20and%20Han%20Wu%20and%20Wei%20Shi%20and%20Ruifeng%20She%20and%20Fangzhou%20Zhu%20and%20Tao%20Zhong%0AAbstract%3A%20%20%20LLMs%20exhibit%20advanced%20reasoning%20capabilities%2C%20offering%20the%20potential%20to%0Atransform%20natural%20language%20questions%20into%20mathematical%20models.%20However%2C%0Aexisting%20open-source%20datasets%20in%20operations%20research%20domain%20lack%20detailed%0Aannotations%20of%20the%20modeling%20process%2C%20such%20as%20variable%20definitions%2C%20focusing%0Asolely%20on%20objective%20values%2C%20which%20hinders%20reinforcement%20learning%20applications.%0ATo%20address%20this%2C%20we%20release%20the%20StructuredOR%20dataset%2C%20annotated%20with%0Acomprehensive%20labels%20that%20capture%20the%20complete%20mathematical%20modeling%20process.%0AWe%20further%20propose%20BPP-Search%2C%20a%20algorithm%20that%20integrates%20reinforcement%0Alearning%20into%20a%20tree-of-thought%20structure%20using%20Beam%20search%2C%20a%20Process%20reward%0Amodel%2C%20and%20a%20pairwise%20Preference%20algorithm.%20This%20approach%20enables%20efficient%0Aexploration%20of%20tree%20structures%2C%20avoiding%20exhaustive%20search%20while%20improving%0Aaccuracy.%20Extensive%20experiments%20on%20StructuredOR%2C%20NL4OPT%2C%20and%20MAMO-ComplexLP%0Adatasets%20show%20that%20BPP-Search%20significantly%20outperforms%20state-of-the-art%0Amethods.%20In%20tree-based%20reasoning%2C%20BPP-Search%20excels%20in%20accuracy%20and%20efficiency%2C%0Aenabling%20faster%20retrieval%20of%20correct%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBPP-Search%253A%2520Enhancing%2520Tree%2520of%2520Thought%2520Reasoning%2520for%2520Mathematical%250A%2520%2520Modeling%2520Problem%2520Solving%26entry.906535625%3DTeng%2520Wang%2520and%2520Wing-Yin%2520Yu%2520and%2520Zhenqi%2520He%2520and%2520Zehua%2520Liu%2520and%2520Xiongwei%2520Han%2520and%2520Hailei%2520Gong%2520and%2520Han%2520Wu%2520and%2520Wei%2520Shi%2520and%2520Ruifeng%2520She%2520and%2520Fangzhou%2520Zhu%2520and%2520Tao%2520Zhong%26entry.1292438233%3D%2520%2520LLMs%2520exhibit%2520advanced%2520reasoning%2520capabilities%252C%2520offering%2520the%2520potential%2520to%250Atransform%2520natural%2520language%2520questions%2520into%2520mathematical%2520models.%2520However%252C%250Aexisting%2520open-source%2520datasets%2520in%2520operations%2520research%2520domain%2520lack%2520detailed%250Aannotations%2520of%2520the%2520modeling%2520process%252C%2520such%2520as%2520variable%2520definitions%252C%2520focusing%250Asolely%2520on%2520objective%2520values%252C%2520which%2520hinders%2520reinforcement%2520learning%2520applications.%250ATo%2520address%2520this%252C%2520we%2520release%2520the%2520StructuredOR%2520dataset%252C%2520annotated%2520with%250Acomprehensive%2520labels%2520that%2520capture%2520the%2520complete%2520mathematical%2520modeling%2520process.%250AWe%2520further%2520propose%2520BPP-Search%252C%2520a%2520algorithm%2520that%2520integrates%2520reinforcement%250Alearning%2520into%2520a%2520tree-of-thought%2520structure%2520using%2520Beam%2520search%252C%2520a%2520Process%2520reward%250Amodel%252C%2520and%2520a%2520pairwise%2520Preference%2520algorithm.%2520This%2520approach%2520enables%2520efficient%250Aexploration%2520of%2520tree%2520structures%252C%2520avoiding%2520exhaustive%2520search%2520while%2520improving%250Aaccuracy.%2520Extensive%2520experiments%2520on%2520StructuredOR%252C%2520NL4OPT%252C%2520and%2520MAMO-ComplexLP%250Adatasets%2520show%2520that%2520BPP-Search%2520significantly%2520outperforms%2520state-of-the-art%250Amethods.%2520In%2520tree-based%2520reasoning%252C%2520BPP-Search%2520excels%2520in%2520accuracy%2520and%2520efficiency%252C%250Aenabling%2520faster%2520retrieval%2520of%2520correct%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BPP-Search%3A%20Enhancing%20Tree%20of%20Thought%20Reasoning%20for%20Mathematical%0A%20%20Modeling%20Problem%20Solving&entry.906535625=Teng%20Wang%20and%20Wing-Yin%20Yu%20and%20Zhenqi%20He%20and%20Zehua%20Liu%20and%20Xiongwei%20Han%20and%20Hailei%20Gong%20and%20Han%20Wu%20and%20Wei%20Shi%20and%20Ruifeng%20She%20and%20Fangzhou%20Zhu%20and%20Tao%20Zhong&entry.1292438233=%20%20LLMs%20exhibit%20advanced%20reasoning%20capabilities%2C%20offering%20the%20potential%20to%0Atransform%20natural%20language%20questions%20into%20mathematical%20models.%20However%2C%0Aexisting%20open-source%20datasets%20in%20operations%20research%20domain%20lack%20detailed%0Aannotations%20of%20the%20modeling%20process%2C%20such%20as%20variable%20definitions%2C%20focusing%0Asolely%20on%20objective%20values%2C%20which%20hinders%20reinforcement%20learning%20applications.%0ATo%20address%20this%2C%20we%20release%20the%20StructuredOR%20dataset%2C%20annotated%20with%0Acomprehensive%20labels%20that%20capture%20the%20complete%20mathematical%20modeling%20process.%0AWe%20further%20propose%20BPP-Search%2C%20a%20algorithm%20that%20integrates%20reinforcement%0Alearning%20into%20a%20tree-of-thought%20structure%20using%20Beam%20search%2C%20a%20Process%20reward%0Amodel%2C%20and%20a%20pairwise%20Preference%20algorithm.%20This%20approach%20enables%20efficient%0Aexploration%20of%20tree%20structures%2C%20avoiding%20exhaustive%20search%20while%20improving%0Aaccuracy.%20Extensive%20experiments%20on%20StructuredOR%2C%20NL4OPT%2C%20and%20MAMO-ComplexLP%0Adatasets%20show%20that%20BPP-Search%20significantly%20outperforms%20state-of-the-art%0Amethods.%20In%20tree-based%20reasoning%2C%20BPP-Search%20excels%20in%20accuracy%20and%20efficiency%2C%0Aenabling%20faster%20retrieval%20of%20correct%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17404v2&entry.124074799=Read"},
{"title": "Understanding complex crowd dynamics with generative neural simulators", "author": "Koen Minartz and Fleur Hendriks and Simon Martinus Koop and Alessandro Corbetta and Vlado Menkovski", "abstract": "  Understanding the dynamics of pedestrian crowds is an outstanding challenge\ncrucial for designing efficient urban infrastructure and ensuring safe crowd\nmanagement. To this end, both small-scale laboratory and large-scale real-world\nmeasurements have been used. However, these approaches respectively lack\nstatistical resolution and parametric controllability, both essential to\ndiscovering physical relationships underlying the complex stochastic dynamics\nof crowds. Here, we establish an investigation paradigm that offers\nlaboratory-like controllability, while ensuring the statistical resolution of\nlarge-scale real-world datasets. Using our data-driven Neural Crowd Simulator\n(NeCS), which we train on large-scale data and validate against key statistical\nfeatures of crowd dynamics, we show that we can perform effective surrogate\ncrowd dynamics experiments without training on specific scenarios. We not only\nreproduce known experimental results on pairwise avoidance, but also uncover\nthe vision-guided and topological nature of N-body interactions. These findings\nshow how virtual experiments based on neural simulation enable data-driven\nscientific discovery.\n", "link": "http://arxiv.org/abs/2412.01491v2", "date": "2024-12-03", "relevancy": 2.1078, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.531}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5299}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20complex%20crowd%20dynamics%20with%20generative%20neural%20simulators&body=Title%3A%20Understanding%20complex%20crowd%20dynamics%20with%20generative%20neural%20simulators%0AAuthor%3A%20Koen%20Minartz%20and%20Fleur%20Hendriks%20and%20Simon%20Martinus%20Koop%20and%20Alessandro%20Corbetta%20and%20Vlado%20Menkovski%0AAbstract%3A%20%20%20Understanding%20the%20dynamics%20of%20pedestrian%20crowds%20is%20an%20outstanding%20challenge%0Acrucial%20for%20designing%20efficient%20urban%20infrastructure%20and%20ensuring%20safe%20crowd%0Amanagement.%20To%20this%20end%2C%20both%20small-scale%20laboratory%20and%20large-scale%20real-world%0Ameasurements%20have%20been%20used.%20However%2C%20these%20approaches%20respectively%20lack%0Astatistical%20resolution%20and%20parametric%20controllability%2C%20both%20essential%20to%0Adiscovering%20physical%20relationships%20underlying%20the%20complex%20stochastic%20dynamics%0Aof%20crowds.%20Here%2C%20we%20establish%20an%20investigation%20paradigm%20that%20offers%0Alaboratory-like%20controllability%2C%20while%20ensuring%20the%20statistical%20resolution%20of%0Alarge-scale%20real-world%20datasets.%20Using%20our%20data-driven%20Neural%20Crowd%20Simulator%0A%28NeCS%29%2C%20which%20we%20train%20on%20large-scale%20data%20and%20validate%20against%20key%20statistical%0Afeatures%20of%20crowd%20dynamics%2C%20we%20show%20that%20we%20can%20perform%20effective%20surrogate%0Acrowd%20dynamics%20experiments%20without%20training%20on%20specific%20scenarios.%20We%20not%20only%0Areproduce%20known%20experimental%20results%20on%20pairwise%20avoidance%2C%20but%20also%20uncover%0Athe%20vision-guided%20and%20topological%20nature%20of%20N-body%20interactions.%20These%20findings%0Ashow%20how%20virtual%20experiments%20based%20on%20neural%20simulation%20enable%20data-driven%0Ascientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520complex%2520crowd%2520dynamics%2520with%2520generative%2520neural%2520simulators%26entry.906535625%3DKoen%2520Minartz%2520and%2520Fleur%2520Hendriks%2520and%2520Simon%2520Martinus%2520Koop%2520and%2520Alessandro%2520Corbetta%2520and%2520Vlado%2520Menkovski%26entry.1292438233%3D%2520%2520Understanding%2520the%2520dynamics%2520of%2520pedestrian%2520crowds%2520is%2520an%2520outstanding%2520challenge%250Acrucial%2520for%2520designing%2520efficient%2520urban%2520infrastructure%2520and%2520ensuring%2520safe%2520crowd%250Amanagement.%2520To%2520this%2520end%252C%2520both%2520small-scale%2520laboratory%2520and%2520large-scale%2520real-world%250Ameasurements%2520have%2520been%2520used.%2520However%252C%2520these%2520approaches%2520respectively%2520lack%250Astatistical%2520resolution%2520and%2520parametric%2520controllability%252C%2520both%2520essential%2520to%250Adiscovering%2520physical%2520relationships%2520underlying%2520the%2520complex%2520stochastic%2520dynamics%250Aof%2520crowds.%2520Here%252C%2520we%2520establish%2520an%2520investigation%2520paradigm%2520that%2520offers%250Alaboratory-like%2520controllability%252C%2520while%2520ensuring%2520the%2520statistical%2520resolution%2520of%250Alarge-scale%2520real-world%2520datasets.%2520Using%2520our%2520data-driven%2520Neural%2520Crowd%2520Simulator%250A%2528NeCS%2529%252C%2520which%2520we%2520train%2520on%2520large-scale%2520data%2520and%2520validate%2520against%2520key%2520statistical%250Afeatures%2520of%2520crowd%2520dynamics%252C%2520we%2520show%2520that%2520we%2520can%2520perform%2520effective%2520surrogate%250Acrowd%2520dynamics%2520experiments%2520without%2520training%2520on%2520specific%2520scenarios.%2520We%2520not%2520only%250Areproduce%2520known%2520experimental%2520results%2520on%2520pairwise%2520avoidance%252C%2520but%2520also%2520uncover%250Athe%2520vision-guided%2520and%2520topological%2520nature%2520of%2520N-body%2520interactions.%2520These%2520findings%250Ashow%2520how%2520virtual%2520experiments%2520based%2520on%2520neural%2520simulation%2520enable%2520data-driven%250Ascientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20complex%20crowd%20dynamics%20with%20generative%20neural%20simulators&entry.906535625=Koen%20Minartz%20and%20Fleur%20Hendriks%20and%20Simon%20Martinus%20Koop%20and%20Alessandro%20Corbetta%20and%20Vlado%20Menkovski&entry.1292438233=%20%20Understanding%20the%20dynamics%20of%20pedestrian%20crowds%20is%20an%20outstanding%20challenge%0Acrucial%20for%20designing%20efficient%20urban%20infrastructure%20and%20ensuring%20safe%20crowd%0Amanagement.%20To%20this%20end%2C%20both%20small-scale%20laboratory%20and%20large-scale%20real-world%0Ameasurements%20have%20been%20used.%20However%2C%20these%20approaches%20respectively%20lack%0Astatistical%20resolution%20and%20parametric%20controllability%2C%20both%20essential%20to%0Adiscovering%20physical%20relationships%20underlying%20the%20complex%20stochastic%20dynamics%0Aof%20crowds.%20Here%2C%20we%20establish%20an%20investigation%20paradigm%20that%20offers%0Alaboratory-like%20controllability%2C%20while%20ensuring%20the%20statistical%20resolution%20of%0Alarge-scale%20real-world%20datasets.%20Using%20our%20data-driven%20Neural%20Crowd%20Simulator%0A%28NeCS%29%2C%20which%20we%20train%20on%20large-scale%20data%20and%20validate%20against%20key%20statistical%0Afeatures%20of%20crowd%20dynamics%2C%20we%20show%20that%20we%20can%20perform%20effective%20surrogate%0Acrowd%20dynamics%20experiments%20without%20training%20on%20specific%20scenarios.%20We%20not%20only%0Areproduce%20known%20experimental%20results%20on%20pairwise%20avoidance%2C%20but%20also%20uncover%0Athe%20vision-guided%20and%20topological%20nature%20of%20N-body%20interactions.%20These%20findings%0Ashow%20how%20virtual%20experiments%20based%20on%20neural%20simulation%20enable%20data-driven%0Ascientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01491v2&entry.124074799=Read"},
{"title": "Understanding complex crowd dynamics with generative neural simulators", "author": "Koen Minartz and Fleur Hendriks and Simon Martinus Koop and Alessandro Corbetta and Vlado Menkovski", "abstract": "  Understanding the dynamics of pedestrian crowds is an outstanding challenge\ncrucial for designing efficient urban infrastructure and ensuring safe crowd\nmanagement. To this end, both small-scale laboratory and large-scale real-world\nmeasurements have been used. However, these approaches respectively lack\nstatistical resolution and parametric controllability, both essential to\ndiscovering physical relationships underlying the complex stochastic dynamics\nof crowds. Here, we establish an investigation paradigm that offers\nlaboratory-like controllability, while ensuring the statistical resolution of\nlarge-scale real-world datasets. Using our data-driven Neural Crowd Simulator\n(NeCS), which we train on large-scale data and validate against key statistical\nfeatures of crowd dynamics, we show that we can perform effective surrogate\ncrowd dynamics experiments without training on specific scenarios. We not only\nreproduce known experimental results on pairwise avoidance, but also uncover\nthe vision-guided and topological nature of N-body interactions. These findings\nshow how virtual experiments based on neural simulation enable data-driven\nscientific discovery.\n", "link": "http://arxiv.org/abs/2412.01491v2", "date": "2024-12-03", "relevancy": 2.1078, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.531}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5299}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20complex%20crowd%20dynamics%20with%20generative%20neural%20simulators&body=Title%3A%20Understanding%20complex%20crowd%20dynamics%20with%20generative%20neural%20simulators%0AAuthor%3A%20Koen%20Minartz%20and%20Fleur%20Hendriks%20and%20Simon%20Martinus%20Koop%20and%20Alessandro%20Corbetta%20and%20Vlado%20Menkovski%0AAbstract%3A%20%20%20Understanding%20the%20dynamics%20of%20pedestrian%20crowds%20is%20an%20outstanding%20challenge%0Acrucial%20for%20designing%20efficient%20urban%20infrastructure%20and%20ensuring%20safe%20crowd%0Amanagement.%20To%20this%20end%2C%20both%20small-scale%20laboratory%20and%20large-scale%20real-world%0Ameasurements%20have%20been%20used.%20However%2C%20these%20approaches%20respectively%20lack%0Astatistical%20resolution%20and%20parametric%20controllability%2C%20both%20essential%20to%0Adiscovering%20physical%20relationships%20underlying%20the%20complex%20stochastic%20dynamics%0Aof%20crowds.%20Here%2C%20we%20establish%20an%20investigation%20paradigm%20that%20offers%0Alaboratory-like%20controllability%2C%20while%20ensuring%20the%20statistical%20resolution%20of%0Alarge-scale%20real-world%20datasets.%20Using%20our%20data-driven%20Neural%20Crowd%20Simulator%0A%28NeCS%29%2C%20which%20we%20train%20on%20large-scale%20data%20and%20validate%20against%20key%20statistical%0Afeatures%20of%20crowd%20dynamics%2C%20we%20show%20that%20we%20can%20perform%20effective%20surrogate%0Acrowd%20dynamics%20experiments%20without%20training%20on%20specific%20scenarios.%20We%20not%20only%0Areproduce%20known%20experimental%20results%20on%20pairwise%20avoidance%2C%20but%20also%20uncover%0Athe%20vision-guided%20and%20topological%20nature%20of%20N-body%20interactions.%20These%20findings%0Ashow%20how%20virtual%20experiments%20based%20on%20neural%20simulation%20enable%20data-driven%0Ascientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520complex%2520crowd%2520dynamics%2520with%2520generative%2520neural%2520simulators%26entry.906535625%3DKoen%2520Minartz%2520and%2520Fleur%2520Hendriks%2520and%2520Simon%2520Martinus%2520Koop%2520and%2520Alessandro%2520Corbetta%2520and%2520Vlado%2520Menkovski%26entry.1292438233%3D%2520%2520Understanding%2520the%2520dynamics%2520of%2520pedestrian%2520crowds%2520is%2520an%2520outstanding%2520challenge%250Acrucial%2520for%2520designing%2520efficient%2520urban%2520infrastructure%2520and%2520ensuring%2520safe%2520crowd%250Amanagement.%2520To%2520this%2520end%252C%2520both%2520small-scale%2520laboratory%2520and%2520large-scale%2520real-world%250Ameasurements%2520have%2520been%2520used.%2520However%252C%2520these%2520approaches%2520respectively%2520lack%250Astatistical%2520resolution%2520and%2520parametric%2520controllability%252C%2520both%2520essential%2520to%250Adiscovering%2520physical%2520relationships%2520underlying%2520the%2520complex%2520stochastic%2520dynamics%250Aof%2520crowds.%2520Here%252C%2520we%2520establish%2520an%2520investigation%2520paradigm%2520that%2520offers%250Alaboratory-like%2520controllability%252C%2520while%2520ensuring%2520the%2520statistical%2520resolution%2520of%250Alarge-scale%2520real-world%2520datasets.%2520Using%2520our%2520data-driven%2520Neural%2520Crowd%2520Simulator%250A%2528NeCS%2529%252C%2520which%2520we%2520train%2520on%2520large-scale%2520data%2520and%2520validate%2520against%2520key%2520statistical%250Afeatures%2520of%2520crowd%2520dynamics%252C%2520we%2520show%2520that%2520we%2520can%2520perform%2520effective%2520surrogate%250Acrowd%2520dynamics%2520experiments%2520without%2520training%2520on%2520specific%2520scenarios.%2520We%2520not%2520only%250Areproduce%2520known%2520experimental%2520results%2520on%2520pairwise%2520avoidance%252C%2520but%2520also%2520uncover%250Athe%2520vision-guided%2520and%2520topological%2520nature%2520of%2520N-body%2520interactions.%2520These%2520findings%250Ashow%2520how%2520virtual%2520experiments%2520based%2520on%2520neural%2520simulation%2520enable%2520data-driven%250Ascientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20complex%20crowd%20dynamics%20with%20generative%20neural%20simulators&entry.906535625=Koen%20Minartz%20and%20Fleur%20Hendriks%20and%20Simon%20Martinus%20Koop%20and%20Alessandro%20Corbetta%20and%20Vlado%20Menkovski&entry.1292438233=%20%20Understanding%20the%20dynamics%20of%20pedestrian%20crowds%20is%20an%20outstanding%20challenge%0Acrucial%20for%20designing%20efficient%20urban%20infrastructure%20and%20ensuring%20safe%20crowd%0Amanagement.%20To%20this%20end%2C%20both%20small-scale%20laboratory%20and%20large-scale%20real-world%0Ameasurements%20have%20been%20used.%20However%2C%20these%20approaches%20respectively%20lack%0Astatistical%20resolution%20and%20parametric%20controllability%2C%20both%20essential%20to%0Adiscovering%20physical%20relationships%20underlying%20the%20complex%20stochastic%20dynamics%0Aof%20crowds.%20Here%2C%20we%20establish%20an%20investigation%20paradigm%20that%20offers%0Alaboratory-like%20controllability%2C%20while%20ensuring%20the%20statistical%20resolution%20of%0Alarge-scale%20real-world%20datasets.%20Using%20our%20data-driven%20Neural%20Crowd%20Simulator%0A%28NeCS%29%2C%20which%20we%20train%20on%20large-scale%20data%20and%20validate%20against%20key%20statistical%0Afeatures%20of%20crowd%20dynamics%2C%20we%20show%20that%20we%20can%20perform%20effective%20surrogate%0Acrowd%20dynamics%20experiments%20without%20training%20on%20specific%20scenarios.%20We%20not%20only%0Areproduce%20known%20experimental%20results%20on%20pairwise%20avoidance%2C%20but%20also%20uncover%0Athe%20vision-guided%20and%20topological%20nature%20of%20N-body%20interactions.%20These%20findings%0Ashow%20how%20virtual%20experiments%20based%20on%20neural%20simulation%20enable%20data-driven%0Ascientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01491v2&entry.124074799=Read"},
{"title": "An Adaptive Grasping Force Tracking Strategy for Nonlinear and\n  Time-Varying Object Behaviors", "author": "Ziyang Cheng and Xiangyu Tian and Ruomin Sui and Tiemin Li and Yao Jiang", "abstract": "  Accurate grasp force control is one of the key skills for ensuring successful\nand damage-free robotic grasping of objects. Although existing methods have\nconducted in-depth research on slip detection and grasping force planning, they\noften overlook the issue of adaptive tracking of the actual force to the target\nforce when handling objects with different material properties. The optimal\nparameters of a force tracking controller are significantly influenced by the\nobject's stiffness, and many adaptive force tracking algorithms rely on\nstiffness estimation. However, real-world objects often exhibit viscous,\nplastic, or other more complex nonlinear time-varying behaviors, and existing\nstudies provide insufficient support for these materials in terms of stiffness\ndefinition and estimation. To address this, this paper introduces the concept\nof generalized stiffness, extending the definition of stiffness to nonlinear\ntime-varying grasp system models, and proposes an online generalized stiffness\nestimator based on Long Short-Term Memory (LSTM) networks. Based on generalized\nstiffness, this paper proposes an adaptive parameter adjustment strategy using\na PI controller as an example, enabling dynamic force tracking for objects with\nvarying characteristics. Experimental results demonstrate that the proposed\nmethod achieves high precision and short probing time, while showing better\nadaptability to non-ideal objects compared to existing methods. The method\neffectively solves the problem of grasp force tracking in unknown, nonlinear,\nand time-varying grasp systems, enhancing the robotic grasping ability in\nunstructured environments.\n", "link": "http://arxiv.org/abs/2412.02335v1", "date": "2024-12-03", "relevancy": 2.0927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.572}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5368}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Grasping%20Force%20Tracking%20Strategy%20for%20Nonlinear%20and%0A%20%20Time-Varying%20Object%20Behaviors&body=Title%3A%20An%20Adaptive%20Grasping%20Force%20Tracking%20Strategy%20for%20Nonlinear%20and%0A%20%20Time-Varying%20Object%20Behaviors%0AAuthor%3A%20Ziyang%20Cheng%20and%20Xiangyu%20Tian%20and%20Ruomin%20Sui%20and%20Tiemin%20Li%20and%20Yao%20Jiang%0AAbstract%3A%20%20%20Accurate%20grasp%20force%20control%20is%20one%20of%20the%20key%20skills%20for%20ensuring%20successful%0Aand%20damage-free%20robotic%20grasping%20of%20objects.%20Although%20existing%20methods%20have%0Aconducted%20in-depth%20research%20on%20slip%20detection%20and%20grasping%20force%20planning%2C%20they%0Aoften%20overlook%20the%20issue%20of%20adaptive%20tracking%20of%20the%20actual%20force%20to%20the%20target%0Aforce%20when%20handling%20objects%20with%20different%20material%20properties.%20The%20optimal%0Aparameters%20of%20a%20force%20tracking%20controller%20are%20significantly%20influenced%20by%20the%0Aobject%27s%20stiffness%2C%20and%20many%20adaptive%20force%20tracking%20algorithms%20rely%20on%0Astiffness%20estimation.%20However%2C%20real-world%20objects%20often%20exhibit%20viscous%2C%0Aplastic%2C%20or%20other%20more%20complex%20nonlinear%20time-varying%20behaviors%2C%20and%20existing%0Astudies%20provide%20insufficient%20support%20for%20these%20materials%20in%20terms%20of%20stiffness%0Adefinition%20and%20estimation.%20To%20address%20this%2C%20this%20paper%20introduces%20the%20concept%0Aof%20generalized%20stiffness%2C%20extending%20the%20definition%20of%20stiffness%20to%20nonlinear%0Atime-varying%20grasp%20system%20models%2C%20and%20proposes%20an%20online%20generalized%20stiffness%0Aestimator%20based%20on%20Long%20Short-Term%20Memory%20%28LSTM%29%20networks.%20Based%20on%20generalized%0Astiffness%2C%20this%20paper%20proposes%20an%20adaptive%20parameter%20adjustment%20strategy%20using%0Aa%20PI%20controller%20as%20an%20example%2C%20enabling%20dynamic%20force%20tracking%20for%20objects%20with%0Avarying%20characteristics.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0Amethod%20achieves%20high%20precision%20and%20short%20probing%20time%2C%20while%20showing%20better%0Aadaptability%20to%20non-ideal%20objects%20compared%20to%20existing%20methods.%20The%20method%0Aeffectively%20solves%20the%20problem%20of%20grasp%20force%20tracking%20in%20unknown%2C%20nonlinear%2C%0Aand%20time-varying%20grasp%20systems%2C%20enhancing%20the%20robotic%20grasping%20ability%20in%0Aunstructured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Grasping%2520Force%2520Tracking%2520Strategy%2520for%2520Nonlinear%2520and%250A%2520%2520Time-Varying%2520Object%2520Behaviors%26entry.906535625%3DZiyang%2520Cheng%2520and%2520Xiangyu%2520Tian%2520and%2520Ruomin%2520Sui%2520and%2520Tiemin%2520Li%2520and%2520Yao%2520Jiang%26entry.1292438233%3D%2520%2520Accurate%2520grasp%2520force%2520control%2520is%2520one%2520of%2520the%2520key%2520skills%2520for%2520ensuring%2520successful%250Aand%2520damage-free%2520robotic%2520grasping%2520of%2520objects.%2520Although%2520existing%2520methods%2520have%250Aconducted%2520in-depth%2520research%2520on%2520slip%2520detection%2520and%2520grasping%2520force%2520planning%252C%2520they%250Aoften%2520overlook%2520the%2520issue%2520of%2520adaptive%2520tracking%2520of%2520the%2520actual%2520force%2520to%2520the%2520target%250Aforce%2520when%2520handling%2520objects%2520with%2520different%2520material%2520properties.%2520The%2520optimal%250Aparameters%2520of%2520a%2520force%2520tracking%2520controller%2520are%2520significantly%2520influenced%2520by%2520the%250Aobject%2527s%2520stiffness%252C%2520and%2520many%2520adaptive%2520force%2520tracking%2520algorithms%2520rely%2520on%250Astiffness%2520estimation.%2520However%252C%2520real-world%2520objects%2520often%2520exhibit%2520viscous%252C%250Aplastic%252C%2520or%2520other%2520more%2520complex%2520nonlinear%2520time-varying%2520behaviors%252C%2520and%2520existing%250Astudies%2520provide%2520insufficient%2520support%2520for%2520these%2520materials%2520in%2520terms%2520of%2520stiffness%250Adefinition%2520and%2520estimation.%2520To%2520address%2520this%252C%2520this%2520paper%2520introduces%2520the%2520concept%250Aof%2520generalized%2520stiffness%252C%2520extending%2520the%2520definition%2520of%2520stiffness%2520to%2520nonlinear%250Atime-varying%2520grasp%2520system%2520models%252C%2520and%2520proposes%2520an%2520online%2520generalized%2520stiffness%250Aestimator%2520based%2520on%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520networks.%2520Based%2520on%2520generalized%250Astiffness%252C%2520this%2520paper%2520proposes%2520an%2520adaptive%2520parameter%2520adjustment%2520strategy%2520using%250Aa%2520PI%2520controller%2520as%2520an%2520example%252C%2520enabling%2520dynamic%2520force%2520tracking%2520for%2520objects%2520with%250Avarying%2520characteristics.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250Amethod%2520achieves%2520high%2520precision%2520and%2520short%2520probing%2520time%252C%2520while%2520showing%2520better%250Aadaptability%2520to%2520non-ideal%2520objects%2520compared%2520to%2520existing%2520methods.%2520The%2520method%250Aeffectively%2520solves%2520the%2520problem%2520of%2520grasp%2520force%2520tracking%2520in%2520unknown%252C%2520nonlinear%252C%250Aand%2520time-varying%2520grasp%2520systems%252C%2520enhancing%2520the%2520robotic%2520grasping%2520ability%2520in%250Aunstructured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Grasping%20Force%20Tracking%20Strategy%20for%20Nonlinear%20and%0A%20%20Time-Varying%20Object%20Behaviors&entry.906535625=Ziyang%20Cheng%20and%20Xiangyu%20Tian%20and%20Ruomin%20Sui%20and%20Tiemin%20Li%20and%20Yao%20Jiang&entry.1292438233=%20%20Accurate%20grasp%20force%20control%20is%20one%20of%20the%20key%20skills%20for%20ensuring%20successful%0Aand%20damage-free%20robotic%20grasping%20of%20objects.%20Although%20existing%20methods%20have%0Aconducted%20in-depth%20research%20on%20slip%20detection%20and%20grasping%20force%20planning%2C%20they%0Aoften%20overlook%20the%20issue%20of%20adaptive%20tracking%20of%20the%20actual%20force%20to%20the%20target%0Aforce%20when%20handling%20objects%20with%20different%20material%20properties.%20The%20optimal%0Aparameters%20of%20a%20force%20tracking%20controller%20are%20significantly%20influenced%20by%20the%0Aobject%27s%20stiffness%2C%20and%20many%20adaptive%20force%20tracking%20algorithms%20rely%20on%0Astiffness%20estimation.%20However%2C%20real-world%20objects%20often%20exhibit%20viscous%2C%0Aplastic%2C%20or%20other%20more%20complex%20nonlinear%20time-varying%20behaviors%2C%20and%20existing%0Astudies%20provide%20insufficient%20support%20for%20these%20materials%20in%20terms%20of%20stiffness%0Adefinition%20and%20estimation.%20To%20address%20this%2C%20this%20paper%20introduces%20the%20concept%0Aof%20generalized%20stiffness%2C%20extending%20the%20definition%20of%20stiffness%20to%20nonlinear%0Atime-varying%20grasp%20system%20models%2C%20and%20proposes%20an%20online%20generalized%20stiffness%0Aestimator%20based%20on%20Long%20Short-Term%20Memory%20%28LSTM%29%20networks.%20Based%20on%20generalized%0Astiffness%2C%20this%20paper%20proposes%20an%20adaptive%20parameter%20adjustment%20strategy%20using%0Aa%20PI%20controller%20as%20an%20example%2C%20enabling%20dynamic%20force%20tracking%20for%20objects%20with%0Avarying%20characteristics.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0Amethod%20achieves%20high%20precision%20and%20short%20probing%20time%2C%20while%20showing%20better%0Aadaptability%20to%20non-ideal%20objects%20compared%20to%20existing%20methods.%20The%20method%0Aeffectively%20solves%20the%20problem%20of%20grasp%20force%20tracking%20in%20unknown%2C%20nonlinear%2C%0Aand%20time-varying%20grasp%20systems%2C%20enhancing%20the%20robotic%20grasping%20ability%20in%0Aunstructured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02335v1&entry.124074799=Read"},
{"title": "Federated Analytics in Practice: Engineering for Privacy, Scalability\n  and Practicality", "author": "Harish Srinivas and Graham Cormode and Mehrdad Honarkhah and Samuel Lurye and Jonathan Hehir and Lunwen He and George Hong and Ahmed Magdy and Dzmitry Huba and Kaikai Wang and Shen Guo and Shoubhik Bhattacharya", "abstract": "  Cross-device Federated Analytics (FA) is a distributed computation paradigm\ndesigned to answer analytics queries about and derive insights from data held\nlocally on users' devices. On-device computations combined with other privacy\nand security measures ensure that only minimal data is transmitted off-device,\nachieving a high standard of data protection. Despite FA's broad relevance, the\napplicability of existing FA systems is limited by compromised accuracy; lack\nof flexibility for data analytics; and an inability to scale effectively. In\nthis paper, we describe our approach to combine privacy, scalability, and\npracticality to build and deploy a system that overcomes these limitations. Our\nFA system leverages trusted execution environments (TEEs) and optimizes the use\nof on-device computing resources to facilitate federated data processing across\nlarge fleets of devices, while ensuring robust, defensible, and verifiable\nprivacy safeguards. We focus on federated analytics (statistics and\nmonitoring), in contrast to systems for federated learning (ML workloads), and\nwe flag the key differences.\n", "link": "http://arxiv.org/abs/2412.02340v1", "date": "2024-12-03", "relevancy": 2.0738, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4208}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4206}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Analytics%20in%20Practice%3A%20Engineering%20for%20Privacy%2C%20Scalability%0A%20%20and%20Practicality&body=Title%3A%20Federated%20Analytics%20in%20Practice%3A%20Engineering%20for%20Privacy%2C%20Scalability%0A%20%20and%20Practicality%0AAuthor%3A%20Harish%20Srinivas%20and%20Graham%20Cormode%20and%20Mehrdad%20Honarkhah%20and%20Samuel%20Lurye%20and%20Jonathan%20Hehir%20and%20Lunwen%20He%20and%20George%20Hong%20and%20Ahmed%20Magdy%20and%20Dzmitry%20Huba%20and%20Kaikai%20Wang%20and%20Shen%20Guo%20and%20Shoubhik%20Bhattacharya%0AAbstract%3A%20%20%20Cross-device%20Federated%20Analytics%20%28FA%29%20is%20a%20distributed%20computation%20paradigm%0Adesigned%20to%20answer%20analytics%20queries%20about%20and%20derive%20insights%20from%20data%20held%0Alocally%20on%20users%27%20devices.%20On-device%20computations%20combined%20with%20other%20privacy%0Aand%20security%20measures%20ensure%20that%20only%20minimal%20data%20is%20transmitted%20off-device%2C%0Aachieving%20a%20high%20standard%20of%20data%20protection.%20Despite%20FA%27s%20broad%20relevance%2C%20the%0Aapplicability%20of%20existing%20FA%20systems%20is%20limited%20by%20compromised%20accuracy%3B%20lack%0Aof%20flexibility%20for%20data%20analytics%3B%20and%20an%20inability%20to%20scale%20effectively.%20In%0Athis%20paper%2C%20we%20describe%20our%20approach%20to%20combine%20privacy%2C%20scalability%2C%20and%0Apracticality%20to%20build%20and%20deploy%20a%20system%20that%20overcomes%20these%20limitations.%20Our%0AFA%20system%20leverages%20trusted%20execution%20environments%20%28TEEs%29%20and%20optimizes%20the%20use%0Aof%20on-device%20computing%20resources%20to%20facilitate%20federated%20data%20processing%20across%0Alarge%20fleets%20of%20devices%2C%20while%20ensuring%20robust%2C%20defensible%2C%20and%20verifiable%0Aprivacy%20safeguards.%20We%20focus%20on%20federated%20analytics%20%28statistics%20and%0Amonitoring%29%2C%20in%20contrast%20to%20systems%20for%20federated%20learning%20%28ML%20workloads%29%2C%20and%0Awe%20flag%20the%20key%20differences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Analytics%2520in%2520Practice%253A%2520Engineering%2520for%2520Privacy%252C%2520Scalability%250A%2520%2520and%2520Practicality%26entry.906535625%3DHarish%2520Srinivas%2520and%2520Graham%2520Cormode%2520and%2520Mehrdad%2520Honarkhah%2520and%2520Samuel%2520Lurye%2520and%2520Jonathan%2520Hehir%2520and%2520Lunwen%2520He%2520and%2520George%2520Hong%2520and%2520Ahmed%2520Magdy%2520and%2520Dzmitry%2520Huba%2520and%2520Kaikai%2520Wang%2520and%2520Shen%2520Guo%2520and%2520Shoubhik%2520Bhattacharya%26entry.1292438233%3D%2520%2520Cross-device%2520Federated%2520Analytics%2520%2528FA%2529%2520is%2520a%2520distributed%2520computation%2520paradigm%250Adesigned%2520to%2520answer%2520analytics%2520queries%2520about%2520and%2520derive%2520insights%2520from%2520data%2520held%250Alocally%2520on%2520users%2527%2520devices.%2520On-device%2520computations%2520combined%2520with%2520other%2520privacy%250Aand%2520security%2520measures%2520ensure%2520that%2520only%2520minimal%2520data%2520is%2520transmitted%2520off-device%252C%250Aachieving%2520a%2520high%2520standard%2520of%2520data%2520protection.%2520Despite%2520FA%2527s%2520broad%2520relevance%252C%2520the%250Aapplicability%2520of%2520existing%2520FA%2520systems%2520is%2520limited%2520by%2520compromised%2520accuracy%253B%2520lack%250Aof%2520flexibility%2520for%2520data%2520analytics%253B%2520and%2520an%2520inability%2520to%2520scale%2520effectively.%2520In%250Athis%2520paper%252C%2520we%2520describe%2520our%2520approach%2520to%2520combine%2520privacy%252C%2520scalability%252C%2520and%250Apracticality%2520to%2520build%2520and%2520deploy%2520a%2520system%2520that%2520overcomes%2520these%2520limitations.%2520Our%250AFA%2520system%2520leverages%2520trusted%2520execution%2520environments%2520%2528TEEs%2529%2520and%2520optimizes%2520the%2520use%250Aof%2520on-device%2520computing%2520resources%2520to%2520facilitate%2520federated%2520data%2520processing%2520across%250Alarge%2520fleets%2520of%2520devices%252C%2520while%2520ensuring%2520robust%252C%2520defensible%252C%2520and%2520verifiable%250Aprivacy%2520safeguards.%2520We%2520focus%2520on%2520federated%2520analytics%2520%2528statistics%2520and%250Amonitoring%2529%252C%2520in%2520contrast%2520to%2520systems%2520for%2520federated%2520learning%2520%2528ML%2520workloads%2529%252C%2520and%250Awe%2520flag%2520the%2520key%2520differences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Analytics%20in%20Practice%3A%20Engineering%20for%20Privacy%2C%20Scalability%0A%20%20and%20Practicality&entry.906535625=Harish%20Srinivas%20and%20Graham%20Cormode%20and%20Mehrdad%20Honarkhah%20and%20Samuel%20Lurye%20and%20Jonathan%20Hehir%20and%20Lunwen%20He%20and%20George%20Hong%20and%20Ahmed%20Magdy%20and%20Dzmitry%20Huba%20and%20Kaikai%20Wang%20and%20Shen%20Guo%20and%20Shoubhik%20Bhattacharya&entry.1292438233=%20%20Cross-device%20Federated%20Analytics%20%28FA%29%20is%20a%20distributed%20computation%20paradigm%0Adesigned%20to%20answer%20analytics%20queries%20about%20and%20derive%20insights%20from%20data%20held%0Alocally%20on%20users%27%20devices.%20On-device%20computations%20combined%20with%20other%20privacy%0Aand%20security%20measures%20ensure%20that%20only%20minimal%20data%20is%20transmitted%20off-device%2C%0Aachieving%20a%20high%20standard%20of%20data%20protection.%20Despite%20FA%27s%20broad%20relevance%2C%20the%0Aapplicability%20of%20existing%20FA%20systems%20is%20limited%20by%20compromised%20accuracy%3B%20lack%0Aof%20flexibility%20for%20data%20analytics%3B%20and%20an%20inability%20to%20scale%20effectively.%20In%0Athis%20paper%2C%20we%20describe%20our%20approach%20to%20combine%20privacy%2C%20scalability%2C%20and%0Apracticality%20to%20build%20and%20deploy%20a%20system%20that%20overcomes%20these%20limitations.%20Our%0AFA%20system%20leverages%20trusted%20execution%20environments%20%28TEEs%29%20and%20optimizes%20the%20use%0Aof%20on-device%20computing%20resources%20to%20facilitate%20federated%20data%20processing%20across%0Alarge%20fleets%20of%20devices%2C%20while%20ensuring%20robust%2C%20defensible%2C%20and%20verifiable%0Aprivacy%20safeguards.%20We%20focus%20on%20federated%20analytics%20%28statistics%20and%0Amonitoring%29%2C%20in%20contrast%20to%20systems%20for%20federated%20learning%20%28ML%20workloads%29%2C%20and%0Awe%20flag%20the%20key%20differences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02340v1&entry.124074799=Read"},
{"title": "Federated Analytics in Practice: Engineering for Privacy, Scalability\n  and Practicality", "author": "Harish Srinivas and Graham Cormode and Mehrdad Honarkhah and Samuel Lurye and Jonathan Hehir and Lunwen He and George Hong and Ahmed Magdy and Dzmitry Huba and Kaikai Wang and Shen Guo and Shoubhik Bhattacharya", "abstract": "  Cross-device Federated Analytics (FA) is a distributed computation paradigm\ndesigned to answer analytics queries about and derive insights from data held\nlocally on users' devices. On-device computations combined with other privacy\nand security measures ensure that only minimal data is transmitted off-device,\nachieving a high standard of data protection. Despite FA's broad relevance, the\napplicability of existing FA systems is limited by compromised accuracy; lack\nof flexibility for data analytics; and an inability to scale effectively. In\nthis paper, we describe our approach to combine privacy, scalability, and\npracticality to build and deploy a system that overcomes these limitations. Our\nFA system leverages trusted execution environments (TEEs) and optimizes the use\nof on-device computing resources to facilitate federated data processing across\nlarge fleets of devices, while ensuring robust, defensible, and verifiable\nprivacy safeguards. We focus on federated analytics (statistics and\nmonitoring), in contrast to systems for federated learning (ML workloads), and\nwe flag the key differences.\n", "link": "http://arxiv.org/abs/2412.02340v1", "date": "2024-12-03", "relevancy": 2.0738, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4208}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4206}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Analytics%20in%20Practice%3A%20Engineering%20for%20Privacy%2C%20Scalability%0A%20%20and%20Practicality&body=Title%3A%20Federated%20Analytics%20in%20Practice%3A%20Engineering%20for%20Privacy%2C%20Scalability%0A%20%20and%20Practicality%0AAuthor%3A%20Harish%20Srinivas%20and%20Graham%20Cormode%20and%20Mehrdad%20Honarkhah%20and%20Samuel%20Lurye%20and%20Jonathan%20Hehir%20and%20Lunwen%20He%20and%20George%20Hong%20and%20Ahmed%20Magdy%20and%20Dzmitry%20Huba%20and%20Kaikai%20Wang%20and%20Shen%20Guo%20and%20Shoubhik%20Bhattacharya%0AAbstract%3A%20%20%20Cross-device%20Federated%20Analytics%20%28FA%29%20is%20a%20distributed%20computation%20paradigm%0Adesigned%20to%20answer%20analytics%20queries%20about%20and%20derive%20insights%20from%20data%20held%0Alocally%20on%20users%27%20devices.%20On-device%20computations%20combined%20with%20other%20privacy%0Aand%20security%20measures%20ensure%20that%20only%20minimal%20data%20is%20transmitted%20off-device%2C%0Aachieving%20a%20high%20standard%20of%20data%20protection.%20Despite%20FA%27s%20broad%20relevance%2C%20the%0Aapplicability%20of%20existing%20FA%20systems%20is%20limited%20by%20compromised%20accuracy%3B%20lack%0Aof%20flexibility%20for%20data%20analytics%3B%20and%20an%20inability%20to%20scale%20effectively.%20In%0Athis%20paper%2C%20we%20describe%20our%20approach%20to%20combine%20privacy%2C%20scalability%2C%20and%0Apracticality%20to%20build%20and%20deploy%20a%20system%20that%20overcomes%20these%20limitations.%20Our%0AFA%20system%20leverages%20trusted%20execution%20environments%20%28TEEs%29%20and%20optimizes%20the%20use%0Aof%20on-device%20computing%20resources%20to%20facilitate%20federated%20data%20processing%20across%0Alarge%20fleets%20of%20devices%2C%20while%20ensuring%20robust%2C%20defensible%2C%20and%20verifiable%0Aprivacy%20safeguards.%20We%20focus%20on%20federated%20analytics%20%28statistics%20and%0Amonitoring%29%2C%20in%20contrast%20to%20systems%20for%20federated%20learning%20%28ML%20workloads%29%2C%20and%0Awe%20flag%20the%20key%20differences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Analytics%2520in%2520Practice%253A%2520Engineering%2520for%2520Privacy%252C%2520Scalability%250A%2520%2520and%2520Practicality%26entry.906535625%3DHarish%2520Srinivas%2520and%2520Graham%2520Cormode%2520and%2520Mehrdad%2520Honarkhah%2520and%2520Samuel%2520Lurye%2520and%2520Jonathan%2520Hehir%2520and%2520Lunwen%2520He%2520and%2520George%2520Hong%2520and%2520Ahmed%2520Magdy%2520and%2520Dzmitry%2520Huba%2520and%2520Kaikai%2520Wang%2520and%2520Shen%2520Guo%2520and%2520Shoubhik%2520Bhattacharya%26entry.1292438233%3D%2520%2520Cross-device%2520Federated%2520Analytics%2520%2528FA%2529%2520is%2520a%2520distributed%2520computation%2520paradigm%250Adesigned%2520to%2520answer%2520analytics%2520queries%2520about%2520and%2520derive%2520insights%2520from%2520data%2520held%250Alocally%2520on%2520users%2527%2520devices.%2520On-device%2520computations%2520combined%2520with%2520other%2520privacy%250Aand%2520security%2520measures%2520ensure%2520that%2520only%2520minimal%2520data%2520is%2520transmitted%2520off-device%252C%250Aachieving%2520a%2520high%2520standard%2520of%2520data%2520protection.%2520Despite%2520FA%2527s%2520broad%2520relevance%252C%2520the%250Aapplicability%2520of%2520existing%2520FA%2520systems%2520is%2520limited%2520by%2520compromised%2520accuracy%253B%2520lack%250Aof%2520flexibility%2520for%2520data%2520analytics%253B%2520and%2520an%2520inability%2520to%2520scale%2520effectively.%2520In%250Athis%2520paper%252C%2520we%2520describe%2520our%2520approach%2520to%2520combine%2520privacy%252C%2520scalability%252C%2520and%250Apracticality%2520to%2520build%2520and%2520deploy%2520a%2520system%2520that%2520overcomes%2520these%2520limitations.%2520Our%250AFA%2520system%2520leverages%2520trusted%2520execution%2520environments%2520%2528TEEs%2529%2520and%2520optimizes%2520the%2520use%250Aof%2520on-device%2520computing%2520resources%2520to%2520facilitate%2520federated%2520data%2520processing%2520across%250Alarge%2520fleets%2520of%2520devices%252C%2520while%2520ensuring%2520robust%252C%2520defensible%252C%2520and%2520verifiable%250Aprivacy%2520safeguards.%2520We%2520focus%2520on%2520federated%2520analytics%2520%2528statistics%2520and%250Amonitoring%2529%252C%2520in%2520contrast%2520to%2520systems%2520for%2520federated%2520learning%2520%2528ML%2520workloads%2529%252C%2520and%250Awe%2520flag%2520the%2520key%2520differences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Analytics%20in%20Practice%3A%20Engineering%20for%20Privacy%2C%20Scalability%0A%20%20and%20Practicality&entry.906535625=Harish%20Srinivas%20and%20Graham%20Cormode%20and%20Mehrdad%20Honarkhah%20and%20Samuel%20Lurye%20and%20Jonathan%20Hehir%20and%20Lunwen%20He%20and%20George%20Hong%20and%20Ahmed%20Magdy%20and%20Dzmitry%20Huba%20and%20Kaikai%20Wang%20and%20Shen%20Guo%20and%20Shoubhik%20Bhattacharya&entry.1292438233=%20%20Cross-device%20Federated%20Analytics%20%28FA%29%20is%20a%20distributed%20computation%20paradigm%0Adesigned%20to%20answer%20analytics%20queries%20about%20and%20derive%20insights%20from%20data%20held%0Alocally%20on%20users%27%20devices.%20On-device%20computations%20combined%20with%20other%20privacy%0Aand%20security%20measures%20ensure%20that%20only%20minimal%20data%20is%20transmitted%20off-device%2C%0Aachieving%20a%20high%20standard%20of%20data%20protection.%20Despite%20FA%27s%20broad%20relevance%2C%20the%0Aapplicability%20of%20existing%20FA%20systems%20is%20limited%20by%20compromised%20accuracy%3B%20lack%0Aof%20flexibility%20for%20data%20analytics%3B%20and%20an%20inability%20to%20scale%20effectively.%20In%0Athis%20paper%2C%20we%20describe%20our%20approach%20to%20combine%20privacy%2C%20scalability%2C%20and%0Apracticality%20to%20build%20and%20deploy%20a%20system%20that%20overcomes%20these%20limitations.%20Our%0AFA%20system%20leverages%20trusted%20execution%20environments%20%28TEEs%29%20and%20optimizes%20the%20use%0Aof%20on-device%20computing%20resources%20to%20facilitate%20federated%20data%20processing%20across%0Alarge%20fleets%20of%20devices%2C%20while%20ensuring%20robust%2C%20defensible%2C%20and%20verifiable%0Aprivacy%20safeguards.%20We%20focus%20on%20federated%20analytics%20%28statistics%20and%0Amonitoring%29%2C%20in%20contrast%20to%20systems%20for%20federated%20learning%20%28ML%20workloads%29%2C%20and%0Awe%20flag%20the%20key%20differences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02340v1&entry.124074799=Read"},
{"title": "Preliminary Investigation into Data Scaling Laws for Imitation\n  Learning-Based End-to-End Autonomous Driving", "author": "Yupeng Zheng and Zhongpu Xia and Qichao Zhang and Teng Zhang and Ben Lu and Xiaochuang Huo and Chao Han and Yixian Li and Mengjie Yu and Bu Jin and Pengxuan Yang and Yuhang Zheng and Haifeng Yuan and Ke Jiang and Peng Jia and Xianpeng Lang and Dongbin Zhao", "abstract": "  The end-to-end autonomous driving paradigm has recently attracted lots of\nattention due to its scalability. However, existing methods are constrained by\nthe limited scale of real-world data, which hinders a comprehensive exploration\nof the scaling laws associated with end-to-end autonomous driving. To address\nthis issue, we collected substantial data from various driving scenarios and\nbehaviors and conducted an extensive study on the scaling laws of existing\nimitation learning-based end-to-end autonomous driving paradigms. Specifically,\napproximately 4 million demonstrations from 23 different scenario types were\ngathered, amounting to over 30,000 hours of driving demonstrations. We\nperformed open-loop evaluations and closed-loop simulation evaluations in 1,400\ndiverse driving demonstrations (1,300 for open-loop and 100 for closed-loop)\nunder stringent assessment conditions. Through experimental analysis, we\ndiscovered that (1) the performance of the driving model exhibits a power-law\nrelationship with the amount of training data; (2) a small increase in the\nquantity of long-tailed data can significantly improve the performance for the\ncorresponding scenarios; (3) appropriate scaling of data enables the model to\nachieve combinatorial generalization in novel scenes and actions. Our results\nhighlight the critical role of data scaling in improving the generalizability\nof models across diverse autonomous driving scenarios, assuring safe deployment\nin the real world. Project repository:\nhttps://github.com/ucaszyp/Driving-Scaling-Law\n", "link": "http://arxiv.org/abs/2412.02689v1", "date": "2024-12-03", "relevancy": 2.0574, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5254}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5208}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preliminary%20Investigation%20into%20Data%20Scaling%20Laws%20for%20Imitation%0A%20%20Learning-Based%20End-to-End%20Autonomous%20Driving&body=Title%3A%20Preliminary%20Investigation%20into%20Data%20Scaling%20Laws%20for%20Imitation%0A%20%20Learning-Based%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Yupeng%20Zheng%20and%20Zhongpu%20Xia%20and%20Qichao%20Zhang%20and%20Teng%20Zhang%20and%20Ben%20Lu%20and%20Xiaochuang%20Huo%20and%20Chao%20Han%20and%20Yixian%20Li%20and%20Mengjie%20Yu%20and%20Bu%20Jin%20and%20Pengxuan%20Yang%20and%20Yuhang%20Zheng%20and%20Haifeng%20Yuan%20and%20Ke%20Jiang%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Dongbin%20Zhao%0AAbstract%3A%20%20%20The%20end-to-end%20autonomous%20driving%20paradigm%20has%20recently%20attracted%20lots%20of%0Aattention%20due%20to%20its%20scalability.%20However%2C%20existing%20methods%20are%20constrained%20by%0Athe%20limited%20scale%20of%20real-world%20data%2C%20which%20hinders%20a%20comprehensive%20exploration%0Aof%20the%20scaling%20laws%20associated%20with%20end-to-end%20autonomous%20driving.%20To%20address%0Athis%20issue%2C%20we%20collected%20substantial%20data%20from%20various%20driving%20scenarios%20and%0Abehaviors%20and%20conducted%20an%20extensive%20study%20on%20the%20scaling%20laws%20of%20existing%0Aimitation%20learning-based%20end-to-end%20autonomous%20driving%20paradigms.%20Specifically%2C%0Aapproximately%204%20million%20demonstrations%20from%2023%20different%20scenario%20types%20were%0Agathered%2C%20amounting%20to%20over%2030%2C000%20hours%20of%20driving%20demonstrations.%20We%0Aperformed%20open-loop%20evaluations%20and%20closed-loop%20simulation%20evaluations%20in%201%2C400%0Adiverse%20driving%20demonstrations%20%281%2C300%20for%20open-loop%20and%20100%20for%20closed-loop%29%0Aunder%20stringent%20assessment%20conditions.%20Through%20experimental%20analysis%2C%20we%0Adiscovered%20that%20%281%29%20the%20performance%20of%20the%20driving%20model%20exhibits%20a%20power-law%0Arelationship%20with%20the%20amount%20of%20training%20data%3B%20%282%29%20a%20small%20increase%20in%20the%0Aquantity%20of%20long-tailed%20data%20can%20significantly%20improve%20the%20performance%20for%20the%0Acorresponding%20scenarios%3B%20%283%29%20appropriate%20scaling%20of%20data%20enables%20the%20model%20to%0Aachieve%20combinatorial%20generalization%20in%20novel%20scenes%20and%20actions.%20Our%20results%0Ahighlight%20the%20critical%20role%20of%20data%20scaling%20in%20improving%20the%20generalizability%0Aof%20models%20across%20diverse%20autonomous%20driving%20scenarios%2C%20assuring%20safe%20deployment%0Ain%20the%20real%20world.%20Project%20repository%3A%0Ahttps%3A//github.com/ucaszyp/Driving-Scaling-Law%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreliminary%2520Investigation%2520into%2520Data%2520Scaling%2520Laws%2520for%2520Imitation%250A%2520%2520Learning-Based%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DYupeng%2520Zheng%2520and%2520Zhongpu%2520Xia%2520and%2520Qichao%2520Zhang%2520and%2520Teng%2520Zhang%2520and%2520Ben%2520Lu%2520and%2520Xiaochuang%2520Huo%2520and%2520Chao%2520Han%2520and%2520Yixian%2520Li%2520and%2520Mengjie%2520Yu%2520and%2520Bu%2520Jin%2520and%2520Pengxuan%2520Yang%2520and%2520Yuhang%2520Zheng%2520and%2520Haifeng%2520Yuan%2520and%2520Ke%2520Jiang%2520and%2520Peng%2520Jia%2520and%2520Xianpeng%2520Lang%2520and%2520Dongbin%2520Zhao%26entry.1292438233%3D%2520%2520The%2520end-to-end%2520autonomous%2520driving%2520paradigm%2520has%2520recently%2520attracted%2520lots%2520of%250Aattention%2520due%2520to%2520its%2520scalability.%2520However%252C%2520existing%2520methods%2520are%2520constrained%2520by%250Athe%2520limited%2520scale%2520of%2520real-world%2520data%252C%2520which%2520hinders%2520a%2520comprehensive%2520exploration%250Aof%2520the%2520scaling%2520laws%2520associated%2520with%2520end-to-end%2520autonomous%2520driving.%2520To%2520address%250Athis%2520issue%252C%2520we%2520collected%2520substantial%2520data%2520from%2520various%2520driving%2520scenarios%2520and%250Abehaviors%2520and%2520conducted%2520an%2520extensive%2520study%2520on%2520the%2520scaling%2520laws%2520of%2520existing%250Aimitation%2520learning-based%2520end-to-end%2520autonomous%2520driving%2520paradigms.%2520Specifically%252C%250Aapproximately%25204%2520million%2520demonstrations%2520from%252023%2520different%2520scenario%2520types%2520were%250Agathered%252C%2520amounting%2520to%2520over%252030%252C000%2520hours%2520of%2520driving%2520demonstrations.%2520We%250Aperformed%2520open-loop%2520evaluations%2520and%2520closed-loop%2520simulation%2520evaluations%2520in%25201%252C400%250Adiverse%2520driving%2520demonstrations%2520%25281%252C300%2520for%2520open-loop%2520and%2520100%2520for%2520closed-loop%2529%250Aunder%2520stringent%2520assessment%2520conditions.%2520Through%2520experimental%2520analysis%252C%2520we%250Adiscovered%2520that%2520%25281%2529%2520the%2520performance%2520of%2520the%2520driving%2520model%2520exhibits%2520a%2520power-law%250Arelationship%2520with%2520the%2520amount%2520of%2520training%2520data%253B%2520%25282%2529%2520a%2520small%2520increase%2520in%2520the%250Aquantity%2520of%2520long-tailed%2520data%2520can%2520significantly%2520improve%2520the%2520performance%2520for%2520the%250Acorresponding%2520scenarios%253B%2520%25283%2529%2520appropriate%2520scaling%2520of%2520data%2520enables%2520the%2520model%2520to%250Aachieve%2520combinatorial%2520generalization%2520in%2520novel%2520scenes%2520and%2520actions.%2520Our%2520results%250Ahighlight%2520the%2520critical%2520role%2520of%2520data%2520scaling%2520in%2520improving%2520the%2520generalizability%250Aof%2520models%2520across%2520diverse%2520autonomous%2520driving%2520scenarios%252C%2520assuring%2520safe%2520deployment%250Ain%2520the%2520real%2520world.%2520Project%2520repository%253A%250Ahttps%253A//github.com/ucaszyp/Driving-Scaling-Law%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preliminary%20Investigation%20into%20Data%20Scaling%20Laws%20for%20Imitation%0A%20%20Learning-Based%20End-to-End%20Autonomous%20Driving&entry.906535625=Yupeng%20Zheng%20and%20Zhongpu%20Xia%20and%20Qichao%20Zhang%20and%20Teng%20Zhang%20and%20Ben%20Lu%20and%20Xiaochuang%20Huo%20and%20Chao%20Han%20and%20Yixian%20Li%20and%20Mengjie%20Yu%20and%20Bu%20Jin%20and%20Pengxuan%20Yang%20and%20Yuhang%20Zheng%20and%20Haifeng%20Yuan%20and%20Ke%20Jiang%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Dongbin%20Zhao&entry.1292438233=%20%20The%20end-to-end%20autonomous%20driving%20paradigm%20has%20recently%20attracted%20lots%20of%0Aattention%20due%20to%20its%20scalability.%20However%2C%20existing%20methods%20are%20constrained%20by%0Athe%20limited%20scale%20of%20real-world%20data%2C%20which%20hinders%20a%20comprehensive%20exploration%0Aof%20the%20scaling%20laws%20associated%20with%20end-to-end%20autonomous%20driving.%20To%20address%0Athis%20issue%2C%20we%20collected%20substantial%20data%20from%20various%20driving%20scenarios%20and%0Abehaviors%20and%20conducted%20an%20extensive%20study%20on%20the%20scaling%20laws%20of%20existing%0Aimitation%20learning-based%20end-to-end%20autonomous%20driving%20paradigms.%20Specifically%2C%0Aapproximately%204%20million%20demonstrations%20from%2023%20different%20scenario%20types%20were%0Agathered%2C%20amounting%20to%20over%2030%2C000%20hours%20of%20driving%20demonstrations.%20We%0Aperformed%20open-loop%20evaluations%20and%20closed-loop%20simulation%20evaluations%20in%201%2C400%0Adiverse%20driving%20demonstrations%20%281%2C300%20for%20open-loop%20and%20100%20for%20closed-loop%29%0Aunder%20stringent%20assessment%20conditions.%20Through%20experimental%20analysis%2C%20we%0Adiscovered%20that%20%281%29%20the%20performance%20of%20the%20driving%20model%20exhibits%20a%20power-law%0Arelationship%20with%20the%20amount%20of%20training%20data%3B%20%282%29%20a%20small%20increase%20in%20the%0Aquantity%20of%20long-tailed%20data%20can%20significantly%20improve%20the%20performance%20for%20the%0Acorresponding%20scenarios%3B%20%283%29%20appropriate%20scaling%20of%20data%20enables%20the%20model%20to%0Aachieve%20combinatorial%20generalization%20in%20novel%20scenes%20and%20actions.%20Our%20results%0Ahighlight%20the%20critical%20role%20of%20data%20scaling%20in%20improving%20the%20generalizability%0Aof%20models%20across%20diverse%20autonomous%20driving%20scenarios%2C%20assuring%20safe%20deployment%0Ain%20the%20real%20world.%20Project%20repository%3A%0Ahttps%3A//github.com/ucaszyp/Driving-Scaling-Law%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02689v1&entry.124074799=Read"},
{"title": "Active Negative Loss: A Robust Framework for Learning with Noisy Labels", "author": "Xichen Ye and Yifan Wu and Yiwen Xu and Xiaoqiang Li and Weizhong Zhang and Yifan Chen", "abstract": "  Deep supervised learning has achieved remarkable success across a wide range\nof tasks, yet it remains susceptible to overfitting when confronted with noisy\nlabels. To address this issue, noise-robust loss functions offer an effective\nsolution for enhancing learning in the presence of label noise. In this work,\nwe systematically investigate the limitation of the recently proposed Active\nPassive Loss (APL), which employs Mean Absolute Error (MAE) as its passive loss\nfunction. Despite the robustness brought by MAE, one of its key drawbacks is\nthat it pays equal attention to clean and noisy samples; this feature slows\ndown convergence and potentially makes training difficult, particularly in\nlarge-scale datasets. To overcome these challenges, we introduce a novel loss\nfunction class, termed Normalized Negative Loss Functions (NNLFs), which serve\nas passive loss functions within the APL framework. NNLFs effectively address\nthe limitations of MAE by concentrating more on memorized clean samples. By\nreplacing MAE in APL with our proposed NNLFs, we enhance APL and present a new\nframework called Active Negative Loss (ANL). Moreover, in non-symmetric noise\nscenarios, we propose an entropy-based regularization technique to mitigate the\nvulnerability to the label imbalance. Extensive experiments demonstrate that\nthe new loss functions adopted by our ANL framework can achieve better or\ncomparable performance to state-of-the-art methods across various label noise\ntypes and in image segmentation tasks. The source code is available at:\nhttps://github.com/Virusdoll/Active-Negative-Loss.\n", "link": "http://arxiv.org/abs/2412.02373v1", "date": "2024-12-03", "relevancy": 2.0357, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5365}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5106}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Negative%20Loss%3A%20A%20Robust%20Framework%20for%20Learning%20with%20Noisy%20Labels&body=Title%3A%20Active%20Negative%20Loss%3A%20A%20Robust%20Framework%20for%20Learning%20with%20Noisy%20Labels%0AAuthor%3A%20Xichen%20Ye%20and%20Yifan%20Wu%20and%20Yiwen%20Xu%20and%20Xiaoqiang%20Li%20and%20Weizhong%20Zhang%20and%20Yifan%20Chen%0AAbstract%3A%20%20%20Deep%20supervised%20learning%20has%20achieved%20remarkable%20success%20across%20a%20wide%20range%0Aof%20tasks%2C%20yet%20it%20remains%20susceptible%20to%20overfitting%20when%20confronted%20with%20noisy%0Alabels.%20To%20address%20this%20issue%2C%20noise-robust%20loss%20functions%20offer%20an%20effective%0Asolution%20for%20enhancing%20learning%20in%20the%20presence%20of%20label%20noise.%20In%20this%20work%2C%0Awe%20systematically%20investigate%20the%20limitation%20of%20the%20recently%20proposed%20Active%0APassive%20Loss%20%28APL%29%2C%20which%20employs%20Mean%20Absolute%20Error%20%28MAE%29%20as%20its%20passive%20loss%0Afunction.%20Despite%20the%20robustness%20brought%20by%20MAE%2C%20one%20of%20its%20key%20drawbacks%20is%0Athat%20it%20pays%20equal%20attention%20to%20clean%20and%20noisy%20samples%3B%20this%20feature%20slows%0Adown%20convergence%20and%20potentially%20makes%20training%20difficult%2C%20particularly%20in%0Alarge-scale%20datasets.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%20novel%20loss%0Afunction%20class%2C%20termed%20Normalized%20Negative%20Loss%20Functions%20%28NNLFs%29%2C%20which%20serve%0Aas%20passive%20loss%20functions%20within%20the%20APL%20framework.%20NNLFs%20effectively%20address%0Athe%20limitations%20of%20MAE%20by%20concentrating%20more%20on%20memorized%20clean%20samples.%20By%0Areplacing%20MAE%20in%20APL%20with%20our%20proposed%20NNLFs%2C%20we%20enhance%20APL%20and%20present%20a%20new%0Aframework%20called%20Active%20Negative%20Loss%20%28ANL%29.%20Moreover%2C%20in%20non-symmetric%20noise%0Ascenarios%2C%20we%20propose%20an%20entropy-based%20regularization%20technique%20to%20mitigate%20the%0Avulnerability%20to%20the%20label%20imbalance.%20Extensive%20experiments%20demonstrate%20that%0Athe%20new%20loss%20functions%20adopted%20by%20our%20ANL%20framework%20can%20achieve%20better%20or%0Acomparable%20performance%20to%20state-of-the-art%20methods%20across%20various%20label%20noise%0Atypes%20and%20in%20image%20segmentation%20tasks.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Virusdoll/Active-Negative-Loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Negative%2520Loss%253A%2520A%2520Robust%2520Framework%2520for%2520Learning%2520with%2520Noisy%2520Labels%26entry.906535625%3DXichen%2520Ye%2520and%2520Yifan%2520Wu%2520and%2520Yiwen%2520Xu%2520and%2520Xiaoqiang%2520Li%2520and%2520Weizhong%2520Zhang%2520and%2520Yifan%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520supervised%2520learning%2520has%2520achieved%2520remarkable%2520success%2520across%2520a%2520wide%2520range%250Aof%2520tasks%252C%2520yet%2520it%2520remains%2520susceptible%2520to%2520overfitting%2520when%2520confronted%2520with%2520noisy%250Alabels.%2520To%2520address%2520this%2520issue%252C%2520noise-robust%2520loss%2520functions%2520offer%2520an%2520effective%250Asolution%2520for%2520enhancing%2520learning%2520in%2520the%2520presence%2520of%2520label%2520noise.%2520In%2520this%2520work%252C%250Awe%2520systematically%2520investigate%2520the%2520limitation%2520of%2520the%2520recently%2520proposed%2520Active%250APassive%2520Loss%2520%2528APL%2529%252C%2520which%2520employs%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520as%2520its%2520passive%2520loss%250Afunction.%2520Despite%2520the%2520robustness%2520brought%2520by%2520MAE%252C%2520one%2520of%2520its%2520key%2520drawbacks%2520is%250Athat%2520it%2520pays%2520equal%2520attention%2520to%2520clean%2520and%2520noisy%2520samples%253B%2520this%2520feature%2520slows%250Adown%2520convergence%2520and%2520potentially%2520makes%2520training%2520difficult%252C%2520particularly%2520in%250Alarge-scale%2520datasets.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520loss%250Afunction%2520class%252C%2520termed%2520Normalized%2520Negative%2520Loss%2520Functions%2520%2528NNLFs%2529%252C%2520which%2520serve%250Aas%2520passive%2520loss%2520functions%2520within%2520the%2520APL%2520framework.%2520NNLFs%2520effectively%2520address%250Athe%2520limitations%2520of%2520MAE%2520by%2520concentrating%2520more%2520on%2520memorized%2520clean%2520samples.%2520By%250Areplacing%2520MAE%2520in%2520APL%2520with%2520our%2520proposed%2520NNLFs%252C%2520we%2520enhance%2520APL%2520and%2520present%2520a%2520new%250Aframework%2520called%2520Active%2520Negative%2520Loss%2520%2528ANL%2529.%2520Moreover%252C%2520in%2520non-symmetric%2520noise%250Ascenarios%252C%2520we%2520propose%2520an%2520entropy-based%2520regularization%2520technique%2520to%2520mitigate%2520the%250Avulnerability%2520to%2520the%2520label%2520imbalance.%2520Extensive%2520experiments%2520demonstrate%2520that%250Athe%2520new%2520loss%2520functions%2520adopted%2520by%2520our%2520ANL%2520framework%2520can%2520achieve%2520better%2520or%250Acomparable%2520performance%2520to%2520state-of-the-art%2520methods%2520across%2520various%2520label%2520noise%250Atypes%2520and%2520in%2520image%2520segmentation%2520tasks.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Virusdoll/Active-Negative-Loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Negative%20Loss%3A%20A%20Robust%20Framework%20for%20Learning%20with%20Noisy%20Labels&entry.906535625=Xichen%20Ye%20and%20Yifan%20Wu%20and%20Yiwen%20Xu%20and%20Xiaoqiang%20Li%20and%20Weizhong%20Zhang%20and%20Yifan%20Chen&entry.1292438233=%20%20Deep%20supervised%20learning%20has%20achieved%20remarkable%20success%20across%20a%20wide%20range%0Aof%20tasks%2C%20yet%20it%20remains%20susceptible%20to%20overfitting%20when%20confronted%20with%20noisy%0Alabels.%20To%20address%20this%20issue%2C%20noise-robust%20loss%20functions%20offer%20an%20effective%0Asolution%20for%20enhancing%20learning%20in%20the%20presence%20of%20label%20noise.%20In%20this%20work%2C%0Awe%20systematically%20investigate%20the%20limitation%20of%20the%20recently%20proposed%20Active%0APassive%20Loss%20%28APL%29%2C%20which%20employs%20Mean%20Absolute%20Error%20%28MAE%29%20as%20its%20passive%20loss%0Afunction.%20Despite%20the%20robustness%20brought%20by%20MAE%2C%20one%20of%20its%20key%20drawbacks%20is%0Athat%20it%20pays%20equal%20attention%20to%20clean%20and%20noisy%20samples%3B%20this%20feature%20slows%0Adown%20convergence%20and%20potentially%20makes%20training%20difficult%2C%20particularly%20in%0Alarge-scale%20datasets.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%20novel%20loss%0Afunction%20class%2C%20termed%20Normalized%20Negative%20Loss%20Functions%20%28NNLFs%29%2C%20which%20serve%0Aas%20passive%20loss%20functions%20within%20the%20APL%20framework.%20NNLFs%20effectively%20address%0Athe%20limitations%20of%20MAE%20by%20concentrating%20more%20on%20memorized%20clean%20samples.%20By%0Areplacing%20MAE%20in%20APL%20with%20our%20proposed%20NNLFs%2C%20we%20enhance%20APL%20and%20present%20a%20new%0Aframework%20called%20Active%20Negative%20Loss%20%28ANL%29.%20Moreover%2C%20in%20non-symmetric%20noise%0Ascenarios%2C%20we%20propose%20an%20entropy-based%20regularization%20technique%20to%20mitigate%20the%0Avulnerability%20to%20the%20label%20imbalance.%20Extensive%20experiments%20demonstrate%20that%0Athe%20new%20loss%20functions%20adopted%20by%20our%20ANL%20framework%20can%20achieve%20better%20or%0Acomparable%20performance%20to%20state-of-the-art%20methods%20across%20various%20label%20noise%0Atypes%20and%20in%20image%20segmentation%20tasks.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Virusdoll/Active-Negative-Loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02373v1&entry.124074799=Read"},
{"title": "Scaffold or Crutch? Examining College Students' Use and Views of\n  Generative AI Tools for STEM Education", "author": "Karen D. Wang and Zhangyang Wu and L'Nard Tufts II and Carl Wieman and Shima Salehi and Nick Haber", "abstract": "  Developing problem-solving competency is central to Science, Technology,\nEngineering, and Mathematics (STEM) education, yet translating this priority\ninto effective approaches to problem-solving instruction and assessment remain\na significant challenge. The recent proliferation of generative artificial\nintelligence (genAI) tools like ChatGPT in higher education introduces new\nconsiderations about how these tools can help or hinder students' development\nof STEM problem-solving competency. Our research examines these considerations\nby studying how and why college students use genAI tools in their STEM\ncoursework, focusing on their problem-solving support. We surveyed 40 STEM\ncollege students from diverse U.S. institutions and 28 STEM faculty to\nunderstand instructor perspectives on effective genAI tool use and guidance in\nSTEM courses. Our findings reveal high adoption rates and diverse applications\nof genAI tools among STEM students. The most common use cases include finding\nexplanations, exploring related topics, summarizing readings, and helping with\nproblem-set questions. The primary motivation for using genAI tools was to save\ntime. Moreover, over half of student participants reported simply inputting\nproblems for AI to generate solutions, potentially bypassing their own\nproblem-solving processes. These findings indicate that despite high adoption\nrates, students' current approaches to utilizing genAI tools often fall short\nin enhancing their own STEM problem-solving competencies. The study also\nexplored students' and STEM instructors' perceptions of the benefits and risks\nassociated with using genAI tools in STEM education. Our findings provide\ninsights into how to guide students on appropriate genAI use in STEM courses\nand how to design genAI-based tools to foster students' problem-solving\ncompetency.\n", "link": "http://arxiv.org/abs/2412.02653v1", "date": "2024-12-03", "relevancy": 2.0192, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5171}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4964}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaffold%20or%20Crutch%3F%20Examining%20College%20Students%27%20Use%20and%20Views%20of%0A%20%20Generative%20AI%20Tools%20for%20STEM%20Education&body=Title%3A%20Scaffold%20or%20Crutch%3F%20Examining%20College%20Students%27%20Use%20and%20Views%20of%0A%20%20Generative%20AI%20Tools%20for%20STEM%20Education%0AAuthor%3A%20Karen%20D.%20Wang%20and%20Zhangyang%20Wu%20and%20L%27Nard%20Tufts%20II%20and%20Carl%20Wieman%20and%20Shima%20Salehi%20and%20Nick%20Haber%0AAbstract%3A%20%20%20Developing%20problem-solving%20competency%20is%20central%20to%20Science%2C%20Technology%2C%0AEngineering%2C%20and%20Mathematics%20%28STEM%29%20education%2C%20yet%20translating%20this%20priority%0Ainto%20effective%20approaches%20to%20problem-solving%20instruction%20and%20assessment%20remain%0Aa%20significant%20challenge.%20The%20recent%20proliferation%20of%20generative%20artificial%0Aintelligence%20%28genAI%29%20tools%20like%20ChatGPT%20in%20higher%20education%20introduces%20new%0Aconsiderations%20about%20how%20these%20tools%20can%20help%20or%20hinder%20students%27%20development%0Aof%20STEM%20problem-solving%20competency.%20Our%20research%20examines%20these%20considerations%0Aby%20studying%20how%20and%20why%20college%20students%20use%20genAI%20tools%20in%20their%20STEM%0Acoursework%2C%20focusing%20on%20their%20problem-solving%20support.%20We%20surveyed%2040%20STEM%0Acollege%20students%20from%20diverse%20U.S.%20institutions%20and%2028%20STEM%20faculty%20to%0Aunderstand%20instructor%20perspectives%20on%20effective%20genAI%20tool%20use%20and%20guidance%20in%0ASTEM%20courses.%20Our%20findings%20reveal%20high%20adoption%20rates%20and%20diverse%20applications%0Aof%20genAI%20tools%20among%20STEM%20students.%20The%20most%20common%20use%20cases%20include%20finding%0Aexplanations%2C%20exploring%20related%20topics%2C%20summarizing%20readings%2C%20and%20helping%20with%0Aproblem-set%20questions.%20The%20primary%20motivation%20for%20using%20genAI%20tools%20was%20to%20save%0Atime.%20Moreover%2C%20over%20half%20of%20student%20participants%20reported%20simply%20inputting%0Aproblems%20for%20AI%20to%20generate%20solutions%2C%20potentially%20bypassing%20their%20own%0Aproblem-solving%20processes.%20These%20findings%20indicate%20that%20despite%20high%20adoption%0Arates%2C%20students%27%20current%20approaches%20to%20utilizing%20genAI%20tools%20often%20fall%20short%0Ain%20enhancing%20their%20own%20STEM%20problem-solving%20competencies.%20The%20study%20also%0Aexplored%20students%27%20and%20STEM%20instructors%27%20perceptions%20of%20the%20benefits%20and%20risks%0Aassociated%20with%20using%20genAI%20tools%20in%20STEM%20education.%20Our%20findings%20provide%0Ainsights%20into%20how%20to%20guide%20students%20on%20appropriate%20genAI%20use%20in%20STEM%20courses%0Aand%20how%20to%20design%20genAI-based%20tools%20to%20foster%20students%27%20problem-solving%0Acompetency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaffold%2520or%2520Crutch%253F%2520Examining%2520College%2520Students%2527%2520Use%2520and%2520Views%2520of%250A%2520%2520Generative%2520AI%2520Tools%2520for%2520STEM%2520Education%26entry.906535625%3DKaren%2520D.%2520Wang%2520and%2520Zhangyang%2520Wu%2520and%2520L%2527Nard%2520Tufts%2520II%2520and%2520Carl%2520Wieman%2520and%2520Shima%2520Salehi%2520and%2520Nick%2520Haber%26entry.1292438233%3D%2520%2520Developing%2520problem-solving%2520competency%2520is%2520central%2520to%2520Science%252C%2520Technology%252C%250AEngineering%252C%2520and%2520Mathematics%2520%2528STEM%2529%2520education%252C%2520yet%2520translating%2520this%2520priority%250Ainto%2520effective%2520approaches%2520to%2520problem-solving%2520instruction%2520and%2520assessment%2520remain%250Aa%2520significant%2520challenge.%2520The%2520recent%2520proliferation%2520of%2520generative%2520artificial%250Aintelligence%2520%2528genAI%2529%2520tools%2520like%2520ChatGPT%2520in%2520higher%2520education%2520introduces%2520new%250Aconsiderations%2520about%2520how%2520these%2520tools%2520can%2520help%2520or%2520hinder%2520students%2527%2520development%250Aof%2520STEM%2520problem-solving%2520competency.%2520Our%2520research%2520examines%2520these%2520considerations%250Aby%2520studying%2520how%2520and%2520why%2520college%2520students%2520use%2520genAI%2520tools%2520in%2520their%2520STEM%250Acoursework%252C%2520focusing%2520on%2520their%2520problem-solving%2520support.%2520We%2520surveyed%252040%2520STEM%250Acollege%2520students%2520from%2520diverse%2520U.S.%2520institutions%2520and%252028%2520STEM%2520faculty%2520to%250Aunderstand%2520instructor%2520perspectives%2520on%2520effective%2520genAI%2520tool%2520use%2520and%2520guidance%2520in%250ASTEM%2520courses.%2520Our%2520findings%2520reveal%2520high%2520adoption%2520rates%2520and%2520diverse%2520applications%250Aof%2520genAI%2520tools%2520among%2520STEM%2520students.%2520The%2520most%2520common%2520use%2520cases%2520include%2520finding%250Aexplanations%252C%2520exploring%2520related%2520topics%252C%2520summarizing%2520readings%252C%2520and%2520helping%2520with%250Aproblem-set%2520questions.%2520The%2520primary%2520motivation%2520for%2520using%2520genAI%2520tools%2520was%2520to%2520save%250Atime.%2520Moreover%252C%2520over%2520half%2520of%2520student%2520participants%2520reported%2520simply%2520inputting%250Aproblems%2520for%2520AI%2520to%2520generate%2520solutions%252C%2520potentially%2520bypassing%2520their%2520own%250Aproblem-solving%2520processes.%2520These%2520findings%2520indicate%2520that%2520despite%2520high%2520adoption%250Arates%252C%2520students%2527%2520current%2520approaches%2520to%2520utilizing%2520genAI%2520tools%2520often%2520fall%2520short%250Ain%2520enhancing%2520their%2520own%2520STEM%2520problem-solving%2520competencies.%2520The%2520study%2520also%250Aexplored%2520students%2527%2520and%2520STEM%2520instructors%2527%2520perceptions%2520of%2520the%2520benefits%2520and%2520risks%250Aassociated%2520with%2520using%2520genAI%2520tools%2520in%2520STEM%2520education.%2520Our%2520findings%2520provide%250Ainsights%2520into%2520how%2520to%2520guide%2520students%2520on%2520appropriate%2520genAI%2520use%2520in%2520STEM%2520courses%250Aand%2520how%2520to%2520design%2520genAI-based%2520tools%2520to%2520foster%2520students%2527%2520problem-solving%250Acompetency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaffold%20or%20Crutch%3F%20Examining%20College%20Students%27%20Use%20and%20Views%20of%0A%20%20Generative%20AI%20Tools%20for%20STEM%20Education&entry.906535625=Karen%20D.%20Wang%20and%20Zhangyang%20Wu%20and%20L%27Nard%20Tufts%20II%20and%20Carl%20Wieman%20and%20Shima%20Salehi%20and%20Nick%20Haber&entry.1292438233=%20%20Developing%20problem-solving%20competency%20is%20central%20to%20Science%2C%20Technology%2C%0AEngineering%2C%20and%20Mathematics%20%28STEM%29%20education%2C%20yet%20translating%20this%20priority%0Ainto%20effective%20approaches%20to%20problem-solving%20instruction%20and%20assessment%20remain%0Aa%20significant%20challenge.%20The%20recent%20proliferation%20of%20generative%20artificial%0Aintelligence%20%28genAI%29%20tools%20like%20ChatGPT%20in%20higher%20education%20introduces%20new%0Aconsiderations%20about%20how%20these%20tools%20can%20help%20or%20hinder%20students%27%20development%0Aof%20STEM%20problem-solving%20competency.%20Our%20research%20examines%20these%20considerations%0Aby%20studying%20how%20and%20why%20college%20students%20use%20genAI%20tools%20in%20their%20STEM%0Acoursework%2C%20focusing%20on%20their%20problem-solving%20support.%20We%20surveyed%2040%20STEM%0Acollege%20students%20from%20diverse%20U.S.%20institutions%20and%2028%20STEM%20faculty%20to%0Aunderstand%20instructor%20perspectives%20on%20effective%20genAI%20tool%20use%20and%20guidance%20in%0ASTEM%20courses.%20Our%20findings%20reveal%20high%20adoption%20rates%20and%20diverse%20applications%0Aof%20genAI%20tools%20among%20STEM%20students.%20The%20most%20common%20use%20cases%20include%20finding%0Aexplanations%2C%20exploring%20related%20topics%2C%20summarizing%20readings%2C%20and%20helping%20with%0Aproblem-set%20questions.%20The%20primary%20motivation%20for%20using%20genAI%20tools%20was%20to%20save%0Atime.%20Moreover%2C%20over%20half%20of%20student%20participants%20reported%20simply%20inputting%0Aproblems%20for%20AI%20to%20generate%20solutions%2C%20potentially%20bypassing%20their%20own%0Aproblem-solving%20processes.%20These%20findings%20indicate%20that%20despite%20high%20adoption%0Arates%2C%20students%27%20current%20approaches%20to%20utilizing%20genAI%20tools%20often%20fall%20short%0Ain%20enhancing%20their%20own%20STEM%20problem-solving%20competencies.%20The%20study%20also%0Aexplored%20students%27%20and%20STEM%20instructors%27%20perceptions%20of%20the%20benefits%20and%20risks%0Aassociated%20with%20using%20genAI%20tools%20in%20STEM%20education.%20Our%20findings%20provide%0Ainsights%20into%20how%20to%20guide%20students%20on%20appropriate%20genAI%20use%20in%20STEM%20courses%0Aand%20how%20to%20design%20genAI-based%20tools%20to%20foster%20students%27%20problem-solving%0Acompetency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02653v1&entry.124074799=Read"},
{"title": "Collaborative Feature-Logits Contrastive Learning for Open-Set\n  Semi-Supervised Object Detection", "author": "Xinhao Zhong and Siyu Jiao and Yao Zhao and Yunchao Wei", "abstract": "  Current Semi-Supervised Object Detection (SSOD) methods enhance detector\nperformance by leveraging large amounts of unlabeled data, assuming that both\nlabeled and unlabeled data share the same label space. However, in open-set\nscenarios, the unlabeled dataset contains both in-distribution (ID) classes and\nout-of-distribution (OOD) classes. Applying semi-supervised detectors in such\nsettings can lead to misclassifying OOD class as ID classes. To alleviate this\nissue, we propose a simple yet effective method, termed Collaborative\nFeature-Logits Detector (CFL-Detector). Specifically, we introduce a\nfeature-level clustering method using contrastive loss to clarify vector\nboundaries in the feature space and highlight class differences. Additionally,\nby optimizing the logits-level uncertainty classification loss, the model\nenhances its ability to effectively distinguish between ID and OOD classes.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nperformance compared to existing methods.\n", "link": "http://arxiv.org/abs/2411.13001v2", "date": "2024-12-03", "relevancy": 2.0057, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5222}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4867}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Feature-Logits%20Contrastive%20Learning%20for%20Open-Set%0A%20%20Semi-Supervised%20Object%20Detection&body=Title%3A%20Collaborative%20Feature-Logits%20Contrastive%20Learning%20for%20Open-Set%0A%20%20Semi-Supervised%20Object%20Detection%0AAuthor%3A%20Xinhao%20Zhong%20and%20Siyu%20Jiao%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20Current%20Semi-Supervised%20Object%20Detection%20%28SSOD%29%20methods%20enhance%20detector%0Aperformance%20by%20leveraging%20large%20amounts%20of%20unlabeled%20data%2C%20assuming%20that%20both%0Alabeled%20and%20unlabeled%20data%20share%20the%20same%20label%20space.%20However%2C%20in%20open-set%0Ascenarios%2C%20the%20unlabeled%20dataset%20contains%20both%20in-distribution%20%28ID%29%20classes%20and%0Aout-of-distribution%20%28OOD%29%20classes.%20Applying%20semi-supervised%20detectors%20in%20such%0Asettings%20can%20lead%20to%20misclassifying%20OOD%20class%20as%20ID%20classes.%20To%20alleviate%20this%0Aissue%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20termed%20Collaborative%0AFeature-Logits%20Detector%20%28CFL-Detector%29.%20Specifically%2C%20we%20introduce%20a%0Afeature-level%20clustering%20method%20using%20contrastive%20loss%20to%20clarify%20vector%0Aboundaries%20in%20the%20feature%20space%20and%20highlight%20class%20differences.%20Additionally%2C%0Aby%20optimizing%20the%20logits-level%20uncertainty%20classification%20loss%2C%20the%20model%0Aenhances%20its%20ability%20to%20effectively%20distinguish%20between%20ID%20and%20OOD%20classes.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Feature-Logits%2520Contrastive%2520Learning%2520for%2520Open-Set%250A%2520%2520Semi-Supervised%2520Object%2520Detection%26entry.906535625%3DXinhao%2520Zhong%2520and%2520Siyu%2520Jiao%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520Current%2520Semi-Supervised%2520Object%2520Detection%2520%2528SSOD%2529%2520methods%2520enhance%2520detector%250Aperformance%2520by%2520leveraging%2520large%2520amounts%2520of%2520unlabeled%2520data%252C%2520assuming%2520that%2520both%250Alabeled%2520and%2520unlabeled%2520data%2520share%2520the%2520same%2520label%2520space.%2520However%252C%2520in%2520open-set%250Ascenarios%252C%2520the%2520unlabeled%2520dataset%2520contains%2520both%2520in-distribution%2520%2528ID%2529%2520classes%2520and%250Aout-of-distribution%2520%2528OOD%2529%2520classes.%2520Applying%2520semi-supervised%2520detectors%2520in%2520such%250Asettings%2520can%2520lead%2520to%2520misclassifying%2520OOD%2520class%2520as%2520ID%2520classes.%2520To%2520alleviate%2520this%250Aissue%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520termed%2520Collaborative%250AFeature-Logits%2520Detector%2520%2528CFL-Detector%2529.%2520Specifically%252C%2520we%2520introduce%2520a%250Afeature-level%2520clustering%2520method%2520using%2520contrastive%2520loss%2520to%2520clarify%2520vector%250Aboundaries%2520in%2520the%2520feature%2520space%2520and%2520highlight%2520class%2520differences.%2520Additionally%252C%250Aby%2520optimizing%2520the%2520logits-level%2520uncertainty%2520classification%2520loss%252C%2520the%2520model%250Aenhances%2520its%2520ability%2520to%2520effectively%2520distinguish%2520between%2520ID%2520and%2520OOD%2520classes.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Feature-Logits%20Contrastive%20Learning%20for%20Open-Set%0A%20%20Semi-Supervised%20Object%20Detection&entry.906535625=Xinhao%20Zhong%20and%20Siyu%20Jiao%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=%20%20Current%20Semi-Supervised%20Object%20Detection%20%28SSOD%29%20methods%20enhance%20detector%0Aperformance%20by%20leveraging%20large%20amounts%20of%20unlabeled%20data%2C%20assuming%20that%20both%0Alabeled%20and%20unlabeled%20data%20share%20the%20same%20label%20space.%20However%2C%20in%20open-set%0Ascenarios%2C%20the%20unlabeled%20dataset%20contains%20both%20in-distribution%20%28ID%29%20classes%20and%0Aout-of-distribution%20%28OOD%29%20classes.%20Applying%20semi-supervised%20detectors%20in%20such%0Asettings%20can%20lead%20to%20misclassifying%20OOD%20class%20as%20ID%20classes.%20To%20alleviate%20this%0Aissue%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20termed%20Collaborative%0AFeature-Logits%20Detector%20%28CFL-Detector%29.%20Specifically%2C%20we%20introduce%20a%0Afeature-level%20clustering%20method%20using%20contrastive%20loss%20to%20clarify%20vector%0Aboundaries%20in%20the%20feature%20space%20and%20highlight%20class%20differences.%20Additionally%2C%0Aby%20optimizing%20the%20logits-level%20uncertainty%20classification%20loss%2C%20the%20model%0Aenhances%20its%20ability%20to%20effectively%20distinguish%20between%20ID%20and%20OOD%20classes.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13001v2&entry.124074799=Read"},
{"title": "Class-wise Autoencoders Measure Classification Difficulty And Detect\n  Label Mistakes", "author": "Jacob Marks and Brent A. Griffin and Jason J. Corso", "abstract": "  We introduce a new framework for analyzing classification datasets based on\nthe ratios of reconstruction errors between autoencoders trained on individual\nclasses. This analysis framework enables efficient characterization of datasets\non the sample, class, and entire dataset levels. We define reconstruction error\nratios (RERs) that probe classification difficulty and allow its decomposition\ninto (1) finite sample size and (2) Bayes error and decision-boundary\ncomplexity. Through systematic study across 19 popular visual datasets, we find\nthat our RER-based dataset difficulty probe strongly correlates with error rate\nfor state-of-the-art (SOTA) classification models. By interpreting sample-level\nclassification difficulty as a label mistakenness score, we further find that\nRERs achieve SOTA performance on mislabel detection tasks on hard datasets\nunder symmetric and asymmetric label noise. Our code is publicly available at\nhttps://github.com/voxel51/reconstruction-error-ratios.\n", "link": "http://arxiv.org/abs/2412.02596v1", "date": "2024-12-03", "relevancy": 1.9998, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4991}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-wise%20Autoencoders%20Measure%20Classification%20Difficulty%20And%20Detect%0A%20%20Label%20Mistakes&body=Title%3A%20Class-wise%20Autoencoders%20Measure%20Classification%20Difficulty%20And%20Detect%0A%20%20Label%20Mistakes%0AAuthor%3A%20Jacob%20Marks%20and%20Brent%20A.%20Griffin%20and%20Jason%20J.%20Corso%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20framework%20for%20analyzing%20classification%20datasets%20based%20on%0Athe%20ratios%20of%20reconstruction%20errors%20between%20autoencoders%20trained%20on%20individual%0Aclasses.%20This%20analysis%20framework%20enables%20efficient%20characterization%20of%20datasets%0Aon%20the%20sample%2C%20class%2C%20and%20entire%20dataset%20levels.%20We%20define%20reconstruction%20error%0Aratios%20%28RERs%29%20that%20probe%20classification%20difficulty%20and%20allow%20its%20decomposition%0Ainto%20%281%29%20finite%20sample%20size%20and%20%282%29%20Bayes%20error%20and%20decision-boundary%0Acomplexity.%20Through%20systematic%20study%20across%2019%20popular%20visual%20datasets%2C%20we%20find%0Athat%20our%20RER-based%20dataset%20difficulty%20probe%20strongly%20correlates%20with%20error%20rate%0Afor%20state-of-the-art%20%28SOTA%29%20classification%20models.%20By%20interpreting%20sample-level%0Aclassification%20difficulty%20as%20a%20label%20mistakenness%20score%2C%20we%20further%20find%20that%0ARERs%20achieve%20SOTA%20performance%20on%20mislabel%20detection%20tasks%20on%20hard%20datasets%0Aunder%20symmetric%20and%20asymmetric%20label%20noise.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/voxel51/reconstruction-error-ratios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-wise%2520Autoencoders%2520Measure%2520Classification%2520Difficulty%2520And%2520Detect%250A%2520%2520Label%2520Mistakes%26entry.906535625%3DJacob%2520Marks%2520and%2520Brent%2520A.%2520Griffin%2520and%2520Jason%2520J.%2520Corso%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520framework%2520for%2520analyzing%2520classification%2520datasets%2520based%2520on%250Athe%2520ratios%2520of%2520reconstruction%2520errors%2520between%2520autoencoders%2520trained%2520on%2520individual%250Aclasses.%2520This%2520analysis%2520framework%2520enables%2520efficient%2520characterization%2520of%2520datasets%250Aon%2520the%2520sample%252C%2520class%252C%2520and%2520entire%2520dataset%2520levels.%2520We%2520define%2520reconstruction%2520error%250Aratios%2520%2528RERs%2529%2520that%2520probe%2520classification%2520difficulty%2520and%2520allow%2520its%2520decomposition%250Ainto%2520%25281%2529%2520finite%2520sample%2520size%2520and%2520%25282%2529%2520Bayes%2520error%2520and%2520decision-boundary%250Acomplexity.%2520Through%2520systematic%2520study%2520across%252019%2520popular%2520visual%2520datasets%252C%2520we%2520find%250Athat%2520our%2520RER-based%2520dataset%2520difficulty%2520probe%2520strongly%2520correlates%2520with%2520error%2520rate%250Afor%2520state-of-the-art%2520%2528SOTA%2529%2520classification%2520models.%2520By%2520interpreting%2520sample-level%250Aclassification%2520difficulty%2520as%2520a%2520label%2520mistakenness%2520score%252C%2520we%2520further%2520find%2520that%250ARERs%2520achieve%2520SOTA%2520performance%2520on%2520mislabel%2520detection%2520tasks%2520on%2520hard%2520datasets%250Aunder%2520symmetric%2520and%2520asymmetric%2520label%2520noise.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/voxel51/reconstruction-error-ratios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-wise%20Autoencoders%20Measure%20Classification%20Difficulty%20And%20Detect%0A%20%20Label%20Mistakes&entry.906535625=Jacob%20Marks%20and%20Brent%20A.%20Griffin%20and%20Jason%20J.%20Corso&entry.1292438233=%20%20We%20introduce%20a%20new%20framework%20for%20analyzing%20classification%20datasets%20based%20on%0Athe%20ratios%20of%20reconstruction%20errors%20between%20autoencoders%20trained%20on%20individual%0Aclasses.%20This%20analysis%20framework%20enables%20efficient%20characterization%20of%20datasets%0Aon%20the%20sample%2C%20class%2C%20and%20entire%20dataset%20levels.%20We%20define%20reconstruction%20error%0Aratios%20%28RERs%29%20that%20probe%20classification%20difficulty%20and%20allow%20its%20decomposition%0Ainto%20%281%29%20finite%20sample%20size%20and%20%282%29%20Bayes%20error%20and%20decision-boundary%0Acomplexity.%20Through%20systematic%20study%20across%2019%20popular%20visual%20datasets%2C%20we%20find%0Athat%20our%20RER-based%20dataset%20difficulty%20probe%20strongly%20correlates%20with%20error%20rate%0Afor%20state-of-the-art%20%28SOTA%29%20classification%20models.%20By%20interpreting%20sample-level%0Aclassification%20difficulty%20as%20a%20label%20mistakenness%20score%2C%20we%20further%20find%20that%0ARERs%20achieve%20SOTA%20performance%20on%20mislabel%20detection%20tasks%20on%20hard%20datasets%0Aunder%20symmetric%20and%20asymmetric%20label%20noise.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/voxel51/reconstruction-error-ratios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02596v1&entry.124074799=Read"},
{"title": "Multi-scale and Multi-path Cascaded Convolutional Network for Semantic\n  Segmentation of Colorectal Polyps", "author": "Malik Abdul Manan and Feng Jinchao and Muhammad Yaqub and Shahzad Ahmed and Syed Muhammad Ali Imran and Imran Shabir Chuhan and Haroon Ahmed Khan", "abstract": "  Colorectal polyps are structural abnormalities of the gastrointestinal tract\nthat can potentially become cancerous in some cases. The study introduces a\nnovel framework for colorectal polyp segmentation named the Multi-Scale and\nMulti-Path Cascaded Convolution Network (MMCC-Net), aimed at addressing the\nlimitations of existing models, such as inadequate spatial dependence\nrepresentation and the absence of multi-level feature integration during the\ndecoding stage by integrating multi-scale and multi-path cascaded convolutional\ntechniques and enhances feature aggregation through dual attention modules,\nskip connections, and a feature enhancer. MMCC-Net achieves superior\nperformance in identifying polyp areas at the pixel level. The Proposed\nMMCC-Net was tested across six public datasets and compared against eight SOTA\nmodels to demonstrate its efficiency in polyp segmentation. The MMCC-Net's\nperformance shows Dice scores with confidence intervals ranging between (77.08,\n77.56) and (94.19, 94.71) and Mean Intersection over Union (MIoU) scores with\nconfidence intervals ranging from (72.20, 73.00) to (89.69, 90.53) on the six\ndatabases. These results highlight the model's potential as a powerful tool for\naccurate and efficient polyp segmentation, contributing to early detection and\nprevention strategies in colorectal cancer.\n", "link": "http://arxiv.org/abs/2412.02443v1", "date": "2024-12-03", "relevancy": 1.9903, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20and%20Multi-path%20Cascaded%20Convolutional%20Network%20for%20Semantic%0A%20%20Segmentation%20of%20Colorectal%20Polyps&body=Title%3A%20Multi-scale%20and%20Multi-path%20Cascaded%20Convolutional%20Network%20for%20Semantic%0A%20%20Segmentation%20of%20Colorectal%20Polyps%0AAuthor%3A%20Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Muhammad%20Yaqub%20and%20Shahzad%20Ahmed%20and%20Syed%20Muhammad%20Ali%20Imran%20and%20Imran%20Shabir%20Chuhan%20and%20Haroon%20Ahmed%20Khan%0AAbstract%3A%20%20%20Colorectal%20polyps%20are%20structural%20abnormalities%20of%20the%20gastrointestinal%20tract%0Athat%20can%20potentially%20become%20cancerous%20in%20some%20cases.%20The%20study%20introduces%20a%0Anovel%20framework%20for%20colorectal%20polyp%20segmentation%20named%20the%20Multi-Scale%20and%0AMulti-Path%20Cascaded%20Convolution%20Network%20%28MMCC-Net%29%2C%20aimed%20at%20addressing%20the%0Alimitations%20of%20existing%20models%2C%20such%20as%20inadequate%20spatial%20dependence%0Arepresentation%20and%20the%20absence%20of%20multi-level%20feature%20integration%20during%20the%0Adecoding%20stage%20by%20integrating%20multi-scale%20and%20multi-path%20cascaded%20convolutional%0Atechniques%20and%20enhances%20feature%20aggregation%20through%20dual%20attention%20modules%2C%0Askip%20connections%2C%20and%20a%20feature%20enhancer.%20MMCC-Net%20achieves%20superior%0Aperformance%20in%20identifying%20polyp%20areas%20at%20the%20pixel%20level.%20The%20Proposed%0AMMCC-Net%20was%20tested%20across%20six%20public%20datasets%20and%20compared%20against%20eight%20SOTA%0Amodels%20to%20demonstrate%20its%20efficiency%20in%20polyp%20segmentation.%20The%20MMCC-Net%27s%0Aperformance%20shows%20Dice%20scores%20with%20confidence%20intervals%20ranging%20between%20%2877.08%2C%0A77.56%29%20and%20%2894.19%2C%2094.71%29%20and%20Mean%20Intersection%20over%20Union%20%28MIoU%29%20scores%20with%0Aconfidence%20intervals%20ranging%20from%20%2872.20%2C%2073.00%29%20to%20%2889.69%2C%2090.53%29%20on%20the%20six%0Adatabases.%20These%20results%20highlight%20the%20model%27s%20potential%20as%20a%20powerful%20tool%20for%0Aaccurate%20and%20efficient%20polyp%20segmentation%2C%20contributing%20to%20early%20detection%20and%0Aprevention%20strategies%20in%20colorectal%20cancer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520and%2520Multi-path%2520Cascaded%2520Convolutional%2520Network%2520for%2520Semantic%250A%2520%2520Segmentation%2520of%2520Colorectal%2520Polyps%26entry.906535625%3DMalik%2520Abdul%2520Manan%2520and%2520Feng%2520Jinchao%2520and%2520Muhammad%2520Yaqub%2520and%2520Shahzad%2520Ahmed%2520and%2520Syed%2520Muhammad%2520Ali%2520Imran%2520and%2520Imran%2520Shabir%2520Chuhan%2520and%2520Haroon%2520Ahmed%2520Khan%26entry.1292438233%3D%2520%2520Colorectal%2520polyps%2520are%2520structural%2520abnormalities%2520of%2520the%2520gastrointestinal%2520tract%250Athat%2520can%2520potentially%2520become%2520cancerous%2520in%2520some%2520cases.%2520The%2520study%2520introduces%2520a%250Anovel%2520framework%2520for%2520colorectal%2520polyp%2520segmentation%2520named%2520the%2520Multi-Scale%2520and%250AMulti-Path%2520Cascaded%2520Convolution%2520Network%2520%2528MMCC-Net%2529%252C%2520aimed%2520at%2520addressing%2520the%250Alimitations%2520of%2520existing%2520models%252C%2520such%2520as%2520inadequate%2520spatial%2520dependence%250Arepresentation%2520and%2520the%2520absence%2520of%2520multi-level%2520feature%2520integration%2520during%2520the%250Adecoding%2520stage%2520by%2520integrating%2520multi-scale%2520and%2520multi-path%2520cascaded%2520convolutional%250Atechniques%2520and%2520enhances%2520feature%2520aggregation%2520through%2520dual%2520attention%2520modules%252C%250Askip%2520connections%252C%2520and%2520a%2520feature%2520enhancer.%2520MMCC-Net%2520achieves%2520superior%250Aperformance%2520in%2520identifying%2520polyp%2520areas%2520at%2520the%2520pixel%2520level.%2520The%2520Proposed%250AMMCC-Net%2520was%2520tested%2520across%2520six%2520public%2520datasets%2520and%2520compared%2520against%2520eight%2520SOTA%250Amodels%2520to%2520demonstrate%2520its%2520efficiency%2520in%2520polyp%2520segmentation.%2520The%2520MMCC-Net%2527s%250Aperformance%2520shows%2520Dice%2520scores%2520with%2520confidence%2520intervals%2520ranging%2520between%2520%252877.08%252C%250A77.56%2529%2520and%2520%252894.19%252C%252094.71%2529%2520and%2520Mean%2520Intersection%2520over%2520Union%2520%2528MIoU%2529%2520scores%2520with%250Aconfidence%2520intervals%2520ranging%2520from%2520%252872.20%252C%252073.00%2529%2520to%2520%252889.69%252C%252090.53%2529%2520on%2520the%2520six%250Adatabases.%2520These%2520results%2520highlight%2520the%2520model%2527s%2520potential%2520as%2520a%2520powerful%2520tool%2520for%250Aaccurate%2520and%2520efficient%2520polyp%2520segmentation%252C%2520contributing%2520to%2520early%2520detection%2520and%250Aprevention%2520strategies%2520in%2520colorectal%2520cancer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20and%20Multi-path%20Cascaded%20Convolutional%20Network%20for%20Semantic%0A%20%20Segmentation%20of%20Colorectal%20Polyps&entry.906535625=Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Muhammad%20Yaqub%20and%20Shahzad%20Ahmed%20and%20Syed%20Muhammad%20Ali%20Imran%20and%20Imran%20Shabir%20Chuhan%20and%20Haroon%20Ahmed%20Khan&entry.1292438233=%20%20Colorectal%20polyps%20are%20structural%20abnormalities%20of%20the%20gastrointestinal%20tract%0Athat%20can%20potentially%20become%20cancerous%20in%20some%20cases.%20The%20study%20introduces%20a%0Anovel%20framework%20for%20colorectal%20polyp%20segmentation%20named%20the%20Multi-Scale%20and%0AMulti-Path%20Cascaded%20Convolution%20Network%20%28MMCC-Net%29%2C%20aimed%20at%20addressing%20the%0Alimitations%20of%20existing%20models%2C%20such%20as%20inadequate%20spatial%20dependence%0Arepresentation%20and%20the%20absence%20of%20multi-level%20feature%20integration%20during%20the%0Adecoding%20stage%20by%20integrating%20multi-scale%20and%20multi-path%20cascaded%20convolutional%0Atechniques%20and%20enhances%20feature%20aggregation%20through%20dual%20attention%20modules%2C%0Askip%20connections%2C%20and%20a%20feature%20enhancer.%20MMCC-Net%20achieves%20superior%0Aperformance%20in%20identifying%20polyp%20areas%20at%20the%20pixel%20level.%20The%20Proposed%0AMMCC-Net%20was%20tested%20across%20six%20public%20datasets%20and%20compared%20against%20eight%20SOTA%0Amodels%20to%20demonstrate%20its%20efficiency%20in%20polyp%20segmentation.%20The%20MMCC-Net%27s%0Aperformance%20shows%20Dice%20scores%20with%20confidence%20intervals%20ranging%20between%20%2877.08%2C%0A77.56%29%20and%20%2894.19%2C%2094.71%29%20and%20Mean%20Intersection%20over%20Union%20%28MIoU%29%20scores%20with%0Aconfidence%20intervals%20ranging%20from%20%2872.20%2C%2073.00%29%20to%20%2889.69%2C%2090.53%29%20on%20the%20six%0Adatabases.%20These%20results%20highlight%20the%20model%27s%20potential%20as%20a%20powerful%20tool%20for%0Aaccurate%20and%20efficient%20polyp%20segmentation%2C%20contributing%20to%20early%20detection%20and%0Aprevention%20strategies%20in%20colorectal%20cancer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02443v1&entry.124074799=Read"},
{"title": "Multi-scale and Multi-path Cascaded Convolutional Network for Semantic\n  Segmentation of Colorectal Polyps", "author": "Malik Abdul Manan and Feng Jinchao and Muhammad Yaqub and Shahzad Ahmed and Syed Muhammad Ali Imran and Imran Shabir Chuhan and Haroon Ahmed Khan", "abstract": "  Colorectal polyps are structural abnormalities of the gastrointestinal tract\nthat can potentially become cancerous in some cases. The study introduces a\nnovel framework for colorectal polyp segmentation named the Multi-Scale and\nMulti-Path Cascaded Convolution Network (MMCC-Net), aimed at addressing the\nlimitations of existing models, such as inadequate spatial dependence\nrepresentation and the absence of multi-level feature integration during the\ndecoding stage by integrating multi-scale and multi-path cascaded convolutional\ntechniques and enhances feature aggregation through dual attention modules,\nskip connections, and a feature enhancer. MMCC-Net achieves superior\nperformance in identifying polyp areas at the pixel level. The Proposed\nMMCC-Net was tested across six public datasets and compared against eight SOTA\nmodels to demonstrate its efficiency in polyp segmentation. The MMCC-Net's\nperformance shows Dice scores with confidence intervals ranging between (77.08,\n77.56) and (94.19, 94.71) and Mean Intersection over Union (MIoU) scores with\nconfidence intervals ranging from (72.20, 73.00) to (89.69, 90.53) on the six\ndatabases. These results highlight the model's potential as a powerful tool for\naccurate and efficient polyp segmentation, contributing to early detection and\nprevention strategies in colorectal cancer.\n", "link": "http://arxiv.org/abs/2412.02443v1", "date": "2024-12-03", "relevancy": 1.9903, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20and%20Multi-path%20Cascaded%20Convolutional%20Network%20for%20Semantic%0A%20%20Segmentation%20of%20Colorectal%20Polyps&body=Title%3A%20Multi-scale%20and%20Multi-path%20Cascaded%20Convolutional%20Network%20for%20Semantic%0A%20%20Segmentation%20of%20Colorectal%20Polyps%0AAuthor%3A%20Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Muhammad%20Yaqub%20and%20Shahzad%20Ahmed%20and%20Syed%20Muhammad%20Ali%20Imran%20and%20Imran%20Shabir%20Chuhan%20and%20Haroon%20Ahmed%20Khan%0AAbstract%3A%20%20%20Colorectal%20polyps%20are%20structural%20abnormalities%20of%20the%20gastrointestinal%20tract%0Athat%20can%20potentially%20become%20cancerous%20in%20some%20cases.%20The%20study%20introduces%20a%0Anovel%20framework%20for%20colorectal%20polyp%20segmentation%20named%20the%20Multi-Scale%20and%0AMulti-Path%20Cascaded%20Convolution%20Network%20%28MMCC-Net%29%2C%20aimed%20at%20addressing%20the%0Alimitations%20of%20existing%20models%2C%20such%20as%20inadequate%20spatial%20dependence%0Arepresentation%20and%20the%20absence%20of%20multi-level%20feature%20integration%20during%20the%0Adecoding%20stage%20by%20integrating%20multi-scale%20and%20multi-path%20cascaded%20convolutional%0Atechniques%20and%20enhances%20feature%20aggregation%20through%20dual%20attention%20modules%2C%0Askip%20connections%2C%20and%20a%20feature%20enhancer.%20MMCC-Net%20achieves%20superior%0Aperformance%20in%20identifying%20polyp%20areas%20at%20the%20pixel%20level.%20The%20Proposed%0AMMCC-Net%20was%20tested%20across%20six%20public%20datasets%20and%20compared%20against%20eight%20SOTA%0Amodels%20to%20demonstrate%20its%20efficiency%20in%20polyp%20segmentation.%20The%20MMCC-Net%27s%0Aperformance%20shows%20Dice%20scores%20with%20confidence%20intervals%20ranging%20between%20%2877.08%2C%0A77.56%29%20and%20%2894.19%2C%2094.71%29%20and%20Mean%20Intersection%20over%20Union%20%28MIoU%29%20scores%20with%0Aconfidence%20intervals%20ranging%20from%20%2872.20%2C%2073.00%29%20to%20%2889.69%2C%2090.53%29%20on%20the%20six%0Adatabases.%20These%20results%20highlight%20the%20model%27s%20potential%20as%20a%20powerful%20tool%20for%0Aaccurate%20and%20efficient%20polyp%20segmentation%2C%20contributing%20to%20early%20detection%20and%0Aprevention%20strategies%20in%20colorectal%20cancer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520and%2520Multi-path%2520Cascaded%2520Convolutional%2520Network%2520for%2520Semantic%250A%2520%2520Segmentation%2520of%2520Colorectal%2520Polyps%26entry.906535625%3DMalik%2520Abdul%2520Manan%2520and%2520Feng%2520Jinchao%2520and%2520Muhammad%2520Yaqub%2520and%2520Shahzad%2520Ahmed%2520and%2520Syed%2520Muhammad%2520Ali%2520Imran%2520and%2520Imran%2520Shabir%2520Chuhan%2520and%2520Haroon%2520Ahmed%2520Khan%26entry.1292438233%3D%2520%2520Colorectal%2520polyps%2520are%2520structural%2520abnormalities%2520of%2520the%2520gastrointestinal%2520tract%250Athat%2520can%2520potentially%2520become%2520cancerous%2520in%2520some%2520cases.%2520The%2520study%2520introduces%2520a%250Anovel%2520framework%2520for%2520colorectal%2520polyp%2520segmentation%2520named%2520the%2520Multi-Scale%2520and%250AMulti-Path%2520Cascaded%2520Convolution%2520Network%2520%2528MMCC-Net%2529%252C%2520aimed%2520at%2520addressing%2520the%250Alimitations%2520of%2520existing%2520models%252C%2520such%2520as%2520inadequate%2520spatial%2520dependence%250Arepresentation%2520and%2520the%2520absence%2520of%2520multi-level%2520feature%2520integration%2520during%2520the%250Adecoding%2520stage%2520by%2520integrating%2520multi-scale%2520and%2520multi-path%2520cascaded%2520convolutional%250Atechniques%2520and%2520enhances%2520feature%2520aggregation%2520through%2520dual%2520attention%2520modules%252C%250Askip%2520connections%252C%2520and%2520a%2520feature%2520enhancer.%2520MMCC-Net%2520achieves%2520superior%250Aperformance%2520in%2520identifying%2520polyp%2520areas%2520at%2520the%2520pixel%2520level.%2520The%2520Proposed%250AMMCC-Net%2520was%2520tested%2520across%2520six%2520public%2520datasets%2520and%2520compared%2520against%2520eight%2520SOTA%250Amodels%2520to%2520demonstrate%2520its%2520efficiency%2520in%2520polyp%2520segmentation.%2520The%2520MMCC-Net%2527s%250Aperformance%2520shows%2520Dice%2520scores%2520with%2520confidence%2520intervals%2520ranging%2520between%2520%252877.08%252C%250A77.56%2529%2520and%2520%252894.19%252C%252094.71%2529%2520and%2520Mean%2520Intersection%2520over%2520Union%2520%2528MIoU%2529%2520scores%2520with%250Aconfidence%2520intervals%2520ranging%2520from%2520%252872.20%252C%252073.00%2529%2520to%2520%252889.69%252C%252090.53%2529%2520on%2520the%2520six%250Adatabases.%2520These%2520results%2520highlight%2520the%2520model%2527s%2520potential%2520as%2520a%2520powerful%2520tool%2520for%250Aaccurate%2520and%2520efficient%2520polyp%2520segmentation%252C%2520contributing%2520to%2520early%2520detection%2520and%250Aprevention%2520strategies%2520in%2520colorectal%2520cancer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20and%20Multi-path%20Cascaded%20Convolutional%20Network%20for%20Semantic%0A%20%20Segmentation%20of%20Colorectal%20Polyps&entry.906535625=Malik%20Abdul%20Manan%20and%20Feng%20Jinchao%20and%20Muhammad%20Yaqub%20and%20Shahzad%20Ahmed%20and%20Syed%20Muhammad%20Ali%20Imran%20and%20Imran%20Shabir%20Chuhan%20and%20Haroon%20Ahmed%20Khan&entry.1292438233=%20%20Colorectal%20polyps%20are%20structural%20abnormalities%20of%20the%20gastrointestinal%20tract%0Athat%20can%20potentially%20become%20cancerous%20in%20some%20cases.%20The%20study%20introduces%20a%0Anovel%20framework%20for%20colorectal%20polyp%20segmentation%20named%20the%20Multi-Scale%20and%0AMulti-Path%20Cascaded%20Convolution%20Network%20%28MMCC-Net%29%2C%20aimed%20at%20addressing%20the%0Alimitations%20of%20existing%20models%2C%20such%20as%20inadequate%20spatial%20dependence%0Arepresentation%20and%20the%20absence%20of%20multi-level%20feature%20integration%20during%20the%0Adecoding%20stage%20by%20integrating%20multi-scale%20and%20multi-path%20cascaded%20convolutional%0Atechniques%20and%20enhances%20feature%20aggregation%20through%20dual%20attention%20modules%2C%0Askip%20connections%2C%20and%20a%20feature%20enhancer.%20MMCC-Net%20achieves%20superior%0Aperformance%20in%20identifying%20polyp%20areas%20at%20the%20pixel%20level.%20The%20Proposed%0AMMCC-Net%20was%20tested%20across%20six%20public%20datasets%20and%20compared%20against%20eight%20SOTA%0Amodels%20to%20demonstrate%20its%20efficiency%20in%20polyp%20segmentation.%20The%20MMCC-Net%27s%0Aperformance%20shows%20Dice%20scores%20with%20confidence%20intervals%20ranging%20between%20%2877.08%2C%0A77.56%29%20and%20%2894.19%2C%2094.71%29%20and%20Mean%20Intersection%20over%20Union%20%28MIoU%29%20scores%20with%0Aconfidence%20intervals%20ranging%20from%20%2872.20%2C%2073.00%29%20to%20%2889.69%2C%2090.53%29%20on%20the%20six%0Adatabases.%20These%20results%20highlight%20the%20model%27s%20potential%20as%20a%20powerful%20tool%20for%0Aaccurate%20and%20efficient%20polyp%20segmentation%2C%20contributing%20to%20early%20detection%20and%0Aprevention%20strategies%20in%20colorectal%20cancer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02443v1&entry.124074799=Read"},
{"title": "PITN: Physics-Informed Temporal Networks for Cuffless Blood Pressure\n  Estimation", "author": "Rui Wang and Mengshi Qi and Yingxia Shao and Anfu Zhou and Huadong Ma", "abstract": "  Monitoring blood pressure with non-invasive sensors has gained popularity for\nproviding comfortable user experiences, one of which is a significant function\nof smart wearables. Although providing a comfortable user experience, such\nmethods are suffering from the demand for a significant amount of realistic\ndata to train an individual model for each subject, especially considering the\ninvasive or obtrusive BP ground-truth measurements. To tackle this challenge,\nwe introduce a novel physics-informed temporal network~(PITN) with adversarial\ncontrastive learning to enable precise BP estimation with very limited data.\nSpecifically, we first enhance the physics-informed neural network~(PINN) with\nthe temporal block for investigating BP dynamics' multi-periodicity for\npersonal cardiovascular cycle modeling and temporal variation. We then employ\nadversarial training to generate extra physiological time series data,\nimproving PITN's robustness in the face of sparse subject-specific training\ndata. Furthermore, we utilize contrastive learning to capture the\ndiscriminative variations of cardiovascular physiologic phenomena. This\napproach aggregates physiological signals with similar blood pressure values in\nlatent space while separating clusters of samples with dissimilar blood\npressure values. Experiments on three widely-adopted datasets with different\nmodailties (\\emph{i.e.,} bioimpedance, PPG, millimeter-wave) demonstrate the\nsuperiority and effectiveness of the proposed methods over previous\nstate-of-the-art approaches. The code is available\nat~\\url{https://github.com/Zest86/ACL-PITN}.\n", "link": "http://arxiv.org/abs/2408.08488v2", "date": "2024-12-03", "relevancy": 1.9806, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5158}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5047}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PITN%3A%20Physics-Informed%20Temporal%20Networks%20for%20Cuffless%20Blood%20Pressure%0A%20%20Estimation&body=Title%3A%20PITN%3A%20Physics-Informed%20Temporal%20Networks%20for%20Cuffless%20Blood%20Pressure%0A%20%20Estimation%0AAuthor%3A%20Rui%20Wang%20and%20Mengshi%20Qi%20and%20Yingxia%20Shao%20and%20Anfu%20Zhou%20and%20Huadong%20Ma%0AAbstract%3A%20%20%20Monitoring%20blood%20pressure%20with%20non-invasive%20sensors%20has%20gained%20popularity%20for%0Aproviding%20comfortable%20user%20experiences%2C%20one%20of%20which%20is%20a%20significant%20function%0Aof%20smart%20wearables.%20Although%20providing%20a%20comfortable%20user%20experience%2C%20such%0Amethods%20are%20suffering%20from%20the%20demand%20for%20a%20significant%20amount%20of%20realistic%0Adata%20to%20train%20an%20individual%20model%20for%20each%20subject%2C%20especially%20considering%20the%0Ainvasive%20or%20obtrusive%20BP%20ground-truth%20measurements.%20To%20tackle%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20physics-informed%20temporal%20network~%28PITN%29%20with%20adversarial%0Acontrastive%20learning%20to%20enable%20precise%20BP%20estimation%20with%20very%20limited%20data.%0ASpecifically%2C%20we%20first%20enhance%20the%20physics-informed%20neural%20network~%28PINN%29%20with%0Athe%20temporal%20block%20for%20investigating%20BP%20dynamics%27%20multi-periodicity%20for%0Apersonal%20cardiovascular%20cycle%20modeling%20and%20temporal%20variation.%20We%20then%20employ%0Aadversarial%20training%20to%20generate%20extra%20physiological%20time%20series%20data%2C%0Aimproving%20PITN%27s%20robustness%20in%20the%20face%20of%20sparse%20subject-specific%20training%0Adata.%20Furthermore%2C%20we%20utilize%20contrastive%20learning%20to%20capture%20the%0Adiscriminative%20variations%20of%20cardiovascular%20physiologic%20phenomena.%20This%0Aapproach%20aggregates%20physiological%20signals%20with%20similar%20blood%20pressure%20values%20in%0Alatent%20space%20while%20separating%20clusters%20of%20samples%20with%20dissimilar%20blood%0Apressure%20values.%20Experiments%20on%20three%20widely-adopted%20datasets%20with%20different%0Amodailties%20%28%5Cemph%7Bi.e.%2C%7D%20bioimpedance%2C%20PPG%2C%20millimeter-wave%29%20demonstrate%20the%0Asuperiority%20and%20effectiveness%20of%20the%20proposed%20methods%20over%20previous%0Astate-of-the-art%20approaches.%20The%20code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/Zest86/ACL-PITN%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPITN%253A%2520Physics-Informed%2520Temporal%2520Networks%2520for%2520Cuffless%2520Blood%2520Pressure%250A%2520%2520Estimation%26entry.906535625%3DRui%2520Wang%2520and%2520Mengshi%2520Qi%2520and%2520Yingxia%2520Shao%2520and%2520Anfu%2520Zhou%2520and%2520Huadong%2520Ma%26entry.1292438233%3D%2520%2520Monitoring%2520blood%2520pressure%2520with%2520non-invasive%2520sensors%2520has%2520gained%2520popularity%2520for%250Aproviding%2520comfortable%2520user%2520experiences%252C%2520one%2520of%2520which%2520is%2520a%2520significant%2520function%250Aof%2520smart%2520wearables.%2520Although%2520providing%2520a%2520comfortable%2520user%2520experience%252C%2520such%250Amethods%2520are%2520suffering%2520from%2520the%2520demand%2520for%2520a%2520significant%2520amount%2520of%2520realistic%250Adata%2520to%2520train%2520an%2520individual%2520model%2520for%2520each%2520subject%252C%2520especially%2520considering%2520the%250Ainvasive%2520or%2520obtrusive%2520BP%2520ground-truth%2520measurements.%2520To%2520tackle%2520this%2520challenge%252C%250Awe%2520introduce%2520a%2520novel%2520physics-informed%2520temporal%2520network~%2528PITN%2529%2520with%2520adversarial%250Acontrastive%2520learning%2520to%2520enable%2520precise%2520BP%2520estimation%2520with%2520very%2520limited%2520data.%250ASpecifically%252C%2520we%2520first%2520enhance%2520the%2520physics-informed%2520neural%2520network~%2528PINN%2529%2520with%250Athe%2520temporal%2520block%2520for%2520investigating%2520BP%2520dynamics%2527%2520multi-periodicity%2520for%250Apersonal%2520cardiovascular%2520cycle%2520modeling%2520and%2520temporal%2520variation.%2520We%2520then%2520employ%250Aadversarial%2520training%2520to%2520generate%2520extra%2520physiological%2520time%2520series%2520data%252C%250Aimproving%2520PITN%2527s%2520robustness%2520in%2520the%2520face%2520of%2520sparse%2520subject-specific%2520training%250Adata.%2520Furthermore%252C%2520we%2520utilize%2520contrastive%2520learning%2520to%2520capture%2520the%250Adiscriminative%2520variations%2520of%2520cardiovascular%2520physiologic%2520phenomena.%2520This%250Aapproach%2520aggregates%2520physiological%2520signals%2520with%2520similar%2520blood%2520pressure%2520values%2520in%250Alatent%2520space%2520while%2520separating%2520clusters%2520of%2520samples%2520with%2520dissimilar%2520blood%250Apressure%2520values.%2520Experiments%2520on%2520three%2520widely-adopted%2520datasets%2520with%2520different%250Amodailties%2520%2528%255Cemph%257Bi.e.%252C%257D%2520bioimpedance%252C%2520PPG%252C%2520millimeter-wave%2529%2520demonstrate%2520the%250Asuperiority%2520and%2520effectiveness%2520of%2520the%2520proposed%2520methods%2520over%2520previous%250Astate-of-the-art%2520approaches.%2520The%2520code%2520is%2520available%250Aat~%255Curl%257Bhttps%253A//github.com/Zest86/ACL-PITN%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PITN%3A%20Physics-Informed%20Temporal%20Networks%20for%20Cuffless%20Blood%20Pressure%0A%20%20Estimation&entry.906535625=Rui%20Wang%20and%20Mengshi%20Qi%20and%20Yingxia%20Shao%20and%20Anfu%20Zhou%20and%20Huadong%20Ma&entry.1292438233=%20%20Monitoring%20blood%20pressure%20with%20non-invasive%20sensors%20has%20gained%20popularity%20for%0Aproviding%20comfortable%20user%20experiences%2C%20one%20of%20which%20is%20a%20significant%20function%0Aof%20smart%20wearables.%20Although%20providing%20a%20comfortable%20user%20experience%2C%20such%0Amethods%20are%20suffering%20from%20the%20demand%20for%20a%20significant%20amount%20of%20realistic%0Adata%20to%20train%20an%20individual%20model%20for%20each%20subject%2C%20especially%20considering%20the%0Ainvasive%20or%20obtrusive%20BP%20ground-truth%20measurements.%20To%20tackle%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20physics-informed%20temporal%20network~%28PITN%29%20with%20adversarial%0Acontrastive%20learning%20to%20enable%20precise%20BP%20estimation%20with%20very%20limited%20data.%0ASpecifically%2C%20we%20first%20enhance%20the%20physics-informed%20neural%20network~%28PINN%29%20with%0Athe%20temporal%20block%20for%20investigating%20BP%20dynamics%27%20multi-periodicity%20for%0Apersonal%20cardiovascular%20cycle%20modeling%20and%20temporal%20variation.%20We%20then%20employ%0Aadversarial%20training%20to%20generate%20extra%20physiological%20time%20series%20data%2C%0Aimproving%20PITN%27s%20robustness%20in%20the%20face%20of%20sparse%20subject-specific%20training%0Adata.%20Furthermore%2C%20we%20utilize%20contrastive%20learning%20to%20capture%20the%0Adiscriminative%20variations%20of%20cardiovascular%20physiologic%20phenomena.%20This%0Aapproach%20aggregates%20physiological%20signals%20with%20similar%20blood%20pressure%20values%20in%0Alatent%20space%20while%20separating%20clusters%20of%20samples%20with%20dissimilar%20blood%0Apressure%20values.%20Experiments%20on%20three%20widely-adopted%20datasets%20with%20different%0Amodailties%20%28%5Cemph%7Bi.e.%2C%7D%20bioimpedance%2C%20PPG%2C%20millimeter-wave%29%20demonstrate%20the%0Asuperiority%20and%20effectiveness%20of%20the%20proposed%20methods%20over%20previous%0Astate-of-the-art%20approaches.%20The%20code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/Zest86/ACL-PITN%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08488v2&entry.124074799=Read"},
{"title": "PITN: Physics-Informed Temporal Networks for Cuffless Blood Pressure\n  Estimation", "author": "Rui Wang and Mengshi Qi and Yingxia Shao and Anfu Zhou and Huadong Ma", "abstract": "  Monitoring blood pressure with non-invasive sensors has gained popularity for\nproviding comfortable user experiences, one of which is a significant function\nof smart wearables. Although providing a comfortable user experience, such\nmethods are suffering from the demand for a significant amount of realistic\ndata to train an individual model for each subject, especially considering the\ninvasive or obtrusive BP ground-truth measurements. To tackle this challenge,\nwe introduce a novel physics-informed temporal network~(PITN) with adversarial\ncontrastive learning to enable precise BP estimation with very limited data.\nSpecifically, we first enhance the physics-informed neural network~(PINN) with\nthe temporal block for investigating BP dynamics' multi-periodicity for\npersonal cardiovascular cycle modeling and temporal variation. We then employ\nadversarial training to generate extra physiological time series data,\nimproving PITN's robustness in the face of sparse subject-specific training\ndata. Furthermore, we utilize contrastive learning to capture the\ndiscriminative variations of cardiovascular physiologic phenomena. This\napproach aggregates physiological signals with similar blood pressure values in\nlatent space while separating clusters of samples with dissimilar blood\npressure values. Experiments on three widely-adopted datasets with different\nmodailties (\\emph{i.e.,} bioimpedance, PPG, millimeter-wave) demonstrate the\nsuperiority and effectiveness of the proposed methods over previous\nstate-of-the-art approaches. The code is available\nat~\\url{https://github.com/Zest86/ACL-PITN}.\n", "link": "http://arxiv.org/abs/2408.08488v2", "date": "2024-12-03", "relevancy": 1.9806, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5158}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5047}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PITN%3A%20Physics-Informed%20Temporal%20Networks%20for%20Cuffless%20Blood%20Pressure%0A%20%20Estimation&body=Title%3A%20PITN%3A%20Physics-Informed%20Temporal%20Networks%20for%20Cuffless%20Blood%20Pressure%0A%20%20Estimation%0AAuthor%3A%20Rui%20Wang%20and%20Mengshi%20Qi%20and%20Yingxia%20Shao%20and%20Anfu%20Zhou%20and%20Huadong%20Ma%0AAbstract%3A%20%20%20Monitoring%20blood%20pressure%20with%20non-invasive%20sensors%20has%20gained%20popularity%20for%0Aproviding%20comfortable%20user%20experiences%2C%20one%20of%20which%20is%20a%20significant%20function%0Aof%20smart%20wearables.%20Although%20providing%20a%20comfortable%20user%20experience%2C%20such%0Amethods%20are%20suffering%20from%20the%20demand%20for%20a%20significant%20amount%20of%20realistic%0Adata%20to%20train%20an%20individual%20model%20for%20each%20subject%2C%20especially%20considering%20the%0Ainvasive%20or%20obtrusive%20BP%20ground-truth%20measurements.%20To%20tackle%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20physics-informed%20temporal%20network~%28PITN%29%20with%20adversarial%0Acontrastive%20learning%20to%20enable%20precise%20BP%20estimation%20with%20very%20limited%20data.%0ASpecifically%2C%20we%20first%20enhance%20the%20physics-informed%20neural%20network~%28PINN%29%20with%0Athe%20temporal%20block%20for%20investigating%20BP%20dynamics%27%20multi-periodicity%20for%0Apersonal%20cardiovascular%20cycle%20modeling%20and%20temporal%20variation.%20We%20then%20employ%0Aadversarial%20training%20to%20generate%20extra%20physiological%20time%20series%20data%2C%0Aimproving%20PITN%27s%20robustness%20in%20the%20face%20of%20sparse%20subject-specific%20training%0Adata.%20Furthermore%2C%20we%20utilize%20contrastive%20learning%20to%20capture%20the%0Adiscriminative%20variations%20of%20cardiovascular%20physiologic%20phenomena.%20This%0Aapproach%20aggregates%20physiological%20signals%20with%20similar%20blood%20pressure%20values%20in%0Alatent%20space%20while%20separating%20clusters%20of%20samples%20with%20dissimilar%20blood%0Apressure%20values.%20Experiments%20on%20three%20widely-adopted%20datasets%20with%20different%0Amodailties%20%28%5Cemph%7Bi.e.%2C%7D%20bioimpedance%2C%20PPG%2C%20millimeter-wave%29%20demonstrate%20the%0Asuperiority%20and%20effectiveness%20of%20the%20proposed%20methods%20over%20previous%0Astate-of-the-art%20approaches.%20The%20code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/Zest86/ACL-PITN%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPITN%253A%2520Physics-Informed%2520Temporal%2520Networks%2520for%2520Cuffless%2520Blood%2520Pressure%250A%2520%2520Estimation%26entry.906535625%3DRui%2520Wang%2520and%2520Mengshi%2520Qi%2520and%2520Yingxia%2520Shao%2520and%2520Anfu%2520Zhou%2520and%2520Huadong%2520Ma%26entry.1292438233%3D%2520%2520Monitoring%2520blood%2520pressure%2520with%2520non-invasive%2520sensors%2520has%2520gained%2520popularity%2520for%250Aproviding%2520comfortable%2520user%2520experiences%252C%2520one%2520of%2520which%2520is%2520a%2520significant%2520function%250Aof%2520smart%2520wearables.%2520Although%2520providing%2520a%2520comfortable%2520user%2520experience%252C%2520such%250Amethods%2520are%2520suffering%2520from%2520the%2520demand%2520for%2520a%2520significant%2520amount%2520of%2520realistic%250Adata%2520to%2520train%2520an%2520individual%2520model%2520for%2520each%2520subject%252C%2520especially%2520considering%2520the%250Ainvasive%2520or%2520obtrusive%2520BP%2520ground-truth%2520measurements.%2520To%2520tackle%2520this%2520challenge%252C%250Awe%2520introduce%2520a%2520novel%2520physics-informed%2520temporal%2520network~%2528PITN%2529%2520with%2520adversarial%250Acontrastive%2520learning%2520to%2520enable%2520precise%2520BP%2520estimation%2520with%2520very%2520limited%2520data.%250ASpecifically%252C%2520we%2520first%2520enhance%2520the%2520physics-informed%2520neural%2520network~%2528PINN%2529%2520with%250Athe%2520temporal%2520block%2520for%2520investigating%2520BP%2520dynamics%2527%2520multi-periodicity%2520for%250Apersonal%2520cardiovascular%2520cycle%2520modeling%2520and%2520temporal%2520variation.%2520We%2520then%2520employ%250Aadversarial%2520training%2520to%2520generate%2520extra%2520physiological%2520time%2520series%2520data%252C%250Aimproving%2520PITN%2527s%2520robustness%2520in%2520the%2520face%2520of%2520sparse%2520subject-specific%2520training%250Adata.%2520Furthermore%252C%2520we%2520utilize%2520contrastive%2520learning%2520to%2520capture%2520the%250Adiscriminative%2520variations%2520of%2520cardiovascular%2520physiologic%2520phenomena.%2520This%250Aapproach%2520aggregates%2520physiological%2520signals%2520with%2520similar%2520blood%2520pressure%2520values%2520in%250Alatent%2520space%2520while%2520separating%2520clusters%2520of%2520samples%2520with%2520dissimilar%2520blood%250Apressure%2520values.%2520Experiments%2520on%2520three%2520widely-adopted%2520datasets%2520with%2520different%250Amodailties%2520%2528%255Cemph%257Bi.e.%252C%257D%2520bioimpedance%252C%2520PPG%252C%2520millimeter-wave%2529%2520demonstrate%2520the%250Asuperiority%2520and%2520effectiveness%2520of%2520the%2520proposed%2520methods%2520over%2520previous%250Astate-of-the-art%2520approaches.%2520The%2520code%2520is%2520available%250Aat~%255Curl%257Bhttps%253A//github.com/Zest86/ACL-PITN%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PITN%3A%20Physics-Informed%20Temporal%20Networks%20for%20Cuffless%20Blood%20Pressure%0A%20%20Estimation&entry.906535625=Rui%20Wang%20and%20Mengshi%20Qi%20and%20Yingxia%20Shao%20and%20Anfu%20Zhou%20and%20Huadong%20Ma&entry.1292438233=%20%20Monitoring%20blood%20pressure%20with%20non-invasive%20sensors%20has%20gained%20popularity%20for%0Aproviding%20comfortable%20user%20experiences%2C%20one%20of%20which%20is%20a%20significant%20function%0Aof%20smart%20wearables.%20Although%20providing%20a%20comfortable%20user%20experience%2C%20such%0Amethods%20are%20suffering%20from%20the%20demand%20for%20a%20significant%20amount%20of%20realistic%0Adata%20to%20train%20an%20individual%20model%20for%20each%20subject%2C%20especially%20considering%20the%0Ainvasive%20or%20obtrusive%20BP%20ground-truth%20measurements.%20To%20tackle%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20physics-informed%20temporal%20network~%28PITN%29%20with%20adversarial%0Acontrastive%20learning%20to%20enable%20precise%20BP%20estimation%20with%20very%20limited%20data.%0ASpecifically%2C%20we%20first%20enhance%20the%20physics-informed%20neural%20network~%28PINN%29%20with%0Athe%20temporal%20block%20for%20investigating%20BP%20dynamics%27%20multi-periodicity%20for%0Apersonal%20cardiovascular%20cycle%20modeling%20and%20temporal%20variation.%20We%20then%20employ%0Aadversarial%20training%20to%20generate%20extra%20physiological%20time%20series%20data%2C%0Aimproving%20PITN%27s%20robustness%20in%20the%20face%20of%20sparse%20subject-specific%20training%0Adata.%20Furthermore%2C%20we%20utilize%20contrastive%20learning%20to%20capture%20the%0Adiscriminative%20variations%20of%20cardiovascular%20physiologic%20phenomena.%20This%0Aapproach%20aggregates%20physiological%20signals%20with%20similar%20blood%20pressure%20values%20in%0Alatent%20space%20while%20separating%20clusters%20of%20samples%20with%20dissimilar%20blood%0Apressure%20values.%20Experiments%20on%20three%20widely-adopted%20datasets%20with%20different%0Amodailties%20%28%5Cemph%7Bi.e.%2C%7D%20bioimpedance%2C%20PPG%2C%20millimeter-wave%29%20demonstrate%20the%0Asuperiority%20and%20effectiveness%20of%20the%20proposed%20methods%20over%20previous%0Astate-of-the-art%20approaches.%20The%20code%20is%20available%0Aat~%5Curl%7Bhttps%3A//github.com/Zest86/ACL-PITN%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08488v2&entry.124074799=Read"},
{"title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on\n  Retrieval-Augmented Generation", "author": "Junyuan Zhang and Qintong Zhang and Bin Wang and Linke Ouyang and Zichen Wen and Ying Li and Ka-Ho Chow and Conghui He and Wentao Zhang", "abstract": "  Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\ncarefully selected unstructured PDF documents from six real-world RAG\napplication domains, along with Q&As derived from multimodal elements in\ndocuments, challenging existing OCR solutions used for RAG To better understand\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\nthat none is competent for constructing high-quality knowledge bases for RAG\nsystems. We then systematically evaluate the impact of these two noise types\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\nsystems. Code: https://github.com/opendatalab/OHR-Bench\n", "link": "http://arxiv.org/abs/2412.02592v1", "date": "2024-12-03", "relevancy": 1.9772, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCR%20Hinders%20RAG%3A%20Evaluating%20the%20Cascading%20Impact%20of%20OCR%20on%0A%20%20Retrieval-Augmented%20Generation&body=Title%3A%20OCR%20Hinders%20RAG%3A%20Evaluating%20the%20Cascading%20Impact%20of%20OCR%20on%0A%20%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Junyuan%20Zhang%20and%20Qintong%20Zhang%20and%20Bin%20Wang%20and%20Linke%20Ouyang%20and%20Zichen%20Wen%20and%20Ying%20Li%20and%20Ka-Ho%20Chow%20and%20Conghui%20He%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Retrieval-augmented%20Generation%20%28RAG%29%20enhances%20Large%20Language%20Models%20%28LLMs%29%20by%0Aintegrating%20external%20knowledge%20to%20reduce%20hallucinations%20and%20incorporate%0Aup-to-date%20information%20without%20retraining.%20As%20an%20essential%20part%20of%20RAG%2C%0Aexternal%20knowledge%20bases%20are%20commonly%20built%20by%20extracting%20structured%20data%20from%0Aunstructured%20PDF%20documents%20using%20Optical%20Character%20Recognition%20%28OCR%29.%20However%2C%0Agiven%20the%20imperfect%20prediction%20of%20OCR%20and%20the%20inherent%20non-uniform%0Arepresentation%20of%20structured%20data%2C%20knowledge%20bases%20inevitably%20contain%20various%0AOCR%20noises.%20In%20this%20paper%2C%20we%20introduce%20OHRBench%2C%20the%20first%20benchmark%20for%0Aunderstanding%20the%20cascading%20impact%20of%20OCR%20on%20RAG%20systems.%20OHRBench%20includes%20350%0Acarefully%20selected%20unstructured%20PDF%20documents%20from%20six%20real-world%20RAG%0Aapplication%20domains%2C%20along%20with%20Q%26As%20derived%20from%20multimodal%20elements%20in%0Adocuments%2C%20challenging%20existing%20OCR%20solutions%20used%20for%20RAG%20To%20better%20understand%0AOCR%27s%20impact%20on%20RAG%20systems%2C%20we%20identify%20two%20primary%20types%20of%20OCR%20noise%3A%0ASemantic%20Noise%20and%20Formatting%20Noise%20and%20apply%20perturbation%20to%20generate%20a%20set%20of%0Astructured%20data%20with%20varying%20degrees%20of%20each%20OCR%20noise.%20Using%20OHRBench%2C%20we%0Afirst%20conduct%20a%20comprehensive%20evaluation%20of%20current%20OCR%20solutions%20and%20reveal%0Athat%20none%20is%20competent%20for%20constructing%20high-quality%20knowledge%20bases%20for%20RAG%0Asystems.%20We%20then%20systematically%20evaluate%20the%20impact%20of%20these%20two%20noise%20types%0Aand%20demonstrate%20the%20vulnerability%20of%20RAG%20systems.%20Furthermore%2C%20we%20discuss%20the%0Apotential%20of%20employing%20Vision-Language%20Models%20%28VLMs%29%20without%20OCR%20in%20RAG%0Asystems.%20Code%3A%20https%3A//github.com/opendatalab/OHR-Bench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCR%2520Hinders%2520RAG%253A%2520Evaluating%2520the%2520Cascading%2520Impact%2520of%2520OCR%2520on%250A%2520%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DJunyuan%2520Zhang%2520and%2520Qintong%2520Zhang%2520and%2520Bin%2520Wang%2520and%2520Linke%2520Ouyang%2520and%2520Zichen%2520Wen%2520and%2520Ying%2520Li%2520and%2520Ka-Ho%2520Chow%2520and%2520Conghui%2520He%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520Generation%2520%2528RAG%2529%2520enhances%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%250Aintegrating%2520external%2520knowledge%2520to%2520reduce%2520hallucinations%2520and%2520incorporate%250Aup-to-date%2520information%2520without%2520retraining.%2520As%2520an%2520essential%2520part%2520of%2520RAG%252C%250Aexternal%2520knowledge%2520bases%2520are%2520commonly%2520built%2520by%2520extracting%2520structured%2520data%2520from%250Aunstructured%2520PDF%2520documents%2520using%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529.%2520However%252C%250Agiven%2520the%2520imperfect%2520prediction%2520of%2520OCR%2520and%2520the%2520inherent%2520non-uniform%250Arepresentation%2520of%2520structured%2520data%252C%2520knowledge%2520bases%2520inevitably%2520contain%2520various%250AOCR%2520noises.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520OHRBench%252C%2520the%2520first%2520benchmark%2520for%250Aunderstanding%2520the%2520cascading%2520impact%2520of%2520OCR%2520on%2520RAG%2520systems.%2520OHRBench%2520includes%2520350%250Acarefully%2520selected%2520unstructured%2520PDF%2520documents%2520from%2520six%2520real-world%2520RAG%250Aapplication%2520domains%252C%2520along%2520with%2520Q%2526As%2520derived%2520from%2520multimodal%2520elements%2520in%250Adocuments%252C%2520challenging%2520existing%2520OCR%2520solutions%2520used%2520for%2520RAG%2520To%2520better%2520understand%250AOCR%2527s%2520impact%2520on%2520RAG%2520systems%252C%2520we%2520identify%2520two%2520primary%2520types%2520of%2520OCR%2520noise%253A%250ASemantic%2520Noise%2520and%2520Formatting%2520Noise%2520and%2520apply%2520perturbation%2520to%2520generate%2520a%2520set%2520of%250Astructured%2520data%2520with%2520varying%2520degrees%2520of%2520each%2520OCR%2520noise.%2520Using%2520OHRBench%252C%2520we%250Afirst%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520current%2520OCR%2520solutions%2520and%2520reveal%250Athat%2520none%2520is%2520competent%2520for%2520constructing%2520high-quality%2520knowledge%2520bases%2520for%2520RAG%250Asystems.%2520We%2520then%2520systematically%2520evaluate%2520the%2520impact%2520of%2520these%2520two%2520noise%2520types%250Aand%2520demonstrate%2520the%2520vulnerability%2520of%2520RAG%2520systems.%2520Furthermore%252C%2520we%2520discuss%2520the%250Apotential%2520of%2520employing%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520without%2520OCR%2520in%2520RAG%250Asystems.%2520Code%253A%2520https%253A//github.com/opendatalab/OHR-Bench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCR%20Hinders%20RAG%3A%20Evaluating%20the%20Cascading%20Impact%20of%20OCR%20on%0A%20%20Retrieval-Augmented%20Generation&entry.906535625=Junyuan%20Zhang%20and%20Qintong%20Zhang%20and%20Bin%20Wang%20and%20Linke%20Ouyang%20and%20Zichen%20Wen%20and%20Ying%20Li%20and%20Ka-Ho%20Chow%20and%20Conghui%20He%20and%20Wentao%20Zhang&entry.1292438233=%20%20Retrieval-augmented%20Generation%20%28RAG%29%20enhances%20Large%20Language%20Models%20%28LLMs%29%20by%0Aintegrating%20external%20knowledge%20to%20reduce%20hallucinations%20and%20incorporate%0Aup-to-date%20information%20without%20retraining.%20As%20an%20essential%20part%20of%20RAG%2C%0Aexternal%20knowledge%20bases%20are%20commonly%20built%20by%20extracting%20structured%20data%20from%0Aunstructured%20PDF%20documents%20using%20Optical%20Character%20Recognition%20%28OCR%29.%20However%2C%0Agiven%20the%20imperfect%20prediction%20of%20OCR%20and%20the%20inherent%20non-uniform%0Arepresentation%20of%20structured%20data%2C%20knowledge%20bases%20inevitably%20contain%20various%0AOCR%20noises.%20In%20this%20paper%2C%20we%20introduce%20OHRBench%2C%20the%20first%20benchmark%20for%0Aunderstanding%20the%20cascading%20impact%20of%20OCR%20on%20RAG%20systems.%20OHRBench%20includes%20350%0Acarefully%20selected%20unstructured%20PDF%20documents%20from%20six%20real-world%20RAG%0Aapplication%20domains%2C%20along%20with%20Q%26As%20derived%20from%20multimodal%20elements%20in%0Adocuments%2C%20challenging%20existing%20OCR%20solutions%20used%20for%20RAG%20To%20better%20understand%0AOCR%27s%20impact%20on%20RAG%20systems%2C%20we%20identify%20two%20primary%20types%20of%20OCR%20noise%3A%0ASemantic%20Noise%20and%20Formatting%20Noise%20and%20apply%20perturbation%20to%20generate%20a%20set%20of%0Astructured%20data%20with%20varying%20degrees%20of%20each%20OCR%20noise.%20Using%20OHRBench%2C%20we%0Afirst%20conduct%20a%20comprehensive%20evaluation%20of%20current%20OCR%20solutions%20and%20reveal%0Athat%20none%20is%20competent%20for%20constructing%20high-quality%20knowledge%20bases%20for%20RAG%0Asystems.%20We%20then%20systematically%20evaluate%20the%20impact%20of%20these%20two%20noise%20types%0Aand%20demonstrate%20the%20vulnerability%20of%20RAG%20systems.%20Furthermore%2C%20we%20discuss%20the%0Apotential%20of%20employing%20Vision-Language%20Models%20%28VLMs%29%20without%20OCR%20in%20RAG%0Asystems.%20Code%3A%20https%3A//github.com/opendatalab/OHR-Bench%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02592v1&entry.124074799=Read"},
{"title": "Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic\n  Gradients", "author": "Zakaria Patel and Sebastian J. Wetzel", "abstract": "  It has been demonstrated in many scientific fields that artificial neural\nnetworks like autoencoders or Siamese networks encode meaningful concepts in\ntheir latent spaces. However, there does not exist a comprehensive framework\nfor retrieving this information in a human-readable form without prior\nknowledge. In order to extract these concepts, we introduce a framework for\nfinding closed-form interpretations of neurons in latent spaces of artificial\nneural networks. The interpretation framework is based on embedding trained\nneural networks into an equivalence class of functions that encode the same\nconcept. We interpret these neural networks by finding an intersection between\nthe equivalence class and human-readable equations defined by a symbolic search\nspace. The approach is demonstrated by retrieving invariants of matrices and\nconserved quantities of dynamical systems from latent spaces of Siamese neural\nnetworks.\n", "link": "http://arxiv.org/abs/2409.05305v2", "date": "2024-12-03", "relevancy": 1.9719, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4926}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients&body=Title%3A%20Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients%0AAuthor%3A%20Zakaria%20Patel%20and%20Sebastian%20J.%20Wetzel%0AAbstract%3A%20%20%20It%20has%20been%20demonstrated%20in%20many%20scientific%20fields%20that%20artificial%20neural%0Anetworks%20like%20autoencoders%20or%20Siamese%20networks%20encode%20meaningful%20concepts%20in%0Atheir%20latent%20spaces.%20However%2C%20there%20does%20not%20exist%20a%20comprehensive%20framework%0Afor%20retrieving%20this%20information%20in%20a%20human-readable%20form%20without%20prior%0Aknowledge.%20In%20order%20to%20extract%20these%20concepts%2C%20we%20introduce%20a%20framework%20for%0Afinding%20closed-form%20interpretations%20of%20neurons%20in%20latent%20spaces%20of%20artificial%0Aneural%20networks.%20The%20interpretation%20framework%20is%20based%20on%20embedding%20trained%0Aneural%20networks%20into%20an%20equivalence%20class%20of%20functions%20that%20encode%20the%20same%0Aconcept.%20We%20interpret%20these%20neural%20networks%20by%20finding%20an%20intersection%20between%0Athe%20equivalence%20class%20and%20human-readable%20equations%20defined%20by%20a%20symbolic%20search%0Aspace.%20The%20approach%20is%20demonstrated%20by%20retrieving%20invariants%20of%20matrices%20and%0Aconserved%20quantities%20of%20dynamical%20systems%20from%20latent%20spaces%20of%20Siamese%20neural%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Form%2520Interpretation%2520of%2520Neural%2520Network%2520Latent%2520Spaces%2520with%2520Symbolic%250A%2520%2520Gradients%26entry.906535625%3DZakaria%2520Patel%2520and%2520Sebastian%2520J.%2520Wetzel%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520demonstrated%2520in%2520many%2520scientific%2520fields%2520that%2520artificial%2520neural%250Anetworks%2520like%2520autoencoders%2520or%2520Siamese%2520networks%2520encode%2520meaningful%2520concepts%2520in%250Atheir%2520latent%2520spaces.%2520However%252C%2520there%2520does%2520not%2520exist%2520a%2520comprehensive%2520framework%250Afor%2520retrieving%2520this%2520information%2520in%2520a%2520human-readable%2520form%2520without%2520prior%250Aknowledge.%2520In%2520order%2520to%2520extract%2520these%2520concepts%252C%2520we%2520introduce%2520a%2520framework%2520for%250Afinding%2520closed-form%2520interpretations%2520of%2520neurons%2520in%2520latent%2520spaces%2520of%2520artificial%250Aneural%2520networks.%2520The%2520interpretation%2520framework%2520is%2520based%2520on%2520embedding%2520trained%250Aneural%2520networks%2520into%2520an%2520equivalence%2520class%2520of%2520functions%2520that%2520encode%2520the%2520same%250Aconcept.%2520We%2520interpret%2520these%2520neural%2520networks%2520by%2520finding%2520an%2520intersection%2520between%250Athe%2520equivalence%2520class%2520and%2520human-readable%2520equations%2520defined%2520by%2520a%2520symbolic%2520search%250Aspace.%2520The%2520approach%2520is%2520demonstrated%2520by%2520retrieving%2520invariants%2520of%2520matrices%2520and%250Aconserved%2520quantities%2520of%2520dynamical%2520systems%2520from%2520latent%2520spaces%2520of%2520Siamese%2520neural%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients&entry.906535625=Zakaria%20Patel%20and%20Sebastian%20J.%20Wetzel&entry.1292438233=%20%20It%20has%20been%20demonstrated%20in%20many%20scientific%20fields%20that%20artificial%20neural%0Anetworks%20like%20autoencoders%20or%20Siamese%20networks%20encode%20meaningful%20concepts%20in%0Atheir%20latent%20spaces.%20However%2C%20there%20does%20not%20exist%20a%20comprehensive%20framework%0Afor%20retrieving%20this%20information%20in%20a%20human-readable%20form%20without%20prior%0Aknowledge.%20In%20order%20to%20extract%20these%20concepts%2C%20we%20introduce%20a%20framework%20for%0Afinding%20closed-form%20interpretations%20of%20neurons%20in%20latent%20spaces%20of%20artificial%0Aneural%20networks.%20The%20interpretation%20framework%20is%20based%20on%20embedding%20trained%0Aneural%20networks%20into%20an%20equivalence%20class%20of%20functions%20that%20encode%20the%20same%0Aconcept.%20We%20interpret%20these%20neural%20networks%20by%20finding%20an%20intersection%20between%0Athe%20equivalence%20class%20and%20human-readable%20equations%20defined%20by%20a%20symbolic%20search%0Aspace.%20The%20approach%20is%20demonstrated%20by%20retrieving%20invariants%20of%20matrices%20and%0Aconserved%20quantities%20of%20dynamical%20systems%20from%20latent%20spaces%20of%20Siamese%20neural%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05305v2&entry.124074799=Read"},
{"title": "Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic\n  Gradients", "author": "Zakaria Patel and Sebastian J. Wetzel", "abstract": "  It has been demonstrated in many scientific fields that artificial neural\nnetworks like autoencoders or Siamese networks encode meaningful concepts in\ntheir latent spaces. However, there does not exist a comprehensive framework\nfor retrieving this information in a human-readable form without prior\nknowledge. In order to extract these concepts, we introduce a framework for\nfinding closed-form interpretations of neurons in latent spaces of artificial\nneural networks. The interpretation framework is based on embedding trained\nneural networks into an equivalence class of functions that encode the same\nconcept. We interpret these neural networks by finding an intersection between\nthe equivalence class and human-readable equations defined by a symbolic search\nspace. The approach is demonstrated by retrieving invariants of matrices and\nconserved quantities of dynamical systems from latent spaces of Siamese neural\nnetworks.\n", "link": "http://arxiv.org/abs/2409.05305v2", "date": "2024-12-03", "relevancy": 1.9719, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4926}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients&body=Title%3A%20Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients%0AAuthor%3A%20Zakaria%20Patel%20and%20Sebastian%20J.%20Wetzel%0AAbstract%3A%20%20%20It%20has%20been%20demonstrated%20in%20many%20scientific%20fields%20that%20artificial%20neural%0Anetworks%20like%20autoencoders%20or%20Siamese%20networks%20encode%20meaningful%20concepts%20in%0Atheir%20latent%20spaces.%20However%2C%20there%20does%20not%20exist%20a%20comprehensive%20framework%0Afor%20retrieving%20this%20information%20in%20a%20human-readable%20form%20without%20prior%0Aknowledge.%20In%20order%20to%20extract%20these%20concepts%2C%20we%20introduce%20a%20framework%20for%0Afinding%20closed-form%20interpretations%20of%20neurons%20in%20latent%20spaces%20of%20artificial%0Aneural%20networks.%20The%20interpretation%20framework%20is%20based%20on%20embedding%20trained%0Aneural%20networks%20into%20an%20equivalence%20class%20of%20functions%20that%20encode%20the%20same%0Aconcept.%20We%20interpret%20these%20neural%20networks%20by%20finding%20an%20intersection%20between%0Athe%20equivalence%20class%20and%20human-readable%20equations%20defined%20by%20a%20symbolic%20search%0Aspace.%20The%20approach%20is%20demonstrated%20by%20retrieving%20invariants%20of%20matrices%20and%0Aconserved%20quantities%20of%20dynamical%20systems%20from%20latent%20spaces%20of%20Siamese%20neural%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Form%2520Interpretation%2520of%2520Neural%2520Network%2520Latent%2520Spaces%2520with%2520Symbolic%250A%2520%2520Gradients%26entry.906535625%3DZakaria%2520Patel%2520and%2520Sebastian%2520J.%2520Wetzel%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520demonstrated%2520in%2520many%2520scientific%2520fields%2520that%2520artificial%2520neural%250Anetworks%2520like%2520autoencoders%2520or%2520Siamese%2520networks%2520encode%2520meaningful%2520concepts%2520in%250Atheir%2520latent%2520spaces.%2520However%252C%2520there%2520does%2520not%2520exist%2520a%2520comprehensive%2520framework%250Afor%2520retrieving%2520this%2520information%2520in%2520a%2520human-readable%2520form%2520without%2520prior%250Aknowledge.%2520In%2520order%2520to%2520extract%2520these%2520concepts%252C%2520we%2520introduce%2520a%2520framework%2520for%250Afinding%2520closed-form%2520interpretations%2520of%2520neurons%2520in%2520latent%2520spaces%2520of%2520artificial%250Aneural%2520networks.%2520The%2520interpretation%2520framework%2520is%2520based%2520on%2520embedding%2520trained%250Aneural%2520networks%2520into%2520an%2520equivalence%2520class%2520of%2520functions%2520that%2520encode%2520the%2520same%250Aconcept.%2520We%2520interpret%2520these%2520neural%2520networks%2520by%2520finding%2520an%2520intersection%2520between%250Athe%2520equivalence%2520class%2520and%2520human-readable%2520equations%2520defined%2520by%2520a%2520symbolic%2520search%250Aspace.%2520The%2520approach%2520is%2520demonstrated%2520by%2520retrieving%2520invariants%2520of%2520matrices%2520and%250Aconserved%2520quantities%2520of%2520dynamical%2520systems%2520from%2520latent%2520spaces%2520of%2520Siamese%2520neural%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients&entry.906535625=Zakaria%20Patel%20and%20Sebastian%20J.%20Wetzel&entry.1292438233=%20%20It%20has%20been%20demonstrated%20in%20many%20scientific%20fields%20that%20artificial%20neural%0Anetworks%20like%20autoencoders%20or%20Siamese%20networks%20encode%20meaningful%20concepts%20in%0Atheir%20latent%20spaces.%20However%2C%20there%20does%20not%20exist%20a%20comprehensive%20framework%0Afor%20retrieving%20this%20information%20in%20a%20human-readable%20form%20without%20prior%0Aknowledge.%20In%20order%20to%20extract%20these%20concepts%2C%20we%20introduce%20a%20framework%20for%0Afinding%20closed-form%20interpretations%20of%20neurons%20in%20latent%20spaces%20of%20artificial%0Aneural%20networks.%20The%20interpretation%20framework%20is%20based%20on%20embedding%20trained%0Aneural%20networks%20into%20an%20equivalence%20class%20of%20functions%20that%20encode%20the%20same%0Aconcept.%20We%20interpret%20these%20neural%20networks%20by%20finding%20an%20intersection%20between%0Athe%20equivalence%20class%20and%20human-readable%20equations%20defined%20by%20a%20symbolic%20search%0Aspace.%20The%20approach%20is%20demonstrated%20by%20retrieving%20invariants%20of%20matrices%20and%0Aconserved%20quantities%20of%20dynamical%20systems%20from%20latent%20spaces%20of%20Siamese%20neural%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05305v2&entry.124074799=Read"},
{"title": "Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic\n  Gradients", "author": "Zakaria Patel and Sebastian J. Wetzel", "abstract": "  It has been demonstrated in many scientific fields that artificial neural\nnetworks like autoencoders or Siamese networks encode meaningful concepts in\ntheir latent spaces. However, there does not exist a comprehensive framework\nfor retrieving this information in a human-readable form without prior\nknowledge. In order to extract these concepts, we introduce a framework for\nfinding closed-form interpretations of neurons in latent spaces of artificial\nneural networks. The interpretation framework is based on embedding trained\nneural networks into an equivalence class of functions that encode the same\nconcept. We interpret these neural networks by finding an intersection between\nthe equivalence class and human-readable equations defined by a symbolic search\nspace. The approach is demonstrated by retrieving invariants of matrices and\nconserved quantities of dynamical systems from latent spaces of Siamese neural\nnetworks.\n", "link": "http://arxiv.org/abs/2409.05305v2", "date": "2024-12-03", "relevancy": 1.9717, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4925}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients&body=Title%3A%20Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients%0AAuthor%3A%20Zakaria%20Patel%20and%20Sebastian%20J.%20Wetzel%0AAbstract%3A%20%20%20It%20has%20been%20demonstrated%20in%20many%20scientific%20fields%20that%20artificial%20neural%0Anetworks%20like%20autoencoders%20or%20Siamese%20networks%20encode%20meaningful%20concepts%20in%0Atheir%20latent%20spaces.%20However%2C%20there%20does%20not%20exist%20a%20comprehensive%20framework%0Afor%20retrieving%20this%20information%20in%20a%20human-readable%20form%20without%20prior%0Aknowledge.%20In%20order%20to%20extract%20these%20concepts%2C%20we%20introduce%20a%20framework%20for%0Afinding%20closed-form%20interpretations%20of%20neurons%20in%20latent%20spaces%20of%20artificial%0Aneural%20networks.%20The%20interpretation%20framework%20is%20based%20on%20embedding%20trained%0Aneural%20networks%20into%20an%20equivalence%20class%20of%20functions%20that%20encode%20the%20same%0Aconcept.%20We%20interpret%20these%20neural%20networks%20by%20finding%20an%20intersection%20between%0Athe%20equivalence%20class%20and%20human-readable%20equations%20defined%20by%20a%20symbolic%20search%0Aspace.%20The%20approach%20is%20demonstrated%20by%20retrieving%20invariants%20of%20matrices%20and%0Aconserved%20quantities%20of%20dynamical%20systems%20from%20latent%20spaces%20of%20Siamese%20neural%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Form%2520Interpretation%2520of%2520Neural%2520Network%2520Latent%2520Spaces%2520with%2520Symbolic%250A%2520%2520Gradients%26entry.906535625%3DZakaria%2520Patel%2520and%2520Sebastian%2520J.%2520Wetzel%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520demonstrated%2520in%2520many%2520scientific%2520fields%2520that%2520artificial%2520neural%250Anetworks%2520like%2520autoencoders%2520or%2520Siamese%2520networks%2520encode%2520meaningful%2520concepts%2520in%250Atheir%2520latent%2520spaces.%2520However%252C%2520there%2520does%2520not%2520exist%2520a%2520comprehensive%2520framework%250Afor%2520retrieving%2520this%2520information%2520in%2520a%2520human-readable%2520form%2520without%2520prior%250Aknowledge.%2520In%2520order%2520to%2520extract%2520these%2520concepts%252C%2520we%2520introduce%2520a%2520framework%2520for%250Afinding%2520closed-form%2520interpretations%2520of%2520neurons%2520in%2520latent%2520spaces%2520of%2520artificial%250Aneural%2520networks.%2520The%2520interpretation%2520framework%2520is%2520based%2520on%2520embedding%2520trained%250Aneural%2520networks%2520into%2520an%2520equivalence%2520class%2520of%2520functions%2520that%2520encode%2520the%2520same%250Aconcept.%2520We%2520interpret%2520these%2520neural%2520networks%2520by%2520finding%2520an%2520intersection%2520between%250Athe%2520equivalence%2520class%2520and%2520human-readable%2520equations%2520defined%2520by%2520a%2520symbolic%2520search%250Aspace.%2520The%2520approach%2520is%2520demonstrated%2520by%2520retrieving%2520invariants%2520of%2520matrices%2520and%250Aconserved%2520quantities%2520of%2520dynamical%2520systems%2520from%2520latent%2520spaces%2520of%2520Siamese%2520neural%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Form%20Interpretation%20of%20Neural%20Network%20Latent%20Spaces%20with%20Symbolic%0A%20%20Gradients&entry.906535625=Zakaria%20Patel%20and%20Sebastian%20J.%20Wetzel&entry.1292438233=%20%20It%20has%20been%20demonstrated%20in%20many%20scientific%20fields%20that%20artificial%20neural%0Anetworks%20like%20autoencoders%20or%20Siamese%20networks%20encode%20meaningful%20concepts%20in%0Atheir%20latent%20spaces.%20However%2C%20there%20does%20not%20exist%20a%20comprehensive%20framework%0Afor%20retrieving%20this%20information%20in%20a%20human-readable%20form%20without%20prior%0Aknowledge.%20In%20order%20to%20extract%20these%20concepts%2C%20we%20introduce%20a%20framework%20for%0Afinding%20closed-form%20interpretations%20of%20neurons%20in%20latent%20spaces%20of%20artificial%0Aneural%20networks.%20The%20interpretation%20framework%20is%20based%20on%20embedding%20trained%0Aneural%20networks%20into%20an%20equivalence%20class%20of%20functions%20that%20encode%20the%20same%0Aconcept.%20We%20interpret%20these%20neural%20networks%20by%20finding%20an%20intersection%20between%0Athe%20equivalence%20class%20and%20human-readable%20equations%20defined%20by%20a%20symbolic%20search%0Aspace.%20The%20approach%20is%20demonstrated%20by%20retrieving%20invariants%20of%20matrices%20and%0Aconserved%20quantities%20of%20dynamical%20systems%20from%20latent%20spaces%20of%20Siamese%20neural%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05305v2&entry.124074799=Read"},
{"title": "Exploring the hierarchical structure of human plans via program\n  generation", "author": "Carlos G. Correa and Sophia Sanborn and Mark K. Ho and Frederick Callaway and Nathaniel D. Daw and Thomas L. Griffiths", "abstract": "  Human behavior is often assumed to be hierarchically structured, made up of\nabstract actions that can be decomposed into concrete actions. However,\nbehavior is typically measured as a sequence of actions, which makes it\ndifficult to infer its hierarchical structure. In this paper, we explore how\npeople form hierarchically structured plans, using an experimental paradigm\nwith observable hierarchical representations: participants create programs that\nproduce sequences of actions in a language with explicit hierarchical\nstructure. This task lets us test two well-established principles of human\nbehavior: utility maximization (i.e. using fewer actions) and minimum\ndescription length (MDL; i.e. having a shorter program). We find that humans\nare sensitive to both metrics, but that both accounts fail to predict a\nqualitative feature of human-created programs, namely that people prefer\nprograms with reuse over and above the predictions of MDL. We formalize this\npreference for reuse by extending the MDL account into a generative model over\nprograms, modeling hierarchy choice as the induction of a grammar over actions.\nOur account can explain the preference for reuse and provides better\npredictions of human behavior, going beyond simple accounts of compressibility\nto highlight a principle that guides hierarchical planning.\n", "link": "http://arxiv.org/abs/2311.18644v2", "date": "2024-12-03", "relevancy": 1.9652, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5416}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20hierarchical%20structure%20of%20human%20plans%20via%20program%0A%20%20generation&body=Title%3A%20Exploring%20the%20hierarchical%20structure%20of%20human%20plans%20via%20program%0A%20%20generation%0AAuthor%3A%20Carlos%20G.%20Correa%20and%20Sophia%20Sanborn%20and%20Mark%20K.%20Ho%20and%20Frederick%20Callaway%20and%20Nathaniel%20D.%20Daw%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Human%20behavior%20is%20often%20assumed%20to%20be%20hierarchically%20structured%2C%20made%20up%20of%0Aabstract%20actions%20that%20can%20be%20decomposed%20into%20concrete%20actions.%20However%2C%0Abehavior%20is%20typically%20measured%20as%20a%20sequence%20of%20actions%2C%20which%20makes%20it%0Adifficult%20to%20infer%20its%20hierarchical%20structure.%20In%20this%20paper%2C%20we%20explore%20how%0Apeople%20form%20hierarchically%20structured%20plans%2C%20using%20an%20experimental%20paradigm%0Awith%20observable%20hierarchical%20representations%3A%20participants%20create%20programs%20that%0Aproduce%20sequences%20of%20actions%20in%20a%20language%20with%20explicit%20hierarchical%0Astructure.%20This%20task%20lets%20us%20test%20two%20well-established%20principles%20of%20human%0Abehavior%3A%20utility%20maximization%20%28i.e.%20using%20fewer%20actions%29%20and%20minimum%0Adescription%20length%20%28MDL%3B%20i.e.%20having%20a%20shorter%20program%29.%20We%20find%20that%20humans%0Aare%20sensitive%20to%20both%20metrics%2C%20but%20that%20both%20accounts%20fail%20to%20predict%20a%0Aqualitative%20feature%20of%20human-created%20programs%2C%20namely%20that%20people%20prefer%0Aprograms%20with%20reuse%20over%20and%20above%20the%20predictions%20of%20MDL.%20We%20formalize%20this%0Apreference%20for%20reuse%20by%20extending%20the%20MDL%20account%20into%20a%20generative%20model%20over%0Aprograms%2C%20modeling%20hierarchy%20choice%20as%20the%20induction%20of%20a%20grammar%20over%20actions.%0AOur%20account%20can%20explain%20the%20preference%20for%20reuse%20and%20provides%20better%0Apredictions%20of%20human%20behavior%2C%20going%20beyond%20simple%20accounts%20of%20compressibility%0Ato%20highlight%20a%20principle%20that%20guides%20hierarchical%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520hierarchical%2520structure%2520of%2520human%2520plans%2520via%2520program%250A%2520%2520generation%26entry.906535625%3DCarlos%2520G.%2520Correa%2520and%2520Sophia%2520Sanborn%2520and%2520Mark%2520K.%2520Ho%2520and%2520Frederick%2520Callaway%2520and%2520Nathaniel%2520D.%2520Daw%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Human%2520behavior%2520is%2520often%2520assumed%2520to%2520be%2520hierarchically%2520structured%252C%2520made%2520up%2520of%250Aabstract%2520actions%2520that%2520can%2520be%2520decomposed%2520into%2520concrete%2520actions.%2520However%252C%250Abehavior%2520is%2520typically%2520measured%2520as%2520a%2520sequence%2520of%2520actions%252C%2520which%2520makes%2520it%250Adifficult%2520to%2520infer%2520its%2520hierarchical%2520structure.%2520In%2520this%2520paper%252C%2520we%2520explore%2520how%250Apeople%2520form%2520hierarchically%2520structured%2520plans%252C%2520using%2520an%2520experimental%2520paradigm%250Awith%2520observable%2520hierarchical%2520representations%253A%2520participants%2520create%2520programs%2520that%250Aproduce%2520sequences%2520of%2520actions%2520in%2520a%2520language%2520with%2520explicit%2520hierarchical%250Astructure.%2520This%2520task%2520lets%2520us%2520test%2520two%2520well-established%2520principles%2520of%2520human%250Abehavior%253A%2520utility%2520maximization%2520%2528i.e.%2520using%2520fewer%2520actions%2529%2520and%2520minimum%250Adescription%2520length%2520%2528MDL%253B%2520i.e.%2520having%2520a%2520shorter%2520program%2529.%2520We%2520find%2520that%2520humans%250Aare%2520sensitive%2520to%2520both%2520metrics%252C%2520but%2520that%2520both%2520accounts%2520fail%2520to%2520predict%2520a%250Aqualitative%2520feature%2520of%2520human-created%2520programs%252C%2520namely%2520that%2520people%2520prefer%250Aprograms%2520with%2520reuse%2520over%2520and%2520above%2520the%2520predictions%2520of%2520MDL.%2520We%2520formalize%2520this%250Apreference%2520for%2520reuse%2520by%2520extending%2520the%2520MDL%2520account%2520into%2520a%2520generative%2520model%2520over%250Aprograms%252C%2520modeling%2520hierarchy%2520choice%2520as%2520the%2520induction%2520of%2520a%2520grammar%2520over%2520actions.%250AOur%2520account%2520can%2520explain%2520the%2520preference%2520for%2520reuse%2520and%2520provides%2520better%250Apredictions%2520of%2520human%2520behavior%252C%2520going%2520beyond%2520simple%2520accounts%2520of%2520compressibility%250Ato%2520highlight%2520a%2520principle%2520that%2520guides%2520hierarchical%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20hierarchical%20structure%20of%20human%20plans%20via%20program%0A%20%20generation&entry.906535625=Carlos%20G.%20Correa%20and%20Sophia%20Sanborn%20and%20Mark%20K.%20Ho%20and%20Frederick%20Callaway%20and%20Nathaniel%20D.%20Daw%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Human%20behavior%20is%20often%20assumed%20to%20be%20hierarchically%20structured%2C%20made%20up%20of%0Aabstract%20actions%20that%20can%20be%20decomposed%20into%20concrete%20actions.%20However%2C%0Abehavior%20is%20typically%20measured%20as%20a%20sequence%20of%20actions%2C%20which%20makes%20it%0Adifficult%20to%20infer%20its%20hierarchical%20structure.%20In%20this%20paper%2C%20we%20explore%20how%0Apeople%20form%20hierarchically%20structured%20plans%2C%20using%20an%20experimental%20paradigm%0Awith%20observable%20hierarchical%20representations%3A%20participants%20create%20programs%20that%0Aproduce%20sequences%20of%20actions%20in%20a%20language%20with%20explicit%20hierarchical%0Astructure.%20This%20task%20lets%20us%20test%20two%20well-established%20principles%20of%20human%0Abehavior%3A%20utility%20maximization%20%28i.e.%20using%20fewer%20actions%29%20and%20minimum%0Adescription%20length%20%28MDL%3B%20i.e.%20having%20a%20shorter%20program%29.%20We%20find%20that%20humans%0Aare%20sensitive%20to%20both%20metrics%2C%20but%20that%20both%20accounts%20fail%20to%20predict%20a%0Aqualitative%20feature%20of%20human-created%20programs%2C%20namely%20that%20people%20prefer%0Aprograms%20with%20reuse%20over%20and%20above%20the%20predictions%20of%20MDL.%20We%20formalize%20this%0Apreference%20for%20reuse%20by%20extending%20the%20MDL%20account%20into%20a%20generative%20model%20over%0Aprograms%2C%20modeling%20hierarchy%20choice%20as%20the%20induction%20of%20a%20grammar%20over%20actions.%0AOur%20account%20can%20explain%20the%20preference%20for%20reuse%20and%20provides%20better%0Apredictions%20of%20human%20behavior%2C%20going%20beyond%20simple%20accounts%20of%20compressibility%0Ato%20highlight%20a%20principle%20that%20guides%20hierarchical%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18644v2&entry.124074799=Read"},
{"title": "Exploring the hierarchical structure of human plans via program\n  generation", "author": "Carlos G. Correa and Sophia Sanborn and Mark K. Ho and Frederick Callaway and Nathaniel D. Daw and Thomas L. Griffiths", "abstract": "  Human behavior is often assumed to be hierarchically structured, made up of\nabstract actions that can be decomposed into concrete actions. However,\nbehavior is typically measured as a sequence of actions, which makes it\ndifficult to infer its hierarchical structure. In this paper, we explore how\npeople form hierarchically structured plans, using an experimental paradigm\nwith observable hierarchical representations: participants create programs that\nproduce sequences of actions in a language with explicit hierarchical\nstructure. This task lets us test two well-established principles of human\nbehavior: utility maximization (i.e. using fewer actions) and minimum\ndescription length (MDL; i.e. having a shorter program). We find that humans\nare sensitive to both metrics, but that both accounts fail to predict a\nqualitative feature of human-created programs, namely that people prefer\nprograms with reuse over and above the predictions of MDL. We formalize this\npreference for reuse by extending the MDL account into a generative model over\nprograms, modeling hierarchy choice as the induction of a grammar over actions.\nOur account can explain the preference for reuse and provides better\npredictions of human behavior, going beyond simple accounts of compressibility\nto highlight a principle that guides hierarchical planning.\n", "link": "http://arxiv.org/abs/2311.18644v2", "date": "2024-12-03", "relevancy": 1.9652, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5416}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20hierarchical%20structure%20of%20human%20plans%20via%20program%0A%20%20generation&body=Title%3A%20Exploring%20the%20hierarchical%20structure%20of%20human%20plans%20via%20program%0A%20%20generation%0AAuthor%3A%20Carlos%20G.%20Correa%20and%20Sophia%20Sanborn%20and%20Mark%20K.%20Ho%20and%20Frederick%20Callaway%20and%20Nathaniel%20D.%20Daw%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Human%20behavior%20is%20often%20assumed%20to%20be%20hierarchically%20structured%2C%20made%20up%20of%0Aabstract%20actions%20that%20can%20be%20decomposed%20into%20concrete%20actions.%20However%2C%0Abehavior%20is%20typically%20measured%20as%20a%20sequence%20of%20actions%2C%20which%20makes%20it%0Adifficult%20to%20infer%20its%20hierarchical%20structure.%20In%20this%20paper%2C%20we%20explore%20how%0Apeople%20form%20hierarchically%20structured%20plans%2C%20using%20an%20experimental%20paradigm%0Awith%20observable%20hierarchical%20representations%3A%20participants%20create%20programs%20that%0Aproduce%20sequences%20of%20actions%20in%20a%20language%20with%20explicit%20hierarchical%0Astructure.%20This%20task%20lets%20us%20test%20two%20well-established%20principles%20of%20human%0Abehavior%3A%20utility%20maximization%20%28i.e.%20using%20fewer%20actions%29%20and%20minimum%0Adescription%20length%20%28MDL%3B%20i.e.%20having%20a%20shorter%20program%29.%20We%20find%20that%20humans%0Aare%20sensitive%20to%20both%20metrics%2C%20but%20that%20both%20accounts%20fail%20to%20predict%20a%0Aqualitative%20feature%20of%20human-created%20programs%2C%20namely%20that%20people%20prefer%0Aprograms%20with%20reuse%20over%20and%20above%20the%20predictions%20of%20MDL.%20We%20formalize%20this%0Apreference%20for%20reuse%20by%20extending%20the%20MDL%20account%20into%20a%20generative%20model%20over%0Aprograms%2C%20modeling%20hierarchy%20choice%20as%20the%20induction%20of%20a%20grammar%20over%20actions.%0AOur%20account%20can%20explain%20the%20preference%20for%20reuse%20and%20provides%20better%0Apredictions%20of%20human%20behavior%2C%20going%20beyond%20simple%20accounts%20of%20compressibility%0Ato%20highlight%20a%20principle%20that%20guides%20hierarchical%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520hierarchical%2520structure%2520of%2520human%2520plans%2520via%2520program%250A%2520%2520generation%26entry.906535625%3DCarlos%2520G.%2520Correa%2520and%2520Sophia%2520Sanborn%2520and%2520Mark%2520K.%2520Ho%2520and%2520Frederick%2520Callaway%2520and%2520Nathaniel%2520D.%2520Daw%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Human%2520behavior%2520is%2520often%2520assumed%2520to%2520be%2520hierarchically%2520structured%252C%2520made%2520up%2520of%250Aabstract%2520actions%2520that%2520can%2520be%2520decomposed%2520into%2520concrete%2520actions.%2520However%252C%250Abehavior%2520is%2520typically%2520measured%2520as%2520a%2520sequence%2520of%2520actions%252C%2520which%2520makes%2520it%250Adifficult%2520to%2520infer%2520its%2520hierarchical%2520structure.%2520In%2520this%2520paper%252C%2520we%2520explore%2520how%250Apeople%2520form%2520hierarchically%2520structured%2520plans%252C%2520using%2520an%2520experimental%2520paradigm%250Awith%2520observable%2520hierarchical%2520representations%253A%2520participants%2520create%2520programs%2520that%250Aproduce%2520sequences%2520of%2520actions%2520in%2520a%2520language%2520with%2520explicit%2520hierarchical%250Astructure.%2520This%2520task%2520lets%2520us%2520test%2520two%2520well-established%2520principles%2520of%2520human%250Abehavior%253A%2520utility%2520maximization%2520%2528i.e.%2520using%2520fewer%2520actions%2529%2520and%2520minimum%250Adescription%2520length%2520%2528MDL%253B%2520i.e.%2520having%2520a%2520shorter%2520program%2529.%2520We%2520find%2520that%2520humans%250Aare%2520sensitive%2520to%2520both%2520metrics%252C%2520but%2520that%2520both%2520accounts%2520fail%2520to%2520predict%2520a%250Aqualitative%2520feature%2520of%2520human-created%2520programs%252C%2520namely%2520that%2520people%2520prefer%250Aprograms%2520with%2520reuse%2520over%2520and%2520above%2520the%2520predictions%2520of%2520MDL.%2520We%2520formalize%2520this%250Apreference%2520for%2520reuse%2520by%2520extending%2520the%2520MDL%2520account%2520into%2520a%2520generative%2520model%2520over%250Aprograms%252C%2520modeling%2520hierarchy%2520choice%2520as%2520the%2520induction%2520of%2520a%2520grammar%2520over%2520actions.%250AOur%2520account%2520can%2520explain%2520the%2520preference%2520for%2520reuse%2520and%2520provides%2520better%250Apredictions%2520of%2520human%2520behavior%252C%2520going%2520beyond%2520simple%2520accounts%2520of%2520compressibility%250Ato%2520highlight%2520a%2520principle%2520that%2520guides%2520hierarchical%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20hierarchical%20structure%20of%20human%20plans%20via%20program%0A%20%20generation&entry.906535625=Carlos%20G.%20Correa%20and%20Sophia%20Sanborn%20and%20Mark%20K.%20Ho%20and%20Frederick%20Callaway%20and%20Nathaniel%20D.%20Daw%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Human%20behavior%20is%20often%20assumed%20to%20be%20hierarchically%20structured%2C%20made%20up%20of%0Aabstract%20actions%20that%20can%20be%20decomposed%20into%20concrete%20actions.%20However%2C%0Abehavior%20is%20typically%20measured%20as%20a%20sequence%20of%20actions%2C%20which%20makes%20it%0Adifficult%20to%20infer%20its%20hierarchical%20structure.%20In%20this%20paper%2C%20we%20explore%20how%0Apeople%20form%20hierarchically%20structured%20plans%2C%20using%20an%20experimental%20paradigm%0Awith%20observable%20hierarchical%20representations%3A%20participants%20create%20programs%20that%0Aproduce%20sequences%20of%20actions%20in%20a%20language%20with%20explicit%20hierarchical%0Astructure.%20This%20task%20lets%20us%20test%20two%20well-established%20principles%20of%20human%0Abehavior%3A%20utility%20maximization%20%28i.e.%20using%20fewer%20actions%29%20and%20minimum%0Adescription%20length%20%28MDL%3B%20i.e.%20having%20a%20shorter%20program%29.%20We%20find%20that%20humans%0Aare%20sensitive%20to%20both%20metrics%2C%20but%20that%20both%20accounts%20fail%20to%20predict%20a%0Aqualitative%20feature%20of%20human-created%20programs%2C%20namely%20that%20people%20prefer%0Aprograms%20with%20reuse%20over%20and%20above%20the%20predictions%20of%20MDL.%20We%20formalize%20this%0Apreference%20for%20reuse%20by%20extending%20the%20MDL%20account%20into%20a%20generative%20model%20over%0Aprograms%2C%20modeling%20hierarchy%20choice%20as%20the%20induction%20of%20a%20grammar%20over%20actions.%0AOur%20account%20can%20explain%20the%20preference%20for%20reuse%20and%20provides%20better%0Apredictions%20of%20human%20behavior%2C%20going%20beyond%20simple%20accounts%20of%20compressibility%0Ato%20highlight%20a%20principle%20that%20guides%20hierarchical%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18644v2&entry.124074799=Read"},
{"title": "EnrichEvent: Enriching Social Data with Contextual Information for\n  Emerging Event Extraction", "author": "Mohammadali Sefidi Esfahani and Mohammad Akbari", "abstract": "  Social platforms have emerged as crucial platforms for disseminating\ninformation and discussing real-life social events, offering researchers an\nexcellent opportunity to design and implement novel event detection frameworks.\nHowever, most existing approaches only exploit keyword burstiness or network\nstructures to detect unspecified events. Thus, they often need help identifying\nunknown events regarding the challenging nature of events and social data.\nSocial data, e.g., tweets, is characterized by misspellings, incompleteness,\nword sense ambiguation, irregular language, and variation in aspects of\nopinions. Moreover, extracting discriminative features and patterns for\nevolving events by exploiting the limited structural knowledge is almost\ninfeasible. To address these challenges, in this paper, we propose a novel\nframework, namely EnrichEvent, that leverages the linguistic and contextual\nrepresentations of streaming social data. In particular, we leverage contextual\nand linguistic knowledge to detect semantically related tweets and enhance the\neffectiveness of the event detection approaches. Eventually, our proposed\nframework produces cluster chains for each event to show the evolving variation\nof the event through time. We conducted extensive experiments to evaluate our\nframework, validating its high performance and effectiveness in detecting and\ndistinguishing unspecified social events.\n", "link": "http://arxiv.org/abs/2307.16082v6", "date": "2024-12-03", "relevancy": 1.9554, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnrichEvent%3A%20Enriching%20Social%20Data%20with%20Contextual%20Information%20for%0A%20%20Emerging%20Event%20Extraction&body=Title%3A%20EnrichEvent%3A%20Enriching%20Social%20Data%20with%20Contextual%20Information%20for%0A%20%20Emerging%20Event%20Extraction%0AAuthor%3A%20Mohammadali%20Sefidi%20Esfahani%20and%20Mohammad%20Akbari%0AAbstract%3A%20%20%20Social%20platforms%20have%20emerged%20as%20crucial%20platforms%20for%20disseminating%0Ainformation%20and%20discussing%20real-life%20social%20events%2C%20offering%20researchers%20an%0Aexcellent%20opportunity%20to%20design%20and%20implement%20novel%20event%20detection%20frameworks.%0AHowever%2C%20most%20existing%20approaches%20only%20exploit%20keyword%20burstiness%20or%20network%0Astructures%20to%20detect%20unspecified%20events.%20Thus%2C%20they%20often%20need%20help%20identifying%0Aunknown%20events%20regarding%20the%20challenging%20nature%20of%20events%20and%20social%20data.%0ASocial%20data%2C%20e.g.%2C%20tweets%2C%20is%20characterized%20by%20misspellings%2C%20incompleteness%2C%0Aword%20sense%20ambiguation%2C%20irregular%20language%2C%20and%20variation%20in%20aspects%20of%0Aopinions.%20Moreover%2C%20extracting%20discriminative%20features%20and%20patterns%20for%0Aevolving%20events%20by%20exploiting%20the%20limited%20structural%20knowledge%20is%20almost%0Ainfeasible.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20namely%20EnrichEvent%2C%20that%20leverages%20the%20linguistic%20and%20contextual%0Arepresentations%20of%20streaming%20social%20data.%20In%20particular%2C%20we%20leverage%20contextual%0Aand%20linguistic%20knowledge%20to%20detect%20semantically%20related%20tweets%20and%20enhance%20the%0Aeffectiveness%20of%20the%20event%20detection%20approaches.%20Eventually%2C%20our%20proposed%0Aframework%20produces%20cluster%20chains%20for%20each%20event%20to%20show%20the%20evolving%20variation%0Aof%20the%20event%20through%20time.%20We%20conducted%20extensive%20experiments%20to%20evaluate%20our%0Aframework%2C%20validating%20its%20high%20performance%20and%20effectiveness%20in%20detecting%20and%0Adistinguishing%20unspecified%20social%20events.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16082v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnrichEvent%253A%2520Enriching%2520Social%2520Data%2520with%2520Contextual%2520Information%2520for%250A%2520%2520Emerging%2520Event%2520Extraction%26entry.906535625%3DMohammadali%2520Sefidi%2520Esfahani%2520and%2520Mohammad%2520Akbari%26entry.1292438233%3D%2520%2520Social%2520platforms%2520have%2520emerged%2520as%2520crucial%2520platforms%2520for%2520disseminating%250Ainformation%2520and%2520discussing%2520real-life%2520social%2520events%252C%2520offering%2520researchers%2520an%250Aexcellent%2520opportunity%2520to%2520design%2520and%2520implement%2520novel%2520event%2520detection%2520frameworks.%250AHowever%252C%2520most%2520existing%2520approaches%2520only%2520exploit%2520keyword%2520burstiness%2520or%2520network%250Astructures%2520to%2520detect%2520unspecified%2520events.%2520Thus%252C%2520they%2520often%2520need%2520help%2520identifying%250Aunknown%2520events%2520regarding%2520the%2520challenging%2520nature%2520of%2520events%2520and%2520social%2520data.%250ASocial%2520data%252C%2520e.g.%252C%2520tweets%252C%2520is%2520characterized%2520by%2520misspellings%252C%2520incompleteness%252C%250Aword%2520sense%2520ambiguation%252C%2520irregular%2520language%252C%2520and%2520variation%2520in%2520aspects%2520of%250Aopinions.%2520Moreover%252C%2520extracting%2520discriminative%2520features%2520and%2520patterns%2520for%250Aevolving%2520events%2520by%2520exploiting%2520the%2520limited%2520structural%2520knowledge%2520is%2520almost%250Ainfeasible.%2520To%2520address%2520these%2520challenges%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aframework%252C%2520namely%2520EnrichEvent%252C%2520that%2520leverages%2520the%2520linguistic%2520and%2520contextual%250Arepresentations%2520of%2520streaming%2520social%2520data.%2520In%2520particular%252C%2520we%2520leverage%2520contextual%250Aand%2520linguistic%2520knowledge%2520to%2520detect%2520semantically%2520related%2520tweets%2520and%2520enhance%2520the%250Aeffectiveness%2520of%2520the%2520event%2520detection%2520approaches.%2520Eventually%252C%2520our%2520proposed%250Aframework%2520produces%2520cluster%2520chains%2520for%2520each%2520event%2520to%2520show%2520the%2520evolving%2520variation%250Aof%2520the%2520event%2520through%2520time.%2520We%2520conducted%2520extensive%2520experiments%2520to%2520evaluate%2520our%250Aframework%252C%2520validating%2520its%2520high%2520performance%2520and%2520effectiveness%2520in%2520detecting%2520and%250Adistinguishing%2520unspecified%2520social%2520events.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16082v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnrichEvent%3A%20Enriching%20Social%20Data%20with%20Contextual%20Information%20for%0A%20%20Emerging%20Event%20Extraction&entry.906535625=Mohammadali%20Sefidi%20Esfahani%20and%20Mohammad%20Akbari&entry.1292438233=%20%20Social%20platforms%20have%20emerged%20as%20crucial%20platforms%20for%20disseminating%0Ainformation%20and%20discussing%20real-life%20social%20events%2C%20offering%20researchers%20an%0Aexcellent%20opportunity%20to%20design%20and%20implement%20novel%20event%20detection%20frameworks.%0AHowever%2C%20most%20existing%20approaches%20only%20exploit%20keyword%20burstiness%20or%20network%0Astructures%20to%20detect%20unspecified%20events.%20Thus%2C%20they%20often%20need%20help%20identifying%0Aunknown%20events%20regarding%20the%20challenging%20nature%20of%20events%20and%20social%20data.%0ASocial%20data%2C%20e.g.%2C%20tweets%2C%20is%20characterized%20by%20misspellings%2C%20incompleteness%2C%0Aword%20sense%20ambiguation%2C%20irregular%20language%2C%20and%20variation%20in%20aspects%20of%0Aopinions.%20Moreover%2C%20extracting%20discriminative%20features%20and%20patterns%20for%0Aevolving%20events%20by%20exploiting%20the%20limited%20structural%20knowledge%20is%20almost%0Ainfeasible.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20namely%20EnrichEvent%2C%20that%20leverages%20the%20linguistic%20and%20contextual%0Arepresentations%20of%20streaming%20social%20data.%20In%20particular%2C%20we%20leverage%20contextual%0Aand%20linguistic%20knowledge%20to%20detect%20semantically%20related%20tweets%20and%20enhance%20the%0Aeffectiveness%20of%20the%20event%20detection%20approaches.%20Eventually%2C%20our%20proposed%0Aframework%20produces%20cluster%20chains%20for%20each%20event%20to%20show%20the%20evolving%20variation%0Aof%20the%20event%20through%20time.%20We%20conducted%20extensive%20experiments%20to%20evaluate%20our%0Aframework%2C%20validating%20its%20high%20performance%20and%20effectiveness%20in%20detecting%20and%0Adistinguishing%20unspecified%20social%20events.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16082v6&entry.124074799=Read"},
{"title": "Projection Abstractions in Planning Under the Lenses of Abstractions for\n  MDPs", "author": "Giuseppe Canonaco and Alberto Pozanco and Daniel Borrajo", "abstract": "  The concept of abstraction has been independently developed both in the\ncontext of AI Planning and discounted Markov Decision Processes (MDPs).\nHowever, the way abstractions are built and used in the context of Planning and\nMDPs is different even though lots of commonalities can be highlighted. To this\nday there is no work trying to relate and unify the two fields on the matter of\nabstractions unraveling all the different assumptions and their effect on the\nway they can be used. Therefore, in this paper we aim to do so by looking at\nprojection abstractions in Planning through the lenses of discounted MDPs.\nStarting from a projection abstraction built according to Classical or\nProbabilistic Planning techniques, we will show how the same abstraction can be\nobtained under the abstraction frameworks available for discounted MDPs. Along\nthe way, we will focus on computational as well as representational advantages\nand disadvantages of both worlds pointing out new research directions that are\nof interest for both fields.\n", "link": "http://arxiv.org/abs/2412.02615v1", "date": "2024-12-03", "relevancy": 1.4137, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4776}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Projection%20Abstractions%20in%20Planning%20Under%20the%20Lenses%20of%20Abstractions%20for%0A%20%20MDPs&body=Title%3A%20Projection%20Abstractions%20in%20Planning%20Under%20the%20Lenses%20of%20Abstractions%20for%0A%20%20MDPs%0AAuthor%3A%20Giuseppe%20Canonaco%20and%20Alberto%20Pozanco%20and%20Daniel%20Borrajo%0AAbstract%3A%20%20%20The%20concept%20of%20abstraction%20has%20been%20independently%20developed%20both%20in%20the%0Acontext%20of%20AI%20Planning%20and%20discounted%20Markov%20Decision%20Processes%20%28MDPs%29.%0AHowever%2C%20the%20way%20abstractions%20are%20built%20and%20used%20in%20the%20context%20of%20Planning%20and%0AMDPs%20is%20different%20even%20though%20lots%20of%20commonalities%20can%20be%20highlighted.%20To%20this%0Aday%20there%20is%20no%20work%20trying%20to%20relate%20and%20unify%20the%20two%20fields%20on%20the%20matter%20of%0Aabstractions%20unraveling%20all%20the%20different%20assumptions%20and%20their%20effect%20on%20the%0Away%20they%20can%20be%20used.%20Therefore%2C%20in%20this%20paper%20we%20aim%20to%20do%20so%20by%20looking%20at%0Aprojection%20abstractions%20in%20Planning%20through%20the%20lenses%20of%20discounted%20MDPs.%0AStarting%20from%20a%20projection%20abstraction%20built%20according%20to%20Classical%20or%0AProbabilistic%20Planning%20techniques%2C%20we%20will%20show%20how%20the%20same%20abstraction%20can%20be%0Aobtained%20under%20the%20abstraction%20frameworks%20available%20for%20discounted%20MDPs.%20Along%0Athe%20way%2C%20we%20will%20focus%20on%20computational%20as%20well%20as%20representational%20advantages%0Aand%20disadvantages%20of%20both%20worlds%20pointing%20out%20new%20research%20directions%20that%20are%0Aof%20interest%20for%20both%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProjection%2520Abstractions%2520in%2520Planning%2520Under%2520the%2520Lenses%2520of%2520Abstractions%2520for%250A%2520%2520MDPs%26entry.906535625%3DGiuseppe%2520Canonaco%2520and%2520Alberto%2520Pozanco%2520and%2520Daniel%2520Borrajo%26entry.1292438233%3D%2520%2520The%2520concept%2520of%2520abstraction%2520has%2520been%2520independently%2520developed%2520both%2520in%2520the%250Acontext%2520of%2520AI%2520Planning%2520and%2520discounted%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529.%250AHowever%252C%2520the%2520way%2520abstractions%2520are%2520built%2520and%2520used%2520in%2520the%2520context%2520of%2520Planning%2520and%250AMDPs%2520is%2520different%2520even%2520though%2520lots%2520of%2520commonalities%2520can%2520be%2520highlighted.%2520To%2520this%250Aday%2520there%2520is%2520no%2520work%2520trying%2520to%2520relate%2520and%2520unify%2520the%2520two%2520fields%2520on%2520the%2520matter%2520of%250Aabstractions%2520unraveling%2520all%2520the%2520different%2520assumptions%2520and%2520their%2520effect%2520on%2520the%250Away%2520they%2520can%2520be%2520used.%2520Therefore%252C%2520in%2520this%2520paper%2520we%2520aim%2520to%2520do%2520so%2520by%2520looking%2520at%250Aprojection%2520abstractions%2520in%2520Planning%2520through%2520the%2520lenses%2520of%2520discounted%2520MDPs.%250AStarting%2520from%2520a%2520projection%2520abstraction%2520built%2520according%2520to%2520Classical%2520or%250AProbabilistic%2520Planning%2520techniques%252C%2520we%2520will%2520show%2520how%2520the%2520same%2520abstraction%2520can%2520be%250Aobtained%2520under%2520the%2520abstraction%2520frameworks%2520available%2520for%2520discounted%2520MDPs.%2520Along%250Athe%2520way%252C%2520we%2520will%2520focus%2520on%2520computational%2520as%2520well%2520as%2520representational%2520advantages%250Aand%2520disadvantages%2520of%2520both%2520worlds%2520pointing%2520out%2520new%2520research%2520directions%2520that%2520are%250Aof%2520interest%2520for%2520both%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Projection%20Abstractions%20in%20Planning%20Under%20the%20Lenses%20of%20Abstractions%20for%0A%20%20MDPs&entry.906535625=Giuseppe%20Canonaco%20and%20Alberto%20Pozanco%20and%20Daniel%20Borrajo&entry.1292438233=%20%20The%20concept%20of%20abstraction%20has%20been%20independently%20developed%20both%20in%20the%0Acontext%20of%20AI%20Planning%20and%20discounted%20Markov%20Decision%20Processes%20%28MDPs%29.%0AHowever%2C%20the%20way%20abstractions%20are%20built%20and%20used%20in%20the%20context%20of%20Planning%20and%0AMDPs%20is%20different%20even%20though%20lots%20of%20commonalities%20can%20be%20highlighted.%20To%20this%0Aday%20there%20is%20no%20work%20trying%20to%20relate%20and%20unify%20the%20two%20fields%20on%20the%20matter%20of%0Aabstractions%20unraveling%20all%20the%20different%20assumptions%20and%20their%20effect%20on%20the%0Away%20they%20can%20be%20used.%20Therefore%2C%20in%20this%20paper%20we%20aim%20to%20do%20so%20by%20looking%20at%0Aprojection%20abstractions%20in%20Planning%20through%20the%20lenses%20of%20discounted%20MDPs.%0AStarting%20from%20a%20projection%20abstraction%20built%20according%20to%20Classical%20or%0AProbabilistic%20Planning%20techniques%2C%20we%20will%20show%20how%20the%20same%20abstraction%20can%20be%0Aobtained%20under%20the%20abstraction%20frameworks%20available%20for%20discounted%20MDPs.%20Along%0Athe%20way%2C%20we%20will%20focus%20on%20computational%20as%20well%20as%20representational%20advantages%0Aand%20disadvantages%20of%20both%20worlds%20pointing%20out%20new%20research%20directions%20that%20are%0Aof%20interest%20for%20both%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02615v1&entry.124074799=Read"},
{"title": "Multimodal Remote Sensing Scene Classification Using VLMs and Dual-Cross\n  Attention Networks", "author": "Jinjin Cai and Kexin Meng and Baijian Yang and Gang Shao", "abstract": "  Remote sensing scene classification (RSSC) is a critical task with diverse\napplications in land use and resource management. While unimodal image-based\napproaches show promise, they often struggle with limitations such as high\nintra-class variance and inter-class similarity. Incorporating textual\ninformation can enhance classification by providing additional context and\nsemantic understanding, but manual text annotation is labor-intensive and\ncostly. In this work, we propose a novel RSSC framework that integrates text\ndescriptions generated by large vision-language models (VLMs) as an auxiliary\nmodality without incurring expensive manual annotation costs. To fully leverage\nthe latent complementarities between visual and textual data, we propose a dual\ncross-attention-based network to fuse these modalities into a unified\nrepresentation. Extensive experiments with both quantitative and qualitative\nevaluation across five RSSC datasets demonstrate that our framework\nconsistently outperforms baseline models. We also verify the effectiveness of\nVLM-generated text descriptions compared to human-annotated descriptions.\nAdditionally, we design a zero-shot classification scenario to show that the\nlearned multimodal representation can be effectively utilized for unseen class\nclassification. This research opens new opportunities for leveraging textual\ninformation in RSSC tasks and provides a promising multimodal fusion structure,\noffering insights and inspiration for future studies. Code is available at:\nhttps://github.com/CJR7/MultiAtt-RSSC\n", "link": "http://arxiv.org/abs/2412.02531v1", "date": "2024-12-03", "relevancy": 1.7099, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5822}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Remote%20Sensing%20Scene%20Classification%20Using%20VLMs%20and%20Dual-Cross%0A%20%20Attention%20Networks&body=Title%3A%20Multimodal%20Remote%20Sensing%20Scene%20Classification%20Using%20VLMs%20and%20Dual-Cross%0A%20%20Attention%20Networks%0AAuthor%3A%20Jinjin%20Cai%20and%20Kexin%20Meng%20and%20Baijian%20Yang%20and%20Gang%20Shao%0AAbstract%3A%20%20%20Remote%20sensing%20scene%20classification%20%28RSSC%29%20is%20a%20critical%20task%20with%20diverse%0Aapplications%20in%20land%20use%20and%20resource%20management.%20While%20unimodal%20image-based%0Aapproaches%20show%20promise%2C%20they%20often%20struggle%20with%20limitations%20such%20as%20high%0Aintra-class%20variance%20and%20inter-class%20similarity.%20Incorporating%20textual%0Ainformation%20can%20enhance%20classification%20by%20providing%20additional%20context%20and%0Asemantic%20understanding%2C%20but%20manual%20text%20annotation%20is%20labor-intensive%20and%0Acostly.%20In%20this%20work%2C%20we%20propose%20a%20novel%20RSSC%20framework%20that%20integrates%20text%0Adescriptions%20generated%20by%20large%20vision-language%20models%20%28VLMs%29%20as%20an%20auxiliary%0Amodality%20without%20incurring%20expensive%20manual%20annotation%20costs.%20To%20fully%20leverage%0Athe%20latent%20complementarities%20between%20visual%20and%20textual%20data%2C%20we%20propose%20a%20dual%0Across-attention-based%20network%20to%20fuse%20these%20modalities%20into%20a%20unified%0Arepresentation.%20Extensive%20experiments%20with%20both%20quantitative%20and%20qualitative%0Aevaluation%20across%20five%20RSSC%20datasets%20demonstrate%20that%20our%20framework%0Aconsistently%20outperforms%20baseline%20models.%20We%20also%20verify%20the%20effectiveness%20of%0AVLM-generated%20text%20descriptions%20compared%20to%20human-annotated%20descriptions.%0AAdditionally%2C%20we%20design%20a%20zero-shot%20classification%20scenario%20to%20show%20that%20the%0Alearned%20multimodal%20representation%20can%20be%20effectively%20utilized%20for%20unseen%20class%0Aclassification.%20This%20research%20opens%20new%20opportunities%20for%20leveraging%20textual%0Ainformation%20in%20RSSC%20tasks%20and%20provides%20a%20promising%20multimodal%20fusion%20structure%2C%0Aoffering%20insights%20and%20inspiration%20for%20future%20studies.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/CJR7/MultiAtt-RSSC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Remote%2520Sensing%2520Scene%2520Classification%2520Using%2520VLMs%2520and%2520Dual-Cross%250A%2520%2520Attention%2520Networks%26entry.906535625%3DJinjin%2520Cai%2520and%2520Kexin%2520Meng%2520and%2520Baijian%2520Yang%2520and%2520Gang%2520Shao%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520scene%2520classification%2520%2528RSSC%2529%2520is%2520a%2520critical%2520task%2520with%2520diverse%250Aapplications%2520in%2520land%2520use%2520and%2520resource%2520management.%2520While%2520unimodal%2520image-based%250Aapproaches%2520show%2520promise%252C%2520they%2520often%2520struggle%2520with%2520limitations%2520such%2520as%2520high%250Aintra-class%2520variance%2520and%2520inter-class%2520similarity.%2520Incorporating%2520textual%250Ainformation%2520can%2520enhance%2520classification%2520by%2520providing%2520additional%2520context%2520and%250Asemantic%2520understanding%252C%2520but%2520manual%2520text%2520annotation%2520is%2520labor-intensive%2520and%250Acostly.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520RSSC%2520framework%2520that%2520integrates%2520text%250Adescriptions%2520generated%2520by%2520large%2520vision-language%2520models%2520%2528VLMs%2529%2520as%2520an%2520auxiliary%250Amodality%2520without%2520incurring%2520expensive%2520manual%2520annotation%2520costs.%2520To%2520fully%2520leverage%250Athe%2520latent%2520complementarities%2520between%2520visual%2520and%2520textual%2520data%252C%2520we%2520propose%2520a%2520dual%250Across-attention-based%2520network%2520to%2520fuse%2520these%2520modalities%2520into%2520a%2520unified%250Arepresentation.%2520Extensive%2520experiments%2520with%2520both%2520quantitative%2520and%2520qualitative%250Aevaluation%2520across%2520five%2520RSSC%2520datasets%2520demonstrate%2520that%2520our%2520framework%250Aconsistently%2520outperforms%2520baseline%2520models.%2520We%2520also%2520verify%2520the%2520effectiveness%2520of%250AVLM-generated%2520text%2520descriptions%2520compared%2520to%2520human-annotated%2520descriptions.%250AAdditionally%252C%2520we%2520design%2520a%2520zero-shot%2520classification%2520scenario%2520to%2520show%2520that%2520the%250Alearned%2520multimodal%2520representation%2520can%2520be%2520effectively%2520utilized%2520for%2520unseen%2520class%250Aclassification.%2520This%2520research%2520opens%2520new%2520opportunities%2520for%2520leveraging%2520textual%250Ainformation%2520in%2520RSSC%2520tasks%2520and%2520provides%2520a%2520promising%2520multimodal%2520fusion%2520structure%252C%250Aoffering%2520insights%2520and%2520inspiration%2520for%2520future%2520studies.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/CJR7/MultiAtt-RSSC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Remote%20Sensing%20Scene%20Classification%20Using%20VLMs%20and%20Dual-Cross%0A%20%20Attention%20Networks&entry.906535625=Jinjin%20Cai%20and%20Kexin%20Meng%20and%20Baijian%20Yang%20and%20Gang%20Shao&entry.1292438233=%20%20Remote%20sensing%20scene%20classification%20%28RSSC%29%20is%20a%20critical%20task%20with%20diverse%0Aapplications%20in%20land%20use%20and%20resource%20management.%20While%20unimodal%20image-based%0Aapproaches%20show%20promise%2C%20they%20often%20struggle%20with%20limitations%20such%20as%20high%0Aintra-class%20variance%20and%20inter-class%20similarity.%20Incorporating%20textual%0Ainformation%20can%20enhance%20classification%20by%20providing%20additional%20context%20and%0Asemantic%20understanding%2C%20but%20manual%20text%20annotation%20is%20labor-intensive%20and%0Acostly.%20In%20this%20work%2C%20we%20propose%20a%20novel%20RSSC%20framework%20that%20integrates%20text%0Adescriptions%20generated%20by%20large%20vision-language%20models%20%28VLMs%29%20as%20an%20auxiliary%0Amodality%20without%20incurring%20expensive%20manual%20annotation%20costs.%20To%20fully%20leverage%0Athe%20latent%20complementarities%20between%20visual%20and%20textual%20data%2C%20we%20propose%20a%20dual%0Across-attention-based%20network%20to%20fuse%20these%20modalities%20into%20a%20unified%0Arepresentation.%20Extensive%20experiments%20with%20both%20quantitative%20and%20qualitative%0Aevaluation%20across%20five%20RSSC%20datasets%20demonstrate%20that%20our%20framework%0Aconsistently%20outperforms%20baseline%20models.%20We%20also%20verify%20the%20effectiveness%20of%0AVLM-generated%20text%20descriptions%20compared%20to%20human-annotated%20descriptions.%0AAdditionally%2C%20we%20design%20a%20zero-shot%20classification%20scenario%20to%20show%20that%20the%0Alearned%20multimodal%20representation%20can%20be%20effectively%20utilized%20for%20unseen%20class%0Aclassification.%20This%20research%20opens%20new%20opportunities%20for%20leveraging%20textual%0Ainformation%20in%20RSSC%20tasks%20and%20provides%20a%20promising%20multimodal%20fusion%20structure%2C%0Aoffering%20insights%20and%20inspiration%20for%20future%20studies.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/CJR7/MultiAtt-RSSC%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02531v1&entry.124074799=Read"},
{"title": "Unveiling Concept Attribution in Diffusion Models", "author": "Quang H. Nguyen and Hoang Phan and Khoa D. Doan", "abstract": "  Diffusion models have shown remarkable abilities in generating realistic and\nhigh-quality images from text prompts. However, a trained model remains\nblack-box; little do we know about the role of its components in exhibiting a\nconcept such as objects or styles. Recent works employ causal tracing to\nlocalize layers storing knowledge in generative models without showing how\nthose layers contribute to the target concept. In this work, we approach the\nmodel interpretability problem from a more general perspective and pose a\nquestion: \\textit{``How do model components work jointly to demonstrate\nknowledge?''}. We adapt component attribution to decompose diffusion models,\nunveiling how a component contributes to a concept. Our framework allows\neffective model editing, in particular, we can erase a concept from diffusion\nmodels by removing positive components while remaining knowledge of other\nconcepts. Surprisingly, we also show there exist components that contribute\nnegatively to a concept, which has not been discovered in the knowledge\nlocalization approach. Experimental results confirm the role of positive and\nnegative components pinpointed by our framework, depicting a complete view of\ninterpreting generative models. Our code is available at\n\\url{https://github.com/mail-research/CAD-attribution4diffusion}\n", "link": "http://arxiv.org/abs/2412.02542v1", "date": "2024-12-03", "relevancy": 1.1441, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5938}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.575}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Concept%20Attribution%20in%20Diffusion%20Models&body=Title%3A%20Unveiling%20Concept%20Attribution%20in%20Diffusion%20Models%0AAuthor%3A%20Quang%20H.%20Nguyen%20and%20Hoang%20Phan%20and%20Khoa%20D.%20Doan%0AAbstract%3A%20%20%20Diffusion%20models%20have%20shown%20remarkable%20abilities%20in%20generating%20realistic%20and%0Ahigh-quality%20images%20from%20text%20prompts.%20However%2C%20a%20trained%20model%20remains%0Ablack-box%3B%20little%20do%20we%20know%20about%20the%20role%20of%20its%20components%20in%20exhibiting%20a%0Aconcept%20such%20as%20objects%20or%20styles.%20Recent%20works%20employ%20causal%20tracing%20to%0Alocalize%20layers%20storing%20knowledge%20in%20generative%20models%20without%20showing%20how%0Athose%20layers%20contribute%20to%20the%20target%20concept.%20In%20this%20work%2C%20we%20approach%20the%0Amodel%20interpretability%20problem%20from%20a%20more%20general%20perspective%20and%20pose%20a%0Aquestion%3A%20%5Ctextit%7B%60%60How%20do%20model%20components%20work%20jointly%20to%20demonstrate%0Aknowledge%3F%27%27%7D.%20We%20adapt%20component%20attribution%20to%20decompose%20diffusion%20models%2C%0Aunveiling%20how%20a%20component%20contributes%20to%20a%20concept.%20Our%20framework%20allows%0Aeffective%20model%20editing%2C%20in%20particular%2C%20we%20can%20erase%20a%20concept%20from%20diffusion%0Amodels%20by%20removing%20positive%20components%20while%20remaining%20knowledge%20of%20other%0Aconcepts.%20Surprisingly%2C%20we%20also%20show%20there%20exist%20components%20that%20contribute%0Anegatively%20to%20a%20concept%2C%20which%20has%20not%20been%20discovered%20in%20the%20knowledge%0Alocalization%20approach.%20Experimental%20results%20confirm%20the%20role%20of%20positive%20and%0Anegative%20components%20pinpointed%20by%20our%20framework%2C%20depicting%20a%20complete%20view%20of%0Ainterpreting%20generative%20models.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mail-research/CAD-attribution4diffusion%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Concept%2520Attribution%2520in%2520Diffusion%2520Models%26entry.906535625%3DQuang%2520H.%2520Nguyen%2520and%2520Hoang%2520Phan%2520and%2520Khoa%2520D.%2520Doan%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520shown%2520remarkable%2520abilities%2520in%2520generating%2520realistic%2520and%250Ahigh-quality%2520images%2520from%2520text%2520prompts.%2520However%252C%2520a%2520trained%2520model%2520remains%250Ablack-box%253B%2520little%2520do%2520we%2520know%2520about%2520the%2520role%2520of%2520its%2520components%2520in%2520exhibiting%2520a%250Aconcept%2520such%2520as%2520objects%2520or%2520styles.%2520Recent%2520works%2520employ%2520causal%2520tracing%2520to%250Alocalize%2520layers%2520storing%2520knowledge%2520in%2520generative%2520models%2520without%2520showing%2520how%250Athose%2520layers%2520contribute%2520to%2520the%2520target%2520concept.%2520In%2520this%2520work%252C%2520we%2520approach%2520the%250Amodel%2520interpretability%2520problem%2520from%2520a%2520more%2520general%2520perspective%2520and%2520pose%2520a%250Aquestion%253A%2520%255Ctextit%257B%2560%2560How%2520do%2520model%2520components%2520work%2520jointly%2520to%2520demonstrate%250Aknowledge%253F%2527%2527%257D.%2520We%2520adapt%2520component%2520attribution%2520to%2520decompose%2520diffusion%2520models%252C%250Aunveiling%2520how%2520a%2520component%2520contributes%2520to%2520a%2520concept.%2520Our%2520framework%2520allows%250Aeffective%2520model%2520editing%252C%2520in%2520particular%252C%2520we%2520can%2520erase%2520a%2520concept%2520from%2520diffusion%250Amodels%2520by%2520removing%2520positive%2520components%2520while%2520remaining%2520knowledge%2520of%2520other%250Aconcepts.%2520Surprisingly%252C%2520we%2520also%2520show%2520there%2520exist%2520components%2520that%2520contribute%250Anegatively%2520to%2520a%2520concept%252C%2520which%2520has%2520not%2520been%2520discovered%2520in%2520the%2520knowledge%250Alocalization%2520approach.%2520Experimental%2520results%2520confirm%2520the%2520role%2520of%2520positive%2520and%250Anegative%2520components%2520pinpointed%2520by%2520our%2520framework%252C%2520depicting%2520a%2520complete%2520view%2520of%250Ainterpreting%2520generative%2520models.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/mail-research/CAD-attribution4diffusion%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Concept%20Attribution%20in%20Diffusion%20Models&entry.906535625=Quang%20H.%20Nguyen%20and%20Hoang%20Phan%20and%20Khoa%20D.%20Doan&entry.1292438233=%20%20Diffusion%20models%20have%20shown%20remarkable%20abilities%20in%20generating%20realistic%20and%0Ahigh-quality%20images%20from%20text%20prompts.%20However%2C%20a%20trained%20model%20remains%0Ablack-box%3B%20little%20do%20we%20know%20about%20the%20role%20of%20its%20components%20in%20exhibiting%20a%0Aconcept%20such%20as%20objects%20or%20styles.%20Recent%20works%20employ%20causal%20tracing%20to%0Alocalize%20layers%20storing%20knowledge%20in%20generative%20models%20without%20showing%20how%0Athose%20layers%20contribute%20to%20the%20target%20concept.%20In%20this%20work%2C%20we%20approach%20the%0Amodel%20interpretability%20problem%20from%20a%20more%20general%20perspective%20and%20pose%20a%0Aquestion%3A%20%5Ctextit%7B%60%60How%20do%20model%20components%20work%20jointly%20to%20demonstrate%0Aknowledge%3F%27%27%7D.%20We%20adapt%20component%20attribution%20to%20decompose%20diffusion%20models%2C%0Aunveiling%20how%20a%20component%20contributes%20to%20a%20concept.%20Our%20framework%20allows%0Aeffective%20model%20editing%2C%20in%20particular%2C%20we%20can%20erase%20a%20concept%20from%20diffusion%0Amodels%20by%20removing%20positive%20components%20while%20remaining%20knowledge%20of%20other%0Aconcepts.%20Surprisingly%2C%20we%20also%20show%20there%20exist%20components%20that%20contribute%0Anegatively%20to%20a%20concept%2C%20which%20has%20not%20been%20discovered%20in%20the%20knowledge%0Alocalization%20approach.%20Experimental%20results%20confirm%20the%20role%20of%20positive%20and%0Anegative%20components%20pinpointed%20by%20our%20framework%2C%20depicting%20a%20complete%20view%20of%0Ainterpreting%20generative%20models.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mail-research/CAD-attribution4diffusion%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02542v1&entry.124074799=Read"},
{"title": "Supervised Multiple Kernel Learning approaches for multi-omics data\n  integration", "author": "Mitja Briscik and Gabriele Tazza and Marie-Agnes Dillies and L\u00e1szl\u00f3 Vid\u00e1cs and S\u00e9bastien Dejean", "abstract": "  Advances in high-throughput technologies have originated an ever-increasing\navailability of omics datasets. The integration of multiple heterogeneous data\nsources is currently an issue for biology and bioinformatics. Multiple kernel\nlearning (MKL) has shown to be a flexible and valid approach to consider the\ndiverse nature of multi-omics inputs, despite being an underused tool in\ngenomic data mining. We provide novel MKL approaches based on different kernel\nfusion strategies. To learn from the meta-kernel of input kernels, we adapted\nunsupervised integration algorithms for supervised tasks with support vector\nmachines. We also tested deep learning architectures for kernel fusion and\nclassification. The results show that MKL-based models can outperform more\ncomplex, state-of-the-art, supervised multi-omics integrative approaches.\nMultiple kernel learning offers a natural framework for predictive models in\nmulti-omics data. It proved to provide a fast and reliable solution that can\ncompete with and outperform more complex architectures. Our results offer a\ndirection for bio-data mining research, biomarker discovery and further\ndevelopment of methods for heterogeneous data integration.\n", "link": "http://arxiv.org/abs/2403.18355v2", "date": "2024-12-03", "relevancy": 1.3957, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4634}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Multiple%20Kernel%20Learning%20approaches%20for%20multi-omics%20data%0A%20%20integration&body=Title%3A%20Supervised%20Multiple%20Kernel%20Learning%20approaches%20for%20multi-omics%20data%0A%20%20integration%0AAuthor%3A%20Mitja%20Briscik%20and%20Gabriele%20Tazza%20and%20Marie-Agnes%20Dillies%20and%20L%C3%A1szl%C3%B3%20Vid%C3%A1cs%20and%20S%C3%A9bastien%20Dejean%0AAbstract%3A%20%20%20Advances%20in%20high-throughput%20technologies%20have%20originated%20an%20ever-increasing%0Aavailability%20of%20omics%20datasets.%20The%20integration%20of%20multiple%20heterogeneous%20data%0Asources%20is%20currently%20an%20issue%20for%20biology%20and%20bioinformatics.%20Multiple%20kernel%0Alearning%20%28MKL%29%20has%20shown%20to%20be%20a%20flexible%20and%20valid%20approach%20to%20consider%20the%0Adiverse%20nature%20of%20multi-omics%20inputs%2C%20despite%20being%20an%20underused%20tool%20in%0Agenomic%20data%20mining.%20We%20provide%20novel%20MKL%20approaches%20based%20on%20different%20kernel%0Afusion%20strategies.%20To%20learn%20from%20the%20meta-kernel%20of%20input%20kernels%2C%20we%20adapted%0Aunsupervised%20integration%20algorithms%20for%20supervised%20tasks%20with%20support%20vector%0Amachines.%20We%20also%20tested%20deep%20learning%20architectures%20for%20kernel%20fusion%20and%0Aclassification.%20The%20results%20show%20that%20MKL-based%20models%20can%20outperform%20more%0Acomplex%2C%20state-of-the-art%2C%20supervised%20multi-omics%20integrative%20approaches.%0AMultiple%20kernel%20learning%20offers%20a%20natural%20framework%20for%20predictive%20models%20in%0Amulti-omics%20data.%20It%20proved%20to%20provide%20a%20fast%20and%20reliable%20solution%20that%20can%0Acompete%20with%20and%20outperform%20more%20complex%20architectures.%20Our%20results%20offer%20a%0Adirection%20for%20bio-data%20mining%20research%2C%20biomarker%20discovery%20and%20further%0Adevelopment%20of%20methods%20for%20heterogeneous%20data%20integration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Multiple%2520Kernel%2520Learning%2520approaches%2520for%2520multi-omics%2520data%250A%2520%2520integration%26entry.906535625%3DMitja%2520Briscik%2520and%2520Gabriele%2520Tazza%2520and%2520Marie-Agnes%2520Dillies%2520and%2520L%25C3%25A1szl%25C3%25B3%2520Vid%25C3%25A1cs%2520and%2520S%25C3%25A9bastien%2520Dejean%26entry.1292438233%3D%2520%2520Advances%2520in%2520high-throughput%2520technologies%2520have%2520originated%2520an%2520ever-increasing%250Aavailability%2520of%2520omics%2520datasets.%2520The%2520integration%2520of%2520multiple%2520heterogeneous%2520data%250Asources%2520is%2520currently%2520an%2520issue%2520for%2520biology%2520and%2520bioinformatics.%2520Multiple%2520kernel%250Alearning%2520%2528MKL%2529%2520has%2520shown%2520to%2520be%2520a%2520flexible%2520and%2520valid%2520approach%2520to%2520consider%2520the%250Adiverse%2520nature%2520of%2520multi-omics%2520inputs%252C%2520despite%2520being%2520an%2520underused%2520tool%2520in%250Agenomic%2520data%2520mining.%2520We%2520provide%2520novel%2520MKL%2520approaches%2520based%2520on%2520different%2520kernel%250Afusion%2520strategies.%2520To%2520learn%2520from%2520the%2520meta-kernel%2520of%2520input%2520kernels%252C%2520we%2520adapted%250Aunsupervised%2520integration%2520algorithms%2520for%2520supervised%2520tasks%2520with%2520support%2520vector%250Amachines.%2520We%2520also%2520tested%2520deep%2520learning%2520architectures%2520for%2520kernel%2520fusion%2520and%250Aclassification.%2520The%2520results%2520show%2520that%2520MKL-based%2520models%2520can%2520outperform%2520more%250Acomplex%252C%2520state-of-the-art%252C%2520supervised%2520multi-omics%2520integrative%2520approaches.%250AMultiple%2520kernel%2520learning%2520offers%2520a%2520natural%2520framework%2520for%2520predictive%2520models%2520in%250Amulti-omics%2520data.%2520It%2520proved%2520to%2520provide%2520a%2520fast%2520and%2520reliable%2520solution%2520that%2520can%250Acompete%2520with%2520and%2520outperform%2520more%2520complex%2520architectures.%2520Our%2520results%2520offer%2520a%250Adirection%2520for%2520bio-data%2520mining%2520research%252C%2520biomarker%2520discovery%2520and%2520further%250Adevelopment%2520of%2520methods%2520for%2520heterogeneous%2520data%2520integration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Multiple%20Kernel%20Learning%20approaches%20for%20multi-omics%20data%0A%20%20integration&entry.906535625=Mitja%20Briscik%20and%20Gabriele%20Tazza%20and%20Marie-Agnes%20Dillies%20and%20L%C3%A1szl%C3%B3%20Vid%C3%A1cs%20and%20S%C3%A9bastien%20Dejean&entry.1292438233=%20%20Advances%20in%20high-throughput%20technologies%20have%20originated%20an%20ever-increasing%0Aavailability%20of%20omics%20datasets.%20The%20integration%20of%20multiple%20heterogeneous%20data%0Asources%20is%20currently%20an%20issue%20for%20biology%20and%20bioinformatics.%20Multiple%20kernel%0Alearning%20%28MKL%29%20has%20shown%20to%20be%20a%20flexible%20and%20valid%20approach%20to%20consider%20the%0Adiverse%20nature%20of%20multi-omics%20inputs%2C%20despite%20being%20an%20underused%20tool%20in%0Agenomic%20data%20mining.%20We%20provide%20novel%20MKL%20approaches%20based%20on%20different%20kernel%0Afusion%20strategies.%20To%20learn%20from%20the%20meta-kernel%20of%20input%20kernels%2C%20we%20adapted%0Aunsupervised%20integration%20algorithms%20for%20supervised%20tasks%20with%20support%20vector%0Amachines.%20We%20also%20tested%20deep%20learning%20architectures%20for%20kernel%20fusion%20and%0Aclassification.%20The%20results%20show%20that%20MKL-based%20models%20can%20outperform%20more%0Acomplex%2C%20state-of-the-art%2C%20supervised%20multi-omics%20integrative%20approaches.%0AMultiple%20kernel%20learning%20offers%20a%20natural%20framework%20for%20predictive%20models%20in%0Amulti-omics%20data.%20It%20proved%20to%20provide%20a%20fast%20and%20reliable%20solution%20that%20can%0Acompete%20with%20and%20outperform%20more%20complex%20architectures.%20Our%20results%20offer%20a%0Adirection%20for%20bio-data%20mining%20research%2C%20biomarker%20discovery%20and%20further%0Adevelopment%20of%20methods%20for%20heterogeneous%20data%20integration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18355v2&entry.124074799=Read"},
{"title": "The Cost of Consistency: Submodular Maximization with Constant Recourse", "author": "Paul D\u00fctting and Federico Fusco and Silvio Lattanzi and Ashkan Norouzi-Fard and Ola Svensson and Morteza Zadimoghaddam", "abstract": "  In this work, we study online submodular maximization, and how the\nrequirement of maintaining a stable solution impacts the approximation. In\nparticular, we seek bounds on the best-possible approximation ratio that is\nattainable when the algorithm is allowed to make at most a constant number of\nupdates per step. We show a tight information-theoretic bound of $\\tfrac{2}{3}$\nfor general monotone submodular functions, and an improved (also tight) bound\nof $\\tfrac{3}{4}$ for coverage functions. Since both these bounds are attained\nby non poly-time algorithms, we also give a poly-time randomized algorithm that\nachieves a $0.51$-approximation. Combined with an information-theoretic\nhardness of $\\tfrac{1}{2}$ for deterministic algorithms from prior work, our\nwork thus shows a separation between deterministic and randomized algorithms,\nboth information theoretically and for poly-time algorithms.\n", "link": "http://arxiv.org/abs/2412.02492v1", "date": "2024-12-03", "relevancy": 1.5537, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4226}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3863}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Cost%20of%20Consistency%3A%20Submodular%20Maximization%20with%20Constant%20Recourse&body=Title%3A%20The%20Cost%20of%20Consistency%3A%20Submodular%20Maximization%20with%20Constant%20Recourse%0AAuthor%3A%20Paul%20D%C3%BCtting%20and%20Federico%20Fusco%20and%20Silvio%20Lattanzi%20and%20Ashkan%20Norouzi-Fard%20and%20Ola%20Svensson%20and%20Morteza%20Zadimoghaddam%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20online%20submodular%20maximization%2C%20and%20how%20the%0Arequirement%20of%20maintaining%20a%20stable%20solution%20impacts%20the%20approximation.%20In%0Aparticular%2C%20we%20seek%20bounds%20on%20the%20best-possible%20approximation%20ratio%20that%20is%0Aattainable%20when%20the%20algorithm%20is%20allowed%20to%20make%20at%20most%20a%20constant%20number%20of%0Aupdates%20per%20step.%20We%20show%20a%20tight%20information-theoretic%20bound%20of%20%24%5Ctfrac%7B2%7D%7B3%7D%24%0Afor%20general%20monotone%20submodular%20functions%2C%20and%20an%20improved%20%28also%20tight%29%20bound%0Aof%20%24%5Ctfrac%7B3%7D%7B4%7D%24%20for%20coverage%20functions.%20Since%20both%20these%20bounds%20are%20attained%0Aby%20non%20poly-time%20algorithms%2C%20we%20also%20give%20a%20poly-time%20randomized%20algorithm%20that%0Aachieves%20a%20%240.51%24-approximation.%20Combined%20with%20an%20information-theoretic%0Ahardness%20of%20%24%5Ctfrac%7B1%7D%7B2%7D%24%20for%20deterministic%20algorithms%20from%20prior%20work%2C%20our%0Awork%20thus%20shows%20a%20separation%20between%20deterministic%20and%20randomized%20algorithms%2C%0Aboth%20information%20theoretically%20and%20for%20poly-time%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Cost%2520of%2520Consistency%253A%2520Submodular%2520Maximization%2520with%2520Constant%2520Recourse%26entry.906535625%3DPaul%2520D%25C3%25BCtting%2520and%2520Federico%2520Fusco%2520and%2520Silvio%2520Lattanzi%2520and%2520Ashkan%2520Norouzi-Fard%2520and%2520Ola%2520Svensson%2520and%2520Morteza%2520Zadimoghaddam%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520online%2520submodular%2520maximization%252C%2520and%2520how%2520the%250Arequirement%2520of%2520maintaining%2520a%2520stable%2520solution%2520impacts%2520the%2520approximation.%2520In%250Aparticular%252C%2520we%2520seek%2520bounds%2520on%2520the%2520best-possible%2520approximation%2520ratio%2520that%2520is%250Aattainable%2520when%2520the%2520algorithm%2520is%2520allowed%2520to%2520make%2520at%2520most%2520a%2520constant%2520number%2520of%250Aupdates%2520per%2520step.%2520We%2520show%2520a%2520tight%2520information-theoretic%2520bound%2520of%2520%2524%255Ctfrac%257B2%257D%257B3%257D%2524%250Afor%2520general%2520monotone%2520submodular%2520functions%252C%2520and%2520an%2520improved%2520%2528also%2520tight%2529%2520bound%250Aof%2520%2524%255Ctfrac%257B3%257D%257B4%257D%2524%2520for%2520coverage%2520functions.%2520Since%2520both%2520these%2520bounds%2520are%2520attained%250Aby%2520non%2520poly-time%2520algorithms%252C%2520we%2520also%2520give%2520a%2520poly-time%2520randomized%2520algorithm%2520that%250Aachieves%2520a%2520%25240.51%2524-approximation.%2520Combined%2520with%2520an%2520information-theoretic%250Ahardness%2520of%2520%2524%255Ctfrac%257B1%257D%257B2%257D%2524%2520for%2520deterministic%2520algorithms%2520from%2520prior%2520work%252C%2520our%250Awork%2520thus%2520shows%2520a%2520separation%2520between%2520deterministic%2520and%2520randomized%2520algorithms%252C%250Aboth%2520information%2520theoretically%2520and%2520for%2520poly-time%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Cost%20of%20Consistency%3A%20Submodular%20Maximization%20with%20Constant%20Recourse&entry.906535625=Paul%20D%C3%BCtting%20and%20Federico%20Fusco%20and%20Silvio%20Lattanzi%20and%20Ashkan%20Norouzi-Fard%20and%20Ola%20Svensson%20and%20Morteza%20Zadimoghaddam&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20online%20submodular%20maximization%2C%20and%20how%20the%0Arequirement%20of%20maintaining%20a%20stable%20solution%20impacts%20the%20approximation.%20In%0Aparticular%2C%20we%20seek%20bounds%20on%20the%20best-possible%20approximation%20ratio%20that%20is%0Aattainable%20when%20the%20algorithm%20is%20allowed%20to%20make%20at%20most%20a%20constant%20number%20of%0Aupdates%20per%20step.%20We%20show%20a%20tight%20information-theoretic%20bound%20of%20%24%5Ctfrac%7B2%7D%7B3%7D%24%0Afor%20general%20monotone%20submodular%20functions%2C%20and%20an%20improved%20%28also%20tight%29%20bound%0Aof%20%24%5Ctfrac%7B3%7D%7B4%7D%24%20for%20coverage%20functions.%20Since%20both%20these%20bounds%20are%20attained%0Aby%20non%20poly-time%20algorithms%2C%20we%20also%20give%20a%20poly-time%20randomized%20algorithm%20that%0Aachieves%20a%20%240.51%24-approximation.%20Combined%20with%20an%20information-theoretic%0Ahardness%20of%20%24%5Ctfrac%7B1%7D%7B2%7D%24%20for%20deterministic%20algorithms%20from%20prior%20work%2C%20our%0Awork%20thus%20shows%20a%20separation%20between%20deterministic%20and%20randomized%20algorithms%2C%0Aboth%20information%20theoretically%20and%20for%20poly-time%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02492v1&entry.124074799=Read"},
{"title": "What should a neuron aim for? Designing local objective functions based\n  on information theory", "author": "Andreas C. Schneider and Valentin Neuhaus and David A. Ehrlich and Abdullah Makkeh and Alexander S. Ecker and Viola Priesemann and Michael Wibral", "abstract": "  In modern deep neural networks, the learning dynamics of the individual\nneurons is often obscure, as the networks are trained via global optimization.\nConversely, biological systems build on self-organized, local learning,\nachieving robustness and efficiency with limited global information. We here\nshow how self-organization between individual artificial neurons can be\nachieved by designing abstract bio-inspired local learning goals. These goals\nare parameterized using a recent extension of information theory, Partial\nInformation Decomposition (PID), which decomposes the information that a set of\ninformation sources holds about an outcome into unique, redundant and\nsynergistic contributions. Our framework enables neurons to locally shape the\nintegration of information from various input classes, i.e. feedforward,\nfeedback, and lateral, by selecting which of the three inputs should contribute\nuniquely, redundantly or synergistically to the output. This selection is\nexpressed as a weighted sum of PID terms, which, for a given problem, can be\ndirectly derived from intuitive reasoning or via numerical optimization,\noffering a window into understanding task-relevant local information\nprocessing. Achieving neuron-level interpretability while enabling strong\nperformance using local learning, our work advances a principled\ninformation-theoretic foundation for local learning strategies.\n", "link": "http://arxiv.org/abs/2412.02482v1", "date": "2024-12-03", "relevancy": 1.9025, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4724}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20should%20a%20neuron%20aim%20for%3F%20Designing%20local%20objective%20functions%20based%0A%20%20on%20information%20theory&body=Title%3A%20What%20should%20a%20neuron%20aim%20for%3F%20Designing%20local%20objective%20functions%20based%0A%20%20on%20information%20theory%0AAuthor%3A%20Andreas%20C.%20Schneider%20and%20Valentin%20Neuhaus%20and%20David%20A.%20Ehrlich%20and%20Abdullah%20Makkeh%20and%20Alexander%20S.%20Ecker%20and%20Viola%20Priesemann%20and%20Michael%20Wibral%0AAbstract%3A%20%20%20In%20modern%20deep%20neural%20networks%2C%20the%20learning%20dynamics%20of%20the%20individual%0Aneurons%20is%20often%20obscure%2C%20as%20the%20networks%20are%20trained%20via%20global%20optimization.%0AConversely%2C%20biological%20systems%20build%20on%20self-organized%2C%20local%20learning%2C%0Aachieving%20robustness%20and%20efficiency%20with%20limited%20global%20information.%20We%20here%0Ashow%20how%20self-organization%20between%20individual%20artificial%20neurons%20can%20be%0Aachieved%20by%20designing%20abstract%20bio-inspired%20local%20learning%20goals.%20These%20goals%0Aare%20parameterized%20using%20a%20recent%20extension%20of%20information%20theory%2C%20Partial%0AInformation%20Decomposition%20%28PID%29%2C%20which%20decomposes%20the%20information%20that%20a%20set%20of%0Ainformation%20sources%20holds%20about%20an%20outcome%20into%20unique%2C%20redundant%20and%0Asynergistic%20contributions.%20Our%20framework%20enables%20neurons%20to%20locally%20shape%20the%0Aintegration%20of%20information%20from%20various%20input%20classes%2C%20i.e.%20feedforward%2C%0Afeedback%2C%20and%20lateral%2C%20by%20selecting%20which%20of%20the%20three%20inputs%20should%20contribute%0Auniquely%2C%20redundantly%20or%20synergistically%20to%20the%20output.%20This%20selection%20is%0Aexpressed%20as%20a%20weighted%20sum%20of%20PID%20terms%2C%20which%2C%20for%20a%20given%20problem%2C%20can%20be%0Adirectly%20derived%20from%20intuitive%20reasoning%20or%20via%20numerical%20optimization%2C%0Aoffering%20a%20window%20into%20understanding%20task-relevant%20local%20information%0Aprocessing.%20Achieving%20neuron-level%20interpretability%20while%20enabling%20strong%0Aperformance%20using%20local%20learning%2C%20our%20work%20advances%20a%20principled%0Ainformation-theoretic%20foundation%20for%20local%20learning%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520should%2520a%2520neuron%2520aim%2520for%253F%2520Designing%2520local%2520objective%2520functions%2520based%250A%2520%2520on%2520information%2520theory%26entry.906535625%3DAndreas%2520C.%2520Schneider%2520and%2520Valentin%2520Neuhaus%2520and%2520David%2520A.%2520Ehrlich%2520and%2520Abdullah%2520Makkeh%2520and%2520Alexander%2520S.%2520Ecker%2520and%2520Viola%2520Priesemann%2520and%2520Michael%2520Wibral%26entry.1292438233%3D%2520%2520In%2520modern%2520deep%2520neural%2520networks%252C%2520the%2520learning%2520dynamics%2520of%2520the%2520individual%250Aneurons%2520is%2520often%2520obscure%252C%2520as%2520the%2520networks%2520are%2520trained%2520via%2520global%2520optimization.%250AConversely%252C%2520biological%2520systems%2520build%2520on%2520self-organized%252C%2520local%2520learning%252C%250Aachieving%2520robustness%2520and%2520efficiency%2520with%2520limited%2520global%2520information.%2520We%2520here%250Ashow%2520how%2520self-organization%2520between%2520individual%2520artificial%2520neurons%2520can%2520be%250Aachieved%2520by%2520designing%2520abstract%2520bio-inspired%2520local%2520learning%2520goals.%2520These%2520goals%250Aare%2520parameterized%2520using%2520a%2520recent%2520extension%2520of%2520information%2520theory%252C%2520Partial%250AInformation%2520Decomposition%2520%2528PID%2529%252C%2520which%2520decomposes%2520the%2520information%2520that%2520a%2520set%2520of%250Ainformation%2520sources%2520holds%2520about%2520an%2520outcome%2520into%2520unique%252C%2520redundant%2520and%250Asynergistic%2520contributions.%2520Our%2520framework%2520enables%2520neurons%2520to%2520locally%2520shape%2520the%250Aintegration%2520of%2520information%2520from%2520various%2520input%2520classes%252C%2520i.e.%2520feedforward%252C%250Afeedback%252C%2520and%2520lateral%252C%2520by%2520selecting%2520which%2520of%2520the%2520three%2520inputs%2520should%2520contribute%250Auniquely%252C%2520redundantly%2520or%2520synergistically%2520to%2520the%2520output.%2520This%2520selection%2520is%250Aexpressed%2520as%2520a%2520weighted%2520sum%2520of%2520PID%2520terms%252C%2520which%252C%2520for%2520a%2520given%2520problem%252C%2520can%2520be%250Adirectly%2520derived%2520from%2520intuitive%2520reasoning%2520or%2520via%2520numerical%2520optimization%252C%250Aoffering%2520a%2520window%2520into%2520understanding%2520task-relevant%2520local%2520information%250Aprocessing.%2520Achieving%2520neuron-level%2520interpretability%2520while%2520enabling%2520strong%250Aperformance%2520using%2520local%2520learning%252C%2520our%2520work%2520advances%2520a%2520principled%250Ainformation-theoretic%2520foundation%2520for%2520local%2520learning%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20should%20a%20neuron%20aim%20for%3F%20Designing%20local%20objective%20functions%20based%0A%20%20on%20information%20theory&entry.906535625=Andreas%20C.%20Schneider%20and%20Valentin%20Neuhaus%20and%20David%20A.%20Ehrlich%20and%20Abdullah%20Makkeh%20and%20Alexander%20S.%20Ecker%20and%20Viola%20Priesemann%20and%20Michael%20Wibral&entry.1292438233=%20%20In%20modern%20deep%20neural%20networks%2C%20the%20learning%20dynamics%20of%20the%20individual%0Aneurons%20is%20often%20obscure%2C%20as%20the%20networks%20are%20trained%20via%20global%20optimization.%0AConversely%2C%20biological%20systems%20build%20on%20self-organized%2C%20local%20learning%2C%0Aachieving%20robustness%20and%20efficiency%20with%20limited%20global%20information.%20We%20here%0Ashow%20how%20self-organization%20between%20individual%20artificial%20neurons%20can%20be%0Aachieved%20by%20designing%20abstract%20bio-inspired%20local%20learning%20goals.%20These%20goals%0Aare%20parameterized%20using%20a%20recent%20extension%20of%20information%20theory%2C%20Partial%0AInformation%20Decomposition%20%28PID%29%2C%20which%20decomposes%20the%20information%20that%20a%20set%20of%0Ainformation%20sources%20holds%20about%20an%20outcome%20into%20unique%2C%20redundant%20and%0Asynergistic%20contributions.%20Our%20framework%20enables%20neurons%20to%20locally%20shape%20the%0Aintegration%20of%20information%20from%20various%20input%20classes%2C%20i.e.%20feedforward%2C%0Afeedback%2C%20and%20lateral%2C%20by%20selecting%20which%20of%20the%20three%20inputs%20should%20contribute%0Auniquely%2C%20redundantly%20or%20synergistically%20to%20the%20output.%20This%20selection%20is%0Aexpressed%20as%20a%20weighted%20sum%20of%20PID%20terms%2C%20which%2C%20for%20a%20given%20problem%2C%20can%20be%0Adirectly%20derived%20from%20intuitive%20reasoning%20or%20via%20numerical%20optimization%2C%0Aoffering%20a%20window%20into%20understanding%20task-relevant%20local%20information%0Aprocessing.%20Achieving%20neuron-level%20interpretability%20while%20enabling%20strong%0Aperformance%20using%20local%20learning%2C%20our%20work%20advances%20a%20principled%0Ainformation-theoretic%20foundation%20for%20local%20learning%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02482v1&entry.124074799=Read"},
{"title": "SpaCE: The Spatial Confounding Environment", "author": "Mauricio Tec and Ana Trisovic and Michelle Audirac and Sophie Woodward and Jie Kate Hu and Naeem Khoshnevis and Francesca Dominici", "abstract": "  Spatial confounding poses a significant challenge in scientific studies\ninvolving spatial data, where unobserved spatial variables can influence both\ntreatment and outcome, possibly leading to spurious associations. To address\nthis problem, we introduce SpaCE: The Spatial Confounding Environment, the\nfirst toolkit to provide realistic benchmark datasets and tools for\nsystematically evaluating causal inference methods designed to alleviate\nspatial confounding. Each dataset includes training data, true counterfactuals,\na spatial graph with coordinates, and smoothness and confounding scores\ncharacterizing the effect of a missing spatial confounder. It also includes\nrealistic semi-synthetic outcomes and counterfactuals, generated using\nstate-of-the-art machine learning ensembles, following best practices for\ncausal inference benchmarks. The datasets cover real treatment and covariates\nfrom diverse domains, including climate, health and social sciences. SpaCE\nfacilitates an automated end-to-end pipeline, simplifying data loading,\nexperimental setup, and evaluating machine learning and causal inference\nmodels. The SpaCE project provides several dozens of datasets of diverse sizes\nand spatial complexity. It is publicly available as a Python package,\nencouraging community feedback and contributions.\n", "link": "http://arxiv.org/abs/2312.00710v3", "date": "2024-12-03", "relevancy": 1.8122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaCE%3A%20The%20Spatial%20Confounding%20Environment&body=Title%3A%20SpaCE%3A%20The%20Spatial%20Confounding%20Environment%0AAuthor%3A%20Mauricio%20Tec%20and%20Ana%20Trisovic%20and%20Michelle%20Audirac%20and%20Sophie%20Woodward%20and%20Jie%20Kate%20Hu%20and%20Naeem%20Khoshnevis%20and%20Francesca%20Dominici%0AAbstract%3A%20%20%20Spatial%20confounding%20poses%20a%20significant%20challenge%20in%20scientific%20studies%0Ainvolving%20spatial%20data%2C%20where%20unobserved%20spatial%20variables%20can%20influence%20both%0Atreatment%20and%20outcome%2C%20possibly%20leading%20to%20spurious%20associations.%20To%20address%0Athis%20problem%2C%20we%20introduce%20SpaCE%3A%20The%20Spatial%20Confounding%20Environment%2C%20the%0Afirst%20toolkit%20to%20provide%20realistic%20benchmark%20datasets%20and%20tools%20for%0Asystematically%20evaluating%20causal%20inference%20methods%20designed%20to%20alleviate%0Aspatial%20confounding.%20Each%20dataset%20includes%20training%20data%2C%20true%20counterfactuals%2C%0Aa%20spatial%20graph%20with%20coordinates%2C%20and%20smoothness%20and%20confounding%20scores%0Acharacterizing%20the%20effect%20of%20a%20missing%20spatial%20confounder.%20It%20also%20includes%0Arealistic%20semi-synthetic%20outcomes%20and%20counterfactuals%2C%20generated%20using%0Astate-of-the-art%20machine%20learning%20ensembles%2C%20following%20best%20practices%20for%0Acausal%20inference%20benchmarks.%20The%20datasets%20cover%20real%20treatment%20and%20covariates%0Afrom%20diverse%20domains%2C%20including%20climate%2C%20health%20and%20social%20sciences.%20SpaCE%0Afacilitates%20an%20automated%20end-to-end%20pipeline%2C%20simplifying%20data%20loading%2C%0Aexperimental%20setup%2C%20and%20evaluating%20machine%20learning%20and%20causal%20inference%0Amodels.%20The%20SpaCE%20project%20provides%20several%20dozens%20of%20datasets%20of%20diverse%20sizes%0Aand%20spatial%20complexity.%20It%20is%20publicly%20available%20as%20a%20Python%20package%2C%0Aencouraging%20community%20feedback%20and%20contributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00710v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaCE%253A%2520The%2520Spatial%2520Confounding%2520Environment%26entry.906535625%3DMauricio%2520Tec%2520and%2520Ana%2520Trisovic%2520and%2520Michelle%2520Audirac%2520and%2520Sophie%2520Woodward%2520and%2520Jie%2520Kate%2520Hu%2520and%2520Naeem%2520Khoshnevis%2520and%2520Francesca%2520Dominici%26entry.1292438233%3D%2520%2520Spatial%2520confounding%2520poses%2520a%2520significant%2520challenge%2520in%2520scientific%2520studies%250Ainvolving%2520spatial%2520data%252C%2520where%2520unobserved%2520spatial%2520variables%2520can%2520influence%2520both%250Atreatment%2520and%2520outcome%252C%2520possibly%2520leading%2520to%2520spurious%2520associations.%2520To%2520address%250Athis%2520problem%252C%2520we%2520introduce%2520SpaCE%253A%2520The%2520Spatial%2520Confounding%2520Environment%252C%2520the%250Afirst%2520toolkit%2520to%2520provide%2520realistic%2520benchmark%2520datasets%2520and%2520tools%2520for%250Asystematically%2520evaluating%2520causal%2520inference%2520methods%2520designed%2520to%2520alleviate%250Aspatial%2520confounding.%2520Each%2520dataset%2520includes%2520training%2520data%252C%2520true%2520counterfactuals%252C%250Aa%2520spatial%2520graph%2520with%2520coordinates%252C%2520and%2520smoothness%2520and%2520confounding%2520scores%250Acharacterizing%2520the%2520effect%2520of%2520a%2520missing%2520spatial%2520confounder.%2520It%2520also%2520includes%250Arealistic%2520semi-synthetic%2520outcomes%2520and%2520counterfactuals%252C%2520generated%2520using%250Astate-of-the-art%2520machine%2520learning%2520ensembles%252C%2520following%2520best%2520practices%2520for%250Acausal%2520inference%2520benchmarks.%2520The%2520datasets%2520cover%2520real%2520treatment%2520and%2520covariates%250Afrom%2520diverse%2520domains%252C%2520including%2520climate%252C%2520health%2520and%2520social%2520sciences.%2520SpaCE%250Afacilitates%2520an%2520automated%2520end-to-end%2520pipeline%252C%2520simplifying%2520data%2520loading%252C%250Aexperimental%2520setup%252C%2520and%2520evaluating%2520machine%2520learning%2520and%2520causal%2520inference%250Amodels.%2520The%2520SpaCE%2520project%2520provides%2520several%2520dozens%2520of%2520datasets%2520of%2520diverse%2520sizes%250Aand%2520spatial%2520complexity.%2520It%2520is%2520publicly%2520available%2520as%2520a%2520Python%2520package%252C%250Aencouraging%2520community%2520feedback%2520and%2520contributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00710v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaCE%3A%20The%20Spatial%20Confounding%20Environment&entry.906535625=Mauricio%20Tec%20and%20Ana%20Trisovic%20and%20Michelle%20Audirac%20and%20Sophie%20Woodward%20and%20Jie%20Kate%20Hu%20and%20Naeem%20Khoshnevis%20and%20Francesca%20Dominici&entry.1292438233=%20%20Spatial%20confounding%20poses%20a%20significant%20challenge%20in%20scientific%20studies%0Ainvolving%20spatial%20data%2C%20where%20unobserved%20spatial%20variables%20can%20influence%20both%0Atreatment%20and%20outcome%2C%20possibly%20leading%20to%20spurious%20associations.%20To%20address%0Athis%20problem%2C%20we%20introduce%20SpaCE%3A%20The%20Spatial%20Confounding%20Environment%2C%20the%0Afirst%20toolkit%20to%20provide%20realistic%20benchmark%20datasets%20and%20tools%20for%0Asystematically%20evaluating%20causal%20inference%20methods%20designed%20to%20alleviate%0Aspatial%20confounding.%20Each%20dataset%20includes%20training%20data%2C%20true%20counterfactuals%2C%0Aa%20spatial%20graph%20with%20coordinates%2C%20and%20smoothness%20and%20confounding%20scores%0Acharacterizing%20the%20effect%20of%20a%20missing%20spatial%20confounder.%20It%20also%20includes%0Arealistic%20semi-synthetic%20outcomes%20and%20counterfactuals%2C%20generated%20using%0Astate-of-the-art%20machine%20learning%20ensembles%2C%20following%20best%20practices%20for%0Acausal%20inference%20benchmarks.%20The%20datasets%20cover%20real%20treatment%20and%20covariates%0Afrom%20diverse%20domains%2C%20including%20climate%2C%20health%20and%20social%20sciences.%20SpaCE%0Afacilitates%20an%20automated%20end-to-end%20pipeline%2C%20simplifying%20data%20loading%2C%0Aexperimental%20setup%2C%20and%20evaluating%20machine%20learning%20and%20causal%20inference%0Amodels.%20The%20SpaCE%20project%20provides%20several%20dozens%20of%20datasets%20of%20diverse%20sizes%0Aand%20spatial%20complexity.%20It%20is%20publicly%20available%20as%20a%20Python%20package%2C%0Aencouraging%20community%20feedback%20and%20contributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00710v3&entry.124074799=Read"},
{"title": "Bias Analysis of AI Models for Undergraduate Student Admissions", "author": "Kelly Van Busum and Shiaofen Fang", "abstract": "  Bias detection and mitigation is an active area of research in machine\nlearning. This work extends previous research done by the authors to provide a\nrigorous and more complete analysis of the bias found in AI predictive models.\nAdmissions data spanning six years was used to create an AI model to determine\nwhether a given student would be directly admitted into the School of Science\nunder various scenarios at a large urban research university. During this time,\nsubmission of standardized test scores as part of an application became\noptional which led to interesting questions about the impact of standardized\ntest scores on admission decisions. We developed and analyzed AI models to\nunderstand which variables are important in admissions decisions, and how the\ndecision to exclude test scores affects the demographics of the students who\nare admitted. We then evaluated the predictive models to detect and analyze\nbiases these models may carry with respect to three variables chosen to\nrepresent sensitive populations: gender, race, and whether a student was the\nfirst in his or her family to attend college. We also extended our analysis to\nshow that the biases detected were persistent. Finally, we included several\nfairness metrics in our analysis and discussed the uses and limitations of\nthese metrics.\n", "link": "http://arxiv.org/abs/2412.02528v1", "date": "2024-12-03", "relevancy": 1.7815, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4431}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias%20Analysis%20of%20AI%20Models%20for%20Undergraduate%20Student%20Admissions&body=Title%3A%20Bias%20Analysis%20of%20AI%20Models%20for%20Undergraduate%20Student%20Admissions%0AAuthor%3A%20Kelly%20Van%20Busum%20and%20Shiaofen%20Fang%0AAbstract%3A%20%20%20Bias%20detection%20and%20mitigation%20is%20an%20active%20area%20of%20research%20in%20machine%0Alearning.%20This%20work%20extends%20previous%20research%20done%20by%20the%20authors%20to%20provide%20a%0Arigorous%20and%20more%20complete%20analysis%20of%20the%20bias%20found%20in%20AI%20predictive%20models.%0AAdmissions%20data%20spanning%20six%20years%20was%20used%20to%20create%20an%20AI%20model%20to%20determine%0Awhether%20a%20given%20student%20would%20be%20directly%20admitted%20into%20the%20School%20of%20Science%0Aunder%20various%20scenarios%20at%20a%20large%20urban%20research%20university.%20During%20this%20time%2C%0Asubmission%20of%20standardized%20test%20scores%20as%20part%20of%20an%20application%20became%0Aoptional%20which%20led%20to%20interesting%20questions%20about%20the%20impact%20of%20standardized%0Atest%20scores%20on%20admission%20decisions.%20We%20developed%20and%20analyzed%20AI%20models%20to%0Aunderstand%20which%20variables%20are%20important%20in%20admissions%20decisions%2C%20and%20how%20the%0Adecision%20to%20exclude%20test%20scores%20affects%20the%20demographics%20of%20the%20students%20who%0Aare%20admitted.%20We%20then%20evaluated%20the%20predictive%20models%20to%20detect%20and%20analyze%0Abiases%20these%20models%20may%20carry%20with%20respect%20to%20three%20variables%20chosen%20to%0Arepresent%20sensitive%20populations%3A%20gender%2C%20race%2C%20and%20whether%20a%20student%20was%20the%0Afirst%20in%20his%20or%20her%20family%20to%20attend%20college.%20We%20also%20extended%20our%20analysis%20to%0Ashow%20that%20the%20biases%20detected%20were%20persistent.%20Finally%2C%20we%20included%20several%0Afairness%20metrics%20in%20our%20analysis%20and%20discussed%20the%20uses%20and%20limitations%20of%0Athese%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias%2520Analysis%2520of%2520AI%2520Models%2520for%2520Undergraduate%2520Student%2520Admissions%26entry.906535625%3DKelly%2520Van%2520Busum%2520and%2520Shiaofen%2520Fang%26entry.1292438233%3D%2520%2520Bias%2520detection%2520and%2520mitigation%2520is%2520an%2520active%2520area%2520of%2520research%2520in%2520machine%250Alearning.%2520This%2520work%2520extends%2520previous%2520research%2520done%2520by%2520the%2520authors%2520to%2520provide%2520a%250Arigorous%2520and%2520more%2520complete%2520analysis%2520of%2520the%2520bias%2520found%2520in%2520AI%2520predictive%2520models.%250AAdmissions%2520data%2520spanning%2520six%2520years%2520was%2520used%2520to%2520create%2520an%2520AI%2520model%2520to%2520determine%250Awhether%2520a%2520given%2520student%2520would%2520be%2520directly%2520admitted%2520into%2520the%2520School%2520of%2520Science%250Aunder%2520various%2520scenarios%2520at%2520a%2520large%2520urban%2520research%2520university.%2520During%2520this%2520time%252C%250Asubmission%2520of%2520standardized%2520test%2520scores%2520as%2520part%2520of%2520an%2520application%2520became%250Aoptional%2520which%2520led%2520to%2520interesting%2520questions%2520about%2520the%2520impact%2520of%2520standardized%250Atest%2520scores%2520on%2520admission%2520decisions.%2520We%2520developed%2520and%2520analyzed%2520AI%2520models%2520to%250Aunderstand%2520which%2520variables%2520are%2520important%2520in%2520admissions%2520decisions%252C%2520and%2520how%2520the%250Adecision%2520to%2520exclude%2520test%2520scores%2520affects%2520the%2520demographics%2520of%2520the%2520students%2520who%250Aare%2520admitted.%2520We%2520then%2520evaluated%2520the%2520predictive%2520models%2520to%2520detect%2520and%2520analyze%250Abiases%2520these%2520models%2520may%2520carry%2520with%2520respect%2520to%2520three%2520variables%2520chosen%2520to%250Arepresent%2520sensitive%2520populations%253A%2520gender%252C%2520race%252C%2520and%2520whether%2520a%2520student%2520was%2520the%250Afirst%2520in%2520his%2520or%2520her%2520family%2520to%2520attend%2520college.%2520We%2520also%2520extended%2520our%2520analysis%2520to%250Ashow%2520that%2520the%2520biases%2520detected%2520were%2520persistent.%2520Finally%252C%2520we%2520included%2520several%250Afairness%2520metrics%2520in%2520our%2520analysis%2520and%2520discussed%2520the%2520uses%2520and%2520limitations%2520of%250Athese%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias%20Analysis%20of%20AI%20Models%20for%20Undergraduate%20Student%20Admissions&entry.906535625=Kelly%20Van%20Busum%20and%20Shiaofen%20Fang&entry.1292438233=%20%20Bias%20detection%20and%20mitigation%20is%20an%20active%20area%20of%20research%20in%20machine%0Alearning.%20This%20work%20extends%20previous%20research%20done%20by%20the%20authors%20to%20provide%20a%0Arigorous%20and%20more%20complete%20analysis%20of%20the%20bias%20found%20in%20AI%20predictive%20models.%0AAdmissions%20data%20spanning%20six%20years%20was%20used%20to%20create%20an%20AI%20model%20to%20determine%0Awhether%20a%20given%20student%20would%20be%20directly%20admitted%20into%20the%20School%20of%20Science%0Aunder%20various%20scenarios%20at%20a%20large%20urban%20research%20university.%20During%20this%20time%2C%0Asubmission%20of%20standardized%20test%20scores%20as%20part%20of%20an%20application%20became%0Aoptional%20which%20led%20to%20interesting%20questions%20about%20the%20impact%20of%20standardized%0Atest%20scores%20on%20admission%20decisions.%20We%20developed%20and%20analyzed%20AI%20models%20to%0Aunderstand%20which%20variables%20are%20important%20in%20admissions%20decisions%2C%20and%20how%20the%0Adecision%20to%20exclude%20test%20scores%20affects%20the%20demographics%20of%20the%20students%20who%0Aare%20admitted.%20We%20then%20evaluated%20the%20predictive%20models%20to%20detect%20and%20analyze%0Abiases%20these%20models%20may%20carry%20with%20respect%20to%20three%20variables%20chosen%20to%0Arepresent%20sensitive%20populations%3A%20gender%2C%20race%2C%20and%20whether%20a%20student%20was%20the%0Afirst%20in%20his%20or%20her%20family%20to%20attend%20college.%20We%20also%20extended%20our%20analysis%20to%0Ashow%20that%20the%20biases%20detected%20were%20persistent.%20Finally%2C%20we%20included%20several%0Afairness%20metrics%20in%20our%20analysis%20and%20discussed%20the%20uses%20and%20limitations%20of%0Athese%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02528v1&entry.124074799=Read"},
{"title": "A Novel Approach to Comprehending Users' Preferences for Accurate\n  Personalized News Recommendation", "author": "Yunyong Ko and Seongeun Ryu and Sang-Wook Kim", "abstract": "  Personalized news recommendation aims to assist users in finding news\narticles that align with their interests, which plays a pivotal role in\nmitigating users' information overload problem. Although many recent works have\nbeen studied for better personalized news recommendation, the following\nchallenges should be explored more: (C1) Comprehending manifold intents coupled\nwithin a news article, (C2) Differentiating varying post-read preferences of\nnews articles, and (C3) Addressing the cold-start user problem. To tackle the\naforementioned challenges together, in this paper, we propose a novel\npersonalized news recommendation framework (CROWN) that employs (1)\ncategory-guided intent disentanglement for (C1), (2) consistency-based news\nrepresentation for (C2), and (3) GNN-enhanced hybrid user representation for\n(C3). Furthermore, we incorporate a category prediction into the training\nprocess of CROWN as an auxiliary task, which provides supplementary supervisory\nsignals to enhance intent disentanglement. Extensive experiments on two\nreal-world datasets reveal that (1) CROWN provides consistent performance\nimprovements over ten state-of-the-art news recommendation methods and (2) the\nproposed strategies significantly improve the accuracy of CROWN.\n", "link": "http://arxiv.org/abs/2310.09401v4", "date": "2024-12-03", "relevancy": 1.8071, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4547}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4519}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Approach%20to%20Comprehending%20Users%27%20Preferences%20for%20Accurate%0A%20%20Personalized%20News%20Recommendation&body=Title%3A%20A%20Novel%20Approach%20to%20Comprehending%20Users%27%20Preferences%20for%20Accurate%0A%20%20Personalized%20News%20Recommendation%0AAuthor%3A%20Yunyong%20Ko%20and%20Seongeun%20Ryu%20and%20Sang-Wook%20Kim%0AAbstract%3A%20%20%20Personalized%20news%20recommendation%20aims%20to%20assist%20users%20in%20finding%20news%0Aarticles%20that%20align%20with%20their%20interests%2C%20which%20plays%20a%20pivotal%20role%20in%0Amitigating%20users%27%20information%20overload%20problem.%20Although%20many%20recent%20works%20have%0Abeen%20studied%20for%20better%20personalized%20news%20recommendation%2C%20the%20following%0Achallenges%20should%20be%20explored%20more%3A%20%28C1%29%20Comprehending%20manifold%20intents%20coupled%0Awithin%20a%20news%20article%2C%20%28C2%29%20Differentiating%20varying%20post-read%20preferences%20of%0Anews%20articles%2C%20and%20%28C3%29%20Addressing%20the%20cold-start%20user%20problem.%20To%20tackle%20the%0Aaforementioned%20challenges%20together%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Apersonalized%20news%20recommendation%20framework%20%28CROWN%29%20that%20employs%20%281%29%0Acategory-guided%20intent%20disentanglement%20for%20%28C1%29%2C%20%282%29%20consistency-based%20news%0Arepresentation%20for%20%28C2%29%2C%20and%20%283%29%20GNN-enhanced%20hybrid%20user%20representation%20for%0A%28C3%29.%20Furthermore%2C%20we%20incorporate%20a%20category%20prediction%20into%20the%20training%0Aprocess%20of%20CROWN%20as%20an%20auxiliary%20task%2C%20which%20provides%20supplementary%20supervisory%0Asignals%20to%20enhance%20intent%20disentanglement.%20Extensive%20experiments%20on%20two%0Areal-world%20datasets%20reveal%20that%20%281%29%20CROWN%20provides%20consistent%20performance%0Aimprovements%20over%20ten%20state-of-the-art%20news%20recommendation%20methods%20and%20%282%29%20the%0Aproposed%20strategies%20significantly%20improve%20the%20accuracy%20of%20CROWN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09401v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Approach%2520to%2520Comprehending%2520Users%2527%2520Preferences%2520for%2520Accurate%250A%2520%2520Personalized%2520News%2520Recommendation%26entry.906535625%3DYunyong%2520Ko%2520and%2520Seongeun%2520Ryu%2520and%2520Sang-Wook%2520Kim%26entry.1292438233%3D%2520%2520Personalized%2520news%2520recommendation%2520aims%2520to%2520assist%2520users%2520in%2520finding%2520news%250Aarticles%2520that%2520align%2520with%2520their%2520interests%252C%2520which%2520plays%2520a%2520pivotal%2520role%2520in%250Amitigating%2520users%2527%2520information%2520overload%2520problem.%2520Although%2520many%2520recent%2520works%2520have%250Abeen%2520studied%2520for%2520better%2520personalized%2520news%2520recommendation%252C%2520the%2520following%250Achallenges%2520should%2520be%2520explored%2520more%253A%2520%2528C1%2529%2520Comprehending%2520manifold%2520intents%2520coupled%250Awithin%2520a%2520news%2520article%252C%2520%2528C2%2529%2520Differentiating%2520varying%2520post-read%2520preferences%2520of%250Anews%2520articles%252C%2520and%2520%2528C3%2529%2520Addressing%2520the%2520cold-start%2520user%2520problem.%2520To%2520tackle%2520the%250Aaforementioned%2520challenges%2520together%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Apersonalized%2520news%2520recommendation%2520framework%2520%2528CROWN%2529%2520that%2520employs%2520%25281%2529%250Acategory-guided%2520intent%2520disentanglement%2520for%2520%2528C1%2529%252C%2520%25282%2529%2520consistency-based%2520news%250Arepresentation%2520for%2520%2528C2%2529%252C%2520and%2520%25283%2529%2520GNN-enhanced%2520hybrid%2520user%2520representation%2520for%250A%2528C3%2529.%2520Furthermore%252C%2520we%2520incorporate%2520a%2520category%2520prediction%2520into%2520the%2520training%250Aprocess%2520of%2520CROWN%2520as%2520an%2520auxiliary%2520task%252C%2520which%2520provides%2520supplementary%2520supervisory%250Asignals%2520to%2520enhance%2520intent%2520disentanglement.%2520Extensive%2520experiments%2520on%2520two%250Areal-world%2520datasets%2520reveal%2520that%2520%25281%2529%2520CROWN%2520provides%2520consistent%2520performance%250Aimprovements%2520over%2520ten%2520state-of-the-art%2520news%2520recommendation%2520methods%2520and%2520%25282%2529%2520the%250Aproposed%2520strategies%2520significantly%2520improve%2520the%2520accuracy%2520of%2520CROWN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09401v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Approach%20to%20Comprehending%20Users%27%20Preferences%20for%20Accurate%0A%20%20Personalized%20News%20Recommendation&entry.906535625=Yunyong%20Ko%20and%20Seongeun%20Ryu%20and%20Sang-Wook%20Kim&entry.1292438233=%20%20Personalized%20news%20recommendation%20aims%20to%20assist%20users%20in%20finding%20news%0Aarticles%20that%20align%20with%20their%20interests%2C%20which%20plays%20a%20pivotal%20role%20in%0Amitigating%20users%27%20information%20overload%20problem.%20Although%20many%20recent%20works%20have%0Abeen%20studied%20for%20better%20personalized%20news%20recommendation%2C%20the%20following%0Achallenges%20should%20be%20explored%20more%3A%20%28C1%29%20Comprehending%20manifold%20intents%20coupled%0Awithin%20a%20news%20article%2C%20%28C2%29%20Differentiating%20varying%20post-read%20preferences%20of%0Anews%20articles%2C%20and%20%28C3%29%20Addressing%20the%20cold-start%20user%20problem.%20To%20tackle%20the%0Aaforementioned%20challenges%20together%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Apersonalized%20news%20recommendation%20framework%20%28CROWN%29%20that%20employs%20%281%29%0Acategory-guided%20intent%20disentanglement%20for%20%28C1%29%2C%20%282%29%20consistency-based%20news%0Arepresentation%20for%20%28C2%29%2C%20and%20%283%29%20GNN-enhanced%20hybrid%20user%20representation%20for%0A%28C3%29.%20Furthermore%2C%20we%20incorporate%20a%20category%20prediction%20into%20the%20training%0Aprocess%20of%20CROWN%20as%20an%20auxiliary%20task%2C%20which%20provides%20supplementary%20supervisory%0Asignals%20to%20enhance%20intent%20disentanglement.%20Extensive%20experiments%20on%20two%0Areal-world%20datasets%20reveal%20that%20%281%29%20CROWN%20provides%20consistent%20performance%0Aimprovements%20over%20ten%20state-of-the-art%20news%20recommendation%20methods%20and%20%282%29%20the%0Aproposed%20strategies%20significantly%20improve%20the%20accuracy%20of%20CROWN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09401v4&entry.124074799=Read"},
{"title": "Haptic Stiffness Perception Using Hand Exoskeletons in Tactile Robotic\n  Telemanipulation", "author": "Gabriele Giudici and Claudio Coppola and Kaspar Althoefer and Ildar Farkhatdinov and Lorenzo Jamone", "abstract": "  Robotic telemanipulation - the human-guided manipulation of remote objects -\nplays a pivotal role in several applications, from healthcare to operations in\nharsh environments. While visual feedback from cameras can provide valuable\ninformation to the human operator, haptic feedback is essential for accessing\nspecific object properties that are difficult to be perceived by vision, such\nas stiffness. For the first time, we present a participant study demonstrating\nthat operators can perceive the stiffness of remote objects during real-world\ntelemanipulation with a dexterous robotic hand, when haptic feedback is\ngenerated from tactile sensing fingertips. Participants were tasked with\nsqueezing soft objects by teleoperating a robotic hand, using two methods of\nhaptic feedback: one based solely on the measured contact force, while the\nsecond also includes the squeezing displacement between the leader and follower\ndevices. Our results demonstrate that operators are indeed capable of\ndiscriminating objects of different stiffness, relying on haptic feedback alone\nand without any visual feedback. Additionally, our findings suggest that the\ndisplacement feedback component may enhance discrimination with objects of\nsimilar stiffness.\n", "link": "http://arxiv.org/abs/2412.02613v1", "date": "2024-12-03", "relevancy": 1.594, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5168}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Haptic%20Stiffness%20Perception%20Using%20Hand%20Exoskeletons%20in%20Tactile%20Robotic%0A%20%20Telemanipulation&body=Title%3A%20Haptic%20Stiffness%20Perception%20Using%20Hand%20Exoskeletons%20in%20Tactile%20Robotic%0A%20%20Telemanipulation%0AAuthor%3A%20Gabriele%20Giudici%20and%20Claudio%20Coppola%20and%20Kaspar%20Althoefer%20and%20Ildar%20Farkhatdinov%20and%20Lorenzo%20Jamone%0AAbstract%3A%20%20%20Robotic%20telemanipulation%20-%20the%20human-guided%20manipulation%20of%20remote%20objects%20-%0Aplays%20a%20pivotal%20role%20in%20several%20applications%2C%20from%20healthcare%20to%20operations%20in%0Aharsh%20environments.%20While%20visual%20feedback%20from%20cameras%20can%20provide%20valuable%0Ainformation%20to%20the%20human%20operator%2C%20haptic%20feedback%20is%20essential%20for%20accessing%0Aspecific%20object%20properties%20that%20are%20difficult%20to%20be%20perceived%20by%20vision%2C%20such%0Aas%20stiffness.%20For%20the%20first%20time%2C%20we%20present%20a%20participant%20study%20demonstrating%0Athat%20operators%20can%20perceive%20the%20stiffness%20of%20remote%20objects%20during%20real-world%0Atelemanipulation%20with%20a%20dexterous%20robotic%20hand%2C%20when%20haptic%20feedback%20is%0Agenerated%20from%20tactile%20sensing%20fingertips.%20Participants%20were%20tasked%20with%0Asqueezing%20soft%20objects%20by%20teleoperating%20a%20robotic%20hand%2C%20using%20two%20methods%20of%0Ahaptic%20feedback%3A%20one%20based%20solely%20on%20the%20measured%20contact%20force%2C%20while%20the%0Asecond%20also%20includes%20the%20squeezing%20displacement%20between%20the%20leader%20and%20follower%0Adevices.%20Our%20results%20demonstrate%20that%20operators%20are%20indeed%20capable%20of%0Adiscriminating%20objects%20of%20different%20stiffness%2C%20relying%20on%20haptic%20feedback%20alone%0Aand%20without%20any%20visual%20feedback.%20Additionally%2C%20our%20findings%20suggest%20that%20the%0Adisplacement%20feedback%20component%20may%20enhance%20discrimination%20with%20objects%20of%0Asimilar%20stiffness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHaptic%2520Stiffness%2520Perception%2520Using%2520Hand%2520Exoskeletons%2520in%2520Tactile%2520Robotic%250A%2520%2520Telemanipulation%26entry.906535625%3DGabriele%2520Giudici%2520and%2520Claudio%2520Coppola%2520and%2520Kaspar%2520Althoefer%2520and%2520Ildar%2520Farkhatdinov%2520and%2520Lorenzo%2520Jamone%26entry.1292438233%3D%2520%2520Robotic%2520telemanipulation%2520-%2520the%2520human-guided%2520manipulation%2520of%2520remote%2520objects%2520-%250Aplays%2520a%2520pivotal%2520role%2520in%2520several%2520applications%252C%2520from%2520healthcare%2520to%2520operations%2520in%250Aharsh%2520environments.%2520While%2520visual%2520feedback%2520from%2520cameras%2520can%2520provide%2520valuable%250Ainformation%2520to%2520the%2520human%2520operator%252C%2520haptic%2520feedback%2520is%2520essential%2520for%2520accessing%250Aspecific%2520object%2520properties%2520that%2520are%2520difficult%2520to%2520be%2520perceived%2520by%2520vision%252C%2520such%250Aas%2520stiffness.%2520For%2520the%2520first%2520time%252C%2520we%2520present%2520a%2520participant%2520study%2520demonstrating%250Athat%2520operators%2520can%2520perceive%2520the%2520stiffness%2520of%2520remote%2520objects%2520during%2520real-world%250Atelemanipulation%2520with%2520a%2520dexterous%2520robotic%2520hand%252C%2520when%2520haptic%2520feedback%2520is%250Agenerated%2520from%2520tactile%2520sensing%2520fingertips.%2520Participants%2520were%2520tasked%2520with%250Asqueezing%2520soft%2520objects%2520by%2520teleoperating%2520a%2520robotic%2520hand%252C%2520using%2520two%2520methods%2520of%250Ahaptic%2520feedback%253A%2520one%2520based%2520solely%2520on%2520the%2520measured%2520contact%2520force%252C%2520while%2520the%250Asecond%2520also%2520includes%2520the%2520squeezing%2520displacement%2520between%2520the%2520leader%2520and%2520follower%250Adevices.%2520Our%2520results%2520demonstrate%2520that%2520operators%2520are%2520indeed%2520capable%2520of%250Adiscriminating%2520objects%2520of%2520different%2520stiffness%252C%2520relying%2520on%2520haptic%2520feedback%2520alone%250Aand%2520without%2520any%2520visual%2520feedback.%2520Additionally%252C%2520our%2520findings%2520suggest%2520that%2520the%250Adisplacement%2520feedback%2520component%2520may%2520enhance%2520discrimination%2520with%2520objects%2520of%250Asimilar%2520stiffness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Haptic%20Stiffness%20Perception%20Using%20Hand%20Exoskeletons%20in%20Tactile%20Robotic%0A%20%20Telemanipulation&entry.906535625=Gabriele%20Giudici%20and%20Claudio%20Coppola%20and%20Kaspar%20Althoefer%20and%20Ildar%20Farkhatdinov%20and%20Lorenzo%20Jamone&entry.1292438233=%20%20Robotic%20telemanipulation%20-%20the%20human-guided%20manipulation%20of%20remote%20objects%20-%0Aplays%20a%20pivotal%20role%20in%20several%20applications%2C%20from%20healthcare%20to%20operations%20in%0Aharsh%20environments.%20While%20visual%20feedback%20from%20cameras%20can%20provide%20valuable%0Ainformation%20to%20the%20human%20operator%2C%20haptic%20feedback%20is%20essential%20for%20accessing%0Aspecific%20object%20properties%20that%20are%20difficult%20to%20be%20perceived%20by%20vision%2C%20such%0Aas%20stiffness.%20For%20the%20first%20time%2C%20we%20present%20a%20participant%20study%20demonstrating%0Athat%20operators%20can%20perceive%20the%20stiffness%20of%20remote%20objects%20during%20real-world%0Atelemanipulation%20with%20a%20dexterous%20robotic%20hand%2C%20when%20haptic%20feedback%20is%0Agenerated%20from%20tactile%20sensing%20fingertips.%20Participants%20were%20tasked%20with%0Asqueezing%20soft%20objects%20by%20teleoperating%20a%20robotic%20hand%2C%20using%20two%20methods%20of%0Ahaptic%20feedback%3A%20one%20based%20solely%20on%20the%20measured%20contact%20force%2C%20while%20the%0Asecond%20also%20includes%20the%20squeezing%20displacement%20between%20the%20leader%20and%20follower%0Adevices.%20Our%20results%20demonstrate%20that%20operators%20are%20indeed%20capable%20of%0Adiscriminating%20objects%20of%20different%20stiffness%2C%20relying%20on%20haptic%20feedback%20alone%0Aand%20without%20any%20visual%20feedback.%20Additionally%2C%20our%20findings%20suggest%20that%20the%0Adisplacement%20feedback%20component%20may%20enhance%20discrimination%20with%20objects%20of%0Asimilar%20stiffness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02613v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


